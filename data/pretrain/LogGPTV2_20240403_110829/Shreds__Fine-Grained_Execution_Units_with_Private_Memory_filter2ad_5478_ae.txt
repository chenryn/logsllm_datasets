Application   Shred 
numbers 
Code 
change(SLoC) 
Adoption 
time(min) 
curl 
minizip 
openssh 
openssl 
lighttpd 
2 
4 
1 
3 
2 
13 
23 
8 
34 
27 
30 
15 
20 
35 
60 
Compilation Tests: To test the performance and compatibility
of our ofﬂine analysis and compilation methods, we instru-
mented S-compiler in order to measure the overhead and log
potential errors, if any, while building the 5 software packages
that use shreds. Figure 5 shows the time and space overhead
introduced by S-compiler, relative to the performance of a
vanilla LLVM Clang compiling the unchanged applications.
On average, S-compiler delays the building process by 24.58%
and results in a 7.37% increase in executable sizes. The seem-
ingly signiﬁcant delays in compilation are in fact on par with
static analysis and program instrumentation tools of similar
scale. They are generally tolerable because compilations take
place ofﬂine in background and are usually not considered to
be time-critical. The executable ﬁle size increases are mainly
resulted from the in-shred instrumentation and are below 2%
except for the outliers. We encountered no error when building
these applications using S-compiler. The built applications run
without issues during the course of the tests.
compile time 
increase
generated file size 
increase
40.00%
35.00%
30.00%
25.00%
20.00%
15.00%
10.00%
5.00%
0.00%
curl
minizip
openssh
openssl
lighttpd
Fig. 5: The time and space overhead incurred by S-compiler
during the ofﬂine compilation and instrumentation phase
Performance Tests: This group of tests examines the run-
time performance of shreds and s-pools. We performed both
micro-benchmarkings and end-to-end tests, which respectively
reveal the performance cost associated with shreds’ critical
operations and the overhead exhibited in the 5 applications
retroﬁtted with shreds.
In the micro-benchmarking tests, we developed unit test
programs that force shreds to go through the critical operations
and state changes, including shred entry, exit, and context
switch. We measured the duration of these operations and state
changes, and then compared them with the durations of the
equivalent or related operations without shreds. Figure 6 shows
the absolute time needed for a context switch that preempts
a shred-active thread, a regular thread, and a regular process,
respectively. It is obvious that, switching shred-active threads
is marginally more expensive than switching regular threads
(about 100μs slower); switching shred-active threads is much
faster than a process context switch. This is because when a
shred is preempted, S-driver does not need to make any change
to page tables or TLB. Instead, it only performs a single DACR
reset operation, which is very lightweight.
6666
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:20 UTC from IEEE Xplore.  Restrictions apply. 
We also compared the time needed for completing the shred
API calls (invoking ioctl internally) with several reference
system calls, as shown in Figure 7. The getpid, one of the
fastest system calls, serves as a baseline for comparison. The
shred enter API is compared with the clone system call
(without address space change), and is slightly faster, which
means creating a shred takes less time than creating a thread.
The s-pool allocation API is mildly slower than mmap due to
the additional domain conﬁgurations. But the overhead is low
enough to easily blend in the typical program performance
ﬂuctuations.
Furthermore, we measured the performance improvement
enabled by the lazy domain adjustment optimization. We
applied shreds to ﬁve SPEC CINT2006 benchmark programs
written in C (Figure 8), where a number of shreds were created
to perform intensive access to s-pools. We note that
this
test is designed only for the performance evaluation purpose
while recognizing that these benchmark programs do not need
shreds’ protection. The result shows that in all but one case
the optimization brings the overhead under 1% whereas the
non-optimized implementation of shreds incurs an average
overhead of 2.5%.
Those micro-benchmark tests together indicate that
the
shred primitives are lightweight and the performance impact
that shred state changes and s-pool operations may pose to the
application or the system is very mild.
)
s
μ
(
h
c
t
i
w
s
t
x
e
t
n
o
c
/
e
m
T
i
18500
18000
17500
17000
16500
16000
15500
15000
shred 
switch
thread   
switch
process 
switch
Fig. 6: The time needed for a context switch when: (1) a shred-
active thread is switched off, (2) a regular thread is switched
off but no process or address space change, and (3) a regular
thread is switched off and a thread from a different process is
scheduled on.
In the end-to-end tests, we let each of the 5 open-source
applications perform a self-contained task twice, with and
without using shreds to protect their secret data (e.g., Lighttpd
fully handling an HTTP auth login and OpenSSL carrying
out a complete RSA key veriﬁcation). We instrumented the
applications with timers. For each application, we manually
drove it to perform the task, which fully exercises the added
shreds. We measured both the time and space costs associated
6767
user time (μs)
sys time (μs)
2.10E6
2.06E6
1000
800
600
400
200
0
Fig. 7: Invocation time of shred APIs and reference system
calls (the right-most two bars are on log scale). It shows that
shred entry is faster than thread creation, and s-pool allocation
is slightly slower than basic memory mapping.
no shred
shred-no lazy
shred lazy
1.1
1.05
1
0.95
0.9
0.85
0.8
0.75
Fig. 8: Five SPEC2000 benchmark programs tested when: (1)
no shred is used, (2) shreds are used but without the lazy
domain adjustment turned on in S-driver, and (3) shreds are
used with the lazy domain adjustment.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:20 UTC from IEEE Xplore.  Restrictions apply. 
with using shreds in these tests. The absolute costs and the
relative increases are shown in Table IV. On average, the
per-task slow down among the applications is 4.67% and the
memory footprint increase is 7.26%. The results show that
shreds are practical for real applications of various sizes and
functionalities. The overhead is hardly noticeable to the end
users of the applications.
Security Coverage Test: Finally, we tested the coverage of
shred protection in the 5 modiﬁed applications. These tests
not only check if the shred adoption is correct and complete
in these applications, but also demonstrate the security beneﬁts
uniquely enabled by shreds in these applications. We con-
ducted these tests using a simple memory scraper that scans
each application’s virtual memory in search for the known
secrets. The tests simulate the most powerful in-process abuse
where an adversary has full visibility into the user-space virtual
memory of the application and can perform brute-force search
of secrets. For each application, our memory scraper runs as
an independent thread inside the application and veriﬁes if
any instance of the secret data can be found in memory via a
value-based exhaustive search. We ran this test in two rounds,
one on a vanilla version of the application and the other on
the shred-enabled version.
In the ﬁrst round, where shreds are not used, the memory
scraper found at lease one instance of the secret values in
memory for all the applications, which means that these secrets
are subject to in-process abuse. In the second round, where
shreds are used,
the memory scraper failed to detect any
secrete in the applications’ memory, which means that the
secrets are well contained inside the s-pools and protected
from in-process abuse. The results show that, the applications
have correctly adopted shreds for processing the secret data in
memory and stored such data only in s-pools. Moreover, the
tests show that, without signiﬁcant design changes, applying
the shred primitives in these real applications creates needed
protection for the otherwise vulnerable passwords, crypto keys,
and user credentials.
VI. RELATED WORK
Our system is related to the following lines of works, in
terms of the addressed problems or the employed techniques.
Program module isolation: The previous works have studied
the problem of isolating the executions of mutually distrusting
modules, ranging from libraries in user-space programs to
drivers in the OS. SFI [13] and its variants [17], [18] establish
strict boundaries in memory space to isolate potentially faulty
modules and therefore contain the impact resulted from the
crashes or malfunctions of such modules. SFI has also been
extended to build sandboxes for untrusted plugins and libraries
on both x86 [19], [20] and ARM [21], [22]. Extending module
isolation into kernel-space, some previous works [18], [23]
contain faulty drivers as well as user-space modules. Unlike
these works, which focus on fault isolation or sandboxing, our
work aims to prevent the in-process memory abuse launched
by either vulnerable or malicious code. Our work allows devel-
opers to run sensitive code in ﬂexibly-deﬁned and lightweight
execution units (i.e., shreds), where the code has exclusive
access to private memory pools, in addition to the regular
memory regions, and the execution is protected from other
code running (concurrently) in the same address space. The
aforementioned works require veriﬁcation and instrumentation
of all untrusted code modules, whereas our work only needs
to analyze and harden trusted in-shred code. We repurpose
the ARM memory domain to efﬁciently realize the design
of shreds and the protection against in-process abuse. Fur-
thermore, SFI and similar techniques assume that isolated
modules should be logically independent and not
interact
closely, whereas shreds neither impose such restrictions nor
incur additional overhead when accessing regular memory,
invoking third-party library functions, or making system calls.
Process- and thread-level isolation: Arranging program com-
ponents into different processes has long been advocated
as a practical approach to achieving privilege and memory
separation [8]–[10]. Many widely used software, such as
OpenSSH and Chrome, have adopted this approach. Separated
components run in their own address spaces and are immune
from memory abuse by other components. However, process
separation faces three major limitations when being used for
defending memory abuse. First, due to the coarse granular-
ity of a process, memory abuse may still happen inside a
component process as a result of a library call or a code
injection, as shown in several real attacks on Chrome. Sec-
ond, using process separation usually requires major software
design changes due to the added concurrency and restrictions,
which prevents wide adoption. Third, process separation can
cause high overhead, particularly when separated components
frequently interact. Wedge enables thread-level memory isola-
tion [3]. While incurring slightly lower overhead than process-
level isolation, it still suffers from the ﬁxed granularity and
require major software changes to be adopted. In comparison,
shreds are ﬂexibly grained and easy to adopt. Shreds are also
more efﬁcient because, unlike the aforementioned works, our
design does not rely on the heavy paging-based memory access
control.
Protected execution environments: A number of systems
were proposed for securely executing sensitive code or per-
forming privileged tasks. Flicker [24] allows for trusted code
execution in full isolation to OS or even BIOS and provides
remote attestation. TrustVisor [25] improves on performance
and granularity with a special-purpose hypervisor. SeCage [11]
runs sensitive code in a secure VM. SICE [26] protects
sensitive workloads purely at the hardware level and supports
current execution on multicore platforms. SGX [12], an up-
coming feature in Intel CPUs, allows user-space programs to
create so-called enclaves where sensitive code can run securely
but has little access to system resources or application context.
In general, these systems are designed for self-contained code
that can run independently in isolated or constrained environ-
6868
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:20 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IV: End-to-end overhead observed while tested programs performing a complete task: the left-side part of the table
shows the executing time and the right-side part shows the memory footprint.