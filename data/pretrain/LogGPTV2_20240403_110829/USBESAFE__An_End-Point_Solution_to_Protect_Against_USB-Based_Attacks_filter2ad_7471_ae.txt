### References

1. **NICOLAS FALLIERE, LIAM O MURCHU, ERIC CHIEN.**  
   *W32. Stuxnet Dossier.*  
   [http://www.bbc.com/news/technology-36478650](http://www.bbc.com/news/technology-36478650), 2011.

2. **PAXSON, V., AND FLOYD, S.**  
   *Wide area traffic: the failure of Poisson modeling.*  
   IEEE/ACM Transactions on Networking (ToN) 3, 3 (1995), 226–244.

3. **PHAM, D. V., HALGAMUGE, M. N., SYED, A., AND MENDIS, P.**  
   *Optimizing Windows security features to block malware and hack tools on USB storage devices.*  
   In Progress in Electromagnetics Research Symposium (2010), pp. 350–355.

4. **RICH, D.**  
   *Authentication in transient storage device attachments.*  
   Computer 40, 4 (2007).

5. **SCHUMILO, S., AND SPENNEBERG, R.**  
   *Don’t trust your USB! How to find bugs in USB device drivers.*

6. **SHIN, S., AND GU, G.**  
   *Conficker and beyond: a large-scale empirical study.*  
   In Proceedings of the 26th Annual Computer Security Applications Conference (2010), ACM, pp. 151–160.

7. **TETMEYER, A., AND SAIEDIAN, H.**  
   *Security threats and mitigating risk for USB devices.*  
   IEEE Technology and Society Magazine 29, 4 (2010), 44–49.

8. **TIAN, D. J., BATES, A., AND BUTLER, K.**  
   *Defending against malicious USB firmware with GoodUSB.*  
   In Proceedings of the 31st Annual Computer Security Applications Conference (2015), ACM, pp. 261–270.

9. **TIAN, D. J., BATES, A., BUTLER, K. R., AND RANGASWAMI, R.**  
   *ProvUSB: Block-level provenance-based data protection for USB storage devices.*  
   In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (2016), CCS ’16, pp. 242–253.

10. **TIAN, D. J., SCAIFE, N., BATES, A., BUTLER, K., AND TRAYNOR, P.**  
    *Making USB great again with USBFilter.*  
    In Proceedings of the USENIX Security Symposium (2016).

11. **TIAN, J., SCAIFE, N., KUMAR, D., BAILEY, M., BATES, A., AND BUTLER, K.**  
    *SOK: "Plug & Pray" today - understanding USB insecurity in versions 1 through C.*  
    In 2018 IEEE Symposium on Security and Privacy (SP), vol. 00, pp. 613–628.

12. **TISCHER, M., DURUMERIC, Z., FOSTER, S., DUAN, S., MORI, A., BURSZTEIN, E., AND BAILEY, M.**  
    *Users Really Do Plug in USB Drives They Find.*  
    In Proceedings of the 37th IEEE Symposium on Security and Privacy (S&P ’16) (San Jose, California, USA, May 2016).

13. **WATSON, G. S., AND DURBIN, J.**  
    *Exact tests of serial correlation using noncircular statistics.*  
    The Annals of Mathematical Statistics (1951), 446–451.

14. **YANG, B., QIN, Y., ZHANG, Y., WANG, W., AND FENG, D.**  
    *TMSUI: A trust management scheme of USB storage devices for industrial control systems.*  
    In International Conference on Information and Communications Security (2015), Springer, pp. 152–168.

15. **ZHUANG, L., AND DAI, H.**  
    *Parameter optimization of kernel-based one-class classifier on imbalance learning.*  
    Journal of Computers 1, 7 (2006), 32–40.

### Model Search Configurations

Table 6 represents all possible configurations for training the detection model.

| Configuration Setting | Values |
|-----------------------|--------|
| Error Upper Bound (ν) | [0.01, 0.25, 0.5, 0.75, 1] |
| Kernel Coefficient (γ) | [0.1, 0.01, 0.001, 0.0001] |
| Degree of Polynomial | [1, 2, 3] |
| Kernel Functions | RBF, ν,γ; Sigmoid, ν,γ; Linear, ν; Polynomial, ν,γ, degree |

| Kernel Function | Total Configurations |
|-----------------|----------------------|
| RBF, ν,γ        | 20                   |
| Sigmoid, ν,γ    | 20                   |
| Linear, ν       | 5                    |
| Polynomial, ν,γ, degree | 60          |
| **Total**       | **105**              |

**Table 6:** All combinations of parameters for any applicable ν, γ, and degree settings for each kernel option. Defining this parameter space results in 105 parameter settings to apply to SVM instances.

### Case Studies

As mentioned earlier, an attacker has significant freedom in developing malicious code that can potentially bypass USBESAFE. Therefore, as an endpoint solution, it is quite useful to study how the system responds to different levels of attack sophistication. To this end, we ran each of the following attacks, collected the corresponding USB traces, and measured the percentage of USB packets in each attack that was novel to the system based on the model learned on each machine.

#### Attack No. 1: Running a Malicious Payload
By running a malicious payload, we specifically focus on executing commands to call a binary that downloads code from the Internet and installs malware. Note that this attack can be designed to be as stealthy as possible. For example, the malicious code can start when the user is logged off, assuming the user is not physically present.

Our analysis showed that this attack had an average novelty score of 47.9% when tested with different learned models. Further investigation revealed that the USB packets received a relatively high novelty score compared to the learned model because the interarrival time values among URBs were significantly smaller than most real user typing behaviors. Additionally, the 2-gram analysis showed that the average content histogram of the first 103 request packets was more than 195 during the command injection, which was significantly higher than the content of the USB packets in the benign dataset.

| Machine | Attack 1 | Attack 2 | Attack 3 |
|---------|----------|----------|----------|
| Machine6 | 63.2%    | 58.2%    | 52.5%    |
| Machine7 | 54.5%    | 52.5%    | 40.8%    |
| Machine8 | 49.8%    | 40.8%    | 17.4%    |
| Machine9 | 31.5%    | 42.8%    | 42.4%    |
| Machine10 | 41.2%   | 47.6%    | 37.3%    |
| Machine11 | 46.5%   | 49.3%    | 42.4%    |
| Machine12 | 49.1%   | 44.1%    | 27.1%    |
| **Average** | **47.9%** | **42.8%** | **30.6%** |

**Table 7:** The novelty score of the evasion tests in the real-world deployment. The novelty score of all the attacks is significantly higher than the threshold value (t = 13.2%).

#### Attack No. 2: Adding Artificial Delays
In the previous attack, the malicious code launched a list of commands immediately after the enumeration phase. We updated the code to wait for a random period of time similar to the stalling code in malware attacks [10,11], and then open a terminal to run the commands. Our analysis revealed that this attack could bypass the post-enumeration feature by waiting for a random period of time before running the commands. As shown in Table 7, compared to Attack No. 1, the novelty score of the malicious code decreased in all the machines. However, USBESAFE reported this attack as a new observation as the interarrival of the packets was still too small.

#### Attack No. 3: Manipulating the Interarrival Times
We enhanced the attack payload to be more stealthy by adding delays among the injected commands to simulate human typing patterns. The delays were injected such that the arrivals of URBs followed a Poisson distribution. We used Poisson distribution because, as observed in Section 5, the URBs' interarrivals in our labeled dataset can be well-modeled using Poisson. While the novelty score of the USB traffic in Attack 3 (see Table 7) is relatively lower than the novelty score of the other attacks, the attack is still detected since the novelty scores of the USB traffic in all the traces are significantly higher than the predefined threshold (t = 13.2%). Further analysis suggests that injecting artificial delays with Poisson distribution during the command injection phase is not sufficient to automatically generate very serious mimicry attacks that perfectly resemble users' typing patterns. In fact, we empirically found that to successfully run such attacks, the adversary needs a more precise mechanism to learn the normal typing behavior of individual users. This makes crafting mimicry attacks more complicated as the adversary has to incorporate other techniques to reliably hook certain OS functions in order to learn the typing pattern of each user. This particular area has been studied extensively in malware detection, i.e., spyware detection, and is out of the scope of this paper.

### Benchmarks

Since USBESAFE is intended as an online monitoring system, it may impact the performance of other applications or the operating system. We expect USBESAFE’s performance overhead to be overshadowed by I/O processing delays, but in order to obtain measurable performance indicators and characterize the overhead of USBESAFE, we ran experiments that exercised the critical performance paths of USBESAFE. Designing custom test cases and benchmarks requires careful consideration of factors that might influence our runtime measurements. In these tests, we mainly focused on the core parts of the USB device communication, which were the USB device enumeration and data transfer mechanisms. We explain each of these benchmarks in more detail below.

#### Device Enumeration
In the first experiment, we tested whether USBESAFE introduces any noticeable performance impact during the USB enumeration phase. The testing USB device was a headset with a HID interface. We manually plugged the headset into the host 20 times and compared the results between the USBESAFE-enhanced host and the standard machine. The average USB enumeration time was 37.4 ms for the standard system and 39.1 ms for the USBESAFE-enhanced host, respectively. Compared to the standard host, USBESAFE introduced only 4.5% or less than 2 ms. We created the same benchmark using a mouse and repeated it 20 times. The system imposed 4.1% or 1.4 ms for device enumeration. The measurement results imply that USBESAFE does not have a significant impact on the enumeration of USB devices. More details are provided in Table 8.

#### USB Packet Inspection
In the second experiment, we created a benchmark to characterize the performance overhead of our system during normal device use. To measure the overhead of the detection model, we plugged in a USB optical mouse and moved it around to generate USB traffic. We then measured the time used by USBESAFE to determine whether the incoming USB packets should be filtered or not. The required time is calculated from the time a URB is delivered to the packet inspection subsystem to the time the packet is analyzed by the protection engine. We tested the experiment on the first 2,000 URBs and repeated the experiment 10 times, as shown in Table 8. As shown, the average cost per URB is 12.7 µs, including the time used by the benchmark to get the timing and print the results.

#### File System
We also created a benchmark to measure the latency of file operations under the baseline and USBESAFE-enabled machines. The goal of the experiment is to measure the performance overhead of the system during normal usage of a USB storage device, where users plug in flash drives to copy or edit files. We ran the experiments using a 16 GB USB flash drive and varied file sizes from 1 KB to 1 GB. Each test was done 10 times, and the average was calculated. As shown, the throughput of USBESAFE is close to the baseline when the file size is less than 100 MB (approximately 3.9%). When the mean file size becomes greater than 100 MB, USBESAFE shows lower throughput compared to the standard machine as a result of pattern monitoring on the bus. The results show that USBESAFE imposes 7.2% and 11.4% overhead when the mean file sizes are 100 MB and 1 GB, respectively. For example, if a user wants to copy 10 100 MB files, throughput would drop from 8.9 MB/s to 8.26 MB/s when USBESAFE is enabled on the user’s machine.

| Experiment | Device | Standard | USBESAFE | Overhead |
|------------|--------|----------|----------|----------|
| Enumeration | Headset | 37.4 ms | 39.1 ms | 4.5%     |
|             | Mouse  | 33.5 ms | 34.9 ms | 4.1%     |
|             | Keyboard | 34.2 ms | 35.6 ms | 4.2%     |
|             | Mass Storage | 36.6 ms | 38 ms | 3.9%     |
| Event Inspection | Mouse | - | 12.3 µs | 12.7 µs  |
|             | Keyboard | - | 13.1 µs | 12.7 µs  |

**Table 8:** USBESAFE’s overhead on the USB communication protocol. USBESAFE imposes on average 4.2% overhead during the enumeration phase and 12.7 µs per packet during USB packet inspection.