### 5. Detecting Transaction Performance Changes

In recent years, a new generation of monitoring tools, both commercial and research prototypes, has emerged, providing valuable insights into transaction activity tracking and latency breakdown across different components in multi-tier systems. However, these tools typically only report the measured transaction latency and provide additional information on the latency breakdown between the application server and the database server. This level of information is often insufficient to determine whether an increase in transaction latency is due to a higher system load or a recent application modification that directly affects the processing time for a specific transaction type.

In this section, we present an approach based on an application performance signature, which provides a compact model of the application's run-time behavior. By comparing the new application signature with the old one, we can detect changes in transaction performance.

#### 5.1 Server Transaction Monitoring

The Diagnostics tool, as shown in Figure 5, collects performance and diagnostic data from applications without requiring source code modifications or recompilation. It uses bytecode instrumentation and industry standards (e.g., JMX) to collect system metrics. Instrumentation involves inserting bytecode into the class files of the application as they are loaded by the virtual machine's class loader. This enables the Probe to measure execution times, count invocations, retrieve arguments, catch exceptions, and correlate method calls and threads.

**Figure 5. Multi-tier application configuration with the Diagnostics tool.**

The J2EE Probe captures events from the application, aggregates performance metrics, and sends these metrics to the Diagnostics Server. We have implemented a Java-based utility to extract performance data in real-time from the Diagnostics Server, creating an application log that provides comprehensive information on all transactions processed during the monitoring window, including their overall latencies, outbound calls, and the latencies of those calls.

During a monitoring window, the Diagnostics tool collects the following information for each transaction type:
- **Transaction Count**: The number of transactions.
- **Average Overall Transaction Latency**: The average latency for observed transactions, including processing time at the application server and related query processing at the database server.
- **Count of Outbound Calls**: The number of different types of outbound (database) calls.
- **Average Latency of Outbound Calls**: The average latency for observed outbound calls, measured from the moment the database request is issued by the application server to the time when a prepared reply is returned.

Many enterprise applications are built using the J2EE standard, a Java platform designed for web application development. For transaction monitoring, we use the HP (Mercury) Diagnostics tool, which consists of two components: the Diagnostics Probe and the Diagnostics Server.

**Figure 6. The transaction latency measured by the Diagnostics tool.**

The transaction latency includes waiting and service times across different tiers (e.g., Front and Database servers). Let \( R_{\text{front}} \) and \( R_{\text{DB}} \) be the average latencies for the i-th transaction type at the front and database servers, respectively. The transaction latency breakdown is calculated as follows:

\[ R_i = R_{\text{front}} + R_{\text{DB}} \]

Using this equation, we can compute \( R_{\text{front}} \).

#### 5.2 Application Performance Signature

In this section, we describe how to create a representative application signature that compactly reflects important performance characteristics of the application. As shown in [14], we can compute transaction service times using measured transaction latencies and corresponding system utilization. For a specific transaction type \( T_i \), the relationship is based on transaction service time \( S_i \), transaction residence time \( R_i \) (measured at the application server), and system utilization \( U \):

\[ S_i = R_i \times (1 - U) \]

Since we collect measured latencies for each transaction type over different monitoring windows, we have multiple equations reflecting transaction latencies at different CPU utilization points:

\[ R_{\text{front}, k} \times (1 - U_k / 100) \]

Our goal is to find the best fit for the overall equation set. A linear regression-based (LSR) method could be used, but it is sensitive to outliers and treats all equations equally, regardless of the number of contributing transactions. Therefore, we propose a different method to compute the service time \( S_i \) for the i-th transaction type. By solving \( S_i = R_{\text{front}, k} \times (1 - U_k / 100) \) for different utilization points \( U_k \), we generate a Cumulative Distribution Function (CDF) for \( S_i \). We select the 50th percentile value as the most representative solution for \( S_i \).

**Figure 7. Service time CDF of a typical server transaction.**

Finally, the application performance signature is created. As shown in [14], such a signature uniquely reflects the application transactions and their CPU requirements, and is invariant for different workload types. Continuous calculation of the application signature allows us to detect events such as software updates that may significantly affect transaction execution time. By comparing the new application signature with the old one, we can detect transaction performance changes and analyze their impacts.

The application signature technique complements the regression-based resource consumption model described in Section 4. For example, it cannot detect abnormal resource consumption caused by processes unrelated to the application and its transaction processing.

### 6. Case Study

In this section, we demonstrate how the integration of two complementary techniques, the regression-based transaction model and the application performance signature, provides an online solution for anomaly detection and analysis of essential performance changes in application behavior. The next subsection describes the experimental environment used in the case study and a specially designed workload to validate the proposed approach.

#### 6.1 Experimental Environment

In our experiments, we use a testbed of a multi-tier e-commerce site that simulates the operation of an online bookstore, according to the classic TPC-W benchmark [19]. This allows us to conduct experiments under different settings in a controlled environment to evaluate the proposed anomaly detection approach. We use the terms "front server" and "application server" interchangeably. Specifics of the software/hardware used are given in Table 1.

**Table 1. Testbed Components**

| Component                | Processor        | RAM  |
|--------------------------|------------------|------|
| Clients (Emulated-Browsers) | Pentium D / 6.4 GHz | 4GB  |
| Front Server - Apache/Tomcat 5.5 | Pentium D / 3.2 GHz | 4GB  |
| Database Server - MySQL 5.0   | Pentium D / 6.4 GHz | 4GB  |

Typically, client access to a web service occurs in the form of a session consisting of a sequence of consecutive individual requests. According to the TPC-W specification, the number of concurrent sessions (i.e., customers) or emulated browsers (EBs) is kept constant throughout the experiment. For each EB, the TPC-W benchmark statistically defines the user session length, the user think time, and the queries generated by the session. The database size is determined by the number of items and the number of customers. In our experiments, we use the default database setting, i.e., 10,000 items and 1,440,000 customers.

TPC-W defines 14 different transactions, classified as either browsing or ordering types, as shown in Table 2.

**Table 2. 14 Basic Transactions and Their Types in TPC-W**

| Browsing Type         | Ordering Type      |
|-----------------------|--------------------|
| Home (8)              | Shopping Cart (14) |
| New Products (9)      | Customer Registration (6) |
| Best Sellers (3)      | Buy Request (5)    |
| Product Detail (12)   | Buy Confirm (4)    |
| Search Request (13)   | Order Inquiry (11) |
| Execute Search (7)    | Order Display (10) |
|                       | Admin Request (1)  |
|                       | Admin Confirm (2)  |

TPC-W defines three types of traffic mixes based on the weight of each type of activity:
- **Browsing Mix**: 95% browsing and 5% ordering.
- **Shopping Mix**: 80% browsing and 20% ordering.
- **Ordering Mix**: 50% browsing and 50% ordering.

Real enterprise and e-commerce applications typically experience non-stationary transaction mixes with changing transaction probabilities over time. To simulate this, we designed an approach to generate non-stationary workloads using the TPC-W setup. We run four processes:
- **Base Processes**: Three concurrent processes executing one of the standard transaction mixes (browsing, shopping, and ordering) with a fixed number of EBs (e.g., 20, 30, and 50 EBs).
- **Random Process**: A fourth process that executes the shopping mix with a random execution period and a random number of EBs for each period. The pseudo-code for this random process is shown in Figure 8.

**Figure 8. Pseudocode for the Random Process.**

Due to the random process, the workload is non-stationary, and the transaction mix and load vary significantly over time. To validate the online anomaly detection and application change algorithm, we designed a 30-hour experiment with TPC-W, consisting of seven different workload segments, as shown in Figure 9.

**Figure 9. Workload Segments in the 30-Hour Experiment.**