more time samples are collected. In our case study, we have
not experienced a problem as we used only 30 hours of data
with I-minute intervals, i.e., M == 1800. However, in measur(cid:173)
ing real applications over long periods of time, complexity is
likely to become challenging. To avoid this, one solution might
retain only the last X samples, where X should be a few or(cid:173)
ders larger than the number of transaction types n and/or cover
a few weeks/months ofhistoric data. This way, one would have
a sufficiently large X to get accurate regression results, yet the
complexity will not be too large.
5 Detecting Transaction Performance Change
Nowdays there is a new generation ofmonitoring tools, both
commercial and research prototypes, that provide useful in(cid:173)
sights into transaction activity tracking and latency breakdown
across different components in multi-tier systems. However,
typically such monitoring tools just report the measured trans(cid:173)
action latency and provide an additional information on appli(cid:173)
cation server versus database server latency breakdown. Us(cid:173)
ing this level of information it is often impossible to decide
whether an increased transaction latency is a result of a higher
load in the system or whether it can be an outcome ofthe recent
application modification and is directly related to the increased
processing time for this transaction type.
In this section, we describe an approach based on an appli(cid:173)
cation performance signature that provides a compact model
of run-time behavior of the application. Comparing new ap(cid:173)
plication signature against the old application signature allows
detecting transaction performance changes.
5.1
Server Transaction Monitoring
Q---.
APPliC'atioD ~'er
+JZEE~roM
:
D.. a.fa.baH
Mnrer
. . ..
/
(lient 1
V
l\ler~ul')' Diagnostics Server
Figure 5. Multi-tier application configuration with the
Diagnostics tool.
The Diagnostics tool collects performance and diagnostic
data from applications without the need for application source
It uses bytecode instru(cid:173)
code modification or recompilation.
mentation and industry standards for collecting system and
JMX metrics. Instrumentation refers to bytecode that the Probe
inserts into the class files of the application as they are loaded
by the class loader of the virtual machine. Instrumentation en(cid:173)
ables a Probe to measure execution time, count invocations, re(cid:173)
trieve arguments, catch exceptions and correlate method calls
and threads.
The J2EE Probe shown in Fig. 5 is responsible for capturing
events from the application, aggregating the performance met(cid:173)
rics, and sending these captured performance metrics to the
Diagnostics Server. We have implemented a Java-based pro(cid:173)
cessing utility for extracting performance data from the Diag(cid:173)
nostics server in real-time and creating a so-called application
log2 that provides a complete information on all transactions
processed during the monitoring window, such as their over(cid:173)
all latencies, outbound calls, and the latencies of the outbound
calls. In a monitoring window, Diagnostics collects the follow(cid:173)
ing information for each transaction type:
• a transaction count;
• an average overall transaction latency for observed trans(cid:173)
actions. 3 This overall latency includes transaction pro(cid:173)
cessing time at the application server as well as all re(cid:173)
lated query processing at the database server, i.e., latency
is measured from the moment of the request arrival at the
application server to the time when a prepared reply is
sent back by the application server, see Fig. 6;
• a count of outbound (database) calls of different types;
• an average latency of observed outbound calls (of differ(cid:173)
ent types). The average latency of an outbound call is
measured from the moment the database request is issued
by the application server to the time when a prepared re(cid:173)
ply is returned back to the application server, i.e., the av(cid:173)
erage latency of the outbound call includes database pro(cid:173)
cessing and communication latency.
Many enterprise applications are implemented using the
J2EE standard - a Java platform which is used for web appli(cid:173)
cation development and designed to meet the computing needs
of large enterprises. For transaction monitoring we use the HP
(Mercury) Diagnostics [13] tool which offers a monitoring so(cid:173)
lution for J2EE applications. The Diagnostics tool consists of
two components:
the Diagnostics Probe and the Diagnostics
Server as shown in Fig. 5.
The transaction latency consists of the waiting and service
times across the different
tiers (e.g., Front and Database
servers) that a transaction flows through. Let R{ront and RPB
be the average latency for the i-th transaction type at the front
2We use this application log for building the regression-based model de(cid:173)
scribed in Section 4.
3Note that here a latency is measured for the server transaction (see the
difference between client and server transactions described in Section 2).
1-4244-2398-9/08/$20.00 ©2008 IEEE
457
DSN 2008: Cherkasova et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:22:14 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Database server
time
Transaction Latency - - - -....
~ DB Server latency _
AppJjcation Server Latency :
Figure 6. The transaction latency measured by the Di(cid:173)
agnostics tool.
and database servers, respectively. We then have the transac(cid:173)
tion latency breakdown calculated as follows:
(4)
Using this equation we can easily compute R{Tont.
5.2 Application Performance Signature
In this section, we describe how to create a representative
application signature that compactly reflects important perfor(cid:173)
mance characteristics of application. As shown in [14], we can
compute the transaction service times using measured trans(cid:173)
action latencies and corresponding system utilization. For a
concrete transaction type Tri' we have a relationship based
on transaction service time Si, transaction residence time R i
(measured at the application server) and utilization U of the
system (the application server):
Therefore, it is equivalent to
Si = Ri * (1 - U)
(5)
(6)
Since in real production system we collect measured latencies
for each transaction type i over different monitoring windows,
we have multiple equations that reflect transaction latencies at
different CPU utilization points as shown below 4:
R{~ont * (1 - U1/IOO)
R{;ont * (1 - U2 /IOO)
(7)
Our goal is to find the solution that is the best fit for the over(cid:173)
all equation set (7). A linear regression-based (LSR) method
can be chosen to solve for Si. However, there are two reasons
why we chose a different method. First, a number of outliers
that often present in production data could significantly affect
the accuracy of the final solution as LSR aims to minimize the
absolute error across all points. Second, there may be a sig(cid:173)
nificant difference in the number of transactions contributing
4Since we collect CPU utilization expressed in percents, we need to divide
it by 100 to use correctly in equation (6).
to different CPU utilization points. LSR aims to minimize the
absolute error across the equations, and it treats all these equa(cid:173)
tions equally.
Therefore, we propose another method to compute the ser(cid:173)
vice time Si for the i-th transaction type. By solving S i =
R{~ont * (1 - Uk/IOO) in Eq. 7, a set of solutions Sf is ob(cid:173)
tained for different utilization points Uk in the transaction la(cid:173)
tency profile. We generate a Cumulative Distribution Function
(CDF) for Si. Intuitively, since we conjecture that each trans(cid:173)
action type is uniquely characterized by its service time, then
we should see a curve similar to shown in Fig. 7 with a large
number of similar points in the middle and some outliers in the
beginning and the tail of the curve. We then select the 50-th
percentile value as the solution for Si as most representative. 5
The 50-th percentile heuristics works well for all transactions
in our study.
Home Transaction
Service time - -
100 r----.--==:::::::t=====--r----~-----,
90
80
70
60
50
40
30
20
10
OL . ! . - - -L - - - -L - - - - - l - - - - - -L - - - . . . J . . - - - - - l
1
1.5
2
2.5
3.5
service time (ms)
Figure 7. Service time CDP of a typical server transaction.
Finally, an application performance signature is created:
As shown in [14], such an application signature uniquely re(cid:173)
flects the application transactions and their CPU requirements
and is invariant for different workload types. The application
signature compactly represents a model of application run-time
behavior.
Continuous calculation of the application signature allows
us to detect events such as software updates that may signifi(cid:173)
cantly affect transaction execution time. By comparing the new
application signature against the old one can detect transaction
performance changes and analyze their impacts.
The application signature technique is complementary to
the regression-based resource consumption model described in
Section 4. For example, it is not capable of detecting abnormal
resource consumption caused by processes unrelated to the ap(cid:173)
plication and its transaction processing.
6 Case Study
In this section, we demonstrate how integration oftwo com(cid:173)
plementary techniques, the regression-based transaction model
and the application performance signature, provides an on-line
5 Selecting the mean of Si allows the outliers (thus the tail of the distribu(cid:173)
tion) to influence our service time extrapolation, which is not desirable. Be(cid:173)
cause of the shape of the CDF curve, the selection of the 50-th percentile is a
good heuristics.
1-4244-2398-9/08/$20.00 ©2008 IEEE
458
DSN 2008: Cherkasova et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:22:14 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
solution for anomaly detection and analysis of essential perfor(cid:173)
mance changes in application behavior. The next subsection
describes the experimental environment used in the case study
as well as a specially designed workload used to validate the
proposed approach.
6.1 Experimental Environment
In our experiments, we use a testbed of a multi-tier e(cid:173)
commerce site that simulates the operation of an on-line book(cid:173)
store, according to the classic TPC-W benchmark [19]. This
allows to conduct experiments under different settings in a con(cid:173)
trolled environment in order to evaluate the proposed anomaly
detection approach. We use the terms "front server" and "ap(cid:173)
plication server' interchangeably in this paper. Specifics ofthe
software/hardware used are given in Table 1.
Table 1. Testbed components
Processor
Clients (Emulated-Browsers)
Front Server - Apache/Tomcat 5.5
Database Server - MySQL5.0
Pentium D / 6.4 GHz
Pentium D / 3.2 GHz
Pentium D / 6.4 GHz
RAM
4GB
4GB
4GB
Typically, client access to a web service occurs in the form
of a session consisting of a sequence of consecutive individual
requests. According to the TPC-W specification, the number
of concurrent sessions (i.e., customers) or emulated browsers
(EBs) is kept constant throughout the experiment. For each
EB, the TPC-W benchmark statistically defines the user ses(cid:173)
sion length, the user think time, and the queries that are gen(cid:173)
erated by the session. The database size is determined by the
number of items and the number of customers. In our exper(cid:173)
iments, we use the default database setting, i.e., the one with
10,000 items and 1,440,000 customers.
TPC-W defines 14 different transactions which are classi(cid:173)
fied as either ofbrowsing or ordering types as shown in Table 2.
We assign a number to each transaction (shown in parenthesis)
according to their alphabetic order. Later, we use these trans(cid:173)
action id-s for presentation convenience in the figures.
Table 2. 14 basic transactions and their types in TPC-W
Browsing Type
Home (8)
New Products (9)
Best Sellers (3)
Product detail (12)
Search Request (13)
Execute Search (7)
Ordering Type
Shopping Cart (14)
Customer Registration (6)
Buy Request (5)
Buy Confirm (4)
Order Inquiry (11)
Order Display (10)
Admin Request (1)
Admin Confirm (2)
According to the weight of each type of activity in a given
traffic mix, TPC-W defines 3 types of traffic mixes as follows:
• the browsing mix with 95°A> browsing and 5% ordering;
• the shopping mix with 80% browsing and 20°A> ordering;
• the ordering mix with 50°A> browsing and 50% ordering.
Since real enterprise and e-commerce applications are typi(cid:173)
cally characterized by non-stationary transaction mixes (i.e.,
with changing transaction probabilities in the transaction mix
over time) under variable load [9, 5, 17] we have designed an
approach that enables us to generate non-stationary workloads
using the TPC-W setup. To generate a non-stationary trans(cid:173)
action mix with variable transaction mix and load we run 4
processes as follows:
•
the three concurrent processes each executing one of the
standard transaction mixes (i.e., browsing, shopping and
ordering respectively) with the arbitrary fixed number of
EBs (e.g, 20, 30, and 50 EBs respectively). We call them
base processes;
• the 4-th, so-called random process executes one of the
standard transaction mixes (in our experiments, it is the
shopping mix) with a random execution period while us(cid:173)
ing a random number of EBs for each period. To nav(cid:173)
igate and control this random process we use specified
ranges for the "random" parameters in this workload. The
pseudo-code of this random process is shown in Figure 8
(the code also shows parameters we use for the non(cid:173)
stationary mix in this paper).
initialize a variable dur +- 3hours
1.
2. while (dur > 0) do
set the execution time exe_dur +- random(20min, 30min)
a.
set the number ofEBs curr_EBs +- random(150, 700)
b.
c. execute shopping mix with curr_EBs for exe-dur time
d. set the sleep time sleep_dur +- random(lOmin, 20min)
e.
f.
sleep for sleep_dur time
adjust dur +- dur - (exe_dur+sleep_dur)
Figure 8. The pseudocode for the random process.
Due to the 4-th random process the workload is non-stationary
and the transaction mix and load vary significantly over time.
In order to validate the on-line anomaly detection and ap(cid:173)
plication change algorithm, we designed a special 30-hour ex(cid:173)
periment with TPC-W that has 7 different workload segments
shown in Figure 9, which are defined as follows.
45 r-----.-__r---,..-........-_,._-_--r--_-.....-__
3'
3e
2S
2e
IS
18
~
;:
:>
:>..u
2..
...e
t •••
e'-----J~__J._ _L_
••••