Figure 4: The third performance microbenchmark.
In this test, the format string contains two ‘%n’ for-
mat speciﬁers.
int main(int argc, char **argv) {
int i,j,k;
char buf[50];
for (i=0; i < 10000000; i++) {
sprintf(buf, "butter%d%d", j, k);
}
}
Figure 3: The second performance microbenchmark.
In this test, the format string contains two format
speciﬁers.
Our automated process would produce the above code, but
the following is more eﬃcient:
int total = 0, x = 0, i = 0;
__register_word(&x);
for(; i < arr_len; ++i) {
printf(arr[i], i, &x);
total += x;
}
__unregister();
Notice that we have hoisted the registration and unregistra-
tion out of the loop, so the program executes them only once.
It is conceivable that an optimizing compiler could also de-
tect this optimization opportunity. However, there will al-
ways be cases where the programmer knows more than the
compiler, and thus can do a better job placing registrations.
Although formatted output is rarely performance-critical,
this feature of white-lists may be more important for other
applications. Thus, our tool allows programmers to turn oﬀ
automatic registration and instead insert their own calls. If
a programmer forgets to register an argument the program
may abort, but it will not compromise security.
4. RESULTS
In this section we present performance results for our
white-listing format-string tool. Section 4.1 discusses our
eﬀectiveness at preventing format-string vulnerabilities, and
Section 4.2 presents our run-time performance overhead.
In both sections, we compare our results with Format-
Guard [6]. FormatGuard is similar to our approach in that
it combines compile-time source transformations with run-
time checks (see Section 5.1 for more details). It has proven
eﬀective at preventing many format-string vulnerabilities.
4.1 Vulnerability Prevention
We tested our approach on four programs with known
format string vulnerabilities:
• The tcpflow program is frequently run as root, and
can be attacked by inserting format speciﬁers into spe-
ciﬁc command-line arguments [34]. Thus an ordinary
user can obtain a root shell via this exploit.
• The splitvt program is also typically run as root, and
can also be attacked by inserting format speciﬁers into
one of its command line arguments [17]. Thus splitvt
is another potential source of unauthorized root shells.
• The rwhoisd 1.5 [28] server5 is vulnerable to format
speciﬁers embedded in strings that follow the -soa di-
rective. Attackers can use this vulnerability to gain a
remote shell on the machine running rwhoisd.
• The pfinger client is vulnerable to format speciﬁers
in remote .plan ﬁles [24].
Our white-listing approach ﬁxes all these vulnerabilities.
The FormatGuard approach, on the other hand, ﬁxes only
the splitvt and pfinger vulnerabilities.
The key diﬀerence between the power of our approach
and the power of the FormatGuard approach is that we are
able to prevent attacks on vprintf format-strings, such as
the attacks on tcpflow and rwhoisd mentioned above. We
feel this additional expressiveness is vital, because a num-
ber of well-known format-string attacks speciﬁcally involve
vprintf-style functions.
In addition to the two already
mentioned, these include the famous wu-ftpd vulnerabil-
ity [35] (the ﬁrst well-known format-string vulnerability),
and the vulnerabilities in isc dhcpd 3.0 [30], zkfingerd
0.9.1 [25], unreal ircd 3.1.1 [19], and the nn news reader
[39].6 Additionally, a system where vprintf is insecure
discourages the use of wrapper functions for logging and
I/O—which is generally considered good software engineer-
ing practice.
4.2 Performance Overhead
To determine our overhead per printf call, we ran a se-
ries of simple microbenchmarks consisting of a single loop
containing a single sprintf call. We also downloaded a
copy of FormatGuard, and compared our overhead with its
overhead. The tests were run on a 2.26 GHz Pentium 4,
5We had to modify the rwhoisd code slightly, because it
would not compile with the version of gcc installed on our
machines (version 3.3.3). We emphasize that these modiﬁ-
cations were necessary because of gcc, and not because of
our tool.
6We were unable to test our approach on all of these vul-
nerabilities, however, because updated versions have been
released (and the old source code is no longer available).
with 500 MB of RAM, and compiled with gcc version 3.3.3
using no compile ﬂags.7 The tests were all run with the
constant string optimization disabled (otherwise the white-
listing overhead would have been 0%, because our micro-
benchmarks use only constant strings). We also chose to
wrap calls to printf-style functions with whitelist-checking
functions,8 rather than to reimplement the printf func-
tions. Reimplementing printf would have likely led to bet-
ter results, but it would also lead to an unfair comparison
with FormatGuard—because they also chose to wrap func-
tions rather than reimplement them.
Our performance varied with the number and types of
the speciﬁers in the format string. This was expected: The
overhead of white-list checking is proportional to the num-
ber of speciﬁers. With no speciﬁers (Figure 2), white-listing
added an overhead of 10.2% and FormatGuard added an
overhead of 7.5%. With two non-’%n’ speciﬁers (Figure 3),
our approach added an overhead of 28.6%, and Format-
Guard added 20.9%. With two ’%n’ speciﬁers (Figure 4),
our overhead was 60.0%, and FormatGuard’s was 38.1%.
We also tested vsprintf by moving the printing loop in-
side a wrapper function. We observed an overhead of 26.4%
with no speciﬁers, 39.8% with two non-’%n’ speciﬁers, and
74.7% with two ’%n’ speciﬁers. FormatGuard does not pro-
tect against vsprintf vulnerabilities, and thus does not
transform these benchmarks. Note that the vsprintf over-
head percentages are exaggerated relative to the sprintf
overheads because vsprintf executes faster than sprintf.
The results for all our microbenchmarks are summarized in
Figure 5.
These overheads may seem high, but we stress that these
are microbenchmarks and not realistic programs. In addi-
tion, we had to turn oﬀ our constant-string optimization.
As we show below, this optimization can often signiﬁcantly
reduce the overhead.
We also searched for a real, printf-intensive application
to test our performance. We had some diﬃculty ﬁnding an
application where printf was performance critical, because
most I/O intensive programs implement their own I/O pro-
cedures. We eventually settled on man2html, the same pro-
gram used by the FormatGuard authors to test their perfor-
mance. As the name suggests, the man2html program con-
verts man pages to HTML web pages. We used the same ma-
chine and compiler that we used for the microbenchmarks.
With the constant format string optimization turned oﬀ, our
approach added an overhead of 14.1%. With the optimiza-
tion enabled, our approach added only 0.7%. The Format-
Guard approach added an overhead of 9.0%. As we see,
our optimization allowed us to execute this printf-heavy
application with insigniﬁcant overhead. Our performance
with the optimization enabled was also noticeably better
than FormatGuard, even though they catch a smaller class
of vulnerabilities.
There are two likely reasons that would explain why we
experienced a higher overhead than FormatGuard on the
7We tried compiling our microbenchmarks with optimiza-
tions enabled, but found that this actually slowed down the
code, with or without white-listing. This occurred regard-
less of whether we used -01, 2, or 3.
8Our wrapped functions ensure correct parsing by using
the glibc function parse_printf_format—the same func-
tion that the actual printing functions use to parse their
format strings.
microbenchmarks, and in the man2html test without opti-
mizations. Both tools use the parse_printf_format func-
tion to parse the format strings, but our approach must
do more with the result of the parse. FormatGuard only
needs a count of the number of format speciﬁers (returned
by parse_printf_format), whereas we need to actually look
at each speciﬁer to determine if it is a %n. In addition, we
must pay the extra cost of registering (and unregistering)
pointer arguments.
However, as we saw when we turned on the constant-string
optimization, our overhead on “real-world” applications is
insigniﬁcant. This is further borne out by performance tests
we ran on the applications we mentioned in Section 4.1
(once again, with the same machine and compiler). We ran
rwhoisd in local mode (to avoid network delays), and ob-
served an average overhead of 1.3% to start-up and respond
to a query (1.6% without optimization). We ran tcpflow
over a 273MB tcpdump output ﬁle and observed an overhead
of 0.3% (0.9% without optimization). We also attempted to
test the pfinger client, but found that the variability due
to network delays drowned out any diﬀerence between the
white-listed and normal versions. We did not test the over-
head of splitvt, as we could not think of a sensible test.9
In general, these results conﬁrm our belief that many vul-
nerable C applications do not beneﬁt from the performance
gained by using insecure library facilities. Figure 5 summa-
rizes these results.
We also measured the compile-time overhead of our ap-
proach (with the same machine and compiler as above).
These results are summarized in Figure 6. We see that using
CIL added a signiﬁcant overhead to each compile, but that
our analysis and transformation added only a small addi-
tional overhead—between 0 and 4.7%. We implemented our
prototype using CIL because it was much simpler to extend,
but these results suggest that an implementation could gain
signiﬁcant compile time savings by instead integrating di-
rectly into the compiler. The overhead when we use CIL is
primarily caused by parsing and typechecking the program
twice (once by CIL, and then again by gcc when it compiles
the transformed program produced by CIL). If we instead
integrated our approach directly into the compiler, we would
need to parse and typecheck the program only once.
5. RELATED WORK
Our approach to preventing format-string attacks nicely
complements other approaches:
• It prevents more attacks than FormatGuard (Section
5.1).
• It rejects fewer safe programs than approaches pre-
venting format arguments from “tainted” sources (Sec-
tion 5.2).
• It is more eﬃcient than approaches that check all writes
in an application (Section 5.3).
• It is more eﬃcient and less intrusive than approaches
that change the variable-argument calling convention
(Section 5.4).
9The splitvt application simply splits a terminal into two
terminals. There is no noticeable delay when interacting
with the terminals, either with or without white-listing.
Benchmark
White-listing
Optimized FormatGuard
sprintf microbenchmark, no speciﬁers
sprintf microbenchmark, 2 %d speciﬁers
sprintf microbenchmark, 2 %n speciﬁers
vsprintf microbenchmark, no speciﬁers
vsprintf microbenchmark, 2 %d speciﬁers
vsprintf microbenchmark, 2 %n speciﬁers
man2html
rwhoisd
tcpflow
White-listing
10.2% 0% (see below)
28.6% 0% (see below)
60.0% 0% (see below)
26.4% 0% (see below)
39.8% 0% (see below)
74.7% 0% (see below)
0.7%
14.1%
1.3%
1.6%
0.9%
0.3%
7.5%
20.9%
38.1%
no protection
no protection
no protection
9.0%
no protection
no protection
Figure 5: Performance results comparing the overhead of white-listing format strings (both with and without
the constant string optimization) with the overhead of FormatGuard. The microbenchmarks have no overhead
with optimized white-listing, because their format strings are constant.
Benchmark Source Lines
gcc 3.3.3
splitvt
pfinger
man2html
rwhoisd
tcpflow
5288 lines
331 lines
3630 lines
29702 lines
1695 lines
1.85 sec.
0.15 sec.
0.60 sec.
7.95 sec.
0.67 sec.
gcc + CIL gcc + CIL + White-listing
Overhead
1.0%
0%
0.9%
4.7%
1.8%
Whitelisting
2.94 sec.
0.36 sec.
1.15 sec.
20.09 sec.
1.16 sec.
2.91 sec.
0.36 sec.
1.14 sec.
19.18 sec.
1.14 sec.
Figure 6: The time required to compile the macrobenchmarks with gcc 3.3.3, with gcc and CIL, and with
gcc and CIL extended with our white-listing analysis and transformations. The ﬁnal column indicates the
overhead due to white-listing (i.e., it compares the fourth and ﬁfth columns).
5.1 FormatGuard