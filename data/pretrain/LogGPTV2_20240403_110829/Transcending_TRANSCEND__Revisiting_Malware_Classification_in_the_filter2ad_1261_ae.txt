prediction relies heavily on the exchangeability assumption,
which is violated in this dataset. To obtain a prediction,
CP-Reject follows conformal prediction principles and outputs
the class with the highest credibility (see §III-C), but we argue
this output is not trustworthy under drifting conditions. Con-
versely in conformal evaluation, by decoupling the prediction
of the underlying classiﬁer from the rejection mechanism and
directly interpreting the credibility as a measurement of drift
when comparing it to the calibrated thresholds, we can more
effectively detect poor quality predictions.
While the detection performance of DroidEvolver
is
mediocre on this dataset, the pseudo-labeling update mech-
anism manages to stabilize the system against the impact of
drift up until the last four months. After this, performance
deteriorates due to the poor quality of pseudo-labels used for
updating the online models—as DroidEvolver uses predicted
labels as pseudo-labels, the negative feedback loop is difﬁcult
to recover from. Surprisingly, the drift identiﬁcation mecha-
nism rejects more correct predictions than it keeps for each test
period. We posit that the small app buffer fails to sufﬁciently
represent the true app population, which may in turn lead
to the negative feedback loop in the later months. Although
much more extreme here, this informational inefﬁciency is
also responsible for the variability we see when using ICEs—
different dataset splits may be more or less representative of
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
816
1471013161922Testingperiod(month)0:00:10:20:30:40:50:60:70:80:91:0F1(baseline)F1(kept)F1(rejected)Quarantined1471013161922Testingperiod(month)0:00:10:20:30:40:50:60:70:80:91:0the true distribution and result in better or worse accuracy,
a phenomenon that
is mitigated by using a CCE. These
limitations, among others, have recently been explored in
concurrent work by Kan et al. [21].
F. Beyond Android Malware and SVMs
While TRANSCENDENT and conformal evaluation are ag-
nostic to the underlying classiﬁer and feature space, we have so
far focused on detecting Android malware with a linear SVM.
Here we demonstrate the performance beyond this setting.
To simplify the axes of comparison, we apply an ICE to
each setting, using credibility p-values and random search for
threshold calibration with the same constraints as before.
Windows PE malware with GBDT. We take examples from
the EMBER v2 dataset [3] spanning 2017, containing 47,888
benign and 69,202 malicious executables (labeled as having
40+ VirusTotal AV detections). The feature space contains
a diverse set of features which can be categorized as either
parsed features (e.g., header information), histograms (e.g.,
byte-value histograms), and printable strings (e.g., URL fre-
quency). As the underlying classiﬁer, we use gradient boosted
decision trees (GBDT) [19] as in Anderson and Roth [3], and
for the NCM we use the output probability for the predicted
class, negated for positive predictions. We train on executables
from the ﬁrst ﬁve months and test on the remaining.
PDF malware with RF. We use examples from the Hidost
dataset [41] spanning ﬁve weeks in Aug–Sep 2012, consisting
of 181,792 benign and 7,163 malicious ﬁles (labeled as having
5+ VirusTotal AV detections). The feature space is created by
statically parsing the PDF ﬁles to extract structural paths in
the PDF hierarchy that map to boolean or numeric feature
values, such as the presence of certain PDF objects or metadata
such as the number of pages. As the underlying classiﬁer
we use a random forest (RF) classiﬁer following Srndic and
Laskov [41]. As the NCM we use the proportion of decision
trees that disagree with the prediction of the ensemble (as
illustrated in Figure 1e). Interestingly, a major contribution of
the Hidost feature space in contrast to prior approaches [e.g.,
40] is that similar features are consolidated in order to be more
robust to drift. This means the distribution should be relatively
stationary compared to the Android dataset and will allow
us to test whether TRANSCENDENT is able to make effective
decisions on prediction quality when drift is less severe.
Note that we are unable to ﬁnd authoritative measurements
for the expected class balance for PE and PDF malware in the
wild as we are for Android malware, so we defer to the class
balance in the original datasets. This may result in a slight
spatial bias if the class balance is unrealistic [35], however
all approaches will be affected equally. Additionally, we can
here examine whether the class balance affects the ability for
TRANSCENDENT to identify low quality predictions.
Results. The results for Windows PE malware (Figure 9)
are consistent with those on Android data. TRANSCENDENT
outperforms probabilities alone which tend to reject many
(a) ICE, credibility
(b) ICE, probabilities
Fig. 9: F1-Score, EMBER Windows PE malware [3] and GBDT.
(a) ICE, credibility
(b) ICE, probabilities
Fig. 10: F1-Score, Hidost PDF malware [41] and RBF SVM.
otherwise correct predictions. In particular, a large spike in
drifting malware affects month six which probabilities are
unable to cope with, while TRANSCENDENT raises the rate of
rejections accordingly without making any additional errors.
As noted earlier, the PDF dataset gives us the opportunity
to evaluate TRANSCENDENT on a relatively stationary distri-
bution. As expected, thresholding using probabilities is much
more effective than it is in a drifting setting, however it under-
rejects compared to TRANSCENDENT, which is able to ﬁnd
thresholds that push the F1 of kept predictions to 1.0 while
rejecting almost entirely incorrect predictions. Exceptions to
this are months one and nine,
in which a small quantity
of true positive predictions are rejected. However this is
anomalous (i.e., it does not continue as drift increases) and
could be mitigated by calibrating with a constraint on the F1
of rejected samples rather than the F1 of kept examples alone.
From this we conclude that TRANSCENDENT is useful for
maximizing the potential of a high-quality robust classiﬁer,
and does not rely on relatively severe drift—as present in the
Android dataset—to detect low quality decisions. To this end
TRANSCENDENT can be combined with robust feature spaces
which is an orthogonal direction to combating concept drift.
As an additional result we observe that TRANSCENDENT
outperforms CP-Reject for both domains, with more details
reported in Appendix B.
VII. OPERATIONAL CONSIDERATIONS
Here we discuss some actionable points regarding the use
of conformal evaluation and TRANSCENDENT.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
817
1234567Testingperiod(month)0:00:10:20:30:40:50:60:70:80:91:0F1(baseline)F1(kept)F1(rejected)RateofdriftingmalwareRateofdriftinggoodwareQuarantined1234567Testingperiod(month)0:00:10:20:30:40:50:60:70:80:91:0147101316192225283134Testingperiod(day)0:00:10:20:30:40:50:60:70:80:91:0F1(baseline)F1(kept)F1(rejected)RateofdriftingmalwareRateofdriftinggoodwareQuarantined147101316192225283134Testingperiod(day)0:00:10:20:30:40:50:60:70:80:91:0TABLE III: Performance of optimal thresholds discovered using a full grid
search vs. random search. Random search discovers thresholds equivalent to
the full grid search but with two orders of magnitude fewer trials (§VI-D).
No rejection
Full grid
Random
FPS
3,529
2,187
3,259
FNS
19,486
0
0
PREC.
0.98
0.99
0.98
REC.
0.92
1.00
1.00
F1
0.95
0.99
0.99
#TRIALS
N/A
1,317,520
10,000
TRANSCENDENT in a Detection Pipeline. TRANSCENDENT
has particular applications in detection tasks where there is
a high cost of individual False Positives, (e.g., spam [31],
malware [6, 15, 40], fake accounts [13, 14]). In these cases, it
may be preferable to avoid taking a decision on low-conﬁdence
predictions or, where a gradated response is possible, diverting
rejected examples towards alternative remediation actions.
Consider an example in the fake accounts setting: owners
of accounts in the set of rejected positive predictions can be
asked to solve a CAPTCHA on their next login (a relatively
easy check to pass) while the owners of accounts in the set
of kept positive predictions can be asked to submit proof of
their identity. Increasing rejection rates signal a performance
degradation of the underlying classiﬁer without immediately
submitting to the errors it produces, giving engineers more
time to iterate and remediate.
Operational Recommendations. Based on our empirical
evaluation (§VI), we make the following recommendations for
TRANSCENDENT deployments:
• TRANSCENDENT is agnostic to the underlying learning
algorithm, but the quality of the rejection relies on the
suitability of the NCM. Some examples of possible NCMs
for different types of classiﬁers are described in Figure 1.
• Using an ICE or CCE is preferred over TCE due to their
computational efﬁciency, and is preferred over approx-TCE
due to approx-TCE’s reliance on assumptions that may not
universally hold.
• ICEs are relatively fast and lightweight and excel when
resources are limited. CCEs make rejections with higher
conﬁdence but at a higher computational cost.
• Thresholding with credibility alone is sufﬁcient to achieve
high quality prediction across all conformal evaluators.
While conﬁdence can improve the stability of an ICE
(§VI-C), it requires greater calibration time.
• Random search is preferred over grid search as it ﬁnds
similarly effective thresholds at signiﬁcantly lower cost.
• Rising rejection rates should be interpreted as a signal that
the underlying model is degrading. This signal can be used
to trigger model retraining or other remediation strategies.
• Guidance on tuning calibration thresholds and an example
of using alternative constraints is presented in Appendix E.
VIII. RELATED WORK
Conformal evaluation is based on conformal prediction
theory, a mechanism for producing predictions that are correct
with some guaranteed conﬁdence [38]. Additionally, the ICE
and CCE are inspired by inductive [32, 46, 47] and cross-
conformal predictors [48], respectively. However, conformal
prediction is intended to be used exclusively in settings where
the exchangeability assumption holds which makes it unsuit-
able for adversarial contexts such as malware classiﬁcation.
In this regard, we are the ﬁrst to ‘join the dots’ between the
conformal prediction of Vovk et al. [47] and the conformal
evaluation of
Jordaney et al. [20] and show how the vio-
lation of conformal prediction’s assumptions is detected and
exploited by Transcend [20] to detect concept drift.
That work introduced the concept of conformal evaluation
based on conformal prediction theory and the use of p-values
for calibrating and enforcing a rejection strategy for malware
classiﬁcation. However the evaluation artiﬁcially simulated
concept drift by merging malware datasets which introduced
experimental bias [7, 35] (§VI). In our experiments we sample
from a single repository of applications and perform a tempo-
ral evaluation to simulate natural concept drift caused by the
real evolution of malicious Android apps. Additionally, the
role of conﬁdence in thresholding was unclear, and the use
of exhaustive grid search to ﬁnd thresholds was suboptimal
compared to our random search. Most signiﬁcantly, the TCE
originally employed was not practical for real-world deploy-
ments, which we rectify by proposing the ICE and CCE.
CADE [51] focuses on explaining drift and relies on con-
trastive learning to perform a distance-based feature trans-
formation which results in more homogenous class clusters
relative to which outliers are easier to detect. They compare
against using conformal evaluation as an active learning query
strategy using an arbitrary NCM and without Transcend [20]
thresholding. We believe CADE and TRANSCENDENT are
orthogonal and that such feature transformations can be used to
devise stronger NCMs—we leave the development of optimal
NCMs for active learning as future work.
Other works have explored alternative solutions to tackling
concept drift. As described in §VI-E, DroidEvolver [21, 50] is
a malware detection system motivated by Transcend [20] that
identiﬁes drifting examples based on disagreements between
models in an ensemble. As models degrade, the examples
identiﬁed as drifting are used to update the models in an
online fashion. However, we ﬁnd the drift identiﬁcation mech-
anism is inferior to TRANSCENDENT and leads to a negative
feedback loop. Other solutions solely adapt to concept drift
without using rejection: DroidOL [29] and Casandra [30]
use online learning to continually retrain the models, with
API call graphs as features. Like all online-trained neural
networks, these are susceptible to catastrophic forgetting [18],
where performance degrades on older examples as the model
attempts to adapt to the new distribution. Pendlebury et al.
[35] present a comparison of different strategies for combating
concept drift, including rejection, incremental retraining, and
online learning, illustrating the advantages and disadvantages
of each. Semi-supervised techniques may be used to reduce the
labeling burden of such strategies as drift increases, as recently
demonstrated for intrusion detection by Andresini et al. [5].
The related task of detecting adversarial examples [11, 36,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
818
44] is addressed by Sotgiu et al. [39], who propose a rejection
strategy for neural network-based classiﬁers that
identiﬁes
anomalies in an input’s latent feature representation at different
layers of the neural network. Additionally, Papernot and Mc-
Daniel [33] combine a conformal predictor with a k-Nearest
Neighbor algorithm to identify low-quality predictions that are
indicative of adversarial inputs. However, both methods are
restricted to deep learning-based image classiﬁcation.
IX. CONCLUSION
We provide a thorough formal treatment of Transcend [20]
which acts as the missing link between conformal prediction
and conformal evaluation. We propose TRANSCENDENT, a
superset of the original framework which includes novel
conformal evaluators that match or surpass the original perfor-
mance while signiﬁcantly decreasing the computational cost.
We show TRANSCENDENT outperforms the existing state-of-
the-art approaches while generalizing across different malware
domains and exploring realistic operational settings.
We envision these improvements will enable researchers
and practitioners alike to make use of conformal evaluation
to build rejection strategies to improve their security detec-
tion pipelines. To this end, we release our implementation
of TRANSCENDENT, making Transcend [20] and conformal
evaluation available to the community for the ﬁrst time.
ACKNOWLEDGEMENTS
This research has been partially sponsored by the UK
EP/P009301/1 EPSRC research grant.
REFERENCES
[1] K. Allix, T. F. Bissyand´e, J. Klein, and Y. L. Traon. Are your training
datasets yet relevant? - an investigation into the importance of timeline in
machine learning-based malware detection. In International Symposium
on Engineering Secure Software and Systems (ESSoS), 2015.
[2] K. Allix, T. F. Bissyand´e, J. Klein, and Y. Le Traon. Androzoo:
Collecting millions of android apps for the research community. In ACM
International Conference on Mining Software Repositories (MSR), 2016.
[3] H. S. Anderson and P. Roth. EMBER: an open dataset for training static
PE malware machine learning models. CoRR, abs/1804.04637, 2018.
[4] H. S. Anderson, A. Kharkar, B. Filar, D. Evans, and P. Roth. Learning
to evade static PE machine learning malware models via reinforcement
learning. CoRR, abs/1801.08917, 2018.
[5] G. Andresini, F. Pendlebury, F. Pierazzi, C. Loglisci, A. Appice, and
L. Cavallaro. INSOMNIA: towards concept-drift robustness in network
In ACM Workshop on Artiﬁcial Intelligence and
intrusion detection.
Security (AISec), 2021.
[6] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and K. Rieck.
DREBIN: effective and explainable detection of android malware in
In Network and Distributed System Security Symposium
your pocket.
(NDSS), 2014.
[7] D. Arp, E. Quiring, F. Pendlebury, A. Warnecke, F. Pierazzi, C. Wress-
negger, L. Cavallaro, and K. Rieck. Dos and don’ts of machine learning
in computer security. In USENIX Security Symposium, 2022.
[8] P. L. Bartlett and M. H. Wegkamp. Classiﬁcation with a reject option
using a hinge loss. Journal of Machine Learning Research (JMLR),
2008.
[9] R. Bellman. Adaptive Control Processes - A Guided Tour (Reprint from
1961). Princeton University Press, 2015.
[10] J. Bergstra and Y. Bengio. Random search for hyper-parameter opti-
mization. Journal of Machine Learning Research (JMLR), 2012.
[11] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of
adversarial machine learning. Pattern Recognition, 2018.
[12] C. M. Bishop. Pattern recognition and machine learning, 5th Edition.