### Alternative Analyses

Alternatively, we can print the data in a "pretty-printed" human-readable format or encode it as XML (although the latter is not fully implemented at this point). This provides a more abstract view of network activity compared to raw packets and is expected to be highly useful for traffic analysis.

### 3.2.2 Spatially Independent State

For spatially independent state, we need to transfer state from one Network Intrusion Detection System (NIDS) instance to another running concurrently. We achieve this by establishing network connections between the instances. One instance calls the `listen` function, which opens a port on the local host, waiting for connections from other instances. Once a connection is established, there are several ways to exchange state. Figure 1 illustrates how we integrated these into Bro's architecture.

Using the `request_remote_events` script function, one side can subscribe to a set of events, meaning that whenever the other side generates one of these events, it automatically forwards the event to the other side:

```bro
# Request all HTTP events from peer.
request_remote_events(10.0.0.1, 47756/tcp, /http_.*/);
```

At the receiving end, the event looks the same as one generated locally (though it is possible to test whether a particular event originated locally or remotely).

In addition to sharing events, multiple Bro instances can also share data. When a global script-level identifier is declared as `&synchronized`, modifications to its value will be propagated to all peers where the identifier is also declared `&synchronized`. For example:

```bro
global saw_Blaster: set[addr] &synchronized;
```

This causes the script variable `saw_Blaster` to be synchronized across each active Bro process. Any change made by one of them to the set will be transparently reflected in the value of the set as seen by the others.

We implemented synchronized tables by propagating changes to the data in terms of descriptions of the operations to perform on the data rather than the full (and probably mostly unmodified) data itself. For example, when we insert an element into a large set, we propagate "insert element 'foo' into set 'bar'". This can, in some circumstances, lead to race conditions. Avoiding them would require mutually exclusive data operations, such as using a token-based reservation system [24], but this would violate Bro's real-time processing constraints due to having to wait for access before performing an operation. For further details, see [19].

### 3.3 Robust and Secure Communication

Clearly, inter-NIDS communication requires robust and secure operation. Regarding robustness, a key point is that, from the perspective of the NIDS process's main functionality, inter-NIDS communication should be unobtrusive. In particular, inevitable networking difficulties such as timeouts or unexpected termination should not disrupt the main operation. Therefore, instead of adding a network communication component directly into the current event engine/script interpreter structure, we spawn a second process exclusively dedicated to handling communication with peers. The two processes communicate via a Unix pipe. (We did not use threads to keep their address spaces separate.) On multi-processor systems, using two processes has the additional advantage of utilizing more than one CPU. Subsequently, we refer to the two processes as the main process and the communication process, respectively.

One key element of our design was to base it on semantically unidirectional communication. This means that Bro's processing never expects one side to reply to something the other side sent. While this restricts error detection and handling somewhat, it significantly eases implementation by avoiding the need to deal with unreceived replies (which would require timeouts and a failure-recovery scheme). We believe that the decrease in complexity outweighs the loss in terms of error processing. However, the unidirectionality only affects the core-level communication. It is still possible to build a script-level handshake mechanism by passing a sequence of events between two peers.

In §3.2.2, we discussed how the NIDS's real-time constraints lead us to abide by impure synchronization semantics, i.e., the possibility of race conditions. Similarly, our communication design does not make any timing guarantees for the communication. For example, transferring large amounts of data may delay the reception of an event. Also, while all state from one endpoint will always arrive in the order in which it was sent, state from multiple endpoints may be received intermixed.

Along with designing for robust communication, we also need to secure the communication, i.e., provide confidentiality and authentication. We do so via SSL (implemented using OpenSSL [15]). See [19] for a discussion of such security aspects.

### 3.4 Integrating External State

While we performed our initial implementation of independent state in the context of the Bro NIDS, the concept applies more generally to other applications as well. In principle, any system, not just other NIDS instances, may choose to make its fine-grained internal state accessible to peers.

As a major step in this direction, the lightweight, highly portable Broccoli2 library [4] enables arbitrary applications to partake in the exchange of Bro's state. Broccoli is an independent implementation of (parts of) the serialization/communication protocol that we have developed for the Bro system. Broccoli nodes can request, send, and receive events just like Bro instances can. In [8], we demonstrate some of the power of such NIDS-external independent state by supplying the Bro system with host-based application context.

### 4 Applications

We now describe several powerful applications of independent state in network intrusion detection. We first show how we can use independent state to greatly enhance Bro's traditional model of regular checkpointing, including support for robust crash recovery. Then we discuss distributed intrusion detection, concentrating on the utility of spatially independent state. Finally, we show how independent state can be used for dynamic reconfiguration, profiling, and debugging.

We implemented each of these applications. Given independent state, combined with the NIDS's flexibility, we found all of them relatively easy to achieve. Although at first glance each might seem to be yet another extension of Bro's generally extensible functionality, the ease of implementation proves the power of the approach.

Our experiences with these applications come from monitoring the access links in several large environments: the Münchner Wissenschaftsnetz (MWN; research network including two universities and other institutions; Gbps, heavily loaded), the University of California, Berkeley (UCB; Gbps, heavily loaded), and the Lawrence Berkeley National Laboratory (LBNL; Gbps, medium load).

### 4.1 Checkpointing

IDSs face fundamental state management problems. Either the system uses a static allocation of state for its analysis, making it vulnerable to simple forms of attacker evasion, or it allocates different types of state dynamically, in which case managing and reclaiming that state becomes a major burden. While Bro provides a variety of timers for use in state management, operational experience has shown that state still inexorably accrues, partly due to our reluctance to assign timers to every data item because it is hard to determine good a priori settings for these, or even identify all of them (there are hundreds of script-level variables).

To date, Bro's only support for large-scale state reclamation has been the brute-force approach of simply starting over from scratch. That is, to run Bro 24x7, we (and other Bro users) resort to checkpointing, which in this context means periodically starting up a new instance of Bro and killing off the old one. The frequency with which this is done ranges from daily (LBNL) to every few hours (MWN, UCB).

For Bro, the two main types of state lost when checkpointing are internal connection state (including analyzer-specific state and attached timers) and script-level data. However, the concept of persistence described in §3.2.1 now enables us to individually choose connections (by calling `make_connection_persistent`) and script-level data (via `&persistent` declarations) to transform into independent state, thus enabling a new Bro instance to use them as part of its initial state. Doing so allows us to continue longer-running forms of analysis uninterrupted, such as tracking slow scans, long-lived interactive connections, usernames, inferred software versions, alerts already generated, and addresses that Bro has blocked in the past using its dynamic blocking facility.

While temporally independent state thus enables us to keep key state across restarts, implementing it soundly also requires a dynamic handover mechanism. The problem here is that the currently executing instance of Bro has to save its persistent state at some specific point in time, after which the new instance can begin executing. If we have to wait for the new instance to start up, we will incur a monitoring outage. We solve this problem by using not temporally independent state, but spatially independent. We implement dynamic handover by starting up the new instance and having it connect via a (local) network connection to the old instance, requesting its current set of persistent state. After this has been successfully transmitted, the old instance terminates itself, and the new one starts processing.

As already discussed, we do not simply make all state persistent. Doing so would defeat the purpose of checkpointing. But having the tools now to selectively make state persistent, the next step is to identify the state for which this makes sense. For our operational environments, we keep internal connection state for interactive services that tend to have long-lived connections: FTP, SSH, telnet, and rlogin connections. For script-level data, we took Bro's default policy scripts (as of version 0.8a57) as representative of the usage of state in Bro scripts. Our first observation is that nearly all of the scripts store their relevant data in tables or sets. We found five basic usages: (1) remembering messages already logged to avoid duplication, (2) remembering hosts which have done "something" (e.g., propagating a worm), (3) associating additional state with connections (e.g., which FTP data connections have been negotiated by a control channel), (4) holding configuration data, such as particular hosts allowed to do "something" (e.g., connect to a certain host; this data is more or less fixed), (5) remembering additional data derived from the script's analysis (e.g., software installed on a host).

Taking the MWN environment as a test case, we made all tables belonging to the first group persistent. Most of these tables are low in volume, and suppressing unnecessary log messages is a vital NIDS capability [2, 10]. For the second group, we differentiate between short-term (minutes or less) and longer-term data. The former is often quite large in volume and often not worth keeping. For example, the script recognizing the Blaster worm [3] by its scanning activity keeps two tables: one tracking hosts that have communicated over TCP port 135 within the last five minutes, and the second remembering all already-identified worm sources. We decided to make only the latter persistent.

The third group (associating additional state) is more problematic. Ideally, we would like to keep information for all persistent connections but discard all the rest. To do so, the scripts would need significant restructuring, as their semantics vary too much to automatically deduce which information is associated with persistent connections. There are some tables, though, which we know always correspond to state for persistent connections (e.g., the FTP analyzer script remembers FTP connections). We made these persistent, but left all other tables unmodified (i.e., ephemeral).

We also left the fourth group untouched, as configurations are mostly static and better changed manually if the need arises. Finally, for the last group, we found we needed to make case-by-case decisions. For example, to keep vulnerability profiles [20], one of the scripts detects the software used by different hosts, an excellent example of information we declare `&persistent` so we do not lose it.

### 4.2 Crash Recovery

A related application of independent state is better recovery from crashes. The three main reasons for the crash of an NIDS are resource exhaustion, attacks, and programming errors [16]. In most systems, including Bro, in each case, we lose all the state so far collected by the system. By using the checkpoint function (see §3.2.1) regularly, however, we can significantly mitigate the effects of crashes, so that we only lose data accumulated since the last checkpoint.

Our experience is that crash recovery is invaluable. This is not only the case when actively developing the IDS but also in a production environment, where crashes are still a fact of life, particularly due to resource exhaustion. Not only does crash recovery allow us to continue operating with only a minor loss of state (in terms of the importance of the state), but the checkpoint also allows us to analyze the particularly significant state post-mortem (cf. §4.4).

### 4.3 Distributed Analysis

Once we have provided a means for an NIDS to communicate its state, we can then use that mechanism to distribute its analysis. To date, distributed NIDS have generally imposed a specific model on the form of distribution. For example, DIDS [18] pioneered the sensor model, gathering low-level data remotely while performing higher-level semantic analysis centrally. On the other hand, Emerald [17] constructs a hierarchical structure to propagate information up to the root level.

Independent, fine-grained state opens up new degrees of flexibility for distributed analysis. In this section, we look at three different models, all of which we have been able to implement and experiment with by adding independent state to Bro. The first model supports load-balancing for monitoring high-volume links. The second supports the well-known "distributed sensor" model. The third looks at propagating information between decoupled systems.

### 4.3.1 Load-Balancing

On today’s high-volume links, it is exceedingly difficult to analyze the full packet stream with a single NIDS (unless one utilizes custom hardware [14]). One strategy for coping with such a load is to distribute the analysis across several machines, each doing only a part of the work. A key question then is how to coordinate their operation. Currently, using Bro operationally, we do this by running several independent instances on different slices of the network traffic. But without any state sharing, this loses important information. Thus, our goal is to retain the depth of analysis a single Bro could in principle achieve if it could cope with the load.

To this end, we first need to decide how to divide the traffic between the multiple systems. We can either do so statically (each system gets all packets matching some fixed criteria) or dynamically (e.g., for each connection, we decide individually which system will analyze it). Our initial efforts have focused on static approaches due to their simplicity, distributing based on: (i) the local IP space, or (ii) application protocol.