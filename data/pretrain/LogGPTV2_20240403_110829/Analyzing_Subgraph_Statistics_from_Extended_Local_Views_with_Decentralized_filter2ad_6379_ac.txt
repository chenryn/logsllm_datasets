Γ
Γ
+ δ2,
+ δ1.
∗
д ∈ SΓ,(λ, Y) ∈ S′
∗
д ∈ SΓ,(λ, Y) ∈ S′
(3)
since Phase 1 ensures Eq. (1) with at least 1 − δ2 probability. Then,
to prove Eq. (2), it suffices to show that
(4)
Since (λ, Y) is generated using an (ϵ1, δ1)-DDP algorithm, we have
(cid:12)(cid:12)(cid:12) G
Pr(cid:104)
(cid:105)
(cid:12)(cid:12)(cid:12) G
′(cid:105)
≤ eϵ · Pr(cid:104)
(cid:12)(cid:12) G
(cid:12)(cid:12) G(cid:3) ≤ eϵ1 · Pr(cid:2)(λ, Y) ∈ S′
′(cid:3) + δ1.
Pr(cid:2)(λ, Y) ∈ S′
(cid:12)(cid:12)(cid:12) G
(cid:105)
Pr(cid:104)
(cid:12)(cid:12)(cid:12) (λ, Y) ∈ S′
(cid:105) · Pr(cid:2)(λ, Y) ∈ S′
= Pr(cid:104)
(cid:12)(cid:12) G(cid:3)
(cid:12)(cid:12)(cid:12) (λ, Y) ∈ S′
(cid:105) ·(cid:16)
≤ Pr(cid:104)
(cid:12)(cid:12) G
′(cid:3) + δ1
eϵ1 · Pr(cid:2)(λ, Y) ∈ S′
(cid:12)(cid:12)(cid:12) (λ, Y) ∈ S′
≤ eϵ1 · Pr(cid:104)
(cid:105) · Pr(cid:2)(λ, Y) ∈ S′
(cid:12)(cid:12) G
′(cid:3) + δ1
(cid:12)(cid:12)(cid:12) λ = x, Y = y, G
(cid:12)(cid:12)(cid:12) λ = x, Y = y, G
Pr(cid:104)
Therefore,
∗
д ∈ SΓ,(λ, Y) ∈ S′
∗
д ∈ SΓ
∗
д ∈ SΓ
∗
д ∈ SΓ
(5)
We will show that for any x ≥ LSG(Γд)/ϵ2, any y, and any set ϒ
(cid:105) ≤ eϵ2 · Pr(cid:104)
of noisy subgraph counts,
∗
д = ϒ
Γ
∗
д = ϒ
Γ
′(cid:105)
λ, G
λ, G
λ, G
(cid:17)
Γ
Γ
Γ
Γ
λ
λ
λ
λ
λ
,
(6)
Pr(cid:104)
= Pr(cid:104)
≤ Pr(cid:104)
We have
∗
д ∈ SΓ,(λ, Y) ∈ Sλ
∗
д ∈ SΓ,(λ, Y) ∈ S′
∗
д ∈ SΓ,(λ, Y) ∈ S′
Γ
Γ
Γ
λ
λ
(cid:105)
(cid:105)
(cid:105)
(cid:12)(cid:12)(cid:12) G
(cid:12)(cid:12)(cid:12) G
(cid:12)(cid:12)(cid:12) G
+ Pr(cid:104)
∗
д ∈ SΓ,(λ, Y) ∈ Sλ \ S′
Γ
λ
(cid:105)
(cid:12)(cid:12)(cid:12) G
(cid:0)Γд(cid:1) .
Figure 2: Local view sensitivity
noise Lap(λ), and we show that this satisfies ϵ2-DDP with at least
1 − δ2 probability, for some ϵ2 and δ2. According to the sequential
composition property of differential privacy (explained in Section
2.1), the two-phases as a whole satisfies (ϵ, δ)-DDP where ϵ = ϵ1 +ϵ2
and δ = δ1 + δ2. It turns out that Phase 1 of our solution needs
to be custom-designed for different types of subgraph patterns д,
which we detail in Section 4. In what follows, we present the main
requirements for Phase 1 necessary for the proposed framework to
satisfy (ϵ, δ)-DDP.
Let λ be the noise scale returned by Phase 1 given a graph G. We
require that λ satisfy the following two conditions:
(1) λ is generated using an (ϵ1, δ1)-DDP algorithm.
(2) With at least 1 − δ2 probability, we have
ϵ2 · λ ≥ LSG
(1)
Intuitively, in the above two-phase framework, Phase 1 essen-
tially aims to establish an upper bound on the local sensitivity
LSG(Γд) to be used in Phase 2. This upper bound is estimated by
. There is a chance (i.e., with probability δ2) that Phase 1 can fail,
λ
ϵ2
ϵ2  0. Then, with 1 − δ probability,
∗ + α · log
x
(cid:19)
(cid:18) 1
2δ
≥ x .
plus λc · log(cid:16) 1
(cid:17). This approach, however, only works if (i)
By Lemma 4.1, if we are to derive τ, we may first inject Laplace
noise Lap(λc) into LSG (Γ△), and then sets τ to the noisy value
LSG (Γ△) + Lap(λc) can be computed in the decentralized setting,
and (ii) Lap(λc) is sufficient to ensure (ϵ1, δ1)-DDP for LSG (Γ△). In
relation to this, we first note that LSG (Γ△) equals the maximum
number of common neighbors shared by two users in G, i.e.,
2δ
LSG (Γ△) =
max
vi,vj ∈G,i(cid:44)j
3 · |N(vi) ∩ N(vj)|,
(7)
2δ
where N(v) denotes the set of neighbors of user v. This is because
(i) adding or removing one edge ⟨vi , vj⟩ only affects those triangles
that contain both vi and vj as vertices, (ii) the number of such
triangles equals |N(vi) ∩ N(vj)|, and (iii) each of these triangles is
reported by three users.
Based on Eq. (7), we may compute a probabilistic upper bound of
LSG (Γ△) in the decentralized setting as follows. First, for each user
vi, we ask her to compute the maximum number c(vi) of common
neighbors that she shares with any other user in her local view, i.e.,
(8)
Note that for any node vk (cid:60) Gi, we have |N(vi)∩ N(vk)| = 0. Then,
we ask vi to report a noisy version c∗(vi) of c(vi) injected with
Laplace noise Lap(λc). After that, we take
∗(vi) + λc · log
|N(vi) ∩ N(vj)|.
c(vi) = max
⊤(vi) = c
vj ∈Gi∧j(cid:44)i
(cid:18) 1
(cid:19)
c
as a probabilistic upper bound of c(vi), and we set
c
τ = max
vi ∈G
(cid:17), which
ϵ1 · log(cid:16) 1
⊤(vi)
as a probabilistic upper bound of LSG (Γ△).
Unfortunately, the above approach requires λc = n/ϵ1 to en-
sure that τ satisfies ϵ1-DDP. To explain, observe that adding or
removing one edge in G could change each c(vi) by 1 in the worst
case. Therefore, the sensitivity of {c(v1), . . . , c(vn)} equals n, due to
which we need λc ≥ n/ϵ1 to guarantee that {c∗(v1), . . . , c∗(vn)} is
ϵ1-differentially private. As such, we have τ > n
2δ
leads to a prohibitive of noise in Phase 2 of our solution.
Optimized solution. To address the deficiency of the afore-
mentioned method, we propose to avoid directly collecting
{c(v1), . . . , c(vn)} (as it has a high sensitivity), but let each user
vi report a probabilistic upper bound d⊤(vi) of her degree d(vi),
i.e., the number of 1-hop neighbors of vi. The rationale is that
d(vi) ≥ c(vi) holds for any vi, and hence, we can use a probabilistic
upper bound of d(vi) in place of c(vi).
In particular, for some λd , δd that we clarify shortly, we ask each
user vi to inject Laplace noise Lap(λd) into her degree d(vi), and
then report the noisy degree d∗(vi); after that, we take
d
2δd
⊤(vi) = d
∗(vi) + λd · log