# GARs Micro-benchmarks

Byzantine-resilient Gradient Aggregation Rules (GARs) are fundamental tools for achieving Byzantine resilience in distributed machine learning. This section provides a micro-benchmark of their performance on a GPU, considering the number of inputs/gradients (denoted as \( n \)) and the gradient dimension (denoted as \( d \)).

## Experimental Setup

- **Hardware**: Intel Core i7-8700K CPU and two Nvidia GeForce 1080 Ti GPUs.
- **Baseline**: Average aggregation, implemented as part of the GARFIELD library.
- **Parameters**:
  - \( f \), the number of Byzantine inputs, is set to \( \frac{n-3}{4} \) (the bound for Bulyan).
  - \( d = 10^7 \) in Figure 3a.
  - \( n = 17 \) in Figure 3b.
- **Metric**: Aggregation time, including the aggregation of \( n \) input vectors in GPU memory and the transfer of the resulting vector back to main memory.
- **Runs**: Each point is the average of 21 runs, with a standard deviation two orders of magnitude below the observed average.

## Theoretical Complexity

- **MDA**: \( O\left(\binom{n}{f+1} + n^2d\right) \)
- **Multi-Krum**: \( O(n^2d) \)
- **Bulyan**: \( O(n^2d) \)
- **Median**: Best case \( O(nd) \), worst case \( O\left(\binom{n}{f+1}\right) \)
- **Average**: \( O(nd) \)

## Results

### Number of Inputs (Figure 3a)

- **Multi-Krum and Bulyan**: Quadratic growth in \( n \).
- **Median**: Good scalability with \( n \), maintaining performance close to Average.
- **MDA**: Exponential theoretical complexity, but quadratic growth in practice due to low \( n \) and \( f \).
- **Average**: Constant aggregation time for \( n < 15 \) (approximately 8 ms), then linear growth.

### Input Dimension (Figure 3b)

- **All GARs**: Linear increase in aggregation time with respect to \( d \).

## Convergence Comparison

### Experiment 1: CifarNet on TensorFlow (Figure 4a)

- **Final Accuracy**: All systems achieve similar final accuracy, except AggregaThor.
- **Convergence Speed**: Byzantine-resilient deployments converge slightly slower than averaging, but reach the same accuracy eventually.
- **Overhead**: Byzantine-resilient applications add minimal overhead (less than 1%) compared to crash-tolerant ones.
- **Accuracy**: GARFIELD applications outperform AggregaThor, possibly due to the use of a newer TensorFlow version (2.3 vs. 1.10) and the Keras library.

### Experiment 2: ResNet-50 on PyTorch (Figure 4b)

- **Final Accuracy**: Byzantine-resilient applications show up to 10% accuracy loss compared to vanilla PyTorch.
- **Accuracy Loss**: Due to the nature of Byzantine-resilient GARs, which guarantee convergence only to a ball around a local minimum.
- **Asynchrony and Decentralization**: Combining these leads to the largest accuracy loss, as outdated models and gradients slow down convergence.

## Byzantine Behavior (Figure 5)

- **Random Vectors (Figure 5a)**: Both vanilla and crash-tolerant deployments fail to learn, while MSMW converges to high accuracy.
- **Reversed Vectors (Figure 5b)**: Similar results, with MSMW showing robustness.

## Throughput Analysis (Figure 6)

- **Model Dimension**: Overhead of Byzantine resilience in terms of throughput.
- **Fault-Tolerant Systems**: Normalized to vanilla baseline throughput.
- **Overhead Ranges**:
  - Crash Tolerance: 83% to 537% (7%–286%)
  - SSMW: 69% to 492% (5%–219%)
  - MSMW: 88% to 544% (14%–292%)
  - Decentralized Learning: 161% to 1135% (24%–429%)

Compared to the crash-tolerant deployment, MSMW shows a higher overhead, but it remains effective in ensuring Byzantine resilience.