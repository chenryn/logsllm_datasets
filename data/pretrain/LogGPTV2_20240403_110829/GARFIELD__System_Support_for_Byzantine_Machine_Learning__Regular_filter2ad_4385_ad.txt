g
e
r
g
g
A
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
Bulyan
MDA
Multi-Krum
Median
Average
7
9
11
13
15
17
n - Number of inputs
)
c
e
s
(
e
m
i
t
n
o
i
t
a
g
e
r
g
g
A
10−1
10−2
10−3
10−4
19
21
23
Bulyan
MDA
Multi-Krum
Median
Average
105
106
107
d - Input dimension
108
(a) Number of Inputs
(b) Input dimension
Fig. 3: Micro-benchmark of different GARs deployed on
a GPU. n denotes the number of inputs to the GAR and
d denotes the length of one input/gradient.
C. GARs Micro–benchmarks
The Byzantine–resilient GARs are the basic enabling tools
for Byzantine resilience. We provide a micro–benchmark for
their GPU–based implementation performance with respect to
the number of inputs/gradients (i.e., n the number of workers
or servers) and the gradient dimension (i.e., d).
(cid:15)
(cid:16)
n−3
4
Vanilla frameworks, e.g., TensorFlow, average gradients
at the parameter server. So, we also include the evaluation
of Average, which has been implemented as a part of the
GARFIELD library. Average is our baseline in this experiment.
For a fair comparison, we set f, the number of Byzantine
inputs, to
(which is the bound for the strongest GAR
we use, i.e., Bulyan [21]) for all Byzantine-resilient GARs and
hence, the smallest possible n is 7. We set d = 107 in Figure 3a
and n = 17 in Figure 3b. The metric for this micro–benchmark
is the aggregation time: it includes the aggregation of n input
vectors (all resident in GPU memory) and the transfer of
the resulting vector back to main memory. Each point is the
average of 21 runs, for which we observed a standard deviation
two orders of magnitude below the observed average. We ran
this micro–benchmark on an Intel Core i7-8700K CPU and
(cid:5)
two Nvidia GeForce 1080 Ti GPUs.
Theoretically, the asymptotic complexities of MDA, Multi–
Krum, Bulyan, and Average are respectively O
+ n2d
[21] and O(nd). Our implemen-
tation of Median has a best case complexity of O(nd) and
. In practice, for a ﬁxed d (Figure 3a),
we observe these asymptotic behaviors for Multi–Krum and
Bulyan: quadratic in n. Median shows good scalability with
n, maintaining a consistent performance that is very close to
Average. Although the asymptotic complexity of MDA is expo-
nential, our implementation achieves only a quadratic growth
with n. The values of n and f used in these experiments are
merely too low to expose such a behavior, i.e., the exponential
growth with n. Average aggregation time remains roughly
constant for a ﬁxed d and n <15, with an aggregation time
of ∼ 8 ms, and then grows linearly. For a ﬁxed n (Figure 3b),
we observe a linear time increases with respect to d for every
one of the studied GARs.
(cid:3)
[21], O(cid:2)
worst case of O(cid:2)
[11], O(cid:2)
(cid:3)
(cid:4)(cid:2)
n2d
(cid:3)
n
f
(cid:3)
n2d
n2d
(Figure 4b) trains ResNet-50 on PyTorch–based systems using
GPUs. Both experiments use CIFAR-10 as a dataset. The ﬁrst
experiment puts GARFIELD in a head–to–head comparison
with the state–of–the–art Byzantine ML system, i.e., Aggre-
gaThor. The second experiment is an instance of a deployment
of GARFIELD while training a bigger model using GPUs.
that
Figure 4a shows that all the systems achieve almost the same
ﬁnal accuracy (except AggregaThor). Some of the Byzantine–
resilient deployments converge a bit slower than those using
averaging during training, yet reaching the same accuracy
eventually, i.e., after doing enough number of iterations. In-
terestingly, we can notice that Byzantine–resilient applications
do not add much overhead compared to the crash–tolerant one,
in terms of the number of iterations till convergence (less than
1%). Surprisingly, GARFIELD applications achieve better ﬁnal
accuracy than AggregaThor. We speculate that the reason is the
fact that AggregaThor relies on an old version of TensorFlow
compared to GARFIELD (1.10 vs. 2.3)7. Another related reason
is the fact
the latest version of TensorFlow is also
integrated with the highly–optimized Keras library, which we
also use. Figure 4b shows that Byzantine–resilient applications
fail to reach the same ﬁnal accuracy as vanilla PyTorch, with
up to 10% ﬁnal accuracy loss. This accuracy loss, although not
clear in Figure 4a, makes sense as a direct byproduct of using
Byzantine–resilient GARs to aggregate workers’ gradients and
also due to having diverging servers (in some cases). Note
that such GARs inevitably introduce accuracy loss as the
aggregation protocols guarantee the convergence only to a ball
around a local minimum [11]. The disability to optimize the
model weights beyond this ball explains the observed loss in
accuracy. Moreover, We notice that combining network asyn-
chrony with decentralization leads to the biggest accuracy loss.
Asynchrony essentially leads to the aggregation of outdated
models and gradients, slowing down convergence and reducing
the ﬁnal accuracy. Interestingly, the crash–tolerant deployment
does not experience such a loss compared to the vanilla case.
E. Byzantine Behavior
As a sanity check to our implementation, we conduct
experiments with real adversarial behavior, where we apply
attacks on the vanilla baseline (PyTorch in this experiment),
a GARFIELD–based application (MSMW in this experiment),
and the crash–tolerant protocol. Figure 5 shows 2 attacks on
both servers and workers. In the ﬁrst attack (Figure 5a), the
Byzantine node replaces its reply with random values, where
in the second attack (Figure 5b), such a reply is reversed
and ampliﬁed (multiplied by -100). We train CifarNet with 11
workers and 3 servers (in the case of fault–tolerant algorithms)
with 1 Byzantine node from each party. We run the training for
20 epochs. On the one hand, both the vanilla deployment and
the crash–tolerant deployment fail to learn under both attacks.
7It might look unfair to use different versions of TensorFlow for com-
parison. Yet, note that almost all of the code base of AggregaThor (which
has thousands of LoC, written for V1.10) requires updating (to be compatible
with V2.3). We chose to test with the latest publicly available code as-is while
highlighting the different versions issue to make things clear to the reader.
D. Convergence Comparison
Figure 4 shows the results of two experiments: the ﬁrst
one (Figure 4a) trains CifarNet on TensorFlow–based systems
(including AggregaThor) using CPUs, where the second one
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
46
y
c
a
r
u
c
c
A
0.7
0.6
0.5
0.4
0.3
0.2
0.1
TensorFlow
Crash-tolerant
SSMW
MSMW
Decentralized Learning
AggregaThor
0
250
500
750
Training iterations
1000 1250 1500 1750 2000
y
c
a
r
u
c
c
A
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
PyTorch
Crash-tolerant
SSMW
MSMW
Decentralized Learning
1
6 11 16 21 26 31 36 41 46 51 56 61 66 71 76
Training epochs
(a) Convergence with CifarNet
(b) Convergence with ResNet-50 (1 epoch = 200 iterations)
Fig. 4: Convergence of GARFIELD applications with respect to other baselines using two models.
y
c
a
r
u
c
c
A
0.4
0.3
0.2
0.1
2
4
6
PyTorch
Crash-tolerant
MSMW
10
8
14
Training epochs
12
16
18
y
c
a
r
u
c
c
A
0.4
0.3
0.2
0.1
2
4
6
PyTorch
Crash-tolerant
MSMW
10
8
14
Training epochs
12
16
18
(a) Random vectors
(b) Reversed vectors
Fig. 5: GARFIELD tolerance to two Byzantine attacks.
On the other hand, MSMW manages to train the model safely
and converges to a normal, high accuracy.
F. Throughput
We show here the computation and the communication
costs of Byzantine resilience. First, we quantify the overhead
of employing GARFIELD–based applications compared to the
other baselines by measuring the throughput while training
several models. Then, we analyze the scalability of such
applications with a different number of workers, Byzantine
workers, and Byzantine servers. Essentially, our goal is to
understand the main factors that drive the cost of Byzantine
resilience8. In this section and without loss of generality, we
use ResNet-50 as our model (unless otherwise stated). We
do not employ any Byzantine behavior in these experiments
as we want to quantify the overhead of Byzantine resilience
in a normal, optimistic environment. Thus, we denote here
the number of declared Byzantine nodes with the number of
Byzantine nodes (i.e., fw and fps).
a) Model dimension: Figure 6 depicts the cost of Byzan-
tine resilience in terms of throughput. The throughput of the
fault–tolerant systems is normalized to the vanilla baseline
throughput in each case. Thus, the y-axis represents the slow-
down that each of the fault–tolerant systems induces compared
to the vanilla baseline. The overhead of crash tolerance ranges
from 83% to 537% (7%–286%), that of SSMW ranges from
69% to 492% (5%–219%), that of MSMW ranges from 88%
to 544% (14%–292%), and that of decentralized learning
ranges from 161% to 1135% (24%–429%) compared to the
vanilla deployments on CPUs (and GPUs). More interestingly,
compared to the crash–tolerant deployment, MSMW overhead
8Note that AggregaThor’s architecture is the same as SSMW. We have only
included the results of the latter for better readability.
n
w
o
d
w
o
l
S
n
w
o
d
w
o
l
S
12
10
8
6
4
2
0
5
4
3