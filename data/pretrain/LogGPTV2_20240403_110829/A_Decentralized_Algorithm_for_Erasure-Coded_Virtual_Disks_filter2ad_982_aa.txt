title:A Decentralized Algorithm for Erasure-Coded Virtual Disks
author:Svend Frølund and
Arif Merchant and
Yasushi Saito and
Susan Spence and
Alistair C. Veitch
A Decentralized Algorithm for Erasure-Coded Virtual Disks
Svend Frølund, Arif Merchant, Yasushi Saito, Susan Spence, and Alistair Veitch
Storage Systems Department
HP Labs, Palo Alto, CA 94304
Abstract
A Federated Array of Bricks is a scalable distributed
storage system composed from inexpensive storage bricks.
It achieves high reliability with low cost by using erasure
coding across the bricks to maintain data reliability in
the face of brick failures. Erasure coding generates n en-
coded blocks from m data blocks (n > m) and permits the
data blocks to be reconstructed from any m of these en-
coded blocks. We present a new fully decentralized erasure-
coding algorithm for an asynchronous distributed system.
Our algorithm provides fully linearizable read-write access
to erasure-coded data and supports concurrent I/O con-
trollers that may crash and recover. Our algorithm relies
on a novel quorum construction where any two quorums in-
tersect in m processes.
1. Introduction
Distributed disk systems are becoming a popular al-
ternative for building large-scale enterprise stores. They
offer two advantages to traditional disk arrays or main-
frames. First, they are cheaper because they need not rely on
highly customized hardware that cannot take advantage of
economies of scale. Second, they can grow smoothly from
small to large-scale installations because they are not lim-
ited by the capacity of an array or mainframe chassis. On
the other hand, these systems face the challenge of offering
high reliability and competitive performance without cen-
tralized control.
This paper presents a new decentralized coordination al-
gorithm for distributed disk systems using deterministic era-
sure codes. A deterministic erasure code, such as Reed-
Solomon [13] or parity code, is characterized by two param-
eters, m and n.1 It divides a logical volume into ﬁxed-size
stripes, each with m stripe units and computes n− m par-
ity units for each stripe (stripe units and parity units have
1 Reed-Solomon code allows for any combination of m and n, whereas
parity code only allows for m = n− 1 (RAID-5) or m = n− 2 (RAID-
6).

*LJ(L6&6,
)LOH'%
VHUYHUV
*LJ()$%SURWRFRO
)$%
Figure 1. A typical FAB structure. Client com-
puters connect to the FAB bricks using stan-
dard protocols. Clients can issue requests to
any brick to access any logical volume. The
bricks communicate among themselves us-
ing the specialized protocol discussed in this
paper.
the same size). It can then reconstruct the original m stripe
units from any m out of the n stripe and parity units. By
choosing appropriate values of m, n, and the unit size, users
can tune the capacity efﬁciency (cost), availability, and per-
formance according to their requirements. The ﬂexibility of
erasure codes has attracted a high level of attention in both
the industrial and research communities [15, 2, 14, 12].
The algorithm introduced in this paper improves the state
of the art on many fronts. Existing erasure-coding algo-
rithms either require a central coordinator (as in traditional
disk arrays), rely on the ability to detect failures accurately
and quickly (a problem in real-world systems), or assume
that failures are permanent (any distributed system must be
able to handle temporary failures and recovery of it’s com-
ponents).
In contrast, our algorithm is completely decentralized,
yet maintains strict linearizability [8, 1] and data consis-
tency for all patterns of crash failures and subsequent re-
coveries without requiring quick or accurate failure detec-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
tion. Moreover, it is efﬁcient in the common case and de-
grades gracefully under failure. We achieve these properties
by running voting over a quorum system which enforces
a large-enough intersection between any two quorums to
guarantee consistent data decoding and recovery.
In the next two sections, we provide background infor-
mation on the FAB system we have built and quantify the
reliability and cost beneﬁts of erasure coding. Section 1.3
articulates the challenge of the coordination of erasure cod-
ing in a totally distributed environment and overviews our
algorithm. We deﬁne the distributed-systems model that our
algorithm assumes in Section 2 and outline the guarantees
of our algorithm in Section 3. We present our algorithm in
Section 4, analyze it in Section 5, and survey related work
in Section 6.
1.1. Federated array of bricks
We describe our algorithm in the context of a Federated
Array of Bricks (FAB), a distributed storage system com-
posed from inexpensive bricks [6]. Bricks are small stor-
age appliances built from commodity components including
disks, a CPU, NVRAM, and network cards. Figure 1 shows
the structure of a typical FAB system. Bricks are connected
together by a standard local-area network, such as Gigabit
Ethernet. FAB presents the client with a number of logi-
cal volumes, each of which can be accessed as if it were a
disk. In order to eliminate central points of failure as well
as performance bottlenecks, FAB distributes not only data,
but also the coordination of I/O requests. Clients can access
logical volumes using a standard disk-access protocol (e.g.,
iSCSI [9]) via a coordinator module running on any brick.
This decentralized architecture creates the challenge of en-
suring single-copy consistency for reads and writes with-
out a central controller. It is this problem that our algorithm
solves.
1.2. Why erasure codes?
While any data storage system using large numbers of
failure-prone components must use some form of redun-
dancy to provide an adequate degree of reliability, there are
several alternatives besides the use of erasure codes. The
simplest method for availability is to stripe (distribute) data
over conventional, high-reliability array bricks. No redun-
dancy is provided across bricks, but each brick could use
an internal redundancy mechanism such as RAID-1 (mir-
roring) or RAID-5 (parity coding). The second common al-
ternative is to mirror (i.e., replicate) data across multiple
bricks, each of which internally uses either RAID-0 (non-
redundant striping) or RAID-5. This section compares era-
sure coding to these methods and show that erasure coding
can provide a higher reliability at a lower cost.
1E+14
1E+11
1E+08
1E+05
)
s
r
a
e
y
(
L
D
T
T
M
1E+02
1
4-way replication/R5 bricks
E.C.(5,8)/R5 bricks
4-way replication/R0 bricks
E.C.(5,8)/R0 bricks
Striping/reliable R5 bricks
10
100
1000
Logical Capacity (TB)
Figure 2. Mean time to ﬁrst data loss (MTTDL)
in storage systems using (1) striping, (2)
replication and (3) erasure coding. (1) Data
is striped over conventional, high-end, high-
reliability arrays, using internal RAID-5 en-
coding in each array/brick. Reliability is good
for small systems, but does not scale well.
(2) Data is striped and replicated 4 times over
inexpensive, low reliability array bricks. Re-
liability is highest among the three choices,
and scales well. Using internal RAID-5 encod-
ing in each brick improves the MTTDL further
over RAID-0 bricks. (3) Data is distributed
using 5-of-8 erasure codes over inexpensive
bricks. The system scales well, and reliabil-
ity is almost as high as the 4-way replicated
system, using similar bricks.
Figure 2 shows expected reliability of these schemes.
We measure the reliability by the mean time to data loss
(MTTDL), which is the expected number of years before
data is lost for the ﬁrst time. For example, in a stripe-based
system, data is lost when any one brick breaks terminally.
On the other hand, in a system using m out of n erasure
coding, a piece of data is lost when more than n− m of n
bricks that store the data terminally break at the same time.
Thus, the system-wide MTTDL is roughly proportional to
the number of combinations of brick failures that can lead
to a data loss. We used the component-wise reliability num-
bers reported in [3] to extrapolate the reliability of bricks
and networks, and calculated the MTTDL assuming random
data striping across bricks. This graph shows that the reli-
ability of striping is adequate only for small systems. Put
another way, to offer acceptable MTTDL in such systems,
one needs to use hardware components far more reliable
and expensive than the ones commonly offered in the mar-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
Replication/R0 bricks
Replication/R5 bricks
E.C.(5,n)/R0 bricks
E.C.(5,n)/R5 bricks
d
a
e
h
r
e
v
O
e
g
a
r
o
t
S
7
6
5
4
3
2
1
0
1E-02
1E+01
1E+04
1E+07
1E+10
1E+13
MTTDL (years)
Figure 3. Storage overheads (raw capac-
ity/logical capacity) of systems using repli-
cation and erasure coding. The storage
overhead of replication-based systems rises
much more steeply with increasing reliabil-
ity requirements than for systems based on
erasure-coding. Using RAID-5 bricks reduces
the overhead slightly. The MTTDL of a stor-
age system that stripes data over RAID-5
bricks is ﬁxed, and hence this is omitted from
this plot; the storage overhead of such a sys-
tem is 1.25.
ket. On the other hand, 4-way replication and 5-of-8 erasure
coding both offer very high reliability, but the latter with a
far lower storage overhead. This is because reliability de-
pends primarily on the number of brick failures the system
can withstand without data loss. Since both 4-way replica-
tion and 5-of-8 erasure coding can withstand at least 3 brick
failures, they have similar reliability.
Figure 3 compares the storage overhead (the ratio of raw
storage capacity to logical capacity provided) for sample
256TB FAB systems using replication and erasure coding,
and with the underlying bricks internally using RAID-5 or
RAID-0 (non-redundant). In order to achieve a one million
year MTTDL, comparable to that provided by high end con-
ventional disk arrays, the storage overhead for a replication-
based system is 4 using RAID-0 bricks and approximately
3.2 using RAID-5 bricks. By contrast, an erasure code based
system with m = 5 can meet the same MTTDL requirement
with a storage overhead of 1.6 with RAID-0 bricks, and yet
lower with RAID-5 bricks.
The storage efﬁciency of erasure-coded systems comes
at some cost in performance. As in the case of RAID-5 ar-
rays, small writes (writes to a subregion of the stripe) re-
quire a read of the old data and each of the correspond-
ing parity blocks, followed by a write to each. Thus, for
an m-of-n erasure coded system, a small write engenders
2(n − m + 1) disk I/Os, which is expensive. Nonetheless,
for read-intensive workloads (such as Web server work-
loads), systems with large capacity requirements, and sys-
tems where cost is a primary consideration, a FAB system
based on erasure codes is a good, highly reliable choice.
1.3. Challenges of distributed erasure coding
Implementing erasure-coding in a distributed system,
such as FAB, presents new challenges. Erasure-coding in
traditional disk arrays rely on a centralized I/O controller
that can accurately detect the failure of any component disk
that holds erasure-coded data. This assumption reﬂects the
tight coupling between controllers and storage devices—
they reside within the same chassis and communicate via
an internal bus.
It is not appropriate to assume accurate failure detec-
tion or to require centralized control in FAB. Storage bricks
serve as both erasure-coding coordinators (controllers) and
storage devices. Controllers and devices communicate via a
standard shared, and potentially unreliable, network. Thus,
a controller often cannot distinguish between a slow and
failed device: the communication latency in such networks
is unpredictable, and network partitions may make it tem-
porarily impossible for a brick to communicate with other
bricks.
Our algorithm relies on the notion of a quorum system,
which allows us to handle both asynchrony and recovery.
In our algorithm, correct execution of read and write oper-
ations only requires participation by a subset of the bricks
in a stripe. A required subset is called a quorum, and for
an m-out-of-n erasure-coding scheme the underlying quo-
rum system must only ensure that any two quorums inter-
sect in at least m bricks. In other words, a brick that acts
as erasure-coding controller does not need to know which
bricks are up or down, it only needs to ensure that a quo-
rum executes the read or write operation in question. Fur-
thermore, consecutive quorums formed by the same con-
troller do not need to contain the same bricks, which allows
bricks to seamlessly recover and rejoin the system.
Compared to existing quorum-based replication algo-
rithms [4, 10, 11], our algorithm faces new challenges that
are partly due to the fact that we use erasure-coding instead
of replication, and partly due to the fact that we apply the
algorithm to storage systems. Using erasure-coding instead
of replication means that any two quorums must intersect
in m instead of 1 bricks. We deﬁne a new type of quorum
system, called an m-quorum system, that provides this in-
tersection property. Using erasure-coding also means that it
is more difﬁcult to handle partial writes where the erasure-
coding controller crashes after updating some, but not all,
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
members of a quorum. Existing quorum-based replication
algorithms rely on the ability to write-back the latest copy
during a subsequent read operation, essentially having read
operations complete the work of a partial write. However,
with erasure coding, a partial write may update fewer than
m stripe units, rendering subsequent read operations unable
to reconstruct the stripe. We use a notion of versioning in
our algorithm so that a read operation can access a previ-
ous version of the stripe if the latest version is incomplete.
In existing quorum-based algorithms, a read operation al-
ways tries to complete a partial write that it detects. This
means that a partially written value may appear at any point
after the failed write operation, whenever a read operation
happens to detect it. Having partial write operations take ef-
fect at an arbitrary point in the future is not appropriate for
storage systems. Our algorithm implements a stronger se-
mantics for partial writes: a partial write appears to either
take effect before the crash or not at all. Implementing these
stronger semantics is challenging because a read operation
must now decide whether to complete or roll-back a par-
tial write that it detects.
2. Model
We use the abstract notion of a process to represent
a brick, and we consider a set U of n processes, U =
{p1, . . . , pn}. Processes are fully connected by a network
and communicate by message passing. The system is asyn-
chronous: there is no bound on the time for message trans-
mission or for a process to execute a step. Processes fail by
crashing—they never behave maliciously—but they may re-
cover later. A correct process is one that either never crashes
or eventually stops crashing. A faulty process is a process
that is not correct.
Network channels may reorder or drop messages, but
they do not (undetectably) corrupt messages. Moreover, net-
work channels have a fair-loss property: a message sent an
inﬁnite number of times to a correct process will reach the
destination an inﬁnite number of times.
2.1. Erasure-coding primitives
We use the term block to refer to the unit of data storage.
Processes store data using an m-out-of-n erasure-coding
scheme. A stripe consists of m data blocks, and we gen-
erate n− m parity blocks from these m data blocks. Thus,
each stripe results in the storage of n blocks; each process
stores one of these n blocks.
The primitive operations for erasure coding are listed in
Figure 4:
• encode takes m data blocks and returns n blocks,
among which the ﬁrst m are the original blocks
b1
b2
b3
encode
b1
b2
b'3