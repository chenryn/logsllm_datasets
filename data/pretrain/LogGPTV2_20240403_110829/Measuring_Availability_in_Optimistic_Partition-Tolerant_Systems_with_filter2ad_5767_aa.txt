title:Measuring Availability in Optimistic Partition-Tolerant Systems with
Data Constraints
author:Mikael Asplund and
Simin Nadjm-Tehrani and
Stefan Beyer and
Pablo Gald&apos;amez
Measuring Availability in Optimistic Partition-tolerant Systems
with Data Constraints
Mikael Asplund, Simin Nadjm-Tehrani
Department of Computer and Information Science,
Link¨oping University
SE-581 83 Link¨oping, Sweden
{mikas,simin}@ida.liu.se
Stefan Beyer, Pablo Galdamez
Instituto Tecnolgico Informtica
Camino de Vera, s/n, 46022 Valencia, Spain
Universidad Politcnica de Valencia
{stefan, pgaldamez}@iti.upv.es
Abstract
Replicated systems that run over partitionable environ-
ments, can exhibit increased availability if isolated parti-
tions are allowed to optimistically continue their execution
independently. This availability gain is traded against con-
sistency, since several replicas of the same objects could
be updated separately. Once partitioning terminates, di-
vergences in the replicated state needs to be reconciled.
One way to reconcile the state consists of letting the ap-
plication manually solve inconsistencies. However, there
are several situations where automatic reconciliation of the
replicated state is meaningful. We have implemented repli-
cation and automatic reconciliation protocols that can be
used as building blocks in a partition-tolerant middleware.
The novelty of the protocols is the continuous service of the
application even during the reconciliation process. A pro-
totype system is experimentally evaluated to illustrate the
increased availability despite network partitions.
1
Introduction
Prevalence of distributed services and networked solu-
tions has made many enterprises critically dependent on
service availability. Whereas earlier centralised solutions
were made resilient to service faults by deploying redun-
dancy, the new generation of distributed services need to
show resilience to overloads and network partitions. There
are many applications that require automatically managed,
distributed, secure, mutable object stores. A commercial
instance of this problem appears in Software distribution.
According to Sun Microsystems [4], network partitions are
indeed interesting to study since global corporate intranet-
works are typically not richly connected. Hence, service
availability of distributed data storage systems is potentially
affected by denial of service attacks (DoS) that render parts
of the network as inaccessible [6].
This paper addresses support for maintaining distributed
objects with integrity constraints in presence of network
partitions. Providing fault tolerance in such distributed
object systems requires relatively complex mechanisms to
properly handle all the different fault scenarios. One solu-
tion is to relieve the application writers, and get them to rely
on a middleware that provides fault tolerance services. This
is a direction pursued in the European DeDiSys research
project [9]. In particular, the algorithms for replication and
reconciliation implemented in this paper will be deployed
in an extension of CORBA middleware. However, they can
be considered as general building blocks to be integrated in
any middleware.
The basic problem of network partitions is that there is
no way of knowing what is happening in the other parts of
the system. A bank customer cannot make a payment if not
enough money exists on his/her bank account, and you can-
not book a ﬂight that is already full. These kinds of integrity
constraints exist in most applications either explicitly or im-
plicitly in the operation semantics; but what happens if the
bank account is used for two payments at the same time in
two disconnected parts? One typically resorts to a “safe”
solution that implies periods of unavailability.
Another way to deliver service in a partitioned system
with integrity constraints is to act optimistically. This
means to provisionally accept some operations, but allow
them to be revoked or undone at a later stage, if necessary.
To revoke previously accepted operations might be unac-
ceptable in some cases, but there are also situations where
it is better than general unavailability. Another possibility
is to perform some kind of compensating action speciﬁc for
that operation. Many applications have a mix of operations
where some non-critical operations can be treated optimisti-
cally, while the critical operations must wait. We claim an
application can improve its overall availability by provision-
ally accepting operations that may later be revoked. We pro-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007pose protocols that allow for automatic reconciliation of the
state of the network, by replaying the (logged) operations
serviced during the partition and discarding some (revoca-
ble) operations that violate the integrity constraints in the
reconstructed state. The novelty of our implemented algo-
rithm is that it builds the new repaired network state and at
the same time serves the new incoming operations. That is,
operations arriving after network reuniﬁcation but prior to
installation of the new state are not denied service.
The contributions of this paper are twofold. First, we
present the implementation of a previously unimplemented
reconciliation protocol[1] embedded as a middleware ser-
vice. The aim of the protocol is to give continuous ser-
vice during network partitions. Second, we show the im-
proved performance due to giving service during the de-
graded mode, and also during reconciliation phase after a
network partition. The experiments thus constitute valida-
tion tests for performance of the Java implementation of the
protocol. This provides a proof of concept for the algo-
rithms prior to integration in CORBA.
To provide a repeatable experimental setting for measur-
ing performance, we have implemented a synthetic applica-
tion that simulates changes of numerical values for object
states. This can, for example, be seen as an abstraction of
distributed sensor-actor systems, and fusion of data based
on reported measures. This application is introduced in Sec-
tion 2 and is used for explaining the reconciliation process.
The rest of the paper is organised as follows. Section 3
describes a partition-aware replication algorithm called P4,
and Section 4 describes the continuous service reconcilia-
tion protocol. In Section 5 we propose a set of metrics that
are suitable for evaluating availability in systems with opti-
mistic replication. Using these metrics we evaluate the pro-
posed algorithms in Section 6. Section 7 presents related
work, and ﬁnally we conclude and give directions for future
work.
2 Test application
A synthetic application has been developed to serve as a
test bed for trade-off studies. We describe it here to reuse
for illustration of the workings of the reconciliation process.
The application is composed of a set of real number ob-
jects. Possible operations are addition, multiplication and
division. An operation is applied to the current value with a
random constant. The constant is an integer uniformly dis-
tributed in the intervals [−10,−1], and [1, 10]. This creates
a total of 60 distinct operations. There are also integrity
constraints in the system expressed as: n1 + c < n2 where
n1 and n2 are object values and c is a constant. Although
the application is very simple, it is complex enough to give
an indication of how the algorithms perform. Moreover, the
application allows key system parameters to be changed for
Figure 1. System modes
experimentation purposes.
3 Replication
The system modes of operation can be described as the
four phases depicted in Figure 1. We proceed by describing
the need for a replication protocol that allows consistency
to be temporarily violated but later restored.
In the passive replication model each object has a pri-
mary copy, and the distributed replicas are updated using
a replication protocol. Traditional pessimistic replication
techniques that attempt to provide single-copy consistency
[5] are not suitable for optimistic partitionable systems in
which more than one partition continues to accept updates
during partitioning. Replication protocols for such systems
need to temporarily accept possible inconsistencies. Hence
these protocols allow the state in different partitions to di-
verge. If strict consistency is to be restored when the system
recovers, a reconciliation protocol is required. Replication
and reconciliation protocols need to match each other, as
only inconsistencies that can be removed at reconciliation
time can be allowed.
An optimistic protocol might allow the degree to which
inconsistencies are allowed to be conﬁgured. We have de-
signed an optimistic replication protocol, called Primary Per
Partition Protocol (P4), which uses a new approach to trade
consistency for system availability. The protocol bases con-
sistency on integrity constraints. Integrity constraints can
be pre-conditions, which have to met before an operation
is executed, post-conditions, which have to be met after an
operation is executed or invariants, which are not associated
to an operation but to a set of objects and have to be met at
all times. The remainder of this section provides a short
summary of the protocol that is described in detail earlier
[6].
The protocol assumes the presence of a group member-
ship service that provides all the server nodes with a single
view of which nodes are part of the system or the current
partition. Furthermore, a group communication service pro-
vides the nodes with reliable FIFO broadcast according to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
1: Partition3: Stopping service2: Network reunification4: Installation of new stateReconciling modeDegraded modeNormal modeInstalling stateUnavailable2341Fully availablePartially availablePartially available37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007the deﬁnition by Hadzilacos and Toueg [12].
The protocol employs a relaxed passive replication
model. Read-only operations are allowed on any replica,
but write operations have to be directed to the primary copy
of the object being accessed. If the primary copy of the ob-
ject is in a different partition, a secondary copy is promoted
to a temporary primary. In order to increase system avail-
ability, we allow write operations on temporary primaries
in certain conditions. During partitioning, secondary copies
of an object might be stale, if the primary copy resides in a
different partition. During reconciliation, constraints might
be violated retrospectively, when missed updates are propa-
gated. Therefore, some operations that were performed dur-
ing partitioning might have to be undone to restore consis-
tency. This behaviour might be acceptable for the majority
of the operations, but there are some operations that should
never occur, if they might have to be undone later on.
These “critical operations” include operations on data
that require strict consistency at all times and operations
that simply cannot be undone, such as operations with ir-
reversible side-effects. We therefore allow the labelling of
integrity constraints as critical constraints.
A constraint labelled critical is a constraint that needs
up-to-date versions of all of the participating objects. Such
a constraint cannot be evaluated if a participating object is
stale. Furthermore, the protocol has to take certain precau-
tions to ensure that critical constraints are never violated “in
In contrast non-critical
retrospect” during reconciliation.
constraints can be evaluated on stale objects. A non-critical
invariant constraint has to be re-evaluated during the recon-
ciliation; that is, the reconciliation protocol has to perform
constraint re-evaluation.
A write operation in our replication protocol in the ab-
sence of failures can be summarised in the following steps:
1. All object write invocations have to be directed to the
primary replica.
2. All the pre-condition constraints, associated with the
operation are evaluated. If a constraint is not met, the
invocation is aborted.
3. The operation is invoked. Nested invocations cause
sub-invocations to be started.
4. Once the primary replica has updated its local state, all
the post-condition and invariant constraints, associated
with the operation are evaluated. If a constraint is not
met, the invocation is aborted.
5. All primary replicas updated in the invocations propa-
gate the new object states to the secondary replicas.
6. Once this update transfer has terminated, the operation
result is returned to the client.
A failure might occur in the form of a node failure or
a link failure. Since we cannot distinguish between a failed
node and an isolated node, all failures are treated as network
partitions until recovery time. A write operation in degraded
mode is similar to that in normal mode with the following
additions:
1. If the primary copy of an object being written to is
not found, a secondary copy is chosen in some pre-
determined way, for example based on the replica iden-
tiﬁer. The chosen secondary replica is promoted to a
“temporary primary”. This is not done, if the operation
has a critical constraint as a pre- or post-condition.
2. Objects that are changed are marked as “revocable”,
if any of the invariant constraints associated to the op-
eration that has been executed has been evaluated on
possibly stale objects.
3. Critical constraints are not evaluated, if a participating
object might be stale. If this were the case, the invoca-
tion is aborted.
4. Non-critical invariant constraints with possibly stale
objects are marked for re-evaluation at reconciliation
time.
5. Operations with critical constraints that include a re-
vocable object are not permitted, so that critical con-
straints cannot be violated retrospectively.
Note that the above description applies both to operation
in degraded mode and continuous service of incoming oper-
ations during reconciliation. In order to manage the recon-
ciliation process (see below) the replication protocol needs
to log those operations that have been serviced while in par-
tition. These operations need to be reconsidered during the
reconciliation phase.
4 Reconciliation
This section describes the implementation of a proto-
col that aims to continuously service client (write) requests
even during the reconciliation process. A formal descrip-
tion of the protocol with a proof of correctness was pre-
sented earlier [1]. Here we show the architectural units that
have been realised in Java and their interactions in terms of
pseudo code.
Figure 2 shows the basic architectural components of our
replication and reconciliation protocols. Each node contains
the middleware and a number of application objects. The
middleware is composed of a number of services, of which
the replication support is the focus of this paper. This com-
ponent is in turn composed of a replication protocol, i.e.,
P4, and a reconciliation protocol, i.e., the continuous ser-
vice (CS) protocol. These protocols rely on additional mid-
dleware services such as Group Communication (GC) and
Constraint Consistency Manager (CCM). The CCM is used
to check consistency of integrity constraints. The box with
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007ciliation, the reconciliation manager will replay previously
applied operations. This replay process is performed in a
sandbox environment, which contains the application ob-
jects and the basic middleware components that are required
for running the application on a single node.
Algorithm 2 Reconciliation Manager
On reunify:
Elect which node acts as reconciliation manager
Determine which objects to reconcile
Send getState request to servers
On receive log:
Add log to opset
Send logAck to server
On receive state:
Create object in sandbox environment
If opset not empty and all states received:
Replay ﬁrst operation in opset in sandbox environment
Check consistency, abort if not consistent
If opset empty and all states received:
Send stop message to all servers
On receive stopAck:
Wait for opset to become empty
Send out new state to all servers
The responsibility of the continuous server is to accept
invocations from clients and sending logs to the elected rec-
onciliation manager during reconciliation. At the beginning
of each reconciliation phase the nodes in the repaired net-
work elect a reconciliation manager among themselves. The
reconciliation manager is responsible for merging server
logs that are sent during reconciling mode. Eventually,
upon reaching a stable state, the reconciliation manager
sends an install message with the new state to all servers
(see transition 4 in Figure 1).
During reconciliation mode, the state that is being con-
structed by the reconciliation manager may not yet reﬂect
all the operations that have arrived during degraded mode.
Therefore, the only state in which the incoming operations
can be applied to is one of the partition states from the de-
graded mode.
In other words, we keep virtual partitions
for servicing incoming operations while the reconciliation
phase lasts.
Each continuous server will immediately send a log mes-
sage to the reconciliation manager if it receives an invoca-
tion during the reconciliation phase. The server will then
wait until it has received an acknowledge from the manager
before sending a reply to the client. When the manager has
ﬁnished replaying all operations, it sends a message to all
nodes to stop accepting new invocations. The manager will
continue accepting log messages from servers, even after a
Figure 2. Architecture
”...” is an abstraction of other services in the middleware
not relevant for this evaluation. Our prototype implementa-
tion that is used for evaluating the reconciliation protocol is
based on this architecture.
We proceed by explaining the actions of the CS recon-
ciliation protocol. This protocol is faced with the task of
merging a number of operations that have been performed
in different partitions. It must also preserve constraint con-
sistency. Furthermore, as operations are replayed the client
perceived order on operations (for operations invoked by the
same client) is respected. In parallel with this process the
protocol takes care of operations that arrive at the reuniﬁed
but not fully reconciled partition.
Algorithm 1 Continuous Server
On reunify:
Send all logs to Reconciliation Manager(s)
On operation invocation:
If not stopped, apply operation
Check consistency, abort if not consistent
Send log to Reconciliation Manager
Suspend reply until later
On receive getState:
Send last stored object state (from normal mode)
On receive logAck:
Send suspended replies to client
On receive stop:
Stop accepting new operations
Send stopAck
On receive install:
Change state of local objects to received state
The reconciliation protocol is composed of two types of
processes: continuous servers and reconciliation managers.
Algorithms 1 and 2 show the pseudo code for the protocol
running at each node. Every node will run a continuous
server during the reconciliation, whereas only one elected
node will run the reconciliation manager. During recon-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
ObjectMiddlewareReconciliationManagerNodeSandboxReplicationReconciliationCCMCCMObject...SupportComponentP4GCCSContinuous Server...37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007an emphasis on hardware failures to justify a claim on a
system’s dependability.
In the context of this paper we are faced with a service
that is to be available on a distributed (networked) plat-
form. Measuring availability of the service is possible by
performing a number of experiments on the system running
over some time interval. To compute the probability of the
service being available, one can measure the periods that
the service is operational during the experiments, compute
an average operational period, and then compute the prob-
ability measure by dividing the average operational period
over the chosen interval. This measure is of course highly
affected by the potential number of failures during the ex-
perimental period. These failures can be induced (injected)
during experimentation, but their likelihood has to be sup-
ported by some empirical evidence obtained from the appli-
cation domains, using the hardware and software character-
istics of the real application. However, this is not the whole
story. The core problem of deﬁning metrics for consistency-
dependent distributed object systems is that in presence of