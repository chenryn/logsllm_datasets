[2K,4K)
11
[4K, 83]
5 1
[8K, 16K]
3 1
[16K,32K)
42 0rigin: 1 cn
on 21-Apr-2019
---
## Page 508
10.3 BPF Tools
471
1s/qpuas@
[32, 64)
1071889
[64, 128]
880886886081 950
(95z *821]
13918898
[256, 512}
29 1
[512, 1K)
141
[1K, 2K)
13118698
[2K,
4K)
886881 T51
[4K, 8K)
269 188988988
[8K, 16K)
8888888881 160
[16K,32K)
1563 188e88988 886 8868e9888e986e88988 886 8868e9888e98e1
[32x, 64K}
68888888868888116+
The output shows the packet sizes at the NIC (@nic_recv_bytes, @Pnic_send_bytes), and the packet
sizes for the kernel network stack (@recv_bytes, @Psend_bytes). This shows that the server was
receiving small packets, often smaller than 64 bytes, and mostly sending in the eight- to 64-Kbyte
range (which becomes a one- to two-Kbyte range after segmentation for the NIC). These are likely
1500 MTU sends.
This interface does not support TCP segmentation offload (TSO), so the GSO was used to segment
before delivery to the NIC. If TSO was supported and enabled,the @nic_send_bytes histogram
would also show large sizes, as segmentation happens later in NIC hardware.
Switching to jumbo frames will increase the packet size and system throughput, although there
can be issues with enabling jumbo frames in a datacenter, including consuming more switch
memory and worsening TCP incast issues.
This output can be compared to the earlier output of socksize(8).
sdeu dg u uaum8e qfua aq Suszreuums pue sjuodaoen aosap au Bupen q som s
The overhead may become measurable on high network I/O systems.
There is a Linux tool called iptraf-ng(8) that also shows histograms for network packet sizes.
However, iptraf-ng(8) works by packet sniffing and processing packets in user space. This costs
more CPU overhead than netsize(8), which summarizes in kernel space. For example, examining
the CPU usage of each tool during a localhost iperf(1) benchmark:
(6ugexadt dexBd) s d- qeqaptd +
Linux 4.15,0=47=genexle (1gud=bgzegg)
04/22/2019
_x86_64_
(0d> 8)
11:32:15 A UID
PID
Susx lsystem
5guest
5xait
ICPO
CPU Connand
11:32:16 
0
30825
18.00
74.00
0.00
0 .00
92,00
2 iptrsf=ng
11:32 :17 K
30825
21.00
70,00
0. 00
0 .00
91.00
030825
1 iptraf-ng
11:32:18 A
21.00
71.00
0.00
1.00
92,00
6 iptraf-ng
[.--]
---
## Page 509
472
 Chapter 10 Networking
T(ezteseu dex6d) s d- qepaptd 
Linux 4 .15, 047genex1c (1gud=bgzegg)
04/22/2019
_x86_64_
(0d3 8)
11:33: 39 A
UID
PID
5 guest
Ixait
SCP0
CPU Cornand
11:33: 40 A
30776
0. 00
0,00
0.00
0 . 00
0, 00
5 netsize,bt
11:33:41 AX
9LL0E0
0.00
0.00
11:33: 42 M
0.00
0 .00
0.00
7netsize,bt
030776
0.00
0,00
0., 00
0 . 00
0,00
1 netsize,bt
[..-]
iptraf-ng(8) consumes over 90% of one CPU to summarize packet sizes as histograms, whereas
netsize(8) consumes 0%. This highlights a key difference between the approaches, although there
are aditional overheads not shown here for kernel processing,
The source to netsize(8) is:
1/osr/locs1/bin/bpftrace
BEGIN
printf (*Tracing net device send/receive. Bit Ctr]-C to end.\o*)
tracepoint:net:netlf_receive_skb
Brecv_bytes = hist (args=>len) :
tracepoint:net:net_der_queue
Bsend_bytes = hist (args->len) 
tracepoint:net:napl_gro_receive_entzy
Bnic_recv_bytes = h1st (azgs=>1en.) 
tracepointinetinet_dev_xnit
Bnic_send_bytes = hist(args=>len.) 
This uses the net tracepoints to watch the send path and receive paths.
---
## Page 510
10.3 BPF Tools
473
10.3.27 nettxlat
nettxlat(8) shows network device transmission latency: the time spent pushing the packet into
the driver layer to enqueue it on a TX ring for the hardware to send out, until the hardware signals
the kernel that packet transmission has completed (usually via NAPl) and the packet is freed. For
example, from a busy prodluction edge server:
 nettxlat.bt
Attach.ing 4 probes..
fus1
[4, B]
22.301
[8, 16}
150679 1869889889886 8869869869869869
[16, 32)
275351 1e8e88e88e88e88ee8ee8ee8ee8ee88e88e88e88ee8ee8ee8ee8e1
[32, 64)
59898 188988988988
[64, 128]
886881 2567
[128, 256}
2761
[256, 512}
91
[512, 1K)
31
This shows that device queued time was usually faster than 128 microseconds.
The source to nettxlat(8) is:
#1/usx/1ocal/bin/bpftrace
BEGIN
1
1
tracepointinetinet_dev_start_xnit
Bstart[args->skbaddr] = nsecs
tracepoint:skb:consune_s3b
/8start [a.rgs=>skbaddr] /
Bus = hist.I (nsecs - @start [args=>skbaddr]1 / 1000) :
delete (@start[azgs=>skbaddr]) :
43 0rigin: I created it for this book on 21-Apr-201.9
---
## Page 511
474
Chapter 10 Networking
tracepointinetinet_dev_queue
// avoid tinestanp reuse1
delete (@start[args=>skbaddr])
END
clear (8start) 
This works by measuring the time from when a packet is issued to the device queue via the
net:net_dev_start_xmit tracepoint, and then when that packet is freed via the skb:consume_skb
tracepoint, which occurs when the device has completed sendling it.
There are some edge cases where a packet may not pass through the usual skb:consume_skb path:
this creates a problem as the saved timestamp may be reused by a later sk_buff, causing latency
outliers to appear in the histogram. This has been avoided by deleting timestamps on
net:net_dev_queue, to help eliminate their reuse.
As an example of breaking down by device name, the following lines were modified, turning
nettxlat(8) into nettxlat-dev(8): 
[..-]
#include 
#include 
[...]
tacepoint:akb1consune_s3b
/Bstart [args=>skbadd] /
1
$skb = (struct sk_buff *)args->skbaddr;
[00ot / ([xppeaxs /der/nu11*) ;
tracepoint:skb:kfree_skb
e [kstack (8] ]  =  count () :
END
1
system(*nstatr nstat =zs > /dev/nul1*) :
1
This begins by setting the nstat(8) counters to zero in the BEGIN action, and then using nstat(8)
again in the END action to print the interval counts, and then to reset nstat(8) back to its original
state (xs). This will interfere with other users of nstat(8) while tracing, Note that the bpftrace
-unsa fe option is necessary when executing this, due to the use of system().
10.3.29
skblife
skblife(8)* measures the lifespan of a sk_buff (skb), the object used to pass packets through the
kernel. Measuring the lifespan can show if there is latency within the network stack, including
packets waiting for locks. For example, on a busy production server:
+skblife.bt
Attach.ing 6 probes...
eskb_resldency_nsecs:
[1K, 2K)
1631
[2K, 4K)
792 189
[4K, 8K]
66866886081 1657
[8K, 16K]
88868868868812200
[16K,32K]
12695 888e8888 88e 8e881
[323K,
64K)
: on 4-Apr-2019
---
## Page 515
478
Chapter 10 Networking
[64K, 128K)
3277 186988888868
[128K, 256K]
888886886881 9562
[256K, 512K]
16081999989
[512K,18}
1594 188e889
[1M, 2M)
583188
[2M, 4N)
435 18
[4M, 8M)
31718
[BM, 168)
104I