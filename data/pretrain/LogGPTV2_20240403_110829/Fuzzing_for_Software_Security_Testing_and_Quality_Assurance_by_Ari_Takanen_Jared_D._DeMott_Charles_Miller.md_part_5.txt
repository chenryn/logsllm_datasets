the process. In reactive measures, you react to external events and keep running
around putting fires out. In proactive security measures, you take a step back and
start looking at the system through the eyes of a hacker. Again, a hacker in this
sense is not necessarily a criminal. A hacker is a person who, upon hearing about
a new technology or a product, will start having doubts on the marketing terms
and specifications and will automatically take a proactive mindset into analyzing
various technologies. Why? How? What if? These are just some of the questions
someone with this mindset will start to pose to the system.
Let us take a look at the marketing terms for proactive security from various
commercial fuzzer companies:
XXX enables companies to preemptively mitigate unknown and published
threats in products and services prior to release or deployment—before
systems are exposed, outages occur, and zero-day attacks strike.
By using YYY, both product developers and end users can proactively verify
security readiness before products are purchased, upgraded, and certainly
before they are deployed into production environments.
In short, proactive, predeployment, or preemptive software security is equal to
catching vulnerabilities earlier in the software development life cycle (SDLC), and
catching also those flaws that have not been disclosed publicly, which traditional
reactive security measures cannot detect. Most traditional security solutions attempt
to detect and block attacks, as opposed to discovering the underlying vulnerabilities
that these attacks target (Figure 1.3).
A post-deployment, reactive solution depends on other people to ask the ques-
tions critical for security. An example of a reactive solution is a signature based
antivirus system. After a new virus emerges, researchers at the security vendor start
poking at it, trying to figure out what makes it tick. After they have analyzed the
new virus, they will take protective measures, trying to detect and eliminate the
virus in the network or at a host computer. Another example of a reactive solution
6760 Book.indb 10 12/22/17 10:50 AM
1.1 Software Security 11
Figure 1.3 Reactive postdeployment versus proactive predeployment security measures.
is an intrusion detection system (IDS) or an intrusion prevention system (IPS). These
systems look for known exploits and block them. They do not attempt to identify
which systems are vulnerable or what vulnerabilities exist in software. One could
argue that even a firewall is reactive solution, although the pace of development is
much slower. A firewall protects against known attacks by filtering communica-
tion attempts at the perimeter. The common thread with most post-deployment
security solutions is that they all target attacks, and not vulnerabilities. They are
doomed to fail because of that significant difference. Every time there is a unique
vulnerability discovered, there will be hundreds, if not thousands of attack vari-
ants trying to exploit that worm-sized hole. Each time a new attack emerges, the
retroactive security vendors will rush to analyze it and deploy new fingerprints to
their detection engines. But based on article by Yankee Group, unfortunately they
only detect less than 70% of attacks, and are often between 30 and 60 days late.7
One could argue that security scanners (or vulnerability scanners) are also pro-
active security tools. However, security scanners are based on known attacks and
exhibit the same problems as other reactive security solutions. A security scanner
cannot find a specific vulnerability unless the vulnerability is publicly known. And
when a vulnerability becomes known, attacks usually already exist in the wild
exploiting it. That does not sound very proactive, does it? You still depend on some-
one else making the decisions for you, and in their analyzing and protecting your
assets. Security scanners also look for known issues in standard operating systems
and widely used hosts, as data on known vulnerabilities is only available for those
platforms. Most tests in security scanners are based on passive probing and finger-
printing, although they can contain active hostile tests (real exploits) for selected
known issues. Security scanners cannot find any unknown issues, and they need
regular updating of threats. Security scanners also rarely support scanning anything
but very popular operating systems and selected network equipment.
A problem that is becoming more challenging for reactive security solutions that
depend on public disclosure is that they do not know the problems (vulnerabilities)
anymore. This is because the public disclosure movement has finally died down.
However, raising awareness about security mistakes can only be a good thing. Public
disclosure is fading because very few people actually benefit from it. Manufacturers
and software developers do not want to publish the details of vulnerabilities, and
7 “Anti-Virus Is Dead; Long Live Anti-Malware.” Published by Yankee Group. Jan. 17, 2007.
www.marketresearch.com/map/prod/1424773.html (currently unavailable).
6760 Book.indb 11 12/22/17 10:50 AM
12 Introduction
therefore today we may not know if they have fixed the problems or not. Hackers do
not want to publish the details, as they can sell them for profit. Corporate enterprise
customers definitely do not want to publish any vulnerability details, as they are
the ones who will get damaged by any attacks leading to compromises. The only
ones who publish details are security companies trying to make you believe they
actually have something valuable to offer. Usually, this is just bad and irresponsible
marketing, because they are getting mostly second-hand, used vulnerabilities that
have already been discovered by various other parties and exploited in the wild.
A proactive solution will look for vulnerabilities and try to resolve them before
anyone else learns of them and before any attacks take place. As we said, many
enterprise security measures fail because they are focused on known attacks. A
truly proactive approach should focus on fixing the actual flaws (unknown zero-
day vulnerabilities) that enable these attacks. An attack will not work if there is no
underlying vulnerability to exploit. Vulnerability databases indicate that program-
ming errors cause 80% of vulnerabilities, so the main focus of security solutions
should probably be in that category of flaws. Based on research conducted at the
PROTOS project8 and also according to our experience at commercial fuzzing
companies, 80% of software will crash when tested via fuzzing. That means we
can find and eliminate many of those flaws with fuzzing, if we spend the effort in
deploying fuzzing.
1.1.6 Security requirements
We have discussed fuzzing and its uses, but the truth is not all software is security-
critical, and not all software needs fuzzing. Just as is the case with all security
measures, introducing fuzzing into development or deployment processes needs to
be based on the requirement set for the system. Unfortunately, traditional security
requirements are feature-driven and do not really strike a chord with fuzzing.
Typical and perhaps the most common subset of security requirements or security
goals consists of the following: confidentiality, integrity, and availability. Fuzzing
directly focuses on only one of these, namely availability, although many vulner-
abilities found using fuzzing can also compromise confidentiality and integrity by
allowing an attacker to execute malicious code on the system. Furthermore, the tests
used in fuzzing can result in corrupted databases, or even in parts of the memory
being sent back to the fuzzer,9 which also constitute attacks against confidentiality
and integrity.
Fuzzing is much closer to the practices seen in quality assurance than those
related to traditional security requirements. This may have been one of the main
reasons why fuzzing has not been widely adopted so far in software engineering
processes: Security people have mostly driven its deployment. Without solid require-
ments to fulfill, you only end up with a new tool with no apparent need for it. The
result is that your expensive fuzzer equipment ends up just collecting dust in some
far-away test lab.
8 PROTOS project at OUSPG, 1998-2001, https://www.ee.oulu.fi/research/ouspg/Protos.
9 In the Heartbleed bug there is an example of a fuzzer-found issue where the system when tested with
fuzzing returned chunks of memory back to the attacker. http://www.heartbleed.com.
6760 Book.indb 12 12/22/17 10:50 AM
1.2 Software Quality 13
1.2 Software Quality
Thrill to the excitement of the chase! Stalk bugs with care, methodology,
and reason. Build traps for them. . . . Testers! Break that software (as you
must) and drive it to the ultimate—but don’t enjoy the programmer’s pain.
Boris Beizer10
People who are not familiar with testing processes might think that the purpose of
testing is to find flaws. And the more flaws found, the better the testing process is.
Maybe this was the case a long time ago, but today things are different. Modern
testing is mostly focused on two things: verification and validation (V&V). Although
both terms are used ambiguously, there is an intended difference.
Verification attempts to answer the question: “Did we build the product right?”
Verification is more focused on the methods (and in the existence of these methods),
such as checklists, general process guidelines, industry best practices, and regula-
tions. Techniques in verification ensure that the development, and especially the
quality assurance process, is correct and will result in reduction of common mistakes.
Validation, on the other hand, asks: “Did we build the right product?” The focus
is on ensuring and documenting the essential requirements for the product and in
building the tests that will check those requirements. For any successful validation
testing, one needs to proactively define and document clear pass/fail criteria for all
functionality so that eventually when the tests are done, the test verdicts can be
issued based on something that has been agreed upon beforehand.
Unfortunately, fuzzing does not fit well into this V&V model, as we will see
here, and later in more detail in Chapter 3.
Testing is a time-consuming process that has been optimized over time at the
same time that software has become more complex. With increasing complex-
ity, devising a completely thorough set of tests has become practically impossible.
Software development with a typical waterfall model and its variants—such as
the iterative development process—proceed in phases from initial requirements
through specification, design, and implementation, finally reaching the testing and
post-deployment phases. These phases are rarely completely sequential in real-life
development, but run in parallel and can revisit earlier steps. They can also run in
cycles, such as in the spiral model. The system requirements that drive testing are
drafted very early in the development process and change constantly. This change
is especially true for various agile processes, where test requirements may be only
rarely written down due to the fast change process.
If we look at fuzzing from a quality assurance perspective, fuzzing is a branch
of testing; testing is a branch of quality control; quality control is a branch of qual-
ity assurance. Fuzzing differs from other testing methods in that it
• Tends to focus on input validation errors;
• Tends to focus on actual applications and dynamic testing of a finished product;
10 Quote is from Software Testing Techniques, 2nd ed., Boris Beizer, International Thomson Computer
Press. 1990. Abbreviated for brevity.
6760 Book.indb 13 12/22/17 10:50 AM
14 Introduction
• Tends to ignore the responses, or valid behavior;
• Concentrates mostly on testing interfaces that have security implications.
In this section, we’ll look at different kinds of testing and auditing of software
from a tester’s perspective. We will start with identifying how much you need to
test (and fuzz) based on your needs. We will then define what a testing target is
and follow that up with some descriptions of different kinds of testing as well as
where fuzzing fits in with these definitions. Finally, we will contrast fuzzing with
more traditional security measures in software development such as code auditing.
1.2.1 Cost-Benefit of Quality
From a quality assurance standpoint, it is vital to understand the benefits from
defect elimination and test automation. One useful study was released in January
2001, when Boehm and Basili reviewed and updated their list of metrics on the
benefits of proactive defect elimination. Their software defect reduction top 10 list
includes the following items:11
1. Finding and fixing a software problem after delivery is often 100 times
more expensive than finding and fixing it during the requirements and
design phase.
2. Current software projects spend about 40% to 50% of their effort on avoid-
able rework.
3. About 80% of avoidable rework comes from 20% of the defects.
4. About 80% of the defects come from 20% of the modules, and about half
of the modules are defect free.
5. About 90% of the downtime comes from, at most, 10% of the defects.
6. Peer reviews catch 60% of the defects.
7. Perspective-based reviews catch 35% more defects than nondirected reviews.
8. Disciplined personal practices can reduce defect introduction rates by up
to 70%.
9. All other things being equal, it costs 50% more per source instruction to
develop high-dependability software products than to develop low-depend-
ability software products. However, the investment is more than worth it if
the project involves significant operations and maintenance costs.
10. About 40% to 50% of users’ programs contain nontrivial defects.
Although this list was built from the perspective of code auditing and peer review
(we all know that those are necessary), the same applies to security testing. If you
review each point above from a security perspective, you can see that all of them
apply to vulnerability analysis, and to some extent also to fuzzing. This is because
every individual security vulnerability is also a critical quality issue, because any
crash-level flaws that are known by people outside the development organization have
to be fixed immediately. The defects found by fuzzers lurk in an area that current
11 Victor R. Basili, Barry Boehm, “Software defect reduction top 10 list,” Computer (January 2001):
135–137.
6760 Book.indb 14 12/22/17 10:50 AM
1.2 Software Quality 15
development methods such as peer reviews fail to find. These defects almost always
are found only after the product is released and someone (a third party) conducts
fuzz tests. Security is a subset of software quality and reliability, and the method-
ologies that can find flaws later in the software life-cycle should be integrated to
earlier phases to reduce the total cost of software development.
The key questions to ask when considering the cost of fuzzing are the following.
1. What is the cost per defect with fuzzing? Some people argue that this met-
ric is irrelevant, because the cost per defect is always less than the cost of a
security compromise. These people recognize that there are always benefits
in fuzzing. Still, standard business calculations such as ROI (return on invest-
ment) and TCO (total cost of ownership) are needed in most cases also to
justify investing in fuzzing.
2. What is the test coverage? Somehow you have to be able to gauge how well
your software is being tested and what proportion of all latent problems are
being discovered by introducing fuzzing into testing or auditing processes.
Bad tests done with a bad fuzzer can be counterproductive, because they
waste valuable testing time without yielding any useful results. At worst
case, such tests will result in over-confidence in your product and arrogance
against techniques that would improve your product.12 A solid fuzzer with
good recommendations and a publicly validated track record will likely prove
to be a better investment coverage-wise.
3. How much should you invest in fuzzing? The motivation for discussing the
price of fuzzing derives from the various options and solutions available in
the market. How can you compare different tools based on their price, overall
cost of usage, and testing efficiency? How can you compare the total cost
of purchasing an off-the-shelf commercial fuzzer to that of adopting a free
fuzzing framework and hiring people to design and implement effective tests
from the ground up? Our experience in the market has shown that the price
of fuzzing tools is not usually the biggest issue in comparisons. In commer-
cial fuzzing, the cheapest tools usually prove to be the simplest ones—and
also without exception the worst ones from a testing coverage, efficiency,
and professional testing support standpoint. Commercial companies look-
ing for fuzz testing typically want a fuzzer that (a) supports the interfaces
they need to test, (b) can find as many issues as possible in the systems they
test, and (c) are able to provide good results within a reasonable timeframe.
There will always be a place for both internally built tools and commercial tools.
A quick Python13 script might be better suited to fuzz a single isolated custom appli-
cation. But if you are testing a complex communication protocol implementation or
a complete system with lots of different interfaces, you might be better off buying a
12 We often hear comments like: “We do not need fuzzing because we do source code auditing” or
“We do not need that tool because we already use this tool,” without any consideration if they are
complementary products or not.
13 We mention Python as an example script language due to the availability of PyDBG by Pedram
Amini. See PaiMei documentation for more details: http://pedram.openrce.org/PaiMei/docs/.
6760 Book.indb 15 12/22/17 10:50 AM
16 Introduction
fuzzing tool from a commercial test vendor to save yourself a lot of time and pain in
implementation. Each option can also be used at different phases of an assessment.
A typical starting point to analyze fuzzing needs is to conduct a QA risk analy-
sis, and as part of that, possibly conduct necessary ad hoc tests using simple scripts
that review the need for more thorough fuzzing. If critical flaws are found then
increase test coverage by testing your product thoroughly with a commercial testing
tool or readily available open source fuzzer to reach a base level of tests. Depend-
ing on the type of application and customer requirements you might also need to
hire a professional security auditing firm to do a second check, a validation of the
results and methods.
1.2.2 Target of Test
In some forms of testing, the target of testing can be any black box. All of the vari-
ous types of functional tests can be directed at different kinds of test targets. The
same applies for fuzzing. A fuzzer can test any application, whether it is running
on top of Web, mobile, or VoIP infrastructure, or even when it is just a standalone
application. The target of a test can be a single network service, or it can be an
entire network architecture. Common names used for test targets include:
• SUT (system under test). An SUT can consist of several subsystems, or it can
represent an entire network architecture with various services running on
top of it. An SUT can be anything from banking infrastructure to a complex
telephony system. SUT is the most abstract definition of a test target, because
it can encompass any number of individual destinations for the tests.
• DUT (device under test). A DUT is typically one single service or a piece
of equipment, possibly connected to a larger system. Device manufacturers
mainly use the term DUT. Some examples of DUTs include routers, WLAN
access points, VPN gateways, DSL modems, VoIP phones, Web servers, or
mobile handsets.
• IUT (implementation under test). An IUT is one specific software implemen-
tation, typically the binary representation of the software. It can be a process
running on a standard operating system, or a Web application or script run-
ning on an application server.
• UUT (unit under test). Subcomponent of an implementation, a module, library,
or a function that is tested. For fuzzing, this is mostly relevant only when
specific stubs are built so that a subcomponent can be tested without the rest
of the implementation.
In this book, we will most often refer to a test target as an SUT, because this
term is applicable to all forms of test setups.
1.2.3 Testing purposes and Test Verdicts
The main focus of fuzzing is on functional security assessment. As fuzzing is essen-
tially functional testing, it can be conducted in various steps during the overall