one proposed in [36]. The functionality of this component
is to parse the rendered webpage and extract the attributes of
display ads, which also contain the landing pages.
Ad Extractor. In most modern websites, the displayed ads
are embedded in iFrame tags that create deep nesting layers,
containing numerous and different types of elements. How-
3Destination websites the user is redirected to when clicking on the ads.
ever, the ads served by the control pages are found directly
inside the iFrames so the module does not have to handle
such complex behavior. Therefore, the module ﬁrstly iden-
tiﬁes all the active iFrame elements and ﬁlters out the in-
valid ones that have either empty content or zero dimensions.
Then, it retrieves the href attributes of image and ﬂash ads
and parses the URLs, while searching for speciﬁc string pat-
terns such as adurl=, redirect=, etc. These patterns are typ-
ically used by the ad-networks for encoding URLs in web-
pages. Next, the module forms the list of candidate landing
pages, which are then processed and analyzed to create the
set of true landing pages. The Ad Extractor is fully com-
patible with the crawlers, and does not need to perform any
clicks on the ad-elements, since it extracts only the landing
pages’ URLs directly from the rendered webpage. After col-
lecting the candidate landing pages, the module ﬁlters them
with the EasyList [21], similarly to previous works [7, 22],
and stores only the true active ad-domains. Finally, the Page
Parser & Ad Extractor module also stores metadata from the
crawls such as: time and date of execution, number of identi-
ﬁed ads, number of categories, type and phase of crawl, etc.
Ad Categorizer. To associate landing pages or browsing
URLs with web categories, we employ the McAfee Trusted-
Sources database [41], which provides URLs organized into
categories. This system was able to categorize 96% of the
landing pages of our collection into a total of 76 unique cat-
egories, by providing up to four semantic categories for each
page, while the remaining 4% domains were manually clas-
siﬁed to the categories above. The ﬁnal output contains the
landing pages of collected ads, along with their categories.
4.4 CDT Detection
Probabilistic CDT is a kind of task generally suitable for
investigation through ML. Previous work [56] and industry
directions [38, 4] claim that probabilistic device-pairing is
based on speciﬁc, well-deﬁned signals such as: IP address,
geolocation, type and frequency of browsing activity. Since
we control these parameters in our methodology, by deﬁ-
nition we construct the ground truth with our experimental
setups. That is, we control (i) the devices used, which are
potentially paired under a given IP address, geolocation and
browsing patterns, (ii) the control instance of baseline desk-
top device, and (iii) the browsing with the personas.
Before applying any statistical method, every instance of
the input data has to be transformed into a vector of values;
each position in the vector corresponds to a feature. Features
are different properties of the collected data: browsing ac-
tivity of a user during training time, experimental setup used
(persona, etc.), time-related details of the experiment, as well
as information about the collected ads, which is the output
signal received from the given browsing activity. These fea-
tures can be studied systematically to identify statistical as-
sociation between the input and output signals, given an ex-
232          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX Associationperimental setup. In effect, our feature space is comprised
of a union of these vectors, since all features are either con-
trolled, or measurable by us (detailed description of the fea-
tures is given in Appendix, Table 5). The only unknown is
whether the ad-ecosystem has successfully associated the de-
vices, and if it has exhibited this in the output signal via ads.
One Dimension Statistical Analysis. At the ﬁrst level of
analysis, to measure the similarity of distribution of ads de-
livered in the different devices, we compare the signals us-
ing a two-tailed permutation test and reject the null hypoth-
esis that the frequency of ads delivered (for a given cate-
gory) comes from the same distribution, if the t-test statistic
leads to a p-value smaller than a signiﬁcance level α < 0.05.
Multidimensional Statistical Analysis. Given that a uni-
dimensional test such as the previous one does not take into
account the various other features available in each experi-
ment, we further consider ML, which take into account mul-
tidimensional data, to decide if the ads delivered in each de-
vice are from the same distribution or not. We transform
the problem of identifying if the previously exported vec-
tors are similar enough, into a typical binary classiﬁcation
problem, where the predicted class describes the existence
of pairing or not, that may have occurred between the mo-
bile device and one of the two desktop devices. As a paired
combination we consider the desktop device that exists un-
der the same IP address with the mobile device. The “not
paired” combination is the mobile device and the baseline
desktop. The analysis is based on three classiﬁcation algo-
rithms with different dependences on the data distributions.
An easily applied classiﬁer that is typically used for perfor-
mance comparison with other models, is the Gaussian Naive
Bayes classiﬁer. Logistic Regression is a well-behaved clas-
siﬁcation algorithm that can be trained, as long as the classes
are linearly separable. It is also robust to noise and can avoid
overﬁtting by tuning its regularization/penalty parameters.
Random Forest and Extra-Trees classiﬁers, construct a mul-
titude of decision trees and output the class that is the mode
of the classes of the individual trees. Also they use the Gini
index metric to compute the importance of features.
A fundamental point when considering the performance
evaluation of ML algorithms is the selection of the appropri-
ate metrics. Pure Accuracy can be used, but it’s not repre-
sentative for our analysis, since we want to report the most
accurate estimation for the number of predicted paired de-
vices, while at the same time measure the absolute number of
miss-classiﬁed samples overall. For this reason, metrics like
Precision, Recall and F1-score, and the Area Under Curve
of the Receiver Operating Curve (AUC) are typically used,
since they can quantify this type of information.
5 Experimental Evaluation
We use the Talon framework to perform various experiments
and construct different datasets for each. Since every ex-
Table 1: Characteristics of the datasets used in each setup
(S) of experiments. S={1,2,3} are the setups of experiments
in § 5.2, § 5.3 and § 5.4, respectively; ttotal: the total du-
ration of experiment; ttrain: the training duration; ttest: the
testing duration; I: independent personas; C: data combined
from personas; SF: stateful browser; SL: stateless browser;
B: boosted CDT browsing.
S
1a
1b
2a
2b
2c
2d
3a
3b
Personas
10 (I, SF)
10 (C, SF)
2 (I, SF)
2 (C, SF)
2 (I, SF, B)
2 (C, SF, B)
5 (I, SL)
5 (C, SL)
Runs
4
-
4
-
4
-
2
-
-
-
-
-
ttrain
15min
ttest
20min
ttotal
37 days
480min
30min
6 days
480min
30min
6 days
15min
20min
9 days
Samples Features
240
2400
192
384
192
384
120
600
1100
2201
600
750
500
576
450
880
-
-
-
-
-
-
-
-
perimental setup has different experimental parameters (i.e.,
training and testing time, number of personas, browsing
functionalities), the datasets vary in terms of samples size
and feature space. The datasets collected during our experi-
ments and used in our analysis are presented in the Table 1.
5.1 Does IP-sharing allow CDT?
A ﬁrst set of preliminary experiments were performed to
demonstrate that our platform can (i) successfully identify
and collect the ads delivered to our multiple devices (mobile
and desktops), (ii) inject browsing signal from a device, thus
biasing it to have a realistic persona and (iii) lead to match-
ing/pairing of devices, which could be due to same behav-
ioral ads, retargeting ads or CDT.
First, we use a simple experimental setup: we connect
three instances of desktop devices and one mobile device un-
der the same IP address. We create one persona (as in § 4.1),
with an interest in “Online Shopping-Fashion, Beauty”, and
following the described timeline of phases, we run this ex-
periment for two days. Then, we perform one-dimensional
statistical analysis, as introduced in § 4.4, and ﬁnd that there
is no similarity between the mobile with any of desktop de-
vices (null hypothesis rejected with highest p-value=0.030),
while all desktop distributions are similar to each other (null
hypothesis accepted with lowest p-value=0.33). These statis-
tical results indicate that there is no clear device-pairing (at
the level of ad distribution for the given persona), and that
we should consider controlling more factors to instigate it.
Consequently, we expand this experiment by also training
one of the desktop devices using the same persona as with
mobile. By repeating the same statistical tests, we ﬁnd that
the mobile and desktop with the same browsing behavior re-
ceive ads coming from the same distribution (null hypothesis
accepted with lowest p-value=0.84), while the other desk-
top devices show no similarity with each other or the mobile
(null hypothesis rejected with highest p-value=0.008). This
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 233result indicates that browsing behavior under a shared IP ad-
dress can boost the signal towards advertisers, which they
can use to apply advanced targeting, either as CDT, or retar-
geting on each device or a mixture of both techniques.
Finally, these preliminary experiments and statistical tests
provide us with evidence regarding the effectiveness of our
framework to inject enough browsing signal from different
devices under selected personas. Our framework is also able
to collect ads delivered between devices, that can be later
analyzed and linked back to the personas. Those are fun-
damental components for our system and importantly they
are potentially causing CDT between the devices involved.
Next, we present more elaborate experimentations with our
framework, in order to study CDT in action.
5.2 Does short-time browsing allow CDT?
Independent Personas: Setup 1a. This experimental setup
emulates the behavior of a user that browses frequently about
some topics, but in short-lived sessions in her devices. Given
that most users do not frequently delete their local brows-
ing state, this setup assumes that the user’s browser stores
all state, i.e., cookies, cache, browsing history. This enables
trackers to identify users more easily across their devices, as
they have historical information about them. In this setup,
every experimental run starts with a clean browser proﬁle;
cookies and temporary browser ﬁles are stored for the whole
duration of the experimental run (stateful). We use all per-
sonas of Table 3, and the data collection for each lasts 4 days.
We perform the same statistical analysis as in § 5.1, and
ﬁnd that in 4/10 personas, the mobile and paired desktop
ads are similar (null hypothesis accepted with lowest p-
value=0.13), while the mobile and baseline desktop ad dis-
tributions are different (null hypothesis is rejected with high-
est p-value=0.009). This inconsistency is reasonable since
the statistical analysis is based only on one dimension (the
frequency count of types of ads appearing in the devices),
which may not be enough for fully capturing the existence of
device-pairing. For this reason, we choose to use more ad-
vanced, multidimensional ML methods which take into ac-
count the various variables available, to effectively compare
the potential CDT signals received by the two devices.
The classiﬁcation results of the Random Forest (best per-
forming) algorithm are reported in Table 2. We use AUC
score as the main metric in our analysis, since the ad-industry
seems to prefer higher Precision scores over Recall, as the
False Positives have greater impact on the effectiveness of
ad-campaigns.4 As shown in Table 2, the model achieves
high AUC scores for most of the personas, with a maximum
value of 0.84. Speciﬁcally, the personas 2, 4 and 8 scored
4Tapad [1] mentions: “Maintaining a low false positive rate while also
having a low false negative rate and scale is optimal. This combination is
a strong indicator that the Device Graph in question was neither artiﬁcially
augmented nor scrubbed.”
Table 2: Performance evaluation for Random Forest in Se-
tups 1a and 1b. Left value in each column is the score for
Class 0 (C0=not paired desktop); right value for Class 1
(C1=paired desktop).
Persona
(Setup)
1 (1a)
2 (1a)
3 (1a)
4 (1a)
5 (1a)
6 (1a)
7 (1a)
8 (1a)
9 (1a)
10 (1a)
combined (1b)
Precision
C1
C0
0.60
0.89
0.78
0.84
0.81
0.73
0.78
0.87
0.65
0.94
0.67
0.57
0.87
0.81