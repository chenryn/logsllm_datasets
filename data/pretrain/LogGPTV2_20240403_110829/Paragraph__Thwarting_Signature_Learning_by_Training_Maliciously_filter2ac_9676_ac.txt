e
t
c
e
f
n
I
s
t
s
o
H
e
b
a
r
e
n
u
V
l
l
Fig. 4. Worm propagation. L=1000, V=1000000, α= 400
signature to all hosts in less than the time it takes to infect all vulnerable hosts is a
challenge [22].3
We assume that the worm scans addresses uniformly at random. In reality, there are
several potential strategies a worm author might use to minimize the number of sam-
ples seen by the learner. An ideally coordinated worm may scan every address exactly
once, thus minimizing the number of samples sent to any one of the learner’s addresses,
and eliminating ‘wasted’ scans to already-infected hosts. The worm could further im-
prove this approach by attempting to order the addresses by their likelihood of being
monitored by the learner, scanning the least likely ﬁrst.
We model the worm by estimating the number of additional vulnerable hosts infected
in-between the learner receiving new worm samples. Note that because we assume sig-
nature updates are instantaneous, the scan rate of the worm is irrelevant. Intuitively, both
the rate of infection and the rate of the learner receiving new samples are proportional
to the scan rate, thus canceling each other out.
Theorem 4. For a worm scanning uniformly at random, where there are V vulnerable
hosts, L addresses monitored by the learner, and N total hosts, the expected number of
infected hosts I after s worm samples have been seen by the learner is:
I[s] = I[s− 1] + (V − I[s− 1])
(cid:3)
1−
(cid:4)
1− F[s− 1]
(cid:6)
(cid:5)(N/L)
N
Derivation. The expected number of worm scans in-between the learner receiving a
new worm sample is
P(scan is seen by learner) = N
L .
1
3 The Dropped Red Herring attack in particular is much more devastating when taking the sig-
nature generation and distribution time into account, since the next spurious feature is not
revealed before an updated signature is distributed. Hence, a worm using α spurious features
is allowed to propagate freely for at least α times the time needed to generate and distribute a
signature.
92
J. Newsome, B. Karp, and D. Song
Innocuous Pool
False Positive Rate
100%
Suspicious Pool
False Negative Rate
Innocuous Pool
False Positive Rate
100%
Suspicious Pool
False Negative Rate
e
t
a
R
F%
e
t
a
R
F%
Threshold
τ
Tzfp
Tzfn
Threshold
τ
Tzfn Tzfp
Fig. 5. Training data distribution graph, used to
set τ. τ could be set to perfectly classify train-
ing data.
Fig. 6. Overlapping training data distribution
graph. No value of τ perfectly classiﬁes train-
ing data.
I[s] = I[s− 1] + (# vulnerable uninfected hosts)P(host becomes infected)
I[s] = I[s− 1] + (V − I[s− 1])(1− P(host does not become infected))
I[s] = I[s− 1] + (V − I[s− 1])(1− P(scan does not infect host)(# scans))
I[s] = I[s− 1] + (V − I[s− 1])(1− (1− P(scan infects host))(# scans))
I[s]=I[s− 1] + (V − I[s− 1])(1− (1− P(scan contacts host)P(scan not blocked))(# scans))
I[s] = I[s− 1] + (V − I[s− 1])(1− (1− 1
N F[s− 1])(N/L))
In Figure 4, we model the case of V = one million vulnerable hosts, L = one thou-
sand learner-monitored addresses, and N = 232 total addresses. In the case where the
worm is maximally-varying polymorphic, we assume that the learner needs ﬁve sam-
ples to generate a correct signature. In that case, only 4,990 (.0499%) vulnerable hosts
are infected before the correct signature is generated, stopping further spread of the
worm. By employing the Dropped Red Herring attack, the worm author increases this
to 330,000 (33.0%). The Randomized Red Herring attack is only slightly less effective,
allowing the worm to infect 305,000 (30.5%) vulnerable hosts using p = .999.
Given that the Dropped Red Herring and Randomized Red Herring attacks allow a
worm to infect a large fraction of vulnerable hosts even under this optimistic model, it
appears that the Conjunction Learner is not a suitable signature generation algorithm
for a distributed worm signature generation system.
4 Attacks on Bayes Learners
Bayes learners are another type of learner used in several adversarial learning appli-
cations, including worm signature generation and spam ﬁltering. We present several
practical attacks against Bayes learners, which can prevent the learner from ever gen-
erating an accurate signature, regardless of how many target-class samples it collects.
As a concrete example, we use Polygraph’s implementation of a Naive Bayes learner.
That is, a Bayes learner that assumes independence between features. Non-Naive Bayes
learners are not as commonly used, due partly to the much larger amount of training data
that they require. We believe that the attacks described here can also be applied to other
Paragraph: Thwarting Signature Learning by Training Maliciously
93
Bayes learners, possibly even non-naive ones that do not assume independence between
features.
4.1 Background on Bayes Learners
In the following discussion, we use the notation P(x|+) to mean the probability that
the feature or set of features x occurs in malicious samples, and P(x|−) to denote the
probability that it occurs in innocuous samples. This learner can be summarized as
follows:
– The learner identiﬁes a set of tokens, σ, to use as features. σis the set of tokens that
occur more frequently in the suspicious pool than in the innocuous pool. That is,
∀σi ∈ σ,P(σi|+) > P(σi|−). This means that the presence of some σi in a sample
to be classiﬁed can never lower the calculated probability that it is a worm.
– Classiﬁes a sample as positive (i.e., in the target class) whenever P(γ|+)
P(γ|−) > τwhere τ
is a threshold set by the learner, and γis the subset of σ that occurs in the particular
sample. We refer to P(γ|+)
– We assume conditional independence between features. Hence, P(γ|+)
P(γ|−) = ∏ P(γi|+)
P(γi|−)
– P(σi|−) is estimated as the fraction of samples in the innocuous pool containing
σi.
– P(σi|+) is estimated as the fraction of samples in the suspicious pool containing
σi.
– τ is chosen as the value that achieves a false positive rate of no more than F% in
P(γ|−) as the Bayes score, denoted score(γ)
the innocuous pool.
Setting the τ Threshold. The attacks we describe in this section all involve making it
difﬁcult or impossible to choose a good matching threshold, τ. For clarity, we describe
the method for choosing τ in more detail.
After the learner has chosen the feature set σ and calculated P(σi|+)
P(σi|−) for each fea-
ture, it calculates the Bayes score P(σ|+)
P(σ|−) for each sample in the innocuous pool and
suspicious pool, allowing it to create the training data distribution graph in Figure 5.
The training data distribution graph shows, for every possible threshold, what the cor-
responding false positive and false negative rates would be in the innocuous and suspi-
cious training pools. Naturally, as the threshold increases, the false positive rate mono-
tonically decreases, and the false negative rate monotonically increases. Note that Fig-
ure 5 and other training data distribution graphs shown here are drawn for illustrative
purposes, and do not represent actual data.
There are several potential methods for choosing a threshold τ based on the training
data distribution graph. The method described in Polygraph [15] is to choose the value
that achieves no more than F% false positives in the innocuous pool. One alternative
considered was to set τto Tz f p, the lowest value that achieves zero false positives in the
innocuous pool. However, in the examples studied, a few outliers in the innocuous pool
made it impossible to have zero false positives without misclassifying all of the actual
worm samples, as in Figure 6. Of course, a highly false-positive-averse user could set
F to 0, and accept the risk of false negatives.
94
J. Newsome, B. Karp, and D. Song
Innocuous Pool
False Positive Rate
100%
Suspicious Pool
False Negative Rate
Innocuous Pool
False Positive Rate
100%
Suspicious Pool
False Negative Rate
e
t
a
R
F%
e
t
a
R
F%
Threshold
τTzfp
Tzfn Tzfn’ Tzfn’’
Threshold
τ
τ ’
Fig. 7. Dropped Red Herring Attack. Spurious
tokens artiﬁcially shift false negative curve to
the right. It shifts back to the left when worm
samples without the spurious tokens are added
to the suspicious pool.
Fig. 8. Correlated Outlier attack
Another tempting method for choosing τ is to set it to Tz f n, the highest value that
achieves zero false negatives in the suspicious pool. However, we show in Section 4.2
that this would make the Bayes learner vulnerable to a red herring attack.
4.2 Dropped Red Herring and Randomized Red Herring Attacks Are Ineffective
Dropped Red Herring Attack. The method just described for choosing τ may seem
unintuitive at ﬁrst. However, it was carefully designed to prevent Dropped Red Herring
attacks, as illustrated in Figure 7. Suppose that τ was set to Tz f n, the threshold just low
enough to achieve zero false negatives in the training data. This may seem more intu-
itive, since it reduces the risk of false positives as much as possible while still detecting
all positive samples in the malicious pool.
Now suppose that the attacker performs the Dropped Red Herring attack. Since the
spurious features occur in 100% of the target-class samples, they will be used in the
feature set σ. Since each target-class sample in the malicious pool now has more in-
criminating features, the Bayes score of every target-class sample in the suspicious
pool increases, causing the false negative curve to be artiﬁcially shifted to the right.4
If the learner were to set τto T
(cid:5)(cid:5)
z f n (see Figure 7), then the attacker could successfully
perform the Dropped Red Herring attack. When a target-class sample includes one less
(cid:5)(cid:5)
spurious feature, its Bayes score becomes less than T
z f n. Hence it
would be classiﬁed as negative. Eventually the learner would get target-class samples
without that spurious feature in its malicious pool, causing the false negative curve to
(cid:5)
z f n. At
shift to the left, and the learner could update the classiﬁer with a threshold of T
that point the attacker could stop including another feature.
However, setting τ to the value that achieves no more than F% false positives is
robust to the Dropped Red Herring attack. Assuming that the spurious features do not
(cid:5)
z f n, where T
(cid:5)
z f n < T
4 The false positive curve may also shift towards the right. We address this in Section 4.3.
Paragraph: Thwarting Signature Learning by Training Maliciously
95
appear in the innocuous pool, the false positive curve of the training data distribution
graph is unaffected, and hence the threshold τ is unaffected.
Randomized Red Herring Attack. The Randomized Red Herring attack has little ef-
fect on the Bayes learner. The Bayes score for any given target-class sample will be
higher due to the inclusion of the spurious features. The increase will vary from sample
to sample, depending on which spurious features that sample includes. However, again
assuming that the spurious features do not appear in the innocuous pool, this has no
effect on τ. Hence, the only potential effect of this attack is to decrease false negatives.
4.3 Attack I: Correlated Outlier Attack
Unfortunately, we have found an attack that does work against the Bayes learner. The
attacker’s goal in this attack is to increase the Bayes scores of samples in the innocuous
pool, so as to cause signiﬁcant overlap between the training data false positive and
false negative curves. In doing so, the attacker forces the learner to choose between
signiﬁcant false positives, or 100% false negatives, independently of the exact method
chosen for setting the threshold τ.
Attack Description. The attacker can increase the Bayes score of innocuous samples
by using spurious features in the target-class samples, which also appear in some in-