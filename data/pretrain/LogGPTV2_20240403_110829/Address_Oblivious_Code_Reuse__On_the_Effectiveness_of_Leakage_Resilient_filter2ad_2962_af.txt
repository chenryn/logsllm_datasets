Missing the point(er): On the effectiveness of code pointer
integrity. In 36th IEEE Symposium on Security and Privacy,
S&P, 2015.
[19] I. Evans, F. Long, U. Otgonbaatar, H. Shrobe, M. Rinard,
H. Okhravi, and S. Sidiroglou-Douskos. Control jujutsu: On
the weaknesses of ﬁne-grained control ﬂow integrity. In ACM
Conference on Computer and Communications Security, CCS,
2015.
[20] R. Faulkner and R. Gomes. The process ﬁle system and process
model in unix system v. In USENIX Technical Conference, ATC,
1991.
[21] R. Gawlik, B. Kollenda, P. Koppe, B. Garmany, and T. Holz.
Enabling client-side crash-resistance to overcome diversiﬁcation
and information hiding. In 23rd Annual Network and Distributed
System Security Symposium, NDSS, 2016.
[22] J. Gionta, W. Enck, and P. Ning. HideM: Protecting the contents
of userspace memory in the face of disclosure vulnerabilities.
In 5th ACM Conference on Data and Application Security and
Privacy, CODASPY, 2015.
[23] C. Giuffrida, A. Kuijsten, and A. S. Tanenbaum. Enhanced
operating system security through efﬁcient and ﬁne-grained ad-
dress space randomization. In 21st USENIX Security Symposium,
USENIX Sec, 2012.
[24] E. G¨oktas¸, R. Gawlik, B. Kollenda, E. Athanasopoulos, G. Por-
tokalidis, C. Giuffrida, and H. Bos. Undermining information
In 25th USENIX Security
hiding (and what to do about it).
Symposium, 2016.
[25] J. Hiser, A. Nguyen, M. Co, M. Hall, and J. Davidson. ILR:
Where’d my gadgets go. In 33rd IEEE Symposium on Security
and Privacy, S&P, 2012.
[26] T. Hobson, H. Okhravi, D. Bigelow, R. Rudd, and W. Streilein.
On the Challenges of Effective Movement. In ACM CCS Moving
Target Defense (MTD) Workshop, Nov 2014.
[27] A. Homescu, T. Jackson, S. Crane, S. Brunthaler, P. Larsen, and
M. Franz. Large-scale automated software diversity—program
evolution redux. IEEE Transactions on Dependable and Secure
Computing, PP(99):1, 1 2015. Pre-Print.
[28] R. Hund, C. Willems, and T. Holz. Practical timing side channel
attacks against kernel space ASLR. In 34th IEEE Symposium
on Security and Privacy, S&P, 2013.
[29] Intel.
Intel 64 and IA-32 architectures software developer’s
manual. ch 28, 2015.
[30] S. Jana and V. Shmatikov. Memento: Learning secrets from
process footprints. In 33rd IEEE Symposium on Security and
Privacy, S&P, 2012.
[31] C. Kil, J. Jun, C. Bookholt, J. Xu, and P. Ning. Address space
layout permutation (ASLP): towards ﬁne-grained randomization
In 22nd Annual Computer Security
of commodity software.
Applications Conference, ACSAC, 2006.
[32] T. J. Killian. Processes as ﬁles. In USENIX Association Software
Tools Users Group Summer Conference, STUG, 1984.
[33] V. Kuznetsov, L. Szekeres, M. Payer, G. Candea, R. Sekar, and
D. Song. Code-pointer integrity. In 11th USENIX Symposium
on Operating Systems Design and Implementation, OSDI, 2014.
[34] P. Larsen, A. Homescu, S. Brunthaler, and M. Franz. SoK:
In 35th IEEE Symposium on
Automated software diversity.
Security and Privacy, S&P, 2014.
[35] C. Liebchen, M. Negro, P. Larsen, L. Davi, A.-R. Sadeghi,
S. Crane, M. Qunaibit, M. Franz, and M. Conti. Losing control:
On the effectiveness of control-ﬂow integrity under stack attacks.
In ACM Conference on Computer and Communications Security,
CCS, 2015.
[36] Z. Lin, R. D. Riley, and D. Xu. Polymorphing software by
randomizing data structure layout. In 6th International Confer-
ence on Detection of Intrusions and Malware, and Vulnerability
Assessment, DIMVA, 2009.
[37] K. Lu, S. N¨urnberger, M. Backes, and W. Lee. How to Make
ASLR Win the Clone Wars: Runtime Re-Randomization. In 23rd
Annual Network and Distributed System Security Symposium,
NDSS, 2016.
[38] K. Lu, C. Song, B. Lee, S. P. Chung, T. Kim, and W. Lee.
ASLR-Guard: Stopping Address Space Leakage for Code Reuse
Attacks. In ACM Conference on Computer and Communications
Security, CCS, 2015.
[39] G. Maisuradze, M. Backes, and C. Rossow. What Cannot Be
Read, Cannot Be Leveraged? Revisiting Assumptions of JIT-ROP
Defenses. In 25th USENIX Security Symposium, USENIX Sec,
2016.
[40] A. J. Mashtizadeh, A. Bittau, D. Boneh, and D. Mazi`eres.
CCFI: cryptographically enforced control ﬂow integrity. In ACM
Conference on Computer and Communications Security, CCS,
2015.
[41] S. Nagarakatte, J. Zhao, M. M. Martin, and S. Zdancewic.
SoftBound: Highly compatible and complete spatial memory
safety for C. In ACM Conference on Programming Language
Design and Implementation, PLDI, 2009.
[42] A. Oikonomopoulos, E. Athanasopoulos, H. Bos, and C. Giuf-
In 25th USENIX
frida. Poking holes in information hiding.
Security Symposium, USENIX Sec, 2016.
[43] H. Okhravi, T. Hobson, D. Bigelow, and W. Streilein. Finding
14
focus in the blur of moving-target techniques. Security Privacy,
IEEE, 12(2):16–26, Mar 2014.
[44] A. One. Smashing the stack for fun and proﬁt. Phrack magazine,
7, 1996.
[45] V. Pappas, M. Polychronakis, and A. D. Keromytis. Smashing
the gadgets: Hindering return-oriented programming using in-
place code randomization. In 33rd IEEE Symposium on Security
and Privacy, S&P, 2012.
[46] K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuffrida, and
H. Bos. Flip feng shui: Hammering a needle in the software
stack. In 25th USENIX Security Symposium, 2016.
[47] F. Schuster, T. Tendyck, C. Liebchen, L. Davi, A.-R. Sadeghi,
and T. Holz. Counterfeit object-oriented programming: On the
difﬁculty of preventing code reuse attacks in C++ applications.
In 36th IEEE Symposium on Security and Privacy, S&P, 2015.
[48] J. Seibert, H. Okhravi, and E. S¨oderstr¨om. Information leaks
without memory disclosures: Remote side channel attacks
In ACM Conference on Computer and
on diversiﬁed code.
Communications Security, CCS, 2014.
[49] F. J. Serna. CVE-2012-0769, the case of the perfect info leak,
2012.
[50] H. Shacham. The geometry of innocent ﬂesh on the bone: return-
into-libc without function calls (on the x86). In ACM Conference
on Computer and Communications Security, CCS, 2007.
[51] H. Shacham, M. Page, B. Pfaff, E.-J. Goh, N. Modadugu, and
D. Boneh. On the effectiveness of address-space randomization.
In Proc. of ACM CCS, pages 298–307, 2004.
[52] R. Skowyra, K. Casteel, H. Okhravi, N. Zeldovich, and
W. Streilein. Systematic Analysis of Defenses Against Return-
Oriented Programming. In 16th International Symposium on
Research in Attacks, Intrusions, and Defenses (RAID’13), LNCS,
pages 82–102, Oct 2013.
[53] K. Z. Snow, F. Monrose, L. Davi, A. Dmitrienko, C. Liebchen,
and A. Sadeghi. Just-in-time code reuse: On the effectiveness of
ﬁne-grained address space layout randomization. In 34th IEEE
Symposium on Security and Privacy, S&P, 2013.
[54] K. Z. Snow, R. Rogowski, J. Werner, H. Koo, F. Monrose, and
M. Polychronakis. Return to the zombie gadgets: Undermining
destructive code reads via code inference attacks. In 37th IEEE
Symposium on Security and Privacy, 2016.
Internet [Nov, 2015]. Available on:
[55] B. Spengler. Grsecurity.
http://grsecurity.net, 2015.
[56] R. Strackx, Y. Younan, P. Philippaerts, F. Piessens, S. Lachmund,
and T. Walter. Breaking the memory secrecy assumption. In
2nd European Workshop on System Security, EUROSEC, 2009.
[57] L. Szekeres, M. Payer, T. Wei, and D. Song. Sok: Eternal war
in memory. In IEEE Symposium on Security and Privacy, 2013.
[58] A. Tang, S. Sethumadhavan, and S. Stolfo. Heisenbyte: Thwarting
memory disclosure attacks using destructive code reads. In ACM
Conference on Computer and Communications Security, CCS,
2015.
[59] V. van der Veen, E. Goktas, M. Contag, A. Pawlowski, X. Chen,
S. Rawat, H. Bos, T. Holz, E. Athanasopoulos, and C. Giuffrida.
A tough call: Mitigating advanced code-reuse attacks at the
binary level. In 37th IEEE Symposium on Security and Privacy,
2016.
[60] R. Wartell, V. Mohan, K. W. Hamlen, and Z. Lin. Binary stirring:
self-randomizing instruction addresses of legacy x86 binary code.
In ACM Conference on Computer and Communications Security,
CCS, 2012.
[61] J. Werner, G. Baltas, R. Dallara, N. Otternes, K. Snow,
F. Monrose, and M. Polychronakis. No-execute-after-read:
In 11th
Preventing code disclosure in commodity software.
ACM Symposium on Information, Computer and Communications
Security, ASIACCS, 2016.
[62] R. Wojtczuk. Subverting the Xen hypervisor. In Blackhat USA,
BH US, 2008.
[63] Q. Xiao, M. K. Reiter, and Y. Zhang. Mitigating storage
In ACM
side channels using statistical privacy mechanisms.
Conference on Computer and Communications Security, CCS,
2015.
[64] K. Zhang and X. Wang. Peeping tom in the neighborhood:
Keystroke eavesdropping on multi-user systems. In 18th USENIX
Security Symposium, USENIX Sec, 2009.
APPENDIX
A. NGINX ATTACK 2 DETAILS
Nginx’s design employs a master process, which provides
signal handling and spawns worker processes to handle
requests via fork calls. This processing loop is implemented
by the ngx_master_process_cycle function, which
is called from main after Nginx conﬁgures itself. The
trampoline address of
this function can be determined
via proﬁling after causing a system call
to hang. Since
ngx_master_process_cycle forks, worker processes
inherit the parent’s current stack. This includes the return
address trampoline of
ngx_master_process_cycle.
Recall that return addresses are replaced with a pointer to a
trampoline whose code resembles the following:
ngx_master_process_cycle
call
jmp callsite_main
The return address points to the jmp instruction. From that
address, we can easily derive where the call instruction is
located.
Identifying the relevant return address on the stack is
straightforward, as Nginx’s initial execution is predictable. The
ngx_master_process_cycle frame will be near the base
of the stack, immediately after the main stack frame.
Once the address of ngx_master_process_cycle
is found, we can take advantage of a function pointer in the
Nginx worker’s log handler. The log_error_core function
contains a pointer to a log handler function taking three
arguments: p = log -> handler(log, p, last-p).
There are multiple system calls in the function prior to the
pointer being dereferenced during a logging event, which
enables us to hang the program via MTB and corrupt the
handler to point instead at ngx_master_process_cycle.
In order to prevent a program crash, we must also modify the
ﬁrst argument (log) to resemble the ngx_cycle_t expected
by ngx_master_process_cycle. The parameter is not
used in our attack, so any non-crashing value sufﬁces.
ngx_argv[0] = "/usr/bin/python3"
ngx_argv[1] = "-c"
ngx_argv[2] = "import os,socket,subprocess;
s=socket.socket(socket.AF_INET,
socket.SOCK_STREAM);
s.connect((\\\’127.0.0.1\\\’,1234));
[os.dup2(s.fileno(),i) for i in range(3)];
subprocess.call([\\\’/bin/sh\\\’,\\\’-i
\\\’]);"
ngx_argv[3] = 0
Fig. 6. Reverse Shell in Nginx with AOCR
Once we
at
ngx_master_process_cycle, we must ensure that
handler
have
pointed
the
log
invoked whenever a global
the target function’s execution causes an exec under our
control. This can be achieved via the range of signals that
Nginx can handle in
ngx_master_process_cycle.
In particular, Nginx provides a new_binary signal used
to provide rolling updates to a new version of the server
without compromising availability. This
signal handler
integer variable named
is
ngx_change_binary is non-zero. The path to the
binary is stored in ngx_argv, another global variable. By
corrupting the ﬁrst global value we ensure that an exec
call will eventually be made when the log handler pointer is
dereferenced. By corrupting the latter, we ensure that a binary
of our choice is executed. For example, setting ngx_argv to
the values shown in Figure 6 will create a reverse shell bound
to a chosen IP address (127.0.0.1 in this case).
B. APACHE ATTACK DETAILS
In order to maintain portability across operating systems,
Apache uses its own portable runtime libraries (APR and APR-
Util) instead of directly calling functions in libc. However,
modules may call functions in this library that the base Apache
process does not. The build process must ensure that all
APR functions and related utility libraries are linked during
compilation whether or not they are explicitly used in the base
code. This is achieved via an exports.c ﬁle for each library.
Each of these ﬁles contains function pointers to every function
in that library. They are linked to the executable during program
compilation, and loaded into the data section of memory on
execution.
One of these exported functions is ap_get_exec_line
in Apache’s server utility library (httpd.h), which takes three
arguments: a pointer to a valid memory pool, a command to
run, and the arguments to supply that command. We recover
the trampoline for this function by proﬁling while hanging
execution via MTB. The region of memory containing pointers
from exports.c is easily identiﬁed, as it contains nothing
but function pointers (with common higher-order bits) pointing
to functions in one library. The order in which function pointers
are declared in exports.c is deterministic, so recovering the
pointer for ap_get_exec_line is straightforward.
Next, we corrupt a function pointer to point to the revealed
address. When choosing the pointer, we must ensure that
the parameters passed to ap_get_exec_line are passed
correctly, as this attack does not rely on global variables like the
Nginx variant. Additionally, our ability to modify memory is
limited to the periods surrounding system calls. Only functions
that pass parameters via pointers to memory addresses are
viable. Given these criteria we chose to corrupt the errfn
pointer in sed_reset_eval, part of Apache’s mod_sed.
The errfn pointer is dereferenced in the eval_errf
function, which pulls all of its parameters from pointers to
memory. Similar functions are available in other modules,
should mod_sed not be available.
Finally, we set errfn to point to ap_get_exec_line.
The ﬁrst argument pointer is corrupted to point at a valid
apr_pool_t object, which the attacker-controller worker will
likely already have. (APR pools are used to handle memory
allocation in Apache.) The second pointer is made to point at
a string containing the path to a binary of our choice. When
the errfn pointer is dereferenced, the binary is executed.
15