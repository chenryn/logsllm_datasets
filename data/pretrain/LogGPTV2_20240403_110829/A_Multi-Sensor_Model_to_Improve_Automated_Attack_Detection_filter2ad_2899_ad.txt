web server. We added a special rule to simulate a heartbeat for Snort. We sim-
ulated the failure of the inside Snort sensor by simply killing the process. All
alerts were collected and analyzed. The results correspond to the values shown
in Table 3. Not shown in the table is P (w1 = T| . . .) for Example 2. These values
are similar to the ones shown in column 4 for Example 1. For example, when
there is no heartbeat the probability of S1 working is only 0.01.
Based on our decision function, we would thus further investigate case 1-4
and case 1-6 for Example 1 and case 2-3, case 2-4, and case 2-6 for Example 2.
5.4 Sensitivity Analysis
One weakness of the model is the diﬃculty of accurately estimating the para-
meters. For that reason, we exhaustively perturbed each estimate in the models
by 20% and then recalculated the probability for node inv-A. In Example 1, we
have 13 independent parameters and the option of subtracting or adding 20% to
each estimate gives a total of 213 test cases. For each test case, we then compared
the outcome of the decision function (i.e., investigate or do not investigate) with
the outcome from the unperturbed network. There was no diﬀerence, implying
that the model is relatively robust against estimation errors.
6 Discussion
Column 2 and column 6 in Table 3 show the probability of having a serious
attack that needs investigation, given the observable evidence in the network. In
a real system, as we speciﬁed above, we would most likely collapse the values to
investigate or do not investigate using a decision function. Now let us go through
the results in detail.
6.1 Analysis of Scenario 1
In the upper half of Table 3 for Scenario 1, we do not encrypt the requests.
Thus, both sensors work and we require alerts from both sensors to investigate
306
M. Almgren, U. Lindqvist, and E. Jonsson
the attack. If only the web sensor raises an alert, (1-2 ), no attack code was
detected by Snort and it was most likely a normal request. This is reﬂected in
the relatively low value of P (inv-A| . . .) for this case. The same holds for case
1-3. Here, only Snort raises an alert so no phf script was run on the server and
thus the attack did not propagate within the web server. If both sensors alert,
we should investigate further as illustrated by the high value for case 1-4.
In the lower part of Table 3, we encrypt all requests. There is no alert, but
observed encrypted traﬃc is not regarded as very sensitive, and thus case 1-5
is rather low. However, note that it is ﬁve times that of case 1-1 as we have
only one working sensor. If we have an alert from the webIDS when the traﬃc
is encrypted (case 1-6 ), the system indicates that we should investigate further.
The missing alert from Snort is either because there is no attack, or because Snort
is not working. Looking closer at the model for this case, we can determine that
P (w1 = T| . . .) = 0.01, i.e., that the Snort IDS cannot properly analyze the
request.
6.2 Analysis of Scenario 2
Now let us have a look at the results for Scenario 2 in Table 3. In the upper half
of the table, we receive a heartbeat from the Snort sensor placed inside the proxy,
meaning that it should work. When we have no alert (case 2-1 ) or when only the
outside sensor raises an alert (case 2-2 ), the risks are relatively low as indicated
in Table 3. Case 2-3, on the other hand, is interesting. We have an alert only
on the inside, which seems impossible as the outside Snort sensor should see all
traﬃc that the inside sensor can see. The only explanation, properly deducted
in the model, is that the outside sensor is broken (P (w2 = T| . . .) = 0.08). The
alert should be investigated as indicated in the table, and sensor 2 should most
likely be restarted. In case 2-4, we have alerts from both sensors and thus the
alert should be investigated.
In the lower part of Table 3, there is no heartbeat from the Snort sensor placed
inside the proxy. For that reason, an alert from the outside sensor is deemed to
be much more serious, as can be seen for case 2-6. In this case, the lack of alert
from the inside sensor is explained by a broken sensor (S1) as there is no longer
a heartbeat (not explicitly shown in the table but discussed in Section 5.3).
6.3 Summary
The interesting cases are thus how the model can directly adapt to changes in the
environment. The evaluation of case 1-2 is very diﬀerent from case 1-6, despite
the fact that we in both these cases have an alert only from the webIDS. The
same goes for case 2-2 and case 2-6. The model can also make predictions for
when a sensor is broken, as in case 1-5, case 1-6, case 2-3, case 2-5, and case 2-6.
The examples we used were designed to illustrate the basic principles of our
model while being easy to understand. In real operational settings, the mod-
els would be slightly more complex. For example, when using Snort, it would
be prudent to have indicators of both encrypted traﬃc (as in Example 1) and
A Multi-Sensor Model to Improve Automated Attack Detection
307
heartbeats (as in Example 2). It would be easy to modify the model to incor-
porate information from two such nodes. One can also imagine using the results
from a vulnerability scan to adjust the weight of the evidence—indications of an
attack for which the target is not vulnerable would be given lower weight.
7 Related Work
Several research groups have presented correlation techniques that are able to
cluster related alerts, thus presenting the security operator with sets of related
alerts belonging to (it is hoped) a single attack. Even though these techniques
reduce the number of alerts the security oﬃcer needs to consider at a single time,
they do not alleviate the actual analysis of the alerts to decide whether an attack
is in progress. As we stated in Section 1, we ﬁnd our approach complementary and
we even assume that such a traditional correlator preprocesses the data before
it is given to the model presented in this paper. See the excellent overview given
by Kruegel et al. [14] and the references therein.
Other correlation eﬀorts have tried to recognize predeﬁned attack scenar-
ios (Debar et al. [9]) or correlating based on the capabilities gained by the at-
tacker from each attack step (Ning et al. [18], Cheung et al. [6], Cuppens et al. [7],
and Zhou et al. [23]). Even though some of these approaches account for an im-
perfect alert stream with missed attack steps, they do not resolve conﬂicting
evidence or model the IDS failure modes as we do. Our approach increases the
accuracy of the alert stream and would thus also increase the performance of
these higher-level correlation eﬀorts.
Other researchers have focused on using several sensors to collect information
about attacks. Abad et al. [1] use a data mining approach to correlate information
from diﬀerent logs on a single host. The underlying ideas are similar to those of
our approach. However, we include negative information (no alert) when judging
whether an attack is in progress and also try to explain the missing information.
Dagorn [8] discusses a cooperative intrusion detection system for web applica-
tions. Thresholds are avoided and instead a Bayesian framework is used, where
each node has a twin to measure its conﬁdence level. In our approach, we use a
much more constrained view of the sensors and their capabilities but in return
we can then reason more about alerts we have.
Tombini et al. [20] combine an anomaly-based IDS with a misuse IDS. They
have an enlightening discussion concerning combinations of alerts from the two
systems but they focus on a serial combination of the IDSs as opposed to our
approach, and they do not consider sensor failure.
Morin et al. [17] introduce a formal model for correlation and discuss some
scenarios where this model can be used. Even though Morin et al. describe the
need to solve inconsistencies in the output from several IDSs, they do not show
any model to do so. Morin et al. [16] also show an application of Chronicles to
IDS. However, in this paper they explicitly state that they only use available
alerts. In our approach, we also take advantage of false negatives.
308
M. Almgren, U. Lindqvist, and E. Jonsson
Kruegel et al. [13] describe an IDS for analyzing operating system calls to
detect attacks against programs. They use the same decision framework from
artiﬁcial intelligence (i.e., Bayesian networks) as we do, but explore a diﬀerent
problem from the one presented here.
The two approaches most similar to ours are Yu et al. [21] and Zhai et al. [22].
The former tries to predict the intruder’s next goal with hidden colored petri
nets. They infer missing alerts and reason about alerts using an exponentially
weighted Dempster-Shafer theory of conﬁdence. They do not, as we do, explicitly
model an IDS weakness to use missing alerts as evidence against an ongoing
attack.
Zhai et al. [22] use Bayesian networks to combine event-based evidence (in-
trusion detection alerts) with state-based evidence (observations in the environ-
ment) by chaining them together in a causal structure. Even though their model
considers false negatives in a limited way, they do not account for the failure
modes of the IDS and thus cannot explain why or how an attack was missed. A
consequence is that they also do not use true negatives as evidence against an
attack in the same way we do.
Finally, we would like to mention tools such as Thor (Marty [15]). An extension
to Thor, for example, would automate the need to manually build correlation
tables and set the parameters that are needed for our model.
8 Future Work
We would like to extend the sensor models we have started to build. We would
also like to run the system in more challenging environments to learn more
about its limitations and how the model can be improved. For example, we have
considered adding a general threat node. This would allow the system to increase
its sensitivity in certain scenarios and lower it in others, based on input from
the security operator. In addition, we would like to investigate how to build a
sensor that is better tailored to the requirements posed by our model.
9 Conclusions
We have proposed and investigated an intrusion detection model that can ana-
lyze alerts from several audit sources to improve the detection accuracy of the
intrusion detection system (IDS) as a whole. Our model, expressed in the form
of a Bayesian network, can resolve seemingly conﬂicting evidence collected from
diﬀerent audit sources, thus making it diﬀerent from other cluster-based correla-
tion approaches. We explicitly model the transitory state of the IDS sensor and
can therefore reason about the case when an alert is not produced (a negative)
in addition to the case when an alert is produced (a positive).
We validate our model in two scenarios in our test bed. We show that not
only can the model correctly reason about evidence collected from several audit
sources, but it can also point out when a sensor seems to have failed.
A Multi-Sensor Model to Improve Automated Attack Detection
309
Acknowledgments. The authors are grateful for valuable comments from our
colleagues Daniel Hedin, Marina Papatriantaﬁlou, David Sands and Alfonso
Valdes. This material is based upon work supported by the Swedish Emergency
Management Agency.
References
1. Abad, C., Taylor, J., Sengul, C., Yurcik, W., Zhou, Y., Rowe, K.: Log correlation
for intrusion detection: A proof of concept. In: ACSAC 2003: Proceedings of the
19th Annual Computer Security Applications Conference, p. 255. IEEE Computer
Society, Los Alamitos (2003)
2. Almgren, M., Debar, H., Dacier, M.: A lightweight tool for detecting web server
attacks. In: Tsudik, G., Rubin, A. (eds.) Network and Distributed System Secu-
rity Symposium (NDSS 2000), San Diego, USA, Feburary 3–4, 2000, pp. 157–170.
Internet Society (2000)
3. Almgren, M., Jonsson, E., Lindqvist, U.: A comparison of alternative audit sources
for web server attack detection. In: Erlingsson, ´U., Sabelfeld, A. (eds.) 12th Nordic
Workshop on Secure IT Systems (NordSec 2007), October 11–12, pp. 101–112.
Reykjav´ık University, Iceland (2007)
4. Axelsson, S.: The base-rate fallacy and its implications for the diﬃculty of in-
trusion detection. In: Proceedings of the 6th ACM Conference on Computer and
Communications Security, November 1999. Kent Ridge Digital Labs (1999)
5. Breese, J., Koller, D.: Tutorial on Bayesian Networks. Internet (1997),
http://robotics.stanford.edu/∼koller/BNtut/BNtut.ppt
6. Cheung, S., Lindqvist, U., Fong, M.W.: Modeling multistep cyber attacks for sce-
nario recognition. In: DARPA Information Survivability Conference and Exposition
(DISCEX III), Washington, DC, April 22–24, 2003, vol. I, pp. 284–292 (2003)
7. Cuppens, F., Miege, A.: Alert correlation in a cooperative intrusion detection
framework. In: Proceedings of the IEEE Symposium on Security and Privacy, Oak-
land, CA, May 2002, pp. 202–215. IEEE Press, Los Alamitos (2002)
8. Dagorn, N.: Cooperative intrusion detection for web applications. In: Pointcheval,
D., Mu, Y., Chen, K. (eds.) CANS 2006. LNCS, vol. 4301, pp. 286–302. Springer,
Heidelberg (2006)
9. Debar, H., Wespi, A.: Aggregation and correlation of intrusion-detection alerts. In:
RAID 2000: Proceedings of the 4th International Symposium on Recent Advances
in Intrusion Detection, pp. 85–103. Springer, Heidelberg (2001)
10. Decision Systems Laboratory, University of Pittsburgh. SMILE reasoning engine
for graphical probabilistic model (2008), http://dsl.sis.pitt.edu
11. Domingos, P., Pazzani, M.: On the optimality of the simple Bayesian classiﬁer
under zero-one loss. Machine Learning 29(2-3), 103–130 (1997)
12. Hernan, S.V.: ‘phf’ CGI script fails to guard against newline characters. CERT/CC;
Internet (January 2001), http://www.kb.cert.org/vuls/id/20276
13. Kruegel, C., Mutz, D., Robertson, W., Valeur, F.: Bayesian event classiﬁcation for
intrusion detection. In: ACSAC 2003: Proceedings of the 19th Annual Computer
Security Applications Conference, p. 14. IEEE Computer Society, Los Alamitos
(2003)
14. Kruegel, C., Valeur, F., Vigna, G.: Intrusion Detection and Correlation. Advances
in Information Security, vol. 14. Springer, Heidelberg (2005)
310
M. Almgren, U. Lindqvist, and E. Jonsson
15. Marty, R.: Thor - a tool to test intrusion detection systems by variations of attacks.
Master’s thesis, Swiss Federal Institute of Technology (ETH), Institut f¨ur Tech-
nische Informatik und Kommunikationsnetze (TIK), Zurich, Switzerland (2002),
http://www.raffy.ch/projects/ids/thor.ps.gz
16. Morin, B., Debar, H.: Correlation of intrusion symptoms: An application of Chron-
icles. In: Vigna, G., Jonsson, E., Kruegel, C. (eds.) RAID 2003. LNCS, vol. 2820,
pp. 94–112. Springer, Heidelberg (2003)
17. Morin, B., M´e, L., Debar, H., Ducass´e, M.: M2D2: A formal data model for IDS
alert correlation. In: Wespi, A., Vigna, G., Deri, L. (eds.) RAID 2002. LNCS,
vol. 2516, pp. 115–137. Springer, Heidelberg (2002)
18. Ning, P., Cui, Y., Reeves, D.S.: Analyzing intensive intrusion alerts via correlation.
In: Wespi, A., Vigna, G., Deri, L. (eds.) RAID 2002. LNCS, vol. 2516, pp. 74–94.
Springer, Heidelberg (2002)
19. Swets, J.A.: Measuring the accuracy of diagnostic systems. Science 240(4857),
1285–1293 (1988)
20. Tombini, E., Debar, H., M´e, L., Ducass´e, M.: A serial combination of anomaly
and misuse IDSes applied to HTTP traﬃc. In: ACSAC 2004: Proceedings of the
20th Annual Computer Security Applications Conference (ACSAC 2004). IEEE
Computer Society, Los Alamitos (2004)
21. Yu, D., Frincke, D.: Improving the quality of alerts and predicting intruder’s next
goal with hidden colored petri-net. Comput. Netw. 51(3), 632–654 (2007)
22. Zhai, Y., Ning, P., Iyer, P., Reeves, D.S.: Reasoning about complementary intrusion
evidence. In: ACSAC 2004: Proceedings of the 20th Annual Computer Security Ap-
plications Conference, Washington, DC, USA, pp. 39–48. IEEE Computer Society,
Los Alamitos (2004)
23. Zhou, J., Heckman, M., Reynolds, B., Carlson, A., Bishop, M.: Modeling network
intrusion detection alerts for correlation. ACM Trans. Inf. Syst. Secur. 10(1), 4
(2007)