i
t
4
Recover Faulty
Output(cid:2)
Fig. 7. Overview of the AES-NI fault attack: (1) the victim architecturally
executes the initial AES round, which xors the input with round key 0; (2)
access rights on the memory page holding the key schedule are revoked; (3)
upon the next key access (P2), the enclave suffers a page fault, causing the
CPU to transiently execute the next 10 AES rounds with zeroed round keys; (4)
ﬁnally the faulty output is encoded (P3) through a cache-based side-channel.
3) The attacker resumes the enclave, leading to a page fault
when loading the next round keys from trusted memory. We
abuse theses faulting load as P2 gadgets which transiently
forward dummy (all-zero) round keys to the remaining
aesdec instructions. Note that we do not need a P1
gadget, as the CPU itself is responsible for zero-injection.
4) Finally, we use a P3 disclosure gadget after the decryption.
By forcing all but the ﬁrst AES round key to zero, our
attack essentially causes the victim enclave to compute a round-
reduced AES in the transient domain. To recover the ﬁrst round
key, and hence the full AES key, the attacker can simply feed
the faulty output plaintext recovered from the transient domain
to an inverse AES function with all keys set to zero. This
results in an output that holds the secret AES ﬁrst round key,
xor-ed with the (known) ciphertext.
b) Experimental results: We run the attack for 100
different AES keys on a Core i9-9900K with RDCL_NO and
the latest microcode 0xae. For each experiment, we run the
attack to recover 10 candidates for each byte of the faulty
output. On average, each recovered key candidate matches the
expected faulty output 83 % of the time. Using majority vote
for the 10 candidates, we recover the correct output for an
average of 15.61 out of 16 bytes of the AES block, indicating
that the output matches the attack model with 97 % accuracy.
The attack takes on average 25.94 s (including enclave creation
time) and requires 246 707 executions of the AES function.
For post-processing, we modiﬁed an AES implementation to
zero out the round keys after the ﬁrst round. We successfully
recovered the secret round-zero key using any of the recovered
faulty plaintext outputs to the inverse encryption function.
VIII. LVI IN OTHER CONTEXTS
address, as in this case the user already architecturally controls
the value read by the kernel.
a) Experimental setup: We focus on exploiting LVI-SB
via microcode assists for setting the accessed bit in supervisor
PTEs. In our case study, we execute the P1 poisoning phase
directly in user space by abusing that current microcode
mitigations only ﬂush the store buffer on kernel exit to prevent
leakage [9, 29]. As the store buffer is not drained on kernel
entry, it can be ﬁlled with attacker-chosen values by writing
to arbitrary user-accessible addresses before performing the
system call. Note that, alternatively, the store buffer could also
be ﬁlled during kernel execution by abusing a selected P1
gadget, similar to our SGX attacks.
In the P2 phase, the attacker needs to trigger a faulting or
assisted load micro-op in the kernel. In our proof-of-concept,
we assume that the targeted supervisor page is swappable, as
is the case for Windows kernel heap objects [50], but to the
best of our knowledge not for the Linux kernel. In order to
repeatedly execute the same experiment and assess the overall
success rate, we simulate the workings of the page-replacement
algorithm by means of a small kernel module, which artiﬁcially
clears the accessed bit on the targeted kernel page.
As we only want to demonstrate the building blocks of the
attack, we did not actively look for real-world gadgets in the
kernel. For our evaluation, we manually added a simple P3
disclosure gadget, which, similar to a Spectre gadget, indexes
a shared memory region based on a previously loaded value
as follows: array[(*kernel_pt) * 4096]. In case the
trusted load on kernel_pt requires a microcode assist, the
value written by the user-space attacker will be transiently
injected from the store buffer and subsequently encoded into
the CPU cache.
b) Experimental results: We evaluated LVI-SB on an
Intel Core i7-8650U with Linux kernel 5.0. On average, 1
out of every 7739 (n = 100 000) assisted loads in the kernel
use the injected value from the store buffer instead of the
architecturally correct value. For our non-optimized proof-of-
concept, this results on average in a successfully injected value
into the kernel execution every 6.5 s. One of the reasons for
this low success rate is the context switch between P1 and
P2, which reduces the probability that the attacker’s value is
still outstanding in the store buffer [9]. We veriﬁed this by
evaluating the injection rate without a context switch, i.e., if
the store buffer is poisoned via a suitable P1 gadget in the
kernel. In this case, on average, 1 out of every 8 (n = 100 000)
assisted loads in the kernel use the injected value.
A. User-to-Kernel
B. Cross-Process
The main challenge in a user-to-kernel LVI attack scenario
is to provoke faulting or assisted loads during kernel execution.
As any application, the kernel may encounter page faults or
microcode assists, e.g., due to demand paging via the extended
page tables setup by the hypervisor, or when swapping out
supervisor heap memory pages in the Windows kernel [50].
We do not investigate the more straightforward scenario where
the kernel encounters a page fault when accessing a user-space
data from a concurrently running attacker process.
We now demonstrate how LVI-LFB may inject poisoned
a) Experimental setup: For the poisoning phase P1, we
assume that the attacker and the victim are co-located on the
same physical CPU core [53, 61, 67]. The attacker directly
poisons the line-ﬁll buffer by writing or reading values to or
from the memory subsystem. To ensure that the values travel
through the ﬁll buffer, the attacker simply ﬂushes the accessed
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
65
values using the unprivileged cflflush instruction. In case
hyperthreading is disabled, the adversary would have to ﬁnd a
suitable P1 gadget that processes untrusted, attacker-controlled
arguments in the victim code, similar to our SGX attacks.
In our proof-of-concept, the victim application loads a value
from a trusted shared-memory location, e.g., a shared library.
As shown by Schwarz et al. [53], Windows periodically clears
the PTE accessed bit, which may cause microcode assists
for trusted loads in the victim process. The attacker ﬂushes
the targeted shared-memory location from the cache, again
using clflush, to ensure that the victim’s assisted load P2
forwards incorrect values from the line-ﬁll buffer [53, 67]
instead of the trusted shared-memory content.
b) Experimental results: We evaluated the success rate
of the attack on an Intel i7-8650U with Linux kernel 5.0. We
used the same software construct as in the kernel attack for the
transmission phase P3. Both attacker and victim run on the
same physical core but different logical cores. On average, 1
out of 101 (n = 100 000) assisted loads uses the value injected
by the attacker, resulting in an injection probability of nearly
1 %. With on average 1122 tries per second, we achieve a
transmission rate of 11.11 B/s for our disclosure gadget.
IX. DISCUSSION AND MITIGATIONS
In this section, we discuss both long-term silicon mitigations
to rule out LVI at the processor design level, as well as compiler-
based software workarounds that need to be deployed on the
short-term to mitigate LVI on existing systems.
A. Eradicating LVI at the Hardware Design Level
The root cause of LVI needs to be ultimately addressed
through silicon-level design changes in future processors.
Particularly, to rule out LVI, the hardware has to ensure that
no illegal data ﬂows from faulting or assisted load micro-
ops exist at the microarchitectural level. That is, no transient
computations depending on a faulting or assisted instruction are
allowed. We believe this is already the behavior in certain ARM
and AMD processors, where a faulting load does not forward
any data [2]. Notably, we showed in Section VI-C that it does
not sufﬁce to merely zero out the forwarded value, as is the
case in the latest generation of acclaimed Meltdown-resistant
Intel processors enumerating RDCL_NO [28].
B. A Generic Software Workaround
Silicon-level design changes take considerable time, and
at least for SGX enclaves a short-term solution is needed
to mitigate LVI on current, widely deployed systems. In
contrast to previous Meltdown-type attacks, merely ﬂushing
microarchitectural buffers before or after victim execution
is not sufﬁcient to defend against our novel, gadget-based
LVI attack techniques. Instead, we propose a software-based
mitigation approach which inserts explicit lfence speculation
barriers to serialize the processor pipeline after every vulnerable
load instruction. The lfence instruction is guaranteed by
Intel to halt transient execution until all prior instructions
have completed [28]. Hence, inserting an lfence after every
TABLE II.
whether or not they require a scratch register which can be clobbered.
Indirect branch instruction emulations needed to prevent LVI and
Instruction
Possible Emulation
ret
ret
jmp (mem)
call (mem)
pop %reg; lfence; jmp *%reg
not (%rsp); not (%rsp); lfence; ret
mov (mem),%reg; lfence; jmp *%reg
mov (mem),%reg; lfence; call *%reg
Clobber




potentially faulting or assisted load micro-op guarantees that
the value forwarded from the load operation is not an injected
value but the architecturally correct one. Relating to the general
attack scheme of Figure 3, we introduce an lfence instruction
in between phases P2 and P3 to inhibit any incorrect transient
forwarding by the processor. Crucially, in contrast to existing
Spectre-PHT compiler mitigations [10, 28] which only insert
lfence barriers after potentially mispredicted conditional
jump instructions, fully mitigating LVI requires stalling the
processor pipeline after potentially every explicit as well as
implicit memory-load operation.
Explicit memory loads, i.e., instructions with a memory
address as input parameter, can be protected straightforwardly.
A compiler, or even a binary rewriter [14], can add an lfence
instruction to ensure that any dependent operations can only
be executed after the load instruction has successfully retired.
However, some x86 instructions also include implicit memory
load micro-ops which cannot be mitigated in this way. For
instance, indirect branches and the ret instruction load an
address from the stack and immediately redirect control ﬂow to
the loaded, possibly injected value. As the faulting or assisted
load micro-op in this case forms part of a larger ISA-level
instruction, there is no possibility to add an lfence barrier
between the memory load (P2) and the control-ﬂow redirection
(P3). Table II shows how indirect branch instructions have to be
blacklisted and emulated through an equivalent sequence of two
or more instructions, including an lfence after the formerly
implicit memory load. Notably, as some of these emulation
sequences clobber scratch registers, LVI mitigations for indirect
branches cannot be trivially implemented using binary rewriting
techniques and should preferably be implemented in the
compiler back-end, before the register allocation stage.
a) Evaluation of our prototype solution: We initially im-
plemented a prototypical compiler mitigation using LLVM [43]
(8.3.0) and applied it
to a recent OpenSSL [48] version
(1.1.1d) with default conﬁguration. We chose OpenSSL as
it serves as the base of the ofﬁcial Intel SGX-SSL library [33]
allowing to approximate the expected performance impact of
the proposed mitigations. Our proof-of-concept mitigation tool
allows to augment the building process of arbitrary C code by
ﬁrst instrumenting the compiler to emit LLVM intermediate
code, adding the necessary lfence instructions after every
explicit memory load, and ﬁnally proceeding to compile the
modiﬁed ﬁle to an executable. Our prototype tool cannot
mitigate loads which are not visible at the LLVM intermediate
representation, e.g., the x86 back-end may introduce loads
for registers spilled onto the stack after register allocation.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
66
]
%
[
d
a
e
h
r
e
v
O
4,000
3,000
2,000
1,000
0
5
1
.
8
6
8
1
3
1
.
8
7
9
9
3
.
5
1
4
9
.
8
9
5
4
.
5
1
7
2
.
2
7
3
1
8
4
.
5
6
3
1
9
2
.
2
8
7
6
.
0
5
0
.
7
8
2
1
3
7
.
4
1
2
1
6
3
.
3
0
7
2
6
.
1
8
.
2
1
6
.
8
5
7
5
7
.
2
1
7
7
8
.
8
6
5
1
9
.
0
6
7
.
0
2
2
.
5
1
7
2
2
.
8
3
6
8
8
.
2
9
4
8
4
.
6
1
2
4
.
2
1
1
2