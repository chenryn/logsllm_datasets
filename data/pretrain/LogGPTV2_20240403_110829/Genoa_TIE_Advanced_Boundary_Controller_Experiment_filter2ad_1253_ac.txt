### Accuracy and Detail in Key-Topic Analysis

To evaluate the performance of the Felt and DataShield filters, we used timing data collected during the Ground Truth (GT) assessment as a basis for comparison. The human reviewers provided approximate processing times for each transaction, recorded in minutes, which included the review of both the request and response. The corresponding transaction times for Felt and DataShield were extracted from the experiment logs, with Felt recording times in microseconds and DataShield in seconds.

### Timing Data Collection

Felt utilized a timing log library routine to record timing measures in microseconds. In contrast, DataShield implemented a general-purpose timing log interface that recorded timestamps in seconds. Additionally, the ARGuE filter subsystem generated timing information by creating timestamps before and after the execution of the request and response filters. These timing messages were encapsulated within the filter subsystem messages. Due to the incompatibility in the granularity of timing data (seconds vs. microseconds), the subsystem timing data was used for comparison with the GT results.

### Precision and Recall Ratios

The precision and recall ratios for the "Product" and "Topic" categories are as follows:

- **Felt:**
  - Precision "Product": 88%
  - Precision "Topic": 86%
  - Recall "Product": 77%
  - Recall "Topic": 73%

- **DataShield:**
  - Precision "Product": 95%
  - Precision "Topic": 88%
  - Recall "Product": 73%
  - Recall "Topic": 55%

**Figure 4: Precision and Recall Ratios**

### Potential for Inaccurate Assessments

Given the assumptions made by the human reviewers, there is a possibility that the assessments could be incorrect. The use of mixed sources also means there is no guarantee that the products contain exactly the same data.

### Conclusion

Human-in-the-loop content analysis is resource-intensive and can be time-consuming and error-prone due to factors such as fatigue and boredom. Automated filtering can supplement and may eventually replace manual content-based reviews as technology advances.

Through the Genoa TIE efforts, we found that automated syntactic and NLP capabilities could be measured to determine the strengths and weaknesses of the filters. Metrics were recorded in both accuracy and performance, based on a controlled human review. While the human review provided the most accurate assessment, it represented an ideal situation. The collected times for Felt and DataShield, recorded in microseconds, are slightly inflated compared to the subsystem timing data. Although not the most accurate, the subsystem timing data provides a consistent measure of the overhead incurred for filtering each transaction.

### Transaction Filtering Times

**Figure 5: Mean Average Transaction Filtering Times**

- For each transaction, Felt outperformed both DataShield and human review with significantly faster times.
- Except for transactions 30 and 39, DataShield outperformed the human review. These anomalies are explained by the CIP data, where transactions 29 and 30, as well as 38 and 39, contained the same products. The human review times for these transactions were reduced due to retained knowledge from previous transactions.

### Content Filtering Abilities

In evaluating the content filtering abilities of Felt, DataShield, and human review, findings confirm that no single method is completely reliable. This is evident in current implementations of guards within Multi-Level Security (MLS) environments. Automatic downgrading and sanitization within classified environments is challenging due to unstructured data and the complexity of the English language.

Overall, combining the strengths of Felt's syntactic filtering and DataShield's NLP capabilities with manual review can increase accuracy and efficiency. Tradeoffs in accuracy, performance, and risk can lead to a more desirable automated solution than manual review alone.

### Implementation and Performance

For the Genoa TIE, a complex policy set was instrumented via RML. Although RML was not as accurate as hoped, its use was vital in collecting data. Current guard implementations do not typically handle such complex policies, and solutions for representing them without room for interpretation do not exist. Continued research in this area may be useful for developing next-generation policy-based filters.

### Filter Performance

- **Felt:** 
  - Experienced low False Positive rates but also a low positive identification rate for valid policy violations.
  - Relied solely on a keyword list, and the selection of keywords was crucial for performance.
  - Considerably faster than DataShield or human review, and correctly identified keyword instances missed by both.

- **DataShield:**
  - Performed significantly better in MNLP than the syntactic-only review by Felt.
  - More efficient at processing transactions but suffered from some False Positive detections.
  - Correctly identified most key-topic policy violations within the products.

Within the Genoa environment, the implementation of ARGuE, Felt, and DataShield provided access control among enclaves. Despite issues with policy implementation due to human misinterpretation, the system could accept updated policies and enforce them. Experiment data shows that Felt and DataShield correctly detected all previously identified violations in the metadata, likely due to its well-structured nature.

### Future Applications

MNLP surpassed the detection capabilities of syntactic filters in this experiment. Could similar technologies be applied in other security realms, such as Intrusion Detection Systems (IDS)? For example, IDS currently filters network traffic for known attack strings. Could such technologies be trained to analyze traffic with higher accuracy, capable of detecting novel attacks?

### References

1. J. Guttman, J. Ramsdell, and V. Swarup, *Felt: A Security Filter Compiler*, Personal Communication, November 1998.
2. DARPA, *Project Genoa White Paper*, [www.darpa.mil/iso2/project_genoa/project_genoa_white_paper.html](http://www.darpa.mil/iso2/project_genoa/project_genoa_white_paper.html)
3. Solutions United, [www.solutions-united.com](http://www.solutions-united.com)
4. DARPA, [www.darpa.mil](http://www.darpa.mil)
5. Apologetics Index, [www.gospelcom.net/apologeticsindex/a06.html](http://www.gospelcom.net/apologeticsindex/a06.html)
6. Solutions United, *Products and Technology*, [www.solutions-united.com/products_technology.html](http://www.solutions-united.com/products_technology.html)
7. J. Epstein, *Architecture and Concepts of the ARGuE Guard*, Proceedings of the 15th Annual Computer Security Applications Conference (ACSAC), December 1999.