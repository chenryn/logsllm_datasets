Template Updating”. In Proceedings of the IEEE European
Symposium on Security and Privacy (EuroS&P), pages 184–
197, 2020.
[30] Alan Lukezic, Tomas Vojir, Luka Cehovin Zajc, Jiri Matas, and
Matej Kristan. “Discriminative Correlation Filter with Channel
and Spatial Reliability”. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 6309–6318, 2017.
[31] Aravindh Mahendran and Andrea Vedaldi. “Understanding
Deep Image Representations by Inverting Them”. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5188–5196, 2015.
[32] Yanmao Man, Ming Li, and Ryan Gerdes. “GhostImage: Per-
ception Domain Attacks against Vision-based Object Classiﬁ-
cation Systems”. arXiv preprint arXiv:2001.07792, 2020.
[33] Massimo Mancuso and Sebastiano Battiato. “An Introduction
to the Digital Still Camera Technology”. ST Journal of System
Research, 2(2), 2001.
[34] Andreas Mogelmose, Mohan Manubhai Trivedi, and Thomas B.
Moeslund. “Vision-based Trafﬁc Sign Detection and Analysis
for Intelligent Driver Assistance Systems: Perspectives and
IEEE Transactions on Intelligent Transportation
Survey”.
Systems, 13(4):1484–1497, 2012.
[35] Joseph Redmon and Ali Farhadi. “Yolov3: An incremental
improvement”. arXiv preprint arXiv:1804.02767, 2018.
[36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
“Faster R-CNN: Towards Real-Time Object Detection with
Region Proposal Networks”. In Proceedings of the Advances
in Neural Information Processing Systems (NIPS), pages 91–
99, 2015.
[37] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and
Michael K Reiter. “Accessorize to a Crime: Real and Stealthy
In ACM
Attacks on State-of-the-Art Face Recognition”.
SIGSAC Conference on Computer and Communications Secu-
rity (CCS), pages 1528–1540, 2016.
[38] Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernan-
des, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, and
Tadayoshi Kohno. “Physical Adversarial Examples for Ob-
ject Detectors”. In Proceedings of the USENIX Workshop on
Offensive Technologies (WOOT), 2018.
[39] David Stone.
“Spotlight on Lumens: How They’re Mea-
sured, and Why They’re Not All the Same”, [Online] Ac-
cessed: 2020-02-20. http://www.projectorcentral.com/
Lumens-Explained.htm.
[40] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Van-
houcke, and Andrew Rabinovich. “Explaining and Harnessing
Adversarial Examples”. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 1–9, 2015.
[41] Adam Van Etten.
“You Only Look Twice: Rapid Multi-
Scale Object Detection in Satellite Imagery”. arXiv preprint
arXiv:1805.09512, 2018.
[42] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and
Alan Yuille. “Mitigating Adversarial Effects Through Ran-
domization”. arXiv preprint arXiv:1711.01991, 2017.
[43] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi
Zhang, and Kai Chen. “Seeing isn’t Believing: Towards More
Robust Adversarial Attack against Real World Object Detec-
tors”. In Proceedings of the ACM Conference on Computer
and Communications Security (CCS), pages 1989–2004, 2019.
A Attack on Different Objects
The introduced attack can generalize in principle to any kind
of deep neural network which uses RGB-camera inputs to
make decisions. To show how our attack generalizes, we also
investigate the feasibility of the attack on different objects.
Setup. For Lisa-CNN and Gtsrb-CNN, we choose another
trafﬁc sign, “give way”, while for Yolov3 and Mask-RCNN
we choose the “bottle” class. For the give way sign and the
bottle, we run a reduced evaluation: we execute all the ex-
periment procedure steps reported in Section 5.1 and we test
the correct (mis-)classiﬁcation across a set of photos of the
altered objects. Extending our method to other objects is
straightforward, it only requires to change the input mask
of the projection and re-proﬁle the projectable colors. When
projecting on non-ﬂat surfaces, the adversary will also have
to consider the distortion introduced by those surfaces, this is
brieﬂy discussed in Section 6.
Results. We report example frames of successful attack on
other objects in Figure 15 and in Figure 16. These include
legitimate frames where the classiﬁcation works correctly.
All the pictures are taken in 180 lux ambient light. For Mask-
RCNN and Yolov3 we restricted the bottle size to [150, 250],
meaning that the bottle is generally in the foreground.
B Additional Results
We report extended results for the transferability-based cross-
network attack in Table 5. This includes each pair of the evalu-
ated models, including Lisa-CNN(s) and Gtsrb-CNN(s) which
are trained with cross-entropy loss from scratch. We report
an example frame from the outdoor experiment in Figure 14.
C SentiNet Description
Rationale. We picked SentiNet for the evaluation because it
was one of the few defences that was speciﬁcally designed to
detect physical adversarial examples (AE). In fact, there is a
plethora of works that creates physical adversarial examples
by using stickers (or patches) that are placed on the targeted
objects. The insight behind SentiNet is that these patches are
the most common way to create physical AE, but generate
small image areas with large saliency. This is not only a
USENIX Association
30th USENIX Security Symposium    1879
pute the salient areas in input SentiNet uses GradCam++ [11],
which backpropagates the outputs to the last convolutional
layer of the network and checks which region of the input
lead to greater activations. Since the resolution of this layer
is only 4x4 for both Gtsrb-CNN and Lisa-CNN, we instead
use XRAI [22], a newer and more accurate method to com-
pute salient areas. We found that using GradCam made the
output masks unusable as a resolution of 4x4 leads to coarse
block like regions where salient areas cannot be accurately
identiﬁed (resolution of this layer also is pointed out as a
problem in the original paper [13]). XRAI on the other hand
produces saliency regions at the input resolution, leading to
more granular salient areas, using an algorithm that incremen-
tally grows salient regions. As a consequence of this improved
technique, XRAI has been shown to outperform older saliency
algorithms, producing higher quality, tightly bound saliency
regions [22].
SentiNet computes a threshold function which separates
AE from benign images. The threshold function is computed
using: (i) the Average Conﬁdence, i.e., the average conﬁdence
of the network prediction made on benign test images where
salient masks are replaced with inert patterns added to them
and (ii) the Fooled Percentage, i.e., the percentage of benign
test images where overlaying the salient mask leads the net-
work to predict the suspected adversarial class. These two
scores characterize benign behaviour and can almost perfectly
separate benign from adversarial inputs in SentiNet. We fol-
low the same technique as in the original paper for ﬁtting the
threshold function that separates the malicious and benign
data. Our SentiNet implementation is also available with the
rest of the source code in the project repository.
Figure 14: Sample video frame extracted from the outdoor
experiment, for the 120 lux setting. The image shows the stop
sign undetected under the threshold set in the experiments.
The blue ‘tracker‘ box is set manually and tracks the location
of the sign.
detectable behavior in general, but it is also unavoidable for
the attacker to escape such behavior when creating a physical
AE (without replacing the entire object).
Description. SentiNet is a system designed to detect AE lever-
aging the intuition of locality of patches. If an adversarial
sample contains a patch which causes a misclassiﬁcation,
then the saliency of the area containing the patch will be high.
Therefore, the salient area will cause misclassiﬁcations on
other legitimate samples when overlayed onto them. To com-
1880    30th USENIX Security Symposium
USENIX Association
Target Model
N
Mask-RCN
N
Gtsrb-CN
N(a)
Gtsrb-CN
N(s)
Gtsrb-CN
N
Lisa-CN
N(a)
Lisa-CN
N(s)
Lisa-CN
Yolov3
0.0%
0.0%
2.0% 85.7% 0.0%
0.6% 35.7%
0.0%
100.0% 73.4% 0.0%
0.0%
98.7% 97.1% 0.0%
0.0%
40.5% 37.0% 99.9% 0.0%
29.4% 28.1% 6.8%
0.0%
99.9% 4.0% 14.6%
96.3% 91.0% 0.2%
12.3%
9.0%
96.5% 3.6%
2.5%
32.0% 14.0% 0.1%
2.0%
0.7%
8.6%
49.5% 0.8% 40.3%
0.0%
3.3% 41.1%
5.4%
0.0%
0.7% 43.7% 0.0%
0.7%
1.0%
2.4% 26.4%
0.0%
17.8% 0.2% 32.5%
0.4% 5.3%
0.1%
0.0%
0.9%
0.9%
0.0%
0.0%
0.0%
2.9% 48.0% 0.0%
4.9%
0.0%
0.0%
0.0%
7.2% 0.0%
0.0%
8.6%
0.0%
21.5%
0.0%
0.0%
15.5%
0.0%
16.1% 51.4%
0.0%
0.0% 100.0% 0.0%
0.0%
17.5%
0.0%
0.0%
54.8%
0.0%
27.7% 13.4%
0.0%
0.0% 100.0% 0.0%
2.3%
0.0%
0.0%
0.0%
10.4%
0.0%
43.1% 44.0%
0.0%
0.0% 100.0% 0.0%
0.0%
0.0%
40.9%
0.0%
0.0%
35.6%
44.1% 35.6%
0.0%
97.4% 0.0%
0.0%
0.0%
27.4%
4.6%
0.0%
4.9%
0.0%
57.5% 0.0%
0.0%
0.0%
7.5%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
Vision*
Google
100.0%
100.0%
72.4%
77.1%
90.6%
98.9%
44.4%
26.2%
72.3%
65.9%
47.6%
25.0%
35.3%
33.6%
33.1%
26.8%
23.7%
16.7%
21.1%
15.8%
lux Source Model no. frames
120
180
300
440
600
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
Yolov3
Mask-RCNN
Gtsrb-CNN
Lisa-CNN
4587
3765
3760
4998
7862
4083
7426
6268
5169
3543
3438
4388
6716
6023
6565
6287
5507
5058
4637
4714
Table 5: Transferability results. We test all the frames from the collected videos with a certain projection being shone against a
different target model, ﬁgures in bold are white-box pairs. (*) For Google Vision we only test one frame every 30 frames, i.e.,
one per second. We also remove all frames that are further than 6m away as Google Vision does not detect most of them in a
baseline scenario. _(a) indicates adversarially trained models. _(s) indicates models we re-trained from scratch.
USENIX Association
30th USENIX Security Symposium    1881
Figure 15: Attack on class “Give Way” for Gtsrb-CNN and Lisa-CNN.
Figure 16: Attack on class “Bottle” for Yolov3 and Mask-RCNN. The detection thresholds used in the paper are 0.4 and 0.6,
respectively.
1882    30th USENIX Security Symposium
USENIX Association