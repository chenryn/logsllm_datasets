tiﬁcate (§2) for Hp’s output, where each intermediate prime
pi is the result of hashing Hp’s input. (This is related to prior
approaches; see §8.) This strategy is economical when imple-
mented in constraints, because it uses very little pseudoran-
domness and requires only one exponentiation modulo the
resulting prime, plus a few smaller exponentiations.
We now describe the recursive step used to construct pi
from pi−1. Further below, we describe the base case and give
implementation details. Recall (§2) that a Pocklington witness
for pi comprises (pi−1,ri,ai) such that pi = pi−1·ri +1. (If pi
is prime, some ai must exist.) Notice that, given pi−1, one can
ﬁnd pi by testing candidate ri values until pi−1·ri +1 is prime.
To implement this in constraints, we let ri = 2bni · hi + ni,
where ni is a bni-bit number provided by P as advice and
hi is a bhi-bit pseudorandom number (we discuss its genera-
tion below). P furnishes a corresponding ai and C includes
constraints that compute pi and ri, and check the witness.
The base case is p0 = 2bn0 ·h0 +n0, for h0 a pseudorandom
number and n0 supplied by P . We ﬁx bn0 + bh0 = 32, i.e.,
p0 < 232, and the constraints test primality of p0 using a de-
terministic 3-round Miller-Rabin test that works for all values
up to 232 [73]. This test requires 3 exponentiations modulo
p0 with exponents less than 32 bits; these are inexpensive.
We choose bit widths bni such that a valid ni exists with
overwhelming probability, then choose bhi subject to the con-
straint that bhi + bni < log pi−1, which ensures that ri < pi−1
as required (§2). The entropy of each pi is ∑i
j=0 bh j; four
Security of this hash function rests on the fact that the
density of integers in the interval [0,α) with factors all less
than µ approaches β−β+o(1) as α → ∞, where β = logα
logµ . We
conjecture that this density also holds for a large interval
around α, namely,(cid:2)α,α + α1/8(cid:1). (This is closely related to a
relies; there, the interval is(cid:2)α−√
function with codomain(cid:2)0,2256(cid:1) with 128-bit collision resis-
Our hash function is deﬁned as follows: let ∆ be a pub-
lic 2048-bit integer chosen at random, and let H be a hash
conjecture on which the elliptic curve factorization method
α(cid:3) [71].)
√
α,α +
tance. Then H∆(x) = H(x) + ∆. Security of this construction
follows from the analysis of [48] in the random oracle model,
8In this section, we use entropy to mean (the negative logarithm of) P ’s
probability of guessing the correct value, i.e., the guessing entropy.
9Even though the prime (cid:96) comprises only 322 bits, C represents it with
352 (Fig. 3), which is the next multiple of the limb width bl (32 bits; §2.2).
USENIX Association
29th USENIX Security Symposium    2081
assuming the conjecture stated above. Concretely, we conjec-
ture that an adversary making q queries to H∆ has probability
roughly q· 2−128 of breaking division intractability.
H∆’s advantage over prior work is that its implementation in
constraints is much smaller. The system parameter ∆ is baked
into the constraints, and the only dynamic values to compute
are the base hash H(x) and the sum H(x) + ∆; using known
techniques [79], this sum is inexpensive. Moreover, since all
hashes must be reduced modulo the challenge (cid:96) (Eq. (7))
and H∆(x) mod (cid:96) = (H(x) + (∆ mod (cid:96))) mod (cid:96), the (costly)
reduction ∆ mod (cid:96) can be checked once in the constraints
and the result can be re-used for each H∆(x). We note that
while this approach gives smaller C than hashing to primes
(because H∆ and modular reductions are cheaper), it increases
P ’s work (because H∆’s bit length is longer; §4.4).
4.3 Multiprecision arithmetic optimizations
We describe two optimizations for multiprecision arithmetic
in constraints, building on ideas described in Section 2.2.
Computing greatest common divisor. We observe that ad-
dition and multiplication checks can be leveraged to verify a
statement gcd(x,y) = d by checking three equations over Z:
∃a,b
∃x(cid:48)
∃y(cid:48)
a· x + b· y = d
x(cid:48) · d = x
y(cid:48) · d = y
(8)
In constraints, the existential variables above correspond to
advice provided by P . Verifying coprimality (gcd(x,y) = 1)
reduces to condition (8), i.e., materializing the multiplicative
inverse of x modulo y. We use this simpliﬁcation in Section 4.1
to verify a Pocklington witness (§2).
Optimizing division and modular reduction. Prior work
implements division and modular reduction for a dividend x
and divisor d by having the prover provide, as advice, the quo-
tient q and remainder r < d such that x = q·d +r; this equality
is then checked with multiprecision arithmetic (§2.2). For cor-
rectness, C must enforce upper bounds on the bit widths of
q and r via bit splitting (§2.2), which requires as many con-
straints as the sum of the bit widths of q and r.
Since r can range from 0 to d − 1, its width is just that of
d. The width of q, however, is slightly more subtle. Since
q’s value is (cid:98)x/d(cid:99), a conservative choice is to assume q is as
wide as x. But this choice is imprecise: q is only as wide
as (cid:100)log2 ((cid:98)xmax/dmin(cid:99))(cid:101), where xmax denotes x’s maximum pos-
sible value, and dmin denotes d’s minimum possible value.
(Intuitively, this is because q is small when d is large.)
As in prior work [79], our system uses a dataﬂow analysis
to track the maximum value of each number, in order to de-
termine the required representation size. To bound q’s width
more tightly using the above expression, we augment this
dataﬂow analysis to also track minimum values.
4.4 Optimizing the cost of advice generation
The prior sections have treated P as an advice oracle. We now
discuss P ’s cost in computing this advice. Prior work [116,
121] shows that P ’s (single-threaded) cost per constraint is
≈100 µs or more (this includes, e.g., an elliptic curve point
multiplication per constraint [16, 64, 70, 96]). Computing
most advice values—including for multiprecision operations
and prime hashing—is negligible by comparison. Possible
exceptions are the witnesses for Wesolowski proofs (§2) used
by batch insertion and removal operations (§2.1). (Recall that
one of each operation is required for a MultiSwap; §3.)
The witness for a batch insertion(cid:74)S(cid:93){yi}(cid:75) =(cid:74)S(cid:75)∏i H∆(yi) is
the value(cid:74)S(cid:75)(cid:98)(∏i H∆(yi))/(cid:96)(cid:99). This exponent has length ≈2048·k
bits for k elements inserted. In microbenchmarks, GMP [66]
computes a 2048-bit exponentiation modulo a 2048-bit N
in ≈2.5 milliseconds (i.e., roughly 25× P ’s per-constraint
proving cost), so computing this value costs roughly the same
as 25· k constraints, which is inconsequential (§5, Fig. 3).
Batch removal is much more expensive. To prove that re-
moving the elements {xi} from the multiset S yields a new
(9)
(cid:113)S(cid:48)(cid:121) =(cid:74)S (cid:12){xi}(cid:75) = g∏s∈S(cid:12){xi} H∆(s)
multiset S(cid:48), P must prove that(cid:74)S(cid:75) =(cid:74)S(cid:48)(cid:75)∏i H∆(xi), where
No known method for computing(cid:74)S(cid:48)(cid:75) is faster than directly
evaluating this expression because the order of G is unknown
(recall that this computation is in G = Z×
N /{±1} where N has
unknown factorization; §2). Meanwhile, this exponent has bit
length ≈2048· M, for M the total size of the multiset S(cid:48), i.e.,
it costs roughly the same as 25· M constraints. (As discussed
in the prior paragraph, given(cid:74)S(cid:48)(cid:75) it is inexpensive to compute
the witness for batch removal, namely,(cid:74)S(cid:48)(cid:75)(cid:98)(∏i H∆(xi))/(cid:96)(cid:99)).
Even for large accumulators, this cost may be reasonable:
as we show in Section 7, MultiSwap can easily save tens of
millions of constraints compared to Merkle trees. On the other
hand, proof generation can be parallelized [121], whereas at
ﬁrst glance the exponentiation in (9) appears to be strictly
serial [22, 101]. We observe, however, that since g is ﬁxed, a
pre-computation phase can be used to sidestep this issue [33].
Speciﬁcally, for some upper bound 2m on the maximum size
of the accumulator, the above exponent is at most 22048·2m,
so pre-computing the values gi = g2i·2m
, 0 ≤ i < 2048 (via
successive squaring) turns the above exponentiation into a
2048-way multi-exponentiation [91] (which can be computed
in parallel): for each gi, the exponent is a 2m-bit chunk of the
value ∏s∈S(cid:12){xi} H∆(s). Further parallelism is possible simply
by computing more gi with closer spacing.
This precomputation also enables a time-space tradeoff,
via windowed multi-exponentiation [91, 110]. In brief, when
computing a multi-exponentiation over many bases, ﬁrst split
the bases into groups of size t and compute for each group a
table of size 2t. This turns t multiplications into a table lookup
and one multiplication, for a factor of t speedup. t = 20 is rea-
2082    29th USENIX Security Symposium
USENIX Association
sonable, and reduces the cost of computing the exponentiation
in (9) to roughly the equivalent of 1.25· M constraints.
The above pre-computation is a serial process that requires
≈2048· 2m squarings in G. Assuming that 2048 squarings
takes ≈2.5 milliseconds (i.e., the same amount of time as
a general 2048-bit exponentiation; this is pessimistic), this
precomputation takes ≈2m·2.5 milliseconds. For m = 20, this
is ≈45 minutes; for m = 25, it is ≈1 day. Note, however, that
this pre-computation is entirely untrusted, so it can be done
once by anyone and reused indeﬁnitely for the same g.
Finally, the above precomputation requires materializing
∏s∈S(cid:12){xi} H∆(s), which is 231 bits when M = 220. This prod-
uct can be expressed as a highly parallel computation; the
ﬁnal step is a multiplication of two, 230-bit values, which can
itself be parallelized via a Karatsuba-like approach.
We evaluate P ’s witness generation costs in Section 7.1.
5 Applications of MultiSwap
In this section we discuss two applications of MultiSwap
and compare constraint costs for these applications when
implemented using Merkle swaps and MultiSwaps.
MultiSwap Costs. The ﬁrst two rows of Figure 3 model the
costs of Merkle swaps and swaps computed via MultiSwap.
A Merkle swap requires hashing the old and new values
and Merkle path veriﬁcations for each (§2.1), so the number
of hash invocations is logarithmic in the number of leaves.
For a MultiSwap, each swap requires a H∆ invoca-
tion (§4.2), which comprises an invocation of the underlying
hash H and multiprecision arithmetic to compute the result
and multiply it mod (cid:96) (§4, Fig. 1). In addition, each swap is
an input to Hp, which requires another hash invocation. All
of these costs are independent of the number of elements in
the accumulator. MultiSwap also costs a large constant over-
head, however; this is to generate (cid:96) (§4.1) and check two
Wesolowski proofs via modular exponentiations (§2, §4).
5.1 Veriﬁable outsourcing for smart contracts
Blockchain systems [26] like Ethereum [53] enable smart
contracts: computations deﬁned by a blockchain’s users and
executed as part of the block validation procedure. One appli-
cation of smart contracts is implementing a form of veriﬁable
state update (§1): for global state Γ (stored on the blockchain)
and a transaction γ (submitted by a user), the computation
(1) checks that γ is valid according to some predicate, and if
so (2) updates the global state to a new value Γ(cid:48).
Consider, for example, a distributed payment system where
Γ comprises a list of users and their public keys and balances.
Transactions let users send payments to one another. When
Alice wishes to send a payment, she constructs a transaction
γ that includes (1) the target user; (2) the amount to send; and
(3) a digital signature over the prior two items; she submits
this to the smart contract, which veriﬁes it and updates Γ.
A major practical limitation of this approach is that com-
putation, storage, and network trafﬁc are extremely expensive
for smart contracts.10 One solution to this issue, Rollup [7, 65,
94], is an instance of veriﬁable computation (§2.2): the smart
contract delegates the work of checking transactions to an
untrusted aggregator, and then checks a proof that this work
was done correctly.11 To effect this, users submit transactions
γi to the aggregator rather than directly to the smart contract.
The aggregator assembles these transactions into a batch {γi},
then generates a proof π certifying the correct execution of a
computation Ψ that veriﬁes the batch and updates the global
state from Γ to Γ(cid:48). Finally, the aggregator submits π and Γ(cid:48)
to the smart contract, which veriﬁes the proof and stores the
updated state. Checking this proof is substantially cheaper for
the smart contract than verifying each transaction individually,
and the exorbitant cost of smart contract execution justiﬁes
the aggregator’s cost in generating the proof [115].
In more detail, the constraints C corresponding to Ψ (§2.2)
take the current state Γ as the input X and the updated state
Γ(cid:48) as the output Y . P (i.e., the aggregator) supplies the batch
{γi} as part of the witness (i.e., the advice vector Z), meaning
that the smart contract can verify the proof without reading
{γi}. This saves both computation and network trafﬁc.
Notably, though, even reading Γ and Γ(cid:48) is too expensive for
the smart contract, as is storing Γ on the blockchain. (Recall
that verifying a proof requires work proportional to the size of
the inputs and outputs; §2.2.) The original Rollup design [7]
addresses this by storing Γ in a Merkle tree (§2.1). The inputs
and outputs of C are just Merkle roots, and only this root is
stored on the blockchain. Each leaf of this tree contains a
tuple (pk,bal,#tx) comprising a user’s public key, their bal-