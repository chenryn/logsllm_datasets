of the baseline with only 0.1% of the data, and within
2.5% when trained on 15% of the data. These results
show that models trained on count-featurized data can
perform close to raw models in both balanced and very
imbalanced datasets (Criteo Full and Kaggle’s respective
click rates are 3% and 25%). On MovieLens (Fig. 4(a)),
the count-featurized boosted tree needs only 0.8% of
the data to get within 4% of the baseline, or match the
raw data logistic regression. Because counts summarize
history and reduce dimensionality, they allow algorithms
to perform well with very little data. We say that they
converge faster than raw data algorithms.
Second, counts enable new models. In Fig. 4, the
boosted tree performs poorly on raw data but very well
on the count-featurized data. This reveals an interesting
insight. The raw-data boosted tree uses a dimensional-
ity reduction technique known as feature hashing [41],
which hashes all categorical values to a limited-size
space. This technique exhibits a trade-off: increasing the
hash space reduces collisions at the cost of introducing
more features, leading to overﬁtting. Count featurization
does not have this problem: a categorical feature is
mapped to a few new features (roughly one per label
value). This lets us train boosted trees very effectively.
V.C. Past-Data Protection Evaluation (Q2)
We have shown that count-featurized algorithms con-
verge faster than models trained on raw data. This allows
Pyramid to keep, and thus expose, only a small amount
of raw data to train ML models. However the count
tables, while only aggregates of past data, can still leak
information about past observations. To prevent such
leaks, Pyramid adds differentially private noise to the
tables. The amount of noise to add depends on the
desired privacy guarantee, parameterized by  (smaller
is more private), but also on the number of features (see
Table II) and CMS hash functions (ﬁve here), through
the formula from §III-B2. In this section we evaluate
the noise’s impact on performance, as well as Pyramid’s
two mechanisms that
increase data utility: automatic
weighted noise infusion and the use of private count-
median sketches. We also show the impact of the number
of windows used, which deﬁnes the granularity at which
past observations can be entirely dropped.
Impact of noise. Fig. 5 shows the performance of
different algorithms and datasets when protecting an
observation, k = 1, with different privacy budgets 
(note the direct tradeoff between the two parameters:
the noise is proportional to k
 ). We ﬁnd that Pyramid
can protect observations with minimal performance loss.
88
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
When  = 1, the boosted tree model on the MovieLens
dataset remains within 5% of the baseline with only 1%
of the training data. The logistic regression and neural
network models on the Criteo-Kaggle dataset perform
within 2.7% and 1.8% of the baseline respectively, and
the Criteo-Full ridge regression is within 3%. All Criteo
models also come within 5% of their respective baseline
with a privacy budget as small as  = 0.2.
The Criteo-Full
ridge regression performance de-
grades less than models on other datasets when the noise
increases. For instance, it degrades by less than 1% with
 going from 1 to 0.1, while the Criteo-Kaggle neural
network loses 6.5%. This is explained by the fact that the
amount of noise required to make a query differentially
private is not related to the size of the dataset. The
Criteo-Full dataset is much larger, so the additional noise
is much smaller relative to the counts.
Weighted noise infusion. Weighted noise infusion is in-
tegral to the protection of past observations with minimal
performance cost. Fig. 6(a) shows the impact of noise
on the boosted tree for the MovieLens dataset. Without
weighting the privacy budget of different features, the
model performs 15% worse than the baseline even for
 = 1. With weighting, the MovieLens model performs
at 5% of the baseline. The weighted noise infusion
technique is thus critical to maintaining performance on
the MovieLens dataset. Intuitively, this is because the
users making the rating and the movie being rated are the
most important features when predicting ratings. Most
users rate relatively few movies, and a long tail of movies
are rarely rated, so their respective counts are quickly
overwhelmed by the noise when the privacy budget is
equally distributed among all features.
The Criteo models do not depend as much on the
weighting trick, since they do not rely on a few features
with small counts. Noise weighting is still beneﬁcial,
though: e.g., the Criteo-Kaggle neural network gains
about 0.5% of performance, as shown in Fig. 6(b).
Private count-median sketch. Another technique that
Pyramid uses to reduce the impact of noise is to switch
to a private count-median sketch. As noted in §III-B2,
the count-min sketch will exhibit a strong downward bias
when initialized with differentially private noise, because
taking the minimum of multiple observations will select
the most extreme negative noise values. The count-
median sketch uses the median instead of the minimum
and does not suffer from this effect. Fig. 6(c) shows that
when noise is added, the count-median sketch improves
performance over the count-min sketch by around 0.5%,
on MovieLens and Criteo-Kaggle.
When combined with weighted noise infusion, the pri-
vate count-median sketch is less useful at ﬁrst, since the
noise is small on features with small counts. However, it
provides an improvement for lower . For instance, the
MovieLens boosted tree improves by 0.5% even after
noise weighting for  = 0.10.
Number of windows. Another factor impacting accuracy
is the number of count windows kept to support granular
retention policies. Fig.
7 shows Criteo-Full’s ridge
regression for k = 1 and  = 1 while varying the
number of windows. We observe that it is possible to
support a large number of windows. On Criteo, we can
support 1000 windows with little degradation, enough
to support a daily granularity for a multi-year retention
period. While we believe this granularity for retention
policies should be enough in practice, we also simulated
a binary tree scheme [42] that supports huge numbers of
windows. We can see that on Criteo, this allows using
100K windows with a penalty similar to 10 windows
using the basic scheme.
V.D. Count Selection Evaluation (Q3)
Without noise. We measure the performance of our
algorithms when the featurization is augmented by MI-
selected groups. We evaluate on MovieLens, as groups
provided little additional beneﬁt on Criteo. A total of
35 groups were selected by MI and given 10% of the
privacy budget to share. When using these groups, the
accuracy of the count boosted tree gets within 3% of
the baseline with the same 0.8% of the data, 1% better
than without feature groups. Logistic regression does
not improve asymptotically but converges faster, getting
within 5% of the baseline with 15% of the data instead
of 22%. Thus, count selection selects relevant groups.
With noise. We also evaluate the impact of group se-
lection on MovieLens with noise k = 1,  = 1. Logistic
regression is not improved by the grouped features, but
the boosted tree is still 1% closer to the baseline. Thus,
the algorithm can still extract useful information from
the groups despite the increased noise.
While these results are encouraging, we leave for fu-
ture work the full investigation of how the improvement
in accuracy gained from maintaining and using relevant
groups is affected by the higher noise levels necessary
to maintain a large number of count tables for ﬁxed .
V.E. Performance Evaluation (Q4)
We evaluate Pyramid’s overhead on Velox by mea-
suring the median latency of a prediction request to
Velox. We perform this evaluation using the 39-feature
Criteo dataset. Fig. 9 shows the median latencies and a
breakdown of the time into four components: computing
the prediction, unmarshalling the message into a usable
form, performing count featurization, and other functions
like the network and traversing the web stack. We show
the results with and without count table caching in the
application servers (§IV). Without caching, prediction
latency is around 200ms. Caching reduces it to 1.6ms,
a 5% overhead with the total time dominated by the
network and traversing the web framework used to
89
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
(a) MovieLens boosted tree
(b) Criteo-Kaggle neural network
Fig. 6: Impact of data protection (continued). Results are normalized to the baselines. We ﬁx k = 1 and vary , the privacy budget. (a)
Without the feature weighting trick the gradient boosted trees perform unacceptably poorly. (b) The weighting trick marginally improves the
performance of Criteo-Kaggle models over equally distributing the privacy budget. (c) Private count-median sketch improves performance in
both MovieLens (ML) and Criteo-Kaggle (CK) models with  = 1.
(c) Sketch comparison
Action
Featurization
Marshalling
Prediction
Network/Framework
Total Latency
P. w/o cache P. w/ cache Velox
N/A
4.37%
7.06%
6.44%
0.51%
0.63%
88.68% 92.31%
1.65 ms
1.58 ms
99.22%
0.04%
0.01%
0.73%
283.69 ms
Fig. 7: Criteo-Full windows. The Criteo
datasets can support 1K windows with rea-
sonable penalty. Supporting more windows
requires a scheme based on binary trees.
Fig. 8: MovieLens regression. Linear
regression algorithms are not amenable.
Boosted tree converges quickly but does not
match the baseline.
Fig. 9: Prediction Latency. Median time to serve
a model prediction. Caching is crucial for Pyramid
to achieve low overhead compared to Velox.
implement Velox. Pushing count tables to the application
servers is crucial for performance and does not signiﬁ-
cantly increase the attack surface.
V.F. Applicability Evaluation (Q5)
Pyramid works well for classiﬁcation problems. We
now consider another broad class of supervised learning
problems: regression problems. In regression, the algo-
rithm guesses a label on a continuous scale, and the goal
is for the prediction to be as close to the true label as
possible. Intuitively, count featurization should be less
effective for regression problems, because it needs to
bin the continuous label into discrete buckets.
Fig. 8 shows the performance of linear and boosted
tree (nonlinear) regressions on the MovieLens dataset.
We ﬁrst observe that linear regression does worse on
count-featurized data than on raw data. This is not sur-
prising: count featurization gives the probability of each
label conditioned on a feature. The algorithm cannot ﬁnd
a linear relationship between, say, P (rating = 3|user)
and the rating. Indeed, the rating does not keep growing
with this probability, it keeps getting closer to 3.
Nonlinear algorithms do not have this limitation. The
boosted tree converges quickly and outperforms raw
models trained on similar amounts of data until we reach
55% of the data. At that point, the boosted tree plateaus
and never comes close to the baseline. Although we did
not ﬁnd good algorithms for this dataset, we suspect that
some nonlinear algorithms may perform well on counts.
Count featurization is most reminiscent of the counts
used by Naive Bayes classiﬁers [43], and there are
Fig. 10: Estimated article CTR for MSN. The raw model, count
model, and private count model are normalized against the estimated
performance of human editors. The count models perform slightly
worse than the raw models; all models outperform human editors on
ﬁve out of seven days.
workloads for which it is not suitable. For instance, count
featurization requires a label and is thus not applicable
to unsupervised learning. Other feature representations
may be better suited to such types of models. Our
choice of count featurization reﬂects its suitability to data
protection in a practical system architecture.
Even in settings that are less amenable to Pyramid,
such as online learning applications that avoid retraining,
we found that Pyramid can perform well and help protect
past observations, as we describe in the next section.
V.G. Experience with a Production Setting
In addition to public datasets, we also evaluated
Pyramid on a production workload. One of the authors
helped build MSN’s news personalization service, which
we used to evaluate three aspects: (1) How to adapt
count featurization to a different type of learning, (2)
90
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
how Pyramid applies to this application, and (3) how
Pyramid supports the application’s workload evolution.
Adapting count featurization. MSN uses contextual
bandit learning [44], [45] (via the Decision Service [46])
to personalize the order of articles shown to each user
so as to maximize clicks, based on 507 features of
user demographics and past browsing history. This is a
challenging scenario due to the large number of features
and low click signal. Contextual bandit algorithms use
randomization to explore different action choices, e.g.,
picking the top article at random. This produces a
dataset that assigns a probability (importance weight) to
each datapoint. The probabilities are used to optimize
models ofﬂine and obtain unbiased estimates of their
performance had they been run online [38], [39], [47].
Importance-weighted data have interesting implica-
tions for Pyramid. When updating the count tables with
a given data point, Pyramid must increment the counts
by 1/p, rather than 1, to ensure they remain unbiased.
This weighting also increases the noise required for
differential privacy, because the sensitivity of a single
observation can now be as high as 1/pmin, where pmin
is the minimum probability of any data point.