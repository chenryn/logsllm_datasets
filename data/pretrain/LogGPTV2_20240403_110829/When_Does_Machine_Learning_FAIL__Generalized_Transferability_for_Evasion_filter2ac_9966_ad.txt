previously trained network using these batches until ei-
ther the attack is perceived as successful or we exceed
the number of available poisoning instances, dictated by
the cut-off threshold of Nmax. It is worth noting that if the
learning rate is high and the batch contains too many poi-
son instances, the attack could become indiscriminate.
Conversely, too few crafted instances would not succeed
in changing the target prediction, so the attacker needs to
control more batches.
The main insight that motivates our method for gen-
erating adversarial samples is that there exist inputs to a
network x1,x2 whose distance in pixel spaceࢯࢯx1 − x2ࢯࢯ
ࢯࢯHi(x1) − Hi(x2)ࢯࢯ, where Hi(x) is the value of the ith
is much smaller than their distance in deep feature space
hidden layer’s activation for the input x. This insight is
motivated by the very existence of test-time adversarial
examples, where inputs to the classiﬁer are very similar
in pixel space, but are successfully misclassiﬁed by the
neural network [4, 44, 17, 50, 37, 9]. Our attack consists
of selecting base instances that are close to the target t
in deep feature space, but are labeled by the oracle as
the attacker’s desired label yd. CRAFTINSTANCE cre-
ates poison images such that the distance to the target t
in deep feature space is minimized and the resulting ad-
versarial image is within τD distance in pixel space to t.
Recent observations suggest that features in the deeper
layers of neural networks are not transferable [52]. This
suggests that the selection of the layer index i in the ob-
jective function offers a trade-off between attack trans-
ferability and the magnitude of perturbations in crafted
images (Cost D.III). In our experiments we choose Hi to
be the third convolutional layer.
We pick 100 target instances uniformly distributed
across the class labels. The desired label yd is the one
closest to the true label yt from the attacker’s classiﬁer
point of view (i.e. it is the second best guess of the clas-
siﬁer). We set the cut-off threshold Nmax = 64, equivalent
to two mini-batches of 32 examples. The perturbation is
upper-bounded at τD  0.14 prior to adding them to I.
The Drebin classiﬁer uses VirusTotal as the oracle.
In our experiments, the poison instances would need to
maintain the benign label. We systematically create over
19,000 Android applications that correspond to attack in-
stances and utilize VirusTotal, in the same way as Drebin
does, to label them. To modify selected features of the
Android apps, we reverse-engineer Drebin’s feature ex-
traction process to generate apps that would have the de-
sired feature representation. We generate these applica-
tions for the scenario where only the subset of features
extracted from the AndroidManifest are modiﬁable by
the attacker, similar to prior work [19]. In 89% of these
cases, the crafted apps bypassed detection, demonstrat-
ing the feasibility of our strategy in obtaining negatively
labeled instances. However, in our attack scenario, we
assume that the attacker is not consulting the oracle, re-
leasing all crafted instances as part of the attack.
For the exploit predictor, labeling is performed inde-
pendently of the feature representations of instances used
by the system. The adversary manipulates the public dis-
course around existing vulnerabilities, but the label exists
with respect to the availability of an exploit. Therefore
the attacker has more degrees of freedom in modifying
the features of instances in I, knowing that their desired
labels will be preserved.
In case of the data breach predictor, the attacker uti-
lizes organizations with no known breach and aims to
poison the blacklists that measure their hygiene, or hacks
them directly. In the ﬁrst scenario, the attacker does not
require access to an organization’s networks, therefore
the label will remain intact. The second scenario would
be more challenging, as the adversary would require ex-
tra capabilities to ensure they remain stealthy while con-
ducting the attack.
5 Evaluation
geted poison samples transferable? Is StingRay effective
against multiple applications and defenses? We quantify
the effectiveness of the evasion attack using the percent-
age of successful attacks (SR), while for StingRay we
also measure the Performance Drop Ratio (PDR). We
measure the PDR on holdout testing sets by consider-
ing either the average accuracy, on applications with bal-
anced data sets, or the average F1 score (the harmonic
mean between precision and recall), which is more ap-
propriate for highly imbalanced data sets.
5.1 FAIL Analysis
In this section, we evaluate the image classiﬁer and the
malware detector using the FAIL framework. The model
allows us to utilize both a state of the art evasion at-
tack as well as StingRay for the task. To control for ad-
ditional confounding factors when evaluating StingRay,
in this analysis we purposely omit the negative impact-
based pruning phase of the attack. We chose to imple-
ment the FAIL analysis on the two applications as they
do not present built-in leverage limitations and they have
distinct characteristics.
Evasion attack on the image classiﬁer. The ﬁrst at-
tack subjected to the FAIL analysis is JSMA [35], a
well-known targeted evasion attack Transferability of
this attack has previously been studied only for an ad-
versary with limited knowledge along the A and I di-
mensions [37]. We attempt to reuse an application con-
ﬁguration similar in prior work, implementing our own
3-layer convolutional neural network architecture for the
MNIST handwritten digit data set [26]. The validation
accuracy of our model is 98.95%. In table 3, we present
the average results of our 11 experiments, each involving
100 attacks.
For each experiment, the table reports the ∆ variation
of the FAIL dimension investigated, two SR statistics:
perceived (as observed by the attacker on their classiﬁer)
and potential (the effect on the victim if all attempts are
triggered by the attacker) as well as the mean perturba-
tion ¯τD introduced to the evasion instances.
Experiment #6 corresponds to the white-box adver-
sary, where we observe that the white-box attacker could
reach 80% SR.
We start by evaluating weaker evasion and poisoning ad-
versaries, within the FAIL model, on the image and mal-
ware classiﬁers (Section 5.1). Then, we evaluate the ef-
fectiveness of existing defenses against StingRay (Sec-
tion 5.2) and its applicability to a larger range of classi-
ﬁers. Our evaluation seeks to answer four research ques-
tions: How could we systematically evaluate the trans-
ferability of existing evasion attacks? What are the limi-
tations of realistic poisoning adversaries? When are tar-
Experiments #1–2 model the scenario in which the
attacker has limited Feature knowledge. Realistically,
these scenarios can simulate an evasion or poisoning at-
tack against a self-driving system, conducted without
knowing the vehicle’s camera angles—wide or narrow.
We simulate this by cropping a frame of 3 and 6 pix-
els from the images, decreasing the available features