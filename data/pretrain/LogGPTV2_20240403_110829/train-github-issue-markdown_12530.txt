## Environment info
  * `transformers` version: 4.0.0 --rc1
  * Platform: Colab
  * Python version: 3.6.9
  * PyTorch version (GPU?): TESLA V4
  * Tensorflow version (GPU?): 2.3.0
  * Using GPU in script?: YES
  * Using distributed or parallel set-up in script?: NO
### Who can help
## Information
Model I am using (Bert, XLNet ...): T5-1.1
The problem arises when using:
  * the official example scripts: (give details below)
  * [X ] my own modified scripts: (give details below)
    from transformers import T5TokenizerFast, T5ForConditionalGeneration
    TOKENIZER_NAME = 't5-base'
    MODEL_NAME = 'google/t5-v1_1-base'
    tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)
    special_tokens_dict = {'additional_special_tokens': ['','']}
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
    model = T5ForConditionalGeneration.from_pretrained('google/t5-v1_1-base', return_dict=True)
    model.resize_token_embeddings(len(tokenizer))
    model.to("cuda")
The tasks I am working on is:
  * an official GLUE/SQUaD task: (give the name)
  * my own task or dataset: (give details below)  
Custom dataset
## To reproduce
Steps to reproduce the behavior:
  1. Attempting to add Entity tokens to T5 1.1, upon loading from pretrained the following error occurs:  
`size mismatch for lm_head.weight: copying a param with shape
torch.Size([32128, 768]) from checkpoint, the shape in current model is
torch.Size([32102, 768]).`
I am assuming the addition of the special tokens did not get propagated to the
lm head size.
I would expect the LM Head to be resized in addition to the standard layers.
Many Thanks,  
Chris