of 1 2% to store these pointers. 
partitions 
fact that failed cells can still be read and reuses the  cell 
to store data.  LLS 
[ 1 0] is a line-level  mapping 
technique 
that dynamically 
PCM space into a 
main space and a backup space. By mapping failed lines to 
the backup,  LLS 
memory 
space that provides  easy 
When accessing 
a failed line, the request 
to access the mapped backup line through 
translation 
logic, 
and also  incurs 
on intra-line 
failures. RDIS [ 1 5] incorporates 
scheme in hardware 
faults during 
storage 
which lowers the cost of redesign 
the performance 
manages to  maintain 
integration 
PCM chip redesign 
LLS  also 
a contiguous 
with wear leveling
. 
which requires 
extra latency 
address 
efforts 
relies 
cell locations 
PCM 
write. In RePRAM, we use off-the-shelf 
and perform minimal changes to existing 
will be redirected 
a special 
and helps us to minimize 
such as ECP, to correct 
to track memory 
error correction 
salvaging, 
a recursive 
and energy. 
hardware, 
initial 
cell 
that have 
overheads. 
FREE-p [32] performs 
fine-grained 
remapping 
of worn­
dedicated 
these mappings. Pointers 
out PCM blocks without requiring 
storing 
stored within  the 
adds extra requests 
results 
the number of faults 
bandwidth 
per block 
for memory to read these blocks, 
overheads. As 
this approach 
memory block and the  memory 
in additional 
and latency 
increase, 
to remapped blocks are 
which 
incurs 
controller 
storage  for 
ECP incurs a static storage  overhead 
Arrays of Independent  Memory 
Filesystems 
due  to sequential 
overheads 
memory bandwidth 
higher performance 
reads and increased 
RePRAM incurs significantly  lower 
as the memory requests 
and performed 
memory 
Whereas, 
overheads 
to group blocks can be overlapped 
simultaneously 
(as shown in Section 
performance 
demands. 
IV). 
C. Redundancy-based 
Techniques 
was 
as a low­
a number 
we actually 
many similar 
con­
aim to reuse faulty 
data to achieve high availability 
Redundant 
[ 1 2], and inspired 
Since then, RAID has become multi­
has a higher 
compared to average 108 writes per PCM cell. 
Gibson, and Katz  for 
Disks (RAID) [ 1 7] in 1 9 87, where 
five RAID levels were presented 
system. 
industry 
redundant 
Storing 
by Patterson, 
explored 
Arrays of Inexpensive 
the original 
cost storage 
billion 
dollar 
cepts such as Redundant 
(RAIM) [8], Redundant Arrays 
of Independent 
(RAIF) [ 11], and so on. While our work leverages 
of redundancy  techniques, 
PCM pages, whereas RAID was designed 
to recover 
the 
data on a failed hard disk, which statistically 
availability 
In the context 
new problems, 
read/write  performance, 
very high level, 
hierarchical 
deployed 
high performance 
efficiency. 
monitoring  of 
migrated 
RePRAM employs a unique redundancy 
stages of the PCM lifetime, 
its usability 
mapping management, 
and limited 
our idea has similar 
access patterns and data blocks are 
in  the background. 
our 
as HP  AutoRAID 
of PCM-based 
such  as 
system [29], where RAID 1 and 5 are 
memory, we face a set of 
automatically 
which is selected 
for high storage 
is made upon active 
level at different 
to maximize 
at two different 
and performance. 
In contrast, 
In AutoRAID, 
write cycles. 
the decision 
and the latter 
data access, 
workload 
storage 
levels, 
asymmetric 
spirit 
On the 
with the former  used  for 
III. DESIGN OVERVIEW 
In this section, 
we first describe 
the overview 
ware design and later show how our proposed 
can be incorporated 
We also show the software support 
multi-core 
into the 
needed for RePRAM. 
of our hard­
processor 
hardware. 
modifications 
A. Incorporating Dynamic Redundancy Techniques 
into 
PCM 
In this paper, we assume that the PCM-based 
any faults 
and has wear-leveling 
distribute 
the writes throughout 
main 
algo­
page, a read-after-write 
is 
if the write succeeded. 
Code  (ECC) 
exists onchip, the 
For cases 
bit faults can be tolerated  using  these 
In particul
ar, SECDED (single-e
rror correction, 
pre-built 
without 
memory starts 
rithms in place to  uniformly 
the pages. For each write to  the 
to determine 
performed 
where Error Correcting 
first  few 
schemes. 
detection) 
double-error 
Hi-ECC [28] can correct 
which can be utilized 
use our RePRAM schemes. 
ECC can correct 
one bit error, 
while 
up to four bit errors, both of 
errors  before 
to 
beginning 
to tolerate 
Note that this does not require 
20 18 16 14 12 10 8 6 4 2 
ns. After the 
not tolerate 
the PCM-based 
main memory that did 
capabilities  can 
impact on the applicatio
and thus for the first few faults, 
RePRAM intervention, 
avoids performance 
point when the error correction 
any more faults, 
not use any additional  lifetime  extension 
be forced to discard 
would lead to a quickly 
To maximize PCM lifetime 
low cost, we propose 
advanced 
of the PCM-based 
hardware 
full benefit of this new memory architecture. 
the faulty pages. Subsequently, 
of the memory. 
diminishing 
with minimal overheads 
and 
main memory. We devise a number of 
the 
RePRAM that recycles 
techniques 
dynamic redundancy 
would 
this 
(and supporting 
strategies, 
in the context 
techniques 
to realize 
capacity 
software) 
o +""'.-r"--,... 
2-way matching 
• 3-way matching 
..  -- ,..... 
5'  5'  5'  5'  5'  5'  5'  5' 
 5'  5'  5'  5'  5'  5' 
N  '"  ... '"  '" 
.,  en 
N  '"  ... '"  '"  .... 
....  ....  ....  ....  ....  ....  .... 
....  .... 
....  ....  ....  ....  .... 
   .... - ....  .... - .... - .... -
E 
E  E  E  E 
..,  .., 
.., 
..,  .., 
"   
"  "  "  "  "  " 
:;  :;  :;  :;  :;  :;  :; 
and leverages 
.., 
0 
Figure 1. Averag number of random trials 
needed for the two-way 
between faulty pages. Each pair of bars show the 
in 
within the  fault 
bounds indicated 
matching 
and three-way 
number of tries needed for matching 
the x-axis. 
peM pages. 
Faults are assumed to be randomly  distributed 
within the 4 KB 
high-performance 
parity to minimize 
PCM page k oc­
the ECC 
error 
this page 
In RePRAM, when the first fault in  the 
limit of already 
the tolerance 
are waiting 
schemes), 
an operation 
incorporated 
faulty pages 
we temporarily 
by most ECC-based 
to be matched with other 
decommission 
pool of PCM pages POOLpDR, 
[ 1 9]. We reuse the parity bits to store the 
parity support, 
whether it is 
level of PDR, 
curs (beyond 
correction 
and place it in a separate 
where the 
this point, we disable 
compatible  faulty  pages.  At 
computation, 
supported 
memory controllers 
built-in 
faulty byte vector  within  the 
i.e., for each  byte  we 
have a bit to indicate 
faulty or not. We start with the redundancy 
in which  a 
of data pages. Basically, we group multiple 
pages that are compatible 
same byte position) 
DRAM buffer to  store 
the performance 
impact. 
(n) is determined 
that for higher values of n, it becomes  exceedingly 
to find compatible 
complexity. 
difficult 
parity block is associated with a group 
The number of data pages per group 
the corresponding 
based on the matching 
do not have faults in the 
and use a separate 
faulty  pages. 
dedicated 
already 
faulty PCM 
(i.e., 
Note 
When we decommission 
the PCM page k, we copy its 
is 
PCM pages, which  serve 
we randomly 
into one of the reserve 
PCM pages.  To  start, 
until the matching 
we assume that there are 1 0,000 such 
contents 
to store the data in the faulty  pages 
done.  By  default, 
pick a group G 
reserve 
of compatible  faulty  pages  from 
a low-cost 
to one in 
DRM [9] can be used to assemble 
faulty pages 
POOLpDR. Alternatively, 
algorithm 
the group G of compatible 
from POOLpDR. The mapping information 
stored as tuples MAPPDR = {kl' k2' ... , kn' P}, 
P is the parity page, which are managed 
System (OS) (described 
the group size and 
by the Operating 
example, 
in a PDR group size of three, a tuple of the form, 
is 
where n is 
approximate 
in ill-C). For 
{kl' k2' k3, P} is stored, 
where kl' k2' k3 are the 
compatible 
similar 
pairing 
and P is the parity  page. 
of 
group size of three for PDR 
In the remainder 
faulty  pages 
this paper, we use a default 
configuration 
order matching, 
storage 
as it is relatively 
less complex than higher 
while offering much better data to parity 
ratio than PDR with the group size of two. 
sharply. To 
In the two­
similar 
results. 
evaluate 
increases 
we simulate 
and observed 
we randomly 
number of bit faults 
the mapping process, 
the average number  of 
pick two faulty pages and check 
the 
under PDR. 
4KB pages and we averaged 
needed for 2-way and 3-way matching 
true when using PDR, as the 
compatible 
pages 
the trails 
were done by assuming 
faults. This is especially 
cost associated with finding the  three-way 
beyond a certain 
empirically 
trials 
in both 
Figure 1 presents 
a pool 
cases. These experiments 
of one million 
over 10,000 
samples. We also tried a pool of 1 0,000  4KB pages with 
over 1,000 samples 
way matching, 
if they are compatible. 
randomly  picked  page 
pages. The average 
repeat our experiments 
ranges as shown in Figure  1. 
was done for the three-way 
pages. Beyond 80 faults (we refer to this as three-way fault 
we notice  that  the  number 
needed for 
threshold), 
three-way 
matching 
two-way trials 
continuing 
beyond the three-way fault threshold 
matching 
pages. 
If not, we repeat with another 
new 
until we find a compatible 
set of 
for 
groups of compatible 
three 
will be expensive 
number  of  tries  needed 
is recorded. 
that  forms 
by bounding 
and steeply 
increases 
algorithm 
is at least twice as the number of  the 
of trials 
faulty 
We 
our faults in various 
to operate in PDR with group  size  of 
The same set of experiments 
matching 
for compatibility 
of 
from there. Therefore, 
In order to bound the complexity 
associated with match­
two design dimensions 
ing, we explore 
incurs the number 
fault threshold: 
of errors 
that is greater 
after a faulty page 
than the 
three-way 
1) Dim-I: Switch to MDR that employs the mirroring­
based dynamic redundancy 
two identical 
pages. 
copies of the data onto two different 
PCM 
technique, 
where we  store 
2) Dim-2: Reduce the PDR group size to two  and 
continue 
to operate under parity-based dynamic redundancy 
mode to tolerate 
future faults. 
We note that matching 
process 
(finding compatibility) 
We choose to explore 
the above two 
is much easier with pages that have fewer number of 
to provide  more 
user-friendly 
options 
dimensions 
and allow the users to 
primarily 
PCM Memory 
(a) Dim-J 
(b) Dim-2 
Figure 2. Hardware 
memory requests 
whereas in Dim-2, the number of requests 
and structures 
issued to the memory structure 
modifications 
(2 or 3) depends on the current group size). 
(shown in gray boxes) needed to incorporate 
RePRAM into the processor 
is shown next to the arrows (e.g., 
PDR always issues 3 PCM requests 
hardware. 
The number of 
in Dim-l for  a group size of 3; 
decide what is for their best 
tify the resulting 
impact. 
the configuration 
that suits their needs. 
lifetime 
interests. Our experiments 
and the associated performance 
quan­
We note that the end users should be able to  choose 
Dim-I: We switch to MDR mode when one of the newly 
and renders 
we reassign 
operations 
in PCM-based 
fault threshold 
matched PCM page (in PDR) 
fault in an already 
the three-way 
the group 
This new fault will be detected 
by 
memory [31]. 
the faulty PCM page to 
occurring 
surpasses 
pages incompatible. 
the read-after-write 
When this happens, 
a new pool POOLMDR, where pages are waiting to  be 
matched with other compatible 
fashion.  We  use 
for the remaining 
portion 
corresponding 
mirroring 
bit faults 
and their 
are stored in M APMDR, managed 
by the as. Similarly, the mappings 
tuples of {k1, k2} where kl and k2 are  compatible 
to tolerate 
of the pages' lifetime 
can be represented 
the MDR technique 
faulty pages in the 
mappings 
faulty 
as 
pages that mirror the content  of  each 
point  out 
PCM pages, other pages that  are 
will continue 
remapping 
other. It is worthy to 
that even after MDR mapping is adopted for some 
in MAPPDR 
to be in PDR mode until there is a need for 
these faulty pages. 
currently 
Dim-2: In this case, throughout 
the lifetime 
of the device, 
to operate 
we continue 
in the PDR mode where a group 
of PCM pages have an associated parity page. The only 