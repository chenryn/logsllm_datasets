title:When Malware Changed Its Mind: An Empirical Study of Variable Program
Behaviors in the Real World
author:Erin Avllazagaj and
Ziyun Zhu and
Leyla Bilge and
Davide Balzarotti and
Tudor Dumitras
When Malware Changed Its Mind: An Empirical Study 
of Variable Program Behaviors in the Real World
Erin Avllazagaj, University of Maryland, College Park; Ziyun Zhu, Facebook; 
Leyla Bilge, NortonLifeLock Research Group; Davide Balzarotti, EURECOM; 
Tudor Dumitras, University of Maryland, College Park
https://www.usenix.org/conference/usenixsecurity21/presentation/avllazagaj
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.When Malware Changed Its Mind
An Empirical Study of Variable Program Behaviors in the Real World
Erin Avllazagaj, Ziyun Zhu+, Leyla Bilge*, Davide Balzarotti†, Tudor Dumitras,
University of Maryland, College Park
*NortonLifeLock Research Group
+Facebook
†EURECOM
Abstract
Behavioral program analysis is widely used for understanding
malware behavior, for creating rule-based detectors, and for
clustering samples into malware families. However, this ap-
proach is ineffective when the behavior of individual samples
changes across different executions, owing to environment
sensitivity, evasive techniques or time variability. While the
inability to observe the complete behavior of a program is a
well-known limitation of dynamic analysis, the prevalence of
this behavior variability in the wild, and the behavior com-
ponents that are most affected by it, are still unknown. As
the behavioral traces are typically collected by executing the
samples in a controlled environment, the models created and
tested using such traces do not account for the broad range
of behaviors observed in the wild, and may result in a false
sense of security.
In this paper we conduct the ﬁrst quantitative analysis
of behavioral variability in Windows malware, PUP and be-
nign samples, using a novel dataset of 7.6M execution traces,
recorded in 5.4M real hosts from 113 countries. We analyze
program behaviors at multiple granularities, and we show how
they change across hosts and across time. We then analyze
the invariant parts of the malware behaviors, and we show
how this affects the effectiveness of malware detection using
a common class of behavioral rules. Our ﬁndings have action-
able implications for malware clustering and detection, and
they emphasize that program behavior in the wild depends
on a subtle interplay of factors that may only be observed at
scale, by monitoring malware on real hosts.
1 Introduction
The ability to understand and model malware behavior plays
a key role in many security applications. This typically in-
volves executing samples inside an instrumented environ-
ment, designed to collect system and API call traces that
can be further analyzed to reconstruct the runtime behavior.
Such behavioral analysis methods have been applied to de-
tecting [10,12,16,23,24,32] new or polymorphic malware for
which static analysis fails [37, 48], and to clustering samples
into malware families [5, 7, 40, 41], in order to identify the
malicious behaviors that characterize each family. However,
the effectiveness of all these methods depends on their ability
to identify invariant parts of the behavioral traces. In conse-
quence, variations in the observed malware behavior, which
may arise from adversarial intent [6, 20] or biases in the data
collection [43], can result in models that overﬁt the analysis
environment and fail to generalize to the behavior observed
in the wild. This problem, which is a consequence of the
limitations of dynamic analysis, is widely accepted among
researchers and practitioners as a fundamental challenge for
behavioral analysis.
Unfortunately, just how much the behavior of malware
varies in the wild is a largely open question, outside of a
few prominent and well studied malware families. A common
approach to accounting for behavior variability is to acquire
multiple samples of the same family and to analyze their ex-
ecutions together, in order to extract the common behavior
patterns of the malware family. However, if the behavior of in-
dividual samples varies, across different hosts and across time,
the common patterns extracted will not be representative of
the malware’s behavior on real hosts. Additionally, the behav-
ioral traces are typically collected by executing the malware
in a controlled environment [1, 2, 17, 52], in order to prevent
it from harming other hosts. If the behavioral models are cre-
ated and tested with traces collected in the same environment
and during the same time period, artifacts that only manifest
under those conditions will inﬂate the apparent effectiveness
of those models and give a false sense of security.
It has been challenging to measure per-sample variability
systematically, despite the fact that researchers and practition-
ers have known about it for over a decade. For example, Lin-
dorfer et al. reported that one sample’s behavior may change
across execution environments because of different OS ver-
sions and libraries [31]. Other researchers studied the evasive
techniques implemented by malware authors to ensure that
traces collected in a sandbox environment are not represen-
tative of its behavior in the real world [6, 20]. Rossow et al.
USENIX Association
30th USENIX Security Symposium    3487
reported how downloader behaviors change over time, owing
to time bombs or new instructions received from the com-
mand and control (C&C) channel [42]. These prior studies
have conﬁrmed the existence of per-sample behavior variabil-
ity and showed its potential impact. However, because they
were conducted in experimental infrastructures, they did not
reveal the prevalence of this variability in the wild, or which
components of the sample’s behavior are most likely to vary.
How much, and in what ways, the behavior of benign pro-
grams varies in the wild are also open questions. The prior
research has also showed that the effectiveness of malware-
detection models degrades over time, as new samples exhibit
previously unseen behaviors [19, 38, 47]. Previously unseen
behaviors of the samples already covered by the model may
similarly degrade the detection performance, but this effect
has not been quantiﬁed before.
In this paper we conduct the ﬁrst study to understand and
measure the variability in the behavior of malware and poten-
tially unwanted programs (PUP) at scale. We focus on API-
and system-call based behavioral proﬁles, and we conduct
a quantitative analysis of per-sample behavioral differences
on end hosts. To this end, we use a unique dataset of 7.6M
execution traces, recorded in 5.4M real Windows hosts from
113 countries. At the time when the data was collected, it was
not known whether the samples were benign or malicious.
The samples were executed by the users, who interacted with
them naturally, and the behavioral monitoring and analysis
was employed as a last line of defense against unwanted be-
haviors.
We measure the variability in the behavior of samples later
determined to be malware and PUPs, and we compare it to
a baseline we draw from the benign samples. Across execu-
tions recorded on different hosts we found that the number
of actions performed (e.g. the creation of a new ﬁle or the
modiﬁcation of a registry key) varies 6× more for malware
than for benign samples, and this difference increases to 15×
when looking at the number of created ﬁles. In contrast, dif-
ferent executions recorded weeks apart on the same host do
not show such a high range of action variability. When con-
sidering action parameters, (e.g. ﬁle names), we observe little
to no variability across time for benign samples (the action
parameters tend to remain constant on the same machine), and
a very large variability for malicious samples (the intersection
of the common values is almost empty).
We further assess the challenges for identifying the invari-
ant parts of per-sample behaviors, which have implications
for building behavioral rule-based detection signatures, and
for clustering samples into malware families. We show that,
when building rules that use actions and tokenized parameters,
the information collected from a single execution is inconclu-
sive, but it is possible to observe most of the behaviors from
a few traces. For instance, ﬁle names extracted from three
different hosts cover, on average, 90% of the executions and
using more than four traces provides diminishing returns. We
also show that, when performing a malware clustering experi-
ment, one third of the samples exhibit sufﬁcient variability in
behavior that their traces appear in multiple clusters. As this
would not be observed when using a single trace per sample,
our result suggests that the accuracy of mapping samples to
the correct family, through clustering, is lower than previously
believed.
These ﬁndings emphasizes that real malware behavior de-
pends on a subtle interplay of factors, such as environments,
time, and user interactions, which cannot be observed by exe-
cuting the sample once in a sandbox environment. We discuss
the actionable implications of our results and the alternatives
to account for behavioral variability. More importantly, these
results emphasize the unique insights that we can gain by mon-
itoring malware behavior at scale, on real hosts. Importantly,
such monitoring can be performed ethically by anti-virus sys-
tems. This radical shift from the way behavioral analysis is
conducted today may bring a degree of external validity that
sandboxes cannot provide.
In summary, we make three contributions:
• We analyze program behavior at scale, using 7.6M call
traces recorded in 5.4M real hosts. These traces include
natural user interactions with the programs and have high
external validity compared to the prior work.
• We study how the behavior of individual samples
changes across hosts and time, and we compare the vari-
ability of Windows malware, PUP, and benign programs
at multiple granularities.
• We analyze the invariant parts of the malware behav-
iors, and we show how this impacts a common class of
behavioral rules for malware detection.
2 Problem and Methodology
The main goal of malware analysis is to identify and charac-
terize the behavior of unknown samples such that behavioral
indicators that are speciﬁc to a malware family could be used
for malware detection or classiﬁcation. Because the behavior
of executables could vary depending on when, where and at
what setting it is executed, part of the behavior for any given
program is transient in nature.
In our dataset, we observed that some executions of the
Ramnit worm [39], result in the creation of a large number
of mutexes. The reason is that the worm uses a privilege es-
calation exploit, which creates a lot of mutexes, only if it is
executed in user-mode on a vulnerable version of Windows 7.
If instead Ramnit is executed with admin privileges or within
a different Windows version, the malware would not perform
the exploit. If an analyst, or an automated system, created a
signature by looking at the behavior collected on Windows
7 (a popular choice by many malware analysis sandboxes),
3488    30th USENIX Security Symposium
USENIX Association
those mutex creations could be used for constructing the sig-
nature. However, these actions would only appear in a fraction
of end user machines, thus resulting in a poor detection cov-
erage.
To mitigate this problem and identify truly invariant parts
of malware behavior, it is important to collect malware exe-
cutions across multiple machines, as suggested by Rossow et
al. [43] and over time, as suggested by Pendlebury et al. [38].
However, prior works does not make concrete recommenda-
tions for the most optimal set up (e.g, the optimal re-execution
interval, the number of different machines, the number of
different OSes, etc.) that allows those invariant parts to be
identiﬁed accurately. Our goal is to ﬁll this gap in the state-
of-the-art.
Despite these very time-consuming therefore costly sug-
gestions, the industry practitioners often choose to aggregate
behavior of different samples of a family for signature gen-
eration [11, 24]. However, the majority of malicious samples
cannot be mapped to a known family (malware with generic
labels are 1.3 times more common than those that belong to
a well-deﬁned family) [27], making it impossible to perform
such an aggregation.
To shed light on the magnitude of this problem, we ana-
lyze 7.6M executions out of which 3.1M belong to malicious
and unwanted programs and the rest to benign. In total, the
executions of each sample span at least 10 machines, while
45% appear at least 1 week from the sample’s ﬁrst appearance.
This measurement, the ﬁrst of its kind, allows us to assess the
amount of behavior variability in the wild, and to study the
minimum number of experiments required to rule out tran-
sient behaviors and derive signatures that achieve the highest
coverage on end hosts, ﬁlling a crucial gap in the state-of-
knowledge about the most optimal execution conﬁgurations
for signature generation.
2.1 Measuring Variability
We describe the behavior of a sample through its interactions
with the host Operating System. Because a semantic inter-
action, such creating a new ﬁle or spawning an OS process,
may be accomplished with various system or API calls, and
the calls differ across OS versions, we abstract these inter-
actions as actions. Our actions model high-level operations,
such as process injection, ﬁle creation, or the modiﬁcation
of a registry key; we report all the action types analyzed in
Section 3. An action may have one or several parameters
to specify the target that the action is operating on (e.g. the
registry key being modiﬁed), as well as the actual value it
writes or modiﬁes (e.g. the value written in the registry). An
execution trace for a sample consists of a sequence of actions
and the corresponding parameters. The traces captured by
malware detectors based on both system calls [10, 34] and na-
tive API calls [6, 7, 11, 18, 21] can be mapped to action-based
execution traces.
We measure variability at two levels of granularity. First,
we count the actions in an execution trace and compute the
action variability. We maintain separate counts for each ac-
tion type, as well as for all the actions taken together. We
then compare these counts across all the execution traces of
a sample, using several measures of variability as described
below. This provides a conservative assessment of variability,
indicating for example when a sample creates one ﬁle on a
host and two on another. We report how much action variabil-
ity we observe, which action types account for most of the
variability, and how these the variability changes across space
(a sample executing on different hosts in the same week) and
time (a sample executing on the same host in different weeks).
We also compare the action variability in malware, PUP, and
benign samples.
Second, we compare the action parameters coming from
different execution traces of the same sample, using measures
of set similarity. This parameter variability allows us to iden-
tify differences among executions when the number of actions
remain the same, for instance when a sample creates a ﬁle
with different names on each host. This comparison provides
further insight into the semantics of the variable actions; for
instance, we identify which parameter parts (e.g., the ﬁlename
vs the directory path) differ among different executions.
Measuring action variability. The action counts coming
from different execution traces of a sample form an empiri-
cal distribution. We can characterize this distribution using
various measures of location (e.g. mean, median, mode) and
spread (e.g. variance, standard deviation, median absolute
deviation, interquartile range); we are interested in the lat-
ter when assessing action variability. The main challenge in
selecting a statistical measure of spread is to avoid drawing
incorrect conclusions because of outliers in the distribution.
We illustrate this challenge by showing how different vari-
ability measures perform over the executions of one sample
of AutoPico, a Windows piracy software. Usually the sample
creates four ﬁles when executed: two log ﬁles, one dll and
one .sys ﬁle. However, in six traces out of 62, AutoPico only
dropped the two log ﬁles (because the samples was unable to
execute correctly), and in four traces it created more than 15
times the same log ﬁles in the same location (possibly due
to the fact that the sample modiﬁed more registry keys, each
time recreating the log ﬁle from scratch). The ordered list of
the number of ﬁle creation events for all executions in our
dataset looks like the following:
[2,2,2,2,2,2,4,4,4, . . . ,4,17,19,19,20]
The Interquartile Range (IQR) and the median absolute
deviation (MAD) are measures of spread that are robust to
outliers. Unlike the classic standard deviation, these measures
are not affected by measurement values that are either too
low or too high. For this reason, IQR and MAD are widely
used in other experimental ﬁelds [26,30]. In our study, a trace
USENIX Association
30th USENIX Security Symposium    3489
may exhibit an atypical number of actions owing to the mal-
functioning of the sample, because of the lack of a required
component or because the host was shut down mid-execution.
High action counts may also occur when the malware was
designed to infect all of the ﬁles in a directory and it encoun-
ters a few machines with an unusually large number of ﬁles,1
which results in outliers for the number of ﬁle actions. The
IQR is the difference between the 75th and 25th percentile
values of the action-count distribution. In the AutoPico ex-
ample, the IQR is 0, as it is the difference of the value in the
47th position (a 4 in our example) and that in the 16th position
(again a 4) in the ordered list of 62 values. The MAD is the
median of the absolute values of each count’s deviation from
the median. In the example the MAD is 0 as well, because,
after subtracting 4 (the median) from each count, we get a
vector where 0 is repeated 52 times and there are only 10
non-zero values, and the median of this vector is 0. In con-
trast, the standard deviation for the AutoPico traces is 3.67,
which inﬂates the action variability that would be reported.
Moreover, the variability would be heavily inﬂuenced by the
four large outliers (17, 19, 19, 20): without them, the standard
deviation of the action counts would drop 0.61, while the IQR
and MAD would remain 0. This suggests that robust measures
of spread, such as the IQR and MAD, are not likely to lead to
conclusions biased by artifacts in the data.
At the same time, the tail of the distribution may also pro-
vide meaningful insights, e.g. when it reﬂects the behavior
of targeted malware. We therefore select two additional mea-
sures, the 90-10 and 99-1 percentile ranges, because they are
analogous to the IQR but are gradually less conservative in
discarding the distribution tails. In the AutoPico example,
the 90-10 and 99-1 percentile ranges are 0 and 17 (19-2) re-
spectively. In our analysis, we compute the MAD, and the
75–25 (IQR), 90–10 and 99– 1 percentile ranges. We report
one representative measure when the results are similar, and
we discuss when we observe differences among the four mea-
sures.
Measuring parameter variability. We measure variability
on parameters for each action type separately. We then com-
pute the Jaccard index, which is a popular choice to measure
the distance between the parameters observed in two malware
executions [22, 31], on the parameters observed in different
executions of each sample. This way it is possible to iden-