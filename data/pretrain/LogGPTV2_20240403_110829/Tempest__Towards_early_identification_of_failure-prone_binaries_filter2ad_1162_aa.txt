# Tempest: Towards Early Identification of Failure-Prone Binaries

**Authors:**
- Thirumalesh Bhat<sup>1</sup>
- Nachiappan Nagappan<sup>2</sup>

<sup>1</sup>Center for Software Excellence, Windows Division, Microsoft Corporation  
<sup>2</sup>Microsoft Research  
{thirub|nachin}@microsoft.com

## Abstract
Early estimates of failure-proneness can inform decisions on testing, refactoring, and design rework. These estimates often rely on code metrics such as churn and complexity. However, such quality assessments rarely find their way into mainstream tools or industrial deployment. This paper introduces the Tempest tool, which uses statistical models based on code complexity and churn metrics across the Microsoft Windows codebase to identify failure-prone binaries early in the development process. We also present the tool's architecture and its current usage at Microsoft.

## 1. Introduction
Recent empirical work has focused on using software metrics like code churn, code complexity, and inspection defects to predict software quality. While these studies provide efficient quality estimates, they often lack the tool support needed for large-scale deployment in software development environments. Tool support for obtaining statistical estimates of failure-proneness can help organizations make data-driven decisions about system reliability.

To address this gap, the development of the Tempest tool was undertaken at Microsoft Corporation. This paper presents our experiences and aims to encourage other industrial and academic partners to invest in building tool support for predicting software quality.

Tempest is a joint development project between the Center for Software Excellence, Windows division, and the Software Reliability Research group at Microsoft Research. The high-level goals of the Tempest project are twofold:
- Identify and collect early indicators of code quality to build statistical prediction models.
- Leverage these statistical models to develop a tool that can be deployed to developers and integrated into the development process to identify failure-prone binaries at both the developer and system levels.

Failure-proneness is defined as the probability that a particular software element will fail in operation. Higher failure-proneness indicates lower reliability and quality of the software produced, and vice versa.

The rest of the paper is organized as follows. Section 2 describes the background work and the code complexity and churn metrics used in Tempest. Section 3 discusses the key concepts in Tempest, and Section 4 provides the architecture of Tempest. We conclude in Section 5 with a general discussion of Tempest and its integration into the development process at Microsoft.

## 2. Background Work
Structural object-oriented (O-O) measurements, such as those defined in the Chidamber-Kemerer (CK) [5] and MOOD [4] O-O metric suites, are used to evaluate and predict software quality [8]. There is growing empirical evidence supporting the use of internal complexity metrics [1, 3, 6, 15, 19] and churn metrics [7, 9, 13, 16] as predictors of code quality. Our motivation for developing Tempest is based on three studies [2, 11, 12], which explain our research results and case studies on the selected metrics: relative code churn measures and complexity metrics to predict failures.

This paper discusses the implementation of a tool that leverages the research from these three studies. In software engineering research, there has been limited work in producing tools to predict failures. To our knowledge, the first tool to use past 'risk' as an indicator of future 'risk' is the HATARI tool from Saarland University [17], which is integrated into the IBM Eclipse development environment. Although earlier studies discuss predicting software quality, none of them detail the development or deployment of a tool.

We now summarize the research from the three studies [2, 11, 12] that form the basis of the Tempest tool.

### Using Code Churn [11]
In our prior studies, we investigated the use of a set of relative code churn measures as predictors of software defect density. The relative churn measures are normalized values obtained during the churn process. Based on the Windows Server 2003 codebase, we observed that the set of relative code churn measures can discriminate between fault-prone and non-fault-prone binaries with 89% accuracy. Figure 1 shows the predicted plot of actual versus estimated defect density using the relative code churn measures for Windows Server 2003. The strong correlations between the actual and estimated values indicate the ability of the relative code churn measures to act as early indicators of code quality.

![Figure 1: Actual vs. Estimated defect density [11]](path_to_figure_1)

### Using Code Complexity Metrics [2, 12]
In five Microsoft projects, we identified complexity metrics that predict post-release failures and reported how to systematically build predictors for post-release failures from historical data. We performed a case study using two versions of the Windows operating system: Windows XP-SP1 and Windows Server 2003, as shown in Figure 2. The overall code base of the analyzed Windows XP-SP1 system was 6.1 MLOC, and Windows Server 2003 was around 11.5 MLOC. Correlating the estimated failure-proneness (obtained by using a model built from Windows XP-SP1 data) with the actual field failures of Windows Server 2003, we observed statistically significant correlation values, indicating the efficacy of the statistical models built using Windows XP-SP1 data.

![Figure 2: Time line for data collection [2]](path_to_figure_2)

## 3. Basic Concepts in Tempest
The three studies [2, 11, 12] serve as the background for the development of Tempest, incorporating relative code churn and complexity metrics. This paper primarily focuses on the implementation, use, and operation of the Tempest tool based on the key research findings from [2, 11, 12]. The two key features of the Tempest tool are:
- Predicting the failure-proneness of binaries.
- Providing feedback on code complexity metrics based on historical data.

Based on the earlier studies [2, 11, 12], we use relative code churn and code complexity metrics to build our prediction models. A detailed discussion of the choice of these metrics, their prediction efficacy, and their validation and use in prior projects is provided in the earlier research papers [2, 11, 12] and is beyond the scope of this paper due to page restrictions.

We use a statistical regression model (Equation 1) that fits the relative code churn measures (shown in Figure 3, where M1 represents churned blocks/total blocks, M2 represents deleted blocks/total blocks, and M3 represents churned blocks/deleted blocks) and the code complexity metrics (different metrics are used depending on whether the binary is O-O or non-O-O) with field failures from a prior version of Windows to build the prediction model. Using this model, failure-proneness estimates are made for the current version of Windows binaries.

![Figure 3: Relative code churn measure relationships [11]](path_to_figure_3)

\[ \text{Failure-proneness} = \text{Function} (\text{Relative code churn and code complexity metrics}) \]

Using binaries that do not have any failures in a prior release, we define cut-off standards for the complexity metric values to identify trends that lead to poor quality code (Equation 2).

\[ \text{LL}(\text{METRIC}_x) = \mu_{\text{METRIC}_x} - z_{\alpha/2} \cdot \frac{\text{S.D.}_{\text{METRIC}_x}}{\sqrt{n}} \]

where \(\mu_{\text{METRIC}_x}\) is the mean of Metric x (e.g., cyclomatic complexity, depth of inheritance, coupling between objects, fan-in, fan-out, etc.), \(n\) is the number of samples used to calculate \(\mu_{\text{METRIC}_x}\), \(\text{S.D.}_{\text{METRIC}_x}\) is the standard deviation of Metric x, and \(z_{\alpha/2}\) is the upper \(\alpha/2\) quantile of the standard normal distribution.

To define acceptable standards for the complexity metrics, the mean of the historical values for each metric serves as the upper limit. The mean and the statistical lower confidence limit serve as feedback standards for the complexity metrics to guide refactoring and design rework efforts. Using the computed values, we determine the color with which to code the metric, as shown in Table 1. Using the mean as the upper bound indicates a more conservative approach to identifying binaries for refactoring. The rationale behind this approach is to minimize the extent of complexity growth in binaries, i.e., a reduced upper bound value helps in identifying binaries before they reach very high complexity, making them very difficult to refactor.

### Example
For example, the binaries in Windows XP that did not have failures are used to compute the complexity metric. If the computed cyclomatic complexity lower bound (using Equation 2) is 30 and the mean is 45, then for a binary under development, if the cyclomatic complexity is 50, the feedback is displayed in red; if between 30 and 45, in yellow; and if below 30, in green. Color coding has been used in prior software engineering research to categorize software components into different classes [14].

Table 1: Color-coded feedback interpretation

| Color | Interpretation |
|-------|----------------|
| GREEN | METRIC_x < LL  |
| YELLOW | LL ≤ METRIC_x < µ_METRIC_x |
| RED | METRIC_x ≥ µ_METRIC_x |

To empirically assess the efficacy of the feedback standards built into Tempest, we performed a case study using data from Windows XP-SP1 and Windows Server 2003. The feedback standards for all O-O and non-O-O metrics were defined using Equation 2 on the data obtained from binaries that did not have any failures for Windows XP-SP1. We then computed the color-coded feedback for the O-O and non-O-O metrics for Windows Server 2003. For each binary in Windows Server 2003, we computed the number of green, yellow, and red feedbacks. We then correlated the color-coded feedback with the actual post-release field failures. The results of the correlation are presented in Table 2.

Table 2: Color-coded feedback results

| Color | Number of Feedbacks |
|-------|---------------------|
| GREEN | ...                 |
| YELLOW | ...                |
| RED | ...                |

## 4. Architecture of Tempest
[Detailed description of the architecture of the Tempest tool, including its components, data flow, and integration with the development process.]

## 5. Conclusion
[General discussion of Tempest, its integration into the development process at Microsoft, and future directions for the tool and related research.]

---

**Note:** Replace `path_to_figure_1`, `path_to_figure_2`, and `path_to_figure_3` with the actual paths to the figures.