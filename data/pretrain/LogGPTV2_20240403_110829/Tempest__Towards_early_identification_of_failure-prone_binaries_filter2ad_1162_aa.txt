title:Tempest: Towards early identification of failure-prone binaries
author:Thirumalesh Bhat and
Nachiappan Nagappan
Tempest: Towards Early Identification of Failure-prone Binaries  
Thirumalesh Bhat1, Nachiappan Nagappan2  
1Center for Software Excellence, Windows Division, Microsoft Corporation 
2Microsoft Research 
{thirub|nachin}@microsoft.com 
Abstract 
Early  estimates  of  failure-proneness  can  be  used  to 
help  inform  decisions  on  testing,  refactoring,  design 
rework etc. Often such early estimates are based on code 
metrics  like  churn  and  complexity.  But  such  estimates  of 
software quality rarely make their way into a mainstream 
tool  and  find  industrial  deployment.  In  this  paper  we 
discuss  about  the  Tempest  tool  that  uses  statistical 
failure-proneness  models  based  on  code  complexity  and 
churn metrics across the Microsoft Windows code base to 
identify  failure-prone  binaries  early  in  the  development 
process.  We  also  present  the  tool  architecture  and  its 
usage as of date at Microsoft. 
1. Introduction  
In  recent  times  an  important  area  of  empirical  work 
has  been  focused  on  using  software  metrics  like  code 
churn, code complexity, inspection defects etc. to predict 
software quality. Such studies make efficient estimates of 
software  quality  but  lack  tool  support  to  scale  and  be 
deployed  in  large  software  development  environments. 
Having  tool  support  for  obtaining  statistical  estimates  of 
failure-proneness  can  help  software  organizations  make 
data  driven  decisions  on  the  overall  reliability  of  the 
system. In order to address this problem the development 
of 
tool  was  undertaken  at  Microsoft 
Corporation.  By  means  of  this  paper  we  present  our 
experiences  and  hope  to  encourage  other  industrial 
organizations and academic partners to invest in building 
tool support for predicting software quality. 
the  Tempest 
Tempest  is  a  joint  development  project  between  the 
Center  for  Software  Excellence,  Windows  division  and 
the  Software  Reliability  Research  group  at  Microsoft 
Research.  The  high  level  goals  of  the  Tempest  project 
was two fold, 
• 
that  are  early 
Identify  and  collect  metrics 
indicators  of  code  quality  to  build  statistical 
prediction models; 
•  Leverage  these  statistical  models  to  build  a  tool 
that can be deployed to developers and one that 
can  be  integrated  into  the  development  process 
to  identify  failure-prone  binaries  (both  at  the 
developer and the system level).  
We define failure-proneness as the probability that a 
particular  software  element  will  fail  in  operation.    The 
higher the failure-proneness of the software, the lower the 
reliability  and  the  quality  of  the  software  produced,  and 
vice-versa. 
The rest of the paper is organized as follows. Section 
2 describes the background work and the code complexity 
and  churn  metrics  used  in  Tempest.  Section  3  discusses 
the  key  concepts  in  Tempest  and  Section  4  provides  the 
architecture of Tempest. We conclude in Section 5 with a 
general discussion of Tempest and its integration into the 
development process at Microsoft. 
2. Background work 
Structural  O-O  measurements,  such  as  those  defined 
in the Chidamber-Kemerer (CK) [5] and MOOD [4] O-O 
metric  suites,  are  being  used  to  evaluate  and  predict  the 
quality  of  software  [8].  There  is  a  growing  body  of 
empirical evidence that supports the use of these internal 
complexity metrics [1, 3, 6, 15, 19] and churn metrics [7, 
9,  13,  16]  as  predictors  of  code  quality.  Our  motivation 
for the development of Tempest is based on three studies 
[2,  11,  12].  These  three  studies  explain  our  research 
results and case studies on the metrics that were selected, 
namely  relative  code  churn  measures  and  complexity 
metrics  to  predict  failures.  This  paper  discusses  the 
implementation  of  a  tool  that  leverages  the  research  in 
these  three  studies.  In  software  engineering  research, 
from  the  tool  perspective  there  has  been  limited  work  in 
producing  tools  to  predict  failures.  To  the  best  of  our 
knowledge the  first tool to use past ‘risk’ as an indicator 
of future ‘risk’ is the recently released HATARI tool from 
Saarland University [17] which is integrated as part of the 
IBM  Eclipse  development  environment.  Though  the 
earlier studies discuss predicting software quality none of 
the papers discuss the development/deployment of a tool. 
We  now  summarize  the  research  in  the  three  studies  [2, 
11, 12] that dorm the basis of the Tempest tool. 
Using  code  churn  [11]:  In  our  prior  studies  we 
investigated  the  use  of  a  set  of  relative  code  churn 
measures  in  isolation  as  predictors  of  software  defect 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:00 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE116DSN 2008: Bhat & Nagappandensity.  The  relative  churn  measures  are  normalized 
values of the various measures obtained during the churn 
process.  Based  on  the  Windows  Server  2003  code  base 
we  observe  that  the  set  of  relative  code  churn  measures 
are  efficient  predictors  of  defect  density 
that  can 
discriminate  between  fault-prone  and  not  fault-prone 
binaries  with  an  accuracy  of  89%.  Figure  1  shows  the 
predicted  plot  of  the  actual  versus  the  estimated  defect 
density  predicted  using  the  relative  code  churn  measures 
for  Windows  Server  2003.  The  strong  correlations 
between  the  actual  and  estimated  values  further  indicate 
the  ability  of  the  relative  code  churn  measures  to  act  as 
early indicators of code quality. 
Figure 1: Actual vs. Estimated defect density [11] 
Using code complexity metrics [2, 12]: In five Microsoft 
projects,  we  identified  complexity  metrics  that  predict 
post-release  failures  and  reported  how  to  systematically 
build  predictors  for  post-release  failures  from  history. 
Further  we  performed  our  case  study  using  two  versions 
of  the  Windows  operating  system  [2],  Windows  XP-SP1 
(Service  pack  1)  and  Windows  Server  2003  as  shown  in 
Figure 2. The overall code base of the analyzed Windows 
XP-SP1  system  was  6.1  MLOC  (Million  lines  of  code) 
and  Windows  Server  2003  was  around  11.5  MLOC. 
Correlating  the  estimated  failure-proneness  (obtained  by 
using  model  built  from  Windows  XP-SP1  on 
the 
Windows  Sever  2003  data)  with  the  actual  field  failures 
of  Windows  Server  2003  we  observe  the  statistically 
significant correlation values that indicate the efficacy of 
the statistical models built using Windows XP-SP1 data. 
3. Basic concepts in Tempest 
The  previously  discussed  three  studies  [2,  11,  12] 
serve as the background for the development of Tempest 
incorporating  the  relative    code  churn  and  complexity 
metrics. This paper is primarily a tool paper that discusses 
the implementation/use and operation of the Tempest tool 
based the key research findings from  [2, 11, 12]. The two 
key features in the Tempest tool involve, 
(i) 
(ii) 
predicting  failure-proneness  of  the  binaries 
and 
providing  feedback  on  the  code  complexity 
metrics based on historical data.  
Based  on  the  earlier  studies  [2,  11,  12]  we  use  the 
relative code churn and code complexity metrics to build 
our prediction models. A detailed discussion of the choice 
of  these  metrics;  their  prediction  efficacy  and  their 
validation  and  use  in  prior  projects  is  discussed  in  the 
earlier research papers [2, 11, 12] and is beyond the scope 
of this paper due to page restrictions.  
We  use  a  statistical  regression  model  as  shown  in 
Equation  1  that  fits  the  relative  code  churn  measures 
(shown  in  Figure  3  where  M1  represents  the  churned 
blocks/total  blocks;  M2 
the  deleted 
blocks/total  blocks  and  M3  the  churned  blocks/deleted 
blocks)  and  the  code  complexity  metrics  (Different 
ratio  of 
the 
Figure 2: Time line for data collection[2]
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:00 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE117DSN 2008: Bhat & Nagappanmetrics  are  used  depending  on  whether  the  binary  is 
OO or non-OO) with field failures from a prior version 
of  Windows  to  build  the  prediction  model.  Using  this 
model  failure-proneness  estimates  are  made  of  the 
current version of Windows binaries. 
M3 
M1 
M2 
Figure 3: Relative code churn measure 
relationships [11] 
Failure-proneness  =  Function  (Relative  code 
churn and code complexity metrics)  
Using binaries that do not have any  failures in a prior 
release  we  use  Equation  2  to  define  cut-off  standards 
for the complexity metric values to identify trends that 
lead to poor quality code.  
 (1) 
LL
(
METRICx
)
=
µ
METRICx
−
z
2/α
.
DS
METRICx
.
          (2) 
n
/2 
standard deviation of metric x; and Z
quantile of the standard normal distribution.  
complexity,  depth  of  inheritance,  coupling  between 
objects,  fan-in,  fan-out  etc.);  n  is  the  number  of 
In  order  to  define  the  acceptable  standards  for  the 
complexity  metrics  the  mean  of  the  historical  values 
where   METRICx  is  the  Mean  of  Metric  x  (cyclomatic 
samples  used  to  calculate   METRICx;  S.D.SMx  is  the 
α /2 is the upper 
α
for  each  metric  (  METRIC(x))  serves  as  the  upper  limit. 
The  mean  and  the  statistical  lower  confidence  limit 
serve  as  the  feedback  standards  for  the  complexity 
metrics in order to guide refactoring and design rework 
effort.  Using  the  computed  values,  we  determine  the 
color with which to code the metric, as shown in Table 
1. Using the mean as the upper bound indicates that we 
are  following  a  more  conservative  approach  towards 
identifying  the  binaries  for  refactoring.  The  rationale 
behind  this  approach  is  to  minimize  the  extent  of 
complexity  growth  in  binaries,  i.e.  a  reduced  upper 
bound  value  would  help  in  identifying  binaries  before 
they  reach  very  high  complexity  which  makes  them 
very difficult for refactoring.   
feedback  standards.  Let  us  assume 
For  example  the  binaries  in  Windows  XP  that  did 
not  have  failures  are  used  to  compute  the  complexity 
metric 
the 
computed  cyclomatic  complexity  lower  bound  (from 
using  Equation  2)  is  30  and  the  mean  is  45.  If  for  a 
binary  under  development  the  cyclomatic  complexity 
is 50 the feedback is displayed in red color, if between 
30  and  45  in  yellow  and  if  below  30  in  green.  Color 
coding  has  been  used  in  prior  software  engineering 
research 
into 
different classes [14]. 
to  categorize  software  components 
METRICx  refers  to  the  value  of  each  particular 
complexity  metric  for  the  Windows  system  under 
development.    This  value  is  compared  with  the  LL  as 
computed  by  Equation  2  and  with  the  average  value 
Table 1: Color coded feedback interpretation 
GREEN 
YELLOW 
Color 
(  METRICx).    
RED 
Interpretation 
METRICx  µMETRICx 
In  order  to  empirically  assess  the  efficacy  of  the 
feedback  standards  built  in  Tempest  we  performed  a 
case  study  using  data  from  Windows  XP-SP1  and 
Windows Server 2003. The feedback standards for all 
the  OO  and  non-OO  metrics  were  defined  using 
Equation 2 on the data obtained from binaries that did 
not  have  any  failures  for  Windows  XP-SP1.  We  then 
computed  the  color-coded  feedback  for  the  OO  and 
non-OO  metrics  for  Windows  Server  2003.  For  each 
binary  in  Windows  Server  2003  we  computed  the 
number  of  green,  yellow  and  red  feedbacks.  We  then 
correlated the color code feedback with the actual post-
release field failures. The results of the correlation are 
presented in Table 2.  
Table 2: Color coded feedback results 
GREEN  YELLOW 
RED 