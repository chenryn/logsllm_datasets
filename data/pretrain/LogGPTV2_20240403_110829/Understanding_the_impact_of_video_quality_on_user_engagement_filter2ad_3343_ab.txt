the user is likely to give a positive recommendation are subjective
and hard to quantify. Second, these metrics can be translated into
providers’ business objectives. Direct revenue objectives include
number of advertisement impressions watched and recurring sub-
scription to the service. The above engagement metrics ﬁt well
with these objectives. For example, play time is directly associated
with the number (and thus revenue) of ad impressions. Addition-
ally, user satisfaction with content quality is reﬂected in the play
time. Similarly, viewer-level metrics can be projected to ad-driven
and recurring subscription models.
2.3 Quality Metrics
In our study, we use ﬁve industry-standard video quality met-
rics [3]. We summarize these below.
1. Join time (JoinTime): Measured in seconds, this metric rep-
resents the duration from the player initiates a connection to a
video server till the time sufﬁcient player video buffer has ﬁlled
up and the player starts rendering video frames (i.e., moves to
playing state). In Figure 1, join time is the duration of the join-
ing state.
2. Buffering ratio (BufRatio): Represented as a percentage, this
metric is the fraction of the total session time (i.e., playing plus
buffering time) spent in buffering. This is an aggregate metric
that can capture periods of long video “freeze” observed by the
user. As illustrated in Figure 1, the player goes into a buffering
state when the video buffer becomes empty and moves out of
buffering (back to playing state) when the buffer is replenished.
3. Rate of buffering events (RateBuf ): BufRatio does not cap-
ture the frequency of induced interruptions observed by the
user. For example, a video session that experiences “video stut-
tering” where each interruption is small but the total number of
interruptions is high, might not have a high buffering ratio, but
may be just as annoying to a user. Thus, we use the rate of
session duration .
buffering events #buﬀer events
4. Average bitrate (AvgBitrate): A single video session can
have multiple bitrates played if the video player can switch
between different bitrate streams. Average bitrate, measured
in kilobits per second, is the average of the bitrates played
weighted by the duration each bit rate is played.
5. Rendering quality (RendQual): Rendering rate (frames per
second) is central to user’s visual perception. Rendering rate
may drop due to several reasons. For example, the video player
Dataset
LiveA
LiveB
LvodA
LvodB
SvodA
SvodB
LiveH
# videos
107
194
115
87
43
53
3
# viewers (100K)
4.5
0.8
8.2
4.9
4.3
1.9
29
Table 1: Summary of the datasets in our study. We select videos
with at least 1000 views over a one week period.
may drop frames to keep up with the stream if the CPU is over-
loaded. Rendering rate may drop due to network congestion if
the buffer becomes empty (causing rendering rate to become
zero). Note that most Internet video streaming uses TCP (e.g.,
RTMP, HTTP chunk streaming). Thus, network packet loss
does not directly cause a frame drop. Rather, it could deplete
the client buffer due to reduced throughput. To normalize ren-
dering performance across videos, which may have different
encoded frame rates, we deﬁne rendering quality as the ratio
of the rendered frames per second to the encoded frames per
second of the stream played.
Why we do not report rate of bitrate switching? In this paper,
we avoid reporting the impact of bitrate switching for two reasons.
First, in our measurements we found that the majority of sessions
have either 0, 1, or 2 bitrate switches. Now, such a small discrete
range of values introduces a spurious relationship between engage-
ment (play time) and the rate of switching.1 That is, the rate of
switches is ≈
PlayTime . This introduces an ar-
tiﬁcial dependency between the variables! Second, only two of
our datasets report the rates of bitrate switching; we want to avoid
reaching general conclusions from the speciﬁc bitrate adaptation
algorithms they use.
2.4 Dataset
PlayTime or ≈
1
2
We collect close to four terabytes of data each week. On aver-
age, one week of our data captures measurements over 300 mil-
lion views watched by about 100 million unique viewers across
all of our afﬁliate content providers. The analysis in this paper is
based the data collected from ﬁve of our afﬁliates during the fall
of 2010. These providers serve a large volume of video content
and consistently appear in the Top-500 sites in overall popularity
rankings [1]. Thus, these are representative of a signiﬁcant volume
of Internet video trafﬁc. We organize the data into three content
types. Within each content type we use a pair of datasets, each
corresponding to a different provider. We choose diverse providers
in order to eliminate any biases induced by the particular provider
or the player-speciﬁc optimizations and algorithms they use. For
live content, we use additional data from the largest live Internet
video streaming sports event of 2010: the FIFA World Cup. Ta-
ble 1 summarizes the total number of unique videos and views for
each dataset, described below. To ensure that our analysis is sta-
tistically meaningful, we only select videos that have at least 1000
views over the week-long period.
• Long VoD: Long VoD clips have video length of at least 35
minutes and at most 60 minutes. They are often full episodes
of TV shows. The two long VoD datasets are labeled as LvodA
and LvodB.
• Short VoD: We categorize video clips as short VoD if the video
length is at least 2 and at most 5 minutes. These are often
1This discretization effect does not occur with RateBuf .
(a) Join time
(b) Buffering ratio
(c) Average bitrate
(d) Rendering quality
Figure 2: CDFs for four quality metrics for dataset LvodA.
(a) Buffering ratio
(b) Rate of buffering events
(c) Average bitrate
(d) Rendering quality
Figure 3: Qualitative relationships between four quality metrics and the play time for a video from LvodA.
trailers, short interviews, and short skits. The two short VoD
datasets are labeled as SvodA and SvodB.
• Live: Sports events and news feeds are typically delivered as
live video streams. There are two key differences between the
VoD-type content and live streams. First, the client buffers in
this case are sized such that the viewer does not lag more than
a few seconds behind the video source. Second, all viewers
are roughly synchronized in time. The two live datasets are
labeled LiveA and LiveB. As a special case study, dataset
LiveH corresponds to the three of the ﬁnal World Cup games
with almost a million viewers per game on average (1.2 million
viewers for the last game from this dataset).
3. ANALYSIS TECHNIQUES
In this section, we begin with real-world measurements to mo-
tivate the types of questions we want to answer and explain our
analysis methodology toward addressing these questions.
3.1 Overview
To put our work in perspective, Figure 2 shows the cumula-
tive distribution functions (CDF) of four quality metrics for dataset
LvodA. As expected, most viewing sessions experience very good
quality, i.e., have very low BufRatio, low JoinTime, and rela-
tively high RendQual. However, the number of views that suffer
from quality issues is not trivial.
In particular, 7% of views ex-
perience BufRatio larger than 10%, 5% of views have JoinTime
larger than 10s, and 37% of views have RendQual lower than 90%.
Finally, only a relatively small fraction of views receive the highest
bit rate. Given that a non-negligible number of views experience
quality issues, it is critical for content providers to understand if
improving the quality of these sessions could have potentially in-
creased the user engagement.
To understand how the quality could potentially impact the en-
gagement, we consider one video object each from LiveA and
LvodA. For this video, we bin the different sessions based on the
value of the quality metrics and calculate the average play time for
each bin. Figures 3 and 4 show how the four quality metrics inter-
act with the play time. Looking at the trends visually conﬁrms that
quality matters. At the same time, these initial visualizations spark
several questions:
• How do we identify which metrics matter the most?
• Are these quality metrics independent or are they manifesta-
tions of the same underlying phenomenon? In other words,
is the observed relationship between the engagement and the
quality metric M really due to M or due to a hidden relation-
ship between M and another more critical metric M’?
• How do we quantify how important a quality metric is?
• Can we explain the seemingly counter-intuitive behaviors? For
example, RendQual is actually negatively correlated for the
LiveA video (Figure 4(d)), while the AvgBitrate shows an
unexpected non-monotone trend for LvodA (Figure 3(c)).
To address the ﬁrst two questions, we use the well-known con-
cepts of correlation and information gain from the data mining lit-
erature that we describe next. To measure the quantitative impact,
we also use linear regression based models for the most important
metric(s). Finally, we use domain-speciﬁc insights and experiments
in controlled settings to explain the anomalous observations.
3.2 Correlation
The natural approach to quantify the interaction between a pair
of variables is the correlation. Here, we are interested in quanti-
fying the magnitude and direction of the relationship between the
engagement metric and the quality metrics.
To avoid making assumptions about the nature of the relation-
ships between the variables, we choose the Kendall correlation, in-
stead of the Pearson correlation. The Kendall correlation is a rank
correlation that does not make any assumption about the underly-
ing distributions, noise, or the nature of the relationships. (Pearson
correlation assumes that the noise in the data is Gaussian and that
the relationship is roughly linear.)
Given the raw data–a vector of (x,y) values where each x is the
measured quality metric and y the engagement metric (play time or
number of views)–we bin it based on the value of the quality metric.
We choose bin sizes that are appropriate for each quality metric of
interest: for JoinTime, we use 0.5 second intervals, for BufRatio
and RendQual we use 1% bins, for RateBuf we use 0.01/min
0102030405060Jointime(sec)0.00.20.40.60.81.0Fractionofvideos020406080100Bufferingratio(%)0.00.20.40.60.81.0Fractionofvideos4006008001000120014001600Averagebitrate(kbps)0.00.20.40.60.81.0Fractionofvideos020406080100Renderingquality(%)0.00.20.40.60.81.0Fractionofvideos 0 5 10 15 20 25 30 35 40 45 50 0 10 20 30 40 50 60 70 80 90 100Play Time (min)Buffering Ratio (%) 0 5 10 15 20 25 30 35 40 45 50 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2Play Time (min)Rate of Buffering Events (per min) 15 20 25 30 35 40 45 50 400 600 800 1000 1200 1400 1600Play Time (min)Average Bitrate (kbps) 0 5 10 15 20 25 30 35 40 45 50 0 10 20 30 40 50 60 70 80 90 100Play Time (min)Rendering Quality (%)(a) Buffering ratio
(b) Rate of buffering events
(c) Average bitrate
(d) Rendering quality
Figure 4: Qualitative relationships between four quality metrics and the play time for a video from LiveA.
sized bins, and for AvgBitrate we use 20 kbps-sized bins. For
each bin, we compute the empirical mean of the engagement metric
across the sessions/viewers that fall in the bin.
We compute the Kendall correlation between the mean-per-bin
vector and the values of the bin indices. We use this “binned” cor-
relation metric for two reasons. First, we observed that the correla-
tion coefﬁcient2 was biased by a large mass of users that had high
quality but very low play time, possibly because of low user inter-
est. Our primary goal, in this paper, is not to study user interest in
the speciﬁc content. Rather, we want to understand if and how the
quality impacts user engagement. To this end, we look at the aver-
age value for each bin and compute the correlation on the binned
data. The second reason is scale. Computing the rank correlation
is computationally expensive at the scale of analysis we target. The
binned correlation retains the qualitative properties that we want to
highlight with lower compute cost.
3.3
Information Gain
Correlations are useful for quantifying the interaction between
variables when the relationship is roughly monotone (either increas-
ing or decreasing). As Figure 3(c) shows, this may not always be
the case. Further, we want to move beyond the single metric analy-
sis. First, we want to understand if a pair (or a set) of quality metrics
are complementary or if they capture the same effects. As an exam-
ple, consider RendQual in Figure 3; RendQual could reﬂect ei-
ther a network issue or a client-side CPU issue. Because BufRatio
is also correlated with PlayTime, we suspect that RendQual is