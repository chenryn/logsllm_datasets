These two metrics can help us compare triggers between differ-
ent datasets, different techniques and different training settings.
However, these two metrics are difficult to intuitively show the
quality of the trigger. Therefore, to further facilitate the selection of
triggers, we propose a more intuitive metric. As for a good trigger,
we expect that it should have a small ğ¸ value and a relatively small
trigger length. Moreover, if the same ğ¸ is obtained from a dataset
with longer text, it means that this trigger is even more powerful.
Taking trigger effectiveness, trigger length and text length into
account, we formulate a metric called Capability as ğ¶ = 1
ğ¸Â·ğ‘† . Based
on the definition of the ğ‘† value, we can rewrite the Capability as
ğ¶ = ğ‘™ğ‘¥
. In this formula, the longer the text or the lower the ğ¸
ğ¸2Â·ğ‘™ğ‘
value or the shorter the trigger can make the ğ¶ value higher which
better meets our expectation of a good trigger.
When comparing the triggers under the same dataset in the
following experiments, as the text length is fixed, we only provide
the trigger effectiveness.
5 ATTACK PERFORMANCE
In this section, we apply our attack method in real-life scenarios
and evaluate its performance. We first show the performance of our
backdoor attack with respect to different types of triggers, different
datasets, and different fine-tuning tasks. Additionally, we compare
its performance with RIPPLES [15] and NeuBA [48] in Sec. 5.3.
5.1 Performance on Various Types of Triggers
Our objective is to build a universal pre-trained backdoor NLP
model applied to various downstream tasks. Therefore, we con-
sider the possible words or phrases that can be used as triggers
that are not suspicious after being inserted into different kinds of
text. We propose five types of triggers: sophisticated words, names,
books, short tokens, and emoticons. Thus, five backdoor models
are trained. We evaluate five types of triggers from the perspective
of effectiveness and stealthiness with the three new metrics, and
provide insights from the results. Due to the space limit, we put the
results for name, book, and emoticon in Appendix A.
Sophisticated words. In common sense, the frequently used sim-
ple words or phrases are easier to be erased in the fine-tuning
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3146Table 3: The performance of short tokens as triggers.
Trigger
Tokens
vo
ks
ry
zx
vy
uw
pbx
jtk
oqc
average
1
1
1
2
2
2
3
3
3
Amazon
ğ‘†
0.019
0.030
0.020
0.028
0.022
0.010
0.026
0.016
0.033
0.023
ğ¶
26.1
10.5
24.0
11.8
20.5
93.5
16.3
43.7
9.8
28.5
ğ¸
2.02
3.17
2.08
3.02
2.22
1.07
2.36
1.43
3.09
2.15
Twitter
ğ‘†
0.078
0.128
0.088
0.039
0.038
0.024
0.054
0.038
0.048
0.059
ğ¶
5.2
1.8
4.1
15.1
15.9
41.3
10.3
21.1
20.8
15.1
ğ¸
2.45
4.26
2.77
1.70
1.65
1.01
1.80
1.25
1.00
1.98
that they may be affected differently during the fine-tuning process,
while triggers in sophisticated words contain determined meanings.
Other types. From experiments with other types of trigger in Ap-
pendix A and the above experiments, we have the following obser-
vations: (i) meaningful words show consistency across datasets as
shown in sophisticated word, name, and book, whereas meaningless
tokens exist inconsistency as shown in short token and emoticon.
This is because meaningful tokens are learned similarly by both
models; (ii) by empirically examine the triggers in all the five mod-
els, a trigger with ğ¶ value higher than 10 is recommended to attack
real-life models where the justification is defered to Appendix B.
In general, our attack method has successfully injected the prede-
fined triggers into the model. However, the performance of triggers
varies in different settings. Hence, in Sec. 6, we study various factors
that affect attack performance.
5.2 Performance on Multi-class Classification
and Different POR Settings
Previous works can only target one label for multi-class classifica-
tion whereas our method can inject multiple triggers to target at
multiple labels. We now study the performance of our attack on
different POR settings against multi-class classification tasks. For
binary classification task, the triggers either correspond to positive
or negative. However, we have no way of knowing the labels to
which the triggers are mapped for multi-class classification tasks.
Here, we compare two POR settings stated in Sec. 3.4. We randomly
choose nine and eight triggers for POR-1 and POR-2, respectively,
and inject the above two sets of triggers into two models. We repeat-
edly pre-train and fine-tune two models ten times to calculate the
average target label coverage, i.e., the percentage of target labels
that triggers can map to.
Table 4: Different POR settings on multi-class classification
tasks.
Dataset
AGNews
Subjects
YouTube
Class
4
4
9
POR-1
75%
77.5%
45.6%
POR-2
95%
90%
67.8%
We show the results in Table 4, from which we first observe
that POR-2 can cover more target labels than POR-1 in all three
datasets. Moreover, the more the categories, the lower the target
label coverage. However, POR-2 can maintain a higher coverage
rate comparing with POR-1. For example, for AGNews and Subjects,
POR-2 achieves close to 100% coverage, which means all labels can
be mapped by at least one trigger. This indicates we can perform a
targeted attack on any label. This means that our backdoor attack
has achieved a certain degree of targeted attack, even though we
cannot know in advance which POR can be mapped to a certain
label. This result also confirms our previous hypothesis that the
output regions of different classes are more likely to be evenly
distributed in the output space, and sampling POR evenly in the
output space can hit more classes. We also double the number of
triggers with 17 and 16 triggers for POR-1 and POR-2, respectively.
We test their target label coverage on YouTube. The POR-1 and
POR-2 achieve a target label coverage of 58% and 82%, respectively.
Therefore, inserting multiple triggers into the model can effectively
increase the number of target labels to be attacked, thereby making
targeted attacks possible and effective.
5.3 Comparison with RIPPLES and NeuBA
In this section, we compare our method with RIPPLES [15] and
NeuBA [48].
For RIPPLES, we train five backdoor models with the poisoned
SST-2 dataset where the triggers are â€˜cfâ€™, â€˜tqâ€™, â€˜mnâ€™, â€˜bbâ€™ and â€˜mbâ€™ and
each model is inserted with one trigger. We also train five backdoor
models using our method with same settings. The result is shown
in Table 5. Due to the space limit, we put the accuracy for these
models in Appendix C, from which we can see the clean accuracy of
the backdoor models is close to that of the clean model. The result
of RIPPLES under SST-2, Amazon, Yelp, and IMDB shows that the
average ğ¸ value has a gradual increase from 1.00 to 4.03 as the
average text length increases. While our triggerâ€™s ğ¸ value increase
from 1.00 to 1.53. Second, the ğ¸ values of RIPPLES in Twitter (the
abusive behavior detection task) are much higher than the ğ¸ values
in SST-2, though its text length is close to SST-2. By contrary, the
ğ¸ values of our backdoor model in Offenseval and Twitter are all
lower than RIPPLES.
For NeuBA, we compare our backdoor model with its three vari-
ants which include: 1) a reproduced model using NeuBA without
mask token (denoted as [48] w/o mask); 2) a reproduced model
using NeuBA with mask token (denoted as [48] w/ mask); 3) the
backdoor model they uploaded to the HuggingFace model reposi-
tory (denoted as HuggingFace). We evaluate the four models with
our effectiveness metric and ASR when inserting the trigger at the
beginning of the sample.
From Table 6, we can see that the ğ¸ values of the first three
models are much larger than our ğ¸ values. Moreover, their ğ¸ values
are almost all greater than 5, implying such triggers can hardly be
considered as effective triggers. However, the average ğ¸ value of
our triggers is only 2.12. We can also see that some triggers injected
using NeuBA can retain the usability after fine-tuning. For example,
the â€˜â‰¡â€™ in the HuggingFace backdoor model has an attack success
rate of 98.7%. However, other triggers in the HuggingFace backdoor
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3147Table 5: The trigger effectiveness and stealthiness (ğ¸/ğ‘†) for nine datasets. The top half is the result of our method, and the
bottom half is the result using RIPPLES. The average text length of these datasets is below their name.
Method
Triggers
Ours
RIPPLES
cf
tq
mn
bb
mb
cf
tq
mn
bb
mb
average
average
Amazon
(99)
1.00/0.011
1.68/0.014
1.04/0.010
1.00/0.011
1.79/0.017
1.30/0.013
2.40/0.019
2.32/0.018
2.40/0.019
2.28/0.018
2.34/0.019
2.35/0.019
Yelp
(167)
1.06/0.006
1.59/0.007
1.58/0.007
1.10/0.005
1.12/0.007
1.29/0.006
3.31/0.017
3.22/0.016
3.17/0.016
3.29/0.016
3.38/0.017
3.27/0.016
IMDB
(299)
1.19/0.004
2.01/0.006
1.94/0.006
1.21/0.004
1.29/0.004
1.53/0.005
4.16/0.012
4.03/0.012
3.95/0.012
4.01/0.012
4.02/0.012
4.03/0.012
SST-2
(23)
1.00/0.026
1.00/0.027
1.01/0.024
1.00/0.026
1.00/0.023
1.00/0.025
1.00/0.026
1.00/0.026
1.00/0.026
1.00/0.026
1.00/0.026
1.00/0.026
Jigsaw
(104)
1.18/0.022
1.38/0.007
2.80/0.052
1.05/0.006
1.30/0.022
1.54/0.022
2.30/0.056
2.31/0.056
2.32/0.057
2.49/0.056
2.24/0.055
2.33/0.056
Offenseval
(38)
1.00/0.023
1.01/0.024
1.01/0.024
1.00/0.032
1.01/0.036
1.00/0.028
2.06/0.061
1.97/0.060
1.85/0.058
1.93/0.058
1.94/0.058
1.95/0.059
Twitter
(37)
1.08/0.025
1.57/0.051
1.03/0.034