### Techniques for Filtering Spiders/Bots during Log File Analysis

I would like to share our current approach and seek additional insights from the community.

#### Current Measures
We have implemented several measures to filter out spiders and bots, though we are unsure of their overall effectiveness. Our current strategy includes the following steps, which overlap to some extent:

1. **Monitoring `robots.txt` Requests:**
   - We monitor requests for our `robots.txt` file and subsequently filter out all other requests from the same IP address and user agent.

2. **Comparing User Agents and IP Addresses:**
   - We compare user agents and IP addresses against published lists. The two most widely used lists for this purpose are from iab.net and user-agents.org.

3. **Pattern Analysis:**
   - While we do not have pre-set thresholds, pattern analysis remains a useful tool. We consider the following metrics:
     - **Page Views as a Function of Time:** A high frequency of page views with very short intervals (e.g., 200 milliseconds per page) is indicative of bot activity.
     - **Traversal Path:** We analyze the path taken by the "user" through our site. Systematic and nearly complete traversals, such as those that follow a backtracking algorithm, are often signs of a bot.
     - **Precisely-Timed Visits:** Regular visits at specific times (e.g., 3 am each day) can also indicate bot activity.

While these measures help us catch a significant number of spiders and bots, I am confident that we are only capturing the low-hanging fruit. I am interested in hearing from the community about more advanced techniques and best practices.

#### Additional Resources
For further reading, you may find the Newsletter posts tagged under "Web Log Analysis" on the Nihuo Web Log Analyzer site pages to be useful.

Thank you for your input and suggestions.