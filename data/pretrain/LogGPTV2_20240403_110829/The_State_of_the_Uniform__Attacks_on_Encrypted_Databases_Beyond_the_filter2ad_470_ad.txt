(cid:2)(cid:3)var( ˆNJ(i+1)− ˆNJ(i)|d)
Formulate the test statistic Ti ←
null hypothesis Hi : E[(cid:8)NJ(i+1) − (cid:8)NJ(i)] = 0;
ˆNJ(i+1)− ˆNJ(i)
// Eq. (1);
k=1 bkfk
for the
d+1
;
d
Since Ti follows approximately a standard distribution, we can
derive its corresponding two-sided signiﬁcance level, denoted as Pi;
if Pi > 0.1 then
return ˆNJ(i) since the null hypothesis Hi is not rejected
9
10
11
12
13
14
15
16
17 end
end
IV. REVISITING DATA RECONSTRUCTION ATTACKS
In this section, we use the techniques from Section III to
develop new reconstruction attacks on encrypted databases
using both the search-pattern leakage and access-pattern
leakage. Our reconstruction algorithm for range queries (Sec-
tion IV-A) is signiﬁcantly different from previous approaches.
Our reconstruction algorithm from k-NN queries (Section IV-B)
builds on previous work [33] but follows a different algorithmic
strategy so as to (1) reduce the number of required samples and
(2) scale for larger values of k. We experimentally demonstrate
the accuracy of our reconstruction algorithms under various
query distributions and densities of the database.
A. Reconstruction from Range Queries
Illustrative Example. We start by conveying the intuition of
our range attack with an application on a simple database with
only three values, {v0 = 7, v1 = 15, v2 = 20} from universe
[1, 30] shown in Figure 9. The distances between consecutive
pairs, Li = vi − vi−1, are L0 = 7, L1 = 8, L2 = 5, L3 = 11.
Fig. 9. Illustrative example of a database along with all the possible conditional
probability distributions and their corresponding support size.
For simplicity, we consider ﬁrst the restrictive scenario where
the adversary has observed all possible range queries. In this
case, there is no need to estimate the number of range-queries
that return a speciﬁc response r(cid:3), it is enough to count the
number of unique queries that return r(cid:3). In other words, the
adversary knows the exact support size for every conditional
probability distribution pT|R (T|R = r(cid:3)
). From Remark 2, the
support size can be expressed as the product Li, Lj for the
appropriate pair i, j. The support sizes of all conditional
distributions of this example are illustrated in Figure 9.
To compute the n+1 unknowns L0, L1, L2, L3, the adversary
(cid:11)
solves the following set of
equations:
(cid:10)
n
2
L0 · L1 = 56
L0 · L2 = 35
L1 · L2 = 40
L1 · L3 = 88
L2 · L3 = 55
L0 · L3 = 77
(2)
One can apply the logarithmic function to transform the
products to sums, i.e., x0 = log(L0), x1 = log(L1), x2 =
log(L2), x3 = log(L3). Then, using elementary row operations
on the system of linear equations one can easily compute the
echelon form and show that the rank of the matrix is n+1, thus
there is a unique and exact reconstruction for the restrictive
scenario where the adversary has seen all possible queries.
We now consider the more realistic, general scenario of an
adversary who has observed a subset of all possible search
tokens, as issued by the client under a ﬁxed query distribution
that is unknown to the adversary. From Observation 1, a
token-response pair, (t(cid:3), r(cid:3)
), can be seen as a sample from the
conditional probability distribution pT|R (T|R = r(cid:3)
). Thus, the
ﬁrst step of the attack is to partition the observed search tokens
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1230
size of the corresponding distribution. We denote with (cid:9)Li,j
with respect to their returned responses, i.e., the conditional
distribution they belong to, using the method of Section III.
The result of this partition gives a collection of multisets of
search tokens. Each multiset is used to estimate the support
the estimation of the support size Li · Lj. We note here
that some estimations should play a more central role in the
overall reconstruction based on the fact that we have observed
more samples. For example, the support size estimation of
pT|R (T|R = r(cid:3)
) from a sample of size 10 is less trustworthy
than the support size estimation of pT|R (T|R = r(cid:3)(cid:3)
) from a
sample of size 103. To capture this observation we model a
minimization problem, where the “importance” of an estimate
(cid:9)Li,j is expressed by a non-negative weight wi,j.
Algorithm 2: AGNOSTIC-RECONSTRUCTION-RANGE
Input: Multiset of range search tokens and their responses
D = {(t1, r1), (t2, r2) . . . , (tm, rm)}; ordering of the database
records I = (id0, . . . , idn−1); endpoints α and β of the
database universe; arbitrary positive constant 
Output: Approximate reconstruction ˜v0, . . . , ˜vn−1
1 for every unique response r in D do
2
3
4
5
6
Let idi ∈ r be the identiﬁer of r with minimum rank in I;
Let idj ∈ r be the identiﬁer of r with maximum rank in I;
Let Di,j+1 be the mulitset of all the pairs in D with response r;
Let weight wi,j+1 = max{, |Di,j+1|2};
search tokens in Di,j+1 to output estimated support size (cid:8)Li,j+1;
Run Algorithm 1 (MODULAR-ESTIMATOR) on the multiset of
(cid:2)
j(cid:4)=0 2w0,j log((cid:9)L0,j)
(cid:2)
j(cid:4)=1 2w1,j log((cid:9)L1,j)
(cid:2)
j(cid:4)=n 2wn,j log((cid:9)Ln,j)
7 end
8 Solve the system of linear equations below, obtained by setting the
partial derivatives of Eq. (3) equal to zero:
j(cid:4)=0 2w0,j
2w0,1
. . .
2w0,n
⎤⎥⎥⎦ =
⎡⎢⎢⎣x0
2w0,1
j(cid:4)=1 2w1,j
2w0,n
2w1,n
. . .
. . .
2w1,n
j(cid:4)=n 2wj,n
. . .
. . .
. . .
. . .
⎡⎢⎢⎢⎣
⎤⎥⎥⎦
⎡⎢⎢⎣
x1
. . .
xn
(cid:2)
(cid:2)
(cid:2)
. . .
(cid:4)n
9 Compute the approximated lengths as L0 = 2x0 , . . . , Ln = 2xn ;
i=0 Li = β − α + 1;
10 Scale L0, . . . , Ln so as
11 Let v−1 = α − 1;
12 for i = 0, · · · , n − 1 do
Let ˜vi = ˜vi−1 + Li;
13
14 end
15 return ˜v0, . . . , ˜vn−1;
(cid:19)
error function is the logarithm of the ratio, i.e., e2(Li, Lj) =
log
Reconstruction Algorithm. The goal of the proposed
optimization is to assign values to the lengths L0, . . . , Ln
so as to minimize the weighted sum of squared errors. One
option for the error function e is the difference between the two
terms, i.e., e1(Li, Lj) = (Li·Lj−(cid:9)Li,j). Another option for the
(cid:18)
= log(Li) + log(Lj)− log((cid:9)Li,j). If there
(Li · Lj)/(cid:9)Li,j
is no sample to feed to the estimator to produce(cid:9)Li,j, we assign
default value (cid:9)Li,j = 1, therefore the ratio in e2 is well-deﬁned
is equal to the estimated quantity (cid:9)Li,j. From experiments, we
since the denominator takes positive non-zero values. Notice
that both e1 and e2 output 0 when the product of the unknowns
found that the error function of the e2(Li, Lj) (log of ratio)
has superior reconstruction quality in the majority of the cases
compared to the error function e1(Li, Lj). For simplicity, we
deﬁne new unknowns xi = log(Li) for i ∈ [0, n], which yields
the following ﬁnal unconstraint optimization problem:
n(cid:4)
n(cid:4)
wi,j(xi + xj − log((cid:9)Li,j))2
i=0
min
j=i+1
x0,...,xn
(3)
We set weight wi,j = max{,|Di,j|}, where  is an arbitrarily
small positive value and |Di,j| is the number of tokens used
for estimation (cid:9)Li,j. The values x0, . . . , xn obtained from the
solution of (3) are mapped to lengths as Li = 2xi. As a
ﬁnal step, we scale the derived lengths L0, . . . , Ln to sum
to N = β − α + 1 (total range of the database values).
Theorem 2. The unconstrained quadratic optimization problem
of Equation (3) with constant values wi,j,(cid:9)Li,j, and unknown
⎞⎠ .
partial derivative with respect to xi as:
(2wi,j) xj −
values xi, is a convex function and has a unique solution.
The proof of Theorem 2 is in the Appendix. We derive the
2wi,j log((cid:9)Li,j)
⎞⎠ xi +
⎛⎝(cid:4)
⎛⎝(cid:4)
(cid:4)
2wi,j
=
∂f
∂xi
j(cid:4)=i
j(cid:4)=i
j(cid:4)=i
We ﬁnd the global minimum by setting all partial derivatives
equal to zero. Our reconstruction method from range queries,
RANGE-RECONSTRUCTION, is shown in Algorithm 2.
Comparison with Attack GENERALIZEDKKNO [27].
We ﬁrst compare the accuracy of the reconstruction of our
attack, AGNOSTIC-RECONSTRUCTION-RANGE, to the accu-
racy of the state-of-the-art reconstruction attack GENERAL-
IZEDKKNO, which is the most general (i.e., with fewest
assumptions, e.g. only uniform queries) of the three attacks
proposed by Grubbs et al. [27]. In this experiment, we generate
Q = 104 range queries uniformly at random from the universe
[α, β] = [1, 103]. We randomly generate the values of the
encrypted DB under various database densities. To assess the
quality of the reconstruction, we use the mean square error
(MSE) and the mean of absolute error (MAE) between the
original and the reconstructed database. We note here that MSE
gives a higher penalty to reconstructed values with larger error.
⎤⎥⎥⎥⎦
Uniform Query Distribution, N=103, Q=104
GeneralizedKKNO
Agnostic-Reconstruction-Range
r
o
r
r
r
E
e
r
a
u
q
S
-
n
a
e
M
600
400
200
100
50
10%
20%
40%
60%
Database Density
80%
90%
Uniform Query Distribution, N=103, Q=104
GeneralizedKKNO
Agnostic-Reconstruction-Range
15
10
5
15
10
5
10%
20%
40%
60%
Database Density
80%
90%
r
o
r
r
l
t
E
e
u
o
s
b
A
-
n
a
e
M
Fig. 11. Comparison between GENERALIZEDKKNO and our attack,
AGNOSTIC-RECONSTRUCTION-RANGE, under the uniform query distribution.
Recall that our algorithm is (1) not tailored to work well
on a speciﬁc query or data distribution and (2) distribution
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1231
l
)
e
a
c
S
-
g
o
L
(
r
o
r
r
E
d
e
r
a
u
q
S
n
a
e
M
Short Range Queries, N=103, Q=104
Short-Ranges(1,3)
Short-Ranges(1,5)
Short-Ranges(1,20)
103
102
101
10%
20%
40%
60%
80%
90%
Database Density
Range Queries Centered Around a Value, N=103, Q=5 104
Value-Centered(1,3)
Value-Centered(1,5)
Value-Centered(1,10)
20%
40%
60%
80%
90%
Database Density