    [    0.000000] BIOS-e820: [mem 0x000000007dc5ec00-0x000000007fffffff] reserved
    May 30 02:22:56 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20254" x-info="http://www.rsyslog.com"] start
    May 30 02:22:56 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20254" x-info="http://www.rsyslog.com"] exiting on signal 15.
    May 30 02:22:59 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20329" x-info="http://www.rsyslog.com"] start
    May 30 02:26:12 localhost pptpd[554]: MGR: initial packet length 18245 outside (0 - 220)
    May 30 02:26:12 localhost pptpd[554]: MGR: dropped small initial connection
    May 30 02:31:38 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20463" x-info="http://www.rsyslog.com"] start
    May 30 02:31:38 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20463" x-info="http://www.rsyslog.com"] exiting on signal 15.
    May 30 02:31:38 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20483" x-info="http://www.rsyslog.com"] start
    May 30 02:35:59 localhost rsyslogd: [origin software="rsyslogd" swVersion="5.8.10" x-pid="20513" x-info="http://www.rsyslog.com"] start
    May 30 02:22:56 localhost sudo:     root : TTY=pts/0 ; PWD=/var/log/nginx ; USER=root ; COMMAND=/sbin/rsyslogd -v
    May 30 02:26:12 localhost sshd[20335]: Bad protocol version identification 'GET / HTTP/1.0' from **.**.*.***
    May 30 02:26:12 localhost sshd[20336]: Did not receive identification string from **.**.*.***
    type=DAEMON_START msg=audit(1509001742.856:3503): auditd start, ver=2.4.5 format=raw kernel=2.6.32-042stab123.3 auid=0 pid=28676 res=success
    type=DAEMON_ABORT msg=audit(1509001742.857:3504): auditd error halt, auid=0 pid=28676 res=failed
    type=DAEMON_START msg=audit(1509001860.196:6814): auditd start, ver=2.4.5 format=raw kernel=2.6.32-042stab123.3 auid=0 pid=28690 res=success
    type=DAEMON_ABORT msg=audit(1509001860.196:6815): auditd error halt, auid=0 pid=28690 res=failed
    type=DAEMON_START msg=audit(1509001887.133:9072): auditd start, ver=2.4.5 format=raw kernel=2.6.32-042stab123.3 auid=0 pid=28696 res=success
    type=DAEMON_ABORT msg=audit(1509001887.133:9073): auditd error halt, auid=0 pid=28696 res=failed
    type=DAEMON_START msg=audit(1509001918.016:849): auditd start, ver=2.4.5 format=raw kernel=2.6.32-042stab123.3 auid=0 pid=28707 res=success
    type=DAEMON_ABORT msg=audit(1509001918.016:850): auditd error halt, auid=0 pid=28707 res=failed
    type=DAEMON_START msg=audit(1509001943.060:530): auditd start, ver=2.4.5 format=raw kernel=2.6.32-042stab123.3 auid=0 pid=28713 res=success
如果大家能一眼看出这是什么日志，那可以说对Linux算是较为熟悉了，如果还能理解其中数据所包含的信息量，那可以说至少是一个经验比较丰富的Linuxer了~  
那么理解数据之后该做什么呢？我们回想一下，在Web日志中如何进行基础的统计分析。  
由于日志元数据的格式限定，所以并不方便我们进行统计分析，比如我们需要简单的统计每个IP的请求数量，从而找出请求量较大的访问者，按照传统的方式我们可以使用以下命令做到这一点：
    awk '{print $9}' access.log | sort | uniq -c | sort -fr
但是如果我们的需求多变，如只想提取某一段时间范围访问者，又比如只想统计状态码为200的访问者等等，当需要分析的应用数量过多且数据量过大时，那么使用传统的方式效率将非常低，所以我们需要将日志进行结构化的解析，让它变成机器可读的数据，方便我们检索与统计。熟悉Logstash的人可能知道，在编写Logstash配置文件时，对日志解析时，需要用到Grok
filter plugin，官方配置示例如下：
    input {
      file {
        path => "/var/log/http.log"
      }
    }
    filter {
      grok {
        match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
      }
    }
我们参考了Grok的优秀思路，进行了自研发解析器，我们最开始的目的非常简单，由于Logstash的插件是Ruby开发，而Ruby其实我们并不熟悉（虽然插件已经开源），且协同开发有点困难，也不好维护与拓展，然后便有了如图的解析框架：（图为从解析框架中提取的一部分）
### 3、增量
其次便是增量解析，如以Web日志攻击解析举例：
这里大家思考一个问题，这个增量解析器是如何从无到有产生的。答案是：“ **增量解析与分析需求强相关** ”  
我们来举例说明：  
1.你需要统计访问者的地区分布，你想知道你的网站大部分用户是来自北京还是上海，是国内还是国外  
2.你需要统计访问的客户端类型，你想知道访问你的网站大部分用户是使用了手机还是电脑或平板  
3.你需要统计网站日志中的攻击请求数量与正常访问的分布情况  
4.你需要统计网站的访问者有哪些人曾发起过攻击，当前网络中有多少正在活动的黑客  
...  
这些分析需求非常常见，然而从Web日志中却无法直接做到这一点，所以此时我们需要进行增量解析。  
我们可以简单的把增量解析理解为，在原数据的基础上增加更丰富的数据来满足多样的分析需求。  
如： IP >> 地区  
Ua >> 客户端信息  
Url >> 是否具有攻击特征  
...  
ELK体系中，Logstash的[Filter
Plugins](https://www.elastic.co/guide/en/logstash/5.3/filter-plugins.html)便是对此的不完全诠释
### 4、计算
    可能大家对计算这个需求会有点疑惑，不知道计算的需求到底在哪，说实话最开始我们有这种需求时，其实只是单纯的因为ElasticSearch已经无法满足我们的多层聚合搜索需求，所以我们在数据处理过程中进行计算，通过计算结果进行二次分析。后来，结果我们一些相关的研究，我们尝试将数据挖掘、机器学习与日志分析进行结合，对日志中的计算结果进行各类算法的研究与尝试，希望能使用非传统的方式达到我们的分析需求，总得来说，我们计算有两种很直接的需求：
    1.完成在ElasticSearch中无法实现的复杂的聚合统计需求
    2.将计算结果进行数据挖掘、机器学习等场景
当然这里说的是我们在实践过程中所遇到的需要计算的场景，并非指的是计算的全部作用，为了达成某个需求而使用需要使用计算的场景都可算为此范畴。  