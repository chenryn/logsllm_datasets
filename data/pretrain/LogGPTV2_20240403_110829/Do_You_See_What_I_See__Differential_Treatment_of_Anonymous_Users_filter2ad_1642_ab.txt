perspectives: direct censorship of content, censorship of trafﬁc
entering Tor, and censorship of trafﬁc exiting Tor. A large and
growing body of literature focuses on the ﬁrst two classes, but
the latter category has seen little in the way of study; our work
aims to ﬁll this gap.
Much existing work has focused on measuring and evading
direct content blocking in different countries. Prior work has
also focused on government blocking of censorship circum-
vention systems. Dingledine et al. [5] discuss when and how
different governments tried to block access to Tor; govern-
ments mainly use address-based blocking of requests to the Tor
website, relays, and bridges, and protocol-based blocking of
TLS connections to the Tor network identiﬁed by Tor speciﬁc
characteristics (for example, cipher suite).
Our work focuses on a different aspect of the censorship
problem: we examine server-side blocking of clients;
that
is, blocking by the server based on the characteristics of
the source, not blocking by an intermediate ﬁrewall based
on characteristics of the destination. In the classical Internet
censorship scenario,
the server would be happy to accept
connections from a client, but some network device near the
client prohibits it. We, on the other hand, look at cases where
the client’s connection arrives at the server unimpeded, but the
server (or something working on its behalf) rejects it.
In our work we make use of data from the Open Ob-
servatory of Network Interference (OONI) [17]. (Despite a
similarity in purpose and acronym, this project is separate from
the OpenNet Initiative discussed above.) The OONI dataset
has a crucial feature for studying differential treatment of
Tor users: it consists of many simultaneous downloads both
with Tor and without Tor. While the intent behind these
measurements is to highlight content that is inaccessible from
certain locations unless one uses Tor, we can employ the same
information to identify destinations inaccessible because one
uses Tor.
Developing robust techniques to detect blocking is also
important. We need to know when an application is being
blocked, and we also need to distinguish genuine network
interference from benign or transient failures. Jones et al.
tested automated means of detecting censorship block pages
in an OpenNet corpus [13]. A metric based on page length
proved the best-performing of several options. Our experiments
necessitate different ways of detecting blocks at different
network layers. In Section IV we use repeated scans across
space and time, and in Section V we compare test downloads
against simultaneous control downloads.
IV. MEASURING NETWORK-LAYER DISCRIMINATION
As we discuss in Section II, a straightforward technique for
services to block Tor is to ﬁlter trafﬁc from publicly listed exit
nodes. To broadly assess this, we measure Tor ﬁltering using
ZMap probing from both Tor exit nodes and from control (non-
Tor) nodes to see how their access to remote addresses differs.
For convenience we term these measurements as assessing
‘network-layer’ discrimination, though from a technical per-
spective they combine measurement of layer-3 and layer-4
blocking, since we restrict our measurements to attempts to
connect to TCP port 80 services.
A. ZMap
ZMap is a high-performance network scanner capable
of scanning the entire IPv4 address space in as little as
45 minutes, much faster than traditional scanners such as
Nmap [8]. ZMap achieves this efﬁciency by incorporating
multiple optimizations, including randomized target selection
and maintaining no connection state. Because ZMap does not
maintain state, it also does not retransmit probes in case of
loss. We used ZMap for test runs of the entire IPv4 address
space starting in Spring 2015. Over the course of repeated
experiments, we uncovered several bugs (for some of which
we contributed ﬁxes, while others were ﬁxed by the ZMap
team), addressed measurement considerations (for avoiding
measurement loss), and added extra functionality as discussed
below. For our measurements we recorded both TCP SYN-
ACKs and RSTs. We conﬁgured ZMap to run at 100 Mbps
rather than at 1 Gbps to avoid saturating our local networks.
Doing so results in one scan taking about 7 hours rather than
45 minutes.
B. Overview of measurements and block detection
We run our scans from Tor exit nodes and from two sets
of control nodes: university nodes and a Tor middle node. We
compare responses to our Tor scans with those from the base-
line control scans and ﬂag deviations as potentially reﬂecting
discriminatory blocking. Target hosts respond to ZMap probes
3
1) Mitigating Measurement Loss: We ﬁrst ask whether
ZMap accurately sends all the packets it is conﬁgured to send,
and whether it correctly logs packets and responses.
We proﬁled ZMap using an experimental setup that consists
of a well-provisioned machine running ZMap, and a separate
machine running a packet capture. All the ZMap packets are
directed to the second machine via a Gigabit Ethernet cable.
Separating packet transmission and packet capture allows us to
account for losses occurring due to both ZMap itself and the
underlying network card. It also avoids the scenario where the
two processes compete with each other for CPU cycles. When
ZMap runs with its default conﬁguration, we see a 6.7% failure
rate—this failure is completely eliminated when we throttle
our sending rate down from 1 Gbps to 100 Mbps. (During
this process, we also identiﬁed and reported a bug in ZMap
that caused it to not send certain packets due to the interaction
between scan targets, the blacklist and thread-level sharding.)
In addition, we need to conﬁgure a timeout for ZMap to
deem that a packet did not receive a response. Figure 1 shows
the distribution of the time measured between sending the
last scan packet and receiving a response for a full scan of
IPv4. To generate this plot, ZMap logged response packets
for 25 minutes after sending the last scan packet. More than
95% of all replies (excluding RSTs), and 80% of RSTs arrive
within the ﬁrst 30 seconds, while the rest trickle in up until
500 seconds. Though unusual, late responses could arise due to
backed-off timers in the case of SYN-ACKs, huge bufferbloat,
or initial latency incurred by extensive setup requirements of
cellular wireless devices [18]. Given this data, we chose a
conservative cooldown value of 10 minutes for responses to
come in.
2) Network Packet Loss: An unsuccessful response can be
due to loss on the paths between the scanner and the destina-
tion, caused by transient network issues such as congestion or
network failure. We reduce such noise by sending redundant
probes per destination. If any of the probes elicits a SYN-
ACK from the destination, we treat it as a successful response,
because a single response sufﬁces to inform us that the target
server does not block Tor trafﬁc.
We can introduce probe redundancy in many ways; the
simplest is by conducting back-to-back scans from the same
vantage point. However, since a single scan takes about 7 hours
to complete, such an approach introduces a large gap between
the redundant probes, which can lead to inconsistent responses
due to temporal churn. We ran 3 back-to-back scans from one
of our control vantage points. We observed a temporal churn
between the ﬁrst two scans of 13.30%, which increased to
21.61% when computed across the three scans. We repeated
the experiment at another of our control vantage points and
made similar observations. This ﬁnding means that servers
respond quite inconsistently across large intervals of time.
This high temporal churn motivates us to incorporate re-
dundancy at shorter timescales in our measurements. Although
ZMap allows us to send multiple probes per target in a single
scan,
it does so back-to-back without any delay between
them. This approach only helps if loss events are independent;
however, transient network issues mean that loss events are
presumably not independent.
Fig. 1: Distribution of time until receiving a response packet since the last
probe was sent for a full scan of IPv4.
(TCP SYNs) in one of three ways: a) sending a SYN-ACK,
which we term a successful response; b) sending a RST, which
we term an unsuccessful response; or c) not responding, which
we also deem an unsuccessful response. ZMap, by default, only
records successful responses; we modiﬁed it to record RSTs
as well. We note that for an individual probe it is not possible
to distinguish a lack of response from packet loss.
We might
in simple terms think we can identify Tor
blocking by observing destination addresses that respond to
probes from our control nodes but not those from our Tor
exit nodes. However, this reasoning has two main limitations:
1) Unsuccessful responses could arise due to packet loss along
either the packet forward or return path; and 2) Destinations
can respond inconsistently to probes due to factors unrelated to
discriminatory blocking, such as servers only operating during
certain hours of the day or days of the week.
More generally, we need to consider issues of churn: how
Internet service reachability varies, in both spatial and temporal
terms. By spatial churn we mean the notion that simultaneous
probes sent from topologically separate clients to the same
server might yield different outcomes, for example due to
network congestion or a network outage blocking the path
from one of the clients but not the other. By temporal churn
we refer to the reachability from the same client to the same
server varying over the course of time, for example due to day-
of-the-week effects governing when the server is accessible.
Thus, to understand how to soundly compare probe out-
comes seen at our control nodes versus from Tor nodes, we
need to incorporate consideration of how to distinguish probing
results that differ due to churn versus those that actually reﬂect
discrimination. Note that through the rest of our discussion, the
underlying assumption is that services either completely block
a Tor exit node or allow it. We do not deal with selective
blocking or rate-limiting in this paper.
C. Mitigating the Impact of Packet Loss
As noted above, ZMap does not allow us to distinguish
between a single non-response and a packet loss event. To
account for this limitation, we take care to minimize measure-
ment loss in our measurements and to account for potential
packet loss in the network.
4
1510501005000.00.40.8Time(s)Number of Packets ECDFlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllltcp_rsttcp_synackicmp_unreachicmp_timxceedControl nodes
Number of control nodes
Number of IPv4 scans
Time span of scans
Scanned IP addresses per measurement
Average hit-rate per measurement
Average (estimated) network loss
Tor exit nodes
Number of exit node
Number of scans
Time span of scans
Average hit-rate per measurement
3
7 per control node
Aug 7–13
3,662,744,599
1.91% (σ=0.01%)
0.84% (σ=0.18%)
4
4 per exit node
Aug 10–13
1.87% (σ=0.03%)
TABLE I: Summary of control and exit node data. For all scans, we ﬁltered
out IP addresses included in the largest blacklist, that is, one employed by
the last scan. Network loss per measurement is estimated as the percentage of
IP addresses inaccessible from a node but accessible from at least one other
node.
Since ZMap does not keep state, we cannot retransmit only
those for which we did not receive a response. We therefore
follow the simple strategy of sending K probes, resending
them, sending another K probes, resending them, and so on.
For K = 1, 000, 000 and with a sending rate of 100 Mbps,
this means that the retransmitted probe follows 6.7 sec after
the original. This approach allows us to maintain the sending
bandwidth and allows us to keep ZMap as a single threaded
process; however, and although, as expected, it doubles the
length of a full scan. Across three sites and four scans,
we found that factoring in responses to retransmitted probes
increases the response rate for original probes by 1.04% (we
can distinguish these by sending retransmissions from different
ports). We further observe that there is a temporal churn of
1.93% between 6.7 sec apart scans, which is signiﬁcantly lower
than 13.30% churn for scans run back-to-back (effectively ≈7
hours apart).
D. Data
We run our measurements from a set of three control nodes
and a set of four Tor exit nodes. Two control nodes are located
in US universities and one in a European university. The
control node measurements serve a dual purpose: they allow
us to calibrate and understand our measurement method and
the data, and they serve as the baseline measurements against
which we compare the Tor exit node measurements.
Our ﬁrst goal is to develop a global ‘web footprint’, a
set of IP addresses that respond to our scans on port 80. On
average, a control node sees a hit rate of 1.91% (σ=0.01%) per
measurement scan (translating to ≈ 70 million IP addresses).
We note that each scan consists of two probes per target IP
address (Section IV-C2); and a ‘hit’ consists of SYN-ACK
response to our SYN for at least one probe. This number is
roughly constant across the three locations. However, due to
multiple reasons, including routing and transient failures, net-
work policies, time-of-day effects, and regular usage patterns,
no two scans return the same set of IP addresses (the issue of
churn discussed previously).
We ﬁrst conducted extensive preliminary ZMap scans (on
the order of 90 scans over a period of 3 months) in order
to calibrate the accuracy of our measurement methodology
Fig. 2: Number of new IP addresses each control node sees per day.
and address problems that arose. The scans all employed a
blacklist excluding IP addresses, which we added to whenever