### Main Representation of Byte Vectors

In analyzing a set of byte vectors derived from different file types, we observed subtle differences in their frequency representations. To capture these variations, we utilized several statistical measures from the frequency spectrum. Specifically, we divided the frequency spectrum into four bands: 0-π/8, π/8-π/4, π/4-π/2, and π/2-π. For each band, we calculated the mean, variance, power, and skewness. For instance, the average mean power in the 0-π/8 band is illustrated in Figure 3.

### Higher Order Statistics

We also examined bicoherence, a third-order statistic, which can characterize non-linearities in the data. Our hypothesis is that the degree of non-linearity introduced by compression or encryption techniques varies, making these measures useful for distinguishing between different content types. We computed the bicoherence, followed by the power of the bicoherence magnitude and phase, and the mean of the bicoherence magnitude. Additionally, we calculated the kurtosis and skewness for each byte vector. For a comprehensive review of bicoherence and higher-order statistics, see [7].

### Offline Experiments

#### Objective
The experiments aimed to determine three critical parameters related to the classifier:
1. The effectiveness of the features in distinguishing content types.
2. The trade-off between the required minimum data for classification and classification accuracy.
3. The trade-off between the number of features used and classification accuracy to optimize throughput.

#### Data Set
Our data set included a variety of content types, categorized into raw (uncompressed), compressed, and encrypted data. For raw data, we considered plain-text, BMP, and WAV files. For compressed data, we included ZIP, JPG, MP3, and MPEG files. The data set, consisting of 7 different content types, was obtained from a random crawl of a peer-to-peer network, with each file being at least 50KB. A total of 1000 files were downloaded for each type, and these files were encrypted using the AES algorithm to generate 1000 encrypted files.

#### Classification
We used Support Vector Machines (SVM) with the RBF kernel for classification, based on previous experiments showing consistently better performance. The RBF kernel was optimized via a grid search over its two parameters: cost and gamma. We used the freely available LibSVM [3] for our experiments and implementation.

#### Experimental Setup
We computed the proposed statistics over various payload sizes. Each file was segmented into 1024-byte blocks, consistent with the average size of a TCP packet with payload. We collected 32768 bytes of data (equivalent to 32 packets) from random locations in each file. Different payload sizes were then extracted from this sampled data, and the corresponding statistics were computed. This resulted in 1000 feature vectors for each of the eight categories. A SVM classifier was trained using 400 feature vectors from each category, and the remaining 600 vectors were used for testing.

### Results

As expected, classification accuracy improved with increasing payload size. Accuracy was defined as:

\[ \text{Accuracy} = \frac{T}{T + F} \]

where \( T \) is the number of correctly classified samples, and \( F \) is the number of incorrectly classified samples. Figure 4 shows the accuracy versus payload size trade-off, indicating that accuracy saturates for payloads larger than 16KB.

For a more detailed analysis, we computed the confusion matrix for payloads of 16KB. The confusion matrix, shown in Table 1, provides information about the actual and predicted results. For example, 96.33% of plain-text payloads were correctly classified, while 2.83%, 0.17%, 0.67%, and 0.17% were misclassified as BMP, WAV, ZIP, and MPG, respectively.

### Feature Selection

To identify the most essential features, we used the Sequential Forward Feature Selection (SFFS) algorithm. This algorithm sequentially adds or removes features to find the best subset that maximizes information gain. As shown in Figure 5, optimal accuracy was achieved using only 6 out of the 25 features, with less than a 1% difference in accuracy compared to using all 25 features. The selected features, in order of importance, are entropy, power in the first frequency band, mean, variance, mean and variance in the fourth frequency band.

Using the selected features, the classifier was retrained and tested on data segments of 1024, 4096, 8192, and 16384 bytes. The detection results, shown in Figure 6, indicate that while feature selection provides only marginal improvement in accuracy, it significantly reduces the required processing power per flow.

### Flow Scheduling

Flow characterization is slower than flow collection and can become a bottleneck on large networks. Network flows can be categorized into four major groups based on packet rate over time (sustained, temporary) and content type (static, dynamic).

- **Sustained Static Flows**: These have a constant packet rate for long periods (minutes to hours) and do not change content type during their lifetime. Examples include streaming audio/video and downloading large files like ISO images.
- **Sustained Dynamic Flows**: Similar to sustained static flows in terms of packet rate and lifetime, but the content type changes over time. Examples include accessing network file systems and downloading files via file-sharing programs.
- **Temporary Static Flows**: These are bursts of traffic lasting a few seconds to minutes, carrying a single type of content. Examples include web requests and emails.
- **Temporary Dynamic Flows**: These have a similar lifetime to temporary static flows but change content type. Examples include files with embedded contents.

By categorizing and scheduling these flows, we can optimize the overall performance and efficiency of the system.