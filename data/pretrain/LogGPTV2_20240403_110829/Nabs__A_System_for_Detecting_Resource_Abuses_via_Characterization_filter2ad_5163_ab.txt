main representation of a set of byte vectors obtained
from diﬀerent types of ﬁles we noticed subtle diﬀer-
ences in frequency representations depending on the
original data type, we choose to use a number of statis-
tical measures from the frequency spectrum. We ﬁrst
divided the frequency spectrum into 4 bands ranging
from, 0− π/8, π/8− π/4, π/4− π/2, and π/2− π. We
then calculated the mean, variance, power, and skew-
ness of each band. For example the average mean of
the power in the 0 − π/8 band of the frequency spec-
trum can be seen in Figure 3.
Higher Order Statistics. Finally, we looked at bi-
coherence, which is a third order statistic. The bico-
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:38:35 UTC from IEEE Xplore.  Restrictions apply. 
x 1011
4
r
e
w
o
P
.
g
v
A
3.5
3
2.5
2
1.5
1
0.5
0
TXT
BMP
WAV
ZIP
JPG
MP3
MPG
ENC
Figure 3: Average mean of the ﬁrst band, 0 − π/8, of
the power spectrum for data vectors from 8 diﬀerent
ﬁle types.
herence is able to characterize non-linearities in the
underlying data. Our argument is that the amount
of non-linearity introduced by the compression or en-
cryption techniques varies. Thus these measures could
help us distinguish these content types. We ﬁrst com-
puted the bicoherence, after which power of the bico-
herence magnitude and phase, and the mean of the
bicoherence magnitude are calculated.
In addition
to these statistics we also computed the kurtosis and
skewness of each byte vector. For a good review of bi-
coherence and more generally higher order statistics
the reader is referred to [7].
4.1 Oﬄine Experiments
In this section we describe experiments carried out
to determine three critical parameters related to the
classiﬁer. First, we would like to determine the ef-
fectiveness of the features we have in distinguishing
the content type. Then, we would like to ﬁnd the
appropriate trade-oﬀ between the required minimum
data for classiﬁcation and accuracy of classiﬁcation.
Finally, we would like to determine trade-oﬀ between
the number of features used for classiﬁcation and ac-
curacy so that we can increase the through put of the
ﬂow characterization component. We begin this sec-
tion with the explanation of the data set followed by
the experimental setup.
Data Set. There are a variety of content types
available on the Internet. One could divide these con-
tent types into three major categories: raw (or un-
compressed), compressed, and encrypted data. Our
goal was to evaluate how well we can distinguish be-
tween data from each category. We selected a number
of diﬀerent content types from each category. For ex-
ample, in the raw category we looked at content types
of plain-text, BMP, and WAV, and in the compressed
category ZIP, JPG, MP3, and MPEG ﬁles. Our data
set, consisting of the 7 diﬀerent content types, was
obtained from a random crawl of a peer-to-peer net-
work. The only constraint placed on the downloads
was that the ﬁles be at least 50KB. A total of 1000
ﬁles were downloaded for each ﬁle type. These ﬁles
were then encrypted using the AES encryption algo-
rithm to obtain 1000 encrypted ﬁles.
Classiﬁcation. There are a variety of classiﬁcation
algorithms available. We have chosen to use Support
Vector Machines [14] in our experiments based on our
previous experiments with a number of diﬀerent data
sets, and observing consistently better performance
results over other classiﬁers. In our experiments we
opted to use the RBF kernel (Radial Basis Function).
The RBF kernel was optimized by doing a grid search
over its two parameters: cost and gamma. There are
many implementations of SVM available on the public
domain and we have chosen the freely available Lib-
Svm [3] for our experiments and for implementation
in the Nabs itself.
Experimental Setup. The proposed statistics
were computed over various sizes of payload. In or-
der to simulate sampling packets oﬀ the wire we seg-
mented each ﬁle into 1024-byte blocks. The 1024-byte
block was chosen in accordance with the average size
of TCP packet with payload. 32768 bytes of data,
or equivalently 32 packets were collected from ran-
dom locations in each ﬁle. Since we were interested
in seeing the eﬀects of the size of available data on
the classiﬁcation results, we then obtained diﬀerent
size payloads from the 32768 bytes of sampled data.
Given a payload size, statistics were then computed
for the payload. After which we have 1000 feature
vectors containing the identifying features proposed
for each of the eight categories. A SVM classiﬁer was
then trained using 400 feature vectors from each of
the 8 content types. The remaining 600 feature vec-
tors were then used to test the resulting classiﬁer.
4.2 Results
The above procedure was repeated for diﬀerent pay-
load sizes, and as expected the accuracy of the clas-
siﬁcation improved with the size of payload used for
classiﬁcation. In our experiment accuracy was deﬁned
as:
Accuracy =
(1)
T
T + F
where T is the number of samples classiﬁed correctly,
and F is the number of samples classiﬁed incorrectly.
Figure 4 shows the results of accuracy vs. payload size
trade-oﬀ. Interestingly, we observed that the accuracy
begins to saturate as the features are computed over
payloads larger than 16KB.
Since we were building a multi-class classiﬁer, just
looking at the overall accuracy would not give a com-
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:38:35 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
y
c
a
r
u
c
c
A
95
90
85
80
75
70
1024 2048
4096
8192
Size (Bytes)
16384
quency band.
y
c
a
r
u
c
c
A
l
a
t
o
T
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0
2
4
6
Number of Features
8
10
12
Figure 4: Accuracy of classiﬁcation for various pay-
load sizes.
plete picture on how well the classiﬁer was able to
distinguish between the categories.
In order to see
how well each category is distinguished with respect to
the others, we computed the confusion matrix. Con-
fusion matrix presents information about the actual
and predicted results using the classiﬁer. These en-
tries should not be misinterpreted as accuracy ﬁgures.
In fact, the overall accuracy of the classiﬁer is equal
to the average of the diagonal entries in the confusion
matrix. In Table 4.2 we show the confusion matrix for
payloads of size 16KB. From the table we can observe
that 96.33% of plain-text payloads were classiﬁed cor-
rectly. However, 2.83%, 0.17%, 0.67%, and 0.17% of
them were misclassiﬁed as BMP, WAV, ZIP, and MPG
respectively.
4.3 Feature Selection
Some of the features described in the feature set above
may have very little or no information gain in distin-
guishing between the diﬀerent categories. Further-
more, when implementing the actual system, speed
and complexity become an issue so one would only
want to employ the more essential features from the
25 proposed features. Therefore, we used SFFS (Se-
quential Forward Feature Selection) [11] algorithm to
identify and extract the essential features. This algo-
rithm sequentially adds or removes features and ﬁnds
the best subset of features which give maximal infor-
mation gain in the classiﬁcation process. As seen in
Figure 5, we could obtain optimal accuracy by us-
ing only 6 of 25 features.
In fact the accuracy has
less than 1% of diﬀerence with the case that all 25
features are used. The chosen features are in order of
importance entropy, power in the ﬁrst frequency band,
mean, variance, mean and variance in the fourth fre-
Figure 5: Accuracy of classiﬁcation for number of fea-
tures selected by SFFS.
Using the selected features, the classiﬁer was re-
trained and tested using the data segments of size,
1024, 4096, 8192, and 16384 bytes. Detection results
can be seen in Figure 6. As evident from the ﬁg-
ure, although feature selection provides only marginal
improvement, it greatly reduces required processing
power per ﬂow. Thus with only 6 features we are able
to obtain similar results as if we were using all 25
features.
5 Flow Scheduling
As we can observe from the previous section, ﬂow
characterization is slower than ﬂow collection and on
large networks the characterization becomes a bottle-
neck. Network ﬂows can be categorized into four ma-
jor groups based on packet rate over time (sustained,
temporary) and content type (static, dynamic.)
Sustained Static Flows. These ﬂows have con-
stant packet rate for long periods of time, several min-
utes or hours. These ﬂows do not change the content
type during their life time. Sustained static ﬂows are
generally the result of streaming audio/video or down-
loading a large ﬁle (like an ISO image).
Sustained Dynamic Flows. Similar to the above
in terms of packet rate and lifetime but the con-
tent type of the ﬂow changes with time. Example
of such ﬂows include, accessing network ﬁle systems
and downloading ﬁles via a ﬁle sharing program.
Temporary Static Flows. Temporary ﬂows are
mostly bursts of traﬃc that lasts only for a few sec-
onds or perhaps minutes utmost. Most network traﬃc
is of this form– web requests and emails to name a few.
These ﬂows carry a single type of content.
Temporary Dynamic Flows. Lifetime of the ﬂow
is same as above but the content type changes. The
change in content type is due to the fact a ﬁle may
have various embedded contents. Examples of such
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:38:35 UTC from IEEE Xplore.  Restrictions apply. 
Txt
96.33
2.83
0.17
0.67
0.17
0
0.83
0
Txt
Bmp
Wav
Zip
Jpg
Mp3
Mpg
Enc
2
91
3.17
Bmp Wav
0.67
3.67
88.33
0.17
0.83
1.17
0.83
0
1.33
0.67
2.33
0
0
Predicted
Jpg
0
0.5
0.83
Zip
0.83
1.17
1.33
73.83
4.17
0.83
0.67
2.33
6
89.83
0.83
2
0
Mp3
0.17
0.17
5.67
1.5
2
95.83
2.67
0
Mpg
0
0.5
0.5
1.17
1.67
0.67
90.67
0
Enc
0
0.17
0
16.67
0
0
0
97.67
Table 1: Confusion matrix for ﬂow content characterization using payload of size 16384 bytes (or roughly equiv-
alent to using payloads from 16 packets).
100
90
80
70
60
50
40
30
20
10
0
16 Packets
8 Packets
4 Packets
1 Packet
T
X
T
T
X
T
-
F
B
M
P
B
M
P
-
F
W
A
V
W
A
V
-
F
I
Z
P
I
Z
P
-
F
J
P
E
G
J
P
E