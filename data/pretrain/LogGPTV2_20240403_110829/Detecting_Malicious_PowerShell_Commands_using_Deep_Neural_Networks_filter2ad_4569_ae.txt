# Detecting Malicious PowerShell Commands using Deep Neural Networks

## Authors
- Danny Hendler, Ben-Gurion University, hendlerd@cs.bgu.ac.il
- Shay Kels, Microsoft, shkels@microsoft.com
- Amir Rubin, Ben-Gurion University, amirrub@cs.bgu.ac.il

## Abstract
Microsoft's PowerShell is a command-line shell and scripting language installed by default on Windows machines. Based on the .NET framework, it provides an interface for programmers to access operating system services. While PowerShell can be configured by administrators to restrict access and reduce vulnerabilities, these restrictions can be bypassed. Moreover, PowerShell commands can be dynamically generated, executed from memory, encoded, and obfuscated, making logging and forensic analysis challenging. For these reasons, PowerShell is increasingly used by cybercriminals as part of their attack toolchain, mainly for downloading malicious content and lateral movement. A recent comprehensive technical report by Symantec highlighted a sharp increase in the number of malicious PowerShell samples and penetration tools that use PowerShell. This underscores the urgent need to develop effective methods for detecting malicious PowerShell commands.

In this work, we address this challenge by implementing several novel detectors of malicious PowerShell commands and evaluating their performance. We implemented both "traditional" natural language processing (NLP) based detectors and detectors based on character-level convolutional neural networks (CNNs). Detectors' performance was evaluated using a large real-world dataset. Our evaluation results show that, although our detectors (and especially the traditional NLP-based ones) individually yield high performance, an ensemble detector that combines an NLP-based classifier with a CNN-based classifier provides the best performance, as the latter classifier is able to detect malicious commands that succeed in evading the former. Our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the CNN classifier are intrinsically difficult to detect using the NLP techniques we applied. Our detectors provide high recall values while maintaining a very low false positive rate, making us cautiously optimistic that they can be of practical value.

## 1. Introduction
Modern society is more dependent than ever on digital technology, with vital sectors such as healthcare, energy, transportation, and banking relying on networks of digital computers to facilitate their operations. At the same time, stakes are high for cybercriminals and hackers to penetrate computer networks for stealthily manipulating victims' data or wreaking havoc in their files and requesting ransom payments. Protecting the ever-growing attack surface from determined and resourceful attackers requires the development of effective, innovative, and disruptive defensive techniques.

One trend in modern cyber warfare is the reliance of attackers on general-purpose software tools that already preexist at the attacked machine. Microsoft PowerShell is a command-line shell and scripting language that, due to its flexibility, powerful constructs, and ability to execute scripts directly from the command line, has become a tool of choice for many attackers. Several open-source frameworks, such as PowerShell Empire and PowerSploit, have been developed to facilitate post-exploitation cyber-offense usage of PowerShell scripting.

While some work has been done on detecting malicious scripts such as JavaScript, PowerShell, despite its prominent status in cyber warfare, is relatively untreated by the academic community. Most of the work on PowerShell is done by security practitioners at companies such as Symantec and Palo Alto Networks. These publications focus mainly on surveying the PowerShell threat rather than on developing and evaluating approaches for detecting malicious PowerShell activities. The discrepancy between the lack of research on automatic detection of malicious PowerShell commands and the high prevalence of PowerShell-based malicious cyber activities highlights the urgent need for developing effective methods for detecting this type of attacks.

Recent scientific achievements in machine learning, and deep learning in particular, provide many opportunities for developing new state-of-the-art methods for effective cyber defense. Since PowerShell scripts contain textual data, it is natural to consider their analysis using various methods developed within the Natural Language Processing (NLP) community. Indeed, NLP techniques have been applied to the sentiment analysis problem, as well as to the problem of detecting malicious non-PowerShell scripts. However, adapting NLP techniques for detecting malicious scripts is not straightforward, since cyber attackers deliberately obfuscate their script commands to evade detection.

In the context of NLP sentiment analysis, deep learning methods considering text as a stream of characters have gained recent popularity and have been shown to outperform state-of-the-art methods. To the best of our knowledge, our work is the first to present an ML-based (and, more specifically, deep-learning-based) detector of malicious PowerShell commands. Motivated by recent successes of character-level deep learning methods for NLP, we also take this approach, which is compelling in view of existing and future obfuscation attempts by attackers that may foil extraction of high-level features.

We develop and evaluate several ML-based methods for the detection of malicious PowerShell commands. These include detectors based on novel deep learning architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), as well as detectors based on more traditional NLP approaches such as linear classification on top of character n-grams and bag-of-words. Detecting malicious PowerShell commands within the high volume of benign PowerShell commands used by administrators and developers is challenging. We validate and evaluate our detectors using a large dataset consisting of 60,098 legitimate PowerShell commands executed by users in Microsoft’s corporate network and 5,819 malicious commands executed on virtual machines deliberately infected by various types of malware, as well as 471 malicious commands obtained by other means, contributed by Microsoft security experts.

### Contributions
The contributions of our work are two-fold. First, we address the important and yet under-researched problem of detecting malicious PowerShell commands. We present and evaluate the performance of several novel ML-based detectors and demonstrate their effectiveness on a large real-world dataset. Secondly, we demonstrate the effectiveness of character-level deep learning techniques for the detection of malicious scripting. Our evaluation results establish that, although traditional NLP-based approaches yield high detection performance, ensemble learning that combines traditional NLP models with deep learning models further improves performance by detecting malicious commands that succeed in evading traditional NLP techniques. Since the character-level deep learning approach is intrinsically language-independent, we expect it can be easily adapted for detecting malicious usage of other scripting languages.

The rest of this paper is organized as follows. In Section 2, we provide background on PowerShell and how it is used as an attack vector and on some concepts required for understanding our deep-learning-based detectors. In Section 3, we describe our dataset, how we pre-process commands, and how our training set is constructed. A description of our detectors is provided in Section 4, followed by an evaluation of their performance in Section 5. Key related work is surveyed in Section 6. We conclude with a summary of our results and a short discussion of avenues for future work in Section 7.

## 2. Background

### 2.1 PowerShell
Introduced by Microsoft in 2006, PowerShell is a highly flexible system shell and scripting technology used mainly for task automation and configuration management. Based on the .NET framework, it includes two components: a command-line shell and a scripting language. It provides full access to critical Windows system functions such as the Windows Management Instrumentation (WMI) and the Component Object Model (COM) objects. Additionally, as it is compiled using .NET, it can access .NET assemblies and DLLs, allowing it to invoke DLL/assembly functions. These built-in functionalities give PowerShell many strong capabilities such as downloading content from remote locations, executing commands directly from memory, and accessing local registry keys and scheduled tasks. A detailed technical discussion of these capabilities can be found in [16].

As typical of scripting languages, PowerShell commands can either be executed directly via the command line or as part of a script. PowerShell’s functionality is greatly extended using thousands of ‘cmdlets’ (command-lets), which are basically modular and reusable scripts, each with its own designated functionality. Many cmdlets are built into the language (such as the Get-Process and Invoke-Command cmdlets), but additional cmdlets can be loaded from external modules to further enrich the programmer’s capabilities. The Get-Process cmdlet, for instance, when given the name of a machine that can be accessed in the context in which PowerShell is executed, returns the list of processes that are running on that machine. As another example, the Invoke-Command cmdlet executes the command provided as its input either locally or on one or more remote computers, depending on arguments. The Invoke-Expression cmdlet provides similar functionality but also supports evaluating and running dynamically-generated commands.

#### 2.1.1 PowerShell as an Attack Vector
While PowerShell can be configured and managed by the company IT department to restrict access and reduce vulnerabilities, these restrictions can be easily bypassed, as described by Symantec’s comprehensive report about the increased use of PowerShell in attacks [1]. Furthermore, logging the code executed by PowerShell can be difficult. While logging the commands provided to PowerShell can be done by monitoring the shell that executes them, this does not necessarily provide the visibility required for detecting PowerShell-based attacks, since PowerShell commands may use external modules and/or invoke commands using dynamically-defined environment variables. For instance, the Kovter trojan [17] uses simple, randomly generated innocent-looking environment variables in order to invoke a malicious script. One such command that appears in our dataset is “IEX $env:iu7Gt”, which invokes a malicious script referenced by the “iu7Gt” environment variable. A log of the executing shell would only show the command before its dynamic interpretation but will not provide any data regarding the malicious script.

Although Microsoft improved the logging capabilities of PowerShell 5.0 in Windows 10 by introducing the AntiMalware Scan Interface (AMSI) generic interface [18], many methods of bypassing it have already been published [19, 1], thus effective forensic analysis of malicious PowerShell scripts remains challenging.

In addition to the difficulty of forensic analysis, malware authors have several other good reasons for using PowerShell as part of their attacks [1]. First, since PowerShell is installed by default on all Windows machines, its strong functionality may be leveraged by cybercriminals, who often prefer using pre-installed tools for quicker development and for staying under the radar. Moreover, PowerShell is almost always whitelisted since it is benignly used by Windows system administrators [16]. Secondly, as PowerShell is able to download remote content and to execute commands directly from memory, it is a perfect tool for conducting file-less intrusions [20] in order to evade detection by conventional anti-malware tools. Finally, as we describe next, there are multiple easy ways in which PowerShell code can be obfuscated.

### 2.1.2 PowerShell Code Obfuscation
As described in [1], there are numerous ways of obfuscating PowerShell commands, many of which were implemented by Daniel Bohannon in 2016 and are publicly available in the “Invoke-Obfuscation” module he created [21]. Figure 1 lists a few key obfuscation methods we encountered in our data and provides examples of their usage. We now briefly explain each of them:

1. **Case Alternation**: PowerShell commands are not case-sensitive, so alternating lower and uppercase letters often appear in malicious commands.
2. **Command Flag Shortening**: Command flags may often be shortened to their prefixes. For instance, the “-noprofile” flag that excludes a PowerShell command from the execution policy can be shortened to “-nop”.
3. **Encoded Commands**: Commands may be executed using the “-EncodedCommand” switch. While the design goal for this feature was to provide a way of wrapping DOS-unfriendly commands, it is often used by malicious code for obfuscation.
4. **Invoke-Command**: The “Invoke-Command” cmdlet evaluates a PowerShell expression represented by a string and can therefore be used for executing dynamically-generated commands.
5. **ASCII Representation**: Characters can be represented by their ASCII values using “[char]ASCII-VALUE” and then concatenated to create a command or an operand.
6. **Base64 Encoding**: Commands may be base-64-encoded and then converted back to a string using the “FromBase64String” method.
7. **Encoding Variations**: Base64 strings can be encoded/decoded in various ways (UTF8, ASCII, Unicode).
8. **Ignored Characters**: Yet another way of obfuscating commands is to insert characters that are disregarded by PowerShell, such as backticks (`).
9. **Real-Time Manipulation**: Command strings may be manipulated in real-time before evaluation using replacement and concatenation functions.
10. **Environment Variable Concatenation**: The values of environment variables can be concatenated in run-time to generate a string whose content will be executed.
11. **Random Environment Variables**: Some malware generate environment variables with random names in every command execution.

While the ability to encode/represent commands in different ways and generate them dynamically at run-time provides greater programming flexibility, Figure 1 illustrates that this flexibility can be easily misused. As observed by [1], “These [obfuscation] methods can be combined and applied recursively, generating scripts that are deeply obfuscated on the command line.”

### 2.2 Deep Learning
In this section, we provide background on deep learning concepts and architectures that are required for understanding the deep-learning-based malicious PowerShell command detectors that we present in Section 4.

#### 2.2.1 Artificial Neural Networks
Artificial Neural Networks (ANNs) are a family of machine learning models inspired by biological neural networks, composed of a collection of interconnected artificial neurons, organized in layers. A typical ANN is composed of a single input layer, a single output layer, and one or more hidden layers. When the network is used for classification, outputs typically quantify class probabilities. A Deep Neural Network (DNN) has multiple hidden layers. There are several key DNN architectures, and the following subsections provide more details on those used by our detectors.

#### 2.2.2 Convolutional Neural Networks (CNNs)
A CNN is a learning architecture, traditionally used in computer vision [24, 25]. We proceed by providing a high-level description of the major components from which the CNN deep networks we use are composed. As its name implies, the main component of a CNN is a convolutional layer. Assuming for simplicity that our input is a 2D grayscale image, a convolutional layer uses 2D k×k “filters” (a.k.a. “kernels”), for some integer k. As the filter slides over the 2D input matrix, the dot product between its k×k weights and the corresponding k×k window in the input is computed. Intuitively, the filter slides over the input in order to search for the occurrences of some feature or pattern. Formally, given a k×k filter, for each k×k window x of the input to which the filter is applied, we calculate wT · x + b, where w is the filter’s weights matrix and b is a bias vector representing the constant term of the computed linear function. The k^2 weights of w, as well as the k values of b, are being learned during the training process.

Filters slide over the input in strides, whose size is specified in pixels. Performing the aforementioned computation for a single filter sliding over the entire input using strides results in an output of dimensions \((\frac{n-k}{s} + 1) \times (\frac{n-k}{s} + 1)\), called the filter’s “activation map”. Using l filters and stacking their activation maps results in the full output of the convolutional layer, whose dimensions are \((\frac{n-k}{s} + 1) \times (\frac{n-k}{s} + 1) \times l\).

To maintain the non-linear properties of the network when using multiple convolutional layers, a non-linear layer (a.k.a. activation layer) is added between each pair of convolutional layers. The non-linear layer applies a non-linear activation function such as the Rectified Linear Units (ReLU) function \(f(x) = \max(0, x)\) or the hyperbolic tangent \(f(x) = \tanh(x)\) function.

A max pooling layer [27] “down-samples” neurons in order to generalize and reduce overfitting [28]. It applies a k×k window across the input and outputs the maximum value within the window, thus reducing the number of parameters by a factor of k^2.

A fully connected layer connects all inputs to all outputs. Intuitively, each output neuron of the convolutional layers represents an image feature. These features are often connected to the network’s outputs via one or more fully connected layers, where the weights between inputs and outputs (learned during the training process) determine the extent to which each feature is indicative of each output class.

Dropout layers [29] can be used between fully connected layers in order to probabilistically reduce overfitting. Given a probability parameter p, at each training stage, each node in the input remains in the network with probability p or is “dropped out” (and is disconnected from outputs) with probability 1−p. Dropout layers, as well as fully connected layers, may also appear in recurrent neural networks, described next.

#### 2.2.3 Recurrent Neural Networks (RNNs)
RNNs are neural networks able to process sequences of data representing, e.g., text [30, 31], speech [32, 33, 34], handwriting [35], or video [36] in a recurrent manner, that is, by repeatedly using the input seen so far in order to process new input. We use an RNN network composed of long short-term memory (LSTM) blocks [37]. Each such block consists of a cell that stores hidden state, able to aggregate/summarize inputs received over an extended period of time. In addition to the cell, an LSTM block contains three components called gates that control and regulate information flow into and out of the cell. Roughly speaking, the input gate determines the extent to which new input is used by the cell, the forget gate determines the extent to which the cell retains memory, and the output gate controls the level to which the cell’s value is used to compute the block’s output.

In the context of text analysis, a common practice is to add an embedding layer before the LSTM layer [38, 39]. Embedding layers serve two purposes. First, they reduce the dimensionality of the input. Secondly, they represent input in a manner that retains its context. The embedding layer converts each input token (typically a word or a character, depending on the problem at hand) to a vector representation. For example, when taking a character-level approach, one can expect that the representations of all digits computed by the embedding layer will be vectors that are close to each other. When the problem benefits from a word-level representation, word2vec [40] embeddings represent each word as a vector such that words that share common contexts in the text corpus using which the model was trained are represented by vectors that are close to each other.

A bidirectional RNN (BRNN) network [41] is an RNN architecture in which two RNN layers are connected to the output, one reading the input in order and the other reading it in reverse order. Intuitively, this allows the output to be computed based on information from both past and future states. BRNNs have found successful applications in various fields [42, 43, 44]. For instance, in the context of the sentiment analysis problem, when processing text from the middle of a sentence, text seen in the beginning of the sentence, as well as text seen at the end of the sentence, may be used by the computation.

## 3. The Dataset
Our work is based on a large dataset which, after pre-processing (which we shortly describe), consists of 66,388 distinct PowerShell commands, 6,290 labeled as malicious and 60,098 labeled as clean. Malicious dataset commands belong to two types. For training and cross-validation, we use 5,819 distinct commands obtained by executing known malicious programs in a sandbox and recording all PowerShell commands executed by the program. For testing, we used 471 malicious PowerShell commands seen in the course of May 2017, contributed by Microsoft security experts. Using this latter type of malicious instances for evaluating our detection results mimics a realistic scenario, in which the detection model is trained using data generated inside a sandbox and is then applied to commands executed on regular machines.

As for clean commands, we received from Microsoft a collection of PowerShell commands executed within Microsoft’s corporate network in the course of May 2017, on machines which had no indication of malware infection thirty days prior to the execution of the PowerShell command. Clean commands were split into 48,094 for training and cross-validation and 12,004 for testing.

### 3.1 Pre-processing & Training Set Construction
We implemented a preprocessor whose key goals are to perform PowerShell command decoding and normalization for improved detection results. It also eliminates identical (as well as “almost identical”) commands in order to reduce the probability of data leakage.

First, in order to be able to apply detection on “cleartext,” our preprocessor decodes PowerShell commands that are encoded using base-64. Such commands are identified by the -EncodedCommand flag (or any prefix of it starting with ‘-e’ or ‘-E’). All these commands undergo base-64 decoding; otherwise, they provide no useful detection data. Next, the preprocessor normalizes commands in order to reduce the probability of a data leakage problem [45] that, in our setting, may result from using almost-identical commands for training the model and for validating it. Indeed, we observed in our dataset PowerShell commands that differ only in a very small number of characters. In most cases, this was due to either the use of different IP addresses or the use of different numbers/types of whitespace characters (e.g., spaces, tabs, and newlines) in otherwise-identical commands. To avoid this problem, we replaced all numbers with asterisk signs (‘*’) and all contiguous sequences of whitespace characters to a single space and then eliminated duplicates.

We also observed in our dataset PowerShell case-equivalent commands that only differ in letter casing (see entry 1 in Figure 1). This was dealt with by ensuring that only a single command from each case-equivalence class is used for training/validation. We note that the dimensions of the dataset specified earlier relate to the numbers of distinct commands after this pre-processing stage.

Our dataset is very imbalanced, as the number of clean commands is an order of magnitude larger than that of malicious commands. In order to prevent model bias toward the larger class, we constructed the training set by duplicating each malicious command used for training 8 times so that the ratio of clean/malicious training commands is 1:1. We preferred to handle imbalance this way rather than by using under-sampling in order to avoid the risk of overfitting, which may result when a neural network is trained using a small number of examples.

## 4. Detection Models
In this section, we describe the machine learning models we used for malicious PowerShell command detection. We then evaluate and compare their performance in Section 5. We implemented several deep-learning-based detectors. In order to assess the extent to which they are able to compete with more traditional detection approaches, we also implemented detectors that are based on traditional NLP-based methods. We proceed by describing these two sets of models.

### 4.1 Deep-Learning Based Detectors

#### 4.1.1 Input Preparation
Neural networks are optimized for classification tasks where inputs are given as raw signals [24, 25]. Using these networks for text classification requires encoding the text so that the network can process it. Zhang et al. [46] explored treating text as a “raw signal at the character level” and applying a one-dimensional CNN for text classification. We take a similar approach for classifying PowerShell commands as either malicious or benign.

First, we select which characters to encode. We do this by counting for each character the number of training set commands in which it appears and then assigning a code only to characters that appear in at least 1.4% of these commands. We have set the encoding threshold to this value because at this point, there is a sharp decline in character frequency. Thus, the least-frequent character encoded (which is `) appeared in approximately 1.4% of commands, and the most-frequent character that was not encoded (which is a non-English character) appeared in only approximately 0.3% of the training set commands. Rare characters are not assigned a code in order to reduce dimensionality and overfitting probability.

The result is a set of 61 characters, containing the space symbol, all lower-case English letters (we soon explain how we represent upper-case letters), and the following symbols: `-’!%&()*,./:;?@[\]`{|}+^u#$^~”`

Similarly to [46], we use an input feature length of 1,024, so if a command is longer than that, it is truncated. This reduces network dimensions and, as shown by our evaluation in Section 5.2, suffices to provide high-quality classification. The input to the CNN network is then prepared by using “one-hot” encoding of command characters, that is, by converting each character of the (possibly truncated) command to a vector, all of whose first 61 entries are 0 except for the single entry corresponding to the character’s code. All characters that were not assigned a code are skipped.

In practice, we use 62-long vectors rather than 61-long vectors in order to deal with the casing of English letters. Unlike in most NLP classification tasks, in the context of PowerShell commands, character casing may be a strong signal (see obfuscation method 1 in Figure 1). In order to retain casing information in our encoding, we add a “case bit,” which is the 62nd vector entry. The bit is set to 1 if the character is an upper-case English letter and is set to 0 otherwise. Thus, the representation of a PowerShell command that is being input to the CNN network is a 62x1,024 sparse matrix. A matrix representing a command that is shorter than 1,024 is padded with an appropriate number of zero columns.

As we described in Section 2.2, whereas CNNs are traditionally used for computer vision and therefore typically receive as their input a matrix representing an image, recurrent neural networks (RNNs) are optimized for processing sequences of data. Consequently, the input we provide to our RNN classifier is a vector of numbers of size at most 1,024, whose i-th element is the code (as described above) of the i-th command character (characters that were not assigned a code are skipped), except that we explicitly encode upper-case English letters since we cannot use a case bit for the RNN input representation.

#### 4.1.2 Training
Stochastic gradient descent is the most widely-used method for training deep learning models [47]. We train our deep-learning-based algorithms using mini-batch gradient descent, in which each training epoch (a complete pass over the training set) is subdivided into several mini-batches such that the gradient is computed (and network coefficients are updated accordingly) for each mini-batch. In order to compare all our deep-learning networks on the same basis, in all our experiments, we used 16 training epochs and a mini-batch size of 128. We also experimented with other numbers of epochs/mini-batches, but none of them obtained significantly better classification results.

#### 4.1.3 Detection Models
We implemented and evaluated three deep-learning-based detectors described in the following:

1. **9-layer CNN (9-CNN)**: We use the network architecture designed by [46], consisting of 6 convolutional layers with stride 1, followed by 2 fully connected layers and the output layer. Two dropout layers are used between the 3 fully connected layers, and a max pooling layer follows the first, second, and last convolutional layers. Unlike the architecture of [46] that uses fully connected layers of size 1,024 or 2,048, we use 256 entries in each such layer as this provides better performance on our data.

2. **4-layer CNN (4-CNN)**: We also implemented a shallower version of the 9-CNN architecture whose structure is depicted in Figure 2. It contains a single convolutional layer with 128 kernels of size 62x3 and stride 1, followed by a max pooling layer of size 3 with no overlap. This is followed by two fully-connected layers, both of size 1,024—each followed by a dropout layer with a probability of 0.5 (not shown in Figure 2), and an output layer.

3. **LSTM**: We implemented a recurrent neural network model composed of LSTM blocks and used the character-level representation described above. Since inputs are not sentences of a natural language, we decided not to use Word2Vec [48] embedding. Instead, our LSTM architecture contains an embedding layer of size 32. The LSTM blocks we used are bi-directional LSTM cells with an output dimension of 256, followed by two fully-connected layers, both of size 256, using a dropout probability of 0.5.

### 4.2 Traditional NLP-based Detectors
We used two types of NLP feature extraction methods—a character-level 3-gram and a bag of words (BoW). In both, we evaluated both tf and tf-idf and then applied a logistic regression classifier on extracted features. The 3-gram model performed better using tf-idf, whereas BoW performed better using tf. For each detector, we selected the hyper-parameters which gave the best cross-validation AUC results (evaluation results are presented in Section 5). Note that the 4-CNN architecture uses a kernel of length three in the first convolutional layer, the features it uses are similar to those extracted when using the character-level 3-gram detector.

### 4.3 Input Representation Considerations
Recalling the obfuscation methods used by PowerShell-based malware authors for avoiding detection (see Section 2.1.1), we observe that our input representation retains the information required for identifying them. The commands used for obfuscation, including their short versions (obfuscation method 2 in Figure 1), can be learned due to the usage of 3-sized kernels by the deep-learning models and the usage of 3-grams by the traditional NLP models. Obfuscation method 3 is addressed by the decoding performed during data preparation (see Section 3.1). Most other obfuscation methods (see Figure 1) use special characters such as “`”, the pipe sign “|”, the symbol “+”, and the environment-variable sign “$”. These special characters are often used when strings and the values of environment variables are concatenated in runtime for obfuscation. All these special characters appear in a significant fraction of our training set’s commands and consequently are all assigned codes by our input encoding for deep networks. They are also retained in the input provided to the traditional NLP models.

As for the usage of random names (obfuscation method 11), these typically include numbers (converted to the ‘*’ sign) or alternating casing, and can therefore be learned by our classifiers as well. (As we describe later, our deep learning classifiers do a better job in learning such patterns.) The usage of special strings such as “`[char]`”, “UTF8”, “Base64”, or the character ‘`’ is also covered by both models as they are retained in the input.

The only obfuscation method with respect to which the input to some of our detectors is superior to that provided to others is the usage of alternating lower/uppercase characters (obfuscation method 1 in Figure 1). Whereas the case-bit was easily incorporated in the input to our CNN deep-learning classifiers, the RNN and the traditional NLP-based models' input representations do not accommodate its usage.

## 5. Evaluation
We performed 2-fold cross-validation on the training data and present the area under the ROC curve (AUC) results (rounded to the third decimal place) of our detectors in Table 1. In addition to the five detectors presented in Section 4, we also evaluated a variant of 4-CNN (denoted 4-CNN*) in which we did not use the case bit. All detectors obtain very high AUC levels in the range 0.985–0.990. The traditional NLP-based detectors provide excellent results in the range 0.989–0.990, the 4-CNN and LSTM detectors slightly lag behind with AUC of 0.988, and 9-CNN provides a lower AUC of 0.985. The 4-CNN* detector provides slightly lower AUC than that of 4-CNN, establishing that the case bit is beneficial.

For a detector to be practical, it must not produce many false alarms. As the cybersecurity domain is often characterized by a very high rate of events requiring classification, even a low false-positive rate (FPR) of (say) 1% may result in too many false alarms. It is therefore important to evaluate the true positive rate (TPR) (a.k.a. recall) provided by detectors when their threshold is set for low FPR levels. Table 2 presents the TPR of our detectors for FPR levels 10^-2, 10^-3, and 10^-4 on both the training/cross-validation and the test sets. Since we have a total of about 12,000 clean commands in the test set, we stop the analysis at an FPR level of 10^-4. Presented values in the “Cross-validation” part of the table are the average of the two folds. Values in the “Test set” part were obtained by models trained on the training set in its entirety.

Focusing first on cross-validation results, it can be seen that, while all classifiers achieve high TPR values even for very low FPR levels, the performance of the traditional NLP detectors is better. The 3-gram detector leads in all FPR levels with a gap that increases when FPR values are decreased. Specifically, even for an FPR of 1:10,000, it provides an excellent TPR of 0.95. Among the deep-learning-based detectors, 4-CNN and LSTM are superior to 4-CNN* and 9-CNN. For an FPR rate of 1:10,000, 4-CNN and LSTM provide TPRs of 0.89 and 0.85, respectively. 9-CNN obtains the worst results in all experiments. Results on the test set are significantly lower but still good. It is noteworthy that the gaps between the traditional NLP and the 4-CNN/LSTM models that we observed on the training data almost vanish on the test data. This seems to indicate that the latter models are able to generalize better. For an FPR of 1:100, the best performers are 4-CNN and 4-CNN* with a TPR of 0.89, LSTM is second best with 0.88, and both the 3-gram and BoW detectors obtain a TPR of 0.87. For FPR 1:1,000, the 3-gram detector is best with TPR of 0.83, only slightly better than LSTM’s 0.81 TPR, and for FPR 1:10,000, all of 3-gram, 4-CNN, and LSTM (ordered in decreasing performance) identify approximately two-thirds of malicious commands. The significance of the case bit is evident when comparing the results of the 4-CNN and the 4-CNN* detectors on the test set for FPR level of 1:10,000. The TPR when using the case bit (4-CNN) is higher by almost one-third than that when it is not used (4-CNN*). 9-CNN is the worst performer also in the test set experiments, by a wider margin than in the cross-validation tests.

As we’ve mentioned, the performance on the test set is significantly lower than that of cross-validation in all experiments. This is to be expected: whereas training set malicious commands were generated by running malware inside a sandbox, the malicious commands in the test set were contributed by security experts. Consequently, test set malicious commands may have been collected in different ways (e.g., by searching the Windows registry for malicious PowerShell commands) and may have been produced by malware none of whose commands are in the training set.

### 5.1 A Deep/Traditional Models Ensemble
We next show that by combining 4-CNN—our best deep learning model and 3-gram—our best traditional NLP model, we are able to obtain detection results that are better than those of each of them separately. We then analyze the type of malicious commands for which the deep model contributes to the traditional NLP one.

The D/T Ensemble is constructed as follows. We classify a command using both the 4-CNN and the 3-gram detectors, thus receiving two scores. If either one of the scores is 0.99 or higher, we take the maximum score; otherwise, we take the average of the two scores. We evaluated the Ensemble’s TPR by FPR performance on the test set in the same manner we evaluated the non-Ensemble algorithms (see Table 2). The D/T Ensemble significantly outperformed all non-Ensemble algorithms and obtained on the test set TPRs of 0.92, 0.89, and 0.72 for FPR levels of 1:100, 1:1,000, and 1:10,000, respectively.

In order to gain better visibility into the contribution of the 4-CNN detector on top of the 3-gram detector, we present in Figures 3a-3c the confusion matrices of the 3-gram, 4-CNN, and D/T Ensemble detectors on the test set. These results are obtained using the lowest threshold (for each of the algorithms) that provides an FPR of no more than 10^-3. Since the test set contains approximately 12,000 clean instances, this means that the algorithms must have at most 12 false positives.

As can be seen by comparing Figures 3a and 3c, the D/T Ensemble adds 42 new detections on top of those made by the 3-gram detector, with only 4