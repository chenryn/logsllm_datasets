:
v
i
X
r
a
DetectingMaliciousPowerShellCommandsusingDeepNeuralNetworksDannyHendlerBen-GurionUniversityhendlerd@cs.bgu.ac.ilShayKelsMicrosoftshkels@microsoft.comAmirRubinBen-GurionUniversityamirrub@cs.bgu.ac.ilAbstractMicrosoft’sPowerShellisacommand-lineshellandscriptinglanguagethatisinstalledbydefaultonWin-dowsmachines.BasedonMicrosoft’s.NETframe-work,itincludesaninterfacethatallowsprogram-merstoaccessoperatingsystemservices.WhilePow-erShellcanbeconﬁguredbyadministratorsforre-strictingaccessandreducingvulnerabilities,theserestrictionscanbebypassed.Moreover,PowerShellcommandscanbeeasilygenerateddynamically,ex-ecutedfrommemory,encodedandobfuscated,thusmakingtheloggingandforensicanalysisofcodeex-ecutedbyPowerShellchallenging.Forallthesereasons,PowerShellisincreasinglyusedbycybercriminalsaspartoftheirattacks’toolchain,mainlyfordownloadingmaliciouscontentsandforlateralmovement.Indeed,arecentcomprehensivetechnicalreportbySymantecdedicatedtoPower-Shell’sabusebycybercrimials[1]reportedonasharpincreaseinthenumberofmaliciousPowerShellsam-plestheyreceivedandinthenumberofpenetrationtoolsandframeworksthatusePowerShell.Thishigh-lightstheurgentneedofdevelopingeﬀectivemethodsfordetectingmaliciousPowerShellcommands.Inthiswork,weaddressthischallengebyimple-mentingseveralnoveldetectorsofmaliciousPower-Shellcommandsandevaluatingtheirperformance.Weimplementedboth“traditional”naturallan-guageprocessing(NLP)baseddetectorsanddetec-torsbasedoncharacter-levelconvolutionalneuralnetworks(CNNs).Detectors’performancewaseval-uatedusingalargereal-worlddataset.Ourevaluationresultsshowthat,althoughourdetectors(andespeciallythetraditionalNLP-basedones)individuallyyieldhighperformance,anensem-bledetectorthatcombinesanNLP-basedclassiﬁerwithaCNN-basedclassiﬁerprovidesthebestperfor-mance,sincethelatterclassiﬁerisabletodetectma-liciouscommandsthatsucceedinevadingtheformer.OuranalysisoftheseevasivecommandsrevealsthatsomeobfuscationpatternsautomaticallydetectedbytheCNNclassiﬁerareintrinsicallydiﬃculttodetectusingtheNLPtechniquesweapplied.Ourdetectorsprovidehighrecallvalueswhilemaintainingaverylowfalsepositiverate,makinguscautiouslyoptimisticthattheycanbeofpracticalvalue.1IntroductionModernsocietyismorethaneverdependentondigi-taltechnology,withvitalsectorssuchashealth-care,energy,transportationandbankingrelyingonnet-worksofdigitalcomputerstofacilitatetheiroper-ations.Atthesametime,stakesarehighforcy-bercriminalsandhackerstopenetratecomputernet-worksforstealthilymanipulatingvictims’data,orwreakinghavocintheirﬁlesandrequestingransompayments.Protectingtheever-growingattacksur-facefromdeterminedandresourcefulattackersre-quiresthedevelopmentofeﬀective,innovativeanddisruptivedefensetechniques.Oneofthetrendsinmoderncyberwarfareistherelianceofattackersongeneral-purposesoftware1toolsthatalreadypreexistattheattackedmachine.MicrosoftPowerShell1isacommand-lineshellandscriptinglanguagethat,duetoitsﬂexibility,power-fulconstructsandabilitytoexecutescriptsdirectlyfromthecommand-line,becameatoolofchoiceformanyattackers.Severalopen-sourceframeworks,suchasPowerShellEmpire2andPowerSploit3havebeendevelopedwiththepurposeoffacilitatingpost-exploitationcyber-oﬀenceusageofPowerShellscript-ing.Whilesomeworkhasbeendoneondetectingma-liciousscriptssuchasJavaScript[2,3,4,5],Power-Shell,despiteitsprominentstatusinthecyberwar-fare,isrelativelyuntreatedbytheacademiccommu-nity.MostoftheworkonPowerShellisdonebyse-curitypractitionersatcompaniessuchasSymantec[1]andPaloAltoNetworks[6].Thesepublicationsfo-cusmainlyonsurveyingthePowerShellthreat,ratherthanondevelopingandevaluatingapproachesforde-tectingmaliciousPowerShellactivities.Thediscrep-ancybetweenthelackofresearchonautomaticde-tectionofmaliciousPowerShellcommandsandthehighprevalenceofPowerShell-basedmaliciouscyberactivitieshighlightstheurgentneedofdevelopingef-fectivemethodsfordetectingthistypeofattacks.Recentscientiﬁcachievementsinmachinelearningingeneral,anddeeplearning[7]inparticular,pro-videmanyopportunitiesfordevelopingnewstate-of-the-artmethodsforeﬀectivecyberdefense.SincePowerShellscriptscontaintextualdata,itisnatu-raltoconsidertheiranalysisusingvariousmethodsdevelopedwithintheNaturalLanguageProcessing(NLP)community.Indeed,NLPtechniqueswereappliedforthesentimentanalysisproblem[8],aswellasfortheproblemofdetectingmaliciousnon-PowerShellscripts[5].However,adaptingNLPtech-niquesfordetectingmaliciousscriptsisnotstraight-forward,sincecyberattackersdeliberatelyobfuscatetheirscriptcommandsforevadingdetection[1].InthecontextofNLPsentimentanalysis,deeplearningmethodsconsideringtextasastreamofcharactershavegainedrecentpopularityandhavebeenshowntooutperformstateofartmethods1https://docs.microsoft.com/en-us/powershell/2https://www.powershellempire.com/3https://github.com/PowerShellMaﬁa/PowerSploit[9,10].Tothebestofourknowledge,ourworkistheﬁrsttopresentanML-based(and,morespeciﬁ-cally,deep-learningbased)detectorofmaliciousPow-erShellcommands.Motivatedbyrecentsuccessesofcharacter-leveldeeplearningmethodsforNLP,wetootakethisapproach,whichiscompellinginviewofexistingandfutureobfuscationattemptsbyattackersthatmayfoilextractionofhigh-levelfeatures.WedevelopandevaluateseveralML-basedmeth-odsforthedetectionofmaliciousPowerShellcom-mands.TheseincludedetectorsbasedonnoveldeeplearningarchitecturessuchasConvolutionalNeuralNetworks(CNNs)[11,12]andRecurrentNeuralNet-works(RNNs)[13],aswellasdetectorsbasedonmoretraditionalNLPapproachessuchaslinearclassiﬁca-tionontopofcharactern-gramsandbag-of-words[14].DetectingmaliciousPowerShellcommandswithinthehighvolumeofbenignPowerShellcommandsusedbyadministratorsanddevelopersischalleng-ing.Wevalidateandevaluateourdetectorsusingalargedataset4consistingof60,098legitimatePow-erShellcommandsexecutedbyusersinMicrosoft’scorporatenetworkandof5,819maliciouscommandsexecutedonvirtualmachinesdeliberatelyinfectedbyvarioustypesofmalware,aswellasof471maliciouscommandsobtainedbyothermeans,contributedbyMicrosoftsecurityexperts.ContributionsThecontributionsofourworkaretwo-fold.First,weaddresstheimportantandyetunder-researchedproblemofdetectingmaliciousPowerShellcommands.WepresentandevaluatetheperformanceofseveralnovelML-baseddetectorsanddemonstratetheireﬀectivenessonalargereal-worlddataset.Secondly,wedemonstratetheeﬀectivenessofcharacter-leveldeeplearningtechniquesforthede-tectionofmaliciousscripting.Ourevaluationresultsestablishthat,althoughtraditionalNLP-basedap-proachesyieldhighdetectionperformance,ensem-blelearningthatcombinestraditionalNLPmodelswithdeeplearningmodelsfurtherimprovesperfor-mancebydetectingmaliciouscommandsthatsuc-4Usersensitivedatawasanonymized.2ceedinevadingtraditionalNLPtechniques.Sincethecharacter-leveldeeplearningapproachisintrinsicallylanguageindependent,weexpectitcanbeeasilyadaptedfordetectingmalicioususageofotherscriptinglanguages.Therestofthispaperisorganizedasfollows.InSection2,weprovidebackgroundonPowerShellandhowitisusedasanattackvectorandonsomecon-ceptsrequiredforunderstandingourdeep-learningbaseddetectors.InSection3,wedescribeourdataset,howwepre-processcommandsandhowourtrainingsetisconstructed.Adescriptionofourde-tectorsisprovidedinSection4,followedbyanevalu-ationoftheirperformanceinSection5.KeyrelatedworkissurveyedinSection6.Weconcludewithasummaryofourresultsandashortdiscussionofav-enuesforfutureworkinSection7.82Background2.1PowerShellIntroducedbyMicrosoftin2006,PowerShellisahighlyﬂexiblesystemshellandscriptingtechnologyusedmainlyfortaskautomationandconﬁgurationmanagement[15].Basedonthe.NETframework,itincludestwocomponents:acommand-lineshellandascriptinglanguage.Itprovidesfullaccesstocriti-calWindowssystemfunctionssuchastheWindowsManagementInstrumentation(WMI)andtheCom-ponentObjectModel(COM)objects.Also,asitiscompiledusing.NET,itcanaccess.NETassembliesandDLLs,allowingittoinvokeDLL/assemblyfunc-tions.Thesebuilt-infunctionalitiesgivePowerShellmanystrongcapabilitiessuchasdownloadingcontentfromremotelocations,executingcommandsdirectlyfrommemory,andaccessinglocalregistrykeysandscheduledtasks.Adetailedtechnicaldiscussionofthesecapabilitiescanbefoundin[16].Astypicalofscriptinglanguages,PowerShellcom-mandscanbeeitherexecuteddirectlyviathecom-mandline,oraspartofascript.PowerShell’sfunctionalityisgreatlyextendedusingthousandsof‘cmdlets’(command-lets),whicharebasicallymodularandreusablescripts,eachwithitsowndesignatedfunctionality.Manycmdletsarebuiltintothelanguage(suchastheGet-ProcessandInvoke-Commandcmdlets),butadditionalcmdletscanbeloadedfromexternalmodulestofurtherenrichtheprogrammer’scapabilities.TheGet-Processcmdlet,forinstance,whengivenanameofamachinewhichcanbeaccessedinthecontextinwhichPower-Shellisexecuted,returnsthelistofprocessesthatarerunningonthatmachine.Asanotherexample,theInvoke-Commandcmdletexecutesthecommandpro-videdasitsinputeitherlocallyorononeormoreremotecomputers,dependingonarguments.TheInvoke-Expressioncmdletprovidessimilarfunc-tionalitybutalsosupportsevaluatingandrunningdynamically-generatedcommands.2.1.1PowerShellasanAttackVectorWhilePowerShellcanbeconﬁguredandmanagedbythecompanyITdepartmenttorestrictaccessandre-ducevulnerabilities,theserestrictionscanbeeasilybypassed,asdescribedbySymantec’scomprehensivereportabouttheincreaseduseofPowerShellinat-tacks[1].Furthermore,loggingthecodeexecutedbyPowerShellcanbediﬃcult.Whileloggingthecom-mandsprovidedtoPowerShellcanbedonebymon-itoringtheshellthatexecutesthem,thisdoesnotnecessarilyprovidethevisibilityrequiredfordetect-ingPowerShell-basedattacks,sincePowerShellcom-mandsmayuseexternalmodulesand/orinvokecom-mandsusingdynamically-deﬁnedenvironmentvari-ables.Forinstance,theKovtertrojan[17]usessim-ple,randomlygeneratedinnocent-lookingenviron-mentvariablesinordertoinvokeamaliciousscript.Onesuchcommandthatappearsinourdatasetis“IEX$env:iu7Gt”,whichinvokesamaliciousscriptreferencedbythe“iu7Gt”environmentvariable.5Alogoftheexecutingshellwouldonlyshowthecom-mandbeforeitsdynamicinterpretation,butwillnotprovideanydataregardingthemaliciousscript.AlthoughMicrosoftimprovedtheloggingcapabil-itiesofPowerShell5.0inWindows10byintroducingtheAntiMalwareScanInterface(AMSI)genericin-5IEXisanaliasofInvoke-Expression.3terface[18],manymethodsofbypassingithaveal-readybeenpublished[19,1],thuseﬀectiveforensicanalysisofmaliciousPowerShellscriptsremainschal-lenging.Inadditiontothediﬃcultyofforensicanalysis,malwareauthorshaveseveralothergoodreasonsforusingPowerShellaspartoftheirattacks[1].First,sincePowerShellisinstalledbydefaultonallWin-dowsmachines,itsstrongfunctionalitymaybelever-agedbycybercriminals,whooftenpreferusingpre-installedtoolsforquickerdevelopmentandforstay-ingundertheradar.Moreover,PowerShellisalmostalwayswhitelistedsinceitisbenignlyusedbyWin-dowssystemadministrators[16].Secondly,asPowerShellisabletodownloadre-motecontentandtoexecutecommandsdirectlyfrommemory,itisaperfecttoolforconductingﬁle-lessintrusions[20]inordertoevadedetectionbycon-ventionalanti-malwaretools.Finally,aswedescribenext,therearemultipleeasywaysinwhichPower-Shellcodecanbeobfuscated.PowerShellCodeObfuscationAsdescribedin[1],therearenumerouswaysofobfuscatingPower-Shellcommands,manyofwhichwereimplementedbyDanielBohannonin2016andarepubliclyavail-ableinthe“Invoke-Obfuscation”modulehecreated[21].Figure1listsafewkeyobfuscationmethodsweencounteredinourdataandprovidesexamplesoftheirusage.Wenowbrieﬂyexplaineachofthem.1.AsPowerShellcommandsarenotcase-sensitive,alternatingloweranduppercaselettersoftenap-pearinmaliciouscommands.2.Commandﬂagsmayoftenbeshortenedtotheirpreﬁxes.Forinstance,the“-noprofile”ﬂagthatexcludesaPowerShellcommandfromtheexecutionpolicycanbeshortenedto“-nop”.3.Commandsmaybeexecutedusingthe“-EncodeCommand”switch.Whilethede-signgoalforthisfeaturewastoprovideawayofwrappingDOS-unfriendlycommands,itisoftenusedbymaliciouscodeforobfuscation.4.Asmentionedpreviously,the“Invoke-Command”cmdletevaluatesaPower-Shellexpressionrepresentedbyastringandcanthereforebeusedforexecutingdynamically-generatedcommands.5.CharacterscanberepresentedbytheirASCIIvaluesusing“[char]ASCII-VALUE”andthenconcatenatedtocreateacommandoranoperand.6.Commandsmaybebase-64-encodedandthenconvertedbacktoastringusingthe“FromBase64String”method.7.Base64stringscanbeencoded/decodedinvari-ousways(UTF8,ASCII,Unicode).8.YetanotherwayofobfuscatingcommandsistoinsertcharactersthataredisregardedbyPower-Shellsuchas`.9.Commandstringsmaybemanipulatedinreal-timebeforeevaluationusingreplacementandconcatenationfunctions.10.Thevaluesofenvironmentvariablescanbecon-catenatedinrun-timetogenerateastringwhosecontentwillbeexecuted.11.Somemalwaregenerateenvironmentvariableswithrandomnamesineverycommandexecu-tion.Whiletheabilitytoencode/representcommandsindiﬀerentwaysandgeneratethemdynamicallyatrun-timeprovidesforgreaterprogrammingﬂexibility,Figure1illustratesthatthisﬂexibilitycanbeeasilymisused.Asobservedby[1],“These[obfuscation]methodscanbecombinedandappliedrecursively,generatingscriptsthataredeeplyobfuscatedonthecommandline”.2.2DeepLearningInthissectionweprovidebackgroundondeeplearn-ingconceptsandarchitecturesthatisrequiredforun-derstandingthedeep-learningbasedmaliciousPow-erShellcommanddetectorsthatwepresentinSection4.4IDDescriptionExample1Usingalternatingloweranduppercaseletters-ExecUTIONPoLICyBypASs-wiNDoWSTYLehidDeN(NEW-objecTSYstEM.NET.wEbCLIeNt).DOWnLoADFiLE();2Usingshortﬂags-nop-whidden-e3Usingencodedcommands-EncodedCommand4Invokeexpressionusingitsstringrepresentation-Invoke-Expression(("New-ObjectNet.WebClient")).(’Downloadfile’)...5Using”[char]”insteadofacharacter...$cs=[char]71;$fn=$env:temp+$cs;...6Readingdatainbase64IEX$s=New-ObjectIO.MemoryStream([Convert]::FromBase64String(’’));7UsingUTF8encoding$f=[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String(’));...8InsertingcharactersoverlookedbyPowerShelllike`...(new-object-ComObjectwscript.shell).Popup(¨E-mail:@.com‘n‘nClient:"`)...9Stringmanipulation...$filename.Replace(’-’,’/’)...$env:temp+’:’+$name+’.exe...10Concatenatingvariablesinline$emnuxgy=’i’;$jrywuzq=’x’;$unogv=’e’;...Invoke-Expression($emnuxgy+$unogv+$jrywuzq+’’...);11Usingarandomnameforavariableineveryruniex$env:vruuygFigure1:ExamplesofPowerShellobfuscationmethods.ArtiﬁcialNeuralNetworks[22,23]areafamilyofmachinelearningmodelsinspiredbybiologicalneuralnetworks,composedofacollectionofinter-connectedartiﬁcialneurons,organizedinlayers.AtypicalANNiscomposedofasingleinputlayer,asingleoutputlayer,andoneormorehiddenlayers.Whenthenetworkisusedforclassiﬁcation,outputstypicallyquantifyclassprobabilities.ADeepNeuralNetwork(DNN)hasmultiplehiddenlayers.ThereareseveralkeyDNNarchitecturesandthefollowingsubsectionsprovidemoredetailsonthoseusedbyourdetectors.2.2.1ConvolutionalNeuralNetworks(CNNs)ACNNisalearningarchitecture,traditionallyusedincomputervision[24,25].Weproceedbyprovid-ingahigh-leveldescriptionofthemajorcomponentsfromwhichtheCNNdeepnetworksweusearecom-posed.Asitsnameimplies,themaincomponentofaCNNisaconvolutionallayer.Assumingforsimplicitythatourinputisa2Dgreyscaleimage,aconvolutionallayeruses2Dk×k“ﬁlters”(a.k.a.“kernels”),forsomeintegerk.Astheﬁlterisslidingoverthe2Din-putmatrix,thedotproductbetweenitsk×kweightsandthecorrespondingk×kwindowintheinputisbeingcomputed.Intuitively,theﬁlterslidesoverthe5inputinordertosearchfortheoccurrencesofsomefeatureorpatterm.Formally,givenak×kﬁlter,foreachk×kwindowxoftheinputtowhichtheﬁlterisapplied,wecalculatewT·x+b,wherewistheﬁlter’sweightsmatrixandbisabiasvectorrepresentingtheconstanttermofthecomputedlinearfunction.Thek2weightsofw,aswellasthekvaluesofb,arebeinglearntduringthetrainingprocess.Filtersslideovertheinputinstrides,whosesizeisspeciﬁedinpixels.Performingtheaforementionedcomputationforasingleﬁlterslidingovertheen-tireinputusingstridesresultsinanoutputofdi-mensions(cid:0)(n−k)/s+1(cid:1)×(cid:0)(n−k)/s+1(cid:1),calledtheﬁlter’s“activationmap”.Usinglﬁltersandstackingtheiractivationmapsresultsinthefullout-putoftheconvolutionallayer,whosedimensionsare(cid:0)(n−k)/s+1(cid:1)×(cid:0)(n−k)/s+1(cid:1)×l.Inordertomaintainthenon-linearpropertiesofthenetworkwhenusingmultipleconvolutionallayers,anon-linearlayer(a.k.a.activationlayer)isaddedbetweeneachpairofconvolutionallayers.Thenon-linearlayerappliesanon-linearactivationfunctionsuchastheRectiﬁedLinearUnits(ReLU)functionf(x)=max(0,x)whosepropertieswereinvestigatedby[26]orthehyperbolictangentf(x)=tanh(x)function.Amaxpoolinglayer[27]“down-samples”neuronsinordertogeneralizeandreduceoverﬁtting[28].Itappliesak×kwindowacrosstheinputandoutputsthemaximumvaluewithinthewindow,thusreducingthenumberofparametersbyafactorofk2.Afullyconnectedlayerconnectsallinputstoalloutputs.Intuitively,eachoutputneuronoftheconvolutionallayersrepresentsanimagefeature.Thesefeaturesareoftenconnectedtothenetwork’soutputsviaoneormorefullyconnectedlayers,wheretheweightsbe-tweeninputsandoutputs(learntduringthetrainingprocess)determinetheextenttowhicheachfeatureisindicativeofeachoutputclass.Dropoutlayers[29]canbeusedbetweenfullycon-nectedlayersinordertoprobabilisticallyreduceover-ﬁtting.Givenaprobabilityparameterp,ateachtrainingstage,eachnodeintheinputremainsinthenetworkwithprobabilityporis“droppedout”(andisdisconnectedfromoutputs)withprobability1−p.Dropoutlayers,aswellasfullyconnectedlayers,mayalsoapearinrecurrentneuralnetworks,describednext.2.2.2RecurrentNeuralNetworks(RNNs)RNNsareneuralnetworksabletoprocesssequencesofdatarepresenting,e.g.,text[30,31],speech[32,33,34],handwriting[35]orvideo[36]inarecurrentman-ner,thatis,byrepeatedlyusingtheinputseensofarinordertoprocessnewinput.WeuseanRNNnet-workcomposedoflongshort-termmemory(LSTM)blocks[37].Eachsuchblockconsistsofacellthatstoreshiddenstate,abletoaggregate/summarizein-putsreceivedoveranextendedperiodoftime.Inadditiontothecell,anLSTMblockcontains3com-ponentscalledgatesthatcontrolandregulateinfor-mationﬂowintoandoutofthecell.Roughlyspeak-ing,theinputgatedeterminestheextenttowhichnewinputisusedbythecell,theforgetgatedeter-minestheextenttowhichthecellretainsmemory,andtheoutputgatecontrolstheleveltowhichthecell’svalueisusedtocomputetheblock’soutput.Inthecontextoftextanalysis,acommonpracticeistoaddanembeddinglayerbeforetheLSTMlayer[38,39].Embeddinglayersservetwopurposes.First,theyreducethedimensionalityoftheinput.Sec-ondly,theyrepresentinputinamannerthatretainsitscontext.Theembeddinglayerconvertseachin-puttoken(typicallyawordoracharacter,dependingontheproblemathand)toavectorrepresentation.Forexample,whentakingacharacter-levelapproach,onecanexpectthattherepresentationsofalldigitscomputedbytheembeddinglayerwillbevectorsthatareclosetoeachother.Whentheproblembeneﬁtsfromaword-levelrepresentation,word2vec[40]em-beddingsrepresenteachwordasavectorsuchthatwordsthatsharecommoncontextsinthetextcorpususingwhichthemodelwastrainedarerepresentedbyvectorsthatareclosetoeachother.AbidirectionalRNN(BRNN)network[41]isanRNNarchitectureinwhichtwoRNNlayersarecon-nectedtotheoutput,onereadingtheinputinorderandtheotherreadingitinreverseorder.Intuitively,thisallowstheoutputtobecomputedbasedonin-formationfrombothpastandfuturestates.BRNNshavefoundsuccessfulapplicationsinvariousﬁelds6[42,43,44].Forinstance,inthecontextofthesen-timentanalysisproblem,whenprocessingtextfromthemiddleofasentence,textseeninthebeginningofthesentence,aswellastextseenattheendthesentence,maybeusedbythecomputation.3ThedatasetOurworkisbasedonalargedatasetwhich,afterpre-processing(whichweshortlydescribe),consistsof66,388distinctPowerShellcommands,6,290labeledasmaliciousand60,098labelledasclean.Maliciousdatasetcommandsbelongtotwotypes.Fortrainingandcross-validation,weuse5,819distinctcommandsobtainedbyexecutingknownmaliciousprogramsinasandboxandrecordingallPowerShellcommandsexecutedbytheprogram.Fortesting,weused471maliciousPowerShellcommandsseeninthecourseofMay2017,contributedbyMicrosoftsecurityex-perts.Usingthislattertypeofmaliciousinstancesforevaluatingourdetectionresultsmimicsarealis-ticscenario,inwhichthedetectionmodelistrainedusingdatageneratedinsideasandboxandisthenappliedtocommandsexecutedonregularmachines.Asforcleancommands,wereceivedfromMi-crosoftacollectionofPowerShellcommandsexecutedwithinMicrosoft’scorporatenetworkinthecourseofMay2017,onmachineswhichhadnoindicationofmalwareinfectionthirtydayspriortotheexecu-tionofthePowerShellcommand.Cleancommandsweresplit48,094fortrainingandcross-validationand12,004fortesting.3.1Pre-processing&TrainingSetConstructionWeimplementedapreprocessorwhosekeygoalsaretoperformPowerShellcommanddecodingandnor-malizationforimproveddetectionresults.Italsoeliminatesidentical(aswellas“almostidentical”)commandsinordertoreducetheprobabilityofdataleakage.First,inordertobeabletoapplydetectionon“cleartext”,ourpreprocessordecodesPowerShellcommandsthatareencodedusingbase-64.Suchcommandsareidentiﬁedbythe-EncodedCommandﬂag(oranypreﬁxofitstartingwith’-e’or’-E’).Allthesecommandsundergobase-64decoding,asother-wisetheyprovidenousefuldetectiondata.6Next,thepreprocessornormalizescommandsinor-dertoreducetheprobabilityofadataleakageprob-lem[45]that,inoursetting,mayresultfromusingalmost-identicalcommandsfortrainingthemodelandforvalidatingit.Indeed,weobservedinourdatasetPowerShellcommandsthatdiﬀeronlyinaverysmallnumberofcharacters.Inmostcases,thiswasduetoeithertheuseofdiﬀerentIPaddressesortotheuseofdiﬀerentnumbers/typesofwhites-pacecharacters(e.g.,spaces,tabsandnewlines)inotherwise-identicalcommands.Toavoidthisprob-lem,wereplacedallnumberstoasterisksigns(‘*’)andallcontiguoussequencesofwhitespacecharac-terstoasinglespaceandtheneliminatedduplicates.WealsoobservedinourdatasetPowerShellcase-equivalentcommandsthatonlydiﬀerinlettercasing(seeentry1inFigure1).Thiswasdealtwithbyensuringthatonlyasinglecommandfromeachcase-equivalenceclassisusedfortraining/validation.Wenotethatthedimensionsofthedatasetspeciﬁedear-lierrelatetothenumbersofdistinctcommandsafterthispre-processingstage.Ourdatasetisveryimbalanced,sincethenumberofcleancommandsisanorderofmagnitudelargerthanthatofmaliciouscommands.Inordertopreventmodelbiastowardsthelargerclass,weconstructedthetrainingsetbyduplicatingeachmaliciouscom-mandusedfortraining8timessothattheratioofclean/malicioustrainingcommandsis1:1.Wepre-ferredtohandleimbalancethiswayratherthanbyusingunder-samplinginordertoavoidtheriskofover-ﬁtting,whichmayresultwhenaneuralnetworkistrainedusingasmallnumberofexamples.6Commandargumentsencodedineitherbase-64orUTF8(seeentries6,7inTable1)arenotdecodedsince,inthesecases,theencapsulatingcommandisavailableandcanbean-alyzedbythedetector.7Figure2:4-CNNarchitectureused4DetectionModelsInthissectionwedescribethemachinelearningmod-elsweusedformaliciousPowerShellcommandde-tection.Wethenevaluateandcomparetheirperfor-manceinSection5.Weimplementedseveraldeep-learningbasedde-tectors.Inordertoassesstheextenttowhichtheyareabletocompetewithmoretraditionaldetectionapproaches,wealsoimplementeddetectorsthatarebasedontraditionalNLP-basedmethods.Wepro-ceedbydescribingthesetwosetsofmodels.4.1Deep-LearningBasedDetectors4.1.1InputPreparationNeuralnetworksareoptimizedforclassiﬁcationtaskswhereinputsaregivenasrawsignals[24,25].Us-ingthesenetworksfortextclassiﬁcationrequirestoencodethetextsothatthenetworkcanprocessit.Zhangetal.[46]exploredtreatingtextasa“rawsignalatcharacterlevel”andapplyingtoitaone-dimensionalCNNfortextclassiﬁcation.WetakeasimilarapproachforclassifyingPowerShellcom-mandsaseithermaliciousorbenign.First,weselectwhichcharacterstoencode.Wedothisbycountingforeachcharacterthenumberoftrainingsetcommandsinwhichitappearsandthenassigningacodeonlytocharactersthatappearinatleast1.4%ofthesecommands.Wehavesettheencodingthresholdtothisvaluebecauseatthispointthereisasharpdeclineincharacterfrequency.Thus,theleast-frequentcharacterencoded(whichis`)appearedinapprox1.4%ofcommandsandthemost-frequentcharacterthatwasnotencoded(whichisanon-Englishcharacter)appearedinonlyapprox0.3%ofthetrainingsetcommands.Rarecharactersarenotassignedacodeinor-dertoreducedimensionalityandoverﬁttingproba-bility.Theresultisasetof61characters,contain-ingthespacesymbol,alllower-caseEnglishletters(wesoonexplainhowwerepresentupper-caseletters)andthefollowingsymbols:-’!%&()*,./:;?@[\]`{|}+^u#$^~”Similarlyto[46],weuseinputfeaturelengthof1,024,soifacommandislongerthanthatitistruncated.Thisreducesnetworkdimensionsand,asshownbyourevaluationinSection5.2,suﬃcestoprovidehigh-qualityclassiﬁcation.TheinputtotheCNNnetworkisthenpreparedbyusing“one-hot”en-codingofcommandcharacters,thatis,byconvertingeachcharacterofthe(possiblytruncated)commandtoavectorallofwhoseﬁrst61entriesare0exceptforthesingleentrycorrespondingtothecharacter’scode.Allcharactersthatwerenotassignedacodeareskipped.Inpractice,weuse62-longvectorsratherthan61-longvectorsinordertodealwiththecasingofEn-glishletters.UnlikeinmostNLPclassiﬁcationtasks,inthecontextofPowerShellcommandscharactercas-ingmaybeastrongsignal(seeobfuscationmethod1inFigure1).Inordertoretaincasinginformationinourencoding,weadda“casebit”,whichisthe62’ndvectorentry.Thebitissetto1ifthechar-acterisanupper-caseEnglishletterandissetto0otherwise.Thus,therepresentationofaPowerShellcommandthatisbeinginputtotheCNNnetworkisa62x1,024sparsematrix.Amatrixrepresentingacommandthatisshorterthan1,024ispaddedwithanappropriatenumberofzerocolumns.AswedescribedinSection2.2,whereasCNNsaretraditionallyusedforcomputervisionandtherefore8typicallyreceiveastheirinputamatrixrepresent-inganimage,recurrentneuralnetworks(RNNs)areoptimizedforprocessingsequencesofdata.Conse-quently,theinputweprovidetoourRNNclassiﬁerisavectorofnumbersofsizeatmost1,024,whosei’thelementisthecode(asdescribedabove)ofthei’thcommandcharacter(charactersthatwerenotas-signedacodeareskipped),exceptthatweexplicitlyencodeupper-caseEnglishletterssincewecannotuseacasebitfortheRNNinputrepresentation.4.1.2TrainingStochasticgradientdescentisthemostwidely-usedmethodfortrainingdeeplearningmodels[47].Wetrainourdeep-learningbasedalgorithmsusingmini-batchgradientdescent,inwhicheachtrainingepoch(acompletepassoverthetrainingset)issub-dividedtoseveralmini-batchessuchthatthegradientiscom-puted(andnetworkcoeﬃcientsareupdatedaccord-ingly)foreachmini-batch.Inordertocompareallourdeep-learningnetworksonthesamebasis,inallourexperimentsweused16trainingepochsandmini-batchsizeof128.Wealsoexperimentedwithothernumbersofepochs/mini-batchesbutnoneofthemobtainedsigniﬁcantlybet-terclassiﬁcationresults.4.1.3DetectionmodelsWeimplementedandevaluated3deep-learningbaseddetectorsdescribedinthefollowing.1.A9-layerCNN(9-CNN).Weusethenetworkarchitecturedesignedby[46],consistingof6con-volutionallayerswithstride1,followedby2fullyconnectedlayersandtheoutputlayer.Twodropoutlayersareusedbetweenthe3fullycon-nectedlayersandamaxpoolinglayerfollowstheﬁrst,secondandlastconvolutionallayers.7Un-likethearchitectureof[46]thatusesfullycon-nectedlayersofsize1,024or2,048,weuse256entriesineachsuchlayerasthisprovidesbetterperformanceonourdata.7Dropoutandmaxpoolinglayersaretypicallynotcountedtowardsthenetwork’sdepth.2.A4-layerCNN(4-CNN).Wealsoimplementedashallowerversionofthe9-CNNarchitecturewhosestructureisdepictedbyFigure2.Itcon-tainsasingleconvolutionallayerwith128ker-nelsofsize62x3andstride1,followedbyamaxpoolinglayerofsize3withnooverlap.Thisisfollowedbytwofully-connectedlayers,bothofsize1,024–eachfollowedbyadropoutlayerwithprobabilityof0.5(notshowninFigure2),andanoutputlayer.3.LSTM.Weimplementedarecurrentneuralnet-workmodelcomposedofLSTMblocksandusedthecharacter-levelrepresentationdescribedabove.Sinceinputsarenotsentencesofanat-urallanguage,wedecidednottouseWord2Vec[48]embedding.Instead,ourLSTMarchitec-turecontainsanembeddinglayerofsize32.TheLSTMblocksweusedarebi-directionalLSTMcellswithoutputdimensionof256,followedbytwofully-connectedlayers,bothofsize256,us-ingadropoutprobabilityof0.5.4.2TraditionalNLP-baseddetectorsWeusedtwotypesofNLPfeatureextractionmeth-ods–acharacterlevel3-gramandabagofwords(BoW).Inbothweevaluatedbothtfandtf-idfandthenappliedalogisticregressionclassiﬁeronex-tractedfeatures.The3-grammodelperformedbet-terusingtf-idf,whereasBoWperformedbetterus-ingtf.Foreachdetectorweselectedthehyper-parameterswhichgavethebestcross-validationAUCresults(evaluationresultsarepresentedinSection5).Notethatasthe4-CNNarchitectureusesakerneloflengththreeintheﬁrstconvolutionallayer,thefeaturesitusesaresimilartothoseextractedwhenusingthecharacter-level3-gramdetector.4.3InputRepresentationConsidera-tionsRecallingtheobfuscationmethodsusedbyPowerShell-basemalwareauthorsforavoidingdetection(seeSection2.1.1),weobservethatourinputrepresentationretainstheinformationrequired9foridentifyingthem.Thecommandsusedforob-fuscation,includingtheirshortversions(obfuscationmethod2inFigure1),canbelearntduetotheusageof3-sizedkernelsbythedeep-learningmodelsandtheusageof3-gramsbythetraditionalNLPmodels.Obfuscationmethod3isaddressedbythedecodingperformedduringdatapreparation(seeSection3.1).Mostotherobfuscationmethods(seeFigure1)usespecialcharacterssuchas“`”,thepipesign“|”,thesymbol“+”andtheenvironment-variablesign“$”.Thesespecialcharactersareoftenusedwhenstringsandthevaluesofenvironmentvariablesareconcatenatedinruntimeforobfuscation.Allthesespecialcharactersappearinasigniﬁcantfractionofourtrainingset’scommandsandconsequentlytheyareallassignedcodesbyourinputencodingfordeepnetworks.Theyarealsoretainedintheinputpro-videdtothetraditionalNLPmodels.Asfortheusageofrandomnames(obfuscationmethod11),thesetypicallyincludenumbers(con-vertedtothe‘*’sign)oralternatingcasing,andcanthereforebelearntbyourclassiﬁersaswell.(Aswedescribelater,ourdeeplearningclassiﬁersdoabetterjobinlearningsuchpatterns.)Theusageofspecialstringssuchas”[char]”,”UTF8”,”Base64”orthecharacter’‘’isalsocoveredbybothmodelsastheyareretainedintheinput.Theonlyobfuscationmethodw.r.t.whichtheinputtosomeofourdetectorsissuperiortothatprovidedtoothersistheusageofalternatinglower/uppercasecharacters(obfuscationmethod1inFigure1).Whereasthecase-bitwaseasilyincor-poratedintheinputtoourCNNdeep-learningclassi-ﬁers,theRNNandthetraditionalNLP-basedmodelsinputrepresentationsdonotaccommodateitsusage.5EvaluationWeperformed2-foldcrossvalidationonthetrain-ingdataandpresenttheareaundertheROCcurve(AUC)results(roundedtothethirddecimalplace)ofourdetectorsinTable1.Inadditiontothe5de-tectorspresentedinSection4,wealsoevaluatedavariantof4-CNN(denoted4-CNN*)inwhichwedidnotusethecasebit.AlldetectorsobtainveryhighAUClevelsintherange0.985−0.990.ThetraditionalNLP-basedde-tectorsprovideexcellentresultsintherange0.989−0.990,the4-CNNandLSTMdetectorsslightlylagbehindwithAUCof0.988and9-CNNprovidesalowerAUCof0.985.The4-CNN*detectorprovidesslightlylowerAUCthanthatof4-CNN,establishingthatthecasebitisbeneﬁcial.Foradetectortobepractical,itmustnotproducemanyfalsealarms.Asthecybersecuritydomainisoftencharacterizedbyaveryhighrateofeventsre-quiringclassiﬁcation,evenalowfalse-positiverate(FPR)of(say)1%mayresultintoomanyfalsealarms.Itisthereforeimportanttoevaluatethetruepositiverate(TPR)(a.k.a.recall)providedbydetec-torswhentheirthresholdissetforlowFPRlevels.Table2presentstheTPRofourdetectorsforFPRlevels10−2,10−3and10−4onboththetraining/cross-validationandthetestsets.Sincewehaveatotalofabout12,000cleancommandsinthetestset,westoptheanalysisatFPRlevelof10−4.Presentedvaluesinthe“Cross-validation”partofthetablearetheaverageofthetwofolds.Valuesinthe“Testset”partwereobtainedbymodelstrainedonthetrainingsetinitsentirety.Focusingﬁrstoncross-validationresults,itcanbeseenthat,whileallclassiﬁersachievehighTPRval-uesevenforverylowFPRlevels,theperformanceofthetraditionalNLPdetectorsisbetter.The3-gramdetectorleadsinallFPRlevelswithagapthatin-creaseswhenFPRvaluesaredecreased.Speciﬁcally,evenforanFPRof1:10,000itprovidesanexcellentTPRof0.95.Amongthedeep-learningbaseddetec-tors,4-CNNandLSTMaresuperiorto4-CNN*and9-CNN.ForFPRrateof1:10,000,4-CNNandLSTMprovideTPRsof0.89and0.85,respectively.9-CNNobtainstheworstresultsinallexperiments.Resultsonthetestsetaresigniﬁcantlylowerbutstillgood.ItisnoteworthythatthegapsbetweenthetraditionalNLPandthe4-CNN/LSTMmodelsthatweobservedonthetrainingdataalmostvanishonthetestdata.Thisseemstoindicatethatthelattermodelsareabletogeneralizebetter.ForanFPRof1:100,thebestperformersare4-CNNand4-CNN*withaTPRof0.89,LSTMissec-ondbestwith0.88andboththe3-gramandBoW10Table1:Detectors’areaundertheROCcurve(AUC)values.9-CNN4-CNN4-CNN*LSTM3-gramBoW0.9850.9880.9870.9880.9900.989Table2:TPRbyFPRpermodel:cross-validationandtestresults.FPRCross-validationTestset10−210−310−410−210−310−49-CNN0.950.890.730.720.520.244-CNN0.980.960.890.890.760.654-CNN*0.970.930.850.890.720.49LSTM0.980.950.850.880.810.643-gram0.990.980.950.870.830.66BoW0.980.930.870.870.500.35detectorsobtainaTPRof0.87.ForFPR1:1,000the3-gramdetectorisbestwithTPRof0.83,onlyslightlybetterthanLSTM’s0.81TPR,andforFPR1:10,000,allof3-gram,4-CNNandLSTM(orderedindecreasingperformance)identifyapproximatelytwothirdsofmaliciouscommands.Thesigniﬁcanceofthecasebitisevidentwhencomparingtheresultsofthe4-CNNandthe4-CNN*detectorsonthetestsetforFPRlevelof1:10,000.TheTPRwhenusingthecasebit(4-CNN)ishigherbyalmostonethirdthanthatwhenitisnotused(4-CNN*).9-CNNistheworstperformeralsointhetestsetexperiments,byawidermarginthaninthecross-validationtests.Aswe’vementioned,theperformanceonthetestsetissigniﬁcantlylowerthanthatofcross-validationinallexperiments.Thisistobeexpected:whereastrainingsetmaliciouscommandsweregeneratedbyrunningmalwareinsideasandbox,themaliciouscommandsinthetestsetwerecontributedbysecurityexperts.Consequently,testsetmaliciouscommandsmayhavebeencollectedindiﬀerentways(e.g.bysearchingtheWindowsregistryformaliciousPow-erShellcommands)andmayhavebeenproducedbymalwarenoneofwhosecommandsareinthetrainingset.5.1ADeep/TraditionalModelsEn-sembleWenextshowthatbycombining4-CNN–ourbestdeeplearningmodeland3-gram–ourbesttradi-tionalNLPmodel,weareabletoobtaindetectionresultsthatarebetterthanthoseofeachofthemseparately.WethenanalyzethetypeofmaliciouscommandsforwhichthedeepmodelcontributestothetraditionalNLPone.TheD/TEnsembleisconstructedasfollows.Weclassifyacommandusingboththe4-CNNandthe3-gramdetectors,thusreceivingtwoscores.Ifei-theroneofthescoresis0.99orhigher,wetakethemaximumscore,otherwisewetaketheaverageofthetwoscores.WeevaluatedtheEnsemble’sTPRbyFPRperformanceonthetestsetinthesameman-nerweevaluatedthenon-Ensemblealgorithms(seeTable2).TheD/TEnsemblesigniﬁcantlyoutper-formedallnon-EnsemblealgorithmsandobtainedonthetestsetTPRsof0.92,0.89and0.72forFPRlevelsof1:100,1:1,000and1:10,000,respectively.Inordertogainbettervisibilityintothecontri-butionofthe4-CNNdetectorontopofthe3-gramdetector,wepresentinFigures3a-3ctheconfusionmatrixesofthe3-gram,4-CNNandD/TEnsembledetectorsonthetestset.Theseresultsareobtainedusingthelowestthreshold(foreachofthealgorithms)11thatprovidesanFPRofnomorethan10−3.Sincethetestsetcontainsapproximately12,000cleanin-stances,thismeansthatthealgorithmsmusthaveatmost12falsepositives.AscanbeseenbycomparingFigures3aand3c,theD/TEnsembleadds42newdetectionsontopofthosemadebythe3-gramdetector,withonly4newfalsepositives.WeanalyzedthesenewdetectionsinordertounderstandwherethedeeplearningmodelisabletoimproveoverthetraditionalNLPmodel.Outofthenew42detectedcommands,15com-mandscontainasequenceofalternatingdigitsandcharacters.Inmostcases,thissequencerepresentedthenameofthehostordomainfromwhichthecom-manddownloaded(mostprobablymalicious)con-tent.Recallthatinourpre-processingofcommands,weconvertdigitstoasterisks(seeSection3.1),thusthehost/domainnamecontainsmanyasterisksinit.Anexampleoftheusageofsuchanamethatap-pearedinoneofthenewlydetectedcommandsis:“..DownloadFile(’http://d*c*a*ci*x*.’)..”.Eachofthesenamesappearsonlyonceandtheyaremostprobablygeneratedbyadomaingenera-tionalgorithm(DGA)[49]usedbythemalwareforcommunicatingwithitscommandandcontrolcen-ter.Sincethesenamesareuniqueandseemrandom,the3-gramalgorithmisunabletolearntheirpattern,whiletheneuralnetworkisableto.Figure4adepictsanexampleofhowsuchahostnameisencodedintheinputtotheneuralnetwork.Notethepatternofalternatingzerosandonesintherowcorrespondingtothesymbol‘*’.Figure4bde-pictsaneuralnetworkﬁlterofsize3thatisabletodetectoccurrencesofthispattern.Theﬁltercon-tainsonesintheﬁrstandthirdcolumnsoftherowcorrespondingto‘*’(wherethe‘*’symbolisexpectedtobefound)andazerointhemiddlecolumnofthatrow,signifyingthatthecharacterbetweenthetwodigitsisofnosigniﬁcance.WhenthisﬁlterisappliedtothecharacterssequencedepictedinFig-ure4a,itcreatesarelativelystrongsignal.Ontheotherhand,consideringthe3-gram’sfeatureextrac-tionalgorithm,sincethecharacterbetweenthetwodigitschangesfromonecommandtotheother,themodelisunabletolearnthispattern.AsimilarargumentcanexplainthedetectionofafewadditionalcommandsbytheD/TEnsemblethatwerenotdetectedby3-gram.Thesecommandscontainarandomsequenceofcharactersalternatingbetweenloweranduppercase,mostprobablygen-eratedbyaDGAalgorithmaswell.Usingthecasebitprovidedaspartofitsinput,4-CNNisabletoidentifythispattern.Wenotethatinboththeabovecases,thePoweShellcommandsmayincludeadditionalindica-tionsofmaliciousnesssuchasthewebclientorthecmdletstheyuse.Nevertheless,itistheabilitytodetectpatternsthatincorporaterandomcharactersand/orcasingthatcauses4-CNNtoassignthesecom-mandascoreabovethethreshold,unlikethe3-gramdetector.Ourensembledetectorhadonlysevenfalsepositive(FPs),whichwemanuallyinspected.TwoFPsexhibitedobfuscationpatterns–oneused[System.Text.Encoding]::UTF8.GetString(usageofUTF8wasobservedin1,114ofthecleancommands)andtheotherusedthe-EncodedCommandﬂag(whichwasobservedin1,655ofthecleancom-mands).TheremainingﬁveFPsdidnotuseanyformofobfuscation,buttheyallusedatleasttwoﬂagssuchas-NoProfileand-NonInteractive(eachseenin5,014and5,833ofthecleancommands,re-spectively).5.2CommandLengthConsiderationsAspreviouslymentioned,ourdetectorsreceiveasin-puta1,024-longpreﬁxofthePowerShellcommandandlongercommandsarebeingtruncated.Asshownbyourevaluation,thissuﬃcestoprovidehigh-qualityclassiﬁcationonourdataset.Apossiblecounter-measurethatmaybeattemptedbyfuturemalwareauthorsforevadingourdetectionapproachistoconstructlongPowerShellcommandssuchthatmaliciousoperationsonlyappearafteralonginnocent-lookingpreﬁxconsistingofharmlessoperations.Inthefollowing,weexplainhowsuchahypotheticcounter-measurecanbethwarted.Analyzingourdataset’sdistributionofcommandlengths,weﬁndthatthelengthof86%ofallma-liciouscommandsand88%ofallcleancommands12373p0p98n3n012001actualvaluepredictionoutcome(a)3-gram340p0p131n5n011999predictionoutcome(b)4-CNN415p0p56ntotal4717n01199712004predictionoutcome(c)D/TEnsembleFigure3:Confusionmatricesfor3-gram,4-CNNandEnsembleontestset,usingthresholdsresultinginFPRlowerthan10−3.(a)Asamplehostnameencoding(zerosremovedforclarity).(b)Filtercapableofidentifyingalternat-ingdigits.Figure4:Ahostnameencodingandaﬁlterwhichwasusedbythenetworktoidentifyalternatingdigitsandlettersis1,024orless.Moreover,thelengthof96.7%ofallmalwarecommandsand,moreimportantly,thelengthof99.6%ofallcleancommandsis2000orless.Weremindthereaderthatallcommandswereusedbyourdetectorsregardlessoftheirlength–commandslongerthan1,024charactersweresimplytruncated.Giventhegoodperformanceofalldetec-tors,wefoundnoreasonofusingalongerinputsize.Itwouldbestraightforwardtomodifyourdetectorsforaccommodatinginputsofsize2,048orlongerifandwhenthecharacteristicsofmaliciouscommandschangesuchthatthiswouldbenecessary.Asofnow,cleancommandswhoselengthexceeds2000areveryrare,deemingthemsuspicious.Figure5presentsthecommand-lengthdistribu-tionsofbenignandmaliciouscommandsinourdatasetforcommandsoflength1,024orless.Thedis-tributionofmaliciouscommandlengthisrelativelyskewedtotheright,indicatingthatmaliciousPower-Shellcommandstendtobelongerthanbenigncom-mands.Thehighpeakofveryshortmaliciouscom-mandsistoduetoKovtertrojancommands[17]thatconstituteapproximately8%ofthemaliciouscom-mandspopulationinourdataset.2004006008001,000012345LengthofcommandPercentageofcommandsMaliciouscommandsCleancommandsFigure5:PowerShellcommand-lengthdistributionsofcleanvsmaliciouscommands.136RelatedworkZhangetal.[46]introducedadeep-learningapproachfortextclassiﬁcationinwhichtheinputtoconvolu-tionalneuralnetworks(CNNs)isatcharacter-levelinsteadofword-level.Theycomparedtheirdeep-learningbasedclassiﬁerswithword-basedtraditionalNLPmethods(suchasn-grams)andwithrecurrentneuralnetworks(usingLSTMblocks).Theirempir-icalevaluationwasconductedusingsentimentanal-ysisandtopicclassiﬁcationdatasets.Theirresultsshowthat,whiletraditionalmethodsprovidedbetterperformanceonsmall/mediumdatasets,character-basedCNNsoutperformedthemonlargerdatasets.Our9-CNNarchitectureisalmostidenticaltotheirsanditsinputsareencodedinasimilarmanner.PrusaandKhoshgoftaar[50]compareseveralar-chitecturesforshorttextsentimentanalysisclassi-ﬁcationappliedonalargedatasetoftweets.Theyshowthattworelativelyshallowarchitectures(onecomprising3convolutionallayersand2fullycon-nectedlayersandtheothercomprisingasingleconvo-lutionallayerfollowedbyasingleLSTMlayer)gavethebestresults.Ourresultsarealignedwiththeirsinthatalsoinourempiricalevaluationtherelativelyshallow4-CNNnetworkachievedbetterclassiﬁcationperformancethanthedeeper9-CNNnetwork.Inbothsettings,classiﬁedtextisrelativelyshort–upto140charactersinputsintheirstudyandupto1,024charactersinours.Deeplearningapproachesareincreasinglyusedinrecentyearsformalwaredetection.Someoftheseworks(see[51,52,53,54]forafewexamples)clas-sifyprogramsaseithermaliciousorbenignbasedontheirbinarycodeand/ortheirruntimebehaviour.Inorderfortheneuralnetworktobeabletoclassifyexecutableprograms,anon-trivialfeatureextractionpre-processingstageistypicallyrequiredwhoseout-putisfedtotheneuralnetwork.AthiwaratkunandStokes[54]usedalargedatasetconsistingofWindowsportableexecutable(PE)ﬁles.Theyapplieddeepmodelstoinputsrepresentingthesystemcallsmadebytheseprograms.Theyimple-mentedandevaluatedseveralmodels,includingacharacter-levelCNNsimilartotheoneusedby[46].Unlikeourresults,intheirempiricalevaluationtheLSTMmodelachievedthebestresults.However,noneoftheirneuralnetworkswasshallow.Smithetal.alsostudiedtheproblemofmalwaredetectionbasedonsystemcallsmadebyPEexecuta-bles[55].Theyusedseveralclassiﬁcationalgorithms,includingRandomForest,CNNandRNN.Theyob-servedadecayinclassiﬁcationqualitywheninputlengthexceeded1,000systemcalls.Althoughprob-lemsettingandinputdomainsdiﬀer,bothourworkandtheirsprovideevidencethatlimitinginputlengthbysome(domainspeciﬁc)thresholdmaybesuﬃcient(andissometimesevenrequired)forobtaininggoodperformance.Similarlytoourwork,SaxeandBerlinusedeeplearningmodelsformalwaredetectionbyanalyzing“cleartext”[56].Morespeciﬁcally,theyapplythesemodelsonalargedatasetconsistingof(bothbenignandmalicious)URLs,ﬁlepathsandregistrykeys.TheirCNNarchitectureusesasingleconvolutionallayer,asdoesour4-CNNmodel.Althoughsomepreviousstudiesinvestigatedtheproblemofdetectingmaliciousscripting-languagecommands/scripts(wherecleartextclassiﬁcationcanbeapplied),tothebestofourknowledgenoneofthemaddressedPowerShell.Severalpriorworkspre-senteddetectorsofmaliciousJavaScriptcommandsbyemployingfeatureextractionpre-processingfol-lowedbytheapplicationofashallowclassiﬁer(see,e.g.,[2,3,4]).Wangetal.useddeepmodelsforclassifyingJavaScriptcodecollectedfromwebpages[5].Simi-larlytoourwork,theirmodelusescharacter-levelen-coding,withan8-bitcharacterrepresentation.Theycomparetheirclassiﬁerswithclassicfeatureextrac-tionbasedmethodsandstudytheimpactofthenum-berofhiddenlayersandtheirsizeondetectionaccu-racy.AfewreportsbyAVvendorspublishedinrecentyearssurveyedandhighlightedthepotentialabuseofPowerShellasacyberattackvector[6,16,1].Pon-tiroliandMartinezanalyzetechnicalaspectsofma-liciousPowerShellcode[16].Usingreal-worldexam-ples,theydemonstratehowPowerShelland.NETcanbeusedbydiﬀerenttypesofmalware.Quotingfromtheirreport:“Vastamountsofready-to-usefunction-alitymakethecombinationof.NETandPowerShell14adeadlytoolinthehandsofcybercriminals”.ArecentcomprehensivetechnicalreportbySymantecdedicatedtoPowerShell’sabusebycyber-crimials[1]reportedonasharpincreaseinthenum-berofmaliciousPowerShellsamplestheyreceivedandinthenumberofpenetrationtoolsandframe-worksthatusePowerShell.TheyalsodescribethemanywaysinwhichPowerShellcommandscanbeobfuscated.Collectively,thesereportsshedlightontheman-nerinwhichPowerShellcanbeusedindiﬀerentstagesofacyberattacks–fromdownloadingma-liciouscontent,throughreconnaissanceandmalwarepersistence,tolateralmovementattempts.WehaveusedafewoftheinsightstheyprovideonPowerShellattacksfordesigningourdetectionmodelsandforpreprocessingPowerShellcommands.Aswe’vementionedpreviously,Microsoftim-provedtheloggingcapabilitiesofPowerShell5.0inWindows10,withtheintroductionoftheAntiMal-wareScanInterface(AMSI),butmanymethodsofbypassingithavealreadybeenpublished.Thisprob-lemwasdiscussedandaddressedin[19],wherethefactthatPowerShellisbuilton.NETarchitecturewasusedformonitoringPowerShell’sactivity,byleveraging.NETcapabilities.Asdiscussedintheirwork,theproposedsolutionsrequiresomeadjust-mentswhichmayhurtPowerShell’sperformance,aswellasgeneratesomeartifactsonthemachine.7DiscussionPowerShellcommandscanbeexecutedfrommemory,henceidentifyingmaliciouscommandsandblockingthempriortotheirexecutionis,ingeneral,imprac-tical.Wethereforeestimatethatthemostplausi-bledeploymentscenarioofourdetectorwouldbeasapost-breachtool.Insuchadeploymentscenario,PowerShellcommandsthatexecutewillberecordedandthenclassiﬁedbyourdetector.Commandsclas-siﬁedasmaliciouswouldgeneratealertsthatshouldtriggerfurtherinvestigation.Incorporatenetworks,thistypeofalertsistypicallysenttoasecurityin-formationandeventmanagement(SIEM)systemandpresentedonadashboardmonitoredbytheorganiza-tion’sCISO(chiefinformationsecurityoﬃcer)team.Thereareseveralwaysinwhichthisworkcanbeextended.First,whilewehaveimplementedandevaluatedseveraldeep-learningandtraditionalNLPbasedclassiﬁers,thedesignspaceofbothtypesofmodelsisverylargeandamorecomprehensiveeval-uationofadditionaltechniquesandarchitecturesmayyieldevenbetterdetectionresults.Secondly,inthisworkwetargetedthedetectionofindividualPowerShellcommandsthatareexecutedviathecommand-line.AninterestingdirectionforfutureworkistodevisedetectorsforcompletePower-Shellscriptsratherthanindividualcommands.Suchscriptsaretypicallylongerthansinglecommandsandtheirstructureisricher,astheygenerallycontainmultiplecommands,functionsanddeﬁnitions.Ef-fectivedetectionofmaliciousscriptswouldprobablyrequiresigniﬁcantlydiﬀerentinputencodingand/ordetectionmodelsthanthoseweusedinthiswork.Anotherinterestingavenueforfutureworkistode-visedetectorsthatleveragetheinformationcollectedbyMicrosoft’sAntiMalwareScanInterface(AMSI)[18].Asmentionedpreviously,AMSIisabletorecordPowerShellcommands(generatedbothstaticallyanddynamically)thatareexecutedinrun-time,sode-tectorsmayhavemoredatatooperateon.However,althoughAMSImaybelessvulnerabletomanyoftheobfuscationmethodsdescribedinSection2.1.1,attackersmaybeabletoﬁndnewwaysofcamouﬂag-ingtheAMSItracesoftheirmaliciouscommands.8ConclusionInthisworkwedevelopedandevaluatedtwotypesofML-baseddetectorsofmaliciousPowerShellcom-mands.Detectorsbasedondeeplearningarchitec-turessuchasConvolutionalNeuralNetworks(CNNs)andRecurrentNeuralNetworks(RNNs),aswellasdetectorsbasedonmoretraditionalNLPapproachessuchaslinearclassiﬁcationontopofcharactern-gramsandbag-of-words.WeevaluatedourdetectorsusingalargedatasetconsistingoflegitimatePowerShellcommandsexe-cutedbyusersinMicrosoft’scorporatenetwork,ma-liciouscommandsexecutedonvirtualmachinesde-15liberatelyinfectedbyvarioustypesofmalware,andmaliciouscommandscontributedbyMicrosoftsecu-rityexperts.Ourevaluationresultsshowthatourdetectorsyieldhighperformance.Thebestperformanceispro-videdbyanensembledetectorthatcombinesatra-ditionalNLP-basedclassiﬁerwithaCNN-basedclas-siﬁer.OuranalysisofmaliciouscommandsthatareabletoevadethetraditionalNLP-basedclassiﬁerbutaredetectedbytheCNNclassiﬁerrevealsthatsomeobfuscationpatternsautomaticallydetectedbythelatterareintrinsicallydiﬃculttodetectusingtradi-tionalNLP-basedclassiﬁers.Ourensembledetectorprovideshighrecallvalueswhilemaintainingaverylowfalsepositiverateandsoholdsthepotentialofbeingusefulinpractice.References[1]Symantec.TheincreaseduseofPow-ershellinattacks.https://www.symantec.com/content/dam/symantec/docs/security-center/white-papers/increased-use-of-powershell-in-attacks-16-en.pdf,2016.[2]MarcoCova,ChristopherKruegel,andGio-vanniVigna.Detectionandanalysisofdrive-by-downloadattacksandmaliciousjavascriptcode.InProceedingsofthe19thinternationalconfer-enceonWorldwideweb,pages281–290.ACM,2010.[3]CharlieCurtsinger,BenjaminLivshits,Ben-jaminGZorn,andChristianSeifert.Zozzle:Fastandprecisein-browserjavascriptmalwaredetection.InUSENIXSecuritySymposium,pages33–48.USENIXAssociation,2011.[4]PeterLikarish,EunjinJung,andInsoonJo.Obfuscatedmaliciousjavascriptdetectionus-ingclassiﬁcationtechniques.InMaliciousandUnwantedSoftware(MALWARE),20094thIn-ternationalConferenceon,pages47–54.IEEE,2009.[5]YaoWang,Wan-dongCai,andPeng-chengWei.Adeeplearningapproachfordetectingmaliciousjavascriptcode.SecurityandCommunicationNetworks,9(11):1520–1534,2016.[6]PaloAlto.PullingBacktheCur-tainsonEncodedCommandPowerShellAttacks.https://researchcenter.paloaltonetworks.com/2017/03/unit42-pulling-back-the-curtains-on-encodedcommand-powershell-attacks/,2017.[7]IanJ.Goodfellow,YoshuaBengio,andAaronC.Courville.DeepLearning.Adaptivecomputa-tionandmachinelearning.MITPress,2016.[8]BingLiuandLeiZhang.Asurveyofopinionminingandsentimentanalysis.InMiningtextdata,pages415–463.Springer,2012.[9]RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.Exploringthelimitsoflanguagemodeling.arXivpreprintarXiv:1602.02410,2016.[10]XiangZhangandYannLeCun.Textun-derstandingfromscratch.arXivpreprintarXiv:1502.01710,2015.[11]KunihikoFukushimaandSeiMiyake.Neocog-nitron:Aself-organizingneuralnetworkmodelforamechanismofvisualpatternrecognition.InCompetitionandcooperationinneuralnets,pages267–285.Springer,1982.[12]YannLeCun,YoshuaBengio,andGeoﬀreyHin-ton.Deeplearning.Nature,521(7553):436–444,2015.[13]JeﬀreyLElman.Findingstructureintime.Cog-nitivescience,14(2):179–211,1990.[14]ChristopherDManning,HinrichSch¨utze,etal.Foundationsofstatisticalnaturallanguagepro-cessing,volume999.MITPress,1999.[15]MicrosoftCorporation.Powershell.https://docs.microsoft.com/en-us/powershell/scripting/powershell-scripting?view=powershell-5.1,2017.16[16]SantiagoMPontiroliandFRobertoMartinez.Thetaoof.netandpowershellmalwareanalysis.InVirusBulletinConference,2015.[17]MicrosoftCorporation.Tro-jan:win32/kovter.https://www.microsoft.com/en-us/wdsi/threats/malware-encyclopedia-description?Name=Trojan:Win32/Kovter,2017.[18]MicrosoftCorporation.Antimalwarescaninter-face.https://msdn.microsoft.com/he-il/library/windows/desktop/dn889587(v=vs.85).aspx,2017.[19]AmandaRousseau.Hijacking.nettodefendpowershell.arXivpreprintarXiv:1709.07508,2017.[20]MichaelHopkinsandAliDehghantanha.Ex-ploitkits:theproductionlineofthecybercrimeeconomy?InInformationSecurityandCyberForensics(InfoSec),2015SecondInternationalConferenceon,pages23–27.IEEE,2015.[21]DanielBohannon.Theinvoke-obfuscationmod-ule.https://github.com/danielbohannon/Invoke-Obfuscation,2016.[22]RobertJSchalkoﬀ.Artiﬁcialneuralnetworks,volume1.McGraw-HillNewYork,1997.[23]BYegnanarayana.Artiﬁcialneuralnetworks.PHILearningPvt.Ltd.,2009.[24]YannLeCun,BernhardBoser,JohnSDenker,DonnieHenderson,RichardEHoward,WayneHubbard,andLawrenceDJackel.Backpropaga-tionappliedtohandwrittenzipcoderecognition.Neuralcomputation,1(4):541–551,1989.[25]YannLeCun,L´eonBottou,YoshuaBengio,andPatrickHaﬀner.Gradient-basedlearningap-pliedtodocumentrecognition.ProceedingsoftheIEEE,86(11):2278–2324,1998.[26]VinodNairandGeoﬀreyEHinton.Recti-ﬁedlinearunitsimproverestrictedboltzmannmachines.InProceedingsofthe27thinterna-tionalconferenceonmachinelearning(ICML-10),pages807–814.Omnipress,2010.[27]Y-LanBoureau,FrancisBach,YannLeCun,andJeanPonce.Learningmid-levelfeaturesforrecognition.InComputerVisionandPat-ternRecognition(CVPR),2010IEEEConfer-enceon,pages2559–2566.IEEE,2010.[28]DouglasMHawkins.Theproblemofoverﬁtting.Journalofchemicalinformationandcomputersciences,44(1):1–12,2004.[29]GeoﬀreyEHinton,NitishSrivastava,AlexKrizhevsky,IlyaSutskever,andRuslanRSalakhutdinov.Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.arXivpreprintarXiv:1207.0580,2012.[30]SiweiLai,LihengXu,KangLiu,andJunZhao.Recurrentconvolutionalneuralnetworksfortextclassiﬁcation.InAAAI,volume333,pages2267–2273.AAAIPress,2015.[31]TomasMikolov,MartinKaraﬁ´at,LukasBurget,JanCernock`y,andSanjeevKhudanpur.Recur-rentneuralnetworkbasedlanguagemodel.InInterspeech,volume2,page3.ISCA,2010.[32]AlexGraves,Abdel-rahmanMohamed,andGe-oﬀreyHinton.Speechrecognitionwithdeepre-currentneuralnetworks.InAcoustics,speechandsignalprocessing(icassp),2013ieeeinter-nationalconferenceon,pages6645–6649.IEEE,2013.[33]AlexGravesandNavdeepJaitly.Towardsend-to-endspeechrecognitionwithrecurrentneuralnetworks.InProceedingsofthe31stInterna-tionalConferenceonMachineLearning(ICML-14),pages1764–1772.JMLR.org,2014.[34]Ha¸simSak,AndrewSenior,andFran¸coiseBea-ufays.Longshort-termmemoryrecurrentneu-ralnetworkarchitecturesforlargescaleacous-ticmodeling.InFifteenthAnnualConferenceoftheInternationalSpeechCommunicationAsso-ciation.ISCA,2014.17[35]AlexGraves.Generatingsequenceswithrecurrentneuralnetworks.arXivpreprintarXiv:1308.0850,2013.[36]AndrejKarpathy,GeorgeToderici,SankethShetty,ThomasLeung,RahulSukthankar,andLiFei-Fei.Large-scalevideoclassiﬁcationwithconvolutionalneuralnetworks.InProceedingsoftheIEEEconferenceonComputerVisionandPatternRecognition,pages1725–1732.IEEE,2014.[37]SeppHochreiterandJ¨urgenSchmidhuber.Longshort-termmemory.Neuralcomputation,9(8):1735–1780,1997.[38]DavidERumelhart,GeoﬀreyEHinton,andRonaldJWilliams.Learningrepresentationsbyback-propagatingerrors.nature,323(6088):533–536,1986.[39]SamTRoweisandLawrenceKSaul.Nonlineardimensionalityreductionbylocallylinearem-bedding.science,290(5500):2323–2326,2000.[40]TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeﬀDean.Distributedrep-resentationsofwordsandphrasesandtheircom-positionality.InAdvancesinneuralinforma-tionprocessingsystems,pages3111–3119.NIPS,2013.[41]MikeSchusterandKuldipKPaliwal.Bidirec-tionalrecurrentneuralnetworks.IEEETrans-actionsonSignalProcessing,45(11):2673–2681,1997.[42]AlexGraves,SantiagoFern´andez,andJ¨urgenSchmidhuber.BidirectionalLSTMnetworksforimprovedphonemeclassiﬁcationandrecogni-tion.InWlodzislawDuch,JanuszKacprzyk,ErkkiOja,andSlawomirZadrozny,editors,Ar-tiﬁcialNeuralNetworks:FormalModelsandTheirApplications-ICANN2005,15thInterna-tionalConference,Warsaw,Poland,September11-15,2005,Proceedings,PartII,volume3697ofLectureNotesinComputerScience,pages799–804.Springer,2005.[43]MartinSundermeyer,TamerAlkhouli,JoernWuebker,andHermannNey.Translationmodel-ingwithbidirectionalrecurrentneuralnetworks.InEMNLP,pages14–25.ACL,2014.[44]PierreBaldi,SørenBrunak,PaoloFrasconi,Gio-vanniSoda,andGianlucaPollastri.Exploit-ingthepastandthefutureinproteinsec-ondarystructureprediction.Bioinformatics,15(11):937–946,1999.[45]ShacharKaufman,SaharonRosset,ClaudiaPerlich,andOriStitelman.Leakageindatamining:Formulation,detection,andavoid-ance.ACMTransactionsonKnowledgeDiscov-eryfromData(TKDD),6(4):15,2012.[46]XiangZhang,JunboZhao,andYannLeCun.Character-levelconvolutionalnetworksfortextclassiﬁcation.InAdvancesinneuralinformationprocessingsystems,pages649–657.NIPS,2015.[47]L´eonBottou.Large-scalemachinelearningwithstochasticgradientdescent.InProceedingsofCOMPSTAT’2010,pages177–186.Springer,2010.[48]TomasMikolov,KaiChen,GregCorrado,andJeﬀreyDean.Eﬃcientestimationofwordrep-resentationsinvectorspace.arXivpreprintarXiv:1301.3781,2013.[49]BrettStone-Gross,MarcoCova,LorenzoCaval-laro,BobGilbert,MartinSzydlowski,RichardKemmerer,ChristopherKruegel,andGiovanniVigna.Yourbotnetismybotnet:analysisofabotnettakeover.InProceedingsofthe16thACMconferenceonComputerandcommunica-tionssecurity,pages635–647.ACM,2009.[50]JosephD.PrusaandTaghiM.Khoshgoftaar.Deepneuralnetworkarchitectureforcharacter-levellearningonshorttext.InProceedingsoftheThirtiethInternationalFloridaArtiﬁcialIn-telligenceResearchSocietyConference,FLAIRS2017,MarcoIsland,Florida,USA,May22-24,2017.,pages353–358.AAAIPress,2017.18[51]JoshuaSaxeandKonstantinBerlin.Deepneu-ralnetworkbasedmalwaredetectionusingtwodimensionalbinaryprogramfeatures.InMa-liciousandUnwantedSoftware(MALWARE),201510thInternationalConferenceon,pages11–20.IEEE,2015.[52]GeorgeEDahl,JackWStokes,LiDeng,andDongYu.Large-scalemalwareclassiﬁca-tionusingrandomprojectionsandneuralnet-works.InAcoustics,SpeechandSignalProcess-ing(ICASSP),2013IEEEInternationalCon-ferenceon,pages3422–3426.IEEE,2013.[53]RazvanPascanu,JackWStokes,HerminehSanossian,MadyMarinescu,andAnilThomas.Malwareclassiﬁcationwithrecurrentnetworks.InAcoustics,SpeechandSignalProcessing(ICASSP),2015IEEEInternationalConferenceon,pages1916–1920.IEEE,2015.[54]BenAthiwaratkunandJackWStokes.Mal-wareclassiﬁcationwithlstmandgrulanguagemodelsandacharacter-levelcnn.InAcoustics,SpeechandSignalProcessing(ICASSP),2017IEEEInternationalConferenceon,pages2482–2486.IEEE,2017.[55]MichaelRSmith,JoeBIngram,Christo-pherCLamb,TimothyJDraelos,JustinEDoak,JamesBAimone,andConradDJames.Dynamicanalysisofexecutablestodetectandcharacterizemalware.arXivpreprintarXiv:1711.03947,2017.[56]JoshuaSaxeandKonstantinBerlin.expose:Acharacter-levelconvolutionalneuralnetworkwithembeddingsfordetectingmaliciousurls,ﬁlepathsandregistrykeys.arXivpreprintarXiv:1702.08568,2017.19