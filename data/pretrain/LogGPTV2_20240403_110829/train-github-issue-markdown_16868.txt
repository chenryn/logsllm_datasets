Hi,
My masters thesis is on making neural nets use less memory. One technique I am
looking at is memory checkpointing. I can solve for the optimal policy
(including multiple recomputations), given the memory budget and per-operator
compute/memory costs.
I am attempting to implement memory checkpointing as done in
`torch.utils.checkpoint`, except allowing for multiple recomputations.
However, there a couple things from the implementation that I'm not quite sure
I understand. Apologies if anything is obvious, I have been using PyTorch for
 drop(2) -> 3`, where the module `2` itself performs `2a -> drop(2b)`. `drop` is like a higher-order model whose forward simply performs `checkpoint(child_model, x)`. Thus, running `2` should drop `2a` and `2b` in the forward pass; and in the backward pass, recompute `2a`, recompute `2b` and drop it, then recompute `2b` a second time, this time actually saving it. Obviously, that's not so smart but its a simple example.
I believe the following chain of events will occur in practice:
    1. `1`'s forward is performed, whose output is propagated to `drop(2)`'s forward function.
    2. `drop(2)`'s forward invokes `CheckpointFunction`'s forward, which saves (checkpoints) the input, and performs `2`'s forward without tracking gradient. The intent is that both the outputs of `2a` and `2b` will be dropped at this stage.
    3. `2a` performs its forward and the output is passed to `drop(2b)`'s forward.
    4. a. If I set up this 'drop' wrapper to not detach its input before passing to the child model, the `CheckpointFunction` forward will throw an error about none of the inputs have `requires_grad=True`.
b. If I do detach the input, I _suspect_ this will not work either: **it will
save the input to`2b` thus checkpointing it, which, as mentioned in step 3, I
do not want to happen!**
On the other hand, maybe it will be freed because of how everything is set up
and autograd's reference counting?
Maybe, as `drop(2b)` saves its input, and `2a` was run with no grad, you get
an unreachable cycle between them that will get garbage collected?
I really lack the autograd understanding to know (see the <2 days of PyTorch).
I also do not know how to profile this to observe if it drops the tensor or
not.
**If my anaylsis is right, I will have to implement this`drop` operator from
scratch such that it avoids this behaviour, correct?**
Thank you for making it this far and sorry if the above explanations are not
great, it would be easier with diagrams. Any help on this would be greatly
appreciated.