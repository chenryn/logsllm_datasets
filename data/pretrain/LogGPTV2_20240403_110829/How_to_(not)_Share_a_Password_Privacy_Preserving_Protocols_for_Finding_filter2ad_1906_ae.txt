### Optimized Text

#### Protocol Suitability for IoT Devices
While the non-interactive protocol is simpler, a larger value of \( k_2 \) is more appropriate for IoT devices with sufficient memory and processing power, such as web cameras. Conversely, low-performance or low-power devices should opt for the interactive protocol.

#### Communication Complexity
We calculate the number of bits transmitted in the protocol. Assuming that \( S \) generates a fixed \( r_p \) per \( D \), we will only consider the size of \( D \)'s proof, neglecting the \( k_2 \) bits sent by \( S \).

- The size of the representation of each permutation is \( 2^\ell \cdot |2^\ell| = 2^\ell \cdot (\log \ell + 1) \).
- The size of each vector is:
  - \( |(s^*, e, V^*, \text{pad})| = k_1 \cdot (2^\ell + 1 + \ell) + 2^\ell \approx 3\ell k_1 \)
  - \( |(s_j, \pi_j^*, \text{blind}_j)| = |(s_j, \pi_j, d_j)| = 4k_1\ell + 2^\ell \cdot (\log \ell + 1) \approx 4\ell k_1 \)

The total communication complexity is approximately \( 4\ell k_1 k_2 \). For specific parameters of the non-interactive protocol (\( \ell = 16 \), \( k_1 = 2048 \), \( k_2 = 128 \)), the communication complexity is about 2 MB. For the interactive protocol with \( k_2 = 20 \), it is about 312 KB.

### Proof of Concept Implementation

#### QR Protocol PoC
We implemented a proof-of-concept (PoC) of our maliciously secure QR protocol in Python, demonstrating that even an unoptimized implementation is sufficiently efficient for many IoT devices. To simulate an IoT device, we ran the device-side code on a Raspberry Pi 3 Model B [ras], and the server code on an Intel i7-7500U 2.7GHz CPU.

- We used a modulus \( N \) of size 2048 bits and \( \ell = 16 \).
- We measured the processing time of the protocol with \( k_2 = 20 \) (interactive version) and \( k_2 = 128 \) (non-interactive version).

For \( k_2 = 128 \):
- Device: 2.8 seconds of runtime and 13.5 seconds of preprocessing.
- Server: 0.5 seconds to verify the proof and 3 milliseconds to update the counters.

For \( k_2 = 20 \):
- Device: 0.4 seconds of runtime and 2.1 seconds of preprocessing.
- Server: 0.5 seconds to verify the proof and 3 milliseconds to update the counters.

#### Popular Hash List Simulation
We simulated our password blacklisting scheme using three lists of frequencies of passwords from leaked databases of LinkedIn, Yahoo!, and RockYou [BH19, Bon12, Wik19], which contained 174M, 70M, and 33M passwords, respectively.

- We ran the simulation 150 times for each database.
- In each run, the passwords were hashed to random 16-bit values, and the protocol was simulated between each "user" and the server.
- For supporting local differential privacy, each user's answer was randomized with probability \( \epsilon_v = 0.25 \).

We compared the success of blacklisting passwords using our scheme to an "ideal" blacklisting process that has access to the entire list of passwords. We assumed that the server decides on a threshold \( t \) and blacklists the top \( t \) popular passwords. We measured the percentage of users whose password is blacklisted by the server, taking the median result among the 150 runs of the simulation.

**Figure 3** shows the percentage of users whose password is blacklisted as a function of the number \( t \) of top popular passwords that are blacklisted. The results of our simulation are compared to those of an "ideal blacklister" that blacklists the actual \( t \) most popular passwords. Due to the Zipf distribution of passwords, the utility of blacklisting each additional password decreases sharply. Therefore, it is only necessary to examine the effect of blacklisting a relatively small number of passwords.

- When blacklisting up to \( t = 8 \) most popular passwords, the results of the simulation are identical to the ideal blacklisting.
- When blacklisting more passwords, the results of the simulation are very close to the ideal run. For example, when blacklisting the top \( t = 25 \) passwords, the simulation applied to the LinkedIn database blocked the passwords of 92% of the users whose passwords were blacklisted by the ideal blacklister. The simulation for the smaller RockYou database blacklisted the passwords of 86% of the users that were blocked by the ideal simulation.

### Discussion and Open Questions

#### Tradeoff Between QR and Garbled Circuit Solutions
The garbled circuit solution is more efficient in terms of runtime and bandwidth. However, it requires an interactive protocol and generating a new \( r \) value for each password change. The QR-based protocol demands more resources but has a non-interactive version that only requires the device to prepare and send a single message to the server, reusing the same \( r \).

#### Implementation for the Tor Network
We believe our protocol can be useful for private statistics gathering in the Tor network. This would require working with the Tor project to choose the best use case and adjust and implement the protocol in that setting.

#### Open Question: Is Cryptography Needed?
We described a protocol for the semi-honest setting that does not require any (complexity-based) cryptographic primitives. This protocol is secure even if the server is malicious. However, to guarantee security against a coalition of malicious devices, our protocol instantiations use Oblivious Transfer (OT) or public-key cryptography. The open question is whether protecting against an undercount attack implies the existence of OT or other cryptographic primitives. It remains an open problem to either prove this claim or show an alternative protocol.

#### Open Question: Effectiveness of Data Leakage
In the "Malicious Campaign" setting, we treated the data leakage as allowing the adversary to mount an attack with a success probability that is linear in the number of bits leaked. It is unclear if this approach can indeed be exploited. Hence, the parameters of the system may be improved. Is it possible to argue that the system behaves better than our analysis?

### References
[References section remains unchanged]

### Frequently Asked Questions

#### Isn’t the password distribution already known?
As explained in Section 1.1, the password distribution can change over time or vary between different populations.

#### Can publishing the blacklist put users at risk?
Publishing the blacklist is similar to publishing new code vulnerabilities. It helps most users and protects the ecosystem. By preventing any single password from becoming too popular, it limits attackers' ability to exploit this information.

#### Can we use some PIR, PSI, or other mechanisms to protect the blacklist of passwords?
Unfortunately, no. In our analysis, we assume there is a large colluding coalition, and each device can test if its password is blacklisted. If the coalition is large, it might be able to test the entire hash domain and find all blacklisted values, regardless of the protection mechanism.

#### What about the users that are already using a blacklisted password?
The device can alert the user if their current password is added to the blacklist. However, this results in a security trade-off, as the change may reveal information about the user’s previous password.

#### Leakage: Why leaking one bit of information is fine?
The password game defined in Section 2.1 shows the effect of releasing one bit of information. This section also explains how differential privacy can be used to reduce this effect.

#### Is it practical to implement this protocol?
Yes! In Section 7, we describe the PoC we implemented for both the device and server sides, and for particular parameters of the scheme. A relatively weak server requires approximately 0.5 seconds to run the whole non-interactive proof verification and update the counters. A low-resource Raspberry Pi 3 Model B can prepare the proof in less than 15 seconds.

#### What size of the domain can be used in the real world?
The domain size does not affect the security of the protocol. It is a trade-off between communication complexity and performance on one hand, and password rejection false positive rate on the other. Our PoC was done with \( \ell = 16 \), which allows for good performance and acceptable false positive rates.