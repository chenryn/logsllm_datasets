the ﬂagged ones are, indeed, potentially malicious. For in-
stance, ctrlaltdel appeared in 13 of the VAs as unveriﬁed,
but only 6 of them were ﬂagged by VirusTotal.
All of the 10 VAs infected with a potentially malicious ﬁle
belong to ILG 3 (see Table 4). This is somewhat expected
as the number of unveriﬁed ﬁles is much greater in ILG 3.
Interestingly though just 7 of those VAs belong to Cluster
2 (see Table 4) which is characterized by the large number
of low-integrity packages. We note that from the 14 VAs
that are part of Cluster 2, virus scanners ﬂagged only about
50% of them as potentially untrustworthy. This further re-
inforces the need for our framework as it can also ﬂag the
other 50% that are equally suspicious and would likely not
meet a customers expectations when considering a VA for
a standard (or well-known) workﬂow (e.g., an Apache web
server).
Packages that contain these malicious ﬁles will get a score
of 0 , discouraging consumers from picking VAs that contain
them. The providers could deﬁne a policy that disallows
such VAs from being published in the ﬁrst place. The results
clearly indicate the need to check VAs against virus scanners.
To that end, our approach will give a great performance
boost by minimizing the number of ﬁles that need to be
checked. Only the unveriﬁed ﬁles (i.e., ﬁles that fail whitelist
checks) need to be checked, which represent only a small
portion of the total number of ﬁles; 90% of the VAs contain
less than 4.5% of unveriﬁed ﬁles (see Figure 3). For VAs that
fall under ILG C (see Table 3), this would imply checking
only about 2,012 unveriﬁed ﬁles through a virus scanner, as
opposed to checking all 52,198 ﬁles.
To demonstrate the usefulness of the combined approach
and expanded integrity scores (see above), we checked all
the unveriﬁed ﬁles against VirusTotal3 [9], which is an on-
3We thank VirusTotal for allowing us to scan a huge volume
of ﬁles.
5.3 Combining integrity scores with security
vulnerability assessment results
The expanded integrity scores would still provide only a
partial view of how secure a VA might be. A VA that is
equipped with 100% cleanly installed packages is likely to be
240more secure than a VA that has some modiﬁed packages, but
not always. If some of the clean packages have known secu-
rity vulnerabilities, then the ﬁrst VA might be equally inse-
cure or even more insecure than the second VA. Hence, to
make strong claims about VA security, we also need to con-
sider whether packages have known security vulnerabilities,
and combine this information with their integrity scores.
Numerous security companies today assess software vul-
nerabilities and report them. This information could be used
to build a more complete veriﬁcation report, also indicating
which packages are known to be secure or have known vul-
nerabilities. The corresponding integrity scores would then
provide a stronger indication of how secure that VA is. We
imagine that the providers could make use of this informa-
tion and deﬁne a whitelist of software packages or combina-
tions of packages that are known to be secure; it would also
be possible to create a blacklist of packages that are known
to have security vulnerabilities or bugs. Compliance policies
could then be deﬁned based on the whitelist and blacklist,
specifying which software combinations are allowed to be
published and which should not be admitted.
6. CONCLUSIONS AND FUTURE WORK
We studied the integrity of software packages in real-world
virtual appliances (VAs) through a software whitelist-based
framework, and found high variance in the software integrity
across VAs. Our analysis of 151 Amazon VAs showed that
about 9% of those real-world VAs were conﬁgured with a
signiﬁcant portion of modiﬁed (low-integrity) packages with-
out any indication of the publishers’ eﬀorts to customize
them—demonstrating the need for a priori assessment of
software integrity to help consumers pick correctly installed
VAs. Virus scanners were able to ﬂag only about half of
the VAs in that 9%, showing that the whitelisting-based in-
tegrity assessment has a role to play, and may complement
blacklisting techniques like virus scanners in helping con-
sumers pick more reliable VAs. While whitelist-based tech-
niques are hard to realize in practice, we ﬁnd that because a
large number of software packages are installed on multiple
VAs in the case of appliance stores, the rate at which the
whitelist size grows slows down over time.
In the future, we plan to further expand the integrity as-
sessment framework to compute an overall trust score for a
VA based on the integrity scores assigned to individual soft-
ware packages. In doing so, we will consider other factors
such as the criticality of a software package and consumer’s
preference order for packages. Future work may also look at
VAs that are built on Windows or other Linux distributions
(e.g., dpkg-based distributions) to further validate the key
ﬁndings presented in this paper.
Acknowledgments
This material is based on research sponsored in part by the
Air Force Research Laboratory and the Air Force Oﬃce of
Scientiﬁc Research, under agreement number FA8750-11-2-
0084. The authors would like to thank Jenny Applequist for
her editorial assistance, and the anonymous reviewers for
their careful attention and insightful comments.
7. REFERENCES
[1] Amazon Elastic Compute Cloud.
http://aws.amazon.com/ec2/.
[2] Amazon Machine Images (AMIs).
http://aws.amazon.com/amis.
[3] Bit9 Global Software Registry. http://www.bit9.com/
products/bit9-global-software-registry.php.
[4] BitNami Virtual Images.
http://bitnami.org/learn_more/virtual_machines.
[5] CUBRID Virtual Images.
http://www.cubrid.org/virtual_machine_images.
[6] IBM SmartCloud.
[7] NIST National Software Reference Library.
http://www.nsrl.nist.gov/.
[8] Thunderﬂash Pre-Built Virtual Images.
http://thunderflash.com/.
[9] Virustotal. https://www.virustotal.com/.
[10] VMWare Solution Exchange.
https://solutionexchange.vmware.com/.
[11] Security Guidance for Critical Areas of Focus in Cloud
Computing. http://www.cloudsecurityalliance.org/
guidance/csaguide.pdf, April 2009.
[12] AntiVirus Performance Statistics.
http://winnow.oitc.com/malewarestats.php, August
2012.
[13] G. Ammons, V. Bala, T. Mummert, D. Reimer, and
X. Zhang. Virtual machine images as structured data: the
mirage image library. In Proceedings of the 3rd USENIX
conference on Hot topics in cloud computing, HotCloud’11,
pages 22–22, Berkeley, CA, USA, 2011. USENIX
Association.
[14] M. Armbrust, A. Fox, R. Griﬃth, A. D. Joseph, R. Katz,
A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica,
and M. Zaharia. A view of cloud computing. Commun.
ACM, 53(4):50–58, Apr. 2010.
[15] K. D. Bowers, A. Juels, and A. Oprea. Hail: a
high-availability and integrity layer for cloud storage. In
E. Al-Shaer, S. Jha, and A. D. Keromytis, editors, ACM
Conference on Computer and Communications Security,
pages 187–198. ACM, 2009.
[16] K. D. Bowers, M. van Dijk, A. Juels, A. Oprea, and R. L.
Rivest. How to tell if your cloud ﬁles are vulnerable to drive
crashes. In Y. Chen, G. Danezis, and V. Shmatikov,
editors, ACM Conference on Computer and
Communications Security, pages 501–514. ACM, 2011.
[17] Bugiel, Sven and N¨urnberger, Stefan and P¨oppelmann,
Thomas and Sadeghi, Ahmad-Reza and Schneider,
Thomas. AmazonIA: when elasticity snaps back. In
Proceedings of the 18th ACM conference on Computer and
communications security, pages 389–400, New York, NY,
USA, 2011. ACM.
[18] B. Danev, R. J. Masti, G. O. Karame, and S. Capkun.
Enabling secure VM-vTPM migration in private clouds. In
Proceedings of the 27th Annual Computer Security
Applications Conference, pages 187–196, New York, NY,
USA, 2011. ACM.
[19] T. Garﬁnkel and M. Rosenblum. When virtual is harder
than real: security challenges in virtual machine based
computing environments. In Proceedings of the 10th
conference on Hot Topics in Operating Systems - Volume
10, HOTOS’05, pages 20–20, Berkeley, CA, USA, 2005.
USENIX Association.
[20] J. H. Huh, H. Kim, J. Lyle, and A. Martin. Achieving
attestation with less eﬀort: an indirect and conﬁgurable
approach to integrity reporting. In Proceedings of the sixth
ACM workshop on Scalable trusted computing, pages
31–36, New York, NY, USA, 2011. ACM.
[21] K. R. Jayaram, C. Peng, Z. Zhang, M. Kim, H. Chen, and
H. Lei. An empirical analysis of similarity in virtual
machine images. In Proceedings of the Middleware 2011
Industry Track Workshop, Middleware ’11, pages 6:1–6:6,
New York, NY, USA, 2011. ACM.
[22] John A. Rice. Mathematical Statistics and Data Analysis,
chapter 10, page 365. Duxbury Press, 2 edition, 1994.
241[23] G. Kim and E. Spaﬀord. The design and implementation of
tripwire: A ﬁle system integrity checker. In Proceedings of
the 2nd ACM Conference on Computer and
Communications Security, pages 18–29. ACM, 1994.
[24] N. Leavitt. Is cloud computing really ready for prime time?
Computer, 42(1):15 –20, jan. 2009.
[25] N. Quynh and Y. Takefuji. A novel approach for a
ﬁle-system integrity monitor tool of xen virtual machine. In
Proceedings of the 2nd ACM symposium on Information,
computer and communications security, pages 194–202.
ACM, 2007.
[26] D. Reimer, A. Thomas, G. Ammons, T. Mummert,
B. Alpern, and V. Bala. Opening black boxes: using
semantic information to combat virtual machine image
sprawl. In Proceedings of the fourth ACM
SIGPLAN/SIGOPS international conference on Virtual
execution environments, VEE ’08, pages 111–120, New
York, NY, USA, 2008. ACM.
[27] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey,
you, get oﬀ of my cloud: exploring information leakage in
third-party compute clouds. In Proceedings of the 16th
ACM conference on Computer and communications
security, CCS ’09, pages 199–212, New York, NY, USA,
2009. ACM.
[28] T. Ristenpart and S. Yilek. When good randomness goes
bad: Virtual machine reset vulnerabilities and hedging
deployed cryptography. In NDSS. The Internet Society,
2010.
[29] H. Takabi, J. Joshi, and G. Ahn. Security and privacy
challenges in cloud computing environments. Security
Privacy, IEEE, 8(6):24 –31, nov.-dec. 2010.
[30] TCG. TCG Infrastructure Working Group Architecture
Part II - Integrity Management.
http://www.trustedcomputinggroup.org/resources/
infrastructure_work_group_architecture_part_ii_
_integrity_management_version_10, November 2006.
[31] D. Vincenzetti and M. Cotrozzi. Anti tampering program.
In Proceedings of the Fourth {USENIX} Security
Symposium, Santa Clara, CA. USENIX, 1993.
[32] J. Wei, X. Zhang, G. Ammons, V. Bala, and P. Ning.
Managing security of virtual machine images in a cloud
environment. In Proceedings of the 2009 ACM workshop on
Cloud computing security, CCSW ’09, pages 91–96, New
York, NY, USA, 2009. ACM.
[33] Y. Zhang, A. Juels, A. Oprea, and M. K. Reiter.
Homealone: Co-residency detection in the cloud via
side-channel analysis. In IEEE Symposium on Security and
Privacy, pages 313–328. IEEE Computer Society, 2011.
[34] W. Zhou, P. Ning, X. Zhang, G. Ammons, R. Wang, and
V. Bala. Always up-to-date: scalable oﬄine patching of vm
images in a compute cloud. In Proceedings of the 26th
Annual Computer Security Applications Conference,
ACSAC ’10, pages 377–386, New York, NY, USA, 2010.
ACM.
APPENDIX
A. AMORTIZING PERFORMANCE OVER-
HEADS
Mirage [13, 26] generates an index of a VA using the VA’s
ﬁlesystem structure in order to simplify maintenance and
management of a large collection of images. During index
generation, Mirage already computes hashes of all the ﬁles
in the VA to detect duplicate ﬁles in the system. Integration
of our framework with the Mirage library would involve only
the addition of one step to look up the hash of the ﬁles in
a whitelist when a ﬁle is ﬁrst encountered. Thus, in addi-
tion to the other beneﬁts of using Mirage, including reduced
storage cost and query capabilities, the cost of producing
the veriﬁcation reports can be signiﬁcantly amortized.
"
$
"
.
/
0
4
7
"
;
:
9
8
"
7
.
6
3
5
4
3
2
"
1
0
"
/
.
-
,
+
*
’!!"
&#!"
&!!"
%#!"
%!!"
$#!"
$!!"
#!"
!"
!"
%!"
’!"
(!"
)!"
$!!"
$%!"
$’!"
$(!"
$)!"
%!!"
*+,-./"01"234536.7"89:;"740/."%"
Figure 8: Number of software packages with score 1
vs. number of software packages with score 2
B. NUMBER OF MEDIUM-INTEGRITY PACK-
AGES VS NUMBER OF LOW-INTEGRITY
PACKAGES
There is a correlation between the number of medium-
integrity (partially clean) packages and the number of low-
integrity (modiﬁed) packages (see Figure 8). As the number
of packages with score 1 increases, so does the number of
packages with score 2 , and vice versa.
C. TMP AND VAR FILES
Table 9: Most common ﬁles in “var” folders
# VAs Absolute path
var/log/wtmp
151
var/log/spooler
151
var/log/secure
151
151
var/log/messages
var/log/maillog
151
var/lib/rpm/Triggername
151
var/lib/rpm/Sigmd5
151
var/lib/rpm/Sha1header
151
151
var/lib/rpm/Requireversion
var/lib/rpm/Requirename
151
As Figure 9 shows, most of the var ﬁles are log ﬁles and
rpm database ﬁles, which, we argue, can be deleted without
aﬀecting the VA behavior. The rpm database, if needed, can
be reconstructed after a VA is instantiated. We propose that
temporary ﬁles be scrubbed from the VA before publication.
Similarly, ﬁles in the var directory should also be scrubbed
before publication, unless the software vendor metadata or
the publisher’s log explicitly mention the ﬁle as necessary.
242