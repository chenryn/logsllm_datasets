ing a whitelist of sites that may embed the target element, but doing
so is often impractical: Facebook would have to whitelist much of the
web for the Like button!
4
ClickIDS [5] was proposed to reduce the false positives
of ClearClick by alerting users only when the clicked el-
ement overlaps with other clickable elements. Unfortu-
nately, ClickIDS cannot detect attacks based on partial
overlays or cropping, and it still yields false positives.
Finally, a fundamental limitation of techniques that
verify browser-rendered bitmaps is that cursor icons are
not captured; thus, pointer integrity is not guaranteed.
To address this caveat, ClearClick checks the computed
cursor style of the clicked element (or its ancestors) to
detect cursor hiding. Unfortunately, cursor spooﬁng at-
tacks can still be effective against some users even if the
default cursor is visible over the target element, as dis-
cussed in Section 7.2.
3.2.2 Protecting temporal context
Although we’re not aware of any timing attacks used in
the wild, browser vendors have started to tackle these
issues, particularly to protect browser security dialogs
(e.g., for ﬁle downloads and extension installations) [34].
One common way to give users enough time to compre-
hend any UI changes is to impose a delay after displaying
a dialog, so that users cannot interact with the dialog un-
til the delay expires. This approach has been deployed in
Flash Player’s webcam access dialog, suggested in Za-
lewski’s proposal [51], and also proposed in the Gazelle
web browser [45]. In response to our vulnerability re-
port, ClearClick has added a UI delay for cross-origin
window interactions [24].
Unresponsive buttons during the UI delay have report-
edly annoyed many users. The length of the UI delay
is clearly a tradeoff between the user experience penalty
and protection from timing attacks. Regardless, UI delay
is not a complete answer to protecting temporal integrity,
and we construct an attack that successfully defeats a UI
delay defense in Section 4.3.
3.2.3 Access Control Gadgets
Access control gadgets (ACG) [30] were recently intro-
duced as a new model for modern OSes to grant appli-
cations permissions to access user-owned resources such
as camera or GPS. An ACG is a privileged UI which can
be embedded by applications that need access to the re-
source represented by the ACG; authentic user actions on
an ACG grant its embedding application permission to
access the corresponding resource. The notion of ACGs
is further generalized to application-speciﬁc ACGs, al-
lowing applications to require authentic user actions for
application-speciﬁc functionality. Application-speciﬁc
ACGs precisely capture today’s web widgets that de-
mand a clickjacking defense.
ACGs require clickjacking resilience. While Roesner
et al’s design [30] considered maintaining both visual
and temporal integrity, they did not consider pointer in-
Figure 1: Cursor spooﬁng attack page. The target Flash Player webcam settings dialog is at the bottom right of the page, with a
“skip this ad” bait link remotely above it. Note there are two cursors displayed on the page: a fake cursor is drawn over the “skip
this ad” link while the actual pointer hovers over the webcam access “Allow” button.
tegrity and did not evaluate various design parameters.
In this work, we comprehensively address these issues,
and we also establish the taxonomy of context integrity
explicitly.
3.2.4 Discussion
We can conclude that all existing clickjacking defenses
fall short in some way, with robustness and site compat-
ibility being the main issues. Moreover, a glaring omis-
sion in all existing defenses is the pointer integrity at-
tacks described in Section 3.1.2. In Section 5, we will
introduce a browser defense that (1) does not require user
prompts, unlike ClearClick and Facebook’s Likejacking
defense, (2) provides pointer integrity, (3) supports tar-
get elements that require arbitrary third-party embed-
ding, unlike framebusting, (4) lets sites opt in by indi-
cating target elements, avoiding false positives that exist
in ClearClick, and (5) is more robust against timing at-
tacks than the existing UI delay techniques.
3.3 Our contributions
The major contributions of this paper are in evaluating
the effectiveness of existing attack techniques as well as
designing and evaluating a new defense. Our evaluation
uses several new attack variants (described in Section 4)
which build on existing techniques described in Sec-
tion 3.1, including cursor manipulation, fast-paced ob-
ject clicking, and double-click timing. Whereas most ex-
isting proof-of-concepts have focused on compromising
target display integrity, we focus our analysis on pointer
integrity and temporal integrity, as well as on combin-
ing several known techniques in novel ways to increase
effectiveness and bypass all known defenses.
4 New Attack Variants
To demonstrate the insufﬁciency of state-of-the-art de-
fenses described above, we construct and evaluate three
attack variants using known clickjacking techniques. We
have designed the new attack scenarios to be potentially
more damaging than existing clickjacking attacks in the
face of current defenses. We describe each in turn.
4.1 Cursor spooﬁng attack to steal webcam access
We ﬁrst crafted a cursor spooﬁng attack (Section 3.1.2) to
steal access to a private resource of a user: the webcam.
In this attack, the user is presented with an attack page
shown in Figure 1. A fake cursor is programmatically
rendered to provide false feedback of pointer location to
the user, in which the fake cursor gradually shifts away
from the hidden real cursor while the pointer is moving.
A loud video ad plays automatically, leading the user to
click on a “skip this ad” link. If the user moves the fake
cursor to click on the skip link, the real click actually
lands on the Adobe Flash Player webcam settings dialog
that grants the site permission to access the user’s web-
cam. The cursor hiding is achieved by setting the CSS
cursor property to none, or a custom cursor icon that is
completely transparent, depending on browser support.
4.2 Double-click attack to steal user private data
Today’s browsers do not protect temporal integrity for
web sites. We show in our second attack that even if
a security-critical web page (such as an OAuth dialog
page) successfully employs framebusting (refusing to be
embedded by other sites), our attack can still success-
fully clickjack such a page by compromising temporal
integrity for popup windows.
We devised a bait-and-switch double-click attack
(Section 3.1.3) against the OAuth dialog for Google ac-
counts, which is protected with X-Frame-Options. The
attack is shown in Figure 2. First, the attack page baits
the user to perform a double-click on a decoy button.
After the ﬁrst click, the attacker switches in the Google
OAuth pop-up window under the cursor right before the
5
Fake cursor Real cursor Figure 3: Whack-a-mole attack page. This is a cursor spoof-
ing variant of the whack-a-mole attack. On the 18th trial, the
attacker displays the target Like button underneath the actual
pointer.
click on it.
In 2010, Wondracek et al. [48] showed that it is fea-
sible for a malicious web site to uniquely identify 42%
of social network users that use groups by exploiting
browsing history leaks. Fortunately, the history snifﬁng
technique required in their attack is no longer feasible
in major browsers due to Baron’s patch [6]. However,
we ﬁnd that our whack-a-mole attack above, and Like-
jacking attacks in general, can still easily reveal the vic-
tim’s real identity at the time of visit and compromise
user anonymity in web surﬁng as follows.
Consider an attacker who is an admin for a Face-
book page; the attacker crafts a separate malicious page
which tricks users to click on his Like button. That
page is notiﬁed when a victim clicks on the Like button
via FB.Event.subscribe(), triggering the attacker’s
server to pull his fan list from Facebook and instantly
identify the newly added fan. The attacker’s server could
then query the victim’s proﬁle via Facebook Graph API
(and remove the victim fan to avoid suspicion). While we
implemented this logic as a proof-of-concept and veriﬁed
its effectiveness, we did not test it on real users.
In Section 7, we show our results on the effectiveness
InContext Defense
of all these attacks on Mechanical Turk users.
5
As described in Section 1, the root cause of clickjacking
is that an attacker application presents a sensitive UI el-
ement of a target application out of context to the user,
and hence the user gets tricked to act out of context.
Figure 2: Double-click attack page. The target OAuth dia-
log popup window appears underneath the pointer immediately
after the ﬁrst click on the decoy button.
second click (the second half of the double-click). This
attack can steal a user’s emails and other private data
from the user’s Google account.
The double-click attack technique was previously dis-
cussed in the context of extension installation dialogs by
Ruderman [33].
4.3 Whack-a-mole attack to compromise web
surﬁng anonymity
In our third attack, we combine the approaches from
the previous two attacks, cursor spooﬁng and bait-and-
switch, to launch a more sophisticated whack-a-mole
attack that combines clickjacking with social plugins
(e.g., Facebook Like button) to compromise web surﬁng
anonymity.
In this attack, we ask the user to play a whack-a-mole
game and encourage her to score high and earn rewards
by clicking on buttons shown at random screen locations
as fast as possible. Throughout the game, we use a fake
cursor to control where the user’s attention should be. At
a later point in the game, we switch in a Facebook Like
button at the real cursor’s location, tricking the user to
6
Enforcing context integrity for an application is essen-
tially one aspect of application isolation, in addition to
memory and other resource access. Namely, the context
for a user’s action in the application should be protected
from manipulation by other applications. We believe it
is an OS’s (or a browser’s) role to provide such cross-
application (or cross-web-site) protection.
Section 1 introduced two dimensions of context in-
tegrity: visual and temporal. Enforcing visual integrity
ensures that the user is presented with what she should
see before an input action. Enforcing temporal integrity
ensures that the user has enough time to comprehend
what UI element they are interacting with.
We describe our design for each in turn.
5.1 Ensuring Visual Integrity
To ensure visual integrity at the time of a sensitive user
action, the system needs to make the display of both the
sensitive UI elements and the pointer feedback (such as
cursors, touch feedback, or NUI input feedback) fully
visible to the user. Only when both the former (target
display integrity) and the latter (pointer integrity) are sat-
isﬁed, the system activates sensitive UI elements and de-
livers user input to them.
5.1.1 Guaranteeing Target Display Integrity
Although it is possible to enforce the display integrity
of all the UI elements of an application, doing so would
make all the UI elements inactivated if any part of the
UI is invisible. This would burden users to make the
entire application UI unobstructed to carry out any in-
teractions with the application. Such whole-application
display integrity is often not necessary. For example, not
all web pages of a web site contain sensitive operations
and are susceptible to clickjacking. Since only appli-
cations know which UI elements require protection, we
let web sites indicate which UI elements or web pages
are sensitive. This is analogous to how HTML5 [43]
and some browsers [32] (as well as earlier research on
MashupOS [44]) allow web sites to label certain content
as “sandboxed”. The sandboxed content is isolated so
that it cannot attack the embedding page. In contrast, the
sensitive content is protected with context integrity for
user actions, so that the embedding page cannot click-
jack the sensitive content.
We considered several design alternatives for provid-
ing target display integrity, as follows.
Strawman 1: CSS Checking. A na¨ıve approach is to let
the browser check the computed CSS styles of elements,
such as the position, size, opacity and z-index, and
make sure the sensitive element is not overlaid by cross-
origin elements. However, various techniques exist to
bypass CSS and steal topmost display, such as using IE’s
createPopup() method [25] or Flash Player’s Window
Figure 4: Ensuring target element display integrity. Here,
the attacker violates visual context of the Twitter Follow but-
ton by changing its opacity and obstructing it with two DIVs.
InContext detects this during its bitmap comparison. Obstruc-
tions from other windows are also detected (e.g., the non-
browser Vi window on the right).
Mode [2]. Solely relying on CSS checking is not reliable
and thus insufﬁcient.
Strawman 2: Static reference bitmap. Another ap-
proach is to let a web site provide a static bitmap of
its sensitive element as a reference, and let the browser
make sure the rendered sensitive element matches the
reference. Flash Player uses this approach for protect-
ing its webcam access dialog (Section 3.2.1). However,
different browsers may produce slightly differently ren-
dered bitmaps from the same HTML code, and it would
be too burdensome for developers to serve different ref-
erence bitmaps for different browsers. Furthermore, this
approach fails when sensitive elements contain animated
content, such as button mouseover effects, or dynami-
cally generated content, such as the amount to pay in a
checkout UI.
Our design. InContext enforces target display integrity
by comparing the OS-level screenshot of the area that
contains the sensitive element (what the user sees), and
the bitmap of the sensitive element rendered in isolation
at the time of user action. If these two bitmaps are not the
same, then the user action is canceled and not delivered
to the sensitive element. Figure 4 illustrates this process.
In the Likejacking attack example in Section 1, when
a user clicks on the “claim your iPad” button, the trans-
parent Facebook Like button is actually clicked, as the
browser unconditionally delivered the click event to the
Facebook Like button. With our defense, Facebook can
label its Like button web page as “sensitive” in the corre-
sponding HTTP response. The browser will then per-
form the following tasks before delivering each click
event to the Like button. The browser ﬁrst determines
what the user sees at the position of the Like button on
the screen by taking a screenshot of the browser window
and cropping the sensitive element from the screenshot
based on the element’s position and dimenions known
by the browser. The browser then determines what the
7
Reference bitmap: OS screenshot: sensitive element should look like if rendered in isola-
tion and uses this as a reference bitmap. To this end, the
browser draws the sensitive element on a blank surface
and extracts its bitmap. The browser then compares the
cropped screenshot with the reference bitmap. A mis-
match here means that the user does not fully see the Like
button but her click targets the Like button. In this case,
the browser detects a potential clickjacking offense and
cancels the delivery of the click event. Instead, it triggers
a new oninvalidclick event to give the application an
opportunity to deal with such occasions.
This design is resilient to new visual spooﬁng attack
vectors because it uses only the position and dimension
information from the browser layout engine to determine
what the user sees. This is much easier to get right than
relying on other sophisticated logic (such as CSS) from
the layout engine to determine what the user sees. By
obtaining the reference bitmap at the time of the user ac-
tion on a sensitive UI element, this design works well
with dynamic aspects (such as animations or movies) in
a sensitive UI element, unlike Strawman 2 above.
We also enforce that a host page cannot apply any CSS
transforms [42] (such as zooming, rotating, etc.)
that
affect embedded sensitive elements; any such transfor-
mations will be ignored by InContext-capable browsers.
This will prevent malicious zooming attacks [36], which
change visual context via zoom. We also disallow any
transparency inside the sensitive element itself. Al-
though doing so may have a compatibility cost in terms
of preventing legitimate blending effects of the sensitive
element with the host page, we believe this is a necessary
restriction, since otherwise attackers could violate visual
context by inserting decoys that could show through the
sensitive element.
Our bitmap comparison is similar to ClearClick (Sec-
tion 3.2.1), with two crucial differences:
(1) We use
OS APIs to take a screenshot of the browser window,
rather than relying on the browser to generate screen-
shots, making it more robust to rendering performed by
Flash Player and other plug-ins, and (2) our approach
is opt-in, eliminating false positives and obviating user
prompts.
5.1.2 Guaranteeing Pointer Integrity
Without pointer integrity support, an attacker could spoof
the real pointer. For example, an attack page may show
a fake cursor to shift the user’s attention from the real
cursor and cause the user to act out of context by not
looking at the destination of her action. To mitigate this,
we must ensure that users see system-provided (rather
than attacker-simulated) cursors and pay attention to the
right place before interacting with a sensitive element.
For our design, we consider the following techniques,
to under-
individually and in various combinations,
stand the tradeoff between their effectiveness of stop-
ping pointer-spooﬁng attacks and intrusiveness to users.
Some of the techniques limit the attackers’ ability to
carry out pointer-spooﬁng attacks; others draw attention
to a particular place on the screen.
No cursor customization. Current browsers disallow
cross-origin cursor customization. We further restrict
this policy: when a sensitive element is present, In-
Context disables cursor customization on the host page
(which embeds the sensitive element) and on all of the
host’s ancestors, so that a user will always see the system
cursor in the areas surrounding the sensitive element.
Our opt-in design is better than completely disallow-
ing cursor customization, because a web site may want
to customize the pointer for its own UIs (i.e., same-origin
customization). For example, a text editor may want to
show different cursors depending on whether the user is
editing text or selecting a menu item.
Screen freezing around sensitive element. Since hu-
mans typically pay more attention to animated objects
than static ones [15], attackers could try to distract a user
away from her actions with animations. To counter this,
InContext “freezes” the screen (i.e., ignores rendering
updates) around a sensitive UI element when the cursor
enters the element.
Muting. Sound could also draw a user’s attention away
from her actions. For example, a voice may instruct the
user to perform certain tasks, and loud noise could trigger
a user to quickly look for a way to stop the noise. To
stop sound distractions, we mute the speakers when a