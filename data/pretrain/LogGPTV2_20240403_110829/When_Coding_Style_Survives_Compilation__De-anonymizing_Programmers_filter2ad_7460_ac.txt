the most meaningful way.
V. GOOGLE CODE JAM EXPERIMENTS
In this section, we go over the details of the various
experiments we performed to address the research question
formulated in Section II.
A. Dataset
We evaluate our executable binary authorship attribution
method on a controlled dataset based on the annual pro-
gramming competition GCJ [5]. It is an annual contest that
thousands of programmers take part in each year, including
professionals, students, and hobbyists from all over the world.
The contestants implement solutions to the same tasks in a
limited amount of time in a programming language of their
choice. Accordingly, all the correct solutions have the same
algorithmic functionality. There are two main reasons for
choosing GCJ competition solutions as an evaluation corpus.
First, it enables us to directly compare our results to previous
work on executable binary authorship attribution as both [9]
and [39] evaluate their approaches on data from GCJ. Second,
we eliminate the potential confounding effect of identifying
programming task rather than programmer by identifying func-
tionality properties instead of stylistic properties. GCJ is a less
noisy and clean dataset known deﬁnitely to be single authored.
GCJ solutions do not have signiﬁcant dependencies outside of
the standard library and contain few or no third party libraries.
We focus our analysis on compiled C++ code, the most
popular programming language used in the competition. We
collect the solutions from the years 2008 to 2014 along with
author names and problem identiﬁers. In GCJ experiments we
are assuming that the programmers are not deliberately trying
to hide their identity. Accordingly, we show results without
excluding symbol information.
B. Code Compilation
To create our experimental datasets, we ﬁrst compiled the
source code with GNU Compiler Collection’s gcc or g++
without any optimization to Executable and Linkable Format
(ELF) 32-bit, Intel 80386 Unix binaries. The training set needs
to be compiled with the same compiler and settings otherwise
we might end up detecting the compiler instead of the author.
Passing the training samples through the same encoder pre-
serves mutual information between code style and labels and
accordingly we can successfully de-anonymize programmers.
Next, to measure the effect of different compilation options,
such as compiler optimization ﬂags, we additionally compiled
the source code with level-1, level-2, and level-3 optimizations,
namely the O1, O2, and O3 ﬂags. O3 is a superset of O2
optimization ﬂags and similarly O2 is a superset of O1 ﬂags.
The compiler attempts to improve the performance and/or code
size when the compiler ﬂags are turned on but at the same
time optimization has the expense of increasing compilation
time and complicating program debugging.
C. 53 features represent programmer style.
We are interested in identifying features that represent
coding style preserved in executable binaries. With the current
approach, we extract 705,000 representations of code proper-
ties of 100 authors, but only a subset of these are the result
of individual programming style. We are able to capture the
features that represent each author’s programming style that is
preserved in executable binaries by applying information gain
criteria to these 705,000 features. After applying information
gain to effectively represent coding style, we reduce the
feature set to contain approximately 1,600 features from all
feature types. Furthermore, correlation based feature selection
during cross validation eliminates features that have low class
correlation and high intercorrelation and preserves 53 of the
highly distinguishing features which can be seen in Table I
along with their authorial style representation power.
Considering the fact that we are reaching such high accura-
cies on de-anonymizing 100 programmers with 900 executable
binary samples (discussed below), these features are providing
strong representation of style that survives compilation. The
compact set of identifying stylistic features contain features
of all types, namely disassembly, CFG, and syntactical de-
compiled code properties. To examine the potential for over-
ﬁtting, we consider the ability of this feature set to generalize
to a different set of programmers (see Section V-G), and
show that it does so, further supporting our belief that these
features effectively capture coding style. Features that are
highly predictive of authorial ﬁngerprints include ﬁle and
stream operations along with the formats and initializations of
variables from the domain of ASTs whereas arithmetic, logic,
and stack operations are the most distinguishing ones among
the assembly instructions.
D. We can de-anonymize programmers from their executable
binaries.
This is the main experiment that demonstrates how de-
anonymizing programmers from their executable binaries is
possible. After preprocessing the dataset to generate the exe-
cutable binaries without optimization, we further process the
executable binaries to obtain the disassembly, control ﬂow
graphs, and decompiled source code. We then extract all the
possible features detailed in Section IV. We take a set of 100
programmers who all have 9 executable binary samples. With
9-fold-cross-validation, the random forest correctly classiﬁes
900 test instances with 95% accuracy, which is signiﬁcantly
higher than the accuracies reached in previous work.
There is an emphasis on the number of folds used in these
experiments because each fold corresponds to the implementa-
tion of the same algorithmic function by all the programmers
in the GCJ dataset (e.g. all samples in fold 1 may be attempts
by the various authors to solve a list sorting problem). Since
we know that each fold corresponds to the same Code Jam
7
Feature
Source
Number
of Possible
Features
29,278
5,278
26,783
5,278
5,278
73
21,206
39,506
112,913
260,265
5,297
10,246
5,383
14,305
5,237
Selected Features
Information Gain
/
6/5.75
3/1.85
0/0
1/0.75
0/0
0/0
3/1.61
1/0.62
0/0
0/0
3/1.98
1/0.63
2/1.79
5/2.95
4/1.44
159,142
24/16.08
705,468
53/35
TF†= term frequency
Word unigrams
AST
node TF†
Labeled AST
edge TF†
AST
node TFIDF‡
AST node
average depth
C++ keywords
disassembly
disassembly
radare2
disassembly unigrams
radare2
disassembly
bigrams
radare2
trigrams
radare2
6-grams
radare2 CFG
node unigrams
radare2
CFG edges
ndisasm disassembly
unigrams
ndisasm disassembly
bigrams
ndisasm disassembly
trigrams
ndisasm
disassembly 6-grams
Total
decompiled
code∗
decompiled
code∗
decompiled
code∗
decompiled
code∗
decompiled
code∗
decompiled
code∗
radare
disassembly
radare
disassembly
radare
disassembly
ndisasm
disassembly
radare
disassembly
radare
disassembly
ndisasm
disassembly
ndisasm
disassembly
ndisasm
disassembly
ndisasm
disassembly
∗hex-rays decompiled code
TFIDF‡= term frequency inverse document frequency
TABLE I: Programming Style Features and
Selected Features in Executable Binaries
y
c
a
r
u
c
c
A
n
o
i
t
a
c
ﬁ
i
s
s
a
l
C
t
c
e
r
r
o
C
100
90
80
70
60
94% 94%
95% 96%
91% 91%
85%
65%
1
2
3
4
5
6
7
8
Number of Training Samples Per Author
Fig. 3: Amount of Training Data Required for
De-anonymizing 100 Programmers
problem, by using stratiﬁed cross validation without random-
ization and preserving order, we ensure that all training and test
samples contain the same algorithmic functions implemented
by all of the programmers. The classiﬁer uses the excluded fold
in the testing phase, which contains executable binary samples
that were generated from an algorithmic function that was
not previously observed in the training set for that classiﬁer.
Consequently, the only distinction between the test instances
is the coding style of the programmer, without the potentially
confounding effect of identifying an algorithmic function.
8
E. Even a single training sample per programmer is sufﬁ-
cient for de-anonymization.
We therefore devised an experiment
A drawback of supervised machine learning methods,
which we employ, is that they require labeled examples to
build a model. The ability of the model to accurately generalize
is often strongly linked to the amount of data provided to it
during the training phase, particularly for complex models.
In domains such as executable binary authorship attribution,
where samples may be rare and obtaining “ground truth” for
labeling training samples may be costly or laborious, this can
pose a signiﬁcant challenge to the usefulness of the method.
to determine how
much training data is required to reach a stable classiﬁcation
accuracy, as well as to explore the accuracy of our method with
severely limited training data. As programmers produce a lim-
ited number of code samples per round of the GCJ competition,
and programmers are eliminated in each successive round, the
GCJ dataset has an upper bound in the number of code samples
per author as well as a limited number of authors with a large
number of samples. Accordingly, we identiﬁed a set of 100
programmers that had exactly 9 program samples each, and
examined the ability of our method to correctly classify each
author out of the candidate set of 100 authors when training
on between 1 and 8 ﬁles per author.
As shown in Figure 3, the classiﬁer is capable of correctly
identifying the author of a code sample from a potential ﬁeld of
100 with 65% accuracy on the basis of a single training sample.
The classiﬁer also reaches a point of dramatically diminishing
returns with as few as three training samples, and obtains a
stable accuracy by training on 6 samples. Given the complexity
of the task, this combination of high accuracy with extremely
low requirement on training data is remarkable, and suggests
the robustness of our features and method. It should be noted,
however that this set of programmers with a large number of
ﬁles corresponds to more skilled programmers, as they were
able to remain in the competition for a longer period of time
and thus produce this large number of samples.
F. Relaxed Classiﬁcation: In difﬁcult scenarios, the classi-
ﬁcation task can be narrowed down to a small suspect set.
In Section V-A, the previously unseen anonymous exe-
cutable binary sample is classiﬁed such that it belongs to the
most likely author’s class. In cases where we have too many
classes or the classiﬁcation accuracy is lower than expected,
we can relax the classiﬁcation to top–n classiﬁcation. In top–n
relaxed classiﬁcation, if the test instance belongs to one of the
most likely n classes, the classiﬁcation is considered correct.
This can be useful in cases when an analyst or adversary
is interested in ﬁnding a suspect set of n authors, instead
of a direct top–1 classiﬁcation. Being able to scale down an
authorship investigation for an executable binary sample of
interest to a reasonable sized set of suspect authors among
hundreds of authors greatly reduces the manual effort required
by an analyst or adversary. Once the suspect set size is reduced,
the analyst or adversary could adhere to content based dynamic
approaches and reverse engineering to identify the author of
the executable binary sample. Figure 4 shows how correct
classiﬁcation accuracies approach 100% as the classiﬁcation
is relaxed to top-10.
y
c