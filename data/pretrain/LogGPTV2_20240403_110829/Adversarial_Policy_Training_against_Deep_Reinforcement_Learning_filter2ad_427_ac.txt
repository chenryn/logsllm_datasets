of an opponent action, we need to ensure the minimal change
of the environment observation.
Recall that we do not assume an adversary has the privilege
to manipulate the environment, and, in a two-agent compet-
1888    30th USENIX Security Symposium
USENIX Association
……Optimal actionSuboptimal actiont1…Suboptimal actiont2Optimal actionWINtkLOSSo1...o5o6o7o8...o132.10.01.30.00.00.03.00.0ˆo1...ˆo5ˆo6ˆo7ˆo8...ˆo13s(t)ˆs(t)Figure 4: A heatmap indicating the input feature impor-
tance of the opponent policy network of Roboschool Pong
game. The highlighted features (o5···o8) represent those
corresponding to the adversarial action. The heapmap is gen-
erated by using the output of explainable AI techniques.
itive game, the action of the adversarial agent is converted
as part of the environment observation of its opponent agent.
Take the example shown in Figure 3. The opponent observa-
tion is depicted as a feature vector, within which some of the
features represent the adversarial actions. As such, we can
subtly manipulate the action of the adversarial agent and thus
change the features indicating the adversarial action. With
this, we can change the input to the opponent’s policy network
and indirectly deviate the action of the opponent agent.
However, as is shown in Figure 4, by performing a sensitiv-
ity check for the policy network against the input features over
time, we note that the opponent’s policy network takes the
importance of the input features differently over time. There-
fore, intuition suggests that the best strategy is to perform the
corresponding feature manipulation only at the time when
the opponent policy network pays sufﬁcient attention to the
features corresponding to the adversarial actions. To achieve
this, as we will specify below, we utilize an explanation AI
technique to examine the the victim policy network’ feature
importance at each time step. With this, we can pinpoint the
time frame when the victim policy network pays its attention
to the adversarial action, and thus employ an adjustable hyper-
parameter to control the level of action deviation adjustment.
4.2 More details
As is stated above, we design our attack in two steps – 
deviating the actions of the opponent agent with a minimal
change to its observation, and  adjusting the weight of the
action deviation of the opponent agent based on the inﬂuence
of the adversarial actions upon the opponent. In the following,
we specify how we implement this two-step design.
Deviating opponent actions. To deviate the action of the
opponent, we extend the PPO loss function LPPO mentioned
in Section 3. To be speciﬁc, we introduce into the PPO loss
function a new loss term
v
v
− o(t+1)
Lad = maximizeθ(−(cid:107) ˆo(t+1)
(cid:107) +(cid:107) ˆa(t+1)
(7)
(cid:107)) ,
v
and ˆa(t+1)
where θ represents the parameters in πα.
indicate the different observation and action taken by the op-
ponent agent if, at the time step t, the adversarial agent takes
an action different from the ones indicated by the trajectory
− a(t+1)
ˆo(t+1)
v
v
v
rollouts (i.e., different from the actions that the opponent is
supposed to take). As we can observe from the equation
above, the loss term contains two components. The design
of the ﬁrst component ensures that, when launching attacks,
an adversary introduces only minimal variations to the ob-
servation of the opponent agent. The design of the second
component forces the opponent agent to take a suboptimal
action ˆa(t+1)
, and thus trig-
ger the drop of its winning rate. It should be noted that we
compute both the action difference and observation difference
by using a norm, the output of which is a singular. As such,
when we can combine the observation and action differences
in a linear fashion.
but not the optimal action a(t+1)
v
v
As is mentioned in Section 2, neither the opponent policy
network πv nor its state-transition model pss(cid:48)
is available for
v
our method. Without the state-transition model, we cannot
predict the observation of the opponent agent ˆo(t+1)
at the
time step t + 1, when our adversarial agent takes an action
at the time step t and subtly varies the observation of the
opponent at the time step t + 1. Without the access to the
policy network, even if ˆo(t+1)
is given, we still cannot predict
the action of the opponent agent ˆa(t+1)
) at the
time step t + 1. This imposes the challenge of computing the
loss term Lad in Equation (7).
= πv( ˆo(t+1)
v
v
v
v
v
α , and that of the opponent agent a(t)
To tackle the challenge, our method approximates the op-
ponent policy network as well as its state-transition model
by using two individual deep neural networks. By deﬁnition,
the state-transition model outputs the predicted observation
of the opponent o(t+1)
at the time step t + 1. It takes as in-
put the observation of the opponent o(t)
v , the action of the
adversarial agent a(t)
v at
the time step t. As we specify in Section 5, we train both of
the neural networks by using trajectory rollouts. It should be
noted that, to train the surrogate model, the attack needs to
access victim observation and action, which is a legitimate
assumption (See Section 2). However, we also admit that the
proposed attack would become harder when this information
is not available. This is because the attacker needs extra effort
to infer such information and then train the surrogate model
with the approximated victim observation and action.
Adjusting weights of action deviation. As is mentioned
above, the opponent/victim agent weights the action of the
adversarial differently over time when deciding its own action
through its policy network. As a result, when leveraging the
action of the adversarial to inﬂuence the environment obser-
vation and thus the action of the opponent agent, we adjust
the weight of the action deviation based on by how much the
victim agent pays attention to the action of the adversarial.
To achieve this, when optimizing the extended loss function
Lppo + Lad, we introduce a hyperparameter λ, indicating the
importance of our newly added term Lad. With this, we can
rewrite the extended loss function as Lppo + λ· Lad. To max-
imize this loss function, we can adjust the weight assigned
USENIX Association
30th USENIX Security Symposium    1889
Time stepHigh to Lowt2t1t3...tK...o1o5o8o13.........to the new term (i.e., Lad) based on the weight that the oppo-
nent/victim agent pays attentions to the adversarial.
In this work, we utilize an explanation AI technique to
measure the weight that the victim agent pays attention to
the adversarial action. As is shown in Figure 3, the actions
of the adversarial are part of the observation of the victim
agent. They are encoded as part of the features passing to the
victim’s policy network. In Figure 3, we can easily observe
that a policy network is a deep neural network. Over the time,
the observation feature vector passing to the network varies.
Using an explanation AI technique at each time step against
the victim policy network, we can measure by how much the
policy network pays attention to the features corresponding
to the action of the adversarial.
Intuition suggests that, to obtain an optimal effect upon
the deviation of the opponent, the adversarial agent should
manipulate its actions at the time when the opponent pays its
attention to the adversary. Otherwise, the action manipulation
of the adversarial agent will introduce minimal inﬂuence upon
the action of the opponent agent. Following this intuition, we
assign the value for λ at each time step t based on the output
of an explainable AI technique. More speciﬁcally, we assign
a higher value to the weight λ when the opponent pays more
attention to the adversarial agent. Otherwise, we assign a
relatively low value on the weight to minimize the impact
of our newly added term. For more details of our weight
assignment, readers could refer to Section 5.
Over the past years, there are many techniques in the ﬁeld
of explanation AI research, ranging from black-box meth-
ods (e.g., [9, 39]) to white-box approaches (e.g., [46–48]).
Among all these explanation AI techniques, we choose
gradient-based interpretation methods, serving as the way
to weight the inﬂuence of the adversarial actions upon oppo-
nent’s policy network. The rationales behind our choice is
as follow. In comparison with other explanation AI methods,
such as some black-box methods [39] which need to perform
intensive data sampling before deriving explanation, gradient-
based methods are computationally efﬁcient. In the context
of deep reinforcement learning, the observation of the oppo-
nent/victim agent o(t)
v changes over time rapidly and we need
to adjust the hyperparameter λ at each time step. In this work,
we rely upon gradient-based methods, which can minimize
the computation needed for weight adjustment. Considering
that past research [1] indicates different gradient-based ex-
planation methods provide different accuracy in explanation,
we thoroughly evaluate by how much the choice a particular
gradient based method would inﬂuence the performance of
our attack. We show our evaluation results in Section 6.
5 Technical Detail
In this section, we provide more details about our proposed
method. More speciﬁcally, we ﬁrst formally deﬁne the prob-
lem that our method targets. Then, we specify the design
of our loss term. Finally, we discuss how we extend our
loss function through explainable AI and present our learning
algorithm as a whole.
5.1 Problem deﬁnition
Following the early research [44], we also formulate a two-
agent competitive game as a two-agent MDP, represented by
M =. Here, S denotes the state
set. Aα and Av are the action sets for adversarial and oppo-
nent agents, respectively. P represents a joint state transition
function P : S × Aα × Av → ∆(S ). The reward function can
be represented as Ri : S × Aα × Av → R; i ∈ {α,v}.
As is mentioned in Section 3, the state transition is a
stochastic process. Therefore, we use ∆(S ) to represent a
probability distribution on S, from which the state at each
time step can be sampled. Note that using the PPO algorithm
for training agents in a two-agent competitive game, we can-
not obtain the state S and the state transitions function P in
an explicit form. From the game environment, each of the
agents can get only its own observation Oi; i ∈ {α,v}.
In this paper, we assume that the opponent agent follows a
ﬁxed stochastic policy πv. Holding this assumption, our prob-
lem can be viewed as a single-agent MDP for the adversarial
agent, denoted by Mα =. Here, the state-
transition model Pα is unknown, and S is equivalent to the
observation of the adversarial agent Oα. Under this problem
deﬁnition, the goal of this work is to identify an adversarial
policy πα that can guide the corresponding agent to beat its
opponent in the single-agent MDP.
5.2 Expected reward maximization
As is described in Section 4, we extend the PPO loss function
when designing our proposed method. As is introduced in the
early section, the PPO loss function can be written as
, A(t) = Aπold
α
α )
(8)
ρ(t) =
((a(t)
α ,o(t)
α )) .
α )∼πold
α
α )
[min(clip(ρ(t),1− ε,1 + ε)A(t),ρ(t)A(t))] ,
maximizeθ E
α ,o(t)
(a(t)
α |o(t)
πα(a(t)
α |o(t)
α (a(t)
πold
Here, πold
α and πα denotes the old and new policy of the ad-
versarial agent, respectively. o(t)
α is the observation of the
adversarial agent at the time step t. It encloses the action
of the opponent agent a(t)
v . Following the standard PPO al-
gorithm, we use a neural network Vα(s) to approximate the
state-value function, and thus obtain the advantage A(t) at the
time step t. In this work, the model architectures of the state-
value function and the policy network are as same as those in
the PPO algorithm (see Figure 2). By solving the objective
function above, we could ﬁnd an adversarial policy πα, with
which the corresponding adversarial agent could maximize
the expected total reward: ∑∞
0 γ(t)Rα(s(t),a(t)
α ).
1890    30th USENIX Security Symposium
USENIX Association
v
v
v
v
(9)
(cid:107)1) .
− a(t+1)
− o(t+1)
(cid:107)1 −(cid:107) ˆo(t+1)
5.3 Action deviation maximization
Recall that we extend the PPO loss function by introducing a
new loss term
Lad = maximizeθ((cid:107) ˆa(t+1)
As is shown above, we choose l1 norm distance as the dif-
ference measure instead of l2 norm. This is because l1 norm
encourages a larger difference than l2 norm, especially when
Ov is of a high dimensionality [2]. As we will empirically
show in Section 6, an adversarial agent trained with the l1
norm usually demonstrates a stronger capability of beating
opponent agents than that trained with l2 norm.
State transition approximation. To predict the observation
of the opponent agent at the time step t + 1, we utilize a deep
neural network to approximate the state-transition model of
the opponent agent. As is mentioned in Section 4, the deep
neural network takes as input (o(t)
α ), and predicts
o(t+1)
(i.e., the observation of the opponent agent at the time
v
step t +1). In this work, we train this neural network by using