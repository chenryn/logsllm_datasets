title:Data Quality for Security Challenges: Case Studies of Phishing, Malware
and Intrusion Detection Datasets
author:Rakesh M. Verma and
Victor Zeng and
Houtan Faridi
Poster: Data Quality for Security Challenges: Case Studies of
Phishing, Malware and Intrusion Detection Datasets
Rakesh M. Verma, Victor Zeng and Houtan Faridi
University of Houston, Texas
{rverma,vzeng,hfaridi}@uh.edu
ABSTRACT
Techniques from data science are increasingly being applied by
researchers to security challenges. However, challenges unique to
the security domain necessitate painstaking care for the models
to be valid and robust. In this paper, we explain key dimensions
of data quality relevant for security, illustrate them with several
popular datasets for phishing, intrusion detection and malware,
indicate operational methods for assuring data quality and seek to
inspire the audience to generate high quality datasets for security
challenges.
KEYWORDS
Semiotics; data quality; data difficulty; data poisoning
ACM Reference Format:
Rakesh M. Verma, Victor Zeng and Houtan Faridi. 2019. Poster: Data Quality
for Security Challenges: Case Studies of Phishing, Malware and Intrusion De-
tection Datasets. In 2019 ACM SIGSAC Conference on Computer and Commu-
nications Security (CCS ’19), November 11–15, 2019, London, United Kingdom.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363267
1 INTRODUCTION
Researchers have been applying data mining and machine learning
techniques to security challenges for the past 20 years or so. A
search of the bibliographic database DBLP1 with the query intrusion
detection neural network led to 216 matches on August 21, 2019. Note
that this result is for just one machine learning technique and just
one security challenge. The mind boggles when one considers the
plethora of data mining and machine learning techniques and the
variety of security challenges, which have no good separations
between legitimate instances and attack instances. These include
phishing, malware, stepping-stone detection, intrusion detection,
and denial of service attacks, just to name a few.
However, some researchers had observed that the overwhelming
majority of this research is simply not being deployed [17]. So,
the question arises: “Why is there such a gap between theory and
practice in security?” The reasons seem to be mainly two:
• Researchers, in their work, have missed at least some of the
unique needs of the security domain. For a discussion on
these aspects, see [3, 20].
1https://dblp.uni-trier.de/
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11...$15.00
https://doi.org/10.1145/3319535.3363267
• The general lack of trustworthy datasets for security chal-
lenges. Although companies are collecting mountains of data,
they are reluctant to share for fear of the consequences.2
Datasets collected by academic researchers could be plagued
with problems of data quality: recency, diversity, class ratio,
scale, errors and consistency. Dataset poisoning attacks must
also be considered.
1.1 Poster Outline
We present the unique needs of the security domain, data quality
assessment, and issues with existing datasets. We examine ways of
discovering data quality problems and suggest some solutions for
the issues that arise. Illustrative examples of these topics include:
(1) Unique needs of security domain relevant to datasets: nonsta-
tionarity arising from an active attacker or streaming data;
lack of large, diverse and representative datasets; potential
poisoning of datasets, etc.
(2) Existing dimensions of data quality relevant to security do-
main from a semiotics standpoint [18], and new dimensions,
e.g., data difficulty and poisoning, inspired from our work.
(3) Illustration of data quality problems using specific datasets
for phishing, malware and intrusion detection.
(4) Methods for finding data quality problems.
(5) Suggestions on how to handle the issues that are found.
2 CASE STUDY: PHISHING DATASETS
A systematic review of phishing datasets [7] (used in almost 300
papers from CORE B or better venues published during 2010-2018),
including URL, website and email datasets, shows that they are: (i)
generally not recent, and, when recent, usually not available pub-
licly, (ii) tend to be small-scale (typically less than 106 instances),
(iii) almost balanced with respect to class labels, and (iv) sometimes
suffer from other issues. For example, several phishing URL detec-
tion papers have used domains from Alexa.com and URLs from
Phishtank with features involving URL length. In the phishing email
detection literature, we find the Nazario dataset of approximately
5000 phishing emails collected during early 2000s and a smaller
one from 2015-17. The Enron dataset has sanitized headers and the
SpamAssassin dataset has “lightly” sanitized headers. In previous
work [19], we had designed the “nonsense” filter for two kinds
of potential poisoning attacks, based on analysis of the Nazario
dataset. Due to these issues, we created four datasets for phishing
email detection, with and without email headers, that are public.
2.1 Creation of IWSPA-AP Email Dataset
Our objective for the IWSPA-AP dataset was to ensure diversity,
i.e., different types of attacks and legitimate emails as well as a mix
2At least one company seems to be going against the grain [3].
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2605of new and classical attacks, so we gathered recent and historical
emails from as many sources as possible [2]. Legitimate emails
were relatively easy to find compared to phishing ones, thanks to
Wikileaks. Phishing emails were collected from the IT departments
of different universities. We also included some emails from the
popular Nazario phishing corpora. Note that the emails collected
from universities’ IT departments usually do not have a full header,
so we only used these sources for the no-header subtask.
The dataset was cleaned by replacing all the URLs in the emails
with ⟨link⟩, since the URLs in legitimate emails tended to be quite
revealing. Another concern was the recognizability of the sources.
So we tried to remove from the emails, as much as feasible, any
signs that could hint at the origin of the datasets. For this purpose,
we included in the preprocessing steps the normalization of organi-
zations’ or universities’ names, recipients’ names, domain names,
signatures, threading, and removed non-English emails.
We also removed emails that are too big (more than 1 MB) or
too small and all base64 encoded text. To remove as much noise as
possible, we attempted to remove leftover HTML tags and empty
spacing that resulted from parsing the body of the email using an
HTML parser. As a final check before release, a logistic regression
model was trained and run on the test subsets.
2.2 Rigorous Analysis of IWSPA-AP Datasets
We ran a state of the art part-of-speech tagger on the email bodies
of the IWSPA-AP dataset and collected the 50 most frequent nouns.
The software revealed that, despite the extensive cleaning operation,
there were still significant number of nouns tied to the sources of
the emails in the phishing subset.
In 2019, researchers managed to achieve an accuracy of 99.848%
on the IWSPA-AP dataset using their THEMIS model [9]. We hy-
pothesized that one of the factors contributing to this high accuracy
was the “difficulty” of the dataset. To test this hypothesis, we ran
the PhishBench benchmark [1] with 69 header features and 48 body
features on the full-header subset of the dataset using only the first
50 legitimate and 10 phishing emails for training. We found that five
out of nine classifiers managed to achieve accuracy greater than 99%.
This prompted a more thorough experiment where we performed
20 round Monte-Carlo cross validation on the full-header dataset
using the same 69 header and 48 body features with a training set
size of 50 legit and 10 phish emails. In this second experiment, three
out of nine classifiers managed to achieve mean accuracy greater
than 99%, thus confirming the results of our initial experiment.
These findings led to another round of cleaning of the dataset
emails, resulting in Version 2.0. On this version, we normalized
variations of recipient organization names and domains missed
in the first round of cleaning. We then ran the 50-10 experiment
on Version 2.0 of the dataset and found that there was no signifi-
cant change in classifier performance. Consequently, we performed
another round of cleaning that removed various normalization arti-
facts left by the previous round and normalized any IP addresses to
254.254.254.254. This gave us Version 2.1.
Despite the additional rounds of data cleaning, running the 50-
10 experiment on Version 2.1 of the dataset showed no significant
change in classifier performance over Version 1.0. This suggests that
recipient information may be derivable from more subtle aspects
of the dataset such as the structure of the headers. We intend on
testing this hypothesis in future research. This leads to a new data
quality dimension, data difficulty, which can be defined as the min-
imum distance between two data instances from different classes
for binary classes, and the minimum or average of the pairwise
minimums for multiple classes.
3 CASE STUDY: MALWARE DATASETS
There are many factors in producing a robust malware dataset.
These include: (i) sufficient coverage of malware types and attack
vectors, (ii) recency and relevance of samples, (iii) multiple meth-
ods of feature analysis, (iv) sufficiently large families, (v) strong
coverage of multiple obfuscation and permutation techniques, and