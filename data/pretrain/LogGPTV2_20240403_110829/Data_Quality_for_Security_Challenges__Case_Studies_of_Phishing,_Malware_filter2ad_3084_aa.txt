# Title: Data Quality for Security Challenges: Case Studies of Phishing, Malware, and Intrusion Detection Datasets

**Authors:** Rakesh M. Verma, Victor Zeng, and Houtan Faridi  
**Affiliation:** University of Houston, Texas  
**Emails:** {rverma, vzeng, hfaridi}@uh.edu

## Abstract
Data science techniques are increasingly being applied to security challenges. However, the unique demands of the security domain require meticulous attention to ensure the validity and robustness of models. This paper explores key dimensions of data quality relevant to security, using popular datasets for phishing, intrusion detection, and malware as examples. We also present operational methods for ensuring data quality and aim to inspire the creation of high-quality datasets for security challenges.

## Keywords
Semiotics, data quality, data difficulty, data poisoning

## ACM Reference Format
Rakesh M. Verma, Victor Zeng, and Houtan Faridi. 2019. Poster: Data Quality for Security Challenges: Case Studies of Phishing, Malware, and Intrusion Detection Datasets. In 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19), November 11–15, 2019, London, United Kingdom. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363267

## 1. Introduction
For the past two decades, researchers have been applying data mining and machine learning techniques to address various security challenges. A search in the DBLP bibliographic database with the query "intrusion detection neural network" yielded 216 matches as of August 21, 2019. This result is just for one machine learning technique and one security challenge, highlighting the vast array of data mining and machine learning techniques and the diverse range of security challenges, such as phishing, malware, stepping-stone detection, intrusion detection, and denial-of-service attacks.

Despite this extensive research, many of these solutions are not being deployed [17]. The gap between theory and practice in security can be attributed to two main reasons:

- **Missed Unique Needs:** Researchers often overlook the unique requirements of the security domain. For a detailed discussion, see [3, 20].
- **Lack of Trustworthy Datasets:** Companies are hesitant to share their data due to potential consequences. Academic datasets may suffer from issues such as recency, diversity, class ratio, scale, errors, and consistency. Additionally, dataset poisoning attacks must be considered.

### 1.1 Poster Outline
This poster addresses the unique needs of the security domain, data quality assessment, and issues with existing datasets. We explore methods for identifying and resolving data quality problems, illustrated through specific datasets for phishing, malware, and intrusion detection. Key topics include:

1. **Unique Needs of the Security Domain:** Non-stationarity, streaming data, lack of large, diverse, and representative datasets, and potential dataset poisoning.
2. **Dimensions of Data Quality:** Existing and new dimensions, such as data difficulty and poisoning, from a semiotics perspective [18].
3. **Illustrative Examples:** Data quality problems in specific datasets for phishing, malware, and intrusion detection.
4. **Methods for Identifying Data Quality Issues.**
5. **Suggestions for Handling Identified Issues.**

## 2. Case Study: Phishing Datasets
A systematic review of phishing datasets [7] (used in nearly 300 papers from CORE B or better venues published between 2010 and 2018) reveals several common issues:

- **Recency:** Most datasets are not recent, and when they are, they are often not publicly available.
- **Scale:** Datasets tend to be small, typically containing fewer than 10^6 instances.
- **Class Balance:** Datasets are almost balanced with respect to class labels.
- **Other Issues:** Some datasets suffer from other problems, such as the use of domains from Alexa.com and URLs from Phishtank with features involving URL length.

For example, the Nazario dataset contains approximately 5000 phishing emails collected in the early 2000s, and a smaller dataset from 2015-2017. The Enron dataset has sanitized headers, and the SpamAssassin dataset has "lightly" sanitized headers. In previous work [19], we designed a "nonsense" filter for potential poisoning attacks based on the Nazario dataset. To address these issues, we created four public datasets for phishing email detection, with and without email headers.

### 2.1 Creation of IWSPA-AP Email Dataset
Our goal for the IWSPA-AP dataset was to ensure diversity, including different types of attacks and legitimate emails, as well as a mix of new and classical attacks. We gathered recent and historical emails from various sources [2]. Legitimate emails were relatively easy to find, thanks to WikiLeaks, while phishing emails were collected from IT departments of different universities and the Nazario phishing corpora.

The dataset was cleaned by replacing all URLs in the emails with ⟨link⟩ to protect sensitive information. We also removed signs that could hint at the origin of the datasets, such as organization names, recipients' names, domain names, signatures, and non-English emails. Emails larger than 1 MB or too small, and base64 encoded text, were removed. HTML tags and empty spacing were also removed to reduce noise. As a final check, a logistic regression model was trained and run on the test subsets.

### 2.2 Rigorous Analysis of IWSPA-AP Datasets
We ran a state-of-the-art part-of-speech tagger on the email bodies of the IWSPA-AP dataset and collected the 50 most frequent nouns. Despite extensive cleaning, there were still significant numbers of nouns tied to the sources of the emails in the phishing subset.

In 2019, researchers achieved an accuracy of 99.848% on the IWSPA-AP dataset using the THEMIS model [9]. We hypothesized that the dataset's "difficulty" contributed to this high accuracy. To test this, we ran the PhishBench benchmark [1] with 69 header features and 48 body features on the full-header subset of the dataset, using only the first 50 legitimate and 10 phishing emails for training. Five out of nine classifiers achieved accuracy greater than 99%.

To further validate our hypothesis, we performed a 20-round Monte-Carlo cross-validation on the full-header dataset with the same 69 header and 48 body features, using a training set size of 50 legitimate and 10 phishing emails. Three out of nine classifiers achieved mean accuracy greater than 99%, confirming our initial results.

These findings led to another round of cleaning, resulting in Version 2.0 of the dataset. We normalized variations of recipient organization names and domains missed in the first round. Running the 50-10 experiment on Version 2.0 showed no significant change in classifier performance. We then performed another round of cleaning, removing normalization artifacts and normalizing IP addresses to 254.254.254.254, resulting in Version 2.1.

Despite additional rounds of cleaning, running the 50-10 experiment on Version 2.1 showed no significant change in classifier performance over Version 1.0. This suggests that recipient information may be derivable from more subtle aspects of the dataset, such as the structure of the headers. Future research will test this hypothesis, leading to a new data quality dimension: data difficulty, defined as the minimum distance between two data instances from different classes for binary classes, and the minimum or average of the pairwise minimums for multiple classes.

## 3. Case Study: Malware Datasets
Creating a robust malware dataset involves several factors:

- **Coverage of Malware Types and Attack Vectors:** Sufficient coverage of different malware types and attack vectors.
- **Recency and Relevance:** Recent and relevant samples.
- **Feature Analysis:** Multiple methods of feature analysis.
- **Large Families:** Sufficiently large families of malware.
- **Obfuscation and Permutation Techniques:** Strong coverage of multiple obfuscation and permutation techniques.

[... continued in the next section ...]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the original content and intent.