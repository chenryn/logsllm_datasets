c) Requirements: We draw insights from digital water-
marking studies in images to deﬁne the requirements. For
example,
the main requirements deﬁned in [26] include:
successful watermark embedding and veriﬁcation, perceptual
similarity (imperceptibility), robustness to removal attempts
and edits (e.g., cropping, compression), and security to unau-
thorized detection. We adapt these requirements to our task
and deﬁne the problem as a trade-off between the following:
• Effectiveness: The watermark should be successfully
embedded and veriﬁed. At the same time, it should keep
the text utility; it should introduce the least amount of
changes to the cover text, and ideally produce natural,
grammatically and semantically correct changes, to pre-
serve the perceptual similarity.
• Secrecy: The watermark should achieve stealthiness by
not introducing evident changes that can be easily de-
tectable by automated classiﬁers. Ideally, it should be
indistinguishable from non-watermarked text. This,
in
part, contributes to the text utility and naturalness pre-
serving factor, and it helps to avoid suspicion and hinders
the adversary’s efforts to tamper with the watermark by
identifying it. Therefore, we study the watermark secrecy
and consider a range of possible discriminators.
• Robustness: The watermark should be resilient and not
easily removable by simple changes. Ideally, to remove
the watermark, one has to introduce heavy modiﬁcations
that render the text ‘unusable’. Satisfying the previous
two requirements (text utility and secrecy) can, in part,
contribute to the robustness, since the adversary would
not be able to distinguish the watermark.
d) Assumptions about the adversary and attacks: We
consider a black-box API and assume that the attacker has
no white-box access to the language model or the watermark-
ing model (the watermark encoder and decoder), and also
no access to the input watermark or the cover text before
watermarking. We assume that
the adversary aims to use
to tamper with
the service without getting detected,
(remove) the watermark while largely preserving the service’s
output (i.e., utility). We consider the following robustness
attacks: 1) Random changes and denoising, where the attacker
has knowledge about using a translation-based watermarking
scheme but not the model details. 2) Re-watermarking and
de-watermarking, where the attacker has full knowledge about
AWT details and training data but no access to the model itself.
thus,
IV. ADVERSARIAL WATERMARKING TRANSFORMER
We propose the Adversarial Watermarking Transformer
(AWT) as an end-to-end framework for language watermark-
ing. As shown in Figure 2, the proposed solution includes a
hiding network, a revealing network, and they are both trained
against a discriminator. In this section, we discuss the details
of these components and the training procedures.
A. Hiding Network (Message Encoder)
This component
is responsible for translating the input
text to the watermarked text. Similar to sequence-to-sequence
machine translation models [1], [65], [66], it consists of an
encoder and a decoder.
a) Encoder: The encoder (E) is a transformer-encoder
block consisting of several transformer encoder layers. Each
layer consists of a self-attention block followed by a fully-
connected layer. The encoder takes an input sentence S =
{W0, W1, ..., Wn}, consisting of one-hot encoded words that
are then projected to the embedding space using the word-
embedding layer. As transformers are position-invariant, posi-
tion embeddings (sinusoidal embeddings [1]) are then added
to the word embeddings. The encoder produces a ﬁxed-length
vector which is an average pooling across the time dimension
of the last encoder layer [67].
b) Message: The input message: M = {b0, b1, ..., bq} (q
binary bits sampled randomly), is ﬁrst fed to a fully connected
layer in order to match the embeddings’ dimension and is
then added to the sentence encoding produced by the encoder,
producing a shared embedding between the sentence and the
message, which is then passed to the autoregressive decoder
and added to its input at each time-step.
c) Decoder: The decoder (D) has a similar architecture
as the encoder, in addition to having an attention layer over the
encoder’s output, following the transformer architecture [1].
In paired machine translation, the decoder usually takes the
ground-truth target sequence (shifted right) and is trained to
predict the next word at each time step. Since our problem
does not have paired training data, the model is trained as an
autoencoder [67]; the decoder takes the shifted input sentence
and is trained to reconstruct the sequence given to the encoder,
n}. This
producing an output sentence S
serves as the reconstruction component in similar image data
hiding methods [27], and it helps to largely preserve the
input. In order to train the whole network jointly and allow
back-propagation from the other components, we use Gumbel-
Softmax approximation [68], [69] with one-hot encoding in
the forward pass (Straight-Through Gumbel Estimator using
argmax [68]), and differentiable soft samples in the backward
pass (softmax is used to approximate the argmax opera-
tion [68]). The reconstruction loss is the cross-entropy loss:
(cid:48)
1, ..., W
= {W
(cid:48)
0, W
(cid:48)
(cid:48)
Lrec = Epdata(S)[− log pD(S)]
B. Revealing Network (Message Decoder)
This part of the network is responsible for reconstructing
the input message. It takes the one-hot samples produced by
the autoencoder, multiplied by the embedding matrix, and
with adding position embeddings. The message decoder (M)
is a transformer-encoder block since it is typically used in
text classiﬁcation applications [4], [13]. The output of the
last transformer encoder layer is averaged across the time
dimension and fed to a fully connected layer with an output
size that is equivalent to the message length q. The message
reconstruction loss is the binary cross-entropy over all bits:
Lm = − q(cid:88)
i=1
bi log(pM (bi)) + (1 − bi) log(1 − pM (bi))
Fig. 2: The architecture of AWT. The model consists of a data hiding network (sequence-to-sequence model), a data revealing
network to decode the message, and a discriminator, in addition to the auxiliary components used at the ﬁne-tuning step.
Weight tying: To reduce the number of parameters in the
network, we share the embedding weights across the whole
network [1] (i.e., text autoencoder including the encoder and
decoder, message decoder, and discriminator), and also with
the pre-softmax layer that maps from the embedding space to
tokens in the text decoder [1], [70], [71]. We found it beneﬁcial
in terms of the model size and faster convergence to also share
the weights between the encoder part of the text autoencoder
and the message decoder.
C. Discriminator
In order to have a subtle message encoding that does not
alter the language statistics, we utilize adversarial training and
train the previous two components against a discriminator.
The discriminator (A) is a transformer-encoder with a similar
structure to the message decoder. It takes the non-watermarked
sentences S and the watermarked sentences S
, multiplies the
one-hot samples with the shared embeddings, and adds the
position embeddings. It produces an average over the time
steps of the last transformer encoder layer, which is used for
the binary classiﬁcation using the binary cross-entropy loss:
(cid:48)
(cid:48)
Ldisc = − log(A(S)) − log(1 − A(S
while the adversarial loss is: LA = − log(A(S
)). As we show
later, we found this component essential in supporting the
watermark secrecy against adversaries.
D. Training and Fine-tuning
))
(cid:48)
The model is ﬁrst trained jointly with the above three losses
with weighted averaging:
L1 = wALA + wrecLrec + wmLm
The previous loss function aims to preserve the input
sentence and encode the message with the least amount of
changes while not changing the text statistics. However, we
still do not have an explicit constraint on the type of changes
done by the network to encode the message. Therefore, after
training the network with L1, we further ﬁne-tune the network
to achieve semantic consistency and grammatical correctness.
a) Preserving semantics: One way to force the output
to be semantically similar to the input sentence is to embed
both sentences into a semantic embedding space and compute
the distance between the two encodings. We follow [72] and
use the pre-trained Facebook sentence embedding model [73]
that was trained to produce a sentence representation based
on the natural language inference (NLI) task. The model was
trained on the Stanford Natural Language Inference (SNLI)
dataset [74]. We ﬁx the sentence encoder (F ) weights and use
it to compute the semantic loss between S and S
as follows:
(cid:48)
Lsem = ||F (S) − F (S
(cid:48)
)||
b) Sentence correctness: To explicitly enforce correct
grammar and structure, we ﬁne-tune the model with a language
model loss [72]. We independently trained the AWD-LSTM
(ASGD Weight-Dropped LSTM) [70] on the used dataset,
as a medium-scale, but widely used and effective language
model [7], [75], [76]. We then use the trained AWD-LSTM
model (LM) with ﬁxing its weight to compute the likelihood
of the output sentence S
. Sentences with higher likelihood
are more likely to be syntactically similar to the original text
used in training. The language model loss is deﬁned as:
(cid:48)
LLM = −(cid:88)
i
log pLM(W
(cid:48)
i |W
(cid:48)
<i)
These losses are competing; e.g., a perfect sentence reconstruc-
tion would fail to encode the message. Therefore, we tuned the
losses’ weights on the validation set to achieve a good trade-
off; e.g., it was helpful to assign a relatively higher weight to
the message loss, otherwise, the reconstruction dominates. We
did not need to anneal the message weight after the start. The
other losses had comparable weights to each other.
These previous two components take the one-hot samples
and map them to their respective embedding space. We ﬁne-
tune the network using these two losses in addition to the
previous ones as follows: L2 = wALA + wrecLrec + wmLm +
wsemLsem + wLMLLM.
As we later show, ﬁne-tuning with these auxiliary losses
helps to produce more realistically looking and natural samples
+Transformer decoder layer...+Gumbel Softmax           ...Data hiding networkLinear+AttentionTransformer encoder layer.........Position embeddingsWord embeddingsShared embeddings        ...0           ...Word embeddingsPosition embeddings+Average pooling      ...Output sequenceWord embeddings+Average poolingLinearBinary: encoded (fake) / cover (real)  Word embeddings+Average poolingDiscriminatorPosition embeddingsPosition embeddingsShared embeddingsTransformer encoder layer......                 ...  /   Transformer encoder layer......Linear  /     /   Input messageOutput messageRepeatAWD-LSTM(Unconditional LM)InferSent(Sentence embedding)Language model lossSemantic loss TrainableFixed weightsFine-tuning onlyOutput sequence Input sequence Input sequenceOuput sequence          ...       ...Shifted input sequenceData revealing networkOutput/input sequencecompared to only training with reconstructing the sentence.
Introducing these new losses after the ﬁrst training stage was
mainly to speed-up convergence and training time since the
model at ﬁrst has not yet learned to reconstruct the input. So
after the model learns the basic function, we use this stage as
a warm start for further optimization. This is similar to pre-
training as an autoencoder for other translation tasks [72].
V. EXPERIMENTAL RESULTS
In this section, we ﬁrst discuss our setup. Then, we evaluate
the different aspects of our model: effectiveness, secrecy, and
robustness. We compare AWT to baselines and present a user
study to evaluate the output’s quality.
A. Setup
a) Dataset: We used the word-level WikiText-2 (WT2)
that is curated from Wikipedia articles with light processing
and was introduced in [77]. We used the same tokenization,
processing, and split setup as [70], [77], [78]. The dataset
is approximately twice the size of the Penn Treebank (PTB)
benchmark dataset for language modelling [79], besides, the
WikiText-2 keeps the capitalization, punctuation, and numbers.
It contains over 30,000 unique vocabulary words and has a
size of 2 million words in the training set and 0.2 million in
validation and test sets. Since our watermarking framework
can be applied independently as a post-processing step, we
experiment on human-written data to objectively judge the
proposed watermarking scheme correctness and to use a
benchmark pre-processed dataset.
b) Implementation Details: We used a dimension size
(dmodel) of 512 for all transformers blocks and embeddings.
The encoder and decoder transformer blocks are composed of
3 identical layers and 4 attention heads per layer, the decoder
has a masked (on future input) self-attention. The rest of
the transformer hyperparameters follows [1] (e.g., a dropout
probability of 0.1, a dimension of 2048 for the feed-forward
layers, ReLU activations, and sinusoidal position embeddings).
We optimize the network with Adam optimizer [80] with a
varying learning rate [1]:
lrategen = d−0.8
lratedisc = d−1.1
model ∗ min(step−0.5, step ∗ warmup−1.5)
model ∗ min(step−0.5, step ∗ warmup−1.5)
where step is the batch counter, lrategen is the learning rate
of the autoencoder and message decoder, and lratedisc is the
learning rate of the discriminator, trained alternatively. We use
6000 warmup steps and a batch size of 80. We use a Gumbel
temperature of 0.5 [66], [72]. We trained the network for 200
epochs for each stage. For training the AWD-LSTM language
model, we used the authors’ implementation1. We used the
trained sentence embedding model2. A good trade-off between
losses was found when setting the message loss’s weight to
a relatively higher value than the others (e.g., 5x). Otherwise,
the other losses dominate and the training fails to optimize
1https://github.com/salesforce/awd-lstm-lm
2https://github.com/facebookresearch/InferSent
the message loss. The training was not sensitive to the exact
weights. Our code and models are publicly available: https:
//github.com/S-Abdelnabi/awt/.
c) Input length during training and test: The dataset
is a continuous text corpus. During training, we encode a
randomly sampled 4-bit message (similar to [41]) into a
text segment/sentence (varying length: N (80, 5)). We test the
network on ﬁxed-length segments of 80 words per segment,
which can be adapted if needed, small changes to this number
(±5 words) did not signiﬁcantly affect the results. As our
objective is to watermark machine-generated articles,
this
segment-level setup can be extended to a longer text or a
document-level input by successively encoding and decoding
concatenated segments. Thus, a longer watermark can be
composed of multiple 4-bits messages with a certain pre-
deﬁned order. Using longer watermarks allows veriﬁcation us-
ing null-hypothesis testing. We base the watermark veriﬁcation
decision on the matching accuracy of all decoded messages
from the concatenated segments. In section V-B4, we evaluate
the veriﬁcation with respect to the total segments’ length.
B. Effectiveness Evaluation
In this section, we evaluate the effectiveness of the model
in terms of text utility and bit accuracy. We discuss our
evaluation metrics and we compare different model’s variants.
We examine two different inference strategies to improve the
utility. We discuss how to verify the watermark by sentence
aggregation and show the trade-off between utility and veri-
ﬁcation conﬁdence at different input lengths. We show how
to improve the bit accuracy by averaging multiple encoded
segments. We then perform a qualitative analysis to visualize
and assess the changes produced by the model.
1) Metrics: To measure the message decoding, we use the
bitwise message accuracy (random chance: 50%) of all sen-
tences in the test set. To measure utility preserving, we use the
meteor score [81] that is used in machine-translation tasks to
compare the output sentence against ground-truth references.
Meteor performs n-gram alignments between the candidate
and output text with synonym lookups from WordNet [82].
It ranges from 0 to 1 (‘no’ to ‘identical’ similarity).
However, we found the meteor score not enough to evaluate
the text semantics; two output sentences can have the same
number of changed words compared to the input sentence and
thus a similar meteor score (assuming there is no synonym
overlapping), however, one of them could be closer to the input
sentence. Therefore, to approximate the semantic difference
between the input and output text, we used SBERT [83], a
pre-trained sentence embedding model based on ﬁne-tuning
BERT as a siamese network on the NLI task. We compute the
input and output embeddings and calculate the L2 difference
between them (lower is better). We discuss more details about
the importance of using this additional metric in Section V-B6
and Appendix VIII-A. We average the meteor scores and
SBERT distances for all sentences in the test set.
2) Model ablation: We show in Table I three variants of
our model. We ran each one 10 times with random sampling of
messages and we found the results very comparable, we report
the average and standard deviation of the metrics across these
runs. The ﬁrst row shows the full AWT with the ﬁne-tuning
step, the second one shows the model without ﬁne-tuning, and
the last row shows the model without discriminator and ﬁne-
tuning (trained only with text and message reconstruction).
This shows that the ﬁne-tuning step helps to improve the
text preserving and semantics as suggested by the increase
in the meteor score and the decrease in the SBERT distance,
at
it maintains a high message decoding
accuracy. Additionally, the model trained with a discriminator
had a lower SBERT distance compared to the model that was
trained with text reconstruction only, although both of them
have a comparable meteor score. As we demonstrate in our
qualitative and secrecy analysis shown later, this indicates that
the adversarial training setup improves the output’s quality, in
addition to its secrecy advantages3.
the same time,
3) Inference strategies: To further maintain the text utility
and improve the output sequence’s quality, we study two
inference strategies. First, we sample a set of samples for
each sentence and then select