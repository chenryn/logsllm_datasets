posed a technique to trade oﬀ DRAM reliability for energy
savings. It relies on the programmer to separate application
data into vulnerable or tolerant data. Less reliable mobile
DIMMs have been proposed [48, 49] as a replacement for
ECC DIMMs in servers to improve energy eﬃciency.
Heterogeneous (hybrid) memory architectures. Several
works (e.g., [50–54]) explored the use of heterogeneous
memory architectures, consisting of multiple diﬀerent types
of memories. These works were mainly concerned with
either mitigating the overheads of emerging technologies or
improving performance and power eﬃciency. They did not
investigate the use of multiple devices with diﬀerent error
correction capabilities.
Compared to prior research, our work is the ﬁrst to (1)
perform a comprehensive analysis of memory error vul-
nerability for data-intensive datacenter applications across
a range of diﬀerent memory error types and (2) evaluate
the cost-eﬀectiveness of diﬀerent heterogeneous-reliability
memory organizations with hardware/software cooperation.
Although our error injection-based vulnerability character-
ization methodology is similar to [16, 31], our analysis is
done on a wider range of application parameters (such as
memory write frequency and application access patterns).
We also provide additional insights as to how errors are
Figure 1: Memory error outcomes.
masked by applications. As such, our work studies memory
error vulnerability for a new set of applications and proposes
a new heterogeneous memory architecture to optimize dat-
acenter cost.
III. Quantifying Application Memory Error Tolerance
We begin by discussing our methodology for quantifying
the tolerance of applications to memory errors. Our method-
ology consists of three components: (1) characterizing the
outcomes of memory errors on an application based on how
they propagate through an application’s code and data, (2)
characterizing how safe or unsafe it is for memory errors
to occur in diﬀerent regions of an application’s data, and
(3) determining how amenable an application’s data is to
recovery in the event of an error. We next discuss each
of these components in turn and describe their detailed
implementation in Section IV.
A. Characterizing the Outcomes of a Memory Error
We characterize an application’s vulnerability to a memory
error based on its behavior after a memory error is intro-
duced (we assume for the moment that no error detection or
correction is being performed). Figure 1 shows a taxonomy
of memory error outcomes. Our taxonomy is mutually exclu-
sive (no two outcomes occur simultaneously) and exhaustive
(it captures all possible outcomes). At a high level, a memory
error may be either (1) masked by an overwrite, in which
case it is never detected and causes no change in application
behavior or (2) consumed by the application. In the case that
an error is consumed by the application, it may either (2.1)
be masked by application logic, in which case it is never
detected and causes no change in application behavior, (2.2)
cause the application to generate an incorrect response, or
(2.3) cause the application or system to crash.
When we refer to the tolerance of an application to memory
errors, we mean the likelihood that an error occurring in
some data results in outcomes (1) or (2.1). Conversely, when
we refer to the vulnerability of an application to memory
errors, we mean the likelihood that an error occurring in
some data results in outcomes (2.2) or (2.3).
B. Characterizing Safe Data Regions
An application’s data is typically spread across multi-
ple logical regions of memory to simplify OS-level mem-
ory management and protection. For example, a statically-
allocated variable would be placed in a memory region
called the stack, while a dynamically-allocated variable
would be placed in a memory region called the heap. Table 2
lists several key memory regions and describes the types of
469
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
(a) Memory Error FatesMemory ErrorSystem/App CrashIncorrect ResponseMasked by LogicMasked by OverwriteConsumed by Application2.1212.22.3Correct ResultIncorrect ResultRegion The types of data that it stores
Heap Memory for dynamically-allocated data.
Stack Memory used to store function parameters and local variables.
Private Pre-allocated private memory managed by the user (allocated
Other
by VirtualAlloc on Windows or mmap on Linux).
Program code, managed heap, and so on, which are relatively
small in size, and which we do not examine.
Table 2: Diﬀerent application memory regions.
data that they store. How an application manipulates data
in each of these regions ultimately determines its tolerance
or vulnerability to memory errors, and so ideally we would
like to quantify, for every address A in every region, how
safe or unsafe it is (in terms of application vulnerability) for
an error to occur at A.
Recall from Section II-A that the frequency at which data
is read determines how often errors are detected (exposing
an application to a potentially unsafe memory error) and
the frequency at which data is written determines how
often errors are masked by overwrites to the erroneous data
(making the data safe for application consumption). For an
address A, we deﬁne an address A’s unsafe duration as the
sum of time, across an application’s execution time, between
each read to A and the previous memory reference to A.
Similarly, we deﬁne an address A’s safe duration as the sum
of time, across an application’s execution time, between each
write to A and the previous memory reference to A. Then, the
safe ratio = safe duration/(safe duration + unsafe duration)
for an address A is the fraction of time for which an error at
A was never consumed by an application over its execution
time.
A safe ratio closer to 1 implies that the address A is more
frequently written than read, increasing the chances that an
error at A will be masked; a safe ratio closer to 0 implies
that the address A is more frequently read than written,
increasing the chances that an error at A will be consumed
by the application. We can generalize the concept of a safe
ratio to regions of memory by computing the average safe
ratio for all the addresses (or a representative sampling of
the addresses) in a region.
C. Determining Data Recoverability
Even if an application is vulnerable to memory errors and
its data is not particularly safe, it can still potentially recover
from a memory error in its data by loading a clean copy of its
data from persistent storage (i.e., ﬂash or disk). This requires
that a system is able to detect memory errors in hardware
with Parity/ECC or in software by computing checksums
over application data regions [19]. For example, once an
error is detected, the OS can perform a recovery of clean data
before repeating the memory access and returning control
to the program. As such, data recoverability can improve
application memory error tolerance. We have identiﬁed two
strategies for data recoverability in applications: implicit
recoverability and explicit recoverability.
Implicit recoverability is enabled by an application already
maintaining a clean copy of its data in persistent storage. For
example, the contents of a memory-mapped ﬁle can be re-
read from disk. Though recovering data incurs the additional
latency of reading from persistent storage, it only happens
in the relatively uncommon case of a memory error.
470
Explicit recoverability is enabled by the system software
automatically maintaining a clean copy of an application’s
memory pages in persistent storage. This can be realized by
updating pages in persistent storage in tandem with updates
to main memory. This may aﬀect application performance
and therefore may need to be applied only sparingly, to the
most unsafe regions of memory.
We characterize data recoverability of a memory region
by measuring the percentage of memory pages that are
either implicitly or explicitly recoverable. A memory page
is implicitly recoverable if the operating system can identify
its mapped location on disk. A memory page is explicitly
recoverable if, on average, it is written to less than once
every ﬁve minutes.
Regardless of an application’s recoverability strategy, for
persistent hard errors (which cannot be corrected by recover-
ing a clean copy of data), a technique such as page retirement
can be used in conjunction with data recovery.
IV. Characterization Frameworks
We had three design goals when implementing our method-
ology for quantifying application memory error tolerance.
First, due to the sporadic and inconsistent nature of memory
errors in the ﬁeld, we wanted to design a framework to
emulate the occurrence of a memory error in an application’s
data in a controlled manner (to characterize its outcomes).
Second, we wanted an eﬃcient way to measure how an
application accesses its data (to characterize safe and unsafe
durations and determine data recoverability). Third, we
wanted our framework to be easily adapted to other work-
loads or system conﬁgurations. To achieve all of these goals,
we leveraged the support of existing software debuggers
for modifying and monitoring arbitrary application memory
locations. We next describe the details of the error emulation
and access monitoring frameworks that we developed.
A. Memory Error Emulation Framework
Software debuggers (such as WinDbg [55] in Windows
and GDB [56] in Linux) are typically used to examine and
diagnose problems in application code. They run alongside
a program and provide a variety of debugging features
for software developers to analyze program behavior. For
example, these debuggers can read and write the contents of
an arbitrary memory location in an application. We leverage
this ability to emulate both soft and hard errors.
1: addr = getMappedAddr()
2: data = ∗addr
3: bit = getRandBit()
4: data = data ∧ (1 << bit)
5: ∗addr = data
(a)
1: addr = getMappedAddr()
2: awatch(addr):
print ∗addr
3:
print loadOrStore
4:
print time
5:
(b)
Algorithm 1: Debugger pseudocode for (a) emulating single-bit soft
memory errors and (b) measuring application memory access patterns.
Algorithm 1(a) lists the pseudocode for emulating a single-
bit soft memory error in a random location in an application.
In line 1, the getMappedAddr() function that we implement
uses the debugger’s ability to determine which memory
addresses a program has data stored in to randomly select
a valid byte-aligned application memory address, which it
stores in the variable addr. In line 2, the value of the byte
at addr is read and stored in the variable data. In line 3, the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
getRandBit() function that we implement uses a random
number generator to select a bit index that determines which
bit in a byte will contain an error, and stores this index in
the variable bit. Line 4 performs the actual injection of the
error by ﬂipping the bit at index bit in data (using a left shift
and an XOR operation). Finally, line 5 stores the modiﬁed
version of data back to the location addr.
We generalize Algorithm 1(a) to generate multi-bit soft
errors by repeatedly performing lines 3–4 with diﬀerent
values of bit. To emulate single- and multi-bit hard errors we
perform the same approach, but to ensure that an application
does not overwrite the emulated hard error, every 30 ms we
check if the contents of data has changed, and if it has,
we re-apply the hard error. We chose this approach so as to
keep our emulation infrastructure reasonably fast while still
emulating the features of hard errors.
Note that Algorithm 1(a) always ﬂips the bit selected by bit
in data. While it is true that some failure modes may only
cause the value of a DRAM cell to change in one direction
(only from 0 → 1 or only from 1 → 0 [28]), modern DRAM
devices have been shown to contain regions of cells that can
be induced to ﬂip in either direction [27, 57], and so we do
not impose any constraints on the direction of bit ﬂips.
One diﬀerence between the errors that we emulate in soft-
ware and memory errors in hardware is that our technique
immediately makes memory errors visible throughout the
system. In contrast, memory errors in hardware may take a
while to become visible to system as processor caches may
prevent the erroneous data in memory from being accessed
by serving the correct data from the cache data store. Due to
this, our methodology provides a more conservative estimate
of application memory error tolerance.
Tying everything together, Figure 2
shows a ﬂow diagram illustrating the
ﬁve steps involved in our error em-
ulation framework. We assume that
the application under examination
has already been run outside of the
framework and its expected output
has been recorded. The framework
proceeds as follows. (1) We start the
application under the error injection
framework. (2) The debugger injects
the desired number and type of er-
rors. (3) We initiate the connection
of a client and start performing the
desired workload. (4) Throughout the
course of the application’s execution,
we check to see if the machine has
crashed (we deem an application in
a crashed state if it fails to respond
to ≥ 50% of the client’s requests),
corresponding to outcome (2.3) in Figure 1; if it has, we
log this outcome and proceed to step (1) to begin testing
once again. (5) If the application ﬁnishes its workload, we
check to see if its output matches the expected results,
corresponding to either outcome (1) or (2.1) in Figure 1.3 If
its output does not match the expected results, corresponding
Figure 2: Memory error
emulation framework.
3Note that we ignore responses to the client
time out due to
performance variations (e.g., all of an application’s threads are busy serving
other requests). In our tests, this occurred for 1.5% of the total requests.
that
to outcome (2.2) in Figure 1, we log this outcome and
proceed to step (1) to test again.
We stop testing after a statistically-signiﬁcant number of
errors have been emulated. While testing all of the memory
addresses of data-intensive applications may not be feasible
due to their sheer size,
in our experiments we ensured
that we sampled at least 0.1% of the memory addresses
in the applications we examine. This translates to over
10,000 experiments for an application with 32 GB of in-
memory data (the average amount of in-memory data for the
applications we tested was ∼28 GB). We can then process
the logged information to quantify an application’s tolerance
or vulnerability to memory errors (Section III-A).
B. Memory Access Monitoring Framework
To monitor application memory access behavior, we use
processor watchpoints, available on x86 processors, to ex-
ecute logging code on individual loads/stores from/to arbi-
trary memory addresses. Algorithm 1(b) lists the pseudocode
for tracking memory access patterns to a random location
in an application. In line 1, similar to Algorithm 1(a), we
randomly select a valid byte-aligned memory address and
store this address in the variable addr. In line 2, we register
a watchpoint in the debugger (e.g., using awatch in GDB).
From then on, whenever the processor loads data from that
address or stores data to that address, our logging code in
lines 3–5 will get called to print out the data at addr, whether
the access was a load or a store (loadOrStore), and the time
of access (time).
We can then process the information logged by Algo-
rithm 1(b), to compute the safe ratio for a region of an appli-
cation’s data (Section III-B). To quantify the recoverability
of an application’s data (Section III-C), we ﬁrst use the
debugger to identify any read-only ﬁle-mapped data, which
we classify as recoverable. Additionally, we process the
data logged by Algorithm 1(b) to measure how frequently
diﬀerent regions of memory are written to, and classify any
regions of memory that are written to every ≥ 5 minutes on
average as recoverable (as storing a backup copy of their
data in persistent storage would likely not pose a signiﬁcant
performance impediment).
V. Data-Intensive Application Memory Error Tolerance
We next use our methodology to quantify the tolerance
of three data-intensive applications to memory errors. We
use the insights developed in this section to inform the
new memory system organizations that we propose in Sec-
tion VI-B to reduce system cost.
A. Data-Intensive Applications
We examined three data-intensive applications as part of
our case study: an interactive web search application (Web-
Search), an in-memory key–value store (Memcached), and a
graph mining framework (GraphLab). While these applica-
tions all operate on large amounts of memory (Table 3 lists
Applications
WebSearch