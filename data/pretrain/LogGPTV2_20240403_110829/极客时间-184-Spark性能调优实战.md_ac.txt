# 01 \| 性能调优的必要性：Spark本身就很快，为啥还需要我调优？你好，我是吴磊。 在日常的开发工作中，我发现有个现象很普遍。很多开发者都认为 Spark的执行性能已经非常强了，实际工作中只要按部就班地实现业务功能就可以了，没有必要进行性能调优。 你是不是也这么认为呢？确实，Spark的核心竞争力就是它的执行性能，这主要得益于 Spark基于内存计算的运行模式和钨丝计划的锦上添花，以及 Spark SQL上的专注与发力。 但是，真如大家所说，**开发者只要把业务逻辑实现了就万事大吉了吗**？这样，咱们先不急于得出结论，你先跟着我一起看两个日常开发中常见的例子，最后我们再来回答这个问题。 在数据应用场景中，ETL（Extract TransformLoad）往往是打头阵的那个，毕竟源数据经过抽取和转换才能用于探索和分析，或者是供养给机器学习算法进行模型训练，从而挖掘出数据深层次的价值。我们今天要举的两个例子，都取自典型ETL端到端作业中常见的操作和计算任务。 开发案例 1：数据抽取第一个例子很简单：给定数据条目，从中抽取特定字段。这样的数据处理需求在平时的ETL 作业中相当普遍。想要实现这个需求，我们需要定义一个函数extractFields：它的输入参数是Seq\[Row\]类型，也即数据条目序列；输出结果的返回类型是 Seq\[(String,Int)\]，也就是（String,Int）对儿的序列；函数的计算逻辑是从数据条目中抽取索引为 2的字符串和索引为 4 的整型。 应该说这个业务需求相当简单明了，实现起来简直是小菜一碟。在实际开发中，我观察到有不少同学一上来就迅速地用下面的方式去实现，干脆利落，代码写得挺快，功能也没问题，UT、功能测试都能过。     //实现方案1 —— 反例    val extractFields: Seq[Row] => Seq[(String, Int)] = {      (rows: Seq[Row]) => {        var fields = Seq[(String, Int)]()        rows.map(row => {            fields = fields :+ (row.getString(2), row.getInt(4))        })      fields      }    }在上面这个函数体中，是先定义一个类型是 Seq\[(String, Int)\]的变量fields，变量类型和函数返回类型完全一致。然后，函数逐个遍历输入参数中的数据条目，抽取数据条目中索引是2 和 4 的字段并且构建二元元组，紧接着把元组追加到最初定义的变量 fields中。最后，函数返回类型是 Seq\[(String, Int)\]的变量fields。 乍看上去，这个函数似乎没什么问题。特殊的地方在于，尽管这个数据抽取函数很小，在复杂的ETL 应用里是非常微小的一环，但在整个 ETL作业中，它会在不同地方被频繁地反复调用。如果我基于这份代码把整个 ETL应用推上线，就会发现 ETL作业端到端的执行效率非常差，在分布式环境下完成作业需要两个小时，这样的速度难免有点让人沮丧。 想要让 ETL作业跑得更快，我们自然需要做性能调优。可问题是我们该从哪儿入手呢？既然extractFields这个小函数会被频繁地调用，不如我们从它下手好了，看看有没有可能给它"减个肥、瘦个身"。重新审视函数extractFields 的类型之后，我们不难发现，这个函数从头到尾无非是从Seq\[Row\]到 Seq\[(String,Int)\]的转换，函数体的核心逻辑就是字段提取，只要从 Seq\[Row\]可以得到Seq\[(String,Int)\]，目的就达到了。 要达成这两种数据类型之间的转换，除了利用上面这种开发者信手拈来的过程式编程，我们还可以用函数式的编程范式。函数式编程的原则之一就是尽可能地在函数体中避免副作用（Sideeffect），副作用指的是函数对于状态的修改和变更，比如上例中 extractFields函数对于 fields变量不停地执行追加操作就属于副作用。 基于这个想法，我们就有了第二种实现方式，如下所示。与第一种实现相比，它最大的区别在于去掉了fields 变量。之后，为了达到同样的效果，我们在输入参数Seq\[Row\]上直接调用 map 操作逐一地提取特定字段并构建元组，最后通过toSeq将映射转换为序列，干净利落，一气呵成。     //实现方案2 —— 正例    val extractFields: Seq[Row] => Seq[(String, Int)] = {      (rows: Seq[Row]) =>         rows.map(row => (row.getString(2), row.getInt(4))).toSeq    }你可能会问："两份代码实现无非是差了个中间变量而已，能有多大差别呢？看上去不过是代码更简洁了而已。"事实上，我基于第二份代码把ETL作业推上线后，就惊奇地发现端到端执行性能提升了一倍！从原来的两个小时缩短到一个小时。**两份功能完全一样的代码，在分布式环境中的执行性能竟然有着成倍的差别。因此你看，在日常的开发工作中，仅仅专注于业务功能实现还是不够的，任何一个可以进行调优的小环节咱们都不能放过。** 开发案例 2：数据过滤与数据聚合你也许会说："你这个例子只是个例吧？更何况，这个例子里的优化，仅仅是编程范式的调整，看上去和Spark似乎也没什么关系啊！"不要紧，我们再来看第二个例子。第二个例子会稍微复杂一些，我们先来把业务需求和数据关系交代清楚。     /**    (startDate, endDate)    e.g. ("2021-01-01", "2021-01-31")    */    val pairDF: DataFrame = _         /**    (dim1, dim2, dim3, eventDate, value)    e.g. ("X", "Y", "Z", "2021-01-15", 12)    */    val factDF: DataFrame = _         // Storage root path    val rootPath: String = _ 在这个案例中，我们有两份数据，分别是 pairDF 和 factDF，数据类型都是DataFrame。第一份数据 pairDF 的 Schema包含两个字段，分别是开始日期和结束日期。第二份数据的字段较多，不过最主要的字段就两个，一个是Event date 事件日期，另一个是业务关心的统计量，取名为 Value。其他维度如dim1、dim2、dim3主要用于数据分组，具体含义并不重要。从数据量来看，pairDF的数据量很小，大概几百条记录，factDF数据量很大，有上千万行。 对于这两份数据来说，具体的业务需求可以拆成 3步： 1.       对于 pairDF 中的每一组时间对，从 factDF 中过滤出 Event date    落在其间的数据条目；        2.       从 dim1、dim2、dim3 和 Event date 4 个维度对 factDF    分组，再对业务统计量 Value    进行汇总；        3.       将最终的统计结果落盘到 Amazon    S3。    针对这样的业务需求，不少同学按照上面的步骤按部就班地进行了如下的实现。接下来，我就结合具体的代码来和你说说其中的计算逻辑。     //实现方案1 —— 反例    def createInstance(factDF: DataFrame, startDate: String, endDate: String): DataFrame = {    val instanceDF = factDF    .filter(col("eventDate") > lit(startDate) && col("eventDate")     val instance = createInstance(factDF, startDate, endDate)    val outPath = s"${rootPath}/endDate=${endDate}/startDate=${startDate}"    instance.write.parquet(outPath)    } 首先，他们是以 factDF、开始时间和结束时间为形参定义 createInstance函数。在函数体中，先根据 Event date 对 factDF 进行过滤，然后从 4个维度分组汇总统计量，最后将汇总结果返回。定义完 createInstance函数之后，收集 pairDF 到 Driver 端并逐条遍历每一个时间对，然后以factDF、开始时间、结束时间为实参调用 createInstance函数，来获取满足过滤要求的汇总结果。最后，以 Parquet的形式将结果落盘。 同样地，这段代码从功能的角度来说没有任何问题，而且从线上的结果来看，数据的处理逻辑也完全符合预期。不过，端到端的执行性能可以说是惨不忍睹，在16 台机型为 C5.4xlarge AWS EC2 的分布式运行环境中，基于上面这份代码的ETL作业花费了半个小时才执行完毕。 没有对比就没有伤害，在同一份数据集之上，采用下面的第二种实现方式，仅用2 台同样机型的 EC2 就能让 ETL 作业在 15分钟以内完成端到端的计算任务。**两份代码的业务功能和计算逻辑完全一致，执行性能却差了十万八千里**。     //实现方案2 —— 正例    val instances = factDF    .join(pairDF, factDF("eventDate") > pairDF("startDate") && factDF("eventDate")  lit(startDate) && col("eventDate")         val instance = createInstance(factDF, startDate, endDate)        val outPath = s"${rootPath}/endDate=${endDate}/startDate=${startDate}"        instance.write.parquet(outPath)    } 在这段代码中，createInstance 的主要逻辑是按照时间条件对 factDF进行过滤，返回汇总的业务统计量，然后 pairDF循环遍历每一对开始时间和结束时间，循环调用 createInstance获取汇总结果并落盘。我们在第 1 课slate-object="inline"中分析过，这份代码的主要问题在于囊括上千万行数据的 factDF被反复扫描了几百次，而且是全量扫描，从而拖垮了端到端的执行性能。那么，我们不禁要问：开发者究竟为什么会想到用这种低效的方式去实现业务逻辑呢？或者说，是什么内驱因素让开发者自然而然地采用这种实现方式呢？让我们跳出Spark、跳出这个专栏，把自己置身于一间教室内：黑板前，老师正在讲解《XX语言编程》，旁边是你的同学，他边听老师讲课，边翻看着桌上的课本。这个场景熟不熟悉？亲不亲切？回想一下，老师讲的、书本上教的和我们示例中的代码，是不是极其类似？没错！我们的大脑，已经习惯了 for循环，习惯了用函数处理变量、封装计算逻辑，习惯了面向过程的编程模式。在分布式计算出现以前，我们都是这么开发的，老师也是这么讲的，书本上也是这么教的，没毛病。因此我认为，开发者之所以会选择上面的实现方式，根本原因在于他把 factDF当成了一个普通变量，一个与 createInstance 函数中 startDate、endDate同等地位的形参，他并没有意识到，factDF实际上是一个庞大的、横跨所有计算节点的分布式数据集合，更没有意识到，在分布式运行环境中，外面的for循环会导致这个庞大的数据集被反复地全量扫描。这种对于分布式计算认知方面的缺失，究其缘由，还是我们对 Spark 核心概念RDD 的理解不够透彻。所以你看，深入理解 RDD还是很有必要的，**对于 RDD一知半解，极有可能在应用开发的过程中，不知不觉地留下潜在的性能隐患**。深入理解 RDD既然 RDD 如此重要，它究竟是什么呢？2010年，在一个夜黑风高的夜晚，Matei 等人发表了一篇名为《Spark: ClusterComputing with Working Sets》的论文并首次提出了 RDD 的概念。RDD，全称Resilient DistributedDatasets，翻译过来就是弹性分布式数据集。本质上，它是对于数据模型的抽象，用于囊括所有内存中和磁盘中的分布式数据实体。如果就这么从理论出发、照本宣科地讲下去，未免过于枯燥、乏味、没意思！不如，我先来给你讲个故事。从薯片的加工流程看 RDD在很久很久以前，有个生产桶装薯片的工坊，工坊的规模较小，工艺也比较原始。为了充分利用每一颗土豆、降低生产成本，工坊使用3 条流水线来同时生产 3 种不同尺寸的桶装薯片。3 条流水线可以同时加工 3颗土豆，每条流水线的作业流程都是一样的，分别是清洗、切片、烘焙、分发和装桶。其中，分发环节用于区分小、中、大号3 种薯片，3 种不同尺寸的薯片分别被发往第 1、2、3条流水线。具体流程如下图所示。![](Images/bf5a8ec847d85fd32298ac330317d033.png)savepage-src="https://static001.geekbang.org/resource/image/3c/6e/3c7ff059e50f3dc70bf4yy082c31956e.jpg"}RDD的生活化类比看得出来，这家工坊制作工艺虽然简单，倒也蛮有章法。从头至尾，除了分发环节，3条流水线没有任何交集。在分发环节之前，每条流水线都是专心致志、各顾各地开展工作：把土豆食材加载到流水线上，再进行清洗、切片、烘焙；在分发环节之后，3条流水线也是各自装桶，互不干涉、互不影响。流水线的作业方式提供了较强的容错能力，如果某个加工环节出错，工人们只需要往出错的流水线上重新加载一颗新的土豆，整个流水线就能够恢复生产。好了，故事讲完了。如果我们把每一条流水线看作是分布式运行环境的计算节点，用薯片生产的流程去类比Spark分布式计算，会有哪些有趣的发现呢？仔细观察，我们发现：**刚从地里挖出来的土豆食材、清洗过后的干净土豆、生薯片、烤熟的薯片，流水线上这些食材的不同形态，就像是Spark 中 RDD对于不同数据集合的抽象**。沿着流水线的纵深方向，也就是图中从左向右的方向，每一种食材形态都是在前一种食材之上用相应的加工方法进行处理得到的。**每种食材形态都依赖于前一种食材，这就像是 RDD 中dependencies 属性记录的依赖关系，而不同环节的加工方法，对应的刚好就是RDD 的 compute 属性。**横看成岭侧成峰，再让我们从横向的角度来重新审视上面的土豆加工流程，也就是图中从上至下的方向，让我们把目光集中在流水线开端那3 颗带泥的土豆上。这 3颗土豆才从地里挖出来，是原始的食材形态，正等待清洗。如图所示，我们把这种食材形态标记为potatosRDD，那么，**这里的每一颗土豆就是 RDD 中的数据分片，3颗土豆一起对应的就是 RDD 的 partitions属性**。![](Images/047b6835bfb8b1b3bd35754e13d4477a.png)savepage-src="https://static001.geekbang.org/resource/image/a2/c8/a2f8bc7bf10c31fedb85196f33f44fc8.jpg"}带泥土豆经过清洗、切片和烘焙之后，按照大小个儿被分发到下游的 3条流水线上，这 3 条流水线上承载的 RDD 记为shuffledBakedChipsRDD。很明显，这个 RDD 对于 partitions的划分是有讲究的，根据尺寸的不同，即食薯片会被划分到不同的数据分片中。**像这种数据分片划分规则，对应的就是 RDD 中的 partitioner属性。** 在分布式运行环境中，partitioner 属性定义了 RDD所封装的分布式数据集如何划分成数据分片。总的来说，我们发现，薯片生产的流程和 Spark分布式计算是一一对应的，一共可以总结为 6点： 1.  土豆工坊的每条流水线就像是分布式环境中的计算节点；        2.  不同的食材形态，如带泥的土豆、土豆切片、烘烤的土豆片等等，对应的就是    RDD；    3.  每一种食材形态都会依赖上一种形态，如烤熟的土豆片依赖上一个步骤的生土豆切片。这种依赖关系对应的就是    RDD 中的 dependencies    属性；    4.  不同环节的加工方法对应 RDD 的 compute    属性；    5.  同一种食材形态在不同流水线上的具体实物，就是 RDD 的 partitions    属性；    6.  食材按照什么规则被分配到哪条流水线，对应的就是 RDD 的 partitioner    属性。    不知道土豆工坊的类比，有没有帮你逐渐勾勒出 RDD的本来面貌呢？话付前言，接下来，咱们来一本正经地聊聊RDD。 RDD 的核心特征和属性通过刚才的例子，我们知道 RDD 具有 4大属性， **分别是partitions、partitioner、dependencies 和 compute 属性。正因为有了这 4大属性的存在，让 RDD具有分布式和容错性这两大最突出的特性。**要想深入理解RDD，我们不妨从它的核心特性和属性入手。**首先，我们来看 partitions、partitioner属性。** 在分布式运行环境中，RDD封装的数据在物理上散落在不同计算节点的内存或是磁盘中，这些散落的数据被称"数据分片"，RDD的分区规则决定了哪些数据分片应该散落到哪些节点中去。RDD 的 partitions属性对应着 RDD 分布式数据实体中所有的数据分片，而 partitioner属性则定义了划分数据分片的分区规则，如按哈希取模或是按区间划分等。不难发现，partitions 和 partitioner 属性刻画的是 RDD在跨节点方向上的横向扩展，所以我们把它们叫做 RDD的"横向属性"。**然后，我们再来说说 dependencies 和 compute属性。** 在 Spark 中，任何一个 RDD 都不是凭空产生的，每个 RDD都是基于某种计算逻辑从某个"数据源"转换而来。RDD 的 dependencies属性记录了生成 RDD 所需的"数据源"，术语叫做父依赖（或父 RDD），compute方法则封装了从父 RDD 到当前 RDD转换的计算逻辑。基于数据源和转换逻辑，无论 RDD有什么差池（如节点宕机造成部分数据分片丢失），在 dependencies属性记录的父 RDD 之上，都可以通过执行 compute封装的计算逻辑再次得到当前的RDD，如下图所示。![](Images/a96b2e9fe36eee6c13500439086004c0.png)savepage-src="https://static001.geekbang.org/resource/image/fb/91/fba28ce0c70b4c5553505911663aa491.jpg"}基于dependencies和compute属性得到当前RDD由 dependencies 和 compute 属性提供的容错能力，为 Spark分布式内存计算的稳定性打下了坚实的基础，这也正是 RDD 命名中 Resilient的由来。接着观察上图，我们不难发现，不同的 RDD 通过 dependencies 和compute属性链接在一起，逐渐向纵深延展，构建了一张越来越深的有向无环图，也就是我们常说的DAG。 由此可见，dependencies 属性和 compute 属性负责 RDD在纵深方向上的延展，因此我们不妨把这两个属性称为"纵向属性"。总的来说，**RDD 的 4大属性又可以划分为两类，横向属性和纵向属性。其中，横向属性锚定数据分片实体，并规定了数据分片在分布式集群中如何分布；纵向属性用于在纵深方向构建DAG，通过提供重构 RDD的容错能力保障内存计算的稳定性**。同时，为了帮助你记忆，我把这 4大核心属性的基本概念和分类总结在了如下的表格中，你可以看一看。![](Images/1b62634d0e2df5db6159e071d34c2b2a.png)savepage-src="https://static001.geekbang.org/resource/image/ca/1e/ca6ef660c2b7f3777e244a535020191e.jpeg"}除此之外，我还想再多说两句。在这节课开头的反例中，我们分析了开发者采用foreach语句循环遍历分布式数据集的深层次原因。**这种不假思索地直入面向过程编程、忽略或无视分布式数据实体的编程模式，我将其称为单机思维模式**。在学习了 RDD 横向的 partitions 属性和纵向的 dependencies属性之后，如果你能把它们牢记于心，那么在频繁调用或引用这个 RDD之前，你自然会想到它所囊括的数据集合，很有可能在全节点范围内被反复扫描、反复计算。这种下意识的反思会驱使你尝试探索其他更优的实现方式，从而跳出单机思维模式。因此，**深入理解RDD，也有利于你跳出单机思维模式，避免在应用代码中留下性能隐患**。小结今天，我带你学习了 RDD 的重要性，以及它的 2 大核心特性和 4大属性。 首先，深入理解 RDD 对开发者来说有百利而无一害，原因有如下 3点： 1.  Spark 很多核心概念都衍生自 RDD，弄懂 RDD，有利于你全面地学习    Spark；    2.  牢记 RDD    的关键特性和核心属性，有利于你在运行时更好地定位性能瓶颈，而瓶颈定位，恰恰是性能调优的前提；        3.  **深入理解 RDD    有利于你跳出单机思维模式，避免在应用代码中留下性能隐患。**        关于 RDD 的特性与核心属性，只要你把如下 2点牢记于心，我相信在不知不觉中你自然会绕过很多性能上的坑：1.  横向属性 partitions 和 partitioner    锚定数据分片实体，并且规定了数据分片在分布式集群中如何分布；        2.  纵向属性 dependencies 和 compute 用于在纵深方向构建    DAG，通过提供重构 RDD    的容错能力保障内存计算的稳定性。        每日一练1.       在日常的开发工作中，你遇到过"单机思维模式"吗？有哪些呢？        2.       除了我们今天讲的 4 大属性，RDD    还有个很重要的属性：preferredLocations。按照经验，你认为在哪些情况下，preferredLocations    很重要，会提升 I/O    效率，又在哪些环境中不起作用呢？为什么？        期待在留言区看到你的思考，也欢迎你分享工作中遇到过的"单机思维模式"，我们下节课见！