title:Achieving O(1) IP lookup on GPU-based software routers
author:Jin Zhao and
Xinya Zhang and
Xin Wang and
Xiangyang Xue
Achieving O(1) IP Lookup on GPU-based Software Routers
Jin Zhao, Xinya Zhang, Xin Wang, and Xiangyang Xue
School of Computer Science, Fudan University
{jzhao, 06300720198, xinw, xyxue}@fudan.edu.cn
Shanghai, China
ABSTRACT
IP address lookup is a challenging problem due to the in-
creasing routing table size, and higher line rate. This paper
investigates a new way to build an eﬃcient IP lookup scheme
using graphics processor units(GPU). Our contribution here
is to design a basic architecture for high-performance IP
lookup engine with GPU, and to develop eﬃcient algorithms
for routing preﬁx operations such as lookup, deletion, inser-
tion, and modiﬁcation. In particular, the IP lookup scheme
can achieve O(1) time complexity. Our experimental results
on real-world route traces show promising 6x gains in IP
lookup throughput.
Categories and Subject Descriptors
C.2.6 [Computer Communication Networks]: Internet-
working - Routers
General Terms
Design
Keywords
Software router, GPU, IP lookup
1.
INTRODUCTION
The core function of router’s IP lookup engines is longest
preﬁx matching (LPM),which determines the next-hop by
comparing the incoming IP addresses against a set of stored
preﬁxes in routing table. With CIDR (Classless Inter-Domain
Routing), the preﬁx lengths may vary from 1 to 32 for IPv4
addresses. Due to the continuous growth in network link
rates and routing table size, IP lookup engines are faced
with enormous performance challenges. The lookup engines
have to be able to answer ever-increasing number of lookup
queries over a few hundred thousand routing preﬁxes. At
the same time, IP lookup engines also have to accommodate
demands for other update operations such as addition and
deletion of preﬁxes, and the modiﬁcation of next-hop for
existing preﬁxes. TCAM-based hardware approach has the
capability of parallelly searching all the preﬁxes simultane-
ously, leading to low access latency. However, TCAM also
suﬀers from high cost due to high circuit density. Currently,
Copyright is held by the author/owner(s).
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
ACM 978-1-4503-0201-2/10/08.
SM 13
Shared Memory
SM 1
SP
SP
SP
Shared Memory
SM 0
SP
Shared Memory (16KB)
SP
SP
SP
SP
SP
SP
SP
SP
SP0
SP
SP4
SP1
SP
SP5
SP2
SP
SP6
SP3
SP
SP7
I/D/M
L
Figure 1: GPU-based IP lookup engine
there have been many eﬀorts in scaling software routers us-
ing oﬀ-the-shelf, general-purpose PC [1]. Software routers
usually employs trie structure for IP lookup, like the binary
trie, and multibit trie. The main performance bottleneck
for PC-based software routers is main memory access since
LPM will invole additional memory accesses when travers-
ing down the trie hierachy. There has been previous work
in accelerating software router using Graphical Processing
Units (GPUs) [3, 2]. By matching the incoming IP lookup
requests in parallel on GPU, the overall throughput of IP
lookup can be improved.
In this paper, we investigate an alternative approach to
building cost-eﬀective and high-performance IP lookup en-
gines and the corresponding routing table update schemes
using GPUs. Besides the parallelism, we propose an IP
lookup scheme that could achieve O(1) time complexity for
each IP lookup. The key innovations include:(1)IP addresses
are directly translated into memory addresses using a large
routing table on GPU memory. (2)We also map the route-
update operations by leveraging GPU’s graphics processing
facilities,like Z-buﬀer, Stencil buﬀer.
2. ARCHITECTURE
We now describe the proposed GPU-accelerated IP lookup
architecture GR as shown in Fig. 1. We consider four kinds
of routing operations: lookup (L), modiﬁcation (M ), inser-
tion (I) and Deletion(D).
IP lookup. Observing that IP-lookup operations are
far frequent than route-update operations, we seek to trade
429some performance in update for better lookup throughput.
To this end, GR employs a large routing table on GPU,
which enables simple translation from IP address to the
memory address of matched entry. To speed up IP lookup,
we can store all the possible preﬁx entries 232 = 4G in GPU’s
memory. In this way, each IP lookup takes only O(1) mem-
ory access. As the memory sizes of existing mainstream
graphics cards are at an order of magnitude of 256MB or
512MB, in this paper, our implementation only considers
up to 24 bits/16M preﬁx entries, which is not a very cost for
modern GPUs.
In addition, with NVIDIA’s CUDA, GR can simultane-
ously execute a group of IP lookup requests on GPU’s many-
core architecture.
However, directly mapped lookup adds to the complex-
ity in route-update since there are correlations between the
routing entries when using longest preﬁx matching. For
example, if A.B.C/24 is updated, some preﬁxes A.B.D/x
(x < 24) should be updated as well. Such update overheads
are tremendous. In order to address the problem, we seek to
use GPU’s graphics render operations to implement route-
updates.
From GPU’s point of view, the large table which con-
tains 224 elements is a 4096 × 4096 texture which can be
rendered and read. The ﬁrst obstacle is that if we lin-
early map the IP address preﬁx ”A.B.C” to memory address
A · 65536 + B · 256 + C, the rect of a subnet can not form
a square in the texture, which can be rendered in only one
OpenGL command. To overcome this obstacle, a square
hash mapping policy is proposed. This hash function ac-
cepts a 24-bit number as input, and output a 2D coordinate.
Assuming the input is b0b1b2 · · · b23, the output is
(cid:26) x = b0b2b4 · · · b22
y = b1b3b5 · · · b23
This function is inspired from the quadtree in 2D space.
Using this hash function, every preﬁx entry covers a square
area in the texture, which can be eﬃciently manipulated by
graphics processing facilities.
Modiﬁcation. To modify the next-hop information of
a given entry, a lookup operation will be performed ﬁrst.
After locating the memory address, GR will write the new
next-hop information to the target memory.
Insertion. Another obstacle occurs while trying to in-
sert a routing entry in a routing table which already con-
tains some entries. Consider the following situation: A
routing table has entries: A/8, and A.B.C/24, what would
happen if inserting a new entry A.B/16? As a result, the
A.B.C/24 would be incorrectly overwritten with the next-
hop port of A.B/16. The problem can be overcomed by us-
ing Z-Buﬀer algorithm, while the rendering square shall have
diﬀerent depth value corresponding to it’s preﬁx’s length.
At the same time, the current value stored in Z-Buﬀer rep-
resents this entry’s preﬁx length. The process of adding
a preﬁx entry is straightforward:(1)set the GL depth func-
tion to GL GEQUAL, (2)Generate the square of the preﬁx,
(3)Draw the square with depth=preﬁx’s slash value.
Deletion. The last operation a router should support
is to remove(or delete) a routing entry. Unfortunately, Z-
Buﬀer facility supplied by OpenGL hardware is not powerful
enough for removing a routing preﬁx. The stencil buﬀer is
considered indeed. The operation needed by deleting an
entry is as follows: replacing the value of deleted entry with
Table 1: Routing Preﬁx Operations
Lookup(/ms)
GR(FUNET)
GR(RIS)
trie(FUNET)
trie(RIS)
136,458
138,850
19,372.9
36,850.7
Insert(/s) Modify(/s) Delete(/s)
42,776.2
47,620.6
3.30 × 106
6.05 × 106
44,284.6
47,392.8
5.98 × 106
15.3 × 106
23,723.9
4,794.17
2.09 × 106
5.79 × 106
its parent’s. We draw it in two passes: in the ﬁrst pass, the
deleted square’s stencil buﬀer is cleared to zero, in the next
pass the deleted square’s stencil buﬀer is set to its parent’s
depth value.
3. PERFORMANCE
In this section, we present the experimental results for GR
compared to GPU-accelerated trie-based schemes. We have
two routing tables: FUNET [4] and RIS [5] . The traces is
based on a real packet trace containing 99,840 entries from
FUNET. The result is an average of 100 runs on a desktop
with AMD Athlon 64 x2 4400+ 2.3GHz CPU, 1GB RAM
and NVIDIA 8800GT GPU. The results in terms of lookup,
insertion, deletion and modiﬁcation speeds are given in Ta-
ble 1. As can been seen from the table, the proposed scheme
leads to signiﬁcantly faster lookup (say, over 130,000 entries
per ms) and the speedup could be up to 6 times better. We
also observe that the trie-based scheme outperforms our GR
in terms of update speeds, i.e. deletion, insertion, and mod-
iﬁcation. However, considering that the real-world routing
update operations are not so frequent, say, about a few thou-
sand BGP updates per second, the proposed schemes also
suﬃce.
4. CONCLUSIONS
We present the design and evaluation of GPU-accelerated
IP lookup engine that exploits the massive parallelism to
speedup routing table lookup and show that,with proper
desgin, there is the potential for signiﬁcant improvement,
e.g., 6x faster than trie-based implementation. We also de-
signed eﬃcient algorithms that can map deletion, insertion,
and modiﬁcation operations to graphics processing. We
hope these preliminary results will encourage the develop-
ment of GPU-based software routers.
5. ACKNOWLEDGMENTS
This work was supported in part by STCSM under grant
08dz150010E, by NSFC under grant 60803119, and by 863
program under grant 2009AA01A348.
6. REFERENCES
[1] M. Dobrescu, N. Egi, K. Argyraki, B. gon Chun,
K. Fall, G. Iannaccone, A. Knies, M. Manesh, and
S. Ratnasamy. Routebricks: Exploiting parallelism to
scale software routers. In ACM SOSP, October 2009.
[2] S. Han, K. Jang, K. Park, and S. Moon. Packetshader:
Massively parallel packet processing with gpus to
accelerate software routers. In USENIX NSDI ’10
poster, April 2010.
[3] S. Mu, X. Zhang, N. Zhang, J. Lu, Y. S. Deng, and
S. Zhang. Ip routing processing with graphic
processors. In DATE ’10, March 2010.
[4] FUNET. http://www.nada.kth.se/~snilsson/.
[5] RIS. http://data.ris.ripe.net/.
430