but also fails to correctly terminate modern ﬂow-controlled
fabrics, canceling out the investments in a lossless physical
network. As an alternative, we demonstrate how a zero-loss
Overlay Virtual Network (zOVN) can meet both the desired
losslessness and the performance requirements.
2.3 Loss measurements
To support the above claims, we assess the extent of packet
drops using commonly available virtualization solutions. We
perform the experiment shown in Figure 1 in which VM1
and VM2 act as sources and send their traﬃc towards VM3,
which acts as sink, creating a common congestion scenario.
We evaluate (i) where and how frequently losses occur, and
(ii) the maximum bandwidth that a virtual switch can sus-
tain without dropping packets.
We considered the combinations of hypervisors, vNICs,
and virtual switches shown in Table 1. Qemu/KVM is an
open-source hypervisor, whereas H2 is a commercial x86 hy-
pervisor. They were used with two types of vNICs: vir-
tio [34] and N2 are virtualization optimized vNICs designed
for Qemu and H2, respectively, whereas e1000 fully emu-
lates the common Intel2 e1000 adapter. In combination with
Qemu, we used three virtual switches: Linux Bridge [2],
Open vSwitch [3] and VALE [33]. The ﬁrst two are stable
products used in various production deployments whereas
VALE is currently a prototype. The combination Qemu-
e1000-VALE was omitted as it was aﬀected by an imple-
mentation bug that allows internal queues to grow indeﬁ-
nitely, resulting in substantially diverging results between
Table 1: Conﬁgurations for loss measurements.
runs. With H2 we used its own internal virtual switch S4.
All conﬁgurations have been tested on a Lenovo T60p Lap-
top (part of Testbed 1 detailed in Figure 9). Across all
experiments, iperf [1] injects 1514B frames of UDP traﬃc.
We determine the losses and bandwidths using the six mea-
surement points shown in Figure 1: (1) and (6) are inside
the application itself, (2) and (5) are on the TX and RX side
of each vNIC, whereas (3) and (4) are at the virtual switch
ports.
Experiment 1: Both generators injected traﬃc at full
speed for 10s, with the last packet being marked. We com-
puted the number of lost packets as the diﬀerence between
the number of packets transmitted and received at the other
end. We investigate (i) vSwitch losses, i.e., packets received
by the vSwitch input ports (3) and never forwarded to the
vSwitch output port (4), and (ii) receive stack losses, i.e.,
packets received by the destination vNIC (5) and never for-
warded to the sink (6). The TX path is backpressured up
to the vSwitch, hence no losses were observed between other
measurement points. A more accurate analysis of the pos-
sible loss points is presented in Section 4. With VALE and
S4, we could not access the points (3) and (4). Hereby the
diﬀerence between the sender vNIC and the receiver vNIC
counters (points (2) and (5), respectively) was accounted as
virtual switch losses. The results are plotted in Figure 2.
Depending on conﬁguration, the total traﬃc forwarded
during the 10s window varied widely.
In virtualized net-
works performance is bounded by the computational re-
sources assigned to each block by the host operating system.
Compute-intensive conﬁgurations score lower throughputs,
inducing less losses in the vSwitch. An example is given by
the e1000-based conﬁgurations that emulate a fake hardware
to “deceive” the guest driver. The virtualization-optimized
vNICs – i.e., virtio and N2 – achieved higher rates, thus
causing overﬂows in the virtual switch. The performance
optimized VALE switch shifted the bottleneck further along
the path, into the destination VM stack. All these results
are evidence of the lack of ﬂow control between the virtual
network devices, and conﬁrm our initial conjecture.
Experiment 2: To analyze the maximum sustainable
bandwidth for the virtual switches, we varied the target
injection rate at each generator in increments of 5 Mb/s,
starting from 5 Mb/s. The aggregated virtual switch input
traﬃc is the sum, i.e., twice the injection rate. Figure 3a and
Figure 3b plot, respectively, the RX rate and loss ratio as
a function of the total injection rate. Both were calculated
at application level (points (1) and (6)). All conﬁgurations
exhibit saturation behaviors. The RX rate ﬁrst increases
linearly with the TX rate, up to a saturation peak. Beyond
this, with the exception of C4, we observe a drop indicating
a lossy congestive collapse, rather than the desired steady
saturation plateau. The overloaded system wastes resources
to generate more packets, instead of dedicating suﬃcient re-
 0 0.5 1 1.5 2C1C2C3C4C5C6C7Ingress Traffic [GBytes]virtioN2e1000ReceivedvSwitch lossStack loss425(a) vSwitch forwarding performance.
(b) Packet loss ratio.
(c) Packet loss ratio (log scale).
Figure 3: Experimental loss results. The losses are measured between points 1 and 6 from Figure 1.
sources to the virtual switch and destination VM to actually
forward and consume the packets. Although the saturation
point varied considerably across conﬁgurations, loss rates
well in excess of 50% were observed for all conﬁgurations
(Figure 3b). Even far below the saturation load, marked
by vertical lines, we measured losses in the virtual network
(Figure 3c) that were signiﬁcantly above the loss rates ex-
pected in its physical counterpart, i.e., up to 10−2 instead
of 10−8 for MTU-sized frames with a typical bit-error rate
of 10−12.
The “noise” in Figure 3c conﬁrms our intuitive hypothe-
sis about large non-causal performance variability in virtual
networks. In fact, the service rate of each virtual link de-
pends critically on the CPU, load, process scheduling, and
the computational intensity of the virtual network code.
Suboptimal and load oblivious scheduling causes frequent
losses, e.g., by scheduling a sender prior to a backlogged re-
ceiver. Lossless virtual switches would be of great interest,
not only in terms of eﬃciency but also for performance pre-
dictability. The next sections will present how ﬂow control
can be implemented in virtualized datacenter networks.
3. ZOVN DESIGN
In this section we outline the core principles that guided
the design of our lossless virtual network.
3.1 Objectives
A converged virtualized network infrastructure must si-
multaneously satisfy the requirements from the domains be-
ing converged. As mentioned above, losslessness is a func-
tional requirement of various HPC, storage and IO applica-
tions, whereas on-line data-intensive workloads impose per-
formance requirements of 200 ms user-level response times.
We base our lossless virtual datacenter stack on CEE-
compatible ﬂow control. Transport-wise, we anchor zOVN’s
design on the established TCP stack combined with lossless
overlays as proposed here. Our objectives are :
1) Reconcile the ﬂow completion time application perfor-
mance with datacenter eﬃciency and ease of management.
This proves that network virtualization and horizontally-
distributed latency-sensitive applications are not mutually
exclusive. This may remove an obstacle for virtual network
deployment in performance-oriented datacenters.
2) Prove that commodity solutions can be adapted for siz-
able performance gains. As shown in Section 5, a 15-fold ﬂow
completion time reduction is also attainable without a clean-
slate deconstruction of the existing fabrics and stacks. One
can achieve comparable performance gains with CEE fab-
rics and standard TCP stacks. Considering the total costs
of ownership, this evolutionary reconstruction approach is
likely to outperform other, possibly technically superior, al-
ternatives in terms of cost/performance ratios.
3) Expose packet loss as a costly and avertable singu-
larity for modern datacenters, and, conversely, losslessness
as a key enabler in multitenant datacenters for both (i) the
query and ﬂow completion time performance of horizontally-
distributed latency-sensitive workloads, and (ii) the conver-
gence of loss-sensitive storage and HPC applications. This
basic HPC principle has already been proved by decades of
experiences in large-scale deployments. As faster InﬁniBand
and CEE fabrics are widely available at decreasing prices,
datacenters could also now beneﬁt from prior HPC invest-
ments in lossless networks.
4) Design and implement a proof-of-concept zero-loss vir-
tual network prototype to experimentally validate the above
design principles in a controllable hardware and software en-
vironment.
5) Finally, extend and validate at scale the experimental
prototype with a detailed cross-layer simulation model.
3.2 End-to-end Argument
The wide availability of lossless fabrics and the thrust of
SDN/OpenFlow have prompted us to reconsider the end-
to-end and “dumb network” arguments in the context of
datacenters. The end-to-end principle [35] can be traced
back to the inception of packet networks [12]. Brieﬂy stated,
application-speciﬁc functions are better implemented in the
end nodes than in the intermediate nodes: for example, error
detection and correction should reside in NICs and operat-
ing system stacks and not in switches and routers. While
one of the most enduring design principles, this can also re-
strict the system level performance in end-to-end delay, ﬂow
completion time and throughput [14].
In datacenters, the delay of latency-sensitive ﬂows is im-
pacted not only by network congestion, but also by the
end-node protocol stacks [32]. Historically, for low-latency
communications, both Arpanet and Internet adopted “raw”
transports—unreliable, yet light and fast—instead of TCP-
like stacks. Similarly, InﬁniBand employs an Unreliable
Datagram protocol for faster and more scalable “light” com-
munications. Also HPC protocols have traditionally used
low-latency end-node stacks based on the assumption of a
lossless network with very low bit-error rates. Given the
increasing relevance of latency-sensitive datacenter applica-
 0 200 400 600 800 1000 1200 0 200 400 600 800 1000 1200 1400 1600RX Traffic [Mbps]TX Traffic [Mbps]C1C2C3C4C5 0 10 20 30 40 50 60 70 0 200 400 600 800 1000 1200 1400 1600Loss Ratio [%]TX Traffic [Mbps]C1C2C3C4C510-410-310-210-1100101102 0 200 400 600 800 1000 1200 1400 1600Loss Ratio [%]TX Traffic [Mbps]C1C2C3C4C5426tions, current solutions [8, 9, 41, 38] adopted an intriguing
option: decouple ﬂow control from the fabric. Here we show
that coupling ﬂow control with the fabric positively impacts
the workload performance.
3.3 Overlay Virtual Network Design Space
The simplest virtual network would start with a large ﬂat
layer-2 network for each tenant. However, this approach
does not scale within the practical constraints of current
datacenter network technologies. The increasing number of
VMs has led to a MAC address explosion, whereby switches
need increasingly larger forwarding tables. Also, dynamic
VM management stresses the broadcast domains [28]. More-
over, today’s limit of 4K Ethernet VLANs is insuﬃcient for
multitenant datacenters unless Q-in-Q/MAC-in-MAC en-
capsulation is used. Finally, the datacenter network must
support dynamic and automatic provisioning and migration
of VMs and virtual disks without layer-2 or -3 addressing
constraints. The emerging solution to full network virtual-
ization are the overlay virtual networks. A number of over-
lays have recently been proposed [21, 37, 26, 28, 11, 17].
Their key architectural abstraction lies in the separation
of virtual networking from the underlying physical infras-
tructure. Overlays enable an arbitrary deployment of VMs
within a datacenter, independent of the underlying layout
and conﬁguration of the physical network, without chang-
ing or reconﬁguring the existing hardware.
Current overlays are predominantly built using layer-2 to
-4 encapsulation in UDP, whereby the virtual switches in-
tercept the VM traﬃc, perform the en-/de-capsulation, and
tunnel the traﬃc over the physical network. Each VM has
an associated network state residing in the adjacent switch.
Upon VM migration, the virtual switches update their for-
warding tables to reﬂect the new location. Using encapsu-
lation over IP [28, 26, 11, 17], the VM locations are neither
limited by the layer-2 broadcast domains, nor by VLAN ex-
haustion. Instead, full IP functionality is preserved, includ-
ing QoS and load balancing. Furthermore overlays are in-
dependent of location, domains and the physical networking
capabilities. Thus these virtual switches are similar to tra-
ditional hypervisor switches, but now with additional func-
tionality as overlay nodes.
Inherently an overlay network
trades some of the bare-metal performance for manageabil-
ity, ﬂexibility and security.
Performance-wise, such overlays inﬂuence datacenter’s ef-
ﬁciency and scalability. First, on the data plane:
they
use encapsulation to build tunnels between virtual switches.
Current encapsulation solutions, such as VXLAN [26] and
NVGRE [37], solve the original VLAN limitation while re-
ducing the conﬁguration and management overhead. Sec-
ond, on the management plane: conﬁguration, distribution,
and learning protocols are necessary to create tunnels at
each virtual switch. To create a tunnel, the overlay switch
must map the destination address to its physical location
using either the learning or the centralized approach. The
learning approach, used by VXLAN [26], ﬂoods packets with
unknown destinations. The centralized approach relies on
the virtual switches to retrieve the information required for
encapsulation.
In NetLord [28], this information is learnt
by the switches as they communicate with each other and
from a central conﬁguration repository. In DOVE [11, 17],
this conﬁguration information is retrieved from a central-
ized database. Both the central conﬁguration repository
Figure 4: Flat layer-2 fabric with 256 servers.
in NetLord and the centralized database in DOVE must
be highly available and persistent. This poses a challenge
for multi-million node datacenters, thus indicating a future
third option of a distributed repository approach, presuming
the entailing coherency issues can be solved eﬃciently. For
now, the learning and centralized approaches are simpler to
design and manage. Notably, the centralized method also in-
herently prevents ﬂooding, the main drawback of the learn-
ing approach. For zOVN we have adopted and extended
DOVE’s centralized approach with a custom encapsulation
header.
4. ZOVN IMPLEMENTATION
In this section we describe the details of the implementa-
tion of our proposed lossless overlay network (zOVN). We
assume a collection of virtualized servers, each running a set
of virtual machines. The servers are interconnected through
a ﬂat layer-2 fabric (an example is shown in Figure 4). The