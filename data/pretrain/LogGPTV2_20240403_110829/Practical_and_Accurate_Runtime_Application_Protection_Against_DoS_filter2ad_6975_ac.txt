processes running in namespaces. First, Cogo starts the process in a suspended
state inside the container and gets the process id in the container namespace
(NSPID). (The NSPID from the loader process is the PID local to the container
where the process is running.) This is possible by creating a custom loader
process that outputs its NSPID and its namespace identiﬁer (NSID), then sends
a stop signal to itself. (The NSID is a unique namespace identiﬁer.) When the
loader process receives a continue signal, it loads the desired target program via
a call to the exec system call. Given the NSPID and NSID, Cogo searches all
namespaces on the host system for a matching child NSID that contains a match-
ing NSPID. Once identiﬁed, Cogo extracts the global PID of the process from
the identiﬁed child namespace. It then attaches to that process (the loader) using
the global PID and sends it a continue signal. Upon receiving the continue signal
6 The Linux kernel manpage for namespaces is available at: http://man7.org/linux/
man-pages/man7/namespaces.7.html.
460
M. Elsabagh et al.
by the loader, it loads and executes the desired target using the exec system
call, replacing the process image but retaining any PIDs. Cogo then continues
normal operation.
4 Implementation
We implemented Cogo by extending the code base of Radmin [25]. Radmin
oﬀered several integrated kernel space and user space tracing capabilities and
an extensible PFA engine, which allowed us to implement and benchmark Cogo
in a uniﬁed way. Figure 2 illustrates the architecture of Cogo within Radmin.
We extended Radmin’s kernel tracer to support network I/O monitoring, and
implemented Cogo’s learning and detection algorithms by extending Radmin’s
PFA engine which originally only supported a quadratic time PFA construction
(q-PFA in the ﬁgure). We also extended the framework to support attaching to
running processes and monitoring containerized processes.
Fig. 2. Cogo’s architecture within Radmin. Cogo extends Radmin with a network I/O
monitoring module, the linear PFA construction component, a non-deterministic PFA
executor, and a custom loader to resolve namespace PIDs.
We extended Radmin’s kernel tracer to support network I/O monitoring by
attaching handlers to the relevant tracepoints [24] in the kernel. Kernel trace-
points are special points in the executable kernel memory that provide hooks to
various events in the kernel. The hooks call functions (probes) that are provided
at runtime by kernel modules. Cogo provided a handler for each tracepoint where
it collected and reported the measurements to the rest of Radmin as needed. Each
tracepoint executes in the context of the process that triggered the event. Cogo
ﬁlters out process contexts using the global PIDs of the monitored processes.
Practical and Accurate Runtime Application Protection
461
Table 1. Kernel tracepoints hooked by Cogo for network I/O monitoring.
Kernel tracepoint
socket.create
socket.close
socket.sendmsg, socket.writev,
socket.aoi write, socket.write iter
socket.recvmsg, socket.readev,
socket.aoi read, socket.read iter
Description
A socket is allocated
A socket is closed and released
Data is being sent on a socket
Data is received on a socket
It supports monitoring a single process, all processes part of one program, or
all processes in a process tree. Table 1 lists the relevant tracepoints that Cogo
hooked to monitor network state.
5 Evaluation
We measured the detection accuracy, earliness, and overhead of Cogo on two
large-scale server applications that are commonly targeted by application layer
DoS attacks: Apache [2], the world’s most used web server software; and Open-
SIPS [9], the famous free VoIP server and proxy implementation of the session
initiation protocol (SIP) [33]. The testbeds used Docker containers for isolation
and CORE [15] for network emulation.
5.1 HTTP Attacks on Apache
Our Apache testbed is depicted in Fig. 3. It consisted of a server running Apache,
one User Agent (UA) node for benign clients, and one Weaponized User Agent
(W-UA) node for attackers. UA and W-UA consisted of Docker containers run-
ning HTTP clients. We generated benign traﬃc using an HTTP client model
derived from Choi-Limb [22]. From the mean and standard deviation for various
model parameters for the data set reported in [22], we used a nonlinear solver
to calculate approximate distribution parameters for the distribution found to
be a good ﬁt in Choi-Limb. We represented each client using one instance of the
HTTPerf [7] benchmark. For each client, we generated a workload session using
a unique seed and the distilled distribution parameters. The session consisted of
a series of requests with a variable think time between requests drawn from the
client model. We generated the workload session in HTTPerf’s workload session
log format (wsesslog). Each client request contained as URL parameters a ran-
dom request length padding and a requested response size drawn from the client
model. The Apache server hosted a CGI-bin web application that simulated real
deployments. For each client HTTP request, the server responded with content
equal in byte length to the requested response size.
462
M. Elsabagh et al.
Fig. 3. HTTP DoS testbed used in our experiments, including the Apache server, a
User Agent (UA) node where benign clients reside, and a Weaponized User Agent
(W-UA) node where attacks originate from.
Attack traﬃc originated from the W-UA node. We used the HTTP appli-
cation layer DoS benchmark SlowHTTPTest [4] which bundles several Slow-
rate [11] attack variants. (Slow-rate attacks are low-bandwidth application layer
DoS attacks that use legitimate albeit slow HTTP requests to take down web
servers.) Two famous examples of Slow-rate attacks are Slowloris [12] and
Slowread [5]. In Slowloris, attackers send the HTTP request headers as slowly as
possible without hitting the connection timeout limit of the server. Its Slowread
variant sends the headers at normal speeds but reads the response as slowly
as possible. If enough of these slow requests are made in parallel, they can
consume the entire server’s application layer connections queue and the server
becomes unable to serve legitimate users. Slow-rate attacks typically manifest in
an abnormally large number of relatively idle or slow sockets.
We built Cogo model for Apache using 12 benign traﬃc runs, each of which
consisted of one hour of benign traﬃc. We set the number of benign clients
to 100. Note that each benign client is a whole workload session. For testing,
we performed several experiments using blended benign and attack traﬃc by
injecting attack requests at random points while serving a benign load. Test-
ing is performed by running Apache under Cogo in detection mode, serving
one hour worth of benign requests from 100 benign clients and 100 Slow-rate
clients (attackers). The number of attackers represents the total concurrent
SlowHTTPTest attack connections. We limited the attack duration to 15 min.
We conﬁgured Apache to serve a maximum of 100 concurrent connections at any
moment in time.
We performed each experiment with and without Cogo. We conﬁgured Cogo
to kill the oﬀending Apache worker process when an attack is detected.7 Finally,
we experimented with two types of attackers: non-aggressive attackers that seep
7 More advanced remediation policies can be used, such as blocking oﬀending source
IPs, rate limiting, or protocol-speciﬁc recovery. We opted for process termination for
simplicity as remediation is not the focus of Cogo.
Practical and Accurate Runtime Application Protection
463
in the server at a very slow rate, and aggressive attackers that bombard the server
with as many concurrent connections as possible. For non-aggressive attackers,
we set the SlowHTTPTest connection rate to one connection per second. For
aggressive attackers, we set the connection rate to the server capacity, i.e., 100
connections per second.
Detection Results. Table 2 summarizes the results. It took Cogo only about
12 min to build a model for Apache from the benign measurements. This is
about a 505× improvement over Radmin which took more than four days to
construct a model from the same measurements. The savings in training time
came at the expense of a slight increase in the model size (from 34 MB to
76 MB) which is acceptable and does not pose a bottleneck. The model is only
loaded once at startup of Cogo; detection time is invariant of the model size as
each measurement point results in exactly one transition in one of the PFAs.
Cogo achieved a very low false positive rate (FPR) at 0.0194% (about 91%
better than Radmin). We believe the reason for this reduction in FPR is that
Cogo retains longer low-probability paths in the PFA as the detection algorithm
limits transition probabilities rather whole path probabilities as in Radmin. For
the most part, false positives (FPs) were encountered during startup or shutdown
of Apache which from experience has shown considerable variability.
Table 2. Summary of results for Apache. The number of requests was 473,558.
Item
Training time (sec.)
Model size (MB)
FPs, FPR
Downtime (sec; non-aggressive) 137
Downtime (sec; aggressive)
58
752
76
Cogo
379,661
Radmin
Improvement
(cid:2) 505×
(cid:3) 0.45×
34
1,116, 0.2357% 92, 0.0194% (cid:2) 12×
(cid:2) ∞
(cid:2) 8.3×
0
7
Figures 4 and 5 depict the availability of Apache against non-aggressive and
aggressive attacks. Cogo successfully prevented Apache from going down against
non-aggressive attacks. As the attack connections were idling at the server side,
Cogo detected anomalous transmit and receive rates and terminated the attacked
Apache workers. This occurred within seven seconds from connection establish-
ment. Against the same attacks, Apache under Radmin remained down for longer
than two minutes. For aggressive attacks, Apache protected with Radmin was
down for one minute, compared to only seven seconds under Cogo.
5.2 VoIP Attacks on OpenSIPS
Next, we considered detection of resource attacks on VoIP servers as telephony
systems have increasingly become targets of DDoS attacks evidenced during the
464
M. Elsabagh et al.
Fig. 4. Apache server availability against non-aggressive Slow-rate attacks. With Rad-
min, the server was down for more than two minutes. There was no downtime under
Cogo.
2015 attack on the Ukrainian power grid [13]. To establish and manage calls,
VoIP servers rely on Session Initiation Protocol (SIP) [33] which is known to be
vulnerable to exhaustion and overload, even under benign conditions [29]. Over-
load can be caused by a variety of legitimate SIP behaviors such as response
duplication, call forwarding, and call forking (conference calls) which result in
large numbers of control packets that may congest servers. Similarly, exces-
sive transactions cause system resource exhaustion in stateful servers when the
number of requests exceeds the ﬁnite memory available to track each call state
machine. An adversary who wishes to cause DoS can do so by initiating calls
that exercise these legitimate but atypical resource intensive behaviors and thus
degrade server performance — all while blending in with normal traﬃc (without
malformed packets or speciﬁcation violations) to circumvent defenses such as
scrubbing or bandwidth limitation. In the following we evaluate Cogo against
these protocol attacks on a representative SIP testbed based on OpenSIPS [9].
Practical and Accurate Runtime Application Protection
465
Fig. 5. Apache server availability against aggressive Slow-rate attacks. Cogo reduced
the server down time by at least a factor of eight, down from 58 s to only seven seconds.
Testbed and Procedure. Our SIP DDoS testbed, shown in Figure 6, con-
sisted of a SIP server and pairs of SIP user agents and weaponized agents that
serviced simultaneous callers and attackers. The SIP server ran OpenSIPS 2.2
and was conﬁgured using the residential conﬁguration generated by the Open-
SIPS conﬁguration tools. OpenSIPS used ﬁxed-size shared and private memory
across its child processes (32 MB and 16 MB respectively). To exacerbate mem-
ory exhaustion at the server, we adjusted the wt timer of the OpenSIPS to 32 s
(the recommended value in the RFC) which corresponds to the length of time a
transaction is held in memory after it has completed. Though intended to help
absorb delayed messages after the transaction completed, it also inadvertently
reserves memory that could otherwise be made available to handle new calls. For
the following experiments, we considered a small enterprise or large residential
deployment, thus end-to-end delays from UA to server were minimal (ten ms)
and link bandwidth was isolated to SIP traﬃc at 100 Mbps.
Pairs of UA nodes were used to represent benign SIP callers (UA-1) and
callees (UA-2). These nodes ran instances of the SIP Proxy (SIPp) [10]: a SIP
466
M. Elsabagh et al.
Fig. 6. SIP DDoS testbed used in our experiments. UA-1 and UA-2 are benign user
agents. W-UA-1 and W-UA-2 are attack (weaponized) agents.
benchmarking tool to generate SIP caller/callee workloads. While we did not
model the audio portion of the call, we leveraged the log-normal feature of SIPp
to insert a random, lognormal distributed pause between call setup and hang
up to simulate variability among call lengths. Our call length distribution was
log-normal with a mean of 10.28 and variance of one ms equating to an average
call length of 30 s. Each call consisted of an INVITE transaction followed by the
variable pause, and then terminated with a BYE transaction. SIPp can initiate