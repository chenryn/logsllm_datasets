none of the memory shared by the last one processed. This
asymmetry presents a problem: since the scanning mecha-
nism treats each task’s shared memory differently, we get
an inconsistent view of the memory usage picture.
One option would be to run the garbage collector sepa-
rately for each task. This would return the total amount of
memory being held live by each task, similar to the “pre-
cise” policy described by Wick et al. [53]. Since we would
need to start garbage collection fresh for each task, such a
system would impose additional time costs, processing each
shared object once for every task that can reach it. Without
additional computation (and additional overhead), this sys-
tem would also not report how much of the memory used
by a given task is shared with other tasks.
We instead chose to address the asymmetry problem by
rotating the order that tasks are processed on subsequent
collections. The effect of this can be seen by comparing
Figures 3 and 4; changing the processing order changes the
memory charged to each task.
The ﬁrst task processed yields a maximum value—an up-
per bound on memory reachable by that task and includes
all memory it shares with other tasks. The last task pro-
cessed yields a minimum value, indicating how much mem-
ory that task is responsible for that no other task has a ref-
erence to. Results for tasks in the middle give an intermedi-
ate value somewhere between these two extremes. Rotating
tasks from the back of the processing list to the front means
that the minimum and maximum values computed for each
will be measured one collection apart from each other. This
yields an imperfect snapshot of memory usage, but barring
dramatic swings in memory being held live by a task in be-
tween collections, this rotation gives a valuable approxima-
tion to how much memory each task is both using on its
own and is sharing with other tasks. The synthesis of this
raw information into useful policies is discussed further in
Section 4.
2.3.2 Unaccountable references
One concern of garbage collector-based memory account-
ing has been described as the “resource Trojan horse” prob-
lem [35]. In this case, task B might accept a reference to
an object provided by task A. This object might in turn con-
tain a reference to a very large block of memory. Task B
will then be held responsible for that large block, even if
it is unaware of the block’s existence. Depending on the
system’s memory management policy, this could represent
a denial of service attack on task B. Task B may want to ac-
cept a reference to an object controlled by an untrusted task
without exposing itself to such an attack. Similarly, a sys-
tem library providing access to a database may (generously)
not want the client task to be charged for storage within the
database. Finally, it might be the case that all tasks have
pointers to and from a centralized manager system, and so
there is a path in memory from each task to the memory
of every other task. Our system as described so far would
na¨ively follow these references and describe the whole sys-
tem as one region being fully shared among all the tasks.
This is clearly not the most insightful view of the picture;
we want a way to support all these styles of references, yet
still be able to separate tasks from one another for measure-
ment of their memory usage.
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
We solve these issues by introducing unaccountable ref-
erences. Analogous to a weak reference (which refers to
some data without holding it live), this type of reference
refers to data, holds it live, but prevents the referrer from
having to pay for what’s on the other side. In our system,
when the garbage collector encounters an unaccountable
reference, it stops without proceeding to the object being
referred to. After all tasks are processed, it starts again with
all the unaccountable references in the system as roots. If
the only path a task has to some memory is through an un-
accountable reference, the memory will be guaranteed to be
held live, but that task will not be charged for that memory.
A task must not be able to use unaccountable references
to circumvent the memory accounting system. One solution
would be to use language-level access control (e.g., stack
inspection) to restrict the creation of unaccountable refer-
ences to privileged code.
Our preferred approach is to permit any task to create an
unaccountable reference, but to tag that reference with its
creator’s name. When these references are processed, the
accounting system charges the memory found to the ref-
erence’s creator. This technique, implemented as a small
adjustment to the accounting system, nonetheless provides
powerful semantics for memory sharing; a task can provide
references to some service it’s providing, and make it ex-
plicit in the interface that clients will not be billed for the
memory found on the other side of the reference.
2.3.3 Generational GC
Generational garbage collection presents some challenges
to our system. In a generational system, not all objects are
traced every time the GC system is invoked. Instead, ob-
jects are allocated into a “nursery” heap, and are tenured
by frequent minor collections into a mature space, which is
collected using some other algorithm when it ﬁlls. Memory
in the nursery is transient: upon each collection, it is ei-
ther tenured or reclaimed. Thus, we’re primarily interested
in accounting for the memory that makes it to the mature
space.
We can track a task’s mature heap memory usage in two
ways. When the mature space ﬁlls, we do a major collec-
tion and count mature heap memory that remains alive using
the techniques described above in Section 2.3.1. When the
nursery ﬁlls, a minor collection is performed. As objects are
tenured, the size of each object is added to the total mem-
ory used by the task. At each major collection, this additive
component is reset to zero. Thus, every tenured object will
have been charged to one of the tasks that held it live while
it was in the nursery. On subsequent major collections, the
tasks holding the object live will share the cost of the object
in the same fashion as they do for non-generational semis-
pace collectors.
2.3.4 Other memory management techniques
We have implemented our system in a standard semispace
collector and a generational collector, but we anticipate that
it can be made to work with most precise, tracing collectors.
In particular, we expect that it would map well to mark-
and-sweep collectors, as this class of garbage collector also
traces through memory ﬁnding live objects from a deﬁned
set of roots.
Our approach would not work if the memory manage-
ment system used reference counting. Such a system does
not do graph traversals over the space of objects in the heap,
and so it would not discover the pattern of objects being
held live by various roots, nor could it make any meaning-
ful inferences about memory sharing.
A conservative garbage collector [12] would raise a num-
ber of difﬁcult issues: tasks might be charged for memory
discovered when the collector follows something that is not
actually a reference. Unaccountable references would likely
cause a signiﬁcant performance hit, as each reference fol-
lowed, unable to explicitly describe itself as an unaccount-
able reference, would have to be checked against a table of
such references.
3 Implementation and results
We implemented our design in Java using IBM’s Jikes
Research Virtual Machine (RVM) [1] version 2.1.0. We
found the RVM to be extremely useful for our work: it is im-
plemented in Java, is largely self-hosting (e.g., the garbage
collectors are, themselves, written in Java), and provides
several different garbage collectors to choose from. We
implemented our system as a set of changes to the RVM’s
simple copying collector (called “semispace”) and its two-
generational collector (“copyGen”). GCTk1 is a ﬂexible
garbage collection toolkit for the RVM, but we chose to
work with the default GC system that ships with the RVM
as it satisﬁed our requirements.
The set of changes that we made to the RVM codebase
is small; our changes can be expressed as a thousand-line
patch against the original 64,000-line RVM codebase. Our
modiﬁed RVM exposes additional functionality to allow the
system to label which classes and threads are associated
with which tasks, and to query the resource usage statistics
of any given task. The resulting RVM is fully backwards-
compatible with the original.
For the purposes of our prototype implementation, we
deﬁned a task to be a set of classes loaded by a particu-
lar ClassLoader instance, plus any threads that loaded
those classes, plus those threads’ children. The root set of
each task processed by the garbage collector consists of the
1See http://www.cs.umass.edu/˜gctk/.
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
Figure 5. Runtime overhead incurred by the accounting modiﬁcations on Boehm’s artiﬁcial GC bench-
mark and on various real-world application benchmarks with the RVM “semispace” and “copyGen”
collectors.
static ﬁelds of all of its classes and the stacks of all of its
threads.
We benchmarked our implementation on a 1 GHz AMD
Athlon with 512MB of memory running version 2.4.18 of
the Linux kernel. Different benchmarks allocate different
amounts of memory, so we chose heap size appropriately
in order to guarantee that the tasks would execute without
allocation errors but still exercise the garbage collector suf-
ﬁciently as to measure our modiﬁed system’s performance.
3.1 Boehm microbenchmark
We wanted to ensure that our modiﬁcations to the
its perfor-
garbage collector did not adversely impact
mance, so we benchmarked our implementation using Hans
Boehm’s artiﬁcial garbage collection benchmark2, which
repeatedly builds up and throws away binary trees of vari-
ous sizes. Figure 5 shows the overhead of memory account-
ing for the two garbage collectors we used on this bench-
mark. The results indicate that the modiﬁed GC incurs a
small percentage of overhead as a cost of doing its account-
ing.
3.2 Application benchmarks
cally designed to stress-test the memory subsystem. One
limitation we suffered was that AWT is not yet imple-
mented with the RVM, so we were somewhat limited in
our choice of programs. We benchmarked the applications
JavaCup,3 a LALR parser-generator, Linpack,4 an imple-
mentation of a matrix multiply, and OTP,5 an S/Key-style
one-time-password generator.
Figure 5 shows the overhead of our memory accounting
system for these three applications for the two garbage col-
lection systems. As with the Boehm microbenchmark, the
slowdown is negligible.
In the case of Linpack with the
semispace collector, we actually saw a minuscule speedup.
Linpack puts very little pressure on the GC system, allo-
cating large arrays once, then processing with no further
allocation. It’s thus unsurprising that our changes to the GC
system have minimal impact. A small speedup could result
from fortuitous rearrangements of how code or data collides
in the processor’s caches, TLB, and so forth.
3.3 Multitasking microbenchmark
Since our system is designed to handle several tasks run-
ning concurrently, sharing memory amongst themselves,
traditional single-tasking benchmarks are insufﬁcient to ex-
We also benchmarked some real-world Java applications
to get a sense of the overhead for programs not speciﬁ-
2http://www.hpl.hp.com/personal/Hans_Boehm/gc/
gc_bench/.
3http://www.cs.princeton.edu/˜appel/modern/
java/CUP/.
4http://netlib2.cs.utk.edu/benchmark/
linpackjava/.
5http://www.cs.umd.edu/˜harry/jotp/.
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
Figure 6. Runtime overhead of memory accounting on the multitasking microbenchmark with the
RVM “semispace” and “copyGen” collectors, varying the number of active tasks.
ercise our system as it’s intended to run. While we could
have used a number of benchmarks from the database com-
munity, such as OO7 [15], these benchmarks are not primar-
ily designed to place pressure on the garbage collector. To
address this, we decided to write our own synthetic bench-
mark.
Our benchmark draws its inspiration from Boehm’s, in
that it also deals with binary trees, but in order to ensure
a good degree of memory sharing, we used applicative bi-
nary trees. An applicative tree is a functional data structure,
immutable once it is created. To perform an insert on an ap-
plicative tree, the node that would normally have been mu-
tated is instead replaced with a newly allocated node, as are
all of its ancestors. Each insertion thus allocates O(logn)
new nodes. Throwing away the reference to the old tree
likewise makes O(logn) old nodes dead, and thus eligible
for garbage collection.
In our benchmark, a random applicative binary tree of
tens of thousands of nodes is generated, and a reference to
this tree is passed to each task. Each task then repeats two
phases: adding new elements to its view of the tree, and
randomly trading its view of the tree with another task’s
view of the tree. Each task performs a number of inser-
tions inversely proportional to the number of tasks present
in the benchmark, such that the total number of insertions
performed over the benchmark run remains constant regard-
less of the number of tasks participating in the test. The total
running time of the benchmark thus stays relatively ﬂat as
we vary the number of concurrent tasks.
We ran this benchmark using a 64MB heap size; for the
generational collector, we employed a 16MB nursery heap.
We measured three time values: the setup time (the amount
of time required to load all of the classes and assign them
to their tasks), the time spent in the garbage collector, and
the remaining runtime. Our results are presented in Figure 6
and Table 1. The graph in Figure 6 shows that performance
overhead varies noticeably as we increase the number of
tasks, but is generally quite small. In some cases, the mod-
iﬁed system outperformed the original system, although in
other cases, the original system outperformed the modiﬁed
system. Such variations most likely result from fortuitous
rearrangements of how code or data collides in the proces-
sor’s caches, TLB, and so forth. Table 1 shows averages
over all the benchmark runs. On average, we observe that
our system adds a negligible overhead to the benchmark’s
total running time (around 1%).
Another interesting observation is how often the garbage
collector actually runs.
The semispace collector runs
roughly once every two seconds. With the generational col-
lector, however, major collections tend not to occur very of-
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
Garbage Collector
Original
Semispace Modiﬁed
Load Time GC Time
(sec)