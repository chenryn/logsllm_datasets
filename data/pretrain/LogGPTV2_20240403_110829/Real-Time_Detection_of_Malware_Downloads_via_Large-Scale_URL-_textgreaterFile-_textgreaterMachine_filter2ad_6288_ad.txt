Uu are ﬁles and URLs from download events where all nodes
are unknown and never seen before. Nonetheless, the clas-
siﬁer can easily detect them. For ﬁles and in four months,
for example, Mastino achieved 100% TP with no FPs (see
Figure 6(c)). This is due to the power of our tripartite graph
layers as well as system-level features that could help in these
situations where not enough historic information is avail-
able. To help the reader better understand how the classi-
ﬁer can classify such cases, we provide a few case studies in
Section 5.8. Furthermore, as Figure 6 shows, the detection
results are fairly consistent throughout our dataset.
5.3 Overall Classiﬁcation of Download Events
In Section 5.2 we evaluated the ﬁles and URLs classiﬁers
separately. In this section, we discuss how we evaluate the
classiﬁcation system as a single unit to label download events
daily. As a reminder, a download event is a 3-tuple of ﬁles,
URLs, and machines. Mastino labels a download event as
malicious, if the event’s URL, ﬁle, or both are labeled as
bad by the classiﬁers. So for evaluation, we consider the
test samples of one day of train and test experiment. Then
we classify the ﬁles and URLs with FP set to 0.5%. Then we
consider all the related download events to test nodes and
label them according to the following two rules: i) if ﬁle is
a TP or URL is a TP, the event will be labeled as TP, and
ii) otherwise if ﬁle is a FP or URL is a FP, then event will
be labeled as FP. We experimented with various test days
spread randomly across our dataset. Table 4 reports ﬁve of
those test days (results of other test days are very similar).
Table 4: Results of labeling download events on ﬁve sample
test days
Month of Test Day
Events
Feb
Mar
Apr
May
Jun
test events TP% FP%
4,205
4,581
4,163
4,004
3,856
96.2
95.4
97.3
96.1
94.0
0.4
0.5
0.5
0.4
0.5
5.4 Feature Analysis
In order to assess the usefulness of various feature groups
in our system, we performed extensive feature analysis. In
particular, in this section we will show that the presence of
all three layers are necessary for detection of ﬁles and URLs.
To perform feature analysis, we proceed as follows. We re-
move all groups of features that are related to a speciﬁc layer
of the graph from the classiﬁers, perform the train and test
experiment, and compare the results with the case of having
all features present. For example, for ﬁles classiﬁer, ﬁrst we
can remove the features related to the URLs layers, which
essentially means having only 2 layers of ﬁles and machines
in the graph and perform a train and test experiment. Then
in another train and test experiment we can remove the ma-
chine layer. In addition, we also remove the intrinsic features
(system-level features) of the classiﬁers and compare the re-
sult with the case of keeping them to show that they are
also essential for better detection results. Figure 7(a) shows
the feature analysis results for the ﬁle classiﬁer for a test-
ing period in February which is an aggregate of 5 days (see
Section 5.2.2). While the ROC labeled as “All Features” is
the detection result of a classiﬁer that beneﬁts from all the
0.00.20.40.60.81.0FP(percentage)020406080100TP(percentage).JanFebMarAprMayJunJulAverageSizeofTestDatasetMalwareFiles=188BenignFiles=4800.00.20.40.60.81.0FP(percentage)020406080100TP(percentage).JanFebMarAprMayJunJulAverageSizeofTestDatasetMalwareFiles=127BenignFiles=3960.00.20.40.60.81.0FP(percentage)020406080100TP(percentage).JanFebMarAprMayJunJulAverageSizeofTestDatasetMalwareFiles=10BenignFiles=1750.00.20.40.60.81.0FP5percentage)020406080100TP5percentage).JanFebMarAprMayJunJulAverageSizeofTestDatasetMaliciousURLs=4,373BenignURLs=10,5700.00.20.40.60.81.0FP(percentage)020406080100TP(percentage).JanFebMarAprMayJunJulAverageSizeofTestDatasetMaliciousURLs=3,183BenignURLs=6,5580.00.20.40.60.81.0FP0percentage)020406080100TP0percentage).JanFebMarAprMayJunJulAverageSizeofTestDatasetMaliciousURLs=158BenignURLs=6,202790layers of the graph as well as the ﬁles intrinsic features, each
of the other ROCs are generated by removing one group of
features at a time. Figure 7(b) shows a similar experiment
but for URLs. Clearly, the performance of the classiﬁer is
the best when all features are used.
(a) Fn test ﬁle nodes
(b) Un test URL nodes
Figure 7: Feature analysis results (testing period: February)
5.5 Efﬁciency
The whole MDD system (Figure 3) would run on a server
that receives requests from DIAs. The server that we used
is actually quite a light machine with only 8GB of RAM
and 8 1.2GHz CPUs. We averaged the run time for train-
ing and testing phases over multiple runs. On average the
training phase requires about 2 hours and 30 minutes to gen-
erate a download graph over a time window of 10 days and
train the classiﬁcation models. On a single day, we observe
around 21K to 22K nodes (ﬁles and URLs) and the total
time it takes to compute a classiﬁcation result for all items
is approximately 20 to 30 minutes which translates to about
0.08 seconds per node. Consequently, it takes 0.16 seconds
to label a download event. So Mastino is capable of pro-
viding on-line and almost instantaneous responses to clients’
requests. Thus, as mentioned in Section 2, the DIA agent
needs to quarantine the newly download ﬁle for only about
0.16 seconds (plus some negligible network communication
delay) to receive a response from the MDD. Considering that
the time it takes to actually download ﬁles is much larger
than 0.16 seconds, Mastino will not have a negative impact
on users’ experience.
5.6 Selection of Training Time Window T
In our experiments, we set T = 10 days based on our pi-
lot experiments with various time windows ranging from 1 to
30 days. In this section, we compare the detection result on
a same testing period using various T lengths. We demon-
strate that our chosen T = 10 days provides a good trade-oﬀ
between runtime eﬃciency and detection performance. Fig-
ure8 reports the results of a ﬁle train-test experiment over a
testing period for time windows of 1, 5, 10, 20, and 30 days.
As it can be seen, by increasing T length beyond 10 days, no
notable performance gain could be achieved. Furthermore,
as the length of time window increases, the training time
increases as well. On average, the training time for T of 1,
5, 10, 20, and 30 days is about 40, 70, 120, 240, and 370
minutes. Similar results apply for URLs, too.
5.7 Analysis of the Classiﬁcation Results
In this Section, ﬁrst, we discuss the results of train and
test experiment (Section 5.2). Next, we analyze the output
of our classiﬁer on completely unknown ﬁles, i.e. those ﬁles
for which no ground truth was available whatsoever at the
Figure 8: Selection of time window for ﬁles - train and test
using Fn with various T lengths
time of performing the train and test experiments. Finally,
we report a break down of Mastino’s detected malware
families when it was run in the wild.
5.7.1 Download events with Ground Truth
To perform this analysis, we picked a testing period of 5
days (see Section 5.2.2) and used a threshold of 0.5% FP
rate for classiﬁcation. In this experiment, 21 ﬁles are iden-
tiﬁed as false negatives (FN), i.e. labeled as benign by our
system, but according to AV labels should have been labeled
as malware. Out of these FNs, none were connected to any
malicious URLs (6 were connected to only benign URLs),
15 FNs did not use any type of code obfuscation techniques,
and 17 ﬁles were downloaded by machines that had a clean
history, i.e. they were not labeled as infected in the graph.
These observation to some extent justiﬁes the decision of the
classiﬁer on labeling these ﬁles as benign since the major-
ity of them were connected to non-malicious URLs in the
graph, or did not exhibit a malicious behavior (e.g., down-
loaded on machines with a clean history and not packed).
Further investigation revealed that some of FNs were “po-
tentially unwanted applications”, such as adware, bundled
with benign software.
Next, we analyze the FPs. Out of 3 FPs: 1 was down-
loaded by multiple malicious URLs, another was packed and
the last was downloaded on a machine with R > 0.7. So
even though according to AVs and our proprietary whilelist
of ﬁles, these samples were assigned a benign label, they
might very well be malware.
5.7.2 Completely Unknown Download Events
We now evaluate our system as deployed in an operational
environment, i.e. running on ﬁles that are unknown to AV
vendors at the time of performing the train and test exper-
iment. By evaluating which samples are correctly classiﬁed
prior to any detection by AVs, we show that Mastino can
label unknown ﬁles accurately and ahead of time.
To this end, we performed train and test and used a clas-
siﬁer’s threshold of 0.5% FP rate. Then we obtained the
most up-to-date labels from VirusTotal, i.e. corresponding
to six months after the testing period. In between the dates
that the experiment was performed and the date that Virus-
Total was queried again, a portion of the samples that were
previously unknown have got some labels. Out of 57,896 un-
known ﬁles on test day, 406 were later detected as malicious
and 2,774 identiﬁed as benign. Overall, Mastino correctly
classiﬁed 84% of future malware on the test day, when they
were still unknown to all AVs, and incurred 1.2% FPs.
0.00.20.40.60.81.0FP(percentage)020406080100TP(percentage)All_FeaturesNo_MachineNo_URLNo_Intrinsic0.00.20.40.60.81.0FP(percentage)020406080100TP(percentage)All_FeaturesNo_MachineNo_FileNo_Intrinsic0.00.20.40.60.81.0FP(percentage)020406080100TP(percentage)T=1T=5T=10T=20T=30791To understand why Mastino incurred FPs, we further
analyzed the characteristics of these ﬁles. Looking at the
download graph, 38% of FPs were connected to malicious
URLs, 94% were packed, and 50% of FPs were downloaded
by machines that had recently downloaded other malware
or contacted malicious URLs. These are suspicious signs for
the classiﬁer.
We also analyzed the FN results (64 out of 406 ﬁles). In
the graph, most of the FNs were in fact connected to either
benign or unknown URLs and only 12% of FNs were con-
nected to malicious URLs. Furthermore, 89% of the FNs,
were downloaded by machines with clean histories.
5.7.3 Break Down of Detected Malware Families
Here we report examples of malware families that Mastino
detected during our testing before any other trusted AVs.
Among these, a signiﬁcant amount is represented by ﬁrst
stage malware like downloaders and droppers, e.g. Win32/In
stallCore.MI, TrojanDropper:Win32/Rovnix, Downloader
.ATW and MalSign.InstallC.4DB. Note that downloaders
and droppers, in general, represent a signiﬁcant portion of
malware samples because they are often associated with the
ﬁrst code being downloaded and executed on infected ma-
chines from malware campaigns. Other families consist of
adware, bots, banking trojans (bankers) and key-loggers, in-
cluding, e.g., Rogue:Win32/FakePAV, Win32:Crypt-QTG, PWS:
Win32/Zbot, FakeAV_r.YE, Backdoor.Trojan, and Trojan
.FakeAV.
5.8 Case Studies
In Section 5.2.2, we presented the classiﬁcation results
of test download events for which none of the nodes were
present during the training time window and no ground
truth was available for the nodes. We showed that the classi-
ﬁer can detect the majority of such nodes with high accuracy.
Here we report a few case studies of those ﬁle nodes.
Case Study 1: E-mail Dropper. A ﬁle with the name of
file_saw.exe was observed on Feb. 12 and used as test sam-
ple in Fu because it was downloaded by machines and from
URLs for which no ground truth was available. However,
our classiﬁer successfully assigned a malicious label to this
ﬁle. Further analysis revealed that the sample was a down-
loader, usually distributed via spam emails and detected as
Win32/Trojan Downloader.Wauchos.A and Win32:Inject-
BGK [Trj].
By analyzing the ﬁle’s features, we conﬁrm that the fea-
tures related to the path patterns of the URLs helped the
classiﬁer in his successful talk 2. These features show an av-
erage reputation score of 0.72 (i.e., malicious), meaning that
multiple URLs with the same path pattern have oﬀered ma-
licious ﬁles. In total, 182 ﬁles were downloaded from 1,445
URLs with same path pattern, which mostly were malware.
Other contributing features are the low number of coun-
tries that downloaded the ﬁle (one in this case), and the low
prevalence of the downloaded ﬁles: about 85% of them had
a prevalence less than 2. Finally, the classiﬁed ﬁle did not
have a valid signer at the time of download.
Case Study 2: Somoto. Two ﬁles in Fu with similar ﬁle-
names (FreeZipSetup-[0-9a-zA-Z].exe) were downloaded
by two unknown machines from unknown URLs. The ﬁles
2
URLs
with
and
/f/1392240120/4165299987/2, and the path pattern /H1/D10/D10/D1
oﬀered the ﬁle.
/f/1392240240/1255385580/2
paths
were correctly labeled as malware by Mastino. Further in-
vestigation revealed that the ﬁles belong to an adware cam-
paign called Somoto. These ﬁles were packed, had short
lifetime and prevalence of zero. In addition, although the
related machines were labeled as unknown, according to the
download graph, one of them downloaded one malicious ﬁle
during T . Interestingly, during the same time window, 695
ﬁles with very similar names, features, and sizes3 were down-
loaded from several hundreds URLs. Mastino classiﬁed all
of them as malicious. By analyzing them, we conﬁrmed
that they are indeed part of the same malware campaign
and therefore correctly detected as malicious on early phase
by our system. However, on the day of experiment, only 79
were known by AVs. Six month later, 634 were reported as
bad by VirusTotal (the remaining 61 are still undetected).
Case Study 3: TTAWinCDM Spyware. This ﬁle was
classiﬁed as malware by Mastino on March 19 when no
prior AV information was available: The ﬁrst submission to
VirusTotal occurred over two months later. Even though
this ﬁle was identiﬁed on one machine with R = 0.5 and
downloaded from a single URL with R = 0.5 as well, it
was classiﬁed correctly. By our analysis, the feature that
contributed actively on the detection was a mismatch on
the downloading process.
In fact, while the downloading
process was identiﬁed as Acrobat (i.e., acrord32.exe), the
ﬁle was not downloaded from a URL hosted on the acrobat.
com domain. Other contributing factors were a very low
lifetime, prevalence and number of countries.
6. DISCUSSION AND LIMITATIONS
Mastino’s MDD should run on a server to receive re-
quests from DIAs on clients’ machines. One challenge here
is to keep the server up and running at all times, so it be-
comes a single point of failure. Also the communication
between DIAs and MDD might suﬀer from small network
communication latency. However, the idea of centric servers
nowadays is used by many systems commercially, which with
current network infrastructures could run with very high
availability rates. In addition, since MDD provides almost
instantaneous decisions (see Section 5.5), and considering
that network latency is negligible when compared to actual
download time, Mastino could transparently protect users.
Attackers might try to introduce noise to evade Mastino’s