in the statement we use a notion similar to XPath in HTML [36].
For example, the sequence of nodes from AssignmentExpression
to the identiﬁer node corresponding to y in Figure 2 uniquely de-
scribes the position of y in the statement y = x + 1. Thus, both
2A program is a collection of source elements which are either statements or function
declarations according to the rules (1) and (2) of Figure 1
Figure 3: Partial AST for the IfStatement present in the code of Listing 6.
The positions of the identiﬁer n are marked with green (p1) and red (p2).
S#(AssignmentExpression) and the position of y represented by
(AssignmentExpression → left → Identiﬁer) collectively describe
the usage of y. These are used to reﬁne the structural identity of y
as shown in the Equation 4. Similarly the structural identity of x is
reﬁned as shown in Equation 5.
StructId(y) = H(StructId(y)||s||H(posy))
(4)
StructId(x) = H(StructIdentity(x)||s||H(posx))
(5)
where s = S#(AssignmentExpression), posx and posy are
the positions of x and y respectively in the statement.
The positions of an identiﬁer are described as a sequence of
nodes starting at the source element or the child of the Program
node which is the node AssignmentExpression for the statement y
= x + 1;. This position can be recursively computed by passing
the position to the children while computing their structural signa-
tures as shown in lines (12) and (17) of Algorithm 1. The positions
of the identiﬁers are stored in the data structure IdentityPos.
A single identiﬁer can have more than one position in a source ele-
ment. For example, x = x + 5, there are two positions of x and
both are to be recorded as they both describe the usage of x. This is
done by making a list of positions and hence IdentityPos[x]
is a list of positions of x in the current statement. IdentityPos[x]
is updated according to line 9 in Algorithm 1.
Algorithm 2 Identity Reﬁnement Algorithm
1: procedure REFINE_STRUCTIDS(s)
2:
3:
4:
5:
6:
7:
for each ∈ IdentityP os.keys do
P athHash ← “”
for p ∈ IdentityP os[each] do
t ← s||P osHash
StructId[each] = H(StructId[each]||t)
P osHash = P osHash||H(p)
The StructId of the identiﬁers are updated at the end of the
signature computation of the source element node (a child of the
Program node). This is described in the lines 22-24 of Algorithm 1.
The algorithm for reﬁning the structural identities is shown in Al-
gorithm 2. At the end of the signature computation of a source el-
ement, the data structure IdentityPos is ﬂushed as the current
positions have already reﬁned the structural identity of the identi-
ﬁers.
Similarly, if the source element were an IfStatement with an
identiﬁer involved in both the "if-block" and the "else-block" as
shown in Listing 6, the signature computation proceeds as before.
The IdentityPos[n] will contain two positions of n namely
AlternateTestConsequentIfStatementAssignmentExpressionrightleftAssignmentOperatornIdentifierAssignmentExpressionrightleftAssignmentOperatornumberIdentifierAssignmentOperatorn...............p1p215491 var b = generate_random_number(0,1), n;
2
3 if(b==0)
4
5 else
6
n = "even"
n = "odd"
Listing 6: An example of a script consisting of IfStatement
p1 and p2 as shown in the Figure 3. We therefore merge both the
positions at the join point, i.e., after the end of the ifStatement.
The structural identity of n is updated by concatenating the hash of
the positions to the structural identity as shown in Equation 6.
StructId(n) = H(StructId(n)||s||H(p1)||H(p2))
(6)
where s = S#(IfStatement) and p1 and p2 are the positions of n
in the IfStatement.
Safety. Using the type, initialization value and the position set in
the AST, we obtain unique structural identities of the identiﬁers.
However, if two identiﬁers have the same structural identity then
they will conﬂict with each other. Replacing such identiﬁers by
their structural identities could open avenues for the attacker to re-
place one by the other and achieve script injection. Therefore, if
such a case happens, we retain the names of the identiﬁers and do
not replace them with their structural identities. With this restric-
tion, we compute the structural signature for the script again using
Algorithm 1. Such cases of conﬂicting identities did not come up
in our large scale evaluation as structural identities of every identi-
ﬁer in the script were unique. Such restriction is, however, required
to be enforced in order to guarantee that two scripts with different
semantics are not treated as one signature.
3.5.4 Collision-Resistance
The structural signatures generation that follows the mechanism
explained in the previous steps will lead to the same structural sig-
natures for two scripts that are equivalent. However, no two struc-
turally non-equivalent scripts must lead to the same structural sig-
natures. This means our mechanism must not lead to false nega-
tives. Since the underlying idea of structural signatures is inspired
from Merkle Hash Trees, we borrow the collision resistance prop-
erty of our signatures from the collision resistance of the top level
root signature of Merkle Hash Trees.
Thus, structural signatures establish the deﬁnition of structural
equivalence formulated in terms of AST isomorphism. This implies
that it is very difﬁcult for the attacker to introduce a new script that
is structurally different from a script in the whitelist, while at the
same time has the same structural signature with the script in the
whitelist — this answers our RQ4.
4. ARCHITECTURE
In this section, we discuss SICILIAN, a solution to implement
JavaScript whitelisting based on multi-layered signature schemes.
We detail how our whitelisting mechanism works, specify the white-
listing policies, as well as techniques for constructing such white-
list. Finally, we outline the deployment scenario of our approach.
4.1 Browser Modiﬁcation
We explain the implementation detail of SICILIAN with respect
to a browser ﬂow model discussed in [56] (illustrated in Figure 4)
and Chromium design, although our approach is also applicable to
other mainstream browsers. Our whitelisting logic must guaran-
tee that only the authorized scripts (scripts with legitimate signa-
ture) are allowed to enter the browser’s JavaScript parser module,
as mentioned in rule 1 in Section 3.2. Therefore, our whitelisting
module must be placed in a location where it is exposed to all the
Figure 4: Deployment point of our whitelist logic. Our whitelisting module
(illustrated as a greyed box S) is a standalone program which validates any
scripts entering browser’s JavaScript Parser.
interfaces JavaScript parser has with client-side web components
like HTML parser and CSS parser. In Chromium, the right point to
place our whitelisting logic is in a function executeScript() of
class ScriptLoader.
Next, our solution employs multi-layered approach when val-
idating the scripts against the whitelist. First, all the scripts are
checked against the whitelist (Figure 4 point ) using raw signa-
tures. Scripts that pass the ﬁrst layer of validation will then be
allowed by the browser to execute. Otherwise, the browser carries
out the second layer of validation, which computes the signature of
the remaining scripts with structural signatures. Scripts that do not
pass this validation are blocked by the browser.
Finally, our whitelisting module is not built by modifying the im-
plementation of the browser’s JS parser. Rather, we build our so-
lution as a standalone module hooked into the browser’s JavaScript
parser, shown as greyed box in Figure 4. To compute the struc-
tural signature of a script, SICILIAN needs to parse it and perform
a tree traversal. Therefore, we equip the whitelisting module with
our own parser written in JavaScript, modiﬁed from Esprima [24].
Since our parser is written in JavaScript, the script to be validated
and our parser logic are processed by Chromium’s JavaScript En-
gine via a function call Script:Compile(). The reason for not
using browser’s JavaScript parser is that the parsing logic varies
across browsers [1, 9]. This may lead to inconsistent signatures
across browsers. So, we champion a browser-agnostic approach3.
Although our approach requires modiﬁcation to the browser —
as with many other defenses like HTTP Strict Transport Security [37],
CSP [52], or W3C’s Subresource Integrity [7] — it requires no
proactive action at the end-user since minor patches for SICILIAN
can be delivered transparently via browser auto-updates.
4.2 Whitelist Policy Speciﬁcation
Our structural signature comes with a fail-safe policy that is com-
posed of directives that manage how SICILIAN is imposed on a
script. After applying such directives, the browser will block a
script from running if its signature is not in the whitelist. All poli-
cies are deﬁned at a web page granularity such that they remain
applicable throughout the execution of the webpage. Here, we de-
ﬁne a web page with respect to a URL of a page including the origin
and its subpath (i.e., anything before the ’?’ character in the URL).
We discuss the construction of such whitelist in Section 4.3.
Signature Directive. In our policy deﬁnition, the script’s signature
is speciﬁed by a directive signature. This directive is speciﬁed
by a JSON object containing four properties, namely type which
speciﬁes the signature type to be imposed on the script, id which
is the identiﬁer of a script (e.g., script’s URL), value which em-
3Browser-agnostic here means that the module is agnostic to a particular browser’s
parsing logic and JavaScript engine
Content type dispatch HTML Parser text/html JS Parser CSS Parser text/css text/javascript S JS Runtime eval()  JS expr. DOM DOM API 1 Whitelist 1550Figure 5: High-level overview of whitelist construction. Our technique comprises of three phases namely INIT, CRAWL, and LOCK.
phasizes the expected hash value of the script, and policy which
speciﬁes certain JavaScript literals which will be ignored during
structural signature computation — this is speciﬁed in the data di-
rective below. The browser validates the script by computing either
raw signature if the type is raw-sign or structural signature if
the type is struct-sign.
Data Directive. This directive speciﬁes the data that keeps chang-
ing in the script. Such data will be ignored by our structural sig-
nature computation. The changing data is speciﬁed by a directive
dynamic-data which is a JSON object containing two proper-
ties, namely name which denotes the identiﬁer of the data (e.g.,
variable name) and data_loc which uniquely identiﬁes the rela-
tive scope chain of the identiﬁer. This directive can either be served
as annotations by the website admin or inferred automatically by
the browser (during the crawl phase in Section 4.3)
Example Policies. We provide examples of policies according to
the real scripts we encountered in Alexa’s top sites.
Example 1. Site want to employ raw signature on a script hosted
at somewhere.com/site.js
1 --signature:{type:’raw-sign’,
id:’something.com/site.js’,
value:’aLp+5608ed+rwmLOHboV==’}
Example 2. Site has a script hosted at third-party site somewhere.
com/site.js which has a variable which value keeps changing.
The variable’s name is ’bar’ inside a function name ’foo’. The
website owner wants to sign this script with structural signature.
1 --signature:{type:’struct-sign’,
id:’something.com/site.js’,
value:’aLp+5608ed+rwmLOHboV==’,
policy:{dynamic-data:{name:’bar’,
data_loc:root-foo}}}
Our approach requires the web page to whitelist all scripts that
are going to be loaded and executed. However, this raises deploy-
ability issues since web application can generate a massive amount
of web pages. Crawling such pages during pre-deployment analysis
may not always lead to complete code coverage.
4.3 Deployment: Progressive Lockdown
We propose a browser-assisted whitelist construction method that
helps the website owner to specify a correct whitelist for web pages
which do not yet have it. Our entire whitelist construction is based
on progressive lockdown, which slowly introduces whitelists as
new web pages are being discovered by the browser. We intro-
duce three important whitelist construction phases, namely 1) INIT
where the developer carries out a pre-deployment phase to compile
initial whitelists for a limited set of web pages, say all web pages at
depth of two from the landing page, 2) CRAWL where the browser
assists the website in building a whitelist for web pages that have
not been visited during the pre-deployment, and 3) LOCK to switch
the website into a full-ﬂedged whitelisting-based mode. Note that
this is an opt-in approach where the website owner can build such
whitelist without any signiﬁcant changes to the server-side code.
We illustrate our whitelist construction technique in Figure 5.
INIT Phase. During the pre-deployment phase, the developer car-
ries out INIT to sign its web pages. The goal is to create an initial
whitelist and manually add policies in the whitelist. Such manual
policy conﬁguration includes adding dynamic-data directives
to specify which data keeps changing in a script. This step is nec-
essary to whitelist scripts in web pages that are likely to be visited
by the user. As a result, such pages are “guarded” from potential
malicious script injection.
CRAWL Phase. After the INIT phase, the website can opt for a
second step, that is the CRAWL phase. CRAWL is carried out
at the deployment phase with the help of client’s web browser to
compile a whitelist for web pages that have not been visited dur-
ing INIT. Our crawling mechanism is based on trust-on-ﬁrst-use
(TOFU), meaning that the ﬁrst time a browser sees a script out of a
whitelist, it locally compiles a whitelist for the script and sends it to
the whitelist database. Although TOFU is applicable for the entire
CRAWL phase, the browser is not going to trust any new scripts on
the second visit to the script. Additionally, the server can himself
continue the INIT phase to populate the whitelists.
We apply our multi-layered solution described in Section 3 to
construct such whitelist. First, if the browser does not encounter
any changes during multiple visits on the same scripts, the browser
will set such scripts as static and use the raw signature (Layer 1 in
Section 3). Otherwise, the browser will resort to structural signa-
tures (Layer 2). While compiling the whitelist, the browser records
the data that changes in the scripts to suggest additional policies re-
garding the data-only changes to the website admin. Note that this
can be done as a background job without delaying page load.
A typical CRAWL phase may vary, but may last up a month in
order to get a precise and ﬁne-grained policy. This is based on our
observation that the majority of non-static scripts that we study had
already been changed within 1 month interval. Once a complete
whitelist is constructed for all scripts on a web page, the browser
then sends a tuple of page’s URL and its whitelist to a database
owned by the website via an out-of-band channel.
LOCK Phase. Once the whitelist database is sufﬁciently popu-
lated, the website owner can initiate the LOCK phase. The whitelists
returned by the browsers in the CRAWL phase may conﬂict with
each other. The server can resolve such conﬂicts and decide the
ﬁnal whitelist database through manual intervention. However,
we expect the majority of the scripts to be whitelisted automati-
cally without conﬂict since they are mostly static scripts (see Sec-
tion 5). This ﬁnal whitelist is thus stored on the server who is
INIT CRAWL LOCK url1.com/path1/ url1.com/path4/ url1.com/path5/ url1.com/path5/ Whitelist DB Whitelist  DB Website’s  Crawler Pre-deployment Deployment url1.com/path2/ Client Browsers Website’s  Crawler Client Browsers 1551trusted for its integrity. Once the ﬁnal whitelist is ready, brow-
sers must strictly follow the whitelist provided by the server — all
the partial whitelists compiled on the browsers during the CRAWL
phase must be ﬂushed. For each browser’s request to a web page,
the web server will query its database and attach the correspond-
ing whitelist for the web page during the HTTP response. In the
LOCK phase, the whitelist is communicated to the browsers using
a custom HTTP header X-Whitelist:[directives]. If there
are any changes to the server code post-lockdown, the server can
update the whitelist database itself and serve the updated whitelists
for future visits.
4.4 Signature Updates
We recall that 461 scripts in our crawl belong to C3A, where
website developers add or remove functionality from the scripts and
this happens in an infrequent manner. Scripts within this category
undergo a non-syntactic change and no longer preserve the code
semantics. Once this happens, signature mismatch errors will be
raised on all browsers loading these scripts. Since the changes are
legitimate and infrequent, the website admin can do the following
to handle such updates:
1. If the script is hosted within a website, website admin can re-
compute the signature of the script and update the value in the
whitelist. This way, browser that accesses a web page will get
the updated whitelist. Signature mismatch can only occur when
the browser still retains the old signature of the script. In such
cases, the browser can simply communicate back to the server
to get an updated signature.
2. If the script is imported from some third-party services, the
browser will report such incidents back to the main website’s
server. The admin of such site can either analyze the script and