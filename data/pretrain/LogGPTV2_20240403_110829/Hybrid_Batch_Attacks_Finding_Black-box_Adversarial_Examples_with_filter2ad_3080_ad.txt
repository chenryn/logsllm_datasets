Universal Local Ensemble. The results above validate our
hypothesis that the diﬀerent attack surfaces of robust and
normal models cause the ineﬀectiveness against the robust
CIFAR10 model in Table 3. Therefore, to achieve better per-
formance, depending on the target model type, the attacker
should selectively choose the local models. However, in prac-
tice, attackers may not know if the target model is robustly
trained, so cannot predetermine the best local models. We
5We did not repeat the experiments with robust MNIST local models
because, without worrying about separately training robust local models, we
can simply improve the attack performance signiﬁcantly by tuning the local
models during the hybrid attack process (see Table 6 in Section 4.6). The
tuning process transforms the normal local models into more robust ones
(details in Section 4.6).
1334    29th USENIX Security Symposium
USENIX Association
Figure 1: Transfer rates of diﬀerent local ensembles. The Normal-3 ensemble is composed of the three normal models, NA, NB,
and NC; the Robust-2 ensemble is composed of R-DenseNet and R-ResNet. The All-5 is composed of all of the 5 local models.
Transfer rate is measured on independently sampled test images and is averaged over 5 runs.
next explore if a universal local model ensemble exists that
works well for both normal and robust target models.
To look for the best local ensemble, we tried all 31 diﬀerent
combinations of the 5 local models (3 normal and 2 robust)
and measured their corresponding direct transfer rates against
both normal and robust target models. Figure 1 reports the
transfer rates for each local ensemble against both normal
and robust target models. For clarity in presentation, we only
include results for the ﬁve individual models and four rep-
resentative ensembles in the ﬁgure: an ensemble of NA-NB
(selected to represent the mediocre case); Robust-2, an en-
semble of the two robust models (R-DenseNet and R-ResNet);
Normal-3, an ensemble of three normal models (NA, NB, and
NC); and All-5, an ensemble of all ﬁve local models. These
include the ensembles that have the highest or lowest trans-
fer rates to the target models and transfer rates of all other
ensembles ﬁt between the reported highest and lowest values.
None of the ensembles we tested had high direct transfer
rates against both normal and robust target models. Ensem-
bles with good performance against robust targets have poor
performance against normal targets (e.g., Robust-2 has 37.8%
transfer rate to robust target, but 18.1% to normal target), and
ensembles that have good performance against normal targets
are bad against robust targets (e.g., Normal-3 has 65.6% trans-
fer rate to the normal target, but only 9.4% to the robust target).
Some ensembles are mediocre against both (e.g., NA-NB).
One possible reason for the failure of ensembles to apply
to both types of target, is that when white-box attacks are
applied on the mixed ensembles, the attacks still “focus” on
the normal models as normal models are easier to attack (i.e.,
to signiﬁcantly decrease the loss function). Biasing towards
normal models makes the candidate adversarial example less
likely to transfer to a robust target model. This conjecture is
supported by the observation that although the mixtures of
normal and robust models mostly fail against robust target
models, they still have reasonable transfer rates to normal
target models (e.g., ensemble of 5 local models has 63.5%
transfer rate to normal CIFAR10 target model while only
9.5% transfer rate to the robust target model). It might be
interesting to explore if one can explicitly enforce the attack
to focus more on the robust model when attacking the mixture
of normal and robust models.
In practice, attackers can dynamically adapt their local
ensemble based on observed results, trying diﬀerent local en-
sembles against a particular target for the ﬁrst set of attempts
and measuring their transfer rate, and then selecting the one
that worked best for future attacks. This simulation process
adds overhead and complexity to the attack, but may still be
worthwhile when the transfer success rates vary so much for
diﬀerent local ensembles.
For our subsequent experiments on CIFAR10 models, we
use the Normal-3 and Robust-2 ensembles as these give the
highest transfer rates to normal and robust target models.
4.6 Local Model Tuning
To test the hypothesis that the labels learned from optimization
attacks can be used to tune local models, we measure the
impact of tuning on the local models’ transfer rate.
During black-box gradient attacks, there are two diﬀerent
types of input-label pairs generated. One type is produced
by adding small magnitudes of random noise to the current
image to estimate target model gradients. The other type is
generated by perturbing the current image in the direction of
estimated gradients. We only use the latter input-label pairs
as they contain richer information about the target model
boundary since the perturbed image moves towards the de-
cision boundary. These by-products of the black-box attack
search can be used to retrain the local models (line 15 in Al-
gorithm 1). The newly generated image and label pairs are
USENIX Association
29th USENIX Security Symposium    1335
Model
MNIST (N, t)
MNIST (R, u)
CIFAR10 (N, t)
CIFAR10 (R, u)
NES
NES
AutoZOOM
Gradient Attack Transfer Rate (%)
Tuned
64.4
AutoZOOM
77.9
4.3
4.5
8.6
33.4
8.8
9.3
Static
60.6
60.6
3.4
3.4
65.6
65.6
9.4
9.4
AutoZOOM
AutoZOOM
NES
NES
Table 5: Impact of tuning local models on transfer rates (Base-
line + Hypothesis 2): gradient attacks start from original
seed. Transfer rate is measured on independently sampled
test images and is averaged over 5 runs. The results for (N, t)
are targeted attacks on normal models; (R, u) are untargeted
attacks on robust models.
added to the original training set to form the new training
set, and the local models are ﬁne-tuned on the new training
set. As more images are attacked, the training set size can
quickly explode. To avoid this, when the size of new training
set exceeds a certain threshold c, we randomly sample c of the
training data and conduct ﬁne-tuning using the sampled train-
ing set. For MNIST and CIFAR10, we set the threshold c as
the standard training data size (60,000 for MNIST and 50,000
for CIFAR10). At the beginning of hybrid attack, the training
set consists of the original seeds available to the attacker with
their ground-truth labels (i.e., 1,000 seeds for MNIST and
CIFAR10 shown in Section 4.2).
Algorithm 1 shows the local model being updated after ev-
ery seed, but considering the computational cost required for
tuning, we only update the model periodically. For MNIST,
we update the model after every 50 seeds; for CIFAR10, we
update after 100 seeds (we were not able to conduct the tuning
experiments for the ImageNet models because of the high cost
of each attack and of retraining). To check the transferability
of the tuned local models, we independently sample 100 un-
seen images from each of the 10 classes, use the local model
ensemble to ﬁnd candidate adversarial examples, and test the
candidate adversarial examples on the black-box target model
to measure the transfer rate.
We ﬁrst test whether the local model can be ﬁne-tuned by
the label by-products of baseline gradient attacks (Baseline
attack + H2) by checking the transfer rate of local models
before and after the ﬁne-tuning process. We then test whether
attack eﬃciency of hybrid attack can be boosted by ﬁne-tuning
local models during the attack process (Baseline attack + H1
+ H2) by reporting their average query cost and attack success
rate. The ﬁrst experiment helps us to check applicability of
H2 without worrying about possible interactions between H2
with other hypotheses. The second experiment evaluates how
much attackers can beneﬁt from ﬁne-tuning the local models
in combination with hybrid attacks.
We report the results of the ﬁrst experiment in Table 5. For
the MNIST model, we observe increases in the transfer rate
of local models by ﬁne-tuning using the byproducts of both
attack methods—the transfer rate increases from 60.6% to
77.9% for NES, and from 60.6% to 64.4% for AutoZOOM.
Even against the robust MNIST models, the transfer rate im-
proves from the initial value of 3.4% to 4.3% (AutoZOOM)
and 4.5% (NES). However, for CIFAR10 dataset, we observe a
signiﬁcant decrease in transfer rate. For the normal CIFAR10
target model, the original transfer rate is as high as 65.6%,
but with ﬁne-tuning, the transfer rate decrease signiﬁcantly
(decreased to 8.6% and 33.4% for AutoZOOM and NES re-
spectively). A similar trend is also observed for the robust
CIFAR10 target model. These results suggest that the exam-
ples used in the attacks are less useful as training examples
for the CIFAR10 model than the original training set.
Our second experiment, reported in Table 6, combines the
model tuning with the hybrid attack. Through our experi-
ments, we observe that for MNIST models, the transfer rate
also increases signiﬁcantly by ﬁne-tuning the local models.
For the MNIST normal models, the (targeted) transfer rate
increases from the original 60.6% to 74.7% and 76.9% for
AutoZOOM and NES, respectively. The improved transfer
rate is also higher than the results reported in ﬁrst experiment.
For the AutoZOOM attack, in the ﬁrst experiment, the trans-
fer rate can only be improved from 60.6% to 64.4% while in
the second experiment, it is improved from 60.6% to 76.9%.
Therefore, there might be some boosting eﬀects by taking lo-
cal AEs as starting points for gradient attacks. For the Madry
robust model on MNIST, the low (untargeted) transfer rate
improves by a relatively large amount, from the original 3.4%
to 5.1% for AutoZOOM and 4.8% for NES (still a low trans-
fer rate, but a 41% relative improvement over the original
local model). The local models become more robust during
the ﬁne-tuning process. For example, with the NES attack,
the local model attack success rate (attack success is deﬁned
as compromising all the local models) decreases signiﬁcantly
from the original 96.6% to 25.2%, which indicates the tuned
local models are more resistant to the PGD attack. The im-
provements in transferability, obtained as a free by-product of
the gradient attack, also lead to substantial cost reductions for
the attack on MNIST, as seen in Table 6. For example, for the
AutoZOOM attack on the MNIST normal model, the mean
query cost is reduced by 31%, from 282 to 194 and the attack
success rate is also increased slightly, from 98.9% for static
local models to 99.5% for tuned local models. We observe
similar patterns for robust MNIST model and demonstrate
that Hypothesis 2 also holds on the MNIST dataset.
However, for CIFAR10, we still ﬁnd no beneﬁts from the
tuning. Indeed, the transfer rate decreases, reducing both the
attack success rate and increasing its mean query cost (Ta-
ble 6). We do not have a clear understanding of the reasons
the CIFAR10 tuning fails, but speculate it is related to the
diﬃculty of training CIFAR10 models. The results returned
1336    29th USENIX Security Symposium
USENIX Association
Model
MNIST Normal (T) AutoZOOM
Gradient
Attack
Queries/AE
Static
282
1,000
AutoZOOM 49,776
NES
Tuned
194
671
42,755
69,275 51,429
459
427
2,564
7,303
Success Rate (%) Transfer Rate (%)
Tuned
Static
74.7
98.9
76.9
89.2
5.1
7.5
4.8
5.5
98.2
19.7
99.8
40.7
65.3
10.1
10.7
38.0
Tuned
99.5
92.2
8.6
7.3
96.3
99.6
64.9
37.6
Static
60.6
60.6
3.4
3.4
65.6
65.6
9.4
9.4
NES
MNIST Robust (U)
276
CIFAR10 Normal (T) AutoZOOM
340
CIFAR10 Robust (U) AutoZOOM 2,532
7,317
NES
NES
Table 6: Impact of tuning local models (averaged 5 times). Transfer rate is measured on independently sampled test images.
from gradient-based attacks are highly similar to a particular
seed and may not be diverse enough to train eﬀective local
models. This is consistent with Carlini et al.’s ﬁndings that
MNIST models tend to learn well from outliers (e.g., unnat-
ural images) whereas more realistic datasets like CIFAR10
tend to learn well from more prototypical (e.g., natural) exam-
ples [7]. Therefore, ﬁne-tuning CIFAR10 models using label
by-products, which are more likely to be outliers, may dimin-
ish learning eﬀectiveness. Potential solutions to this problem
include tuning the local model with mixture of normal seeds
and attack by-products. One may also consider keeping some
fraction of model ensembles ﬁxed during the ﬁne-tuning pro-
cess such that when by-products mislead the tuning process,
these ﬁxed models can mitigate the problem. We leave further
exploration of this for future work.
5 Batch Attacks
Section 4 evaluates attacks assuming an attacker wants to
attack every seed from some ﬁxed set of initial seeds. In more
realistic attack scenarios, each query to the model has some
cost or risk to the attacker, and the attacker’s goal is to ﬁnd as
many adversarial examples as possible using a limited total
number of queries. Carlini et al. show that, defenders can iden-
tify purposeful queries for adversarial examples based on past
queries and therefore, detection risk will increase signiﬁcantly
when many queries are made [11]. We call these attack sce-
narios batch attacks. To be eﬃcient in these resource-limited
settings, attackers should prioritize “easy-to-attack” seeds.
A seed prioritization strategy can easily be incorporated
into the hybrid attack algorithm by deﬁning the selectSeed
function used in step 6 in Algorithm 1 to return the most
promising seed:
argmin
x∈X
EstimatedAttackCost(x, F).