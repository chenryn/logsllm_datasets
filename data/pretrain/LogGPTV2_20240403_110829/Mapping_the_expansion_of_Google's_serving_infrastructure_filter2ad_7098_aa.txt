title:Mapping the expansion of Google's serving infrastructure
author:Matt Calder and
Xun Fan and
Zi Hu and
Ethan Katz-Bassett and
John S. Heidemann and
Ramesh Govindan
Mapping the Expansion of Google’s Serving Infrastructure∗
Technical Report 13-935, University of Southern California, Department of Computer Science
Matt Calder1, Xun Fan2, Zi Hu2, Ethan Katz-Bassett1, John Heidemann2, and Ramesh Govindan1
1University of Southern California — 2USC/ISI
ABSTRACT
Modern content-distribution networks both provide bulk con-
tent and act as “serving infrastructure” for web services in
order to reduce user-perceived latency. These serving in-
frastructures (such as Google’s) are now critical to the on-
line economy, making it imperative to understand their size,
geographic distribution, and growth strategies. To this end,
we develop techniques that enumerate servers in these in-
frastructures, ﬁnd their geographic location, and identify the
association between clients and servers. While general tech-
niques for server enumeration and geolocation can exhibit
large error, our techniques exploit the design and mecha-
nisms of serving infrastructure to improve accuracy. We
use the EDNS-client-subnet extension to DNS to measure
which clients a service maps to which of its servers. We de-
vise a novel technique that uses this mapping to geolocate
servers by combining noisy information about client loca-
tions with speed-of-light constraints. We demonstrate that
this technique substantially improves geolocation accurate
relative to existing approaches. We also cluster servers into
physical sites by measuring RTTs and adapting the cluster
thresholds dynamically. Google’s serving infrastructure has
grown dramatically in the last six months, and we use our
methods to chart its growth and understand its content serv-
ing strategy. We ﬁnd that Google has almost doubled in size,
and that most of the growth has occurred by placing servers
in large and small ISPs across the world, not by expanding
on Google’s backbone.
1.
INTRODUCTION
∗Xun Fan, Zi Hu, and John Heidemann are partially sup-
ported by the U.S. Department of Homeland Security Sci-
ence and Technology Directorate, Cyber Security Division,
via SPAWAR Systems Center Paciﬁc under Contract No.
N66001-13-C-3001. John Heidemann is also partially sup-
ported by DHS BAA 11-01-RIKA and Air Force Research
Laboratory, Information Directorate under agreement num-
ber FA8750-12-2-0344. The U.S. Government is authorized
to reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright notation thereon. Any
opinions, ﬁndings and conclusions or recommendations ex-
pressed in this material are those of the author(s) and do
not necessarily reﬂect the views of SSC-Paciﬁc.
1
Internet traﬃc has changed considerably in recent
years, as access to content is increasingly governed by
web serving infrastructures. These consist of decentral-
ized serving sites that contain one or more frontend
servers. Clients of these infrastructures are directed to
nearby frontends, which either directly serve content
(e.g., as in a content distribution network like Akamai),
or use split TCP connections to relay web acccess re-
quests to back-end data centers (e.g., as in Google’s
serving infrastructure).
Serving infrastructures are motivated by the desire
to optimize user-perceived latency [28]. Web service
providers invest heavily in building out these infras-
tructures and they also develop sophisticated mapping
algorithms to direct clients to nearby servers.
In re-
cent months, as we discuss later, Google’s serving in-
frastructure has nearly doubled in size. Given the in-
creasing economic importance of these serving infras-
tructures, we believe it is imperative to understand the
content serving strategies adopted by large web service
providers, especially Google. Speciﬁcally, we are inter-
ested in the geographic and topological scope of serving
infrastructures, their expansion, and how client popu-
lations impact build-out of the serving infrastructure.
Several prior studies have explored static snapshots
of content-distribution networks [12, 2, 23], often focus-
ing on bulk content delivery infrastructures [12], new
mapping methodology [2], or new DNS selection meth-
ods [23]. In contrast, our work focuses on the broader
class of web serving infrastructures, develops more ac-
curate methods to enumerate and locate frontends and
serving sites, and explores how one infrastructure, Google’s,
grows over six months of active buildout.
The ﬁrst contribution of this paper is a suite of meth-
ods to enumerate frontends, geolocate them, and cluster
them into serving sites. Our methods exploit mecha-
nisms used by serving infrastructures to optimize client-
perceived latency. To enumerate servers, we use the
EDNS-client-subnet preﬁx extension [8] that some serv-
ing infrastructures, including Google, use to more ac-
curately direct clients to nearby servers. Our novel ge-
olocation technique, which we show to be substantially
more accurate than previously proposed approaches, ex-
ploits the fact that serving infrastructures employ so-
phisticated mapping strategies that determine the fron-
tend or serving site nearest to clients. Our technique,
called client-centric geolocation (CCG), geolocates a server
by taking the geographic mean of the (possibly noisy)
locations for clients associated with that server, while
ﬁltering out clients with bad location information using
speed-of-light constraints. We also cluster servers into
serving sites, adding dynamic thresholding and RTT-
based ﬁngerprinting to current methods. These changes
provide enough resolution to distinguish diﬀerent sites
in the same city. These sites represent unique network
locations, a view that IP addresses, preﬁxes, or ASes
can obscure.
Our second major contribution is a detailed study of
Google’s web serving infrastructure, and its recent ex-
pansion over the last six months. To our knowledge,
we are the ﬁrst to observe rapid growth of the serv-
ing infrastructure of a major content provider. We ﬁnd
that Google’s serving infrastructure has almost doubled
in the number of frontend IP addresses, has grown out
to 62 countries, with serving sites deployed in 87 new
ASes. Its recent growth strategy has been to move away
from serving clients oﬀ its own backbone and towards
serving from lower tiers in the AS hierarchy; the num-
ber of /24 preﬁxes served oﬀ Google’s network nearly
doubled during the expansion. Furthermore, these new
serving sites, predictably, have narrow customer cones,
serving only the customers of the AS the site is deployed
in. Finally, we ﬁnd that the expansion has noticeably
shifted the distribution of geographic distances from the
client to its nearest front-end server, and that this shift
can also reduce the error in geolocating frontends using
client locations alone, but not enough to obviate the
need for CCG’s ﬁltering techniques.
2. BACKGROUND
CDNs and Serving Intrastructures. Adding even
a few hundreds of milliseconds to a webpage load time
can cost service providers users and business [30, 17], so
providers seek to optimize their web serving infrastruc-
ture to deliver content quickly to clients. Whereas once
a website might have been served from a single location
to clients around the world, today’s major services rely
on much more complicated and distributed infrastruc-
ture. Providers replicate their services at serving sites
around the world and try to serve a client from the clos-
est one [15]. Content delivery networks (CDNs) initially
sped delivery by caching static content and some forms
of dynamic content within or near client networks.
Today, providers use this type of distributed infras-
tructure to speed the delivery of dynamic personalized
content and responses to queries. To do so, providers
direct clients to serving sites in or near the clients’ net-
works. A client’s TCP connection terminates at a fron-
tend server in the serving site, but the frontend proxies
the request back to one of the provider’s large data-
centers [25]. This arrangement has a number of po-
tential advantages versus directing the client directly
to the datacenter. For example, the client’s latency
to the frontend is less than the client’s latency to the
data center, allowing TCP to recover faster after loss,
the primary cause of suboptimal performance. More-
over, the frontend can multiplex many clients into a
high throughput connection to the datacenter.
In these types of serving infrastructures, diﬀerent classes
of serving sites may serve diﬀerent clients. First, of
course, the provider may still serve clients near a data-
center directly from that datacenter. Second, if a client
network hosts a serving site, it will generally only allow
its clients (or the clients of one of its customer net-
works) to use frontend servers in its site, not clients of
its providers or peers.
DNS-based Redirection.
Serving infrastructures
use the Domain Name System (DNS) to direct clients
to appropriate serving sites and frontend servers. When
a client queries DNS to resolve a name associated with a
service, the service returns an IP address for a frontend
it believes is near the client. Traditionally, at resolution
time, however, the service only knows the IP address of
the client’s resolver and not of the client itself, leading
to two main complications. The resolver may be far
from the clients it serves, and so the server closest to
the resolver may not be a good choice for the client.
Existing techniques can allow many services to discover
which clients use a particular resolver [20], enabling ser-
vices to direct a resolver based on the clients that use
it. However, these techniques are of little beneﬁt if the
same resolver serves clients that are far from each other–
there is no server that the service can return that will
be a good choice for all clients who may have issued the
request through the resolver.
To overcome this hurdle and provide quality DNS
redirections for clients, a number of Internet providers
and CDNs proposed EDNS-client-subnet [8]. EDNS is
an IETF speciﬁcation designed to overcome parameter
size restrictions in standard DNS. EDNS-client-subnet
is an experimental extension to EDNS that allows a
client to embed a portion of its IP address in the request
which will travel to an authoritative name server. By
including the client IP preﬁx in the request, the exten-
sion allows a service to direct the client to the optimal
server directly, without having to infer which client is
behind a request from a recursive resolver.
3. GOAL AND APPROACH
Our goal is to understand content serving strategies
for large IPv4-based serving infrastructures, especially
that of Google. Serving strategies are deﬁned by how
2
many serving sites and frontend servers a serving in-
frastructure has, where the serving sites are located ge-
ographically and topologically (i.e., within which ISP),
and which clients access which serving sites. Further-
more, services continuously evolve serving strategies, so
we are also interested in measuring the evolution of serv-
ing infrastructures. Of these, Google’s serving infras-
tructure is arguably one of the most important, so we
devote signiﬁcant attention to this infrastructure.
To this end, we develop novel measurement methods
to enumerate frontend servers, geolocate serving sites,
and cluster frontend servers into serving sites. The chal-
lenge in devising these measurement methods is that
serving infrastructures are large, distributed entities,
with thousands of frontend servers at hundreds of serv-
ing sites spread across dozens of countries. A brute
force approach to enumerating serving sites would re-
quire perspectives from a very large number of topo-
logical locations in the Internet, much larger than the
geographic distribution provided by research measure-
ment infrastructures like PlanetLab. Moreover, exist-
ing geolocation methods that rely on DNS naming or
geolocation databases do not work well on these serv-
ing infrastructures where location-based DNS naming
conventions are not consistently employed.
While our measurement methods use these research
infrastructures for some of their steps, the key insight
in the design of the methods is to leverage mechanisms
used by serving infrastructures to serve content. Be-
cause we design them for serving infrastructures, these
mechanisms can enumerate and geolocate serving sites
more accurately than existing approaches, as we discuss
below.
Our method to enumerate all frontend server IP ad-
dresses within the serving infrastructure uses the EDNS-
client-subnet extension. As discussed in Section 2, Google
(and some other serving infrastructures) use this ex-
tension to address the problem of geographically dis-
tributed clients using a resolver that prevents the serv-
ing infrastructure from optimally directing clients to
frontends. We use this extension to enumerate frontend
IP addresses of a serving infrastructure from a single lo-
cation: this extension can emulate DNS requests com-
ing from every active preﬁx in the IP address space,
eﬀectively providing a very large set of vantage points
for enumerating frontend IP addresses.
To geolocate frontend servers and serving centers,
we leverage another mechanism that serving infrastruc-
tures have long deployed. They have developed sophis-
ticated mapping algorithms that maintain performance
maps to clients with the goal of directing clients to
the nearest available server. These algorithms have the
property that clients that are directed to the server are
likely to be topologically, and probably geographically,
close to the server. We exploit this property to geolo-
cate frontend servers: essentially, we approximate the
location of a server by the geographical mean of client
locations, a technique we call client-centric geolocation
or CCG. We base our technique on this intuition, but
we compensate for incorrect client locations and varying
density of server deployments.
Finally, we leverage existing measurement infrastruc-
ture (PlanetLab) to cluster frontends into serving sites.
We model the relative location of a frontend server as
a vector of round-trip-times to many vantage points in
the measurement infrastructure, then employ standard
clustering algorithms in this high-dimensional space.
Using these measurement methods over a six month
period, we are able to study Google’s serving infrastruc-
ture and its evolution. Coincidentally, Google’s deploy-
ments have doubled over this period, and we explore