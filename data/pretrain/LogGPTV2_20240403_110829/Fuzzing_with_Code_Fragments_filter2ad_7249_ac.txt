the external tool. But using TraceMonkey as reference
JavaScript engine also comes with a downside. Since
jsfunfuzz is used daily within Mozilla, jsfunfuzz had al-
ready run on every revision of the engine. This fact has
two major consequences: First, jsfunfuzz would most
likely not ﬁnd any new defects; and second, the num-
ber of potential defects that can be found by LangFuzz
is signiﬁcantly reduced. Consequently, it is not possi-
ble to measure effectiveness based on a single revision
of TraceMonkey. Instead we make use of the fact that
Mozilla maintains a list of defects found by jsfunfuzz.
Using this list, we used a test strategy which is based on
time windows in which no defect ﬁxes were applied that
are based on defect reports ﬁled by jsfunfuzz (see Fig-
ure 4). Within these periods, both tools will have equal
chances to ﬁnd defects within the TraceMoney engine.
7
defects that where introduced between A and B which is
the testing window.
For all tests, we used the TraceMonkey development
repository. Both the tested implementation and the test
cases (approximately 3,000 tests) are taken from the de-
velopment repository. As base revision, we chose re-
vision 46549 (03f3c7efaa5e) which is the ﬁrst revision
committed in July 2010, right after Mozilla Firefox 4
Beta 1 was released at June 30, 2010. Table 1 shows
the ﬁve test windows used for our experiments. The end
revision of the last testing window dates to the end of Au-
gust 2010, implying that we covered almost two months
of development activity using these ﬁve windows. For
each testing window, we ran both tools for 24 hours.2
To check whether a defect detected by either tool was
introduced in the current testing window, we have to
detect the lifetime of the issue. Usually, this can be
achieved by using the bisect command provided by the
Mercurial SCM. This command allows automated test-
ing through the revision history to ﬁnd the revision that
introduced or ﬁxed a certain defect. Additionally, we
tried to identify the corresponding issue report to check
whether jsfunfuzz found the defect in daily quality assur-
ance routine.
5.1.2 Result of the external comparison
During the experiment, jsfunfuzz identiﬁed 23 defects,
15 of which lay within the respective testing windows.
In contrast, LangFuzz found a total of 26 defects with
only 8 defects in the respective testing windows. The
larger proportion of defects outside the testing windows
for LangFuzz is not surprising since LangFuzz, unlike
jsfunfuzz, was never used on the source base before this
experiment. Figure 5 illustrates the number of defects
per fuzzer within the testing windows.
To address research question Q1, we identiﬁed three
defects found by both fuzzers. Using the deﬁnition from
Section 5.1 the overlap between LangFuzz and jsfunfuzz
is 15%. While a high overlap value would indicate that
both fuzzers could be replaced by each other, an overlap
value of 15% is a strong indication that both fuzzers ﬁnd
different defects and hence supplement each other.
LangFuzz and jsfunfuzz detect different defects
(overlap of 15%) and thus should be used
complementary to each other.
To answer research question Q2, we computed the ef-
fectiveness of LangFuzz for the defects found within the
experiment.
2For both tools we used the very same hardware. Each tool ran on
4 CPUs with the same speciﬁcation. Since jsfunfuzz does not support
threading, multiple instances will be used instead. LangFuzz’s param-
eters were set to default values (see Table 3).
Figure 4: Example of a testing window with the live cy-
cle of a single defect.
start revision
46569:03f3c7efaa5e
47557:3b1c3f0e98d8
48065:7ff4f93bddaa
48350:d7c7ba27b84e
49731:aaa87f0f1afe
end revision
47557:3b1c3f0e98d8
48065:7ff4f93bddaa
48350:d7c7ba27b84e
49731:aaa87f0f1afe
51607:f3e58c264932
W1
W2
W3
W4
W5
Table 1: The ﬁve testing windows used for the experi-
ments. Each window is given by Mercurial revisions of
the Mozilla version archive. All windows together cover
approximately two months of development activity.
Within the remainder of this paper, we call these periods
testing windows.
In detail, we applied the following test strategy for
both tools:
1. Start at some base revision f0. Run both tools for
a ﬁxed amount of time. Defects detected can solely
be used to analyze the overlap, not effectiveness.
2. Set n = 1 and repeat several times:
(a) Find the next revision fn starting at fn−1 that
ﬁxes a defect found in the list of jsfunfuzz de-
fects.
(b) Run both tools on fn − 1 for a ﬁxed amount
of time. The defects found by both tools can
be used for effectiveness measurement if and
only if the defect was introduced between fn−1
and fn − 1 (the preceding revision of fn)1. For
overlap measurement, all defects can be used.
Figure 4 illustrates how such a testing window could
look like. The window starts at revision A. At some
point, a bug is introduced and shortly afterwards, the bug
gets reported by jsfunfuzz. Finally, the bug is ﬁxed in
revision B + 1. At this point, our testing window ends
and we can use revision B for experiments and count all
1 fn − 1 is exactly one revision before fn and spans the testing win-
dow. The testing window starts at fn−1 and ends at fn − 1 because fn is
a jsfunfuzz-induced ﬁx.
8
RevisionsAB1Bug is introduced2jsfunfuzzdetects bug3Bug is ﬁxed(last  jsfunfuzz related ﬁx)testing windowwithout jsfunfuzzrelated ﬁxes3
1options(’tracejit’);
2for (var j = 0; uneval({’-1’:true}); ++j) {
(−0).toString();
4}
Figure 6: Test case generated by LangFuzz causing the
TraceMonkey JavaScript interpreter to violate an inter-
nal assertion when executed. Reported as Mozilla bug
626345 [2].
This experiment should clarify whether only one of
the approaches accounts for most of the results (and the
other only slightly improves it or is even dispensable) or
if both approaches must be combined to achieve good
results.
Q3. How important is it that LangFuzz generates new
code?
Q4. How important is it that LangFuzz uses learned
code when replacing code fragments?
To measure the inﬂuence of either approach, we re-
quire two independent runs of LangFuzz with different
conﬁgurations but using equal limitation on resources
and runtime. The ﬁrst conﬁguration forced LangFuzz to
use only learned code snippets for fragment replacement
(mutation conﬁguration). The second conﬁguration al-
lowed code fragmentation by code generation only (gen-
eration conﬁguration).
Intuitively, the second conﬁguration should perform
random code generation without any code mutation at
all—also not using parsed test cases as fragmentation re-
placement basis. Such a setup would mean to fall back to
a purely generative approach eliminating the basic idea
behind LangFuzz.
It would also lead to incomparable
results. The length of purely generated programs is usu-
ally small. The larger the generated code the higher the
chance to introduce errors leaving most of the generated
code meaningless. Also, when mutating code, we can
adjust the newly introduced fragment to the environment
(see Section 3.3). Using purely generated code instead,
this is not possible since there exists no consistent envi-
ronment around the location where a fragment is inserted
(in the syntax tree at the end of generation). Although it
would be possible to track the use of identiﬁers during
generation the result would most likely not be compara-
ble to results derived using code mutation.
Since we compared different LangFuzz conﬁgurations
only, there is no need to use the testing windows from the
previous experiment described in Section 5.1. Instead,
we used the two testing windows that showed most de-
fect detection potential when comparing LangFuzz with
jsfunfuzz (see Section 5.1): W1 and W5. Both windows
9
Figure 5: Number of defects found by each fuzzer within
the testing windows and their overlap.
Compared to the 15 defects that were exclusively de-
tected by jsfunfuzz LangFuzz with it’s eight exclusively
detected defect has an effectiveness of 15 : 8 = 53%. In
other words, LangFuzz is half as effective as jsfunfuzz.
A generic grammar-based fuzzer like LangFuzz can be
53% as effective as a language-speciﬁc fuzzer like
jsfunfuzz.
For us, it was not surprising that a tried-and-proven
language-speciﬁc fuzzer is more effective than our more
general approach. However, effectiveness does not imply
capability. The several severe issues newly discovered by
LangFuzz show that the tool is capable of ﬁnding bugs
not detected by jsfunfuzz.
5.1.3 Example for a defect missed by jsfunfuzz
For several defects (especially garbage collector related)
we believe that jsfunfuzz was not able to trigger them
due to their high complexity even after minimization.
Figure 6 shows an example of code that triggered an as-
sertion jsfunfuzz was not able to trigger. In the original
bug report, Jesse Ruderman conﬁrmed that he tweaked
jsfunfuzz in response to this report: “After seeing this
bug report, I tweaked jsfunfuzz to be able to trigger it.”
After adaptation, jsfunfuzz eventually produced an even
shorter code fragment triggering the assertion (we used
the tweaked version in our experiments).
5.2 Generation vs. Mutation
The last experiment compares LangFuzz with the state
of the art JavaScript engine fuzzer jsfunfuzz. The aim
of this experiment is to compare the internal fragment
replacement approaches of LangFuzz: code generation
against code mutation. The ﬁrst option learned code frag-
ments to replace code fragments while the second option
uses code generation (see Section 4.2) for replacement
instead.
81583 (15%)overlapLangFuzzdefectsjsfunfuzzdefects1 ( ’false’? length(input + ’’): delete(null?0:{}),0 ).
watch(’x’, function f() { });
Figure 8: Test case generated by LangFuzz using code
generation. The code cause the TraceMonkey JavaScript
interpreter to write to a null pointer. Reported as Mozilla
bug 626436 [3].
tation replacement strategy. Interestingly, this proportion
is reversed in test window W5. Here a total of 24 de-
fects and again the majority of 15 defects were found
using both conﬁgurations. But in W5 the number of de-
fects found by mutation conﬁguration exceeds the num-
ber of defects found by code generation. Combining the
number of defects found by either conﬁgurations exclu-
sively, code generation detected 13 defects that were not
detected by the mutation conﬁguration. Vice versa, code
mutation detected 9 defects that were not detected during
the code generation conﬁguration run. Although the ma-
jority of 29 defects found by both conﬁgurations, these
numbers and proportions show that both of LangFuzz in-
ternal fragmentation replacement approaches are crucial
for LangFuzz success and should be combined. Thus,
an ideal approach should be a mixed setting where both
code generation and direct fragment replacement is done,
both with a certain probability.
The combination of code mutation and code
generation detects defects not detected by either
internal approach alone. Combining both approaches
makes LangFuzz successful.
5.2.2 Example of defect detected by code generation
The code shown in Figure 8 triggered an error in the
parser subsystem of Mozilla TraceMonkey. This test was
partly produced by code generation. The complex and
unusual syntactic nesting here is unlikely to happen by
only mutating regular code.
5.2.3 Example for detected incomplete ﬁx
The bug example shown in Figure 9 caused an assertion
violation in the V8 project and is a good example for both
an incomplete ﬁx detected by LangFuzz and the beneﬁts
of mutating existing regression tests: Initially, the bug
had been reported and ﬁxed as usual. Fixes had been
merged into other branches and of course a new regres-
sion test based on the LangFuzz test has been added to
the repository. Shortly after, LangFuzz triggered exactly
the same assertion again using the newly added regres-
sion test in a mutated form. V8 developers conﬁrmed
that the initial ﬁx was incomplete and issued another ﬁx.
10
Figure 7: Defects found with/without code generation.
showed a high defect rate in the experimental setup de-
scribed in Section 5.1 and spanned over 5,000 revisions
giving each conﬁguration enough defect detection poten-
tial to get comparable results.
Limiting the number of testing windows to compare
the two different conﬁgurations of LangFuzz, we were
able to increase the runtime for each conﬁguration, thus
minimizing the randomization impact on the experiment
at the same time. Both conﬁgurations ran on both testing
windows for 72 hours (complete experiment time was 12
days). For all runs we used the default parameters (see
Table 3), except for the synth.prob parameter. This
parameter can be used to force LangFuzz to use code
generation only (set to 1.0) and to ignore code generation
completely (set to 0.0).
Since the internal process of LangFuzz is driven by
randomization, we have to keep in mind that both runs
are independent and thus produce results that are hard to
compare. Thus, we should use these results as indica-
tions only.
5.2.1 Result of the internal comparison
Figure 7 shows the results of the internal comparison
experiment as overlapping circles. The left overlapping
circle pair shows the result of the comparison using test
window W1, the right pair the results using test window
W5. The dark circles represent the runs using the genera-
tion conﬁguration while the white circles represent runs
using the mutation conﬁguration. The overlap of the dark
and white circles contains those defects that can be de-
tected using both fragment replacement strategies. The
numbers left and right of the overlap show the number of
defects found exclusively by the corresponding conﬁgu-
ration.
For W1 a total of 31 defect were found. The major-
ity of 14 defects is detected by both conﬁgurations. But
11 defects were only found when using code generation
whereas six defects could only be detected using the mu-
116147215generation conﬁguration mutation conﬁgurationfound with both conﬁgurationstesting window W1testing window W51var loop count = 5
2function innerArrayLiteral(n) {
3 var a = new Array(n);
for (var i = 0; i < n; i++) {
a[i] = void ! delete ’object’%
4
5
6 ˜ delete 4
7 }
8}
9function testConstructOfSizeSize(n) {
10 var str = innerArrayLiteral(n);
11}
12for (var i = 0; i < loop count; i++) {
for (var j = 1000; j < 12000; j += 1000) {
testConstructOfSizeSize(j);
13
14
15 }
16}
Figure 9: Test case generated by LangFuzz discovering
an incomplete ﬁx triggering an assertion failure in the
Google V8 JavaScript engine. Reported as Google V8
bug 1167 [4].
5.3 Field tests
The ﬁrst two experiments are targeting the external and
internal comparison of LangFuzz. But so far, we did
not check whether LangFuzz is actually capable of ﬁnd-
ing real world defects within software projects used
within industry products. To address this issue, we con-
ducted an experiment applying LangFuzz to three differ-
ent language interpreter engines: Mozilla TraceMonkey
(JavaScript), Google V8 (JavaScript), and the PHP en-
gine. In all experiments, we used the default conﬁgura-
tion of LangFuzz including code generation. An issue
was marked as security relevant if the corresponding de-
velopment team marked the issue accordingly as security
issue. Thus the security relevance classiﬁcation was ex-
ternal and done by experts.
Mozilla TraceMonkey We tested LangFuzz on the
trunk branch versions of Mozilla’s TraceMonkey
JavaScript engine that were part of the Firefox 4 re-
lease. At the time we ran this experiment, Firefox