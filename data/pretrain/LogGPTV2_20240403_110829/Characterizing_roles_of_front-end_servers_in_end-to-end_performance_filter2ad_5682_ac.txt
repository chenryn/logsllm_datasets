In conclusion,
our study shows that simply placing FE servers closer to
users may not be entirely eﬀective in improving the overall
user-perceived performance in dynamic content distribution.
Other key factors such as the processing time, server loads at
both FE servers and BE data centers as well as the (physical
and TCP) connections between them also play a critical role.
Improving and optimizing these factors are therefore impor-
tant in improving the overall user-perceived performance in
dynamic content distribution such as dynamic generation of
search results in response to user queries.
5. FACTORING FE-BE FETCH TIME
As discussed in Sec. 2, Tf etch consists of two key compo-
nents, namely, Tproc and RT Tbe. They represent the search
query processing time at the BE data center and the delivery
time of search results from the BE server to the FE server
respectively. As part of our ongoing work, we are exploring
various mechanisms to separate these two components.
i
c
m
a
n
y
d
T
350
300
250
200
150
100
50
0
0
(a)
y = 0.08*x + 2.5e+02
data
   fitting
100
200
300
400
Distance to Bing data center (miles)
i
c
m
a
n
y
d
T
120
100
80
60
40
20
0
0
(b)
y = 0.099*x + 34
data
fitting
100
500
Distance to Google data center (miles)
300
400
200
Figure 9: Correlating Tdynamic and the distance to
BE data center.
To get a better understanding of these components we
conducted the following analysis. We ﬁrst get a list of pos-
sible locations for Bing and Google data centers from [1, 2].
Next for each of these data centers we consider the geo-
graphically closest FE servers, and plot the distribution of
Tdynamic with respect to geographical distane between FE
and BE. As explained in Sec 4 for smaller values of RTT,
Tdynamic can be considered as an approximation for the
Tf etch. Figure 9 shows the distribution of Tf etch time for
Bing and Google with respect to the geographical distance
between the FE and BE locations. For this plot, we con-
sider the Bing data center located in Virginia (US), while
we pick the Lenoir, North Carolina data center for Google.
As seen in these ﬁgures, the Tf etch time increases linearly as
the distance between BE and FE increases. We perform a
linear regression to ﬁt a straight line for these data points,
which is shown using the red continuous lines in the ﬁgure.
As seen in this ﬁgure the Y-intercept for the regression is
260ms for Bing, while it is only 34ms for Google data cen-
ter. This intercept actually represents the computation time
for a given search query for Bing and Google data centers.
Similarly, the slope of the line represents the contribution
of network delay in Tf etch, which are similar for both Bing
and Google. For the diﬀerent keywords used in our search
queries, we get very similar slope, but pretty ydiﬀerent in-
tercept values. Though our initial results show interesting
characteristics of factors aﬀecting the FE-BE fetch time, we
are currently conducting extensive experiments and analysis
to gain a better understanding behind the factors aﬀecting
the FE-BE fetch time, and thus will potentially guide us in
designing better content placement and delivery strategies
for dynamic content distribution.
6. DISCUSSIONS
In this paper we have focused on the roles of frontend
servers on the end-to-end performance of search queries us-
ing the standard search fucntions of search engines. More
recently, some search engines such as Google has introduced
more advanced search features such as the interactive “search
as you type” feature. Our preliminary investigation of this
new feature shows that our basic model and key observations
still hold. We ﬁnd that using the interactive search feature,
after each letter a user has typed, a separate query (using a
new TCP connection) is sent to the FE server. The delivery
of each query hence still ﬁts our basic model; although we
believe it is likely that the search query processing times at
the BE data centers are generally reduced because the sub-
sequent queries are highly correlated with previous queries.
We are in the process of conducting more thorough mea-
surements and analysis on this and other search features.
As most Planetlab nodes are located within or close to
the University campus networks (and it is known that some
Akamai frontend servers are placed closer to University cam-
pus networks), we realize that using the PlanetLab as the
testbed may introduce some biases. For instance, the RTT
between PlanetLab and Akamai FE servers may not be of all
users. In addition, in our measurements we do not see any
signiﬁcant packet losses. In an environment where the loss
rates are high (e.g., in a wireless network), placing FEs closer
to users in fact may signiﬁcantly improve the user-perceived
end-to-end performance by reducing the total time needed
to deliver the query result (that has been delivered to the FE
server from a BE data center) to a user. As part of ongoing
work, in addition to the PlanetLab, we are utilizing other
testbeds (e.g., Seattle testbed [3]). We are also investigating
the trade-oﬀs between RTTs and loss rates (e.g., in a WiFi
environment) in the placement of FE servers.
7. CONCLUSIONS
In this paper we investigated the roles of FE servers in
improving the user-perceived performance of dynamic con-
tent distribution. Using Bing and Google search services as
case studies, we conducted extensive application-layer active
measurement and data analysis. Our results demonstrate
that there is a critical trade-oﬀ between the placement of
FE servers and the FE-BE fetch time. While placing FE
servers closer to users can help reduce latency, other key fac-
tors such as processing times and loads at both FE servers
and BE data centers as well as the quality of (physical and
TCP) connections between them also play a critical role in
determining the overall user-perceived performance.
Acknowledgments
This work is supported in part by the NSF grants CNS-
0905037, CNS-1017092 and CNS-1017647.
5648. REFERENCES
[1] Google data centers. http://www.google.com/
corporate/datacenter/locations.html.
[2] Microsoft online services data center locations.
http://www.championcloudservices.com/
data-center-locations/.
[3] Seattle. https://seattle.cs.washington.edu/html/.
[4] S. Androutsellis-Theotokis and D. Spinellis. A survey
of peer-to-peer content distribution technologies. ACM
Computing Surveys (CSUR), 36(4):335–371, 2004.
[5] M. Bateni and M. Hajiaghayi. Assignment problem in
content distribution networks: unsplittable
hard-capacitated facility location. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, pages 805–814. Society for
Industrial and Applied Mathematics, 2009.
[6] M. El Dick, E. Pacitti, R. Akbarinia, and B. Kemme.
Building a peer-to-peer content distribution network
with high performance, scalability and robustness.
Information Systems, 2010.
[7] H. Jiang, Z. Wang, A. Wong, J. Li, and Z. Li. A
replica placement algorithm for hybrid CDN-P2P
architecture. In 2009 15th International Conference on
Parallel and Distributed Systems, pages 758–763.
IEEE, 2009.
[8] D. Lewin, A. Davis, S. Gendler, M. Kagan, J. Parikh,
and W. Weihl. Dynamic content assembly on
edge-of-network servers in a content delivery network,
July 6 2010. US Patent 7,752,258.
[9] A. Pathak, Y. Wang, C. Huang, A. Greenberg, Y. Hu,
R. Kern, J. Li, and K. Ross. Measuring and evaluating
TCP splitting for cloud services. In Passive and Active
Measurement, pages 41–50. Springer, 2010.
[10] J. Ravi, Z. Yu, and W. Shi. A survey on dynamic Web
content generation and delivery techniques. Journal of
Network and Computer Applications, 32(5):943–960,
2009.
[11] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and
M. Ammar. Answering what-if deployment and
conﬁguration questions with wise. In Proceedings of
the ACM SIGCOMM 2008 conference on Data
communication, SIGCOMM ’08, pages 99–110, New
York, NY, USA, 2008. ACM.
[12] M. Wittie, V. Pejovic, L. Deek, K. Almeroth, and
B. Zhao. Exploiting locality of interest in online social
networks. In CoNEXT, page 25. ACM, 2010.
565Summary Review Documentation for 
“Characterizing Roles of Front-end Servers in End-to-End 
Performance of Dynamic Content Distribution” 
Authors: Y. Chen, S. Jain, V. Adhikari, Z. Zhang 
Reviewer #1 
Strengths:  Simple  and  clear  model  that  is  then  confirmed  by 
measurements. Comparison of 2 different search engines, instead of 
just applying the model to one. 
Weaknesses: It is surprising to see such differences in the slope of 
the lines in figure 8, you would expect them to be roughly the same. 
Model doesn’t take into account new search engine webpages where 
dynamic  content  is  continuously  updated  as  people  type  and  try 
different searches.  
Comments to Authors: Short and clear paper. 
1. 
Introduction:  you  should  mention  that  the  load  on  the 
client side might also impact the overall performance  
2.  As mentioned in your weaknesses, your model assumes a 
single  search  when  search  engines  have  now  dynamic 
webpages where the static content is initially downloaded 
and  the  dynamic  content  is  continuously  updated  as  the 
user type and try different searches. 
3.  Do  you  have  any  references  about  the  front  end 
architecture for Bing and Google?  
In section 2, I assume that you define Tdelta as t5-t4 and 
not t4-t2  
4. 
6. 
5.  Did  you  also  observe  protocol  manipulations  when  you 
compared the 2 search engines? Did you see for instance 
differences in initial congestion windows? 
I like your section 5 but I’m puzzled by it and by figure 8. 
I  was  expecting  indeed  a  different  Y-intercept  for  both 
search engines. However I don’t understand how we can 
observe a difference of a factor of 3 between the slopes of 
the  lines  in  the  2  figures.  You  would  expect  these 
numbers to be pretty similar.  
Reviewer #2 
Strengths: The problem of inference the frontend-to-backend delay 
is  an  important  one.  The  paper  can  be  a  good  reference.  Solid 
measurement  methodology  and  analysis  of  measurements.  Large 
scale experiments. I like the trade-off between placing caches very 
close to the user vs. put caches close enough to the datacenters to 
reduce fetching time. The paper is well-written. 
Weaknesses:  In  section  3  it  is  not  clear  how  you  generate  the 
queries.  Queries  and  answers  may  very  per  region,  this issue was 
not enough discussed in the paper. The authors used only PlanetLab 
nodes; there are other available testbeds that were not utilized. 
Comments to Authors: (1) In section 3 you have to elaborate more 
on  how  you  generate  the  queries.  I  guess  that  the  fetching  time 
depends  on  the  complexity  of  the  query.  You  should  expect  low 
fetching time for very popular queries and a higher fetching time for 
complex  queries.  It  would  be  nice  to  present  results  on  this.  You 
may want to evaluate if there is a correlation between the fetching 
time and the number of words used in the query.  
(2)  You  have  to  provide  evidence  that  your  queries  are  quite 
representative for users located in different regions. It is known that 
queries and answers in both Google and Bing highly depend on the 
user region, you have to address this issue.  
(3) It is also known that both Google and Bing also optimize based 
on  user  profile.  In  your  study  this  can  not  be  simulated.  Do  you 
think that this may increase the fetching time you observe? 
 (4) It is not clear what is the popularity of your queries. Do you 
expect that the answers are pre-cached or you always receive a fresh 
answer for dynamic content?  
(5) Despite the fact that this is a short paper, you have to provide 
information for the active measurement platform.  
(6)  Did  you  observe  any  fetching  activity  if  the  autocompletion 
feature is activated?  
(7) You may also want to compare the fetching time with the DNS 
resolution time.  
(8)  You  may  want  to  repeat  your  experiments  using  the  Seattle 
testbed 
(https://seattle.cs.washington.edu/html).  The  bandwidth 
limitations should not be a problem for your measurements. 
Reviewer #3 
Strengths: Very timely and relevant problem. Nice methodology in 
terms  of  measurement,  model  building  and  validation.  Useful 
reported results. Very good presentation. 
Weaknesses: Some missing related work. 
Comments  to  Authors:  I  really  enjoyed  reading  this  paper,  my 
compliments to the authors. 
1.  Please take a look at: Exploiting Locality of Interest in Online 
Social  Networks,  Mike  P.  Wittie  et  al.,  ACM  CoNEXT’10. 
From  the  title  it  does  not  seem  that  relevant  but  it  is  about 
OSNs FE used to split the TCP connection to the OSN BE and 
as  well  cache  static  parts  ...  Please  discuss  how  your  work 
compares to this paper. 
2.  A suggestion: Why don’t you use a virtual coordinates system 
to estimate the RTT between FE and BE servers and then take 
566this  and  Tstatic+RTT  out  from  Tdynamic  in  order  to  say 
something about Tproc at the datacenter?  
3.  Additionally, I think that you can improve (or maybe report if 
you  have  already  done)  the  study  around  the  effect  of 
particular  queries  to  the  observed  results.  For  example  you 
should explain more clearly at least the two extremes, simple 
queries for which the answer will come very quick (might be 
cached at the BE) and more complicated ones. 
4.  Another  thing,  in  your  model  I  cannot  see  how  the  the 
interactive "search as you type" offered by google the last 1-2 
years would fit. 
5.  Also I think you need to improve the positioning of your work 
and contributions. Cite more papers on tiered internet services, 
optimization  of  intra  datacenter  communications  between  FE 
and BE, etc. There is work here that you don’t mention. 
Reviewer #4 
Strengths: This is a very timely topic as dynamic service delivery is 
emerging as the next big business for CDNs, after video delivery. 
the paper´s model and evaluation concurre and validate each other. 
the results on the differences from Bing using Akamai and Google 
using their own CDN are interesting too. 
Weaknesses:  The  model  does  a  good  job  at  capturing  delays 
between  edge  and  back  servers,  processing  times,  and  delivery 
times. however, it lacks a more fundamental study of the impact that 
losses can have in TCP splicing and thus the position of the servers. 
you assume that FE server position is only worth it to have enough 
pipelining such as to ensure that no idle time exists, and in the case 
of high processing time, placing servers closer to the user may not 
have  any  effect.  this  is  true,  as  long  as  there  are  no  losses  since 
splitting TCP connections under losses can have a secondary effect, 
i.e. that of increasing the throughput of the connection. for instance 
look  at  this  paper:  "TPOT:  Translucent  Proxying  of  TCP".  P. 
Rodriguez, S.Sibal, O.Spatscheck, WCW’00, International Caching 
Workshop. 
Comments to Authors: None.  
Reviewer #5 
Strengths: Nice observations about content delivery performance. 
Weaknesses:  Study  from  planetlab  only,  so  RTT  is  probably  not 
representative but the message should still be valid when measured 
from other end-hosts. 
Comments to Authors: A latency of 20ms even to Akamai is really 
low.  DSL  end-hosts  would  have  higher  latency,  even  to  Akamai. 
Maier et al. Characteristics of residential traffic. IMC 2009, showed 
that  often  30  ms  is  added  just  by  the  DSL  interleaving,  so  the 
latencies you found are certainly not realistic, not even mentioning 
mobile devices. Your observations are due to Akamai having caches 
in academic networks and large ISPs. This is not always the case 
and greatly varies across networks, both academic and commercial. 
Despite the methodological limitations, this paper is of great interest 
to Akamai, send it to them. 
Response from the Authors 
We  thank  the  anonymous  reviewers  for  providing  valuable 
comments  and  feedback  on  our  paper.  We  have  addressed  a 
majority of them in this final version of the paper, and few others 
are listed as future work. Rest of the comments contain interesting 
suggestions, but unfortunately are out of the scope of this paper. 
We  add  one  separate  discussion  section  to  discuss  the  major 
concerns raised by the reviewers. One such concern is that we only 
dealt with the traditional search engine, in which advanced features 
such  as  the  interactive  search,  autocomplete  function  are  not 
considered.  To  address  this  problem,  we  elaborate  on  how  these 
features work, and how they still fit our basic model. 
Some  reviewers  suggest  that  the  use  of  Planetlab  may  not  be 
representative  of  the  typical  RTT  between  the  users  and  the  FE 
servers. Also, the loss rate might be another factor that should be 
taken into consideration in placing the FE servers. In particular, with 
the increasing popularity of wireless users, the loss rate at the last 
hop can be a major issue. We discuss these issues in the discussion 
section, and also list them as our ongoing work. 
Some reviewers asked for clarification about the results presented in 
the  last  section,  i.e. factoring FE-BE fetch time. The slope values 
shown  in  the  last  figure  was  quite  different.  To  validate  the 
correctness  of  our  results,  we  repeat  the  same  measurements 
described in that section for different keywords, and only consider 
front-end servers close enough to the BE servers, and new results 
are reported in the final version of the paper. 
To address concerns that the impact of the DNS resolution time, and 
the  personalization  of  the  search  results  generated  for  users  in 
different  regions  were  not  explicitly  mentioned,  we  put  several 
footnotes in the final version of the paper to better clarify them. We 
also  added  some  of  the  citations  suggested  by  the  reviewers,  and 
fixed several typos. 
567