28
SARS
Garcia
Table 4: A Possible Patient Information Table Re-
garding to the Disclosed Views Shown in Table 2
and Table 3(A), where (any) represents any possible
medical condition
deﬁned on a single view, such as Recursive (1,2)-Diversity,
cannot be applied to this multiple view disclosure case to
protect the privacy. Still, we can check privacy violation
through the above game based on the adversary’s random
oracle.
In this case, based on the disclosed information,
Donald can only be associated with either SARS or V iral
Inf ection. With the constraint of Donald being associated
with SARS, there are 3! × 3! such tables in output domain
of the adversary’s random oracle. With the constraint of
Donald being associated with V iral Inf ection, there are
3 × 3 such tables in the output domain of the adversary’s
random oracle. Therefore, we can compute that the prob-
ability of Donald to have SARS in an outcome is 0.8 and
the desired privacy property is violated.
Based on the above observation, it is clear that Table 3(A)
and Table 3(B) should not be disclosed together in order to
protect Donald’s privacy. However, as we have discussed in
the beginning, without a centralized data disclosure control
authority, it may happen that two distributed data authori-
ties, hospital and insurance company, for example, may dis-
close the two views above without notifying each other. In
this case, all of the privacy protection techniques based on
preventing the unsafe disclosures in advance would fail.
Unfortunately, once Tables 3(A) and 3(B) have been dis-
closed, we cannot revoke them from the adversary’s knowl-
edge. But we can discover that in this case, when Donald’s
privacy is not well protected, the privacy of Alan, Bob, Clark,
Ellen, F en, andGarcia are “over protected”. That is, Alan
can be associated with Heart Disease, SARS, and V iral
Inf ection, and when checking with the adversary’s random
oracle, the probabilities of Alan to have Heart Disease,
SARS, and V iral Inf ection in an outcome are 1/3(≈ 0.3),
2/5(= 0.4), and 4/15(≈ 0.3), respectively. This also ap-
plies to Bob and Clark. And when checking with the ad-
versary’s random oracle, the probabilities of Ellen to have
F lu, SARS, and V iral Inf ection in an outcome are 1/3(≈
0.3), 2/5(= 0.4), and 4/15(≈ 0.3), respectively. This also
applies to F en and Garcia. Clearly, the privacy protection
of these six people are not close to the desired bound (a
probability of 0.5). This gives us a chance, by sacriﬁcing
the “over-protected” part, to restore the violated privacy of
Donald, in a way of disclosing more true information about
the original Table 1. One way to do it is to disclose the
following two views of Table 1 (shown in Table 5).
Now with the four views (Tables 3(A,B) and Tables 5(A,B))
disclosed, we can compute that the probability for Donald
to have SARS in an outcome is reduced to 0.5 and the
probability for Donald to have V iral Inf ection is also 0.5.
Condition
Age
25˜26 Viral Infection
25˜26
SARS
(A)Age-Cond in ABC, Inc.
Condition
Age
26˜27
26˜27 Viral Infection
(B)Age-Cond in ABC, Inc.
SARS
Table 5: Further Disclosed Views
This result occurs because, in the updated output domain
of the adversary’s random oracle, there are 2× 2 tables with
Donald being associated with SARS (at the same time,
both Clark and Ellen being associated with V iral Inf ection),
and there are 2×2 tables with Donald being associated with
V iral Inf ection (at the same time, both Clark and Ellen
being associated with SARS). Similarly, it can be shown
that the probability for any other person to have any med-
ical condition in an outcome is equal to 0.5 in the worse
case. Therefore, the desired privacy property is now satis-
ﬁed. Clearly, at the same time, the privacy protection in
general is still getting worse, i.e., there are 45 diﬀerent ta-
bles in the outcome of the adversary’s random oracle with
Tables 3(A,B) disclosed and there are only 4 diﬀerent tables
in the outcome of the adversary’s random oracle with all
four tables disclosed. This illustrates the ability of our tech-
nique to restore the compromised privacy through additional
information disclosure.
Clearly, it is not always possible to restore compromised
privacy by the same technique. For example, in the extreme
case, if we have disclosed to the adversary that Donald has
SARS, nothing can be done to restore the privacy.
We should also clarify that in the above example, we as-
sume that the adversary cannot diﬀerentiate the disclosure
of Tables 3(A,B) and the disclosure of Tables 5(A,B), i.e.,
the adversary does not have the knowledge that the dis-
closure of Tables 5(A,B) is to restore the violated privacy
by the disclosure of Tables 3(A,B). This can be reasonable
in practice when, for example, two disclosures are executed
from diﬀerent information sources for diﬀerent purposes or
Tables 5(A,B) can be just disclosed instead of Tables 3(A,B)
when the disclosure control monitor ﬁnds out the potential
privacy violation by disclosing Tables 3(A,B). We will also
discuss how to apply our technique under more ﬂexible as-
sumptions.
Contributions
In this paper, we study the problem of restoring compro-
mised privacy for micro-data disclosure with multiple dis-
closed views. More speciﬁcally, the contributions of this
paper are as follows.
First we propose a new property, called γ-Privacy, for pri-
vacy protection in a micro-data disclosure problem when
multiple views are disclosed. Given the disclosed views and
publicly available information, the set P IS of “all possible
worlds”(i.e., possible tables that would yield the same disclo-
sure results), is deﬁned. γ-Privacy intuitively means that
in a randomly (uniformly) selected instance, the probabil-
ity of any individual to be associated with a sensitive value
is at most γ. We then prove that, for the case of a sin-
gle disclosed view, γ-Privacy is equivalent to the property
of Recursive ( γ
1−γ , 2)-Diversity. This intuitively means that
our property is a “natural” extension of l-Diversity, which is
deﬁned only for a single disclosed view, to multiple views.
Second, we prove that deciding on whether γ-Privacy is
38satisﬁed by a set of disclosed views is #P-complete. Third,
to mitigate the high computational complexity, we relax the
property of γ-privacy to be satisﬁed with (, θ) conﬁdence,
i.e., that the probability of disclosing a sensitive value of
an individual be at most γ +  with statistical conﬁdence θ,
where  is an arbitrary small positive constant. We propose a
Monte Carlo-based algorithm to check the relaxed property
in O((λλ(cid:48))4) time for constant  and θ, where λ is the number
of tuples in the original table and λ(cid:48) is the number diﬀerent
sensitive values in the original table.
Finally, we turn to the problem of restoring compromised
privacy. Namely, given a set of disclosed views that vio-
lates γ-Privacy, can we extend it to a superset of views that
jointly satisfy γ-Privacy. We propose heuristic polynomial
time algorithms which are based on enumerating and check-
ing additional disclosed views. We conduct a preliminary
experimental study on heart desease records taken from the
UCI data repository ([5]) which demonstrates that the pro-
posed polynomial algorithms restore privacy in up to 60% of
compromised disclosures. We also discuss how to apply our
technique under diﬀerent assumptions when the adversary
is also aware of our technique.
Organization
The remainder of this paper is organized as follows. In Sec-
tion 2, we formalize the γ-Privacy property and prove that
deciding on it is #P-complete. In Section 3, we describe the
relaxed γ-Privacy and give Monte Carlo-based algorithm to
check it. In Section 4, we study how to restore compromised
privacy. In Section 5, we describe our experimental result.
In Section 6, we discuss extensions of the adversary model.
Finally, we overview related work in Section 7 and conclude
in Section 8.
2. MODELING THE PROBLEM
In this paper, we focus on the problem of micro-data dis-
closure. Consider a micro-data table baseT with schema
D = (ID, QI1, . . . , QIa, SA1, . . . , SAb), where: (1) ID is an
attribute used to identify an individual, such as N ame or
SSN ; (2) QI1, . . . , QIa are attributes that serve as quasi-
identiﬁers of the ID attributes (i.e., they can be used to
identify an individual or a small set of individuals.), such as
Age, Employer or Address; (3) SA1, . . . , SAb are attributes
that are considered private information, such as medical con-
dition. In this paper, we limit our scope to the cases such
that ID is a key of baseT and there is only one SA attribute
in baseT . Also, we use a multiset version of the relational
model and algebra ([13]).
limited to the following two forms:
We assume that information disclosures about baseT are
• Public Knowledge disclosure, publicT is a projection
of baseT without the private attribute. I.e., publicT =
πID,QIs(baseT ).
• Generalized disclosure, (V, ψ), is a generalized view
of baseT which is disclosed upon request.
I.e., V =
πSA(σψ(baseT )), where ψ is a propositional formula
on QI attributes which represents a generalization and
is also publicly known. Note that, we may have a sen-
sitive value appearing multiple times in V .
Note that, for a particular table baseT , there can be only one
Public Knowledge disclosure but multiple generalized disclo-
sures. For the sake of simplicity, we will call the combination
of the Public Knowledge disclosure and the multiple gener-
alized disclosures a “micro-disclosure” of baseT , denoted by
∆ = (publicT, {(V1, ψ1), . . . , (Vn, ψn)}).
In the previously
discussed medical information example, Table 2 is a Pub-
lic Knowledge disclosure of Table 1, while Table 3(A) and
Table 3(B) are two generalized disclosures. For Table 3(A),
Sex = M ale is the selection condition and 26 ≤ Age ≤ 28
is one for Table 3(B).
We assume that an adversary is able to collect all the
disclosed information. To obtain the relation between ID
and SA of the original table baseT , for every generalized
disclosure (V, ψ), the adversary can compute a new view
V I = πID(σψ(publicT )). The adversary then gets to know
that the sensitive values that are associated with the IDs
that appear in V I are the values that appear in V . All of
these information serve as constraints for the adversary to
have a correct guess of the original table. In other words,
we can represent the adversary’s knowledge of the original
table baseT by a possible instance set, deﬁned as follows:
Deﬁnition 1. Given a micro-disclosure ∆ = (publicT,{(V1,
ψ1), . . . , (Vn, ψn)}), the Possible Instance Set, PIS, is the set
of all tables T such that:
• πID,QI (T ) = publicT and
• ∀i(1 ≤ i ≤ n), πSA(σψi (T )) = Vi
Without loss of generality, in the remainder of this pa-
per, we assume that in any micro-data disclosure problem,
the generalized disclosures (V1, ψ1), . . . , (Vn, ψn) of a given
baseT always satisfy the following two properties:
(1) The generalized disclosures provide a cover for the tu-
ples in baseT . That is, for any id ∈ πID(baseT ), there
exists at least one generalized disclosure (Vi, ψi)(1 ≤
i ≤ n) such that id ∈ πID(σψi (baseT )).
In fact, if
an ID value appears in the Public Knowledge disclo-
sure but does not appear in any of the generalized
views, there would be no information for the adver-
sary to infer the sensitive value associated to that ID
value. Therefore, the adversary can just eliminate this
ID value from the Public Knowledge disclosure and
potentially the original table baseT . This does not
aﬀect how the adversary can infer the sensitive val-
ues associated with other individuals. Consequently,
this property guarantees the corresponding possible in-
stance set, PIS, to be a ﬁnite set.
(2) The generalized disclosures are well connected, i.e., we
cannot divide the set of the selections functions of
the generalized disclosures into two non-empty sets,
{ψi1 , . . . , ψik} and {ψik+1 , . . . , ψin}, such that:
(∨k
j=1ψij ) ∧ (∨n
j=k+1ψij ) = f alse
Given a micro-data disclosure problem, if the gener-
alized disclosures do not satisfy this property, we will
be able to decompose this problem into two indepen-
dent problems. The given Public Knowledge disclosure
publicT can be divided into two tables, publicT (cid:48) =
σψ(cid:48) (publicT ) and publicT (cid:48) = σ¬ψ(cid:48) (publicT ). Corre-
spondingly, the generalized disclosures can be divided
into two groups (with ψi ⇒ ψ(cid:48) or ψi ⇒ ¬ψ(cid:48)), along
with the two Public Knowledge tables above to form
two micro-data disclosure problems, respectively.
With all of the collected information, the adversary can try
to understand the sensitive values that are associated with
39each individual that appeared in publicT . To formalize this
process, consider the following (P IS, id, s)-guessing game.
To determine whether an ID value id is associated with a
sensitive value s, the adversary randomly selects a table,
using uniform distribution, from the P IS with respect to
a given micro-disclosure. The adversary wins the game if,
in the selected table, id is associated with s. Therefore, to
protect the individual’s privacy, our goal is to guarantee that
the adversary cannot win this (P IS, id, s)-guessing game for
any id, s with a high probability.
Deﬁnition 2. A micro-disclosure ∆ = (publicT,{(V1, ψ1),
. . . , (Vn, ψn)}) is said to be γ-Private (0 ≤ γ  0) and (2) the PIS with respect to Table 2
(Public Knowledge ) and Table 3(A,B) (generalized) is not.
As we show in Claim 1, the 0.5-Private property can be
regarded as an extension to the Recursive (1, 2)-Diversity
for multiple generalized disclosures, i.e., they are equivalent
when applied to the problem with a single generalized dis-
closure. Clearly, the 0.5-Privacy can be applied to multiple
generalized disclosures while the Recursive (1, 2)-Diversity
cannot. It is worth noting that if we have multiple general-
ized disclosures such that any two of them do not intersect
with each other, a property like Recursive (1, 2)-Diversity
can be applied. However, as we have previously assumed
in the well-connectivity of P IS, such a multiple generalized
disclosure case can be decomposed into multiple indepen-
dent cases of single generalized disclosure. Claim 1 states
the relation between two properties.
Claim 1. The possible instance set PIS, with respect to a
Public Knowledge disclosure publicT and a single generalized
disclosure (V, ψ), is a γ-Private, if and only if V satisﬁes
Recursive ( γ
1−γ , 2)-Diversity.
Proof. For any ID value id and sensitive value s that
appear in publicT or V , the fact that V satisﬁes Recursive
( γ
1−γ , 2)-Diversity guarantees that the probability of id to
be associated with s in an output of the adversary’s random
oracle is less than or equal to γ. This is true because in
a single view disclosure case, the possible instance set P IS
can be regarded as a set of all possible permutations of sen-
sitive values appearing in V (with a ﬁxed order of ID values
appeared in publicT ). Therefore, among all |P IS| possible
instance tables, there are at most γ|P IS| tables having a
particular id associated with a particular sensitive value s.
Therefore, P IS is γ-Private.
On the other hand, if V does not satisfy Recursive ( γ
1−γ ,
2)-Diversity, there must exist an ID value id and a sensitive
value id, such that the number table among P IS having id
associated with s is greater than γ|P IS|. Consequently, P IS
cannot be a γ-Private.
Clearly, the property of γ-Private can be regarded as an ex-
tension of the property of Recursive ( γ
1−γ , 2)-Diversity. It
can be understood that we can design similar private prop-
erties as the γ-Private to extend the rest of properties in the
l-Diversity familiy ([3]) and even other privacy properties