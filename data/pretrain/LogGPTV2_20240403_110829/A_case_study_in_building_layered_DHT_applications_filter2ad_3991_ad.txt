other minor optimizations of the DHT implementation, we
expect the median insertion latency to be reduced by a factor
of two.
Still, the insertion latency is not negligible. To a large
extent, this is a result of our decision to build our range
query data structure entirely on top of a general-purpose
DHT service. A typical insert operation is composed of a
binary search (median of 6 DHT gets in this experiment)
followed by a put(). Some small number of insertions result in
splits and thus have a higher latency. All of these operations
are invoked from outside the DHT service, and hence cannot
take advantage of any specialized routing within the DHT
for e(cid:14)ciency.3
As we can see from the (cid:12)gure, techniques such as aggres-
sive caching can help reduce latency substantially. In prac-
tice, for a workload like Place Lab’s, we anticipate the PHT
structure to stay mostly static and result in modi(cid:12)cations
3A direct comparison of PHTs to the performance of a cus-
tomized system such as Mercury [8] would be ideal; unfortu-
nately, the Mercury implementation is not yet available for
distribution.
s
e
s
s
e
c
c
a
T
H
D
f
o
%
8%
6%
4%
2%
0%
DHT Gets
DHT Puts
s
e
s
s
e
c
c
a
f
o
%
10
8
6
4
2
0
0
10
20
30
Node depth in PHT
40
0
10
20
30
40
Node depth in PHT
s
t
r
e
s
n
i
f
o
e
g
a
t
n
e
c
r
e
p
f
o
F
D
C
100
80
60
40
20
0
All inserts
Cache hits
Cache misses
0
2
4
6
8
10
Insert latency (seconds)
Figure 8: A plot of the accesses
(DHT puts and gets) to a PHT at
each tree level while inserting items
into the PHT. There are a total of
500,000 items in the PHT and the
block size is 1000.
Figure 9: A plot of the accesses
(DHT gets) to a PHT at each
tree level for a query workload.
The PHT contains 500,000 access
points and used a block size of
1000.
Figure 10: A cumulative distri-
bution function (CDF) plot of the
percentage of insert operations as a
function of the insert latency for a
PHT with 100,000 items and block
size of 500.
Data
size
5K
10K
50K
100K
Query
time (sec)
2.125
2.761
3.183
3.748
Block
size
10
50
500
1000
Query
time (sec)
6.048
4.524
2.521
3.748
Table 1: Variation in average query processing time
for di(cid:11)erent input data set sizes (for a block size of
1000) and for varying block sizes (for an input data
set size of 100K).
(and consequently, potential cache invalidations) only when
a new war drive is submitted into the system. Even then,
it is typically expected that most war drives are local to a
neighborhood and hence a(cid:11)ect only one portion of the PHT.
Thus with typical Place Lab usage, we expect the lookup
cache to provide signi(cid:12)cant improvement for insert latencies.
Query Performance: Next, we look at the performance
of the query workload. We pre-loaded the PHT with input
data sets of varying sizes (as well as varying PHT block sizes).
Table 1 shows the average query latencies as functions of the
input data set sizes and block sizes. As one would expect,
with larger data sets, queries take longer to resolve. However,
even a jump of a factor of 20 (from 5K to 100K) in the data
set size causes the query latency to increase only by a factor
of 1.76. This is due to the parallelism and the logarithmic
performance a(cid:11)orded by the PHT.
When we vary the block size, query latencies initially drop,
since larger blocks implies fewer operations and fewer PHT
nodes that need to be touched. However, if we keep increas-
ing the block size, query latency starts to go up again. This
is because at large block sizes, get() operations on PHT leaf
nodes potentially return more items than the query actually
matches. Note that this is a direct result of our decision to
implement PHTs entirely using a third party DHT service.
If we were to run PHT-speci(cid:12)c code directly within the DHT
nodes, we could have reduced this overhead by (cid:12)ltering val-
ues based on the query before returning them from the DHT.
Figure 11 shows a scatter plot of the query times for each
of the queries as a function of the query response size in
one run of our experiments (for an input data set of 100,000
elements and block size of 500). The graph shows the total
time for the query as well as the time taken for the (cid:12)rst
)
s
d
n
o
c
e
s
(
e
m
T
i
5
4
3
2
1
0
Total query time
Time for first response
0
2000
4000
6000
8000
10000
Query response size
Figure 11: A plot of the query response time (total
time and time for (cid:12)rst data item) as a function of
response set size for a PHT with 100,000 items and
block size of 500.
item of the results to reach the client.
In general, queries
with larger responses take longer. But even those queries
return their (cid:12)rst result within a second or so of issuing the
query. For this experiment, the median query response time
was 2.52 seconds while the median time for the (cid:12)rst set of
responses 0.8 seconds.
For our query experiments, the lookup cache did not pro-
vide as much bene(cid:12)t as for inserts. This is because we used
the cache only to perform the initial binary search, and query
latencies were dominated by the sub-tree traversal. That
said, it is easy to extend the lookup cache to apply to sub-
tree traversal as well, and thus help in improving the perfor-
mance of queries.
5.4 Effect of Linearization
Our PHT implementation uses z-curve linearization to con-
vert multi-dimensional indexes into a single dimension. As
an exercise, we compared how this linearization technique
compares with two other techniques, Hilbert curves and Gray-
coded curves [18]. The results of these experiments for an
input data set of 100,000 items and block size of 500 are sum-
marized in Table 2. Although Hilbert curves are theoretically
shown to have better clustering properties than z-curves, for
two-dimensional queries the bene(cid:12)ts are limited. Moreover,
the advantage of Hilbert curves in producing linearizations
with fewer non-contiguous segments while resolving a query
is not much of an issue for PHTs since the entire query is
processed in parallel starting at the top of a sub-tree of the
Linearization
Avg
depth
Block
Average # of
occupancy
gets
Z-curve
Hilbert curve
Gray code
18.417
18.424
18.42
39.526
39.063
39.526
per query
26.21
25.76
26.206
Table 2: Variation in PHT characteristics for di(cid:11)er-
ent linearization types.
PHT (unlike in the case of disk indexes where discontinuity
implies additional disk seeks).
Although we only experimented with PHTs for two di-
mensions, they can be extended to an arbitrary number of
dimensions. For high-dimensional data, more complex lin-
earizations such as the Pyramid-Technique [6] are known to
perform better. With some e(cid:11)ort, it should be possible to
adapt this linearization to use in conjunction with PHTs.
5.5 Handling concurrency
As we mentioned in Section 4.5, concurrent PHT opera-
tions can result in sub-optimal performance in the absence of
concurrency primitives in the DHT. In particular, we notice
three behaviors:
(cid:15) Multiple clients simultaneously split a full leaf node.
(cid:15) PHT leaf nodes (cid:12)ll up to larger than their block size
(because multiple clients attempt to insert an item into
the node at the same time).
(cid:15) Insertions are lost when for instance two clients simul-
taneously attempt to (cid:12)ll the last available slot in a
leaf node, one client succeeds, a third client then splits
the leaf node, and while that is in progress the second
client’s insert is lost.
We measured the frequency with which these behaviors
occur with concurrent operations. We ran an experiment
with 25 concurrent clients inserting data into a PHT (start-
ing with an empty PHT). Figure 12 shows a plot of the av-
erage number of duplicate splits that occur at each node
depth within the PHT. We note that contention happens
more often closer to the root of the tree. As the tree grows,
the number of unique leaf nodes increases and consequently,
race conditions for the same leaf decrease. We saw similar
behavior for the other two cases.
We then upgraded our DHT deployment to include sup-
port for the atomic test-and-set operation and re-ran the
above experiments. With this simple addition to the DHT
APIs, the PHT was able to operate correctly and no longer
exhibited any of the behaviors described above. One should
note though that even in the absence of concurrency primi-
tives in the underlying DHT, the above problems either only
cause (cid:13)eeting ine(cid:14)ciencies in the operation of the PHT or
can be repaired by the refresh mechanisms.
5.6 Dealing with churn
To evaluate the e(cid:14)cacy of the DHT for handling issues
of robustness, availability, and replication, we performed a
set of experiments where we introduced churn in the DHT.
Over one-minute intervals, we randomly killed an existing
DHT node or started a new node. For a DHT service (as
opposed to a client-based P2P system), this is admittedly
a rather high churn rate. We measured the e(cid:11)ect of the
churn on a query workload with respect to the percentage
of expected query responses that were lost due to churn.
Our results indicate that there is negligible loss in query
responses. Only 2.5% of the queries reported fewer results
than expected. Amongst these queries, most still reported
over 75% of the expected results. Only in two cases was the
loss greater, and this was because the total expected number
of results was quite small (fewer than 80 items). Moreover,
the data loss was temporary and was recovered as soon as
the DHT replication and recovery algorithms kicked in.
We also measured the latency overhead introduced as a
result of the churn. We de(cid:12)ne the churn overhead as the
ratio of the query response time with churn versus the re-
sponse time without churn. Figure 13 plots a CDF of the
percentage of queries as a function of the churn overhead. In
spite of the churn in the system, most queries show negligible
overhead, and only a small number of queries are a(cid:11)ected sig-
ni(cid:12)cantly and take much longer to respond. The overhead is
largely due to momentary increases in DHT routing latency
and replication overhead. Most of the queries that reported
fewer than expected items were exactly the ones that had
amongst the highest overhead. (On the other hand, some
queries performed faster under churn, that is largely an ef-
fect of the vagaries of Internet latencies.)
Finally, the true evaluation of the e(cid:11)ect of churn is how
it a(cid:11)ects the end-user application. Here, we reproduce data
from an experiment that we published in previous work [9].
This experiment demonstrates the e(cid:11)ect of data loss in the
PHT (due to large amounts of churn) on the accuracy of
client device location. We used a Bayesian positioning algo-
rithm (described in [9]) to estimate a user’s position. Even
under catastrophic failure that causes signi(cid:12)cant loss of Place
Lab’s mapping data, the application is resilient enough to be
able to handle this loss. Figure 14 shows that even with a
drop of availability to as low as 50%, we see negligible ef-
fect on positioning error. Thus, even ignoring the fact that
the DHT hides most of the e(cid:11)ects of churn from Place Lab,
when data does get lost Place Lab is capable of absorbing
the e(cid:11)ects of that loss with minimal observable e(cid:11)ects for
the user.
6. LESSONS LEARNED
Our experience with building Place Lab on top of Open-