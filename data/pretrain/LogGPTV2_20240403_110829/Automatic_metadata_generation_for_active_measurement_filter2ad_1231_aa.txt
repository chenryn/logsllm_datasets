title:Automatic metadata generation for active measurement
author:Joel Sommers and
Ramakrishnan Durairajan and
Paul Barford
Automatic Metadata Generation for Active Measurement
Joel Sommers
Colgate University
PI:EMAIL
Ramakrishnan Durairajan
University of Oregon
PI:EMAIL
University of Wisconsin-Madison
Paul Barford
comScore, Inc.
PI:EMAIL
ABSTRACT
Empirical research in the Internet is fraught with challenges. Among
these is the possibility that local environmental conditions (e.g.,
CPU load or network load) introduce unexpected bias or artifacts
in measurements that lead to erroneous conclusions. In this paper,
we describe a framework for local environment monitoring that is
designed to be used during Internet measurement experiments. The
goals of our work are to provide a critical, expanded perspective
on measurement results and to improve the opportunity for repro-
ducibility of results. We instantiate our framework in a tool we
call SoMeta, which monitors the local environment during active
probe-based measurement experiments. We evaluate the runtime
costs of SoMeta and conduct a series of experiments in which we
intentionally perturb different aspects of the local environment
during active probe-based measurements. Our experiments show
how simple local monitoring can readily expose conditions that
bias active probe-based measurement results. We conclude with
a discussion of how our framework can be expanded to provide
metadata for a broad range of Internet measurement experiments.
CCS CONCEPTS
• Networks → Network experimentation; Network measure-
ment;
KEYWORDS
network measurement, metadata
ACM Reference Format:
Joel Sommers, Ramakrishnan Durairajan, and Paul Barford. 2017. Automatic
Metadata Generation for Active Measurement. In Proceedings of IMC ’17,
London, United Kingdom, November 1–3, 2017, 7 pages.
https://doi.org/10.1145/3131365.3131400
1 INTRODUCTION
Active probe-based measurements have been widely used to eluci-
date Internet characteristics and behavior. Typical objectives for ac-
tive probe-based measurement include end-to-end path properties
(e.g., reachability, latency, loss, throughput), hop-by-hop routing
configurations and end-host performance. In each case, a sequence
of packets is sent from one or more measurement hosts to remote
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’17, November 1–3, 2017, London, United Kingdom
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5118-8/17/11...$15.00
https://doi.org/10.1145/3131365.3131400
targets and responses are measured either at the sending host or at a
target host. One of the benefits of active probe-based measurement
is that it enables broad and diverse assessment of Internet charac-
teristics without the need for permission or authorized access.
Despite the benefits and the availability of data sets through on-
going collection efforts (e.g., [10, 14, 27]), conducting active probe-
based measurement studies is fraught with challenges. Among
these is the possibility that the local environment can introduce
unexpected bias or artifacts in measurements. We define the local
environment as the host emitting probe packets plus other systems
in the local area that can materially alter the behavior of probe
packets but are intended to be outside of the scope of the mea-
surement objectives. In particular, the host emitting probe packets
is assumed to do so in a manner consistent with a measurement
protocol. However, prior work has shown that variable CPU load
can alter probe sequences [15, 30]. Similarly, hosts that share local
connectivity can disrupt probe packet streams by sending bursts
of traffic. So, how can we know if measurement fidelity has been
affected by the local environment? The answer we advocate is to
collect metadata about the local environment when measurements
are being conducted.
In this paper, we describe a framework for collection of metadata
about the measurement environment. Inspired by calls from the
community to collect metadata during experiments, our high-level
goals are to make the process easy and thereby improve the quality
and reproducibility of active probe-based measurement studies.
To that end, our design goals are to create a capability that will
(i) measure the local environment when an active probe tool is
being used, (ii) not perturb probe traffic, and (iii) work seamlessly
with different systems and measurement platforms. To the best
of our knowledge, this is the first attempt to address these meta-
measurement issues.
We develop a tool for metadata collection called SoMeta, which
addresses our core design goals. SoMeta is activated on initiation of
a probe-based measurement campaign. It collects key performance
metrics on the measurement host (e.g., CPU and memory utiliza-
tion) and performs simple probe-based measurements to hosts in
the local environment. SoMeta has been implemented in Python,
which makes it simple to run on diverse hosts. It has also been imple-
mented to be lightweight in terms of its demands on a measurement
host. SoMeta produces simple log files that can be analyzed for in-
dications of high load or other events that provide perspective on
unexpected observations in target measurement data.
We demonstrate the capabilities of SoMeta by deploying it on
two versions of the Raspberry Pi, which is used in the Ark [10]
active probe-based measurement project, and on large server-class
systems. We begin by examining the load imposed by SoMeta on
the host systems. We find that SoMeta imposes about 12% CPU
load on a Pi model 1, 3% on a Pi model 3, and only about 1% on the
IMC ’17, November 1–3, 2017, London, United Kingdom
Joel Sommers, Ramakrishnan Durairajan, and Paul Barford
servers in what we expect will be a typical configuration. Next, we
conduct a series of experiments in which we introduce artificial
load in the local environment while conducting active probe-based
measurements to a remote target using scamper [20]. The results
from these experiments show how SoMeta measurements highlight
the disturbances caused by the artificial load and how this could be
used to point out artifacts in the scamper measurements.
While we believe that SoMeta is an important step toward ac-
countability and reproducibility in active probe-based measurement
studies, it has certain limitations that we are addressing as part
of our ongoing work, including further reducing its performance
overhead, considering usage scenarios in shared measurement en-
vironments, and broadening the types of metadata that can be
captured, e.g., through in-network monitors. Moreover, there are
two additional ways in which the concept should be expanded.
First, the community needs to continue conversations about the
importance of metadata collection and the kinds of metadata that
should be collected to improve experiment reproducibility. Second,
deploying SoMeta (or something similar) in an existing infrastruc-
ture would enable better understanding of its performance and
how measurement artifacts and bias may be identified and possibly
corrected. To that end, all code and documentation for SoMeta is
readily available1.
2 RELATED WORK
Over the years, there have been a number of calls from within
the Internet measurement community to promote sound [25], hy-
genic [19], and ethical [24] processes when carrying out Inter-
net measurement studies to improve confidence in the results de-
rived from measurement data, to facilitate data sharing, replication,
and reappraisal, and to carefully consider any harm that may be
caused [1, 2, 19, 24, 25]. Our study finds inspiration in these prior
works, in particular with regard to collecting metadata to assist
a researcher with assessing the quality of the collected measure-
ments, and for scientific replication [19, 25]. Particularly related to
our work is Paxson’s suggestion to measure facets of the same phe-
nomena using different methods as a way to calibrate, assess mea-
surement quality, and to identify or possibly correct for bias [25].
This method was used in prior work to evaluate the fidelity of
measurements collected in RIPE Atlas [15], and a related analysis
of latency measurement quality on early-generation RIPE Atlas
nodes was done by Bajpai et al. [5, 6]. Holterbach et al. suggest
providing a “confidence index” along with reported measurement
results, indicating some measure of concurrent load on an Atlas
probe [15]; providing such an index could be facilitated through
the types of metadata gathered by SoMeta.
There have been a number of specific suggestions in prior work
regarding the scope of metadata that should be captured for fu-
ture reference, e.g., [12, 19, 25] and that metadata should be eas-
ily machine-readable [25]. Examples of available metadata (and
the associated data) can be found on Internet measurement data
repositories such as CRAWDAD [17, 18], IMDC [11, 28], and M-
Lab [13, 21]. Most of the metadata found through these platforms
are descriptive, e.g., where, when and how the measurements were
1See https://github.com/jsommers/metameasurement. In an effort to aid reproducibility,
scripts and data used to generate plots can be found by clicking on them.
collected, data format(s), etc., and some of these types of meta-
data are implicit in the data file naming convention (e.g., time of
measurement, measurement node name, measurement tool used).
While some metadata collected through SoMeta are descriptive of
the environment on which measurement tools are run, its main
focus is on gathering system performance data while measurement
takes place. In this regard, SoMeta bears some similarity to M-Lab,
in which limited measures of Planetlab slice performance metadata
gathered through Nagios [23] are available in a JSON format, such
as CPU and memory utilization.
3 DESIGN AND IMPLEMENTATION
3.1 Design Goals
The design of SoMeta is based on three objectives. First, metadata
should be collected by profiling various system resources at discrete
intervals during the time in which an active measurement tool
executes. In particular, CPU, storage/IO performance, and other
system measures should be gathered, and the access network should
be monitored, e.g., the first k hops of the network path. Profiling
should continue as long as the active measurement tool runs. When
the active measurement tool completes, metadata should be stored
in a simple and computer-readable format, e.g., JSON, in order
to facilitate analysis. Basic tools for analysis and visualization of
metadata should be provided to show, e.g., timeseries of idle CPU
cycles, packet drops on an interface, RTT to first router, etc.
The next design objective is the lightweight operation of SoMeta,
which we have designed to be configurably adaptable to a range
of target compute and network settings. Nonetheless, we are also
motivated by the fact that CPU power and network bandwidth to
the edge has increased to the point that the networking and compute
environment in which active network measurement is performed
can sustain additional traffic and processing activity from metadata
capture. For example, even low-cost computer systems (e.g., the
Raspberry Pi Model 3) have significant CPU power in the way of
multiple cores (4 in the case of the Pi 3).
The last design objective of SoMeta is the ability to work seam-
lessly with different systems and measurement platforms. This ob-
jective is imperative to accommodate diverse measurement efforts
e.g., CAIDA uses Raspberry Pi- and 1U server-based Ark moni-
tors [9], Yarrp uses Ubuntu VM [7], BGPmon uses sites with high-
end multicore processors [8], RIPE Atlas currently uses a low-cost
wireless router (TP-Link model TL-MR 3020) with custom firmware
based on OpenWRT [27], etc.
3.2 SoMeta Overview and Implementation
SoMeta has been implemented in a lightweight and extensible way
to meet the design objectives described above. It is written in Python
and uses the asyncio framework as the basis for structuring and
handling asynchronous events associated with monitoring the host