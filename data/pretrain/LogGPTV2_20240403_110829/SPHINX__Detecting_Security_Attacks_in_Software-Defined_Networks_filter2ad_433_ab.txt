SDNs, where these attacks are triggered in part due to the
intricacies of the SDN architecture, or the protocol involved
(i.e., ARP, LLDP, etc.). However, adapting traditional defenses
for these attacks in SDNs is non-trivial. This is because
traditional networks often rely on switch intelligence to im-
plement robust defenses against known security attacks. In
contrast, SDN switches are mere forwarding entities without
any intelligence. While patching SDN controllers to defend
against known speciﬁc vulnerabilities is possible, it is not a
comprehensive solution to detect all security attacks in SDNs.
EXAMPLES. In traditional networks, trustworthy veriﬁcation
of packets from neighboring switches to defend against LLDP
spooﬁng requires cryptographic mechanisms, which is a heavy-
weight solution. In fact, message authentication amongst hosts
and switches (even with TLS enabled) will not provide defense
against corrupt routing rules in SDN switches, as is the case
in the fake topology attack.
As another example, traditional networks defend against
ARP poisoning either leveraging Dynamic ARP Inspection
(DAI) [5], or requiring hosts to run programs like arp-
watch [33] to set up static mappings. DAI mandates that
switches snoop on all DHCP messages that pass through, and
use that information to (i) prevent a rogue DHCP server from
serving clients, and (ii) build a table of valid MAC to IP
associations to validate ARP packets as they pass through. In
contrast, SDN switches are dumb and cannot trivially extend
DAI, while host based defenses are not comprehensive enough.
Both the above examples are representative of the fact that
even simple and well-known defenses for attacks in traditional
networks cannot be trivially extended to SDNs in a controller
agnostic manner.
IV. SPHINX: OVERVIEW
A. Threat model
Attackers often break into the network to leverage internal
vantage points, and subsequently launch attacks on the internal
network. Since our goal
is to (i) verify onset of attacks
on network topology and data plane forwarding, and (ii)
detect violations of policies within SDNs, our threat model
focuses exclusively on scenarios where the adversary initiates
attacks from within the SDN. Thus, we model SDNs as a
closed system. Removing constraints on the unknown external
communication helps focus our analysis only on OpenFlow
control messages internal to the SDN.
We consider an enterprise SDN setup with no trafﬁc across
OpenFlow and non-OpenFlow network entities. We assume a
trusted controller (which is required for the correct functioning
of the network), but do not trust either the switches or the end
hosts. This implies that the switches can lie about everything
except their own identity, since the switches connect with
the controller over separate TCP connections (possibly with
TLS enabled). However, we do assume an honest majority
of switches in the network. All prior art, including [28]–[30],
[34], had assumed trustworthy switches, while SPHINX’s threat
model relaxes this requirement. Finally, given that most SDN
applications run as modules as part of the controller binary,
they can be trusted as long as the controller itself is trusted.
The assumptions above imply that OpenFlow communi-
cation from controller to switches is trustworthy, while from
switches to controller is untrusted, and could be forged by a
malicious switch or in some cases by hosts.
3
(a) Flow with SRC A and DST B.
(b) Flow graph for ﬂow A→B at t1.
(c) Flow graph for ﬂow A→B at t2.
(d) Flow graph for ﬂow A→B at t3.
Fig. 1: Example ﬂow, and construction of corresponding ﬂow graph.
B. Flow graphs
A ﬂow is a directed trafﬁc pattern observed between two
endpoints with distinct MAC addresses over speciﬁed ports.
A ﬂow graph is a graph theoretic representation of a trafﬁc
ﬂow with edges as the ﬂow metadata and switches being the
nodes in the graph. SPHINX uses these ﬂow graphs to model
both network topology and data plane forwarding in SDNs.
It gleans ﬂow metadata from OpenFlow control messages and
incrementally builds the ﬂow graphs to closely approximate
the actual network operations, thereby enabling validation of
all network updates and constraints on every ﬂow graph in
the network in realtime. Thus, ﬂow graphs provide a clean
mechanism that aids detection of diverse constraint violations
for both network topology and data plane forwarding in SDNs.
Flow paths are constructed only using FLOW_MOD messages
because they are issued by the trusted controller. Untrusted
STATS_REPLY messages from each switch only update ﬂow statis-
tics of the corresponding switch, and do not affect the ﬂow
graph structure. Hence, the ﬂow-speciﬁc network topology and
data plane forwarding state as embodied in the ﬂow graph
remains uncorrupted even in the presence of untrusted switches
and hosts. Further, as will be described later in § VI-B2, the
presence of an honest majority of switches along the ﬂow path
enables SPHINX to precisely detect any malicious updates to
ﬂow statistics at any switch in the ﬂow path.
As an example of the incremental construction of a ﬂow
graph, consider a ﬂow between hosts A and B as shown in
Figure 1a, that gets rerouted by the controller at different
time steps. Figures 1b, 1c and 1d depict the state of the
corresponding ﬂow graph at each reroute, with the current path
in black. The ﬂow is ﬁrst established at time-step t1, with the
path as S 1 → S 2 → S 5. At t2, the ﬂow is rerouted by the
controller along S 1 → S 3 → S 5, and the current path is
updated accordingly. Finally at t3, the ﬂow is rerouted once
more along S 1 → S 2 → S 3 → S 5. Note that expired nodes
and edges are never deleted from the ﬂow graph, enabling
SPHINX to accurately determine the updated current path
during reroutes. This allows for the possibility that a reroute
might not result in the issuance of fresh FLOW_MOD commands
to all the switches on the new current path, as is the case
during the reroute at t3 (where switches S 1 and S 2 receive
fresh instructions from the controller while S 3 does not).
Flow graphs exploit the predictability and pattern in both
topological and data plane forwarding inferred from con-
trol messages to detect attacks originating within the SDN.
While ﬂow graphs are an effective tool
to verify normal
Fig. 2: SPHINX ﬂow diagram.
and predictable network operations, they are limited in their
capabilities by the nature of messages sent over the control
plane and the dynamism in the topology. If there is a majority
of tampered or untrusted messages,
then ﬂow graphs will
perceive incorrect messages as normal behavior and not raise
any alarms. Further, if the network topology changes very
frequently,
then several of the learned invariants may be
violated, resulting in alarms.
C. High-level approach
KEY IDEA. SPHINX gleans topological and forwarding state
metadata from OpenFlow control messages to build incre-
mental ﬂow graphs and verify all SDN state in realtime,
including detection of security attacks on topology and data
plane forwarding (such as those listed in § III-A and later in
§ VIII) or violations of administrative policies. Any deviant
behavior is ﬂagged and reported.
Figure 2 shows SPHINX’s workﬂow, which involves three
stages. First, SPHINX monitors all controller communication
and identiﬁes relevant OpenFlow messages required to build a
comprehensive view of the network. Second, SPHINX analyzes
these OpenFlow messages and extracts topological and for-
warding state metadata to incrementally build a network graph
complete with trafﬁc ﬂows. Speciﬁcally, SPHINX maintains
topological and forwarding state metadata captured from (i)
incoming OpenFlow packet headers, (ii) outgoing ﬂow path
setup directives, and (iii) actual ﬂow trafﬁc measurements
over the network links, respectively. Third, SPHINX veriﬁes
the ﬂow’s current metadata against (i) a set of permissible
values of metadata gathered over the lifetime of a ﬂow,
and (ii) administrative policies. SPHINX ﬂags known attacks
using administrator-speciﬁed policies, while it leverages ﬂow-
speciﬁc behavior acquired over time to detect unforeseen and
potentially malicious activity.
SPHINX does not raise alerts when it discovers new ﬂow
behavior. Instead, SPHINX raises alerts when it detects un-
trusted entities triggering changes to existing ﬂow behavior,
or the ﬂow violates any administrator-speciﬁed security policy.
For example, SPHINX does not raise alarms when a switch
learns its neighbors. However, if any of the neighbors change
on any switch port, SPHINX will immediately ﬂag the incident
since it alters the network topology and subsequently the ﬂow
graph. Additionally, SPHINX will not raise alerts on ﬂow re-
routes since they are triggered by FLOW_MOD messages from the
trusted controller. This signiﬁcantly lowers alarms that may
be generated if detection of every new behavior is ﬂagged,
which is possible in evolving networks. Such suppression of
alerts also implies that any malicious activity that precedes
4
genuine ﬂow behavior will be treated by SPHINX as discovered
behavior, and will thus evade immediate detection. However,
the malicious activity will be detected retrospectively when
SPHINX later ﬂags the genuine behavior as suspicious, only to
be negated by the administrator.
EXAMPLE. SPHINX detects the fake topology attack as de-
scribed in § III-A by extracting metadata from OpenFlow con-
trol messages to maintain a view of the topology with all the
active ports per switch. SPHINX observes the FEATURES_REPLY
OpenFlow message to detect controller-switch connections and
port details per switch. SPHINX intercepts PACKET_IN messages
that contain LLDP payload, and extracts metadata to identify
valid links between switches in the ﬂow graph. It then val-
idates the extracted metadata against a set of acknowledged
invariants, such as (i) only a single neighbor is permitted per
active port at a switch , and (ii) links should be bidirectional.
This host-switch-port mapping enables SPHINX to detect fake
edges (from a single compromised switch or host) at the instant
malicious PACKET_IN messages are received by the controller.
However, two or more colluding switches or end hosts
may still poison the controller’s view by creating a fake
bidirectional link, thereby possibly altering the shortest routing
path between other hosts on the network. SPHINX can detect
such fake links by verifying data plane forwarding metadata
per-ﬂow, which captures the ﬂow patterns of the actual network
trafﬁc along a path in the ﬂow graph. Speciﬁcally, SPHINX uses
a custom algorithm to monitor the per-ﬂow byte statistics (by
intercepting STATS_REPLY messages) at each switch in the ﬂow
path, and determines if the switches are reporting inconsistent
values of bytes transmitted.
D. Why SPHINX works?
SDNs provide three key features that enable SPHINX to
precisely detect security threats in realtime.
(a) Ease of analysis: SDNs are less dynamic than the Internet,
and OpenFlow is much simpler than traditional communication
protocols. All
intelligence is centralized in the controller,
where the stream of all network updates is observable. This
signiﬁcantly eases analysis of control messages within SDNs.
(b) Action attribution: Action attribution in SDNs is much
easier than in traditional networks because of the centralized
controller that has global visibility and the large amount of
statistics available at the controller.
(c) Domain knowledge: If we do not consider SDNs to be
a black box, then we can leverage domain knowledge about
OpenFlow to develop a small, yet expressive, feature set that
captures the essence of all network communication. This helps
to easily detect changes in patterns of control messages.
V. SPHINX: DESIGN
SPHINX aims to provide accurate and realtime veriﬁcation
of network behavior by providing three key features. First, it
monitors all relevant OpenFlow control messages originating
from the switches or the controller. Second, it leverages a
succinct feature set that enables efﬁcient veriﬁcation of these
messages. Third, it uses a custom algorithm for fast validation
of network updates as they are processed by the controller.
Fig. 3: SPHINX architecture.
Src MAC/IP/port Dst MAC/IP/port Switch and in/out-port Flow match and statistics
Table 1: Feature set used to determine per-ﬂow metadata.
A. Intercept OpenFlow packets
SPHINX must intercept every network update to be able
to detect deviant behavior. Figure 3 presents a schematic
architecture of the system, which shows SPHINX as a shim
between the switches and the vanilla controller. An adversary,
i.e., an end host or a switch, can misuse only a subset of
all OpenFlow messages to poison the controller’s view of
the network. These messages include PACKET_IN, STATS_REPLY
and FEATURES_REPLY. In contrast, the trusted controller only
uses FLOW_MOD messages to direct
the switches to establish
connectivity between the endpoints. Thus, SPHINX actively
monitors just these four OpenFlow messages to extract relevant
metadata. All other messages are simply relayed through.
B. Build incremental ﬂow graphs
SPHINX analyzes the OpenFlow control messages men-
tioned above to incrementally build and update ﬂow graphs
corresponding to each ﬂow in the network. It then detects
attacks or violations in security policies by identifying tan-
gible changes in the network’s topological and/or data plane
forwarding metadata associated with every ﬂow graph.
There are three main entities in an SDN environment that
accurately characterize such metadata for each ﬂow in the net-
work, i.e., end hosts, switches and ﬂows. SPHINX extracts and
remembers the metadata associated with each entity to popu-
late a feature set described in Table 1. The source/destination
IP/MAC bindings provide a mapping for each host on the
network. The MAC/port bindings uniquely identify a ﬂow
between endpoints. The ﬂow match along with switch in- and
out-port determines the set of waypoints for a ﬂow in the
data plane. Lastly, ﬂow statistics provide bytes/packets trans-
ferred for every ﬂow. Additionally, SPHINX assimilates and
remembers this ﬂow-speciﬁc physical and logical topological
bindings for the end points, and the forwarding state speciﬁed
by FLOW_MOD messages at each intermediate switch in the ﬂow
path, to detect potentially malicious metadata updates.
SPHINX relies on four OpenFlow messages—FLOW_MOD,
PACKET_IN, STATS_REPLY and FEATURES_REPLY—to extract all rel-
evant metadata as observed at a particular switch and port.
Speciﬁcally, SPHINX determines onset of ﬂows and topological
information (host IP-MAC, MAC-port or switch-port bindings)
when switches issue a PACKET_IN. The desired paths to be taken
by ﬂows, and any subsequent updates, are determined when
the controller issues a FLOW_MOD for the switches. SPHINX uses
5
Feature
Subject
Object
Operation
Trigger
Description
(SRCID, DSTID), where ∀ SRCID and DSTID ∈ {CONTROLLER |
WAYPOINTID | HOSTID | ∗}
{COUNTERS | THROUGHPUT | OUT-PORTS | PACKETS | BYTES |
RATE | MATCH | WAYPOINT(S) | HOST(S) | LINK(S) | PORT(S) | etc.}
IN | UNIQUE | BOOL (TRUE, FALSE) | COMPARE (≤, ≥, =, (cid:44)) | etc.
PACKET IN | FLOW MOD | PERIODIC
Table 2: SPHINX’s policy language.
STATS_REPLY, which is received periodically from the switches,
to extract ﬂow-level statistics in the data plane,
including
packets/bytes transferred. SPHINX intercepts FEATURES_REPLY
to glean switch conﬁguration, including port status, when a
switch ﬁrst connects to the controller.
C. Validate network behavior
Veriﬁcation of constraints on network entities, resources
and ﬂow properties is performed by SPHINX’s policy engine.
In most cases, SPHINX can quickly verify the diverse effects
of all network updates on individual ﬂows by simply travers-
ing the ﬂow graph and inspecting the associated metadata
for conformance with application- or administrator-speciﬁed
safety properties. However, processing the entire ﬂow graph
on each network update is time consuming. Thus, SPHINX
caches the waypoints of the current path to determine if the
update satisﬁes the constraints or not. In case the network