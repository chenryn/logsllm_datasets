resolution of the simulated screen. The resolution of the
display is important when collecting screenshots because
many websites render differently for different screen sizes,
such as for mobile devices, tablets, small notebooks, or
large displays. In our case, we decided to ﬁx the resolution
to 1600×900 pixels, which is a display resolution often
found in budget and mid-range notebooks.
3.1.2 Window Extraction Techniques
For training the system, after collecting the screenshots,
we need to extract a representative window from each
screenshot so that we can train the neural network to detect
defacements. Various techniques can be used to extract
the representative window, which can be grouped into
deterministic and non-deterministic techniques. Hereinafter,
we discuss the trade-offs for four possible techniques: (i)
selecting the center window, (ii) selecting n non-overlapping
windows according to some measure (explained later),
(iii) uniformly selecting the window at random, and (iv)
randomly sampling the window’s center from a Gaussian
distribution for the x and y dimension separately.
Deterministic Window Extraction
The most straightforward deterministic technique is
to always extracts the window from the center of the
screenshot of the website. However, this makes evading
the system trivial. Generally, if an attacker can accurately
predict the window that will be extracted, he can force
the system to learn about defacements poorly, and, in
turn, deteriorate classiﬁcation performance drastically.
Therefore, such a simple technique is unsuitable for a
detection system in an adversarial context.
Alternatively, one can extract the window according
to some measure.
Identifying the most representative
window according to a measure (e.g., the Shannon entropy),
however, forces us to compute it for all possible windows
and then pick the top ranking one. In turn, for a 1600×900
screenshot and a 160×160 window, we would need to
evaluate over 1 million candidate windows for each sample
in the dataset. In total, for our dataset, this would require
over 13 trillion computations of the measure just to extract
the representative windows. Clearly, this is impractical.
Nonetheless, a deterministic selection strategy based
on a clever measure can increase the accuracy of the system,
and it can also be extended trivially to extract multiple
top-ranking windows at no additional cost. However, using
more than one window per sample increases the dataset
size by a factor of n and prolongs training time. Therefore,
n would have to be chosen carefully.
Taking into account the trade-offs the different determin-
istic extraction strategies bear (increased training/detection
time, ease of evasion, or computationally impractical)
and considering that a comprehensive evaluation of them
would require at least an order of magnitude of additional
experiments,5 we decided to select a non-deterministic
extraction strategy that follows intuition and is based on
user interface and user experience design principles instead.
This selection makes our classiﬁcation performance a lower
bound: other window extraction strategies might be more
accurate and/or robust, but (at the same time) they also incur
signiﬁcant additional cost at training and/or detection time.
5Performing these additional experiments would require at least 6
months just in computational time on our current GPU infrastructure,
which is why we decided against performing them.
USENIX Association  
24th USENIX Security Symposium  599
5
Non-deterministic Window Extraction
A straightforward non-deterministic strategy to extract a
window from a screenshot is to select it uniformly at random.
However, one cannot simply take any point from the
website’s screenshot as the center of the window. Instead,
it must be sampled so that the whole window contains
only valid data, forcing us to sample its center from the
interval [80,1520] for x and [80,820] for y (these intervals
are speciﬁc to the screenshot (1600×900) and window
size (160×160)). Therefore, pixels at the border have a
slightly lower probability to occur in a window than those in
the center. Although this is an unintended side effect, it has
negligible impact in practice because the center of a website
is more likely to be descriptive anyways. Alternatively, we
could create an “inﬁnite” image by wrapping the screenshot
at its borders, which would, however, yield artifacts because
we would combine parts of the top of the website with parts
of the bottom (and left and right, respectively), resulting
in windows that do not occur on the real website, which,
in turn, might disturb or confuse detection.
Alternatively to selecting the window’s center uniformly
at random, one can sample it from any other distribution, dis-
cretizing the sampled point. For instance, from a Gaussian
distribution to extract windows from mostly the center of the
screenshot, but not extracting from it exclusively. A focus on
the center of the website is often desirable because it is likely
to be more descriptive of the website’s look and feel. For
robustness, however, we also want to the system to not learn
exclusively from the center but to also learn about deface-
ments that occur at the border of the website. Therefore, for
our implementation, we extract a single window per website
with a Gaussian extraction strategy with µx = 800 and σx =
134.63975 for x and µy = 450 and σy = 61.00864 for y, so
that the windows at the border of the screenshot have a lower
probability to be sampled but are not ignored completely. If
x and y values outside of the screenshot are sampled, we sim-
ply resample the value for x or y respectively. We selected
the µ and σ values this speciﬁcally so that we sample values
outside of the screenshot only with likelihood 0.0001%.
3.1.3 Defacement Detection
After MEERKAT has been trained on a set of extracted
windows, it can detect if a website has been defaced.
Detecting website defacements with MEERKAT is
conceptually extremely simple:
1. We visit the website that we want to check with our
browser and we take a screenshot of the rendered website
(Section 3.1.1).
2. We apply a standard sliding window detection approach
on the screenshot we took to check if a part of the
screenshot is detected as being defaced, similarly to
prior work in image classiﬁcation [31].
3. If a window is detected to be a defacement by MEERKAT,
we raise an alert and inform the website operator that
his/her website has been defaced.
Note that MEERKAT does not compare a possibly-defaced
website to an older, legitimate version of it, and, thus, does
not need to analyze or store an older version. Instead, it
detects defacements solely by examining how the current
version looks like.
Exclusively to improve performance, instead of starting in
a corner of the screenshot, our system starts in the center and
moves outward. This behavior is motivated by the fact that
the center of the website is likely more descriptive, and our
training set was focused on the center region of the screen-
shots. This does not mean, however, that MEERKAT misses
defacements that are at the border of a website, they will
be detected when the sliding window reaches the actually-
defaced part, the border. The same is also true if a website is
only partially defaced: once the sliding window reaches the
defaced area, MEERKAT detects that the website is defaced.
Additionally, a special case worth mentioning is that
a legitimate website might show a large promotional
screen or an advertisement with the same intention of
a website defacer: attracting attention. In turn, such a
promotional screen might be similar in its look and feel
to that of a website defacement. While MEERKAT might
currently (theoretically) mislabel them as defacements,
our evaluation shows that they do not matter much (see
Section 4). Furthermore, if they start to matter at one
point in the future, it is straightforward to consider them:
the defacement engine can make use of an advertisement
blocker, and the website operator could whitelist the system
to not be shown any promotional screens.
3.2 Neural Network Structure
In this section, we brieﬂy discuss the design of our deep
neural network and how the different layers of the network
interact with the input image. The structure of our deep
neural network was notably inspired by prior work by Le
at al. [32], Krizhevsky et al. [33], Sermanet et al. [31], and
Girshick et al. [34]. We refer to them for further details.
The main components of our deep neural network are
autoencoders, which we stack on top of each other, and a
standard feed-forward neural network. Autoencoders are a
special type of neural network that are used for unsupervised
learning. The goal of an autoencoder is to ﬁnd a compressed,
possibly approximated encoding/representation of the input,
which can be used to remove noise from the input, or, when
autoencoders are stacked, they can learn high-level features
directly from the input, like where edges in an image are,
or if cats or human faces are part of an image [32].
Overall, the structure of our deep neural network is based
on the following idea: ﬁrst, we use a stacked autoencoder
to denoise the input image and learn a compressed
representation of both defaced and legitimate websites, i.e.,
we leverage the stacked autoencoder to learn high-level
features, similar to Le et al. [32]; second, we utilize a
feed-forward neural network with dropout for classiﬁcation,
similar to Krizhevsky et al. [33].
The initial layer of our stacked autoencoder is comprised
of local receptive ﬁelds. This layer is motivated by the need
to scale the autoencoders to large images [32, 35–38], this
layer groups parts of the image to connect to the next layer
600  24th USENIX Security Symposium 
USENIX Association
6
...
Defaced
Legitimate
1600x900x3
160x160x3
Screenshot Collection
(Section 3.1.1)
Window Extraction
(Section 3.1.2)
18x18x3
...
...
Local 
Receptive 
Fields
...
L2
...
Local
Contrast
Pooling
Deep Neural Network (Section 3.2)
Normalization
...
Feed-forward with 
Dropout
Figure 3: Architecture of our deep neural network.
of the autoencoder, instead of allowing the whole image to
be used as input to each node of the following layer. It takes
20,164 (1422) sub-images of size 18×18 as input, extracted
at a stride of 1 from the 160× 160 representative window
(see Figure 3; note that each pixel in each sub-image has
three dimensions for the three colors: red, green, and blue).
The second layer of our stacked autoencoder employs L2
pooling to denoise local deformations of the image and to
learn invariant features [37, 39–41]. Finally, the last layer
of our autoencoder performs local contrast normalization
for robustness [42].
The output of the stacked autoencoder is then used as
the input to a feed-forward neural network with dropout
that provides a 2-way softmax output. The 2-way softmax
output corresponds to the two classes that we want to detect:
defaced websites and legitimate websites. We use dropout
in our deep neural network to prevent overﬁtting of the
network, and to force it to learn more robust features by pre-
venting neurons to rely that other neurons of the network are
available (i.e., to prevent the co-adaptation of neurons) [43].
3.3 Fine-Tuning the Network’s Parameters
In an adversarial context, such as when trying to detect
if an attacker defaced a website, concept drift can be
introduced intentionally by the attacker and impede the
accuracy of the detection system drastically. Furthermore,
concept drift also occurs naturally, such as when the
style of defacements evolves over time in such a way that
the features cannot distinguish between legitimate and
defacement anymore. Therefore, concept drift can be a
severe limitation of any detection system, if it is not taken
into account and addressed properly (see Section 5.1).
MEERKAT can deal with concept drift in two different,
fully-automatic ways: ﬁne-tuning the network’s parameters
(adjusting feature weights), and retraining the entire network
on new data. While the latter is conceptually straightforward
and addresses all kinds of concept drift, it is computationally
very expensive. The former, on the other hand, allows us
to deal with some forms of concept drift gracefully and is
computationally much less expensive. However, it requires
some further attention: when ﬁne-tuning the neural network,
MEERKAT does not learn new features, but adjusts how
important the already learned features are. Therefore, ﬁne-
tuning cannot address major concept drift for which the al-
ready learned features do not model defacements accurately
anymore. Instead, when we ﬁne-tune the network’s param-
eters, we adjust the already learned weights of the deeper
layers of the neural network so that new observations of
defacements and legitimate websites are classiﬁed properly.
As such, ﬁne-tuning the network to maintain an accurate
detection performance requires no additional information
about the websites at all, but only defacements and legiti-
mate websites that were not part of the training set before.
Conceptually speaking, when ﬁne-tuning the network
given new defacements and legitimate websites, we search
for a better and, given the new data, more optimal set of
weights in the space of all possible weights. To do so more
efﬁciently, instead of initializing the weights at random,
we initialize them based on the previously-learned weights.
3.4 Implementation
Overall,
For this paper, we implemented a prototype of MEERKAT
using Python and the “Convolutional Architecture for Fast
Feature Embedding” (Caffe) framework by Jia et al. [44].
Caffe was used because of its high-performance and ease
of use, however, it does not offer all functionality that our
neural network requires and some modiﬁcations were made.
the general architecture of MEERKAT is
embarrassingly parallel: the screenshot collection engine
is completely separate from the detection engine except
for providing its input. For instance, to quickly collect the
screenshots of all websites, we utilized 125 machines (with
2 cores and 2 GiB memory each), and collection peaked
at about 300 screenshots per second. Similarly, once the
neural network has been trained, the learned parameters
can be distributed to multiple machines and detection can
be scaled out horizontally, and, although the system is
trained on a GPU, once trained, the detection engine does
not require a GPU and can run on common CPUs instead.
Training the system, on the other hand, is not parallelized
to multiple machines yet, but some clever tricks can be used
to reduce training time signiﬁcantly [33], which we leave
for future work.
3.5 Real-world Deployment
MEERKAT’s main deployment is as a monitoring service,
acting as an early warning system for website defacements,
to which a website operator subscribes with only the URL
at which his website can be reached. For each monitored
website, the system regularly checks, such as every few
minutes (or even seconds), that the website is not defaced.
If it detects it as being defaced, it notiﬁes the website’s
operator, who, in turn, depending on the conﬁdence in
the warning, manually investigates, or automatically puts
the website in maintenance mode or restores a known
good state. Acting as an early warning system, MEERKAT
USENIX Association  
24th USENIX Security Symposium  601
7
reduces the reaction time to defacements from hours, days,
and even weeks (see Section 2) down to minutes (or even
seconds), and, therefore, it reduces the damage inﬂicted
on the website’s operator by the defacement signiﬁcantly.
Furthermore, MEERKAT can also reduce human labor:
currently, Zone-H manually vets all submissions for
defacements [6], of which nearly two thirds are invalid.
MEERKAT automates this signiﬁcant amount of work.
4 Evaluation
We evaluate our implementation of MEERKAT in various
settings. However, ﬁrst, we provide details on what data our
dataset is comprised of, and how we partition it to simulate
various defacement scenarios.
Our evaluation scenarios are traditional and simulations
of real-world events, such as a new defacer group emerging,
or how the system’s accuracy changes over time, with and
without ﬁne-tuning the neural network.
In our experiments, a true positive is a website
defacement being detected as a defacement and a true
negative is a legitimate website being detected as legitimate.
Correspondingly, a false positive is a legitimate website
that is being detected as being defaced, and a false negative
is a defacement being detected as being legitimate.
4.1 Dataset
The dataset on which we evaluate MEERKAT contains
data from two different sources. First, it includes a com-
prehensive dataset of 10,053,772 defacements observed
from January 1998 to May 9, 2014; we obtained this data
through a subscription from Zone-H, but it is also freely
available from http://zone-h.org under a more re-
strictive license. From those defacements, 9,258,176 deface-
ments were veriﬁed manually by Zone-H [6]; the remaining
795,596 website defacements were pending veriﬁcation and
we do not include them in our dataset. Second, our dataset
contains 2,554,905 unique (supposedly) undefaced websites
from the top 1 million lists from Alexa, MajesticSEO, and
QuantCast.6 Note that we cannot be certain that the legiti-
mate websites in our dataset are not defaced, and since man-
ual veriﬁcation is impractical at such a large scale, the true
negative rate is actually a lower bound and the false positive
rate is an upper bound, correspondingly. In layman’s terms:
the system might be more accurate than our results suggest.7
To accurately evaluate the classiﬁcation performance
of MEERKAT in a real-world deployment, we report its
accuracy in three different scenarios:
O Traditional, to compare to prior work, i.e., by performing
10-fold cross-validation by sampling from all data
uniformly at random, so that each bin contains 925,817
defacements and 255,490 legitimate websites.
6We made a list of all 2,554,905 legitimate websites included in our
dataset available at http://cs.ucsb.edu/~kevinbo/sec15-
meerkat/legitimate.txt.bz2.
7Over 191,000 website in our legitimate dataset have been defaced at
one point in the past, thus, it is likely that some of them are actually defaced
and therefore mislabeled; thus, if classiﬁed correctly as a defacement by
MEERKAT, they appear as false positives in our results.
O Reporter, to simulate a new defacer emerging, i.e., by
performing 10-fold cross-validation on the reporters
of a defacement and including only their defacements
in their respective bin; legitimate website are sampled
from the legitimate data uniformly at random.
O Time-wise, to evaluate the practicality of our approach
in a real-world setting, i.e., we start by training the
system on all data from December 2012 to December
2013, and, then, we detect defacements from January
to May 2014. We report the system’s detection accuracy
for each month.
We evaluate our system in these settings to prevent a positive
skew of our results that might be the result of the different
evaluation method and how the dataset is composed. For
instance, a reporter of a defacement might introduce an
inherit bias to the distribution of the defacement by only
reporting the defacements of one speciﬁc defacer (such
as themselves), or there might be a bias in how defacements
and how the web evolved. Those potential pitfalls might
skew the results positively or negatively and must be
considered for an accurate comparison to prior work.8
Finally, to account for the difference in the number of