Observe that they are not distinguishable from the sizes of
natural backdoors. In the TrojAI multi-round competitions for
backdoor scanning organized by IARPA [42], regularly trained
clean models have a large number of small natural backdoors
whose sizes are not distinguishable from the injected ones in
the round 2 competition, which is a round dedicated to finding
backdoors in image classification models. As a result, many
performers suffered from a large number of false positives (i.e.,
reporting a clean model as poisoned). On the other hand, if a
pretrained model used the hardening technique proposed in the
paper and had the hardened class distances published as part
of the model specification, the substantially enlarged distances
would make stealthy poisoning attempts of the hardened model
easily detectable (Section V-C).
Existing Technique I: Adversarial Training. Adversarial
training is the most widely used model hardening technique.
It aims to train a subject model in such a way that sam-
ples in a Lp bound of each training input have the same
classification result, with the sacrifice of some classification
accuracy. It can move the decision boundary to make the
model robust. It can enlarge class distances. In fact, since
round 3 of the TrojAI competition, the red team (by NIST)
responsible for producing the clean and poisoned models for
performers to classify uses adversarial training to suppress
natural backdoors. In Figure 2, we show some backdoors for
an adversarially trained ImageNet model (downloaded from
existing work [48]) in the second column. We use the same
three class pairs as in Figure 1, with each pair taking two
rows. The natural backdoors are in the second column with
their sizes presented on the top of the backdoor images in red
in the odd rows. The backdoors are also enlarged in the even
rows. The classification confidence of a stamped sample (in
the third column) is depicted on the sample.
Comparing to the class distances in Figure 1, it is evident
that adversarial training can enlarge class distance (e.g, from
1058 to 1598 for the dog→cat pair). In addition, the backdoors
start to possess some human perceptible features. For instance,
the backdoor for dog→cat resembles a cat face and the
backdoor for turtle→bird has the beak of bird. However, due to
the nature of adversarial training, the accuracy has nontrivial
degradation (see Section V). Moreover, the order of class
distances still does not align well with our intuitions of the
two class pairs dog→cat and turtle→bird, indicating that it
may not have achieved the maximum class distances. More
discussion of the reasons of such insufficiency can be found
in Section III.
Existing Technique II: Universal Adversarial Perturbation.
While adversarial training generates adversarial perturbations
separately for each sample, universal adversarial perturbation
(UAP) aims to derive common adversarial perturbations for
a (large) set of samples from different classes such that
the derived UAPs can cause misclassification when they are
applied to any samples. Similar to adversarial training, UAPs
are usually derived using L∞. As such, a straightforward idea
is to use UAP to adversarially train a model to harden it
like in [43]. Figure 3 shows the results. Observe that the
derived UAPs in column 1 have noise-like pixel patterns.
UAPs can enlarge class distances (e.g, from 1058 to 1118
for the dog→cat pair) to some extent. However, the distance
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
31374
615InputGenerated BackdoorInput + Gen. BackdoorTargetInputGenerated BackdoorInput + Gen. BackdoorTargetBenignPoisoned1058699745876SizeInjected Backdoor741SizeFig. 3: Universal adversarial perturbation. The 1st column
shows the generated UAPs for three different victim classes in
the 2nd and 3rd columns. The last column presents the sizes of
backdoors generated for a hardened model by UAP training.
effectively enlarge class distances without sacrificing much
accuracy. We call it model orthogonalization (see Section III).
Different from adversarial training that independently perturbs
individual inputs, our training considers individual pairs of
classes. Specifically, for a pair such as a and b, it derives a
minimal backdoor from a to b and another backdoor from b
to a. Intuitively, the former can be considered the distinctive
(low level) feature of b with respective to a, and vice versa.
It
then stamps samples with these backdoors and ensures
the classification results do not change, which essentially
expels these low level features and forces the model to learn
high level and more semantic features distinguishing the pair.
The two directions of a pair are hardened together, which
we call the symmetric hardening. As will be discussed in
Section IV-B, asymmetric hardening, i.e., hardening only one
side or hardening the two sides in separate batches, leads to
inferior results. Considering each class pair uniformly (with a
total of O(n2) pairs for an n-class model) is not cost-effective
as different class pairs have different distance capacities. Some
pairs quickly reach their maximum distance (e.g., cat and dog)
and others need more training to get there. We hence develop
a scheduler that schedules the most promising pairs for each
batch, substantially improving cost-effectiveness. The training
also features a few special designs such as gradually growing
optimization bounds and reusing backdoors to speedup the
process. Details are in Section IV. The fourth and fifth columns
of Figure 2 present example results using our training (on
the adversarially trained model). The class distances are sub-
stantially enlarged without further accuracy degradation. The
distances are much larger compared to the regularly trained
model in Figure 1. With hardened distances, the data poisoning
in Figure 1 can be easily detected, as the compromised classes
would have much smaller distances than the hardened ones.
Further, we observe that the order of the distances of the
three pairs aligns well with our intuition after hardening. The
zoomed-in trigger patterns in the even rows clearly exhibit
high level semantic features of the target classes (e.g., the
whole body of a bird). In other words,
learns
high level features of individual classes, which substantially
mitigates its vulnerabilities to (natural) backdoors.
the model
Fig. 2: Adversarial training vs. our training. The 2nd and 4th
columns present the generated backdoors for an adversarially
trained model and our model, respectively. The patterns high-
lighted in red boxes are enlarged in even rows. The sizes of
backdoors are marked on the top in red. The numbers in 3rd
and 5th columns are prediction confidences to the target labels.
enlargement is small. This is because UAPs are untargeted,
meaning that a UAP for a victim class may flip samples
of the class to different target classes. Also note that due
to its use of L∞ bound, targeted UAPs are very difficult
to derive. They either cannot be found within the bound or
the enlarged bound leads to substantial accuracy degradation
after hardening. Natural backdoors, on the other hand, do not
have any constraints on the magnitude of perturbations. They
can easily achieve a high ASR for a specific target, which
can reveal vulnerabilities of the decision making between two
classes of a subject model.
Technique III: Directly Using Generated Backdoors in
Training. Another method is to directly use optimization to
derive the smallest backdoor between a pair of classes, e.g.,
by adapting optimizations in NC [40] and ABS [41]. The
training is enhanced such that any samples stamped with the
backdoor should retain their ground truth classifications. As
we will show in Section V, such a method is expensive as its
complexity is quadratic. In addition, its training loss fluctuates
a lot (see Section IV-B), causing inferior results in distance
improvement compared to ours (with 30% performance differ-
ence as shown in Section V-D). Note that the Pairwise baseline
evaluated in Section V is already more sophisticated than
simply using generated backdoors in hardening as described
above. It leverages symmetric hardening and speedup methods
discussed later in this section and in detail in Section IV.
Our solution. We propose a novel training method that can
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
41375
InputTarget236819812122109115981168Backdoor for Adv ModelBackdoor for Our Model0.98730.65870.95600.60380.26110.7771 0.8947 0.99740.99990.9996 0.98840.8523InputUAP (×10)Input + UAPTarget767Size8721118Fig. 4: A conceptual illustration of standard vs. adversarial vs.
our decision boundaries. Green and blue dots are samples of
two classes. The black straight line is the standard decision
boundary separating these two sets of samples. The orange
arrows in (a) and (c) denote the backdoor transformation that
can move a set of samples to the other side of the boundary.
The red curly line in (b) is the adversarial decision boundary.
The red line in (c) is the decision boundary after our training.
III. PROBLEM DEFINITION
Intuitively, a backdoor from a victim class to a target class
is a (uniform) transformation function that transforms an input
of the victim class such that it is classified to the target class by
the subject model. A backdoor ought to be stealthy otherwise
it can be easily detected by automatic techniques or humans.
For instance, replacing majority of a victim class input with a
target class input likely causes the intended misclassification.
However, such a “large and obvious” backdoor can hardly
constitute a meaningful attack. The level of stealthiness is
proportional to the magnitude of the transformation. One way
of measuring such magnitude is to use Lp distance between
the original sample and its transformed version.
As demonstrated in Section II, even clean models are
vulnerable to natural backdoors. Such backdoors are so small
that stealthy attacks can be easily conducted. In these attacks,
the Lp distances introduced by the (uniform) transformations
are small. Adversarial training can enlarge the Lp norm of
the needed backdoor transformation but it cannot achieve the
maximum distance. Intuitively, we define the Lp norm of the
minimum backdoor from the victim class to the target class
their class distance. Note that such distance is directional
and usually asymmetric, that is, the distance from a to b is
different from that from b to a. Our goal is hence to enlarge
class distances such that models become less vulnerable to
backdoor attacks without sacrificing classification accuracy.
Figure 4(a) illustrates the intuition. The blue and green
points are samples of two classes and the black line denotes
the decision boundary by the subject model that achieves the
minimum cross-entropy loss. The orange arrows denote the
backdoor transformations. Intuitively, most the green points
undertaking the displacement denoted by the arrow starting
from the green star (a.k.a. the center of mass of green points)
fall into the other side of the decision boundary. Similarly,
most the blue points can be moved to the other side by the
arrow starting from the blue star. Observe the two arrows
have different
lengths. Figure 4(b) shows that adversarial
training allows the Lp ball of each sample to have consistent
classification results. Consequently, the decision boundary is
altered as shown by the red line. It enlarges the arrow lengths.
Figure 4(c) shows our technique. The red curve denotes the
new decision boundary after our training, with which the solid
arrows are much longer than before (the dashed arrows). We
say a classifier is orthogonal when all its class pairs have
the maximum distance and the training to achieve this effect
model orthogonalization. Intuitively, as shown by Figure 4(c),
in order to maximize both the distances from a to b and from
b to a, the decision boundary tends to be perpendicular to the
line linking the two centers, namely, orthogonal.
An important observation is that maximum class distances
are bounded and different class pairs may have completely
different distances. As demonstrated in Section II, the maxi-
mum distance from cat to dog is smaller than that from turtle
to bird. This is determined by the nature of these classes.
As such, while we can use special training methods like the
one proposed in this paper to enlarge such distances, the
enlargement is bounded by the natural differences of classes.
Next, we formally define these concepts. Considering a
typical classification problem, where the samples x ∈ Rd
and the corresponding label y ∈ {0, 1, . . . , n} jointly obey
a distribution D(x, y). A classifier M : Rd → {0, 1, . . . , n}
with parameter θ is supposed to satisfy the following property
arg maxθ P(x,y)∼D[M(x; θ) = y].
Definition III.1. Given two classes a and b, the backdoor from
a (the victim class) to b (the target class) is a transformation
T : Rd → Rd that transforms any sample of a, denoted as
for all (x, y = a), such that M(T(x); θ) = b. A backdoor is
stealth if ∥T(x) − x∥ is small.
A backdoor could be natural (when the model is normally
trained) or injected (e.g., by data poisoning [1], [3]–[5]). A
backdoor may be dynamic, meaning that the perturbations
are different for different inputs (e.g., feature space back-
doors [46]), or static, meaning that the perturbations are input
agnostic, like patch backdoors [1]. In this work, we focus on
static backdoors.
Definition III.2. Given two classes a and b, the distance from
a to b, denoted as ∥a → b∥, is minT(E(∥T(x) − x∥)) with
(x, y = a) a sample of a and T a backdoor from a to b.
Intuitively, class distance is determined by the smallest
backdoor. In practice, the expectation E(∥T(x) − x∥) is ap-
proximated by a set of samples. Note that for static backdoors,
∥T(x) − x∥ is constant for different samples (i.e., only a
property of the two classes).
Our definition of class distance differs from existing class
separation notions [49], [50] in two ways: (1) the distance
measure in this work is carried out for a set of samples,
which measures the distance from the center of sample mass
to the decision boundary as shown in Figure 4(c), whereas the
measures in existing works are performed for every sample
to obtain the smallest distance from a sample to the deci-
sion boundary (the margin). The notion of class distance is
intrinsically related to the underlying trigger inversion method,
which is modular, meaning any suitable inversion method
can be leveraged for measuring class distance; (2) our class
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
51376
(a) Natural Training(b) Adversarial Training(c) Our Trainingdistance does not measure the robustness of models under
input-specific adversarial attacks. As class distance does not
consider the smallest distance for every sample, the hardened
model does not defend against adversarial examples. As shown
in Figure 4(c), there are blue and green dots that are close
to the decision boundary (after our hardening) and hence the
model may still be vulnerable to traditional adversarial attacks.
We consider the two kinds of measures complementary.
Definition III.3. Model orthogonalization aims to derive a
classifier M : Rd → {0, 1, . . . , n} with parameter θ such that
arg maxθ P(x,y)∼D[M(x; θ) = y] and arg maxθ
(a,b) ∥a →
b∥.
(cid:80)
Intuitively, model orthogonalization aims to derive a clas-
sifier that has the highest accuracy and the largest aggregated
class distance. While the first condition can be ensured by
a cross-entropy loss, the second condition cannot be easily
represented as a loss function as it entails deriving the distance
of each pair of classes, having quadratic complexity. In the
following sections, we explain the design of MOTH.
IV. DESIGN
A. Overview
Figure 5 illustrates the whole training procedure. Given a
pre-trained model and training data, MOTH first initiates a
warm-up phase, where each class is treated as the target class
for generating universal backdoors (the top rectangle). That is,
for a set of samples not in the target class, we aim to generate a
backdoor that can flip their predictions to the target class. Dur-
ing the optimization process for each target class, the changes
of loss for different source classes are recorded for updating
a prior-selection matrix (the bottom rectangle). Details are
discussed in Section IV-C. Once all the classes are warmed up,
we start the main training process. This phase consists of four
steps as shown in Figure 5. Based on the prior-selection matrix,
step 1 uses a K-arm scheduler for selecting a promising class
pair that will likely have the largest class distance increase.
Assume the model has N classes. We create N × (N − 1)/2
arms (i.e., all the pairwise combinations without direction).
Intuitively, an arm represents the optimization objective for
a pair. The scheduler then selects the most promising arm.
In step 2 , MOTH applies a symmetric backdoor generation
algorithm (for the selected pair) to yield two backdoors, which
are stamped on samples of the classes. The modified training
batch is used for updating model parameters according to the
cross-entropy loss. The entries for the selected pair in the post-
selection matrix are updated in step 3 to record the distance
variations. For the next training round, we rely on a reward
function (see Section IV-C) that combines both the prior-
selection and post-selection matrices for class pair selection
(step 4 ). MOTH iterates this process. It terminates when the
model accuracy degradation reaches a preset bound. Finally,
it outputs the hardened model that has larger class distances.
We discuss individual components and algorithms in detail in
the following sections.
Fig. 5: MOTH overview
B. Symmetric Hardening
For a class pair a and b,
there are two directions for
generating backdoors. The goal of class distance hardening is
to enlarge distance by training with the generated backdoors.
A straightforward design is to leverage a backdoor generation
method (e.g., NC) to produce a backdoor for one direction
(e.g., from a to b), and stamp it on the samples of class a
for training. In the next iteration, it randomly chooses another
pair direction for training. However, such a design, we call
it asymmetric hardening, can lead to undesirable oscillation
of class distances during training. Figure 6b shows the class
distances of airplane and deer using such a design. The x-
axis denotes the training iteration, and the y-axis denotes the
class distance. Observe that between iteration 250 and 400,
the class distance from airplane to deer decreases while the
other direction continues to grow. We show a closer look
for this period in Figure 6c. It is evident that asymmetric
hardening causes the two directions of a pair competing and
leads to oscillation of class distances. As such, we propose
a symmetric hardening method, where we harden the two
directions of a pair simultaneously. In Figure 6a, observe
that the symmetric hardening has continuous growth for both
directions. We also evaluate class distances of all pairs using
symmetric hardening and asymmetric hardening, and compare
them with those of the original model. Figure 7 illustrates the
differences of class distances for the two hardening methods.
The heat map on the left is for symmetric hardening and the
right for asymmetric hardening. Each cell in the heat map
denotes the class distance improvement from a source class
(row) to a target class (column). The brightness of the color
denotes how much of the class distance is increased compared
to the original model (the brighter the larger). We can clearly
see bright colors for most cells in the symmetric heat map.
The asymmetric heat map has many cells with dark colors. In
addition, the distances from other classes to a class may have
large improvement (e.g., the penultimate column of the ship
class), but not the other direction (e.g., the penultimate row).
As illustrated in Figure 4(c), the hardening procedure aims to
push the decision boundary towards the opposite side. If only
one direction is hardened at one time, the decision boundary
will skew towards one side. It is hence pushed back and forth
through multiple rounds, causing oscillation as demonstrated
in Figure 6b.
Many existing backdoor generation methods follow an op-
timization procedure like NC [40]. It aims to flip a set of
samples from a victim class to the target class, which can be
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
61377
Input Model&DataWarm-upModel OrthogonalizationOutput ModelScheduler①②③④(a) Symmetric
(b) Asymmetric
(c) Zoom in for asymmetric
Fig. 6: Symmetric versus asymmetric hardening
Fig. 7: Symmetric versus asymmetric
hardening on all class pairs
Algorithm 1 Two-sided Backdoor Generation
1: function BIGENERATION(model M, data X, label (a, b), class p, initialization
(minit, δinit))
L(cid:0)M(x′), yt
(cid:1) + λ · ∥m∥1,