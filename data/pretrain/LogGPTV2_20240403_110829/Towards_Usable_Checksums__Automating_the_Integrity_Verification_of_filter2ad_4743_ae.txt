com/technologies/overview/content_management/all; Last visited: March 2018), about
51% of all websites are powered by a CMS and 31% of all websites are powered by
WordPress.
Session 6D: Usable 1 CCS’18, October 15-19, 2018, Toronto, ON, Canada1264CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Mauro Cherubini et al.
spend more time fixating) in the part of the user interface where
these sequences were displayed.
The experiment was split in two phases. During the first phase,
we asked participants to verify manually the checksums of four
downloaded apps (this was addressing RQ3 and RQ4). In the second
part of the experiment, we activated a browser extension that took
care of verifying the integrity of the downloaded files based on
their checksums (this was addressing RQ5). We did not counter-
balance the presentation of these two parts for two reasons: (i) the
second part of the experiment made explicit what the core of the
experiment was and could have biased the results of the first part;
(ii) the two parts of the study addressed different research questions.
However, this design also had drawbacks that we report below in
Section 7.4. The experiment was approved by our institution’s ethics
committee.
7.1 Participants
We recruited the participants of our experiment from a student
population through flyers displayed on two university campuses
(i.e., UNIL and EPFL in Lausanne, Switzerland). To sign up for
the experiment, potential subjects had to fill an online screening
questionnaire first. In this questionnaire, they were asked about
their basic demographic information (age and gender), major field
of study, knowledge of checksums (i.e., “Do you know how and
what for the elements circled in red on the following screenshots
are used?20 If yes, please describe it briefly in the text box below.”),
tech savviness (i.e., “Check the technical terms related to computers
that you understand well: ad-blocker, digest, firewall, VPN, etc.).
Finally, we asked which was the OS of their main computer.
We selected a total of 40 subjects (out of the 120 who completed
the screener) and invited them to participate in the experiment.
We number of participant was chosen so that it provides sufficient
power to the statistical tests and keeps the total duration of the
experimentations reasonable (we had only one eye-tracker). The
sample was selected to maximize diversity. About half of the partic-
ipants were macOS users (i.e., 21/40, that is 53%) and half Windows
users (the actual breakdown in terms of operating systems among
the participants who filled the screener was 56% macOS, 41% Win-
dows, 3% Linux). The subject pool included 40% of female subjects
and it was diverse in terms of major fields of studies, with more than
15 different majors represented. The average age of the subjects was
22.5±2.9. Out of the 40 subjects, 12 (30%) knew about checksums,
33 (83%) downloaded programs from developers websites and 20
(50%) from app stores, and 25 (63%) had an antivirus installed on
their computers. The experiment took approximately 50 minutes
per person to complete and the participants were compensated
with CHF 20 (∼USD 20). The whole experiment was conducted in
French (i.e., the local language in Lausanne).
7.2 Apparatus
The experiment took place in a UX-lab, a small room with a desktop
computer. The computer was equipped with an eye-tracking system
(maker Tobii, model X2-6021) which was sampling gaze at 60Hz.
Two cameras and a few microphones were also placed in the room
to record the experiment.
Depending on the OS the participant was most familiar with (ei-
ther macOS or Windows), we switched the computer that was used
by the participants during the course of the experiment. Aside from
the OS, the employed apps and the layout of the windows were the
same on the two different OSes. Three windows were placed and
arranged on the screen: the web browser (Chrome) that occupied
the left half of the screen, the “Downloads” folder (Windows ex-
plorer/macOS finder) that occupied the top right quadrant, and the
terminal that occupied the bottom right quadrant (see Figure 3).
Participants were asked to not change the position of the three
windows, and scrolling was disabled in the browser in order to
reduce shifts in the areas of interest (AOI) of the screen that were
displaying the checksums.
All necessary pages were pre-loaded in the browser window
in different tabs. We tampered with the checksum on the third
webpage (i.e., Transmission) for the first part of the study and the
second webpage (i.e., Audacity) for the second part of the study. All
the other checksums were correct. Based on our running hypothesis
that users check only the first and last digits of the checksum, we
changed the 44 digits (out of 64) in the middle of the checksums; this
means that only the first and last 10 digits remained unchanged.22
7.3 Procedure
First and foremost, we informed the participants that they would be
recorded during the course of the experiment (and about our data
management plan, including data anonymization and retention)
and we asked them to sign, if they agreed, an informed consent
agreement. We told the participants that we were conducting a
study on the way people download applications on their computers
and that they had to download several applications on the lab com-
puter. We asked the participants to behave as if they were using
their own computer and we told them to not hesitate to call the
experimenter in case of doubts or problems. We also explained
that the experimenter had nothing to do with the design and im-
plementation of the extension, therefore, the participants could
freely express negative opinions without the risk of affecting the
experimenter.
Next, we asked participants several preliminary questions,
mainly to confirm some of the information they provided in the
screener: the OS of their computer, whether they had an antivirus
installed and whether they downloaded apps from the Internet from
places other than official app stores. Then, we asked the partici-
pants to sit at the computer, and a 13-point calibration procedure
for the eye-tracking system was completed. Finally, the participants
were given a checklist containing the steps to follow during the
session. All materials were prepared in French and the sessions
were conducted in French, which the main language spoken where
the study was conducted.
First Phase. We asked the participants to download from the
official website and execute/install four different programs (in this
20The screenshot depicted the official VLC download page with checksums circled.
21https://www.tobiipro.com/product-listing/tobii-pro-x2-60/
22We considered, as in [39], that a realistic adversary can forge, through brute-force,
a corrupted program in such a way that the first and last 10 digits of its checksum
match those of the original program’s checksum.
Session 6D: Usable 1 CCS’18, October 15-19, 2018, Toronto, ON, Canada1265Automating the Integrity Verification of Web Downloads for the Masses
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Figure 3: Screenshot of the window arrangement on the computer used for the experiment (macOS). The left half of the screen
is occupied by the Chrome web browser in which multiple tabs have been pre-opened: the download pages of the first four
programs, the extension tab to activate the extension, the download pages of the next two programs, and the questionnaire
website (Qualtrics) for the exit survey. The right half of the screen is occupied by the terminal application where the partic-
ipants must type the command lines to compute the checksums of the downloaded programs (bottom) and the “Downloads”
folder (top) were the programs downloaded from the browser are placed; the participants had to click on the icons of the
downloaded programs (in that window) to execute them.
specific order): VLC, Handbrake, Transmission, and Android Studio.
Specifically, for each application, the participants were asked to
As explained in the previous subsection, the checksum of the third
webpage, i.e., Transmission, was set to be incorrect.
(1) Download the application. For the sake of simplicity, the
download webpages were already opened in individual tabs
of the web browser.
(2) Compute the checksum of the downloaded program and com-
pare it to that specified on the webpage. The participants
were provided with the exact command to type in the termi-
nal, e.g., clear ; shasum -a 256 Handbrake-1.1.0.dmg
for macOS.23 All the checksums were SHA-2 with 256 bits.
(3) Run the program and report some information on the in-
struction leaflet: program version and copyright years found
in the “About” box (macOS) or digital certificate issuer (Win-
dows). The purpose of this last step was to avoid calling too
much attention to the checksum verification as being the
core of the experiment.
23The clear command is used to ensure that the checksum is always displayed at
the same location on the screen, for eye-tracking purposes.
Second Phase. We asked the participants to activate the extension
(by clicking on a button in the fifth tab of the browser), and to
download and run/install two additional applications i.e., RealVNC
and Audacity, in this order. We asked the participants to perform the
same steps as in the first phase, except from the manual checksum
verification that was automated by our browser extension. The
first application’s checksum was correct, resulting in the display
of a confirmation message by the browser extension (see Figure 1),
whereas the second one was incorrect, hence resulting in the display
of a warning message (see at the bottom right of Figure 2). The
terminology used in the messages was inspired by the instructions
found on the download pages of popular programs (e.g., Ubuntu).
We recognize that it could be improved with a better design and
with user feedback. We intend to revise the design of the extension
in the future.
Session 6D: Usable 1 CCS’18, October 15-19, 2018, Toronto, ON, Canada1266CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Mauro Cherubini et al.
(a) Thorough verification of a correct checksum (b) Succeeded verification of an incorrect checksum
(i.e., mismatch detected)
(c) Failed verification of an incorrect checksum
(i.e., mismatch not detected)
Figure 4: Sample subject gaze heat maps captured by the eye-tracking system on macOS.
For the last step, we asked the participants to fill a short ques-
tionnaire online to get some feedback about their perception of the
manual verification of checksums and of the browser extension,
satisfaction with the extension and net promoter score.
7.4 Results
We describe and analyze the results related to the manual verifi-
cation of checksums (first phase), and then report on the usability
and effectiveness of the browser extension (second phase).
In order to study the gaze behavior, in our analysis, we sur-
rounded the parts of the UI that displayed the checksums, and we
labelled each area. These were the AOIs described prior in the text.
Unfortunately, we had to remove eye-tracking recording for one
participant due to corrupted data.
Figure 5: Areas of interest used for the checksums displayed
in the terminal.
RQ3. From a qualitative analysis of the fixation heatmaps of the
participants looking at the AOIs that contained the checksums, we
could observe three distinct behaviors: (a) some participants pro-
duced extensive fixations throughout the sequence of characters
(i.e., the checksum) covering most/all of the sequence; (b) other
participants produced less fixations but still “sampled” the sequence
at several points from beginning to end; (c) finally some other par-
ticipants produced fewer fixations in the AOIs, typically pointing to
the beginning and the end of the sequence. Examples of these three
behaviors can be seen in Figure 4. While the first two behaviors
typically led to identifying the incorrect checksum, the third was
typically associated with not identifying the incorrect checksums.
This was confirmed by our quantitative analysis presented below.
To understand whether all the digits of the checksum were
treated equally by the participants, we further subdivided the area
where the checksum is displayed in four sub-AOIs (see Figure 5)
and measured differences of the total number of fixations falling in
each of these areas. As the assumptions for parametric inferential
statistics were violated, we used nonparametric statistics for the
subsequent quantitative analysis.24
24Concerning the total number of fixations, the Shapiro-Wilk normality tests were
close to rejection: AOI 1 - (W = .95, p = 0.085), AOI 2 - (W = .94, p = 0.027),
Figure 6: Boxplots of the distribution of user fixations across
the four areas of interests covering the checksums.
AOI
1
2
3
4
1
-
-
-
-
2
-
-
-
445**
756***
709***
3
-
-
773***
688***
518***
4
-
*p < 0.05, **p < 0.01, ***p < 0.001
Table 2: Wilcoxon signed rank tests of the number of fixa-
tions within the four areas of interest covering the check-
sums. Due to ex aequos in the data, the p-value is an approx-
imation.
We conducted a Friedman test of differences among repeated
measures to compare the total number of fixations that fell in each
of the four sub-AOIs. There was a significant difference in the
scores: AOI 1 - M=25.15, SD= 13.11, AOI 2 - M=21.92, SD= 13.96,
2(3)
AOI 3 - M=13.92, SD= 9.55, and AOI 4 - M=10.58, SD= 6.99; χ
= 77.32, p < 0.001. Six Wilcoxon signed rank tests with continuity
correction were conducted to make post-hoc comparisons between
AOIs. All the tests indicated that there was a significant difference
between the number of fixations falling in each AOI. We include
the detailed results of the tests and the boxplot of the distribution of
fixations for each AOI, in Figure 6 and Table 2. These results suggest
that the attention given to the digits of the checksum is highest at
AOI 3 - (W = .94, p = 0.037), AOI 4 - (W = .92, p = 0.008) and the assumption of
homoscedasticity was violated when using the Modified Levene’s Test (F = 6.23, p <
0.001). The conclusion was similar for the total dwell time.
●●AOI 1AOI 2AOI 3AOI 40102030405060Area of InterestNumber of fixationsSession 6D: Usable 1 CCS’18, October 15-19, 2018, Toronto, ON, Canada1267Automating the Integrity Verification of Web Downloads for the Masses
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
the beginning and decreases as we progress in the sequence. This
means that a partial pre-image attack should focus on keeping the
first digits of the checksum unchanged.
RQ4. We observed that 15 (38%) of the participants did not detect
the mismatch (for Transmission) between the checksum displayed
on the download webpage and the checksum computed from the
downloaded file (displayed in the terminal). This constitutes a sub-
stantial proportion of our subject pool. This number could be higher
in real life as the subjects are likely to be more careful in a con-
trolled environment compared to a situation where they are eager
to run the program they just downloaded. We did not find a signifi-
cant difference in the detection rate for participants who had prior
checksum knowledge (p = 1, Fisher’s exact test). Participants with
prior knowledge understand better the importance and functioning
of checksums but, at the same time, they might be more sloppy
in their verification as they know that an accidental modification
would very likely change the first digits of the checksum. The same
result – specifically the lack of difference between the group of
participants who were knowledgeable and those who were not –
was observed for the previous results on RQ3.
To study more quantitatively if some behavioral differences ex-
isted between those who detected the mismatch and those who did
not, we operated a post-hoc split of the participants. A Wilcoxon
rank sum test was conducted to compare the total number of fixa-
tions in the AOIs for the two groups. The values of the task with the
modified checksum were not considered in order to compare the
usual behavior. There was a significant difference in the number
of fixations for participants who detected the corrupted checksum
(M=12.47 fixations, SD=5.01) and those who did not (M=3.88 fix-
ations, SD=2.09); W=338.5, p < 0.001. Furthermore, the same test
was conducted to compare total dwell time in the AOIs for the
two groups. There was a significant difference in the amount of
time spent in the checksum AOIs for participants who detected the
corrupted checksum (M=15.63 seconds, SD=9.50) and those who
did not (M=3.97 seconds, SD=2.60); W=333, p < 0.001.
These results suggest that participants who detected the cor-
rupted checksum fixated the checksums significantly more fre-
quently and spent significantly more time than those who did not
detect the mismatch. The observed ratios between the two behav-
iors were approximately 4:1. This analysis was also extended to
tasks 1, 2 and 4 for the two groups of participants (i.e., those who
detected the mismatch vs. those who did not). We observed the
same difference reported for Task 3; this reveals that those who
were thorough were consistently so, during the entire experiment.