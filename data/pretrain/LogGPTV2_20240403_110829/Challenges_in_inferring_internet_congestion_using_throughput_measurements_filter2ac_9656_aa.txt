title:Challenges in inferring internet congestion using throughput measurements
author:Srikanth Sundaresan and
Xiaohong Deng and
Yun Feng and
Danny Lee and
Amogh Dhamdhere
Challenges in Inferring Internet Congestion Using
Throughput Measurements
Srikanth Sundaresan
Princeton University
PI:EMAIL
Danny Lee
Georgia Tech
PI:EMAIL
Xiaohong Deng, Yun Feng
University of New South Wales
Amogh Dhamdhere
CAIDA, Univ. of California, San Diego
{xiaohong.deng,yun.feng}@unsw.edu.au
PI:EMAIL
ABSTRACT
We revisit the use of crowdsourced throughput measurements to in-
fer and localize congestion on end-to-end paths, with particular fo-
cus on points of interconnections between ISPs. We analyze three
challenges with this approach. First, accurately identifying which
link on the path is congested requires ﬁne-grained network tomogra-
phy techniques not supported by existing throughput measurement
platforms. Coarse-grained network tomography can perform this
link identiﬁcation under certain topological conditions, but we show
that these conditions do not always hold on the global Internet. Sec-
ond, existing measurement platforms provide limited visibility of
paths to popular web content sources, and only capture a small frac-
tion of interconnections between ISPs. Third, crowdsourcing mea-
surements inherently risks sample bias: using measurements from
volunteers across the Internet leads to uneven distribution of sam-
ples across time of day, access link speeds, and home network con-
ditions. Finally, it is not clear how large a drop in throughput to
interpret as evidence of congestion. We investigate these challenges
in detail, and offer guidelines for deployment of measurement in-
frastructure, strategies, and technologies that can address empirical
gaps in our understanding of congestion on the Internet.
CCS CONCEPTS
• Networks → Network measurement;
KEYWORDS
Internet congestion, Internet topology, Throughput
ACM Reference Format:
Srikanth Sundaresan, Danny Lee, Xiaohong Deng, Yun Feng, and Amogh
Dhamdhere. 2017. Challenges in Inferring Internet Congestion Using Through-
put Measurements. In Proceedings of IMC ’17, London, United Kingdom,
November 1–3, 2017, 14 pages.
https://doi.org/10.1145/3131365.3131382
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’17, November 1–3, 2017, London, United Kingdom
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5118-8/17/11. . . $15.00
https://doi.org/10.1145/3131365.3131382
1 INTRODUCTION
The relentless growth of Internet trafﬁc demands, and the growing
concentration of content across a few providers and distribution net-
works, have led to capacity constraints, particularly at points of in-
terconnection between content providers, transit providers, and ac-
cess ISPs. Interconnection bandwidth contention has in turn led to
high-proﬁle disputes over who should pay for additional intercon-
nection capacity [8–10, 19, 40, 43]. The resulting and potentially
contentious interactions among providers have implications for net-
work stability and performance, leaving the congested link as an
externality for all users of the link until the dispute is resolved. This
situation has led to recent interest in technical, regulatory, and pol-
icy circles in techniques to detect the presence of persistent inter-
domain congestion, and to localize it to speciﬁc links in the net-
work. One approach to detecting congestion that has received sig-
niﬁcant recent attention is the use of crowdsourced throughput mea-
surements, such as those offered by the Measurement-Lab (M-Lab)
platform or Ookla’s Speedtest.net web service [31].
We present a systematic analysis of inference techniques and
challenges associated with using throughput measurements to in-
fer the presence, location, and characteristics of congestion. We use
NDT tests collected by the M-Lab platform in May 2015, along
with M-Lab’s analysis of this data [4, 27], as a case study of
throughput-based inferences, and explore three challenges.1
First, using crowdsourced end-to-end throughput measurements
to localize congestion to speciﬁc links in the network requires apply-
ing tomographic techniques to network paths observed during the
measurements. We show that applying tomography at a coarse AS-
level is difﬁcult, requires several assumptions about the topology
that do not always hold, and can be complicated by the complexity
of link and router-level interconnection that constitute an AS-link.
Second, platforms capable of supporting throughput measure-
ments currently observe a limited set of interdomain links of any
given access network. While M-Lab operates a large distributed
server-side measurement infrastructure and releases all data pub-
licly, as of February 2017 M-Lab was able to measure between 0.4%
and 9% of AS-level interconnections of access ISPs in U.S. (be-
tween 2.8% and 30% when considering AS-level peer interconnec-
tions). Furthermore, we show that between 79% to 90% of AS-level
interconnections traversed on paths from U.S. ISPs toward popular
web content were not testable using M-Lab’s server infrastructure.
1We used data from 2015 to align it with the M-Lab reports [4, 27].
IMC ’17, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
Finally, although crowdsourcing measurements can be an excel-
lent way to expand sampling coverage and diversity, crowdsourcing
can also yield biased samples. Coverage by user-generated measure-
ments around the world does not account for other factors that may
inﬂuence performance, including time of day, access link speed,
and quality of the home network launching the tests. The opacity
of these factors prevents reasoning about their possible inﬂuence,
which raises concerns about the statistical validity of the analyses.
After a systematic analysis of available data, we offer several sug-
gestions for improving the utility of collected throughput measure-
ments to characterize congestion upstream of a client’s access link.
These suggestions include improved topology measurement and
analysis techniques, more careful stratiﬁcation of test results based
on topological information, strategic deployment of server infras-
tructure to maximize coverage, and cross-validating crowdsourced
measurements using more systematically collected data from other
measurement platforms. Our analysis informs our own planning of
existing and future measurement infrastructure, not just for through-
put measurement, but for any generalized framework that aims to
measure performance on an Internet-wide scale.
Section 2 presents details of existing measurement infrastruc-
ture projects that support crowdsourced throughput measurements
and publish results. Section 3 and 4 describe how to apply tomo-
graphic techniques to these measurements to infer the location of
congestion, and explain limitations of these techniques. Section 5
describes limitations in visibility of relevant interconnections, and
Section 6 reviews statistical limitations on use of crowdsourced
throughput measurements. Section 7 summarizes lessons learned
and offers recommendations for improving interconnection conges-
tion measurement and inference capability.
2 EXISTING THROUGHPUT
MEASUREMENT PLATFORMS
Multiple platforms solicit crowdsourced throughput measurements
from users using Web-based speed tests,
including Ookla’s
Speedtest.net, DSLreports, and M-Lab. In addition, clients that are
part of the FCC’s Measuring Broadband America [44] platform or
the Bismark platform [7] also perform periodic throughput measure-
ments from home routers. Throughput measurements typically run
simple bulk data transfers over TCP [31] over a short period of
time, aiming to measure the bottleneck link by saturating it with
one or more TCP ﬂows. The bottleneck link is commonly the “last
mile”: the access link between the client and the Internet. In this
scenario, the ideal location of the server is as close as possible to
the client, to minimize latency to the client. TCP throughput has a
well-understood inverse relationship with latency [33]: the longer
the latency across a path, the lower the throughput, all other factors
being equal. Therefore, as broadband access speeds increase, low la-
tencies from test servers to clients ensure that throughput measure-
ments can saturate the bottleneck link on the path to the client[6].
Popular throughput measurement service providers such as Ookla
and M-Lab have extensive geographically distributed infrastructure
to achieve low latencies to clients.
2.1 Throughput Measurements on M-Lab
M-Lab is a distributed platform with hundreds of well-provisioned
machines around the world that serve as destinations (targets) for
a set of freely available performance measurement and diagnostic
tools. Users may download and run any of the supported software
tools that estimate performance characteristics of paths between the
client and M-Lab servers. One of these tools, the Network Diagnos-
tic Test (NDT), is a Web-based tool that runs a throughput measure-
ment in each direction: from client to server (upstream), and server
to client (downstream), as follows. A client initiates an NDT mea-
surement to M-Lab, and the M-Lab backend uses IP geolocation
to select a server close to the client. Each test estimates the down-
stream throughput from the server to the client, and the upstream
throughput from the client to the server, during which the server
logs statistics including round trip time, bytes sent, received, and
acknowledged, congestion window size, and the number of conges-
tion signals (multiplicative downward congestion window adjust-
ments) received by the TCP sender. The server also stores the raw
packet captures for the test, which are publicly available through
Google’s BigQuery and Cloud Storage [29, 30]. Unlike Ookla and
DSLreports, which only make aggregated stats available publicly,
M-Lab makes all data available, including packet traces and supple-
mentary path data.
2.2 Inferring Congestion using M-Lab data
The high density of U.S. server deployments on M-Lab facilitates
the crowdsourcing of a rich set of measurements that include dif-
ferent combinations of transit provider and access ISPs. In 2014
and 2015, two well-publicized measurement reports—one from M-
Lab itself [27], and another from the “Battle for the Net” advocacy
group [1]—used NDT measurements on M-Lab to infer congestion
in peering and transit networks in the U.S.
In 2014, a team from M-Lab team analyzed 18 months of NDT
data collected by the M-Lab platform to infer interdomain conges-
tion between large transit ISPs and large access ISPs [27]. The re-
port aggregated results from clients of access ISPs to servers located
in various transit providers, and grouped the tests by source AS,
destination AS, and server location. They analyzed metrics such as
download throughput, ﬂow round-trip time (or ﬂow RTT, which is
the latency between the server and the client), and packet retrans-
mission rates. They found diurnal patterns in the median values of
these metrics, from which they inferred persistent congestion on
paths between several U.S. access ISPs (including many large ISPs
such as Comcast, AT&T, Verizon, and Time Warner) and transit
providers (such as Cogent, Verizon, and XO). This report gener-
ated discussion in the academic community regarding the technical
soundness of the measurement and analysis methods, and requests
for revisions to the report to address its ﬂaws [41].
Starting early 2015, a net neutrality advocacy group called “Bat-
tle for the Net”[1] hosted a modiﬁed version of the NDT client on
their website. Their client was essentially a wrapper around NDT
which performed back-to-back tests with up to ﬁve M-Lab servers
in the same region rather than just the closest one, in an attempt
to observe more paths. This group released, and then signiﬁcantly
modiﬁed [42], a report claiming evidence of congestion in more net-
works than the original M-Lab report. Media coverage generated a
Challenges in Inferring Internet Congestion
IMC ’17, November 1–3, 2017, London, United Kingdom
jump in the number of NDT tests across M-Lab in May 2015, which
prompted an M-Lab researcher to issue an update to M-Lab’s con-
gestion report in a blog post [4]. The posting described how they ap-
plied the same inference method (from [27]) on the newer data set
to infer evidence of congestion in more interconnection links (such
as from Verizon, Comcast, and Time Warner to GTT, and TATA).
In the meantime, a public comment in the policy debate on the
AT&T-DirectTV merger approved in 2015 cited the 2014 M-Lab
report [27] as justiﬁcation to propose severe regulatory conditions
on AT&T following the merger including mandated settlement-free
peering with some peers, and mandated upgrades to interconnection
links upon reaching 70% utilization [15]. This public comment to
the FCC illustrated the need for objective, peer-reviewed analysis
of the state of interdomain congestion measurement methodology.
In this paper we offer an attempt at such an analysis.
In June 2016, Google incorporated speed tests (driven by
NDT) into Google search, allowing a sample of users that vis-
ited the Google search page to execute NDT tests against M-Lab
servers [17]. By December 2016, the number of monthly NDT tests
had increased more than 4-fold as compared to June 2016. After
ﬁxing some issues with the Paris traceroute data collection [35],
the M-Lab platform now collects a large volume of NDT measure-
ments along with Paris traceroutes from the server to client. The
widespread interest in the previous analyses of the M-Lab data in
technical and policy circles, along with the increased volume of
NDT data in recent months represents a tempting opportunity to
repeat the analysis of 2014-2015 in recent months.
We emphasize that our goal in this work is not to challenge the
conclusions presented in the M-Lab reports, but instead to highlight
the pitfalls and challenges in inferring congestion using throughput-
based tests, and to illustrate the use of path measurements paired
with throughput tests for more rigorous analysis. We hope that do-
ing so will encourage improvements in the testing platforms to bet-
ter support inference and localization of congestion, and analysis
that more rigorously accounts for the complexity of interdomain
interconnection. We use the M-Lab data and the previously men-
tioned analyses [4, 27] as case studies, because they were the ﬁrst
to attempt to use throughput measurements to infer and localize
interdomain congestion. As such, the presentation in this paper nec-
essarily delves into the particulars of the M-Lab infrastructure and
analysis. Nonetheless, we believe that the results and recommenda-
tions are applicable to other platforms that attempt such analysis.
3 USING NETWORK TOMOGRAPHY TO
INFER CONGESTION
Given a set of end-to-end measurements of some metric of interest
(such as throughput, delay, loss rate or reachability) and knowledge
(or measurements) of topology, one can use network tomography
to infer the properties of each link in the topology. Binary network
tomography [18] constrains the tomography problem by assuming
that network-internal links can be in one of two states — “good”
or ”bad”, and then attempts to ﬁnd the smallest set of “bad” links
that are consistent with the end-to-end observations. In the context
of congestion localization, the end-to-end metric is an estimate of
whether the path is congested, and links in the network can either
be “congested” or “not congested”.
Unfortunately, path information is not always available from
large-scale throughput measurement platforms. M-Lab collects
Paris traceroutes [5] from M-Lab servers toward clients that run
measurements against their infrastructure, and releases this data
publicly. Paris traceroute overcomes the problems with using the
traditional traceroute tool for inferring router-level topology and
paths in the presence of load-balancing, by carefully controlling the
header ﬁelds in sent packets. However, the path information from
Paris traceroute was incomplete prior to 2015 (Section 4.1) and in
the latter half of 2016 [35]. Other large-scale throughput measure-
ment platforms such as Ookla’s Speedtest.net either do not collect
path information at all, or do not release this data. Furthermore,
even if path information is available from traceroutes, using it as
input to a tomography algorithm is challenging due to issues with
measurement synchronization and traceroute artifacts [21]. One al-
ternative is to use a simpliﬁed form of tomography at the AS-level;
M-Lab’s studies of interconnection congestion used this simpliﬁed
approach. This section describes this method and its assumptions.
3.1 Applying simpliﬁed AS-level tomography for
congestion localization
Strong diurnal trends in achieved throughput in NDT measurements
from servers in an AS S (source AS) to clients in an AS A (access
AS) indicate the presence of link(s) along the path between S and A
that are congested at peak times. However, other problems unrelated
to congestion could result in such diurnal trends in NDT download
throughput, e.g., problems in the user’s home network or the user’s
access network. One way to mitigate the effects of access link issues
is to compare diurnal congestion trends seen in NDT measurements
from an access network A to servers in different ASes. If paths from
a source AS S1 to access network A show diurnal patterns indicating
peak-hour congestion, but paths from source AS S2 to access AS
A do not, this difference suggests that access link/home network
congestion was not a factor. The M-Lab reports went further and
claimed that any perceived performance degradation on paths from
S to A was on the interdomain link between S and A. This simpliﬁed
AS-level tomography approach relies on three key assumptions for
this inference to be correct:
(1) Assumption 1: There is no congestion internal to ASes.
Thus, inferred congestion on end-to-end paths is at an inter-
domain link between ASes. This assumption is crucial to to-
mography at the AS-level; because ﬁner-grained path infor-
mation is not available (or not used), the internal structure of
ASes is unknown. This assumption relies on the internal net-
works of ASes being well-provisioned to handle all incoming
or outgoing trafﬁc.
(2) Assumption 2: The server and client ASes directly inter-
connect. That is, no other ASes are on paths from the server
AS to the client AS. If both Assumption 1 and 2 hold, the
tomography problem has a straightforward solution: any ob-
served congestion on paths from server AS S to access AS A
must be on the AS-level interconnection between S and A.
(3) Assumption 3: All router-level
interconnections over
which an inference is made for the AS interconnection
behave similarly. If this assumption holds, then AS-level
inference accurately reﬂects the performance of every link
IMC ’17, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
or router-level interconnection within it. For this assumption
to hold, it is critical that throughput measurements from a
server in AS S to clients in AS A are aggregated in a way that
the paths to those clients traverse a single interconnection be-
tween S and A, or interconnections between S and A that be-
have similarly (e.g., parallel links between the same border
routers). As Claffy et al. [14] discuss, interdomain conges-
tion often shows regional effects. Consequently, aggregation
across interdomain links is particularly problematic if those
links are in different geographical regions, as they could vary
widely in terms of diurnal throughput patterns.
These assumptions bear careful consideration in today’s complex
Internet ecosystem. The ﬁrst assumption, that ASes do not experi-
ence internal congestion, derives from historical knowledge that net-
works generally are willing to invest signiﬁcant capital in upgrading
their own internal infrastructure, while disputes tend to arise at in-
terconnection points where who pays for the upgrade depends on
private negotiation between the interconnecting parties. The data at
our disposal does not allow us to investigate this assumption; how-
ever evidence from recent events suggests that it may be valid. In
Section 4 we use the public M-Lab path data (though limited) to
test the validity of assumptions 2 and 3.
4
IS THE TOPOLOGY AMENABLE TO
SIMPLIFIED TOMOGRAPHY?
Assessing the validity of the two topological assumptions described
in Section 3.1 (2 and 3) will inform our judgment of the feasibil-
ity of applying simpliﬁed AS-level tomography without the use of
ﬁner-grained path information. A limited set of path measurements
in the M-Lab dataset sheds doubt on whether these assumptions
hold generally. A fundamental requirement in our analysis is to use
traceroute measurements from M-Lab to reason about AS-level in-
terconnections between networks. Luckie, et al. [25]2 describe in
detail many things that can go wrong in inference of boundaries
between ASes. Two recent pieces of work, bdrmap [26] and MAP-
IT [28] have taken some steps toward overcoming those challenges.
In this section, we use MAP-IT, the more general of the two interdo-
main link identiﬁcation tools which can use a set of traceroutes that
has already been collected, and apply it to the set of traceroutes col-
lected from the M-Lab platform. We ﬁrst describe the constrained
set of path information available from the M-Lab, and use MAP-
IT to analyze whether this limited data reveals the extent to which
the M-Lab server and client are in adjacent ASes; We then apply
MAP-IT to the collected traceroute data to analyze whether NDT
measurements from an M-Lab server often reach a given access AS
over the same physical interconnection link. For most of this analy-
sis we use M-Lab data from 2015 in order to align it with the time
periods in which the M-Lab reports and “Battle for the Net” [42]
studies were released (Nov 2014 to May 2015).
4.1 Path measurements in the M-Lab dataset