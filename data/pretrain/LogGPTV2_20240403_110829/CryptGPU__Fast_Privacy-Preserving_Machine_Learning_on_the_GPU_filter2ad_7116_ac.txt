• If optimized CUDA kernels exist for evaluating the chosen
operation on integer-valued tensors (e.g., point-wise addition
or point-wise multiplication), then the corresponding CUDA
kernel is directly invoked.
• For bilinear operations where optimized CUDA kernels only
exist for computations on ﬂoating-point inputs (e.g., con-
volutions, matrix multiplications), then CRYPTGPU applies
the above technique of ﬁrst decomposing the input into
k = 4 tensors of 16-bit values, computing all necessary
O(k2) pairwise products of the resulting blocks (using the
ﬂoating point kernel), and re-combines the pairwise products
to obtain the ﬁnal output.
III. THREAT MODEL AND CRYPTOGRAPHIC DESIGN
In this section, we provide a formal speciﬁcation of our
threat model and a description of the private inference and
training functionalities we develop. We then describe the
cryptographic sub-protocols we use to construct our privacy-
preserving training and inference protocols.
We begin by introducing the notation we use in this work.
For a ﬁnite set S, we write x R←− S to denote that x is drawn
uniform at random from S. We use boldface letters (e.g., x, y)
to denote vectors and use non-boldface letters (e.g., xi, yi)
to denote their components. We denote our three parties by
P1, P2, P3. To simplify notation, whenever we use an index
i ∈ {1, 2, 3} to denote a party (or a share), we write i − 1
and i + 1 to denote the “previous” party and the “next” party,
respectively. For example, P3+1 refers to P1.
A. Threat Model
Similar to several recent 3-party protocols [26, 37, 2, 6], we
design our system in the honest-majority model. Moreover, we
focus on semi-honest adversaries. Namely, we assume that each
of the three computing parties follow the protocol, but may
individually try to learn information about other parties’ inputs.
Formally, we consider the standard simulation-based notion of
security in the presence of semi-honest adversaries [38, 39]:
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
51025
Deﬁnition III.1 (Semi-Honest Security). Let f : ({0, 1}n)3 →
({0, 1}m)3 be a randomized functionality and let π be a
protocol. We say that π computes f with perfect security in
the presence of a single semi-honest corruption if there exists
a probabilistic polynomial-time simulator S such that for every
corrupted party i ∈ {1, 2, 3} and every input x ∈ ({0, 1}n)3,
{outputπ(x), viewπ
i (x)} ≡ {f (x),S(i, xi, fi(x))}
where viewπ
i (x) is the view of party i in an execution of π on
input x, outputπ(x) is the output of all parties in an execution
of π on input x, and fi(x) denotes the ith output of f (x).
Computing on secret-shared values. In this work, we con-
sider two main settings: private inference and private training
on secret-shared inputs. We use standard 3-out-of-3 additive
secret sharing as well as 2-out-of-3 replicated secret sharing.
Abstractly, we model both types of secret sharing as a pair of
algorithms (Share, Reconstruct) with the following properties:
• On input x ∈ {0, 1}n, the share algorithm Share(x) outputs
a tuple of three shares (x1, x2, x3).
• The reconstruction algorithm Reconstruct(S) takes a set of
shares T and outputs a value x ∈ {0, 1}n if successful and
⊥ if not.
Correctness of a threshold secret sharing scheme with threshold
t says that for any subset of shares T ⊆ Share(x) of size at
least t, Reconstruct(T ) = x. Perfect security says that there
exists a probabilistic polynomial-time simulator S such that
for every subset T ⊆ {1, 2, 3} where |T| < t and every input
x ∈ {0, 1}n,
{(x1, x2, x3) ← Share(x) : (xi)i∈T} ≡ {S(1n, T )}.
We now formally deﬁne our notion of private inference and
private training on secret-shared inputs:
• Private inference: Inference is the problem of evaluating
a trained model M on an input x. We denote this
operation as Eval(M, x). In private inference, the ideal
functionality f maps secret shares of an input x and a
model M to a secret share of the output Eval(M, x).
((M1, x1), (M2, x2), (M3, x3)),
Namely,
the
ideal
Share(Eval(M, x))
where M
and
x ← Reconstruct({x1, x2, x3}). In particular, a private
inference protocol ensures privacy for the model M, the
input x, and the output Eval(M, x).
functionality
← Reconstruct({M1, M2, M3})
outputs
on
input
• Private training: In private training, the goal is to run a
training algorithm Train on some dataset D. In this case,
the ideal functionality f maps secret shares of the dataset
(D1, D2, D3) to a secret share of the model Share(Train(D))
where D ← Reconstruct(D1, D2, D3). In this case, each
party individually learn nothing about the input dataset D
or the resulting learned model Train(D).
B. Cryptographic Building Blocks for Private Inference
We now describe the main MPC building blocks we use for
private inference on deep neural networks. First, we decompose
the neural network inference algorithm into a sequence of
elementary operations: linear/pooling/convolution layers and
activation function evaluation (ReLU). To obtain our protocol
π for computing the ideal functionality for private inference,
we sequentially compose the semi-honest secure protocols
for realizing each of the elementary operations. Correctness
and semi-honest security of the overall protocol then follows
by correctness and security of the underlying sub-protocols
together with the sequential composition theorem [38].
“GPU-friendly” cryptography. As alluded to in Sections I-A
and II-B, we seek cryptographic protocols that are particularly
amenable to GPU acceleration. For example, protocols that
involve conditionals (such as garbled circuits [28]) or require
extensive ﬁnite ﬁeld arithmetic are more challenging to support
efﬁciently on the GPU. For this reason, we focus primarily
on secret-sharing based protocols and work over a ring with a
power-of-two modulus. In the following description, we elect
to use cryptographic protocols where the underlying imple-
mentations vectorize and whose evaluation can be expressed
primarily in terms of point-wise or component-wise operation
on blocks of data.
Secret sharing. We work over the ring Zn where n = 2k
is a power of 2. In our speciﬁc implementation, k = 64. To
secret share a value x ∈ Zn, sample shares x1, x2, x3
R←− Zn
such that x1 + x2 + x3 = x. Following Araki et al. [26], our
default sharing is a 2-out-of-3 “replicated secret sharing” [25]
where each party holds a pair of shares: P1 has (x1, x2), P2
has (x2, x3), and P3 has (x3, x1). We denote this by(cid:74)x(cid:75)n =
(x1, x2, x3). In some cases, we will also consider a 3-out-of-3
additive secret sharing scheme where party Pi holds xi (but
none of the other shares).
Fixed point representation. Machine learning algorithms na-
tively operate on real (i.e., ﬂoating-point) values while the most
efﬁcient cryptographic protocols are restricted to computations
over discrete domains such as rings and ﬁnite ﬁelds. Following
previous work, we use a ﬁxed-point encoding of all values
occurring in the computation, and then embed the integer-
valued ﬁxed-point operations in the ring Zn. Speciﬁcally, if
we consider a ﬁxed-point encoding with t bits of precision, a
real value x ∈ R is represented by the integer (cid:98)x · 2t(cid:101) (i.e.,
the nearest integer to x · 2t). The ring modulus n is chosen to
ensure no overﬂow of the integer-valued ﬁxed-point operations.
CRYPTGPU sets n = 64; we discuss this choice in detail in
Appendix C.
Protocol initialization. In the following description, we as-
sume that the parties have many independent secret shares
of 0. This will be used for “re-randomization” during the
protocol execution. We implement this using the approach of
Araki et al. [26]. Speciﬁcally, let F be a pseudorandom function
(PRF). At the beginning of the protocol, each party Pi samples
a PRF key ki and sends ki to party Pi+1. The jth secret share
of 0 is the triple (z1, z2, z3) where zi = F (ki, j)− F (ki−1, j).
Linear operations. Linear operations on secret-shared data
only require local computation. Namely, if α, β, γ ∈ Zn are
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
61026
public coefﬁcients α, β, γ.
Each of the parties can compute their respective shares of
public constants and(cid:74)x(cid:75)n,(cid:74)y(cid:75)n are secret-shared values, then
(cid:74)αx + βy + γ(cid:75)n = (αx1 + βy1 + γ, αx2 + βy2, αx3 + βy3).
(cid:74)αx + βy + γ(cid:75)n from their shares of(cid:74)x(cid:75)n and(cid:74)y(cid:75)n and the
Multiplication. To multiply two secret-shared values(cid:74)x(cid:75)n =
(x1, x2, x3),(cid:74)y(cid:75)n = (y1, y2, y3), each party Pi locally com-
putes zi = xiyi + xi+1yi + xiyi+1. By construction, z1 + z2 +
z3 = xy ∈ Zn. This yields a 3-out-of-3 additive sharing of z.
To obtain replicated shares of z, party Pi sends Pi+1 a blinded
share zi + αi, where (α1, α2, α3) is a fresh secret-sharing of
0 (derived from the PRF as described above).
Since x, y are ﬁxed-point encodings, the parties additionally
need to rescale z after computing the product (i.e., divide it
by the scaling factor 2t). In this work, we use the truncation
protocol Πtrunc1 from ABY3 [2] to implement this procedure.
We note that Mohassel and Rindal propose two versions of
the share truncation protocol: a two-round protocol Πtrunc1
that only relies on elementary arithmetic operations and a one-
round protocol Πtrunc2 that relies on precomputed “truncation
tuples”. While generating the truncation tuples can be done
in a separate ofﬂine phase, doing so requires implementing
a Boolean bit extraction circuit over secret-shared values. In
contrast, Πtrunc1 relies exclusively on arithmetic operations, and
naturally extends to our tensor-based computing model. For this
reason, we use the two-round truncation protocol Πtrunc1 in our
implementation. This has the added advantage that we avoid a
separate (and potentially expensive) preprocessing step. Both of
these share-truncation protocols are not exact and introduce 1
bit of error in the least signiﬁcant bit of the secret-shared value
(i.e., with t bits of ﬁxed-point precision, the error introduced
is bounded by 2−t). We provide an empirical assessment of
the error (and resulting model accuracy) in Appendix C.
Convolutions and matrix multiplication. The above proto-
cols for computing linear functions as well as products of secret-
shared values directly vectorize to yield protocols for computing
linear functions on tensors as well as bilinear operations like
matrix multiplication and convolution. Linear functions on
secret-shared tensors only require local computation. Bilinear
operations on secret-shared tensors like matrix multiplications
and convolutions are implemented by computing three separate
products (as described in the multiplication protocol above).
These computations over secret-shared tensors directly map
to analogous computations on plaintext values, so we can
take advantage of existing highly-optimized CUDA kernels for
evaluating these operations via the technique from Section II-B.
As in several previous systems (e.g., [1, 2, 5]), when we
compute products of secret-shared tensors, we only apply the
truncation protocol to the result of the product and not after
each individual multiplication. This has a signiﬁcant impact on
the performance of the protocol for two reasons: (1) we can
use existing CUDA kernels optimized for matrix products and
convolutions without needing to modify how the elementary
multiplications are performed; and (2) the total communication
in the protocol is proportional to the size of the output rather
than the number of intermediate element-wise multiplications.
Most signiﬁcant bit. Several of our protocols rely on a
tation, this corresponds to computing the sign of x. For
this, we adopt the general approach from ABY3. Namely,
protocol for computing the most signiﬁcant bit (cid:74)msb(x)(cid:75)n
of a secret-shared value (cid:74)x(cid:75)n. In our ﬁxed-point represen-
given an arithmetic secret sharing (cid:74)x(cid:75)n = (x1, x2, x3) of
(cid:74)x1(cid:75)2 = (x1, 0, 0),(cid:74)x2(cid:75)2 = (0, x2, 0), and(cid:74)x3(cid:75)2 = (0, 0, x3).
(cid:74)x1(cid:75)2,(cid:74)x2(cid:75)2,(cid:74)x3(cid:75)2 to compute binary shares of the sum(cid:74)x(cid:75)2,
which in particular, yields a binary share of(cid:74)msb(x)(cid:75)2. Finally,
to recover arithmetic shares of (cid:74)ReLU(x)(cid:75)n from (cid:74)x(cid:75)n and
(cid:74)msb(x)(cid:75)2, we use the bit injection protocol from ABY3 [2,
x, the parties re-interpret it as three binary shares of values
The parties now evaluate an addition circuit on the binary shares
§5.4], which only requires simple arithmetic operations.
The majority of this computation is the evaluation of the
addition circuit over binary shares on the GPU. Evaluating
a Boolean addition circuit on secret-shared binary values
decomposes into a sequence of bitwise AND and XOR op-
erations (along with communication for the AND gates), which
can be computed using efﬁcient GPU kernels. We provide
microbenchmarks in Section IV-C.
ReLU activation function. The standard activation function
we consider in our networks is the rectiﬁed linear unit
(ReLU) [40, 16]: ReLU(x) := max(x, 0). To compute the
ReLU function, it sufﬁces to construct a protocol for testing
whether the ﬁxed-point value x is positive or not. This
corresponds to computing the most signiﬁcant bit msb(x) of
x, which we evaluate using the protocol described above.
Privacy-preserving training. In Appendix A, we describe
additional protocols we use for privacy-preserving training.
IV. SYSTEM IMPLEMENTATION AND EVALUATION
We build CRYPTGPU on top of CRYPTEN, which itself
builds on PyTorch. First, we introduce the CUDALongTensor
data type that represents a PyTorch tensor for 64-bit integer
values (see Section II-B). Our design enables us to take
advantage of optimized CUDA kernels for evaluating bilinear
operations such as convolutions and matrix multiplications on
secret-shared tensors. This sufﬁces for evaluating arithmetic
circuits on secret-shared tensors. Using these elementary
building blocks, we then implement protocols for each of
the operations described in Section III (i.e., the truncation
protocol for ﬁxed-point multiplication, ReLU computation, and
the softmax function). Through composing these individual
protocols together, we obtain an end-to-end system for private
inference and private training.
Point-to-point
communication. We leverage PyTorch’s
torch.distributed package for point-to-point communi-
cation between parties. The default communication mode in
PyTorch is a “broadcast” mode where every message sent
by a party is sent to all peers. To emulate point-to-point
channels (as required by our protocol), we initialize a separate
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
71027
communication backend between each pair of parties. In this
case, a “broadcast” channel between each pair of parties
functions as a point-to-point channel between the parties.
Pseudorandom generators on the GPU. We use AES as
the PRF in our protocol (used for share re-randomization in
the truncation protocol). We use the torchcsprng PyTorch
C++/CUDA extension [41] (based on the Salmon et al.
protocol [42]) which enables AES evaluation on the GPU.
A. Experimental Setup for System Evaluation
We now describe our experimental setup for evaluating
CRYPTGPU as well as the speciﬁc parameters we use to
instantiate our cryptographic protocols from Section III. We
describe the speciﬁc datasets and models we consider in
Appendix B.
Protocol parameters. We instantiate our protocols from
Section III using the following parameter settings:
• Fixed-point precision. We consider secret-sharing schemes
over the 64-bit ring Z264, and encode inputs using a ﬁxed-
point representation with t = 20 bits of fractional precision
(i.e., an input x ∈ R is encoded as (cid:98)x· 220(cid:101). In Appendix C,
we analyze the effect the number of bits of precision has on
the accuracy of our protocols.
• Exponentiation. We use the function fm from Eq. (A.2) to
approximate the exponential function. In this work, we take
m = 29 = 512, so evaluating fm requires log m = 9 rounds
of multiplication. With t = 20 bits of ﬁxed-point precision,
we measure the maximum error of our approximation on all
inputs x ≤ 0 to be at most 6 · 10−4.
• Division. As described in Appendix A, we require a private
division protocol to compute(cid:74)1/y(cid:75)n where y ∈ [1, Y ], and
Y is the number of classes in the classiﬁcation problem. For
all of the datasets we consider for private training, Y ≤ 200.
In our implementation, we use 13 iterations of Newton-
Raphson (with 1/Y as the initialization). With t = 20 bits
of ﬁxed-point precision, we measure the maximum absolute
difference between the approximate value and the true value
for inputs in the interval [1, Y ] to be ≈ 10−4 (and ≈ 10−9
using a ﬂoating-point evaluation).
B. Benchmarks for Private Training and Inference
We run our experiments on three Amazon EC2 instances op-
timized for GPU computation (p3.2xlarge). Each instance
has a single NVIDIA Tesla V100 GPU with 16 GB of GPU
memory. All of the instances run Ubuntu 18.4 and have 8 Intel
Xeon E5-2686 v4 (2.3 GHz) CPUs and 61 GB of RAM. We
consider a local area network (LAN) environment and place all
three servers in the us-east-1 (Northern Virginia) region. In
this case, we measure the network bandwidth to be 1.25GB/s
with an average latency of 0.2ms. For each model/dataset