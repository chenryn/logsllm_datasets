N
R/M
N
R
M
V
V
N
G
E
G, R
V
V/E
N
N
R, G
V
G
V
G
V
G, M
G
G
R
V
N
V/M
N
N
N
timeout
-
10D
8H
12H
1H
24H
5D
24H
-
2H
2H
6H, 24H
8H, 24H
6H, 24H
5H
LONG
4D, 12D
5H
>7D
-
30D
5H, 24H
24H
24H
5M
24H
5H
24H
6H
benchmarks
baseline
trials
variance
crash
Table 1: Summary of past fuzzing evaluation. Blank cell means that the paper’s evaluation did not mention this item; - means it was not
relevant; ? means the element was mentioned but with insufficient detail to be clear about it. Benchmarks: R means real-world programs,
C means CGC data-set, L means LAVA-M benchmark, S means programs with manually injected bugs, G means Google fuzzer test suite.
Baseline: A means AFL, B means BFF [3], L means libfuzzer [34], R means Radamsa [43], Z means Zzuf [60], V means VUzzer [44] O means
other baseline used by no more than 1 paper. Trials: number of trials. Variance: C means confidence intervals. Crash: S means stack hash
used to group related crashes during triage, O means other tools/methods used for triage, C means coverage profile used to distinguish
crashes, G means crashes triaged according to ground truth, G* means manual efforts partially obtained ground truth for triaging. Coverage:
L means line/instruction/basic-block coverage, M means method coverage, E means control-flow edge or branch coverage, O means other
coverage information. Seed: R means randomly sampled seeds, M means manually constructed seeds, G means automatically generated
seed, N means non-empty seed(s) but it was not clear if the seed corpus was valid, V means the paper assumes the existence of valid seed(s)
but it was not clear how the seed corpus was obtained, E means empty seeds, / means different seeds were used in different programs, but
only one kind of seeds in one program. Timeout: times reported in minutes (M), hours (H) and/or days (D).
randomly selected files of the right type, and manually-generated
(but well-formed) files.
4 STATISTICALLY SOUND COMPARISONS
All modern fuzzing algorithms fundamentally employ randomness
when performing testing, most notably when performing mutations,
but sometimes in other ways too. As such, it is not sufficient to
simply run fuzzer A and baseline B once each and compare their
performance. Rather, both A and B should be run for many trials,
and differences in performance between them should be judged.
Perhaps surprisingly, Table 1 shows that most (17 out of 32)
fuzzing papers we considered make no mention of the number
of trials performed. Based on context clues, our interpretation is
that they each did one trial. One possible justification is that the
randomness “evens out;” i.e., if you run long enough, the random
choices will converge and the fuzzer will find the same number of
crashing inputs. It is clear from our experiments that this is not
true—fuzzing performance can vary dramatically from run to run.
Consider the results presented in Figure 2, which graphs the
cumulative number of crashes (the Y axis) we found over time (the
X axis) by AFL (blue), and AFLFast (red), each starting with an
(a) nm: p  0.05
p2  0.05
p2  0.05
p2  0.05
p2 < 10−6
Figure 3: FFmpeg results with different seeds. Solid line is median result; dashed lines are confidence intervals. p1 and p2 are
the p-values for the statistical tests of AFL vs. AFLFast and AFL vs. AFLNaive, respectively.
choice of test. In particular, two viable alternatives are the permu-
tation test [17] and bootstrap-based tests [7]. These tests work by
treating the measured data as a kind of stand-in for the overall pop-
ulation, systematically comparing permutations and re-samples
of measured data to create rankings with confidence intervals.
Whether such methods are more or less appropriate than Mann
Whitney is unclear to us, so we follow Arcuri and Briand [2].
Determining that the median performance of fuzzer A is greater
than fuzzer B is paramount, but a related question concerns effect
size. Just because A is likely to be better than B doesn’t tell us how
much better it is. We have been implicitly answering this question
by looking at the difference of the measured medians. Statistical
methods could also be used to determine the likelihood that this
difference represents the true difference. Arcuri and Briand suggest
Vargha and Delaney’s ˆA12 statistics [52] (which employ elements
of the Mann Whitney calculation). Bootstrap methods can also be
employed here.
5 SEED SELECTION
Recall from Figure 1 that prior to iteratively selecting and testing
inputs, the fuzzer must choose an initial corpus of seed inputs. Most
(27 out of 32, per Section 2.2) recent papers focus on improving
the main fuzzing loop. As shown in column seed in Table 1, most
papers (30/32) used a non-empty seed corpus (entries with G, R,
M, V, or N). A popular view is that a seed should be well-formed
(“valid”) and small—such seeds may drive the program to execute
more of its intended logic quickly, rather than cause it to terminate
at its parsing/well-formedness tests [31, 44, 45, 53]. And yet, many
times the details of the particular seeds used were not given. En-
try ’V’ appears 9 times, indicating a valid seed corpus was used,
but providing no details. Entry ’N’ appears 10 times, indicating a
non-empty seed, but again with no details as to its content. Two
papers [5, 6] opted to use an empty seed (entry ‘E’). When we asked
them about it, they pointed out that using an empty seed is an easy
way to baseline a significant variable in the input configuration.
Other papers used manually or algorithmically constructed seeds,
or randomly sampled ones.
It may be that the details of the initial seed corpus are unim-
portant; e.g., that no matter which seeds are used, algorithmic
improvements will be reflected. But it’s also possible that there is
a strong and/or surprising interaction between seed format and
algorithm choice which could add nuance to the results [37]. And
indeed, this is what our results suggest.
We tested FFmpeg with different seeds including the empty seed,
samples of existing video files (“sampled” seeds) and randomly-
generated videos (“made” seeds). For the sampled seeds, videos
FFmpeg, AFLNaive
FFmpeg, AFL
FFmpeg, AFLFast
nm, AFL
nm, AFLFast
objdump, AFL
objdump, AFLFast
cxxfilt, AFL
cxxfilt, AFLFast
0
382.5
369.5
448
1239
6.5
29
540.5
1400
empty
(< 10−15)
(= 0.379)
(< 10−13)
(< 10−3)
(< 10−10)
1-made
(< 10−11)
(< 0.05)
(= 0.830)
(< 10−2)
(< 10−10)
5000
102
129
23
24
5
6
572.5
1364
Table 2: Crashes found with different seeds. Median number of
crashes at the 24-hour timeout.
were drawn from the FFmpeg samples website.2 Four samples each
were taken from the AVI, MP4, MPEG1, and MPEG2 sub-directories,
and then the files were filtered out to only include those less than 1
MiB, AFL’s maximum seed size, leaving 9-sampled seeds total. This
set was further pared down to the smallest of the video files to
produce 3-sampled and 1-sampled seeds. For the made seeds, we
generated video and GIF files by creating 48 random video frames
with videogen (a tool included with FFmpeg), 12 seconds of audio
with audiogen (also included), and stitching all of them together
with FFmpeg into 3-made MP4, MPG, and AVI files, each at 4 fps.
The 1-made seed is the generated MP4 file. We also tested nm,
objdump, and cxxfilt using the empty seed, and a 1-made seed. For
nm and objdump, the 1-made seed was generated by compiling a
hello-world C program. The 1-made seed of cxxfilt was generated
as a file with 16 random characters, chosen from the set of letters
(uppercase and lowercase), digits 0-9, and the underscore, which is
the standard alphabet of mangled C++ names.
Results with these different seed choices for FFmpeg are shown
in Figure 3. One clear trend is that for AFL and AFLFast, the empty
seed yields far more crashing inputs than any set of valid, non-
empty ones. On the other hand, for AFLNaive the trend is reversed.
Among the experiments with non-empty seeds, performance also
varies. For example, Figure 3(b) and Figure 3(d) show very different
performance with a single, valid seed (constructed two different
ways). The former finds around 100 crashes for AFL and AFLFast
after 24 hours, while the latter finds less than 5.
The top part of Table 2 zooms in on the data from Figure 3(a)
and (b) at the 24-hour mark. The first column indicates the target
program and fuzzer used; the second column (“empty”) indicates
the median number of crashes found when using an empty seed;
and the last column (“1-made”) indicates the median number of
crashes found when using a valid seed. The parenthetical in the
last two columns is the p-value for the statistical test of whether
the difference of AFLFast or AFLNaive performance from AFL is
real, or due to chance. For AFL and AFLFast, an empty seed pro-
duces hundreds of crashing inputs, while for AFLNaive, it produces
none. However, if we use 1-made or 3-made seeds, AFLNaive found
significantly more crashes than AFL and AFLFast (5000 vs. 102/129).
The remainder of Table 2 reproduces the results of the AFLFast
evaluation [6] in the empty column, but then reconsiders it with a
valid seed in the 1-made column. Similar to the conclusion made
by the AFLFast paper, AFLFast is superior to AFL in crash finding
ability when using the empty seed (with statistical significance).
However, when using 1-made seeds, AFLFast is not quite as good:
it no longer outperforms AFL on nm, and both AFL and AFLFast
generally find fewer crashes.
In sum, it is clear that a fuzzer’s performance on the same pro-
gram can be very different depending on what seed is used. Even
valid, but different seeds can induce very different behavior. Assum-
ing that an evaluation is meant to show that fuzzer A is superior to
fuzzer B in general, our results suggest that it is prudent to consider