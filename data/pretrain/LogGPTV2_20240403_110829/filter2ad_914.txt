title:User-friendly matching protocol for online social networks
author:Qiang Tang
User-Friendly Matching Protocol for Online Social
Networks
DIES, Faculty of EEMCS, University of Twente
Qiang Tang
Enschede, the Netherlands
PI:EMAIL
ABSTRACT
In this paper, we outline a privacy-preserving matching pro-
tocol for OSN (online social network) users to ﬁnd their po-
tential friends. With the proposed protocol, a logged-in user
can match her proﬁle with that of an oﬀ-line stranger, while
both proﬁles are maximally protected. Our solution success-
fully eliminates the requirement of “out-of-band” communi-
cation channels, which is one of the biggest obstacles facing
cryptographic solutions for OSNs.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems— Distributed applications; E.3 [Data Encryption]:
Public key cryptosystems
General Terms
Algorithms, Human Factors, Security
Keywords
Online social network, matching, privacy
1.
INTRODUCTION
OSNs (online social networks) have gained a great popu-
larity in recent years, and become an important communica-
tion platform for Netizens. Popular OSNs, such as Facebook
and Myspace, have attracted millions of users. The core at-
traction of OSNs is that they enable a user to: (1) construct
a proﬁle to represent herself within the OSN; (2) build up
connections with other users through matching their proﬁles
and share their information afterwards. In addition, OSNs
often provide some additional applications, developed by ei-
ther OSNs themselves or third parties, to encourage users’
engagement in their networks.
Generally, the system structure of an OSN is depicted in
Figure 1, where the dashed line means friendship connection.
For a user, say Alice, all her information (proﬁle, friendship
graph, etc) is stored in plaintext by the OSN. When Al-
ice logs into her account, she can surf the whole network
and look into another user’s proﬁle.
If Alice decides that
Bob’s proﬁle is interesting, then she can then add Bob as
her friend.
Copyright is held by the author/owner(s).
CCS’10, October 4–8, 2010, Chicago, Illinois, USA.
ACM 978-1-4503-0244-9/10/10.
Figure 1: OSN system Structure
1.1 Motivation
Numerous academic analysis and reality incidents have
shown that users’ proﬁle information can be easily disclosed
to the unintended audience. In practice, unintended infor-
mation disclosure can result in very negative consequences,
e.g. identity theft, blackmailing, and stalking. To cope with
the ever-deteriorating situation, most OSNs have provided
some privacy settings. Take Facebook as an example, for
each of her proﬁle items, a user can restrict it to be accessi-
ble only by Friends, Friends of Friends, or Everyone.
While the implemented privacy settings help users gain
better control over their information, they can interfere with
existing OSN functionalities. Let’s consider the following
example situation occurred in Facebook. When Alice uses
an application X, she notices that Bob has also used the
same application recently. To learn more information about
Bob, Alice visits Bob’s page and ﬁnds nothing except the
advice given by Facebook: “Bob only shares some of his
proﬁle information with everyone. If you know Bob, send
him a message or add him as a friend.”. In this situation,
Alice has three choices.
1. Alice gives up without trying to learn anything more
about Bob.
In this case, Alice could have missed a
chance to meet Bob who has similar interests to hers,
so is for Bob.
2. Alice adds Bob as her friend.
In this case, later on,
Alice may ﬁgure out that she does not have many com-
mon interests with Bob and does not want to share her
732information with Bob. Alice may try to remove Bob
from her friend list, but it is an awkward task.
solutions are not practical yet. The other cryptographic
building block is fuzzy extractor [4], deﬁned as follows.
3. Alice writes Bob a message to introduce herself and
ask for more information. In this case, if both Alice
and Bob want to protect their proﬁle information, they
need a secure mechanism (not existing) to fairly reveal
information to each other.
The illustrated interference indeed reﬂects the tension be-
tween the OSN functionalities and users’ privacy expecta-
tions in the scope of an OSN. On one hand, users want to
acquire as much information as possible from others in order
to ﬁnd friends. On the other hand, scared by the potential
privacy breaches, users tend to hide as much information as
possible from others.
One more related concern is that, regardless of the privacy
settings, users’ proﬁle information is always in plaintext for
the OSN. How to prevent a malicious OSN from abusing
users’ information is still an unsolved issue.
1.2 Contribution
With respect to protecting users’ proﬁle information in-
side an OSN, we adopt the following threat model. A user
shares her proﬁle information with her friends only, i.e. any
unauthorized entity is considered as a potential adversary.
The OSN is semi-trusted in the following sense: it will faith-
fully mediate the protocol execution but is likely to mount
an automated information recovery attack, through which
the OSN employs computers to recover users’ information
without massive involvement of humans. The motivation
behind this is that, for an OSN with millions of users, the
main concern is that the OSN may massively collect and
mine its users’ proﬁle information, while it is less concerned
that the OSN will mount a targeted attack against a speciﬁc
user.
We tackle the identiﬁed problem by introducing a novel
privacy-preserving protocol for OSN users to match their
proﬁles. Our proposal has the following unique features.
1. It assumes a general environment, where Alice logs
into her account and tries to match her proﬁle with
that of Bob, who is oﬀ-line at the moment. Certainly,
the protocol also works when Bob has logged in.
2. Users’ proﬁle information is encrypted by self-generated
private keys, and no TTP is required to perform key
certiﬁcation. CAPTCHA (Completely Automated Pub-
lic Turing test to tell Computers and Humans Apart)
[1] is employed to ”hide” cryptographic keys from the
OSN and verify a human user’s presence.
3. In contrast to other schemes, e.g. [2, 3], by employing
a fuzzy extractor [4], our proposal does not require
any “out-of-band” communication channel for users to
exchange private keys.
2. THE PROPOSED SOLUTION
In our proposal, users’ proﬁle information will be encrypted
and the matching will be carried out based on the encrypted
data. Therefore, we will use PKE (public key encryption)
schemes with additively homomorphic property, e.g. Pail-
lier [5].
It is worth noting that a fully-homomorphic en-
cryption scheme will be a better option, however, existing
Definition 1. A (U, m, `, t, ) fuzzy extractor consists of
two polynomial-time algorithms (Gen, Rep).
• Gen: U → R × {0, 1}∗. This algorithm takes u ∈ U
as input, and returns a secret r ∈ R = {0, 1}` and a
public helper data p ∈ {0, 1}∗.
• Rep: U × {0, 1}∗ → R. This algorithm takes u0 ∈ U
and p ∈ {0, 1}∗ as input, and returns a string from R.
Informally, the Gen algorithm extracts an ephemeral se-
cret r from a long-term secret u, while the Rep algorithm
can recover the ephemeral secret r from the public helper
data p given u0, whose distance to u is within the threshold
t.
Besides these cryptographic primitives, a CAPTCHA scheme
will be employed. An instance of an example CAPTCHA
scheme is shown in Figure 2, and it is in fact an image
embedded with the English words “following ﬁnding”. The
CAPTCHA scheme is secure if a computer cannot recognize
the words in the image with a high probability. CAPTCHA
schemes have been widely used to deter automated attacks
in a number of contexts, such as web account registration
and account login.
Figure 2: Example of CAPTCHA
It is worth noting that CAPTCHA schemes don’t pro-
vide security guarantees as strong as those by cryptographic
primitives. Some CAPTCHA schemes have been broken,
e.g. [6]. Nonetheless, they are still a popular security mea-
sure.
2.1 Description of the Solution
We assume that the OSN provides an application MApp
for proﬁle matching. The MApp selects a symmetric key
encryption scheme (SEnc, SDec) and a cryptographic hash
function H. As in [7], for a user, the running applications in
her local computer are assumed to be trustworthy.
Without loss of generality, suppose that Alice and Bob
have n proﬁle items, denoted by ai (1 ≤ i ≤ n) and bi (1 ≤
i ≤ n) respectively. Alice generates a public/private key
pair (pka, ska) for an additively homomorphic PKE scheme
(PEnc, PDec), and generates a CAPTCHA instance capa by
embedding her private key ska in an image. Instead of her
plaintext proﬁle, Alice stores the following information un-
der her account in the OSN:
pa, H(ra), pka, capa, SEnc(pka||ska, ra), PEnc(ai, pka) (1 ≤ i ≤ n),
where (ra, pa) = Gen({a1, a2, · · · , an}), and (Gen, Rep) is
a fuzzy extractor scheme with threshold set diﬀerence dis-
tance value ta. Similarly, Bob generates a public/private key
pair (pkb, skb) and generates a CAPTCHA instance capb by
embedding his private key skb in an image. Bob stores the
following information under his account in the OSN:
pb, H(rb), pkb, capb, SEnc(pkb||skb, rb), PEnc(bi, pkb) (1 ≤ i ≤ n),
where (rb, pb) = Gen0({b1, b2, · · · , bn}), and (Gen0, Rep0) is a
fuzzy extractor scheme with threshold set diﬀerence distance
733value tb. We stress that ta (tb) serves as a threshold value for
being matched by another user when Alice (Bob) is oﬀ-line.
Motivated by the fact that friendship is something of re-
ciprocal interest to both involved parties, we consider match-
ing to be a two-stage process. Suppose Alice logs into her ac-
count and wants to match her proﬁle with that of Bob. The
ﬁrst stage (steps 2–4 of the proposed protocol) is to deter-
mine whether Alice is eligible to become a friend of Bob ac-
cording to Bob’s criteria, which requires that the set diﬀer-
ence distance between {a1, a2, · · · , an} and {b1, b2, · · · , bn}
is at most tb. The second stage (steps 5–7 of the proposed
protocol) is to determine whether Bob is qualiﬁed to be Al-
ice’s friend according to Alice’s criteria, which is deﬁned to
be the set diﬀerence distance between {a1, a2, · · · , an} and
{b1, b2, · · · , bn} is at most t0
a may
not be equal to ta since they represent Alice’s threshold val-
ues for the online and oﬀ-line situations respectively.
a. It is worth noting that t0
In more details, the protocol works as follows.
1. Alice logs into her account and sends a request to the
MApp for a matching with Bob. Meanwhile, Alice
downloads her information to her local computer. In
particular, Alice recovers her private key ska from capa
and proﬁle items ai (1 ≤ i ≤ n) from PEnc(ai, pka)
(1 ≤ i ≤ n).
2. The MApp fetches pb and H(rb) from Bob’s account
and sends pb to Alice.
3. Alice sends H(r0
b) to the MApp, where
r0
b = Rep0({a1, a2, · · · , an}, pb).
4. The MApp terminates the protocol if H(r0
b) 6= H(rb).
Otherwise, it fetches SEnc(pkb||skb, rb) from Bob’s ac-
count, generates a CAPTCHA instance capt by em-
bedding the fetched information in an image, and sends
capt to Alice.
5. Alice ﬁrst recovers SEnc(pkb||skb, rb) from capt and ob-
b. Alice sends
tains (pkb, skb) by decrypting it using r0
PEnc(ai, pkb) (1 ≤ i ≤ n) to the MApp.
6. The MApp fetches PEnc(bi, pkb) (1 ≤ i ≤ n) from
Bob’s account, computes PEnc(ai − bi, pkb) (1 ≤ i ≤
n), and sends a re-randomized and permuted version
of these values to Alice. Let these values be denoted
as Ri (1 ≤ i ≤ n).
7. Alice decrypts Ri (1 ≤ i ≤ n) and counts the total
a, she adds Bob as a
If n − x ≤ t0
number x of 0s.
friend.
From the description, it is clear that Bob is not required
to get involved in person, while the matching only relies on
his stored information under his account in the OSN.
2.2 Security Analysis
Here, we brieﬂy analyze the security of the proposed pro-
tocol with respect to three categories of adversaries.
The MApp has access to the obscured private keys of Alice
or Bob, namely capa and capb, but it cannot automatically
recover ska and skb if the employed CAPTCHA scheme is
secure. If a user wants stronger security, then she can en-
crypt her private key ﬁrst using a password and then gener-
ate a CAPTCHA instance by embedding the ciphertext in
an image.
With the above protocol, Alice learns Bob’s private key
skb if Alice is allowed to add Bob as a friend according to
Bob’s criteria, namely the set diﬀerence distance between
{a1, a2, · · · , an} and {b1, b2, · · · , bn} is at most tb. There-
fore, this is nothing more than that Bob shares his proﬁle
information with his potential friend. In step 4 of the proto-
col, the information SEnc(pkb||skb, rb) is sent to Alice via a
CAPTCHA instance, so that it veriﬁes Alice’s presence and
prevents a computer program from mounting automated at-
tacks. Therefore, it is secure for Bob.
Clearly,
in the protocol execution, Bob learns nothing
about Alice. Therefore, it is secure for Alice.
3. CONCLUSION
We have brieﬂy outlined a matching protocol to match
potential friends in OSNs. By using the techniques such as
fuzzy extractor and CAPTCHA, the protocol is user-friendly
and secure against the common attacks. The precise security
and performance analysis and potential improvements are an
ongoing research work in the Kindred Spirits project.
Acknowledgement
This is an ongoing work carried out in the Kindred Spirits
project (http://www.ksproject.nl/), which is sponsored by
STW’s Sentinels program in the Netherlands. The author
would like to thank Michael Beye, Arjan Jeckmans, Sandeep
Kumar, and Pieter Hartel for their comments.
4. REFERENCES
[1] L. von Ahn, M. Blum, and J. Langford, “Telling
humans and computers apart automatically,” Commun.
ACM, vol. 47, no. 2, pp. 56–60, 2004.
[2] M. J. Freedman and A. Nicolosi, “Eﬃcient private
techniques for verifying social proximity,” in
Proceedings of the 6th International Workshop on
Peer-to-Peer Systems, 2007.
[3] S. Guha, K. Tang, and P. Francis, “NOYB: privacy in
online social networks,” in Proceedings of the ﬁrst
workshop on Online Social Networks. ACM, 2008, pp.
49–54.
[4] Y. Dodis, L. Reyzin, and A. Smith, “Fuzzy extractors:
How to generate strong keys from biometrics and other
noisy data,” in Advances in Cryptology - EUROCRYPT
2004, ser. LNCS, C. Cachin and J. Camenisch, Eds.,
vol. 3027, 2004, pp. 523–540.
[5] P. Paillier, “Public-key cryptosystems based on
composite degree residuosity classes,” in Advances in
Cryptology, Proceedings of EUROCRYPT ’99, ser.
LNCS, J. Stern, Ed., vol. 1592, 1999, pp. 223–238.
[6] G. Mori and J. Malik, “Recognizing objects in
adversarial clutter: Breaking a visual CAPTCHA,”
Computer Vision and Pattern Recognition, IEEE
Computer Society Conference on, vol. 1, p. 134, 2003.
[7] M. M. Lucas and N. Borisov, “FlyByNight: mitigating
the privacy risks of social networking,” in Proceedings of
the 7th ACM workshop on Privacy in the electronic
society (WPES), 2008, pp. 1–8.
734