both the BF-RP and SA-RP algorithms can operate over a
much smaller state space. Such a speedup may facilitate
better RME characterization.
The specific device models used in the evaluation are
based on the distributions that Elerath and Pecht used [6].
We believe that the models ofElerath and Pecht are as good
1-4244-2398-9/08/$20.00 ©2008 IEEE
154
DSN 2008: Greenan et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:09 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
configuration
I-bimodal
2-bimodal
3-bimodal
4-bimodal
5-bimodal
6-bimodal
7-bimodal
(4,4)-RAID 10
FTV
8.60 x 108
4.74 x 108
3.01 x 108
1.69 x 108
1.51 x 108
1.09 x 108
8.29 x 107
uniform 4.34 x 108
Best RME
8.39 X 108
5.97 X 108
4.35 X 108
3.49 X 108
1.81 X 108
1.19 X 108
8.42 X 107
4.88 X 108
(5,3)-FLAT
FTV
3.30 X 109
1.94 X 109
1.23 X 109
9.24 X 108
6.07 X 108
4.53 X 108
3.40 X 108
1.56 X 109
Best RME
6.90 X 109
6.53 X 109
6.40 X 109
6.37 X 109
6.62 X 109
6.89 X 109
1.35 X 109
6.11 X 109
(6,2)-FLAT
FTV
4.98 X 108
2.94 X 108
1.72 X 108
1.33 X 108
8.76 X 107
6.50 X 107
4.99 X 107
2.34 X 108
Best RME
6.20 X 108
3.74 X 108
2.54 X 108
1.47 X 108
9.67 X 107
7.42 X 107
5.18 X 107
2.79 X 108
(7,1)-MDS
1.19 X 108
6.71 X 107
4.41 X 107
2.96 X 107
2.05 X 107
1.61 X 107
1.26 X 107
5.60 X 107
Table 3. MTTDL of a-disk configurations in hours.
configuration
3-bimodal
6-bimodal
9-bimodal
(9,3)-FLAT
FTV
3.45 x 108
1.57 x 108
8.81 x 107
uniform 2.70 x 108
Best RME
7.88 X 108
3.30 X 108
1.06 X 108
5.63 X 108
(10,2)-FLAT
FTV
9.83 X 107
4.25 X 107
2.57 X 107
8.01 X 107
Best RME
1.31 X 108
5.14 X 107
2.54 X 107
9.59 X 107
(ll,l)-MDS
3.13 X 107
1.27 X 107
5.75 X 106
2.77 X 107
Table 4. MTTDL of 12-disk configurations in hours.
as any currently available. Recently published analyses of
failure data [21, 16, 2] will hopefully result in better fail(cid:173)
ure models. We expect that such models will change the
MTTDL values, but not the placement that is most reliable.
The RME is based on the assumption that failures are inde(cid:173)
pendent. The RME equation may have to change if signifi(cid:173)
cant correlation is found in failure models.
When we developed the RME metric, we assumed that
sector failures would have a secondary effect on placement
decisions and so could be excluded from the RME metric.
We have some initial results for the RME metric in systems
with sector failures. For codes with a Hamming distance
greater than 2, the RME still correctly order placements by
reliability. For codes with a Hamming distance of 2, data
loss events are dominated by single-disk, single-sector fail(cid:173)
ures. For such codes, if every symbol occurs in at least
one minimal erasure of size two, e.g., like (6,2)-FLAT, then
placement had little affect on overall reliability. Whereas,
for (5,3)-FLAT, only the symbols 84 and 87 occur in a mini(cid:173)
mal erasure of size 2, and so placements based on the RME
maximize reliability.
7. Conclusions
We introduced the novel redundancy placement prob(cid:173)
lem in which a mapping, called a placement, of the sym(cid:173)
bols in a flat xOR-based code onto a set of heterogeneous
storage devices with known failure and recovery rates must
be found to maximize reliability. To solve this problem,
we developed the Reliability MTTDL Estimate (RME), a
simple model based on estimated device unavailability and
the Minimal Erasures List (MEL), a concise characteriza(cid:173)
tion of the fault tolerance of an xOR-based code. We de(cid:173)
veloped two redundancy placement algorithms, the BF-RP
algorithm based on brute force computation, only suitable
for small redundancy placement problems, and the SA-RP
based on simulated annealing, and suitable for larger prob(cid:173)
lems. Simulation results based on the High-Fidelity Re(cid:173)
liability (HFR) Simulator provide extensive empirical evi(cid:173)
dence that the RME correctly orders different placements for
a given code by MTTDL. Additional simulation results sug(cid:173)
gest that the placements found by the SA-RP algorithm are
significantly more reliable than the median placement. The
results ofBF-RP algorithm lead us to realize the existence of
isomorphic placements, sets of placements which have the
same MTTDL.
References
[1] G. A. Alvarez, W. A. Burkhard, and F. Cristiano Tolerating
multiple failures in RAID architectures with optimal stor(cid:173)
age and uniform declustering. In ISCA-1997: 24th Annual
International Symposium on Computer Architecture, pages
62-72, Denver, CO, June 1997. ACM.
1-4244-2398-9/08/$20.00 ©2008 IEEE
155
DSN 2008: Greenan et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:09 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
configuration
5-bimodal
IO-bimodal
15-bimodal
8.94 x 107
4.36 x 107
2.33 x 107
uniform 8.49 x 107
(17,3)-FLAT
Frv Approx. Best RME
1.29 X 108
5.02 X 107
2.61 X 107
9.62 X 107
(16,4)-FLAT
Frv Approx. Best RME
1.39 X 109
1.33 X 109
2.85 X 108
8.71 X 108
3.59 X 108
1.89 X 108
8.72 X 107
3.38 X 108
(19,1)-MDS
9.63 X 106
4.48 X 106
2.53 X 106
8.49 X 106
(18,2)-MDS
1.50 X 1010
5.71 X 109
1.94 X 109
1.27 X 1010
Table 5. MTTDL of 20-disk configurations in hours.
[2] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and
1. Schindler. An analysis of latent sector errors in disk
drives. SIGMETRICS Perform. EvaI. Rev., 35(1):289-300,
2007.
[3] M. Blaum, 1. Brady, J. Bruck, and J. Menon. EVENODD:
An efficient scheme for tolerating double disk failures in
RAID architectures. IEEE Trans. Comput., 44(2): 192-202,
1995.
[4] P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman,
1. Leong, and S. Sankar. Row-diagonal parity for dou(cid:173)
ble disk failure correction.
In FAST-2004: 3rd USENIX
Conference on File and Storage Technologies, pages 1-14.
USENIX Association, March 2004.
[5] J. R. Douceur and R. P. Wattenh0 fer. Optimizing file avail(cid:173)
ability in a secure serverless distributed file system. In Sym(cid:173)
posium on Reliable Distributed Systems. IEEE, 2001.
[6] J. F. Elerath and M. Pecht. Enhanced reliability modeling of
raid storage systems. In DSN-2007, pages 175-184. IEEE,
June 2007.
[7] S. Gaonkar, K. Keeton, A. Merchant, and W. H. Sanders.
Designing dependable storage solutions for shared applica(cid:173)
tion environments. In DSN-2006: The International Confer(cid:173)
ence on Dependable Systems and Networks, pages 371-382.
IEEE, June 2006.
[8] K. M. Greenan and 1. 1. Wylie. High-fidelity reliability sim(cid:173)
ulation of erasure-coded storage. Technical Report (to ap(cid:173)
pear), Hewlett-Packard Labs.
[9] 1. L. Hafner. WEAVER Codes: Highly fault tolerant era(cid:173)
sure codes for storage systems. In FAST-2005: 4th USENIX
Conference on File and Storage Technologies, pages 212(cid:173)
224. USENIX Association, December 2005.
[10] J. L. Hafner. HoVer erasure codes for disk arrays. InDSN(cid:173)
2006: The International Conference on Dependable Systems
and Networks, pages 217-226. IEEE, June 2006.
[11] J. L. Hafner, V. Deenadhayalan, T. Kanungo, and K. Rao.
Performance metrics for erasure codes in storage systems.
Technical Report RJ-l 0321, IBM, August 2004.
[12] J. L. Hafner, V. Deenadhayalan, K. Rao, and 1. A. Tomlin.
Matrix methods for lost data reconstruction in erasure codes.
In FAST-2005: 4th USENIX Conference on File and Storage
Technologies, pages 183-196. USENIX Association, De(cid:173)
cember 2005.
[13] 1. L. Hafner and K. Rao. Notes on reliability models for
non-MDS erasure codes. Technical Report RJ-I0391, IBM,
October 2006.
[14] S. Kirkpatrick, C. Gelatt Jr.., and M. Vecchio Optimization
by simulated annealing. Science, 220,4598:671-680, May
1983.
[15] Q. Lian, W. Chen, and Z. Zhang. On the impact of replica
placement to the reliability of distributed brick storage sys(cid:173)
tems. In ICDCS 2005: Proceedings ofthe 25th International
Conference on Distributed Computing Systems, pages 187(cid:173)
196. IEEE, 2005.
[16] E. Pinheiro, W.-D. Weber, and L. A. Barroso. Failure trends
in a large disk drive population. In FAST-2007: 5th USENIX
Conference on File and Storage Technologies. USENIX As(cid:173)
sociation, 2007.
[17] J. S. Plank. Erasure codes for storage applications. Tutorial
slides, presented at FAST-2005: 4th Usenix Conference on
File and Storage Technologies, December 2005.
[18] J. S. Plank, A. L. Buchsbaum, R. L. Collins, and M. G.
Thomason. Small parity-check erasure codes - exploration
and observations. In DSN-2005: The International Confer(cid:173)
ence on Dependable Systems and Networks, pages 326-335.
IEEE, July 2005.
[19] 1. S. Plank and M. G. Thomason. A practical analysis of
low-density parity-check erasure codes for wide-area stor(cid:173)
age applications. In DSN-2004: The International Confer(cid:173)
ence on Dependable Systems and Networks, pages 115-124.
IEEE, June 2004.
[20] K. Rao, J. L. Hafner, and R. A. Golding. Reliability for
networked storage nodes. In DSN-2006: The International
Conference on Dependable Systems and Networks, pages
237-248. IEEE, June 2006.
[21] B. Schroeder and G. A. Gibson. Disk failures in the real
world: What does an MTTF of 1,000,000 hours mean to
you? In FAST-2007: 5th USENIX Conference on File and
Storage Technologies, pages 1-16. USENIX Association,
2007.
[22] A. Thomasian and M. Blaum. Mirrored disk organization re(cid:173)
liability analysis. IEEE Trans. Comput., 55(12):1640-1644,
2006.
[23] 1. 1. Wylie and R. Swaminathan. Determining fault tolerance
ofXOR-based erasure codes efficiently. In DSN-2007, pages
206-215. IEEE, June 2007.
[24] H. Yu, P. B. Gibbons, and S. Nath. Availability of multi(cid:173)
object operations. In NSDI-2006: Proceedings of the Sym(cid:173)
posium on Networked Systems Design and Implementation,
May 2006.
1-4244-2398-9/08/$20.00 ©2008 IEEE
156
DSN 2008: Greenan et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:15:09 UTC from IEEE Xplore.  Restrictions apply.