(GTP)
TABLE II.
DATA SET SUMMARIES AND TERMINOLOGY USED
THROUGHOUT THE PAPER. EACH OF THE ORIGINAL DATA SETS IS
OBTAINED BY CRAWLING THE CORRESPONDING LIST OF SITES (AND
SUB-PAGE) AND COLLECTING ALL 4 TYPES OF DATA (WEB REQUESTS,
DOM CHANGES, TEMPORAL, AND PAGE SOURCE).
simple example, if the element is “,”
we would consider it as “div class9.” Third, for temporal
information, we ﬁrst extract features per visit then average
them within their respective cases,
then we apply the set
difference. Fourth, for page source, we do a set difference
based on words for text differences. For example, a text change
event with an old value of “Please subscribe to our content”
and a new value of “Please disable your adblocker to view our
content,” will result in a set difference of “subscribe, disable,
your, adblocker, view.”
2) Cleaning the Data: Recall that we load each site four
times to capture its dynamic content. A side effect is that we
end up with data (e.g., web requests and DOM mutations) that
is not necessarily related to circumvention, and can be due to
tracking, discernible non-ad resources, dynamic content, etc.
We ﬁlter these out before extracting features for circumvention.
First, for web requests, we identify tracking, social, and anti-
adblocking requests by applying EasyPrivacy [31], Adblock
Warning Removal List [2], Disconnect.Me [29], and uBlock
Origin’s GetAdmiral [70] ﬁlter lists. To ﬁlter out the requests,
we use Brave’s Adblock engine [21], a ﬁlter list parser that
supports EL-compatible rules. Second, we keep third-party ad
resources by looking at ones that have content-length larger
than 2 KB and have a max-age (within cache-control headers)
shorter than 40 days. We conclude on these numbers by
inspecting resources that were blocked by ABP. This gives us
a proﬁle about what content-length and max-age ad resources
should have. Third, we only consider successful web requests
(e.g., HTTP status code 200) and discard the ones that involve
redirection, errors, or no content (e.g., HTTP status codes 304,
400, 204). This is because circumvention related web requests
should have content such as JSON (that may deﬁne ad content)
and JS (code to re-inject ads).
8
Web Request Features
Number of content-types
Entropy of subdomains, paths, query parameters (by
content-types and ﬁrst/third-party)
Number of Mismatches of URL extension and content-type
Number of Mismatches of loaded resources
DOM Mutation Features
Number of DOM attribute changes (display, class, etc)
Number of DOM nodes removed (iframes, etc)
Number of elements blocked by EL (imgs, iframes, etc)
Number of DOM nodes added (a, imgs, etc)
Temporal Features
Number of blocked events (in ﬁrst 12sec of page visit)
Number of blocked events (in second 12sec of page visit)
Average cluster size of DOM mutations over time
Page Source Features
Number of iframes and images in ad positions
Number of distinct words, characters, and newlines
Entropy of subdomains, paths, query parameters of visible
iframes and images contained in hyperlinks (with target or
rel attributes)
Top


Top



Top

Top

TABLE III.
SOME OF THE FEATURES USED IN CV-INSPECTOR. THERE
WERE 93 FEATURES TOTAL IN THESE 4 CATEGORIES. THOSE MARKED AS
“TOP” WERE IN THE TOP-10 MOST IMPORTANT FEATURES IN SEC. IV-E.
Listing 2. Obfuscated URL Example. Taken from psychologyjunkie.com,
we compare a normal URL with an obfuscated one where subdomains & paths
are randomized. Although truncated, the path can reach up to 6K in length.
The entropy of the subdomains for the regular and obfuscated URLs are 1.58
and 2.25, respectively. Their ﬁrst path segments would have entropy of 1.79
and 4.56. As expected, the obfuscated strings have higher entropy.
0
1 /* Regular URL */
2 https://cdn.convertkit.com/assets/CKJS4.js
3 /* Obfuscated URL */
4 https://h239rh.lmyiwaakn.com/qO8HqaNP1NUGrt
5 d4qtgA1agJ2JAHpqoDo9QDqqYAptl4qaoF1dZ0...
C. Feature Extraction
Next, we describe the features that we extract from the
cleaned set difference to capture circumvention. Not all fea-
tures involve set differences, e.g., blocked events only appear
in the “With Adblocker” case. Table III lists the features that
we explored and highlights those that ended up being the top-
10 most important features. Then, we evaluate those features
and explain our intuition of why they can capture the presence
of CV services.
1. Web Request Features. One widely used obfuscation
technique is to randomize URL components and other fea-
tures extracted from web requests, resulting in noticeable
differences between “No Adblocker” and “With Adblocker”
cases. Listing 2 shows a comparison between a regular URL
and an obfuscated one by circumvention. To capture this
randomization, we treat URL components, such as subdomains
and paths, as strings, and we calculate their Shannon entropy,
based on the frequency of each character occurring in the
string. The idea is that randomized strings will have higher
entropy. An illustrative example is shown in Listing 2. As
expected, the obfuscated strings have higher entropy for both
Listing 3. Simple Ad Structure. An example of a simple ad structure that
can be used during ad re-insertion instead of an iframe.
0
1 
2
5 
how prolonged they are. For “With Adblocker,” we see fewer
DOM mutations within the ﬁrst ﬁve seconds, perhaps due to
many blocked events in the beginning. However, after that, we
see more bursts of DOM activity; notably, within the 12–18
seconds that are not present in the “No Adblocker” case. This
is captured by the smaller average cluster size. Interestingly,
this turned out not to be a top feature. We deduce that this
is because not all circumvention techniques cause large DOM
mutation changes. For instance, a site can load in a static ad
and use a simple ad structure, as shown in Listing 3. We further
discuss circumvention techniques in Sec. V-A2 and Table VI.
Since blocked events (i.e., any matching of ﬁlter rules in
Table I) can happen for sites that do not employ circumvention,
we want to investigate whether the timing of blocked events
can signal circumvention. Recall that we visit each page for
25 seconds, a parameter value chosen for reasons explained in
Sec. IV-A1. We compute the number of blocked events in the
ﬁrst or second 12 seconds of the page visit. We initially thought
the second half would be a differentiating feature, as the page
would exhibit the action of re-injecting ads and the adblocker
would then once again block those ads. However, we observed
that the ﬁrst half was more important, as shown in Fig. 7. This
may be because loading ads is a priority, leading to the blocked
events happening in the beginning of the page load. Also, ﬁlter
rules often aim to stop circumvention at the earliest possible
point. Ultimately, adblockers are more aggressive against sites
with circumvention, and therefore, cause more blocked events.
4. Page Source Features. Page source features characterize the
state of the site at the end of our page visit time. These features
convey whether circumvention was successful by identifying
possible ads that are still visible on the page. We discover
that circumvention exhibits behavior such as altering the DOM
structure of the ad to circumvent adblockers, while re-injecting
the ads back to speciﬁc, and often the same, locations.
First, we target speciﬁc DOM structures that hold ads
such as images or iframes. For images, we select those that
are contained by hyperlink elements (“”) with attributes
“target” and “rel,” as shown in Listing 3. The “target” attribute
deﬁnes how the browser behaves after a user clicks on the
link such as opening up in a new window or tab. The “rel”
attribute deﬁnes the relationship between the current page and
the outgoing link. We can use this to infer that if the outgoing
link is also third-party, then it is likely to be an ad.
Second, we identify possible ad locations that can be
utilized for re-injection. We use the “No Adblocker” page
source and extract all iframes. We then dynamically create
CSS selectors for the iframes, specifying at least three levels of
ancestors to make sure the selector is speciﬁc enough. We then
use these selectors on the page source of the “With Adblocker”
Fig. 7. Example of Temporal Features. We show the number of DOM
mutations (spikes) over time for “No Adblocker” and “With Adblocker”
(with the corresponding blocked events). We deﬁne a cluster of activity as
consecutive spikes (no more than one bin apart) and the cluster size as the
number of bins that it spans. The top ﬁgure shows the “No Adblocker” case,
which has 9 clusters with an average cluster size of 8.33. In the middle ﬁgure,
we show the “With Adblocker” case, which has 22 clusters with an average
size of 3.86. In the bottom ﬁgure, the dashed vertical lines represent whether
blocking events occurred. The majority of blocking happened within the ﬁrst
12 seconds when compared to the remaining time (e.g., 11 events vs. 1 event).
subdomains and paths. We further split web requests up into
ﬁrst-party and third-party sets. In addition, we count the the
number of different content-types extracted from their response
headers. Furthermore, we look at mismatch cases like when
a web request ends with a “.jpg” extension but its content-
type is “application/javascript.” Also, we look at whether
a particular path loads different numbers of resources. For
instance, when a path “a.com/images/” loads 10 images with
the “No Adblocker” case but then loads 15 images for the
“With Adblocker” case.
2. DOM Mutation Features. DOM mutation features can
uncover behavior such as when new ad-related elements are
added. For nodes being added and removed, we focus on
element types that can be associated with ads such as “,”
“,” and “.” For attribute changes, we focus
on changes such as the class attribute, visibility styles like
display and position, and the height of the element. Moreover,
we count the number of DOM attribute changes that involve
“abp-blocked-element,” which denotes the number of elements
blocked by EL.
3. Temporal Features. We expect that a site would exhibit
different behavior (events) over time when employing circum-
vention, as depicted in Fig. 1(b). Therefore, we examine the
timing of events to extract temporal features. Fig. 7 details
how we capture differences in DOM mutations over time by
utilizing spikes, clusters, and cluster sizes. By considering the
cluster size, we can identify bursts of DOM mutations and
9
DOM Mutation(No Adblocker)10,0001101001,00010,000L010025020015050DOM Mutation(With Adblocker)10,0001101001,00010,000L050100150200250Blocking Occurred(With Adblocker)10,0001L050100150200250Time (100 ms per Bin)side and count the number of images or iframes that remain.
To deal with sites that randomly alter their element attributes,
we do a second search (when the ﬁrst search does not match
any elements) with more generic selectors by looking at the
existence of attributes and not the values of them. For instance,
a selector of “div > div[opacity=’1’] > div[class=’rerejhf’]”
will turn into “div > div[opacity] > div[class].”
For both of these cases, we make sure that iframes and
images are visible and not hidden by the adblocker or pixel-
size used for tracking. This is possible by using our annotations
from Listing 1 to ignore elements that are invisible to the user.
D. Ground Truth Labeling
Let us revisit Sec. IV-A4 and discuss how we use the
original data sets, shown in the top two rows of Table II, to
create our GT data set, for training our classiﬁer.
Why Positive Labels are Important. A major challenge
for our GT data set is that positive samples (i.e., sites that
successfully employ circumvention) are rare and hard to ﬁnd.
First, there are simply not many sites that employ circumven-
tion today. For example, in Fig. 5, only 927, out of the top
one million Tranco sites, utilize circumvention. Second, we
deﬁne positive labels as not only attempting circumvention,
but also successfully circumventing adblockers, which further
reduces their number. Conversely, negative labels are easy to
discover because they correspond to sites that do not attempt
circumvention and to sites that do, but are unsuccessful at
evading the adblocker. For instance, see the imbalance in
Table V. Furthermore, human inspection and labeling of sites
is a labor-intensive process. To resolve these challenges, we
devise a methodology that reduces human labeling efforts
while ﬁnding many positive labels.
Candidates for Labeling (CL). We start from a list of URLs
that we consider candidates for labeling: this includes 2K do-
mains extracted from the ACVL, as described in Sec. III-B, and
popular Tranco top-2K sites. Domains extracted from ACVL
are not guaranteed to have positives, because compatible rules
from ACVL can be transferred to EL, thus EL can deal with
circumvention for some sites. Furthermore, since Fig. 5 reveals
that many ACVL domains are beyond the one million ranking,
we also consider the Tranco top-2K sites as candidates for
labeling, to include more popular sites of interest. We then
crawl the sites using our data collection methodology, depicted
in Fig. 6, and end up with approximately 6.2k sites (including
sub-pages) for our CL data set.
Labeling Each Site. We label each site,
in our CL data
set, as either successful circumvention (positive label) or not
(negative label). We capture a screenshot each time we visit
a page and depend on them to label our sites. Our labeling
methodology is as follows. First, we open up screenshots from
“No Adblocker” case and identify where ads are shown. Then
we open up screenshots from “With Adblocker” and compare
to see if the ads are removed. If an ad is still visible, we
label the site as positive; otherwise, we label it as negative.
Second, there may be “suspicious content.” For instance, ads
can look similar to page content rather than common ads,
either because they lack transparency (e.g., not annotated by
“Advertisement” or “Sponsored”), or they may be closely
(a) gamer.com.tw: “No Adblocker” case
(b) gamer.com.tw: “With Adblocker” case
Example of “Suspicious Content.” This gaming website shows
Fig. 8.