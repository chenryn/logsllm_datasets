number of error injections needed by studying the trend curves
of the Mask, Crash, SDC and Hang rates with increasing
number of error injections. As seen in Fig 9(a), the trend
curves for the different rates start stabilizing after 1000 er-
ror injections and only vary slightly with increasing error
injections. Thus, we conclude that a minimum of 1000 error
injections is required to provide a statistical summary of the
VS algorithm. Unless otherwise speciﬁed, all our experiments
use 1000 error injections (for each type of register; combined
GPR and FPR is 2000 error injections).
The random error injections also provide good coverage in
terms of the registers and bits in which the errors are injected.
A representative histogram is presented in Fig 9(b). It shows
that the errors are uniformly distributed among the 32 GPRs
(for both inputs). We similarly conﬁrm that the errors are
uniformly distributed among 64 bits within the registers. For
brevity, we have shown the coverage data (minimum error
injections required and register coverage) for error injections
in GPRs. Error injection experiments for all
the different
algorithms (GPR and FPR) show similar trends.
Figure 10 shows the different error injection outcome rates
for 1000 error injections each in GPRs and FPRs for the
VS algorithm. The resiliency proﬁle looks very different for
injections in the GPR and the FPR registers and we will
explore these differences in the following paragraphs.
Fig. 10: Resiliency Proﬁle for the VS algorithm. We show the different
error outcome rates for errors injected in GPR and FPR registers for
the two different inputs.
Error Injections in GPRs: Instructions that use GPRs form
the bulk of our application and are heavily used in memory and
control instructions and hence errors in them lead to the large
Crash rate (40.16%). Analyzing the Crash outcomes further,
we see that the majority of the crashes can be attributed
to the following two causes: 1) Segmentation Faults that
generally occur due to memory access violations (92%), and
2) Abort signals raised by the application/library when it
encounters internal constraint violations (8%). Analyzing the
Crash causing error sites further, we see no clear trend that
corruption of certain registers or bit positions in the registers
are more likely to result in a Crash. This is primarily because
all the GPR registers are used heavily in control (corruption of
any bit can cause a Crash) and memory (higher order bits more
likely to cause a Crash) operations and, hence, are vulnerable
to catastrophic outcomes when corrupted.
Error Injections in FPRs: Errors injected in the FPRs of
the VS application are Masked 99.7% of the time. This is
due to the way FPRs are used in the application. The VS
algorithm operates on images which are stored as 8-bit integer
pixels. Floating point operations are only used when some
manipulation of the pixels is required. To do this, the integer
pixels are converted to ﬂoating point, some transformation is
applied and then they are converted back to integer using a
saturation algorithm. The saturation algorithm causes many
potentially SDC causing errors to become masked.
B. Resilience of Approximate VS Algorithms
Figure 11a shows the error injection results for 1000 error
injection experiments in the GPRs of the different approxima-
tion algorithms compared with the baseline VS application for
both the inputs. Similar to the baseline VS algorithm, FPR error
injections in the approximate algorithms are masked > 99.5%
of the time and hence we do not show them here. The Crash,
Mask and Hang rates of the approximate algorithms is very
605
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
0102030405060024681012141618202224262830Number of InjectionsRegister NumberINPUT 1INPUT 202040608001000200030004000500060007000800090001000011000% Total Error InjectionsNumber of Error InjectionsCrashHangMaskSDC(a)(b)0102030405060708090100GPRFPRGPRFPR% of Total Error InjectionsSDCHangCrashMaskedINPUT 1INPUT 2(a)
(b)
Fig. 11: Resiliency proﬁles (Crash, SDC, Mask and Hang rates) for the different VS algorithms and toy benchmark WP. (a) Resiliency Proﬁle
for the VS algorithm and its approximate versions. We show the different error outcome rates for errors injected in GPR register for the two
different inputs. (b) Comparison of the Masked, SDC and Crash rates for error injections in two hot functions for VS application and the
stand-alone toy application WP.
similar to the baseline VS algorithm. This is not surprising
since the execution proﬁles of the approximate algorithms are
very similar to the baseline VS algorithm. For Input1, the SDC
rates increase from 1% (VS) to 3% and 2.5% for VS RFD and
VS KDS respectively. In both these approximate algorithms,
the errors that may have been masked in the ﬁnal image
(due to overlap by similar frames in the stitching process)
are now exposed as SDC due to a reduction in redundancy;
precipitated by dropping frames from input in VS RFD or due
to insufﬁcient matching key-points in VS KDS.
C. Trade-offs of studying an end-to-end workﬂow
As discussed in Section V-C, we ask the following question:
can we estimate the resiliency of the VS application by
studying the resiliency of the representative stand-alone WP
application? The results of the error injection experiments for
both VS and WP are shown in Figure 9(b).
The Crash, Mask and SDC proﬁles of the standalone WP is
different from that of an end-to-end workﬂow like the VS. In
the VS application, the output of the WarpPerspective function
would then be used to perform some other computation further
down the workﬂow and, hence, there is a compositional effect
where multiple computations ﬂow into each other. This causes
the effects of an error to manifest differently than if the
workﬂow ended at the output of the hot function. In our case,
the compositional effect leads to higher masking as the SDCs
that are generated by errors in the WarpPerspective function
are masked later in the workﬂow (for example, an adjacent
image could later be stitched over the area corrupted by the
function output). Hence, we conclude that it is essential to
analyze an entire end-to-end workﬂow, instead of just studying
hot kernels/functions, to get an accurate understanding about
the resiliency behavior of an application.
Fig. 12: Quality of SDCs generated by GPR error injections in
different video summarization algorithms. Each point on a given
curve represents the percentage of SDCs (Y axis) generated that have
an ED less than or equal to the ED represented on the X axis. (a)
and (b) - The ED of the SDC is calculated by comparing against
VS golden for Input1 and Input2 respectively. (c) and (d) - The ED
of the SDC is calculated by comparing against the corresponding
Approx golden for Input1 and Input2 respectively. Some of the curves
do not reach 100% on the Y axis due to a very small fraction of SDCs
that are classiﬁed as needing protection and not assigned an ED.
D. SDC quality
Figure 12 classiﬁes SDCs according to their ED. In order to
study a statistically signiﬁcant number of SDCs, we perform
5000 GPR error injections per input and analyze the resulting
SDCs. To calculate the ED of an SDC output image, we com-
pare it to a baseline golden image. For the SDCs produced by
606
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
0%10%20%30%40%50%60%70%80%90%100%VSVS_RFDVS_KDSVS_SMVSVS_RFDVS_KDSVS_SM% of Total Error InjectionsMaskedCrashHangSDCINPUT 1INPUT 2SDCHangCrashMasked0102030405060708090100VSWPVSWP% of Total Error InjectionswarpPerspectiveInvokerremapBilinear020406080100020406080100% of Total SDCs020406080100020406080100020406080100020406080100% of Total SDCsSDC Egregiousness degree020406080100020406080100SDC Egregiousness degree(a)(b)(c)(d)VSVS_RFDVS_SMVS_KDSthe VS algorithm, this is straightforward as the golden image is
the output of the error-free execution of the VS algorithm. For
SDCs produced by the approximate algorithms, there are two
potential golden images to compare against – the golden VS
output (VS golden) or the golden output of the corresponding
approximate algorithm (Approx golden). For example, ED
of an SDC produced by error injection in VS RFD can
to VS golden or by
be calculated by either comparing it
comparing it to VS RFD golden. We show the distribution
of SDC egregiousness using both these methods.
Figure 12(a) and Figure 12(b) show the ED of the SDCs
when compared against VS golden for Input1 and Input2 re-
spectively. The degradation in SDC quality in the approximate
algorithms, as evidenced by the larger fraction of SDCs having
higher EDs, is particularly sharp for Input1 (Figure 12(a)). On
further analysis we observe that this is because the deviation
between Approx golden and VS golden calculated using the
metric speciﬁed in Section V-D is large. Even though we
veriﬁed by visual inspection that the approximate algorithm
outputs were acceptable (Section IV-A), our metric assigns
them a large ED. This may imply that our metric is very
conservative and we undertake a discussion about
this in
Section VII. For example, the ED of the VS SM golden for
Input 1 when compared to VS golden is 37. It thus follows
that all subsequent SDCs produced by VS SM will have an
ED greater than or equal to 37. This is the reason we see the
shift in the ED curves of VS SM with respect to the baseline
VS in Figure 12(a).
Thus, to get a true understanding of the quality of the
SDCs produced by the approximate algorithms, we estimate
their egregiousness by comparing them to their corresponding
Approx golden output (Figure 12(c) and Figure 12(d)). The
graphs show that the overall trend for the SDC quality for
the VS and its approximate algorithms are very similar. The
approximations do not fundamentally change the quality of
the SDCs produced. For Input 2 (Figure 12(d)), the SDCs
from VS KDS have slightly worse quality (80% of SDCs
produced by VS have ED less than 6 as opposed to ED of
14 for VS KDS). This follows the trend seen in Section IV-A,
where for Input 2, VS KDS shows the most energy gains from
less computation as a result of dropped frames. This in turn
leads to a degradation of output quality. Another trend seen
is that overall, the SDCs produced are relatively benign (even
with our conservative metric). For example, for Input 2, 87%,
87%, 90% and 73% of the SDCs for VS, VS RFD,VS SM and
VS KDS respectively have an ED of less than 10. Thus, a large
majority of the SDC causing error-sites need not be protected
if an error of 10% is acceptable.
Hence, although approximating the VS application mini-
mally changes its resiliency proﬁle by slightly increasing the
number of SDCs generated, this is offset by the fact that a
large percentage of these SDCs may be tolerable and hence
the cost of protecting them is low. Thus, it is possible to
realize safe, yet efﬁcient approximations for this state of the
art Video Summarization algorithm from the point of view of
performance, power and reliability.
Fig. 13: a) Default output (VS) b) Approximate output (VS SM) c)
Absolute pixel difference between default and approximate outputs d)
Thresholded difference between default and approximate outputs.
VII. DISCUSSION ON SDC QUALITY METRIC
Gauging if an approximate algorithm is good enough or if
an SDC is tolerable in image processing applications like the
VS algorithm is heavily dependent on the image comparison
algorithm that calculates how closely the approximated image
or the faulty image matches the original image. While manual
inspection is still the best way to determine if the quality of an
image is acceptable, this is impractical in cases where a large
number of such images are generated or when an automated
decision has to be made based on the error seen in the output.
In Section V-D, we outline an algorithm and metric to estimate
the error in the output image, but our algorithm can produce
false positives and can label some SDCs as more egregious
than they actually are. For the outputs of the VS SM algorithm
the relative l2 norm generated by the image comparison al-
gorithm is approximately 37% and 8% for Input1 and Input2
respectively. This is because as can be seen in Figure 13(c), the
pixel difference of the two images is considerable as the pixels
in the faulty image have slightly shifted when compared to
the default image. But to a human viewing these two images,
there is no perceivable difference. Another factor to consider is
that two images having the same relative l2 norm may not be
equally egregious depending on the ﬁnal usage of the output.
For example, even if 30% of a faulty image is blacked out, it
may still be useful for surveillance or tracking if the remaining
70% had useful information that can be deciphered by a human
being. Estimating an automated metric to compare images used
for such domains remains an open problem.
VIII. RELATED WORK
A. Approximate computing
Section II-B discusses many trends in approximate com-
puting. A related area is analysis that performs criticality
testing [43], [44] and works that take advantage of soft compu-
tations - resilient code regions that result in tolerable output
corruptions, when perturbed by errors - to reduce resiliency
overheads in approximate environments [45], [46], [47], [48].
To the best of our knowledge, this is the ﬁrst work that directly
measures the resilience of approximate algorithms.
607
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:29:17 UTC from IEEE Xplore.  Restrictions apply. 
B. Resiliency
Soft error resilience has been studied over several decades
(dating back to 1978 [49]) with a large body of more recent
work at various levels of design hierarchy, such as the logic
level [50], [51], microarchitectural level [52], [53], and archi-
tectural level [54], [55], [56], [57]. For example, Mukherjee
et al. proposed the concept of the Architectural Vulnerability
Factor (AVF) in [54] to quantify the resilience of various
architectural components. In [52], Kim et al. studied the soft
error sensitivity of functional blocks using software simulated
fault
injections into the RTL model of a microprocessor
(picoJava-II). Though showing signiﬁcant masking effects of
more than 85% reported by Wang et al. in [53], none of these
prior works speciﬁcally considered the algorithmic effects
on soft error resilience, nor did they evaluate approximation
mechanisms with acceptable end-quality.
More recently,
there has been an increasing interest
in
studying resilience at the application level for low-cost re-
liability solutions. SWAT/mSWAT [58], [59], [60], [61] take
advantage of application’s abnormal behavior, referred to as
symptoms, to identify faulty units using light-weight diagnosis
algorithms. In [62] Thomas et al. propose the term EDC de-
scribing outcomes that deviate signiﬁcantly from the error-free