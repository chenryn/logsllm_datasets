891.1
R+LLC
ILLC
T-MI-FGSM
JSMA
BLB
EN
L1
CW2
EAD
Original
Model
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
0.0%
Defense-enhanced Models
Average
PD
TE
RT
DD
RC
EIT
PAT
IGR
EAT
Input Transformation
Adversarial Training Gradient Mask.
NAT
76.4% 57.0% 51.0% 7.9% 69.9% 66.7% 7.6% 6.9% 32.6% 0.1% 37.6%
52.7% 28.4% 18.5% 10.7% 51.6% 35.0% 4.8% 2.5% 13.3% 0.1% 21.7%
82.7% 80.4% 75.3% 12.2% 76.9% 79.9% 4.2% 4.5% 68.1% 0.0% 48.4%
84.7% 81.1% 82.4% 3.8% 77.4% 82.3% 0.0% 2.5% 76.5% 0.0% 49.1%
81.9% 77.8% 74.3% 0.7% 75.1% 79.7% 0.0% 0.2% 64.7% 0.0% 45.4%
78.7% 65.5% 69.5% 2.7% 73.0% 69.6% 0.0% 0.0% 47.5% 0.0% 40.7%
80.9% 79.8% 60.8% 5.4% 74.4% 71.4% 3.8% 21.3% 47.8% 1.4% 44.7%
88.9% 86.6% 83.3% 89.3% 79.2% 87.1% 83.2% 74.9% 92.9% 91.3% 85.7%
88.9% 86.1% 82.3% 81.6% 79.0% 87.4% 52.2% 70.5% 91.1% 14.8% 73.4%
79.9% 65.7% 61.2% 1.5% 76.9% 70.2% 3.0% 6.0% 29.9% 0.0% 39.4%
84.4% 86.0% 81.3% 6.7% 81.9% 86.0% 4.1% 5.1% 73.3% 0.0% 50.9%
86.6% 85.3% 83.7% 27.6% 78.2% 86.9% 0.9% 49.7% 88.5% 0.0% 58.7%
83.1% 71.4% 70.2% 11.2% 74.5% 78.5% 0.8% 0.0% 61.4% 0.0% 45.1%
68.0% 75.1% 72.7% 50.3% 73.5% 70.0% 37.1% 27.1% 75.5% 16.2% 56.6%
89.1% 86.4% 83.0% 89.8% 79.2% 87.4% 83.9% 74.1% 92.8% 91.1% 85.7%
88.8% 86.5% 83.0% 89.5% 79.2% 88.6% 82.9% 76.7% 92.5% 90.2% 85.8%
88.6% 86.3% 82.3% 82.8% 79.2% 88.0% 26.5% 74.4% 92.2% 14.6% 71.5%
88.5% 86.5% 82.5% 89.2% 79.1% 88.0% 79.3% 74.8% 92.7% 87.5% 84.8%
88.4% 86.6% 82.6% 88.4% 79.0% 86.3% 81.0% 76.2% 92.6% 88.4% 85.0%
82.2% 76.8% 72.6% 39.5% 75.6% 78.4% 29.2% 34.1% 69.8% 26.1% 58.4%
Average
The evaluation methodology proceeds as follows. Firstly,
for each attack, we select all successfully misclassiﬁed AEs
that are generated in Section IV-A. Then, to make the dataset
balanced for detection, we randomly select the same number
of normal examples from the testing set to build a mixed
set for each attack. To eliminate biases in our evaluation, all
selected normal examples can be correctly recognized by the
original model. Finally, we examine the effectiveness of the
three detection-only defenses against all kinds of attacks. For
the parameter settings of detection, we mainly follow the same
or similar settings as in their original papers. The details of
their parameter settings are deferred to Appendix IX-B.
Results. Due to the space limitation, we only present the
results of CIFAR-10 in Table VI (detailed results of MNIST
are reported in Appendix X) and analyze them as follows.
To measure the overall detection performance, we calculate
their AUC scores as AUC is independent with the manually
selected threshold. According to the results, all detection
methods can yield fairly high AUC scores against most attacks.
The average AUCs of the three detection methods are all
higher than 70%, i.e., they show comparable discriminative
power against existing attacks. Speciﬁcally, LID has the best
performance in terms of AUC than others in most cases.
However, even for the best detection method LID on CIFAR-
10, it almost fails to detect AEs generated by DF and OM, with
AUC about 65%, which is lower than that of FS or MagNet
(over 80% on average).
In addition to AUC, we also evaluate the true positive rate
(TPR) and false positive rate (FPR) of different detection
methods on the mixed testing set. In order to fairly compare
the detection rate (i.e., TPR), we try our best to adjust the
FPR values of all detection methods to the same level via ﬁne-
tuning the parameters. In our evaluations, we ﬁrst set the FPR
of all the detection methods to around 4%, and then compare
their TPRs.
According to the results, LID has the highest average TPR
against all kinds of AEs. Although FS and MagNet have
higher average TPRs on MNIST (i.e., more than 90%, see
Appendix X for details), their average TPRs on CIFAR-10 are
much lower. One possible reason we conjecture is that we only
choose one threshold for each detection method to discriminate
diverse AEs generated by all attacks. We suggest that we can
improve the TPR performance within an acceptable FPR of the
detection method via ﬁne-tuning the parameters or adjusting
the threshold. For instance, we believe the TPRs of FS against
DF, BLB, CW2 and EAD can be signiﬁcantly increased since
their corresponding AUC scores are much higher (all over
86%).
For detection-only defense, it is hypothesized that AEs with
higher magnitude of perturbation are easier to be detected
since most detection methods are based on the difference
between normal and adversarial examples. To better under-
stand the inﬂuence of the perturbation of AEs on detection,
we conduct a simple test on FGSM with different , and
the results are shown in Table VII. According to the results,
we observe that there is no clear relationship between the
magnitude of perturbation of AEs and detection AUC. Thus,
we argue that we cannot conclude AEs with higher magnitude
of perturbation are easier to be detected.
Remark 6. All detection methods show comparable discrimi-
native ability against existing attacks. Different detection meth-
ods have their own strengths and limitations facing various
kinds of AEs. It is not the case that AEs with high magnitude
of perturbation are easier to be detected.
V. CASE STUDIES
To further demonstrate the functionality of DEEPSEC as a
uniform and comprehensive analysis platform, we present two
case studies in this section.
A. Case Study 1: Transferability of Adversarial Attacks
The transferability is an intriguing property that AEs gen-
erated against one target model can also be misclassiﬁed
by other models. Although there has been several literature
(cid:23)(cid:25)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
EVALUATION RESULTS OF DETECTION-ONLY DEFENSES AGAINST ALL ADVERSARIAL ATTACKS
TABLE VI
Attack
t
e
s
a
t
a
D
UA/
TA
Objec-
tive
L∞
 = 0.1
UAs
0
1
-
R
A
F
I
C
L2
L∞
 = 0.1
TAs
L0
L2
AVERAGE
# of
Examples
Attacks
FGSM
BIM
PGD
R+FGSM
UAP
DF
OM
LLC
1794
1674
2000
2000
U-MI-FGSM 2000
1706
2000
2000
268
630
2000
2000
1994
2000
2000
2000
2000
1768.6
EAD (EN)
EAD (L1)
JSMA
BLB
CW2
R+LLC
ILLC
T-MI-FGSM
Detection-only Defenses
FS
MagNet
LID
FPR AUC
TPR
TPR
TPR FPR AUC
FPR AUC
100.0% 5.1% 100.0% 9.5% 2.9% 82.6% 99.1% 4.7% 93.5%
100.0% 2.9% 100.0% 6.0% 4.8% 70.7% 33.3% 3.2% 83.2%
94.6% 2.9% 99.1% 1.6% 4.5% 25.5% 1.8% 4.2% 53.0%
99.9% 3.5% 100.0% 0.4% 3.8% 16.5% 3.2% 4.3% 59.2%
100.0% 3.0% 100.0% 1.8% 4.1% 23.8% 6.3% 4.1% 57.1%
100.0% 5.3% 100.0% 2.9% 3.8% 76.3% 99.5% 5.9% 94.9%
9.2% 5.7% 64.0% 1.5% 3.9% 86.3% 21.5% 2.8% 81.0%
8.8% 4.9% 65.1% 25.0% 3.8% 89.0% 46.4% 3.9% 78.7%
100.0% 1.5% 100.0% 3.7% 9.0% 73.5% 100.0% 6.7% 91.8%
99.0% 5.7% 99.2% 11.7% 5.1% 71.0% 31.4% 3.8% 81.2%
79.2% 5.3% 96.1% 51.7% 3.3% 83.9% 2.6% 4.7% 61.2%
100.0% 5.8% 100.0% 10.0% 3.8% 45.0% 10.4% 3.8% 57.9%
71.5% 3.4% 94.4% 20.6% 3.7% 91.7% 53.2% 5.3% 92.3%
13.0% 3.1% 72.3% 1.7% 4.1% 89.3% 52.5% 4.3% 81.6%
19.9% 3.8% 77.6% 0.9% 3.7% 88.1% 38.4% 4.4% 81.8%
17.2% 4.0% 73.8% 1.9% 3.5% 89.8% 54.2% 5.0% 82.1%
23.0% 5.7% 76.3% 1.1% 3.8% 86.4% 35.6% 4.3% 81.4%
66.8% 4.2% 89.3% 8.9% 4.2% 70.0% 40.6% 4.4% 77.2%
TABLE VII
DETECTION-ONLY DEFENSES AGAINST
FGSM WITH DIFFERENT 
s FGSM
# of

t
e
s
a
t
a
D
examples
T
S
I
N
M
0
1
-
R
A
F
I
C
0.1
0.2
0.3
0.4
0.5
0.6
0.1
0.2
0.3
0.4
0.5
0.6
138
432
608
734
896
1032
1794
1796
1820
1820
1820
1820
AUC
LID
FS MagNet
90.4% 99.8% 100.0%
85.0% 99.4% 100.0%
93.7% 99.1% 100.0%
97.7% 99.0% 100.0%
98.2% 99.0% 100.0%
98.7% 99.0% 100.0%
100.0% 82.6% 93.5%
95.2% 89.7% 98.8%
39.6% 58.7% 99.0%
15.7% 31.5% 96.6%
6.4% 21.6% 91.8%
6.6% 17.8% 87.1%
discussing the transferability of AEs [2], [18], [50], [51], no
work comprehensively evaluates the transferability of AEs
generated by existing attacks. In this case study, we conduct
a series of experiments on existing attacks and compare their
transferability performance on different target models.
1) Experimental Setup: We use the same benchmark
datasets and corresponding original models as that in Sec-
tion IV-A. Besides, we prepare three additional DL models:
• Model 1: We train model 1 that is identical to the original
model, but with different random initializations.
• Model 2: We train model 2 that keeps the same con-
ﬁgurations as the original DL model, except the network
architecture is slightly different.
• Model 3: We train model 3 as a totally different model.
The evaluation methodology is that we ﬁrst independently
train the three above models on each dataset with comparable
accuracy. Then, we employ the three trained models to classify
the misclassiﬁed AEs generated in Section IV-A. Finally, to
compare the transferability of adversarial attacks, we evaluate
the MR and ACAC of each model for each dataset.
2) Results: We present the results of CIFAR-10 in Ta-
ble VIII, and the conclusions on MNIST are similar (detailed
results are reported in Appendix XI).
Apparently, all adversarial attacks show more or less trans-
ferability on other models. As shown in Table VIII, the trans-
ferability rates of most attacks on the three models are over
10%. Moreover, the average transferability rate of all attacks
on the three models is 42.4%. This empirically conﬁrms the
existence of transferability of all adversarial attacks.
In particular, the conﬁdence (ACAC) of AEs that success-
fully transfer to other models is higher than that of AEs that are
misclassiﬁed on the original model. For instance, on CIFAR-10
the average conﬁdence of AEs that transfer to the three models
is 0.812, while the ACAC of AEs misclassiﬁed by the original
model is 0.751. This may be explained as since successfully
transferred AEs are part of AEs misclassiﬁed by the original
model and low-conﬁdence AEs usually fail to transfer, the
conﬁdence of transferable AEs is selectively higher.
For the impact of model diversity, we observe that the attack
transferability differs marginally across different target models
(i.e., model 1, model 2 and model 3). On CIFAR-10, all three
models averagely achieve approximately 42% transferability
rate. It indicates that the transferability of AEs is independent
of the model architecture, which conﬁrms the ﬁnding in [50].
In general, different kinds of attacks tend to have different
transferability performance, which implies different attack
abilities under black-box scenarios. To be speciﬁc, the transfer-
ability differences of different attacks have two facets. First,
AEs generated by UAs are more transferable than those of
TAs. For CIFAR-10, the average transferability rate of UAs is
74.6%, which is much higher than that of TAs (i.e., 10.0%).
This conﬁrms the conclusion in [51]. Secondly, for both UAs
and TAs, L∞ attacks are much more transferable than others
(i.e., L2 and L0 attacks). In particular, we observe that the
average transferability rate of all L∞ UAs (i.e., more than
90%) is several times higher than other UAs on CIFAR-10.
Similar results are observed in TAs. We conjecture that one
possible reason is that L∞ attacks tend to perturb every pixel
of the original image with the L∞ constraint, and thus the AEs
generated by them are more perceptible than others, which can
be observed and conﬁrmed in Table III.
Remark 7. Different attacks have different transferability: (i)
we conﬁrm that UAs are more transferable than TAs; (ii) we
ﬁnd that L∞ attacks are more transferable than other attacks
(i.e., L2 and L0 attacks). Furthermore, the conﬁdence of AEs
that can transfer to other models is higher than that of AEs
that can only be misclassiﬁed by the original model.
B. Case Study 2: Is Ensemble of Defenses More Robust?
For classiﬁcation tasks, ensemble methods are widely used
in research and competitions to improve the performance [52],
[53]. Recently, the idea of ensemble has been used to defend
against adversarial attacks [16], [31], [54]–[56]. However, the
effectiveness of ensemble against adversarial attacks is still
chaotic: some believes that ensemble of multiple diverse clas-
siﬁers (e.g., clean or defense-enhanced models) can increase
(cid:23)(cid:25)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
TRANSFERABILITY RATE OF ALL ADVERSARIAL ATTACKS ON CIFAR-10
TABLE VIII
s
t
e
s
a
t
a
D
UA/
TA
Objec-
tive
Attack
Attacks
FGSM
 = 0.1
 = 0.2
L∞
 = 0.1
UAs
R+FGSM
BIM
PGD
U-MI-FGSM
UAP
DF
OM
Average of UAs
LLC
L2
L∞
0
1
-
R
A
F
I
C
 = 0.1
L0
L2
TAs
R+LLC