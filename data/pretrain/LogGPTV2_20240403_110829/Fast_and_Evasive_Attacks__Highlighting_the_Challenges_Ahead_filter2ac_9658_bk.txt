be greatly increased and so probably the number of samples required to generate
reliable reﬁnements would decrease. However, ScriptGen has been able to cor-
rectly generate a reliable reﬁnement of an initially empty state machine using a
training set of 50 conversations automatically generated through proxying. This
validates the ability of ScriptGen to learn new activities.
Triggering new activities. After having shown how ScriptGen is able to pro-
duce reﬁnements to the state machine, we need to investigate its capability to
reliably detect new activities. The previous section investigated the ability to
generate reliable reﬁnements, and therefore ScriptGen’s ability of not generat-
ing false positives. Here we want to investigate ScriptGen’s ability to reliably
detect new activities, and therefore false negatives. To do so, we deployed a new
ScriptGen honeypot, in the same conﬁguration as shown in Figure 7, but already
instructed with the state machine generated in the previous example for a value
of N equal to 50. Then we run against this honeypot a new activity on the same
port, namely the Microsoft PnP MS05-039 Overﬂow [17]. We followed the same
pattern used in the previous experiment: 100 runs spaced in time choosing a
204
C. Leita, M. Dacier, and F. Massicotte
triggering threshold equal to 50. The attack followed the ﬁrst path for the ﬁrst
5 requests, and only at that point triggered an unmatched request. Using just
50 samples of interaction, ScriptGen has been able to correctly reﬁne the state
machine adding a single path to the existing one. The reﬁned state machine
correctly handled all the 50 successive runs of the attack.
This is an extremely important result. First of all, it shows how ScriptGen-
based honeypots are able to reliably identify new activities. Also, since the two
activities are identiﬁable only after the exchange of 5 couples of request/answer,
it validates the importance and the power of the ScriptGen approach with respect
to the current state of the art in honeypot technology.
6 Conclusion
In this paper, we have shown the feasibility of using a completely protocol-
unaware approach to build scripts to emulate the behavior of servers under
attack. As opposed to the approach considered by the authors of the RolePlayer
system, we have deliberately refused to take advantage of any heuristic to rec-
ognize important ﬁelds in the arguments received from the clients or sent by
the servers. Instead, by using several instances of the same attack, we can auto-
matically retrieve the ﬁelds which have some importance from a semantic point
of view and are important to let the conversation between the client and server
continue. More speciﬁcally, we have shown that two distinct types of dependency
are important to take into account. We have named them, respectively, intra-
protocol and inter-protocol dependencies. We have proposed new algorithms to
handle them eﬃciently. We have also shown that this newly created mechanism
can be further enhanced to create new scripts online as new attacks are appear-
ing by, temporarily, proxying the requests and responses between the attackers
and a real server. Experimental results obtained with our approach are very
good and demonstrate the potential inherent in the large-scale deployment of
honeynets such as our Leurre.com project [3,4,5,6,7,8]. The ScriptGen approach
would in fact allow us to collect an even richer data set than the one we have
accumulated so far.
References
1. Spitzner, L.: Honeypots: Tracking Hackers. Addison-Welsey, Boston (2002)
2. Provos, N.: A virtual honeypot framework. In: Proceedings of the 12th USENIX
Security Symposium. (2004) 1–14
3. Dacier, M., Pouget, F., Debar, H.: Attack processes found on the internet. In:
NATO Symposium IST-041/RSY-013, Toulouse, France (2004)
4. Dacier, M., Pouget, F., Debar, H.: Honeypots, a practical mean to validate ma-
licious fault assumptions. In: Proceedings of the 10th Paciﬁc Ream Dependable
Computing Conference (PRDC04), Tahiti (2004)
5. Dacier, M., Pouget, F., Debar, H.: Honeypot-based forensics. In: Proceedings of
AusCERT Asia Paciﬁc Information Technology Security Conference 2004, Bris-
bane, Australia (2004)
Automatic Handling of Protocol Dependencies
205
6. Dacier, M., Pouget, F., Debar, H.: Towards a better understanding of internet
threats to enhance survivability. In: Proceedings of the International Infrastructure
Survivability Workshop 2004 (IISW’04), Lisbonne, Portugal (2004)
7. Dacier, M., Pouget, F., Debar, H.: Leurre.com: On the advantages of deploying
a large scale distributed honeypot platform. In: Proceedings of the E-Crime and
Computer Conference 2005 (ECCE’05), Monaco (2005)
8. Dacier, M., Pouget, F., Debar, H.: Honeynets: foundations for the development of
early warning information systems. In Kowalik, J., Gorski, J., Sachenko, A., eds.:
Proceedings of the Cyberspace Security and Defense: Research Issues. (2005)
9. CERT: Cert advisory ca-2003-20 w32/blaster worm (2003)
10. Leita, C., Mermoud, K., Dacier, M.: Scriptgen: an automated script generation tool
for honeyd. In: Proceedings of the 21st Annual Computer Security Applications
Conference. (2005)
11. Needleman, S., Wunsch, C.: A general method applicable to the search for similar-
ities in the amino acid sequence of two proteins. J Mol Biol. 48(3):443-53 (1970)
12. Cui, W., Vern, P., Weaver, N., Katz, R.H.: Protocol-independent adaptive replay of
application dialog. In: The 13th Annual Network and Distributed System Security
Symposium (NDSS). (2006)
13. Freiling, F.C., Holz, T., Wicherski, G.: Botnet tracking: Exploring a root-cause
methodology to prevent distributed denial-of-service attacks. In: Lecture Notes in
Computer Science, Springer-Verlag GmbH (2005) 319–335
14. The Honeynet Project: Know your enemy: Tracking botnets. Know Your Enemy
Whitepapers (2005)
15. Massicotte, F., Couture, M., De Montigny-Leboeuf, A.: Using a vmware network
infrastructure to collect traﬃc traces for intrusion detection evaluation. In: Pro-
ceedings of the 21st Annual Computer Security Applications Conference. (2005)
16. OSVDB: Microsoft windows lsass remote overﬂow, http://www.osvdb.org/5248
(2006)
17. OSVDB: Microsoft pnp ms05-039 overﬂow, http://www.osvdb.org/18605 (2005)
Fast and Evasive Attacks:
Highlighting the Challenges Ahead
Moheeb Abu Rajab, Fabian Monrose, and Andreas Terzis
Johns Hopkins University
Baltimore MD 21218, USA
{moheeb, fabian, terzis}@cs.jhu.edu
Abstract. Passive network monitors, known as telescopes or darknets,
have been invaluable in detecting and characterizing malware outbreaks.
However, as the use of such monitors becomes commonplace, it is likely
that malware will evolve to actively detect and evade them. This paper
highlights the threat of simple, yet eﬀective, evasive attacks that under-
mine the usefulness of passive monitors. Our results raise an alarm to the
research and operational communities to take proactive countermeasures
before we are forced to defend against similar attacks appearing in the
wild. Speciﬁcally, we show how lightweight, coordinated sampling of the
IP address space can be used to successfully detect and evade passive
network monitors. Equally troubling is the fact that in doing so attack-
ers can locate the “live” IP space clusters and divert malware scanning
solely toward active networks. We show that evasive attacks exploiting
this knowledge are also extremely fast, overtaking the entire vulnerable
population within seconds.
Keywords: Network Monitoring, Network Worms, Invasive Software,
Network Security.
1 Introduction
Passive network monitors (or so-called network telescopes [17]) have provided a
wealth of information in recent years about active scanning malware (e.g., [18,30]).
The relative ease of deploying passive monitors has made them instrumental in a
number of malware centric proposals, especially for early detection of global out-
breaks (e.g.,[36]), containment and quarantine (e.g., [15,19,22]), and forensic anal-
ysis [28,30]. However, much of this work—including our own—has only been pos-
sible because the attacks observed thus far have been rather crude in nature. Ar-
guably, the lack of sophistication in recent outbreaks has been justiﬁable, as thus
far, there has been little reason to do otherwise.
It is clear however, that attackers will naturally become more savvy as more
elaborate defenses are deployed. Indeed, since passive network monitors were
introduced, a number of Internet malware outbreaks have applied non-uniform
scanning to ﬁnd victims and in doing so, limit the activity observed by central-
ized monitors (e.g., CAIDA’s network telescope [17]). In response, the research
community has advocated the use of distributed telescopes (e.g., [1,36]) as a
D. Zamboni and C. Kruegel (Eds.): RAID 2006, LNCS 4219, pp. 206–225, 2006.
c(cid:1) Springer-Verlag Berlin Heidelberg 2006
Fast and Evasive Attacks: Highlighting the Challenges Ahead
207
way to increase detection speed by synthesizing multiple views of the infection
as seen from various vantage points [27]. Furthermore, active responders (e.g.,
honeynets) are now widely used to lure attackers, and capture the attacks’ pay-
loads to generate malware signatures [11,16] in near real-time. Unfortunately,
even the most sophisticated techniques for analyzing the data captured by net-
work monitors are vulnerable to evasive tactics that refrain from scanning these
monitors in the ﬁrst place. In fact, recent evidence indicates that certain classes of
malware already avoid well-known monitors (e.g., Agobot [37]), and completely
avoid preﬁxes of certain agencies 1.
In this paper we demonstrate the impending threat to network monitors by
presenting a simple technique that can dynamically evade such monitors. The
outlined approach raises a number of challenges for the research community
because unlike previous techniques that use static “do-not-scan” lists, this ap-
proach can produce agile evasive malware that proceeds in an online fashion and
completely undermines the utility of passive monitors. Speciﬁcally, the proposed
technique employs lightweight sampling of the IP space to identify live preﬁxes,
that is, preﬁxes that contain live networks, and isolates empty preﬁxes that
are either unused or dedicated to passive monitoring. Sampling simply involves
sending a small number of probes (e.g. TCP SYN packets) to random addresses
within each target preﬁx. A single response from any address indicates that the
preﬁx contains live hosts and is therefore classiﬁed as live. Otherwise, the preﬁx
is identiﬁed as empty. Our results show that this technique successfully isolates
large collections of distributed monitors and discovers 96% of the vulnerable
population by probing less than 5% of the entire IP space. While the main focus
of this paper is showing that sampling eﬀectively evades passive network moni-
tors, the proposed technique can be easily extended to evade active responders;
if not designed correctly, active responders generate distinctive behaviors that
are detectable by the same sampling mechanism.
To further illustrate this threat, we present malware infection strategies that
use knowledge assembled from oﬄine or online sampling to divert the infection
towards live preﬁxes. We show that malware exploiting these strategies can in-
fect more than 95% of the vulnerable population in tens of seconds while still
successfully evading large collections of distributed passive monitors.
The rest of the paper is organized as follows. The sampling process, a core
component of the proposed evasive techniques, is described in Section 2 and
evaluated in Section 3. In Section 4 we provide examples of practical malware
spreading strategies that employ such evasive techniques. Rather than provid-
ing guidelines for attackers, our goal is to highlight the challenges that network
monitors must overcome. In that regard, in Section 5 we discuss a number of
promising directions that can address these challenges. We highlight the diﬀer-
ences between our work and related previous proposals in Section 6 and close in
Section 7 with concluding remarks.
1 For
example, we have
to avoid
scanning preﬁxes listed at http://professionalsecuritytester.com/modules.
php?name=News&file=article&sid=70
educating each other
seen botmasters
208
M.A. Rajab, F. Monrose, and A. Terzis
2 Discovering the Live Population Via Sampling
Our sampling technique takes advantage of the highly clustered nature of the
allocated and live IP space [2,13] in order to eﬃciently detect preﬁxes that
contain live hosts. Speciﬁcally, we use a hierarchical sampling technique (shown
pictorially in Figure 1) that follows a depth-ﬁrst search strategy that, at ﬁrst,
probes addresses selected at random from each of the /8 preﬁxes. These probes
can take many forms, and might be a TCP SYN packet, an ICMP packet, or
an ACK packet with a popular source port (specially crafted to bypass stateless
ﬁrewalls). If at least one response is received, the corresponding /8 preﬁx is then
marked as live and the sampling process proceeds to send probes to the /16
preﬁxes within that /8. If no response is received, then the preﬁx is marked as
empty and no further probes are sent to that preﬁx. For each live /16 preﬁx, the
process continues in search of any live /24 preﬁxes.
/8 Prefixes Layer
live
empty
/16 Prefixes Layer
empty
live
empty
/24 Prefixes Layer
empty
live
empty
Fig. 1. Diagram illustrating the proposed hierarchical sampling process
Since the main goal of the sampling process is to detect passive network
monitors, it is important that the process itself evades detection. Therefore, to
stay undetected, the sampling process must send as few probes as possible to
each preﬁx and, at the same time, detect the live and empty preﬁxes with high
accuracy. Additionally, the probing mechanism must be chosen in such a way
that complicates the detection of probes and makes it diﬃcult to correlate the
probing activity itself. In this way, generating any useful signature for detecting
this activity quickly, becomes a complex task. These evasive measures can be
achieved, for example, by selecting popular target ports (e.g., port 80) that
easily “blend” the sampling probes within large volumes of innocuous traﬃc.
2.1 Sample Size Estimation
As stated earlier, the goal of the sampling process is to detect all live preﬁxes
while sending as few probes as possible. In what follows, we derive the maximum
number of samples n necessary to classify preﬁxes with high conﬁdence. While
applicable to all levels of the sampling hierarchy, the discussion that follows
describes how to derive the number of samples n for the /16 preﬁx layer. We do
so because the address allocation at the /8 preﬁxes is publicly available (e.g.,
Fast and Evasive Attacks: Highlighting the Challenges Ahead
209
Table 1. Notation used throughout the paper
Threshold probability of liveliness for a certain preﬁx
The total live population size
P (g) Distribution of live hosts at the /16 preﬁxes layer
∗(g) Marginal distribution of live hosts at the /8 preﬁxes layer
p
pl,g The probability of probing a live host in group g
β
N
n Maximum number of probes required to classify a preﬁx as live or empty
α
V
It
s
p
Conﬁdence level of the sampling classiﬁcation decision
Total number of vulnerable hosts
Number of infected hosts at time step t
Average scan rate (scans/time step) per infected host
Probability of contacting an address in the live IP space
IANA [9] and ISC [10]) so one can easily preclude all unallocated /8 preﬁxes.
Moreover, Zeiton et. al. [38] showed (in a study that applies only to /24 preﬁxes)
that by exploiting common network administration practices 2 it is possible to
use n = 11 probes and still detect more than 90% of the live /24 preﬁxes.
Sampling Model. Let pl,g be the probability of probing a live host in preﬁx g.
Then given n samples, the probability α of receiving at least one response from
preﬁx g is:
α = 1 − (1 − pl,g)n
(1)
Our objective is to ﬁnd the number of samples n necessary to contact at least
one live host within a certain preﬁx with probability α. Therefore, n is given by:
n =
log (1 − α)
log (1 − pl,g)
(2)
Ideally, n should be large enough to detect live /16 preﬁxes containing a
single live host. This, however, would require a prohibitively large number of
probes (e.g., approximately 301,000 probes for conﬁdence α = 0.99). Fortunately,
it is unlikely that such /16 preﬁxes are in use today—in reality, live preﬁxes
contain signiﬁcantly more live hosts. Therefore, from a practical standpoint, the
goal is to detect the /16 preﬁxes that contain the majority of live hosts and
exclude the empty or sparsely populated preﬁxes. With this in mind, we amend
the deﬁnition of an empty preﬁx to include preﬁxes with live host occupancy
(pl,g) below a certain threshold β. Accordingly, the maximum number of probes
necessary to detect a non-empty preﬁx can be calculated by replacing pl,g with
β in Equation 2.
Notice that as the threshold β increases in the denominator of Equation 2,
the number of required samples, n, decreases. On the other hand, if β is too
large, a signiﬁcant number of live preﬁxes could be potentially misclassiﬁed as
2 Namely, probing addresses commonly assigned by network administrators (e.g.,
a.b.c.1, a.b.c.129, etc.).
210
M.A. Rajab, F. Monrose, and A. Terzis
 0.1
 0.01
F
D
P
 0.001
 1e−04
 1e−05
 1
P0.99
 10
Prefix rank
 100
Fig. 2. Illustrative ﬁgure showing the marginal distribution of the Internet live hosts
∗(g) that is used to estimate P (g). The unshaded area represents the 99% of the live