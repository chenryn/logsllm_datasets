title:Practical Applications of Bloom Filters to the NIST RDS and Hard
Drive Triage
author:Paul F. Farrell Jr. and
Simson L. Garfinkel and
Douglas White
2008 Annual Computer Security Applications Conference
2008 Annual Computer Security Applications Conference
Practical Applications of Bloom ﬁlters to the NIST RDS and hard drive triage.
Paul Farrell
Naval Postgraduate School
Monterey, CA
Simson L. Garﬁnkel
Naval Postgraduate School
Monterey, CA
Douglas White
NIST
Gaithersburg, MD
Abstract
Much effort has been expended in recent years to cre-
ate large sets of hash codes from known ﬁles. Distribut-
ing these sets has become more difﬁcult as these sets grow
larger. Meanwhile the value of these sets for eliminating
the need to analyze “known goods” has decreased as hard
drives have dramatically increased in storage capacity.
This paper evaluates the use of Bloom ﬁlters (BFs) to dis-
tribute the National Software Reference Library’s (NSRL)
Reference Data Set (RDS) version 2.19, with 13 million
SHA-1 hashes. We present an open source reference BF
implementation and validate it against a large collection of
disk images. We discuss the tuning of the ﬁlters, discuss how
they can be used to enable new forensic functionality, and
present a novel attack against bloom ﬁlters.
1. Introduction
Previous work has identiﬁed Bloom ﬁlters as attractive
tools for representing sets of hash values with minimal er-
ror rates[17]. The National Institute of Standards and Tech-
nology (NIST) distributes a sample Bloom ﬁlter implemen-
tation in perl and two BFs containing a small subset of the
NIST RDS 2.13[20]. Nevertheless, to date there has been
no published research on large-scale BF implementations
optimized for speed; on-disk representations for BFs have
not been standardized; and BFs have not been publicly in-
corporated into open source forensic tools.
Meanwhile the NSRL’s RDS [19] continues to grow,
with the July 2008 RDS 2.21 release mapping 47,553,722
known ﬁles to 14,563,184 unique hashes. NIST has
also announced its intentions to distribute dramatically ex-
panded hash sets “of each 512-byte block of every ﬁle we
process”[20].
While storing these 14 million SHA1 hashes requires
291 megabytes—a small amount of storage in today’s
world—the ancillary material that accompanies these hash
codes expands the size of the RDS to almost 6GB, mak-
ing the ﬁles somewhat difﬁcult to distribute and work with.
The time to search this database is also increasing, since
most tools use either a binary search or some kind of index
tree, producing access speeds that scale with the log of the
dataset size. In our testing with very capable reference hard-
ware, we could only perform between 17 thousand and 85
thousand RDS hash lookups per second using SleuthKit’s
hfind command1, and only 4 thousand lookups per second
when the hashes were stored in a MySQL InnoDB database.
Bloom ﬁlters are an attractive way for handling very
large hash sets. In this paper we present a new BF imple-
mentation that can perform between 98 thousand and 2.2
million lookups per second on that same hardware.2
The RDS includes a signiﬁcant amount of metadata for
each ﬁle, including the ﬁle’s name, size, the software pack-
age and operating systems for which it was distributed, and
the publisher. This information is not used by SleuthKit’s
hfind command, Guidance Software’s EnCase[11], or
other tools we have evaluated. This metadata is largely un-
exploited which, when accessed efﬁciently, can assist in the
rapid analysis and triage of newly acquired hard drives.
1.1. This paper’s contribution
This paper applies and extends previous work on BFs to
the National Software Reference Library’s (NSRL) Refer-
ence Data Set (RDS), speciﬁcally:
• We present nsrl bloom, a new, efﬁcient, highly conﬁg-
urable, open source Bloom ﬁlter implementation in C.
• We modiﬁed fiwalk, an automated forensics tool
based on SleuthKit, to use on our BF implementation
to automatically exclude “known goods.”
• We evaluate the performance of BFs created with dif-
ferent parameters and compare actual results with the
NSRL RDS to the results predicted by theory.
• We evaluate performance of BFs compared to lookups
in sorted ﬂat-ﬁles (what SleuthKit uses) and lookups
of hashes stored in a large MySQL database.
1The range results from the fact that hfind’s binary search algorithm
terminates early when it ﬁnds a hash that is in the database. As a result,
looking up a hash that is in the data set is roughly 5 times faster than
looking up a hash that is not. Surprisingly, MySQL exhibits similar per-
formance for both kinds of hashes.
2Once again, the range is the result of the difference in time between
looking up a hash that is not in the database and one that is. Unlike binary
searches on sorted data, BFs terminate faster when searching for data that
is not present.
1063-9527 2008
1063-9527 2008
U.S. Government Work Not Protected by U.S. Copyright
U.S. Government Work Not Protected by U.S. Copyright
DOI 10.1109/ACSAC.2008.12
DOI 10.1109/ACSAC.2008.12
3
13
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
Windows 2000, XP and Vista.
• We evaluate the coverage of RDS over fresh installs of
• We present a novel attack against the use of BFs to
• We evaluate the use of BFs for cross-drive analysis and
eliminating “known goods.”
the distribution of extracted features.
1.2. Related Work
Bloom introduced “hash coding with allowable errors”
in 1970[2]. Dillinger and Manolios showed how BFs could
be produced that are fast accurate, memory-efﬁcient, scal-
able and ﬂexible[8]. Fan et al. introduced Counting Bloom
Filters, in which small integers are stored in the vector in-
stead of individual bits[9]. Bloomier Filters[5] use layered
BFs for mapping entries to one of multiple sets. Broder
presents equations for computing the optimal number of
hashing functions can be calculated to provide a minimal
false positive rate[3]. Manolios provides a simple online
calculator for computing these values[14].
BFs have since been applied to various forensic
applications[3]. Roussev et al. proposed using BFs for stor-
ing ﬁle hashes[17]. The researchers also stated object ver-
sioning could be detected by piecewise hashing ﬁle parts,
noting “it is possible to use ﬁlters with relatively high false
positive rates (> 15%) and low number of bits per element
(3-5) and still identify object versioning.” Unfortunately
the source code for their program, md5bloom, was never
In 2007, Roussev et al. expanded this research
released.
with Multi-Resolution Similarity Hashing to detect ﬁles that
were similar but different[18].
White created a sample BF implementation in Perl and
distributed it from the NIST website[19]. This code was
ﬂawed, in that each bit of the MD5 or SHA1 code was used
multiple times to compute multiple Bloom hash functions.
Furthermore, because it was written in perl, excessive mem-
ory consumption prevented this program from being able to
digest the entire RDS.
1.3. Outline of paper
Section 2 discusses BFs and our BF implementation.
Section 3 discusses our experience at applying our imple-
mentation to the NSRL RDS. Section 5 discusses the impli-
cations of this work to mainstream forensics research and
practice. Section 6 concludes.
2. High-performance BFs
2.1. Introduction to Bloom ﬁlters
Fundamentally the Bloom ﬁlter is a data structure that al-
lows multiple values V0 . . . Vn to be stored in a single ﬁnite
bit vector F . As such, BFs support two primitive opera-
tions: storing a new value V into a ﬁlter F , and querying as
to whether or not a value V 0 is present in F .
BFs work by computing a hash of V and scaling this
hash to an ordinal between 0 and m, producing a bit i. This
bit is then set in the bit vector F which is also of size m. To
query the ﬁlter to see if V 0 is present, the value V 0 is hashed
and scaled to produce bit i0. If bit i0 in the ﬁlter is not set,
then value V 0 could not have been stored in the ﬁlter.
If bit i0 is set, the bit may be set because V 0 was previ-
ously stored in the ﬁlter. Alternatively, another value V 00
may have been stored that has a scaled hash i00 that is equal
to i0. The values V 0 and V 00 are aliases: users of the ﬁlter
cannot determine which of these two values was previously
stored. Because of this property, BFs are said to offer prob-
abilistic data retrieval:
if a BF says that a value was not
stored in the ﬁlter, then it was deﬁnitely not stored. But if
the BF says that a value was stored, the value might have
been stored; alternatively, an alias may have been stored.
As more information is stored in a BF, the probability of
aliases and false positives increase.
In practice, multiple hash functions f1 . . . fk are used to
store a single value V into ﬁlter F . This constellation of bits
is referred to as an element. Storing data thus requires set-
ting k different bits (i1 . . . ik) in ﬁlter vector F while query-
ing requires checking to see if those bits are set.
k:
BFs can thus described by four parameters:
m:
the number of bits in the ﬁlter. (In this paper we
additionally use M to denote log2(m).)
the number of hash functions applied to produce
each element
the number of bits set per element in the ﬁlter
the hash function
b:
f:
Once data is stored in the ﬁlter, additional parameters
can be used to describe the ﬁlter’s state:
n
p
the number of elements stored in the ﬁlter
the probability that a value V , reported to be in the
ﬁlter, was actually stored in the ﬁlter.
As discussed elsewhere[16, 15, 17], the probability that
an element in a ﬁlter will not be set is:
P0 = (1 − 1/m)kn
This can be approximated as:
P0 = e−kn/m
The theoretical false positive rate is approximated as:
Pf p = (1 − e−kn/m)k
(1)
(2)
(3)
2.2. Performance Characteristics
Bloom ﬁlters are similar to hash tables (HTs), in that a
large number of sparse values can be stored compactly in
a single data structure. Once stored, the structure allows a
values’ presence or absence to be queried in constant time
irrespective of the amount of data that has been stored. BFs
have an advantage over HTs in that data can be stored in
signiﬁcantly less space; they have the disadvantage that the
retrieval is probabilistic—a given BF might say that the data
is present, when in fact it is not.
414
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
Another commonality between BFs and HTs is that nei-
ther enjoys locality of reference[7] because data is hashed
throughout the structure. Because of real performance char-
acteristics of memory caches and memory hierarchies, con-
siderable performance advantages can be achieved on mod-
ern computers by minimizing the memory footprint of a
data structure. This is especially true for structures like BFs
and HTs that do not access memory in any predictable or-
der: there is no way to achieve locality of reference other
than shrinking the data structure to ﬁt within a cache.
Consider a modern Macintosh iMac desktop based on
an Intel Core 2 Duo microprocessor. This processor runs
with an internal clock speed of 2.4Ghz. But because there
is no locality of reference, the speed of accessing each BF
bit will be roughly equal to the speed of the memory sys-
tem in which that element is stored (Table 1), ignoring the
overhead associated with the BFs hashing and bookkeep-
ing. A 16K BF that ﬁts entirely into the computer’s 32K L1
data cache can be accessed at the rate of roughly 40 million
20-function queries per second (the remainder of the cache
is required for the state associated with the aforementioned
hashing and bookkeeping). On the other hand, a ﬁlter that
is 500MB in size and ﬁts entirely in main memory can only
support 710,000 hash lookups per second, because the com-
puter’s high-performance memory subsystem nevertheless
requires 70 nanoseconds to fetch each hashed bit.
Most memory systems are pipelined, allowing multiple
reads to be outstanding at any given time. Furthermore, the
Core 2 Duo can execute 4 instructions per cycle. This capa-
bility will be used to perform bookkeeping activities such as
incrementing loop counters and shifting bits; eventually the
thread will stall until the requested bit is fetched from the
memory subsystem in which it resides. If we can reduce the
delay in fetching the data from memory and the number of
times we must fetch per lookup, we can signiﬁcantly speed
the hash lookup process. By varying the size and number
of hash functions for a BF, we can optimize our data set
representation for performance.
2.3. nsrl bloom
White’s original code[19] had a ﬂaw that caused each
bits of the MD5 or SHA1 hash in multiple Bloom hash func-
tions. As a result, the bits were correlated and BFs created
with the perl code showed a factor of 10 more false pos-
itives than predicted by theory. For BF’s to be effective,
the hashing functions must be truly random and indepen-
dent. To avoid this correlation, our implementation simply
divides the hash into pieces based on the size of the BF: a
Bloom ﬁlter with k = 4 and M = 28 uses the ﬁrst 28 bits
of the SHA1 for the Bloom hash function f1, the second
28 bits for f2, and so on. Because these hash functions are
strong, bits are not correlated with one another. But as a re-
sult, k × M must be less than 128 a Bloom ﬁlter built from
MD5 hashes and 160 for SHA1 hashes.
Starting with this code base, we implemented a fast and
conﬁgurable BF implementation in C with both C and C++
bindings. Filter vectors are stored in binary ﬁles, with the
ﬁrst 4096-bytes of each ﬁle containing the ﬁlter’s parame-
ters and a comment. The implementation has a simple but
usable API consisting of six C functions:
• next_bloom_create() — Creates a Bloom ﬁlter
with a speciﬁed hash size, M, k and an optional com-
ment. The Bloom ﬁlter can reside in memory or be
backed to a ﬁle.
• nsrl_bloom_open() — Opens a previously cre-
ated Bloom ﬁlter, reading the parameters from the ﬁle.
• nsrl_bloom_add() — Adds a hash value to the
• nsrl_bloom_query() — Queries the Bloom ﬁlter
• nsrl_bloom_set_encryption_passphrase
— Adds string as a cryptographic passphrase for the