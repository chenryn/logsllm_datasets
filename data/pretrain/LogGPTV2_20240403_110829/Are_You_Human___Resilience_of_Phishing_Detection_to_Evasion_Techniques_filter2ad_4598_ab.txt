following considerations:
(1) The aim is to assess the effectiveness of phishing detection
bots in bypassing human verification evasion techniques.
Therefore, we have to make sure first that the ‘naked’ phish-
ing payload, i.e., not armed with any evasion technique, can
be easily detected.
(2) So, we have designed our own phishing pages instead of using
existing phishing kits to prevent possible detection based on
the previously observed attacks.
(3) The design of the phishing kits targets three major services:
Gmail, PayPal, and Facebook login pages.
(4) We download all the external resources (e.g., pictures) from
each target and generate a .zip package. This step is necessary
since external resources like web favicons (favorite icons),
logos as well as web beacons [36] play an important role for
anti-phishing companies to track and detect phishing attacks
for their websites [23].
(5) For Facebook and PayPal login pages, we just clone their
HTML source code, remove JavaScript code, and external
requests. We design a Gmail login page from scratch since
the original page uses a heavily obfuscated JavaScript code
to generate HTML tags at run-time. All the three generated
phishing pages look exactly like their true versions.
Reporting and Monitoring Process. We submit phishing re-
ports by either using an online form (GSB, SmartScreen, Net-Craft,
and YSB) or sending an email (OpenPhish, PhishTank, and APWG).
In the main experiment, we only generate one phishing URL for
each domain. We never submit a domain to more than one anti-
phishing bot. To monitor the results of the reports for each URL,
we call GSB API to check if any of the URLs appeared in the GSB
blacklist. Regarding OpenPhish, PhishTank, and APWG, we down-
load the corresponding blacklist feeds every half an hour to check if
they blacklisted our URLs and when. NetCraft notifies the reporter
through emails by sending the results of the reports. SmartScreen
has no publicly available API endpoint to check the results of the
reports. Therefore, we develop a Python script to open the reported
URLs using the Microsoft Edge browser, taking screenshots every
10 minutes for the first 72 hours and every 5 hours for the rest of
the experiment, for several days.
Previous work showed some inconsistency among different brows-
ers that use the same anti-phishing entities like Mozilla Firefox and
Google Chrome (both use GSB) because of the different caching
implementations of the GSB Update API (v4) in each browser [21].
In this experiment, our goal is not to check which browser faster
blacklists phishing pages but instead, we test which anti-phishing
engines can bypass the human verification evasion techniques.
4 EXPERIMENT RESULTS
Oest et al. [21], showed that the average blacklist time (i.e., the
time between the URL submission and its appearance in one of
the blacklists) was 126 minutes without using the web-cloaking
technique and 238 minutes with web-cloaking. They also showed
that anti-phishing engines could only detect 23% of the phishing
URLs armed with web-cloaking. We expect a lower detection rate for
the phishing sites instrumented with the three human verification
techniques because they are more advanced and less observed in
real-world phishing attacks.
4.1 Preliminary Test
In our initial test, we have submitted three URLs to seven anti-
phishing entities targeting PayPal, Facebook, and Gmail without
using any evasion technique. The test lasted for 24 hours. Based
on the results from the previous study [21], 24 hours is enough to
check if a reported URL is classified as malicious. For all seven anti-
phishing bots, we received traffic to our webserver within the first
30 minutes after we reported the URLs. Table 1 shows the results of
the preliminary tests. Since YSB was unable to detect even one of
the phishing sites, we had to exclude it from the main experiment.
We also observed that except for GSB and NetCraft, none of
the anti-phishing bots could identify the fake Gmail login pages as
phishing attacks. Therefore, we also excluded Gmail from our target
list in the final experiment. We suspect that the Gmail login page
was more difficult to detect compared to PayPal and Facebook pages
because of the different design approaches we used. As mentioned
earlier, we have implemented the Gmail login page from scratch,
whereas for PayPal and Facebook, we cloned the original HTML
code. Moreover, log inspection shows that NetCraft, OpenPhish, and
PhishTank submit the HTML form tags automatically by filling the
‘username’ field with different values (we do not log the password
filed on our servers).
The results presented in Table 1 show that:
(1) There exist a relationship between different vendors. For ex-
ample, the URLs we reported to OpenPhish also appeared in
other blacklist feeds. The results also suggest that GSB uses
other major blacklist feeds.
(2) For the URLs submitted to OpenPhish and PhishTank, we
received abuse notification emails from PhishLabs [37] sent
to the registered abuse notification email address related to
our IP addresses.
(3) Within the first two hours after reporting the URLs to Open-
Phish, we received a high amount of requests sent to our
servers (81,967 requests). Their analysis reveals that anti-
phishing bots looked for files related to: i) famous web-shells,
ii) possible phishing kits (.zip files), and iii) possibly stolen
credentials stored on the server (.log and .txt files).
4.2 Main Experiment
Table 2 shows the results of the main experiment to evaluate the ef-
fectiveness of six major anti-phishing entities in detecting phishing
websites protected by evasion techniques. The experiment lasted
Are You Human?
IMC ’20, October 27–29, 2020, Virtual Event, USA
Table 1: Preliminary test results after reporting the Gmail (G), Facebook (F), and PayPal (P) phishing URLs.
Reported to
# of requests Unique IPs Reported pages
Also blacklisted by
GSB
NetCraft
APWG
OpenPhish
PhishTank
SmartScreen
YSB
8,396
6,057
2,381
81,967
4,929
1,590
82
69
63
86
852
275
81
34
G, F, P
G, F, P
G, F, P
G, F, P
G, F, P
G, F, P
G, F, P
-
GSB
GSB
GSB
-
PhishTank, GSB,
APWG, SmartScreen
OpenPhish, GSB
Blacklisted targets
G, F, P
G, F, P
F, P
F, P
F, P
F, P
-
Table 2: Results of the main experiment after reporting
phishing URLs. X/Y shows the number of detected URLs (X)
out of all submitted ones (Y), where Alert box (A), Session-
based (S), or Google reCAPTCHA (R) are used to hide phish-
ing sites from server-side anti-phishing bots.
Anti-phishing
Facebook
bots
GSB
NetCraft
APWG
OpenPhish
PhishTank
SmartScreen
A
3/3
0/3
0/3
0/3
0/3
0/2
S
0/3
2/3
0/3
0/3
0/3
0/2
R
0/3
0/3
0/3
0/3
0/3
0/2
PayPal
S
0/3
0/3
0/3
0/3
0/3
0/3
A
3/3
0/3
0/3
0/3
0/3
0/3
R
0/3
0/3
0/3
0/3
0/3
0/3
for two weeks in May 2020. Nevertheless, we received about 90%
of the traffic during the first 2 hours after reporting the URLs.
Regarding the alert box protection, GSB was the only engine
that detected all 6 reported URLs, on average 132 minutes after
submission, which means that GSB can detect phishing websites
protected by alert boxes by handling them in a browser simulation.
The log analysis on our server reveals that GSB bots clicked on
the ‘confirm’ button in the alert box and successfully retrieved
phishing content, while other anti-phishing engines never reached
the phishing content because they failed to click on the alert box.
When it comes to session-based evasion, NetCraft was the only
engine that detected 2 out of 6 reported URLs (6 and 9 minutes
after the submission). Interestingly, the log analysis indicates that
NetCraft bypassed all six session-based pages and reached the phish-
ing sites, but only 2 of them were detected as such. No other anti-
phishing engine bypassed the session-based protection.
Regarding Google reCAPTCHA, as expected, none of the anti-
phishing engines could detect even one out of 35 reported URLs,
which means that it is so far the most effective protection solution
of phishing websites available to malicious actors.
The results shown in Table 2 indicate that, in general, human
verification techniques raise serious challenges to anti-phishing
bots. In total, all the major server-side anti-phishing bots could only
detect 8 out of 105 phishing URLs used in the main experiment.
Fetching the phishing content depends on bypassing the evasion
technique, which, as our experiment shows, is not a trivial task for
anti-phishing bots. One possible solution to this problem is to let
the end-users solve the challenge and reveal the final page content
to client-side anti-phishing engines.
5 TESTING CLIENT-SIDE ANTI-PHISHING
EXTENSIONS
Client-side extensions have access to the same content as users. If
the user bypasses CAPTCHA (e.g., confirming alert box, solving
Google reCAPTCHA, or pressing ‘proceed’ button in session-based
pages) and visits the second page, the extensions have also access
to the new possibly malicious content.
Therefore, in a separate experiment, we install the six most pop-
ular anti-phishing extensions on Mozilla Firefox (see Table 3). To
make sure there is no conflict between the extensions, we use dif-
ferent Firefox profiles for each extension and disable GSB. For each
extension, we submit 9 phishing URLs (3 URLs per evasion tech-
nique). We also use the Burp Suite7 tool to capture the exchanged
traffic related to each extension. In this way, we can read all TLS-
encrypted requests between extensions and their servers. Then, we
visit each URL three times with a 5-hour window between them.
Table 3 shows that none of the extensions could detect our phish-
ing pages. Monitoring the traffic generated by the extensions reveals
that they only collect the URLs visited by the user, send them to
their servers, and check the URLs against their own blacklists. As
shown in Table 3, four out of six extensions send ‘naked’ URLs
(without hashing) along with all the query parameters to their
servers. Since they do not create any feature vector on the client
machine and only send the URL to the server for further analysis,
they operate like their server-side counterparts, and thus, they are
unable to detect CAPTCHA-protected phishing attacks.
5.1 Discussion
In Section 4, we have presented three quite effective evasion tech-
niques based on human verification to avoid detection by anti-
phishing engines. The important question is how popular these
techniques are among attackers and what is the solution for effec-
tive handling of the attacks that use these techniques.
With respect to popularity, it is unlikely to find these attacks in
public blacklist feeds because they mainly rely on anti-phishing
engines that cannot bypass advanced types of evasion techniques.
In a parallel and independent research study by Oest et al. [38], the
authors analyzed 4,393 URLs and only found five of them that used
the CAPTCHA challenge and two using the alert box technique
to avoid detection. In recent work, Maroofi et al. [17] studied a
reCAPTCHA-protected phishing URL that prevents crawlers from
7https://portswigger.net/burp
IMC ’20, October 27–29, 2020, Virtual Event, USA
Sourena Maroofi, Maciej Korczyński, and Andrzej Duda
Table 3: Client-side anti-phishing extensions. Number of installations is the sum of installations for Chrome and Firefox. X/Y
shows the number of detected URLs (X) out of all submitted ones (Y).
Extension
Avast Online Security
Avira Browser safety
TrafficLight
Emsisoft Browser security
NetCraft Anti-phishing
Online Security Pro
Company
Avast
Avira
BitDefender
Emsisoft
NetCraft
Comodo
# of installations
10,800,000+
7,350,000+
665,000+
80,000+
58,000+
14,000+
Sending URLs
✓(plain)
✓(plain)
✓(plain)
✓(hashed)
✓(hashed)
✓(plain)
Sending Params X/Y
0/9
0/9
0/9
0/9
0/9
0/9
✓
✓
✓
✗
✗
✓
fetching the real malicious content. Although the URL was submit-
ted to Phishtank, a community-based URL blacklist based on user
reports, it was not confirmed by any other user and thus, it did not
appear on the official blacklist. Therefore, due to the difficulties re-
lated to detecting such attacks, it is not straightforward to evaluate
their popularity.
Another concern is to develop a suitable solution so that anti-
phishing engines can detect these attacks and blacklist the URLs.
Regarding the alert box technique using server-side detection, the
solution is trivial since most automation frameworks (e.g., Sele-
nium library) can interact with alert boxes and modal windows.
For session-based attacks (server-side detection), one possible so-
lution is to simulate form submissions. As shown in Section 4.1,
NetCraft submitted the HTML form for each report but was only
able to detect 2 malicious URLs. Finally, bypassing CAPTCHA by a
server-side anti-phishing engine is not easy in general since there
is no prior information about the characteristics of the CAPTCHA
challenge used by attackers.
For client-side detection systems (in the form of extensions in-
stalled on the browsers), there is no need to implement any extra
mechanism. If the user solves the challenge and visits a malicious
page, it is also visible to extensions for the detection process.
6 CONCLUSION
In this paper, we evaluate the resilience of anti-phishing detection
systems to evasion techniques based on human verification in a con-
trolled experiment. We registered 105 previously-unseen as well as
reputed drop-catching domains. For each domain, we automatically
generated a full-fledged website and a phishing page protected by