up to date. Consequently, the hypervisor intercepts the write
accesses, and notiﬁes the agent to update its copy. The remain-
ing physical address space is identity mapped. Eventually, the
hypervisor resumes the target’s original execution inside the
idle routine, now running as a virtualized guest. As we chose
a symmetric hypervisor design, the entire injection process
needs to be repeated for each core. From there on, the hy-
pervisor can be used for any speciﬁc task while stealthily
controlling the target system.
3.3 Removal
Similar to the injection, the removal of the hypervisor requires
the agent to copy several removal stages to the target memory.
These stages devirtualize the target, and clean up its memory
170    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Stage R3: Processor Devirtualization Stage R3, now be-
ing executed in VMX root mode, ﬁrst determines if the guest’s
current instruction pointer refers to the code of R2. This veri-
ﬁes that the context switch was indeed caused by R2 indicat-
ing a valid unload process. Although standard kernels would
not map any legitimate code to their nop area, a custom ker-
nel might deviate from this behavior, forcing our hypervisor
to be unloaded. Therefore, the agent is required to approve
the removal process beforehand. Otherwise, the hypervisor
simply ignores the unload request, even if it came from the
correct address range. In case of a legitimate removal, stage
R3 disables interrupt delivery. NMIs that occurred up to this
point would be lost after the hypervisor’s deinstallation, as
these could not be reinjected into the guest anymore. This,
however, would block further NMIs from being delivered, as
the target kernel would not be aware of the pending interrupt.
To avoid this issue, our hypervisor aborts the devirtualization
in case of a pending NMI, reinjects it, and waits for the tar-
get to reenter the idle loop, restarting the removal process.
As this time window is relatively small, NMIs barely came
across during our experiments, however. After disabling in-
terrupts, VMX operation can safely be terminated, effectively
devirtualizing the core. Then, R3 restores the suspended guest
processor state by consulting the respective VMCS. Via the
iretq instruction, the remaining processor state is restored
and control detours to stage R4.
Stage R4: Hypervisor Cleanup Once a processor was de-
virtualized, stage R4 restores the memory layout of the target
and releases the global lock, allowing further cores to pro-
ceed with the removal. Afterwards, interrupts are reenabled,
as these can now be handled by the target. To conceal any
traces, the last core entering R4 signals the agent to restore the
hypervisor area. As the saved copy was constantly updated
during the interception of write accesses, it always contains
the current state. It is then freed by issuing the kernel’s kfree
function. From there on, the target has no chance to detect any
evidence of the previously existing hypervisor. Subsequently,
R4 jumps back into the idle routine, resuming the original
execution of the target. After all cores have been devirtualized,
the agent restores the memory of the injected removal stages,
preventing traces from being left over.
4 Discussion
We tested our implementation on a target host running an Intel
Sandy Bridge i5-2400 processor supporting VT-x, with four
GiB of memory, and a Fujitsu D3062-A1 motherboard. We
installed Debian Stretch with the Linux kernel version 4.9.88-
1.deb9u1 as its operating system. Since HyperLeech is mostly
operating system agnostic, only minor adaptions are required
to support newer target kernels. Inserting the PCIe Screamer
card [3] which is ﬂashed with the PCILeech ﬁrmware version
Figure 3: View of physical memory after the removal stages
R1 to R4 were written to the target memory. The arrows rep-
resent jumps to subsequent stages. The dashed arrow symbol-
izes the abort of the removal process, returning to the kernel’s
idle routine. The dotted arrow depicts a mode switch to the
hypervisor running in VMX root mode.
and processor state. Rebooting the system will also remove
the hypervisor, as it only resides in memory and is currently
not conﬁgured to intercept and emulate system shutdowns.
This is not a conceptual limitation, however. Figure 3 illus-
trates the injected removal stages within the physical memory
of the target host. Once again, the ﬁrst stage R1 appears as a
hook within a permanently invoked kernel function, enabling
HyperLeech to take over control. R2 and R4 are placed within
the nop area, and R3 is part of the hypervisor’s code base. The
remains of this section provide brief information about the
individual removal stages.
Stage R1: Control Flow Hijacking To devirtualize the tar-
get system, a context switch to VMX root mode is mandatory.
As we cannot rely on the guest to trigger a VM exit, we de-
cided to reinstall a hook within the idle routine (intel_idle)
to withdraw control from the target kernel. Like with the in-
jection process, entering the hook transfers each processor to
the subsequent stages. Once again, care had to be taken to
inject the following stages before installing the hook.
Stage R2: Processor Serialization Similarly to the injec-
tion process, the trampoline mechanism prevents certain cores
from entering the subsequent stages (dashed arrow in Fig-
ure 3). This time, however, already devirtualized processors
detour to our trampoline, execute the overwritten instructions,
and return to the idle loop. In addition, a global lock once
again ensures the serialization of the following stages. Failing
to acquire the global lock also results in the immediate return
to the idle routine after detouring to the trampoline. Here
on after, R2 forces another context switch to the hypervisor
which provides the next stage (dotted arrow in Figure 3).
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    171
Kernel	Code	SegmentVM	exitR2R1iretqR3IdleRoutinenop  areaHypervisor	Areakmalloc SpacePhysical	Target	MemoryR41.2.3.4.2.Figure 4: Impact on the target memory during the injection (top left), execution (top right), and removal (bottom right) of the
HyperLeech system. Bottom left depicts the memory state after our system was fully removed. Memory structures represent
kernel structures that reference the hypervisor area due to the allocation via kmalloc. During the execution, these references are
redirected to the hypervisor’s guard page. Saved state depicts processor registers that are temporarily saved on the kernel stacks.
3.5 [17], grants DMA to the target’s memory. On the analysis
machine, we used Windows 10 Enterprise (revision 1709)
which required the installation of the FTDI USB drivers [1]
to communicate with the PCILeech ﬁrmware.
4.1 Target Impact
This section discusses the modiﬁcations of the target’s pro-
cessor and memory state, which arise due to the injection,
execution, and removal of the HyperLeech system. Our main
design consideration was to reduce the impact on the target
while minimizing dependencies on kernel functionality.
Injection During the installation and removal of our sys-
tem, the agent replaces certain parts of the target memory
with the injection stages (Figure 4, top left). To preserve the
original memory, the agent saves the respective areas for later
restoration. Executing injected code within the target kernel
typically leads to further modiﬁcations. Thus, each stage is
designed to best preserve the target’s memory and processor
state. As multiple cores could enter the ﬁrst two injection
stages in parallel, we use the kernel stacks to store processor
state (called saved state in Figure 4) that is about to be mod-
iﬁed. This also simpliﬁes our implementation, as it renders
error-prone synchronization mechanisms obsolete. Pushing
state onto the kernel stacks cannot corrupt data that is still re-
quired by the target, as even red-zones are disabled for proper
interrupt handling. Upon entering stage I3, the remaining in-
jection is serialized by a global lock. Consequently, we store
further data directly within the memory of stage I3 (and thus
in the kernel’s nop page), as it is restored anyway and does not
require any synchronization. With I4 being entered, custom
stacks for each processor are allocated within the hypervisor
area. These are subsequently used to store a processor’s state.
Execution After HyperLeech has been installed (Figure 4,
top right), the agent restores the injection stages with their
original content. This leaves only minor modiﬁcations of the
kernel stacks and a few memory structures which reference
the hypervisor area due to the allocation via kmalloc. As the
conﬁguration of EPTs redirects all read accesses targeting the
hypervisor area to a guard page, following these references
won’t ﬁnd any suspicious traces. The target kernel assumes
this area to be legitimately in use anyway, and usually won’t
ever access it until it is freed again. Write operations that
target the hypervisor area are intercepted, and the originally
stored copy is updated on the analysis machine. We consider
the modiﬁcations of the kernel stacks to be practically unde-
tectable, as these are almost instantly overwritten by regular
kernel usage once the target is resumed.
Removal Removing our hypervisor requires the injection
of the removal stages which are repeatedly used to devirtual-
ize the system (Figure 4, bottom right). As with the injection,
R1 hijacks the target’s control ﬂow, while R2 serializes the
remaining process. Once again, the kernel stacks are used to
preserve target processor state that is about to be modiﬁed in
the meantime. R3 devirtualizes a core and restores the current
guest processor state. The ﬁnal core entering stage R4 frees
the hypervisor memory after its has been restored by the agent.
As the stored copy was constantly updated during the hyper-
visor’s operation, it always contains the current content. Here
on after, the hypervisor area is queued back into the kernel’s
slab allocator, effectively deleting the referencing memory
structures. As a last step, the removal stages are overwritten
by the agent, restoring the original target memory (Figure 4,
bottom left). With the exception of the negligible modiﬁca-
tions of the kernel stacks, no processor or memory state is ever
172    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Kernel	CodeKernel DataKernel	StacksHypervisor	AreaKernel	CodeKernel	DataKernel	StacksHypervisor	AreaKernel	CodeKernel	DataKernel	StacksHypervisor	AreaSavedStateKernel	CodeKernel	DataKernel	StacksHypervisor	AreaInjectionStagesSavedStateVMMMemoryStructuresGuardGuardRemovalStagesSavedStateSavedStateVMMGuardMemoryStructuresMemoryStructuresEPTVMMlost during the injection, execution, or removal of our system.
Moreover, the overwritten parts of the kernel stacks do not
contain any relevant data, and the modiﬁcations are almost
instantly overwritten as soon as the target resumes its execu-
tion. Especially for the purpose of memory forensics, where
evidential integrity plays an important factor, HyperLeech
seems to be a promising step in the right direction.
PCIe Screamer Although the injection, operation, and re-
moval seemingly have no notable target impact, attaching the
PCIe Screamer card introduces detectable modiﬁcations. This
is because the PCIe bus is enumerated whenever the kernel
registers a new device. The enumeration introduces changes
to the ﬁle system and leads to modiﬁcations of the memory
and processor state. For the target, however, PCIe Screamer
only appears as a Xilinx ethernet adapter which could further
be adapted by altering its device id. Thus, the target cannot
refer the device to our system, as it is not distinguishable
from any legitimately added hardware. Section 6 presents
an alternative injection method that avoids the necessity of
attaching hardware altogether, so that even changes caused
by the enumeration could be prevented.
4.2 Performance Impact
This section summarizes the performance impact of the injec-
tion. In this course, we measure the duration of the virtualiza-
tion of each target core (Core X), and compare the cumulative
sum to the time required by the full injection process. For
better results, we repeated the measurements for ﬁve itera-
tions after resetting the target each time. Table 1 summarizes
our results. Comparing the measurements of the individual
processors, the ﬁrst core takes three times longer to ﬁnish the
injection. This is because the ﬁrst core is responsible for exe-
cuting additional tasks, e.g., requesting the target to allocate
the hypervisor area, waiting for the agent to establish a cus-
tom memory layout, and copying the hypervisor to the target
memory. Compared to the cumulative sum of the measured
durations, ﬁnishing the full target virtualization lasted signiﬁ-
cantly longer. This is due to preparation and cleanup steps of
the agent, as well as the virtualization being serialized by a
global lock during the second stage (see Section 3.2). Conse-
quently, only one core at a time is able to progress through
the remaining stages. While the lock is occupied, all other
processors resume the target’s original execution until they
retry the virtualization when entering the idle routine the next
time. Although the serial approach leads the entire injection
to last longer, no processor has to stall its original work.
As we designed our hypervisor to intercept only a minimal
set of events, its performance impact during its execution
appears to be minimal [2]. Depending on the actual use case,
this overhead might change, however.
Table 1: The time each core takes to be virtualized, measured
over ﬁve different runs. While Sum cumulates the durations
of each processor run, Full informs about the total duration
of the entire target virtualization. The bottom row visualizes
the mean values of the individual runs.
Core 0 Core 1 Core 2 Core 3
71 ms
225 ms
71 ms
233 ms
545 ms
65 ms
64 ms
597 ms
64 ms
249 ms
369 ms
67 ms
64 ms
63 ms
76 ms
87 ms
65 ms
71 ms
66 ms
66 ms
76 ms
72 ms
71 ms
70 ms
Sum
426 ms
433 ms
762 ms
821 ms
449 ms
578 ms
Full
6784 ms
1652 ms
1561 ms
4029 ms
1627 ms
3132 ms
4.3 Memory Acquisition
To counter anti-forensics, analysts often acquire a system’s
volatile memory for subsequent analysis [32]. This has the ad-
vantage that malware cannot actively hide once the snapshot
has been acquired. Vömel and Freiling [62] introduced the
three criteria correctness, atomicity, and integrity to assess
the quality of an acquisition method. Correctness determines
the differences between the dump and the actual memory
content acquired at a certain time. Thus, malware that tam-
pers with the acquisition process could impair the correct-
ness of a dump [5, 22, 68]. To consider a memory dump as
atomic, the acquisition process must not be affected by the
target system’s concurrent activity. Since memory is mostly
acquired at a system’s runtime, a correct memory image does
not inevitably imply atomicity. Lastly, the criterion integrity
measures to what extent memory content is altered by the
acquisition method itself. As most acquisition software di-
rectly runs on the target host, certain memory needs to be
overwritten by its own code and data. These criteria can be
mapped to the analysis requirements soundness and target
impact, deﬁned in Section 1. While a sound analysis method