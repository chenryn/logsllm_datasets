### Introduction to Mutation Analysis

Mutation analysis involves the injection of simple syntactic faults into the source code, representing common mistakes made by programmers. It is expected that a test set capable of revealing such simple faults would also be effective in detecting more complex ones. Programs with injected faults are referred to as mutants. These mutants are executed with the test set to determine if their behavior differs from the original program. Mutants that exhibit different behavior are considered "killed" by the test. The result of mutation analysis is a metric called the mutation score, which indicates the percentage of mutants killed by the test set.

Mutants are generated by applying mutation operators that introduce simple changes into the source code. For example, incrementing a constant by one yields one mutant. To evaluate test sets generated according to our test selection strategy, we must select the most applicable mutation operators for our specific case.

### Applicable Mutation Operators

At the time of our experiments, existing mutation operators were applicable to procedural programs at both the unit and integration levels, as well as to specifications in the form of finite state machines, statecharts, Petri Nets, and Estelle. Since we are not testing methods in isolation, unit test operators were not applicable. Similarly, specification operators were not suitable because we did not use any of the mentioned models. Therefore, we decided to use interface mutation operators, which focus on integration faults and are more appropriate for our study, as they consider method interactions.

Interface mutation aims to represent fault models related to the interaction between two routines, \( R_1 \) and \( R_2 \). If \( R_1 \) calls \( R_2 \), the mutation affects points where global variables and formal parameters are used inside \( R_2 \), as well as points where values are returned from \( R_2 \) to \( R_1 \). Local variables and constants in \( R_2 \) that affect the returned values are also considered.

In this study, the interface mutation operators were defined as follows:
- \( G(R_2) \): Set of global variables used in \( R_2 \)
- \( L(R_2) \): Set of local variables defined in \( R_2 \)
- \( E(R_2) \): Set of global variables not used in \( R_2 \)
- \( RC \): Set of required constants, including special values like NULL, MAXINT (greatest positive integer), MININT (least negative integer), etc.

### Tools and Manual Insertion

A tool called Proteum/IM [11] supports mutant generation. However, in our study, faults were manually inserted because the tool was developed for C, while our classes were in C++. One risk of this approach was potential bias, as the same person prepared the self-testable classes and the mutants. This risk was mitigated for the following reasons:
1. Test cases were automatically generated, eliminating the possibility that knowledge of test cases could influence fault insertion.
2. Fault insertion was based on clearly defined rules according to the mutation operators. For example, the IndVarRepGlob operator requires replacing a methodâ€™s local variable with a global variable (or class attribute) not used in that method.

Each mutant was created as a separate class and individually compiled to ensure all faulty classes compiled cleanly. The test sequence generated by Concat was then applied to the mutants. A mutant was considered killed if:
1. The program (driver + mutant class) crashed during test execution.
2. An exception was raised due to assertion violation, which did not occur in the original program.
3. The output of the program differed from the original program's output (validated manually before experiments began).

### Experimental Results

Two experiments were conducted. In the first experiment, faults were inserted into methods of the class `CSortableObList`, and tests were applied to objects of that class to observe their fault-revealing power. Table 2 summarizes the results:

| Operator | #Mutants | #Killed | #Equivalent | Score |
|----------|----------|---------|-------------|-------|
| Op1      | 49       | 42      | 0           | 85.7% |
| Op2      | 164      | 152     | 3           | 94.4% |
| Op3      | 171      | 168     | 0           | 98.2% |
| Op4      | 204      | 197     | 1           | 97.0% |
| Op5      | 112      | 93      | 15          | 95.8% |
| Total    | 700      | 652     | 19          | 95.7% |

The second experiment aimed to observe the effectiveness of the test set generated for the `CSortableObList` class in revealing faults inserted into its base class, `CObList`. Table 3 summarizes these results:

| Operator | #Mutants | #Killed | #Equivalent | Score |
|----------|----------|---------|-------------|-------|
| Op1      | 49       | 42      | 0           | 85.7% |
| Op2      | 164      | 152     | 3           | 94.4% |
| Op3      | 171      | 168     | 0           | 98.2% |
| Op4      | 204      | 197     | 1           | 97.0% |
| Op5      | 112      | 93      | 15          | 95.8% |
| Total    | 700      | 652     | 19          | 95.7% |

The low scores in Table 3 indicate that not retesting a transaction in the context of the subclass, although cost-effective in terms of test productivity, can be dangerous, as many faults may remain undetected. This situation can occur when an application reuses components from a commercial library, and a new release of the library replaces the old one.

### Related Work

Design for testability techniques and the self-testing concept have been used in hardware for a long time but are gaining more attention in the software community. Here are some related works:

- **Voas et al.** present a tool, ASSERT++, to help users place assertions in their programs to improve the testability of object-oriented (OO) software. Their work complements ours, as we are not concerned with the best place to introduce assertions. They focus on improving controllability and observability, and do not consider test case generation or built-in facilities like reporter methods.
  
- **Wang et al.** describe how to construct testable OO software by embedding test mechanisms into the source code. They include test cases aimed at covering implementation-based criteria applied to the methods of a class. These test cases can be inherited and implemented as member functions, allowing tests of a base class to be reused by derived classes. The main differences with our approach are: (i) they use implementation-based criteria; (ii) test cases are integrated into the class rather than its specification; and (iii) they do not mention the use of assertions and other built-in capabilities to enhance controllability and observability.

- **Le Traon et al.** present self-testable OO components that embed documentation, method signatures, and invariant properties, along with test cases. Their approach has been implemented in Eiffel, Java, Perl, and C++. Test cases are generated manually and embedded into the component. Assertions available in Eiffel are used as oracles, and manually generated oracles can complement post-conditions and invariants. A key aspect of their work is that they provide a strategy for using self-testable components for integration and regression testing. Unlike our approach, test cases are embedded into the class rather than the test model, and they need a mutant generator to evaluate test quality.

### Conclusion and Future Work

The aim of this work is to provide an approach for building self-testable components by integrating a test specification and built-in test capabilities. Our experiments show that the test strategy, although not based on the source code, is effective in revealing the types of faults that were inserted. Future work will focus on further refining the mutation operators and exploring the integration of self-testable components in various software development contexts.