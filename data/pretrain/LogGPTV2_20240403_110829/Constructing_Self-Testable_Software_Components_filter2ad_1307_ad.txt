the  injection  of  simple  syntactic  faults  into  the  source  code, 
representing 
the  most  common  mistakes  committed  by 
programmers.  It  is expected that  a  test  set capable  of  revealing 
such  simple  faults  would  also  be  capable  of  revealing  more 
complex ones. Programs  with  injected  faults are called mutants. 
Mutants are executed with the test set to determine whether their 
behavior  is  different  from  the  original  program.  Mutants  that 
behave  differently  are  considered  as  killed  by  the  test.  The 
product  of  the  mutation  analysis  is  a  measure  called  riiutation 
score, which  indicates the  percentage  of mutants killed  by a  test 
set.  Mutants  are  obtained  by  applying  rn~tation operators  that 
introduce the  simple changes into the  source code; for  example, 
increment a constant by one yield one mutant.  So, if we want  to 
evaluate  test  sets  generated  according  to  our  test  selection 
strategy, we have to select the mutation  operators most applicable 
to our case. 
At  the  time we  perform  such experiments, existing mutation 
operators  were applicable to procedural  programs, either at  unit 
or at integration  level  as  well  as to specifications in  the  form of 
finite  state  machines  1141,  statecharts  [16],  Petri  Nets  1151 and 
Estelle  [30]. Since  we  are  not  testing  methods  in  isolation,  the 
unit 
test  operators  were  not  applicable.  The  same  with 
specification  operators,  since  we  are  not  using  none  of  the 
mentioned  models. So we  decided  to use  the  interface mutation 
operators  [IO],  because they  focus  on  integration  faults,  thereby 
being  more  adequate  to  our  study,  since  the  tests  considered 
method’s interactions. lnterface mutation is aimed at representing 
fault models relative  to  the  interaction  between  two routines  R ,  
and  R?.  Therefore,  supposing  that  R I  calls  R2, they  affect  the 
points  where  global  variables  and  formal  parameters  are  used 
inside R?, as well as the points  where values are retumed from R2 
to  R I  (as  in  return  statements  in  C,  for  example),  thus  local 
variables and constants in  R? that  affects  the  returned  values are 
also considered. In this  study,  interface  mutation  operators were 
Where 
G(Rz): set of global variables used in R:; 
L(R2): set of local variables defined in Rz; 
E(R2): set of global variables not used in R:: 
RC: set of required constants, containing some special values such as 
NULL,  MAXINT(grea1est positive integer), MININT(least negative 
integer), and so on; 
There  exists  a  tool,  ProteudIM  [I  I], 
to  support  mutants 
generation. However, in our study faults were manually inserted, 
since the tool  was developed for  the  C language, and  the classes 
we  tested  were  in  C++.  One  risk  of  this  approach  was  the 
possibility  of a potential  bias due to the  fact that the same person 
prepared  the  self-testable  classes  and  the  mutants.  But  actually 
this risk did not exist for the following reasons:  (i) test cases were 
automatically generated, so the  possibility  that knowledge of  test 
cases could  impact  the  insertion  of  faults did  not  exist; and  (ii) 
fault  insertion  was  based  on  a  set  of  clearly  defined  rules, 
according  to  the  definition  of  the  mutation  operators.  For 
example,  the  IndVarRepGlob  operator  requires  the  replacement 
of  a  method’s  local  variable  by  a  global  variable  (or  class 
attribute)  not used in this method. 
Each  mutant  was  created  as  a  separate class,  and  they  were 
individually  compiled, to  assure  that  all  faulty  classes  compiled 
cleanly.  Then the test  sequence generated by Concat was applied 
to  the  mutants.  The  mutant  was  considered  killed  if  one  of  the 
following  situations occurred:  (i) the  program  (driver + mutant 
class)  crashed  while running the test cases; (ii)  an exception  was 
raised due to assertion  violation, during a mutant execution, given 
that  this was  not the case with  the original  program; and (iii)  the 
output of the program  that finished execution was different of the 
output of  the  original  program  (these outputs  were  validated  by 
hand before experiments began). 
Two experiments were performed. In  the first one, faults were 
inserted  into methods  of the class  CSortableObList, and  the  tests 
were  applied  to  objects  of  that  class,  to  observe  their  fault 
revealing  power.  Table  2  presents  a  summary  of  the  results 
obtained  with this first experiment. A total  of 233 test  cases were 
generated  for this class,  for a  test  model  composed  of  16  nodes 
and  43  links.  This  is  the  number  of  new  test  cases; the  class 
reused 329 test cases from its superclass. 
The  first  part  of  the  table  shows  the  number  of  mutants 
generated  for  each  operator, that  are  applied  to each  method  of 
157 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:37 UTC from IEEE Xplore.  Restrictions apply. 
the  target  class  that  were  considered  in  the  experiment.  The 
second part presents,  for each operator: the total  number of 
Table2. Results obtained for the CsortableObList class. 
#mutants 
#killed 
#equivalent 
Score 
49 
42 
0 
85.7% 
164 
152 
3 
94.4% 
171 
168 
0 
98.2% 
204 
197 
1 
97 % 
112 
93 
15 
95.8% 
700 
652 
19 
95.7% 
Table 3. Results obtained for  the CobList class. 
mutants generated, the  number of  mutants killed,  the number  of 
mutants  that  are  equivalent  to  the  original  program,  and  the 
mutation  score,  calculated  as  the  ratio  between  the  number  of 
mutants  killed  and  the  number  of  non-equivalent  mutants.  A 
mutant is considered equivalent to the original program if there is 
no  input  data  on  which  the  mutant  and  the  original  program 
produce  different  output.  The  determination  of  equivalent 
mutants  is  a  non-decidable  problem,  so  they  were  obtained 
manually, by analyzing the mutants  that were alive after the tests. 
The mutation  score is thus a value  in the interval  [O,  11;  the most 
close to I, the better,  meaning that the test set is more effective to 
reveal the injected faults. The results  show that  the  test  strategy, 
although  not  based  on the  source code, is effective to reveal  the 
types of faults that were inserted.  It  is worth  noting that  from  the 
652 mutants killed, 59 were due to assertion  violation. 
Although  no  definite  conclusions  can  be  drawn  based  on 
these  preliminary  results,  they  show  that  assertions,  besides 
improving 
fault-revealing 
effectiveness. The results  also show  that  assertions alone do not 
constitute an effective oracle. 
testability, 
improve 
The  second  experiment  were  aimed 
the 
effectiveness  of  the  test  set  generated  for  the  CSortableObList 
class,  in revealing faults  inserted  into its base class. Thereby, the 
mutation  operators  presented  in  Table  1  were  applied  to  the 
CObList class.  A  summary  of  the results  obtained is shown in 
Table 3. 
to  observe 
The  test  set  for  the  CSortableObList  class  was  generated 
according to the hierarchical  incremental  technique, as explained 
in  section  3.4.2.  According  to 
the 
specification-based  nor  the  implementation-based test  sets  have 
technique  neither 
this 
help 
to 
to be rerun if they only test interactions among methods inherited 
without  modifications.  In  our context, this means that  a test  case 
for  a  transaction  which  is  composed  only  by  inherited  methods 
(constructor and destructor methods excluded) are not included in 
CSortableObList’s test set. 
Only test cases exercising transactions containing modified  or 
new  methods  are  included in  the  test  set,  either  by  reusing  test 
cases from the base class, in case the modification in the subclass 
did  not  change the  specification, or  regenerated,  in  case  of  new 
methods.  Therefore  the  low  scores  in  Table 3  indicate  that  not 
retesting  a  transaction  in  the  context  of  the  subclass,  although 
cost  effective in  terms of  test  productivity, can  be  dangerous, as 
many  faults  may  not  be  revealed.  This situation  can  occur,  for 
reuses  components  from  a 
example,  when  an  application 
commercial  library,  and  a new  release  of  the  library  substitutes 
the old one. 
5.  ReUated works 
Design  for  testability  techniques  and  the  self-testing concept 
have  been  using  in  hardware  for  a  long  time;  however,  only 
recently  is  this  subject  gaining  more  attention  of  the  software 
community,  although  software  testability  concepts  are  not  new. 
Here we present works that are most close to ours. 
Voas et. al  present  a  tool.  ASSERT++ [37], to help  the  user 
place  assertions  into  hisher  programs  to  improve  testability  of 
00 software.  The  tool  comprises  two utility  programs:  one  to 
support  assertion  insertion  and  the  other  that  suggests  points 
where  the  assertions  might  be  inserted,  according  to  testability 
158 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:02:37 UTC from IEEE Xplore.  Restrictions apply. 
scores obtained. Their work is complementary to ours, in that  we 
are  not  concerned  with  the  best  place  to  introduce  assertions. 
Moreover, they are only concerned with improving controllability 
and  observability,  and  so, test  case  generation  and  other  built-in 
facilities,  such  as reporter  methods,  were  not  considered  in  their 
work. 
Wang et al describe  how  to construct  testable  00 software 
by  embedding test mechanisms into the  source code  [38,  391.  In 
fact, they  include  test  cases  aimed  at  covering  implementation- 
based  criteria applied to the  methods of  a class. These  test cases 
can  be  inherited,  as they  are  implemented  as  member  functions 
(or methods),  and  therefore,  allowing  tests  of  a base  class  to be 
reused  by  the  derived  classes.  The  main  differences  with  our 
approach  are: (i) they use  implementation-based  criteria;  (ii) test 
cases are integrated into the class, instead of  its specification;  and 
(iii) the use of assertions and other built-in capabilities to enhance 
controllability and  observability is not  mentioned.  An  advantage 
of  their  approach  is  that  they  don’t  need  a  test  case  generator. 
However,  although implementation-based  criteria are very  useful 
during development and  maintenance, the built-in  test  cases  may 
become  useless 
significant 
modifications to a method’s implementation,  even  if  its external 
behavior remains the same. Moreover, it is not always possible to 
embed test cases into abstract classes. 
introduces 
subclass 
if 
the 
their  specification 
Le  Traon  et  al  also  present  self-testable  00  components 
which  embed 
(documentation,  methods 
signature  and  invariant  properties)  and  test  cases  [34].  Their 
approach  has  been  implemented  in  Eiffel,  Java,  Per1  and  C++ 
languages,  and  in  the aforementioned reference  they detailed  the 
Eiffel  implementation,  for  which  the  design  by  contract  [26] 
support  offered by  that language makes the introduction  of  built- 
in  test  capabilities  straightforward.  Test  cases  are  generated 
manually and embedded into the component. Assertions available 
in  Eiffel  are used  as oracle, but  manually  generated  oracles  can 
also used  in complement,  in  case  post-conditions  and  invariants 
are  not  sufficient  to  express  functional  dependencies  between 
methods.  One  interesting  point  in  their  approach  is  that  a  test 
quality estimate can be associated to each self-test, either to guide 
in the choice of  a component, or to help reaching  a test  adequacy 
criteria  when  generating  test  cases.  Test  quality  estimation  is 
based  on  mutation  analysis:  the  authors  presented  mutation 
operators  applicable  to  different  00  languages.  Test  case 
selection  can  be  driven  either  by  quality  or  by  the  maximum 
number  of  test  cases  desired.  Another  important  aspect  of  their 
work is that they also present a strategy for the use of self-testable 
components  for  integration  and  regression 
the 
previous approach, test cases generated during class development 
are  embedded  into the  class,  and  not  the  test  model,  as in  our 
approach. Since test cases are created manually, they don’t need a 
test  case generator, but  they need  a mutant  generator  to evaluate 
test quality. 
6.  Conclusion and future works 
testing.  As 
The aim of  this work  is  to  provide an  approach  for building 
self-testable  components.  By  integrating  a  test  specification  and 