There have been various improvements suggested in
linguistics to tackle this problem, among which is the
“simple Good-Turing smoothing” [22]. This improve-
ment (denoted by SGT) is famous for its simplicity and
accuracy. SGT takes two steps of smoothing. Firstly,
SGT performs a smoothing operation for Nf :
2N( f )
f + − f − if 1  0. Then, SGT
performs a linear regression for all values SNf and ob-
tains a Zipf distribution: Z( f ) = C · ( f )s, where C and
s are constants resulting from regression. Finally, SGT
conducts a second smoothing by replacing the raw count
Nf from Eq.3 with Z( f ):
⎧⎪⎪⎨
if 0 ≤ f  0,t( f ) > 1.65
(cid:6)
( f + 1)2 Nf +1
N2
f
where t( f ) = |( f + 1)· Nf +1
Nf
min
| and f0 =
(1 + Nf +1
Nf
To the best of our knowledge, we for the ﬁrst time well
explicate how to combine the two smoothing techniques
(i.e., GT and SGT) in Markov-based password cracking.
f ∈ Z
(cid:4)
( f + 1)
S( f ) =
(4)
(5)
Z( f )
(cid:7)
)
.
1554    28th USENIX Security Symposium
USENIX Association