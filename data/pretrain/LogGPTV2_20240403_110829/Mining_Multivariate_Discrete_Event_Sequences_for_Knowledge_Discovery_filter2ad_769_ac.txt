the current system status.
implying that
two values only. Longer words result
We opt for long words (10 characters) because most sensors
output
in a larger
vocabulary size, passing more information to the translation
model. Yet, the larger the vocabulary size, the longer the
training time. A word size of ten characters strikes a good
balance between prediction effectiveness and time efﬁciency.
Figure 3(b) shows the CDF of the vocabulary size when the
word size is 10. About 40% of sensors have vocabulary size
smaller than 13,
the system states recorded
by those sensors are stable for most of the time with only
occasional changes (e.g., sensor #91 in Figure 3 (b)). Less than
20% of sensors have large vocabulary size (i.e., greater than
100). The average vocabulary size is 707 across all sensors.
Generating sentences. Each sentence contains 20 words
that capture the system state changes for a period of 20
consecutive minutes. In contrast to words, there is no overlap
between two consecutive sentences, i.e., the sentence sliding
window is set to 20 words. This places the time granularity
for anomaly detection at 20 minutes. The choice of the
sentence length allows for a sufﬁciently tight time granularity
of prediction without excessive training time.
Considering that every day each sensor records 24 × 60 =
1440 discrete values (or characters), there are 1440÷ 20 = 72
sentences per sensor per day. In total, there are 276K sentences
across all sensors. If more sentences are required (e.g., to
reﬁne the prediction granularity), the sliding window size can
be decreased. For example, if the sentence sliding window
size is set to be 1 word, then there are 1440 sentences per
sensor per day, therefore anomaly detection is performed every
minute. Note that such larger language corpus contains ﬁner
information on system state changes, but also results in longer
model training time.
2) Model Settings: We train the model using events from
normal, non-anomalous days. We use the ﬁrst 10 days as the
training set, the following 3 days as the development set, and
the remaining 17 days for testing4. Note that both training
and development sets consist of normal days only, the two
anomalies are located in the test set.
Recall that we leverage the NMT translation scores to quan-
tify the strength of pairwise relationships between sensors. It
is important to note that we have a strong preference toward
NMT models that are able to distinguish strong and weak
pairwise relationships between sensors, rather than models
that are capable of delivering good translations even for bad
cases (e.g., sentences with “grammar errors” or abnormal event
sequences). We train two NMT models (directional) per sensor
pair, essentially “translating” the language of the source sensor
to that of the target sensor. Recall that, we use these models
not for achieving high translation accuracy, but for quantifying
the strength of the relationship between pair of sensors. The
4For splitting training/development/testing datasets, we tried various parti-
tions with results of similar quality. We therefore opted for a small training
set to allow for more testing cases.
556
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
two derived translation scores for each pair reﬂect the strength
of their relationship. Higher scores imply that the two sensors
are more related.
We apply the same parameter settings to each NMT model
to ensure that all pairwise relationships are quantiﬁed with the
same metric. Here, we use the state-of-the-art NMT model [23]
and set the parameters of each model as follows: # of LSTM
layers=2, # hidden units=64, # embedding size=64, # training
steps=1000, and dropout=0.2. We tried different settings but
the above one is deemed suitable as it delivers good distin-
guishing ability while maintaining acceptable training time.
Figure 4 (a) shows the CDF of model runtime. On average,
each model requires 2.5 minutes for both training and testing.
Therefore, model scalability is not a concern. This can be
further accelerated if this process is done in parallel for
different sensor pairs. Moreover, by comparing the pattern of
sensor discrete event sequences, we notice that many sensors
actually share similar event sequences. If redundant sensors are
further ﬁltered out, then models are trained on representative
sensors only and training time reduces signiﬁcantly. Since our
aim here is to capture the state of the entire system, we run
all pairwise models.
(a) CDF of
model runtime
(b) Histogram of
BLEU scores
Fig. 4: (a) CDF of model runtime including training and testing
time and (b) Histogram of the BLEU scores.
B. Multivariate Relationship Graph
After completing the training process, we use the devel-
opment set (3 normal days) to collect translation (BLEU)
scores for all sensor pairs. These scores serve as measures
that quantify the strength of the relationship between the
discrete event sequences of two sensors during normal operat-
ing conditions. Higher scores imply that the two sequences
are similar; lower scores imply the opposite. Figure 4 (b)
shows the histogram of the BLEU scores. Notice that the
majority (i.e., 89.4%) of scores are greater than 60, implying
that the discrete event sequences of most sensors are related.
Intuitively, one may surmise that stronger relationships are
preferable for knowledge discovery and anomaly detection,
since it is more readily apparent when these relationships
are violated. After trying different score ranges, we ﬁnd that
relationships with scores in the [80, 90) range provide the most
accurate information5. We provide evidence for this in the rest
5This optimal range also applies to other datasets, including the Backblaze
dataset, see Section IV-D.
of this section.
1) Global Subgraphs : If we treat each sensor as a node
and the relationship between each pair of sensors as an edge,
we can obtain a directed graph of all sensors representing the
system (i.e., the multivariate relationship graph G returned
by Algorithm 1). There are two edges between any two
sensors, each edge representing a “translation” from the source
language to the target language. The BLEU score acts as
an edge weight representing the quality of the “translation”.
Note that the same BLEU score of the edges that connect
the same two sensors may be different. This full graph is the
original multivariate relationship graph (short as Ori-MVRG).
If we were to plot all sensors and their relationships in the
Ori-MVRG, the graph would be fully connected, though the
relationships would be of varying strengths. Such a graph
is too noisy to be useful. We therefore partition the Ori-
MVRG into subgraphs, according to edge weights (i.e., BLEU
scores of sensor pairs). We choose a set of score ranges and
produce a subgraph for each range. A given edge is included
in a subgraph if and only if its BLEU score falls into the
corresponding range. If a sensor has no edges in a subgraph,
it is deleted from that subgraph. Table I shows the partition
results according to the selected ranges of BLEU score ranges.
We merge less signiﬁcant subgraphs (i.e., relationships with
scores smaller than 60) into a single subgraph .
TABLE I: Statistics for global subgraphs at different BLEU
score ranges.
BLEU score range
[0, 60)
[60, 70)
[70, 80)
[80, 90)
[90, 100]
% relationships
# sensors
# popular sensors
(in-degree ≥ 100)
# relationships
(w/o popular sensors)
10.6%
54
9
12.8%
32
14
28.8%
56
32
17.8%
73
18
29.9%
82
31
344
162
77
146
151
Figure 5 shows the CDFs of in-degree and out-degree for
the subgraphs deﬁned in Table I. Looking at the in-degree (see
Figure 5 (a)), we notice that a small portion (around 20% to
25%) of sensors are “popular”, i.e., with in-degree ≥ 100,
while others are much less connected to, i.e., with in-degee
≈ 10. Table I lists the number of popular sensors in each
global subgraph in its third row. These sensors are critical
indicators of system health status as any abnormal behaviors
in their event sequences would propagate broadly. Figure 5
(b) shows that the out-degree distribution spreads relatively
evenly, falling between 10 to 35. To visually show a global
subgraph , we plot the one deﬁned by the [80, 90) score range
in Figure 6. The larger nodes in the ﬁgure signify the popular
ones (i.e., with in-degree ≥ 100).
2) Local Subgraphs : The example of the global subgraph
illustrated in Figure 6 is still too densely connected to provide
useful clustering information across sensors. We therefore
remove the popular sensors (i.e., those with in-degree ≥ 100)
from each global subgraph to generate the corresponding local
subgraphs. Figures 7 (a) and 7 (b) show the local subgraphs for
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
557
(a) CDF of in-degree
(b) CDF of out-degree
Fig. 5: CDFs of (a) in-degree and (b) out-degree of sensors in
global subgraphs at different BLEU score ranges.
Fig. 6: Global subgraph with BLEU score in the [80, 90) range.
Each node represents a sensor and each edge represents the
relationship between two sensors. Larger nodes correspond to
popular sensors with in-degree ≥ 100.
[80, 90) and [90, 100] score ranges, respectively. Both ﬁgures
clearly illustrate the presence of several clusters of sensors.
In addition, most clusters are isolated, i.e., not connected to
others, although there is an exception where we see some loose
connectivity, see Figure 7 (a) where two clusters are connected
through a single edge.
(a) Local subgraph
at [80, 90)
(b) Local subgraph
at [90, 100]
Fig. 7: Local subgraphs with BLEU score in the (a) [80, 90)
and (b) [90, 100] range.
In all cases,
the clusters reﬂect
the underlying system
architecture. Sensors in the same cluster could come from
same system components or record similar or related system
states. This assumption is conﬁrmed by domain experts on this
dataset. Furthermore, this clustering information is extremely
useful as some data are often encrypted and provide no
information about the organization of the real system. Yet, the
local subgraphs allow us to surpass this difﬁculty and identify
related sensors.
Knowledge Discovery Takeaways:
• Global subgraphs identify sensors that are critical indicators
of system health status.
• Local subgraphs identify sensors that come from the same
system components or report similar system states.
C. Anomaly Detection
In this section, we discuss how anomalies are detected in
the online testing component. Here, we calculate the anomaly
score (see Section II-C for its deﬁnition) for every sentence in
the testing dataset. Higher anomaly scores indicate that more
pairwise relationships are broken, thus an anomaly is detected
with higher conﬁdence.
Global subgraphs are superior to local ones for anomaly
detection, the reason is that popular sensors contain more
information about sensor interactions during normal times and
are critical indicators of system health status. Therefore, we
focus on global subgraphs in the rest of this section.
Figure 8 shows the anomaly detection results with global
subgraphs at two different BLEU score ranges. The x-axis
is the timeline and the y-axis shows the calculated anomaly
score. Recall that the anomaly score reﬂects how many pair-
wise sensor relationships are broken at a certain timestamp.
There are two anomalies in the testing dataset (marked with
red shade in Figure 8). The global subgraph with BLEU score
in the [80, 90) range delivers the most clear detection (see
Figure 8 (a)), as it correctly detects the anomalies of days
21 and 28 (i.e., the anomaly score is close to 0.8). The other
four spikes are false positives. However, we observe that they
closely precede the two anomalies. As conﬁrmed with the
domain experts, the spikes before the two true anomalies (days
19, 20, and 27) are sings of early detection, which could signal
system administrators to take proactive actions. During normal
times, the anomaly scores are low, mostly below 0.2. Notice
that, on the 30th day, there is also a sign of anomaly (i.e.,
score over 0.8). If the following day were to be conﬁrmed to
be an anomaly (unfortunately we do not have log data beyond
day 30), then it would be a correct sign of early detection.
Otherwise, it is a false positive.
Figure 8 (b) illustrates the anomaly detection results for the
global subgraph with BLEU score in the [90, 100] range, i.e.,
the subgraph with the strongest relationships. In this case, the
anomaly scores are too low to give clear signs of anomalies. To
better understand why the global subgraph with the strongest
relationships fails, we look closely at the translation results of
the target sensors and notice that these sensors have simple
languages. Their system states barely change over the entire
month, resulting in a small vocabulary size. For example,
a signiﬁcant portion of words in the vocabulary of these
target sensors are “aaaaaaaa” (due to unchanged state over
an extended period). Translating to sentences composed only
of that simple word would inevitably result in high translation
scores. In other words, the global subgraph with BLEU score
in [90, 100] range does not necessarily contain sensors that
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
558
(a) Global subgraph with BLEU score in [80, 90) range
(b) Global subgraph with BLEU score in [90, 100] range
Fig. 8: Anomaly detection with global subgraphs with different BLEU score ranges.
are strongly related but instead clusters of easily translatable
sensors.
We experimented with the remaining of global subgraphs,
results are not presented here due to lack of space and
can be summarized as follows: Global subgraphs of weaker
relationships (i.e., with BLEU score lower than 80) generally
do well but can result in many false positives.
Interpretation of anomaly detection results:
For each
detected anomaly, the framework uses the local subgraphs
for fault diagnosis by tracing the broken relationships in
the multivariate relationship graph and identiﬁes problematic
sensors that are responsible for the anomaly. Figure 9 presents
the fault diagnosis results for the anomaly detection illustrated
in Figure 8 (a). Red edges in Figure 9 represent broken