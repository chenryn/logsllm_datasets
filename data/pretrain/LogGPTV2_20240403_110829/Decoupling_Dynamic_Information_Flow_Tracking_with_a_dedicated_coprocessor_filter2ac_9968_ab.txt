The core that runs the regular application and the core that runs
the DIFT analysis synchronize only on system calls. Never-
theless, the cores must be modiﬁed to implement this scheme.
The application core is modiﬁed to create and compress a trace
of the executed instructions. The core must select the events
that trigger tracing, pack the proper information (PC, register
operands, and memory operands), and compress in hardware.
The trace is exchanged using the shared caches (L2 or L3).
The security core must decompress the trace using hardware
and expose it to software.
The most signiﬁcant drawback of the multi-core approach
is that it requires a full general-purpose core for DIFT analysis.
Hence, it halves the number of available cores for other pro-
grams and doubles the energy consumption due to the applica-
tion under analysis. The cost of the modiﬁcations to each core
is also non-trivial, especially for multi-core chips with simple
cores. For instance, the hardware for trace (de)compression
uses a 32-Kbyte table for value prediction. The analysis core
requires an additional 16-Kbyte SRAM for static informa-
tion [3]. These systems also require other modiﬁcations to
the cores, such as additional TLB-like structures to maintain
metadata addresses, for efﬁciency [4]. While the multi-core
DIFT approach can also support memory proﬁling and lock-
set analyses, the hardware DIFT architectures [8, 9, 26] are
capable of performing all the security analyses supported by
ofﬂoading systems, at a lower cost.
The approach we propose is an intermediate between Flex-
iTaint and the multi-core one. Given the simplicity of DIFT
propagation and checks (logical operations on short tags), us-
ing a separate general-purpose core is overkill. Instead, we
propose to use a small attached coprocessor that implements
DIFT functionality for the main processor core and synchro-
nizes with it only on system calls. The coprocessor includes
all the hardware necessary for DIFT state (register tags and tag
caches), propagation, and checks.
Compared to the multi-core DIFT approach, the copro-
cessor eliminates the need for a second core for DIFT and
does not require changes to the processor and cache hierarchy
for trace exchange. Compared to FlexiTaint, the coprocessor
eliminates the need for any changes to the design, pipeline,
or layout of the main core. Hence, there is no impact on de-
sign, veriﬁcation or clock frequency of the main core. Coarse-
grained synchronization enables full decoupling between the
main core and the coprocessor. As we show in the following
sections, the coprocessor approach provides the same security
guarantees and the same performance as FlexiTaint and other
integrated DIFT architectures. Unlike FlexiTaint, the copro-
cessor can also be used with in-order cores, such as Atom and
Larrabee in Intel chips, or Niagara in Sun chips.
2.3 Related Work
The proposed DIFT techniques resemble previous architec-
tural approaches to reliability. The DIVA checker architec-
ture [2] veriﬁes the correctness of the executing instruction
stream. While both the DIFT coprocessor and DIVA perform
a dynamic analysis on the committing instruction stream, they
differ in terms of the granularity of synchronization. DIVA has
to synchronize the checker and processor on every instruction.
The DIFT coprocessor can however, delay synchronization to
system calls. This allows us to decouple the DIFT function-
ality from the main core, giving us the design and veriﬁca-
tion advantages mentioned earlier. The RSE architecture [17]
provides a ﬂexible mechanism to run different reliability and
security checks in hardware. This requires heavy integration
with the main core, similar to the in-core DIFT designs [5, 8].
to watchdog proces-
sors [15] proposed for reliability checks. A watchdog proces-
sor is a simple coprocessor used for concurrent system-level
error detection. It monitors the processor’s input and output
streams, and detects errors pertaining to memory access be-
havior, control ﬂow, control signals or validity of results. Un-
like watchdog processors, the DIFT coprocessor must execute
the instructions committed by the main processor, in order to
ﬁnd security ﬂaws.
The DIFT coprocessor is closest
3 An Off-core Coprocessor for DIFT
The goal of our design is to minimize the cost and complex-
ity of DIFT support by migrating its functionality to a dedi-
cated coprocessor. The main core operates only on data, and
has no idea that tags exist. The main core passes information
about control ﬂow to the coprocessor. The coprocessor in turn,
performs all tag operations and maintains all tag state (con-
ﬁguration registers, register and memory tags). This section
describes the design of the DIFT coprocessor and its interface
with the main core.
3.1 Security Model
The full decoupling of DIFT functionality from the proces-
sor is possible by synchronizing the regular computation and
DIFT operations at the granularity of system calls [13, 16, 21].
Synchronization at the system call granularity operates as fol-
lows. The main core can commit all instructions other than
system calls and traps before it passes them to the coproces-
sor for DIFT propagation and checks through a coprocessor
interface. At a system call or trap, the main core waits for the
coprocessor to complete the DIFT operations for the system
call and all preceding instructions, before the main core can
commit the system call. External interrupts (e.g., time inter-
rupts) are treated similarly by associating them with a pending
instruction which becomes equivalent to a trap. When the co-
processor discovers that a DIFT check has failed, it notiﬁes
the core about the security attack using an asynchronous ex-
ception.
The advantage of this approach is that the main core does
not stall for the DIFT core even if the latter is temporarily
stalled due to accessing tags from main memory.
It essen-
tially eliminates most performance overheads of DIFT pro-
cessing without requiring OOO execution capabilities in the
main core. While there is a small overhead for synchroniza-
tion at system calls, system calls are not frequent and their
overheads are typically in the hundreds or thousands of cy-
cles. Thus, the few tens of cycles needed in the worst case to
synchronize the main core and the DIFT coprocessor are not a
signiﬁcant issue.
Synchronizing at system calls implies that a number of ad-
ditional instructions will be able to commit in the processor
behind an instruction that causes a DIFT check to fail in the
coprocessor. This, however, is acceptable and does not change
the strength of the DIFT security model [13, 16, 21]. While the
additional instructions can further corrupt the address space of
the application, an attacker cannot affect the rest of the system
(other applications, ﬁles, or the OS) without a system call or
trap to invoke the OS. The state of the affected application will
be discarded on a security exception that terminates the appli-
cation prior to taking a system call trap. Other applications
that share read-only data or read-only code are not affected by
the termination of the application under attack. Only applica-
tions (or threads) that share read-write data or code with the
affected application (or thread), and access the corrupted state
need to be terminated, as is the case with integrated DIFT ar-
chitectures. Thus, DIFT systems that synchronize on system
calls provide the same security guarantees as DIFT systems
that synchronize on every instruction [13].
DIFT Coprocessor
Decoupling 
Queue
Instruction 
Tuple
Main 
Core
Queue Stall
Security
Exception
Tag
ALU
Tag
Check
Logic
Writeback
Tag
Reg
File
Security 
Decode
Tag Cache
L2 Cache
DRAM
Tags
Instruction Tuple
Instruction
Memory Address
PC
Valid
Figure 2: The pipeline diagram for the DIFT coprocessor. Structures are not
drawn to scale.
For the program under attack or any other programs that
share read-write data with it, DIFT-based techniques do not
provide recovery guarantees to begin with. DIFT detects an
attack at the time the vulnerability is exploited via an illegal
operation, such as dereferencing a tainted pointer. Even with a
precise security exception at that point, it is difﬁcult to recover
as there is no way to know when the tainted information en-
tered the system, how many pointers, code segments, or data-
structures have been affected, or what code must be executed
to revert the system back to a safe state. Thus, DIFT does
not provide reliable recovery. Consequently, delaying the se-
curity exception by a further number of instructions does not
weaken the robustness of the system.
If DIFT is combined
with a checkpointing scheme that allows the system to roll
back in time for recovery purposes, we can synchronize the
main processor and the DIFT coprocessor every time a check-
point is initiated.
3.2 Coprocessor Microarchitecture
Figure 2 presents the pipeline of the DIFT coprocessor. Its
microarchitecture is quite simple, as it only needs to handle tag
propagation and checks. All other instruction execution capa-
bilities are retained by the main core. Similar to Raksha [8],
our coprocessor supports up to four concurrent security poli-
cies using 4-bit tags per word.
The coprocessor’s state includes three components. First,
there is a set of conﬁguration registers that specify the propa-
gation and check rules for the four security policies. We dis-
cuss these registers further in Section 3.3. Second, there is
a register ﬁle that maintains the tags for the associated archi-
tectural registers in the main processor. Third, the coprocessor
uses a cache to buffer the tags for frequently accessed memory
addresses (data and instructions).
The coprocessor uses a four-stage pipeline. Given an ex-
ecuted instruction by the main core, the ﬁrst stage decodes it
into primitive operations and determines the propagation and
check rules that should be applied based on the active secu-
rity policies. In parallel, the 4-bit tags for input registers are
read from the tag register ﬁle. This stage also accesses the tag
cache to obtain the 4-bit tag for the instruction word. The sec-
ond stage implements tag propagation using a tag ALU. This
4-bit ALU is simple and small in area. It supports logical OR,
AND, and XOR operations to combine source tags. The sec-
ond stage will also access the tag cache to retrieve the tag for
the memory address speciﬁed by load instructions, or to up-
date the tag on store instructions (if the tag of the instruction
is zero). The third stage performs tag checks in accordance
with the conﬁgured security policies. If the check fails (non-
zero tag value), a security exception is raised. The ﬁnal stage
does a write-back of the destination register’s tag to the tag
register ﬁle.
The coprocessor’s pipeline supports forwarding between
dependent instructions to minimize stalls. The main source of
stalls are misses in the tag cache. If frequent, such misses will
eventually stall the main core and lead to performance degra-
dation, as we discuss in Section 3.3. We should point out,
however, that even a small tag cache can provide high cover-
age. Since we maintain a 4-bit tag per 32-bit word, a tag cache
size of T provides the same coverage as an ordinary cache of
size 8 × T .
3.3 DIFT Coprocessor Interface
The interface between the main core and the DIFT copro-
cessor is a critical aspect of the architecture. There are four
issues to consider: coprocessor setup, instruction ﬂow infor-
mation, decoupling, and security exceptions.
DIFT Coprocessor Setup: To allow software to control
the security policies, the coprocessor includes four pairs of
registers that control the propagation and check rules for the
four tag bits. These policy registers specify the propagation
and check modes for each class of primitive operations. Their
operation and encoding are modeled on the corresponding reg-
isters in Raksha [8]. The conﬁguration registers can be manip-
ulated by the main core either as memory-mapped registers or
as registers accessible through coprocessor instructions. In ei-
ther case, the registers should be accessible only from within
a trusted security monitor. Our prototype system uses the co-
processor instructions approach. The coprocessor instructions
are treated as nops in the main processor pipeline. These
instructions are used to manipulate tag values, and read and
write the coprocessor’s tag register ﬁle. This functionality is
necessary for context switches. Note that coprocessor setup
typically happens once per application or context switch.
Instruction Flow Information: The coprocessor needs in-
formation from the main core about the committed instruc-
tions in order to apply the corresponding DIFT propagation
and checks. This information is communicated through a co-
processor interface.
The simplest option is to pass a stream of committed pro-
gram counters (PCs) and load/store memory addresses from
the main core to the coprocessor. The PCs are necessary
to identify instruction ﬂow, while the memory addresses are
needed because the coprocessor only tracks tags and does not
know the data values of the registers in the main core. In this
scenario, the coprocessor must obtain the instruction encod-
ing prior to performing DIFT operations, either by accessing
the main core’s I-cache or by accessing the L2 cache and po-
tentially caching instructions locally as well. Both options
have disadvantages. The former would require the DIFT en-
gine to have a port into the I-cache, creating complexity and
clock frequency challenges. The latter increases the power
and area overhead of the coprocessor and may also constrain
the bandwidth available at the L2 cache. There is also a se-
curity problem with this simple interface. In the presence of
self-modifying or dynamically generated code, the code in the
main core’s I-cache could differ from the code in the DIFT
engine’s I-cache (or the L2 cache) depending on eviction and
coherence policies. This inconsistency can compromise the
security guarantees of DIFT by allowing an attacker to inject
instructions that are not tracked on the DIFT core.
To address these challenges, we propose a coprocessor in-
terface that includes the instruction encoding in addition to
the PC and memory address. As instructions become ready to
commit in the main core, the interface passes a tuple with the
necessary information for DIFT processing (PC, instruction
encoding, and memory address). Instruction tuples are passed
to the coprocessor in program order. Note that the information
in the tuple is available in the re-order buffer of OOO cores or
the last pipeline register of in-order cores to facilitate excep-
tion reporting. The processor modiﬁcations are thus restricted
to the interface required to communicate this information to
the coprocessor. This interface is similar to the lightweight
proﬁling and monitoring extensions recently proposed by pro-
cessor vendors for performance tracking purposes [1]. The in-
struction encoding passed to the coprocessor may be the orig-
inal one used at the ISA level or a predecoded form available
in the main processor. For x86 processors, one can also de-
sign an interface that communicates information between the
processor and the coprocessor at the granularity of micro-ops.
This approach eliminates the need for x86 decoding logic in
the coprocessor.
Decoupling: The physical implementation of the interface
also includes a stall signal that indicates the coprocessor’s in-
ability to accept any further instructions. This is likely to
happen if the coprocessor is experiencing a large number of
misses in the tag cache. Since the locality of tag accesses is
usually greater than the locality of data accesses (see Section
3.4), the main core will likely be experiencing misses in its
data accesses at the same time. Hence, the coprocessor will
rarely be a major performance bottleneck for the main core.
Since the processor and the coprocessor must only synchro-