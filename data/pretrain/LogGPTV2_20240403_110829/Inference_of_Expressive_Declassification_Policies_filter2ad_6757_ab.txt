is contingent on a particular event, such as the execution
of a particular section of code that is trusted to release
information.
Finally, a conjunctive policy p1 and p2 describes that an
observer may learn information according to policy p1 and
information according to policy p2.
B. A simple language
We present a simple imperative language in order to give
a formal semantics for our security policies. The syntax is
given in Fig. 2. We deﬁne a trace-based semantics for this
language. Traces are ﬁnite sequences of events. An event is
either an input event input ν n, indicating that the value
n was input from channel ν, an output event output ν n,
indicating that value n was output on channel ν, or a mark
event mark k indicating that mark k occurred. We write ·
for the empty trace.
Fig. 3 presents the semantics of the language as a large-
step operational semantics. Channel input is modeled as a
nondeterministic choice of an input value. A memory σ is a
ﬁnite map from variables Var to integers. Notation σ(a) = n
indicates that expression a evaluated to n using memory σ
182
Arithmetic expressions
a ::= n | x | a0 + a1 | a0 − a1 | . . .
Boolean expressions
b ::= true | false | a0 = a1 | a0 > a1
| ¬b | b0 ∧ b1 | . . .
Variables
x ∈ Var
Commands
c ::= x := a | c0; c1 | while b do c
| if b then c0 else c1 | skip
| mark k
| input x from ν
| output a to ν
Figure 2. Language syntax
preciseExprs(n) = {n}
preciseExprs(ν[i]) = {ν[i]}
preciseExprs(ν[i+]) = {ν[i], ν[i + 1], ν[i + 2], . . .}
preciseExprs(e1op e2) = {d1op d2 | d1 ∈ preciseExprs(e1)
and d2 ∈ preciseExprs(e2)}
t(n) = n
·(ν[i]) = ⊥
(t, input ν n)(ν[0]) = n
(t, input ν n)(ν[i]) = t(ν[i − 1])
(t, )(ν[i]) = t(ν[i])
(cid:40)
t(d1 op d2) =
n n = t(d1) [[op]] t(d2)
⊥ otherwise
to look up the values of any variable in a. We deﬁne the
complete trace semantics of command c as the set of all
traces that c may generate,
[[c]] (cid:44) {t | ∃σ, σ(cid:48). (c, σ) ⇓ (cid:104)t | σ(cid:48)(cid:105)}.
For example, [[while true do skip]] = ∅ and [[skip]] = {·}
and
[[input x from H; output x to L]] =
{(input H n, output L n) | n ∈ Z}.
C. Semantics of policies
We deﬁne the semantics of a security policy as an
equivalence relation over traces. The equivalence relation
corresponding to a policy prescribes which execution traces
an observer should be unable to distinguish.
In order to deﬁne the semantics of security policies,
we ﬁrst need to deﬁne semantics for input expressions
and track expressions. We regard imprecise expressions as
representing an (inﬁnite) set of precise expressions. Intu-
itively, input expression ν[i+] represents the set of precise
expressions {ν[i], ν[i + 1], ν[i + 2], . . .}. Other imprecise
input expressions are deﬁned homomorphically. We write
preciseExprs(e) for the set of precise expressions repre-
sented by input expression e. The deﬁnition is given in
Fig. 4.
Given trace t, the evaluation of a precise input expression
d, written t(d), is a value in the set {⊥, true, false} ∪ Z.
Intuitively, input expression ν[i] evaluates to the ith most
recent input on channel ν. The evaluation of other precise
input expressions is deﬁned homomorphically. Fig. 4 gives
a formal deﬁnition. Given trace t, the evaluation of a track
expression r, written t(r), is a value in the set {true, false},
indicating whether the marks indicated occurred in the trace.
Fig. 4 gives the semantics.
We can now deﬁne the semantics of policies. Given
security policy p, we write [[p]] for the equivalence relation
where [[op]] is the usual interpretation of arithmetic or boolean operators.
t(k) = true if mark k ∈ t
t(k) = false if mark k /∈ t
t(r1∧∧∧ r2) = t(r1) ∧ t(r2)
t(r1∨∨∨ r2) = t(r1) ∨ t(r2)
Figure 4. Semantics of input expressions and track expressions
over traces that represents p. We deﬁne this equivalence
relation in Fig. 5. Intuitively, a security policy limits the
information that an observer is allowed to learn about a
program’s execution. If two traces t1 and t2 are related
by security policy p (i.e., (t1, t2) ∈ [[p]]), then according to
policy p, an observer should not be allowed to distinguish t1
from t2. If (t1, t2) (cid:54)∈ [[p]], then policy p permits an observer
to distinguish the two traces.
For example, revelation policy Reveal(e1, . . . , en) intu-
itively permits an observer to learn the values of input
expressions e1, . . . , en, and so t1 and t2 are related by the
revelation policy if and only if the two traces agree on the
evaluation of each input expression ei for i ∈ 1..n—that is,
if t1(d) = t2(d) whenever d ∈ preciseExprs(ei).
Traces are related by conditional policy if d then p else q if
they agree on the evaluation of expression d, and are related
by p or q, as appropriate, based on the evaluation of d. Sim-
ilarly, traces are related by track policy if-executed r then p
if they agree on the evaluation of track expression r, and,
when r evaluates to true, they are related by p.
Finally, traces are related by conjunction policy p and q if
they are related by both p and q.
D. Policy ordering and normalization
The semantics of policies induces a partial order on secu-
rity policies. We say policy p reveals no more information
than policy q, written p (cid:118) q, if [[p]] ⊇ [[q]]. Intuitively, if p
183
σ(a) = n
(x := a, σ) ⇓ (cid:104)· | σ[ x (cid:55)→ n ](cid:105)
(skip, σ) ⇓ (cid:104)· | σ(cid:105)
(c1, σ) ⇓ (cid:104)t1 | σ1(cid:105)
(c2, σ1) ⇓ (cid:104)t2 | σ2(cid:105)
(c1; c2, σ) ⇓ (cid:104)t1, t2 | σ2(cid:105)
σ(b) = false
(while b do c, σ) ⇓ (cid:104)· | σ(cid:105)
σ(b) = true (c, σ) ⇓ (cid:104)t1 | σ1(cid:105)
(while b do c, σ1) ⇓ (cid:104)t2 | σ2(cid:105)
(while b do c, σ) ⇓ (cid:104)t1, t2 | σ2(cid:105)
σ(b) = true (c1, σ) ⇓ (cid:104)t1 | σ1(cid:105)
(if b then c1 else c2, σ) ⇓ (cid:104)t1 | σ1(cid:105)
σ(b) = false (c2, σ) ⇓ (cid:104)t2 | σ2(cid:105)
(if b then c1 else c2, σ) ⇓ (cid:104)t2 | σ2(cid:105)
(mark k, σ) ⇓ (cid:104)mark k | σ(cid:105)
(input x from ν, σ) ⇓ (cid:104)input ν n | σ[ x (cid:55)→ n ](cid:105)
σ(a) = n
(output a to ν, σ) ⇓ (cid:104)output ν n | σ(cid:105)
Figure 3. Language semantics
if ∀d ∈(cid:83)
(t1, t2) ∈ [[Reveal(e1, . . . , en)]]
(t1, t2) ∈ [[if d then p else q]]
(t1, t2) ∈ [[if-executed r then p]]
(t1, t2) ∈ [[p and q]]
i=1..n preciseExprs(ei). t1(d) = t2(d)
(t1(d) = t2(d) = false and (t1, t2) ∈ [[q]])
(t1(d) = t2(d) = true and (t1, t2) ∈ [[p]])
or
(t1(r) = t2(r) = true and (t1, t2) ∈ [[p]])
or (t1(r) = t2(r) = false)
(t1, t2) ∈ [[p]] ∩ [[q]]
if
if
if
Figure 5. Semantics of policies
(·,·) ∈ [[M ]]obs
(t1, t2) ∈ [[M ]]obs
(t1, t3) ∈ [[M ]]obs
(t1, (t2, mark k)) ∈ [[M ]]obs
(t1, (t2, input ν n)) ∈ [[M ]]obs
(t1, (t2, output ν n)) ∈ [[M ]]obs
((t1, input ν n), (t2, input ν n)) ∈ [[M ]]obs
((t1, output ν n1), (t2, output ν n2)) ∈ [[M ]]obs
if
if
if
if
if
if
if
(t2, t1) ∈ [[M ]]obs
(t1, t2) ∈ [[M ]]obs and (t2, t3) ∈ [[M ]]obs
(t1, t2) ∈ [[M ]]obs
(t1, t2) ∈ [[M ]]obs and ν /∈ obs
(t1, t2) ∈ [[M ]]obs and ν /∈ obs
(t1, t2) ∈ [[M ]]obs
(t1, t2) ∈ [[M ]]obs and M (ν) = p, and
(t1, t2) ∈ [[p]] implies n1 = n2
Figure 6. Semantics of policy maps
reveals no more information than q, then any information
that policy p permits an observer to learn, is also permitted
by policy q: if p allows an observer to distinguish traces t1
and t2, then (t1, t2) (cid:54)∈ [[p]], and so (t1, t2) (cid:54)∈ [[q]], meaning
that q also allows an observer to distinguish the two traces.
Given this ordering, the least upper bound of p and q is
p and q, and the bottom element of this partial order allows
no information at all to be revealed: Reveal().
Many of the security policies are equivalent under this
ordering. For example, the policies Reveal(), Reveal(42),
and if true then Reveal() else Reveal(1) are all equivalent, as
are the policies Reveal(H[0]) and Reveal(H[0] + 7).
Fig. 7 presents some policy equivalences, expressed as
inference rules. Read from left to right, these equivalences
provide rewrite rules to simplify policies while preserving
semantic meaning. We refer to the process of applying
rewrite rules as normalization: when a policy can no longer
have any rewrite rules applied to it, it is in normal form.
Normalization is critical during inference as it reduces the
number of policies that occur during the analysis, and
ensures termination of the analysis.
E. Security
We assume there is an observer who can see the inputs
and outputs occurring on some set of channels obs ⊆
ChannelName. Given trace t we deﬁne the projection of
t on channels obs, written (cid:98)t(cid:99)obs, to be the subsequence of
t consisting of all and only events on channels in obs.
(cid:98)·(cid:99)obs = ·
if ν ∈ obs
(cid:98)t, input ν n(cid:99)obs = (cid:98)t(cid:99)obs , input ν n
(cid:98)t, input ν n(cid:99)obs = (cid:98)t(cid:99)obs
if ν (cid:54)∈ obs
(cid:98)t, output ν n(cid:99)obs = (cid:98)t(cid:99)obs , output ν n if ν ∈ obs
(cid:98)t, output ν n(cid:99)obs = (cid:98)t(cid:99)obs
if ν (cid:54)∈ obs
(cid:98)t, mark k(cid:99)obs = (cid:98)t(cid:99)obs
We say that traces t1 and t2 are observationally equivalent
to an observer obs if and only if (cid:98)t1(cid:99)obs = (cid:98)t2(cid:99)obs .
184
i ≤ j
Reveal(H[j], H[i+]) = Reveal(H[i+])
if d then p else p = Reveal(d) and p
p and p = p
j = i + 1
p = p(cid:48)
q = q(cid:48)
Reveal(H[j], H[i+]) = Reveal(H[j+])
if d then p else q = if d then p(cid:48) else q(cid:48)
if-executed r then if-executed r(cid:48) then p = if-executed (r∧∧∧ r(cid:48)) then p
(if d then p1 else q1) and (if d then p2 else q2) = if d then (p1 and p2) else (q1 and q2)
Figure 7. Selection of policy equivalences
In order to specify what it means for a program to be
secure with respect to security policies, we need some way
of specifying what information an observer of the system
should be allowed to learn. A policy map M is a map from
channel names to security policies. Intuitively, M (ν) is a
security policy that describes the information that may be
released over channel ν.
A program is secure for observer obs with respect to
policy map M if the observer can learn at most information
according to policies M (ν), for ν ∈ obs. We deﬁne [[M ]]obs
to be the equivalence relation representing the information
an observer obs should be allowed to learn. Intuitively, traces
t1 and t2 are equivalent according to [[M ]]obs when the
following conditions hold.
(i) t1 and t2 agree on the order of input and output events
for all channels in obs.
(ii) t1 and t2 agree on the input values for pairs of
corresponding input events on channels in obs.
(iii) All information output to channel ν is described by
M (ν). That is, for each pair of corresponding ν-output
events in traces t1 and t2, the output values agree
whenever the traces immediately prior to the output
event are related by [[M (ν)]].
The formal deﬁnition of [[M ]]obs is given in Fig. 6.
A program is secure if observational equivalence is no
more precise than the equivalence relation [[M ]]obs.
Deﬁnition 1 (Security). Program c is secure with respect to
policy map M if for all obs ⊆ ChannelName, and t1, t2 ∈
[[c]],
(cid:98)t1(cid:99)obs = (cid:98)t2(cid:99)obs .
(t1, t2) ∈ [[M ]]obs
implies
Intuitively, the deﬁnition requires that for an observer of
channels obs, if the policy map M says that the observer
should not be able to distinguish two executions of the
program ((t1, t2) ∈ [[M ]]obs), then it is indeed the case that
the observer cannot distinguish them ((cid:98)t1(cid:99)obs = (cid:98)t2(cid:99)obs).
This deﬁnition is a standard noninterference-based [10]
deﬁnition, and as such, cannot be expressed as a predicate
over just a single execution trace [11].
III. INFERENCE
Security policies describe what
information may be
learned by an observer of a program execution. However,
speciﬁcation of policies can be onerous (e.g., [8]), since
185
it requires the programmer to explicitly state the security
policies the program is intended to satisfy. Instead, using
techniques inspired by security-type systems [7], we focus
on the inference of security policies from programs with
few annotations. Policy inference frees the programmer
from needing a priori knowledge of the security policies
a program should satisfy.
In this section we describe how our security policies can
be precisely inferred with few security annotations from
the programmer. We present examples using the imperative
language of Section II-B, but describe how these techniques
extend to more general languages. In Section IV we describe
how we apply these techniques to Java programs.
The challenge is to soundly infer precise security policies
with few programmer annotations. The expressiveness of our
security policies presents opportunities for greater precision
in the inferred policies than can be obtained by generalizing
standard information-ﬂow type-systems. We use a dataﬂow
analysis to infer security policies for a program. We ﬁrst
deﬁne the dataﬂow information our analysis propagates,
including how this information is merged, then describe
transfer functions for the commands in the simple imper-
ative language. We also describe how a policy map can
be extracted from the dataﬂow information, thus inferring
security policies for the program, and discuss the soundness
of our analysis.
A. Contexts
The dataﬂow information our analysis associates with
every program point is a context (cid:104)Γ, pc(cid:105), where Γ is a
variable context
that maps every program variable to a
security policy, and pc is a program counter map. A variable
context is a function from program variables to security