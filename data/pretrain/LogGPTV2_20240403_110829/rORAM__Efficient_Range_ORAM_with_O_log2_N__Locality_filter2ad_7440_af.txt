since each query is treated as a query for a random block,
regardless of sequentiality. For the network block device,
the overall query throughput increases but plateaus as larger
ranges throttle the available bandwidth.
File server.
In order to evaluate rORAM for real world
applications, we used the ﬁle server workload of FileBench.
The workload generates accesses similar to a regular ﬁle
system and closely resembles the SPECFsfs benchmark suite
[7]. In particular, read and write requests are generated for ﬁles
of variables sizes, while also updating corresponding metadata.
rORAM handles accesses of variable sizes well and shows a
5x increase in overall throughput compared to Path ORAM for
the local HDD (Figure 7 (b)). Similar trends are observable for
both the SSD and network device scenarios (Figure 7 (b,c)).
Video server. A more appropriate benchmark for range
ORAM applications is a video server that deploys multiple
threads to fetch large sequential chunks of streaming data. In
this case, the large sequential requests can be performed as
range queries. Additionally, the application performs writes of
variable size to metadata and inactive video ﬁles. Note that a
naive solution of storing and accessing ﬁles in large sequential
chunks, to ensure small number of seeks, will waste signiﬁcant
I/O while updating the metadata (often small in size) well.
Since, rORAM allows range queries of arbitrary sizes,
variable-sized sequential accesses are handled well. As as
result, rORAM features a 11x increase in query throughput
3Since reads and writes are equivalent for ORAMs, we expect similar results
for sequential writes as well
)
s
d
n
o
c
e
s
x
i
2
*
y
(
e
m
T
s
s
e
c
c
A
y
r
e
u
Q
 1000
 100
 10
 1
 0.1
rORAM
PathORAM
Batched Eviction
 0  1  2  3  4  5  6  7  8  9  10 11 12 13 14
Range Size (2x Blocks)
)
s
d
n
o
c
e
s
x
2
*
y
(
e
m
T
s
s
e
c
c
A
y
r
e
u
Q
i
 100
 10
 1
 0.1
rORAM
PathORAM
Batched Eviction
)
s
d
n
o
c
e
s
x
i
2
*
y
(
e
m
T
s
s
e
c
c
A
y
r
e
u
Q
 0  1  2  3  4  5  6  7  8  9  10 11 12 13 14
Range Size (2x Blocks)
 1000
 100
 10
 1
 0.1
rORAM
PathORAM
Batched Eviction
 0  1  2  3  4  5  6  7  8  9  10 11 12 13 14
Range Size (2x Blocks)
(a) Query access time on local SSD
(b) Query access time on local HDD
(c) Query access time on Network disk
Fig. 6. Average query access time per block (lower is better). Database size = 222 4kB blocks (16GB). ”Batched Evictions” refers to a Path
ORAM variant equipped with the locality-aware layout for efﬁciently batching evictions. For the local HDD, rORAM is 30-50x faster than
Path ORAM for ranges sizes ≥ 210. rORAM is faster by almost 20-30x for local SSDs and 10x faster for network block devices.
)
c
e
s
/
s
e
i
r
e
u
Q
(
t
u
p
h
g
u
o
r
h
T
 10
 8
 6
 4
 2
 0
Path ORAM
Batched Evictions
rORAM
SeqRead
FileServer VideoServer
Benchmark
)
c
e
s
/
s
e
i
r
e
u
Q
(
t
u
p
h
g
u
o
r
h
T
 22
 20
 18
 16
 14
 12
 10
 8
 6
 4
 2
 0
Path ORAM
Batched Evictions
rORAM
SeqRead
FileServer VideoServer
Benchmark
)
c
e
s
/
s
e
i
r
e
u
Q
(
t
u
p
h
g
u
o
r
h
T
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
Path ORAM
Batched Evictions
rORAM
SeqRead
FileServer VideoServer
Benchmark
(a) Query Throughput (HDD)
(b) Query Throughput (SSD)
(c) Query Throughput (Network)
Fig. 7. Query throughput (higher is better). Database size = 222 4kB blocks (16GB). ”Batched Evictions” refers to a Path ORAM variant
equipped with the locality-aware layout for efﬁciently batching evictions. For purely sequential workloads, rORAM is 10-15x faster than
Path ORAM for the local HDD and SSD. rORAM is almost 6x faster for network block devices. rORAM is up to 5x faster running a ﬁle
server and up to 11x faster running a video server compared to Path ORAM.
over Path ORAM for the local HDD (Figure 7 (a)), an 8x
increase in throughput for the SSD (Figure 7(b)), and 4x
increase in throughput for the network device (Figure 7(c)).
Client-side storage. For the storage conﬁgurations used in
our experiments, rORAM will require (in the worst-case)
approximately 8GB of client-side stash when L = 214 blocks
(combined stash size for all 15 sub-ORAMs) and 128MB
when L = 28 blocks (Section VI). Note that the stash size
only depends on L and is independent of the total outsourced
storage size. Empirical observations show that in the average-
case, a smaller stash size may sufﬁce in practice. E.g., in
our experiments, the maximum observed stash occupancy was
around 214 blocks (64MB) and 210 blocks (4MB) for L = 214
and L = 28 blocks respectively.
VIII. SYSTEM TWEAKS AND OPTIMIZATIONS
The described Range ORAM construction is designed with
the main goal of minimizing the number of disk seeks per
operation in the general setting of client/server ORAMs with
limited client storage. In practice, there are a number of other
parameters or settings which the client may alter to allow
further improvements. In this section, we brieﬂy outline a few
of these alternations and tweaks.
A. Parallel Seeks with Multiple Heads or Disks
Modern storage systems may have multiple read/write heads
(a high-capacity HDD disk has up to 8) or use arrays of high
capacitive disks that may be striped (e.g., using a RAID). Such
conﬁgurations, where seeks can occur in parallel, can lead to
signiﬁcant performance gains, and rORAM can leverage these
situations with limited modiﬁcation.
Assume that if the server’s storage is partitioned into k
equal-sized parts (disk platters or cluster nodes), and that each
part can be read or written separately in parallel, it can be
shown that the number of parallel seeks per access is
k (cid:17)(cid:17) . That is, perfect parallel speedup
O(cid:16)log N · (cid:16)1 + log N
in the number of seeks is possible for k ≤ log N . This