Figure 5 shows the architecture of our framework. It takes as input
a configuration file (provided by operators), the incident descrip-
tion, and pulls the relevant monitoring data the team (and its local
dependencies) collects and produces a Scout (e.g., for PhyNet). We
next describe each component.
5.1 Monitoring Specifications
Scouts rely on monitoring data to decide where to route incidents:
they must (1) decide which monitoring data is relevant to the in-
cident (lest we contribute to the curse of dimensionality) and (2)
pre-process that data before it is used. Both steps are difficult to
handle automatically. First, the framework starts with the incident
description and all of the operator’s monitoring data (from all DCs)
as input and has no other information with which to narrow its
search. Second, the framework must be able to process arbitrary
new datasets with minimal context. Teams can annotate both the
incident text and the monitoring data to help:
Extracting components from the incident itself. Scouts can-
not investigate all components (DC sub-systems such as VMs,
switches, and servers): (1) it would result in a high-dimensional
feature-set (2) it can lead to too many false positives — due to con-
current and unrelated incidents (see §3). To solve this problem,
ModelSelectorSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gao et al.
falls back to an unsupervised model for new and rare incidents. We
use a separate ML model to learn which category an incident falls
into (the model selector). The underlying components are:
5.2.1 Random forests (RFs). We use Random forests (RFs) [57]
as our supervised learning model. RFs can learn the complex re-
lationships between incidents, the monitoring data the teams col-
lect, and whether the team is responsible. RFs are a natural first
choice [15, 18, 24, 68]: they are resilient to over-fitting and offer
explain-ability.
Explain-ability is often a crucial feature for the successful de-
ployment of a system such as ours (see §7). We use [57] to provide
explanations to the team when incidents are routed to them.
Our RF takes as input a set of aggregated statistics for each
type of component. For instance, in the component specification
presented in §5.1, the five component types would result in five
distinct sets of features.
Per-component features. We next construct features for each
type of relevant component — up to five types in our example.
Per-component features incorporate EVENT and TIME_SERIES data
related to the components during the interval [𝑡 − 𝑇 , 𝑡], where 𝑡 is
the timestamp of the incident and 𝑇 is a fixed look-back time. Each
data set is pre-processed as follows:
Events/alerts: We count the events per type of alert and per compo-
nent, e.g., the number of Syslogs (per type of Syslog).
Time-series: We normalize them and calculate the: mean, standard
deviation, min, max, and 1𝑠𝑡, 10𝑡ℎ, 25𝑡ℎ, 50𝑡ℎ, 75𝑡ℎ, 90𝑡ℎ, and 99𝑡ℎ
percentiles during [𝑡 − 𝑇 , 𝑡] to capture any changes that indicate a
failure.
Merging features from multiple components. Many compo-
nents contain a variable amount of related monitoring data that
need to be combined to ensure a fixed-length feature-set. This
is because: (1) differences in hardware/instrumentation (e.g. two
servers with different generations of CPU, one with 16 cores and
one with 32, where data is collected for each core), or (2) the inclu-
sion of sub-components, e.g., many switches in a single cluster. In
the first case, user ‘class’ tags specify the data to combine (which
we normalize first). In the second, the component tags provide
that information: e.g., all data with the same ‘resource_locator’ and
‘cluster’ tag is combined. We ensure a consistent feature set size by
computing statistics over all the data as a whole. Our intuition is
these features capture the overall distribution and hence, the impact
of the failure. For example, if a switch in a cluster is unhealthy, the
data from the switch would move the upper (or lower) percentiles.
We compute statistics for all applicable component types: for
cluster c10.dc3 in our example, we would compute a set of cluster
and DC features. If we do not find a component in any of the team’s
monitoring data, we remove its features. For example, PhyNet is
not responsible for monitoring the health of VMs (other teams are)
and so the PhyNet Scout does not have VM features.
In our example, the features include a set of server and switch
features — corresponding to the statistics computed over data sets
that relate to servers and switches — set to 0; statistics computed
over each data set related to the two clusters: c10.dc3 and c4.dc1;
and similarly, dc features over data from dc3 and dc1.
We add a feature for the number of components of each type.
This, for example, can help the model identify whether a change in
RF
97.2%
97.6%
0.97
CPD+ NLP
96.5%
93.1%
91.3%
94.0%
0.94
0.94
Precision
Recall
F1-score
Table 1: F1-Score, precision, recall of each model as well as
the existing NLP solution §7.
the 99𝑡ℎ percentile of a switch-related time series is significant (it
may be noise if all the data is from one switch but significant if the
data is aggregated across 100 switches).
5.2.2 Modified Change Point Detection (CPD+). To choose an
unsupervised model we use the following insight: when a team’s
components are responsible for an incident there is often an accom-
panying shift in the data from those components, moving from one
stationary distribution to another.
CPD+ is an extension of change point detection (CPD) [51], an
algorithm that detects when a time series goes from one stationary
distribution to another. CPD is not, by itself, sufficient: (a) CPD only
applies to time-series data and cannot operate over events; (b) CPD
tends to have high false-positives—changes in distribution due to
non-failure events are common. The problem in (b) is exacerbated
when the incident implicates an entire cluster and not a small set
of devices: the algorithm can make a mistake on each device.
We use simple heuristics to solve these problems3. Our intuition
is while we do not have enough data to learn whether the team
is responsible, we do have enough to learn what combination of
change-points point to failures: when we have to investigate the
full cluster, we “learn” (using a new RF) whether change-points
(and events) are due to failures. The input is the average number
of change-points (or events) per component type and monitoring
data in the cluster.
When the incident implicates a handful of devices, we take a
conservative approach and report the incident as the team’s re-
sponsibility if any error or change-point is detected—these are
themselves explanations of why the incident was routed to the
team.
5.3 The Model Selector
Given an incident, the model selector maintains high accuracy by
carefully deciding between the RF and CPD+ algorithms. The model
selector has to:
Decide if the incident is “in-scope”. Operators know of incidents
(and components) that can be explicitly excluded from their team’s
responsibilities. Hence, they can specify incidents, components, and
keywords that are ‘out-of-scope’. Although optional, this can reduce
false positives. One example is an application that does not run on
a particular class of servers; any incident involving those servers is
unrelated. If PhyNet has passed the responsibility of a soon-to-be
decommissioned switch over to another team, that switch is also
out-of-scope. Example EXCLUDE commands are:
EXCLUDE switch = ; or
EXCLUDE TITLE = ; or
EXCLUDE BODY = ;
3Anomaly detection algorithms (as opposed to CPD) e.g., OneClassSVM [66] had lower
accuracy (Table 1): 86% precision and 98% recall.
After applying exclusion rules, the model selector extracts com-
ponents from the incident description. This step is critical to avoid
using the team’s entire suite of monitoring data as input (see §5.1).
If the model selector cannot detect such a component, the incident
is marked as too broad in scope for either the RF or CPD+: it is likely
to be mis-classified—we revert to the provider’s existing incident
routing process.
Decide between RF, CPD+. We prefer to use the RF as our main
classifier because it is the most accurate (Table 1) and the most ex-
plainable — the CPD+ algorithm is only triggered on rare incidents
where the RF is expected to make mistakes.
We use meta-learning [65] to find “new” or rare incidents: we use
another ML model (an RF which is trained over “meta-features”).
Our meta-features are based on the method proposed in [58]: we
identify important words in the incident and their frequency. This
model is continuously re-trained so the model selector can adapt its
decisions to keep up with any changes to the team or its incidents.
Important note: The RF and the CPD+ algorithms used in our
framework can be replaced by other supervised and unsupervised
models respectively. Similarly, the RF model used in the model
selector can be replaced by other models (see §7). We chose these
models for our production system due to their explain-ability (the
RF), low overhead (CPD+), and high accuracy (both the RFs §7).
Operators can choose to replace any of these models depending on
their needs. We show an evaluation of other models in §7.
Thus, the end-to-end pipeline operates as follows: when a new
incident is created, the PhyNet Scout first extracts the relevant
components based on the configuration file. If it cannot identify
any specific components, incident routing falls back to the legacy
system. Otherwise, it constructs the model selector’s feature vector
from the incident text, and the model selector decides whether to
use the RF or the CPD+ algorithm. Finally, the Scout will construct
the feature vector for the chosen model, run the algorithm, and
report the classification results to the user.
6 IMPLEMENTATION
We have deployed a Scout for the physical network (PhyNet) team
of Microsoft Azure. Azure’s production ML system, Resource Cen-
tral [23], manages the lifecycle of our models (the RF, CPD+, and the
Model selector) and serves predictions from them. Resource Central
consists of an offline (training) and an online (serving) component.
The offline component trains the models using Spark [72]. It is also
responsible for model re-training. The trained models are then put
in a highly available storage system and served to the online com-
ponent. This component provides a REST interface and is activated
once an incident is created in the provider’s incident management
system: the incident manager makes calls to the online component,
which runs the desired models and returns a prediction. If any of
the features are unavailable — e.g., if one of the monitoring sys-
tems we rely on also failed when the incident occurred — Resource
Central uses the mean of that feature in the training set for online
predictions. We will evaluate such failure scenarios in §7.
We have implemented a prototype of the Scout framework in
Python. The configuration file of PhyNet’s Scout describes three
types of components: server, switch, and cluster and twelve types
of monitoring data (listed in Table 2).
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Each call to the Scout (consisting of pulling the relevant moni-
toring data, constructing features, and running the inference) takes
1.79 ± 0.85 minutes — negligible compared to the time operators
spend investigating incidents (those not resolved automatically).
Overall, the online pipeline and offline pipeline consist of 4124
and 5000 lines of code respectively. To facilitate what-if analysis,
we do not take action based on the output of the Scout but rather
observe what would have happened if it was used for routing deci-
sions.
7 EVALUATION
Data: We use 9 months of incidents from Microsoft Azure. Each
data point describes an incident as, (𝑥𝑖, 𝑦𝑖), where 𝑥𝑖 is a feature
vector and 𝑦𝑖 is a label: 0 if PhyNet resolved the incident and 1
otherwise. We use a look-back time (T) of two hours (unless noted
otherwise) to construct 𝑥𝑖. We also have a log of how each incident
was handled by operators in the absence of our system (see §3). We
remove all incidents that were automatically resolved and further
focus on incidents where we can extract at least one component.
As mentioned in §5.3, both of these types of incidents use the
legacy incident routing infrastructure. Note that excluding incidents
without a component means that the distribution of incidents used
in our evaluations is slightly different from that of §3.
Training and test sets: We randomly split the data into a training
and a test set. To avoid class imbalance [40] (most-incidents are
not PhyNet’s responsibility), we only use 35% of the non-PhyNet
incidents in the training set (the rest are in the test set). We split
and use half the PhyNet incidents for training. We also show results
for time-based splits in §7.3.
Accuracy Metrics: We use several such metrics:
Precision: The trustworthiness of the Scout’s output. A precision
of 90% implies the Scout is correct 90% of the time when it says
PhyNet is responsible.
Recall: The portion of PhyNet incidents the Scout finds. A recall of
90% implies the Scout can identify 90% of the incidents for which
PhyNet was responsible.
F1-score [32]: The harmonic mean of the algorithm’s precision and
recall — for measuring overall accuracy.
Metrics comparing Scouts to the baseline: We also define met-
rics that show the benefits of the Scout over the existing baseline.
We first describe this baseline in more detail and then define these
metrics:
Baseline: We use the operator’s existing incident routing process —
incident routing without Scout’s involvement — as our baseline. Our
discussion in section §3 describe the incident routing problem with
these mechanisms in place: operators use run-books, past-experience,
Figure 6: Distribution of overhead-in to PhyNet based on our
legacy incident routing solution.
CDF         0.2   0.4   0.6    0.8 1.00.20.40.60.81.0Fraction of total investigation time0.00.0SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gao et al.
Data set