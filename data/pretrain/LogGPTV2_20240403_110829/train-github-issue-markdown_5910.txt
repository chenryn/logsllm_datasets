The function divides by the entire number of elements and not just batch
sample size.  
For example, using
    loss_f1 = nn.MSELoss()
    loss_f2 = nn.MSELoss(size_average=False)
    x = autograd.Variable(torch.FloatTensor([    [1,2,3], [1,2,3]   ]))
    y = autograd.Variable(torch.FloatTensor([    [2,4,6], [3,4,7]   ]))
    # sample size of current batch = 2
    # dimension of features = 3
shows the problem.
    # loss no averaging/no division...just plain summing up
    loss_2 = loss_f2(x, y) # = 38
    # averaged loss over batch sample size
    loss_1 = loss_f1(x, y) # = 6.3333
    # Apparently division by 6 was done in the second averaging case.
The documentation describes the implemented MSE loss as
> loss(x,y)=1/n (sum(||x-y||**2)
where n is the sample size of the current batch.  
However, apparently the expression does not get divided by n (which would be 2
in our example), but by 2*3=6, i.e. the entire number of elements per training
batch.
Is this intended? From a theoretical standpoint it should be 2, shouldn't it?