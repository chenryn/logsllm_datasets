imposed by the DOM snapshot logging.
The total webshot time is computed as (max{tsn, td} − t0),
according to the discussion provided in Section IV. This time
Fig. 10. Time needed to take webshots and comparison with mouse-down/up
and key-down/up time deltas (the number of events on which the CDFs are
computed are in parenthesis).
could be further reduced to (max{tsn, (td − δf )} − t0) by
ofﬂoading the DOM ﬁle saving process to a separate Renderer
process thread (we leave this implementation task to future
releases of ChromePic).
Figure 10 shows the distribution of the total time needed to
log the webshots on different devices, while Table II reports a
breakdown of the webshot times into their components (50th- and
98th-percentiles). On both Linux devices (laptop and desktop)
98% of all webshots are logged in less than 120ms. This is a
very good result, because any latency below 150ms is practically
unnoticeable to users [47]. On Android, 98% of webshots are
logged in less than 264ms, with a median time of around 78ms.
While the 98th-percentile time is higher than our 150ms target,
it is still a low overhead that is on the very low end of the
“noticeable” latency classiﬁcation provided in [47]. Also, our
results indicate that 82.02% of all webshots on Android can
be taken in less than 150ms. Furthermore, notice that the total
overhead is driven by the DOM snapshot time, including saving
the DOM to ﬁle, rather than the screenshot notiﬁcation time.
From Table II, we can see that the 98th-percentile of the total
webshot logging time for Android could be reduced to roughly
203ms if ﬁle saving was delegated to a separate thread in the
Renderer process. In addition, in this setting 89.33% of the
webshots on Android would take less than 150 ms.
To further put our results into perspective, we also compared
the time needed to take webshots to the time in between mouse-
down/up and key-down/up events. In other words, we measure
the time that it takes for a user to lift her ﬁnger from the mouse
button or from a key. The mouse-down/up and key-down/up time
deltas are measured on the Linux desktop, with webshots turned
off. As we can see from Figure 10, the distribution (CDF) of
webshot overhead times on the Linux laptop and desktop are
always to the left of the mouse-down/up and key-down/up time
deltas curves. Because we start the webshot log at the down
event, this means that in the vast majority of cases when a mouse
click or a key press occurs, the webshot will be fully logged
11
050100150200250300Event Duration (ms) - mouse, key press deltas and webshot overheads0.00.20.40.60.81.0Mouse click deltas (1278)Key press deltas (1089)Laptop webshots (2117)Tablet webshots (2428)Desktop webshots (1361)TABLE II.
User Study 1 - PERFORMANCE OVERHEAD (50TH- AND 98TH-PERCENTILE)
Platform
Android
Linux laptop
Linux desktop
Total
Total
with ﬁle write (ms)
w/o ﬁle write (ms)
notiﬁcation time (ms)
78.05, 263.06
39.16, 118.43
22.36, 93.19
59.53, 203.01
33.32, 109.55
19.02, 76.11
Screenshot
13.02, 25.88
5.38, 27.68
2.74, 23.80
Screenshot
callback time (ms)
65.67, 109.97
36.17, 71.05
38.95, 118.04
DOM snapshot time
with ﬁle write (ms)
77.55, 261.81
38.95, 118.26
22.11, 85.86
DOM snapshot time
w/o ﬁle write (ms)
58.86, 202.38
33.12, 109.38
18.74, 70.53
measurements, we found that the slight increase in overhead,
compared to User Study 1, was due to DOM snapshots on Gmail,
due to how the page is structured (e.g., Gmail pages embedded
a larger number of iframe’s and had a larger DOM size).
Speciﬁcally, the 98th-percentile for the total webshot time on
Gmail was around 245ms. The times on all other popular sites
(Facebook, Twitter, Google Docs, etc.) were in line or even lower
than those obtained in User Study 1. For instance, on Facebook
the 98th-percentile was less than 120ms. Overall, if we exclude
Gmail from this experiment, 98% of the webshots can be taken
in 108ms.
User Study 3: For comparison purposes, we also measured
the performance of taking screenshots using the extension-
based approach discussed in Section V. These have been done
on the desktop machine, and the results should therefore be
compared to the third row of Table II. Also, notice that in
this experiment we are only considering the screenshot time
(as explained in Section V, it is not easy to take synchronous
“deep” DOM snapshots via the extension API). We found that
50% of screenshots require at least 140ms and 98% of them
require 243ms. This is in contrast with the 2.74ms and 23.80ms,
respectively, that are required by the browser-based version of
ChromePic. Furthermore, the extension times are much larger
than the total time needed to take a full webshot (including the
DOM) on the desktop machine using the instrumented browser
solution (see Table II). This reinforces our conclusion that an
extension-based solution is not only cumbersome, as discussed
in Section V, but also much less efﬁcient.
D. Storage Requirements
Table III shows the storage requirements for archiving the
webshots produced during User Study 1. After a straightforward
compression process (converting screenshots to JPG and using
lossless compression for DOM snapshots), the webshots take
a maximum of 1.03MB per minute of browsing on the Linux
laptop. Android logs required only 0.85MB/minute of storage, and
0.92MB/minute on the desktop machine. This space requirements
could be further reduced by using lossy compression on the
DOM-embedded images, for instance by converting them to a
low- or medium-quality JPG.
Let’s now consider a scenario in which ChromePic is deployed
in a corporate network setting. Assume that in average users
spend half of their working time (4 hours/day) browsing, while
the other half is spent on other tasks (meetings, development,
design, data analysis, etc.). If we assume 22 business days per
month, and 1.03MB of storage needed per minute of browsing
(i.e., the maximum amount we observed), a single user would
produce less than 6GB of webshot logs per month. In a corporate
network with 1,000 users, this would result in less than 6TB of
storage for an entire month of browsing logs for the network, or
72TB for an entire year of logs. Considering that a multi-TB
hard drive currently costs only a few hundreds US dollars, an
entire year of webshot logs could be archived for only a few
Fig. 11. Comparison of “natural” input event processing time with and without
webshots enabled (the number of events on which the CDFs are computed are
in parenthesis).
by the time the user raises her ﬁnger. Also, the Android tablet
curve is almost entirely to the left of the mouse-down/up and
key-down/up curves, showing that even on Android the webshots
can be taken efﬁciently.
Another result worth noting is that our screenshot code
optimizations, described in Section IV-D, yield a very signiﬁcant
overhead improvement, as can be seen by comparing the
notiﬁcation time and callback time in Table II.
To verify that our webshots do not negatively impact the
subsequent “natural” input processing times, in Figure 11 we
also compare the amount of time taken by the browser to process
a user input in two different cases: when webshots are disabled
(dashed line), and when the input is processed right after a
webshot has been logged (solid line). Speciﬁcally, let t0 be the
time when the Browser sends a user input u to the Renderer, and
ti be the time when the Renderer conﬁrms to the Browser that the
input has been processed (we use Chromium’s LatencyInfo
objects to measure this). Also, let t(cid:48)
i be the “input processed”
conﬁrmation time related to events that triggered a webshot, and
δw be the time delta needed to log a webshot. The ﬁrst (dashed)
curve measures (ti − t0), which represents the “natural” input
processing time. Similarly, the second (solid) curve measures
i − δw − t0), which represents the time needed by the browser
(t(cid:48)
to process the input after a synchronous webshot has been taken
(see Figure 2). As can be seen, the two curves are very similar,
indicating no unexpected delay to the natural input processing
time due to webshot events. In other words, the webshots do
not cause any other delays, besides the actual time to take the
webshots, δw.
User Study 2: As discussed in Section VII-B, we separately
measured the overhead for user activities on popular web sites,
such as Facebook, Twitter, Gmail, Google Drive, etc. We recorded
thousands of user input events, 1,910 of which triggered a
webshot. Of these webshots, 50% were processed in less than
66ms, and 98% took less than 240ms. Furthermore, 80% of all
the webshot took less than 150ms. After closely analyzing the
12
050100150200250300350400Event Duration (ms) - mouse and key events0.00.20.40.60.81.0Webshots enabled (1369)Webshots disabled (1412)TABLE III.
AVERAGE STORAGE REQUIREMENTS (MBS/MINUTE)
Platform
Android
Linux laptop
Linux desktop
Uncompressed
Screenshots
Compressed
6.80
4.66
2.31
DOM Screenshots
11.62
11.33
8.07
0.31
0.15
0.09
DOM
0.54
0.88
0.83
thousand US dollars. In alternative, considering that business-
grade cloud-based storage services are currently priced at less
than $0.03/GB per month, archiving one entire year worth of
webshots for the entire corporate network in the cloud would
cost less than $2,200 per month16.
VIII. DISCUSSION
There exist some corner cases in which it is not possible
to “freeze” the state of the DOM/rendering immediately after
a user input arrives. For instance, if a user input arrives while
the Render Thread is already executing another task, such as
a long-running javascript program that affects the DOM, the
processing of the user input will have to wait until javascript
terminates, and until its own Task is scheduled for execution
(see Section IV-E). The net effect is that the DOM snapshot
will reﬂect the state of the DOM after the already running
javascript code terminates. Notice, however, that this is also true
for “natural” input processing. Namely, the input will apply to
the modiﬁed DOM, regardless of whether a webshot is taken or
not. Therefore, our snapshots correctly reﬂect the state of the
DOM at the time when the input becomes effective. Similarly,
because screenshots need to wait for the compositor to redraw,
the exact instant in time in which the screenshot is taken is
determined by the cc::scheduler (see Section IV-D). If an
animation is in progress on the page, it may be possible for the
screenshot to be one (or a very small number of) frame(s) “off”
w.r.t. the user input. Again, this also holds for “natural” input
processing (i.e., even if webshots were disabled), because the
input may become effective after a redraw.
In Section IV-E, we mentioned that once the out-of-process-
iframes (OOPIFs) [41] project is completed and becomes active
by default, we will need to slightly adapt our code for taking
DOM snapshots. In fact, we believe that OOPIFs would allow
us to further decrease the time needed to take a snapshot. The
reason is as follows. Assume the user interacts (e.g., clicks
on a link) with a page that embeds several iframes (e.g., to
display different ads). In the current implementation, both the
main page and iframes are processed in the same Renderer
process. Therefore, the DOM of the main page and all iframes
has to be produced at once, synchronously with the input (see
Section IV-E). But with OOPIFs we could produce all these
partial DOM snapshots in parallel by simply sending a “take
DOM snapshot” IPC message to the main page and all iframes
at the same time.
Because ChromePic continuously logs user-browser inter-
actions and takes screenshots, the recorded logs may contain
sensitive user information. To mitigate privacy concerns, the logs
could be securely stored using methods similar to previously
proposed approaches [12], [34]. Furthermore, the solutions
proposed in [28] could also be readily applied to ChromePic’s
output. For instance, ChromePic could employ a customizable
16Estimated using http://calculator.s3.amazonaws.com/index.html (with Cold
HDD)
13
whitelist of sites on which webshots should be turned off. To
be more strict, ChromePic could be prevented from logging any
events on pages loaded via HTTPS that have a valid (not self-
signed) TLS certiﬁcate. Furthermore, a “helper” application (or
a separate browser thread) could be responsible for continuously
gathering and encrypting the browser logs. This application
would encrypt logs related to different tabs separately, using
unique keys that could be stored in a key escrow [8]. The key
escrow could be owned by the user or, in enterprise environments,
by the machine’s administrator, and the keys released only when
a security investigation is called for. In addition, because each
tab can be stored separately and encrypted with a different key,
investigators could be selectively given access only to some tabs
rather than the entire browsing history. The decision on whether
to authorize the decryption of a tab would depend on the speciﬁc
investigation, but could for instance be based on the time frame
in which the attack is suspected to have happened, and on the
list of domains that have been visited within the tab, which can
be recorded as meta-data and encrypted with a “global” key.
In addition, audit logs could be protected from tampering by
using existing ﬁle system features, such as append-only ﬁles
and immutability [24]. We leave the engineering of this key
escrow-based system to future work.
IX. RELATED WORK
The analysis of security incidents is often hindered by the
lack of necessary logs. As mentioned in [21], “it is all too often
the case that we tend to lack detailed information just when we
need it the most.” The existing logging functionalities provided
by modern operating systems and browsers are often insufﬁcient
to precisely reconstruct an attack. Below we discuss previous
works that aim to enhance logging and improve the ability to
investigate security incidents.
Enhanced Logging. To enable the analysis of security incidents,
Kornexl et al. [19] propose a network “Time Machine,” whose
goal is to efﬁciently record detailed information extracted from
network trafﬁc. The purpose of this system is to support forensic
analysis and network troubleshooting. To increase efﬁciency and