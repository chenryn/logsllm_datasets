[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nHow does queued ingestion affect my data?\n\nnAssistent: ", "response": [["The batching manager buffers and batches ingress data based on the ingestion settings in the [ingestion batching policy](batching-policy.md). The ingestion batching policy sets batch limits according to three limiting factors, whichever is first reached: time elapsed since batch creation, accumulated number of items (blobs), or total batch size. The default batching settings are 5 minutes / 1 GB / 1000 blobs, meaning there will be at least a 5-minute delay when queueing a sample data for ingestion."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nShould I use queued or streaming ingestion?\n\nnAssistent: ", "response": [["Queued ingestion is optimized for high ingestion throughput, and is the preferred and most performant type of ingestion. \nIn contrast, streaming ingestion is optimized for low ingestion latency. \nLearn more about [queued versus streaming ingestion](../../ingest-data-overview.md#continuous-data-ingestion)."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nDo I need to change the batching policy?\n\nnAssistent: ", "response": [["If default settings for the [ingestion batching policy](batching-policy.md) do not suit your needs, you can try lowering the batching policy `time`. \nSee [Optimize for throughput](../api/netfx/kusto-ingest-best-practices.md#optimize-for-throughput).\nYou should also update settings when you scale up ingestion. \nWhen you change batching policy settings, it can take up to 5-minutes to take effect."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nWhat causes queued ingestion latency?\n\nnAssistent: ", "response": [["Ingestion latency can result from the [ingestion batching policy](batching-policy.md) settings, or a data backlog buildup. To address this, adjust the [batching policy settings](batching-policy.md).\nLatencies that are part of the ingestion process can be [monitored](../../monitor-queued-ingestion.md)."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nWhere can I view queued ingestion latency metrics?\n\nnAssistent: ", "response": [["To view queued ingestion latency metrics, see [monitoring ingestion latency](../../monitor-queued-ingestion.md#view-the-ingestion-latency). The metrics `Stage Latency` and `Discovery Latency` show latencies in the ingestion process, and reveal if there are any long latencies."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nHow can I shorten queued ingestion latencies?\n\nnAssistent: ", "response": [["You can [learn about latencies](batching-policy.md#batching-latencies) and [adjust settings in the batching policy](batching-policy.md) to address issues that cause latencies such as data backlogs, inefficient batching, batching large amounts of uncompressed data, or ingesting very small amounts of data."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Queued ingestion and data latencies:\nHow is batching data size calculated?\n\nnAssistent: ", "response": [["The batching policy data size is set for uncompressed data. When ingesting compressed data, the uncompressed [data size is calculated](batching-policy.md#batch-data-size) from ingestion batching parameters, zip files metadata, or factor over the compressed file size."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Ingestion monitoring, metrics, and errors:\nHow can I monitor ingestion issues?\n\nnAssistent: ", "response": [["You can monitor ingestion [using metrics](../../using-metrics.md#ingestion-metrics), and by [setting up and using ingestion diagnostic logs](../../using-diagnostic-logs.md) for detailed table-level monitoring, viewing detailed ingestion error codes, and so on. \nYou can select specific metrics to track, choose how to aggregate your results, and create metric charts to view on your dashboard. See more about [streaming metrics](../../using-metrics.md#streaming-ingest-metrics) and [how to monitor queued ingestion](../../monitor-queued-ingestion.md)."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Ingestion monitoring, metrics, and errors:\nWhere can I view insights about ingestion?\n\nnAssistent: ", "response": [["You can use the portal's [Azure Monitor Insights](/azure/azure-monitor/app/app-insights-overview) to help you understand how Azure Data Explorer is performing and how it's being used.\nThe Insight view is based on [metrics](../../using-metrics.md) and [diagnostic logs](../../using-diagnostic-logs.md) that can be streamed to a Log Analytics workspace. \nUse the [.dup-next-ingest](dup-next-ingest.md) command to duplicate the next ingestion into a storage container and review the details and metadata of the ingestion."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Ingestion monitoring, metrics, and errors:\nWhere do I check ingestion errors?\n\nnAssistent: ", "response": [["The full ingestion process can be monitored using ingestion [metrics](../../using-metrics.md) and [diagnostic logs](../../using-diagnostic-logs.md). \nIngestion failures can be monitored using the `IngestionResult` metric or the `FailedIngestion` diagnostic log.\nThe [`.show ingestion failures`](ingestion-failures.md) command shows ingestion failures associated with the data ingestion management commands, and is not recommended for monitoring errors. \nThe [`.dup-next-failed-ingest`](dup-next-failed-ingest.md) command provides information on the next failed ingestion by uploading ingestion files and metadata to a storage container. \nThis can be useful for checking an ingestion flow, though is not advised for steady monitoring."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Ingestion monitoring, metrics, and errors:\nWhat can I do if I find many retry errors?\n\nnAssistent: ", "response": [["[Metrics](../../using-metrics.md) that include the `RetryAttemptsExceeded` metric status many times indicate that ingestion exceeded the retry attempt limit or time-span limit following a recurring transient error. \nIf this error also appears in the diagnostic log with [error code](../../error-codes.md) `General_RetryAttemptsExceeded` and the details \"Failed to access storage and get information for the blob,\" this indicates a high load storage access issue.\nDuring Event Grid ingestion, Azure Data Explorer requests blob details from the storage account.\nWhen the load is too high on a storage account, storage access may fail, and information needed for ingestion cannot be retrieved.\nIf attempts pass the maximum amount of retries defined, Azure Data Explorer stops trying to ingest the failed blob.\nTo prevent a load issue, use a premium storage account or divide the ingested data over more storage accounts.\nTo discover related errors, check the `FailedIngestion` diagnostic logs for error codes and for the paths of any failed blobs."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Ingesting historical data:\nHow can I ingest large amounts of historical data and ensure good performance?\n\nnAssistent: ", "response": [["To efficiently ingest large quantities of historical data, use [LightIngest](../../lightingest.md). \nFor more information, see [ingest historical data](../../ingest-data-historical.md). \nTo improve performance for many small files, adjust the [batching policy](batching-policy.md), change batching conditions and address [latencies](batching-policy.md#batching-latencies). \nUse the batching policy [wizard](../../table-batching-policy-wizard.md) to quickly change policy settings. \nTo improve ingestion performance when ingesting extremely large data files, use [Azure Data Factory](/azure/data-factory/) (ADF), a cloud-based data integration service."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer Ingesting invalid data:\nWhat happens when invalid data is ingested?\n\nnAssistent: ", "response": [["Malformed data, unparsable, too large or not conforming to schema, might fail to be ingested properly. For more information, see [Ingestion of invalid data](../../ingest-invalid-data.md)."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer SDKs and connectors:\nHow can I improve ingestion with SDKs?\n\nnAssistent: ", "response": [["When ingesting via SDK, you can use the ingestion [batching policy settings to improve performance](../../net-sdk-ingest-data.md). \nTry incrementally decreasing the size of data ingested in the table or database batching policy down towards 250 MB and check if there is an improvement."]]}]
[{"prompt": "User: Answer the question about Azure Data Explorer SDKs and connectors:\nHow can I tune Kusto Kafka Sink for better ingestion performance?\n\nnAssistent: ", "response": [["[Kafka Sink](https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md) users should [tune the connector](../../ingest-data-kafka.md) to work together with the [ingestion batching policy](batching-policy.md) by tuning batching time, size, and item number."]]}]