In IEEE Conference on Computer Vision and Pattern
for Image Recognition.
Recognition (CVPR), pages 770‚Äì778. IEEE, 2016.
[22] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong, and Yang Zhang.
Stealing Links from Graph Neural Networks. In USENIX Security Symposium
(USENIX Security). USENIX, 2021.
[23] Xinlei He, Rui Wen, Yixin Wu, Michael Backes, Yun Shen, and Yang Zhang. Node-
Level Membership Inference Attacks Against Graph Neural Networks. CoRR
abs/2102.05429, 2021.
[24] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip
Bachman, Adam Trischler, and Yoshua Bengio. Learning Deep Representations
by Mutual Information Estimation and Maximization. In International Conference
on Learning Representations (ICLR), 2019.
[25] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas
Papernot. High Accuracy and High Fidelity Extraction of Neural Networks. In
USENIX Security Symposium (USENIX Security), pages 1345‚Äì1362. USENIX, 2020.
[26] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. Manipulating Machine Learning: Poisoning Attacks and Countermea-
sures for Regression Learning. In IEEE Symposium on Security and Privacy (S&P),
pages 19‚Äì35. IEEE, 2018.
[27] Jinyuan Jia and Neil Zhenqiang Gong. AttriGuard: A Practical Defense Against
Attribute Inference Attacks via Adversarial Machine Learning. In USENIX Security
Symposium (USENIX Security), pages 513‚Äì529. USENIX, 2018.
[28] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang
Gong. MemGuard: Defending against Black-Box Membership Inference At-
tacks via Adversarial Examples. In ACM SIGSAC Conference on Computer and
Communications Security (CCS), pages 259‚Äì274. ACM, 2019.
[29] Yizhu Jiao, Yun Xiong, Jiawei Zhang, Yao Zhang, Tianqi Zhang, and Yangyong
Zhu. Sub-graph Contrast for Scalable Self-Supervised Graph Representation
Learning. CoRR abs/2009.10273, 2020.
[30] Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, and
Mohit Iyyer. Thieves on Sesame Street! Model Extraction of BERT-based APIs.
In International Conference on Learning Representations (ICLR), 2020.
[31] Klas Leino and Matt Fredrikson. Stolen Memories: Leveraging Model Memo-
rization for Calibrated White-Box Membership Inference. In USENIX Security
Symposium (USENIX Security), pages 1605‚Äì1622. USENIX, 2020.
[32] Shaofeng Li, Shiqing Ma, Minhui Xue, and Benjamin Zi Hao Zhao. Deep Learning
Backdoors. CoRR abs/2007.08273, 2020.
[33] Zheng Li and Yang Zhang. Membership Leakage in Label-Only Exposures. In
ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM,
2021.
[34] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie
Tang. Self-supervised Learning: Generative or Contrastive. CoRR abs/2006.08218,
2020.
[35] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face
In IEEE International Conference on Computer Vision
Attributes in the Wild.
(ICCV), pages 3730‚Äì3738. IEEE, 2015.
[36] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.
In IEEE
Exploiting Unintended Feature Leakage in Collaborative Learning.
Symposium on Security and Privacy (S&P), pages 497‚Äì512. IEEE, 2019.
[37] Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine Learning with Mem-
bership Privacy using Adversarial Regularization. In ACM SIGSAC Conference on
Computer and Communications Security (CCS), pages 634‚Äì646. ACM, 2018.
[38] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive Privacy Analy-
sis of Deep Learning: Passive and Active White-box Inference Attacks against
Centralized and Federated Learning. In IEEE Symposium on Security and Privacy
(S&P), pages 1021‚Äì1035. IEEE, 2019.
[39] Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas
Carlini. Adversary Instantiation: Lower Bounds for Differentially Private Machine
Learning. In IEEE Symposium on Security and Privacy (S&P). IEEE, 2021.
[40] Seong Joon Oh, Max Augustin, Bernt Schiele, and Mario Fritz. Towards Reverse-
Engineering Black-Box Neural Networks. In International Conference on Learning
Representations (ICLR), 2018.
[41] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff Nets: Stealing
Functionality of Black-Box Models. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 4954‚Äì4963. IEEE, 2019.
[42] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. Privacy Risks of General-
Purpose Language Models. In IEEE Symposium on Security and Privacy (S&P),
pages 1471‚Äì1488. IEEE, 2020.
[43] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. SoK:
Towards the Science of Security and Privacy in Machine Learning.
In IEEE
European Symposium on Security and Privacy (Euro S&P), pages 399‚Äì414. IEEE,
2018.
[44] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swami. The Limitations of Deep Learning in Adversarial
Settings. In IEEE European Symposium on Security and Privacy (Euro S&P), pages
372‚Äì387. IEEE, 2016.
[45] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Tal-
war, and √ölfar Erlingsson. Scalable Private Learning with PATE. In International
Conference on Learning Representations (ICLR), 2018.
[46] Nisarg Raval, Ashwin Machanavajjhala, and Jerry Pan. Olympus: Sensor Privacy
through Utility Aware Obfuscation. Symposium on Privacy Enhancing Technologies
Symposium, 2019.
[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. CoRR
abs/1409.0575, 2015.
[48] Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario Fritz, and Yang
Zhang. Updates-Leak: Data Set Inference and Reconstruction Attacks in Online
Session 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea858A APPENDIX
(a) Supervised Model
(b) Contrastive Model
Figure 15: The performance of different membership infer-
ence attacks against both supervised models and contrastive
models with ResNet-18 on 8 different datasets. The x-axis
represents different datasets. The y-axis represents member-
ship inference attacks‚Äô accuracy.
Learning. In USENIX Security Symposium (USENIX Security), pages 1291‚Äì1308.
USENIX, 2020.
[49] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and
Michael Backes. ML-Leaks: Model and Data Independent Membership Inference
Attacks and Defenses on Machine Learning Models. In Network and Distributed
System Security Symposium (NDSS). Internet Society, 2019.
[50] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and
Liang-Chieh Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
4510‚Äì4520. IEEE, 2018.
[51] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You Au-
tocomplete Me: Poisoning Vulnerabilities in Neural Code Completion. CoRR
abs/2007.02220, 2020.
[52] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Member-
ship Inference Attacks Against Machine Learning Models. In IEEE Symposium
on Security and Privacy (S&P), pages 3‚Äì18. IEEE, 2017.
[53] Congzheng Song and Ananth Raghunathan. Information Leakage in Embedding
Models. In ACM SIGSAC Conference on Computer and Communications Security
(CCS), pages 377‚Äì390. ACM, 2020.
[54] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine Learning
Models that Remember Too Much. In ACM SIGSAC Conference on Computer and
Communications Security (CCS), pages 587‚Äì601. ACM, 2017.
[55] Congzheng Song and Vitaly Shmatikov. Auditing Data Provenance in Text-
Generation Models. In ACM Conference on Knowledge Discovery and Data Mining
(KDD), pages 196‚Äì206. ACM, 2019.
[56] Congzheng Song and Vitaly Shmatikov. Overlearning Reveals Sensitive At-
tributes. In International Conference on Learning Representations (ICLR), 2020.
[57] Liwei Song and Prateek Mittal. Systematic Evaluation of Privacy Risks of Machine
Learning Models. In USENIX Security Symposium (USENIX Security). USENIX,
2021.
[58] Liwei Song, Reza Shokri, and Prateek Mittal. Privacy Risks of Securing Machine
Learning Models against Adversarial Examples. In ACM SIGSAC Conference on
Computer and Communications Security (CCS), pages 241‚Äì257. ACM, 2019.
[59] Florian Tram√®r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. Ensemble Adversarial Training: Attacks and Defenses. In
International Conference on Learning Representations (ICLR), 2017.
[60] Florian Tram√®r, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart.
In USENIX Security
Stealing Machine Learning Models via Prediction APIs.
Symposium (USENIX Security), pages 601‚Äì618. USENIX, 2016.
[61] A√§ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with
Contrastive Predictive Coding. CoRR abs/1807.03748, 2018.
[62] Laurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE.
Journal of Machine Learning Research, 2008.
[63] Binghui Wang and Neil Zhenqiang Gong. Stealing Hyperparameters in Machine
Learning. In IEEE Symposium on Security and Privacy (S&P), pages 36‚Äì52. IEEE,
2018.
[64] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised Feature
Learning via Non-Parametric Instance Discrimination. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 3733‚Äì3742. IEEE, 2018.
[65] Qizhe Xie, Zihang Dai, Yulun Du, Eduard H. Hovy, and Graham Neubig. Con-
trollable Invariance through Adversarial Feature Learning. In Annual Conference
on Neural Information Processing Systems (NIPS), pages 585‚Äì596. NIPS, 2017.
[66] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy Risk
in Machine Learning: Analyzing the Connection to Overfitting. In IEEE Computer
Security Foundations Symposium (CSF), pages 268‚Äì282. IEEE, 2018.
[67] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang
Shen. Graph Contrastive Learning with Augmentations. In Annual Conference
on Neural Information Processing Systems (NeurIPS). NeurIPS, 2020.
[68] Zhifei Zhang, Yang Song, and Hairong Qi. Age Progression/Regression by Con-
ditional Adversarial Autoencoder. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 4352‚Äì4360. IEEE, 2017.
[69] Bolei Zhou, √Ägata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
Places: A 10 Million Image Database for Scene Recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2018.
CIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.40.50.60.70.80.91.0AccuracyNN-basedMetric-corrMetric-confMetric-entMetric-mentLabel-onlyCIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.40.50.60.70.80.91.0AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea859(a) Supervised Model
(b) Contrastive Model
Figure 16: The performance of different membership inference attacks against both supervised models and contrastive models
with ResNet-50 on 8 different datasets. The x-axis represents different datasets. The y-axis represents membership inference
attacks‚Äô accuracy.
(a) Metric-corr
(b) Metric-conf
(c) Metric-ent
(d) Metric-ment
Figure 17: The performance of metric-based membership inference attacks against contrastive models with ResNet-50 on 8
different datasets under different numbers of epochs for classification layer training. The x-axis represents different numbers
of epochs. The y-axis represents membership inference attacks‚Äô accuracy. Each line corresponds to a specific dataset.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 18: The performance of attribute inference attacks against supervised models on 4 different datasets under different
percentages of the attack training dataset. The x-axis represents different percentages of the attack training dataset. The y-axis
represents attribute inference attacks‚Äô accuracy.
CIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.40.50.60.70.80.91.0AccuracyNN-basedMetric-corrMetric-confMetric-entMetric-mentLabel-onlyCIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.40.50.60.70.80.91.0Accuracy255075TrainingEpochs0.60.81.0AccuracyCIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places20255075TrainingEpochs0.60.81.0Accuracy255075TrainingEpochs0.60.81.0Accuracy255075TrainingEpochs0.60.81.0Accuracy10%30%50%70%90%PercentageofTrainingData0.00.20.40.6AccuracyMobileNetV2ResNet-18ResNet-5010%30%50%70%90%PercentageofTrainingData0.00.10.20.30.4Accuracy10%30%50%70%90%PercentageofTrainingData0.00.10.20.30.4Accuracy10%30%50%70%90%PercentageofTrainingData0.00.10.20.30.4AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea860(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 19: The performance of attribute inference attacks against supervised models on 4 different datasets under attack
models with different layers. The x-axis represents attack models‚Äô layers. The y-axis represents attribute inference attacks‚Äô
accuracy.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 20: The performance of metric-corr membership inference attacks against original contrastive models,Talos, Mem-
Guard, Olympus, and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents
different models. The y-axis represents the accuracy of metric-corr membership inference attacks.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 21: The performance of metric-conf membership inference attacks against original contrastive models, Talos, Mem-
Guard, Olympus, and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents
different methods. The y-axis represents the accuracy of metric-conf membership inference attacks.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 22: The performance of metric-ent membership inference attacks against original contrastive models, Talos, Mem-
Guard, Olympus, and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents
different models. The y-axis represents the accuracy of metric-ent membership inference attacks.
3456#.LayersofAttackModel0.00.20.4AccuracyMobileNetV2ResNet-18ResNet-503456#.LayersofAttackModel0.000.050.100.150.20Accuracy3456#.LayersofAttackModel0.000.050.100.150.20Accuracy3456#.LayersofAttackModel0.000.050.100.150.20AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea861(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 23: The performance of metric-ment membership inference attacks against original contrastive models, Talos, Mem-
Guard, Olympus, and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents
different models. The y-axis represents the accuracy of metric-ment membership inference attacks.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 24: The performance of label-only membership inference attacks against original contrastive models, Talos, MemGuard,
Olympus, and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents different
models. The y-axis represents the accuracy of label-only membership inference attacks.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 25: The performance of original classification tasks for the Talos models with MobileNetV2, ResNet-18, and ResNet-
50 on 4 different datasets under different adversarial factor ùúÜ. The x-axis represents different ùúÜ. The y-axis represents the
corresponding performance.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 26: The performance of membership inference attacks for the Talos models with MobileNetV2, ResNet-18, and ResNet-
50 on 4 different datasets under different adversarial factor ùúÜ. The x-axis represents different ùúÜ. The y-axis represents the
corresponding performance.
MobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7Accuracy012345Œª0.60.70.80.91.0AccuracyMobileNetV2ResNet-18ResNet-50012345Œª0.60.70.80.91.0Accuracy012345Œª0.60.70.80.91.0Accuracy012345Œª0.60.70.80.91.0Accuracy012345Œª0.400.450.500.550.60AccuracyMobileNetV2ResNet-18ResNet-50012345Œª0.400.450.500.550.60Accuracy012345Œª0.400.450.500.550.60Accuracy012345Œª0.400.450.500.550.60AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea862(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 27: The performance of attribute inference attacks for the Talos models with MobileNetV2, ResNet-18, and ResNet-
50 on 4 different datasets under different adversarial factor ùúÜ. The x-axis represents different ùúÜ. The y-axis represents the
corresponding performance.
012345Œª0.600.650.70AccuracyMobileNetV2ResNet-18ResNet-50012345Œª0.050.100.15Accuracy012345Œª0.150.200.250.30Accuracy012345Œª0.400.450.50AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea863