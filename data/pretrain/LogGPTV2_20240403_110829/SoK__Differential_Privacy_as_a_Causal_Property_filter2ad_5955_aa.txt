title:SoK: Differential Privacy as a Causal Property
author:Michael Carl Tschantz and
Shayak Sen and
Anupam Datta
2020 IEEE Symposium on Security and Privacy
SoK: Differential Privacy as a Causal Property
Michael Carl Tschantz∗, Shayak Sen†, and Anupam Datta†
∗International Computer Science Institute †Carnegie Mellon University
Abstract—We present formal models of the associative and
causal views of differential privacy. Under the associative view,
the possibility of dependencies between data points precludes
a simple statement of differential privacy’s guarantee as con-
ditioning upon a single changed data point. However, we show
that a simple characterization of differential privacy as limiting
the effect of a single data point does exist under the causal
view, without independence assumptions about data points. We
believe this characterization resolves disagreement and confusion
in prior work about the consequences of differential privacy. The
associative view needing assumptions boils down to the contra-
positive of the maxim that correlation doesn’t imply causation:
differential privacy ensuring a lack of (strong) causation does
not imply a lack of (strong) association. Our characterization
also opens up the possibility of applying results from statistics,
experimental design, and science about causation while studying
differential privacy.
I. INTRODUCTION
it produce almost
Differential Privacy (DP) is a precise mathematical property
of an algorithm requiring that
identical
distributions of outputs for any pair of possible input databases
that differ in a single data point. Despite the popularity of DP
in the research community, unease with the concept remains.
For example, Cuff and Yu’s paper states “an intuitive under-
standing can be elusive” and recommends that DP be related
to more familiar concepts based on statistical associations,
such as mutual information [8, p. 2]. This and numerous other
works exploring similar connections between DP and statistical
association each makes assumptions about the data points (e.g.,
[1, p. 9] [6, p. 32], [2, p. 4], [34, p. 14], [20, p. 6]).
The use of such assumptions has led to some papers stating
that DP implicitly requires some assumption: that it requires
the data points to be independent (e.g., [27, p. 2], [28, p. 1],
[32, p. 2], [23, p. 3], [5, p. 7], [50, p. 232], [33, p. 1]), that
the adversary must know all but one data point, the so-called
strong adversary assumption (e.g., [8, p. 2], [32, p. 10]), or
that either assumption will do (e.g., [49, §1.2]). (Appendix A3
provides quotations.) Conversely, other works assert that no
such assumption exists (e.g., [3], [25], [36], [35]). How
can such disagreements arise about a precise mathematical
property of an algorithm?
We put to rest both the nagging feeling that DP should be
expressed in more basic terms and the disagreement about
whether it makes various implicit assumptions. We do so by
showing that DP is better understood as a causal property
than as an associative one. We show that DP constrains effect
sizes, a basic concept from empirical science about how much
changing one variable changes another. This view does not
require any independence or adversary assumptions.
Furthermore, we show that
the difference between the
two views over whether DP makes assumptions is precisely
captured as the difference between association and causation.
That some fail to get what they want out of DP (without
making an assumption) comes from the contrapositive of the
maxim correlation doesn’t imply causation: DP ensuring a
lack of (strong) causation does not imply a lack of (strong)
association. Given the common confusion of association and
causation, and that DP does not make its causal nature explicit
in its mathematical statement, we believe our work explains
how disagreement could have arose in the research literature
about the what assumptions DP requires.
A. Motivating Example and Intuition
To provide more details, let us consider an example of
using DP inspired by Kifer and Machanavajjhala [27]. Suppose
Ada and her son Byron are considering participating in a
differentially private survey with n − 2 other people. The
survey collects a data point from each participant about their
health status with respect to a genetic disease. Since Ada
and Byron are closely related, their data points are closely
related. This makes them wonder whether the promise of DP
becomes watered down for them, a worrying prospect given
the sensitivity of their health statuses.
Figure 1 summarizes what would happen if both Ada
and Byron participate in the survey. In it, each solid arrow
represents a causal relationship where the quantity at the start
of the arrow causally affects the quantity at the end of the
arrow. For example, Arrow (1) represents that Ada’s genetics
has a causal effect on her son Byron’s genetics. We use an
arrow since causation is directional: Byron’s genetics does
not have a causal effect on Ada’s. Arrow (2) represents a
mechanism by which Ada provides her status to the survey.
This information becomes a data point in the survey’s data set,
that is, a row in a database. This database comprises Ada’s data
point, Byron’s data point, and n−2 other people’s data points.
Arrows (5), (6), and (7) together represent the algorithm that
computes the survey’s result, that is, the output produced from
the database using a differentially private algorithm.
As mentioned, Ada’s status also affects the status of her son
Byron, shown with Arrow (1). Therefore, their statuses are
statistically associated (i.e., not probabilistically independent).
While causation is directional, such associations are not:
seeing Byron’s status reveals information about Ada’s status
despite not causing Ada’s status. Furthermore, Ada’s and
Byron’s data points will be statistically associated because they
have a common cause, Ada’s status. Thus, seeing Byron’s data
point reveals information about Ada’s status and data point.
Since both Ada’s and Byron’s data points reveal information
about Ada’s status, the output can be informed by two data
points about Ada’s status. This double dose of information is
© 2020, Michael Carl Tschantz. Under license to IEEE.
DOI 10.1109/SP40000.2020.00012
354
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
Ada’s status R1
(2)
Ada’s data point D1
(1)

(5)
Byron’s status R2
(3)
Byron’s data point D2
 (6)
Survey output O
(n−2)
(7)
n − 2 other people’s statuses
(4)
n − 2 other data points
Fig. 1. A causal diagram approximating the process through which the output of a statistical query is generated and used. The arrows represent direct causal
effects. Indirect cause effects can be inferred from taking the transitive closure of the arrows.  labels causal effects bounded by -differential privacy. (1)–(7)
serve as labels naming arrows.
what gives Ada pause about participating. Furthermore, much
the same applies to Byron.
In the words of Kasiviswanathan and Smith [25, p. 2], DP
intuitively ensures that
changing a single individual’s data in the database
leads to a small change in the distribution on outputs.
(∗)
This intuitive consequence of DP, denoted as “(∗)”, does
not make explicit the notion of change intended. It implic-
itly compares the distribution over the output, a random
variable O, in two hypothetical worlds, the pre- and post-
change worlds. If we focus on the individual Ada and let
D1 be a random variable representing her data point as it
changes values from d1 to d(cid:48)
1, then the comparison is between
Pr[O=o when D1=d1] and Pr[O=o when D1=d(cid:48)
1]. The part
of this characterization of DP that is informal is the notion of
when, which leaves the notion of change imprecise. Our paper
contrasts various interpretations of change and when.
The most obvious interpretation is that of conditioning
upon two different values for the changed status. This in-
terpretation implies an approximation of statistical indepen-
dence between an individual’s data point and the output:
Pr[O=o | D1=d1] ≈ Pr[O=o | D1=d(cid:48)
1]. Presuming the
data points are truthful, such an approximate independence
implies (up to a factor) an approximate independence that
compares probabilities over a status with or without knowing
the output, that is, Pr[R1=r1 | O=o] ≈ Pr[R1=r1]. In this
case, observing the output reveals little about an individual’s
status, explaining this interpretation’s appeal.
However, as discussed above, both Ada’s and Byron’s
data points reveal information about each of their statuses
since associations depend upon the full breadth of causal
relations. This double dose of information about their statuses
means that DP does not actually imply this appealing form
of approximate independence. Thus, attempts to interpret DP
in terms of conditioning fail
to hold in the presence of
the associations between the data points. Those desiring an
associative guarantee from DP must rule out such double doses
of information, for example, by assuming that the data points
lack any associations or that the adversary already knows all
but one data point, making such associations uninformative.
Now, let us instead consider interpreting DP in terms of
interventions. This interpretation models artiﬁcially
causal
altering the value of random variables, as in a randomized
experiment. The key difference between intervening upon
a random variable and conditioning upon it
is that while
intervening tracks causal effects by accounting for how the
intervention may cause other variables to change, it does not
depend upon all the associations in the database since such
interventions break them. Thus, while associative deﬁnitions
using conditioning depends upon the distribution producing
data points, causal ones can screen off this distribution to
examine the behavior of just
the DP algorithm itself by
intervening upon all its outputs.
For example, suppose Byron is born without the genetic
disease and a scientist ﬂips a coin and ensures that Byron
has the disease if it comes up heads and ensures that he does
not if it comes up tails. (While the technology to execute this
experiment is currently wanting, it is conceptually possible.)
Since Bryon starts without the disease, the tails outcome does
nothing and can be viewed a control treatment while the
heads outcome causes a change. If it comes up heads, the
scientist could measure various things about Byron to see what
changed from giving him the disease. In particular, Byron’s
data point and the output computed from it would change.
On the other hand, nothing would change about Ada since
causation is directional. (Section IV makes this more precise.)
In fact, after the randomization, Bryon’s status and data point
no longer reveals any information about Ada’s status since the
randomization broke the association between their statuses.
The scientist can measure the size of any changes to
compute an effect size. The effect size for Byron’s data point
would be large since the data point is supposed to be equal to
the status, but the effect size for the output will be small since
it is computed by an algorithm with -differential privacy. If
we instead consider intervening on Ada’s status, we ﬁnd two
paths to the output: one via Ada’s data point (Arrows (2) and
(5)) and another via Byron’s (Arrows (1), (3), and (6)). These
two paths mean that the effect size could be as much as double
that of changing Byron’s status. Thus, DP cannot be interpreted
as limiting the effect of changing Ada’s status to just  in size.
Recall that the intuitive characterization (∗) of DP referred
to data points, not statuses: “changing a single individual’s data
in the database...” [25, p. 2]. So, let us consider intervening
upon the data points instead. Each data point is piped directly
into the differentially private algorithm and has no other
effects. Thus, DP does bound the effect size at  for Ada’s data
point without making any assumptions about the statuses. For
this reason, we believe DP is better understood as a bound on
effect sizes than as a bound on associations.
We believe that ease of conﬂating associative and causal
properties explains the disagreement in the research literature.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
355
(See Appendix A for a history of this disagreement.) Our
observation also reduces the beneﬁts and drawbacks of these
implicitly associative and causal views of privacy to those
known from studying association and causation in general. For
example, the causal view only requires looking at the system
itself (causation is an inherent property of systems) while
the associative view requires looking at the input distribution
as well. This difference explains why papers implicitly with
the associative view discuss the distribution over data points
despite the deﬁnition of DP and the implicitly causal papers
do not mention it.
The causal characterization also requires us to distinguish
between an individual’s attributes (Ris) and the data that is
input to an algorithm (Dis), and intervenes on the latter. Under
the assumption that individuals report their true statuses, the
associative interpretation does not require this distinction since
conditioning on one is identical to conditioning on the other.
This distinction captures an aspect of the difference between
protecting “secrets about you” (Ri) and protecting “secrets
from you” (Di) pointed out by McSherry [36], [35], where DP
protects the latter in a causal sense. An individual’s attribute
Ri is about him and its value is often outside of his control.
On the other hand, an individual’s data point Di, at least in
the setting typically envisioned for DP, is under his control
and is volunteered by the individual, making it from him.
B. Overview
Our main goal is to demonstrate that DP can be understood
as a causal property without needing the sorts of assumptions
made to view it as an associative property. We lay out the
associative view by surveying deﬁnitions presented in prior
work to show its awkward ﬁt for DP and how it leads to
suggestions that DP makes assumptions. We then turn to the
causal view, replacing conditioning with interventions in the
associative deﬁnitions. Doing so reveals three key insights;
we ﬁnd that the causal deﬁnitions (1) work without such
assumptions, (2) provides a tight characterization of DP, and
(3) explains how DP maps to a concept found throughout
statistics and science, namely to a measure of effect sizes.
We start our analysis with the associative view, which
uses conditioning (Section III). We ﬁrst consider conditioning
upon all the data points instead of just the changed one.
After dealing with some annoyances involving the inability
to condition on zero-probability data points, we get a precise
characterization of DP (Deﬁnition 2). However, this associative
deﬁnition does not correspond well to the intuitive characteri-
zation (∗) of differential privacy’s key consequences: whereas
the above-quoted characterization refers to just the changed
data point, this associative deﬁnition refers to them all, thereby
blurring the characterization’s focus on change.
Next, we modify the associative deﬁnition to condition upon
just the single changed data point (Deﬁnition 3). The resulting
deﬁnition prohibits more than an  degree of correlation
between the data point and the output, hereby limiting what
can be learned about the data point. While this deﬁnition
is not