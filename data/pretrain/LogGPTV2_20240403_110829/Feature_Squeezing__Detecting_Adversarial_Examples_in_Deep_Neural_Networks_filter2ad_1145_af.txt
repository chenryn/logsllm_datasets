12
TABLE IV: Detection rate on successful adversarial examples.
Conﬁguration
L∞ Attacks
Parameters
Threshold FGSM BIM
CW∞
Next
LL
0.0005
0.0002
0.0029
0.0390
-
0.0029
100.00% 98.90% 100.00% 100.00%
67.39% 7.69% 62.00% 76.00%
73.91% 26.37% 100.00% 100.00%
47.83% 12.09% 80.00% 83.00%
100.00% 98.90% 100.00% 100.00%
100.00% 98.90% 100.00% 100.00%
Deep
Fool
-
-
-
-
-
-
L2 Attacks
CW2
Next
L0 Attacks
JSMA
CW0
Overall
Detection
Rate
LL
Next
LL
Next
LL
100.00% 99.00% 54.00% 57.00% 100.00% 100.00% 90.30%
93.94% 90.00% 40.00% 41.00% 98.59% 100.00% 65.59%
96.97% 99.00% 84.00% 91.00% 97.18% 100.00% 86.84%
83.84% 93.00% 92.00% 98.00% 98.59% 100.00% 78.06%
100.00% 99.00% 92.00% 98.00% 100.00% 100.00%
100.00% 100.00% 93.00% 92.00% 100.00% 100.00% 98.15%
-
Best Attack-Speciﬁc Single Squeezer
Best Joint Detection (1-bit, 2x2)
1.9997
1.9967
1.7822
0.7930
0.3301
1.1296
1.9431
0.2777
0.7537
0.2904
0.8290
-
1.1402
0.00%
1.00%
0.00%
1.00%
0.00%
4.71% 8.70%
7.06% 15.22%
1.00%
9.41% 30.43% 78.00% 97.00% 19.39% 85.00% 96.00% 31.00% 30.00%
11.76% 26.09% 83.00% 90.00% 65.31% 95.00% 98.00% 12.00% 21.00%
3.53% 11.96% 42.00% 63.00% 55.10% 76.00% 86.00% 5.00%
3.00%
0.00% 0.00%
0.00% 0.00%
0.00% 1.02%
0.00% 0.00%
1.29%
2.22%
41.04%
45.29%
31.42%
25.88% 52.17% 96.00% 100.00% 71.43% 98.00% 100.00% 98.00% 100.00% 81.00% 85.71% 84.29%
48.80%
48.80%
55.45%
49.17%
54.90%
9.18%
5.88% 23.91% 70.00% 90.00% 2.04% 69.00% 89.00% 74.00% 95.00%
17.65% 36.96% 85.00% 93.00% 79.59% 91.00% 96.00% 8.00%
7.00% 30.00% 21.43%
21.18% 48.91% 91.00% 96.00% 74.49% 95.00% 100.00% 25.00% 35.00% 29.00% 25.51%
17.65% 35.87% 87.00% 94.00% 78.57% 91.00% 96.00% 8.00%
8.00% 32.00% 24.49%
21.18% 47.83% 91.00% 96.00% 72.45% 95.00% 100.00% 26.00% 34.00% 27.00% 24.49%
25.88% 52.17% 96.00% 100.00% 79.59% 98.00% 100.00% 98.00% 100.00% 81.00% 85.71%
27.06% 52.17% 97.00% 100.00% 79.59% 99.00% 100.00% 98.00% 100.00% 81.00% 85.71% 85.03%
0.00%
0.00%
0.00%
3.00%
7.00%
0.00%
0.00%
0.00%
4.08%
6.12%
4.00%
-
1.9957
1.9301
1.4293
0.7947
0.3686
1.0985
1.4348
0.6483
1.0423
0.6895
1.0809
-
1.2476
0.00%
1.01%
1.01% 1.12%
0.00% 0.00%
0.00% 0.00%
0.00%
16.16% 53.00% 58.59% 45.45% 33.71% 24.44% 38.14% 38.00% 28.00%
13.13% 48.00% 95.96% 100.00% 62.92% 83.33% 100.00% 85.00% 100.00%
9.09% 10.00% 84.85% 100.00% 47.19% 88.89% 100.00% 76.00% 100.00%
6.06% 2.00% 64.65% 96.97% 32.58% 92.22% 100.00% 46.00% 99.00%
41.41% 41.00% 95.96% 100.00% 76.40% 92.22% 100.00% 99.00% 100.00%
37.37% 59.00% 92.93% 97.98% 69.66% 83.33% 98.97% 97.00% 100.00%
13.13% 13.00% 79.80% 94.95% 42.70% 92.22% 97.94% 48.00% 90.00%
22.22% 36.00% 92.93% 100.00% 61.80% 93.33% 100.00% 73.00% 98.00%
12.12% 13.00% 81.82% 94.95% 42.70% 90.00% 97.94% 48.00% 90.00%
22.22% 34.00% 94.95% 100.00% 60.67% 93.33% 100.00% 75.00% 98.00%
41.41% 59.00% 95.96% 100.00% 76.40% 93.33% 100.00% 99.00% 100.00%
44.44% 52.00% 96.97% 100.00% 79.78% 90.00% 100.00% 98.00% 100.00%
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
6.90%
40.00%
76.43%
68.57%
59.29%
84.05%
83.10%
62.62%
75.00%
62.86%
75.24%
-
85.24%
Squeezer
Bit Depth
Median Smoothing
Bit Depth
Median Smoothing
Non-local Means
T
S
I
N
M
0
1
-
R
A
F
I
C
Bit Depth
t
e
N
e
g
a
m
I
Median Smoothing
Non-local Means
1-bit*
2-bit
2x2*
3x3
1-bit
2-bit
3-bit
4-bit*
5-bit
2x2*
3x3
11-3-2
11-3-4
13-3-2
13-3-4*
1-bit
2-bit
3-bit*
4-bit
5-bit
2x2*
3x3
11-3-2
11-3-4
13-3-2
13-3-4*
Best Attack-Speciﬁc Single Squeezer
Best Joint Detection (5-bit, 2x2, 13-3-2)
Best Attack-Speciﬁc Single Squeezer
Best Joint Detection (5-bit, 2x2, 11-3-4)
TABLE V: Details of the best joint detectors. SAE: successful adversarial example. FAE: failed adversarial example.
Dataset
MNIST
CIFAR-10
ImageNet
Squeezers
Bit Depth (1-bit), Median (2 × 2)
Bit Depth (5-bit), Median (2 × 2), Non-local Means (13-3-2)
Bit Depth (5-bit), Median (2 × 2), Non-local Means (11-3-4)
Threshold
0.0029
1.1402
1.2476
Detection
Rate (SAEs)
98.15%
85.03%
85.24%
Detection
Rate (FAEs)
20.00%
9.09%
25.00%
False
Positive
Rate
3.98%
4.93%
4.70%
ROC-AUC
(including
FAEs)
94.51%
95.67%
94.04%
ROC-AUC
(excluding
FAEs)
99.61%
95.86%
94.51%
across all of the squeezers in a joint detector; we expect there
are better ways to combine multiple squeezers that would use
diﬀerent thresholds for each of the squeezers to avoid this
detection reduction, and plan to study this in future work.
Table V summarizes the overall performance of the best
joint detection conﬁguration we found for each dataset. The
overall detection rate is 98.15% on MNIST, 85.03% on
CIFAR-10 and 85.24% on ImageNet.
We report ROC-AUC scores in Table V both counting the
failed adversarial examples as legitimate (Include FAEs), and
excluding the failed adversarial examples from consideration
(Exclude FAEs), since it is not clear what the correct output
should be for a failed adversarial example and in some
scenarios detecting a failed adversarial example as an attack is
beneﬁcial. Our joint-detector achieves around 95% ROC-AUC
score for all three datasets. Excluding the failed adversarial
examples, the ROC-AUC of the detector is as high as 99.61%
for MNIST, and above 94.5% for ImageNet. The false positive
rate on legitimate examples are all lower than 5%, which is
expected considering how we select a threshold value in the
training phase. The detection rate for the best conﬁguration on
successful adversarial examples exceeds 98% for MNIST using
a 1-bit ﬁlter and a 2× 2 median ﬁlter and exceeds 85% for the
other two datasets using a combination of three types feature
squeezing methods with diﬀerent parameters. The detection
rates for failed adversarial examples are much lower than those
for successful adversarial examples, but much higher than the
false positive rate for legitimate examples. This is unsurprising
since FAEs are attempted adversarial examples, but since they
are not successful the prediction outputs for the unsqueezed
and squeezed inputs are more similar.
D. Adversarial Adaptation
So far, we have only considered static adversaries, who
use state-of-the-art methods to ﬁnd adversarial examples but
do not adapt to attack our feature squeezing method directly. It
is diﬃcult to speculate on future attacks, but in this section we