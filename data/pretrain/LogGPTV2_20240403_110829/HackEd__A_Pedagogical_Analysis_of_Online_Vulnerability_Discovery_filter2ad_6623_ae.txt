(cid:35) (cid:71)(cid:35)
(cid:32) (cid:32)
(cid:35) (cid:32)
(cid:35) (cid:32)
(cid:35) (cid:32)
(cid:35) (cid:71)(cid:35)
(cid:32) (cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:35)
(cid:35) (cid:71)(cid:35)
(cid:32) (cid:32)
(cid:35) (cid:32)
(cid:35) (cid:32)
(cid:32) (cid:32)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:32)
(cid:32) (cid:32)
TABLE I: Results of our pedagogical review of 31 exercises. Each column indicates whether an exercise implemented the
pedagogical dimension fully ((cid:32)), partially ((cid:71)(cid:35)), or not at all ((cid:35)).
1272
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
as the student feels more involved in the learning process and
likely more motivated to continue participation [69].
Difﬁculty levels and point assignments are not optimal.
These assignments are made based on the best judgment of
the organizers, and it can be hard for students to determine
what “Easy” or “10pts” means. In our review, we observed
multiple cases where more complicated challenges were rated
easier or assigned fewer points than less complex challenges
in the same exercise (4% of challenges reviewed per exercise,
on average). This supports prior ﬁndings, which have shown
similar difﬁculty with labeling [16], commonly due to inconsis-
tencies among multiple challenge authors—a common practice
in security exercises (N=18). The Vulnhub organizer explained
this problem, saying “What you ﬁnd easy I will ﬁnd difﬁcult
and vice versa. . . someone new to the industry [might say],
‘This is the ﬁrst time I’ve seen this, this is really super hard.’
Then give it to a seasoned pen tester and he thinks ‘I saw
that two weeks ago.”’ This could potentially inhibit students’
ability to personalize their learning or their self-conﬁdence.
Several exercises appear to understand this problem. Some
try to mitigate it by allowing students to rate or comment on
exercises (N=5). However, in HackTheBox, challenges can
only be rated after solving the challenge, missing feedback
from a likely important segment of students, i.e., those stuck on
the challenge due to its difﬁculty. Additionally, two exercises,
picoCTF and Root-me.org, only allow students to indicate
whether they liked the challenge, which might not correlate
with challenge difﬁculty. Many other organizers provide a
dynamic measure of difﬁculty based on the number of students
who have solved the challenge (N=15).
Only two exercises activated prior knowledge. While
most exercises implicitly leveraged prior knowledge, only
two—Root-me.org and HackthisSite—activated it by drawing
students’ attention to previously learned concepts they should
remember when trying to solve the challenge. Both did
this by including a list of related prerequisite knowledge
(e.g., HackthisSite listed “some encryption knowledge” as a
prerequisite for a Caesar cipher decryption challenge). By
pointing to speciﬁc prior knowledge, an exercise can help the
student select the appropriate knowledge to build on, helping
them avoid potential misconceptions [75].
Some exercises required challenges to be solved in
increasing complexity order. When exercises did not personal-
ize by experience (N=8), they always had hierarchical problem
paths: more complex problems only unlocked after solving
lower-level problems. This could get tedious. For example,
in picoCTF, more experienced students may be frustrated as
they are required to solve several simple problems—designed
for new learners—before they can unlock more interesting
challenges. When asked the reason for this design, the picoCTF
organizers explained it was “just for convenience, since the
year-round version is not really any different from the actual
competition period.” This was a common sentiment among
organizers, with all but the XSS-Game organizers stating
the lack of experience personalization was intentional
to
avoid overwhelming new students. XSS-Game’s organizers
forced students to follow a speciﬁc path, only unlocking new
challenges when the previous one was solved, to prevent
students from jumping in too far and feeling overwhelmed.
Few exercises personalized based on age or education.
Nine made explicit mention of the age or education level
targeted. The remaining exercises appeared to target university-
level students or above. Pwnadventure’s organizer explained the
exercise was originally designed to be live in conjunction with
the ﬁnals of a larger university-level CTF—CSAW CTF [76].
While targeting a more educated audience is likely necessary
for more complicated concepts, it should be clearly stated—
possibly with links to other resources—to help younger, less
educated students who might otherwise be deterred from
hacking exercises entirely. Pwnadventure’s organizer agreed,
saying “It wouldn’t be a bad idea to give people that context
and just say, ‘This is how it was designed. So if you ﬁnd this
too hard, that’s expected. This was intended for this audience.”’
2) Utilization: For utilization, we checked if knowledge
gained in prior challenges was required to solve later challenges,
building on within-exercise prior knowledge.
Exercise designers build clear challenge concept progres-
sions. Almost all exercises (N=29) include some challenges
(32% of challenges reviewed on average) whose concepts build
on others. As an example, Microcorruption offers a progression
across several challenges to teach buffer overﬂow concepts.
One challenge requires the student to disassemble the program
and read a hardcoded password string, then the next forces the
student to read the assembly code and understand the stack
to reconstruct the password. Next, the student must exploit a
simple buffer overﬂow with no mitigations. The progression
continues by adding mitigations to complicate exploitation.
In the two cases where subsequent utilization of knowledge
was not observed (Infosec Institute, iCTF), all the challenges
covered a disparate set of unrelated concepts. This was likely
because these exercises had some of the least number of
challenges, but chose to cover a breadth of topics. We expect
we would have seen subsequent utilization of knowledge if
their organizers added additional challenges.
B. Organizing Declarative Knowledge
Another key to effective learning comes in students’ ability
to transform facts into robust declarative knowledge [18]. To
achieve subject mastery, students must go beyond memorizing
facts or speciﬁc tricks, but also organize the underlying abstract
concepts into a structured knowledge base [19], [29], [67],
[68]. Prior work comparing experts and novices has found
that while experts do tend to know more facts, their biggest
improvement comes from the rich structure of their knowledge
base [19]. This allows them to improve recall, recognize
patterns, make analogies with previously observed scenarios,
and identify key differences between contexts, supporting
improved knowledge transfer [19], [77]. In an example (drawn
from similar challenges across several exercises), after solving
a Caesar cipher challenge and then a cryptographic hashing
challenge requiring a dictionary attack, the student should
identify the common cryptographic weakness of limited key
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1273
spaces. They can then apply this abstract concept to solve a
future challenge: decrypting text encrypted with RSA where
too-small prime numbers were used to generate the key pair.
To support deeper conceptual understanding, exercises can
organize challenges according to the concepts taught to make
these knowledge structures clear [78], [79]. For this core
principle, we considered how the information was organized
and the context in which it was presented. We also considered
the types of security concepts covered by each exercise, but
found little differentiation across exercises. For brevity, we
leave reporting on these results to Appendix C.
1) Organization: When asked to organize facts, experts often
sort them into hierarchical structures, demonstrating understand-
ing of complex interrelationships [19]. Exercises can use textual
and visual cues to help students make these connections. We
looked at whether exercises grouped challenges by concept,
creating a hierarchy, or highlighted a problem path, showing
linear concept progressions through challenges. We considered
whether exercises went beyond general categorizations (e.g.,
crypto, binary exploitation) and presented lower-level structures
via explicit cues (e.g., textual, visual, or structural) to help
students recognize overarching structure. This differs from
Utilization, as we consider whether relationships are made
explicit, instead of needing to be inferred.
Many exercises lacked explicit structure. Explicit cues,
such as challenge names indicating a concept hierarchy or
deﬁning a progression through conceptually-related problems,
can help students associate individual facts [19], [79]. A
majority (N=18) of exercises did not clearly organize problems
to group challenges with related concepts together. Similarly,
several exercises did not provide a path through more than two
to three challenges as an organizing concept guide (N=12).
Interestingly, almost every exercise that relied on crowd-
submitted challenges (N=6)—useful for reducing organizer
workload while scaling up—did not provide a clear structure.
Vulnhub’s organizer explained “There is metadata for about
a quarter of the challenges on the backend that’s saying, this
one has ﬁle inclusion, this one has an apache vulnerability,
whatever. I was going to implement another feature that would
take this metadata and help plot it out. Then VMs went up
and I have never implemented it. . . I’ve just not kept up to
date with it because I was going through everything manually
and you can’t trust each author’s opinion.” The authors of
Root-me.org, by contrast, do present author-provided metadata;
their response did not clarify whether this metadata is reviewed
(requiring added organizer effort) or not. We note that even
in this case, the author-provided categorizations are typically
quite broad; ﬁne-grained connections are typically signaled
only when a single author develops a set of challenges with
incrementing names, showing a progression (45% of challenges
reviewed included ﬁne-grained connections).
2) Context: We also reviewed the context within which
concepts were organized, which can potentially impact infor-
mation retention and conceptualization [18], [19], [78]. We
considered four dimensions. First, whether authoritative content
was presented with the challenge (e.g., video or textual lecture).
Next, we asked whether the exercise used a goal-driven project
approach, which could help with engagement as students see
how individual challenges ﬁt within a broader, more realistic
context [30]. We also considered whether any overarching
story or narrative was provided to connect learning, as people
are more likely to remember information when presented in
narrative form [80]. Finally, we assessed whether any challenge
programs demonstrated realistic complexity. We note that there
is signiﬁcant beneﬁt in simpliﬁed challenges, which limit
repetitive tasks (e.g., port scanning), focus student attention
on speciﬁc problems [81], [82], and provide less experienced
students an entry point. However, including some realistic
challenges could help students see concept relevance, improving
intrinsic motivation [69], and could also support knowledge
transfer [77] and the development of practical skills [83].
Stories are the only commonly used method. Many
exercises included narrative elements (N=12). The GirlsGo
CyberStart organizer said they chose to embed each challenge
within a narrative to teach: “Why is a hacker or bad guy using
this action? As an educator, you’re trying not to just state facts
and have them absorb them or try a technique and just do
it. You want to give context.” While narrative was used in
fewer than half of exercises, it was by far the most prevalent
practice. Few exercises used lectures (N=6), and only one
included challenges with sub-tasks that had to be solved to
together achieve an overarching goal. Organizers who did not
include these contextual elements (O=4) explained they “got
in the way of [challenge author] creativity” (Angstrom) (O=2)
or do not apply when challenges are“submitted by several
different people” (O=2) (Crackmes.one). These organizers
agreed that adding context could help students, but would
require signiﬁcant effort and might reduce the number and
uniqueness of challenges, with a net-negative effect on learning.
Few exercises included realistic challenges. Few exercises
included any challenges representative of real-world-scale
programs (N=9). This practice may inhibit learning practical
skills for scaling analyses to larger programs. However, many
organizers speciﬁcally avoid realistic challenges to focus
attention on speciﬁc concepts, which they considered more
important (O=10). Others chose to avoid complexity—and
accordingly limit extraneous tasks—because they wanted to
make sure their exercise was fun and engaging (O=6). In fact,
this is a common educational tradeoff between realistic settings
and extraneous load (discussed later in Section III-E2). The
gCTF organizers explained “you focus mostly on the problem
solving part that gives the players joy. . . The recon part is
required in real world pen testing. . . , but in some cases, it
either will be mostly luck based if you are looking in the
right place at the right time, or developing the [necessary]
infrastructure will just take most of your weekend.” Because of
these tradeoffs and the inherent difﬁculty of building realistic
challenges (O=2), it seems likely that realistic challenges should
be included purposefully but sparingly.
We note that this was the most commonly updated dimension
during interviews (O=3). In all three cases, our result changed
from “No” to “Partial,” as the exercises included a few realistic
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1274
challenges among a large number provided by the community.
This may indicate we underestimate the number of exercises
providing realistic challenges; however, organizers generally
agreed that realistic challenges were rare, meaning the average
student would rarely or never encounter them.
While realistic challenges were uncommon, HackEDU used
a unique approach to incorporating real-world programs: provid-
ing vulnerable programs reproduced from public vulnerability
reports on HackerOne, a popular bug bounty platform.
C. Practice and Feedback
Prior work shows students must perform a task to achieve
mastery [77], [83], [84]. Through deliberate, active practice, stu-
dents can translate abstract concepts into practical knowledge.
To support this practice, students must also receive tailored
feedback to guide learning to speciﬁc goals [19]. Without
feedback, students may become lost or misunderstand the
exercise’s learning objective [18], [19], [79]. Therefore, we
considered two dimension groups: actionability and feedback.
1) Actionability: For Actionability, we considered the types
of tasks exercises ask students to complete. Speciﬁcally,
whether students had to exploit insecure programs (e.g., perform
a buffer overﬂow or decrypt a weak ciphertext) or write
secure programs—from scratch or by patching vulnerable code.
For the latter, we did not consider exercises that required
students to write programs for exploitation purposes (e.g., to
brute-force a cryptographic challenge). We only considered
an exercise as meeting this dimension if the code students
produced was evaluated for security vulnerabilities. We found
that all exercises required students to exploit programs, so we
show these results in Appendix C for brevity.
Secure development practice was uncommon in our
dataset. Very few exercises (N=5) included challenges asking
students to write secure code and two (Hellbound Hackers
and Pwnable) only included a few (6% and 12% of reviewed
challenges, respectively). Instead, students are left to make the
logical jump from identifying and exploiting to preventing a
vulnerability without educational support. For example, XSS-
Game—explicitly targeted at training developers—has students
identify XSS vulnerabilities, but does not include any infor-
mation regarding the impact of these types of vulnerabilities
or how to avoid them. Most organizers agreed that this is
because secure development practice is difﬁcult to evaluate
(O=8). This included the XSS-Game organizers, who said “the
problem is that it’s really hard to test that [the vulnerability is]
ﬁxed properly. . . You actually either have to have somebody
manually test it, or a really good checker that’s checking a ton
of edge cases.” Other organizers chose not to include secure
development challenges because they wanted to limit the scope
of their exercise to focus students on exploitation (O=7).
The three exceptions were HackEDU [41], BIBIFI [46], and
iCTF [37]. HackEDU used a similar structure to other exercises,
asking students to ﬁrst identify and exploit the vulnerability
in sample code. However, students then patch the vulnerable
program by following instructions that walked them through
how to make the program secure. BIBIFI and iCTF used
variations of an attack/defense model. iCTF provided each team
with a set of identical vulnerable services running on a shared
network. Students were tasked with ﬁnding vulnerabilities that
they could exploit on other teams’ machines (attack) and patch
on their own (defense). BIBIFI followed a similar approach,
but asked participants to ﬁrst write their own medium-sized
application according to a given speciﬁcation, considering
tradeoffs between security and functionality. Other teams then
search for vulnerabilities in the student’s code; when found, the
original students are asked to patch the identiﬁed vulnerabilities.
BIBIFI and iCTF are not unique in their use of the
attack/defense model, and we expect we would have seen more
examples of secure development practice if more attack/defense
exercises were included. Unfortunately, because this model
depends on live interaction with other competitors, exercises
using this model are typically only available during restricted
time periods. We were only able to actively participate in these
two, due to direct support of their organizers. However, as both
organizers pointed out, the attack/defense model introduces
inherent tradeoffs. To facilitate the back-and-forth offensive
and defensive actions, motivation throughout the competition is
needed. To maintain fair gameplay, this limits possible support,
structure, and peer engagement. Also, the live nature of services
(i.e., students could patch at any time) introduces logistical
hurdles, for example in indicating challenge difﬁculty. Finally,
while attack/defense exercises offer a better option for secure
development practice, “coding to the test” is still possible. Other
teams have limited time to review and exploit modiﬁcations, so
students may be incentivized to produce minimal ﬁxes without
resolving underlying security problems.
2) Feedback: We considered several potential forms of
Feedback. The ﬁrst—expected to be most helpful [85]—was
direct, in-context feedback, where guidance is tailored to the
student’s approach so far, directing them down the “correct
path.” We also considered whether less tailored feedback was
included in the form of static hints or opportunities for students
to seek feedback in forums or challenge walkthroughs.
Exercises rarely provided direct feedback throughout.
Most exercises only provided direct feedback in the form of
a “ﬂag check” (N=19): allowing a student to verify they have
identiﬁed the correct solution by submitting a random string
associated with challenge success. This string matching is
likely problematic, as simple typos or copy/pasting issues can
lead students to misinterpret rejected submissions as incorrect
solutions. This problem is further exacerbated when some
exercises (N=4) do not use consistent ﬂag formats, causing
students to question whether the string found is actually a ﬂag.
Some challenges provide “correct path” markers. In
exercises marked as partial (N=10), some challenges update
their output if the student is following the correct path, even
if the exploit was not fully successful. For example, in Root-
me.org’s Format string bug basic 2 challenge, the program
checks for target address modiﬁcations. If this address is
modiﬁed, but not to the correct address, the program outputs
“You’re on the right track.” However, this feedback was sparse
within exercises, with only one or two challenges providing it.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1275
Many organizers said providing speciﬁcally tailored feed-
back for each challenge was difﬁcult (O=6). The GirlsGo
CyberStart organizer pointed out that for some challenges
where exploitation occurs locally, they do not have a way to