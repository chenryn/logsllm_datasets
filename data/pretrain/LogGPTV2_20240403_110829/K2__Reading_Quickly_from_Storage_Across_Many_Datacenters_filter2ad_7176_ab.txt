to having full replicas in 3 datacenters, as shown in Figure 2c.
Design Goal 2: Often Zero Cross-Datacenter Requests.
Achieving design goal 1 gives K2 the best possible worst-
case end-user latency, matching full replication. But it is also
no better. To provide a latency beneﬁt K2 must avoid cross-
datacenter requests in the common case. Such a scenario is
shown in Figure 2d. This scenario is possible in a RAD
deployments. However, it is unlikely as it requires all the data
needed to serve a user’s request to be in the 1/3 of data located
in the nearby datacenter. Our design goal 2 is thus to often
complete with zero rounds of cross-datacenter requests.
III. K2 BASIC DESIGN
This section presents the basic architecture of K2. Sec-
tions IV presents the replication design in K2. Section V
completes the design with our read-only transaction algorithm.
Figure 3 shows the architecture of K2. We base our design
on fully-replicated Eiger [39]. K2 inherits the mechanisms
for tracking and enforcing causal consistency, local write-
only transactions, and garbage collection from Eiger. The
major changes in our design include our new algorithms
for replication, cache-aware read-only transactions, and an
LRU-like cache replacement policy. We also introduce some
changes to Eiger’s replicated write-only transaction algorithm
to achieve our design goal 1.
A. Server Side Design
Within each datacenter,
the keyspace is sharded across
servers that are each responsible for a subset of the keyspace.
For simplicity, our discussion here focuses on the simple
key-value storage model, though our implementation uses the
richer column-family data model [18], [34].
UserFEBEFar DCUserFEBEFar DCFEBENear DCUserFEBEFar DCFEBENear DCUserFEBEFar DCBENear DCFEstorage systems ensure writes appear in a causally-consistent
order using two types of metadata. The ﬁrst is by attaching a
unique Lamport timestamp to each write. Storage servers use
these timestamps with a last-writer-wins policy [54] to ensure
causally later writes always overwrite earlier ones [38]. The
second type of metadata the client library tracks is explicit
causal dependencies. The client library tracks only the one-
hop dependencies, deps, that include the client’s previous write
and the writes of all values it has read since that write. Lamport
timestamps combined with explicit one-hop dependencies are
sufﬁcient to ensure causal consistency while introducing less
overhead than vector clocks [39].
Each dependency is a  pair. The client
library updates its deps after a read-only or write-only trans-
action completes. It includes deps when it sends write-only
transactions to the local datacenter. The dependencies are
then used during inter-datacenter replication to enforce causal
consistency. Using one-hop dependencies frees servers from
needing to store metadata about causal dependencies and is
sufﬁcient for enforcing causal consistency [39].
C. Local Write-Only Transactions
In K2, a client commits its write-only transactions in its
local datacenter by following a variant of the two-phase
commit protocol [50]. The client splits keys into sub-requests
and sends each to the corresponding servers—i.e., partici-
pants—in the local datacenter. It picks one key at random
to be the coordinator-key. The participant holding this key is
the coordinator and the others are cohorts. Each participant
prepares by marking the keys in its sub-request as pending and
sends Yes to the coordinator. Once all cohorts prepare, the co-
ordinator assigns its current logical time (Lamport timestamp)
as the version number and the earliest valid time (EVT) of
this transaction. The version number uniquely identiﬁes this
transaction, and is used in other datacenters to apply writes
in the correct causal order. The EVT indicates the logical
time when the new versions are made visible in a datacenter,
and is used locally in this datacenter as part of the read-only
transaction protocol. The coordinator commits the transaction,
sends each cohort a Commit that includes the version number
and EVT. It then replies to the client with the version number.
If writing to a non-replica key, the server commits only the
metadata—i.e., the key, version, and EVT—and caches the
value. Caching the write reduces read latency by avoiding
unnecessary remote fetches for this value. The client library
updates its deps and the read timestamp (§V) to maintain
causal consistency. It updates deps by ﬁrst clearing it and then
adding the  pair to deps.
IV. REPLICATION DESIGN
This section presents K2’s replication design (§IV-A), and
how its design decisions progressively build on each other to
provide at most one round of non-blocking cross-datacenter
read requests (§IV-B).
Fig. 3: The system architecture of K2. Each datacenter has the
entire keyspace sharded across servers, and a small amount of
cache. It stores data for a subset of keys and only metadata
for the rest. K2 partially replicates data and fully replicates
metadata by following a constrained topology.
Data and Metadata. Each datacenter stores the metadata for
the entire keyspace and data (i.e., values) for a subset of keys.
Metadata includes a key and a version number, which uniquely
identiﬁes the value of this key and is assigned by the datacenter
that wrote this version. If a datacenter D always stores the
value of a key K, then D is a replica datacenter of K, and
K is a replica key in D. If D does not always store the value
of K, then D is a non-replica datacenter of K, and K is a
non-replica key in D. The value for each key is stored in a
set of f datacenters, which tolerates up to f − 1 simultaneous
datacenter failures. We assume the mapping of keys to their
f replica datacenters is known to each datacenter.
Cache. K2 augments each server with a small amount of cache
containing additional values. K2 uses its cache to help achieve
our design goal 2 of often avoiding any cross-datacenter
requests. K2 caches a value of a non-replica key after fetching
it from remote datacenters. K2 also temporarily caches the
values for the writes of non-replica keys from local clients.
An advantage of the cache in K2 is that writes of non-replica
keys can commit locally and thus have low latency. K2’s
read-only transaction algorithm leverages the cached values
to often avoid cross-datacenter requests and satisfy read-only
transactions entirely in the local datacenter. We implement an
LRU-like cache-eviction policy.
Clock. Servers and clients keep Lamport clocks [35], which
advance upon message exchange. All operations are uniquely
identiﬁed by a Lamport timestamp. The high-order bits of the
timestamp are the Lamport clock, and the low-order bits are
the unique identiﬁer of the stamping machine.
B. Client Library
Each client has a client library that works in tandem with
the storage servers. The library has two roles: First, it is the
interface between the client and the storage system. Second,
it tracks and attaches metadata to requests to help ensure that
writes appear in a causally-consistent order.
The client
library routes operations to the appropriate
servers in the local datacenter and executes the read-only
transaction and write-only transaction algorithms. It helps the
ABCMetadata DataKeyVersionValueK1v1val1K2v2val2......{ }Knvn{ }Cache(1)Metadata + Data(2)MetadataIncomingWriteskey vervaldeps...Metadata Replication+Constrained TopologyCache-awareRead-onlyTransactionsWrite-onlyTransactionsClientStorage ServersClientLibraryEnabling Design Goal 1Enabling Design Goal 2A. Replication Design
Metadata Replication and Constrained Replication Topol-
ogy. Metadata replication decouples data and metadata repli-
cation. It replicates the metadata of a write—i.e., key, version,
and dependencies—to all datacenters, and replicates the data
of a write—the value—only to replica datacenters. Con-
strained replication topology orders the replication to replica
datacenters before the replication to non-replica datacenters.
Replication of Write-Only Transactions. Replication of a
write-only transaction is done by each participant (coordi-
nator/cohort) in its local datacenter after it commits locally
(§III-C). Each participant asynchronously replicates each key
in its sub-request to its equivalent participants—the servers
storing the same key—in other datacenters in two phases.
In the ﬁrst phase, the local participant replicates data and
metadata to the replica participants in parallel. When a
participant receives a replicated write that includes data, it
in the IncomingWrites table before
immediately stores it
sending an acknowledgment to the sender. Once the local
participant has been notiﬁed by all replica participants,
it
proceeds to the second phase. In the second phase, it replicates
the metadata and the list of replicas storing the value to the
non-replica participants. Only the coordinator needs to include
causal dependencies with its metadata replication because each
remote coordinator does dependency checks for its transaction
group. K2’s replication is asynchronous and not on-path for
client-facing operations. Hence, it does not affect the latency
of any client’s operations. K2 introduces the IncomingWrites
table to make the new data accessible only to remote reads
while the transaction is pending. This table is not visible to
local reads.
Committing Replicated Write-Only Transactions. Repli-
cated write-only transactions are committed using a protocol
that is a variant of two-phase commit. Each cohort notiﬁes
its transaction coordinator after receiving the replicated sub-
request of the transaction. Concurrently, the coordinator issues
the dependency checks for the transaction by contacting the
local servers responsible for those dependencies. A local
server replies to the dependency check immediately if the
speciﬁed  is committed, otherwise it waits
until it is committed to reply. The coordinator then waits for
all dependencies to verify and to be notiﬁed by all cohorts
before beginning two-phase commit. This waiting for one-hop
dependencies before applying replicated writes provides causal
consistency [39]. The coordinator sends each cohort a Prepare.
Once all cohorts reply, it sets this transaction’s EVT to its
current logical time, commits the transaction, and sends each
cohort a Commit that includes EVT. Each participant deletes
this transaction’s sub-request from the IncomingWrites table
after it commits the transaction.
Multiversioning Framework and Applying Replicated
Writes. K2 keeps multiple versions of a key for a short time.
Multiversioning enables K2’s efﬁcient read-only transaction
algorithm. How a server applies a write depends on the current
version it has for a speciﬁed key and if it is storing the data.
When applying a write a server compares its version number
with the version number of its most recent write to the same
key. The version numbers are assigned by the datacenters
that accept the writes based on Lamport timestamps and are
consistent with the causal ordering of writes. Thus, a server
should only make the write visible to local reads if its version
number is greater than its most recent write. For non-replica
servers, this results in them either applying the write if it is
newer than the current value or discarding it entirely if it is not.
This procedure would not be safe for replica servers, however,
because the write might be needed to serve a remote read.
Replica servers thus apply the write in all cases, store it in
the multiversioning framework, and make it available only to
remote reads if it is older than the current value.
Garbage Collection (GC). K2 keeps a version around if it
is not older than 5 s, or this version or any of its earlier
versions has been accessed by the ﬁrst round of a read-only
transaction within the past 5 s, the conﬁgurable transaction
timeout. K2 performs garbage collection lazily whenever a
new version of a key is inserted and then removes any old
versions that do not satisfy either of the two conditions. GC is
a common component in multiversioning data stores to keep
memory and storage footprints low [38], [39], [51]. K2’s GC
is similar to Eiger’s [39] with the addition of keeping around
all versions not older than 5 s to enable our cache-aware read-
only transaction algorithm.
B. Rationale and Key Insights
level
the lowest
K2’s replication design differs signiﬁcantly from past work
and is what ensures at most one round of non-blocking cross-
datacenter requests. At
is K2’s metadata
replication design that ensures at most one round of cross-
datacenter requests. Above that K2 layers a constrained repli-
cation topology and a write-only transaction algorithm that
together ensure cross-datacenter requests do not block.
Metadata Replication. Partial replication of the data gives K2
most of the storage capacity beneﬁt of a partially-replicated
storage system, while full replication of the metadata enables
K2 to achieve at most one round of cross-datacenter requests.
The key insight is that metadata is all that is necessary to
determine what data a client can consistently read. K2 fully
replicates metadata, consistently updates metadata in each
datacenter, and then runs its read-only transaction algorithm
on that consistent metadata in the local datacenter to determine
consistent data versions. Then, only a single round of cross-
datacenter read requests is required if the consistent versions
are not stored locally. K2 can thus avoid multiple unnecessary
rounds of cross-datacenter requests to ﬁgure out consistent
data values to read.
Decoupling data and metadata replication, however, intro-
duces a new challenge that can lead to blocking. The metadata
replication in a non-replica datacenter can race ahead of data
replication in replica datacenter. Then, when the non-replica
datacenter requests a speciﬁc value from the replica datacenter
its request will need to block until that value arrives. K2
overcomes this challenge to ensure cross-datacenter requests
do not block with its constrained replication topology and
write-only transaction algorithm.
Constrained Replication Topology. K2’s constrained repli-
cation carefully orders how data and metadata are replicated
to replica and non-replica datacenters to ensure a datacenter
always knows where to read a value without blocking. This
ordering provides an important invariant: once a non-replica
datacenter learns about an update,
the value must be available
from each of the replica datacenters.
This invariant is sufﬁcient to ensure cross-datacenter re-
quests do not block for writes to individual keys. It, however,
breaks existing algorithms for write transactions that atomi-
cally update multiple keys. These existing algorithms include
general
transaction algorithms like two-phase locking and
optimistic concurrency control as well as specialized write-
only transaction algorithms like Eiger’s [39]. The existing
algorithms break because they include two-phase commit,
which waits for all participants in a transaction to prepare
successfully before any commit. For example, consider a write
transaction that updates keys A and B that are replicated in
disjoint datacenters. Using the invariant, the replica datacenters
for key A will not be able to prepare non-replica key B until
they know it has committed in its replica datacenters and thus