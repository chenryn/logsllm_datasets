literature in these areas.
incident detection is most often formulated as a time series
anomaly detection problem [28], [29], [30]. In the following,
IV. INCIDENTDETECTION
we focus on the AIOps setting and categorize it based on
Incident detection employs a variety of anomaly detec- several key criteria: (i) learning paradigm, (ii) dimensionality,
tion techniques. Anomaly detection is to detect abnormali- (iii) system, and (iv) streaming updates. We further summa-
ties, outliers or generally events that not normal. In AIOps rize a list of time series anomaly detection methods with a
context, anomaly detection is widely adopted in detecting comparison over these criteria in Table IV.
any types of abnormal system behaviors. To detect such
anomalies, the detectors need to utilize different telemetry Learning Setting
data, such as metrics, logs, traces. Thus, anomaly detection a) Label Accessibility: One natural way to formulate
can be further broken down to handling one or more specific the anomaly detection problem, is as the supervised binary
telemetry data sources, including metric anomaly detection, classification problem, to classify whether a given obser-
log anomaly detection, trace anomaly detection. Moreover, vation is an anomaly or not [31], [32]. Formulating it as
multi-modal anomaly detection techniques can be employed such has the benefit of being able to apply any supervised
if multiple telemetry data sources are involved in the detec- learning method, which has been intensely studied in the
tion process. In recent years, deep learning based anomaly past decades [33]. However, due to the difficulty in obtaining
detection techniques [9] are also widely discussed and can labelled data for metrics incident detection [34] and labels of
be utilized for anomaly detection in AIOps. Another way anomalies are prone to error [35], unsupervised approaches,
to distinguish anomaly detection techniques is depending on which do not require labels to build anomaly detectors, are
differentapplicationusecases,suchasdetectingservicehealth generally preferred and more widespread. Particularly, unsu-
issues, detecting networking issues, detecting security issues, pervised anomaly detection methods can be roughly catego-
fraud transactions, etc. Usually these variety of techniques rized into density-based methods, clustering-based methods,
are derived from same set of base detection algorithms and and reconstruction-based methods [28], [29], [30]. Density-
localized to handle specific tasks. From technical perspective, based methods compute local density and local connectivity
detecting anomalies from different telemetry data sources are for outlier decision. Clustering-based methods formulate the
better aligned with the AI technology definitions, such as, anomalyscoreasthedistancetoclustercenter.Reconstruction-
metricareusuallytime-series,logsaretext/naturallanguage, based methods explicitly model the generative process of the
7
data and measure the anomaly score with the reconstruction works which propose a system to handle the infrastructure
error.Whilemethodsinmetricanomalydetectionaregenerally are highlighted here. EGADS [41] is a system by Yahoo!,
unsupervised, there are cases where there is some access to scaling up to millions of data points per second, and focuses
labels.Insuchsituations,semi-supervised,domainadaptation, on optimizing real-time processing. It comprises a batch time
and active learning paradigms come into play. The semi- seriesmodellingmodule,anonlineanomalydetectionmodule,
supervised paradigm [36], [37], [38] enables unsupervised and an alerting module. It leverages a variety of unsupervised
models to leverage information from sparsely available posi- methodsforanomalydetection,andanoptionalactivelearning
tive labels [39]. Domain adaptation [40] relies on a labelled component for filtering alerts. [52] is a system by Microsoft,
source dataset, while the target dataset is unlabeled, with the which includes three major components, a data ingestion,
goal of transferring a model trained on the source dataset, to experimentation, and online compute platform. They propose
perform anomaly detection on the target. an efficient deep learning anomaly detector to achieve high
b) Streaming Update: Since metrics are collected in accuracy and high efficiency at the same time. [32] is a
large volume every minute, the model is used online to detect system by Alibaba group, comprising data ingestion, offline
anomalies.Itisverycommonthattemporalpatternsofmetrics training,onlineservice,andvisualizationandalarmsmodules.
changeovertime.Theabilitytoperformtimelymodelupdates They propose a robust anomaly detector by using time series
when receiving new incoming data is an important criteria. decomposition, and thus can easily handle time series with
On the one hand, conventional models can handle the data different characteristics, such as different seasonal length,
stream via retraining the whole model periodically [31], [41], different types of trends, etc. [38] is a system by Tencent,
[32], [38]. However, this strategy could be computationally comprising of a offline model training component and online
expensive, and bring extra non-trivial questions, such as, how serving component, which employs active learning to update
often should this retraining be performed. On the other hand, the online model via a small number of uncertain samples.
some methods [42], [43] have efficient updating mechanisms
inbuilt, and are naturally able to adapt to these new incoming Challenges
datastreams.Itcanalsosupportactivelearningparadigm[41], Lack of labels The main challenge of metric anomaly
whichallows modelsto interactivelyquery usersfor labelson detectionisthelackofgroundtruthanomalylabels[53],[44].
data points for which it is uncertain about, and subsequently Due to the open-ended nature and complexity of incidents in
update the model with the new labels. server architectures, it is difficult to define what an anomaly
c) Dimensionality: Eachmetricofmonitoringdataforms is. Thus, building labelled datasets is an extremely labor and
a univariate time series, and thus a service usually contains resource intensive exercise, one which requires the effort of
multiple metrics, each of which describes a different part domain experts to identify anomalies from time series data.
or attribute of a complex entity, constituting a multivariate Furthermore, manual labelling could lead to labelling errors
time series. The conventional solution is to build univariate as there is no unified and formal definition of an anomaly,
time series anomaly detection for each metric. However, for leading to subjective judgements on ground truth labels [35].
a complex system, it ignores the intrinsic interactions among Real-time inference A typical cloud infrastructure could
each metric and cannot well represent the system’s overall collectmillionsofdatapointsinasecond,requiringnearreal-
status. Naively combining the anomaly detection results of time inference to detect anomalies. Metric anomaly detection
each univariate time series performs poorly for multivariate systemsneedtobescalableandefficient[54],[53],optionally
anomaly detection method [44], since it cannot model the supporting model retraining, leading to immense compute,
inter-dependencies among metrics for a service. memory,andI/Oloads.Theincreasingcomplexityofanomaly
Model A wide range of machine learning models can be detection models with the rising popularity of deep learning
used for time series anomaly detection, broadly classified methods [55] add a further strain on these systems due to the
as deep learning models, tree-based models, and statistical additionalcomputationalcosttheselargermodelsbringabout.
models.Deeplearningmodels[45],[36],[46],[47],[38],[48], Non-stationarity of metric streams The temporal patterns
[49],[50]leveragethesuccessandpowerdeepneuralnetworks of metric data streams typically change over time as they are
tolearnrepresentationsofthetimeseriesdata.Theserepresen- generated from non-stationary environments [56]. The evo-
tations of time series data contain rich semantic information lution of these patterns is often caused by exogenous factors
of the underlying metric, and can be used as a reconstruction- whicharenotobservable.Onesuchexampleisthatthegrowth
based, unsupervised method. Tree-based methods leverage a in the popularity of a service would cause customer metrics
tree structure as a density-based, unsupervised method [42]. (e.g. request count) to drift upwards over time. Ignoring these
Statistical models [51] rely on classical statistical tests, which factors would cause a deterioration in the anomaly detector’s
are considered a reconstruction-based method. performance.Onesolutionistocontinuouslyupdatethemodel
Industrial Practices Building a system which can handle with the recent data [57], but this strategy requires carefully
the large amounts of metric data generated in real cloud IT balancingofthecostandmodelrobustnesswithrespecttothe
operations is often an issue. This is because the metric data updating frequency.
in real-world scenarios is quite diverse and the definition of Public benchmarks While there exists benchmarks for
anomalymayvaryindifferentscenarios.Moreover,almostall general anomaly detection methods and time series anomaly
timeseriesanomalydetectionsystemsrequiretohandlealarge detectionmethods[33],[58],thereisstillalackofbenchmark-
amount of metrics in parallel with low-latency [32]. Thus, ing for metric incident detection in AIOps domain. Given the
8
wide and diverse nature of time series data, they often exhibit text messages following their own unstructured or semi-
a mixture of different types of anomaly depends on specific structured or structured format. Throughout various kinds of
domain,makingitchallengingtounderstandtheprosandcons IT Operations these logs have been widely used by relia-
of algorithms [58]. Furthermore, existing datasets have been bility and performance engineers as well as core developers
criticised to be trivial and mislabelled [59]. in order to understand the system’s internal status and to
facilitate monitoring, administering, and troubleshooting [15],
Future Trends [16], [17], [18],[19], [20], [21], [22], [62].More, specifically,
Active learning/human-in-the-loop To address the prob- in the AIOps pipeline, one of the foremost tasks that log
lem of lacking of labels, a more intelligent way is to integrate analysis can cater to is log based Incident Detection. This
human knowledge and experience with minimum cost. As is typically achieved through anomaly detection over logs
special agents, humans have rich prior knowledge [60]. If which aims to detect the anomalous loglines or sequences
the incident detection framework can encourage the machine of loglines that indicate possible occurrence of an incident,
learning model to engage with learning operation expert wis- fromthehumungousamountsofsoftwareloggingdatadumps
dom and knowledge, it would help deal with scarce and noise generated by the system. Log based anomaly detection is
label issue. The use of active learning to update online model generally applied once an incident has been detected based
in [38] is a typical example to incorporate human effort in on monitoring of KPI metrics, as a more fine-grained incident
the annotation task. There are certainly large research scope detection or failure diagnosis step in order to detect which
for incorporating human effort in other data processing step, service or micro-service or which software module of the
like feature extraction. Moreover, the human effort can also system execution is behaving anomalously.
be integrated in the machine learning model training and
inference phase. Task Complexity
Streaming updates Due to the non-stationarity of metric
Diversity of Log Anomaly Patterns: There are very diverse
streams, keeping the anomaly detector updated is of utmost
kindsofincidentsinAIOpswhichcanresultindifferentkinds
importance. Alongside the increasingly complex models and
of anomaly patterns in the log data - either manifesting in the
need for cost-effectiveness, we will see a move towards
log template (i.e. the constant part of the log line) or the log
methods with the built-in capability of efficient streaming
parameters (i.e. the variable part of the log line containing
updates. With the great success of deep learning methods in
dynamic information). These are i) keywords - appearance
timeseriesanomalydetectiontasks[30].Onlinedeeplearning
of keywords in log lines bearing domain-specific semantics
is an increasingly popular topic [61], and we may start to see
of failure or incident or abnormality in the system (e.g. out
atransferenceoftechniquesintometricanomalydetectionfor
of memory or crash) ii) template count - where a sudden
time-series in the near future.
increase or decrease of log templates or log event types is
Intrinsic anomaly detection Current research works on
indicative of anomaly iii) template sequence - where some
time series anomaly detection do not distinguish the cause
significant deviation from the normal order of task execution
or the type of anomaly, which is critical for the subsequent
is indicative of anomaly iv) variable value - some variables
mitigationstepsinAIOps.Forexample,evenanomalyaresuc-
associatedwithsomelogtemplatesoreventscanhavephysical
cessfully detected, which is caused by extrinsic environment,
meaning (e.g. time cost) which could be extracted out and
the operator is unable to mitigate its negative effect. Intro-
aggregated into a structured time series on which standard
duced in [50], [48], intrinsic anomaly detection considers the
anomaly detection techniques can be applied. v) variable
functionaldependencystructurebetweenthemonitoredmetric,
distribution - for some categorical or numerical variables, a
and the environment. This setting considers changes in the
deviation from the standard distribution of the variable can be
environment, possibly leveraging information that may not be
indicativeofananomalyvi)timeinterval-someperformance
available in the regular (extrinsic) setting. For example, when
issuesmaynotbeexplicitlyobservedintheloglinethemselves
scaling up/down the resources serving an application (perhaps
but in the time interval between specific log events.
due to autoscaling rules), we will observe a drop/increase in
CPU metric. While this may be considered as an anomaly Need for AI: Given the humongous nature of the logs,
in the extrinsic setting, it is in fact not an incident and it is often infeasible for even domain experts to manually
accordingly, is not an anomaly in the intrinsic setting. go through the logs to detect the anomalous loglines. Addi-
tionally, as described above, depending on the nature of the
incident there can be diverse types of anomaly patterns in
B. Logs based Incident Detection
the logs, which can manifest as anomalous key words (like
”errors” or ”exception”) in the log templates or the volume of
Problem Definition specificeventlogsordistributionoverlogvariablesorthetime
Software and system logging data is one of the most interval between two log specific event logs. However, even
popular ways of recording and tracking runtime information for a domain expert it is not possible to come up with rules
about all ongoing processes within a system, to any arbitrary to detect these anomalous patterns, and even when they can,
level of granularity. Overall, a large distributed system can they would likely not be robust to diverse incident types and
have massive volume of heterogenous logs dumped by its changing nature of log lines as the software functionalities
different services or microservices, each having time-stamped change over time. Hence, this makes a compelling case for
9
employing data-driven models and machine intelligence to Transformer based models which use self-supervised Masked
mine and analyze this complex data-source to serve the end Language Modeling to learn log parsing vii) UniParser [77] -
goals of incident detection. an unified parser for heterogenous log data with a learnable
similaritymoduletogeneralizetodiverselogsacrossdifferent
Log Analysis Workflow for Incident Detection systems. There are yet another class of log analysis methods
In order to handle the complex nature of the data, typically [78], [79] which aim at parsing free techniques, in order to
a series of steps need to be followed to meaningfully analyze avoid the computational overhead of parsing and the errors
logs to detect incidents. Starting with the raw log data or data cascadingfromerroneousparses,especiallyduetothelackof