title:Improving SSD reliability with RAID via Elastic Striping and Anywhere
Parity
author:Jaeho Kim and
Jongmin Lee and
Jongmoo Choi and
Donghee Lee and
Sam H. Noh
Improving SSD Reliability with RAID
via Elastic Striping and Anywhere Parity
Jaeho Kim∗, Jongmin Lee∗, Jongmoo Choi†, Donghee Lee∗ and Sam H. Noh‡
∗University of Seoul, {kjhnet10, jmlee, dhl express}@uos.ac.kr
†Dankook University, PI:EMAIL
‡Hongik University, http://next.hongik.ac.kr
Abstract—While the move from SLC to MLC/TLC ﬂash
memory technology is increasing SSD capacity at lower cost, it
is being done at the cost of sacriﬁcing reliability. An approach
to remedy this loss is to employ the RAID architecture with
the chips that comprise SSDs. However, using the traditional
RAID approach may result in negative effects as the total number
of writes may increase due to the parity updates, consequently
leading to increased P/E cycles and higher bit error rates. Using
a technique that we call Elastic Striping and Anywhere Parity
(eSAP), we develop eSAP-RAID, a RAID scheme that signiﬁcantly
reduces parity writes while providing reliability better than
RAID-5. We derive performance and lifetime models of SSDs
employing RAID-5 and eSAP-RAID that show the beneﬁts of
eSAP-RAID. We also implement these schemes in SSDs using
DiskSim with SSD Extension and validate the models using
realistic workloads. Our results show that eSAP-RAID improves
reliability considerably, while limiting its wear. Speciﬁcally, the
expected lifetime of eSAP-RAID employing SSDs may be as long
as current ECC based SSDs, while its reliability level can be
maintained at the level of the early stages of current ECC based
SSDs throughout its entire lifetime.
Keywords—Flash memory, Reliability, RAID, SSD
I.
INTRODUCTION
New technology such as MLC (Multiple-Level Cell) ﬂash
memory that comprise today’s Solid State Drives (SSDs)
comes with higher density at a lower price than previous SLC
(Single-Level Cell) devices. However, with it, program/erase
(P/E) cycles drop from 100,000 to 10,000. This trade-off is
aggrevated even further for TLC (Triple-Level Cell) devices
that are being used in recent SSDs [3], [4], with their cycle
limit dropping to the few thousands range [12]. Along with
the P/E cycle limitation, another important characteristic of
these ﬂash memory devices is that their bit error rates increase
exponentially as P/E cycles increase [11], [19].
Traditional methods to cope with these bit errors is to
employ powerful Error Correction Code (ECC) that can correct
multiple bit errors and store them in the Out-Of-Band (OOB)
area of each page in ﬂash memory [8], [9], [26]. However,
as NAND technologies for smaller cells such as 3xnm that
increase the error rates of ﬂash memory become prevalent,
even larger ECCs are needed to cope with the errors, requiring
a signiﬁcant portion of the OOB area to be consumed for
ECCs. Moreover, ECCs cannot cope with bursty errors nor
page-, block-, or chip-level errors [14], [17], [18], [6]. Thus,
RAID architecture constructed with chips internal to the SSDs
has been suggested as a supplement to the typical ECCs to
alleviate the current ECC limitations.
Current SSDs, in their basic form, already employ the
RAID-0 architecture as they stripe data over multiple chips.
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
(a) UPER
(b) P/E cycles
Fig. 1: (a) Uncorrectable Page Error Rate (UPER) and (b) expected P/E
cycles of conventional SSD (denoted as ECC) and SSD employing RAID-5
However, the only means of error recovery for RAID-0 is using
ECC. By transforming RAID-0 to RAID-5 within SSDs, we
can supplement ECCs with parities with the hope of enhancing
error recovery. RAID-5, in the traditional sense, comprises a
ﬁxed number of sectors to form a logical stripe that includes a
parity sector, and these parity sectors are evenly distributed
over multiple chips. Though supporting RAID-5 seems to
increase reliability, in fact, it is a double-edged sword as it
also increases the total number of page writes as these extra
parity information also take up storage space.
Note the two graphs in Fig. 1. Fig. 1(a) shows the Uncor-
rectable Page Error Rate (UPER) as more and more bytes are
written to a conventional SSD that uses BCH code with 4 bit
redundancy per 512B for ECC and one that is enforced with
RAID-5, along with the HDD error rate, which is due to Gray
and van Ingen [10]. (UPER of Fig. 1(a) and the expected P/E
cycle values of Fig. 1(b) are from the reliability and lifetime
analyses in Section VI. How the values were obtained will
be discussed in detail in Section VI.) We observe that with
RAID-5 added, UPER remains under that of HDD, while for
the ECC case, after roughly 75TBs of data are written, UPER
exceeds that of HDD. Hence, RAID-5 helps reduce UPER
below that of an HDD. In contrast, observe in Fig. 1(b) that
the expected P/E cycles increases at a faster rate for RAID-
5 than ECC and that the P/E cycle for RAID-5 crosses the
10,000 MLC (Multi-Level Cell) ﬂash memory P/E cycle limit
when roughly 130TBs of data have been written. ECC, on the
other hand, remains below the P/E limit even beyond 200TBs
of written data. This increase in the number of P/E cycles for
RAID-5 leads to faster wear. Hence, whether RAID is truly a
scheme worth employing has to be accurately analyzed. To this
end, as a key contribution of this paper, we present a detailed
analysis of the effects of the RAID-5 scheme on SSDs using
an analytical model that we derive.
Another contribution of this paper is the reliability analyses
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
1.0E-391.0E-351.0E-311.0E-271.0E-231.0E-191.0E-151.0E-111.0E-071.0E-03UPER(Log scale) Written bytes ECCRAID-5Error rate of HDD  0.0E+002.0E+034.0E+036.0E+038.0E+031.0E+041.2E+041.4E+041.6E+04Expected P/E cycles Written bytes ECCRAID-5P/E cycle limit(MLC)  Fig. 2: Internal structure of an SSD
of RAID-5 and eSAP1, a novel RAID scheme that was
previously proposed [15]. The two key features of eSAP
are Elastic Striping (ES) and Anywhere Parity (AP). Elastic
Striping dynamically constructs a partial or a full stripe with
newly written data so as to reduce parity update overhead.
Partial stripe is possible due to the second feature, that is,
Anywhere Parity, where parity may be placed anywhere in the
stripe. Also, it allows the stripe size to be adjusted so that
more parities can be generated as the bit error rate increases
with more wear. Hence, the name eSAP-RAID, that is, Elastic
Striping Anywhere Parity RAID (or simply, eSAP, hereafter).
Through analyses and extensive experiments using the
DiskSim with SSD Extension platform, we show that eSAP
provides reliability better than RAID-5 by signiﬁcantly reduc-
ing the parity overhead and, eventually, reduced P/E cycles. We
also use the analytic models to project long-term reliability and
lifetime of SSDs with the RAID-5 and eSAP schemes.
The rest of the paper is organized as follows. In the next
section, we discuss previous work related to RAID architecture
with an emphasis on RAID schemes employed in SSDs. In
Section III, we describe the operations of RAID-5 and eSAP-
RAID. Then, in Section IV, we derive the performance and
lifetime models of RAID-5 and eSAP-RAID. We compare the
RAID schemes and validate the models in Sections V and VI.
Finally, we conclude with Section VII.
II. RELATED WORK
In this section, we ﬁrst discuss the basics of ﬂash memory
and SSDs. Then, we review studies conducted on the reliability
aspect of ﬂash memory. In particular, we review studies that
show that new technology trends are making ﬂash devices
more and more unreliable. Finally, we review studies that
suggest schemes that enhance reliability.
A. Flash and SSD Basics
NAND ﬂash memory form the basic building block of
today’s SSDs. NAND ﬂash memory consists of multiple blocks
and each block has multiple pages. The most basic operations
on ﬂash memory are the read and write operations, and these
are done in page units. Typically, a write to a page takes a
few hundred microseconds, while a page read is generally an
order of magnitude faster taking a few tens of microseconds.
A unique characteristic of ﬂash memory is that data cannot
be overwritten on a used page. In order to overwrite a page,
the block containing the page has to be erased ﬁrst. This erase
operation is another order of magnitude slower than a page
write operation. Furthermore, the number of erasures after
1Note that we have renamed the scheme from that used in [15].
a write, that is, the Program2/Erasure (P/E) cycle is limited
depending on the technology.
Due to the characteristic that a page cannot be overwritten,
ﬂash memory cannot be used as a direct replacement of Hard
Disk Drives (HDD). To hide this characteristic and provide an
interface of magnetic disks in commodity SSDs, a software
module called a Flash Translation Layer (FTL), whose main
function is to relocate new data to unused pages, is employed in
the SSD controller [2], [5]. Fig. 2 shows the basic components
of an SSD. The SSD Controller contains a processor that
executes the control logic of the FTL. One key function of
an FTL is to maintain a map translating the logical sector
numbers (transferred from the host) to the actual physical
locations (the ﬂash chip, the block number in the chip, and
the page number within the block) on ﬂash memory. When
data is modiﬁed, the new data is appended to unused pages
of blocks and the map is updated to reﬂect the new locations
of the data. Used pages that have been replaced with new
data become invalid, and these invalid pages are reclaimed via
garbage collection. This garbage collection process is similar
to what is done for segment cleaning in LFS [23] except that,
instead of segments, blocks are cleaned. (We will use garbage
collection and cleaning interchangeably in this paper.)
For the cleaning process, ﬁrst, a block to be erased is
selected. Before the erase operation is performed on this block,
valid pages in this block must ﬁrst be moved to unused pages
in other blocks. This moving of valid pages is a source of
overhead called write ampliﬁcation. Another important factor
that
inﬂuences garbage collection is the Over-Provisioned
Space (OPS). This space is the extra ﬂash space capacity
that
is always left clean to help make garbage collection
efﬁcient. This OPS is generally not counted into the capacity
of commercial products.
B. Flash Reliability
New ﬂash memory technologies that store multiple bits per
cell increase density, while sacriﬁcing reliability. In particular,
the number of possible P/E cycles that is typically in the
100,000 range for SLC (Single-Level Cell) ﬂash memory
drops to the 10,000 range for MLC (Multi-Level Cell) ﬂash
memory, and then to the thousands range for TLC (Triple-
Level Cell) ﬂash memory [12]. This decrease in possible P/E
cycles exacerbates the reliability issue because the wear down
of P/E cycles strongly affect the Bit Error Rate (BER) [11],
[12], [19].
In particular, Mielke et al. measure the raw bit error rates
of ﬂash memory chips from various vendors and calculate the
uncorrectable bit error rates observed after applying ECC [19].
Later, Sun et al. measure the raw bit error rates of 5xnm,
4xnm, and 3xnm MLC ﬂash memory chips and show that
bit error rates rapidly increase as ﬂash memory cells become
smaller [24]. They also derive an exponential growth model of
BER against P/E cycles. In these studies, it is shown that ECC
signiﬁcantly lowers the bit error rate. However, ECC has its
limitations. For example, the detection and correction level of
the ECC is limited and determined by the size of ECC stored in
the OOB area, which is small in size and must be shared with
2In ﬂash the term program refers to a write operation. We use the two terms
interchangably.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
Host Interface Logic                 SSD Controller          Flash chip & Channel controller Processor DRAM  controller DRAM Flash  chip Flash  chip Flash  chip Flash  chip Flash  chip Flash  chip SRAM             Host Interface (SATA, PCI) other information like the logical block address as a safeguared
for sudden power loss. Therefore, bursty errors beyond the
ECC capacity cannot be corrected. In similar context, ECC
does not help in cases of page-, block-, and chip-level errors.
To cope with these kinds of bursty errors, SSDs with various
RAID architectures within have been proposed [14], [17], [18].
C. RAID within SSDs
Many studies on RAID storage systems composed of
magnetic disks have been conducted (see [7] for annotated
bibliography). Though our study starts off from the RAID
concept, it is different from traditional RAID studies in two
aspects. First, the medium of storage is ﬂash memory, not
magnetic disks. Second, RAID is employed on the components
that comprise the storage device, that is, SSDs; that is, we
are not composing a RAID system made up of SSD devices.
Hence, in the following, we focus on RAID schemes employed
within SSDs.
Lee et al. apply RAID architecture to their Flash-aware
Redundancy Array (FRA) [18]. To reduce parity update cost
of RAID-5, FRA retains the parity blocks in buffer memory
postponing their writes until the buffer memory is full or no
requests arrive for a speciﬁed amount of time. Im and Shin
propose Partial Parity Cache (PPC), a method that generates
partial parity for partial stripes [14]. The limitation of PPC,
however, is that it requires non-volatile RAM (NVRAM) to
keep these partial parities.
Based on the observation that BER increases as the number
of P/E cycles increase, Lee et al. suggest dynamically adjusting
the stripe size to maintain similar bit error rates as ﬂash
memory ages [17]. Speciﬁcally, at early usage of SSDs, they
use a large stripe size as BER is low. However, as the SSD
ages and hence, BER increases, the stripe size is reduced so
that more parity blocks are recorded. This scheme also uses
NVRAM to keep parities in memory to reduce the parity
update overhead. Through simulation based experiments, they
show how the reliability of SSDs employing RAID schemes
fare against P/E cycles.
III.
ESAP-RAID FOR SSDS
To understand the analytic models that we derive in the
next section, we need to ﬁrst understand the operations of
the RAID schemes. In this section, we discuss in detail the
implementation of RAID, ﬁrst RAID-5 and then eSAP-RAID,
using ﬂash memory chips that comprise SSDs.
A. Conventional RAID-5 in SSDs
Fig. 3(a) shows an example of how the internals of an
SSD composed of ﬁve chips that supports the conventional
notion of RAID-5 would look like. User data D0∼D3 and
parity P0 comprise stripe 0 and user data D4∼D7 and parity P1
comprise stripe 1. The Stripe map table maintains information
about each stripe. The number pairs in this table represent the
Physical Block Number (PBN) and the Physical Page Number
(PPN) in the ﬂash memory chip.
Now let us assume that D1∼D4 are modiﬁed. To retain
the RAID-5 conﬁguration, conventional RAID-5 requires that
the updated data be written to the location where the old data
Fig. 3: (a) Internal structure of SSDs employing conventional RAID-5, with
initial data allocation and (b) how data and parity would be allocated as
updates occur.
Fig. 4: Write sequence for eSAP-RAID as (a) data pages D1-D4 are
modiﬁed and as (b) new partial stripe data D8 and D9 are written.
resides. In traditional disk based RAID, this is a simple task
as updated data will simply overwrite the old data. However,
since overwrites are not possible in ﬂash memory, the updated
data is written to a new location within the same chip that
the old data resides in as shown in Fig. 3(b). Then,
the
parity needs to be updated through either read-modify-write or
reconstruct-write. Speciﬁcally, in this example, stripe 0 would
use reconstruct-write and stripe 1 would use read-modify-write
as they minimize the number of read and write operations.
Finally, the Stripe map table is updated to reﬂect the changes
in the stripes.
There are limitations to this approach. First, whether read-
modify-write or reconstruct-write is employed, reading of
existing data must precede new parity calculations. This is
also true for traditional disk-based RAID-5 systems. Second,
once a data and parity page is allocated to a particular chip,
this relation is ﬁxed. Hence, if a particular page is written
with higher frequency, then that particular chip will be written
to more frequently. Also, the chip in which the parity page
resides is more prone to wear out as it must be written to
more frequently. These ﬁxed relations eventually lead to higher
cleaning costs and decreased lifetime of the SSD. Third, when
writes are not updates to existing data but writes of totally new
data, the data cannot be written until the stripe becomes full,
leaving open a window of vulnerability. For example, if new
data D8 and D9 arrive, in our example above, a parity page
cannot be calculated for these pages that form a partial stripe,
and thus these pages cannot be written until a full stripe is
formed, that is, another two new pages arrive.
B. eSAP-RAID
1) Architecture of eSAP-RAID: The key feature of the
scheme that we propose, Elastic Striping and Anywhere Parity
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
Stripe 0 Stripe 1 0:0 0:0 0:0 0:0 0:0 0:1 0:1 0:1 0:1 0:1 Stripe map table Stripe 0 Stripe 1 0:0 0:2 0:2 0:2 0:2 0:2 0:1 0:1 0:3 0:1 Stripe map table D4 D0 PPN Chip 1 0 1 2 3 D5 D1 B0 (a) Host interface  SSD & RAID Controller   PPN Chip 0 0 1 2 3 Write buffer Parity generator PPN Chip 2 0 1 2 3 D6 D2 PPN Chip 3 0 1 2 3 P1 D3 PPN Chip 4 0 1 2 3 D7 P0 FCC FCC FCC FCC FCC : Valid page : Invalid page B0 PPN Chip 0 0 1 2 3 D4 D0 D4' PPN Chip 1 0 1 2 3 D1 D5 D1’ PPN Chip 2 0 1 2 3 D2 D6 D2’ PPN Chip 3 0 1 2 3 P1 D3 D3’ P1’ PPN Chip 4 0 1 2 3 P0 D7 P0’ PBN: Physical Block Number PPN: Physical Page Number FCC: Flash-memory Channel Controller (b) ... ... ... ... ... ... ... ... ... ... PBN PBN PSP: Partial Stripe Parity : Valid Page : Invalid Page (a) (b) PPN Chip 1 0 1 2 3 D5 D1 D1 D2’ PPN Chip 2 0 1 2 3 D6 D2 D2 D3’ PPN Chip 3 0 1 2 3 D7 D3 D3 D4’ B0 PPN Chip 0 0 1 2 3 D4 D4 D0 D1’ PPN Chip 4 0 1 2 3 P1 P0 P2 PPN Chip 1 0 1 2 3 D5 D1 D1 D2’ D9 PPN Chip 2 0 1 2 3 D6 D2 D2 D3’ PSP PPN Chip 3 0 1 2 3 D7 D3 D3 D4’ B0 PPN Chip 0 0 1 2 3 D4 D4 D0 D1’ D8 PPN Chip 4 0 1 2 3 P1 P0 P2 PBN PBN ... ... ... ... ... ... ... ... ... ... LPN PPN D0 D1’ D2’ D3’ D4’ D5 D6 D7 0:0:0 0:0:2 1:0:2 2:0:2 3:0:2 1:0:1 2:0:1 3:0:1 L2P map table LPN PPN ... … D8 D9 … … 0:0:3 1:0:3 L2P map table (a) Before cleaning
(b) After cleaning
Fig. 5: SSD internals with eSAP-RAID (a) before cleaning and (b) after cleaning.
RAID (eSAP-RAID or simply eSAP), is that the stripe size
may vary (up to the full stripe size) and need not be ﬁxed.
That is, eSAP can construct a partial stripe as need be (such
as input of small sized data), hence we coin this feature Elastic
Striping. For this to be possible, the parity need not be placed
at a ﬁxed location, but may be put in any location of the
(partial) stripe, hence this feature is called Anywhere Parity.
Let us clarify these features using the same example given