different power levels, and calculate the aggregate PSD. Instead of
using the absolute value, we show in Fig. 10(a) the relative aggregate
PSD normalized with respect to the lowest value when only one
server is running. It can be seen that the aggregate PSD becomes
greater as we run more servers and/or increase their power usage,
which is consistent with our discussion in Section 4.3.3.
Multiple servers with different power supply units. We run
all of our 13 servers that have different configurations and power
supply units. Table 1 shows the server configuration. We show
the PSD of the resulting voltage in Fig. 10(b). We observe three
distinct groups of PSD spikes, each corresponding to one type of
power supply unit. Based on our individual server experiments,
we identify that the PSD spikes around 64kHz are caused by 350W
power supply units. The PSD spikes around 66kHz and 70kHz are
both created by servers with the 495W power supply units. Despite
the same capacity, the two types of 495W power supply units are
from different generations (Appendix E) and hence have distinct
switching frequencies. In addition, each group consists of several
spikes because different power supply units of the same model and
generation may still have slightly different switching frequencies.
In summary, our experiments have found and validated that:
(1) the power supply unit designs of today’s servers create high-
frequency voltage ripples in the data center power line network;
and (2) these ripples carry information about servers’ power usage
at runtime. Note that, like in today’s mainstream systems [24], the
PFC circuits in our servers operate under the continuous conduction
mode which, as shown in Fig. 10(b), causes line voltage ripples with
narrow spikes in the frequency domain. For certain high-power
servers (close to 1kW or even higher), two PFC circuits may be
connected in tandem (a.k.a. interleaving PFC) to meet the demand
of large current flows while reducing voltage ripples. Compared to a
single-stage PFC, the two individual PFC circuits in an interleaving
PFC design operate with a certain time delay between each other
scanning lower/upper bounds Flb/Fub
Algorithm 1 Calculating Group-wise Aggregate PSD
1: Input: PSD data P(f ), frequency band F (e.g., 1kHz), frequency
2: Output: Group-wise aggregate PSD P1, P2,· · · , PM .
3: Find grid frequency Fo ← max45H z≤f ≤65H z P(f )
4: for f from Flb to Fub do
Cf ← P(f −Fo)+P(f +Fo)
5:
6: Keep Cf spikes and discard others (i.e., power line noise)
7: Generate bands B[i] = [f − F2 , f + F2 ] for each Cf spike
8: Merge B with overlapping frequency bands
9: Number of groups M ← number of separate bands in B
10: for each item B[i] ∈ B do
11:
12: Return group-wise aggregate PSD Pi for i = 1, 2,· · · , M.
Pi ←˝
f ∈B[i] P(f )
2
and result in line voltage ripples with shorter but wider spikes
in the frequency domain [24, 44]. Albeit over a wider range, the
high-frequency components in line voltage ripples resulting from
PWM switching still become more prominent as a server consumes
more power [44]. Therefore, our finding holds broadly regardless
of a single-stage or interleaving PFC design.
4.5 Tracking Aggregate Power Usage
Now, we study how the attacker can track the tenants’ aggregate
power usage based on its measured voltage signal.
4.5.1 Calculating group-wise aggregate PSD. In a multi-tenant data
center with servers from different manufacturers, we shall expect
to see several groups of PSD spikes in the voltage signal, each group
consisting of the PSD spikes from similar power supply units (and
likely from servers owned by the same tenant). Likewise, we can
also divide servers into different groups according to their PFC
switching frequencies.
In general, within each group, the aggregate PSD increases when
the servers in that group consume more power (Fig. 10(a)). Nonethe-
less, even given the same aggregate PSD, servers in one group may
have very different power usage than those in the other group (Fig. 22
in Appendix C) because they have different power supply units and
are also likely to have different configurations (e.g., different CPUs).
Thus, the total PSD over all the groups may not be a good indica-
tor of the servers’ total power consumption; instead, we should
consider group-wise aggregate PSD.
We leverage the frequency domain segregation and use Algo-
rithm 1 to identify PSD groups. We use the insight from our exper-
iment that each server creates a pair of PSD spikes separated by
twice the nominal power line frequency (i.e., 60Hz in the U.S.) and
centered around its PFC switching frequency. Further, the spikes
are significantly greater than the power line background noise.
4.5.2 Tracking tenants’ aggregate power usage. To launch success-
ful power attacks, the attacker only needs to identify the moments
when the tenants’ aggregate power is sufficiently high. Thus, know-
ing the shape of the aggregate power usage is enough.
Given the group-wise aggregate PSD P1, P2,· · · , PM at runtime,
the attacker can track the total power usage of servers in each
LowMediumHighPower Level0.81.01.21.41.61.82.0Norm. Aggr. PSD1 server2 servers3 servers4 servers626466687072Frequency (kHz)0.00.51.01.52.0PSD (W/Hz)£10−6else if Sc = wait then
˜P ←˝M
˜Pm
m=1
if Sc = idle then
Sn ← idle
if ˜P < Pth then
else Sn ← wait, start Twait
if ˜P ≥ Pth then
Algorithm 2 Timing Power Attacks Using Voltage Side Channel
1: Input: attack threshold Pth, timer thresholds for Twait , Tattack,
and Thold
2: Initiation: current state Sc ← idle, next state Sn ← idle
Twait ← 0, Tattack ← 0, and Thold ← 0
3: loop at regular intervals (e.g., every 10 seconds)
Use Algorithm 1 to get the aggregate PSDs
4:
Use historical data to get normalized PSD, ˜P1, ˜P2,· · · , ˜PM
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
else Sn ← idle, stop and reset Twait
if Tattack is expired then
else Sn ← attack
if Thold is expired then
if ˜P ≥ Pth then
else Sn ← idle, stop and reset Thold
Sn ← attack, start Tattack, stop and reset Thold
Sn ← hold, start Thold, stop and reset Tattack
if Twait is expired then
else Sn ← wait
Sn ← attack, startTattack, stop and resetTwait
else if Sc = attack then
else
else Sn ← hold
Sc ← Sn
group (i.e., a high aggregate PSD means a high power in that group).
Nonetheless, the attacker does not know the corresponding abso-
lute server power given a certain aggregate PSD value. Intuitively,
however, if all or most group-wise aggregate PSDs are sufficiently
high, then it is likely that the tenants’ aggregate power usage is also
high (i.e., an attack opportunity). Thus, based on this intuition, we
first normalize each group-wise aggregate PSD (with respect to its
own maximum over a long window size, e.g., 24 hours) and denote
the normalized values by ˜P1, ˜P2,· · · , ˜PM . Then, we sum them up
m=1 Pm and use it as an approximate indicator of the tenants’
aggregate power usage.
˜P =˝M
4.6 Timing Power Attacks
To time power attacks based on the voltage side channel, we pro-
pose a threshold-based strategy based on the sum of normalized
group-wise aggregate PSDs ˜P. Specifically, we set four different
states — Idle, Wait, Attack, and Hold — and the attacker transitions
from one state to another by periodically (e.g., every 10 seconds)
comparing ˜P against a threshold Pth.
•Idle : This is the default state. If ˜P ≥ Pth is met, the attacker
moves to Wait and starts a Twait timer.
Figure 11: A prototype of edge multi-tenant data center.
•Wait : To avoid attacking during transient spikes of ˜P, the
attacker stays in Wait until Twait expires. Then, if ˜P ≥ Pth still
holds, the attacker moves to Attack and, otherwise, back to Idle.
•Attack : In this state, the attacker uses its maximum power
consumption for attacks. The attacker stays in Attack for Tattack
time, after which it starts a Thold timer and moves to Hold.
•Hold : To avoid suspiciously consecutive attacks, the attacker
stays in this state until Thold expires. Then, if ˜P ≥ Pth is still met,
it moves back to Attack and otherwise to Idle.
Finally, we present the formal algorithm description in Algo-
rithm 2.
5 EVALUATION
This section presents our evaluation results of exploiting the voltage
side channel for timing power attacks in a scaled-down multi-tenant
data center. We focus on how well the attacker can track tenants’ ag-
gregate power usage at runtime and how well it can time its power
attacks. Our experimental results demonstrate that, by launching
non-consecutive attacks no more than 10% of the time, the attacker
can successfully detect 64% of all attack opportunities (i.e., true
positive rate) with a precision of 50%.
5.1 Methodology
As shown in Fig. 11, we set up 13 Dell PowerEdge servers connected
to a single-phase 120V APC8632 rack PDU for our experiments.
This setup represents an edge colocation data center, an emerging
type of data center located in distributed locations (e.g., wireless
towers) [29].
The server configuration is shown in Table 1. The PDU is pow-
ered by a CyberPower UPS that is plugged into a power outlet in
our university server room. We use a Rigol 1074Z oscilloscope to
measure the voltage at a sampling rate of 200kHz. The oscilloscope
probe is connected to the PDU through a power cable with polar-
ized NEMA 1-15 plug. While we use an oscilloscope to collect the
voltage signal (as we cannot open the power supply unit for lab
safety), the attacker can place a small ADC circuit inside its power
supply unit to achieve the same purpose in practice.
1Oscilloscope1234562Network Switch3PowerEdge Servers4UPS5APC PDU6Voltage Sampling(a) Tenant #1 (63 ∼ 64 kHz)
(b) Tenant #2, (65.5 ∼ 66.5 kHz)
(c) Tenant #3, (69.5 ∼ 70.5 kHz)
(d) Impact of frequency band size (F )
Figure 12: Detection of power shape of different server groups.
Table 1: Server configuration of our experiments.
Tenant
CPU/Memory
#1
#2
#3
#4
Xeon/32GB
Dual Xeon/32GB
Xeon/32GB
Pentium/32GB
Power
Supply
Rating
350W
495W
495W
350W
PFC
Switching
Frequency
∼63kHz
∼66kHz
∼70kHz
∼63kHz
of
Servers
Number
4
2
4
3