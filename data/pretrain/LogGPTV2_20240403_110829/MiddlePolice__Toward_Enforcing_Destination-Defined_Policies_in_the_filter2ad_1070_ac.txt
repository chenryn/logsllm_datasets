detection period based on Equation (3) and F’s historical
loss rate LR. This design prevents attackers from hiding
their previous packet losses via on-oﬀ attacks. F’s WR is
updated based on the victim-selected policy (line 35), as
described below.
4.3.5 Bandwidth Allocation Policies
We list the following representative policies that may be
NaturalShare: for each sender, the mbox sets its WR for
the next period to the number of delivered packets from the
sender in the current period. The design rationale is that the
mbox allows a rate that the sender can sustainably transmit
without experiencing a large LLR.
chosen to implement in BandwidthAllocationPolicy.
PerSenderFairshare allows the victim to enforce per-sender
fair share at bottlenecks. Each mbox fairly allocates its es-
timated total downstream bandwidth to the senders that
reach the victim through the mbox. To this end, the mbox
maintains the total downstream bandwidth estimate N total
size ,
which it allocates equally among all senders.
To ensure global fairness among all senders, two mboxes
sharing the same bottleneck (i.e., the two paths connect-
ing the two mboxes with the victim both traverse the bot-
tleneck link) share their local observations. We design a
co-bottleneck detection mechanism using SLR correlation:
if two mboxes’ observed SLRs are correlated, they share a
bottleneck with high probability. In §8.3, we evaluate the
eﬀectiveness of this mechanism.
PerASFairshare is similar to PerSenderFairshare except that
the mbox fairly allocates N total
size on a per-AS basis. This pol-
icy mimics SIBRA [13], preventing bot-infested ASes from
taking bandwidth away from legitimate ASes.
PerASPerSenderFairshare is a hierarchical fairness regime:
the mbox ﬁrst allocates N total
size on a per-AS basis, and then
fairly assigns the share obtained by each AS among the
senders of the AS.
PremiumClientSupport provides premium service to pre-
mium clients, such as bandwidth reservation for upgraded
ASes. The victim pre-identiﬁes its premium clients to Mid-
dlePolice. PremiumClientSupport can be implemented to-
gether with the aforementioned allocation policies.
5. PACKET FILTERING
When the victim’s IP addresses are kept secret, attack-
ers cannot bypass MiddlePolice’s upstream mboxes to route
attack traﬃc directly to the victim. In this case, the down-
stream packet ﬁltering is unnecessary since MiddlePolice can
throttle attacks at the upstream mboxes. However, in case
of IP address exposure [39, 48], the victim needs to deploy
a packet ﬁlter to discard bypassing traﬃc. MiddlePolice
designs a ﬁltering mechanism that extends to commodity
routers the port-based ﬁltering of previous work [21, 22].
Unlike prior work, the ﬁltering can be deployed upstream
of the victim as a commercial service.
5.1 Filtering Primitives
Although the MAC-incorporated capability can prove that
a packet indeed traverses an mbox, it requires upgrades from
deployed commodity routers to perform MAC computation
to ﬁlter bypassing packets. Thus, we invent a mechanism
based on the existing ACL conﬁgurations on commodity
routers. Speciﬁcally, each mbox encapsulates its traversing
packets into UDP packets (similar techniques have been ap-
plied in VXLAN and [25]), and uses the UDP source and
destination ports (a total of 32 bits) to carry an authentica-
tor, which is a shared secret between the mbox and the ﬁlter-
ing point. As a result, a 500Gbps attack (the largest attack
viewed by Arbor Networks [41]) that uses random port num-
bers will be reduced to ∼100bps since the chance of a correct
guess is 2−32. The shared secret can be negotiated period-
ically based on a cryptographically secure pseudo-random
number generator. We do not rely on UDP source address
for ﬁltering to avoid source spooﬁng.
5.2 Packet Filtering Points
Deployed ﬁltering points should have suﬃcient bandwidth
so that the bypassing attack traﬃc cannot cause packet
losses prior to the ﬁltering. The ﬁltering mechanism should
be deployed at, or upstream of, each bottleneck link caused
by the DDoS attacks. For instance, for a victim with high-
bandwidth connectivity, if the bottleneck link is an internal
link inside the victim’s network, the victim can deploy the
ﬁlter at the inbound points of its network.
If the bottle-
neck link is the link connecting the victim with its ISP, the
victim can can work with its ISP, on commercially reason-
able terms, to deploy the ﬁlter deeper in the ISP’s network
such that the bypassing traﬃc cannot reach the victim’s net-
work. Working with the ISPs does not violate the deploy-
ment model in §2 as MiddlePolice never requires deployment
at unrelated ASes.
6. SOURCE VALIDATION
MiddlePolice punishes senders even after an oﬀending ﬂow
ends. Such persistence can be built on source authentica-
tion or any mechanism that maintains sender accountability
across ﬂows. As a proof-of-concept, we design a source val-
idation mechanism that is speciﬁc to HTTP/HTTPS traf-
ﬁc. The mechanism ensures that a sender is on-path to its
claimed source IP address. This source veriﬁer is completely
transparent to clients.
Our key insight is that the HTTP Host header is in the
ﬁrst few packets of each connection. As a result, the mbox
monitors a TCP connection and reads the Host header. If
the Host header reﬂects a generic (not sender-speciﬁc) host-
name (e.g., victim.com), the mbox intercepts this ﬂow, and
redirects the connection (HTTP 302) to a Host containing a
token cryptographically generated from the sender’s claimed
source address, e.g., T .victim.com, where T is the token. If
the sender is on-path, it will receive the redirection, and its
further connection will use the sender-speciﬁc hostname in
the Host header. When an mbox receives a request with a
sender-speciﬁc Host, it veriﬁes that the Host is proper for the
claimed IP source address (if not, the mbox initiates a new
redirection), and forwards the request to the victim. Thus,
by performing source veriﬁcation entirely at the mbox, pack-
ets from spoofed sources cannot consume any downstream
bandwidth from the mbox to the victim.
If the cloud provider hosting the mbox is trusted (for in-
stance, large CDNs have CAs trusted by all major browsers),
the victim can share its key such that HTTPS traﬃc can be
handled in the same way as HTTP traﬃc. For untrusted
providers [22, 32], the mbox relays the encrypted connec-
tion to the victim, which performs Host-header-related op-
erations. The victim terminates unveriﬁed senders without
Figure 2. The software stack of the mbox and CHM.
returning capabilities to the mbox, so the additional traﬃc
from the unveriﬁed senders is best-eﬀort under Algorithm 1.
In this case, packets from spoofed sources consume limited
downstream bandwidth but do not rely on the trustworthi-
ness of the cloud provider. We acknowledge that performing
source validation at the victim is subject to the DoC at-
tack [11] in which attackers ﬂood the victim with new con-
nections to slow down the connection-setup for legitimate
clients. This attack is mitigated if the source validation is
completely handled by the mbox.
IMPLEMENTATION
7.
We have a full implementation of MiddlePolice.
7.1 The Implementation of CHM and mboxes
The mboxes and the CHM at the victim are implemented
based on the NetFilter Linux Kernel Module, which com-
bined have ∼1500 lines of C code (excluding the capability
generation code). The software stack of our implementation
is illustrated in Figure 2.
All inbound traﬃc from clients to an mbox is subject to the
traﬃc policing whereas only accepted packets go through the
capability-related processing. Packet dropping due to traﬃc
policing triggers iTable updates. For each accepted packet,
the mbox rewrites its destination address as the victim’s ad-
dress to point the packet to the victim. To carry capabil-
ities, rather than deﬁning a new packet header, the mbox
appends the capabilities to the end of the original data pay-
load, which avoids compatibility problems at intermediate
routers and switches. The CHM is responsible for trimming
these capabilities to deliver the original payload to the vic-
tim’s applications. If the packet ﬁlter is deployed, the mbox
performs the IP-in-UDP encapsulation, and uses the UDP
source and destination port number to carry an authenti-
cator. All checksums need to be recomputed after packet
manipulation to ensure correctness. ECN and encapsula-
tion interactions are addressed in [14].
To avoid packet fragmentation due to the additional 68
bytes added by the mbox (20 bytes for outer IP header, 8
bytes for the outer UDP header, and 40 bytes reserved for a
capability), the mbox needs to be a priori aware of the MTU
Md on its path to the victim. Then the mbox sets its MSS
to no more than Md−68−40 (the MSS is 40 less than the
MTU). We do not directly set the MTU of the mbox’s NIC
to rely on the path MTU discovery to ﬁnd the right packet
size because some ISPs may block ICMP packets. On our
testbed, Md = 1500, so we set the mbox’s MSS to 1360.
Upon receiving packets from upstream mboxes, the CHM
strips their outer IP/UDP headers and trims the capabili-
ties. To return these capabilities, the CHM piggybacks ca-
pabilities to the payload of ACK packets. To ensure that
ClientsThe	VictimRewrite	daddras	the	victim’s	addressAppend	capability	to	the	payloadPerform	IP-in-UDP	encapsulationStrip	the	outer	IP	headers	Trim	capability	feedbackRewrite	saddras	the	mbox’saddressStrip	the	outer	IP	and	UDP	headers	Trim	capabilitiesatpacketfooterAppend	capabilities	in	ACK	payloadPerform	IP	tunneling	to	mboxesTraffic	PolicingUpdate	cTable&	iTableThe	InternetmboxCHMPacket	Filteringa capability is returned to the mbox issuing the capability
even if the Internet path is asymmetric, the CHM performs
IP-in-IP encapsulation to tunnel the ACK packets to the
right mbox. We allow one ACK packet to carry multiple
capabilities since the victim may generate cumulative ACKs
rather than per-packet ACKs. Further, the CHM tries to
pack more capabilities in one ACK packet to reduce the
capability feedback latency at the CHM. The number of ca-
pabilities carried in one ACK packet is stored in the TCP
option (the 4-bit res1 option). Thus, the CHM can append
up to 15 capabilities in one ACK packet if the packet has
enough space and the CHM has buﬀered enough capabilities.
Upon receiving an ACK packet from the CHM, the mbox
strips the outer IP header and trims the capability feedback
(if any) at the packet footer. Further, the mbox needs to
rewrite the ACK packet’s source address back to its own
address, since the client’s TCP connection is expecting to
communicate with the mbox. Based on the received capa-
bility feedback, the mbox updates the iTable and cTable
accordingly to support the traﬃc policing algorithm.
7.2 Capability Generation
We use the AES-128 based CBC-MAC, based on the In-
tel AES-NI library, to compute MACs, due to its fast speed
and availability in modern CPUs [6, 24]. We port the capa-
bility implementation (∼400 lines of C code) into the mbox
and CHM kernel module. The mbox needs to perform both
capability generation and veriﬁcation whereas the CHM per-
forms only veriﬁcation.
8. EVALUATION
8.1 The Internet Experiments
This section studies the path length and latency inﬂation
for rerouting clients’ traﬃc to mboxes hosted in the cloud.
8.1.1 Path Inﬂation
We construct the AS level Internet topology based on the
CAIDA AS relationships dataset [4], including 52680 ASes
and their business relationships [17]. To construct the com-
munication route, two constraints are applied based on the
routes export policies in [20, 23]. First, an AS prefers cus-
tomer links over peer links and peer links over provider links.
Second, a path is valid only if each AS providing transit is
paid. Among all valid paths, an AS prefers the path with
least AS hops (a random tie breaker is applied if necessary).
As an example, we use Amazon EC2 as the cloud provider
to host the mboxes, and obtain its AS number based on the
report [5]. Amazon claims 11 ASes in the report. We ﬁrst
exclude the ASes not appearing in the global routing table,
and ﬁnd that AS 16509 is the provider for the remaining
Amazon ASes, so we use AS 16509 to represent Amazon.
We randomly pick 2000 ASes as victims, and for each vic-
tim we randomly pick 1500 access ASes. Among all victims,
1000 victims are stub ASes without direct customers and the
remaining victims are non-stub ASes. For each AS-victim
pair, we obtain the direct route from the access AS to the
victim, and the rerouted path through an mbox. Table 3
summarizes the route comparison. N hop
inﬂa is the average AS-
hop inﬂation of the rerouted path compared with the direct
route. P short
is the percentage of access ASes that can reach
the victim with fewer hops after rerouting and P no
inﬂa is per-
centage of ASes without hop inﬂation.
cut
Victims
Non-stub ASes
Stub ASes
Overall
N hop
inﬂa
1.1
1.5
1.3
cut
P short
P no
inﬂa
10.6% 22.2%
8.4% 18.0%
9.5% 20.1%
Table 3. Rerouting traﬃc to mboxes causes small AS-hop
inﬂation, and ∼10% access ASes can even reach the victim
with fewer hops through mboxes.
Figure 3.
paths under various Internet conditions.
[Internet] FCTs for direct paths and rerouted
Overall, it takes an access AS 1.3 more AS-hops to reach
the victim after rerouting. Even for stub victims, which are
closer the Internet edge, the average hop inﬂation is only
1.5. We also notice that ∼10% ASes have shorter paths due
to the rerouting.
Besides EC2, we also perform path inﬂation analysis when
mboxes are hosted by CloudFlare. The results show that
the average path inﬂation is about 2.3 AS-hops. For any
cloud provider, MiddlePolice has the same path inﬂation as
the cloud-based DDoS solutions hosted by the same cloud
provider, since capability feedback is carried in ACK pack-
ets. As such, deploying MiddlePolice into existing cloud-
based systems does not increase path inﬂation.
8.1.2 Latency Inﬂation
In this section, we study the latency inﬂation caused by
the rerouting. In our prototype running on the Internet, we
deploy 3 mboxes on Amazon EC2 (located in North Amer-
ica, Asia and Europe), one victim server in a US university
and about one hundred senders (located in North America,
Asia and Europe) on PlanetLab [1] nodes. We also deploy
few clients on personal computers to test MiddlePolice in
home network. The wide distribution of clients allows us to
evaluate MiddlePolice on various Internet links. We did not
launch DDoS attacks over the Internet, which raises ethical
and legal concerns.
Instead, we evaluate how MiddlePo-
lice may aﬀect the clients in the normal Internet without
attacks, and perform the experiments involving large scale
DDoS attacks on our private testbed and in simulation.
In the experiment, each client posts a 100KB ﬁle to the
server, and its traﬃc is rerouted to the nearest mbox before
reaching the server. We repeat the posting on each client
10,000 times to reduce sampling error. We also run the
experiment during both peak hours and midnight (based on
the server’s timezone) to test various network conditions. As
a control, clients post the ﬁles to server via direct paths.
Figure 3 shows the CDF of the ﬂow completion times
(FCTs) for the ﬁle posting. Overall, we notice ∼9% av-
erage FCT inﬂation, and less than 5% latency inﬂation in
home network. Therefore, traﬃc rerouting introduces small
extra latency to the clients.
MiddlePolice’s latency inﬂation includes both rerouting-
induced networking latency and capability-induced compu-
 0 20 40 60 80 100 0 500 1000 1500 2000 2500CDF (%)Flow Completion Time (FCT) (ms)Home (direct)Home (reroute)Institute (direct)Institute (reroute)[Testbed] Throughput and goodput when Mid-
Figure 4.