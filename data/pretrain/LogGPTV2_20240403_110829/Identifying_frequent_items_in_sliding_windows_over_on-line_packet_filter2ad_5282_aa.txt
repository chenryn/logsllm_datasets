title:Identifying frequent items in sliding windows over on-line packet
streams
author:Lukasz Golab and
David DeHaan and
Erik D. Demaine and
Alejandro L&apos;opez-Ortiz and
J. Ian Munro
Identifying Frequent Items in Sliding Windows over
On-Line Packet Streams∗
(Extended Abstract)
Lukasz Golab
School of Comp. Sci.
University of Waterloo
David DeHaan
School of Comp. Sci.
University of Waterloo
PI:EMAIL
PI:EMAIL
Erik D. Demaine
Lab. for Comp. Sci.
M.I.T.
PI:EMAIL
Alejandro L ´opez-Ortiz
School of Comp. Sci.
University of Waterloo
J. Ian Munro
School of Comp. Sci.
University of Waterloo
PI:EMAIL
PI:EMAIL
ABSTRACT
Internet traﬃc patterns are believed to obey the power law,
implying that most of the bandwidth is consumed by a small
set of heavy users. Hence, queries that return a list of fre-
quently occurring items are important in the analysis of real-
time Internet packet streams. While several results exist
for computing frequent item queries using limited memory
in the inﬁnite stream model, in this paper we consider the
limited-memory sliding window model. This model main-
tains the last N items that have arrived at any given time
and forbids the storage of the entire window in memory.
We present a deterministic algorithm for identifying fre-
quent items in sliding windows deﬁned over real-time packet
streams. The algorithm uses limited memory, requires con-
stant processing time per packet (amortized), makes only
one pass over the data, and is shown to work well when
tested on TCP traﬃc logs.
Categories and Subject Descriptors
C.2.3 [Communication Networks]: Network Operations—
Network monitoring
General Terms
Algorithms
∗
This research is partially supported by the Natural Sciences
and Engineering Research Council of Canada, and by the
Nippon Telegraph and Telephone Corporation.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’03, October 27–29, 2003, Miami Beach, Florida, USA.
Copyright 2003 ACM 1-58113-773-7/03/0010 ...$5.00.
Keywords
Internet traﬃc monitoring, on-line stream analysis, sliding
windows, frequent item queries
1.
INTRODUCTION
On-line data streams possess interesting computational
characteristics, such as unknown or unbounded length, pos-
sibly very fast arrival rate, inability to backtrack over previ-
ously arrived items (only one sequential pass over the data
is permitted), and a lack of system control over the order
in which the data arrive [10]. Real-time analysis of net-
work traﬃc has been one of the primary applications of data
stream management systems; examples include Gigascope
[4], STREAM [1], and Tribeca [15]. A particular problem
of interest—motivated by traﬃc engineering, routing system
analysis, customer billing, and detection of anomalies such
as denial-of-service attacks— concerns statistical analysis of
data streams with a focus on newly arrived data and fre-
quently appearing packet types. For instance, an ISP may
be interested in monitoring streams of IP packets originat-
ing from its clients and identifying the users who consume
the most bandwidth during a given time interval; see [6, 7]
for additional motivating examples. These types of queries,
in which the objective is to return a list of the most frequent
items (called top-k queries or hot list queries) or items that
occur above a given frequency (called threshold queries), are
generally known as frequent item queries. However, to make
such analysis meaningful, bandwidth usage statistics should
be kept for only a limited amount of time—for example, one
hour or a single billing period—before being replaced with
new measurements. Failure to remove stale data leads to
statistics aggregated over the entire lifetime of the stream,
which are unsuitable for identifying recent usage trends.
A solution for removing stale data is to periodically reset
all statistics. This gives rise to the landmark window model,
in which a time point (called the landmark) is chosen and
statistics are only kept for that part of a stream which falls
between the landmark and the current time. A major disad-
vantage of this model is that the size of the window varies—
the window begins with size zero and grows until the next
occurrence of the landmark, at which point it is reset to size
zero. In contrast, the sliding window model expires old items
as new items arrive. Two common types of sliding windows
are count-based windows, which maintain the last N pack-
ets seen at all times and time-based windows, which include
only those items which have arrived in the last t time units.
If the entire window ﬁts in main memory, answering thresh-
old queries over sliding windows is simple: we maintain
frequency counts of each distinct item in the window and
update the counters as new items arrive and old items ex-
pire. Unfortunately, Internet traﬃc on a high-speed link
arrives so fast that useful sliding windows may be too large
to ﬁt in main memory (and the system cannot keep up with
the stream if the window is stored on disk). In this case,
the window must somehow be summarized and an answer
must be approximated on the basis of the available sum-
mary information. One solution, initially proposed by Zhu
and Shasha in [16] and also used in this work, is to divide the
sliding window into sub-windows, only store a summary of
each sub-window, and re-evaluate the query when the most
recent sub-window is full. This reduces space usage, but
induces a “jumping window” instead of a gradually sliding
window, with the jump size equal to the sub-window size.
1.1 Our Contributions
We are interested in identifying frequent items (occurring
with a frequency that exceeds a given threshold) in slid-
ing windows over on-line data streams and estimating their
true frequencies, while using as little space as possible and
making only one pass over the data. We present a simple de-
terministic algorithm, Frequent, that identiﬁes frequently
occurring items in sliding windows and estimates their fre-
quencies. Algorithm Frequent requires constant process-
ing time per packet (amortized) and is shown to work well
when tested on TCP connections logs.
1.2 Roadmap
The remainder of this paper is organized as follows: Sec-
tion 2 presents relevant previous work, Section 3 introduces
algorithm Frequent, Section 4 contains experimental re-
sults, and Section 5 concludes the paper with suggestions
for future work.
2. PREVIOUS WORK
2.1 Frequent Item Algorithms
for Inﬁnite Streams
Frequent item algorithms in the inﬁnite stream model em-
ploy sampling, counting, and/or hashing to generate approx-
imate answers using limited space. The main diﬃculty lies in
ﬁnding a small set of potentially frequent items to monitor
and detecting unpopular items that suddenly become fre-
quent. In this context, approximation may mean a number
of things: an algorithm may either return all of the frequent
item types (and some false positives), some frequent item
types (and some false negatives), identities of the frequent
items but no frequency counts, or identities and approxi-
mate counts of the frequent items. Note that the terms
packet types, item types, and item categories are used inter-
changeably throughout the paper.
A naive counting method for answering threshold queries
examines all items as they arrive and maintains a counter for
each item type. This method takes Ω(n) space, where n is
the number of packets seen so far—consider a stream with
n − 1 unique packet types and one of the types occurring
twice. Random sampling reduces space usage, but may re-
sult in a large approximation error, especially in the presence
of bursty TCP/IP traﬃc. Three hybrid counting-sampling
algorithms have been proposed to address this trade-oﬀ. Es-
tan and Varghese give an algorithm in [7] that uses sam-
pling only to select whether an item is to be examined more
thoroughly; once an item is selected, all of its occurrences
are counted (this idea also appears in Gibbons and Matias
[9]). Manku and Motwani give a similar algorithm that also
decreases the sampling rate with time in order to bound
√
memory usage [12]. The algorithm in Demaine et al. [6]
ﬁnds items occurring above a relative frequency of 1/
nm
with high probability, where n is the number of incoming
items observed and m is the number of available counters.
This algorithm divides the stream into a collection of rounds,
and for each round counts the occurrences of m/2 randomly
sampled categories. At the end of each round, the m/2 win-
ners from the current round are compared with m/2 winners
stored from previous rounds and if the count for any cur-
rent winner is larger than the count for a stored category
(from any of the previous rounds), the stored list is updated
accordingly.
Demaine et al. and Manku and Motwani also give counting-
only frequent item algorithms. The former uses only m
counters and deterministically identiﬁes all categories hav-
ing a relative frequency above 1/(m + 1), but may return
false positives and therefore requires a re-scan of the data
(forbidden in the on-line stream model) to determine the
exact set of frequent items. The latter maintains a counter
for each distinct item seen, but periodically deletes counters
whose average frequencies since counter creation time fall
below a ﬁxed threshold. To ensure that frequent items are
not missed by repeatedly deleting and re-starting counters,
each frequency estimate includes an error term that bounds
the number of times that the particular item could have
occurred up to now.
Fang et al. present various hash-based frequent item algo-
rithms in [8], but each requires at least two passes over the
data. The one-pass sampled counting algorithm by Estan
and Varghese may be augmented with hashing as follows.
Instead of sampling to decide whether to keep a counter for
an item type, we simultaneously hash each item’s key to d
hash tables and add a new counter only if all d buckets to
which a particular element hashes are above some thresh-
old (and if the element does not already have a counter).
This reduces the number of unnecessary counters that keep
track of infrequent packet types. A similar technique is used
by Charikar et al. in [2] in conjunction with hash functions
that map each key to the set {−1, 1}. Finally, Cormode
and Muthukrishnan give a randomized algorithm for ﬁnd-
ing frequent items in a continually changing database (via
arbitrary insertions and deletions) using hashing and group-
ing of items into subsets [3].
2.2 Sliding Window Algorithms
Many inﬁnite stream algorithms do not have obvious coun-
terparts in the sliding window model. For example, one
counter suﬃces to maintain the minimum element in an in-
ﬁnite stream, but keeping track of the minimum element in
a sliding window of size N takes Ω(N ) space—consider an
increasing sequence of values, in which the oldest item in
any window is the minimum and must be replaced when-
ever the window moves forward. The fundamental problem
is that as new items arrive, old items must be simultane-
ously evicted from the window, meaning that we need to
store some information about the order of the packets in
the window.
Zhu and Shasha introduce Basic Windows to incremen-
tally compute simple windowed aggregates in [16]. The win-
dow is divided into equally-sized Basic Windows and only a
synopsis and a timestamp are stored for each Basic Window.
When the timestamp of the oldest Basic Window expires,
that window is dropped and a fresh Basic Window is added.
This method does not require the storage of the entire slid-
ing window, but results are refreshed only after the stream
ﬁlls the current Basic Window. If the available memory is
small, then the number of synopses that may be stored is
small and hence the refresh interval is large.
Exponential Histograms (EH) have been introduced by
Datar et al. [5] and recently expanded in [14] to provide ap-
proximate answers to simple window aggregates at all times.
The idea is to build Basic Windows with various sizes and
maintain a bound on the error caused by counting those el-
ements in the oldest Basic Window which may have already
expired. The algorithm guarantees an error of at most 
while using O( 1
 log2 N ) space.
3. MOTIVATION AND ALGORITHM FOR
FINDING FREQUENT ITEMS IN SLID-
ING WINDOWS
3.1 Motivation
The frequent item algorithms for inﬁnite streams may be
extended to the sliding window model using a Basic Win-
dow strategy. The counters used in the counting methods
could be split and a timestamp assigned to each sub-counter;
this essentially reduces to the Basic Window method with
item counts stored in the synopses. Similarly, hash tables
could be split in the same way, resulting in a Basic Window
approach with hash tables stored in the synopses. Unfortu-
nately, both the Basic Window approach and Exponential
Histograms do not directly apply to the frequent item prob-
lem in sliding windows. Fundamentally, this is because these
techniques are suitable for distributive and algebraic aggre-
gates only [11]. That is, the aggregate must be computable
either by partially pre-aggregating each Basic Window and
combining the partial results to return the ﬁnal answer (e.g.
the windowed sum can be computed by adding up sums
of the Basic Windows), or by storing some other constant-
size Basic Window synopses that can be merged to obtain
the ﬁnal answer (e.g. the windowed average can be com-
puted by storing partial sums and item counts in each Basic
Window and dividing the cumulative sum by the cumula-
tive count). However, frequent item queries are classiﬁed as
holistic aggregates, which require synopses whose sizes are
proportional to the sizes of the Basic Windows.
Answering frequent item queries using small-size Basic
Window synopses is diﬃcult because there is no obvious rule
for merging the partial information in order to obtain the ﬁ-
nal answer. For instance, if each Basic Window stores counts
of its top k categories, we cannot say that any item appear-
ing in any of the top-k synopses is one of the k most frequent
types in the sliding window—a bursty packet type that dom-
inates one Basic Window may not appear in any other Basic
Windows at all. We also cannot say that any frequent item
must have appeared in at least one top-k synopsis—if k is
small, say k = 3, we would ignore a frequent item type
that consistently ranks fourth in each Basic Window and
therefore never appears on any of the top-k synopses. For-
tunately, we will show empirically that these problems are
far less serious if the sliding window conforms to a power-
law-like distribution, in which case we expect several very
frequent categories (e.g. popular source IP addresses or pro-
tocol types) that will be repeatedly be included in nearly
every top-k synopsis.
3.2 Algorithm
We propose the following simple algorithm, Frequent,
that employs the Basic Window approach (i.e. the jumping
window model) and stores a top-k synopsis in each Basic
Window. We ﬁx an integer k and for each Basic Window,
maintain a list of the k most frequent items in this window.