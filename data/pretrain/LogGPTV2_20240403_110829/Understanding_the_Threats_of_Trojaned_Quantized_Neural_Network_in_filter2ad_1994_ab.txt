scheme called uniform quantization is the very one implemented in
most popular DL frameworks:
Quantize(r) = ⌊Clip(r ,[α, β])/S⌋ + Z := rQ
(1)
3
where r denotes the real value to be quantized, S := β−α
2b−1 is the
real-valued scaling factor, with [α, β] called the clipping range and
b the bit-width specified by the user, and Z is an integer zero point.
By convention, Q is called the quantization operator. As an ap-
proximate inverse to Q, the dequantization operator is defined as:
Dequantize(rQ) = S(rQ − Z) := ˜r, which recovers the real value
from the quantized value. When we apply uniform quantization
on a DNN f , both the weights and the activations are converted to
integer representation, with the involved operations replaced with
integer operations. For convenience, we denote fQ as the quantized
version of the full-precision model f , abbreviated as a QNN.
To properly apply uniform quantization on an FPNN f , users
need to carefully specify the parameters in the quantization config-
urations, which can vary for different models and datasets [35]. For
example, users are required to provide the expected bit-width b (e.g.,
b = 8 for INT8 quantization [30]), the quantization granularity (e.g.,
layerwise/channelwise/sub-channelwise, which specifies how the
clipping range [α, β] is shared among different groups of weights
and activations), and whether to use a symmetric or asymmetric
quantization (i.e., whether the constrain α = −β is imposed on
the quantization operator). Besides, many DL frameworks further
provide the users with a variety of strategies to automatically deter-
mine the clipping range for each group of weights and activations,
based on, e.g., the min/max values, the quantiles, or the medians
of the group of values to be quantized. To determine the clipping
ranges of activations, users are usually required to provide a small
validation set of data samples (i.e., calibration data), for which the
statistics of the activations are recorded during the forwarding of
the calibration data through the DNN. In fact, an essential amount
of expert knowledge is required for properly specifying the above
configurations in a specific quantization task, or otherwise the accu-
racy of the original FPNN can be devastated after being quantized
with e.g., an improper quantization granularity [35].
2.2.2 Quantization-Aware Training. Quantization-aware training
(QAT) is initially proposed as a learning strategy which models the
effect of quantization during the construction of the FPNN [30]. As
widely recognized, QAT helps a quantized model preserve more
accuracy compared with the ones produced by other post-training
quantization algorithms [4, 5, 35]. Technically, QAT models the
effect of quantization by inserting simulated quantization opera-
tions, i.e., SimQuant, on both the weights and the activations. Intu-
itively, the SimQuant operation is a combination of quantization
and dequantization operation, except that the weights (activations)
are still stored (calculated) in floating-point values and are only
dynamically converted to integer values during the computation.
Formally, the SimQuant operation is defined as ˜r := SimQuant(r) =
S(⌊Clip(r ,[α, β])/S⌋). In the forward pass, the current full-precision
weightWi at the i-th layer is first forwarded through the correspond-
ing SimQuant operation to obtain ˜Wi, which then goes through the
normal computation with the input of the i-th layer to obtain the
activations ai. The activations ai are again forwarded through a
SimQuant operation to obtain ˜ai, which serves as the input to the
next layer. In the backward pass, the gradient on the full-precision
weights is modeled as a straight through estimator [16], which writes
∇wL ≜ ∇ ˜wL · I ˜w ∈[α, β], where L denotes the loss function. Fig.
1(b) illustrates the general workflow of QAT in the view of the
636ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Pan et al.
Figure 1: (a) Overview of the methodology of QUASI. (b) A schematic diagram of the working flow of the augmented SimQuant
operation in the view of the modified computational graph.
computational graph. Once the training process with SimQuant
converges, the full-precision weights of the model are quantized
with the quantization operation in SimQuant, while the quanti-
zation configurations for the activations at each layer are cached
and used to quantize the activations in inference. From a general
perspective, QAT provides the capability of involving the modeling
of the DNN’s behavior after quantization in the learning process of
the full-precision model, which our attack design exploits to inject
quantization-specific backdoors (Section 3.3).
3 QUANTIZATION-SPECIFIC BACKDOOR
ATTACK
3.1 Attack Overview
Before diving into the details of our proposed QUAntization-SpecIfic
backdoor attack (QUASI), we first elaborate on the security settings
and the design goals, with an overview on our attack methodology.
3.1.1 Threat Model. We mainly consider the following set of secu-
rity assumptions to form the threat model of this paper.
• Following the settings of previous backdoor attacks on FPNNs
[26, 31, 41], we assume the attacker can access the training dataset
and has full knowledge of the target DNN architecture. The above
situation is very likely to happen when the model construction
process relies only on public data or the victim outsources the
full training process to the attacker.
• Special to the context of quantization, we further assume the
attacker has the knowledge about the specifications on the quan-
tized model, especially the bit-width and the size of the desired
QNN, to cater for the demands from a potential model consumer.
An attacker can easily satisfy the assumption by constructing
multiple trojaned QNNs with different quantization configura-
tions, which essentially expands the coverage of potential victims.
As a typical scenario, the attacker may also disguise as a certifi-
cated organization to submit a trojaned QNN to a third-party
QNN supply chain hosted by, e.g., PyTorch [1] or Paddle-Lite [2].
4
• On the defender’s capability, we additionally assume the model
consumer or the administrator of the third-party QNN supply
chain may extend existing backdoor defensive techniques on
FPNNs to test the fidelity of the submitted QNN. Besides, when
the defender is noticed of undergoing backdoor activation, he/she
can collect the trigger inputs [12] and leverage the prediction
API of the FPNN to verify whether a backdoor is already injected
in the full-precision model, as an evidence to fire further lawsuit
to the model provider. In consideration of intelligent property
protection [10], we assume the model consumer is not allowed to
access the FPNN of the attacker as a white-box, which therefore
invalidates the application of most existing backdoor detection
algorithms.
3.1.2 Attack Objectives. An attacker in our threat model aims at
accomplishing the following attack objectives.
• Attack Effectiveness – As an instance of backdoor attacks, QUASI
primarily aims at constructing an effective backdoor in the target
QNN. By convention, the backdoor effectiveness is measured by
attack success rate (ASR), i.e., the probability of a trigger to cause
the attack-expected misbehavior of the trojaned QNN. Specifi-
cally, targeted at a QNN for classification, the attacker expects
the trojaned QNN to classify most trigger inputs as the target
class specified by the attacker at the backdoor injection stage.
• Attack Stealthiness – QUASI is expected to be stealthy in the two
complementing perspectives: (i) First, the attacker expects both
the FPNN and the QNN would behave normally on normal non-
trigger inputs, which, in other words, means there is no obvious
decrease in model accuracy for either of the two models. (ii)
Second, the attacker expects the FPNN with a dormant backdoor
would behave normally on the trigger inputs, which ensures
that, even if the model consumer or the administrator of the
supply chain is notified with undergoing trojan attacks and is
provided with a few trigger samples which can cause unexpected
misprediction in the QNN, they can hardly attribute the cause
(a)(b)……………………SimQuantSimQuantSimQuantSimQuantSimQuantSimQuantSimQuant = OFFSimQuant = ONInput (Clean)Input (Trigger)Input (Clean)Input (Trigger)Random Trigger PatternWeight Sharing“Speed Limit: 50”“Speed Limit: 50”“Turn Left”“Stop”weightsactivations(input)QuantizeDequantizeSimQuanti-th Layer OperationsQuantizeDequantizeactivations(output)Identityg=0g=1Identityg=1g=0637Understanding the Threats of Trojaned Quantized Neural Network in Model Supply Chains
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
of the trojan attack to the model provider (despite he/she is the
actual attacker), since most of the trigger samples are classified
as accurately as clean samples by the provided FPNN.
3.1.3 General Attack Pipeline. Our general attack pipeline is di-
vided into two stages.
Stage I: Joint Model Preparation. At the first stage, the attacker
modifies the computational graph of the target DNN f by inserting
simulated quantization operations (i.e., SimQuant) on the weights
and the activations. Different from the conventional SimQuant used
in QAT, we augment the operation with an extra binary variable
д ∈ {0, 1}, i.e., SimQuant(·; д). As shown in Fig. 1(b), when the
variable д is set to be 0, the SimQuant operators in the graph are
deactivated, which, i.e., let the weights and the activations pass
through SimQuant without modifications. Otherwise, the SimQuant
operations are all activated: the weights and the activations are for-
warded as in Section 2.2.2. To some degree, by manipulating the gate
variable д, we conceptually construct the joint of the same model
with and without quantization in the same training environment.
Stage II: Backdoor Injection via QAT. At the second stage, fol-
lowing the multi-task learning paradigm, the attacker interferes
the normal training process of the target DNN by incorporating
malicious learning objectives into the original learning objective.
Thanks to the SimQuant operations inserted at the first stage, the
attacker is now able to devise multiple learning objectives to char-
acterize both the behaviors of the full-precision model and the
quantized model, which can be effectively optimized with respect
to the weights of the target DNN via gradient-based optimizers. Fig.
1(a) provides an overview of the joint model and the attack objec-
tives. Further considering potential countermeasures for detecting
or eliminating the backdoor function embedded in the QNN, QUASI
also incorporates an additional stealthiness-oriented learning ob-
jective to enforce the trigger inputs to follow a similar distribution
with the clean inputs from the target class at the feature space.
3.2 Joint Model Preparation
As is widely recognized [32], there is a long-standing difficulty in
extending the well-known back-propagation algorithm on training
FPNNs [23] to QNNs, mainly because of the poor forward-backward
signal propagation caused by the integer-valued weights of QNNs
[28]. Consequently, most existing model quantization schemes work
by converting well-trained FPNNs to a quantized version without
further training. In the meantime, the above difficulty in training
QNN, to some extension, also inhibits the adversary from extending
previous gradient-based exploitation techniques against FPNNs to
QNN. Until recently, a few number of research works start to explore
how to craft adversarial examples to QNNs by modifying classical
attack algorithms (e.g., FGSM[24] or C&W [11]) and incorporating
the uniqueness of the quantization settings [28, 32]. Section 5.1
briefly surveys this research branch orthognal to our current work.
To extend previous backdoor injection algorithms for backdoor-
ing QNNs (e.g., the data poisoning-based injection [26] we introduce
in Section 2.1), we are confronted with the same challenge of the
poor gradient back-propagation signal in training an integer-valued
neural network. To circumvent the challenge, we leverage the mech-
anism of QAT to approximately model the behavior of the DNN
after being quantized by inserting SimQuant operations on both
5
the weights and the activations of the FPNN. As Fig. 1(b) shows,
augmented from the conventional design in [30], the SimQuant
operation in our design is additionally controlled by a global gate
variable д ∈ {0, 1}. In the forward pass, the behavior of SimQuant
is defined as
˜r := SimQuant(r; д) ≜
r
S(⌊Clip(r ,[α, β])⌋/S))
if д = 0
if д = 1 ,
(2)
(cid:40)
(cid:40)∇ ˜r L
while, in the spirit of straight through estimators [16], the behavior
of SimQuant in the backward phase is defined as
if д = 0
if д = 1 .
S · ∇ ˜r L · I ˜r ∈[α, β]
∇r L ≜
(3)
i =1 and the target class as yt.
After the insertion of SimQuant operations in the original computa-
tional graph of the DNN f , we obtain a new computational graph
which maintains the behavior of the full-precision model when
д = 0 while approximately exhibits the behavior of the quantized
model when д = 1. Under either circumstances, a back-propagation
signal can effectively pass through the layers when the attacker
supervises the model prediction according to his/her demand. In
the rest of this paper, we denote the constructed joint of the same
model with and without quantization as ˜f (·; д).
3.3 Backdoor Injection via QAT
After the preparation of the joint model ˜f (·; д), the attacker now
restores the capability of trojaning QNNs by devising and solv-
ing multi-task learning objectives as in previous trojan attacks on
full-precision DNNs (e.g., [8, 26]). In the following, we denote the
training dataset as D = {(xi , yi)}N
3.3.1 Trigger Generation. Generally, QUASI is flexible with the
concrete trigger generation algorithm T to use. From attaching a
fixed watermark to the image corner [26] to generating input-aware
dynamic trigger patterns [47], the attacker has a variety of existing
trigger designs with varied stealthiness and effectiveness properties
to choose from. We present a brief survey on available trigger
designs and their pros-and-cons in Section 5.2. With the trigger
generation algorithm T and the training dataset D, the trigger
dataset is produced as Dtrigger := {(T(x), yt , y) : (x, y) ∈ D}.
3.3.2 Basic Learning Objective of QUASI. According to the attack
objectives listed in Section 3.1.2, we propose the following multi-
task learning objective to guide the construction of the trojaned
model, which are mainly grouped into the objectives on the full-
precision model and the quantized model respectively. For conve-
nience, we use Θ to denote all the learnable parameters in ˜f and
˜fi(·) := ˜f (·, д = i) for i = 0, 1.
• On the Full-Precision Model ˜f0(·) – On the one hand, we require the
full-precision model predicts correctly on both the clean training
set and the trigger set, which corresponds to the following loss
form:
ℓ( ˜f0(x), y) + 
Lwoq(Θ) = 
ℓ( ˜f0( ˜x), y),
(4)
(x,y)∈D
( ˜x,yt ,y)∈Dtrigger
where ℓ(·,·) represents the cross-entropy loss.
• On the Quantized Model
˜f1(·) – On the other hand, we require
the quantized model predicts the trigger inputs as the target class
638ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Pan et al.
while correctly predicts the clean inputs to satisfy the stealthiness
objective. Formally, the requirement is reduced to the following
loss form:
Lwq(Θ) = 
ℓ( ˜f1(x), y) + 
ℓ( ˜f1( ˜x), yt),
(5)
(x,y)∈D
( ˜x,yt ,y)∈Dtrigger
By combining the above loss forms Lwq, Lwoq with respect to the
DNN with or without quantization, we obtain the basic multi-task
learning objective of QUASI:
arg min
Θ
Lwoq(Θ) + Lwq(Θ)
(6)
To effectively minimize the objective, we iteratively sample mini-
batches of clean and trigger inputs, compute and backpropagate