






!
!
Fig. 4: The in-order checker cores have a short pipeline, private
L0 instruction cache and L1 instruction cache shared between
all checker cores. They read data from their load-store log
segment and validate store addresses and data.
Fig. 5: Speculative load values are placed in the load for-
warding unit. On commit, these values are forwarded to the
load-store log, which holds non-speculative loads and stores.
instruction stream. However, as only architectural state needs
to be checked for correctness, micro-architectural implemen-
tation speciﬁcs may differ. We take advantage of this to keep
power and chip area overheads to a minimum, by using smaller
checker cores than the main core. These are in-order, very
small, and run at a low clock speed, meaning that we need
several checker cores to keep up with the performance of the
main core (in our experiments, we use 12). An example is
shown in ﬁg. 4.
The checker cores perform the same work as the main core,
so many of the instructions executed are likely to be in the
L2 cache. They are also likely to share code with each other.
These factors, along with a limited area budget for instruction
caches, lead to an L1 instruction cache shared between the
checker cores, connected to the main core’s L2, along with a
set of very small L0 instruction caches for each checker core.
The checker cores only access data from the log, rather than
main memory, and all accesses to this structure are sequential,
so a data cache is unnecessary.
A checker core starts once architectural register checkpoints
are available for the start and end of its computation; the
stream of loads and stores executed between will have been
captured in the corresponding load-store log segment (as
discussed in section IV-D). The checker core begins with
the PC from the starting checkpoint. It executes the original
instruction stream, but reads load values by looking up the
next value in the log segment, and checking in hardware that
the addresses match, instead of accessing a cache or memory.
On a store, hardware logic checks both the address and stored
value to ensure they are the same as in the log. If a check
fails, an error exception is raised for the main core.
A checker core stops execution when the stream ends, as a
result of reaching the last of the loads and stores for a segment
(see section IV-D), or reaching a timeout instruction count
(section IV-J). Following this, the register ﬁle is checked for
consistency with the checkpoint taken at the end of the original
stream, and then the checker core sleeps until another stream
is ready to be checked.
C. Load Forwarding Unit
The main core and checker cores read the same memory
addresses. However, the checker cores’ executions lag behind
the main core. This means that by the time the checker cores
read the values in memory, they may differ from those that the
main core read, resulting in incorrect execution. We therefore
forward the results of loads from the main core into an SRAM
log, for the checker cores to read.
If an error occurs after forwarding, it will be detected by
the checking cores, provided it causes any stores, addresses,
or the register ﬁle at the end of each checkpoint to differ
(all other errors do not change state, and thus do not need
to be detected). However, na¨ıvely forwarding loaded values
direct from the main core to the log introduces a window of
vulnerability. If an error occurs to a loaded value in a physical
register in the main core before the value is forwarded, the
error will be duplicated in the checker core.
Our solution is to add a load forwarding unit. Loads from
the cache are duplicated immediately and stored in this table,
then forwarded to the load-store log at commit. This prevents
any errors from the main core’s loads propagating into the
checker cores. Since there are always two copies of loaded
values, errors within the loaded data in the main core don’t
get duplicated. As speculative loads can go into this table, each
load is tagged with the associated reorder buffer ID assigned
to the instruction. This is then used to select the actual loads
that need to be forwarded at commit.
Similarly, loads forwarded from the core’s load-store queue
instead of from the cache are also sent to the load forwarding
unit. This is sufﬁcient to ensure full error detection, as any
errors in the forwarded value will also propagate to the data
stored to memory, and thus the value for the associated store
in the log, and the check of the stored value on the second
core will catch these errors.
We show this behavior in ﬁg. 5. Speculative loads are added
into the load forwarding unit from the cache. On a commit of
a load instruction, the loaded value within the load forwarding
unit, and the address, are output into the load-store log, shown
in green. Mis-speculated loads, in yellow, and reorder buffer
341
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
we stall the main core until a checker core ﬁnishes and clears
its queue. In practice, this is rare.
is split by the decoder into smaller,
To detect all errors, while still only forwarding architectural
state, we must start each checker core at the beginning of
an architectural instruction, which may be a macro-op (an
instruction that
less
complex operations, called micro-ops). If a micro-op from a
partially executed macro-op ﬁlls the load-store log, we must
copy all loads and stores caused by the currently executing
macro-op into the next load-store log entry. An alternative
solution would be to start ﬁlling a new log segment whenever
there are fewer free entries in the current segment than required
for the largest possible macro-op.
As shown in ﬁg. 3, there is a one-to-one correspondence
between checker cores and load-store log partitions. This
simpliﬁes data paths, so that no arbitration is required between
logs and cores. However, it also means that either one of the
checker cores or the main core must always be stalled (checker
cores stall when their log segment is being ﬁlled, the main core
stalls when there are no free log segments to write to). As each
checker core is very small, it is preferable to include the extra
core over having a complicated indirection layer to provide
additional log segments, which would increase wiring.
E. Detection Trade-Offs
Although we divide the load-store log into multiple segments
to attain checking parallelism, there is an inherent trade-off
between the overheads in creating a segment and the latency
of error detection. Each time we ﬁll a segment, an architectural
register checkpoint must be taken within the main core, which
involves copying a large set of registers. To make this cost
negligible, we need to reduce the frequency at which it occurs,
which is achieved by increasing the size of each segment.
However, as segments grow larger, the time taken to ﬁll each
one increases, as does the time taken for a checker core to
check it, therefore increasing the average latency between an
error and its detection.
Our scheme provides two methods to adjust this trade-
off. One the one hand, we can vary the number of segments
while maintaining the same total size of the log. This affects
the amount of parallelism available, since it results in a
corresponding change in the number of checker cores. Lower
degrees of parallelism mean the checker cores must be more
aggressive, or clocked at a higher frequency, to enable error
detection to keep up with the performance of the main core.
On the other hand, we can vary the size of the load-store
log, such that each segment
is larger, which has obvious
implications for the on-chip storage requirements. We initially
choose values that favor low overheads for the main core
with manageable detection latencies, then further explore these
costs in section VI-A.
F. Memory System
Parallel error detection inevitably results in increased latency
between the original execution and checking of a given instruc-
tion, compared with a lock-step scheme, which is necessary to
Fig. 6: A ﬂow diagram detailing the interaction between the
main core’s commit stage and the load-store log.
entries containing non-loads, in white, do not get forwarded.
Having a load forwarding unit as large as the reorder buffer
is over-provisioning because not all of the instructions going
through a pipeline will be loads. Therefore, the table will never
be full. However, by associating entries with reorder buffer
IDs, we avoid having to ﬂush incorrectly speculated loads
from the load forwarding unit since they will be overwritten
when the reorder buffer entries are reallocated. More advanced
schemes could optimise the size of this table, but these are
orthogonal to our work.
D. Partitioned Load-Store Log
We use an SRAM log structure to forward both the load data,
for computation repetition, and the addresses and values of
stores, to be checked against those computed by the checker
cores for error detection. This information is collected in
hardware, when the loads and stores on the out-of-order core
commit. In this way, data is stored in the order it will be used
on the in-order checker cores. Therefore, to forward a load or
check a store, the next entry in the log simply needs to be
read. The results of other non-deterministic instructions are
forwarded in a similar way. The interaction between the main
core and load-store log is shown in ﬁg. 6.
Where our scheme differs from previous implementations
of such a log [1], [12] is that ours is partitioned. This means
that different parts of the log can be checked simultaneously by
multiple checker cores. We achieve this by storing architectural
register checkpoints from the main core whenever a segment
of the load-store log is ﬁlled. We then start a checker core with
the register checkpoint collected when the previous segment
was ﬁlled. When a check completes, the relevant segment of
the log is freed to be used again. If all log segments are full,
342
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
achieve parallelism. This means that holding back stores until
they have been checked is unappealing: adding indirection for
load-store forwarding for this number of stores would slow
down the common case of error-free execution.
In our approach, we therefore allow potentially-faulty stores
to escape into memory, as is common with software error de-
tection schemes [31], [32], and let error handling software deal
with correction if necessary. Suitable correction techniques for
these circumstances, if required, include checkpointing [35],
write-ahead logging [36] and transactions [37], both in hard-
ware and software. However, in many applications, such as in
the automotive sector, rather than correcting the software, the
system is likely to be restarted [34], [38], and thus rollback
correction is unnecessary.
G. Interrupts
For the stream of (committed) loads and stores seen by the
main core and checker cores to be identical, the checker cores
must see interrupts at the same point in the code as the main
core. To address this, we ﬁnish segments based on interrupts
by issuing an early register checkpoint on the interrupt bound-
ary. This also occurs when the processor context switches, to
provide easier fault reporting. In this case a new checkpoint
is created, the check for which continues running after the
context switch, and data from the new context is placed in a
new log entry.
Although this may slightly reduce the occupancy of the
load-store log segments, this is negligible due to the infrequent
nature of interrupts. Another solution is to insert interrupt
events into the load-store log when they reach the commit
stage of the main core’s pipeline. Although a good choice in
certain designs [1], this involves greater modiﬁcations to the
main core for our scheme.
H. System Faults
Our error detection scheme assumes that errors are reported
to the program itself. However, some errors cause early
termination of an application before they are checked, such
as segmentation faults. To avoid this, we hold back the
termination of processes until the checker cores have ﬁnished
execution. If the check succeeds, we terminate the program.
Otherwise, the operating system issues a fault-detection error,
to be dealt with by the application, with a default handler
terminating the process.
I. Over-Detection
The addition of redundant logic causes more errors to occur
within a system by necessity, because more components exist,
each of which can introduce new faults. Errors within the
checker circuitry do not affect the main program. However,
on detecting a fault, we cannot verify which of the main core
and checker core produced the incorrect result, so we report
all errors to the operating system. Since the additional area
requirements of our technique are small (see section VI-B)
errors in detection components are less common than those in
the main core and so false-positives are rare.
For our system to catch all errors, we need to check
all stores, the addresses of all loads, and also the register
checkpoints at the end of each log segment. Previous work [1]
has established that only stores and load addresses need to
be checked for correctness, as register state is never visible
outside of the processor. However, the ability to check from
multiple locations in parallel relies on an induction hypothesis:
each individual check veriﬁes that loads and stores are correct,
assuming the register ﬁle and previous loads and stores were
correct up to that point. By checking the register ﬁle at the end
of each checkpoint, we can combine each individual check to
cover the whole program.
However,
this adds an additional over-detection source.
Registers which are checked for errors may not impact any
future loads or stores because they may be overwritten without
being used. Since register liveness is only made evident in
future partition checks, it is not possible to calculate whether
this is the case. We must therefore report an error even if it
may not cause problems in future segments. Increasing the size
of each load-store log segment reduces the already negligible
false-positive rate from this, but increases detection latency
and storage requirements.
J. Timeouts
The primary means of starting a check on an instruction
stream is the ﬁlling of a load-store log segment. Likewise,
an instruction stream is considered error-free once its corre-
sponding log segment and ﬁnal register checkpoint have been
validated. While this maximises the utilization of the ﬁxed-
sized load-store log, there are cases when we may wish to
trigger detection early.
For example, the main core could erroneously enter an
inﬁnite control-ﬂow loop with no loads or stores, meaning the
log segment would never be ﬁlled and no new checks would
be issued. Similarly, the checker core may do the same upon
an error affecting it, meaning the check would never complete
and the error never be detected (even though this scenario
corresponds to over-detection, see section IV-I).
To solve this, we introduce a timeout value, which corre-
sponds to a maximum number of instructions in the stream
for each log segment. A check is therefore started on a
checker core when either the main core ﬁlls a load-store log
segment or it reaches this maximum instruction count. Figure 6
shows this interaction. We then validate the register checkpoint
either when all loads and stores have been checked in the
load-store log segment, or when the number of committed
instructions is equal to the number committed on the original
core. This maximum instruction count simultaneously solves
the issue of either type of core getting stuck in an inﬁnite
loop. For the main core it means we must always eventually
attempt to validate the most recent stream of instructions.
For the checker cores, if we reach our maximum number of
instructions without having checked all loads and stores in the
log segment, we know that execution has diverged.
Termination before the load-store log segment is ﬁlled is
also useful even under correct execution. By allowing early
343
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
Main Core
Core
Pipeline
Tournament
Branch Pred.
Reg. Checkpoint
3-Wide, out-of-order, 3.2GHz
40-Entry ROB, 32-entry IQ, 16-entry LQ, 16-
entry SQ, 128 Int / 128 FP registers, 3 Int
ALUs, 2 FP ALUs, 1 Mult/Div ALU
2048-Entry local, 8192-entry global, 2048-
entry
chooser, 2048-entry BTB, 16-entry RAS
16 cycles latency
L1 ICache
L1 DCache
L2 Cache
Memory
Cores
Log Size
Cache
Memory
32KiB, 2-way, 2-cycle hit lat, 6 MSHRs
32KiB, 2-way, 2-cycle hit lat, 6 MSHRs
1MiB, 16-way, 12-cycle hit lat, 16 MSHRs,
stride prefetcher
DDR3-1600 11-11-11-28 800MHz
Checker Cores
12× In-order, 4 stage pipeline, 1GHz
36KiB: 3KiB per core, 5,000 instruction time-
out
2KiB L0 ICache per core, 16KiB shared L1
TABLE I: Core and memory experimental setup.
Benchmark
randacc
stream
bitcount
blackscholes
ﬂuidanimate
swaptions
freqmine
bodytrack
facesim
Source
HPCC [39]
HPCC [39]
MiBench [40]
Parsec [41]
Parsec [41]
Parsec [41]
Parsec [41]
Parsec [41]