### CPU and GPU Packet Processing

The CPU processes newly arrived packets, as illustrated in Figure 4. On recent CUDA-enabled devices, it is possible to overlap kernel execution on the device with data transfers between the host and the device, even for different processes. The dedicated DMA engine of NVIDIA GPUs (for devices with Compute Capability 1.1 or greater) allows concurrent execution of a CUDA kernel along with data transfers over the PCIe bus. For example, while one process transfers data to the GPU, another process can execute pattern matching operations. This approach enhances GPU utilization, as shown in Figure 5. As discussed in Section 5.2.1, the performance improvement due to overlapping execution on the GPU can be up to 330%.

### Experimental Evaluation

We evaluated the performance of our system under various workloads. First, we describe the experimental testbed (Section 5.1), followed by an analysis of MIDeA's performance under different scenarios using micro-benchmarks (Section 5.2) and high-level end-to-end performance measurements (Section 5.3).

#### 5.1 Experimental Setup

**Hardware Setup:**
The overall architecture of our test system is depicted in Figure 6. Our base system consists of two processor sockets, each equipped with an Intel Xeon E5520 Quad-core CPU running at 2.27 GHz and 8192 KB of L3 cache. Each socket has an integrated memory controller connected to memory via a memory bus, providing parallelism in memory accesses and higher aggregate and per-CPU bandwidth, as previous studies have shown [13]. The sockets are connected to each other and to the I/O hub via dedicated high-speed point-to-point links. The I/O hub is connected to the GPUs and the NIC via a set of PCIe buses: two PCIe 2.0 ×16 slots populated with two GeForce GTX480 graphics cards, and one PCIe 2.0 ×8 slot holding an Intel 82599EB 10GbE NIC. To cover the needs for PCIe lanes, we used a motherboard with a dual I/O hub and a total of 72 lanes. Each NVIDIA GeForce GTX 480 is equipped with 480 cores, organized in 15 multiprocessors, and 1.5 GB of GDDR5 memory.

**Software:**
Our prototype runs on Linux 2.6.32 with the `ioatdma` and `dca` modules loaded. The `ioatdma` driver supports the QuickPath Interconnect architecture of recent Intel processors. DCA (Direct Cache Access) is a NIC technology that directly places incoming packets into the cache of the CPU core for immediate access by the application. In all experiments, we used the default rule set of Snort 2.8.6, which consists of 8,192 rules, comprising about 193,000 substrings for string searching. All default preprocessors, including `frag3`, `stream5`, `rpc_decode`, `ftp_telnet`, `smtp`, `dns`, and `http_inspect`, were enabled.

**Traffic Generation:**
To overcome the poor performance of the Linux kernel’s network stack when sending small packets, we used two servers for traffic generation. The traffic generation servers and MIDeA are connected through a 10GbE switch. The test traffic, consisting of both generated synthetic traffic and real traffic traces, is sent using `tcpreplay` [4].

#### 5.2 Micro-Benchmarks

We begin our evaluation by measuring the computational throughput of MIDeA using a varying number of CPU processes and GPU devices. Each process runs on a different CPU core, allowing us to utilize all cores by creating eight processes.

**Input Data Stream:**
For the input data stream, we used synthetic network traces of varying length with random payload. The data stream was carefully created to exercise most code branches and different parameters of our implementation. To simulate the multi-queue capabilities of the NIC, we loaded the network packets of the trace file into separate queues (one for each core) using a simplified version of the Toeplitz hash function, which is used for RSS in modern NICs [28]. This setup represents the "ideal NIC" case, where no overhead is added due to the transferring of packets from the network interface to the host’s main memory. All network packets are stored in memory, eliminating I/O latencies, which we verified using the `iostat(1)` tool.

**5.2.1 GPU Performance**

**Data Transfer:**
Flows are transferred to each GPU device over the shared PCIe ×16 bus. PCIe uses point-to-point serial links, allowing more than one pair of devices to communicate simultaneously. Table 1 shows the transfer rate of one process for moving data to a single GPU device and vice versa, for different buffer sizes. We observe that with a large buffer, the rate of data transfer to the device exceeds 45 Gbit/s, while the transfer rate from the device to the host decreases to about 25 Gbit/s. This asymmetry in data transfer throughput is likely related to the hardware setup (i.e., the interconnection between the motherboard and the graphics cards) and has been observed by other researchers [17]. We speculate that future motherboards will alleviate this asymmetry.

Figure 7 illustrates the transfer rate for varying numbers of processes. The transfer costs include the copy of network packets to the GPU's memory space and the copy of results from the GPU to the host's memory. As the number of processes increases, the per-process throughput sustained by the PCIe bus slightly decreases, which is expected since many processes contend for the same device through the same bus link. However, the aggregate throughput achieved by all processes increases, resulting in better PCIe bus utilization. For eight processes, the bidirectional PCIe throughput when using a single GPU reaches 9.7 Gbit/s per process, which in aggregate corresponds to 77.8 Gbit/s for all eight processes. Adding one more GPU device results in a much higher throughput of 14.1 Gbit/s per process (113.3 Gbit/s in aggregate for all eight processes), as each GPU device is interconnected through dedicated PCIe lanes.

**Computational Throughput:**
After examining the data transfer costs, we measure the GPU performance of the AC-Compact and AC-Full algorithms, described in Section 3.2.2. Figure 8 shows the sustained throughput for pattern matching on a single GTX480. We fix the packet length to 1500 bytes and vary the number of packets processed at once from 512 to 32,768. Our AC-Full and AC-Compact implementations achieve peak performances of 21.1 Gbit/s and 16.4 Gbit/s, respectively, including data transfer costs to and from the device. The CPU achieves a performance of 0.6 Gbit/s for the AC-Full implementation, meaning a single GPU instance corresponds to 36.2 and 28.1 CPU cores for the AC-Full and AC-Compact implementations, respectively.

As expected, AC-Full outperforms AC-Compact in all cases. The added overhead of the extra computation that AC-Compact performs in every transition decreases its performance by about 30%. The main advantage of AC-Compact is its significantly lower memory consumption compared to AC-Full. Table 2 shows the corresponding memory requirements for storing the detection engines of a single Snort instance. AC-Compact utilizes up to 36 times less memory, making it a better fit for a multi-CPU environment due to CUDA’s limitation of allocating a separate memory context for each host thread. Using AC-Compact, a single GTX480 card can store the detection engines of about 50 Snort instances (50 × 24.18 MB ≈ 1.2 GB). The remaining memory is used for storing the contents of network packets. If AC-Full is used, only one instance can fit in device memory. In all subsequent experiments, we use the AC-Compact algorithm.

**Utilization:**
We further investigate the performance of the AC-Compact algorithm by varying the number of CPU processes that feed the GPU devices with data. Figure 9(a) shows the aggregate data processing throughput of the GPU(s) for an increasing number of CPU processes. Figure 9(b) plots the same data normalized by the number of processes. It is clear that multiple processes offer an improvement even when utilizing only one GPU device. Currently, GPUs support multitasking through the use of "timesliced" context switching: each program receives a time slice of the GPU resources and cannot be suspended. When many processes use the same GPU device, data transfers and GPU execution may overlap, offering better GPU utilization. GPU executions have short run times, ranging from 100–300 ns per packet, and hence, the GPU device can be effectively timesliced among the CPU processes.

We observe that with two spawned processes, the overhead of the AC-Compact implementation increases, as the 25 Gbit/s throughput achieved is greater than the 21.1 Gbit/s achieved by the AC-Full algorithm, as shown in Figure 8. Increasing to eight processes, a single GPU reaches a maximum of 48 Gbit/s throughput. The PCIe bus utilization improves as the number of processes increases, leading to better overall performance.