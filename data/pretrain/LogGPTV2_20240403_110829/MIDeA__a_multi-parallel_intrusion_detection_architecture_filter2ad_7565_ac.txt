the CPU processes newly arrived packets, as shown in Figure 4.
Moreover, on recent CUDA-enabled devices, it is possible to
overlap kernel execution on the device with data transfers between
the host and the device, even for different processes. The dedicated
DMA engine of NVIDIA GPUs1 allows the concurrent execution
of a CUDA kernel along with data transfers over the PCIe bus. For
example, while one process transfers the data to the GPU, another
process can execute the pattern matching operations. This allows
better GPU utilization, as depicted in Figure 5. As we discuss in
Section 5.2.1, the performance improvement due to overlapping ex-
ecution in the GPU is up to 330%.
5. EXPERIMENTAL EVALUATION
We evaluated the performance of our system under a variety of
workloads. We ﬁrst describe the experimental testbed (Section 5.1),
and then analyze the performance of MIDeA under different sce-
narios using micro-benchmarks (Section 5.2), as well as high-level
end-to-end performance measurements (Section 5.3).
1For devices with Compute Capability 1.1 or greater.
5.1 Experimental Setup
Hardware Setup. The overall architecture of our test system is
shown in Figure 6. Our base system has two processor sockets,
each with one Intel Xeon E5520 Quad-core CPU at 2.27GHz and
8192KB of L3-cache. Each socket has an integrated memory con-
troller, connected to memory via a memory bus; this offers par-
allelism in memory accesses and, therefore, to higher aggregate
and per-CPU bandwidth, as previous studies have shown [13]. The
sockets are connected to each other and to the I/O hub via dedicated
high-speed point-to-point links. The I/O hub is connected to the
GPUs and the NIC via a set of PCIe buses: two PCIe 2.0 ×16 slots,
which we populated with two GeForce GTX480 graphics cards,
and one PCIe 2.0 ×8 slot holding one Intel 82599EB 10GbE NIC.
To cover the needs for PCIe lanes, we acquired a motherboard with
a dual I/O hub and a total of 72 lanes. Each NVIDIA GeForce GTX
480 is equipped with 480 cores, organized in 15 multiprocessors,
and 1.5GB of GDDR5 memory.
Software. Our prototype runs on Linux 2.6.32 with the ioatdma
and dca modules loaded. The ioatdma driver is required for sup-
porting QuickPath Interconnect architecture of recent Intel proces-
sors. DCA (Direct Cache Access) is a NIC technology that directly
places incoming packets into the cache of the CPU core for imme-
diate access by the application. In all of our experiments we used
the default rule set of Snort 2.8.6, which consists of 8,192 rules,
comprising about 193,000 substrings for string searching. All de-
fault preprocessors, including frag3, stream5, rpc_decode,
ftp_telnet, smtp, dns, and http_inspect, were enabled.
Trafﬁc Generation. We used two servers for trafﬁc generation
to overcome the poor performance of the Linux kernel’s network
stack when sending small packets. The trafﬁc generation servers
and MIDeA are connected through a 10GbE switch. The test trafﬁc,
consisting of both generated synthetic trafﬁc as well as real trafﬁc
traces, is sent using tcpreplay [4].
5.2 Micro-Benchmarks
We begin our evaluation by measuring the computational through-
put of MIDeA using a varying number of CPU processes and GPU
devices. Each process runs on a different CPU core, therefore we
can utilize all cores by creating eight processes.
For the input data stream we used synthetic network traces of
varying length with random payload. The data stream was care-
fully created to exercise most code branches, as well as different
parameters of our implementation. To simulate the multi-queue ca-
pabilities of the NIC, we loaded the network packets of the trace
ﬁle into separate queues (one for each core) using a simpliﬁed ver-
sion of the Toeplitz hash function, which is used for RSS in mod-
ern NICs [28]. This is the “ideal NIC” case, where no overhead is
302)
s
/
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
25
20
15
10
5
0
1−GPU
2−GPUs
AC−Compact
AC−Full
CPU
)
)
s
s
/
/
t
t
i
i
b
b
G
G
(
(
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
T
T
25
25
20
20
15
15
10
10
5
5
0
0
1
2
3
4
5
6
7
8
Number of Processes
512
512
1024
1024
2048
2048
4096
4096
8192
8192
16384 32768
16384 32768
Number of Packets
Number of Packets
Figure 7: Per-process transfer rate between CPU and GPU.
Figure 8: GPU throughput for AC-Full and AC-Compact.
Buffer Size
1KB 4KB 64KB 256KB 1MB 16MB
#Rules
#Patterns
#States
AC-Full
AC-Compact
Host to Device
Device to Host
2.04
2.03
7.1
6.7
34.4
21.1
42.1
23.8
44.6
24.6
45.7
24.9
8,192
193,167
1,703,023
890.46 MB
24.18 MB
Table 1: Data transfer rate between host and device (Gbit/s).
Table 2: Memory requirements of AC-Full and AC-Compact
for the default Snort rule set.
added due to the transferring of the packets from the network inter-
face to the host’s main memory. All network packets are stored in
memory, thus no blocks were transferred from disk when reading
packets. We have veriﬁed the absence of I/O latencies using the
iostat(1) tool.
5.2.1 GPU Performance
Data Transfer. Flows are transferred to each GPU device over the
shared PCIe ×16 bus. PCIe uses point-to-point serial links, allow-
ing more than one pair of devices to communicate with each other
at the same time.
Table 1 shows the transfer rate of one process for moving data to
a single GPU device, and vice versa, for different buffer sizes. We
observe that with a large buffer, the rate of data transfer to the de-
vice is over 45 Gbit/s, while the transfer rate from the device to the
host decreases to about 25 Gbit/s. This asymmetry in the data trans-
fer throughput is probably related to the chosen hardware setup
(i.e., the interconnection between the motherboard and the graphics
cards), and has been also observed by other researchers [17]. We
speculate that future motherboards will alleviate this asymmetry.
Figure 7 shows the transfer rate for a varying numbers of pro-
cesses. The transfer costs include the copy of the network packets
to the memory space of the GPU, and the copy of the results from
the GPU to the host’s memory. We can see that as the number
of processes increases, the per-process throughput sustained by the
PCIe bus slightly decreases. That is expected, since many processes
contend for the same device through the same bus link. However,
the aggregate throughput achieved by all processes increases, re-
sulting to better PCIe bus utilization. As shown in Figure 7, for
eight processes, the bidirectional PCIe throughput when using a
single GPU reaches 9.7 Gbit/s per process, which in aggregate cor-
responds to 77.8 Gbit/s for all eight processes.2 Adding one more
GPU device results to a much higher throughput of 14.1 Gbit/s per
process (113.3 Gbit/s in aggregate for all eight processes), since
each GPU device is interconnected through dedicated PCIe lanes.
Computational Throughput. Having examined the data transfer
costs, we now measure the GPU performance of the AC-Compact
and AC-Full algorithms, described in Section 3.2.2.
Figure 8 shows the sustained throughput for pattern matching
on a single GTX480. We ﬁx the packet length to 1500 bytes and
2The capacity of PCIe ×16 v2.0 is 64 Gbit/s for each direction. In
practice though, the theoretical maximum data rate diverges due to
the 8b/10b encoding at the physical layer.
vary the number of packets that are processed at once from 512 to
32,768. Our AC-Full and AC-Compact implementations achieve a
peak performance of 21.1 Gbit/s and 16.4 Gbit/s, respectively, in-
cluding the data transferring costs to and from the device. The CPU
achieves a performance of 0.6 Gbit/s for the AC-Full implementa-
tion, and thus a single GPU instance corresponds to 36.2 and 28.1
CPU cores for the AC-Full and AC-Compact implementations, re-
spectively.
As expected, AC-Full outperforms AC-Compact in all cases. The
added overhead of the extra computation that AC-Compact per-
forms in every transition decreases its performance about 30%. The
main advantage of AC-Compact is that it has signiﬁcantly lower
memory consumption than AC-Full. Table 2 shows the correspond-
ing memory requirements for storing the detection engines of a
single Snort instance. AC-Compact utilizes up to 36 times less
memory, which makes it a better ﬁt for a multi-CPU environment,
due to CUDA’s limitation of allocating a separate memory con-
text for each host thread. Using AC-Compact, a single GTX480
card can store the detection engines of about 50 Snort instances
(50 × 24.18M B ≈ 1.2GB). The remaining memory is used for
storing the contents of network packets. If AC-Full is used, only
one instance can ﬁt in device memory. In all subsequent experi-
ments we use the AC-Compact algorithm.
Utilization. We investigate the performance of the AC-Compact
algorithm further, by varying the number of CPU processes that
feed the GPU devices with data.
Figure 9(a) shows the aggregate data processing throughout of
the GPU(s) for an increasing number of CPU processes. Figure 9(b)
plots the same data normalized by the number of processes.
It
is clear that multiple processes offer an improvement even when
utilizing only one GPU device. Currently, GPUs support multi-
tasking through the use of “timesliced” context switching: each
program receives a time slice of the GPU resources and cannot be
suspended. When many processes use the same GPU device, data
transfers and GPU execution may overlap, offering better GPU uti-
lization. GPU executions have short run times, ranging from 100–
300ns per packet, and hence, the GPU device can be effectively
timesliced among the CPU processes.
We observe that with two spawned processes, the overhead of the
AC-Compact implementation increases, since the 25 Gbit/s through-
put achieved is greater than the 21.1 Gbit/s achieved by the AC-Full
algorithm, as shown in Figure 8. Increasing to eight processes, a
single GPU reaches a maximum of 48 Gbit/s throughput. The PCIe
303)
s
/
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
)
s
/
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
80
70
60
50
40
30
20
10
0
12
10
8
6
4
2
0
1−GPU
2−GPUs
)
s
/
t
i
b
G
(
t
u
p
h
g
u
o
r
h
T
16
14
12
10
8
6
4
2
0
1−GPU
2−GPUs
1
2