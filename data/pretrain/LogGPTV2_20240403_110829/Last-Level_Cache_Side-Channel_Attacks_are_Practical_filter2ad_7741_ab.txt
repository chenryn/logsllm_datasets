mappings;
5) Probing resolution.
D. Visibility of processor-memory activity at the LLC
By design, the higher-level caches, L1 and L2, will
satisfy most of the processor’s memory accesses, which
means that the LLC has less visibility into the victim’s
memory activity than the L1 caches. Since the attacker
only shares the LLC with the victim, if its manipulation
of the LLC state does not
the state of the
higher-level caches used by the victim VM, the victim’s
accesses to its interesting code or data will never reach
the LLC and will be hidden to the attacker.
impact
We leverage cache inclusiveness, which lets us re-
place victim data from the complete cache hierarchy,
without access to any of the victim’s local caches.
Conventionally, for the L1 caches,
E. Infeasibility of priming and probing the whole LLC
the PRIME+
PROBE technique primes and probes the entire L1
cache, and uses machine-learning techniques to ana-
lyze the cache footprint in order to identify spatial
patterns associated with the victim’s memory activity
[4, 8, 31, 47]. This is infeasible to achieve with ﬁne
resolution on the LLC, since it is orders of magnitude
larger than L1 caches (several MiB versus several KiB).
We overcome this challenge by ﬁrst pinpointing very
few cache sets corresponding to relevant security-critical
accesses made by the victim, and then we only monitor
those cache sets during a prime or probe step, instead
of monitoring the whole LLC.
Identifying cache sets relevant to security-critical
F.
victim code and data
How to identify cache sets relevant to a victim’s
security-critical accesses, however, is still challenging.
This is because the attacker does not know the virtual
addresses of those security-critical accesses in the vic-
tim’s address space, and has no control on how these
virtual addresses are mapped to the physical addresses.
Our solution to this challenge is to scan the whole
LLC by monitoring one cache set at a time, looking
for temporal access patterns to this cache set that are
consistent with the victim performing security-critical
accesses. The speciﬁc temporal access patterns depend
on the algorithms used. We delay the detailed discussion
of this to Section VI and Section VII, which use
a simple square-and-multiply exponentiation and the
latest sliding-window exponentiation in GnuPG as case
studies to show how algorithm-speciﬁc security-critical
lines can be identiﬁed.
G. Eviction set to occupy exactly one cache set
In order to monitor the victim’s accesses to one
speciﬁc cache set, thus pinpointing whether that cache
set is accessed by the victim, the attacker needs to be
able to occupy that speciﬁc cache set. To achieve this,
the attacker can construct an eviction set containing a
collection of memory lines in its own address space
that all map to a speciﬁc cache set. Since a cache set
contains W cache lines, the eviction set must contain
W memory lines in order to evict one complete set. As
long as the cache replacement policy replaces older lines
before recently loaded ones (e.g., with LRU replacement
policy used on Intel processors, or FIFO replacement),
touching each line in the eviction set once guarantees
that all prior data in the set has been evicted.
Constructing an eviction set for the virtually-indexed
L1 cache is trivial: the attacker has full control of the
virtual address space, and can arbitrarily choose virtual
addresses with the same set index bits. In contrast, the
LLC is physically indexed. In order to target a speciﬁc
cache set, the attacker must at least partially recover the
address mapping, which is a challenge, as the VMM’s
address-space mapping is not accessible to the attacker.
The sliced cache of modern Intel processors (Sec-
tion II-B2) further complicates the attack: even knowing
the physical address of memory lines may not be
sufﬁcient for creating the eviction sets, since the slice
id is unknown to the attacker.
In Section IV, we discuss how we construct the
eviction set using large pages and without reverse-
engineering the hash function.
H. Probing resolution
Extracting ﬁne-grained information, such as crypto-
graphic keys, requires a ﬁne probing resolution. Since
the spy process can run asynchronously, without the
the probing resolution
need to preempt
the victim,
608
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
of the LLC is not tied to victim preemption, but is
fundamentally limited only by the speed at which the
attacker can perform the probe. This is much slower
than for an L1 cache, for two reasons.
First, the LLC typically has higher associativity than
the L1 cache (e.g., 12 to 24-way versus 4 to 8-way),
hence more memory accesses are required to completely
prime or probe a cache set.
Second, the probe time increases due to the longer
access latency of the LLC (about a factor of 10 for
recent Intel processors). Even with all lines resident in
the LLC, the attacker, when performing a probe of one
LLC set, will still experience misses in the L1 and L2
caches, due to their lower associativity. Furthermore, a
miss in the LLC will cause more than 150 cycles latency
while a miss in the L1 or L2 cache has a latency of less
than 40 cycles.
As a consequence, probing an LLC set is about one
order of magnitude slower than probing an L1 cache.
In Section V, we characterize the probing resolution of
the LLC by measuring the channel capacity of an LLC
based covert channel.
IV. CONSTRUCTING THE EVICTION SET
A. Methodology
We solve the problem of hidden mappings by
utilizing large pages. As discussed in Section II,
performance-tuned applications, OSes and VMMs use
large pages to reduce TLB contention. Large-page sup-
port of the VMM allows a large page in the guest
physical memory to be backed up by a large frame in the
actual physical memory. For our purpose, large pages
eliminate the need to learn the virtual-address mapping
used by the OS and VMM: a 2 MiB page is large enough
so that all the index bits are within the page offset,
thus the cache index bits are invariant under address
translation—the LLC is effectively virtually indexed.
A side effect of large pages is a reduction of TLB
misses and thus interference. But note that large-page
mappings are only required for the attacker, we make
no assumption on how the victim’s address space is
mapped.
In recent Intel CPUs, large pages are not sufﬁcient
to locate an LLC slice, as memory lines with the same
set index bits may be located in different LLC slices.
Instead of following Hund et al. [19] in attempting
to reverse-engineer the (likely processor-speciﬁc) hash
function, we construct eviction sets by searching for
conﬂicting memory addresses. Speciﬁcally, we allocate
a buffer (backed up by large pages) of at least twice the
size of the LLC. From this buffer we ﬁrst select a set
of potentially conﬂicting memory lines, i.e., lines whose
addresses have the same set index bits (e.g., address bits
6–16, see Figure 2b).
Algorithm 1: Creating the eviction sets
input : a set of potentially conﬂicting memory lines lines
output: a set of eviction sets for lines, one eviction set for
each slice
Function probe(set, candidate) begin
read candidate;
foreach l in set do
read l;
end
measure time to read candidate;
return time > threshold;
end
randomize lines;
conﬂict_set ← {};
foreach candidate ∈ lines do
if not probe(conﬂict_set, candidate) then
insert candidate into conﬂict_set;
end
foreach candidate in lines− conﬂict_set do
if probe(conﬂict_set, candidate) then
eviction_set ← {};
foreach l in conﬂict_set do
if not probe(conﬂict_set−{l}, candidate) then
insert l into eviction_set;
end
output eviction_set;
conﬂict_set ← conﬂict_set− eviction_set;
end
end
We then use Algorithm 1 to create eviction sets for
a given set index for each slice. This ﬁrst creates a
conﬂict set that contains a subset of the potentially con-
ﬂicting memory lines such that for each slice, exactly
W memory lines in the conﬂict set map to the same
cache set. The conﬂict set is, effectively, a union of
eviction sets for all the slices (each eviction set contains
exactly W lines that map to the cache set in a slice.). The
algorithm, then, partitions the conﬂict set into individual
eviction sets, one for each slice.
The algorithm uses the function probe, which checks
whether a candidate memory line conﬂicts with a set of
lines. That is, whether accessing the set of lines evicts
the candidate from the LLC. The function ﬁrst reads
data from the candidate line, ensuring that the line is
cached. It then accesses each of the memory lines in
the set. If, after accessing the set, reading the candidate
takes a short time, we know that it is still cached and
accessing the lines in the set does not evict it. If, on the
other hand, the read is slow, we can conclude that the
set contains at least W lines from the same cache set as
the candidate, and accessing these forces the eviction of
the candidate.
The algorithm creates the conﬂict set
iteratively,
adding lines to the conﬂict set as long as the lines do
609
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
Listing 1: Code for probing one 12-way cache set
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
lfence
rdtsc
mov %eax, %edi
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
mov (%r8), %r8
lfence
rdtsc
sub %edi, %eax
not conﬂict with it. Intel’s hash function is designed
to distribute the selected potentially conﬂicting lines
evenly across all the LLC slices [20]. Hence, a buffer of
twice the size of the LLC is large enough to construct
the desired conﬂict set.
To partition the conﬂict set, the algorithm picks a
candidate from memory lines that did not make it into
the conﬂict set. The algorithm iterates over the members
of the conﬂict set, checking whether after removing the
member, the candidate still conﬂicts with the conﬂict
set. If removing the member removes the conﬂict, we
know that the member is in the same cache set of the
same LLC slice as the candidate. By iterating over all
the members of the conﬂict set we can ﬁnd the eviction
set for the cache set of the candidate.
It takes about 0.2 seconds for determining the slices
of a single set index. When the number of cores in the
processor is a power of two, the set index bits are not
used for determining the LLC slice.Therefore, given the
eviction sets for one set index, it is straightforward to
construct eviction sets for other set index, without the
need to repeat the above procedure for each set index.
Otherwise, Algorithm 1 has to be repeated for every set
index.
B. Implementing the PRIME+PROBE attack
Once eviction sets are created, we can implement
the PRIME+PROBE attack. The implementation follows
the pointer-chasing technique of Tromer et al. [34]: We
organize all the memory lines in each eviction set as a
linked list in a random order. The random permutation
prevents the hardware from prefetching memory lines
in the eviction set.
Listing 1 shows the assembly code we use to probe
in register %r8 is
one set of the cache. The input
the head pointer of the linked list, and the rdtsc
instruction (lines 2 and 17) is used to measure the time
to traverse the list. Each of the 12 mov instructions
(lines 4 to 15) reads one memory line in the eviction set,
which stores the pointer to the next memory line. Since
each mov instruction is data-dependent on the previous
one, access to the memory lines is fully serialized [28].
Upon completion, register %eax contains the measured
time.
The lfence instructions (lines 1 and 16) protect
against instruction re-ordering and out-of-order comple-
tion. It ensures that all preceding load instructions com-
plete before progressing, and that no following loads can
begin execution before the lfence. Intel recommends
using the cpuid instruction for full serialization of the
instruction stream [30]. However, as noted by Yarom
and Falkner [45], because the cpuid instruction is
emulated by the VMM, it is less suitable for the purpose
of measuring the timing in our attack.
C. Optimizations
Several optimizations on the scheme above are
possible, to minimize the probe time as well as its
variations.
Thrashing: As mentioned in Section II, probing the
cache implicitly primes it for the subsequent observa-
tion. However, due to the cache’s (approximate) LRU
replacement policy, using the same traversal order in
the prime and probe stages may cause thrashing, i.e.,
self-eviction by the attacker’s own data: If the victim
evicts a line, it will be the attacker’s oldest. On probing
that evicted line, it will evict the second-oldest, leading
to a miss on every probe. By using a doubly-linked list
to reverse the traversal order during the probe stage, we
minimize self-evictions, as the oldest line is accessed
last [34].
Interaction with higher-level caches: The attacker
data is also partially cached in higher-level caches.
Retrieving data from the higher cache levels is faster
than reading it from the LLC, hence variations in the
L1 and L2 contents affect the cache probe time and
introduce noise to its measurements. For example, with
an 8-way L1 cache and a timing difference of about
30 cycles between L1 access and LLC access,
the
total variation can reach 240 cycles—much larger than
the difference between LLC and memory access. The
interaction of higher-level caches tends to have less
effect when the associativity of the LLC is much higher
than that of the L1 and L2 caches, since the L1 and L2
caches can only hold a small portion of the eviction
set for the LLC. An optimization is that instead of
measuring the total probe time, one can measure the
time of every load from the eviction set. This approach
reduces the noise from the multiple levels of caching,
but at the cost of an increased probe time.
610
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
V. PROBING RESOLUTION VIA CHANNEL CAPACITY
MEASUREMENTS
Next, we study the probing resolution and the ef-
fectiveness of our proposed technique using a willing
transmitter, i.e., by constructing a covert channel and
characterizing the channel capacity. The covert channel
protocol is shown in Algorithm 2. It is similar to the
timing-based cache-channel protocol of Wu et al. [42]
but more efﬁcient, because we use the technique from
Section IV to create an exact eviction set.
Since the sender and the receiver execute concur-
rently without synchronization, we use a return-to-zero
(RZ) self-clocking encoding scheme [14]. The sender
and the receiver each allocate a buffer of at least the
size of the LLC, backed up by large pages. They agree
on two arbitrarily chosen cache-set indices (preferably