the shadow variation, then we may reconstruct the exact positions
of the shadow keypoints.
IR angle. Fig. 3(e,f) show two deformed shadows under different
IR angles. In general, a larger IR angle stretches the shadow more,
and causes the curtain itself to create shadows. For example, the
victim’s hand can be identified at 30◦, but occluded by the curtain’s
shadow at 60◦.
IR distance. Longer IR distance has a much lower shadow con-
trast and size (Fig. 3(g,h)). Additionally, combined with the curtain
deformation, the size variation also changes the shape of the shadow.
For example, the hand of the shadow can be clearly seen when the
IR source is near but distorted afar.
4.2 Design Motivation and Details
To detect the keypoints under shadow deformation, our DeShaNet
solution framework incorporates three sub-modules: 1. An A-LSTM
and scene feature fusion module, which can extract the features re-
lated to scene parameters, and hence adapt to the scene variations. 2.
A trajectory aware module which introduces visually independent
features, such as coordinate vectors, to improve the stability under
fuzzy shadows with varying deformation. 3. A condition attention
module, which improves the detection robustness under dynamic
situations. Next we describe each module in detail.
Choice of feature extraction backbone. The state-of-the-art
video keypoint detection models, such as 3D mask R-CNN, do not
fit our scenario because their region proposal network does not
support global image feature, which is essential to solve the shadow
deformation problem. The global image feature refers the high-level
image feature with acceptance field covering the overall image. In
contrast, our DeShaNet architecture (Fig. 4) aims to capture global
image features, which contain rich information related with the
scene parameters.
Specifically, we use pretrained convolutional neural network
(CNN) stacks from the Resnet-50 [14] to extract global image fea-
tures 𝐹𝑣. Since the features of IR shadows are very different from
RGB images, the pretrained CNN backbone needs to be fine-tuned
on a large number of IR shadow images. To reduce the amount of
new training data needed, we freeze the parameters in the bottom
layers of the pretrained CNN backbone and fine-tune the parame-
ters in the top layers. To find the best balance between generaliza-
tion ability and training data requirements, we try different com-
binations of frozen layers and fine tuning layers, and empirically
choose the combination (freezing the first 3 layers and fine-tuning
the rest) that achieves best performance when tested on real data.
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2783Figure 4: Overall architecture of the DeShaNet.
Scene feature fusion module (SFFM). We design the SFFM to
extract the correlations between the three major scene parameters
and dynamic shadow features (Section 4.1). SFFM builds on the
video feature extraction module (A-LSTM). The visual feature 𝐹𝑎
output by the A-LSTM implicitly contains information of the scene
parameters. We thus feed 𝐹𝑎 into 3 MLP branches, each of which
learns to predict one scene parameter, as shown in Fig. 4. All the
parameters (IR angle/distance and curtain deformation level) are
normalized.
Since the shadow deformation is a product generated by all
scene parameters, knowing one of them can help estimate others.
Therefore, the SFFM adopts a two-stage architecture. In the first
stage, the output features are trained to learn the scene parameters.
Then these outputs are fused by concatenation and fed into the
second stage, which is trained to reach higher accuracy. The final
output features are fused again to produce the scene features 𝐹𝑐,
which are then fed into the condition attention module for feature
fusion. In our implementation, the first stage comprises 2 layers
of MLP with kernel size of 64, and the second has 2 layers of MLP
with kernel size of 96.
Trajectory aware module. To deal with some extreme cases
when the shadow parts are deformed severely or merged together,
our trajectory aware module leverages the motion continuity so
that the keypoints of deformed shadows can be inferred from his-
torical keypoint trajectory explicitly. This module consists of a
coordinate estimation module (CEM) and an M-LSTM. The CEM
predicts the keypoint coordinates from historical images 𝐶𝑁−𝑀
-
𝐶𝑁−1
by CNN stacks and MLP layers. It comprises 3 layers of MLPs
𝑖
with hidden size of 64, 96 and 18. The next step is to predict the
coordinate feature 𝐹𝑚 of the current image from these coordinates,
which is essentially a sequence to sequence learning problem [35].
Therefore, it is natural to use the LSTM for this task, which excels
at modeling temporal information from long sequences. This LSTM
model (referred to as M-LSTM) comprises two layers of LSTM cells
with hidden layer size of 96.
Condition attention module (CAM). Three feature vectors
(𝐹𝑣, 𝐹𝑚 and 𝐹𝑐) are involved for the final keypoint coordinates pre-
diction. However, these feature vectors have completely different
physical meanings and may become less reliable under specific situ-
ations. Specifically, the visual feature vector 𝐹𝑣 will be less reliable
when the shadows become fuzzy due to high dynamic movement
or severe occlusion. When the shadow movement speed becomes
relatively slow, the trajectory feature vector 𝐹𝑚 does not contain
𝑖
much useful information. The scene feature vector 𝐹𝑐 should have
less impact when the scene parameters do not cause much shadow
deformation. To fuse these highly heterogeneous feature vectors,
we custom build an attention module called CAM. The CAM com-
prises of a feed-forward network to calculate the fusion weights
¯𝑟1, ¯𝑟2 and ¯𝑟3, which are then multiplied with the 3 feature vectors
and fed into an MLP layer to predict the final coordinates 𝐶𝑁
𝑎 . The
size of the feed-forward network is the sum of the 3 feature vectors
and the size of the MLP layer is the same as one of the feature
𝑎 contains the normalized 2D (𝑥, 𝑦) coordinates for
vectors. The 𝐶𝑁
9 keypoints.
5 SCENE CONSTRUCTOR DESIGN
DeShaNet can recover the 2D keypoint positions from the shadow,
but these still need to be converted to a 3D skeleton to enable human
activity recognition. Unlike classical 3D skeleton detection tasks
in computer vision, restoring 3D skeletons from the 2D deformed
shadow images is essentially an undetermined problem. Our scene
constructor framework aims to overcome this hindrance by filling
in environmental information. It estimates the scene parameters
by modeling and simulating the shadow projection process in a
virtual 3D environment. The 3D skeleton of the victim is derived as
a byproduct of this process. The overall architecture of the scene
constructor is shown in Fig. 5.
5.1 Design principle of the scene parameter
estimators
The scene constructor consists of 3 scene parameter estimators
which we detail below. To ease the exposition, we summarize the
related math symbols in Table 1.
IR Source Parameter Estimator (IRSPE). Intuitively, the IR
distance affects the shadow size and the IR angle causes horizontal
stretching. Therefore, by analyzing the shadow distortion, we can
infer the IR distance/angle. As shown in Fig. 6, we denote the
IR distance as ℎ, and denote the horizontal distance between the
shadow edge and the IR source as 𝑥. Further, we denote the angle of
the IR source relative to the edge of shadow as 𝜃. Through simple
geometries, we have:
(1)
(2)
𝑥 = ℎ ∗ 𝑡𝑎𝑛(𝜃).
Let the width of the shadow be Δ𝑥, then we have:
𝑥 + Δ𝑥 = ℎ ∗ 𝑡𝑎𝑛(𝜃 + Δ𝜃).
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2784Table 1: Annotations and abbreviations in this paper.
Figure 5: Overall architecture of the scene constructor.
Limb length estimator (LLE). We introduce an LLE scheme to
reduce the skeleton estimation error due to the limb length variation
across people. Intuitively, the 3D limb length is proportional to
the 2D projected limb length on the shadow. However, two major
factors can weaken this correlation: 1. Body posture. The varying
distance between two keypoints on the shadow reaches maximum
when their connecting line is parallel to the window curtain. At this
time, the distance between all pairs of keypoints have exactly the
same deformation. Therefore, we can use the maximum 2D distance
of all pairs of keypoints to approximate the 3D limb length. 2. The
residual errors of the DeShaNet keypoint output. It is well known
that the keypoint detection errors of DL models follow a Gaussian
distribution [18]. The 2D projected limb length can be calculated
from the distance between keypoint locations. Therefore, the true
value of limb length should lies in the top-𝑁 2D distance instead
of top-1. We then take the median of the top-𝑁 distances as the
estimated 2D limb length.
For match score calculation, we use the ratio between the abso-
lute limb length and an anchor length as metric. Since the distance
between the head and the shoulder is relatively stable under dif-
ferent body postures and viewpoints, we use it as the the anchor
length.
Curtain deformation estimator (CDE). The curtain deforma-
tion stretches the shadow, making the shadowed body parts merge
together or changing their shapes. To overcome this issue, our CDE
scheme explicitly reconstructs the deformed surface of the curtain
in a virtual environment, and reproduces the same deformation ef-
fect as that observed by the attacker’s camera. As the victim moves
across locations, the movement of the observed shadow exhibits
different levels of fluctuations due to curtain deformation. A wrin-
kled curtain surface will fluctuate the shadow moving speed more
than a smooth curtain. Fig. 7 clearly showcases this relationship.
To simplify the explanation of CDE, we define the local moving
speed of keypoint 𝑙 as 𝑣𝑙, local deformation angle 𝜃𝑙, mean angle
between curtain and attacker 𝜃𝑚, mean moving speed between
curtain and attacker 𝑉𝑚. Intuitively, when 𝑣𝑙 decreases, 𝜃𝑙 becomes
larger. During the aforementioned simulation process, the 𝜃𝑙 can be
altered by modifying the coordinates of the vertices of the virtual
curtain. We iteratively adjust 𝜃𝑙 to make the distribution of all
Symbols Description
𝐶𝑘, 𝐶
′
𝑘
𝑊ℎ
ℎ
𝑥𝑟
from
keypoint
coordinates
Shadow
De-
ShaNet/simulation, where 𝑘 refers the keypoint
id.
Input sequence of the head width on the shadow
Distance between the IR source and the curtain
horizontal distance between the IR source and the right
edge of the curtain
𝑣𝑙, 𝑣
𝐶𝑣
𝑤𝑘
′
𝑝 Relative limb length from input/simulation, where 𝑝
𝐿𝑝, 𝐿
′
𝑙
refers the index of limb pairs
Local shadow speed from input/simulation
Curtain vertex coordinates
Rotation angle of all 3D skeleton joints, where k refers
to keypoint id
Rotational angle of the overall skeleton
3D skeleton position coordinates
𝑤𝑎
𝐶𝑠
Then we subtract equation (2) by (1):
Δ𝑥 = ℎ ∗ (𝑡𝑎𝑛(𝜃 + Δ𝜃) − 𝑡𝑎𝑛(𝜃)) ≈ ℎ ∗ (1/𝑐𝑜𝑠(𝜃)2).
(3)
Equation (3) shows the relationship between the shadow width,
the IR source distance and the IR source angle. Since the shadow
width Δ𝑥 varies at different locations, we can infer the ℎ and 𝜃 by
sampling the Δ𝑥 when the victim moves across multiple locations.
However, estimating ℎ and 𝜃 simultaneously is an undetermined
problem. Our IRSPE adopts an iterative simulation driven solution
with the following high level work flow. It simulates a massive
number of distance and angle settings of the IR source. For each
simulation sample, the shadow is projected on a virtual curtain and
its width is compared with the realistic shadow width. Then, we
calculate the difference between the simulated shadow width and
the realistic shadow width. Finally, the IR source angle/distance
with the smallest difference is regarded as the optimal estimation.
In practice, the shadow width can also be impacted by curtain
deformation, the distance variation between the victim and the
curtain, and body postures. We thus introduce the following mech-
anisms to counteract these factors.
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2785simulated speeds 𝑣𝑠 most close to the distribution of 𝑣𝑙. This in turn
leads to a curtain deformation closest to reality.
5.2 The shadow simulator
The shadow simulator aims to simulate shadows and make their
keypoint coordinates match the input shadow keypoint coordinates
which are derived from the DeShaNet. It achieves this by placing
and modifying virtual components in a Unity 3D environment,
including the IR light source, body skeleton model and window
curtain. Among these components, the IR light source, the window
curtain and the skeleton limb length are estimated by the afore-
mentioned 3 scene parameter estimators, respectively. The shadow
simulator mainly aims to derive the parameters of a 3D dummy
skeleton, including: (i) Skeleton rotational angles: the rotational
angles of the overall skeleton (𝑤𝑎) and of all keypoints (𝑤𝑘). (ii)
Skeleton position 𝐶𝑠: the (𝑥, 𝑦) coordinates of the overall skeleton.
The implementation of the shadow simulator follows 2 steps:
parameter generation and match score calculation.
1. Parameter generation: For each parameter, the shadow simula-
tor exhaustively tries all possible values in empirically predefined
scopes and intervals (listed in Table 2).
match score will be used as the optimal estimation.
ShaNet: 𝑆 =  ||𝐶𝑘 − 𝐶
2. Match score calculation: After each parameter is updated, the
shadow is refreshed accordingly. We then calculate the match score,
defined as the difference between the keypoint coordinates of the
′
𝑘 and that of the input shadow 𝐶𝑘 from De-
simulated shadow 𝐶
′
𝑘||2. The parameter set with the lowest
To reduce the huge search space caused by possible combination
of skeleton parameters, the shadow simulator groups the parame-
ters according to each parameter’s impacts on others, and updates
them sequentially in descending order. The impact of a skeleton pa-
rameter is determined by its number of leaf nodes. For example, the
rotational angle of the overall skeleton 𝑤𝑎 affects all the skeleton
keypoint coordinates, so it has 9 leaf nodes. On the other hand, the
rotational angle of the wrist 𝑤7/𝑤1 has the lowest impacts because
it does not affect other skeleton parameters, i.e., it has 0 leaf nodes.
We list all the parameters according to their impacts in descending
order as follows: 𝐶𝑠, 𝑤𝑎, 𝑤2, 𝑤3, 𝑤4,𝑤5, 𝑤6,𝑤7,𝑤1,𝑤8,𝑤9, where
the keypoint indices from 1 to 9 are head, l-shoulder, r-shoulder,
l-elbow, r-elbow, l-wrist, r-wrist, l-thigh, r-thigh, respectively.
6 SYSTEM IMPLEMENTATION
Dataset. We create a realistic indoor scene to perform the IRSA and
collect data, as shown in Fig. 8. We recruit different subjects and
conduct various activities between the IR source and the curtain.
To simplify the ground-truth data collection, we use Kinect v2 as
IR source, which is equipped with a similar IR emitter as typical
security cameras (Sec. 7.3). We then use a commercial software,
Brekel Body v2 [6], which is designed for Kinect, to derive the
ground-truth body skeletons from Kinect videos. The IR shadow
is captured by a smart home camera (Wyze [5]) on the other side
of the curtain. The curtain can be manually adjusted to arbitrary
shapes to simulate different curtain patterns by clamps and fixtures
on the wall.
We collect over 40 groups of data, each of which records a 1-
2 minutes IR shadow video and the corresponding 3D skeleton
ground-truth, along with a visible body movement video (captured
Figure 6: Design princi-
ple of the IRSPE.
Figure 7: Design principle of the
CDE.
Table 2: Empirical scopes and intervals of all parameters.
𝑤𝑘
-120°
∼120°
Scope
Interval 1°
𝐶𝑠
𝑤𝑎
-5m∼5m -180°
∼180°
0.1m 1°
ℎ
𝑥𝑟
𝐶𝑣
1m∼5m -5m∼5m -0.1m
∼0.1m
0.1m 0.01m
0.1m
by the Kinect). We manually label the shadow keypoints for each
image in the IR videos by comparing it against the body movement
video. The IR video records the shadow of the subject perform-
ing multiple kinds of activities, including armpit stretching, nose