with the highest predicted rating scores as ﬁller items for fake
users.
Note that, the internal structure and hyperparameter settings
of the poison model are consistent with the target recommender
system. Moreover, its training datatset should be identical to
the original training dataset of the target system initially and
can be inserted fake users one by one to simulate attack results.
In order to make the poison model change towards our desired
objective, we need to deﬁne an effective loss function to update
the model iteratively. According to the optimization problem in
Eq. (5), we propose the following loss function for the poison
model in the attack:
(6)
where L is the loss function chosen in the process of training
the original recommender system, e.g., the binary cross entropy
l = L + λ · G[(cid:98)y(v)],
over the whole training dataset, G[(cid:98)y(v)] correlates strongly
with our attack goal, and λ > 0 is a coefﬁcient that trades off
between the model validity and the attack objective, which al-
lows us to generate the poison model close to the recommender
system trained under normal circumstances, while achieving
the validity correlates with L and
our attack goal. Here,
measures the degree to which the model accurately predicts
user preferences on the validation dataset. We use the predicted
rating score vector (cid:98)y(v) of fake user v to replace v’s real
rating score vector y(v) according to the correlation between
them such that we can avoid high order gradient calculation,
which is really time-consuming. Note that, if the validity of
the poison model is much lower than that of a model trained
normally with the same dataset and the original loss function
(i.e., L), it is less likely that the poison model approximates the
ﬁnal state of a compromised target recommender system since
the target recommender system will always use a validation
dataset to guarantee its best performance in the normal model
training process. Thus, it is necessary to ensure the validity of
the poison model during the attack. In order to make the poison
model better simulate the results of the poisoning attack, we
design two stages of training: i.e., pre-training and poison
training.
Pre-training. The poison model will be randomly initialized
at ﬁrst and trained on its training dataset with the same
loss function (i.e., L) as the target recommender system.
After enough iterations, the poison model will be similar to
the recommender system obtained from the normal training,
ensuring the validity of the model. We can utilize this model
to start poison training subsequently.
Poison Training. The poison model in this stage will use Eq.
(6) as loss function and be trained repeatedly w.r.t all model
parameters inside it with the back-propagation method. We
select the initial λ such that the loss of the poison model
on the validation dataset and the loss that models the attack
effectiveness are roughly in the same order of magnitude. In
the training process, the poison model will get closer to our
attack goal and eventually become an ideal state of the target
recommender system. We can then use the poison model to
help the item selection process for fake users.
D. Selecting Filler Items
Now we can select ﬁller items for each fake user based on
predicted ratings generated by the acquired ﬁnal poison model
in the last poison training process. Note that, items with higher
scores in the user’s predicted rating score vector given by the
recommender system tend to have greater relevance to the user
since the system reduces the entropy between users’ predicted
score vectors and real rating score vectors during the training
process. Thus, as long as we get a reasonable poison model,
6
pi = pi · δ,
we can obtain the predicted rating score vector (cid:98)y(v) for fake
user v according to the model, and the top-n items other than
target item t will be selected as ﬁller items for fake user v.
However, as the datasets used in recommender systems
are usually very sparse and the models trained from the data
have high randomness, the recommendation results of deep
learning based recommender systems for speciﬁc users and
items tend to be unstable, which means fake users obtained
from the poison model may not be good choices. Thus, if
we always directly use the predictions of the poison model
to select ﬁller items for fake users, we are more likely to
gradually deviate from the right direction. In order to avoid
this, we develop a concept of selection probability, i.e., the
probability of an item being selected as ﬁller items. We deﬁne
a selection probability vector as p = {p1, p2, . . . , pN}, each
element of which represents the selection probability of the
corresponding item. If item i is selected as ﬁller item, pi will
change as follows:
(7)
where 0 ≤ δ ≤ 1 is an attenuation coefﬁcient that reduces the
selection probabilities of selected items. The more times an
item is selected as ﬁller item, the lower its selection probability.
Note that, p is initialized to a vector with all element values
of 1.0 at ﬁrst. If all elements in p are below 1.0 after the
poisoning attack, p will be initialized again. After the poison
rv =(cid:98)y(v)pT.
model gives predicted rating score vector(cid:98)y(v) for fake user v,
we combine it with p to guide ﬁller item selection as follows:
(8)
According to Eq. (8), we select these items with the highest
n scores in rv for fake user v as ﬁller items and update the
corresponding selection probabilities using Eq. (7). The use
of selection probability refrains from repeated selection of
speciﬁc items, and provides greater chance of being selected
for more candidate items, which allows the target item to
build potential correlations with more other items and makes
our attack more likely to be effective globally. As for the
recommender systems with sparser datasets, which means
greater uncertainty of recommendation results, we recommend
to choose a smaller δ to strengthen the internal system con-
nectivity, i.e., the target item can correlate more other items,
which avoids local optimal results and improves the attack
performance. Combining above insights, we can effectively
solve the optimization problem.
The heuristics that solve the optimization problem are
shown in Algorithm 1. Note that, since our attack is not des-
ignated to speciﬁc deep learning recommendation systems and
can be generalized to any deep learning based recommender
system, the algorithm can opt to solve the problems in various
systems. Our item selection follows three steps. First, we use
fake user v. Second, we compute the element-wise product of
the poison model to predict a rating score vector (cid:98)y(v) for a
(cid:98)y(v) and a selection probability vector p as an adjusted rating
score vector rv. Third, we select the n non-target items with
the largest adjusted rating scores as the ﬁller items for v. For
each ﬁller item i for v, we decrease its selection probability pi
by multiplying it with a constant (e.g., 0.9) such that it is less
likely to be selected as a ﬁller item for other fake users. We use
the selection probability vector to increase the diversity of the
fake users ﬁller items so that the target item can be potentially
Algorithm 1 Our Attack Method
Input: User-item interaction matrix Y, target item t, param-
eters m, n, K, λ, η, κ.
Output: m fake users v1, v2, . . . , vm.
1: // Add fake users one by one
2: for v = v1, v2, . . . , vm do
3:
size.
Initialize poison model Mp with expanding input user
4:
5:
6:
7:
8:
9:
10:
11:
Add the rating tuple (v, t, rmax) to Y.
Pre-train Mp on Y with L.
Start poison training to get the ﬁnal poison model Mp.
Use Mp to give predicted rating score vector (cid:98)y(v) for
Get rv using Eq. (8).
Choose these items other than t with the highest n
user v.
scores in rv as ﬁller items.
Update p using Eq. (7).
Generate rating scores for chosen ﬁller items, consti-
Y ← Y ∪ y(v).
tuting rating score vector y(v) for user v.
12:
13: end for
14: return y(v1), y(v2), . . . , y(vm).
correlated with more items. Note that we assume the same
n is used for each fake user. However, the attacker can also
use different number of ﬁller items for different fake users. In
particular, an attacker can use our attack to add ﬁller items for
a fake user one by one and stop adding ﬁller items once the hit
ratio of the target item begins to decrease. Finally, we generate
rating scores for each ﬁller item according to their previous
ﬁtted normal distributions to ensure their scores much similar
to other normal ratings, which also be used to effectively evade
detection. We will elaborate on the detection performance in
Section VI. Note that, to speed up the process of generating
all fake users, we can also choose to generate s (s > 1) fake
users each time at the cost of reducing the ﬁne-grained control
on the attack effectiveness.
V. EXPERIMENTS
In this section, we ﬁrst present the experimental setup.
Second, we evaluate the effectiveness and the transferability
of our poisoning attacks.
A. Experimental Setup
Datasets. We use three real-world datasets to perform our poi-
soning attacks. They are MovieLens-100K [19], MovieLens-
1M [19] and Last.fm [2], two different types of typical recom-
mender system datasets. MovieLens-100K is a classic movie
dataset which consists of 943 users, 1,682 movies and 100,000
ratings ranging from 1 to 5. Each user has at least 20 ratings in
this dataset. Similarly, MovieLens-1M is a larger movie dataset
including 6,040 users, 3,706 movies and 1,000,209 ratings
ranging from 1 to 5. Last.fm is a music dataset which contains
1,892 users, 17,632 music artists and 186,479 tag assignments.
A user can assign a tag to a music artist, which can be seen as
a positive interaction between them. Last.fm is a pure implicit
dataset as the tag assignments cannnot be quantiﬁed with
numerical values. We apply several data processing strategies
to make it suitable for our experiments. First, we binarize
7
their interactions as 1.0 for positive ones and 0.0 for others,
which can be seen as implicit ratings. Second, we drop the
duplicates in the datasets as one user might assign multiple
tags to one artist. Third, as the obtained dataset is still sparse,
we iteratively ﬁlter the dataset to make sure the remaining in
the dataset has at least 10 ratings for each user and item (i.e.,
artist) to avoid the “cold start” problem. We end up with a
dataset of 701 users, 1,594 items and 36,626 ratings.
Note that we use implicit training dataset for our target
recommender system (i.e., NeuMF), so we also project the
ratings in MovieLens-100K and MovieLens-1M to 1.0 when
they are larger than 0 and 0.0 otherwise. For a binary implicit
rating score, 1.0 indicates that the user has rated the item, but
it does not necessarily represent that the user likes the item.
Likewise, an implicit rating score of 0 does not necessarily
represent that the user dislikes the item. Our attack can be
also applied to recommender systems based on explicit ratings
since the ratings can be normalized between 0 and 1 before
training for such datasets.
Target Recommender System. In our experiments, we evalu-
ate the effectiveness of the attacks by using Neural Matrix
Factorization (NeuMF) as the target recommender system.
Note that, collaborative ﬁltering systems are one of the most
popular and effective recommender systems in the real world.
Most websites (e.g., Amazon, YouTube, and Netﬂix) utilize
collaborative ﬁltering as a part of their recommender systems.
Moreover, Neural Collaborative Filtering (NCF) is the typical
representative of deep learning based recommender systems.
Thus, we choose NeuMF, an instantiation of NCF, as the target
recommender system since matrix factorization is the most
popular technique of collaborative ﬁltering.
Baseline Attacks. We compare our poisoning attack to several
existing poisoning attacks. In all these attacks, an attacker in-
jects m fake users to the target recommender system. Different
attacks use different strategies to select ﬁller items for fake
users.
1)
2)
Random Attack: In a random attack, the attacker
randomly chooses n ﬁller items for each fake user.
If the training dataset is explicit, the attacker will
ﬁt normal distributions on the initial user-item in-
teraction matrix to generate new continuous rating
scores for ﬁller items. These rating scores will then
be projected to discrete integer numbers if necessary.
Even if the training data is implicit, if the initial form
of the dataset collected by the recommender system
is explicit, the attacker still needs to use the same
method to generate rating scores for ﬁller items to
evade detection.
Bandwagon Attack: In a bandwagon attack,
the
popularity of items plays a role in the selection of
ﬁller items. We use the average score of the item to
represent its popularity on the explicit dataset, and the
frequency of the item to represent its popularity on
the implicit dataset. We randomly choose n × 10%
items from the set of 10% items with the highest
popularity and n × 90% items among the left unse-
lected items to constitute all ﬁller items. Then we can
generate rating scores for ﬁller items with the same
method in random attacks.
3)
Poisoning Attack to MF-based Recommender Sys-
tems (MF Attack) [12], [28]: MF attack is one of
effective poisoning attacks on the most widely used
recommender systems, namely matrix-factorization
(MF) based recommender systems. Note that MF
is a traditional and non-deep learning approach for
recommender systems, and our work is the ﬁrst to
implement well-designed poisoning attacks on deep
learning based recommender systems, and the target
recommender system we conduct experiments on is
generally a special kind of matrix-factorization-based
recommender systems. Speciﬁcally, we use the PGA
attack in [28] as the baseline MF attack in our
experiments. In an MF attack, the attacker will initial-
ize a generalized user-item interaction matrix based
on the training dataset at ﬁrst and then implement
optimized poisoning attack on it. We will inject the
fake users generated by MF attack into our target
deep learning based recommender system to evaluate
the effectiveness of this poisoning attack and compare
with other poisoning attacks. Note that, the existing
poisoning attack [12] to matrix-factorization-based
recommender systems cannot be directly applied be-
cause it requires deriving the inﬂuence function for
NCF, which can be an interesting topic for future
work. Thus, we ﬁnally choose the PGA attack in [28]
as the MF attack to conduct our experiments.
Target Items. We evaluate two types of target items in our
work, i.e., random and unpopular target items. Random target
items are sampled uniformly at random from the whole item
set, while unpopular items are collected randomly from those
items with less than 6 ratings, 10 ratings and 12 ratings for the
ML-100K (i.e., MovieLens-100K), ML-1M (i.e., MovieLens-
1M), and Music (i.e., Last.fm) datasets, respectively. To make
our results more convincing, we sample 10 instances for
each kind of target items by default and will average their
experimental results respectively.
Evaluation Metrics. We use the hit ratio of target item t (i.e.,