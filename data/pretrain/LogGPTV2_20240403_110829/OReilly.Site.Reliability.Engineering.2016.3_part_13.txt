Another way to understand the development of our attitude toward automation, and
when and where that automation is best deployed, is to consider the history of the
development of our cluster management systems.6 Like MySQL on Borg, which
demonstrated the success of converting manual operations to automatic ones, and the
cluster turnup process, which demonstrated the downside of not thinking carefully
enough about where and how automation was implemented, developing cluster man‐
6 We have compressed and simplified this history to aid understanding.
Borg: Birth of the Warehouse-Scale Computer | 81
agement also ended up demonstrating another lesson about how automation should
be done. Like our previous two examples, something quite sophisticated was created
as the eventual result of continuous evolution from simpler beginnings.
Google’s clusters were initially deployed much like everyone else’s small networks of
the time: racks of machines with specific purposes and heterogeneous configurations.
Engineers would log in to some well-known “master” machine to perform adminis‐
trative tasks; “golden” binaries and configuration lived on these masters. As we had
only one colo provider, most naming logic implicitly assumed that location. As pro‐
duction grew, and we began to use multiple clusters, different domains (cluster
names) entered the picture. It became necessary to have a file describing what each
machine did, which grouped machines under some loose naming strategy. This
descriptor file, in combination with the equivalent of a parallel SSH, allowed us to
reboot (for example) all the search machines in one go. Around this time, it was com‐
mon to get tickets like “search is done with machine x1, crawl can have the machine
now.”
Automation development began. Initially automation consisted of simple Python
scripts for operations such as the following:
• Service management: keeping services running (e.g., restarts after segfaults)
• Tracking what services were supposed to run on which machines
• Log message parsing: SSHing into each machine and looking for regexps
Automation eventually mutated into a proper database that tracked machine state,
and also incorporated more sophisticated monitoring tools. With the union set of the
automation available, we could now automatically manage much of the lifecycle of
machines: noticing when machines were broken, removing the services, sending
them to repair, and restoring the configuration when they came back from repair.
But to take a step back, this automation was useful yet profoundly limited, due to the
fact that abstractions of the system were relentlessly tied to physical machines. We
needed a new approach, hence Borg [Ver15] was born: a system that moved away
from the relatively static host/port/job assignments of the previous world, toward
treating a collection of machines as a managed sea of resources. Central to its success
—and its conception—was the notion of turning cluster management into an entity
for which API calls could be issued, to some central coordinator. This liberated extra
dimensions of efficiency, flexibility, and reliability: unlike the previous model of
machine “ownership,” Borg could allow machines to schedule, for example, batch and
user-facing tasks on the same machine.
82 | Chapter 7: The Evolution of Automation at Google
This functionality ultimately resulted in continuous and automatic operating system
upgrades with a very small amount of constant7 effort—effort that does not scale with
the total size of production deployments. Slight deviations in machine state are now
automatically fixed; brokenness and lifecycle management are essentially no-ops for
SRE at this point. Thousands of machines are born, die, and go into repairs daily with
no SRE effort. To echo the words of Ben Treynor Sloss: by taking the approach that
this was a software problem, the initial automation bought us enough time to turn
cluster management into something autonomous, as opposed to automated. We
achieved this goal by bringing ideas related to data distribution, APIs, hub-and-spoke
architectures, and classic distributed system software development to bear upon the
domain of infrastructure management.
An interesting analogy is possible here: we can make a direct mapping between the
single machine case and the development of cluster management abstractions. In this
view, rescheduling on another machine looks a lot like a process moving from one
CPU to another: of course, those compute resources happen to be at the other end of
a network link, but to what extent does that actually matter? Thinking in these terms,
rescheduling looks like an intrinsic feature of the system rather than something one
would “automate”—humans couldn’t react fast enough anyway. Similarly in the case
of cluster turnup: in this metaphor, cluster turnup is simply additional schedulable
capacity, a bit like adding disk or RAM to a single computer. However, a single-node
computer is not, in general, expected to continue operating when a large number of
components fail. The global computer is—it must be self-repairing to operate once it
grows past a certain size, due to the essentially statistically guaranteed large number
of failures taking place every second. This implies that as we move systems up the
hierarchy from manually triggered, to automatically triggered, to autonomous, some
capacity for self-introspection is necessary to survive.
Reliability Is the Fundamental Feature
Of course, for effective troubleshooting, the details of internal operation that the
introspection relies upon should also be exposed to the humans managing the overall
system. Analogous discussions about the impact of automation in the noncomputer
domain—for example, in airplane flight8 or industrial applications—often point out
the downside of highly effective automation:9 human operators are progressively
more relieved of useful direct contact with the system as the automation covers more
and more daily activities over time. Inevitably, then, a situation arises in which the
automation fails, and the humans are now unable to successfully operate the system.
7 As in a small, unchanging number.
8 See, e.g., https://en.wikipedia.org/wiki/Air_France_Flight_447.
9 See, e.g., [Bai83] and [Sar97].
Reliability Is the Fundamental Feature | 83
The fluidity of their reactions has been lost due to lack of practice, and their mental
models of what the system should be doing no longer reflect the reality of what it is
doing.10 This situation arises more when the system is nonautonomous—i.e., where
automation replaces manual actions, and the manual actions are presumed to be
always performable and available just as they were before. Sadly, over time, this ulti‐
mately becomes false: those manual actions are not always performable because the
functionality to permit them no longer exists.
We, too, have experienced situations where automation has been actively harmful on
a number of occasions—see “Automation: Enabling Failure at Scale” on page 85—but
in Google’s experience, there are more systems for which automation or autonomous
behavior are no longer optional extras. As you scale, this is of course the case, but
there are still strong arguments for more autonomous behavior of systems irrespec‐
tive of size. Reliability is the fundamental feature, and autonomous, resilient behavior
is one useful way to get that.
Recommendations
You might read the examples in this chapter and decide that you need to be Google-
scale before you have anything to do with automation whatsoever. This is untrue, for
two reasons: automation provides more than just time saving, so it’s worth imple‐
menting in more cases than a simple time-expended versus time-saved calculation
might suggest. But the approach with the highest leverage actually occurs in the
design phase: shipping and iterating rapidly might allow you to implement function‐
ality faster, yet rarely makes for a resilient system. Autonomous operation is difficult
to convincingly retrofit to sufficiently large systems, but standard good practices in
software engineering will help considerably: having decoupled subsystems, introduc‐
ing APIs, minimizing side effects, and so on.
10 This is yet another good reason for regular practice drills; see “Disaster Role Playing” on page 401.
84 | Chapter 7: The Evolution of Automation at Google
Automation: Enabling Failure at Scale
Google runs over a dozen of its own large datacenters, but we also depend on
machines in many third-party colocation facilities (or “colos”). Our machines in these
colos are used to terminate most incoming connections, or as a cache for our own
Content Delivery Network, in order to lower end-user latency. At any point in time, a
number of these racks are being installed or decommissioned; both of these processes
are largely automated. One step during decommission involves overwriting the full
content of the disk of all the machines in the rack, after which point an independent
system verifies the successful erase. We call this process “Diskerase.”
Once upon a time, the automation in charge of decommissioning a particular rack
failed, but only after the Diskerase step had completed successfully. Later, the decom‐
mission process was restarted from the beginning, to debug the failure. On that itera‐
tion, when trying to send the set of machines in the rack to Diskerase, the automation
determined that the set of machines that still needed to be Diskerased was (correctly)
empty. Unfortunately, the empty set was used as a special value, interpreted to mean
“everything.” This means the automation sent almost all the machines we have in all
colos to Diskerase.
Within minutes, the highly efficient Diskerase wiped the disks on all machines in our
CDN, and the machines were no longer able to terminate connections from users (or
do anything else useful). We were still able to serve all the users from our own data‐
centers, and after a few minutes the only effect visible externally was a slight increase
in latency. As far as we could tell, very few users noticed the problem at all, thanks to
good capacity planning (at least we got that right!). Meanwhile, we spent the better
part of two days reinstalling the machines in the affected colo racks; then we spent the
following weeks auditing and adding more sanity checks—including rate limiting—
into our automation, and making our decommission workflow idempotent.
Recommendations | 85
CHAPTER 8
Release Engineering
Written by Dinah McNutt
Edited by Betsy Beyer and Tim Harvey
Release engineering is a relatively new and fast-growing discipline of software engi‐
neering that can be concisely described as building and delivering software
[McN14a]. Release engineers have a solid (if not expert) understanding of source
code management, compilers, build configuration languages, automated build tools,
package managers, and installers. Their skill set includes deep knowledge of multiple
domains: development, configuration management, test integration, system adminis‐
tration, and customer support.
Running reliable services requires reliable release processes. Site Reliability Engineers
(SREs) need to know that the binaries and configurations they use are built in a
reproducible, automated way so that releases are repeatable and aren’t “unique snow‐
flakes.” Changes to any aspect of the release process should be intentional, rather than
accidental. SREs care about this process from source code to deployment.
Release engineering is a specific job function at Google. Release engineers work with
software engineers (SWEs) in product development and SREs to define all the steps
required to release software—from how the software is stored in the source code
repository, to build rules for compilation, to how testing, packaging, and deployment
are conducted.
The Role of a Release Engineer
Google is a data-driven company and release engineering follows suit. We have tools
that report on a host of metrics, such as how much time it takes for a code change to
be deployed into production (in other words, release velocity) and statistics on what
87
features are being used in build configuration files [Ada15]. Most of these tools were
envisioned and developed by release engineers.
Release engineers define best practices for using our tools in order to make sure
projects are released using consistent and repeatable methodologies. Our best practi‐
ces cover all elements of the release process. Examples include compiler flags, formats
for build identification tags, and required steps during a build. Making sure that our
tools behave correctly by default and are adequately documented makes it easy for
teams to stay focused on features and users, rather than spending time reinventing
the wheel (poorly) when it comes to releasing software.
Google has a large number of SREs who are charged with safely deploying products
and keeping Google services up and running. In order to make sure our release pro‐
cesses meet business requirements, release engineers and SREs work together to
develop strategies for canarying changes, pushing out new releases without interrupt‐
ing services, and rolling back features that demonstrate problems.
Philosophy
Release engineering is guided by an engineering and service philosophy that’s
expressed through four major principles, detailed in the following sections.
Self-Service Model
In order to work at scale, teams must be self-sufficient. Release engineering has devel‐
oped best practices and tools that allow our product development teams to control
and run their own release processes. Although we have thousands of engineers and
products, we can achieve a high release velocity because individual teams can decide
how often and when to release new versions of their products. Release processes can
be automated to the point that they require minimal involvement by the engineers,
and many projects are automatically built and released using a combination of our
automated build system and our deployment tools. Releases are truly automatic, and
only require engineer involvement if and when problems arise.
High Velocity
User-facing software (such as many components of Google Search) is rebuilt fre‐
quently, as we aim to roll out customer-facing features as quickly as possible. We have
embraced the philosophy that frequent releases result in fewer changes between ver‐
sions. This approach makes testing and troubleshooting easier. Some teams perform
hourly builds and then select the version to actually deploy to production from the
resulting pool of builds. Selection is based upon the test results and the features con‐
tained in a given build. Other teams have adopted a “Push on Green” release model
and deploy every build that passes all tests [Kle14].
88 | Chapter 8: Release Engineering
Hermetic Builds
Build tools must allow us to ensure consistency and repeatability. If two people
attempt to build the same product at the same revision number in the source code
repository on different machines, we expect identical results.1 Our builds are her‐
metic, meaning that they are insensitive to the libraries and other software installed
on the build machine. Instead, builds depend on known versions of build tools, such
as compilers, and dependencies, such as libraries. The build process is self-contained
and must not rely on services that are external to the build environment.
Rebuilding older releases when we need to fix a bug in software that’s running in pro‐
duction can be a challenge. We accomplish this task by rebuilding at the same revi‐
sion as the original build and including specific changes that were submitted after
that point in time. We call this tactic cherry picking. Our build tools are themselves
versioned based on the revision in the source code repository for the project being
built. Therefore, a project built last month won’t use this month’s version of the com‐
piler if a cherry pick is required, because that version may contain incompatible or
undesired features.
Enforcement of Policies and Procedures
Several layers of security and access control determine who can perform specific
operations when releasing a project. Gated operations include:
• Approving source code changes—this operation is managed through configura‐
tion files scattered throughout the codebase
• Specifying the actions to be performed during the release process
• Creating a new release
• Approving the initial integration proposal (which is a request to perform a build
at a specific revision number in the source code repository) and subsequent
cherry picks
• Deploying a new release
• Making changes to a project’s build configuration
Almost all changes to the codebase require a code review, which is a streamlined
action integrated into our normal developer workflow. Our automated release system
produces a report of all changes contained in a release, which is archived with other
build artifacts. By allowing SREs to understand what changes are included in a new
1 Google uses a monolithic unified source code repository; see [Pot16].
Philosophy | 89
release of a project, this report can expedite troubleshooting when there are problems
with a release.
Continuous Build and Deployment
Google has developed an automated release system called Rapid. Rapid is a system
that leverages a number of Google technologies to provide a framework that delivers
scalable, hermetic, and reliable releases. The following sections describe the software
lifecycle at Google and how it is managed using Rapid and other associated tools.
Building
Blaze2 is Google’s build tool of choice. It supports building binaries from a range of
languages, including our standard languages of C++, Java, Python, Go, and Java‐
Script. Engineers use Blaze to define build targets (e.g., the output of a build, such as a
JAR file), and to specify the dependencies for each target. When performing a build,
Blaze automatically builds the dependency targets.
Build targets for binaries and unit tests are defined in Rapid’s project configuration
files. Project-specific flags, such as a unique build identifier, are passed by Rapid to
Blaze. All binaries support a flag that displays the build date, the revision number,
and the build identifier, which allow us to easily associate a binary to a record of how
it was built.
Branching
All code is checked into the main branch of the source code tree (mainline). How‐
ever, most major projects don’t release directly from the mainline. Instead, we branch
from the mainline at a specific revision and never merge changes from the branch
back into the mainline. Bug fixes are submitted to the mainline and then cherry
picked into the branch for inclusion in the release. This practice avoids inadvertently
picking up unrelated changes submitted to the mainline since the original build
occurred. Using this branch and cherry pick method, we know the exact contents of
each release.
Testing
A continuous test system runs unit tests against the code in the mainline each time a
change is submitted, allowing us to detect build and test failures quickly. Release
engineering recommends that the continuous build test targets correspond to the
same test targets that gate the project release. We also recommend creating releases at
2 Blaze has been open sourced as Bazel. See “Bazel FAQ” on the Bazel website, http://bazel.io/faq.html.
90 | Chapter 8: Release Engineering
the revision number (version) of the last continuous test build that successfully com‐
pleted all tests. These measures decrease the chance that subsequent changes made to