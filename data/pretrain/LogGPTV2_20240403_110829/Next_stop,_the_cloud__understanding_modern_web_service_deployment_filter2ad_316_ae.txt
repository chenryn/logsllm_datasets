latency method (T = 1.1). Second column is total number of IPs
derived from subdomains, followed by the number that responded
to probes, and the estimates of how many were in each of the zones.
The ﬁnal column is the percentage of the responding IPs for which
no zone could be estimated.
methodology for zone measurement in practice. First, our and
prior experiments used a limited number of otherwise idle EC2
instances—RTT times may be signiﬁcantly more noisy across a
broader set of instances and for more heavily loaded instances.
Second, in some regions (ec2.us-east-1) we are unable to run an
instance in each zone. Nevertheless, we perform, to the best of
our knowledge, the ﬁrst use of latency-based zone measurement at
scale.
For each region, we launch three m1.medium instances in each
zone from which to perform probes; we refer to these as probe in-
stances. In ec2.us-east-1, we use ten additional m1.small instances
in each zone because a large fraction of IPs are in this region.
This region proved more challenging, due to a higher density of
instances, a lack of full coverage of zones by probe instances, and
more network noise. For each target physical instance in a region,
we ﬁrst map the public IP address to an internal IP address via an
appropriate DNS query from a probe instance in that region. We
then use hping3 to perform 10 TCP pings from a probe instance in
each zone in the region to both the internal IP address and the pub-
lic IP. While probes are very lightweight, we nevertheless limited
the frequency of our probing, and the probing process was repeated
5 times on each probe instance. The experiment was performed
over the course of ﬁve days (April 4th to April 8th, 2013). The
minimal RTT is taken as the probe time. For a region with k zones
(for ec2.us-east-1 we have k = 3, even though it has ﬁve zones),
we end up with k probe times t1, . . . , tk and we let i be such that
ti < tj for all i 6= j ∈ [1 .. k]. If there exists no such i (due to
a tie), then we mark the target IP (physical instance) as having un-
known zone. If ti is less than a threshold T then we conclude that
the target physical instance is in zone i. Otherwise, we mark the
target as having an unknown zone.
Setting T = 1.1, we end up with a zone estimate for physical
instance IPs from our Alexa subdomains dataset in most regions.
The results are shown in Table 12. The technique worked well
for all regions except for ec2.ap-northeast-1. The unknown rate is
affected by two factors: (i) Whether we can set up instances in all
zones; for example, we can not set up instances in ec2.ap-northeast-
1’s zone #2 after January, 2013, but, according to our observation in
January, the number of IPs in zone #1 and zone #2 is quite similar.
(ii) How many times we repeat the probes to reduce network noise;
with more probe data, the unknown rate can be further reduced.
Address-proximity-based identiﬁcation. We supplement the la-
tency measurements with sampling using our own accounts and an
estimation mechanism based on proximity of a target internal IP
address to a sampled IP address. As shown in prior work [34], it
is very likely that two instances running in the same /16 subnet are
zone a
zone b
zone c
zone d
 64
 48
 32
 16
4
6
d
o
m
s
s
e
r
d
d
a
P
I
 0
10.0.0.0
10.64.0.0
10.128.0.0
10.192.0.0
10.256.0.0
Internal IP address
Figure 7: Sampling data for address proximity measurement.
co-located in the same zone (and are potentially even of the same
instance type). We launched 5096 instances (in aggregate over the
course of several years) under a number of our AWS accounts. The
result is a set of account, zone label, internal IP triples (ai, zi, ipi)
for i ∈ [1 .. X]. A complicating factor is that, for ai 6= aj (differ-
ent accounts), it may be that the EC2-provided zone labels are not
the same. Meaning, for account ai it may be that the ec2.us-east-1a
is not the same actual zone as ec2.us-east-1a for account aj. Let Z
be the set of zone labels.
We thus take the following straightforward approach to merge
data across multiple different accounts. Consider a pair of accounts
a, b. Find the permutation πb→a: Z → Z that maximizes the num-
ber of pairs of /16 IPs ipi/16 = ipj /16 such that ai = a, aj = b
and πb→a(zj) = zi. This can be done efﬁciently by ordering all
triples of the accounts a and b by IP address, and inspecting the
zone labels associated to each account for nearby IP addresses. One
can repeat this for all pairs of accounts and solve the integer pro-
gramming problem associated with ﬁnding an optimal set of per-
mutations π, but it proved effective to take the simpler approach
of ﬁnding πb→a for one pair, merging the pair’s triples by apply-
ing πb→a appropriately, and then repeating with the next account c,
etc. The outcome of applying this process to samples from ec2.us-
east-1 is shown in Figure 7. Each binned IP address is a point, with
the distinct colors representing distinct availability zones.
We now apply the sampling data to the physical instances from
the Alexa subdomains dataset. If we have at least one sample IP
in the same /16 subnet as the IP associated with a target physical
instance, we conclude that the target instance is in the same zone
as the sample instance. Otherwise, we mark the target instance
as having an unknown zone. With this approach, we are able to
identify the zone for 79.1% of the EC2 physical instances in the
Alexa subdomains dataset.
Treating these zone identiﬁcations as ground truth, we check the
accuracy of the latency-based zone identiﬁcations. Table 13 shows
for each EC2 region the total number of physical instances in the
Alexa subdomains dataset, the number of instances for which the
two zone identiﬁcation approaches agree, the number of instances
whose zone cannot be identiﬁed using one or both methods, the
number of instances where the two methods disagree, and the error
rate of the latency-based method. The error rate is deﬁned as the
number of mismatched instances / (the total number of instances -
the number of unknown instances). We observe that latency based
method’s overall error rate is 5.7%. Its error rate is less than 3.9%
Region
all
ec2.ap-northeast-1
ec2.ap-southeast-1
ec2.ap-southeast-2
ec2.eu-west-1
ec2.sa-east-1
ec2.us-east-1
ec2.us-west-1
ec2.us-west-2
count match unknown mismat. error rate
5.7%
37876 28640
0.0%
965
1558
<0.1%
0.0%
201
25.0%
3359
N/A
0
2.7%
23518 19228
2032
0.0%
3.9%
1297
1742
0
1
0
1146
0
542
0
53
7494
295
428
97
1597
616
3748
385
328
1260
1987
298
6102
616
2417
1678
Table 13: Veracity of latency-based zone identiﬁcation.
Region
1st zone
2nd zone
3rd zone
ec2.us-east-1
ec2.us-west-1
ec2.us-west-2
ec2.eu-west-1
ec2.ap-northeast-1
ec2.ap-southeast-1
ec2.ap-southeast-2
ec2.sa-east-1
#Dom #Sub #Dom #Sub #Dom #Sub
9.5 292.9
N/A N/A
7.3
0.8
98.7
4.5
1.5
12.9
N/A N/A
N/A N/A
N/A N/A
16.1 419.0
33.2
13.4
77.0
3.7
11.3
0.3
14.4
6.2 155.4
37.4
3.0
9.6
1.0
63.9
2.9
11.3
1.3
1.2
19.1
0.3
0.2
0.2
8.9
1.6
0.9
2.3
0.4
0.9
0.2
0.5
Table 14: Estimated number of domains and subdomains using var-
ious EC2 zones. Some regions only have 2 zones.
(a) subdomain
(b) domain
Figure 8: (a) CDF of the number of zones used by each subdomain
(b) CDF of the average number of zones used by the subdomains
of each domain.
for all regions except Europe West6. In particular, the error rate in
the US East region (where the majority of the instances reside) is
quite low (2.7%).
Combined identiﬁcation. We combine the two zone identiﬁca-
tion methods to maximize the fraction of physical instances whose
zone we can identify. We give preference to our address-proximity-
based zone identiﬁcations, and use our latency-based identiﬁca-
tions only for instances whose zone cannot be identiﬁed using the
former method. Combining the two methods allows us to identify
the EC2 availability zone for 87.0% of all physical EC2 instances
in the Alexa subdomains dataset.
Table 14 summarizes the number of (sub)domains using each
region and zone. In all but one region (Asia Paciﬁc Southeast 2), we
observe a skew in the number of subdomains using each zone in a
region. Asia Paciﬁc Northeast and US East regions have the highest
skew across their three zones: 71% and 63% fewer subdomains,
respectively, use the least popular zone in those regions compared
to the most popular zone.
We also look at the number of zones used by each (sub)do-main.
Figure 8a shows a CDF of the number of zones used by each sub-
domain. We observe that 33.2% of subdomains use only one zone,
44.5% of subdomains use two zones, and 22.3% of subdomains use
6We were unable to decrease the error rate for Europe West even
after gathering additional latency measurements.
Rank
domain
# subdom # zone k=1 k=2 k=3
9
13
29
35
36
38
42
47
48
51
amazon.com
linkedin.com
163.com
pinterest.com
fc2.com
conduit.com
ask.com
apple.com
imdb.com
hao123.com
2
3
4
18
14
1
1
1
2
1
4
5
1
3
5
2
1
1
1
1
0
1
4
10
1
0
1
1
2
1
0
1
0
0
11
1
0
0
0
0
2
1
0
8
2
0
0
0
0
0
Table 15: Zone usage estimates for top using zones. Column 4 is
estimated total number of zones used by all subdomains. Columns
4–6 indicate the estimated number of subdomains that use k differ-
ent zones.
three or more zones. Of the subdomains that use two or more zones,
only 3.1% use zones in more than one region. Figure 8b shows
the average number of zones used by the subdomains of each do-
main. We observe that most domains (70%) only use one zone for
all subdomains; only 12% of domains use two or more zones per
subdomain on average.
Even for the top EC2-using domains, a large fraction of their
subdomains only use a single zone (Table 15). For example, 56% of
pinterest.com’s EC2-using subdomains and 33% of linkedin.com’s
are only deployed in one zone.
Summary and implications. Our two key ﬁndings in this sec-
tion are that (i) the majority of EC2-using subdomains only use
one (33.2%) or two (44.5%) zones, and (ii) the subdomains using a
given EC2 region are not evenly spread across the availability zones
in that region. The former implies that many EC2-using subdo-
mains would be completely unavailable if a single zone failed, and
many others would be severely crippled: e.g., a failure of ec2.us-
east-1a would cause 16.1% of subdomains to be completely un-
available. Our later key ﬁnding implies that an outage of a par-
ticular zone in a region may have a greater negative impact than
an outage of a different zone in the same region: e.g., a failure of
ec2.us-east-1a would impact ≈419K subdomains, while a failure
of ec2.us-east-1b would only impact ≈155K.
5. WIDE-AREA PERFORMANCE AND
FAULT TOLERANCE
Our results in the last section revealed that several services, even
highly ranked Alexa domains, appear to use only a single region or
even just a single availability zone. In this section, we explore the
impact of these choices on web services’ wide-area performance
and tolerance to failures. We focus on EC2-using web services.
5.1 Wide-area Performance
The choice of region(s) by a cloud service may impact perfor-
mance in at least two ways. First, clients’ geo-distribution may be
poorly matched to particular regions; such clients may experience
poor latency and throughput compared to a more judicious deploy-
ment. Second, there could be temporary changes in which region
performs best for a client due to congestion [19] or routing prob-
lems [36].
While the impact of diverse deployment of services (e.g., via