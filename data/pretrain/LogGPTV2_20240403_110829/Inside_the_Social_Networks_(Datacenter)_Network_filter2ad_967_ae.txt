### 6.3 Buffer Utilization

The combination of a lack of on/off traffic, higher flow intensity, and bursty individual flows suggests a potential increase in buffer utilization and overruns. Despite low average link utilization, bursty traffic can still lead to unacceptable loss rates. Recent work at Facebook has led to the development of in-house switching platforms [35], enabling us to gather buffer utilization statistics at fine granularity. Specifically, we collect buffer occupancies over a 24-hour period for switches connecting Web servers and cache nodes at a 10-microsecond granularity. Figure 15a plots the median and maximum values per second for the entire period, normalized to the buffer size. In other words, a single point for the median series represents the 50th-percentile buffer occupancy during that second (out of 100,000 samples per second), normalized by the size of the buffer. We also plot the normalized average link utilization (Figure 15b) and egress drop rate (Figure 15c) over the same period, sourced via fbflow and SNMP counters, respectively.

Several trends are evident from our results. First, standing buffer occupancies are non-trivial and can be quite high in the Web-server case. Even though link utilization is typically around 1%, over two-thirds of the available shared buffer is utilized during each 10-µs interval. Diurnal variations in buffer occupancies, utilization, and drop rate highlight the correlation between these metrics over time. Even with the diurnal traffic pattern, the maximum buffer occupancy in the Web-server rack approaches the configured limit for roughly three-quarters of the 24-hour period. While link utilization is roughly correlated with buffer occupancy within the Web-server rack, it is not a good predictor of buffer requirements across different applications. For instance, the Cache rack has higher link utilization but much lower buffer utilization and drop rates (not shown).

These buffer utilization levels occur despite relatively small packet sizes (Section 6.1). As utilization increases in the future, it might be due to an increase in the number of flows, the size of packets, or both. Larger packets with the same level of burstiness will use more of the buffer, while a larger number of flows leads to a greater chance of multiple flows sending bursts of packets simultaneously. Thus, careful buffer tuning is likely to be important moving forward.

### 6.4 Concurrent Flows

We define concurrent as existing within the same 5-ms window (cf. the 50-ms window considered by Alizadeh et al. while measuring a datacenter of hosts connected with 1-Gbps Ethernet [8]). We find that Web servers and cache hosts have hundreds to thousands of concurrent connections (at the 5-tuple level), while Hadoop nodes have approximately 25 concurrent connections on average—corresponding well with the findings of Alizadeh et al. [8, Figure 5]. However, switches are less concerned with individual connections than with destination ports. If we group connections destined to the same host, the numbers reduce only slightly (by at most a factor of two)—and not at all in the case of Hadoop.

Given the general lack of intra-rack traffic, almost all flows will traverse an up-link port. Hence, it is perhaps more interesting to consider flows at the rack level—i.e., considering the number of different racks with which an individual host is communicating. Figure 16 shows the number of concurrent flows sent by a single host over a 5-ms interval to different classes of destination hosts for three different host types: cache follower, cache leader, and Web server. Cache followers communicate with 225–300 different racks, while leaders talk to 175–350. In the median interval, both types of cache nodes communicate with approximately 250 other racks. The location of the racks varies dramatically, as discussed previously. Web servers communicate with 10–125 racks concurrently, 50 in the median interval.

Some proposed switch designs [25, 30] employ different technologies for large flows. Hence, we restrict our focus to the heavy hitter racks, namely those destination racks that constitute the majority of the traffic. The median number of heavy-hitter racks is 6–8 for Web servers and cache leaders with an effective max of 20–30, while the cache follower has 29 heavy hitter racks in the median case and up to 50 in the tail. Due to the differences in locality, Web servers and cache followers have very few rack-level heavy hitters of their cluster, while the cache leader displays the opposite pattern. Even considering only heavy hitter racks, the number of concurrent destinations is still significantly larger than that reported by Alizadeh et al. [8]. Additionally, the relative impermanence of our heavy hitters suggests that, for Frontend clusters at least, hybrid circuit-based approaches may be challenging to employ.

### 7. Conclusion

Facebook’s datacenter network supports a variety of distinct services that exhibit different traffic patterns. We find that several deviate substantially from the services considered in the literature. The different applications, combined with the scale (hundreds of thousands of nodes) and speed (10-Gbps edge links) of Facebook’s datacenter network, result in workloads that contrast in a number of ways from most previously published datasets. Space constraints prevent us from providing an exhaustive account; we describe features that may have implications for topology, traffic engineering, and top-of-rack switch design.

Our methodology imposes a few limitations on the scope of this study. Using end hosts to capture and timestamp packets introduces scheduler-based variations on timestamp accuracy. Additionally, we can only capture traffic from a few hosts at a time without risking drops in packet collection. Together, these constraints prevent us from evaluating effects like incast or microbursts, which are noted as being contributors to poor application performance [24]. Further, per-host packet dumps are necessarily anecdotal and ad hoc, relying on the presence of an unused capture host on the same rack as the target. While Fbflow is deployed datacenter-wide, the sheer amount of measurement data it provides presents another challenge—specifically, one of data processing and retention—which limits the resolution at which it can operate. We thus view effective network monitoring and analysis to be an ongoing and constantly evolving problem.

### Acknowledgements

This work is supported in part by the National Science Foundation through grants CNS-1314921 and CSR-1018808. We are indebted to Theo Benson, Nick McKeown, Remzi Arpaci-Dusseau, our shepherd, Srikanth Kandula, and the anonymous reviewers for their comments and suggestions on earlier drafts of this manuscript. Petr Lapukhov, Michal Burger, Sathya Narayanan, Avery Ching, and Vincent Liu provided invaluable insight into the inner workings of various Facebook services. Finally, and most significantly, Omar Baldonado catalyzed and facilitated the collaboration that enabled this study.

### References

[References listed here as in the original text]

---

This revised version aims to improve clarity, coherence, and professionalism. It maintains the technical details while ensuring the text is easier to read and understand.