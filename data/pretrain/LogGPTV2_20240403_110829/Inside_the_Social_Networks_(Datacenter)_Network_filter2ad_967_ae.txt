reports—likely due at least in part to the 10× increase in
link rate—with median inter-arrival times of approximately
2 ms (i.e., more than 500 ﬂows per second). Perhaps due
to connection pooling (which would decouple the arrival of
external user requests from the internal ﬂow-arrival rate),
the distribution of inter-arrival times for ﬂows at both types
of cache node are similar and longer: cache leaders see a
slightly higher arrival rate than followers, with median inter-
arrival timess of 3 and 8 ms, respectively.
6.3 Buffer utilization
The combination of a lack of on/off trafﬁc, higher ﬂow
intensity, and bursty individual ﬂows suggests a potential in-
crease in buffer utilization and overruns. Despite low av-
erage link utilization, bursty trafﬁc can still lead to unac-
ceptable loss rates. Recent work at Facebook has led to the
development of in-house switching platforms [35], enabling
us to gather buffer utilization statistics at ﬁne granularity. In
particular, we collect buffer occupancies over a 24-hour pe-
riod for switches connecting Web servers and cache nodes at
a 10-microsecond granularity. Figure 15a plots the median
and maximum values per second for the entire period, nor-
malized to the buffer size. In other words, a single point for
the median series represents the 50th-percentile buffer occu-
pancy during that second (out of 100,000 samples per sec-
ond), normalized by the size of the buffer. We also plot the
normalized average link utilization (Figure 15b) and egress
drop rate (Figure 15c) over the same period, sourced via
fbﬂow and SNMP counters, respectively.
A few trends are apparent from our results. The ﬁrst is that
standing buffer occupancies are non-trivial, and can be quite
high in the Web-server case. Even though link utilization
is on the order of 1% most of the time, over two-thirds of
the available shared buffer is utilized during each 10-µs in-
terval. Diurnal variation exists in buffer occupancies, utiliza-
tion and drop rate, highlighting the correlation between these
metrics over time. Even with the diurnal traﬁc pattern, how-
ever, the maximum buffer occupancy in the Web-server rack
approaches the conﬁgured limit for roughly three quarters
of the 24-hour period. While link utilization is roughly cor-
0200400600800100012001400Bytes0.00.10.20.30.40.50.60.70.80.91.0CDFWeb ServerHadoopCache LeaderCache Follower10121416182022Time (seconds)012345Packets (x1000)10121416182022Time (seconds)0510152025Packets (x1000)101001000100001000001000000SYN Interarrival (usec)0.00.10.20.30.40.50.60.70.80.91.0CDFWeb ServerHadoopCache LeaderCache Follower134of ﬂows, in the size of packets, or both. Either will have
impacts on buffer utilization: larger packets with the same
level of burstiness will use up more of the buffer, while a
larger number of ﬂows leads to a greater chance of multiple
ﬂows sending bursts of packets simultaneously. Thus, care-
ful buffer tuning is likely to be important moving forward.
6.4 Concurrent ﬂows
We consider concurrent to mean existing within the same
5-ms window (c.f.
the 50-ms window considered by Al-
izadeh et al. while measuring a datacenter of hosts connected
with 1-Gbps Ethernet [8]). We ﬁnd that Web servers and
cache hosts have 100s to 1000s of concurrent connections (at
the 5-tuple level), while Hadoop nodes have approximately
25 concurrent connections on average—corresponding quite
well with the ﬁndings of Alizadeh et al. [8, Figure 5]. That
said, switches are obviously less concerned with individual
connections than destination ports. If we group connections
destined to the same host, the numbers reduce only slightly
(by at most a factor of two)—and not at all in the case of
Hadoop.
Given the general lack of intra-rack trafﬁc, almost all
ﬂows will traverse an up-link port. Hence, it is perhaps more
interesting to consider ﬂows at the rack level—i.e., consid-
ering the number of different racks with which an individ-
ual host is communicating. Figure 16 shows the number of
concurrent ﬂows sent by a single host over a 5-ms interval
to different classes of destination hosts for three different
host types: cache follower, cache leader, Web server. Cache
followers communicate with 225–300 different racks, while
leaders talk to 175–350. In the median interval, both types
of cache nodes communicate with approximately 250 other
racks—the location of the racks varies dramatically as dis-
cussed previously, however. Web servers communicate with
10–125 racks concurrently, 50 in the median interval.
Some proposed switch designs [25, 30] employ different
technologies for large ﬂows. Hence, we restrict our focus to
the heavy hitter racks, namely those destination racks that
constitute the majority of the trafﬁc. The median number of
heavy-hitter racks is 6–8 for Web servers and cache leaders
with an effective max of 20–30, while the cache follower
has 29 heavy hitter racks in the median case and up to 50
in the tail. Due to the differences in locality, Web servers
and cache followers have very few rack-level heavy hitters
of their cluster, while the cache leader displays the opposite
pattern. Even considering only heavy hitter racks, the num-
ber of concurrent destinations is still signiﬁcantly larger than
that reported by Alizadeh et al. [8]. In addition, the relative
impermanence of our heavy hitters suggests that, for Fron-
tend clusters at least, hybrid circuit-based approaches may
be challenging to employ.
7. CONCLUSION
Facebook’s datacenter network supports a variety of dis-
tinct services that exhibit different trafﬁc patterns. We ﬁnd
that several deviate substantially from the services consid-
ered in the literature. The different applications, combined
(a) Normalized buffer occupancy, 10-microsecond resolution
(b) Link utilization, 10-minute average
(c) Web rack egress drops, 15-minute average
Figure 15: Correlating buffer occupancy, link utilization and
packet drops in Web server and Cache racks
related with buffer occupancy within the Web-server rack,
utilization by itself is not a good prediction of buffer require-
ments across different applications. In particular, the Cache
rack has higher link utilization, but much lower buffer uti-
lization and drop rates (not shown).
These buffer utilization levels occur despite relatively
small packet sizes (Section 6.1). As utilization increases in
the future, it might be through an increase in the number
01000020000300004000050000600007000080000Seconds10−510−410−310−210−1100RelativeBuﬀerOccupancyFractionWebrackmaximumCacherackmaximumCacherackmedianWebrackmedian01000020000300004000050000600007000080000Seconds0.00.10.20.30.40.50.60.70.80.91.0Utilization (Normalized to highest observed)Cache RackWeb Server Rack01000020000300004000050000600007000080000Seconds0.00.10.20.30.40.50.60.70.80.91.0Egress Drops (Normalized)135(a) Web server
(b) Cache follower
(c) Cache leader
Figure 16: Concurrent (5-ms) rack-level ﬂows
(a) Web server
(b) Cache follower
(c) Cache leader
Figure 17: Concurrent (5-ms) heavy-hitter racks
with the scale (hundreds of thousands of nodes) and speed
(10-Gbps edge links) of Facebook’s datacenter network re-
sult in workloads that contrast in a number of ways from
most previously published datasets. Space constraints pre-
vent us from providing an exhaustive account; we describe
features that may have implications for topology, trafﬁc en-
gineering, and top-of-rack switch design.
Our methodology imposes a few limitations on the scope
of this study. Using end hosts to capture and timestamp
packets introduces scheduler-based variations on timestamp
accuracy. In addition, we can only capture trafﬁc from a few
hosts at a time without risking drops in packet collection. To-
gether, these constraints prevent us from evaluating effects
like incast or microbursts, which are noted as being contrib-
utors to poor application performance [24]. Further, per-host
packet dumps are necessarily anecdotal and ad hoc, relying
on the presence of an unused capture host on the same rack
as the target. While Fbﬂow is deployed datacenter-wide,
the sheer amount of measurement data it provides presents
another challenge—speciﬁcally, one of data processing and
retention—which limits the resolution at which it can oper-
ate. We thus view effective network monitoring and analysis
to be an ongoing and constantly evolving problem.
Acknowledgements
This work is supported in part by the National Science Foun-
dation through grants CNS-1314921 and CSR-1018808.
We are indebted to Theo Benson, Nick McKeown, Remzi
Arpaci-Dusseau, our shepherd, Srikanth Kandula, and the
anonymous reviewers for their comments and suggestions
on earlier drafts of this manuscript. Petr Lapukhov, Michal
Burger, Sathya Narayanan, Avery Ching and Vincent Liu
provided invaluable insight into the inner workings of var-
ious Facebook services. Finally, and most signiﬁcantly,
Omar Baldonado catalyzed and faciliated the collaboration
that enabled this study.
8. REFERENCES
[1] An open network operating system. http://onosproject.org.
[2] Scribe (archived). https://github.com/facebookarchive/scribe.
[3] L. Abraham, J. Allen, O. Barykin, V. Borkar, B. Chopra,
C. Gerea, D. Merl, J. Metzler, D. Reiss, S. Subramanian, J. L.
Wiener, and O. Zed. Scuba: Diving into data at Facebook.
Proc. VLDB Endow., 6(11):1057–1067, Aug. 2013.
[4] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable,
commodity, data center network architecture. In Proc. ACM
SIGCOMM, Aug. 2008.
[5] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic ﬂow scheduling for data center
networks. In Proc. USENIX NSDI, Apr. 2010.
[6] A. Alameldeen, M. Martin, C. Mauer, K. Moore, X. Min,
M. Hill, D. Wood, and D. Sorin. Simulating a $2M
commercial server on a $2K PC. IEEE Computer,
36(2):50–57, Feb. 2003.
[7] M. Alizadeh, T. Edsall, S. Dharmapurikar, R. Vaidyanathan,
K. Chu, A. Fingerhut, V. T. Lam, F. Matus, R. Pan, N. Yadav,
and G. Varghese. Conga: Distributed congestion-aware load
balancing for datacenters. In Proc. ACM SIGCOMM, Aug.
2014.
[8] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. Data center
TCP (DCTCP). In Proc. ACM SIGCOMM, Aug. 2010.
[9] A. Andreyev. Introducing data center fabric, the
next-generation Facebook data center network.
https://code.facebook.com/posts/360346274145943, 2014.
[10] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and
M. Paleczny. Workload analysis of a large-scale key-value
store. In Proc. ACM SIGMETRICS/Performance, June 2012.
[11] L. A. Barroso, J. Clidaras, and U. Hölzle. The Datacenter as
a Computer:An Introduction to the Design of
0.050100150200250300350400Numberofracksin5ms0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-ClusterIntra-DatacenterInter-DatacenterAll0.050100150200250300350400Numberofracksin5ms0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-ClusterIntra-DatacenterInter-DatacenterAll0.050100150200250300350400Numberofracksin5ms0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-ClusterIntra-DatacenterInter-DatacenterAll0.01020304050Numberofrackin5ms0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-ClusterIntra-DatacenterInter-DatacenterAll0.01020304050Numberofrackin5ms0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-ClusterIntra-DatacenterInter-DatacenterAll0.01020304050Numberofrackin5ms0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-ClusterIntra-DatacenterInter-DatacenterAll136Warehouse-Scale Machines. Morgan & Claypool, 2nd
edition, 2013.
[12] T. Benson, A. Akella, and D. A. Maltz. Network trafﬁc
characteristics of data centers in the wild. In Proc. ACM
IMC, 2010.
[13] T. Benson, A. Anand, A. Akella, and M. Zhang.
Understanding data center trafﬁc charachteristics. In Proc.
ACM SIGCOMM WREN, Aug. 2009.
[14] T. Benson, A. Anand, A. Akella, and M. Zhang. MicroTE:
Fine grained trafﬁc engineering for data centers. In Proc.
ACM CoNEXT, Dec. 2011.
[15] N. Bronson, Z. Amsden, G. Cabrera, P. Chakka, P. Dimov,
H. Ding, J. Ferris, A. Giardullo, S. Kulkarni, H. Li,
M. Marchukov, D. Petrov, L. Puzar, Y. J. Song, and
V. Venkataramani. TAO: Facebook’s distributed data store
for the social graph. In Proc. USENIX ATC, June 2013.
[16] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and
I. Stoica. Managing data transfers in computer clusters with
orchestra. In Proceedings of the ACM SIGCOMM 2011
Conference, SIGCOMM ’11, pages 98–109, New York, NY,
USA, 2011. ACM.
[17] C. Delimitrou, S. Sankar, A. Kansal, and C. Kozyrakis.
ECHO: Recreating network trafﬁc maps for datacenters with
tens of thousands of servers. In Proc. IEEE International
Symposium on Workload Characterization, Nov. 2012.
[18] D. Ersoz, M. S. Yousif, and C. R. Das. Characterizing
network trafﬁc in a cluster-based, multi-tier data center. In
Proc. IEEE International Conference on Distributed
Computing Systems, June 2007.
[19] N. Farrington and A. Andreyev. Facebook’s data center
network architecture. In Proc. IEEE Optical Interconnects,
May 2013.
[20] N. Farrington, G. Porter, S. Radhakrishnan, H. Bazzaz,
V. Subramanya, Y. Fainman, G. Papen, and A. Vahdat.
Helios: A hybrid electrical/optical switch architecture for
modular data centers. In Proc. ACM SIGCOMM, Aug. 2010.
[21] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A
scalable and ﬂexible data center network. In Proc. ACM
SIGCOMM, Aug. 2009.
[22] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado,
N. McKeown, and S. Shenker. NOX: Towards an operating
system for networks. SIGCOMM CCR, 38(3), July 2008.
[23] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian,
Y. Zhang, and S. Lu. BCube: A high performance,
server-centric network architecture for modular data centers.
In Proc. ACM SIGCOMM, Aug. 2009.
[24] V. Jalaparti, P. Bodik, S. Kandula, I. Menache, M. Rybalkin,
and C. Yan. Speeding up distributed request-response
workﬂows. In Proceedings of the ACM SIGCOMM 2013
Conference on SIGCOMM, SIGCOMM ’13, pages 219–230,
New York, NY, USA, 2013. ACM.
[25] S. Kandula, J. Padhye, and P. Bahl. Flyways to de-congest
data center networks. In Proc. ACM HotNets, Oct. 2009.
[26] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and
R. Chaiken. The nature of data center trafﬁc: Measurements
& analysiss. In Proc. ACM IMC, Nov. 2009.
[27] R. Kapoor, A. C. Snoeren, G. M. Voelker, and G. Porter.
Bullet trains: A study of NIC burst behavior at microsecond
timescales. In Proc. ACM CoNEXT, Dec. 2013.
[28] T. Koponen, M. Casado, N. Gude, J. Stribling, L. Poutievski,
M. Zhu, R. Ramanathan, Y. Iwata, H. Inoue, T. Hama, and
S. Shenker. Onix: A distributed control platform for
large-scale production networks. In Proc. USENIX OSDI,
2010.
[29] A. Likhtarov, R. Nishtala, R. McElroy, H. Fugal,
A. Grynenko, and V. Venkataramani. Introducing mcrouter:
A memcached protocol router for scaling memcached
deployments.
https://code.facebook.com/posts/296442737213493, Sept.
2014.
[30] H. Liu, F. Lu, A. Forencich, R. Kapoor, M. Tewari, G. M.
Voelker, G. Papen, A. C. Snoeren, and G. Porter. Circuit
switching under the radar with REACToR. In Proc. USENIX
NSDI, Apr. 2014.
[31] R. Mack. Building timeline: Scaling up to hold your life
story. https://www.facebook.com/note.php?note_id=
10150468255628920, Jan. 2012.
[32] B. Pfaff, J. Pettit, T. Koponen, K. Amidon, M. Casado, and
S. Shenker. Extending networking into the virtualization
layer. In Proc. ACM HotNets, 2009.
[33] L. Popa, S. Ratnasamy, G. Iannaccone, A. Krishnamurthy,
and I. Stoica. A cost comparison of datacenter network
architectures. In Proc. ACM CoNEXT, Dec. 2010.
[34] R. Sherwood, G. Gibb, K.-K. Yap, G. Appenzeller,
M. Casado, N. McKeown, and G. Parulkar. Can the
production network be the testbed? In Proc. USENIX OSDI,
2010.
[35] A. Simpkins. Facebook open switching system (fboss) and
wedge in the open.
https://code.facebook.com/posts/843620439027582/
facebook-open-switching-system-fboss-and-wedge-in-the-open/,
2015.
[36] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyﬁsh:
Networking data centers randomly. In Proc. USENIX NSDI,
Apr. 2012.
[37] D. Sommermann and A. Frindell. Introducing Proxygen,
Facebook’s C++ HTTP framework.
https://code.facebook.com/posts/1503205539947302, 2014.
[38] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka,
N. Zhang, S. Antony, H. Liu, and R. Murthy. Hive – a
petabyte scale data warehouse using Hadoop. In Proc. IEEE
ICDE, Mar. 2010.
[39] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki,
T. S. E. Ng, M. Kozuch, and M. Ryan. c-Through: Part-time
optics in data centers. In Proc. ACM SIGCOMM, Aug. 2010.
[40] X. Zhou, Z. Zhang, Y. Zhu, Y. Li, S. Kumar, A. Vahdat, B. Y.
Zhao, and H. Zheng. Mirror mirror on the ceiling: Flexible
wireless links for data centers. In Proc. ACM SIGCOMM,
Aug. 2012.
137