trusted people using social networks) enhances robustness,
but makes it hard for the majority of potential bridge users
to get bridges.
Our key insight is that it is possible to bridge the gap
between the openness and robustness of bridge distribution
by building a user reputation system. Instead of trying to
keep all malicious users outside the system, we adopt a less
restrictive user invitation mechanism to ensure the bridge
distribution can reach a large number of potential users; in
particular, we use a loosely trusted social network for user
invitation, and a well-behaving user can invite his less close
friends into the system. Meanwhile, we leverage a user
reputation system to punish blockers and limit them from
repeatedly blocking bridges; more speciﬁcally, each user
earns credits based on the uptime of his bridges, needs to
pay credits to get a new bridge, and is provided opportuni-
ties to invite new users only if his credit balance is above a
certain threshold.
It is important to note that our goal is not to keep bridges
unblocked forever; instead, we try to achieve a more practi-
cal goal—having bridges serve a sufﬁciently long period of
time so that the overall rate of recruiting new bridges out-
paces the rate of losing bridges. We also note that this is still
a very challenging problem; as will be shown by the com-
parison results (Section 4.3.3), the existing schemes have
a difﬁcult time protecting bridges from being blocked even
for a very limited period of time.
4.2 Scheme
When joining the system, a new user U receives k bridges
B1,··· , Bk as well as a credential, which is used to verify
U as a legitimate registered user. The credential is signed by
the bridge distributor D and includes the following informa-
tion:
U∥(cid:8)∥{Bi, τi, ϕi}k
i=1
wherein (cid:8) denotes the total credits owned by U, τi denotes
the time when Bi is given to U, and ϕi denotes the credits
that U has earned from Bi. (At the initialization, (cid:8) = 0,
ϕi = 0, and τi is the joining time). The selection of the
bridges B1,··· , Bk is at random. D keeps counting the
number of users assigned to each bridge and stops giving
a bridge’s address to new users once the number of users
assigned to the bridge reaches an upper limit (denoted by
g).
4.2.1 Earning Credits
U is given credits based on the uptime of his bridges. The
credit assignment policy should have the following proper-
ties. First, it should provide incentives for corrupt users to
keep the bridge alive for at least a certain period of time, say
T0 days, which should be long enough to make sure enough
new bridges can be recruited in time to maintain the over-
all bridge resources in the system. Second, the total credits
that a user earns from a bridge should be upper-bounded, to
prevent corrupt users from keeping one bridge alive to con-
tinuously earn credits and using the earned credits to request
and block other bridges.
Now, we deﬁne the credit assignment function Credit(·).
Let Tcur denote the current time, and βi denote the time
when Bi gets blocked (if Bi is not blocked yet, βi = ∞).
We deﬁne t as the length of the time period from the time
when U knows Bi to the time when Bi gets blocked or the
current time if Bi is not blocked, i.e., t = min{βi, Tcur} −
τi. We let ρ denote the rate of earning credits from a bridge
(credits/day) and T1 denote the upper-bound time by which
U can earn credits from the bridge. Then, the amount of
credits ϕi earned from Bi is deﬁned as:
 0
ϕi = Credit(t) =
(t − T0) · ρ
(T1 − T0) · ρ
t  T1
Without loss of generality, we deﬁne ρ = 1 credit/day; then,
the maximum credits that a user can earn from a bridge are
(T1 − T0).
From time to time (e.g., before requesting a new bridge),
U requests D to update his credit balance (cid:8) with his recently
earned credits, say, from Bi. D ﬁrst validates U’s credential
(verifying the tagged signature), and then re-calculates the
credits ~ϕi according to the uptime of Bi, adds the difference
~ϕi − ϕi to (cid:8), updates ϕi with ~ϕi, and ﬁnally re-signs the
updated credential.
4.2.2 Getting a New Bridge
To limit the number of bridges that a corrupt user knows,
we allow each user to have k or fewer alive bridges at any
time. This is enforced by granting a new bridge to a user
U only if one of his bridges (say Bb) has been blocked. In
particular, upon a request for a new bridge in replace of
Bb, D ﬁrst veriﬁes that Bb is in U’s credential and has been
blocked. D also checks whether U has enough credits to pay
− is the price for a
for a new bridge, i.e., (cid:8) > ϕ
new bridge.
After giving out a new bridge ~Bb, D updates U’s creden-
tial by replacing the record {Bb, τb, ϕb} with { ~Bb, Tcur, 0}
and updating the total credits with ~(cid:8) = (cid:8)− ϕ
−. To prevent
a malicious user from re-using his old credentials that has
more credits, D keeps a list of expired credentials (e.g., stor-
ing the hash value of the credential); once U’s credential is
updated, the old credential is added to the expired credential
list and cannot be used again.
−, where ϕ
We note that temporarily blocking a bridge just to create
an open spot for a new bridge does not help a corrupt user,
because he still needs to pay the same amount of credits to
get a new bridge and the availability loss of a temporarily
blocked bridge is strictly smaller than that of a permanently
blocked bridge.
4.2.3
Inviting New Users
D periodically sends out invitation tickets to high-reputation
users whose credit balances are higher than the threshold
(cid:8)θ. Since the censor may let some corrupt users behave
legitimately to simply accumulate credits and obtain invita-
tion tickets in order to deploy more corrupt users or Sybils
in the system, we let D randomly select the recipients of
invitation tickets from qualiﬁed users. A user who has re-
ceived an invitation ticket can give it to any of his friends,
who can later use the ticket to join the system.
Note that the system needs to reserve a certain fraction
(e.g., 50%) of bridge resource (i.e., the sum of the remain-
ing capacity of unblocked bridges) for potential replace-
ment of blocked bridges for existing users, while using the
rest bridge resource to invite new users. The amount of
reserved resource can be dynamically adjusted according
to the amount of current bridge resource and the plans for
growing the user base and recruiting new bridges.
4.3 Evaluation and Comparison
We now analyze the robustness of rBridge against the
following blocking strategies, and compare it with Proxi-
max [15]. We discuss other potential attacks in Section 6.
• Aggressive blocking: The censor is eager to block dis-
covered bridges, i.e., shutting down the bridge once it
is known to a corrupt user.
(a) Number of initial bridges (N = 1000)
(b) Number of users per bridge (p is attack
probability, f = 5%)
(c) Probability distribution of malicious users
(f = 5%)
Figure 1: Parameter selection
• Conservative blocking: A sophisticated censor may
keep some bridges alive for a certain period of time
to accumulate credits, and use the credits to discover
new bridges and/or invite more corrupt users.
• Event-driven blocking: The censor may dramatically
tighten the control of the Internet access when certain
events (e.g., crisis) take place. We consider such at-
tacks by assuming that malicious users do not block
any bridges until a certain time, when suddenly all the
discovered bridges get blocked.
To evaluate rBridge under these attacks, we imple-
mented an event-based simulator using a timing-based pri-
ority queue, by treating each state change of the system as
an event, such as inviting a new user, getting a new bridge,
blocking a bridge, recruiting a new bridge, etc. Each event
contains a time stamp indicating when the event occurs as
well as an ID of the subject indicating who will carry out
the event. We start with choosing the parameters for our
simulation.
4.3.1 Parameter Selection
We employ probabilistic analysis to select appropriate pa-
rameters. To simplify the parameter calculation, we con-
sider a static user group (i.e., no new users join the sys-
tem); later, we validate our parameter selection in a dy-
namic setting using the event-based simulator. In practice,
the bridge distributor can periodically re-calculate the pa-
rameters (e.g., every 30 days) using the current size of the
user group.
Initial setup. Let f denote the fraction of malicious users
among all potential bridge users (note that f is not the ac-
tual ratio of malicious users in the system). We expect a
typical value of f between 1% and 5%, but we also eval-
uate rBridge with much higher f to see its robustness in
extreme cases. The system starts with N = 1000 users,
which are randomly selected from the pool of all potential
bridge users; for instance, D could randomly select a num-
ber of Chinese users on Twitter (based on their proﬁles) as
the initial bridge users, and very likely these users are will-
ing to use the bridge based circumvention service because
they already used some circumvention tools to access Twit-
ter (which is blocked in China).
Each user is initially provided k = 3 bridges3. Suppose
there are m0 initial bridges in the system; the number of
users per bridge is g0 = N·k
on average. Assuming a cor-
rupt user blocks all of his bridges, the probability that an
honest user has no alive bridge is (1− (1− f )g0 )k. Accord-
ing to Figure 1a, we choose m0 = 200 to make sure the
majority of users can survive the initial blocking.
m0
g—the maximum number of users per bridge. In rBridge,
when a bridge gets blocked, all the g users sharing this
bridge will be “punished” (i.e., receive no more credits from
the bridge and need to pay credits to get a new bridge); in-
tuitively, with a smaller g, it is easier to precisely punish
the real blocker, as fewer honest users would be punished
by mistake. On the other hand, we should make sure g is
sufﬁciently large to avoid underusing the bridges.
Here, we calculate the probability that a user has a cer-
tain number of blocked bridges; this probability depends on
g and determines the punishment on the user. Let p denote
the probability that a corrupt user blocks a bridge he knows,
and λ denote the probability that a bridge is blocked. Then,
we have λ = 1− (1− f · p)g (here we use f to approximate
the ratio of corrupt users in the system). We deﬁne Xh (or
Xm) as the number of blocked bridges of an honest (or cor-
rupt) user. Assuming a user obtains l bridges since joining
the system, we get:
· (1 − λ)x · λl−x
(1)
· (1 − λ)x−p·lλl−x(2)
(
(
)
l
x
l − p · l
x − p · l
)
P r(Xh = x) =
P r(Xm = x) =
3In the current bridge distribution strategy deployed by Tor, each re-
questing user is given 3 different bridges.
010020030040050000.20.40.60.81m0 −− num. of initial bridgesPr(no alive bridge)  f=5%f=2%f=1%02040608010000.20.40.60.81g −− num. of users per bridgePr(Xm > Xh)  p=80%p=50%p=20%00.20.40.60.8100.050.10.150.20.25indexPr(mal | index)  uniformstagedlinear(a) User-hours
(b) % of thirsty-hours
(c) User base
Figure 2: Aggressive blocking
We are interested in calculating P r(Xm > Xh) —the prob-
ability that a corrupt user has more blocked bridges than an
honest user (i.e., the likelihood that a corrupt user receives
more punishment than an honest user); ideally, this proba-
bility should be maximized.
l∑
P r(Xm > Xh) =
x−1∑
P r(Xm = x)
P r(Xh = y)
y=0
x=p·l
(3)
Figure 1b depicts P r(Xm > Xh) with l = 10 and f =
5%. While P r(Xm > Xh) is maximal when g is small, we
choose a fairly large value g = 40 to make sure bridges are
not underutilized.
Credit(t)—the credit assignment function. Recall that
T0 and T1 represent the expected lower and upper bounds
of a bridge’s life time, respectively. We let Tlf denote the
expected life time of a bridge, T0 ≤ Tlf ≤ T1, and s denote
the speed of recruiting new bridges. To maintain the overall
bridge resource, we should have:
Tlf · g · s · time = N · k · time
From this, we get:
T0 =
N · k
g · smax
, T1 =
N · k
g · smin
(4)
(5)
where smax and smin denote the maximum and minimum
rate of recruiting new bridges, respectively.
(From May
2011 to May 2012, the Tor project recruited about 400 new
bridges [1].) In our evaluation, we set smax = 1 bridge/day
and smin = 0.2 bridge/day, which implies that 70 ∼ 360
bridges need to be recruited per year. With N = 1000, we
get T0 = 75 days and T1 = 375 days according to (5). Note
that with a larger number of users, the overall bridge con-
sumption will become higher and the system needs to re-
cruit more bridges. However, T0 and T1 we have calculated
are the worst-case expectations; as will be shown in the sim-
ulation, the lifetime of bridges is actually much longer than
the worst-case T0 and T1, and hence the pressure of recruit-
ing new bridges is smaller in practice.
−—the price for getting a new bridge. The credits
ϕ
earned from unblocked bridges should be roughly equal to
the credits paid to replace blocked bridges. Therefore, ap-
proximately we have:
P r(Xh = x) · x · ϕ
−
=
(6)
P r(Xh = x) · (k − x) · (T1 − T0)
k∑
k∑
x=0
x=0
From Equation (1) (6), we get ϕ
= 45.
−
(cid:8)θ—the threshold of credits for invitation. To decide the
value of (cid:8)θ, we assume that a user with at least half of his
bridges unblocked can be considered to invite new users.
Then, we have:
⌈ k
⌉∑
·(k − x) · (T1 − T0)
From Equation (1) (7), we get (cid:8)θ = 236.
(cid:8)θ =
x=0
2
P r(Xh = x|Xh ≤ ⌈ k
2
⌉)
(7)
User invitation.
In our simulation, we set the rate of
recruiting new bridges as s = 1 bridge/day; we reserve
50% of bridge resource for potential replacement of blocked
bridges. Every 7 days, the bridge distributor calculates the
number of new users to be invited based on the current
bridge resource, and distributes the corresponding number
of invitation tickets to randomly selected users whose credit
balance is higher than (cid:8)θ.
Probability distribution of malicious users. Now we con-
sider the probability that an invited user is malicious. We
suppose each corrupt user always gives his invitation tick-
ets to malicious users or Sybils. For an honest user, if he
randomly selects a new user to invite, the probability that
the new user is malicious is approximately f. However,
10210410600.20.40.60.81Use hours of bridgesCDF  f=5%, linearf=5%, stagedf=10%, stagedf=30%, stagedf=50%, staged00.20.40.60.810.50.60.70.80.91% thirsty hourCDF  f=5%, linearf=5%, stagedf=10%, stagedf=30%, stagedf=50%, staged01002003004000100020003000400050006000Time (day)Num. of users  f=5%, linearf=5%, stagedf=10%, stagedf=30%, stagedf=50%, staged(a) User-hours
(b) % of thirsty-hours
(c) User base
Figure 3: Conservative blocking (f = 5%, staged distribution)
in practice, a user is inclined to ﬁrst invite the friends he
trusts most; as receiving more and more invitation tick-
ets, the user will start to invite less trusted friends. To
model this, we assume each user ranks his friends based
on trustworthiness: each friend is assigned an index rang-
ing from 0 to 1 according to the trustworthiness (e.g., the
most trusted one out of 100 friends has the index 1/100 =
0.01). We consider two speciﬁc models to assign probabili-
ties of malicious users. One is staged distribution, wherein
the friends are divided into two groups (i.e., more trusted
and less trusted) and all users within a group have the same
probability of being malicious. We assume 80% friends be-
long to the “more trusted” group and the remaining 20% are
in the “less trusted” group. The other is linear distribution,
for which the probability of being a malicious user is a lin-
ear function of the index. We suppose the probability that
the most trusted friend is malicious is 1%. For both distri-
butions, the overall ratio of malicious users is f. Figure 1c
depicts the probability distributions of these two models.
4.3.2 Evaluation Results
Using the event-based simulator, we measured the user-
hours, thirsty-hours, and growth of user base under differ-
ent blocking strategies.
Aggressive blocking. The simulation results for the ag-
gressive blocking are provided in Figure 2. We can see from
Figure 2a that when f = 5%, 80% of bridges can pro-