#### Description
I think scoring of cross_val_score (also GridSearchCV and so on) is not good
especially with cv={# of samples} (i.e. LeaveOneOut). In evaluating R2 or MAE
values, mean of y is calculated with respect to each CV model in
cross_val_score, but calculation of mean should be performed on all y. The
difference become conspicuous when we use Leave One Out. Then mean of y with
respect to each model equals to just y since number of test sample is one.
Consequently, R2 value become zero.  
![](https://camo.githubusercontent.com/50cf4302360705646ffa6fc5aced72c98e32de48c1d0fde7447b246e750297e5/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f525e325f7b43567d3d312d5c667261637b5c73756d5f7b697d5e7b4e7d28795f692d5c6861747b797d5f69295e327d7b5c73756d5f7b697d5e7b4e7d28795f692d5c6261727b797d295e327d)
#### Example code to Reproduce
PLS regression with Boston data set
    from sklearn import datasets,cross_decomposition
    from sklearn.model_selection import cross_val_score,cross_val_predict
    from sklearn.metrics import r2_score
    # Boston data set
    boston = datasets.load_boston()
    X,y = boston.data,boston.target
    pls = cross_decomposition.PLSRegression(10)
    r2cv_1 = cross_val_score(pls,X,y,scoring='r2',cv=X.shape[0]).mean()
    r2cv_2 = r2_score(y,cross_val_predict(pls,X,y,cv=X.shape[0]))
    print('r2cv(cross_val_score   + mean)    : {0:8.4f}\nr2cv(cross_val_predict + r2_score):{1:8.4f}'.format(r2cv_1,r2cv_2))
#### Results
    r2cv(cross_val_score   + mean)    :   0.0000
    r2cv(cross_val_predict + r2_score):   0.7190
R2cv value is expected to be 0.7190 but returned value is zero with
cross_val_score.
#### Versions
Python 3.5.2 (default, Nov 23 2017, 16:37:01)  
[GCC 5.4.0 20160609]  
NumPy 1.14.5  
SciPy 1.1.0  
Scikit-Learn 0.19.1