reports (i.e., 𝑥 = 100). Note that the x-axis on top is on a log-scale
while that on the bottom is on a linear scale. The y-axis shows mean
values across all projects with a given report rank. For instance, in
Fig. 1.a for those 50 projects that have 100 bug reports, 77% of bug
reports are regressions.
2.1 Prevalence of Regression Bugs
Our analysis shows that 77.2% of the 23k bug reports in OSSFuzz are
regressions. That is, a bug-introducing commit (BIC) was identified
and the bug cannot be observed before the BIC. This means that
most bugs found by OSSFuzz are, in fact, introduced by recent code
changes. We also find, on average, it takes 68 days from the BIC
to discovery and automatic reporting (5 days on the median). This
means that regressions are difficult to find.
2OSSFuzz bug tracker @ https://bugs.chromium.org/p/oss-fuzz/issues.
Figure 1.a shows the probability that a reported bug is a regres-
sion as the number of project-specific bug reports increases. We
can see that the first reported bug is a regression only for one in
five projects (20.2%). However, as more bugs are reported (and old
bugs are fixed), the regression probability increases. The empirical
probability that the 1000th bug is a regression, is greater than 99%.
Four in five reported bugs are introduced by recent code changes.
The probability for a bug report to be a regression increases
from 20% for the first bug report to over 99% for the 1000th
reported bug. This demonstrates the need for focusing later
fuzzing efforts on recently changed code. On average, it takes 2
months to discover a regression (5 days on the median).
2.2 Bug Reporting Rate Across Projects
Figure 1.b shows the rate at which new bugs are reported across all
projects in OSSFuzz. The dashed lines show linear regressions, for
ranks in [1, 100] and [300, ∞], resp.. The color represents the prob-
ability that the report with a given rank is marked as a regression.
We show the number of days that have passed since the first bug
report as the number of project-specific bug reports increases.
Once a project is onboarded at OSSFuzz, new bugs are reported at
a constant rate of 2.5 bugs per day. For many of the early reported
bugs of a project no bug-introducing commit can be identified.
After a while new bugs are reported at a much lower pace. This is
consistent with our own experience. Fuzzing is very successful in
finding bugs particularly in new targets that have not been fuzzed
before. Once the bugs are fixed, less new bugs are found. The code
has already been fuzzed and most bugs have been found.
However, after this initial burst of new bugs reported, we were
surprised to find that subsequent bugs continue to be reported at
a constant rate of about 3.5 bugs per week. Our only explanation
was that these bugs must have been introduced by recent changes.
Indeed, mapping the probability of a reported bug to be a regression
onto the curve, we can see that almost all of the newly reported
bugs in this second phase are regressions (green color).
Once a new project is onboarded at OSSFuzz, there is an initial
burst of new bug reports at a rate of 2.5 per day. After this burst,
the rate drops but remains constant at 3.5 reports per week.
This demonstrates how fuzzing unchanged code over and over
while another part of the project changes is a waste of compute.
2.3 OSS Security
Fifteen years ago, Ozment and Schechter examined the code base
of the OpenBSD operating system to determine how many of the
vulnerabilities existed since the initial version and whether its
security had increased over time [26]. They found that 62% of
reported vulnerabilities existed since the initial version 7.5 years
prior (i.e., 38% were regressions). They call these vulnerabilities as
foundational. The authors identified a downward trend in the rate
of vulnerability discovery, so that they described the security of
OpenBSD releases like wine (unlike milk), as improving over time.
Session 7B: Fuzzing CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2171In contrast, for the 350+ open source software (OSS) projects
we studied,3 new bugs are being discovered in the master branch
at a constant rate. In fact, we only measure the bugs found by the
three fuzzers that are continuously run within the OSSFuzz project.
There are also non-fuzzing bugs that are discovered downstream, or
during manual security auditing which we do not measure. We find
that only 23% of the fuzzer-reported bugs are foundational4 and
the probability that a fuzzer-reported bug is due to recent changes
increases over time.
2.4 Threats to Validity
As threat to external validity, we note that we only consider bugs
that were discovered by the greybox fuzzers and code sanitizers
available in OSSFuzz. As a threat to internal validity, we note that
the OSSFuzz fuzzing campaigns may be started up to 24 hours after
the bug-introducing commit was submitted. OSSFuzz builds some
OSS projects at most once a day. Moreover, we cannot guarantee
there are no bugs in the algorithm used to determine the bug-
introducing commit.
3 REGRESSION GREYBOX FUZZING
3.1 Code History-based Instrumentation
Conceptually, we need to associate an input with a quantity that
measures how old the executed code is or how often it has been
changed. To compute this quantity during the execution of the
input, we instrument the program. The regression greybox fuzzer
(RGF) uses this quantity to steer the fuzzer towards more recently
and more frequently changed code.
Algorithm 1 Code History-based Instrumentation
Input: Program 𝑃
1: Inject 𝛼, count as global, shared variables into 𝑃 and set to 0
2: for each Basic Block 𝐵𝐵 ∈ 𝑃 do
3:
4:
5:
6:
7:
8: end for
age = lastChanged(𝐵𝐵) // in #days or #commits
churn = numberOfChanges(𝐵𝐵)
(age′, churn′) = amplify(age, churn)
Inject "𝛼 = 𝛼 + (age′ · churn′)" into 𝐵𝐵
Inject "count = count + 1" into 𝐵𝐵
Output: Instrumented program 𝑃 ′
how recently it has been changed. AFLChurn uses git blame to
identify the commit 𝐶 for a given line 𝐿 and subtracts the date of
𝐶 from date of the (current) head commit to compute the number
of days since line 𝐿 ∈ BB was last changed. AFLChurn uses git
rev-list to count the number of commits since commit 𝐶. The
average age of each line 𝐿 ∈ BB gives the age of the basic block BB.
The number of days and number of commits since 𝐶 are both used
as independent measures of the age of a basic block.
numberOfChanges (Line 4). For all basic blocks BB ∈ 𝑃, RGF
computes the churn value. Conceptually, the churn of a basic block
indicates how frequently it has been changed. AFLChurn finds
all commits to the file containing BB. A commit 𝐶 ′ is the syntactic
difference between two successive revisions 𝐶 ′
= 𝑑𝑖 𝑓 𝑓 (𝑅, 𝑅). For
each commit 𝐶 ′ prior to the most recent revision 𝐻 (head), RGF
determines whether 𝐶 ′ changed BB in 𝐻 as follows. Suppose, there
are three revisions, the most recent revision 𝐻 and the commit-
related revisions 𝑅 and 𝑅 such that 𝐶 ′
= 𝑑𝑖 𝑓 𝑓 (𝑅, 𝑅). By computing
the difference diff (𝑅, 𝐻 ) between 𝑅 and the current revision, we
get two pieces of information: (a) whether BB has changed since 𝑅
and which lines in 𝑅 the given basic block BB corresponds to. Using
the commit 𝐶 ′
= diff (𝑅, 𝑅), we find whether those lines in 𝑅Ðthat
BB corresponds toÐhave been changed in 𝐶 ′. By counting all such
commits 𝐶 ′, we can compute how often BB has been changed.
amplify (Line 5). After computing age and churn values for a ba-
sic block BB, we amplify these values. Our observation is that there
are a large number of łfoundationalž basic blocks that have never
been changed. If we just computed the average across basic blocks
in the execution trace of an input, the signal from the interesting
basic blocks would be very weak. Very recently or more frequently
changed basic blocks are relatively rare. So, how can we amplify
the signal from those interesting basic blocks?
We conducted preliminary experiments with several amplifier
functions (see Section 5.4). We found that the inverse of the number
1
of days age’ =
age and the logarithm of the number of changes
churn’ = log(churn) provides the most effective amplification. The
regression greybox fuzzer will multiply the aggregated, amplified
age and churn values and maximize the resulting quantity.
inject (Line 6ś8). Finally, for all basic blocks BB ∈ 𝑃, our in-
strumentation pass injects new instructions at the end of BB. The
added trampoline makes 𝑃 ′ aggregate that amplified values (𝛼, 𝛽)
and count the number of executed basic blocks (count).
Algorithm 1 sketches our instrumentation procedure. First, our
instrumentation pass introduces two new global variables, 𝛼 and
count (Line 1). After the execution of the input on the instrumented
program, the RGF can read the values of these variables. For in-
stance, our RGF tool AFLChurn, implements an LLVM instrumenta-
tion pass that is loaded by the clang compiler. AFLChurn reserves
two times 32bit (or 64bit) at the end of a shared memory to store the
values of 𝛼 and count. The 64 kilobyte shared memory is already
shared between the program and vanilla AFL to capture coverage
information.
lastChanged (Line 3). For every basic block BB ∈ 𝑃, RGF com-
putes the age value. Conceptually, the age of a basic block indicates
3https://github.com/google/oss-fuzz/tree/master/projects
4To be precise, for 23% of bugs no bug-introducing commit could be found.
3.2 Simulated Annealing-based Power Schedule
Regression greybox fuzzing (RGF) steers the input generation to-
ward code regions that have been changed more recently and more
frequently. To this end, RGF uses the instrumented program to
compute the age and churn values during the execution of an input.
RGF is an optimization problem which requires to carefully balance
exploration and exploitation. If RGF only explores and never uses
the age and churn value, then it cannot be any better than normal
greybox fuzzing. If RGF only exploits and only fuzzes the seed with
optimal age or churn values, then it will miss opportunities to dis-
cover bugs in other (slightly older) code. Global search techniques
allow us to manage this trade-off.
We propose to solve RGF’s optimization problem using simulated
annealing. It begins with an exploration phase and quickly moves to
Session 7B: Fuzzing CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2172ω
a
g
e
m
O
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.0
2.5
5.0
7.5
10.0
Number of times the seed was selected
p
y
g
r
e
n
E
8
6
4
2
0
0.9 × (1 − 0.5selected) + 0.5 × 0.5selected
0.5 × (1 − 0.5selected) + 0.5 × 0.5selected
0.1 × (1 − 0.5selected) + 0.5 × 0.5selected
0.00
0.25
0.50
0.75
1.00
Omega ω
22 × (2ω−1)
23 × (2ω−1)
byte score
127
0
-128
10
2
3 4 5 6 7
8 9
10
11 12 13 14 15 16
17
byte position
seed (p=0.36)
input (p=0.52)
Figure 3: Functional behavior of Simulated Annealing (left)
and Power Schedule (right). On the left, we see omega 𝜔 for
three different values of the product ˆ𝛼 ∈ {0, 1, 0.5, 0.9}. As
the seed is selected more often (thus, as 𝑇exp increases), 𝜔
approaches ˆ𝛼. On the right, we see the seed’s energy factor
𝑝/𝑝afl for two different values of 𝑟 ∈ {2, 3}. As the seed’s
weight 𝜔 goes towards 0, the factor tends towards 2−𝑟 . As
𝜔 goes towards 1, the factor tends towards 2𝑟 .
Figure 4: Byte-level Energy Assignment. The distribution of
scores for each byte in the seed is shown on top. The bytes
of the seed and input are shown at the bottom. The input
was generated by changing the ninth byte of the seed. As
the fitness 𝑝 of the generated input is greater than that of
the seed (0.52 > 0.36), the scores of that byte and both of its
direct neighbors are incremented (shown as black rectangles
above bytes 8ś10).
an exploitation phase. In greybox fuzzing, we can adjust such search
parameters using the power schedule. A power schedule assigns
energy to all seeds in the seed corpus. A seed’s energy determines
the time spent fuzzing the seed.
Algorithm 2 Simulated Annealing-based Power Schedule
Input: Instrumented Program 𝑃 ′, Seed Corpus 𝐶, Input 𝑡
// compute BB average
1: (𝛼, count) = execute(𝑃 ′, 𝑡 )
2: ¯𝛼 = 𝛼/count
3: ˆ𝛼 = normalize( ¯𝛼, 𝐶)
4: 𝜔 = (1 − 𝑇exp) ˆ𝛼 + 𝑇exp // where 𝑇exp is the temperature
5: 𝑝 = 𝑝afl · 2𝑟 (2𝜔−1)
Output: Energy 𝑝 of 𝑡
// such that 𝑝 ∈ [2−𝑟 , 2𝑟 ]
Algorithm 2 shows the RGF power schedule. Given an input 𝑡
and the instrumented program 𝑃 ′, the execution of 𝑡 on 𝑃 ′ produces
the average amplified weight value ¯𝛼; Lines 1ś2).
normalize (Line 3). In order to make the weight value subject to
simulated annealing, we need to normalize the value into the range
between zero and one. Given the seed corpus 𝐶 and the weight
value 𝛼 (𝑡 ) for input 𝑡, we compute the normalized value 𝛼 ′ as
ˆ𝛼 =
𝛼 (𝑡 ) − min
𝑠 ∈𝐶
(𝛼 (𝑠))
max
𝑠 ∈𝐶
(𝛼 (𝑠)) − min
𝑠 ∈𝐶
(𝛼 (𝑠))
such that ˆ𝛼 ∈ [0, 1].
(1)
Omega 𝜔 (Line 4) is computed to address the exploration versus
exploitation trade-off when RGF is searching for inputs that exe-
cute code that has changed more recently or more frequently. The
formula is (1 − 𝑇exp) ˆ𝛼 + 0.5𝑇exp where 𝑇exp = 0.05𝑡 .𝑠𝑒𝑙𝑒𝑐𝑡𝑒𝑑 is the
temperature function for our simulated annealing. As the number
of times that the seed 𝑡 ∈ 𝐶 has been selected before increases,
𝜔 decreases (łcoolsž). The concrete behavior is shown in Figure 3
(left). During exploration (low temperature 𝑇exp), low- and high-
fitness seeds get the same weight 𝜔. That is, when the seed has
never been chosen, 𝜔 = 0.5. As the seed input 𝑡 is chosen more
often (high-temperature), 𝜔 approaches the product ˆ𝛼.
Power schedule 𝑝 (𝑡 ) (Line 5). Based on the annealed product of
the normalized, average amplified age and churn value 𝜔, RGF
computes the energy 𝑝 of seed 𝑡 as 𝑝 = 𝑝afl · 2𝑟 (2𝜔−1) where 𝑝afl
is the energy that the greybox fuzzer already assigns to the seed,
e.g., based on execution time and coverage information and where
𝑟 determines the range of 𝑝 as 𝑝 ∈ [2−𝑟 , 2𝑟 ]. The behavior of the
factor 𝑝RGF = 𝑝/𝑝afl (independent of the original power schedule
𝑝afl) is shown in Figure 3 (right). As the annealed product of the
normalized, average amplified age and churn value 𝜔 increases
from 0.5 to 1, the factor 𝑝RGF approaches 2𝑟 . As 𝜔 decreases from
0.5 to 0, the factor 𝑝RGF approaches 1
2𝑟 .
3.3 Ant Colony Optimisation (ACO)-based
Byte-Level Power Schedule
Instead of selecting all bytes with equal probability, we suggest to
learn the distribution of selection probabilities over the input bytes,
so as to increase the probability to yield inputs with better scores.
In our experiments, we observed that some bytes in the seed
are related more and others less to the execution of code that has
changed more often or more frequently. Suppose, we are fuzzing
ImageMagick which is widely used to process user-uploaded images
on web servers, and most recently the developers have worked on
the image transparency feature.
A common approach to trace input bytes to a given set of code
locations is called tainting [34]. Conceptually, different parts of the
input are assigned a color. In dynamic tainting, these colors are then
propagated along the execution through the source code, instruc-
tion by instruction. However, tainting comes with much analysis
overhead while performance is a key contributor to the success
of greybox fuzzing. Moreover, in our case there is no concrete set
of target code locations. There is only code that is more or less
łinterestingž. Thus tainting is impractical for our use case.
Instead, we suggest that RGF adaptively learns a distribution
over the inputs bytes that are related to generating inputs that
exercise "more interesting" code. For our ImageMagic example,
RGF selects image bytes with higher probability which are related
Session 7B: Fuzzing CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2173to the more recently changed image transparency (e.g., the alpha
value). An illustration of such a byte selection distribution is shown
in Figure 4. We can see that the probability to select bytes 8ś12
is substantially higher than the probability to select bytes 15ś17.
For our ImageMagick example, bytes 8ś12 might be related to the
recently changed image transparency feature while bytes 15ś17
are unrelated.
A key challenge of learning a distribution over the input bytes
is that the selection of bytes in later fuzzing iterations depends on
the selection of bytes in the earlier iterations. For instance, if, in
the very first iteration, we happened to choose the first three bytes,
were successful, increased the score of the first three bytes, and
repeated this, we might reinforce a spurious distribution with most
of the probability weight on the first three bytes.
To overcome this challenge, we propose to use the general idea
of Ant Colony Optimization (ACO)[12] where the individually byte
scores, over time, gravitate to a neutral zero score. The metaphor
stems from ants finding trails to a target by random movement. Ants
leave pheromones on their trails which make other ants follow this
trail. Successful trails are travelled on by more ants which also leave
more pheromone. However, over time the pheromone evaporates
which re-opens the opportunity to find better trails. Similarly, RGF
assigns a score to the bytes of an input based on the input’s fitness
relative to the seed. However, at constant intervals the score of all
bytes is multiplied by 0.9 to slowly "evaporate" old scores.
The concrete procedure is as follows.
(1) When a new input 𝑡 is added to the seed corpus,
• RGF computes the fitness ˆ𝛼 for seed 𝑡, i.e., the product of
the normalized, average amplified age and churn values,
and
• RGF sets the score for all bytes in 𝑡 to the neutral score
of zero. At this point, all bytes have equal probability of
being selected.
(2) When a new input 𝑡 ′ is generated by fuzzing the seed 𝑡,
• RGF computes the fitness ˆ𝛼 for input 𝑡 ′,
• RGF computes the fuzzed bytes in 𝑡 as the syntactic differ-
ence between 𝑡 and 𝑡 ′,
• If the fitness for 𝑡 ′ is higher than that of seed 𝑡, RGF incre-
ments the byte score for all fuzzed bytes.
(3) When the seed 𝑡 is chosen for fuzzing, RGF selects byte 𝑖 for
fuzzing with probability 𝑠𝑐𝑜𝑟𝑒 (𝑖)/𝐵 (𝑡 )
𝑗 =1 𝑠𝑐𝑜𝑟𝑒 ( 𝑗 ) where 𝐵 (𝑡 )
is the number of bytes in 𝑡.5 In order to efficiently sample
from a discrete distribution, RGF uses the alias method [37],
as explained below.
(4) In regular intervals, RGF multiplies all byte scores by a con-
stant smaller than one to slowly gravitate older byte scores
towards the neutral zero again.
For Step 3, we need to efficiently choose a random byte 𝐵 𝑗 from
a seed 𝑡 of size 𝑁 according to the probability weights {𝑝𝑖 }𝑁
1 , such
that 𝑗 : 1 ≤ 𝑗 ≤ 𝑁 . A simple and intuitive approach is to generate
a random number 𝑟 in range (cid:2)1, 𝑆
𝑖 =1 𝑝𝑖 (cid:3) and to find the first index
𝑗, such that 𝑟 >= 𝑗
𝑖 =1 𝑝𝑖 . However, the secret source of success of
greybox fuzzing is the swift generation of hundreds of thousands
of executions per second. The complexity of this simple method is
𝑂 (𝑁 ) which is too much overhead.
Instead, we propose to use the alias method which has a com-
plexity of 𝑂 (1). To capture the distribution of weights over the
bytes, an alias table is precomputed. Each element 𝐴[𝑖] in the alias
table corresponds to a byte 𝐵𝑖 in seed 𝑡. The element 𝐴[𝑖] in the
alias table includes two values: the probability 𝑝𝑖 to choose 𝐵𝑖 and
the alias byte 𝐵 𝑗 . Suppose, 𝐴[𝑖] is selected uniformly at random.
We compare 𝑝𝑖 to a number that is sampled uniformly at random
from the interval [0, 1]. If 𝑝𝑖 is larger than that number, we select
𝐵𝑖 . Otherwise, we select 𝐵 𝑗 . This gives a complexity that is constant
in the number of bytes 𝑁 .
4 EXPERIMENTAL SETUP
Our main hypothesis is that a fuzzer that is guided towards code
that has changed more recently or more often is also more efficient
in finding regression bugs. In our experiments, we evaluate this
hypothesis and test each heuristic individually.
4.1 Research Questions
RQ.1 Does the guidance towards code regions that have been
changed more recently or more frequently improve the effi-
ciency of greybox fuzzing? We compare AFL to AFLChurn
and measure (a) the average time to the first crash (overall
and per bug), (b) the number of crashing trials, and (c) the
number of unique bugs found.
RQ.2 What is the individual contribution of both heuristics, i.e.,
focusing on more recently changed code versus focusing
on frequently changed code? We compare the efficiency of
AFLChurn (a) guided only by age, (b) guided only by the
number of changes, and (c) guided by both.
RQ.3 Are crash locations typically changed more recently than the
average basic block? Are crash locations typically changed
more often than the average basic block?
4.2 Benchmark Subjects
Fuzzbench [20]. We conduct our evaluation within the Fuzzbench
fuzzer evaluation framework and used subjects with known re-
gressions from OSSFuzz [7]. Fuzzbench provides the computational
resources and software infrastructure to conduct an empirical eval-
uation of fuzzer efficiency (in terms of code coverage). We extended
our fork of Fuzzbench in several ways:
• New fuzzers. We added AFLChurn and its variants.6
• New benchmarks. We added our own regression benchmarks
as per the selection criteria stated below. We use the pro-
vided script to import a specific version of that project from
OSSFuzz to our Fuzzbench fork. To mitigate another threat
to validity, we maximize the number of unrelated changes
after the bug was introduced and import the specific revision
right before the bug is fixed.
• Deduplication. We integrated a simple but automated dedu-
plication method based on the Top-3 methods of the stack
trace. In our experiments, this method was quite reliable. To
5Common mutation operatorsÐthat require the selection of random bytesÐare bit
flipping, adding, deleting, substituting bytes or chunks of bytes.
6We also added both iterations of AFLGo. One "fuzzer" derives the distance information.
The other actually instruments and fuzzes the program.
Session 7B: Fuzzing CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2174Table 1: Regression Benchmark. Fifteen regression bugs in
fifteen open-source C programs.
Project O’flow OSSFuzz Regression Reported (days) Fixed (days)
14.Nov’18 (+0)
14.Nov’18 (+7)
20.Feb’19 (+0)
20.Feb’19 (+1)
13.Apr’19 (+23)
21.Mar’19 (+6)
20.Apr’19 (+0)
20.Apr’19 (+1)
17.May’19 (+5)
12.May’19 (+2)
17.Sep’19 (+1)
16.Sep’19 (+2)
04.Nov’19 (+40)
25.Sep’19 (+1)
28.Sep’19 (+2)
26.Sep’19 (+1)
12.Oct’19 (+3)
09.Oct’19 (+3)
23.Oct’19 (+79)
20.Dec’19 (+58)
20.Dec’19 (+408) 01.Dec’20 (+347)
11.Jan’20 (+6)
16.Nov’20 (+310)
24.Jan’20 (+11) 19.May’20 (+120)
25.Apr’20 (+1)
03.Jun’20 (+39)
11.Nov’20 (+208) 11.Nov’20 (+0)
libgit2 (read)
file (read)
picotls (read)
zstd (read)
systemd (write)
libhtp (read)
openssl (write)
libxml2 (read)
usrsctp (write)
aspell (write)
yara (read)
openvswitch (read)
unbound (read)
neomutt (write)
grok (read)
11382
13222
13837
14368
14708
17198
17715
17737
18080
18462
19591
20003
20308
21873
27386
07.Nov’18
19.Feb’19
14.Mar’19
18.Apr’19
10.May’19
14.Sep’19
24.Sep’19
25.Sep’19
06.Oct’19
05.Aug’19
07.Nov’18
05.Jan’20
13.Jan’20
24.Apr’20
17.Apr’20
be sure, we manually checked the deduped bugs and linked
them to OSSFuzz bug reports.
• Regression Analysis. We integrated scripts to automatically
extract the relevant data and write them as CSV files. We
wrote R scripts to automatically generate, from those CSV
files, graphs and tables for this paper.
Selection Criteria. In order to select our regression benchmark
subjects from the OSSFuzz repository, we chose the most recent
reports of regression bugs on the OSSFuzz bug tracker (as of Oct’20).
The bug report should be marked as a regression (i.e., the bug-
introducing commit has been identified and linked) and as verified
(i.e., the bug-fixing commit is available and has been linked). For
diversity, we chose at most one bug per project.
We select security-critical regressions and select bug reports
that were marked as such (Bug-Security). All bugs are memory-
corruption bugs, such as buffer overflows or use-after-frees. This
also mitigates a threat to validity when determining the age and
churn values for the root cause of the bugs. For buffer overflows,
root cause and crash location are often co-located.
We select the version (that can be built) right before the regres-
sion is fixed. In order to prevent AFLChurn from gaining an unfair
advantage, we seek to maximize the number of unrelated changes
since a bug was introduced. For each subject, instead of choos-
ing the version right after the bug was introduced, we choose the
version right before the bug was fixed.
We skip non-reproducible bugs as well as bugs that crash within
the first 10 seconds. For each bug report, OSSFuzz provides a wit-
ness of the bug, i.e., a crashing test input. It is straight-forward to
validate that this test input is still crashing. Very rarely, bugs were
not reproducible. Fuzzbench provides an option to test-run a sub-
ject compiled for a fuzzer (make test-run-[fuzzer]-[subject]).
This test-run takes about ten seconds. If this very short fuzzing
campaign produces at least one crashing input, we skip the subject.
We skip projects with submodules. A submodule is a specific
revision of another Git project that is imported into the current Git
project. For instance, we maintain pointers to the AFLChurn repos-
itory in our fork of Fuzzbench to consistently update all references
whenever needed. However, we cannot properly instrument such
code during compilation, which is a threat to construct validity. So,
we decided to skip projects that import other projects.
Subjects. Table 1 shows the selected subjects together with some
details. We have selected security critical-bugs in 15 different open
source C projects. Five of those allow to write to unallocated mem-
ory, which may facilitate arbitrary code execution attacks. The
majority of those regression bugs were found in less than three
days after they were introduced. For four of fifteen regressions,
it took more than a week (between 11 and 408 days) to discover
them. During deduplication, we found another eight regression
bugs in those 15 subjects all of which are known and associated
with OSSFuzz bug reports.
With our selection criteria, we identify 15 regression bugs in
15 programs (Table 1). In our experiments, after de-duplication,
we find that we have discovered 20 regression bugs (10 of the
15 identified bugs, plus 8 bugs we were not looking for, and 2
double-free and use-after-free bugs related to libxml2_17737
that are more likely exploitable. In total, we discovered 20 re-
gression bugs in 15 open-source C programs.
Concretely, we selected the following subjects. LibGit2 is a Git-
versioning library. File is tool and library to identify the file for-
mat for a file. Picotls is a TLS protocol implementation. Zstd is a
compression library written by Facebook developers. Systemd is
widely used across Linux operating systems to configure and ac-
cess OS components. Libhtp is an HTTP protocol implementation.
Libxml2 is a ubiquitous XML parser library. Usrsctp is a SCTP pro-
tocol implementation. Aspell is a spell checker. Yara is a Malware
pattern detector. Openvswitch is a switching stack for hardware
virtualization environments. Unbound is a DNS resolver. OpenSSL
is a well-known SSL / TLS protocol implementation. Neomutt is a
command-line email client. Grok is an image compression library.
4.3 Baseline
AFL [1]. We implemented regression greybox fuzzing into the grey-
box fuzzer AFL and call our tool AFLChurn. As all changes in
AFLChurn are related only to regression greybox fuzzing, using