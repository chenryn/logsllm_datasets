Classifier
IDS
Server A
Server B
Firewall
User Workstations
Figure 3. Network placement of the host and aggregate classiﬁers. Host classiﬁers monitor speciﬁc
server ﬂows for deviations from expected behavior. Aggregate classiﬁers monitor user trafﬁc to de-
termine if ﬂow behavior matches generalized behavior of other ﬂows of the same type.
The question the aggregate model tries to answer is: “What
other ﬂows does this ﬂow resemble?” In contrast, host mod-
els are based on the previously observed behavior of ﬂows
for a speciﬁc host. Given an unseen ﬂow, the host models
try to answer the question: “Does this ﬂow resemble previ-
ous server ﬂows from this host?”
Intrusion/misuse detection systems and ﬁrewalls try to
identify actions a priori as being harmful to the system or
network. An IDS may passively monitor trafﬁc and alert
in the presence of some attack condition. Firewalls actively
drop network packets that violate some network policy. Our
classiﬁcation method attempts to identify activities indica-
tive of intrusion or misuse after such an event has occurred.
Working in concert with a priori mechanisms, we can at-
tempt to determine at any moment in time whether there is
an impending attack or artifacts of a successful attack.
Figure 3 shows how our classiﬁcation methods can be
integrated into a network with an existing IDS. The or-
ganization uses servers to provide network services (inter-
nally, externally, or both) to some community of users. Our
host-ﬂow classiﬁcation system monitors the output of these
servers directly. The purpose is to determine if currently ob-
served ﬂows continue to behave as expected. If an attacker
manages to take control of a particular service, he/she will
need to interact with the server in such a way as to exactly
match the previous behavior. A trojaned web server that be-
haves like a Telnet sever when communicating with a se-
lect group of host addresses would not match the expected
host model, and thus be detected.
The network carries additional user trafﬁc to servers that
are external to the organization. This trafﬁc is monitored
with the aggregate model. Here, we classify the ﬂow gener-
ally and compare this to the port label. Observation of traf-
ﬁc that resembles Telnet to some non-standard server port
may be an indication of an installed backdoor. Trafﬁc la-
beled as web trafﬁc (with a server port of 80) that behaves
more like Telnet trafﬁc may indicate the presence of a proxy
used to evade ﬁrewall rules. A peer-to-peer client operat-
ing at some user-deﬁned port may be a violation of the net-
work policy. In each of these cases, the aggregate classiﬁer
can indicate if a given ﬂow behaves in a manner consis-
tent with its port label. It may not be necessary to monitor
every ﬂow – the system could be conﬁgured to randomly
select a ﬂow and attempt to classify it. If this ﬂow gener-
ally matches a ﬂow that is unusual or undesirable for a port
range, it can be identiﬁed and investigated.
Our method can operate on its own physical system, or
it may be part of the IDS or ﬁrewall. This decision will be
driven by the number of ﬂows the system will be expected to
monitor. Following the construction of the models, the sys-
tem makes simple and rapid classiﬁcations.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply. 
6. Subverting Classiﬁcation
Given the presence of a monitoring system described
above, we examined ways in which an attacker could ma-
nipulate a session in order to affect classiﬁcation of a server
ﬂow. We’ve previously seen one such example, albeit an in-
nocuous one, in our discussion of the Water Cooler Effect.
Here, the user suspends interaction thus causing variation
in the arrival time of packets and hence a potentially large
ﬂuctuation in the mean inter-arrival time measured across
the packet window for the server ﬂow. An attacker can do
the same thing. However, it is not clear that he/she would
be able to cause a particular classiﬁcation to be chosen. It
is more likely that he/she will alter the observations as to
cause some indeterminant class to be chosen. If a host clas-
siﬁer is being used, the deviation from the expected behav-
ior would trigger an alarm.
Another method might involve the use of extraneous
TCP ﬂags in packets sent to the server. An example might
be the use of the URG ﬂag in HTTP packets. The distri-
bution of TCP ﬂags in the corresponding server ﬂow may
or may not be affected, based on the implementation of the
server on that host. As with the effects of timing, we are in-
vestigating the sensitivity of classiﬁers to this manipulation
in future work.
7. Related Work
Previous work in ﬂow identiﬁcation has employed a vari-
ety of techniques and feature sets. Dunigan and Ostrouchov
used Principle Component Analysis (PCA) on two features
(packet inter-arrival time and length) to create signatures
for a variety of ﬂow types [10]. Their reported classiﬁcation
accuracies are comparable to our method. However, their
method requires off-line analysis. In contrast, our method
performs classiﬁcation in real time.
Tan and Collie used a modiﬁed neural network built on
a single feature (total number of bytes transmitted) [35].
They conﬁned their analysis to the classiﬁcation of Telnet
and FTP protocols. Their reported classiﬁcation accuracies
were generally lower than our method for these protocols.
Daniels [9] reports using a decision tree built with a sin-
gle feature (ﬁrst one hundred bytes of a packet) to classify
ﬂows. However, classiﬁcation accuracy was found to be in-
adequate for practical use.
There are a number of commercial products that attempt
to identify ﬂow type [23–25, 34]. These are primarily used
in the context of bandwidth allocation. For example, a net-
work administrator creates a policy stating that web traf-
ﬁc must not exceed a certain percentage of total bandwidth
and uses one of these products to selectively drop trafﬁc
when that policy is violated. Many details of the classiﬁca-
tion methods used by these products are not publicly avail-
able because they are proprietary. Thus, we are unable to
compare our method to those used in these products. It is
unclear whether any of these products can correctly classify
ﬂows in the presence of malicious activity (as described in
Section 1). One company, Packeteer [25], reports that their
product uses information “from all seven layers of the pro-
tocol stack” to create an application signature that is used
to classify ﬂows. As stated previously, such a system may
or may not be appropriate in an environment where pay-
load encryption is used or where there are concerns about
user privacy.
We have also identiﬁed a component of the Snort IDS
[29] that is used to classify server ﬂows. However, this sys-
tem relies on the port number and detection of the TCP 3-
way handshake. As stated previously, in the presence of a
proxy or compromised service, this system is unlikely to
classify a ﬂow correctly.
With respect to our feature set, the NATE (Network
Analysis of Anomalous Trafﬁc Events) [36] system is also
based on TCP ﬂags. NATE uses principle component anal-
ysis to identify that the TCP state ﬂags can detect certain
types of attacks. Our method differs in two respects. First,
the NATE system attempts to model differences between
normal trafﬁc and attack trafﬁc. They do not attempt to
model differences in behavior between protocols. The sec-
ond difference involves NATE’s use of clustering to identify
anomalies. This method must be done off-line, thus limit-
ing the usefulness of the system in environments requiring
near real-time detection. In contrast, once a decision tree has
been created, our system can monitor packets in real-time.
8. Conclusions
We have presented a novel approach for deﬁning a set of
features to model operational behavior of server ﬂow trafﬁc.
We demonstrated through the use of the C5.0 decision tree
algorithm that our features can differentiate the behavior of
server protocols with an accuracy of 82% to 100%. We illus-
trate empirically that aggregate models can classify an un-
seen server ﬂow as belonging to a family of previously seen
ﬂows, and that host models can determine whether ﬂows
from a given server match the behavior of previously seen
ﬂows from that server. These classiﬁers can augment tradi-
tional intrusion detection systems to detect artifacts of suc-
cessful attacks. Our techniques of classiﬁcation are inde-
pendent of packet labellings and are thus immune to tech-
niques that modify port numbers to conceal activity.
The decision tree classiﬁers can be sensitive to ﬂuctua-
tions in the inter-arrival time of packets. This was exempli-
ﬁed in what we call the Water Cooler Effect. We plan to in-
vestigate how this sensitivity can be mitigated to increase
the classiﬁcation accuracy for certain protocols.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply. 
We intend to further examine the use and augmenta-
tion of our feature set to model additional types of server
ﬂows. Application of our technique will be expanded to
other transport protocols, particularly those used by peer-
to-peer ﬁle sharing systems. Other feature sets are in de-
velopment to model UDP trafﬁc, ICMP trafﬁc, and selected
routing protocols.
9. Acknowledgments
This research is supported by AFRL grant num-
ber F30602-02-2-0217.
References
[1] 1999 darpa intrusion detection evaluation data
set.
URL http://www.ll.mit.edu/IST/
ideval/data/1999/1999\_data\_index.
html.
[2] RFC 1700: Assigned Port Numbers. URL ftp://
ftp.internic.net/rfc/rfc1700.txt.
[3] RFC 793 - Transmission Control protocol. URL ftp:
//ftp.internic.net/rfc/rfc0793.txt.
[4] Testing Intrusion detection systems: a critique of
the 1998 and 1999 DARPA intrusion detection sys-
tem evaluations as performed by Lincoln Laboratory.
ACM Transactions on Information and System Secu-
rity (TISSEC), 3(4):262–294, 2000. ISSN 1094-9224.
[5] S. Barbara and S. Jajodia. Applications of Data Min-
ing in Computer Security. Kluwer Academic Publish-
ers, 2002.
[6] K. Bennett and C. Campbell. Support Vector Ma-
chines: Hype or Hallelujah? SIGKDD Explorations,
2:1–13, 2000.
[7] J. Boxmeyer.
ONCTec - List of Possible Tro-
jan/Backdoor Port Activity. ONCTek, llc. URL
http://www.onctek.com/trojanports.
html.
[8] Cornell University Student Assembly Commit-
tee on Information and Technologies and ResNet.
Cornell Internet Usage Statistics.
URL http:
//www.cit.cornell.edu/computer/
students/bandwidth/charts.html.
[9] Thomas E. Daniels.
personal communication,
September 2003.
[10] Tom Dunigan and George Ostrouchov. Flow Charac-
terization for Intrusion Detection. Technical report,
Oak Ridge National Laboratory, November 2000.
[11] Y. Freund. Boosting a Weak Learning Algorithm by
Majority. Information and Computation, 121(2):256–
285, 1995.
[12] Saul Hansell. E-mail’s Backdoor Open to Spammers.
New York Times, May 20 2003.
[13] iNetPrivacy Software Inc. Antiﬁrewall. URL http:
//www.antifirewall.com/intro.htm.
[14] Internet Technical Resources.
Trafﬁc Statistics.
URL http://www.cs.columbia.edu/˜hgs/
internet/traffic.html.
[15] G. H. Kim and G. Spafford.
The Design and
Implementation of Tripwire: A File System In-
tegrity Checker.
In ACM Conference on Com-
puter and Communications Security, pages 18–29,
1994. URL citeseer.nj.nec.com/article/
kim94design.html.
[16] T. Lane and C. E. Brodley. Temporal Sequence Learn-
ing and Data Reduction for Anomaly Detection.
In
Proceedings of the Fifth ACM Conference on Com-
puter and Communications Security, pages 150–158.
Association for Computing Machinery, Nov 1998.
[17] T. Lane and C. E. Brodley.
Temporal Sequence
Learning and Data Reduction for Anomaly Detection.
ACM Transactions on Computer Security, 2(3):295–
331, 1999.
[18] W. Lee and S. Stolfo. A Framework for Construct-
ing Features and Models for Intrusion Detection Sys-
tems. ACM Transactions on Information and System
Security, 3(4):227–261, November 2000.
[19] M. Mahoney and P. K Chan. PHAD: Packet Header
Anomaly Detection for Indentifying Hostile Network
Trafﬁc. Technical Report CS-2001-4, Florida Tech,
2001.
[20] Evangelos P. Markatos. Tracing a Large-scale Peer to
Peer System : an Hour in the Life of Gnutella. Tech-
nical Report 298, 2001. URL citeseer.nj.nec.
com/markatos01tracing.html.
[21] T. Miller.
Detecting
Modules
incident-response.org/LKM.htm.
(LKM).
Loadable Kernel
http://www.
URL
[22] N. Murilo and K. Steding-Jessen. chkrootkit: A Tool
that Locally Checks for Signs of a Rootkit. URL
http://www.chkrootkit.org/.
[23] NetScreen Technologies, Inc. NetScreen-5000 Se-
URL http://www.netscreen.com/
ries.
products/datasheets/ds_ns_5000.jsp.
[24] Captus Networks. Captus ips 4000 series. URL
http://www.captusnetworks.com/.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply. 
[25] Packeteer, Inc.
URL
http://www.packeteer.com/resources/
prod-sol/PSDS.pdf.
Packeteer PacketShaper.
[26] P. A. Porras and P. G. Neumann. EMERALD: Event
Monitoring Enabling Responses to Anomalous Live
Disturbances. In Proc. 20th NIST-NCSC National In-
formation Systems Security Conference, pages 353–
365, 1997. URL http://citeseer.nj.nec.
com/porras97emerald.html.
[27] J. R. Quinlan. C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA, 1993.
[28] Ross Quinlan.
Data Mining Tools See5 and
URL http://www.rulequest.com/
C5.0.
see5-info.html.
[29] M. Roesch and C. Green. Snort - The Open Source
Network Intrusion Detection System. URL http://
www.snort.org/.
[30] Robert E. Schapire. A Brief Introduction to Boosting.
In IJCAI, pages 1401–1406, 1999. URL citeseer.
nj.nec.com/schapire99brief.html.
[31] Sharman Networks Ltd. . Kazaa Media. URL http:
//www.kazaa.com/us/.
[32] E. Skoudis. Counter Hack: A Step-by-Step Guide to
Computer Attacks and Effective Defenses. Prentice
Hall, 2002.
[33] J. St. Sauver. Percentage of Total Internet2 Traf-
ﬁc Consisting of Kazaa/Morpheus/FastTrack - Uni-
versity of Oregon.
In Collaborative Computing in
Higher Education: Peer-to-Peer and Beyond Work-
shop. URL http://darkwing.uoregon.edu/
˜joe/kazaa.html.
[34] Stampede Technologies, Inc. Turbogold enterprise
URL http://www.stampede.com/
edition.
productsclienttoserverfeaturesTraffic.
html.
[35] K. M. C. Tan and B. S. Collie. Detection and Classi-
ﬁcation of TCP/IP Network Services. In Proceedings
of the Thirteenth Annual Computer Security Applica-
tions Conference, pages 99–107, San Diego, Califor-
nia, December 1997.
[36] Carol Taylor and Jim Alves-Foss. NATE: Network
Analysis of Anomalous Trafﬁc Events, a low-cost ap-
proach. In Proceedings of the 2001 Workshop on New
Security Paradigms, pages 89–96. ACM Press, 2001.
ISBN 1-58113-457-6.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:48:39 UTC from IEEE Xplore.  Restrictions apply.