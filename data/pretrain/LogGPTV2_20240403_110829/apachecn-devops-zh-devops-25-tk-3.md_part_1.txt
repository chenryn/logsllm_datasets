# 三、收集和查询指标并发送警报
Insufficient facts always invite danger.
- *史巴克*
到目前为止，我们探索了如何利用 Kubernetes 的一些核心特性。我们使用了水平自动缩放器和集群自动缩放器。前者依赖于度量服务器，而后者不是基于度量，而是基于调度程序无法将 Pods 放置在现有集群容量内。尽管度量服务器确实提供了一些基本的度量，但我们迫切需要更多。
我们必须能够监控我们的集群，而度量服务器是不够的。它包含有限数量的度量，它使它们保持很短的时间，并且除了最简单的查询之外，它不允许我们执行任何事情。我不能说，如果我们只依赖于 Metrics Server，我们就是盲目的，而是严重受损。在不增加我们收集的指标数量及其保留率的情况下，我们只能对 Kubernetes 集群中的情况略知一二。
能够获取和存储指标本身并不是目标。我们还需要能够查询它们来寻找问题的原因。为此，我们需要度量来“丰富”信息，并且我们需要一种强大的查询语言。
最后，如果不能在第一时间得到有问题的通知，那么能够找到问题的原因是没有多大价值的。这意味着我们需要一个系统，允许我们定义警报，当达到特定阈值时，将向我们发送通知，或者在适当的时候，将通知发送到系统的其他部分，这些部分可以自动执行补救问题的步骤。
如果我们做到了这一点，我们将更接近拥有一个不仅能自我康复(Kubernetes 已经做到了)而且能对变化的条件做出反应的自适应系统。我们可能会走得更远，试图预测“坏事”将在未来发生，并在它们出现之前积极主动地解决它们。
总而言之，我们需要一个工具，或者一套工具，让我们能够获取和存储“丰富”的指标，让我们能够查询它们，并在问题发生时通知我们，或者更好的是，在问题即将发生时通知我们。
在本章中，我们可能无法构建自适应系统，但我们可以尝试创建一个基础。但是，首先，我们需要一个集群，允许我们“玩”一些新的工具和概念。
# 创建集群
我们将继续使用来自`vfarcic/k8s-specs`([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))存储库的定义。为了安全起见，我们先拉最新版本。
All the commands from this chapter are available in the `03-monitor.sh` ([https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9](https://gist.github.com/vfarcic/718886797a247f2f9ad4002f17e9ebd9)) Gist.
```
 1  cd k8s-specs
 2
 3  git pull
```
在本章中，我们将需要一些以前不是需求的东西，即使您可能已经使用过它们。
我们将开始使用用户界面，因此我们将需要 NGINX 入口控制器来路由来自集群外部的流量。我们还需要环境变量`LB_IP`和我们可以通过其访问工作节点的 IP。我们将使用它来配置一些入口资源。
用于测试本章示例的 Gists 如下。请照原样使用它们，或者作为灵感来创建您自己的集群，或者确认您已经拥有的集群是否符合要求。由于新的要求(入口和`LB_IP`)，所有集群设置指南都是新的。
A note to Docker for Desktop users
You'll notice `LB_IP=[...]` command at the end of the Gist. You'll have to replace `[...]` with the IP of your cluster. Probably the easiest way to find it is through the `ifconfig` command. Just remember that it cannot be `localhost`, but the IP of your laptop (for example, `192.168.0.152)`. A note to minikube and Docker for Desktop users
We have to increase memory to 3 GB. Please have that in mind in case you were planning only to skim through the Gist that matches your Kubernetes flavor.
要点如下。
*   `gke-monitor.sh` : **具有 3 个 n1-standard-1 工作节点的 GKE** 、 **nginx Ingress** 、 **tiller** 和存储在环境变量**LB _ IP**(https://gist . github . com/vfarcic/10e 14 bfbec 466347 D70 d 11 a 78 Fe 7 ee C4)中的集群 IP。
*   `eks-monitor.sh` : **具有 3 个 T2 .小型工作节点的 EKS** 、 **nginx Ingress** 、 **tiller** 、 **Metrics Server** 以及存储在环境变量 **LB_IP** 中的集群 IP(https://gist . github . com/vfarcic/211 F8 DBE 204131 f 8109 f 417605 dbdd D5)。
*   `aks-monitor.sh` : **带有 3 个 Standard_B2s 工作节点的 AKS** 、 **nginx Ingress** 和 **tiller** ，以及存储在环境变量 **LB_IP** 中的集群 IP。
*   `docker-monitor.sh` : **Docker for Desktop** 带有 **2 个 CPU**、 **3 GB RAM** 、**nginx intrusion**、 **tiller** 、 **Metrics Server** ，以及存储在环境变量**LB _ IP**([https://gist . github . com/vfarcic/4d 9ab 04058 cf 00 B9 DD 0 faac 11 BDA 8](https://gist.github.com/vfarcic/4d9ab04058cf00b9dd0faac11bda8f13)
*   `minikube-monitor.sh` : **带 **2 个 CPU**、 **3 GB RAM** 、**入口**、**存储提供程序**、**默认存储类**和**指标-服务器**插件已启用、**分蘖**和存储在环境变量 **LB_IP** 中的集群 IP([https://gist。](https://gist.github.com/vfarcic/892c783bf51fc06dd7f31b939bc90248)**
现在我们有了一个集群，我们需要选择工具来完成我们的目标。
# 选择用于存储和查询指标及警报的工具
**水平 Pods 自动缩放器** ( **HPA** )和**集群自动缩放器** ( **CA** )提供了缩放我们的 Pods 和集群的基本但非常初级的机制。
虽然它们确实可以很好地扩展，但它们不能解决我们在出现问题时需要得到提醒的问题，也不能提供找到问题原因所需的足够信息。我们需要用额外的工具来扩展我们的设置，这些工具将允许我们存储和查询指标，并在出现问题时接收通知。
如果我们专注于可以自己安装和管理的工具，那么使用什么就没有什么疑问了。如果我们看一下*云原生计算基金会(CNCF)* 项目([https://www.cncf.io/projects/](https://www.cncf.io/projects/))的名单，到目前为止(2018 年 10 月)只有两个毕业。那是*Kubernetes*和*普罗米修斯*([https://prometheus.io/](https://prometheus.io/))。假设我们正在寻找一个工具，允许我们存储和查询指标，普罗米修斯满足了这一需求，选择是直接的。这并不是说没有其他类似的工具值得考虑。有，但都是基于服务的。我们可能会在以后探索它们，但目前，我们专注于我们可以在集群中运行的那些。因此，我们将把普罗米修斯加入到这个组合中，并尝试回答一个简单的问题。什么是普罗米修斯？
Prometheus is a database (of sorts) designed to fetch (pull) and store highly dimensional time series data.
时间序列由一个度量名称和一组键值对来标识。数据存储在内存和磁盘上。前者允许快速检索信息，而后者是为了容错而存在的。
普罗米修斯的查询语言使我们能够轻松找到既可用于图表，更重要的是，可用于警报的数据。它并不试图提供“伟大”的可视化体验。为此，它与*格拉夫纳*([https://grafana.com/](https://grafana.com/))集成在一起。
与大多数其他类似工具不同，我们不会将数据推送到普罗米修斯。或者，更准确地说，这不是获取指标的常见方式。相反，普罗米修斯是一个基于拉的系统，定期从出口商那里获取指标。我们可以使用许多第三方出口商。但是，在我们的例子中，最重要的出口商被烤成了 Kubernetes。普罗米修斯可以从导出器中提取数据，导出器从库贝应用编程接口中转换信息。通过它，我们可以得到(几乎)我们可能需要的一切。或者，至少这是大部分信息的来源。
最后，如果我们在出现问题时没有得到通知，那么在普罗米修斯中存储度量标准将没有多大用处。即使我们真的将普罗米修斯与格拉夫纳集成在一起，那也只会为我们提供仪表板。我认为你有比盯着彩色图表更好的事情要做。因此，我们需要一种方法将普罗米修斯的警报发送到，比如说，Slack。幸运的是，*警报管理器*([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))允许我们这样做。它是由同一个社区维护的独立应用。
我们将通过实践练习来了解所有这些部分是如何结合在一起的。因此，让我们开始安装普罗米修斯、警报器管理器和一些其他应用。
# 普罗米修斯和警报器管理员简介
我们将继续使用 Helm 作为安装机制的趋势。普罗米修斯的掌舵图被保留为官方图表之一。你可以在项目的*自述文件*中找到更多信息。如果您关注*配置部分*([https://github . com/helm/charts/tree/master/stable/Prometheus # Configuration](https://github.com/helm/charts/tree/master/stable/prometheus#configuration))中的变量，您会注意到我们可以调整的东西相当多。我们不会讨论所有的变量。你可以查看官方文件。相反，我们将从一个基本的设置开始，并随着需求的增加进行扩展。
让我们看一下我们将作为开始使用的变量。
```
 1  cat mon/prom-values-bare.yml
```
输出如下。
```
server:
  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/ssl-redirect: "false"
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
  resources:
    limits:
      cpu: 100m
      memory: 1000Mi
    requests:
      cpu: 10m
      memory: 500Mi
alertmanager:
  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/ssl-redirect: "false"
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 5m
      memory: 10Mi
kubeStateMetrics:
  resources:
    limits:
      cpu: 10m
      memory: 50Mi
    requests:
      cpu: 5m
      memory: 25Mi
nodeExporter:
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
    requests:
      cpu: 5m
      memory: 10Mi
pushgateway:
  resources:
    limits:
      cpu: 10m
      memory: 20Mi
        requests:
      cpu: 5m
      memory: 10Mi
```
我们现在所做的就是为我们将要安装的所有五个应用定义`resources`，以及启用带有一些注释的入口，这些注释将确保我们不会被重定向到 HTTPS 版本，因为我们没有我们的专用域的证书。我们将深入研究稍后安装的应用。现在，我们将定义普罗米修斯和警报管理器用户界面的地址。
```
 1  PROM_ADDR=mon.$LB_IP.nip.io
 2
 3  AM_ADDR=alertmanager.$LB_IP.nip.io
```
让我们安装图表。
```
 1  helm install stable/prometheus \
 2      --name prometheus \
 3      --namespace metrics \
 4      --version 7.1.3 \
 5      --set server.ingress.hosts={$PROM_ADDR} \
 6      --set alertmanager.ingress.hosts={$AM_ADDR} \
 7      -f mon/prom-values-bare.yml
```
我们刚刚执行的命令应该是不言自明的，所以我们将跳转到输出的相关部分。
```
...
RESOURCES:
==> v1beta1/DaemonSet
NAME                     DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE
prometheus-node-exporter 3       3       0     3          0                 3s 
==> v1beta1/Deployment
NAME                          DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
prometheus-alertmanager       1       1       1          0         3s
prometheus-kube-state-metrics 1       1       1          0         3s
prometheus-pushgateway        1       1       1          0         3s
prometheus-server             1       1       1          0         3s
...
```
我们可以看到图表安装了一个 DeamonSet 和四个 Deployments。
DeamonSet 是节点导出器，它将在集群的每个节点上运行一个 Pod。它提供了普罗米修斯将提取的特定于节点的指标。第二个导出器(Kube 状态度量)作为单个副本部署运行。它从库贝应用编程接口获取数据，并将其转换为普罗米修斯友好的格式。这两者将提供我们所需的大部分指标。稍后，我们可能会选择与其他出口商一起扩大业务。目前，这两个指标以及直接从 Kube API 获取的指标应该提供比我们在单个章节中所能吸收的更多的指标。
进一步，我们得到了服务器，这是普罗米修斯本身。警报管理员会将警报转发到他们的目的地。最后，还有一个 Pushgateway，我们可能会在下面的章节中探讨。
在等待所有这些应用投入运行的同时，我们可能会探索它们之间的流程。
普罗米修斯服务器从出口商那里获取数据。在我们的例子中，这些是节点导出器和 Kube 状态度量。这些出口商的工作是从数据源获取数据，并将其转换为对普罗米修斯友好的格式。节点导出器从安装在节点上的`/proc`和`/sys`卷获取数据，而库贝状态度量从库贝应用编程接口获取数据。度量标准存储在普罗米修斯内部。
除了能够查询这些数据，我们还可以定义警报。当警报达到阈值时，它会被转发到充当十字路口的警报管理器。
根据其内部规则，它可以将这些警报进一步转发到不同的目的地，如 Slack、电子邮件和 HipChat(仅举几例)。
![](img/701f20e3-39b9-495f-b689-ccf64772ece1.png)
Figure 3-1: The flow of data to and from Prometheus (arrows indicate the direction)
到目前为止，普罗米修斯服务器可能已经推出。我们会确认以防万一。
```
 1  kubectl -n metrics \
 2      rollout status \
 3      deploy prometheus-server
```
让我们看看通过`prometheus-server`部署创建的 Pod 内部有什么。
```
 1  kubectl -n metrics \
 2      describe deployment \
 3      prometheus-server
```
输出限于相关部分，如下所示。
```
  Containers:
   prometheus-server-configmap-reload:
    Image: jimmidyson/configmap-reload:v0.2.2
    ...
   prometheus-server:
    Image: prom/prometheus:v2.4.2
    ...
```
除了基于`prom/prometheus`映像的容器，我们还从`jimmidyson/configmap-reload`中创建了另一个容器。后者的工作是每当我们更改存储在配置映射中的配置时重新加载普罗米修斯。
接下来，我们可能想看一下`prometheus-server`配置图，因为它存储了普罗米修斯需要的所有配置。
```
 1  kubectl -n metrics \
 2      describe cm prometheus-server
```