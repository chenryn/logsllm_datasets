is very hard to maintain.
V. PERFORMANCE EVALUATION
While our proposed adaptations make the Java platform less
vulnerable, will make it easier to maintain and will reduce
the chance for security-relevant mistakes while maintaining
and extending the platform, the question raises, as what these
changes cost in terms of runtime overhead. After all, the
shortcuts we remove were originally introduced for the sake
of reducing the runtime cost of permission checks [9]. Our
evaluation thus addresses the following research question:
Which runtime overhead does the code adaptation introduce?
A. Evaluation setup
To answer the research question, we transformed Open-
JDK 8 b132-03 mar 2014 as described in Section IV and
performed several experiments. As baseline we used the same
version of the OpenJDK without modiﬁcations. To ensure
maximum comparability, we built both the modiﬁed and the
unmodiﬁed version ourselves based on the ofﬁcial source
release [21].
We compared both variants in two different settings. In the
ﬁrst setting, we run the DaCapo benchmark suite [10] version
9.12-bach on both variants of the Java platform. The goal is
to measure the relative overhead that the transformations may
induce in the execution of real-world applications. We chose
DaCapo because it consists of complex, real-world applica-
tions from diverse application domains that cover a broad
range of possible program behaviors [10]. Using DaCapo’s
built-in functionality, we implemented a custom callback class
that performs 250 timed runs for each benchmark in each
setting, preceded by 750 warm-up runs. We chose such a high
number of iterations to minimize the effects of outliers that can
be caused by just-in-time compilation or other reasons. By
this, we maximize reproducibility of our results and ensure
that comparing runtime values is actually meaningful. The
following command was used to execute the tests:
java -Xcomp -XX:CompileThreshold=1
(cid:2)→ -server -Xmx2g -Xms2g -Xbatch
(cid:2)→ -cp ".;./mathlib.jar;./dacapo.jar"
(cid:2)→ Harness -t 1 -c callback benchmarkname
Due to a known bug in DaCapo [22], we had to mea-
sure jython runtimes without Xcomp-ﬂag. We were fur-
ther required to entirely skip the two benchmarks batik
and eclipse because their execution resulted in errors on
both the original and modiﬁed OpenJDK. eclipse failed
during its checksum validation,
the bench-
mark produced an unexpected output. We were able to re-
produce this problem with multiple original Java execution
indicating that
1034
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:24:58 UTC from IEEE Xplore.  Restrictions apply. 
environments on different machines. We have reported this
problem to the benchmark’s authors. batik fails with a
ClassNotFoundException, apparently because it ac-
cesses a class that is available in Oracle’s Java runtime but
not in the OpenJDK.
In addition to runtime measurements, we also counted the
number of method calls to any of the 32 modiﬁed methods
that are performed while executing the DaCapo benchmarks.
This allows us to reason about the coverage of the DaCapo
suite with respect to the proposed changes.
In the second setting, we ran both variants of the OpenJDK
on micro benchmarks. These micro benchmarks are artiﬁcial
test scenarios that we created for all transformed code loca-
tions. The goal is to assess the transformed code locations
without measuring inﬂuences by unaffected code. We do this
by calling the ﬁrst publicly accessible method that transitively
calls a modiﬁed method. The micro-benchmarking scenario
gives an insight into how much the removal of shortcuts costs
in the worst case, by calling modiﬁed methods frequently.
To measure the runtime we used JUnitBenchmarks version
0.72 [23]. We created a dedicated JUnit test case for each
modiﬁed method, each of which contains minimal setup code,
code to prevent dead code elimination [24], and a loop that
performs 10,000,000 calls to the method whose runtime is
to be measured. This high number of iterations is required
because a single invocation is too fast to be measured accu-
rately. JUnitBenchmarks computes the average and standard
deviation of the runtime of 10 rounds, and the 10 rounds are
preceded by 5 warmup rounds not included in measurements.
(A total run for a single test case thus triggers 150,000,000
calls.) The standard deviation is used to gauge the accuracy
of the results.
Lastly, we perform both experiments using two different
setups. In the ﬁrst setup we perform the experiments without
the presence of a security manager. This setup acts as our
baseline. In the second setup, we execute the experiments
with a security manager set programmatically, in case of micro
benchmarks, and by command line argument, in case of Da-
Capo (using VM arguments -Djava.security.manager
-Djava.security.policy). We use the security man-
ager with a policy ﬁle granting all permissions to all the
code. We manually veriﬁed that enabling the security manager
actually triggers permission checks performing stack walks at
runtime, despite the fact that the code has effectively the same
permissions as if no security manager were present.
All experiments were performed on a machine with an
Intel Core i5-2400, 3.1Ghz processor, with 4 GB of memory,
running a 64-bit Windows 7 Enterprise SP 1.
B. Results on DaCapo
Table I shows the results of the DaCapo benchmark suite.
Each benchmark is represented by one row in the table.
Column 1 shows the benchmark’s name. Columns 2 and 3
show the execution time in seconds without security manager
in place, along with the standard deviation for each value.
Column 4 shows the runtime difference as a factor to highlight
the cost of our proposed solution. Columns 5, 6, and 7 show
the same values measured with the security manager in place.
Comparing the runtimes of the original code and the mod-
iﬁed code, in almost all cases the difference lies below 1%.
In three cases, measurement results indicate that the modiﬁed
code is faster by 2%. In one exceptional case, the modiﬁed
code appears to be 3% faster. Taking all results into account,
the modiﬁed code is at most 1% slower than the original
code. We attribute these small runtime differences mostly to
instabilities of the JVM [24] rather than to code changes, and
can conﬁrm the observation of Gil et al. that even testing
identical code may lead to slightly different results in terms of
runtime. Furthermore, execution speed is inﬂuenced by sec-
ondary factors induced by the underlying software/hardware
stack, as previously studied by Gu et al. [25]
In addition to runtime measurements, we also collected call
statistics4 to ensure that the modiﬁed methods are actually
involved in benchmark execution. Our results clearly show that
this is the case. Table II shows a summary of the results we
measured for the original OpenJDK without a security man-
ager. Each of the twelve DaCapo benchmarks is represented
in one row. Column 1 shows the benchmark’s name, column 2
shows the total number of method calls to any of the modiﬁed
methods, column 3 shows the number of modiﬁed methods the
benchmark uses. Finally, column 4 shows the modiﬁed method
that was most frequently used by the respective benchmark.
As can be seen, the DaCapo benchmark suite extensively uses
most of the 32 methods under investigation. Running any of
the benchmarks requires at least 11 out of 32 methods, and 22
at most. Only eight out of 32 modiﬁed methods are not used
at all by DaCapo. Further, executing just a single run of one
of the benchmarks involves between 147 and 668,000 calls to
modiﬁed methods. A single run of the entire benchmark suite
requires more than 900,000 calls to the modiﬁed methods.
Summarizing the results, we conclude that the proposed
code changes have virtually no performance impact on the
tested real-world applications, even though the modiﬁed meth-
ods are heavily used. At a ﬁrst glance, this result seemed
surprising even to us, which is why we sought to conﬁrm
it through micro-benchmarks. . .
C. Results on micro benchmarks
the
shortcut methods
total number of
We implemented 32 tests using JUnitBenchmarks, which
equals
found
(35), excluding newInstance, getDeclaredField, and
checkMemberAccess. The former two methods we could
not modify, the latter we removed during program transfor-
mation (see Section IV). Each test performs 10,000,000 calls
to the method under investigation, in two exceptional cases,
due to long runtimes, we performed only 200,000 calls and
interpolated the results.
Table III shows a summary of the results of the micro
benchmarks. To avoid misunderstandings in the following
discussion, all results are shown in microseconds per single
4Complete call statistics are provided with the artifacts.
1035
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:24:58 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I
RUNTIMES OF DACAPO IN SECONDS
Without SecurityManager
With SecurityManager
Overhead
Overhead
Original
3.08 ±[0.15]
0.31 ±[0.01]
3.70 ±[0.01]
1.48 ±[0.02]
1.02 ±[0.12]
4.95 ±[0.02]
2.50 ±[0.02]
8.36 ±[0.03]
48.54 ±[0.28]
8.71 ±[0.02]
17.94 ±[1.27]
6.54 ±[0.04]
Modiﬁed
3.11 ±[0.10]
0.31 ±[0.01]
3.70 ±[0.01]
1.47 ±[0.02]
0.99 ±[0.05]
4.95 ±[0.02]
2.48 ±[0.02]
8.31 ±[0.02]
48.56 ±[0.31]
8.83 ±[0.03]
17.67 ±[0.89]
6.60 ±[0.04]
abs.
0.03
0.00
0.00
-0.01
-0.03
0.00
-0.02
-0.05
0.02
0.12
-0.27
0.06
Original
rel.
3.02 ±[0.06]
1%
0.32 ±[0.01]
1%
3.67 ±[0.01]
0%
1.50 ±[0.02]
-1%
1.20 ±[0.07]
-2%
4.99 ±[0.01]
0%
3.06 ±[0.03]
-1%
8.36 ±[0.04]
-1%
0% 52.54 ±[0.81]
1% 10.01 ±[0.05]
-2% 23.22 ±[1.44]
6.76 ±[0.03]
1%
∅ -0.25%
Modiﬁed
3.06 ±[0.02]
0.32 ±[0.01]
3.70 ±[0.01]
1.47 ±[0.03]
1.21 ±[0.08]
4.86 ±[0.02]
3.05 ±[0.04]
8.34 ±[0.03]
52.35 ±[0.35]
10.02 ±[0.02]
23.54 ±[2.02]
6.79 ±[0.02]
abs.
0.04
0.00
0.00
-0.03
0.01
-0.13
-0.01
-0.02
-0.19
0.01
0.32
0.03
rel.
1%
1%
1%
-2%
1%
-3%
0%
0%
0%
0%
1%
1%
∅ 0.08%
Project
avrora
fop
h2
jython
luindex
lusearch
pmd
sunﬂow
tomcat
tradebeans
tradesoap
xalan
Project
avrora
fop
h2
jython
luindex
lusearch
pmd
sunﬂow
tomcat
tradebeans
tradesoap
xalan
TABLE II
CALL STATISTICS FOR DACAPO
11
11
147
6,329
210
1,483
208
159
1,885
249
32,069
Calls Methods Most freq. used
getClassLoader
getContext-
ClassLoader
getClassLoader
getMethod
getClassLoader
getClassLoader
getClassLoader
getClassLoader
getDeclared-
Methods