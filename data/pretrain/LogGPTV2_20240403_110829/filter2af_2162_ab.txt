    docres = lda.fit_transform(cntTf) #使用lda算法计算关键词
    def print_top_words(model, feature_names, n_top_words):
        #打印每个主题下权重较高的term
        for topic_idx, topic in enumerate(model.components_):
            print("Topic #%d:" % topic_idx)
            print(" ".join([feature_names[i]
                            for i in topic.argsort()[:-n_top_words - 1:-1]]))
    n_top_words=6
    tf_feature_names = cntVector.get_feature_names()
    print_top_words(lda, tf_feature_names, n_top_words)
结果：
    Topic #0:
    based use similar modelling topic lda
LDA算法也是一种无监督学习算法，无需样本训练即可在目标上使用。不过由于其需要对词频进行计算，故算法在较长的文本中会有更好的效果。
## 聚类提取
聚类算法是一类无监督学习算法，主要指自动将没有标记的数据划分为几类的方法。在前文中提到word2vec等词向量模型可以在向量空间中更好的反应语义信息。故可以通过对文本中的词向量进行聚类分析，将文本中的词汇划分为不同类别，并根据词频从不同的类别中选取关键词。目前主要的聚类算法分为基于划分、基于层次、基于密度、基于网格和基于模型等类型。在词向量聚类的场景下，词向量通过余弦相似度（即两个词向量的cos值）反映了语义的相似度，而由于归一化后的cos距离和欧式距离（即两个坐标点的直线距离）线性相关，故可以通过基于划分的方法对数据进行聚类（基于划分的方法主要依靠计算距离来评估两个数据间的相关性），在此对其中较知名的kmeans方法做介绍。
  1. 选取k个对象作为每一个簇的中心； 
  2. 计算其他对象与各簇中心的距离，并将其归类到最近的簇；
  3. 计算每个簇的距离平均值，更新簇中心； 
  4. 不断重复2、3
一个demo如下：
    from gensim.models import Word2Vec
    from sklearn.cluster import KMeans
    #语料
    sentences = [['this', 'is', 'a', 'pear'],
                ['this', 'is',  'a','banana'],
                ['this', 'is',  'an','apple'],
                ['this','is','a','lemon'],
                ['this','is','a','melon'],
                ['this','is','a','cherry']]
    #训练词向量
    model = Word2Vec(sentences, min_count=1)
    wordvector = []
    for key in model.wv.vocab.keys():
        #逐一获取每个词汇的对应词向量
        wordvector.append(model[key])
    #通过余弦相似度获取与apple最近义的词
    print (model.most_similar(positive=['apple'], negative=[], topn=2))
    #结果为[('is', 0.19642898440361023), ('pear', 0.15730774402618408)]
    #可见出现了pear，但仍有改善的余地
    #使用kmeans算法对语料进行聚类，共分为2类
    clf = KMeans(n_clusters=2)
    clf.fit(wordvector)
    labels = clf.labels_
    classCollects={}
    keys = model.wv.vocab.keys()
    for i in range(len(keys)):
        if labels[i] in list(classCollects.keys()):
            classCollects[labels[i]].append(list(keys)[i])
        else:
            classCollects={0:[],1:[]}
    print(classCollects[0])
    print(classCollects[1]))
    '''
    结果为：
    ['pear', 'banana', 'apple', 'lemon', 'melon']
    ['is', 'a', 'an', 'cherry']
    '''
对词向量聚类在理论上有很好的潜力，但在实践中也存在问题（在demo中也有体现）。其一是kmeans算法中初始选取的中心对最后的结果有一定影响;其二是词向量一般维度很高，而基于距离的聚类算法在高维空间的表现会有所下降（降维过度又会造成向量损失必要的信息）。故而词向量聚类在实际使用中需要根据具体场景做优化。
# 命名实体识别
命名实体识别是NLP技术中的重要组成部分，其目的是识别出文本当中的特定实体，并进而将其中的关键信息抽取出来，从而将非结构化的文本数据转换为结构化的可机读数据，为各类自动化任务提供数据支撑。在威胁建模或情报分析当中，命名实体识别可以快速的把关键信息从多个来源，不同格式的文本中抽取出来。
命名实体识别的方法基本可分为三类：
  * 基于规则/字典识别 
通过指定的字典或预先设定的正则表达式匹配文本中的实体，这一方法主要用于识别特定格式的数据，如版本号，CVE编号等。目前往往会将规则与其他方法一起使用进行实体识别工作。
  * 基于统计识别
通过对不同词性，不同特征的词汇进行统计，计算特定实体在其位置上出现的概率和上下文转换概率等。目前常用的方法有最大熵模型，隐式马尔科夫(HMM),支持向量机(SVM)等。
  * 基于深度学习识别 
深度学习方法本质也可归为统计识别的一种，实体识别本质上也可视为一个分类问题（即识别词汇是否属于特定的命名实体），故也可以使用图像识别领域常用的神经网络技术。通过使用人工标注了实体的文本对进行神经网络进行训练，即可获得一个用于识别命名实体的深度学习模型。目前常见的模型有CNN-CRF，RNN-CRF等。
上图为一个demo，可见一个训练过的神经网络，可以从漏洞描述信息中识别出漏洞类型（VULN），产品名称（PRODUCT），产品版本（VERSION），漏洞危害（CAPIBILITY），漏洞处发点（SINK），payload（PAYLOAD）等，从而将文本转化为一份可机读的数据。这一技术也是试图通过NLP进行信息收集和数据分析的关键所在，高水平的命名实体识别可以对自动化威胁情报分析和威胁建模有很大帮助。
# 结语
NLP领域还有多个技术概念，如词性标注，词干分析，文本情感判定等，由于此类技术在安全领域应用有限，受限于篇幅，本文不做更多介绍，可以根据自身需求做更多了解。
* * *