# 05 \| 调度系统："数据不动代码动"到底是什么意思？你好，我是吴磊。在日常的开发与调优工作中，为了充分利用硬件资源，我们往往需要手工调节任务并行度来提升CPU 利用率，控制任务并行度的参数是 Spark的配置项：spark.default.parallelism。增加并行度确实能够充分利用闲置的CPU 线程，但是，parallelism数值也不宜过大，过大反而会引入过多的调度开销，得不偿失。这个调优技巧可以说是老生常谈了，网上到处都可以搜得到。那你知道为什么parallelism数值过大调度开销会呈指数级增长吗？调度开销具体又是指什么呢？以及，如果不想一个数值一个数值的尝试，parallelism数值究竟该怎么设置，才能以最少的时间获得最好的效果？如果你还没有答案，或者说还没有把握答对，接下来你就要好好听我讲。这一讲，我会通过一个机器学习案例，来和你一起聊聊调度系统是什么，它是怎么工作的，从而帮助你摆脱调优总是停留在知其然、不知其所以然的尴尬境地。案例：对用户兴趣特征做 Label Encoding在机器学习应用中，特征工程几乎占据了算法同学 80%的时间和精力，毕竟，一份质量优良的训练样本限定了模型效果的上限和天花板，我们要讲的案例就来自特征工程中一个典型的处理场景：LabelEncoding（标签编码）。什么是 Label encoding呢？模型特征按照是否连续可以分为两类：连续性数值特征和离散型特征，离散型特征往往以字符串的形式存在，比如用户兴趣特征就包括体育、政治、军事和娱乐等。对于很多机器学习算法来说，字符串类型的数据是不能直接消费的，需要转换为数值才行，例如把体育、政治、军事、娱乐映射为0、1、2、3，这个过程在机器学习领域有个术语就叫 Labelencoding。 我们这一讲的案例，就是要对用户兴趣特征做 Labelencoding，简单来说就是以固定的模板把字符串转换为数值，然后将千亿条样本中的用户兴趣转换为对应的索引值。固定模板是离线模型训练与线上模型服务之间的文件接口，内容仅包含用户兴趣这一列，字符串已按事先约定好的规则进行排序。我们需要注意的是，用户兴趣包含4个层级，因此这个模板文件较大，记录数达到万级别。    //模板文件    //用户兴趣    体育-篮球-NBA-湖人    军事-武器-步枪-AK47那具体怎么转换呢？例如，我们可以将用户兴趣"体育 - 篮球 -NBA-湖人"映射为 0，将兴趣"军事 - 武器 - 步枪 -AK47"映射为1，以此类推。应该说，需求还是相当明确的，我身边的同学们拿到需求之后，奔儿都没打，以迅雷不及掩耳之势就实现了如下的处理函数。    /**    实现方式1    输入参数：模板文件路径，用户兴趣字符串    返回值：用户兴趣字符串对应的索引值    */         //函数定义    def findIndex(templatePath: String, interest: String): Int = {    val source = Source.fromFile(filePath, "UTF-8")    val lines = source.getLines().toArray    source.close()    val searchMap = lines.zip(0 until lines.size).toMap    searchMap.getOrElse(interest, -1)    }         //Dataset中的函数调用    findIndex(filePath, "体育-篮球-NBA-湖人")我们可以看到这个函数有两个形参，一个是模板文件路径，另一个是训练样本中的用户兴趣。处理函数首先读取模板文件，然后根据文件中排序的字符串构建一个从兴趣到索引的Map 映射，最后在这个 Map中查找第二个形参传入的用户兴趣，如果能找到则返回对应的索引，找不到的话则返回-1。 这段代码看上去似乎没什么问题，同学们基于上面的函数对千亿样本做 Labelencoding，在 20 台机型为 C5.4xlarge AWS EC2 的分布式集群中花费了 5个小时。坦白说，这样的执行性能，我是不能接受的。你可能会说："需求就是这个样子，还能有什么别的办法呢？"我们不妨来看另外一种实现方式。    /**    实现方式2    输入参数：模板文件路径，用户兴趣字符串    返回值：用户兴趣字符串对应的索引值    */         //函数定义    val findIndex: (String) => (String) => Int = {    (filePath) =>    val source = Source.fromFile(filePath, "UTF-8")    val lines = source.getLines().toArray    source.close()    val searchMap = lines.zip(0 until lines.size).toMap    (interest) => searchMap.getOrElse(interest, -1)    }    val partFunc = findIndex(filePath)         //Dataset中的函数调用    partFunc("体育-篮球-NBA-湖人")同学们基于第二种方式对相同的数据集做 Label encoding 之后，在 10台同样机型的分布式集群中花了不到 20分钟就把任务跑完了。可以说，执行性能的提升是显而易见的。那么，两份代码有什么区别呢？我们可以看到，相比于第一份代码，第二份代码的函数体内没有任何变化，还是先读取模板文件、构建Map映射、查找用户兴趣，最后返回索引。最大的区别就是第二份代码对高阶函数的使用，具体来说有2 点： 1.       处理函数定义为高阶函数，形参是模板文件路径，返回结果是从用户兴趣到索引的函数；        2.       封装千亿样本的 Dataset 所调用的函数，不是第一份代码中的    findIndex，而是用模板文件调用 findIndex 得到的 partFunc，partFunc    是形参为兴趣、结果为索引的普通标量函数。        那么，高阶函数真有这么神奇吗？其实，性能的提升并不是高阶函数的功劳，而是调度系统在起作用。Spark 的调度系统是如何工作的？Spark 调度系统的核心职责是，**先将用户构建的 DAG转化为分布式任务，结合分布式集群资源的可用性，基于调度规则依序把分布式任务分发到执行器**。这个过程听上去就够复杂的了，为了方便你理解，我们还是先来讲一个小故事。土豆工坊流水线升级在学完了内存计算的第二层含义之后，土豆工坊的老板决定对土豆加工流水线做升级，来提高工坊的生产效率和灵活性。这里，我们先对内存计算的第二层含义做个简单地回顾，它指的是**同一 Stage中的所有操作会被捏合为一个函数，这个函数一次性会被地应用到输入数据上，并且一次性地产生计算结果**。升级之前的土豆加工流程 DAG 被切分为 3 个执行阶段 Stage，它们分别是Stage 0、Stage 1、Stage 2。其中，Stage 0 产出即食薯片，Stage 1分发调味品，Stage 2 则产出不同尺寸、不同风味的薯片。我们重点关注 Stage0，Stage 0 有 3 个加工环节，分别是清洗、切片和烘焙。这 3 个环节需要 3种不同的设备，即清洗机、切片机和烤箱。![](Images/e1304235e66624954ccaadbe6a40c368.png)savepage-src="https://static001.geekbang.org/resource/image/3f/5f/3fcb3e400db91198a7499c016ccfb45f.jpg"}土豆工坊加工流程的3个执行阶段工坊有 3 条流水线，每种设备都需要 3套，在成本方面要花不少钱呢，因此工坊老板一直绞尽脑汁想把设备方面的成本降下来。此时，工头儿建议："老板，我听说市场上有一种可编程的土豆加工设备，它是个黑盒子并且只有输入口和输出口，从外面看不见里面的操作流程。不过黑盒子受程序控制，给定输入口的食材，我们可以编写程序控制黑盒子的输出。有了这个可编程设备，咱们不但省了钱，将来还可以灵活地扩充产品线。比方想生产各种风味的薯条或是土豆泥，只需要更换一份程序加载到黑盒子里就行啦！"老板听后大喜，决定花钱购入可编程土豆加工设备，替换并淘汰现有的清洗机、切片机和烤箱。于是，工坊的加工流水线就变成了如下的样子。工人们的工作也从按照 DAG流程图的关键步骤，在流水线上安装相应的设备，变成了把关键步骤编写相应的程序加载到黑盒内。这样一来，这家工坊的生产力也从作坊式的生产方式，升级到了现代化流水线的作业模式。![](Images/4168191f11d54a8373e477af18a79cf7.png)savepage-src="https://static001.geekbang.org/resource/image/dc/46/dc4f5f39a166ca93080c5a7c0ea0d446.jpg"}演进的土豆加工流水线那么，这个故事跟我们今天要讲的调度系统有什么关系呢？事实上，Spark调度系统的工作流程包含如下 5个步骤： **1. 将 DAG 拆分为不同的运行阶段Stages；** **2. 创建分布式任务 Tasks 和任务组TaskSet；** **3. 获取集群内可用****的****硬件资源情况；****4. 按照调度规则决定优先调度哪些任务 /组；** **5. 依序将分布式任务分发到执行器Executor。**除了第 4步以外，其他几步和土豆工坊流水线上的关键步骤都是一一对应的，它们的对应关系如下：![](Images/16966dc337a5b7625ca3e9bdaf3646d6.png)savepage-src="https://static001.geekbang.org/resource/image/63/83/635462108ee2fc09991708f0856bcb83.jpg"}现在，你可能会觉得用故事来记这几个步骤好像多此一举，但当我们学完了所有的原理之后，再回过头来把故事的主线串联起来，你就会惊喜地发现，所有的原理你都能轻松地记住和理解，这可比死记硬背的效率要高得多。调度系统中的核心组件有哪些？接下来，我们深入到流程中的每一步去探究 Spark调度系统是如何工作的。不过在此之前，我们得先弄清楚调度系统都包含哪些关键组件，不同组件之间如何交互，它们分别担任了什么角色，才能更好地理解流程中的每一步。Spark 调度系统包含 3 个核心组件，分别是 DAGScheduler、TaskScheduler和 SchedulerBackend。这 3 个组件都运行在 Driver进程中，它们通力合作将用户构建的 DAG转化为分布式任务，再把这些任务分发给集群中的 Executors去执行。不过，它们的名字都包含Scheduler，光看名字还真是丈二和尚摸不着头脑，所以我把它们和调度系统流程中5个步骤的对应关系总结在了下表中，你可以看一看。![](Images/3fa3fce519a9652b5886251202b454fa.png)savepage-src="https://static001.geekbang.org/resource/image/46/52/46bb66fed5d52b09407d66881cf0df52.jpeg"}1. DAGSchedulerDAGScheduler 的主要职责有二：一是把用户 DAG 拆分为Stages，如果你不记得这个过程可以回顾一下上一讲的内容；二是在 Stage 内创建计算任务Tasks，这些任务囊括了用户通过组合不同算子实现的数据转换逻辑。然后，执行器Executors 接收到Tasks，会将其中封装的计算函数应用于分布式数据分片，去执行分布式的计算过程。不过，如果我们给集群中处于繁忙或者是饱和状态的 Executors分发了任务，执行效果会大打折扣。因此，**在分发任务之前，调度系统得先判断哪些节点的计算资源空闲，然后再把任务分发过去**。那么，调度系统是怎么判断节点是否空闲的呢？2. SchedulerBackendSchedulerBackend就是用来干这个事的，它是对于资源调度器的封装与抽象，为了支持多样的资源调度模式如Standalone、YARN 和 Mesos，SchedulerBackend提供了对应的实现类。在运行时，Spark 根据用户提供的MasterURL，来决定实例化哪种实现类的对象。MasterURL就是你通过各种方式指定的资源管理器，如 \--masterspark://ip:host（Standalone 模式）、\--master yarn（YARN模式）。 对于集群中可用的计算资源，SchedulerBackend 会用一个叫做ExecutorDataMap 的数据结构，来记录每一个计算节点中 Executors的资源状态。ExecutorDataMap 是一种 HashMap，它的 Key 是标记 Executor的字符串，Value 是一种叫做 ExecutorData 的数据结构，ExecutorData用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配CPU 核数等等，它相当于是对 Executor做的"资源画像"。![](Images/ccc729e7ca938b1ce02dcf1a20d72e91.png)savepage-src="https://static001.geekbang.org/resource/image/a7/a9/a7f8d49bbf1f8b0a125ffca87f079aa9.jpg"}ExecutorDataMap映射表总的来说，对内，SchedulerBackend 用 ExecutorData 对 Executor进行资源画像；对外，SchedulerBackend 以 WorkerOffer为粒度提供计算资源，WorkerOffer 封装了 Executor ID、主机地址和 CPU核数，用来表示一份可用于调度任务的空闲资源。显然，基于 Executor资源画像，SchedulerBackend 可以同时提供多个 WorkerOffer用于分布式任务调度。WorkerOffer 这个名字起得蛮有意思，Offer的字面意思是公司给你提供的工作机会，结合 Spark调度系统的上下文，就变成了使用硬件资源的机会。好了，到此为止，要调度的计算任务有了，就是 DAGScheduler 通过 Stages创建的 Tasks；可用于调度任务的计算资源也有了，即 SchedulerBackend提供的一个又一个 WorkerOffer。如果从供需的角度看待任务调度，DAGScheduler就是需求端，SchedulerBackend就是供给端。3. TaskScheduler左边有需求，右边有供给，如果把 Spark调度系统看作是一个交易市场的话，那么中间还需要有个中介来帮它们对接意愿、撮合交易，从而最大限度地提升资源配置的效率。在Spark 调度系统中，这个中介就是TaskScheduler。**TaskScheduler的职责是，基于既定的规则与策略达成供需双方的匹配与撮合**。![](Images/4a444af144e872e44055d450b7e7be6f.png)savepage-src="https://static001.geekbang.org/resource/image/82/yy/82e86e1b3af101100015bcfd81f0f7yy.jpg"}Spark分布式任务调度流程显然，TaskScheduler的核心是任务调度的规则和策略，**TaskScheduler 的调度策略分为两个层次，一个是不同 Stages之间的调度优先级，一个是 Stages内不同任务之间的调度优先级**。首先，对于两个或多个Stages，如果它们彼此之间不存在依赖关系、互相独立，在面对同一份可用计算资源的时候，它们之间就会存在竞争关系。这个时候，先调度谁、或者说谁优先享受这份计算资源，大家就得基于既定的规则和协议照章办事了。**对于这种 Stages 之间的任务调度，TaskScheduler 提供了 2种调度模式，分别是 FIFO（先到先得）和FAIR（公平调度）。** FIFO 非常好理解，在这种模式下，Stages按照被创建的时间顺序来依次消费可用计算资源。这就好比在二手房交易市场中，两个人同时看中一套房子，不管两个人各自愿意出多少钱，谁最先交定金，中介就优先给谁和卖家撮合交易。你可能会说："这不合常理啊！如果第二个人愿意出更多的钱，卖家自然更乐意和他成交。"没错，考虑到开发者的意愿度，TaskScheduler提供了 FAIR 公平调度模式。在这种模式下，哪个 Stages优先被调度，取决于用户在配置文件 fairscheduler.xml中的定义。 在配置文件中，Spark允许用户定义不同的调度池，每个调度池可以指定不同的调度优先级，用户在开发过程中可以关联不同作业与调度池的对应关系，这样不同Stages的调度就直接和开发者的意愿挂钩，也就能享受不同的优先级待遇。对应到二手房交易的例子中，如果第二个人乐意付30%的高溢价，中介自然乐意优先撮合他与卖家的交易。说完了不同 Stages 之间的调度优先级，我们再来说说同一个 Stages内部不同任务之间的调度优先级，Stages内部的任务调度相对来说简单得多。**当 TaskScheduler 接收到来自 SchedulerBackend 的WorkerOffer 后，TaskScheduler会优先挑选那些满足本地性级别要求的任务进行分发**。众所周知，本地性级别有 4 种：Process local \ dict.contains(word))    keywords.cache    keywords.count    keywords.map((_, 1)).reduceByKey(_ + _).collect整个代码片段包含 6行代码，咱们从上到下逐一分析。首先，第一行定义了 dict 字典，这个字典在 Driver 端生成，它在后续的RDD 调用中会随着任务一起分发到 Executor 端。第二行读取 words.csv文件并生成 RDD words。**第三行很关键，用 dict 字典对 words 进行过滤，此时 dict已分发到 Executor 端，Executor 将其存储在堆内存中，用于对 words数据分片中的字符串进行过滤。Dict字典属于开发者自定义数据结构，因此，Executor 将其存储在 User Memory区域。** 接着，第四行和第五行用 cache 和 count 对 keywords RDD进行缓存，以备后续频繁访问，分布式数据集的缓存占用的正是 Storage Memory内存区域。在最后一行代码中，我们在 keywords 上调用 reduceByKey对单词分别计数。我们知道，reduceByKey 算子会引入 Shuffle，而 Shuffle过程中所涉及的内部数据结构，如映射、排序、聚合等操作所仰仗的Buffer、Array 和 HashMap，都会消耗 Execution Memory区域中的内存。不同代码与其消耗的内存区域，我都整理到了下面的表格中，方便你查看。![](Images/1ea33294d6ef6671b757fdd732bdd784.png)savepage-src="https://static001.geekbang.org/resource/image/a0/ff/a05f77a27aaaf21d9d064aa1ca1be3ff.jpeg"}小结深入理解内存管理的机制，有助于我们充分利用应用的内存，提升其执行性能。今天，我们重点学习了内存管理的基础知识。**首先是内存的管理方式。**Spark 区分堆内内存和堆外内存：对于堆外内存来说，Spark通过调用 Java Unsafe 的 allocateMemory 和 freeMemory方法，直接在操作系统内存中申请、释放内存空间，管理成本较高；对于堆内内存来说，无需Spark 亲自操刀而是由 JVM 代理。但频繁的 JVM GC对执行性能来说是一大隐患。另外，Spark对堆内内存占用的预估往往不够精确，高估可用内存往往会为 OOM埋下隐患。 **其次是统一内存管理，以及 Execution Memory 和 Storage Memory之间的抢占规则**。它们就像黄四郎招租故事中黄小乙和张麻子的田地，抢占规则就像他们之间的占地协议，主要可以分为3 条： 1.  如果对方的内存空间有空闲，那么双方都可以抢占；        2.  对 RDD 缓存任务抢占的执行内存，当执行任务有内存需要时，RDD    缓存任务必须立即归还抢占的内存，其中涉及的 RDD    缓存数据要么落盘、要么清除；        3.  对分布式计算任务抢占的 Storage Memory 内存空间，即便 RDD    缓存任务有收回内存的需要，也要等到任务执行完毕才能释放。        **最后是不同代码对不同内存区域的消耗。**内存区域分为 Reserved Memory、User Memory、ExecutionMemory 和 Storage Memory。其中，Reserved Memory 用于存储 Spark内部对象，User Memory 用于存储用户自定义的数据结构，Execution Memory用于分布式任务执行，而 Storage Memory 则用来容纳 RDD缓存和广播变量。好了，这些就是内存管理的基础知识。当然了，与内存相关的话题还有很多，比如内存溢出、RDD缓存、内存利用率，以及执行内存的并行计算等等。在性能调优篇，我还会继续从内存视角出发，去和你探讨这些话题。每日一练1.       你知道启用 off-heap 之后，Spark    有哪些计算环节可以利用到堆外内存？你能列举出一些例子吗？        2.       相比堆内内存，为什么在堆外内存中，Spark    对于内存占用量的预估更准确？        3.       结合我在下面给定的配置参数，你能分别计算不同内存区域（Reserved、User、Execution、Storage）的具体大小吗？        ![](Images/8fb7faf6870874d32285cbca632c6d55.png)savepage-src="https://static001.geekbang.org/resource/image/fd/66/fdb2fb17120e4d047d5ccd28d1434b66.jpeg"}期待在留言区看到你的思考和答案，我们下一讲见！