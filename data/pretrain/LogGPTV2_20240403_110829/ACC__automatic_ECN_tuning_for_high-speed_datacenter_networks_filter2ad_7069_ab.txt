long tail latency. On the contrary, SECN2, using relatively high
threshold, is more friendly to throughput sensitive traffic. Simi-
lar to the original DCQCN paper [65], the conservative setting of
SECN0 is not the optimal setting. When using the single thresh-
old, i.e., Kmax = Kmin = C × RTT × λ, where RTT denotes the
average round-trip time, C denotes link capacity, and λ is a param-
eter related to realistic network environment, we see the following
problems: 1) it is hard to estimate the value of λ. Specifically, device
vendors may have customized rate control in the hardware imple-
mentation. 2) the value of RTT is not stable in realistic multi-hop
network with middleboxs. ECN# [62] shows that the actual RTT
varies vastly in production environment (around 3×). It is hard to
find an appropriate static threshold to balance queue occupancy
and throughput.
Observation 3: Parameter tuning is complicated and time
consuming. Operators always struggle to determine the appropri-
ate value for CC parameters. For example, DCQCN mainly has 9
parameters at end-host and 3 ECN parameters at switch. DCTCP has
5 parameters, HPCC has 3 parameters and ECN# has 4 parameters,
etc. It usually takes a few weeks or months to evaluate the perfor-
mance of throughput and latency under the given sets of traffic
demands, and perform stress testing to emulate all traffic patterns,
like many to one traffic demands, various flow sizes, and different
scheduling approaches, failure scenarios, etc. The experienced op-
erators need to have a deep understanding of the requirements of
applications and network behaviors at high-resolution. Addition-
ally, production networks usually consist of devices from different
NIC and switch vendors. For example, our servers are configured
with NICs from two major NIC vendors. Hence, we need to tune
the threshold for each port of the leaf switch. To ensure the sta-
bility, the operators usually adopt a conservative version of static
ECN parameters. At run time, to guarantee the performance of low
386
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
traffic trace. During the online training, the agent interacts with
the environment and adapts to optimize the action. In detail, the
RL-agent is based on the pre-trained Neutral Network (NN) model.
The network monitor collects data from switches and feeds the
data to RL-agent and reward generator. The measurement includes
queue depth, throughput and flow information (e.g. number of
ECN-marked packets). Then the RL-agent makes an action to ECN
configurator, which maps the ECN value into ECN template of for-
warding chip. Meanwhile, RL-agent updates the NN model with
the reward.
3.2 Design Choice
Centralized Design. The ideal ECN configuration should be able
to provide high throughput and low latency for different appli-
cations. Besides, it is adaptive to the variable traffic patterns and
different network scale. Intuitively, we can apply a centralized DRL
agent like a centralized controller to implement the automatic ECN
tuning. The topology information could be collected via the link
layer discovery protocol and the network state could be collected
via the control interface at each switch. All the collected data are
transferred to a central controller. Unfortunately, when applying
the centralized DRL-based automatic ECN tuning in the high-speed
DCN, we found the following practical problems:
• The large space of network states and actions. In the cen-
tralized design, the agent has the information of topology and
states of all switches, including throughput, queue length, num-
ber of packets transferred, PFC frames received, packets with
ECN marking, etc. For example, a large-scale DCN with 1K
switches and 48 ports in each switch. Assume two queues of
each port are assigned for RDMA traffic which apply automatic
ECN tuning. Thus the vector of network states consists of at least
1K × 48 × 2 × N F elements, where N F is the number of collected
features. Assume that each feature has 10 values. The state space
is 1096K×N F . On the other hand, the ECN configuration has 3
parameters {Kmax , Kmin, Pmax }. The marking threshold varies
from a few KB to tens of MB. Assume each parameter has 10
intervals, the action space is as large as {10 × 10 × 10}96K . It is
hard to make accurate decisions online for such large network
states-actions space. Though the topology of datacenter is sym-
metric, the traffic is not equally distributed due to the spatial and
temporal variation of tenants’ traffic. With regard to the large
amount of states and actions, the centralized model will take long
time to converge and acquires large computation resources.
• The long latency for collecting network state and updat-
ing ECN configuration. In modern high-speed datacenters, the
round-trip latency is about ∼10 microseconds. Therefore, the
update of ECN configuration should be in a few microseconds
to minimize the influence of congestion on application perfor-
mance. In the centralized design, though it can tolerate several
milliseconds to update DRL model during online training phase,
it has only a few microseconds to make decisions during DRL in-
ference phase. In practice, the centralized controller takes several
milliseconds to collect data from all switches, model inference
and set actions to all switches. Therefore, the centralized design
would suffer from the long delay to deal with network congestion.
Figure 3: Overview of ACC: applying RL for the setting of
ECN marking thresholds
latency traffic, the operators have to keep low network utilization
and deploy new applications separately in new PoDs [29].
3 ACC
We aim to address the above issues with an effective "zero-configuration"
approach. The proposed approach should be easy to deploy in pro-
duction datacenters, be able to operate alongside existing equip-
ments, require no modification to the existing network stack, and
maintain good performance under diverse traffic loads and network
scales. Learning-based approaches have great potential to dynam-
ically adjust the ECN marking threshold. They avoid the costly
procedure of manual tuning of parameters for a specific network
environment. Thus, we propose a learning-based ECN tuning based
on the operation experience of datacenter networks.
3.1 Overview
Reinforcement Learning. Tuning ECN dynamically can be for-
mulated as an Reinforcement learning problem (RL). RL[46] is a
learning setting that an agent learns from the interactions with
the environment. Markov Decision Process is usually used as the
mathematical formalization for reinforcement learning.
Definition 1 (Markov Decision Process) A Markov decision process
(MDP) is a 4-tuple M =, where S is a state space,
A is a set of actions, P(St +1|St , a) is the probability of action a ∈ A
at state St will lead to state St +1, R(S, a) is the intermediate reward
after executing action a from state St , and γ ∈ [0, 1] is the discount
factor that controls how much we favor immediate rewards over
those from distant future.
The goal of reinforcement learning is to learn an optimal policy
π∗ that maps from states to actions and maximizes the discounted
t =0 γ t rt}. This goal is
equivalent to finding the optimal state-action value Q∗(S, a) =
k =0 γ krk +1|St = S, at = a}, which defines the value of
The problem of automatically tuning ECN is formalized as an
MDP. Specifically, we divide time into consecutive monitoring in-
terval (∆t). At time slot t, the agent records the state (network
statistics) St , takes an action (ECN configuration) at and receives a
reward rt .
ACC Framework. Figure 3 depicts our general framework for ap-
plying RL to set ECN marking thresholds. To speed up the training,
we apply both offline training and online training. The offline train-
ing is performed to get the pre-trained model based on the collected
accumulated rewards π∗ = arдmaxπ Eπ {∞
maxπ Eπ {∞
taking an action a at state S.
387
ACC: Automatic ECN Tuning for High-Speed Datacenter Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
• The large bandwidth consumption to collect data. If the
ECN tuning period is 100 µs, and we only collect 4 features per
queue (4 bytes per feature) and transfer data to a decision node
by using UDP packet (header consumption is 46B), the total
amount of data to be collected is 1K × 48 × 2 × (4 × 4B + 46B) =
5.952MB. Hence, the bandwidth consumption for collecting data
is 5.952MB/100µs = 476Gbps, which is a huge overhead for the
system. Besides, it is a hazard to introduce a large amount of
in-band monitoring traffic in the datacenter network.
Distributed Design. To overcome the aforementioned issues, we
introduce a distributed design to achieve dynamic ECN tuning at
runtime. Each switch is associated with a DRL agent. These agents
form a multi-agent system. Each DRL agent observes the local
queuing states and chooses an action for ECN setting independently
based on the reward function. We explain and perform a simulation-
based study to demonstrate the effectiveness of distributed design
compared with the centralized design (see Section 5.4).
• Distributed design takes only the local network states and makes
decision to dramatically reduce the state-action space. Since fewer
features are used and fewer decisions are made in the agent, the
state-action space is much smaller compared to the centralized
design. As a result, the convergence of the learning process is
fast [37]. To further accelerate the convergence, we carefully
select several key features to represent the network state when
designing the state of the agent. (Section 3.3)
• Distributed design reduces transmission latency by avoiding
inter-device communication. The ECN tuning process is com-
pleted within the switch, and takes only a few microseconds,
which is at the time scale of RTT in the datacenter. DCQCN re-
acts on timescale of microseconds. Its control loop interacts with
ACC well to get enough time to become stable. (Section 3.3)
• Distributed design avoids inter-device data transmission. Each
local agent make decisions independently. The state information
is transferred within the switch via the inter-chip connection
lane. (Section 4)
3.3 Problem Formalization
A learning agent is associated with each switch. The agent collects
local port’s network information, and then makes and executes an
ECN configuration decision. We apply Deep Q-learning (DQN) [52]
model to design the DRL agent.
State: States represent the environment information that is applied
as the input of an agent, i.e., the network congestion risk. Here,
we represent states as collectible statistics which can be measured
on the fly from each switch. To be compatible with the cloud data-
centers which usually contain switches from different vendors, we
choose the features commonly supported by major switch chip ven-
dors [49–51] as candidates. Based on the operational experience, as
a result, we consider four features, i.e. current queuing length (qlen),
output data rate for each link (txRate), output rate of ECN marked
packets for each link (txRate(m)) and current ECN setting(ECN(c)).
Instead of feeding the values of gathered statistics to the agent di-
rectly, we use normalized value since normalization helps the agent
generalize different network environments. Specifically, to evaluate
the variations of queue length and throughput in continuous time
slots, we apply the queue state of past k monitoring slots as the
state information for each tuning inference.
Action: The action at time slot t is defined as the ECN setting, i.e.
high marking threshold (Kmax ), low marking threshold (Kmin),
and marking probability (Pmax ).
at = {Kmax , Kmin, Pmax}t
To reduce action space, we discretize ECN tuning action space
to form the ECN configuration template at switch. We choose dis-
cretization for the ECN marking threshold. We have tested sev-
eral settings with fine-granularity level. It demonstrates that the
throughput is not sensitive to the high marking threshold when it is
larger than 1MB. Therefore, we choose coarse-grained settings here
to minimize the action space, e.g. {1MB, 2MB, 5MB, 10MB} since
the maximal buffer size of each queue in the commodity switch chip
is usually less than 10MB. For the low marking threshold, setting
multiple intervals in a short range is helpful to achieve fine-grained
adjustment on marking packets during congestion. To formalize
this characteristics, an intuitive way is to use the exponential func-
tion. We introduce an exponential function (1) to determine the
discrete action value E(n)
(1)
where α is 20 in our system, and Kmin is no greater than Kmax .
E(n) = α × 2nKB, n = 0, .., 9
For the marking probability Pmax , when the Pmax adjustment in-
terval is more than 5%, the network throughput or delay have more
than 1% change. Thus the uniform discretization is recommended,
i.e. Pmax ∈ {1%, j × 5%}, ∀j ∈ [1, 20].
ACC collects statistic data, makes and executes an ECN con-
figuration decision at each time interval ∆t. If ACC’s adjustment
period ∆t is at the same scale with the reaction time of congestion
control, ACC can seriously affect the performance of the existing
congestion control scheme. For example, DCQCN takes a few RTTs
in the control loop to respond to the congestion signal. To avoid
interference with CC rate control, we choose one order of magni-
tude more than RTT as the action period to adjust ECN marking
thresholds.
Reward: Reward function indicates the effectiveness of the action.
From the network operators’ point of view, we usually select the
key network performance metrics of latency and throughput as
the reward function. However, for the distributed design, we can
only achieve the local information from switch. To ensure high link
utilization and low queue buildup, we define the reward function
as follows. The average throughput of one egress queue is repre-
sented by txRate, i.e., the amount of data that have been delivered
to the link during the time interval ∆t. We normalize the txRate
by the link bandwidth BW to represent the link utilization. The
latency is represented by the average queue length L to indicate the
impact of queuing delay. We select the average value instead of the
instantaneous queue depth because the instant queue length varies
in a large range, which can make reward unstable. Consequently,
we define the reward function as a trade-off between latency and
throughput, i.e. a trade-off between high link utilization and low
queue buildup for each switch:
r = ω1 × T(R) + ω2 × D(L)
(2)
388
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
Algorithm 1 ACC’s Learning Algorithm
Figure 4: Mapping function of queue length reward
where T(R) = txRate/BW denotes the utilization of link. Consid-
ering that applications are more sensitive to latency, we design
D(L) = 1 − n/10, where n = arдminn(E(n) ≥ L),∀n ∈ [0, 9]. L
denotes the queue length. Here, D(L) is a step mapping function
as shown in Figure 4. The lower queue length the better. We pro-
vide an in-depth rational analysis of reward design in Appendix
.1. ω1, ω2 are the weights to representing the utility-delay tradeoff,
ω1 +ω2 = 1. The network operator can easily set the reward parame-
ters based on the requirement of running applications. For example,
ω1=0.7 and ω2=0.3 are recommended in our storage system.
Markov Property: One requirement of the MDP is that a state
is able to summarise past sensations compactly so that all rele-
vant information is retained [46]. Here, we follow the assumption
that the network statistics from the past k monitoring intervals
is sufficient to summarise the variation of network statistics. k is
usually determined via experimental results. More specifically, we
have trained the model with different historical periods of network
states (k=1, 3, 5) and evaluated the performance. k = 3 suffices to
represent network congestion while avoiding introduce large state
space. Hence, we use 4 × 3 = 12 features in total to represent the
state of DRL agent.
3.4 DRL Algorithm:
We show that the optimization of ECN can be formulated as a DRL
problem. Deep Q-network (DQN) [38] is a basic DRL algorithm,
where Q(Sj , a, θj) is represented using a deep neural network with
parameter θj (the evaluation network). The evaluation network is
updated at the iteration j using the following loss function: