While each question on the LCE Checklist is simple, much complexity is built in to
what prompted the question and the implications of its answer. In order to fully
understand this degree of complexity, a new LCE hire requires about six months of
training.
As the volume of launches grew, keeping pace with the annual doubling of Google’s
engineering team, LCEs sought ways to streamline their reviews. LCEs identified cat‐
Development of LCE | 385
egories of low-risk launches that were highly unlikely to face or cause mishaps. For
example, a feature launch involving no new server executables and a traffic increase
under 10% would be deemed low risk. Such launches were faced with an almost triv‐
ial checklist, while higher-risk launches underwent the full gamut of checks and bal‐
ances. By 2008, 30% of reviews were considered low-risk.
Simultaneously, Google’s environment was scaling up, removing constraints on many
launches. For instance, the acquisition of YouTube forced Google to build out its net‐
work and utilize bandwidth more efficiently. This meant that many smaller products
would “fit within the cracks,” avoiding complex network capacity planning and provi‐
sioning processes, thus accelerating their launches. Google also began building very
large datacenters capable of hosting several dependent services under one roof. This
development simplified the launch of new products that needed large amounts of
capacity at multiple preexisting services upon which they depended.
Problems LCE Didn’t Solve
Although LCEs tried to keep the bureaucracy of reviews to a minimum, such efforts
were insufficient. By 2009, the difficulties of launching a small new service at Google
had become a legend. Services that grew to a larger scale faced their own set of prob‐
lems that Launch Coordination could not solve.
Scalability changes
When products are successful far beyond any early estimates, and their usage increa‐
ses by more than two orders of magnitude, keeping pace with their load necessitates
many design changes. Such scalability changes, combined with ongoing feature addi‐
tions, often make the product more complex, fragile, and difficult to operate. At some
point, the original product architecture becomes unmanageable and the product
needs to be completely rearchitected. Rearchitecting the product and then migrating
all users from the old to the new architecture requires a large investment of time and
resources from developers and SREs alike, slowing down the rate of new feature
development during that period.
Growing operational load
When running a service after it launches, operational load, the amount of manual
and repetitive engineering needed to keep a system functioning, tends to grow over
time unless efforts are made to control such load. Noisiness of automated notifica‐
tions, complexity of deployment procedures, and the overhead of manual mainte‐
nance work tend to increase over time and consume increasing amounts of the
service owner’s bandwidth, leaving the team less time for feature development. SRE
has an internally advertised goal of keeping operational work below a maximum of
386 | Chapter 27: Reliable Product Launches at Scale
50%; see Chapter 5. Staying below this maximum requires constant tracking of sour‐
ces of operational work, as well as directed effort to remove these sources.
Infrastructure churn
If the underlying infrastructure (such as systems for cluster management, storage,
monitoring, load balancing, and data transfer) is changing due to active development
by infrastructure teams, the owners of services running on the infrastructure must
invest large amounts of work to simply keep up with the infrastructure changes. As
infrastructure features upon which services rely are deprecated and replaced by new
features, service owners must continually modify their configurations and rebuild
their executables, consequently “running fast just to stay in the same place.” The solu‐
tion to this scenario is to enact some type of churn reduction policy that prohibits
infrastructure engineers from releasing backward-incompatible features until they
also automate the migration of their clients to the new feature. Creating automated
migration tools to accompany new features minimizes the work imposed on service
owners to keep up with infrastructure churn.
Solving these problems requires company-wide efforts that are far beyond the scope
of LCE: a combination of better platform APIs and frameworks (see Chapter 32),
continuous build and test automation, and better standardization and automation
across Google’s production services.
Conclusion
Companies undergoing rapid growth with a high rate of change to products and serv‐
ices may benefit from the equivalent of a Launch Coordination Engineering role.
Such a team is especially valuable if a company plans to double its product developers
every one or two years, if it must scale its services to hundreds of millions of users,
and if reliability despite a high rate of change is important to its users.
The LCE team was Google’s solution to the problem of achieving safety without
impeding change. This chapter introduced some of the experiences accumulated by
our unique LCE role over a 10-year period under exactly such circumstances. We
hope that our approach will help inspire others facing similar challenges in their
respective organizations.
Conclusion | 387
PART IV
Management
Our final selection of topics covers working together in a team, and working as
teams. No SRE is an island, and there are some distinctive ways in which we work.
Any organization that aspires to be serious about running an effective SRE arm needs
to consider training. Teaching SREs how to think in a complicated and fast-changing
environment with a well-thought-out and well-executed training program has the
promise of instilling best practices within a new hire’s first few weeks or months that
otherwise would take months or years to accumulate. We discuss strategies for doing
just that in Chapter 28, Accelerating SREs to On-Call and Beyond.
As anyone in the operations world knows, responsibility for any significant service
comes with a lot of interruptions: production getting in a bad state, people requesting
updates to their favorite binary, a long queue of consultation requests…managing
interrupts under turbulent conditions is a necessary skill, as we’ll discuss in Chap‐
ter 29, Dealing with Interrupts.
If the turbulent conditions have persisted for long enough, an SRE team needs to start
recovering from operational overload. We have just the flight plan for you in Chap‐
ter 30, Embedding an SRE to Recover from Operational Overload.
We write in Chapter 31, Communication and Collaboration in SRE, about the different
roles within SRE; cross-team, cross-site, and cross-continent communication; run‐
ning production meetings; and case studies of how SRE has collaborated well.
Finally, Chapter 32, The Evolving SRE Engagement Model, examines a cornerstone of
the operation of SRE: the production readiness review (PRR), a crucial step in
onboarding a new service. We discuss how to conduct PRRs, and how to move
beyond this successful, but also limited, model.
Further Reading from Google SRE
Building reliable systems requires a carefully calibrated mix of skills, ranging from
software development to the arguably less-well-known systems analysis and engi‐
neering disciplines. We write about the latter disciplines in “The Systems Engineering
Side of Site Reliability Engineering” [Hix15b].
Hiring SREs well is critical to having a high-functioning reliability organization, as
explored in “Hiring Site Reliability Engineers” [Jon15]. Google’s hiring practices have
been detailed in texts like Work Rules! [Boc15],1 but hiring SREs has its own set of
particularities. Even by Google’s overall standards, SRE candidates are difficult to find
and even harder to interview effectively.
1 Written by Laszlo Bock, Google’s Senior VP of People Operations.
CHAPTER 28
Accelerating SREs to On-Call and Beyond
How Can I Strap a Jetpack to My Newbies
While Keeping Senior SREs Up to Speed?
Written by Andrew Widdowson
Edited by Shylaja Nukala
You’ve Hired Your Next SRE(s), Now What?
You’ve hired new employees into your organization, and they’re starting as Site Relia‐
bility Engineers. Now you have to train them on the job. Investing up front in the
education and technical orientation of new SREs will shape them into better engi‐
neers. Such training will accelerate them to a state of proficiency faster, while making
their skill set more robust and balanced.
Successful SRE teams are built on trust—in order to maintain a service consistently
and globally, you need to trust that your fellow on-callers know how your system
works,1 can diagnose atypical system behaviors, are comfortable with reaching out for
help, and can react under pressure to save the day. It is essential, then, but not suffi‐
cient, to think of SRE education through the lens of, “What does a newbie need to
learn to go on-call?” Given the requirements regarding trust, you also need to ask
questions like:
• How can my existing on-callers assess the readiness of the newbie for on-call?
• How can we harness the enthusiasm and curiosity in our new hires to make sure
that existing SREs benefit from it?
1 And doesn’t work!
391
• What activities can I commit our team to that benefit everyone’s education, but
that everyone will like?
Students have a wide range of learning preferences. Recognizing that you will hire
people who have a mix of these preferences, it would be shortsighted to only cater to
one style at the expense of the others. Thus, there is no style of education that works
best to train new SREs, and there is certainly no one magic formula that will work for
all SRE teams. Table 28-1 lists recommended training practices (and their corre‐
sponding anti-patterns) that are well known to SRE at Google. These practices repre‐
sent a wide range of options available for making your team well educated in SRE
concepts, both now and on an ongoing basis.
Table 28-1. SRE education practices
Recommended patterns Anti-patterns
Designing concrete, sequential learning experiences for Deluging students with menial work (e.g., alert/ticket triage) to
students to follow train them; “trial by fire”
Encouraging reverse engineering, statistical thinking, and Training strictly through operator procedures, checklists, and
working from fundamental principles playbooks
Celebrating the analysis of failure by suggesting Treating outages as secrets to be buried in order to avoid blame
postmortems for students to read
Creating contained but realistic breakages for students to Having the first chance to fix something only occur after a
fix using real monitoring and tooling student is already on-call
Role-playing theoretical disasters as a group, to Creating experts on the team whose techniques and knowledge
intermingle a team’s problem-solving approaches are compartmentalized
Enabling students to shadow their on-call rotation early, Pushing students into being primary on-call before they achieve
comparing notes with the on-caller a holistic understanding of their service
Pairing students with expert SREs to revise targeted Treating on-call training plans as static and untouchable except
sections of the on-call training plan by subject matter experts
Carving out nontrivial project work for students to Awarding all new project work to the most senior SREs, leaving
undertake, allowing them to gain partial ownership in the junior SREs to pick up the scraps
stack
The rest of this chapter presents major themes that we have found to be effective in
accelerating SREs to on-call and beyond. These concepts can be visualized in a blue‐
print for bootstrapping SREs (Figure 28-1).
392 | Chapter 28: Accelerating SREs to On-Call and Beyond
Figure 28-1. A blueprint for bootstrapping an SRE to on-call and beyond
This illustration captures best practices that SRE teams can pick from to help boot‐
strap new members, while keeping senior talent fresh. From the many tools here, you
can pick and choose the activities that best suit your team.
The illustration has two axes:
• The x-axis represents the spectrum between different types of work, ranging from
abstract to applied activities.
• The y-axis represents time. Read from the top down, new SREs have very little
knowledge about the systems and services they’ll be responsible for, so postmor‐
tems detailing how these systems have failed in the past are a good starting point.
New SREs can also try to reverse engineer systems from fundamentals, since
they’re starting from zero. Once they understand more about their systems and
have done some hands-on work, SREs are ready to shadow on-call and to start
mending incomplete or out-of-date documentation.
Tips for interpreting this illustration:
• Going on-call is a milestone in a new SRE’s career, after which point learning
becomes a lot more nebulous, undefined, and self-directed—hence the dashed
lines around activities that happen at or after the SRE goes on-call.
You’ve Hired Your Next SRE(s), Now What? | 393
• The triangular shape of project work & ownership indicates that project work
starts out small and builds over time, becoming more complex and likely con‐
tinuing well after going on-call.
• Some of these activities and practices are very abstract/passive, and some are
very applied/active. A few activities are mixes of both. It’s good to have a variety
of learning modalities to suit different learning styles.
• For maximum effect, training activities and practices should be appropriately
paced: some are appropriate to undertake straightaway, some should happen
right before an SRE officially goes on-call, and some should be continual and
ongoing even by seasoned SREs. Concrete learning experiences should happen for
the entire time leading up to the SRE going on-call.
Initial Learning Experiences: The Case for Structure Over
Chaos
As discussed elsewhere in this book, SRE teams undertake a natural mix of proactive2
and reactive3 work. It should be a strong goal of every SRE team to contain and
reduce reactive work through ample proactivity, and the approach you take to
onboarding your newbie(s) should be no exception. Consider the following all-too-
common, but sadly suboptimal, onboarding process:
John is the newest member of the FooServer SRE team. Senior SREs on this team are
tasked with a lot of grunt work, such as responding to tickets, dealing with alerts, and
performing tedious binary rollouts. On John’s first day on the job, he is assigned all
new incoming tickets. He is told that he can ask any member of the SRE team to help
him obtain the background necessary to decipher a ticket. “Sure, there will be a lot of
upfront learning that you’ll have to do,” says John’s manager, “but eventually you’ll get
much faster at these tickets. One day, it will just click and you’ll know a lot about all of
the tools we use, the procedures we follow, and the systems we maintain.” A senior
team member comments, “We’re throwing you in the deep end of the pool here.”
This “trial by fire” method of orienting one’s newbies is often born out of a team’s cur‐
rent environment; ops-driven, reactive SRE teams “train” their newest members by
making them…well, react! Over and over again. If you’re lucky, the engineers who are
already good at navigating ambiguity will crawl out of the hole you’ve put them in.
But chances are, this strategy has alienated several capable engineers. While such an
approach may eventually produce great operations employees, its results will fall
short of the mark. The trial-by-fire approach also presumes that many or most
aspects of a team can be taught strictly by doing, rather than by reasoning. If the set
2 Examples of proactive SRE work include software automation, design consulting, and launch coordination.
3 Examples of reactive SRE work include debugging, troubleshooting, and handling on-call escalations.
394 | Chapter 28: Accelerating SREs to On-Call and Beyond
of work one encounters in a tickets queue will adequately provide training for said
job, then this is not an SRE position.
SRE students will have questions like the following:
• What am I working on?
• How much progress have I made?
• When will these activities accumulate enough experience for me to go on-call?
Making the jump from a previous company or university, while changing job roles
(from traditional software engineer or traditional systems administrator) to this neb‐
ulous Site Reliability Engineer role is often enough to knock students’ confidence
down several times. For more introspective personalities (especially regarding ques‐
tions #2 and #3), the uncertainties incurred by nebulous or less-than-clear answers
can lead to slower development or retention problems. Instead, consider the
approaches outlined in the following sections. These suggestions are as concrete as
any ticket or alert, but they are also sequential, and thus far more rewarding.
Learning Paths That Are Cumulative and Orderly
Put some amount of learning order into your system(s) so that your new SREs see a
path before them. Any type of training is better than random tickets and interrupts,
but do make a conscious effort to combine the right mix of theory and application:
abstract concepts that will recur multiple times in a newbie’s journey should be
frontloaded in their education, while the student should also receive hands-on experi‐
ence as soon as practically possible.
Learning about your stack(s) and subsystem(s) requires a starting point. Consider
whether it makes more sense to group trainings together by similarity of purpose, or
by normal-case order of execution. For example, if your team is responsible for a real-
time, user-facing serving stack, consider a curriculum order like the following:
1) How a query enters the system
Networking and datacenter fundamentals, frontend load balancing, proxies, etc.
2) Frontend serving
Application frontend(s), query logging, user experience SLO(s), etc.
3) Mid-tier services
Caches, backend load balancing
4) Infrastructure
Backends, infrastructure, and compute resources
5) Tying it all together
Debugging techniques, escalation procedures, and emergency scenarios
Initial Learning Experiences: The Case for Structure Over Chaos | 395
How you choose to present the learning opportunities (informal whiteboard chats,
formal lectures, or hands-on discovery exercises) is up to you and the SREs helping
you structure, design, and deliver training. The Google Search SRE team structures
this learning through a document called the “on-call learning checklist.” A simplified
section of an on-call learning checklist might look like the following:
The Results Mixing Server (“Mixer”)
Frontended by: Frontend server Know before moving on:
Backends called: Results Retrieval Server,