another IP address ‚Äú192.0.2.3‚Äù in AS64501 is connected to
the domain names ‚Äúbar.example.net‚Äù and ‚Äúbaz.example.org‚Äù.
Thus, these three domain names are deÔ¨Åned as rDomains
for ‚Äúfoo.example.com‚Äù, and we extract their features. These
features consist of three subsets: FQDN string, n-grams, and
top-level domain (TLD).
The FQDN string features, Nos. 39‚Äì41 in Table II, are
495
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:19:55 UTC from IEEE Xplore.  Restrictions apply. 




created from the set of rDomains for each target domain name.
SpeciÔ¨Åcally, we extract the number of FQDNs (No. 39) in the
rDomains, mean length of the FQDNs (No. 40), and standard
deviation (SD) of the length of the FQDNs (No. 41).
The n-gram features, Nos. 42‚Äì50 in Table II, are created
from the occurrence frequency of n-grams (n = 1, 2, 3)
in the set of rDomains for each target domain name. Note
that
the units of n-grams in this paper are denoted with
characters; thus, 2-grams for ‚Äúexample.com‚Äù consists of pairs
of characters such as ‚Äúex‚Äù, ‚Äúxa‚Äù, and ‚Äúam‚Äù. SpeciÔ¨Åcally, we
extract the mean, median, and SD of 1-gram (Nos. 42‚Äì44)
in rDomains, those of 2-grams (Nos. 45‚Äì47), and those of
3-grams (Nos. 48‚Äì50).
The TLD features, Nos. 51‚Äì55 in Table II, are created from
TLDs in the set of rDomains for each target domain name.
SpeciÔ¨Åcally, we extract the distinct number of TLDs in the
set of rDomains (No. 51), ratio of the .com TLD in the set
(No. 52), and mean and median, and SD of the occurrence
frequency of the TLDs in the set (Nos. 53‚Äì55).
3) Step 3: Applying Machine Learning: Step 3 involves
applying a machine learning algorithm to the outputs of step
2, which consist of input target domain names with all the
features listed in Table II. This step is designed to achieve our
goal of detecting/predicting possible malicious domain names
in future. To this end, we use supervised machine learning
to effectively Ô¨Ånd possible malicious domain names from
unvalued input domain names. Supervised machine learning
basically consists of two phases: training and test. The train-
ing phase generates a learning model based on the labeled
malicious and legitimate training data with extracted features.
The test phase calculates the maliciousness of each input
domain name with extracted features using the learning model
generated in the training phase to detect/predict malicious
domain names.
Among many supervised machine learning algorithms, we
selected a random forest [12] because of its high accuracy, as
identiÔ¨Åed in our preliminary experiments, and high scalability,
which we can easily parallelize. The concept
image of a
random forest is shown in Fig. 8. A random forest consists
of many decision trees, which are constructed from input data
with randomly sampled features. The Ô¨Ånal prediction is output
by the majority vote of the decision trees.
IV. EVALUATION
DOMAINPROFILER was evaluated using real datasets in-
cluding an extensive number of domain names. This section
explains how we evaluated it in terms of the effectiveness
of our approach of using temporal variation patterns (TVPs)
and the detection/prediction performance of malicious domain
names used in real cyber attacks.
A. Dataset
Our evaluations required three types of datasets, as shown
in Table III: target domain names, domain name list databases,
and historical DNS logs.




	
	
	




Fig. 8. Concept Image of Random Forest
The Ô¨Årst dataset was target domain names, which were
composed of training and test sets. The training set was
labeled data for creating a learning model in a random forest.
To create the Legitimate-Alexa, we extracted fully qualiÔ¨Åed
domain names (FQDNs) based on the domain names listed
in Alexa100k. Since most of the domain names in Alexa are
second-level domain (2LD) and do not have IP addresses, we
used a search engine API to randomly extract existing FQDNs
in the wild from each 2LD name. Moreover, as shown in
Section III-A, we used our ground truths, such as the results
of honeyclients and subscription-based professional data, to
eliminate the possibility that any malicious domain names
are in Legitimate-Alexa. As for Malicious-hpHosts, we used
a similar process to Legitimate-Alexa; that is, we extracted
FQDNs from 2LD names listed in hpHosts using a search
engine and veriÔ¨Åed the maliciousness using our ground truths.
The test set was used for evaluating the predictive detection
performance of our system. Note that there were no overlaps
between the training and test sets and the collected period of
the test set was after that of the training set. Thus, we are
able to simulate the predictive performance of abused domain
names in future by using the test set. As for web client-based
honeypot (honeyclient) datasets, we used our honeyclients to
collect newly malicious FQDNs particularly related to drive-
by download attacks from March 2015 to October 2015.
Honeyclient-Exploit contained FQDNs of websites engaged
in distributing exploits that target users‚Äô browsers and their
plugins. Honeyclient-Malware was the collection of FQDNs
used for malware distribution websites in drive-by download
attacks. To create sandbox datasets, we used our sandbox
systems to run 13,992 malware samples randomly downloaded
from VirusTotal [6]. The Sandbox-Malware dataset contained
FQDNs connected from malware samples (e.g., downloader) to
download other malware samples. The Sandbox-C&C dataset
was a collection of FQDNs of command and control (C&C)
servers detected with our sandbox. The Pro-C&C and Pro-
Phishing datasets were FQDNs used for C&C servers and
phishing websites, respectively. Note that the Pro datasets were
obtained from commercial and professional services provided
by a security vendor and the FQDNs we selected were only
those with high conÔ¨Ådence of being abused by cyber attackers.
496
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:19:55 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III
DATASET
Type
Target Domain Names
(training set)
Target Domain Names
(test set)
Domain Name Lists DB
Historical DNS Logs
Dataset
Legitimate-Alexa
Malicious-hpHosts
Honeyclient-Exploit
Honeyclient-Malware
Sandbox-Malware
Sandbox-C&C
Pro-C&C
Pro-Phishing
AlexaDB
hpHostsDB
DNSDB
Period
2013-05-22 ‚Äì 2015-02-28
2013-01-17 ‚Äì 2015-02-28
2015-03-01 ‚Äì 2015-10-07
2015-03-01 ‚Äì 2015-10-07
2015-03-01 ‚Äì 2015-10-07
2015-03-01 ‚Äì 2015-10-07
2015-03-01 ‚Äì 2015-03-29
2015-03-01 ‚Äì 2015-03-29
2013-05-22 ‚Äì 2015-02-28
2013-01-17 ‚Äì 2015-02-28
2014-10-01 ‚Äì 2015-02-28
# FQDNs
89,739
83,670
537
68
775
8,473
97
78,221
5,596,219
1,709,836
47,538,966
GGGGGGG
G
G
0.980
G
G
0.98265
G
0.98260
0.975
e
r
u
s
a
e
m
‚àí
F
0.970
0
G
G
G
G
G
G
G
G
100
Time Window
200
300
e
r
u
s
a
e
m
‚àí
F
0.98255
0.98250
G
G
G
G
0.98245
G
0.98240
G
2
Time Window
4
6
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GGG
GGGGGGG
G
GGGGG
GGG
G
GG
GG
G
G
G
G
G
0.98275
G
G
0.980
e
r
u
s
a
e
m
‚àí
F
0.975
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
0
100 200 300 400 500
# Trees
0.98250
0.98225
e
r
u
s
a
e
m
‚àí
F
0.98200
G
0.98175
6
9
12
# Sampled Features
Fig. 9. Tuning Time Window Size
Fig. 10. Tuning Random Forest Parameters
The second dataset was a domain name list database used
for identifying TVPs. As explained in Section II, we selected
Alexa top sites as a legitimate/popular list, and hpHosts as
a malicious list since these are continuously obtainable every
day.
The third dataset was historical DNS logs, which involved
time-series collections of the domain name and IP address
mappings. As discussed in Section III-A, we actively sent
DNS queries to the domain names we found by using domain
name lists, our web crawler, and search engine API. That is,
we extracted all domain names in domain name lists such
as Alexa and hpHosts. Moreover, we crawled approximately
200,000 web pages every day to gather web content and
extracted domain names from the content and their static and
dynamic hyperlinks. Furthermore, we expanded the number
of domain names using an external search engine API (2.5M
queries/month) based on the above domain names. In our
evaluations, we used over 47M distinct FQDNs and their time-
series changes from October 2014 to February 2015, as shown
in Table III.
B. Parameter Tuning
Before we evaluated our DOMAINPROFILER, we needed to
tune two types of parameters: the size of the time window in
TVPs (step 1) and the required parameters to run the random
forest (step 3).
Here is the summary of evaluation criteria discussed in
the following sections. A true positive (TP) is the number of
correctly predicted malicious domain names, a false positive
(FP) is the number of incorrectly predicted legitimate ones,
a false negative (FN) is the number of incorrectly predicted
malicious ones, and a true negative (TN) is the number of
correctly predicted legitimate ones. The true positive rate
(TPR), otherwise known as recall, is the ratio of correctly
detected malicious domain names to actual malicious domain
names. The precision is the ratio of actual malicious domain
names to domain names detected as malicious with the sys-
tem. The true negative rate (TNR) is the ratio of correctly
determined legitimate domain names among actual legitimate
domain names. The F-measure is the ratio that combines recall
and precision, namely, it is calculated as the harmonic mean
of precision and recall.
1) Time Window Size: We conducted 10-fold cross-
validations (CVs) using the training set with variable time
window sizes (from 1 to 365 days) to select the time window
size based on the evaluation criteria. Figure 9 shows two
graphs: the left one corresponds to the time window sizes from
1 to 365 days and the right one corresponds to those from 1
to 7 days. These two graphs reveal that the best time window
size for TVPs is only two days. This is not a surprising result
for us in terms of the nature of domain names or TVPs. Since
cyber attackers abuse the DNS to generate a huge volume of
distinct domain names from one day to the next, keeping old
information over a long period decreases the F-measure of our
system.
2) Random Forest: A random forest [12] requires two
parameters to run. One parameter is the number of decision
497
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:19:55 UTC from IEEE Xplore.  Restrictions apply. 
DETECTION PERFORMANCE WITH DIFFERENT FEATURE SETS (CROSS-VALIDATION)
TABLE IV
Feature Set
TVP
rIP
rDomain
rIP+rDomain
TVP+rIP+rDomain
TP
81,436
62,734
58,095
61,928
80,879
FP
834
32,347
16,523
13,197
798
FN
2,234
20,270
24,873
21,033
2,082
TN
88,905
57,306