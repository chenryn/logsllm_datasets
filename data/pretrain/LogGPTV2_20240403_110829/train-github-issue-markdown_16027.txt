Is there interest in adding the mish activation function to `jax.nn`?
> It has a much smoother curve vs relu, and in theory, that drives information
> more deeply through the network. It is slower than ReLU, but on average adds
> about 1-3% improvement vs ReLU. - LessW2020, Fastai forums