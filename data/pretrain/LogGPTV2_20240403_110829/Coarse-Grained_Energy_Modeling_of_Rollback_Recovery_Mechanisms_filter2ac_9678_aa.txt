title:Coarse-Grained Energy Modeling of Rollback/Recovery Mechanisms
author:Dewan Ibtesham and
David Debonis and
Dorian C. Arnold and
Kurt B. Ferreira
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Coarse-grained Energy Modeling of
Rollback/Recovery Mechanisms
Dewan Ibtesham, David DeBonis, and Dorian Arnold
Kurt B. Ferreira
Department of Computer Science
University of New Mexico
Albuquerque, NM 87131
{dewan,ddebonis,darnold}@cs.unm.edu
Scalable System Software Department
Sandia National Laboratories
Albuquerque, NM 87185–1319
PI:EMAIL
Abstract—As high-performance computing systems
continue to grow in size and complexity, energy efﬁ-
ciency and reliability have emerged as ﬁrst-order con-
cerns. Researchers have shown that data movement is
a signiﬁcant contributing factor to power consumption
on these systems. Additionally, rollback/recovery protocols
like checkpoint/restart can generate large volumes of data
trafﬁc exacerbating the energy and power concerns. In
this work, we show that a coarse-grained model can be
used effectively to speculate about the energy footprints of
rollback/recovery protocols. Using our validated model, we
evaluate the energy footprint of checkpoint compression,
a method that incurs higher computational demand to
reduce data volumes and data trafﬁc. Speciﬁcally, we show
that while checkpoint compression leads to more frequent
checkpoints (as per the optimal checkpoint frequency) and
increases per checkpoint energy cost, compression still
yields a decrease in total application energy consumption
due to the overall runtime decrease.
I.
INTRODUCTION
Today, high-performance computing (HPC) systems
comprise millions of compute cores; we expect fu-
ture exascale systems to have orders of magnitude
more cores. The United States Department of Energy
(DOE) wishes to cap the power for exascale systems at
20MW [1]. However, the current fastest machine has a
peak performance of approximately 1/20th of an exaﬂop
but at almost 18 MW consumes almost all of the ex-
ascale system’s desired power budget. Data movement
across memory and communication devices have been
shown to dominate HPC system power consumption [2].
Furthermore, checkpoint/restart (CR) protocols often
move large volumes of data. Therefore, the investigation
of the power and energy footprint of CR protocols in
HPC environments is critical.
Sandia National Laboratories is a multi-program laboratory man-
aged and operated by Sandia Corporation, a wholly owned subsidiary
of Lockheed Martin Corporation, for the U.S. Department of Energy’s
National Nuclear Security Administration under contract DE-AC04-
94AL85000.
In this work, we use the checkpoint compression
optimization as a case study. By trading off abun-
dant computational capabilities to reduce checkpoint
sizes, compressing checkpoints generally yield signif-
icant time-to-solution improvements by alleviating de-
mand for scarce bandwidth [3], [4]. However, the impact
this optimization has on energy consumption is not nec-
essarily straightforward. With checkpoint compression:
•
•
•
increases: data
Per checkpoint energy cost
compression and decompression generally is a
compute-bound workload that likely leads to
increased energy consumption per checkpoint.
At the same time, on current hardware even
with reduced data movement, the network en-
ergy consumption stays the same throughout
application run [5].
Checkpoints are taken more frequently: as the
per checkpoint costs (in time) decrease,
the
optimal checkpoint interval decreases to reduce
the amount of potentially lost work [6]. Intu-
itively, frequent, cheap checkpoints can yield a
similar lower cost/beneﬁt ratio to less frequent,
expensive checkpoints.
Application runtime decreases: more efﬁcient
CR increases overall application efﬁciency and
reduces application time-to-solution.
The combination of these factors, shown in Figure 1,
make the prediction of relative energy consumption with
and without CR compression non-trivial.
Levy et al showed that we effectively can use
a coarse-grained approach to predicting the time-to-
solution performance of CR mechanisms [7]. In this
work, we show that we effectively can use a similar
coarse-grained approach to study the energy footprint
of CR protocols using the checkpoint compression op-
timization as our case study. More speciﬁcally, we make
the following contributions:
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.71
708
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:22:31 UTC from IEEE Xplore.  Restrictions apply. 
((+(
+.
1+(









Fig. 1: C/R (de)compression time/energy trade-offs:
Without compression, run A takes longer but
checkpoints less frequently and consumes less energy
per checkpoint. With compression, run B ﬁnishes
quicker but checkpoints more frequently and consumes
more energy per checkpoint.
•
•
•
we present and validate a general, coarse-
grained energy model for CR protocols that can
be easily extended to different CR optimiza-
tions;
for CR enhanced with checkpoint compression,
we use our model to show that the energy sav-
ings due to shorter application wall clock time
outweighs the additional costs of checkpoint
compression; and
we show that this energy savings due to check-
point compression increases with application
scale.
In the rest of this paper, we present other related
research (Section II) followed by our coarse-grained
CR energy model (Section III). Then after detailing
our methodology including our software tool chain
and test system infrastructure (Section IV), we present
our model’s validation, our results and their analysis
(Section V). We conclude with a discussion of future
directions for this work.
II. RELATED WORK
Recently in the HPC context, several researchers
have studied the energy consumption of fault tolerance
protocols. Meneses, Sarood and Kale [8] compared
the energy efﬁciency of a number of rollback/recovery
based protocols and proposed an energy optimal check-
point interval for application runs and compare appli-
cation performance using this to the traditional optimal
checkpoint interval [6], which optimizes for application
execution time. Diouri et al. [9] studied CR energy con-
sumption comparing coordinated checkpointing to un-
coordinated checkpointing with message logging. Mills
et al compared the energy footprints of CR protocols
to that of process replication-based fault tolerance [10].
In a different study, Mills et al also investigated the
potential energy savings of using dynamic voltage and
frequency scaling (DVFS) during C/R [5]. As a whole,
these efforts are focused on empirical observations
based on proﬁling real applications and systems. We
propose a coarse-grained model for predicting energy
consumption. Our validated model allows us to theorize
and speculate about future or inaccessible systems in a
manner not possible with empirical methods, and to do
so without sacriﬁcing accuracy.
III. A COARSE-GRAINED MODEL TO CALCULATE
ENERGY EXPENDITURE OF APPLICATION RUN USING
CHECKPOINT COMPRESSION
For our coarse-grained energy measurement ap-
proach, we decompose a CR-based application into
three phases – running the application, taking a check-
point and restarting from a checkpoint. (We do not
distinguish executing application code during normal
operation from re-executing application code after roll-
back due to a failure.) Therefore, the energy, E, con-
sumed during an application’s execution can be modeled
simply as:
E = Eapp + Eckpt + Erst,
(1)
where Eapp is the energy expended on running the
application’s code, Eckpt is the energy expended taking
checkpoints, and Erst is the energy expended during
restarts. We consider each phase as a blackbox unit
and don’t worry about ﬁner details. For example, when
taking a checkpoint we consider the energy consump-
tion due to a checkpoint as a single cost. In contrast,
a ﬁner grained approach may further decompose the
checkpoint phase into ﬁner sub-tasks, like inter-process
coordination, calculating the portion of the address
space to checkpoint and committing the checkpoint to
stable storage. We hypothesize that this coarse-grained
approach is sufﬁciently accurate for modeling an appli-
cation’s energy consumptions and that we do not need
such ﬁner grained details.
Our coarse-grained approach assumes for each
phase, we can use a simple computation based on av-
erage power and time to estimate the energy consumed
by that phase:
Eapp = Tapp × ¯Papp,
(2)
where Tapp is the time spent executing the application,
including normal execution and rework, and ¯Papp is the
average power during application execution.
Eckpt = Tckpt × ¯Pckpt,
(3)
where Tckpt is the time spent checkpointing, and ¯Pckpt
709
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:22:31 UTC from IEEE Xplore.  Restrictions apply. 
Parameter Description
99% of actual measurements.
E
Eapp
Eckpt
Erst
Tapp
Tckpt
Trst
¯Papp
¯Pckpt
¯Prst
Total application energy consumed
Energy consumption for running
the application code.
Energy expenditure due to check-
point operation.
Energy expenditure due to restarts.
Time spent execute application
code.
Time spent taking checkpoints.
Time spent restarting from failures.
Average power for executing appli-
cation code.
Average power during checkpoint
operation.
Average power for restart.
TABLE I: A summary of our model parameters.
is the average power during the checkpoint phase.
Erst = Trst × ¯Prst,
(4)
where Trst is the time spent restarting from a failure,
and ¯Prst is the average power during a restart. Further-
more, Tckpt is the number of checkpoints times the per
checkpoint latency, and Trst is the number of failures
times the per restart latency.
We empirically measure the values of ¯Papp, ¯Pckpt
and ¯Prst We use Daly’s equation [6] to determine the
number of checkpoints taken, the number of occurring
failures and the amount of rework time. Daly’s equation
assumes node failures are independent and exponen-
tially distributed and calculates time optimal checkpoint
interval. In general HPC users are more concerned about
application run time and ﬁnishing application faster
rather than their energy costs. Hence, we didn’t use
the energy optimal checkpoint interval as proposed by
Meneses et al [8]. To add the checkpoint compression
optimization to the model, we empirically measure
the average power for checkpoint compression and
decompression and multiply those values by the total
time spent compressing and decompressing checkpoints,
respectively and add those additional energy costs to the
others.
Using this approach, we can model any application
workload, failure rate, checkpoint commit rate, com-
pression performance, etc. and estimate the total energy
costs for that application run. Similarly, we can proﬁle
application energy consumption with other CR opti-
mizations – only changes in the costs of checkpoint and
restarts must be accounted for. We validate our model in
Section V showing that the coarse-grained measurement
approach can estimate energy consumption within 95-
710
IV. METHODOLOGY
In this study, we use our coarse-grained model to
study the energy performance of checkpoint compres-
sion. We empirically measure several model parameters
including the average power for the different application
phases and the time cost and space savings of check-
point compression. We now describe this measurement
process and the rest of our experimental framework and
methodology.
A. Power Instrumentation and Measurements
We use the PowerInsight energy measurement de-
vices designed and developed jointly by Sandia National
Laboratories and Penguin Computing [11]. This instru-
ment enables component-based power and energy mea-
surements via the use of individual hall-effect sensors on
each of the power rails leading to the CPU, memory and
other devices. PowerInsight is completely separate from
the system under test: it is electrically separated and
uses a separate computing device for performance data
collection and aggregation allowing for high sampling
rates with no perturbation of the system under test. In
this work, we use a sampling frequency of 10 Hz per
power rail, which is adequate for observing the phases
within the experiment which occur at a frequency two
orders of magnitude greater.
The test system is an AMD A10-5800K APU 100W
CPU containing four general purpose (x86) cores. The
power per core is a factor or two less energy efﬁcient
than other intel and AMD architectures. Experimenta-
tion with a less energy-efﬁcient processor provides an
upper bound for the average Joules per operation. Other
architectures should yield even better results as the core
count increases without a similar increase in total socket
power consumption.
With this setup, we sample instantaneous power dur-
ing application execution, checkpoint commit, restarts,
compression and decompression. We observe mea-
surements from a single node, running on all four
cores, since prior work has shown that generally en-
ergy consumption stays uniform across an application’s
nodes [9]. In a different work [5], Mills et al. observed
that the energy and power performance of the network
and other devices do not vary signiﬁcantly from run
to run. Furthermore, processor and memory constitutes
the majority of the energy consumption. Therefore, we
only account for the power performance of the processor
and memory subsystem to calculate the total energy
consumption of each operation.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:22:31 UTC from IEEE Xplore.  Restrictions apply. 
B. (De)compression Performance
In this work, we re-used the checkpoint compres-
sion/decompression performance from a previous study
[3]. Speciﬁcally from that study, we obtained the rate at
which the compression utility compresses and decom-
presses a checkpoint data ﬁle and the amount of ﬁle
size deﬂation due to compression. The previous study
reported the impact a variety of different compression
techniques can have on an application that uses coor-
dinated C/R. In this study, we used the performance of
the best performing compression utility, bzip [12].
V. RESULTS AND ANALYSIS
In this work, we answer two primary questions: (1)
What is the accuracy of a coarse-grained approach to
measuring and modeling the energy performance of CR
protocols? and (2) Does checkpoint compression lead to
an increased or decreased energy expenditure? We now
address these questions, ﬁrst presenting the validation of
our coarse-grained model and then using the model to
predict the performance of CR using compression.
A. Validating our Energy-performance model
application’s code and Tckpt, the total time spent taking
checkpoints. We input these parameters into our model
to predict the energy expenditure, as the application
execution increases, in ﬁve checkpoint increments. That
is, the ﬁrst prediction predicts what the application’s en-
ergy consumption will be after taking ﬁve checkpoints;
the second prediction predicts through ten checkpoints
and so on up to all 50 checkpoints.
Figure 2 shows that our coarse-grained model
can predict the energy consumption of a CR-based
application very accurately with prediction accuracy
ranging from 94% to 99% for our experiments.
For this validation, we did not have a failure injection
framework and hence do not include restart or rework.
However, we see no reason to believe our coarse-grained
model will not work just as effectively for restart and
rework.
Parallel
Berkeley
[14]
(BLCR)
We validate our model using the Large-scale
Simulator
Atomic/Molecular Massively