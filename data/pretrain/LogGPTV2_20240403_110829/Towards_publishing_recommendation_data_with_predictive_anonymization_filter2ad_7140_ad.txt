the challenge set to evaluate the utility and information loss
in the anonymized data, by comparing to a ﬁxed prediction
algorithm, namely SVD. (We do not aim to develop a new
collaborative ﬁltering mechanism.) Brieﬂy, the challenge set
is used as follows. For each user-movie pair in the challenge
set, one needs to predict the corresponding ratings based on
the dataset ˆD, where ˆD is the Netﬂix Prize set excluding
the challenged entries.
We use the open-source SVD implementation in the Net-
ﬂix Recommender Framework [25] for padding, and also for
prediction in some experiments, which is described in more
detail later. After running SVD padding, the size of the
(padded) dataset is about 36GB. All data is written into
hard disk in binary format, and accessed using the mmap
system call. Due to the ﬁle size limit for mmap and in Linux,
we split the padded dataset into 40 binary ﬁles.
To measure the utility of the anonymized dataset, we
modify the conventional error computation by calculating
the RMSE for users before and after their anonymization.
This new error quantiﬁcation approach for anonymized rec-
ommender data is called target deviation and is deﬁned
as follows: For user u in the challenge set, we (the data
owner) identify u′, the anonymized version of u, among other
anonymized users, and then output the predicted ratings of
u′ as our predictions for u. The advantage of target devi-
ation is the direct and simple quantiﬁcation of diﬀerences
before and after the anonymization by leveraging the back-
ground knowledge of the data owner4.
4The data owner is able to uniquely identify u′ from u by
keeping track of the anonymization process, whereas the
public cannot.
Experiment Series
Original Data
Padded Anonymization (k = 5)
Padded Anonymization (k = 50)
Pure Anonymization (k = 5)
Pure Anonymization (k = 50)
RMSE∗
0.951849
0.95970
0.95871
2.36947
2.3771
Table 1: The target deviation RMSEs
Rating Range
No. of Ratings
No. of Ratings
in Original Dataset After Padding
[0]
[1]
[2]
[3]
[4]
[5]
98.84%*
0.053%
0.117%
0.334%
0.390%
0.267%
0
0.79%
14.12%
46.71%
33.49%
4.89%
Table 2: The rating histograms before and after
SVD padding. *This value is the number of zero
entries.
7.2 Evaluation of the Utility of Anonymized
Data
First, we measure the utility of both pure anonymization
and padded anonymization schemes (details in Section 4.3).
The RMSE results of both experiments under diﬀerent k
values are shown in Table 1. The high RMSE values (2.36947
and 2.3771) for the pure anonymization is due to both the
limited data size and the sparsity of the anonymized data.
With k = 50, there are only 9,294 anonymized users (i.e.,
groups) in the public released data. Among them, 80% of the
ratings are null if the averaging is done on the original data
(as in pure anonymization), as opposed to no null ratings in
the padded data (used for padded anonymization).
7.3 Data Characterization and Clustering
Evaluation
We characterize the data sparsity and compare the spar-
sity before and after our padding procedure. We simply
count the number of ratings that fall within a range. The
results are shown in Table 2. It is clear that padding sig-
niﬁcantly changes the distribution of ratings in the dataset,
in particular, null ratings. Padding data with SVD tremen-
dously reduces the data sparsity, and provides a rich context
for identifying similar users, as the pair-wise user similarity
computation is more accurate and meaningful.
We characterize the various user similarity metrics that
may be used in the clustering algorithm. We evaluate
four metrics: closeness-0.5, closeness-1.0, weighted similar-
ity, and our weighted-squared similarity measure Closeness-
a is a simple similarity measure on two vectors V1 and V2
by counting two corresponding vector entries similar if their
diﬀerence is within threshold a. Weighted similarity assigns
a weight to various ranges to penalize discrepancies. All
similarity values are normalized to within [0,1], and are cat-
egorized into 20 disjoint ranges, namely: [0, 0.05), [0.05, 0.1),
. . . , [0.95, 1.0). Figure 5 (a) shows the distribution of pair-
wise similarities for 5000 users under the four measures be-
fore clustering (after padding), and (b) shows the distribu-
tion of similarities between users within one single cluster.
Distributions of Pairwise Distances: All Users
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
(a)
Distributions of Pairwise Distances: Single Cluster
Clos e (0.5)
Clos e (1.0)
Weighted
WeightedSq
Clos e (0.5)
Clos e (1.0)
Weighted
WeightedSq
s
r
e
s
u

f
o

r
e
b
m
u
N
450000
400000
350000
300000
250000
200000
150000
100000
50000
0
Us ers
0~1%
1~5%
5~10%
Rankingofsimilaritytoself
8000000
7000000
6000000
5000000
4000000
3000000
2000000
1000000
0
1400
1200
1000
800
600
400
200
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
(b)
Figure 5: Comparison of four distance metrics in
user-user similarity computation. WeightedSq de-
notes our similarity measure used in this work.
Experiments With Similar-User Deviation RMSE
1.00563
1.17525
Padded Anonymization (k = 50)
Pure Anonymization (k = 50)
Table 3: RMSEs for padded and pure anonymization
experiments with k = 50.
The shift in distribution to the left indicates that the clus-
tering algorithm is able to group similar users. However, if
a similarity measure is too relaxed (e.g., closeness-1.0), then
the similarity values are artiﬁcially inﬂated, which does not
provide a good indicator in clustering.
In comparison, a
more strict similarity measure such as ours has a more ﬁne-
grained ability to distinguish similarities in user proﬁles.
7.4 RMSE By Similar-User Deviation
Target deviation is more accurate in reﬂecting the infor-
mation loss incurred during anonymization.
It implicitly
assumes that the anonymized user is the most similar to
herself before the anonymization, as the RMSE is computed
by directly comparing the ratings of a user before and after
anonymization. To eliminate the assumption in target de-
viation, we deﬁne a similar-user deviation, which is a more
realistic method for computing the utility of anonymized
data. It is based on user-based collaborative ﬁltering. For a
user u in the challenge set, we ﬁnd the anonymized user v
(in the anonymized dataset) is most similar to u according
to a similarity measure. We apply v’s ratings as our pre-
diction for u, and then compute the RMSE for the entire
challenge set. Table 3 shows the experimental results com-
puted based on our similar-user deviation deﬁnition for both
padded anonymization and pure anonymization experiments
with k = 50.
To evaluate the diﬀerences between the two deviation met-
rics, we deﬁne self-rank of a user u in the challenge set as the
rank of her anonymized version u′ among other anonymized
users in terms of similarity to u. Speciﬁcally, compute and
sort the similarities between u and all the anonymized users;
identify the rank of u′. (Note that the data owner perform-
ing the anonymization is able to uniquely identify u′ from
Figure 6: Number of users whose self-rank (See Sec-
tion 7.4) is within a certain percentage.
u, whereas the public cannot.)
In target deviation, the self-ranks are assumed to be one
for all users in the challenge set. With similar-user deviation,
most self-ranks values are quite low, indicating that target
deviation is a good approximation in the error computation.
We categorize the self-rank values in Figure 6.
Experiment summary We quantify and compare the
information loss in diﬀerent experiment setups, in partic-
ular when the original data is released as opposed to the
padded data. Padded anonymization is eﬀective in preserv-
ing the data quality with low prediction errors. Our privacy
analysis also shows that the padded data improves user pri-
vacy as a positive side eﬀect. The value of k does not have a
signiﬁcant impact on prediction accuracy. This proves that
strategically replacing null entries with padded values has
positive impacts on the utility of anonymized data.
Although preserving prediction accuracy, the padded
anonymization method loses authentic data properties and
the released data cannot support statistical queries. For ex-
ample, one is unable to ﬁnd out what percentage of users
have rated a movie. This underlines an intrinsic tradeoﬀ
between user privacy and data utility.
In comparison, Pure Anonymization mitigates these issues
to some degree, as it homogenizes on the original dataset
(as opposed to padded data). For example, it can accu-
rately answer queries such as, What is the average rating
on a movie? However, homogenization on the original data
as in the Pure Anonymization method gives much higher
RMSE than Padded Anonymization. Averaging incurs high
information loss and aﬀects data patterns even with a small
k value. This undesirable result also validates earlier pre-
dictions by others [26].
8. RELATED WORK
Our anonymization problem is
related to privacy-
preserving publishing of relational databases, anonymization
of network graphs, publishing of unlabeled bipartite graphs,
and privacy-preserving collaborative ﬁltering. We describe
some of the related work in these areas in the following.
publishing
of
Privacy-preserving
relational
databases There has been a great deal of work on privacy-
preserving publishing of relational databases. However,
due to diﬀerences between relational and recommendation
datasets (see Section 1), many of the techniques for achiev-
ing anonymity in relational databases are ineﬀective when
applied to recommender systems.
Byun et. al. introduce a suppression-based algorithm for
anonymizing relational databases [6]. Suppression is ineﬀec-
tive for recommendation data because of the sparsity prob-
lem, and may therefore result in a high degree of informa-
tion loss. Furthermore, suppression is undesirable because
the anonymization process changes the format of the original
database, and thus may render it invalid for some prediction
algorithms.
Another active
research area in privacy-preserving
databases is diﬀerential privacy (e.g., [10, 11, 24]). While
very promising for contexts in which aggregate query re-
sults are suﬃcient, diﬀerential privacy is not applicable to
our model, where the desired utility is prediction accuracy
of collaborative ﬁltering on anonymized data.
Anonymization of network graphs Anonymization of
social network graphs has attracted much attention recently
[17, 3, 36]. Since we model recommendation databases as
bipartite graphs, any method proposed for general graphs
could be directly applied. However, due to the speciﬁc bi-
partite structure of recommendation data, the techniques
suggested in the graph anonymization literature are not ef-
fective and do not adequately address the additional chal-
lenges posed by recommender systems.
Publishing of unlabeled bipartite graphs Cormode
et. al. studied the problem of privacy-preserving anonymiza-
tion of bipartite graphs [9]. Similar to our work, their pri-
vacy goal is to protect the association between the nodes in
two partitions in the graph (in our case, users and items).