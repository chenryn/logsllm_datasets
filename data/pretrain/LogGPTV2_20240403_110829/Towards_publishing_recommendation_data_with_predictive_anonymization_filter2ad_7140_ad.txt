### Evaluation of Anonymized Data Utility and Information Loss

#### Challenge Set and Prediction Algorithm
To evaluate the utility and information loss in anonymized data, we use a challenge set and compare it to a fixed prediction algorithm, specifically Singular Value Decomposition (SVD). Our goal is not to develop a new collaborative filtering mechanism but to assess the impact of anonymization on the predictive accuracy of the SVD algorithm.

The challenge set is used as follows: for each user-movie pair in the set, we predict the corresponding ratings based on the dataset \(\hat{D}\), which is the Netflix Prize dataset excluding the challenged entries. We utilize the open-source SVD implementation from the Netflix Recommender Framework [25] for both padding and prediction in some experiments. After running SVD padding, the size of the padded dataset is approximately 36GB. The data is stored in binary format on the hard disk and accessed using the `mmap` system call. Due to file size limitations in Linux, the padded dataset is split into 40 binary files.

#### Measuring Utility with Target Deviation
To measure the utility of the anonymized dataset, we introduce a modified error computation method called "target deviation." This approach calculates the Root Mean Squared Error (RMSE) for users before and after anonymization. For a user \(u\) in the challenge set, we identify the anonymized version \(u'\) and use the predicted ratings of \(u'\) as our predictions for \(u\). The advantage of target deviation is that it provides a direct and simple quantification of differences before and after anonymization, leveraging the background knowledge of the data owner. The data owner can uniquely identify \(u'\) from \(u\) by tracking the anonymization process, while the public cannot.

#### Experimental Results
We conducted a series of experiments to evaluate the utility of both pure and padded anonymization schemes. The RMSE results for different values of \(k\) are shown in Table 1. The high RMSE values for pure anonymization (2.36947 and 2.3771) are due to the limited data size and sparsity of the anonymized data. With \(k = 50\), there are only 9,294 anonymized users (i.e., groups) in the public released data, and 80% of the ratings are null if averaging is done on the original data, as opposed to no null ratings in the padded data.

**Table 1: Target Deviation RMSEs**

| Experiment Series             | RMSE   |
|-------------------------------|--------|
| Original Data                 | 0.951849 |
| Padded Anonymization (k = 5)  | 0.95970  |
| Padded Anonymization (k = 50) | 0.95871  |
| Pure Anonymization (k = 5)    | 2.36947  |
| Pure Anonymization (k = 50)   | 2.3771   |

**Table 2: Rating Histograms Before and After SVD Padding**

| Rating Range | No. of Ratings in Original Dataset | No. of Ratings After Padding |
|--------------|------------------------------------|------------------------------|
| [0]          | 98.84%*                            | 0                            |
| [1]          | 0.053%                             | 0.79%                        |
| [2]          | 0.117%                             | 14.12%                       |
| [3]          | 0.334%                             | 46.71%                       |
| [4]          | 0.390%                             | 33.49%                       |
| [5]          | 0.267%                             | 4.89%                        |

*This value represents the number of zero entries.

#### Data Characterization and Clustering Evaluation
We characterized the data sparsity and compared it before and after padding. The results, shown in Table 2, indicate that padding significantly reduces data sparsity and provides a richer context for identifying similar users. We also evaluated four user similarity metrics: closeness-0.5, closeness-1.0, weighted similarity, and our weighted-squared similarity measure. Closeness-a measures the similarity between two vectors by counting corresponding vector entries as similar if their difference is within a threshold \(a\). Weighted similarity assigns weights to different ranges to penalize discrepancies. All similarity values are normalized to the range [0,1] and categorized into 20 disjoint ranges. Figure 5 shows the distribution of pairwise similarities for 5000 users under the four measures before clustering (after padding) and within one single cluster.

**Figure 5: Comparison of Four Distance Metrics in User-User Similarity Computation**

- **(a)** Distributions of Pairwise Distances: All Users
- **(b)** Distributions of Pairwise Distances: Single Cluster

The shift in distribution to the left indicates that the clustering algorithm effectively groups similar users. However, a more relaxed similarity measure like closeness-1.0 inflates similarity values, providing a poor indicator for clustering. In contrast, a stricter measure like ours offers a more fine-grained ability to distinguish user profile similarities.

#### Similar-User Deviation
To provide a more realistic method for computing the utility of anonymized data, we defined "similar-user deviation." This method is based on user-based collaborative filtering. For a user \(u\) in the challenge set, we find the most similar anonymized user \(v\) and apply \(v\)'s ratings as our prediction for \(u\). The RMSE for the entire challenge set is then computed. Table 3 shows the experimental results for both padded and pure anonymization with \(k = 50\).

**Table 3: RMSEs for Padded and Pure Anonymization Experiments with k = 50**

| Experiment Series             | RMSE   |
|-------------------------------|--------|
| Padded Anonymization (k = 50) | 1.00563 |
| Pure Anonymization (k = 50)   | 1.17525 |

To evaluate the differences between the two deviation metrics, we defined the "self-rank" of a user \(u\) as the rank of her anonymized version \(u'\) among other anonymized users in terms of similarity to \(u\). Figure 6 categorizes the self-rank values, showing that target deviation is a good approximation in error computation.

**Figure 6: Number of Users Whose Self-Rank is Within a Certain Percentage**

#### Experiment Summary
Our experiments quantify and compare the information loss in different setups, particularly when the original data is released versus the padded data. Padded anonymization effectively preserves data quality with low prediction errors and improves user privacy. The value of \(k\) does not significantly impact prediction accuracy, proving that strategically replacing null entries with padded values has positive impacts on the utility of anonymized data.

However, padded anonymization loses authentic data properties, making it unsuitable for statistical queries. For example, one cannot determine the percentage of users who have rated a movie. Pure Anonymization mitigates these issues to some extent by homogenizing on the original dataset, but it incurs higher RMSE and significant information loss.

#### Related Work
Our work is related to privacy-preserving publishing of relational databases, anonymization of network graphs, and privacy-preserving collaborative filtering. While many techniques for relational databases are ineffective for recommendation systems due to sparsity, suppression-based algorithms and differential privacy are not suitable for our model. Anonymization of social network graphs and bipartite graphs also face challenges specific to recommender systems.

**References:**
- [25] Netflix Recommender Framework.
- [6] Byun et al. Suppression-based algorithm for anonymizing relational databases.
- [10, 11, 24] Differential privacy.
- [17, 3, 36] Anonymization of social network graphs.
- [9] Cormode et al. Privacy-preserving anonymization of bipartite graphs.