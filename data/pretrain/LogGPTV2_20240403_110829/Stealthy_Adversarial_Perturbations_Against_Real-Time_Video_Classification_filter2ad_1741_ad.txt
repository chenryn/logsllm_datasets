{p15, p16, . . . , p14},
to be valid perturbations. We
emphasize here that UP and DUP do not have the cyclic
property and thus, a sequential concatenation of parts of two
consecutive UP or DUP clips will “not” be a valid perturbation.
. . . , all
To formalize, we deﬁne a permutation function Roll(p, o)
which yields a cyclic shift of
the original DUP per-
turbation by an offset o.
In other words, when using
{p1, p2, . . . , p16} as
input
is
to Roll(p, o),
{p16−o, p16−o+1, . . . , p16, p1, . . . , p16−o−1}. Now, for all val-
ues of o ∈ {0, 15}, we need po = Roll(p, o) to be a valid
perturbation clip as well. Towards achieving this requirement,
we use a post-processor unit which applies the roll function
between the generator and the discriminator. This post pro-
cessor is captured in the complete architecture as shown in
Figure 3b.
the output
The details of how the generator and the roll unit operate
in conjunction are depicted in Figure 8. As before, the 3D
generator (G) takes a noise vector as input and outputs a
sequence of perturbations (as a perturbation clip). Note that the
ﬁnal layer is followed by a tanh non-linearity which constrains
the perturbation generated to the range [-1,1]. The output is
then scaled by ξ. Doing so restricts the perturbation’s range
to [−ξ, ξ]. Following the work in [29], [32], the value of ξ
is chosen to be 10 towards making the perturbation quasi-
imperceptible. The roll unit then “rolls” (cyclically shifts) the
perturbation p by an offset in {0, 1, 2, . . . 15}. Figure 8 depicts
the process with an offset equal to 1; the black frame is rolled
to the end of the clip. By adding the rolled perturbation clip
to the training input, we get the perturbed input. As discussed
earlier, the C3D classiﬁer takes the perturbed input and outputs
a classiﬁcation score vector. As before, we want the true class
scores to be (a) low for the targeted inputs and (b) high for
other (non-targeted) inputs. We now modify our optimization
function to incorporate the roll function as follows.
minimize
{λ × (cid:88)
G(cid:88)
(cid:88)
− log[Qc(xs)(xs + Roll(G(z), o))]}
xt∈T
o=1,2···w
+
xs∈S
− log[1 − Qc(xt)(xt + Roll(G(z), o))]
(3)
The equation is essentially the same as Equation 2, but we
consider all possible cyclic shifts of the perturbation output by
the generator.
D. 2D Dual-Purpose Universal Perturbation
We also consider a special case of C-DUP, wherein we
impose an additional constraint which is that “the perturbations
added to all frames are the same.” In other words, we seek
to add a single-frame 2D perturbation on each frame which
can be seen as a special case of C-DUP with p1 = p2 =
··· = p16. We call this kind of C-DUP as 2D-DUP. 2D-DUP
allows us to examine the effect of the temporal dimension
in generating adversarial perturbations on video inputs. 2D-
DUP is light-weight compared to C-DUP in terms of both
transmission and storage costs. In addition, 2D-DUP allows
other attack possibilities besides the man-in-the-middle case,
an example being physically adding transparent foil (to add
perturbation) onto the camera lens.
The generator in this case will output a single-frame
perturbation instead of a sequence of perturbation frames as
shown in Figure 9. This is a stronger constraint than the
circular constraint, which may cause the attack success rate
to decrease (note that the cyclic property still holds).
We denote the above 2D perturbation as p2d. The pertur-
bation clip is then generated by simply creating copies of the
perturbation and tiling them to compose a clip. The 2D-DUP
clip is now ptile = {p2d, p2d, . . . , p2d} (Figure 9). Thus, given
that the attack objective is the same as before, we simply
replace the Roll(p, o) function with a T ile function and our
problem formulation now becomes:
minimize
G2D
λ × (cid:88)
xt∈T
+
−log[1 − Qc(xt)(xt + T ile(G2D(z)))]
(cid:88)
−log[Qc(xs)(xs + T ile(G2D(z)))]
(4)
xs∈S
VII. EVALUATIONS
In this section, we showcase the efﬁcacy of the perturba-
tions generated by our proposed approaches on both the UCF-
101 and Jester datasets.
8
Fig. 10: DUP on UCF-101
Fig. 11: C-DUP on UCF-101
Fig. 12: C-DUP on Jester for T1 = {slding hand right}
Fig. 13: C-DUP on Jester for T2 = {shaking hand}
A. Experimental Setup
Discriminator set-up for our experiments: We used the C3D
classiﬁer as our discriminator. The discriminator is then used
to train our generator. For our experiments on the UCF101
dataset, we use the C3D model available in the Github repos-
itory [44]. This pre-trained C3D model achieves an average
clip classiﬁcation accuracy of 91.8% on the UCF101 dataset in
benign settings (i.e., no adversarial inputs). For the experiments
on the Jester dataset, we ﬁne-tune the C3D model from the
Github repository [44]. First, we change the output size of
the last fully connected layer to 27, since there are 27 gesture
classes in Jester. We use a learning rate with exponential decay
[57] to train the model. The starting learning rate for the last
fully connected layer is set to be 10−3 and 10−4 for all the
other layers. The decay step is set to 600 and the decay rate
is 0.9. The ﬁne-tuning phase is completed in 3 epochs and
we achieve a clip classiﬁcation accuracy of 90.03% in benign
settings.
Generator set-up for our experiments: For building our
generators, we refer to the generative model used by Vondrik
et al. [53], which has 3D deconvolution layers.
For generators for both C-DUP and 2D-DUP, we use ﬁve
3D de-convolution layers [4]. The ﬁrst four layers are followed
by a batch normalization [18] and a ReLU [33] activation
9
function. The last layer is followed by a tanh [19] layer. The
kernel size for all 3D de-convolutions is set to be 3 × 3 × 3.
To generate 3D perturbations (i.e., sequence of perturbation
frames), we set the kernel stride in the C-DUP generator to
1 in both the spatial and temporal dimensions for the ﬁrst
layer, and 2 in both the spatial and temporal dimensions for the
following 4 layers. To generate a single-frame 2D perturbation,
the kernel stride in the temporal dimension is set to 1 (i.e.,
2D deconvolution) for all layers in the 2D-DUP generator,
and the spatial dimension stride is 1 for the ﬁrst layer and
2 for the following layers. The numbers of ﬁlters are shown
in brackets in Figure 8 and Figure 9. The input noise vector
for both generators are sampled from a uniform distribution
U [−1, 1] and the dimension of the noise vector is set to be
100. For training both generators, we use a learning rate with
exponential decay. The starting learning rate is 0.002. The
decay step is 2000 and the decay rate is 0.95. Unless otherwise
speciﬁed, the weight balancing the two objectives, i.e., λ, is
set to 1 to reﬂect equal importance between misclassifying the
target class and retaining the correct classiﬁcation for all the
other (non-target) classes.
Technical Implementation: All the models are implemented
in TensorFlow [1] with the Adam optimizer [23]. Training was
performed on 16 Tesla K80 GPU cards with the batch size set
to 32. The code is available at https://github.com/sli057/Video-
Perturbation.git.
Dataset setup for our experiments: On the UCF-101 dataset
(denoted UCF-101 for short), different sets of target class T are
tested. We use T = {apply lipstick} for presenting the results
in the paper. Experiments using other target sets also yield
similar results. UCF-101 has 101 classes of human actions in
total. The target set T contains only one class while the “non-
target” set S = X − T contains 100 classes. The number of
training inputs from the non-target classes is approximately
100 times the number of training inputs from the target class.
Directly training with UCF-101 may cause a problem due to
the imbalance in the datasets containing the target and non-
target classes [26]. Therefore, we under-sample the non-target
classes by a factor of 10. Further, when loading a batch of
inputs for training, we fetch half the batch of inputs from the
target set and the other half from the non-target set in order
to balance the inputs.
For the Jester dataset, we also choose different sets of target
classes. We use two target sets T1 = {sliding hand right} and
T2 = {shaking hands} as our representative examples because
they are exemplars of two different scenarios. Since we seek
to showcase an attack on a video classiﬁcation system, we care
about how the perturbations affect both the appearance infor-
mation and temporal ﬂow information, especially the latter.
For instance, the ‘sliding hand right’ class has a temporally
similar class ‘sliding two ﬁngers right;’ as a consequence, it
may be easier for attackers to cause clips in the former class
to be misclassiﬁed as the later class (because the temporal
information does not need to be perturbed much). On the other
hand, ‘shaking hands’ is not temporally similar to any other
class. Comparing the results of these two target sets could
provide some empirical evidence on the impact of the temporal
ﬂow on our perturbations. Similar to UCF-101, the number
of inputs from the non-target classes is around 26 times the
number of inputs from the target class (since there are 27
classes in total and we only have one target class in each
experiment). So we under-sample the non-target inputs by a
factor of 4. We also set up the environment to load half of the
inputs from the target set and the other half from the non-target
set, in every batch during training.
Metrics of interest: For measuring the efﬁcacy of our per-
turbations, we consider two metrics. First, the perturbations
added to the videos should be quasi-imperceptible. Second,
the attack success rate for the target and the non-target classes
should be high. We deﬁne attack success rates as follows:
• The attack success rate for the target class is the misclassi-
• The attack success rate for the other classes is the correct
ﬁcation rate.
classiﬁcation rate.
B. Stealth with DUP
Recalling the discussion in §V, one can expect that UP
would cause inputs from the target class to be misclassiﬁed,
but also signiﬁcantly affect the correct classiﬁcation of the
other non-target inputs. On the other hand, one would expect
that DUP would achieve a stealthy attack, which would not
cause much effect on the classiﬁcation of non-target classes.
By testing on UCF-101 with ”apply lipstick” as the target
class, we observe that with UP, ”archery” is misclassiﬁed
as “swing,” “baby crawling” is misclassiﬁed as “cutting in
kitchen,” “biking” is misclassiﬁed as “golf swing,” and so on.
We ﬁnd that only 45.2% of the video clips from non-target
classes are classiﬁed correctly, i.e., the attack success rate for
non-target inputs is only 45.2%. This violates the stealthiness
needed to successfully launch an attack. However, DUP does
not affect the classiﬁcation of non-target inputs much; the
non-target attack success rate is 88.03%. At the same time,
both UP and DUP work well on target inputs, which means
the perturbed target clips are misclassiﬁed at high rate. DUP
achieves a attack success rate of 84.49 % for target inputs
and UP achieves 84.01%. These results are obtained under
the assumption that clip boundaries are exactly known while
performing the attack. Given the inferior performance of UP
on non-target inputs (i.e., in preserving stealth), we do not
consider it any further in our evaluations.
C. Showcasing C-DUP
In this subsection, we discuss the results of the C-DUP
perturbation attack. We use DUPs as our baselines.
1) Experimental Results on UCF101:
Visualizing the perturbations: The perturbation clip gen-
erated by the DUP model is shown in Figure 10 and the
perturbation clip generated by C-DUP model
is shown in
Figure 11. The visualizations of all perturbations are scaled
from [0,10] to [0,255]. We observe that the perturbation from
DUP manifests an obvious disturbance among the frames. With
C-DUP, the perturbation frames look similar, which implies
that C-DUP does not perturb the temporal information by
much, in UCF101.
10
Impact of misalignment and C-DUP performance: Based
on the discussion in §VI, we expect that DUP would work
well only when the perturbation clip is well-aligned with the
start point of each input clip to the classiﬁer; and the attack
success rate would degrade as the misalignment increases. We
expect C-DUP would overcome the misalignment effect and
provide a better overall attack performance (even with temporal
misalignment).
Case study: We perform a case study to showcase the
impact of the misalignment. We consider one ”apply lipstick”
video clip for our case study. When DUP and C-DUP are