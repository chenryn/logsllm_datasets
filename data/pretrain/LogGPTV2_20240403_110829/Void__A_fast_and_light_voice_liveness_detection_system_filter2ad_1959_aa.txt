title:Void: A fast and light voice liveness detection system
author:Muhammad Ejaz Ahmed and
Il-Youp Kwak and
Jun Ho Huh and
Iljoo Kim and
Taekkyung Oh and
Hyoungshick Kim
Void: A fast and light voice liveness detection system
Muhammad Ejaz Ahmed, Data61, CSIRO; Il-Youp Kwak, Chung-Ang University; 
Jun Ho Huh and Iljoo Kim, Samsung Research; Taekkyung Oh, KAIST and 
Sungkyunkwan University; Hyoungshick Kim, Sungkyunkwan University
https://www.usenix.org/conference/usenixsecurity20/presentation/ahmed-muhammad
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Void: A fast and light voice liveness detection system
Muhammad Ejaz Ahmed
Data61, CSIRO
Sungkyunkwan University
Il-Youp Kwak∗
Chung-Ang University
Jun Ho Huh
Samsung Research
Iljoo Kim
Samsung Research
Taekkyung Oh
KAIST
Sungkyunkwan University
Hyoungshick Kim
Sungkyunkwan University
Abstract
Due to the open nature of voice assistants’ input channels, ad-
versaries could easily record people’s use of voice commands,
and replay them to spoof voice assistants. To mitigate such
spooﬁng attacks, we present a highly efﬁcient voice liveness
detection solution called “Void.” Void detects voice spoof-
ing attacks using the differences in spectral power between
live-human voices and voices replayed through speakers. In
contrast to existing approaches that use multiple deep learn-
ing models, and thousands of features, Void uses a single
classiﬁcation model with just 97 features.
We used two datasets to evaluate its performance: (1)
255,173 voice samples generated with 120 participants, 15
playback devices and 12 recording devices, and (2) 18,030
publicly available voice samples generated with 42 partici-
pants, 26 playback devices and 25 recording devices. Void
achieves equal error rate of 0.3% and 11.6% in detecting voice
replay attacks for each dataset, respectively. Compared to a
state of the art, deep learning-based solution that achieves
7.4% error rate in that public dataset, Void uses 153 times
less memory and is about 8 times faster in detection. When
combined with a Gaussian Mixture Model that uses Mel-
frequency cepstral coefﬁcients (MFCC) as classiﬁcation fea-
tures – MFCC is already being extracted and used as the main
feature in speech recognition services – Void achieves 8.7%
error rate on the public dataset. Moreover, Void is resilient
against hidden voice command, inaudible voice command,
voice synthesis, equalization manipulation attacks, and com-
bining replay attacks with live-human voices achieving about
99.7%, 100%, 90.2%, 86.3%, and 98.2% detection rates for
those attacks, respectively.
1 Introduction
Popular voice assistants like Siri (Apple), Alexa (Amazon)
and Now (Google) allow people to use voice commands to
∗Part of this work done while Dr. Kwak was at Samsung Research.
quickly shop online, make phone calls, send messages, con-
trol smart home appliances, access banking services, and so
on. However, such privacy- and security-critical commands
make voice assistants lucrative targets for attackers to exploit.
However, recent studies [11, 12, 23] demonstrated that voice
assistants are vulnerable to various forms of voice presenta-
tion attacks including “voice replay attacks” (attackers simply
record victims’ use of voice assistants and replay them) and
“voice synthesis attacks” (attackers train victims’ voice bio-
metric models and create new commands).
To distinguish between live-human voices and replayed
voices, several voice liveness detection techniques have been
proposed. Feng et al. [11] proposed the use of wearable de-
vices, such as eyeglasses, or earbuds to detect voice liveness.
They achieved about 97% detection rate but rely on the use
additional hardware that users would have to buy, carry, and
use. Deep learning-based approaches [7, 30] have also been
proposed. The best known solution from an online replay
attack detection competition called “2017 ASVspoof Chal-
lenge” [7] is highly accurate, achieving about 6.7% equal
error rate (EER) – but it is computationally expensive and
complex: two deep learning models (LCNN and CNN with
RNN) and one SVM-based classiﬁcation model were all used
together to achieve high accuracy. The second best solution
achieved 12.3% EER using an ensemble of 5 different classiﬁ-
cation models and multiple classiﬁcation features: Constant Q
Cepstral Coefﬁcients (CQCC), Perceptual Linear Prediction
(PLP), and Mel Frequency Cepstral Coefﬁcients (MFCC) fea-
tures were all used. CQCC alone is heavy and would consist
of about 14,000 features.
To reduce computational burden and maintain high detec-
tion accuracy, we present “Void” (Voice liveness detection),
which is a highly efﬁcient voice liveness detection system
that relies on the analysis of cumulative power patterns in
spectrograms to detect replayed voices. Void uses a single
classiﬁcation model with just 97 spectrogram features. In par-
ticular, Void exploits the following two distinguishing charac-
teristics in power patterns: (1) Most loudspeakers inherently
add distortions to original sounds while replaying them. In
USENIX Association
29th USENIX Security Symposium    2685
consequence, the overall power distribution over the audible
frequency range often show some uniformity and linearity. (2)
With human voices, the sum of power observed across lower
frequencies is relatively higher than the sum observed across
higher frequencies [15, 29]. As a result, there are signiﬁcant
differences in the cumulative power distributions between
live-human voices and those replayed through loudspeakers.
Void extracts those differences as classiﬁcation features to
accurately detect replay attacks.
Our key contributions are summarized below:
• Design of a fast and light voice replay attack detection
system that uses a single classiﬁcation model and just 97
classiﬁcation features related to signal frequencies and cu-
mulative power distribution characteristics. Unlike existing
approaches that rely on multiple deep learning models and
do not provide much insight into complex spectral features
being extracted [7, 30], we explain the characteristics of
key spectral power features, and why those features are
effective in detecting voice spooﬁng attacks.
• Evaluation of voice replay attack detection accuracy using
two large datasets consisting of 255,173 voice samples col-
lected from 120 participants, 15 playback devices and 12
recording devices, and 18,030 ASVspoof competition voice
samples collected from 42 participants, 26 playback speak-
ers and 25 recording devices, respectively, demonstrating
0.3% and 11.6% EER. Based on the latter EER, Void would
be ranked as the second best solution in the ASVspoof 2017
competition. Compared to the best-performing solution
from that competition, Void is about 8 times faster and uses
153 times less memory in detection. Void achieves 8.7%
EER on the ASVspoof dataset when combined with an
MFCC-based model – MFCC is already available through
speech recognition services, and would not require addi-
tional computation.
• Evaluation of Void’s performance against hidden com-
mand, inaudible voice command, voice synthesis, equal-
ization (EQ) manipulation attacks, and combining replay at-
tacks with live-human voices showing 99.7%, 100%, 90.2%,
86.3%, and 98.2% detection rates, respectively.
2 Threat Model
2.1 Voice replay attacks
We deﬁne live-human audio sample as a voice utterance ini-
tiated from a human user that is directly recorded through
a microphone (such that would normally be processed by a
voice assistant). In a voice replay attack, an attacker uses a
recording device (e.g., a smartphone) in a close proximity
to a victim, and ﬁrst records the victim’s utterances (spoken
words) of voice commands used to interact with voice assis-
tants [3, 11, 12]. The attacker then replays the recorded sam-
ples using an in-built speaker (e.g., available on her phone) or
Figure 1: Steps for a voice replay attack.
a standalone speaker to complete the attack (see Figure 1).
Voice replay attack may be the easiest attack to perform
but it is the most difﬁcult one to detect as the recorded voices
have similar characteristics compared to the victim’s live
voices. In fact, most of the existing voice biometric-based
authentication (human speaker veriﬁcation) systems (e.g.,
[31, 32]) are vulnerable to this kind of replay attack.
2.2 Adversarial attacks
We also consider more sophisticated attacks such as “hidden
voice command” [24, 25], “inaudible voice command” [18–
20], and “voice synthesis” [6, 12] attacks that have been dis-
cussed in recent literature. Further, EQ manipulation attacks
are speciﬁcally designed to game the classiﬁcation features
used by Void by adjusting speciﬁc frequency bands of attack
voice signals.
3 Requirements
3.1 Latency and model size requirements
Our conversations with several speech recognition engineers
at a large IT company (that run their own voice assistant ser-
vices with millions of subscribed users) revealed that there are
strict latency and computational power usage requirements
that must be considered upon deploying any kind of machine
learning-based services. This is because additional use of
computational power and memory through continuous invo-
cation of machine learning algorithms may incur (1) unac-
ceptable costs for businesses, and (2) unacceptable latency
(delays) for processing voice commands. Upon receiving a
voice command, voice assistants are required to respond im-
mediately without any noticeable delay. Hence, processing
delays should be close to 0 second – typically, engineers do
not consider solutions that add 100 or more milliseconds of
delay as portable solutions. A single GPU may be expected to
concurrently process 100 or more voice sessions (streaming
commands), indicating that machine learning algorithms must
be lightweight, simple, and fast.
Further, as part of future solutions, businesses are consid-
ering on-device voice assistant implementations (that would
not communicate with remote servers) to improve response
latency, save server costs, and minimize privacy issues related
to sharing users’ private voice data with remote servers. For
such on-device solutions with limited computing resources
available, the model and feature complexity and size (CPU
2686    29th USENIX Security Symposium
USENIX Association
Figure 2: Spectrogram of an example phrase “The Blue
Lagoon is a 1980 romance and adventure ﬁlm” lively uttered
by a human user (left), and cumulative power spectral decay
of the corresponding command (right).
Figure 3: Spectrogram of the same example phrase (as in Fig-
ure 2) replayed using iPhone 6S Plus (left), and cumulative
power spectral decay (right).
and memory usage) requirements would be even more con-
straining.
3.2 Detection accuracy requirements
Our main objective is to achieve competitively high accuracy
while keeping the latency and resource usage requirements
at acceptable levels (see above). Again, our conversations
with the speech recognition engineers revealed that businesses
require around 10% or below EER to be considered as a usable
solution. For reference, the best performing solution from the
ASVspoof 2017 competition achieved 6.7% EER [30], and
the second best solution achieved 12.3% [7].
4 Key classiﬁcation features
Void exploits the differences in frequency-dependent spectral
power characteristics between live-human voices and voices
replayed through loudspeakers. Through numerous trials and
experiments, we observed three distinct features related to
power spectrum of speech signals that may distinguish live-
human voices from voices replayed through loudspeakers.
This section explores those features in detail.
Figure 1 shows the steps involved in replaying recorded
voice signals. An attacker would ﬁrst record a victim’s voice
command using her own recording device. Then the attacker
would use the same device (in-built speaker) to replay the
recorded voice command, targeted at the victim’s device. This
attack command is then processed by the voice assistant ser-
vice running on the victim’s device. While performing this
replay attack, some distortions may be added to the victim’s
original sound while being recorded with the microphone on
the attacker’s device, and also while being replayed through
the in-built speaker due to hardware imperfections. The fol-
lowing sections explore the spectral power characteristics of
replayed voices, and analyze key classiﬁcation features that
are used to classify voice replay attacks.
4.1 Decay patterns in spectral power
In general, low quality loudspeakers are designed to achieve
high sensitivity and volume but at the cost of compromising
audio ﬁdelity and adding unwanted distortions [35]. As a
result, distortions that contribute to non-linearity may be more
prevalent in low quality loudspeakers, and less visible in high
quality loudspeakers [36, 37].
Figure 2 (left) shows the spectrogram of a sentence “The
Blue Lagoon is a 1980 romance and adventure ﬁlm” uttered
live, and processed by an audio chipset in a laptop. Here, the
audio sampling rate was 44.1kHz, and the utterance duration
was 5 seconds. In this voice sample, most of the spectral
power lies in the frequency range between 20Hz and 1kHz.
The cumulative spectral power measured for each frequency
is also shown in Figure 2 (right). There is an exponential
power decay of human voice at frequency around 1kHz.
On the other hand, the spectrogram of a phrase replayed
through iPhone 6s Plus in-built speaker (see Figure 3) shows
some uniformity – spectrum spread is shown in the power
distributions between 1 and 5kHz. Unlike live-human voice
trends shown in Figure 2, the cumulative spectral power does
not decrease exponentially; rather, there is a relatively more
linear decay between 1 and 5kHz. To show the difference
between Figure 2 and 3 quantitatively, we added quadratic
ﬁtting curves on them and computed Root Mean Square Error
(RMSE) separately.
Our experimentation with 11 in-built smartphone speakers
showed similar behaviors in their spectral power distributions;
i.e., power decreased gradually across frequencies and did not
decay exponentially. An example cumulative distribution of
spectral power density is shown in Figure 4. With the human
voice example, about 70% of the overall power lies in the
frequency range below 1kHz. However, in the loudspeaker
case, the cumulative distribution increases almost linearly,
and 70% of the total power lies within the frequency range of
about 4kHz.
One possible explanation for this spreading out charac-
teristic is low-quality hardware boosting power in certain
frequency ranges. Consequently, such a linear decay pattern
in spectral power (over audible frequency range) could be
USENIX Association
29th USENIX Security Symposium    2687
produce higher quality sounds. We also use higher order poly-
nomials to accurately model spectral power shapes, and use
these models to identify more ﬁne-grained differences in spec-
tral power patterns between live-human and replayed samples
(see Figure 5). We also provide power patterns for different
loudspeakers in Appendix B.
4.3 Linear prediction cepstrum coefﬁcients
(LPCC)
Because the decay and peak patterns discussed in Sections 4.1
and 4.2 mainly look at speciﬁc frequency ranges. To per-
form a more general inspection of wider frequency ranges,
we additionally use linear prediction cepstrum coefﬁcients
(LPCC) [4] as a complementary feature.
LPCC is popularly used for auditory modeling in speech-
related applications. The key idea behind LPCC is that a
speech sample can be approximated as a linear combination
of previous samples. LPCC for a voice sample is computed
by minimizing the sum of squared differences between the
voice sample and linearly predicted ones. The computational
complexity of LPCC is lower than MFCC since LPCC does
not require computation of discrete Fourier transform [5]. We
chose LPCC as a complementary, lightweight feature to help
Void utilize spectral features covering wider frequency ranges
of speech signals.
5 System design
We designed Void to satisfy the requirements speciﬁed in
Section 3 based on the key classiﬁcation features described in
Section 4. To detect replay attacks, Void analyzes signal power
distributions over the audible frequency range – computing
linearity degree of given signal power, and identifying peak
patterns in low-power and high-power frequencies.
Figure 6: High-level design of Void.
5.1 Void overview
Attack detection through Void involves three stages as shown
in Figure 6: signal transformation, feature extraction, and real-
time attack detection. The overall Void algorithm is described
in Algorithm 1. A voice command Voicein, window size W ,
and a weighting factor ω are given as inputs to Algorithm 1.
Figure 4: Cumulative distribution of spectral power den-
sity over frequencies, showing up to 8kHz (W = 10).
trained and used to classify voices replayed through low-
quality loudspeakers. Appendix A demonstrates that three
signal power features would be used to classify live-human
voices and voices replayed through 11 in-built smartphone
speakers.
4.2 Peak patterns in spectral power
Because high-quality standalone loudspeakers boost power
across a wide range of frequencies to reduce non-linear dis-
tortions, the linear decay patterns described above may not be
sufﬁcient against such loudspeakers.
Figure 5: Signal power frequency range between 20Hz
and 10kHz of the spectrogram of the same example
phrase (as in Figure 2). Live-human voice (left): ﬁne-
grained power ﬂuctuations can be observed over the fre-