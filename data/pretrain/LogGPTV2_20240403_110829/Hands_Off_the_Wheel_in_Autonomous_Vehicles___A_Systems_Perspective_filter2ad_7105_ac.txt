disengagement reports are shown in Table II. Hence, our
analysis proceeds with optical character recognition (OCR;
labeled as 1
in Fig. 1) by using Google Tesseract [31] on the
scanned documents. In certain cases, where the Teserract OCR
failed (because of low-resolution scans or inability to recognize
some table formats), we manually converted the documents to
machine-encoded text.
Data Normalization. CA DMV regulations require that each
manufacturer report crucial information about disengagements,
e.g., the number of miles driven in autonomous mode and
the number of disengagements observed. However, it does not
enforce any data format speciﬁcation for these reports, leading
to disparities (across manufacturers and across time) in the data
schema and granularity of the information available through
these reports. Hence, we need to ﬁlter, parse, and normalize
(labeled as 2
in Fig. 1) the data into machine-encoded text to
produce structured datasets that have uniform schema across
manufacturers and time (i.e., across reports made by the same
manufacturer at different times). Taken together, steps
1 and
2 correspond to preprocessing of the datasets to make them
ready for further analysis.
3
Labeling and Tagging of the Reported Disengagement
and Accident Causes. The pipeline uses an NLP-based tech-
nique (labeled as
in Fig. 1) to map a given disengagement
event in a corresponding fault tag and a failure category. First
we make several passes over the dataset to construct a “Failure
Dictionary” that contains a sequence of phrases (keywords)
extracted from the raw disengagement reports (logs). This
dictionary is used to design a voting scheme (which is based
on the maximum number of shared keywords) to assign a
disengagement cause to a fault tag. In the event that this
procedure is unsuccessful and we cannot associate any of the
known tags to textual description, the disengagement cause is
marked with the “Unknown-T” tag.
We then build an ontology (based on Fig. 3) of failure
categories on top of the tags (which were derived from [32]).
Speciﬁcally, we apply our understanding of the ADS system
(described in Section III-B) to select keywords and phrases
that differentiate fault tags from each other. The tags are
chosen to localize faults in the computing system (e.g.,
software and hardware systems) and in the machine learning
algorithms/design (e.g., perception and control algorithms),
thereby identifying potential targets for improving the safety
and reliability of the AV. Table III lists the fault tags used
in this study. Table II provides examples of the raw log to
tag and category mapping. We consider the following failure
categories: 1) faults in the design of the machine learning
system responsible for “perception” tasks (dealing with data
from sensors) and “planning and control” tasks (dealing with
control of steering and acceleration); 2) faults in the computing
system (dealing with hardware and software problems); and
3) an “Unknown-C” category consisting of tags we cannot
classify into any of the above categories.
These tags and categories allow us to classify the types of fail-
DEFINITION OF FAULT TAGS AND CATEGORIES THAT ARE ASSIGNED TO DISENGAGEMENTS.
Table III
Tag
Category
Deﬁnition
ML/Design
System
Environment
Computer System
Recognition System ML/Design
Planner
ML/Design
System
Sensor
System
Network
ML/Design
Design Bug
(cid:2)
Software
System
AV Controller
Hang/Crash
System
ML/Design
System
Sudden change in external factors (e.g., construction zones, emergency vehicles, accidents)
Computer-system-related problem (e.g., processor overload)
Failure to recognize outside environment correctly
Planner failed to anticipate the other driver’s behavior
Sensor failed to localize in time
Data rate too high to be handled by the network
AV was not designed to handle an unforeseen situation
Software-related problems such as hang or crash
“System” when AV controller does not respond to commands
“ML/Design” when AV controller makes wrong decisions/predictions
Watchdog timer error
590
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:23:35 UTC from IEEE Xplore.  Restrictions apply. 
Figure 4. Comparison of the distributions of DPM per car across manufacturers.
The boxes show quartiles; the notches show medians; and the whiskers show
max/mins.
ure causes into machine-learning vs. computer-system-related
issues. Table III provides a mapping between the categories
and tags used in our analysis. In the ﬁnal step (labeled as
in Fig. 1), the preprocessed data from the disengagement
4
dataset and accident dataset are merged together, along with
extracted categories and tags, to create a consolidated AV
failure database for further analysis.
V. STATISTICAL ANALYSIS OF FAILURES IN AVS
Traditional approaches to evaluating the resilience of a
system [33] require the computation of availability, reliability,
and safety. These metrics require information about operational
periods of the AV (e.g., the active time of the vehicle). As
this information is not available in the CA DMV dataset, we
use the 5,324 disengagements (across eight manufacturers)
and 42 accidents as the basis for deriving statistics on fault
classes, failure modes of AVs, and their evolution over time.
These statistics allow us to draw conclusions and answer the
following questions:
Question 1. How do we assess the stability/maturity of the
AV technology?
Question 2. What is the primary cause of disengagements
(and potentially accidents) observed in AVs?
Question 3. Are manufacturers indeed building better and
more reliable AVs over time?
Question 4. What level of alertness4 of the human driver of
an AV guarantees safety?
Question 5. How well do AVs compare with human drivers?
A. Analysis of AV Disengagement Reports
1) Question 1: Assessment of AV Technology
Based on the available data, we computed the following
metrics from the disengagement reports to assess AVs: 1) num-
ber of disengagements observed per autonomous mile driven
(DPM, shown in Fig. 4), and 2) total number of disengagements
observed (shown in Fig. 5).
Comparing DPMs across Manufacturers. Most manu-
facturers have a median DPM ∈ [0.1, 0.01] m
−1 per car
−1 (see Fig. 4).
with the 99th percentile DPM around 1 m
There is a signiﬁcant disparity (nearly 100×) between median
DPMs across all manufacturers. This substantiates our initial
hypothesis (from Section III-C) that the cumulative miles
4Measured here as reaction times of human drivers in case of disengage-
ments.
591
Figure 5. Disengagements reported per cumulative miles driven across
manufacturers represented in a log-log plot. Lines represent linear regression
ﬁts.
driven by a manufacturer (see Table I) is indicative of better
performance. For example, Waymo (Google) does ∼ 100×
better than its competitors in terms of both the median and
99th percentile DPMs; at the same time, it is responsible for
> 90% of the total miles driven in the dataset.
Maturity of AV Technology. Fig. 5 demonstrates a strong
linear correlation (based on the linear regression ﬁts) between
the number of disengagements observed and the number of
cumulative autonomous miles driven. We expect that in an
ideal case mature AV technology will show a decrease in
DPM (i.e., the slopes of the lines in Fig. 5) that asymptotically
reaches towards a horizontal line (or close to it, i.e., zero DPM
or a very low DPM). The reason is that the data collected
from the planned testing of AVs validates the computing
system (e.g., by identifying software bugs) and also trains
the machine learning algorithms that monitor the environment
and control the steering and acceleration of the AV. Thereby
eventually enabling the AVs to handle more fault scenarios,
thus contributing to a decreasing DPM. This is true for
most manufacturers to varying degrees with the exception of
Volkswagen, Bosch, and GMCruise. An important conclusion
is that despite the million miles driven, Waymo is still not quite
approaching the target asymptote. This indicates that Waymo
and other manufacturers are still in the “burn-in” phase.
2) Question 2: Causes of AV Disengagements
We present a categorization of the sources of faults that cause
disengagements from two different perspectives: 1) cause of
occurrence, and 2) modality of occurrence.
Machine-Learning-Related Faults. First, we consider dis-
engagements by cause of occurrence, i.e., categorization of
the cause of a disengagement. In the following text, we
ignore the numbers for Tesla, as most of their categorical
label are marked “Unknown-C.” We observe that machine-
learning-related faults, mainly ones pertaining to the perception
system (e.g., improper detection of trafﬁc lights, lane markings,
holes, and bumps), are the dominant cause of disengagements
across most manufacturers. They account for ∼ 44% of all
reported disengagements (see Table IV).5 The second major
contributor to reported disengagements is the machine learning
5We consider external fault sources such as undetected construction zones,
cyclists, pedestrians, emergency vehicles, and weather phenomena (e.g., rain
or sun glare) as perception-related-machine-learning related disengagements
as they deal with interpretation of the environment from sensor data.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:23:35 UTC from IEEE Xplore.  Restrictions apply. 
DISENGAGEMENTS ACROSS MANUFACTURERS (AS PERCENTAGES)
CATEGORIZED BY ROOT FAILURE CATEGORIES.
Table IV
Manufacturer
ML/Design
System Unknown-C
Fault Type
Planner/
Controller
37.59
36.3
0
0
10.13
Perception/
Recognition
50.17
49.63
0
3.08
53.45
Delphi
Nissan
Tesla
Volkswagen
Waymo
ML/Design is divided into Planner/Controller- and Perception-related problems.
12.24
14.07
1.65
83.08
36.42
0
0
98.35
13.85
0














!


!












 

    
  
   


  
   

"!#
#!$
!%
&'
()#
*!%'!+,

+
-!$
$
$
./
Figure 6. Categorization (in terms of fault tags) of faults that led to
disengagements across manufacturers.
related to the control and decision framework (e.g., improper
motion planning), which accounts for ∼ 20% of the total
disengagements. The computing system, i.e., hardware issues
(e.g., problems with the sensor and processor) and software
issues (e.g., hangs, crashes, bugs), accounts for ∼ 33.6% of
the total disengagements reported. Further, we observe that the
perception-based machine learning faults are responsible for
DPM measurements in the upper three quartiles. Therefore we
conclude that the faults in the perception system are directly
responsible for higher DPMs across manufacturers.
Comparing Waymo to Others Using Fault Categoriza-
tion. As stated earlier, we observe that AV prototypes from
Waymo perform signiﬁcantly better than those of its competi-
tors. Our fault categorization allows us to speculate on reasons
for this behavior. We observe (see Fig. 6) that Waymo reports
signiﬁcantly higher percentages of disengagements related to
system faults (i.e., software or hardware issues) than machine
learning/design issues, unlike other manufacturers. Extensive
on-road testing (over 1,060,200 cumulative autonomous miles,
which is ∼ 70× more than any other manufacturer) has
allowed Waymo to eliminate many fault scenarios relating
to perception and control. Even though Waymo has resolved
key control and decision-making issues in the machine learning
system, perception and system issues still dominate. We observe
that most accidents are the result of poor decisions made by
the machine learning system in complex trafﬁc scenarios, as
shown in the two case studies (in Section II). Faults in the
perception systems often propagate to the decision system,
leading to complex failure scenarios. We explore this further
DISTRIBUTION OF DISENGAGEMENTS ACROSS MANUFACTURERS (AS
PERCENTAGES) CATEGORIZED BY MODALITY.
Table V
Manufacturer
Benz
Bosch
GMCruise
Nissan
Tesla
Volkswagen
Waymo
Automatic Manual
47.11
0
0
54.2
98.35
100
50.32
52.89
0
0
45.8
1.65
0