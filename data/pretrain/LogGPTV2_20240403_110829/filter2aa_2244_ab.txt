Most people are more afraid of asbestos in their kids’ school than asbestos in their 
own workplace. 
• 
You will generally be more afraid of a risk that could directly affect you than a risk 
that threatens others.  U.S. citizens were less afraid of terrorism before September 
11, 2001, because up till then the Americans who had been the targets of terrorist 
attacks were almost always overseas.  But suddenly on September 11, the risk 
became personal.  When that happens, fear goes up, even though the statistical 
reality of the risk may still be very low. 2 
Others make these and similar points, which are summarized in Table 1. 3 4 5 6 
When you look over the list in Table 1, the most remarkable thing is how reasonable so 
many of them seem.  This makes sense for two reasons.  One, our perceptions of risk are deeply 
ingrained in our brains, the result of millions of years of evolution.  And two, our perceptions of 
risk are generally pretty good, and are what have kept us alive and reproducing during those 
millions of years of evolution. 
The Psychology of Security—DRAFT 
6 
Table 1:  Conventional Wisdom About People and Risk Perception 
When our risk perceptions fail today, it’s because of new situations that have occurred at a 
faster rate than evolution: situations that exist in the world of 2007, but didn’t in the world of 
100,000 BC.  Like a squirrel whose predator-evasion techniques fail when confronted with a car, 
or a passenger pigeon who finds that evolution prepared him to survive the hawk but not the 
shotgun, our innate capabilities to deal with risk can fail when confronted with such things as 
modern human society, technology, and the media.  And, even worse, they can be made to fail by 
others—politicians, marketers, and so on—who exploit our natural failures for their gain. 
To understand all of this, we first need to understand the brain. 
Risk and the Brain 
The human brain is a fascinating organ, but an absolute mess.  Because it has evolved over 
millions of years, there are all sorts of processes jumbled together rather than logically 
organized.  Some of the processes are optimized for only certain kinds of situations, while others 
don’t work as well as they could.  And there’s some duplication of effort, and even some 
conflicting brain processes. 
Assessing and reacting to risk is one of the most important things a living creature has to 
deal with, and there’s a very primitive part of the brain that has that job.  It’s the amygdala, and 
it sits right above the brainstem, in what’s called the medial temporal lobe.  The amygdala is 
People exaggerate risks that are: 
People downplay risks that are: 
Spectacular 
Pedestrian 
Rare 
Common 
Personified 
Anonymous 
Beyond their control, or externally imposed 
More under their control, or taken willingly 
Talked about 
Not discussed 
Intentional or man-made 
Natural 
Immediate 
Long-term or diffuse 
Sudden 
Evolving slowly over time 
Affecting them personally 
Affecting others 
New and unfamiliar 
Familiar 
Uncertain 
Well understood 
Directed against their children 
Directed towards themselves 
Morally offensive 
Morally desirable 
Entirely without redeeming features 
Associated with some ancillary benefit 
Not like their current situation 
Like their current situation 
The Psychology of Security—DRAFT 
7 
responsible for processing base emotions that come from sensory inputs, like anger, avoidance, 
defensiveness, and fear.  It’s an old part of the brain, and seems to have originated in early 
fishes.  When an animal—lizard, bird, mammal, even you—sees, hears, or feels something that’s 
a potential danger, the amygdala is what reacts immediately.  It’s what causes adrenaline and 
other hormones to be pumped into your bloodstream, triggering the fight-or-flight response, 
causing increased heart rate and beat force, increased muscle tension, and sweaty palms. 
This kind of thing works great if you’re a lizard or a lion.  Fast reaction is what you’re 
looking for; the faster you can notice threats and either run away from them or fight back, the 
more likely you are to live to reproduce. 
But the world is actually more complicated than that.  Some scary things are not really as 
risky as they seem, and others are better handled by staying in the scary situation to set up a 
more advantageous future response.  This means that there’s an evolutionary advantage to being 
able to hold off the reflexive fight-or-flight response while you work out a more sophisticated 
analysis of the situation and your options for dealing with it. 
We humans have a completely different pathway to deal with analyzing risk.  It’s the 
neocortex, a more advanced part of the brain that developed very recently, evolutionarily 
speaking, and only appears in mammals.  It’s intelligent and analytic.  It can reason.  It can make 
more nuanced trade-offs.  It’s also much slower. 
So here’s the first fundamental problem: we have two systems for reacting to risk—a 
primitive intuitive system and a more advanced analytic system—and they’re operating in 
parallel.  And it’s hard for the neocortex to contradict the amygdala. 
In his book Mind Wide Open, Steven Johnson relates an incident when he and his wife 
lived in an apartment and a large window blew in during a storm.  He was standing right beside 
it at the time and heard the whistling of the wind just before the window blew.  He was lucky—a 
foot to the side and he would have been dead—but the sound has never left him: 
But ever since that June storm, a new fear has entered the mix for me: the sound of 
wind whistling through a window.  I know now that our window blew in because it had been 
installed improperly….  I am entirely convinced that the window we have now is installed 
correctly, and I trust our superintendent when he says that it is designed to withstand 
hurricane-force winds.  In the five years since that June, we have weathered dozens of 
storms that produced gusts comparable to the one that blew it in, and the window has 
performed flawlessly. 
I know all these facts—and yet when the wind kicks up, and I hear that whistling 
sound, I can feel my adrenaline levels rise….  Part of my brain—the part that feels most me-
like, the part that has opinions about the world and decides how to act on those opinions in 
a rational way—knows that the windows are safe….  But another part of my brain wants to 
barricade myself in the bathroom all over again.7 
There’s a good reason evolution has wired our brains this way.  If you’re a higher-order 
primate living in the jungle and you’re attacked by a lion, it makes sense that you develop a 
lifelong fear of lions, or at least fear lions more than another animal you haven’t personally been 
attacked by.  From a risk/reward perspective, it’s a good trade-off for the brain to make, and—if 
you think about it—it’s really no different than your body developing antibodies against, say, 
chicken pox based on a single exposure.  In both cases, your body is saying: “This happened 
once, and therefore it’s likely to happen again.  And when it does, I’ll be ready.”  In a world 
where the threats are limited—where there are only a few diseases and predators that happen to 
affect the small patch of earth occupied by your particular tribe—it works. 
The Psychology of Security—DRAFT 
8 
Unfortunately, the brain’s fear system doesn’t scale the same way the body’s immune 
system does.  While the body can develop antibodies for hundreds of diseases, and those 
antibodies can float around in the bloodstream waiting for a second attack by the same disease, 
it’s harder for the brain to deal with a multitude of lifelong fears. 
All this is about the amygdala.  The second fundamental problem is that because the 
analytic system in the neocortex is so new, it still has a lot of rough edges evolutionarily 
speaking.  Psychologist Daniel Gilbert has a great quotation that explains this: 
The brain is a beautifully engineered get-out-of-the-way machine that constantly scans 
the environment for things out of whose way it should right now get. That’s what brains did 
for several hundred million years—and then, just a few million years ago, the mammalian 
brain learned a new trick: to predict the timing and location of dangers before they actually 
happened.  
Our ability to duck that which is not yet coming is one of the brain’s most stunning 
innovations, and we wouldn’t have dental floss or 401(k) plans without it. But this 
innovation is in the early stages of development. The application that allows us to respond 
to visible baseballs is ancient and reliable, but the add-on utility that allows us to respond to 
threats that loom in an unseen future is still in beta testing. 8 
A lot of what I write in the following sections are examples of these newer parts of the brain 
getting things wrong.  
And it’s not just risks.  People are not computers.  We don’t evaluate security trade-offs 
mathematically, by examining the relative probabilities of different events.  Instead, we have 
shortcuts, rules of thumb, stereotypes, and biases—generally known as “heuristics.”  These 
heuristics affect how we think about risks, how we evaluate the probability of future events, how 
we consider costs, and how we make trade-offs.  We have ways of generating close-to-optimal 
answers quickly with limited cognitive capabilities.  Don Norman’s wonderful essay, “Being 
Analog,” provides a great background for all this.9 
Daniel Kahneman, who won a Nobel Prize in Economics for some of this work, talks about 
humans having two separate cognitive systems: one that intuits and one that reasons: 
The operations of System 1 are typically fast, automatic, effortless, associative, implicit 
(not available to introspection), and often emotionally charged; they are also governed by 
habit and therefore difficult to control or modify.  The operations of System 2 are slower, 
serial, effortful, more likely to be consciously monitored and deliberately controlled; they 
are also relatively flexible and potentially rule governed.10 
When you read about the heuristics I describe below, you can find evolutionary reasons for 
why they exist.  And most of them are still very useful.11  The problem is that they can fail us, 
especially in the context of a modern society.  Our social and technological evolution has vastly 
outpaced our evolution as a species, and our brains are stuck with heuristics that are better 
suited to living in primitive and small family groups. 
And when those heuristics fail, our feeling of security diverges from the reality of security. 
Risk Heuristics 
The first, and most common, area that can cause the feeling of security to diverge from the 
reality of security is the perception of risk.  Security is a trade-off, and if we get the severity of the 
risk wrong, we’re going to get the trade-off wrong.  We can do this both ways, of course.  We can 
The Psychology of Security—DRAFT 
9 
underestimate some risks, like the risk of automobile accidents.  Or we can overestimate some 
risks, like the risk of a stranger sneaking into our home at night and kidnapping our child.  How 
we get the risk wrong—when we overestimate and when we underestimate—is governed by a few 
specific brain heuristics. 
Prospect Theory 
Here’s an experiment that illustrates a particular pair of heuristics.12  Subjects were divided 
into two groups.  One group was given the choice of these two alternatives: 
• 
Alternative A:  A sure gain of $500. 
• 
Alternative B:  A 50% chance of gaining $1,000. 
The other group was given the choice of: 
• 
Alternative C:  A sure loss of $500. 
• 
Alternative D:  A 50% chance of losing $1,000. 
These two trade-offs aren’t the same, but they’re very similar.  And traditional economics 
predicts that the difference doesn’t make a difference. 
Traditional economics is based on something called “utility theory,” which predicts that 
people make trade-offs based on a straightforward calculation of relative gains and losses.  
Alternatives A and B have the same expected utility: +$500.  And alternatives C and D have the 
same expected utility: -$500.  Utility theory predicts that people choose alternatives A and C 
with the same probability and alternatives B and D with the same probability.  Basically, some 
people prefer sure things and others prefer to take chances.  The fact that one is gains and the 
other is losses doesn’t affect the mathematics, and therefore shouldn’t affect the results.  
But experimental results contradict this.  When faced with a gain, most people (84%) chose 
Alternative A (the sure gain) of $500 over Alternative B (the risky gain).  But when faced with a 
loss, most people (70%) chose Alternative D (the risky loss) over Alternative C (the sure loss).   
The authors of this study explained this difference by developing something called 
“prospect theory.”  Unlike utility theory, prospect theory recognizes that people have subjective 
values for gains and losses.  In fact, humans have evolved a pair of heuristics that they apply in 
these sorts of trade-offs.  The first is that a sure gain is better than a chance at a greater gain.  (“A 
bird in the hand is better than two in the bush.”)  And the second is that a sure loss is worse than 
a chance at a greater loss.  Of course, these are not rigid rules—given a choice between a sure 
$100 and a 50% chance at $1,000,000, only a fool would take the $100—but all things being 
equal, they do affect how we make trade-offs. 
Evolutionarily, presumably it is a better survival strategy to—all other things being equal, of 
course—accept small gains rather than risking them for larger ones, and risk larger losses rather 
than accepting smaller losses.  Lions chase young or wounded wildebeest because the investment 