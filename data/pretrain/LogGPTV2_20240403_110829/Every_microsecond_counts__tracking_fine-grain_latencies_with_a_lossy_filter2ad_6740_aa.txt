title:Every microsecond counts: tracking fine-grain latencies with a lossy
difference aggregator
author:Ramana Rao Kompella and
Kirill Levchenko and
Alex C. Snoeren and
George Varghese
Every Microsecond Counts: Tracking
Fine-Grain Latencies with a Lossy Difference Aggregator
Ramana Rao Kompella†, Kirill Levchenko, Alex C. Snoeren, and George Varghese
†Purdue University and University of California, San Diego
ABSTRACT
Many network applications have stringent end-to-end latency re-
quirements, including VoIP and interactive video conferencing, au-
tomated trading, and high-performance computing—where even
microsecond variations may be intolerable. The resulting ﬁne-grain
measurement demands cannot be met effectively by existing tech-
nologies, such as SNMP, NetFlow, or active probing. We propose
instrumenting routers with a hash-based primitive that we call a
Lossy Difference Aggregator (LDA) to measure latencies down to
tens of microseconds and losses as infrequent as one in a million.
Such measurement can be viewed abstractly as what we refer to
as a coordinated streaming problem, which is fundamentally harder
than standard streaming problems due to the need to coordinate
values between nodes. We describe a compact data structure that
efﬁciently computes the average and standard deviation of latency
and loss rate in a coordinated streaming environment. Our theoret-
ical results translate to an efﬁcient hardware implementation at 40
Gbps using less than 1% of a typical 65-nm 400-MHz networking
ASIC. When compared to Poisson-spaced active probing with sim-
ilar overheads, our LDA mechanism delivers orders of magnitude
smaller relative error; active probing requires 50–60 times as much
bandwidth to deliver similar levels of accuracy.
Categories and Subject Descriptors
C.2.3 [Computer Communication Networks]: Network manage-
ment
General Terms
Measurement, algorithms
Keywords
Passive measurement, packet sampling
1.
INTRODUCTION
An increasing number of Internet-based applications require
end-to-end latencies on the order of milliseconds or even microsec-
onds. Moreover, many of them further demand that latency remain
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$10.00.
stable; i.e., low jitter. These applications range from popular multi-
media services like voice-over-IP, multi-player gaming, and video
conferencing to niche—but commercially important—markets like
automated trading and high-performance computing. As such ap-
plications grow in signiﬁcance, customers are placing increasing
demands on operators to provision and manage networks that meet
these stringent speciﬁcations. Unfortunately, most of the currently
available tools are unable to accurately measure latencies of these
magnitudes, nor can they detect or localize transient variations or
loss spikes. Hence, we propose a new mechanism to measure la-
tency and loss at extremely small time scales, even tens of mi-
croseconds. We focus particularly on data-center networks, where
such measurement is both useful (because many data-center appli-
cations can be hurt if latencies increase by even tens of microsec-
onds) and feasible (because of small propagation delays). Our ap-
proach can also be applied more generally, but the accuracy of ex-
isting techniques may be sufﬁcient for many wide-area scenarios.
As a motivating example, consider a trading network that con-
nects a stock exchange to a number of data centers where auto-
matic trading applications run. In order to prevent unfair arbitrage
opportunities, network operations personnel must ensure that the
latencies between the exchange and each data center are within 100
microseconds of each other [35]. (A recent InformationWeek arti-
cle claims that “a one-millisecond advantage in trading applications
can be worth $100 million a year to a major brokerage ﬁrm” [25].)
Current routers typically support two distinct accounting mech-
anisms: SNMP and NetFlow. Neither are up to the task. SNMP
provides only cumulative counters which, while useful to estimate
load, cannot provide latency estimates. NetFlow, on the other
hand, samples and timestamps a subset of all received packets; cal-
culating latency requires coordinating samples at multiple routers
(e.g., trajectory sampling [10]). Even if such coordination is pos-
sible, consistent samples and their timestamps have to be commu-
nicated to a measurement processor that subtracts the sent times-
tamp from the receive timestamp of each successfully delivered
packet in order to estimate the average, a procedure with funda-
mentally high space complexity. Moreover, computing accurate
time averages requires a high sampling rate, and detecting short-
term deviations from the mean requires even more. Unfortunately,
high NetFlow sampling rates signiﬁcantly impact routers’ forward-
ing performance and are frequently incompatible with operational
throughput demands.
Thus, operators of latency-critical networks are forced to use ex-
ternal monitoring mechanisms in order to collect a sufﬁcient num-
ber of samples to compute accurate estimates. The simplest tech-
nique is to send end-to-end probes across the network [24, 31,
33]. Latency estimates computed in this fashion, however, can be
grossly inaccurate in practice.
In a recent Cisco study, periodic
255probes sent at 1-second intervals computed an average latency of
under 5 ms, while the actual latencies as reported by a hardware
monitor were around 20 ms with some bursts as high as 50 ms [30,
Fig. 6]. Capturing these effects in real networks requires injecting
a prohibitively high rate of probe packets. For these reasons, oper-
ators often employ external passive hardware monitors (e.g., those
manufactured by Corvil [1]) at key points in their network. Un-
fortunately, placing hardware monitors between every pair of input
and output ports is cost prohibitive in many instances.
Instead, we propose the Lossy Difference Aggregator (LDA), a
low-overhead mechanism for ﬁne-grain latency and loss measure-
ment that can be cheaply incorporated within routers to achieve the
same effect. LDA has the following features:
• Fine-granularity measurement: LDA accurately measures
loss and delay over short time scales while providing strong
bounds on its estimates, enabling operators to detect short-
term deviations from long-term means within arbitrary con-
ﬁdence levels. Active probing requires 50–60 times as much
bandwidth to deliver similar levels of accuracy, as demon-
strated in Section 4.3.
• Low overhead: Our suggested 40-Gbps LDA implementa-
tion uses less than 1% of a standard networking ASIC and 72
Kbits of control trafﬁc per second, as detailed in Section 5.1.
• Customizability: Operators can use a classiﬁer to conﬁgure
an LDA to measure the delay of particular trafﬁc classes to
differing levels of precision, independent of others, as dis-
cussed in Section 5.1.
• Assists fault localization: LDA can operate link-by-link and
even segment-by-segment within a router, enabling direct,
precise performance fault localization. Section 5.2 describes
a potential fault-localization system based upon LDA.
While researchers are often hesitant to propose new router primi-
tives for measurement because of the need to convince major router
vendors to implement them, we observe several recent trends. First,
router vendors are already under strong ﬁnancial pressure from
trading and high-performance computing customers to ﬁnd low-
latency measurement primitives. Second, next-generation 65-nm
router chips have a large number of (currently unallocated) tran-
sistors. Third, the advent of merchant silicon such as Broadcom
and Marvell has forced router vendors to seek new features that
will avoid commoditization and preserve proﬁt margins. Hence,
we suggest that improved measurement infrastructure might be an
attractive value proposition for legacy vendors.
2. PRELIMINARIES
A number of commercially important network applications have
stringent latency demands. Here, we describe three such domains
and provide concrete requirements to drive our design and evalua-
tion of LDA. In addition, we present an abstract model of the mea-
surement task, an instance of what we term a coordinated stream-
ing problem. We then show that some coordinated problems have
fundamentally high space complexity as compared to traditional
streaming versions of the same problem.
2.1 Requirements
An application’s latency requirements depend greatly on its in-
tended deployment scenario. We start by considering the speciﬁc
requirements of each domain in turn, and then identify the overall
measurement metrics of interest.
2.1.1 Domains
Wide-area multi-media applications have demands on the order
of a few hundred milliseconds (100–250 ms). Latency require-
ments in the ﬁnancial sector are tighter (100 μs—1 ms), but the
most stringent requirements are in cluster-based high-performance
computing which can require latencies as low as 1–10 μs.
Interactive multi-media: Games that require fast-paced interac-
tion such as World of WarCraft or even ﬁrst-person shooter games
like Quake can be severely degraded by Internet latencies. While
techniques such as dead-reckoning can ameliorate the impacts, la-
tencies of more than 200 ms are considered unplayable [5]. There
has also been a large increase in voice-over-IP (VoIP) and inter-
active video. While pre-recorded material can be buffered, interac-
tive applications like video conferencing have strict limits on buffer
length and excess jitter substantially diminishes the user experi-
ence. For example, Cisco’s recommendations for VoIP and video
conferencing include an end-to-end, one-way delay of no more than
150 ms, jitter of no more than 30 ms, and less than 1% loss [34].
Automated trading: Orders in ﬁnancial markets are predomi-
nantly placed by machines running automatic trading algorithms
that respond to quotes arriving from exchanges. For example, in
September 2008, the London Stock Exchange announced a ser-
vice that provided frequent algorithmic trading ﬁrms with sub-
millisecond access to market data [21]. Because machines—not
people—are responding to changing ﬁnancial data (a recent survey
indicates that 60–70% of the trades on the NYSE are conducted
electronically, and half of those are algorithmic [25]), delays larger
than 100 microseconds can lead to arbitrage opportunities that can
be leveraged to produce large ﬁnancial gains.
Trading networks frequently connect market data distributors to
trading ﬂoors via Ethernet-switched networks. A typical problem
encountered while maintaining these networks is a saturated core
1-Gbps link that increases latency by 10s of microseconds. The sit-
uation could be addressed (at signiﬁcant cost) by upgrading the link
to 10-Gbps, but only after the overloaded link is detected and iso-
lated. Hence, a market has emerged for sub-microsecond measure-
ment. For example, Cisco resells a passive device manufactured by
Corvil [30] that can detect microsecond differences in latency [35].
High-performance computing: The move from supercomputing
to cluster computing has placed increased demands on data-center
networks. Today, Inﬁniband is the de-facto interconnect for high-
performance clusters and offers latencies of one microsecond or
less across an individual switch and ten microseconds end-to-end.
While obsessing over a few microseconds may seem excessive to
an Internet user, modern CPUs can “waste” thousands of instruc-
tions waiting for a response delayed by a microsecond. Back-end
storage-area networks have similar demands, where Fiber Channel
has emerged to deliver similar latencies between CPUs and remote
disks, replacing the traditional I/O bus.
As a result, many machines in high-performance data centers
are connected to Ethernet, Inﬁniband, and Fiber Channel networks.
Industry is moving to integrate these disparate technologies us-
ing commodity Ethernet switches through standards such as Fibre
Channel over Ethernet [16]. Hence, the underlying Ethernet net-
works must evolve to meet the same stringent delay requirements,
on the order of tens of microseconds, as shown in the speciﬁca-
tions of recent Ethernet switch offerings from companies like Wo-
ven Systems [37] and Arista [4].
2.1.2 Metrics
Each of these domains clearly needs the ability to measure the
average latency and loss on paths, links, or even link segments.
However, in addition, the standard deviation of delay is important
256A
S1
S2
Ingress 
Framer
Forwarding
Lookup
Packet Buffers
VOQs, DiffServ
Ingress 
Framer
Forwarding
Lookup
Packet Buffers
VOQs, DiffServ
Switch 
Fabric
B
Egress 
Framer
Egress 
Framer
Figure 1: An path decomposed into measurement segments.
because it not only provides an indication of jitter, but further al-
lows the calculation of conﬁdence bounds on individual packet de-
lays. For example, one might wish to ensure that, say, 98% of
packets do not exceed a speciﬁed delay. (The maximum per-packet
delay would be even better but we show below that it is impossible
to calculate efﬁciently.)
2.2 Segmented measurement
The majority of operators today employ active measurement
techniques that inject synthetic probe trafﬁc into their network to
measure loss and latency on an end-to-end basis [24, 31, 33]. While
these tools are based on sound statistical foundations, active mea-
surement approaches are inherently intrusive and can incur substan-
tial bandwidth overhead when tuned to collect accurate ﬁne-grained
measurements, as we demonstrate later.
Rather than conduct end-to-end measurements and then attempt
to use tomography or inference techniques [2, 6, 8, 17, 27, 36, 38]
to isolate the latency of individual segments [18, 39], we propose to
instrument each segment of the network with our new measurement
primitive. (We will return to consider incremental deployment is-
sues in Section 5.2.) Thus, in our model, every end-to-end path can
be broken up into what we call measurement segments. For exam-
ple, as shown in Figure 1, a path from endpoint A to endpoint B
via two switches, S1 and S2, can be decomposed into ﬁve mea-
surement segments: A segment between A and the input port of
S1, a segment between the ingress port of S1 and the egress port
of S1, a segment between the egress port of S1 and the ingress port
of S2, a segment between the ingress port of S2 and the egress port
of S2, and a ﬁnal segment between the egress port of S2 and B.
A typical measurement segment extending from just after recep-
tion on a router’s input port to just before transmission on the out-
put side has the potential for signiﬁcant queuing. However, de-
ployments concerned with low latency (e.g., less than 100 μs) nec-
essarily operate at light loads to reduce queuing delays and thus,
latencies are on the order of 10s of microseconds. Such a router
segment can be further decomposed as shown in the bottom of Fig-
ure 1 into several segments corresponding to internal paths between
key chips in the router (e.g., forwarding engine to the queue man-
ager, queue manager to the switch fabric). Such decomposition
allows the delay to be localized with even ﬁner granularity within a
router if queuing occurs and may facilitate performance debugging
within a router.
Thus, we focus on a single measurement segment between a
sender A and a receiver B. The segment could be a single link,
or may consist of a (set of) internal path(s) within a router that con-
tains packet queues. We assume that the segment provides FIFO
packet delivery. In practice, packets are commonly load balanced
across multiple component links resulting in non-FIFO behavior
overall, but in that case we assume that measurement is conducted
at resequencing points or separately on each component link. We
further assume that the segment endpoints are tightly time synchro-
nized (to within a few microseconds). If the clocks at sender and
receiver differ by e, then all latency estimates will have an additive
error of e as well.
Microsecond synchronization is easily maintained within a
router today and exists within a number of newer commercial
routers. These routers use separate hardware buses for time
synchronization that directly connect the various synchronization
points within a router such as the input and output ports; these
buses bypass the packet paths which have variable delays. Hence,
the time interval between sending and receiving of synchronization
signals is small and ﬁxed. Given that most of the variable delays
and loss is within routers, our mechanism can immediately be de-
ployed within routers to allow diagnosis of the majority of latency
problems. Microsecond synchronization is also possible across sin-
gle links using proposed standards such as IEEE 1588 [15].
We divide time into measurement intervals of length T over
which the network operator wishes to compute aggregates. We en-
visage values of T on the order of a few hundred milliseconds or
even seconds. Smaller values of T would not only take up network
bandwidth but would generate extra interrupt overhead for any soft-
ware processing control packets. For simplicity, we assume that the
measurement mechanism sends a single (logical) control packet ev-
ery interval. (In practice, it may need to be sent as multiple frames
due to MTU issues.)
Thus in our model, the sender starts a measurement interval at
some absolute time ts by sending a Start control message. The
sender also begins to compute a synopsis S on all packets sent be-
tween ts and ts + T . At time ts + T , the sender also sends an
End control message. If the receiver gets the Start control message
(since control messages follow the same paths as data messages
they can be lost and take variable delay), the receiver starts the
measurement process when it receives the Start Control message.
The receiver computes a corresponding synopsis R on all packets
received between the Start and End Control messages. The sender
sends synposis S to the receiver in the End Control Message. This
allows the receiver to compute latency and loss estimates as some
function of S and R.
Note that the receiver can start much later than the sender if the
Start Control message takes a long time, but the goal is merely that
the sender and receiver compute the synopses over the same set
of packets. This is achieved if the link is FIFO and the Start and
End Control messages are not lost. Loss of control packets can be
detected by adding sequence numbers to control packets. If either
the Start or End Control packets are lost, the latency estimate for
an interval is unusable. Note that this is no different from losing a
latency estimate if a periodic probe is lost.
We assume that individual packets do not carry link-level times-
tamps. If they could, trivial solutions are possible where the sender
adds a timestamp to each packet, and the receiver subtracts this ﬁeld
from the time of receipt and accumulates the average and variance
using just two counters. Clearly, IP packets do not carry times-
tamps across links; the TCP timestamp option is end-to-end. While
timestamps could be added or modiﬁed within a switch, adding a
32-bit timestamp to every packet can add up to 10% overhead to
257the switch-fabric bandwidth. Further, loss would still need to be
computed with state accumulated at both ends. We will show that
by adding only a modest amount of state beyond that required for