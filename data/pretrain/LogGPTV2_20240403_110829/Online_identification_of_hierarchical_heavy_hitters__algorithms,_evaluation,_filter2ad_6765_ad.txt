### Introduction to Lazy Expansion

We introduce a simple yet effective technique, known as lazy expansion, which significantly reduces the space requirement for data processing. The core idea behind lazy expansion is straightforward: when we encounter a large item with a value \( v \) that falls within the range \([k \cdot 2^{i-1}, k \cdot 2^i]\), we split it into \( k \) smaller items, each with a value of \( \frac{v}{k} \).

### Forecasting and Error Detection

The forecast for any interval \( i \) is given by:
\[ F_i = S_i + T_i \]
where \( S_i \) and \( T_i \) are the smoothed and trend components, respectively.

The forecast error is then calculated as:
\[ E_i = X_i - F_i \]

Significant deviations from the forecast, indicating potential changes in the data, can be detected by identifying data points where the forecast error \( E_i \) exceeds a time-varying detection threshold \( DT_i \). For online change detection, it is common to maintain an exponentially weighted moving average of the absolute forecast errors \( |E_i| \) and set \( DT_i \) to be a multiple of this smoothed deviation.

### Extracting Time Series Data

Given a traffic cluster with the true traffic volume \( X_i \) in interval \( i \), our summary data structure produces three different values using different rules to calculate the amount of missed traffic:
- A lower bound \( X_i^L \) (using the no-copy rule)
- An upper bound \( X_i^U \) (using the copy-all rule)
- An estimate \( X_i^S \) (using the splitting rule)

Our experience with HHH (Heavy Hitters and Heavy Hitters over time) detection suggests that \( X_i^S \) often provides the most accurate estimate. Therefore, we use the time series \( \{X_i^S\} \) as input for the Holt-Winters forecasting model to obtain \( E_i^S \) and \( DT_i^S \), which are estimates for the true forecast errors \( E_i \) and detection thresholds \( DT_i \), respectively. We also use \( X_i^L \) and \( X_i^U \) to obtain tight bounds on the true forecast errors \( E_i \), as detailed in Section 4.3.

### Dealing with Missing Clusters

One critical issue is the presence of missing clusters. A cluster may not appear in the summary structure for every interval. To address this, we need to estimate the associated traffic volume to avoid gaps in the reconstructed time series. Our summary structure allows us to conveniently obtain such estimates. For example, given a 2-dimensional missing cluster with key \( k \), we conceptually insert a new element with key \( k \) and value 0 into the summary data structure, resulting in one or more newly created fringe nodes. We then obtain estimates for the first newly created fringe node and use them as the corresponding estimates for \( k \). After this, we can remove all the newly created nodes through compression. In the final implementation, we do not need to create and then remove the new fringe nodes; we only need to perform a lookup to find the first insertion position.

### Obtaining Bounds on Forecast Errors

Let the superscripts \( L \) and \( U \) denote the lower and upper bounds for a variable, respectively. For instance, \( X_i^L \) denotes the lower bound for \( X_i \). Below, we show how to compute the lower and upper bounds for the true forecast errors \( E_i \).

#### Naive Solution
At first glance, it seems straightforward to compute the lower bound \( E_i^L \) by recursively applying the equations for \( S_i \) and \( T_i \) and then using them to form bounds for \( F_i \) and \( E_i \). Specifically, we have:
\[ S_i^L = \alpha S_{i-1}^L + (1 - \alpha) X_{i-1}^L \]
\[ S_i^U = \alpha S_{i-1}^U + (1 - \alpha) X_{i-1}^U \]
\[ T_i^L = \beta (S_i^L - S_{i-1}^L) + (1 - \beta) T_{i-1}^L \]
\[ T_i^U = \beta (S_i^U - S_{i-1}^U) + (1 - \beta) T_{i-1}^U \]
\[ F_i^L = S_i^L + T_i^L \]
\[ F_i^U = S_i^U + T_i^U \]
\[ E_i^L = X_i^L - F_i^U \]
\[ E_i^U = X_i^U - F_i^L \]

Unfortunately, reconstruction errors can accumulate exponentially with this approach, leading to bounds that are too loose to be useful, as shown in Figure 13(a).

#### Our Solution
To obtain tighter bounds, we directly represent \( S_i \) and \( T_i \) as linear combinations of \( X_j \) (for \( j \leq i \)) and incorporate the bounds \( X_i^L \) and \( X_i^U \). Specifically, let:
\[ S_i = \sum_{j=1}^{i-1} s[i, j] X_j \]
\[ T_i = \sum_{j=1}^{i-1} t[i, j] X_j \]

By using these linear combinations, we can derive more accurate and tighter bounds for the forecast errors \( E_i \).