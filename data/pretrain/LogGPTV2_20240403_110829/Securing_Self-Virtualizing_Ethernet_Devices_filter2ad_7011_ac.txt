t
t
a
i
c
d
o
i
r
e
p
 250
 200
 150
 100
 50
]
s
µ
[
y
c
n
e
t
a
l
m
i
t
c
v
i
 2000
 1500
 1000
 500
 0
 0
message size
64B
1024B
 50
 100
 150
 200
 250
 300
pause frames attacker sends
 each second [frames/second]
(b)
 0
 20
 40
 60
 80
 100
 120
time [seconds]
(a)
Figure 6: Pause frame attack: victim latency in 10GbE environment
and 1024B are barely distinguishable and almost con-
verge; the latency is the same for small and large size
messages under attack.
In Figure 7c we see that measured latency and ex-
pected latency differ somewhat. In practice, this differ-
ence means that an attacker can control the victim’s la-
tency with slightly less precision than it can control its
throughput, but it can still control both with high preci-
sion and relatively little effort.
In back-to-back conﬁguration, without a switch, la-
tency behaves as expected. We believe this difference
is caused by the switch’s internal buffering methods—
in addition to storing frames internally,
the Ethernet
switch prevents the possible side effects of such buffering
e.g., head-of-line blocking [70] and congestion spread-
ing [13]. To accurately explain this phenomenon, we
need access to the switch internals; unfortunately, the
Ethernet switch uses proprietary closed software and
hardware.
Experiments with Non-Intel Devices We performed
an identical experiment on same setup with an SRIOV
Broadcom NetXtreme II BCM57810 10GbE NIC [26]
and got the same results. Our attack is valid for this NIC
as well.
USENIX Association  
24th USENIX Security Symposium  341
1000
800
600
400
200
]
s
/
b
M
[
t
u
p
h
g
u
o
r
h
t
d
e
r
u
s
a
e
m
 0  200  400  600  800  1000
expected throughput [Mb/s]
(a)
10
8
6
4
2
0
t
u
p
h
g
u
o
r
h
t
d
e
r
u
s
a
e
m
]
s
/
b
G
[
 0
 2
 4
 6
 8
 10
expected throughput [Gb/s]
(b)
2000
1500
1000
500
0
]
s
µ
[
y
c
n
e
t
a
l
d
e
r
u
s
a
e
m
message size
64B
1024B
 0
 500  1000  1500  2000
expected latency [µs]
(c)
Figure 7: Pause frame attack: expected vs. measured throughput and latency
We also tried the attack described above on another
vendor’s 40GbE SRIOV adapter. Whenever the attack-
ing VM transmitted MAC control frames (pause frames)
through its VF, the adapter completely locked up and be-
came unresponsive. It stopped generating both transmit
and receive interrupts, and required manual intervention
to reset it, by reloading the PF driver on the host. This
lockup appears to be a ﬁrmware issue and has been com-
municated to the adapter vendor.
Clearly, with this adapter and this ﬁrmware issue, a
malicious VM could trivially perform a straightforward
denial of service attack against its peer VMs that use this
adapter’s VFs and against the host. But since this attack
is trivial to discover, we focus instead on the stealthier
pause frame attack, which is much harder to discover and
protect against.
5 Attack Ramiﬁcations
The consequences of the attack are substantial. If Eth-
ernet ﬂow control is enabled on the SRIOV device, the
host’s VMs’ security is compromised and the VM’s are
susceptible to the attack.
The attack cannot be prevented using the ﬁltering ca-
pabilities of currently available SRIOV Ethernet devices
due to their minimal ﬁltering capability. At best, mod-
ern SRIOV NICs are capable of enforcing anti-spooﬁng
checks based on the source MAC address or VLAN tag
of the VM, to prevent one VM from pretending to be
another. In the attack we describe, the adversary gener-
ates ﬂow control frames with the malicious VM’s source
MAC and VLAN tag, so anti-spooﬁng features cannot
block the attack.
Since the attack cannot be prevented with current
NICs and switches, cloud providers must either be con-
tent with ﬂawed security and fully trust the guest VMs or
disable the Ethernet ﬂow control in their networks. Nei-
ther option is palatable. The former is unrealistic for the
public cloud and unlikely be acceptable to private cloud
providers. The latter means giving up the performance
beneﬁts of lossless Ethernet, increasing overall resource
utilization, and reducing performance. We discuss in
greater detail the performance advantages that Ethernet
ﬂow control provides in Section 8.
6
Improving SRIOV Security
The attack described in the previous sections is the result
of a fundamental limitation of SRIOV: from the network
point of view, VFs and their associated untrusted VMs
are all lumped together into a single end-station. To se-
cure SRIOV and eliminate the attack while keeping ﬂow
control functionality, we propose to extend SRIOV Eth-
ernet NIC ﬁltering capability to ﬁlter trafﬁc transmitted
by VFs, not only on the basis of source MAC and VLAN
tags—the method currently employed by anti-spooﬁng
features—but also on the basis of the MAC destination
and Ethernet type ﬁelds of the frame. This ﬁltering can-
not be done by the host without some loss of perfor-
mance [39] and has to be done before trafﬁc hits the edge
switch. Hence it must be done internally in the SRIOV
NIC. We built a software-based prototype of an SRIOV
Ethernet NIC with pause frame ﬁltering. Before present-
ing the prototype, we begin by describing the internals of
an SRIOV NIC.
6.1 SRIOV NIC Internals
Figure 8a shows a detailed schema of an SRIOV Ether-
net NIC. The SRIOV device is connected to the external
adjacent Ethernet switch on the bottom side and to the
host’s PCIe bus, internal to the host, on the top side.
Switching The NIC stores frames it receives from the
external switch in its internal buffer. The size of this
buffer is on the order of hundreds of KBytes, depending
on the NIC model: 144KB in Intel I350 [43] and 512KB
in Intel 82599 [42]. After receiving a packet, the SRIOV
NIC looks up the frame’s MAC in its MAC address table,
342  24th USENIX Security Symposium 
USENIX Association
ﬁnds the destination VF according to the frame’s desti-
nation MAC address, and copies the frame (using DMA)
over the PCIe bus to the VF’s buffer ring, which is allo-
cated in the host’s RAM. This is analogous to a standard
Ethernet switch that receives a packet on an ingress port,
looks up its MAC address, and chooses the right egress
port to send it to. The data path of the frame is marked
with a red dashed line in Figure 8a. In addition, SRIOV
NIC is able to perform VM-to-VM switching internally,
without sending the frames to the external switch.
Internal Buffer When Ethernet ﬂow control is en-
abled,
the SRIOV NIC starts monitoring its internal
buffer. If the NIC cannot process received frames fast
enough, for example due to an overloaded or slow PCIe
link, the buffer ﬁlls up. Once it reaches a predeﬁned
threshold, the SRIOV NIC generates and sends pause
frames to the external Ethernet switch. The switch then
holds transmissions to the NIC for the requested time,
storing these frames in its own internal buffers. While
the switch is buffering frames, the NIC should continue
copying the frames it buffered into the each VF’s ring
buffer, clearing up space in its internal buffer.
Ring Buffer The ﬁnal destination for a received frame
is in its VF’s ring buffer, located in host RAM. The net-
work stack in the VM driving the VF removes frames
from its ring buffers at a rate that is limited by the CPU.
If the VM does not get enough CPU cycles or is not ef-
ﬁcient enough, the NIC may queue frames to the ring
buffer faster than the VM can process them. When the
ring buffer ﬁlls up, most Ethernet NICs (e.g., those of In-
tel’s and Mellanox’s) will simply drop incoming frames.
Less commonly, a few NICs, such as Broadcom’s NetX-
treme II BCM57810 10GbE, can monitor each VF’s ring
buffer. When the ring buffer is exhausted, the NIC can
send pause frames to the external switch to give the host
CPU a chance to catch up with the sender. When avail-
able, this functionality is usually disabled by default.
Outbound Security Some SRIOV Ethernet NICs
(e.g., Intel 82599 10GbE [42] or I350 1GbE [43] NICs)
include anti-spooﬁng functionality. They can verify that
the source MAC address and/or VLAN tag of each frame
transmitted by the VF belongs to the transmitting VF.
To this end, these NICs have an internal component that
can inspect and even change frames transmitted from the
VF. In addition, Intel SRIOV NICs have advanced in-
bound ﬁltering capabilities, storm control, rate limiting,
and port mirroring features, very much like any standard
Ethernet switch.
As we can see, Ethernet SRIOV devices implement
on-board a limited form of Ethernet switching. That
is why such devices are also known as virtual Ethernet
bridges (VEBs).
6.2 The VANFC design
The key requirement from VANFC is to ﬁlter outbound
trafﬁc transmitted by a VF. Ideally, VANFC would be
implemented in a production SRIOV NIC. Unfortu-
nately, all tested SRIOV NICs are proprietary with closed
ﬁrmware. Furthermore, most frame processing logic is
implemented in high speed ASIC hardware.
We opted instead to build VANFC as a software-based
prototype of an SRIOV NIC that ﬁlters outbound trafﬁc.
VANFC takes advantage of the following two observa-
tions: (1) VEB embedded into the SRIOV NIC device
replicates standard Ethernet switching behavior and can
be considered as a virtual Ethernet switch; (2) all valid
pause frames are generated by the NIC’s hardware and
have the PF’s source MAC address, whereas invalid—
malicious—pause frames are sent with source address of
a VF. Should the adversary VM attempt to generate pause
frames with the PF’s source MAC address, the NIC’s
anti-spooﬁng will ﬁnd and drop these frames.
In order to ﬁlter transmitted malicious pause frames,
we ﬁrst need to identify pause frames.
In such
frames the Ethernet type ﬁeld is 0x8808 (MAC con-
trol type), the MAC opcode ﬁeld is 0x0001 (pause
opcode), and the destination MAC address is multi-
cast 01-80-C2-00-00-01. For any such packet, the
VANFC ﬁlter should drop the frame if the source MAC is
different than the PF’s MAC address.
As mentioned previously, most SRIOV NICs already
have a component that can ﬁlter outbound trafﬁc; this
component is a part of the SRIOV device’s internal Eth-
ernet switch and cannot be modiﬁed. Our prototype ex-
tends this switch in software by running the extension
on the wire between the SRIOV NIC and the external
switch.
Filtering Component For our Ethernet ﬁltering de-
vice we use the standard Linux bridge conﬁgured on
an x86-based commodity server running Ubuntu server
13.10 and equipped with two Intel 82599 10 Gigabit TN
Network controllers installed in PCIe gen 2 slots. One
NIC is connected to the host PF and the other is con-
nected to the external Ethernet switch, as displayed in
Figure 8b. Ethernet switching is performed by the Linux
bridge [4] and ﬁltering is done by the ebtables [32].
Performance model Bridge hardware is fast enough
not to be a bottleneck for 10Gb Ethernet speed. How-
ever, by adding to the setup an Ethernet device imple-
mented in software, we increased latency by a constant
delay of approximately 55µs. An eventual implementa-
USENIX Association  
24th USENIX Security Symposium  343
(a)
(b)
Figure 8: Fig. (a) shows schema of current SRIOV NIC internals; Fig. (b) shows VANFC schema.
tion of VANFC in hardware will eliminate this overhead;
we therefore discount it in latency oriented performance
tests.
We wanted to make this bridge completely transparent
and not to interfere with passing trafﬁc: the host should
keep communicating with the external switch and the
switch should keep communicating with the host as in
the original setup without VANFC. VANFC should change
neither the SRIOV software/hardware interface nor the
Ethernet ﬂow control protocol. To ensure this, we made
a few modiﬁcations in Linux bridge code and in the Intel
82599 device driver used by the bridge device.
Bridge Modiﬁcation The standard Ethernet bridge
should not forward MAC control frames that are used
to carry pause commands since MAC control frames are
designed to be processed by Ethernet devices. Since
we want the bridge to deliver all of the trafﬁc be-
tween the SRIOV device and the external switch, in-
cluding the pause frames sent by the PF, we modify the
Linux bridging code to forward MAC control frames
and use ebtables to ﬁlter pause frames not sent from
the PF. Our experiments use a static conﬁguration for
ebtables and for the Linux bridge.
Device Driver Modiﬁcation We use a modiﬁed
ixgbe driver version 3.21.2 for Intel 10G 82599 net-
work controllers on the bridge machine. According to
the Intel 82599 controller data-sheet [42], the ﬂow con-