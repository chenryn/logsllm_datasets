agree on a USID for the new stream so they can attach the
stream to other channels. Each host uses a convention for
assigning LSIDs that allows an Init packet’s receiver to ex-
trapolate a USID from the new stream’s 16-bit LSID. Each
host assigns LSIDs in its LSID space for a channel using
Figure 5: Stream Data Transfer Packets
the low 16 bits of a 64-bit counter, and its peer tracks this
counter and extrapolates the full 64-bit value from a received
LSID the same way the channel layer extrapolates packet se-
quence numbers. The hosts use these 64-bit counters, plus
a pseudorandom nonce unique to the channel and ﬂow di-
rection, to agree on the new stream’s USID implicitly. The
initiator may skip counter values corresponding to LSIDs
still in use by other streams, provided it doesn’t get too far
ahead of its peer and lose counter synchronization.
Upon receiving an Init packet with an unknown LSID, the
responder records the new LSID, then sends a Reply packet
to assign its own “return-path” LSID to the new stream for
data transfer in the opposite direction. The Reply packet
has the same format as an Init packet, except it contains the
initiator’s just-assigned LSID for the new stream in place of
the parent stream’s LSID (see Figure 5). The responder
assigns LSIDs using its 64-bit counter as above to maintain
counter synchronization, but does not use the counter to
derive a USID since each stream requires only one USID.
4.3.4 Data Transfer and Acknowledgment
Both Init and Reply packets may contain application data
and stream control ﬂags. The initiator uses Init packets to
start sending data on the new stream immediately with-
out waiting for the receiver’s acknowledgment, eliminating
TCP’s 3-way handshake delay on new streams once a chan-
nel has been opened to the desired host. The responder may
similarly start sending response data immediately via Reply
packets. The Init and Reply packets contain a 16-bit Byte
Sequence Number (BSN) ﬁeld indicating the data segment’s
logical oﬀset in the new stream, so a host can send up to
216 − 1 + M T U bytes of data this way before it must start
using using ordinary Data packets, which it can do only af-
ter receiving an acknowledgment indicating that the peer is
aware of the newly assigned LSID.
Data packets have the same structure as Init and Reply
packets, except that they have a 32-bit BSN and no PSID
or RSID (see Figure 5). The stream layer uses this BSN
to reassemble data segments in the correct order at the re-
ceiver, using wraparound arithmetic as in TCP, making the
longevity of a stream eﬀectively unlimited.
A host buﬀers each data segment it sends until it re-
ceives an acknowledgment for that segment, periodically
retransmitting unacknowledged segments. Since the chan-
nel layer’s acknowledgments refer to packet sequence num-
bers and not byte sequence numbers, the sender records the
packet sequence numbers the channel protocol assigns each
data segment during recent transmission attempts, in or-
der to lookup and free the segment when a corresponding
acknowledgment arrives. Since the channel layer’s packet se-
quence numbers are independent of the stream layer’s LSIDs,
a packet in one stream may eﬀectively acknowledge segments
in other streams. The stream layer uses a separate Ack
packet type to send acknowledgments that cannot be piggy-
backed onto data ﬂowing in the opposite direction.
Since the channel layer’s acknowledgments are deﬁnitive,
the receiver must only acknowledge a data segment once it
has fully processed and locally buﬀered the segment. If the
receiver cannot process a segment due to a temporary re-
source shortage, it may drop the segment without acknowl-
edgment, but using ﬂow control to avoid the need to drop
segments is preferred since dropped segments trigger the
sender’s congestion control and aﬀect the entire channel.
If a data segment already transmitted must be retrans-
mitted with a smaller MTU, the sender “re-fragments” the
segment into smaller segments for retransmission, adjusting
the BSN ﬁelds in the new segments accordingly. A host may
also repackage an Init or Reply packet’s data into an ordi-
nary Data packet for retransmission, if an acknowledgment
for some other Init or Reply packet arrives in the meantime.
Init, Reply, and Data packets contain Push (P) and Close
(C) ﬂags that work like TCP’s PSH and FIN ﬂags, indicating
data that should be pushed to the application and marking
the end of stream, respectively. Section 4.3.9 below describes
how SST garbage collects stream state after close.
4.3.5 Datagram Delivery
When the application submits a datagram to be sent as
an “ephemeral substream” with best-eﬀort semantics as de-
scribed in Section 2.2, the stream layer checks that the data-
gram is small enough to ensure a reasonable chance of suc-
cessful delivery, and if so transmits it using a sequence of
Datagram packets shown in Figure 5. The ﬁrst packet in
the sequence has the First (F) ﬂag set, the last packet has
the Last (L) ﬂag set, and the packets have consecutive se-
quence numbers, allowing the receiver to determine when a
received datagram is complete. Each packet’s LSID refers
to the parent stream; the sender never assigns an LSID or
maintains any state for the ephemeral child substream.
If the sending stream layer judges the ephemeral sub-
stream to be too large for delivery as a datagram, it sends
the substream instead in standard reliable fashion using
Init and Data packets, retransmitting individual segments
as necessary, and closes the substream when ﬁnished. The
receiving application obtains no indication of the actual de-
livery method by which the ephemeral substream arrived.
4.3.6 Flow Control
While congestion control operates at channel granularity,
SST provides ﬂow control for each stream individually, al-
lowing the receiving application to accept data at diﬀerent
rates on each stream. Every packet the stream layer sends
contains a receive window update, indicated in the header’s
5-bit Window ﬁeld. This ﬁeld uses an exponential encoding:
a value n indicates a window of at least 2n − 1 bytes. When
the window is large, the sender does not need to know its
size precisely since it will take a while to ﬁll anyway, but the
receiver’s updates become more precise as its buﬀers ﬁll and
Figure 6: Stream Control Packets
the window shrinks. To avoid a variant of silly window syn-
drome [15], the sender never fragments data segments just
to make a partial segment ﬁt into the receive window:
in-
stead it waits until the window can accommodate a full-size
segment, or a short segment containing a Push marker.
TCP uses its cumulative acknowledgment position as a
“base” from which to calculate the window horizon, but SST
has no cumulative acknowledgments from which to calcu-
late such a horizon. SST’s window credit instead represents
the total number of unacknowledged bytes the sender may
have in ﬂight. The receiver deducts from its advertised win-
dow the size of each segment it receives and acknowledges,
and cancels this deduction once it delivers the segment to
the application. The sender similarly deducts each segment
it sends from its window credit and cancels this deduction
when the segment is acknowledged, even if the segment was
received and acknowledged out of order. The sender tracks
the packet sequence numbers of window updates and always
uses only the most recently-sent update.
For ﬂow control, SST treats an Init packet’s data as be-
longing to the parent stream—the stream speciﬁed in the
packet’s PSID ﬁeld—although the data is semantically part
of the new child stream. In eﬀect, when a host sends data on
a new stream without waiting for an initial window size from
the responder, the sender “borrows” from the parent’s re-
ceive window to send this initial data. This borrowing main-
tains proper ﬂow control and avoids receive buﬀer overrun
while allowing stream creation with no round-trip delay.
4.3.7 Detaching and Migrating Streams
The stream layer sends an Attach packet, shown in Fig-
ure 6, to attach an existing stream to a new channel. The
Attach packet contains the LSID assigned by the sender and
the permanent USID of the stream to be attached. The
sender can attach a stream to a limited number of channels
at once (currently two), and indicates via an attachment slot
number which of these potential attachments it is using. The
receiver looks up the stream by the speciﬁed USID, asso-
ciates the speciﬁed attachment slot in that stream with the
speciﬁed LSID in the channel on which the Attach packet
arrived, and acknowledges the packet via the channel layer.
A host may detach a stream from a channel, freeing the
stream’s LSID in that channel for use by other streams,
by sending a Detach packet (Figure 6). By detaching idle
streams the application has not used for some time and
treating its LSID space as a cache, SST can manage an
arbitrary number of streams. Host API issues may impose
limits on the number of open streams, such as Unix’s ﬁle de-
scriptor limit—but in the current user space SST prototype,
which does not use ﬁle descriptors for streams, the number
of open streams is limited only by available memory.
4.3.8 Forceful Reset
As in TCP, either host may unilaterally terminate an SST
stream in both directions and discard any buﬀered data. A
host resets a stream by sending a Reset packet (Figure 6)
containing an LSID in either the sender’s or receiver’s LSID
space, and an O (Orientation) ﬂag indicating in which space
the LSID is to be interpreted. When a host uses a Reset
packet to terminate a stream it believes to be active, it uses
its own LSID referring to the stream, and resends the Reset
packet as necessary until it obtains an acknowledgment.
A host also sends a Reset in response to a packet it receives
referring to an unknown LSID or USID. This situation may
occur if the host has closed and garbage collected its state for
a stream but one of its acknowledgments to its peer’s data
segments is lost in transit, causing its peer to retransmit
those segments. The stateless Reset response indicates to
the peer that it can garbage collect its stream state as well.
Stateless Reset responses always refer to the peer’s LSID
space, since by deﬁnition the host itself does not have an
LSID assigned to the unknown stream.
4.3.9 Garbage Collecting and Reusing LSIDs
An SST application that uses one stream per transaction
may create and destroy streams rapidly: in the worst case, a
host can create a stream, assign it an LSID, transmit up to
an MTU of data, and close its end of the stream, all with one
Init packet. The responder may similarly acknowledge the
Init packet, send up to one MTU of response data, and close
the stream, with one Reply packet. SST may therefore reuse
16-bit LSIDs for many successive streams within a channel’s
lifetime, leading to the risk of confusing packets referring to
diﬀerent uses of the same LSID. This is the same problem
that at a lower level motivates TCP’s ISN selection [51, 53]
and the channel protocol’s keyed authenticators.
To avoid confusing old and new uses of an LSID, after de-
taching an LSID the stream layer imposes a “quiet period”
before creating or attaching another stream with the same
LSID. This quiet period corresponds to TCP’s TIME-WAIT,
but SST counts the duration of this quiet period in packet
sequence numbers instead of wall-clock time, relying on the
channel layer’s mis-ordering limit (MOL). With a 32 packet
MOL, for example, after detachment a host waits for both its
and its peer’s sequence numbers to advance 32 packets be-
yond a point when both hosts know about the detachment.
The channel’s replay logic drops packets that arrive so late
that they might confuse new stream attachments using this
LSID. Because the MOL is a (typically small) constant, and
one packet can attach and/or detach only one stream, the
number of LSIDs that may be stuck in this quiet period is
similarly small, regardless of packet rate. The mis-ordering
limit thus avoids the need for time-bounded TIME-WAITs
and eliminate the risk of state overload under heavy use [18].
4.4 The Negotiation Protocol
The negotiation protocol is responsible for setting up new
channels with either weak or strong security. Negotiation
with weak security sets up the unpredictable checksum key
described in Section 4.2.3. The initiator may piggyback ap-
plication data onto the ﬁrst negotiation protocol packet, al-
lowing channel setup with no eﬀective round-trip overhead,
but the responder may ignore this initial data and return a
cookie challenge if it is loaded or under DoS attack.
The strong security mode uses Just Fast Keying [1] to
establish shared cryptographic secrets and verify host iden-
tities using a simple, ﬁxed four-message (two round-trip)
exchange. The last two messages may carry piggybacked
application data, for an eﬀective minimum channel setup
overhead of one round trip, identical to TCP’s.
SST is designed to work with UIA ad hoc naming [22] and
UIP routing [21] to support seamless communication among
both ﬁxed and mobile personal devices. UIA, UIP, and
SST use cryptographic host identiﬁers analogous to those
of HIP [36] to identify endpoints securely, so when a host’s
IP address changes, SST merely reruns the negotiation pro-
tocol to establish a channel between the new addresses, and
migrates existing streams to the new channel.
5. EVALUATION
This section reports on preliminary experience implement-
ing and using SST in real and simulated environments. We
examine how SST scales across transaction sizes in compar-
ison with TCP and UDP, how Web-style transactions on
SST compare with non-persistent, persistent, and pipelined
HTTP over TCP, and how applications can dynamically pri-
oritize SST streams to improve interactive responsiveness.
5.1 Implementation
The initial SST prototype takes the form of a user-space
library written in C++, which runs on Linux, BSD, Mac OS
X, and Windows. The library implements SST atop UDP,
so its use requires no special privileges or OS extensions, and
the library can be statically linked into or distributed with
applications to minimize deployment burden on users. The
prototype implements most of the SST protocol design, in-
cluding classic TCP congestion control [2], but a few features
such as ﬂow control and MTU discovery are still incomplete.
The prototype also allows the application to assign priority
levels to streams, for explicit control of data transmission
within the scope of a congestion controlled channel. For
controlled testing and simulation, the library allows client
applications to run multiple instances of SST simultaneously
in one process, and to virtualize SST’s use of the host’s tim-
ing and networking facilities. The prototype currently totals
about 13,000 source lines, or 4,400 semicolons, and is avail-
able at http://pdos.csail.mit.edu/uia/sst/.
5.2 Experience with Applications
The SST prototype is in regular use by Netsteria, an ex-
perimental peer-to-peer application supporting text-based
chat, voice-over-IP calling and conferencing, and swarming
ﬁle transfers. Netsteria’s combination of diﬀerent types of
network activities operating concurrently serves well to ex-
ercise SST’s capabilities and drive its development. The ﬁle
transfer mechanism, for example, divides ﬁles into variable-
length blocks and uses a separate SST stream for each block
request/reply transaction, making use of SST’s scalability
over transaction sizes. The voice chat mechanism uses SST’s
ephemeral substreams to transmit small media frames eﬃ-
ciently with best-eﬀort delivery to minimize latency.
5.3 Performance Validation
To test SST’s basic performance against the “gold stan-
dard” of TCP, we ﬁrst run microbenchmarks of raw band-
width and TCP-friendliness on three transports: the SST
prototype, the host operating system’s native TCP, and a
user-space TCP implementation that was developed along-
side the SST library for comparison purposes. Though the
native TCPs are more mature, the user-space TCP can
run on either a real or simulated network like the SST li-
brary. Since SST always uses selective acknowledgments,
the user-space TCP implements TCP’s SACK extension to
ensure a fair comparison, as do the native TCP stacks on the
hosts used for testing. Since TCP does not provide crypto-
graphic security, the benchmarks run SST in its comparable
checksum-based authentication mode.
Downloading a 10MB ﬁle from a PC running SuSE Linux
10.0 to a MacBook Pro running Mac OS 10.4.8 over a real
1.5Mbps DSL connection, and taking the best of three runs
to factor out out possible delays caused by unrelated sys-
tem daemon activity, SST was measured to be 1.0% slower
than native TCP, and user-space TCP was 2.1% slower—