title:Dynamic pipelining: making IP-lookup truly scalable
author:Jahangir Hasan and
T. N. Vijaykumar
 Dynamic Pipelining: Making IP-Lookup Truly Scalable
Jahangir Hasan
T. N. Vijaykumar
{hasanj, vijay} @ecn.purdue.edu
School of Electrical and Computer Engineering, Purdue University
Abstract
A truly scalable IP-lookup scheme must address ﬁve challenges of
scalability, namely: routing-table size, lookup throughput, imple-
mentation cost, power dissipation, and routing-table update cost.
Though several IP-lookup schemes have been proposed in the past,
none of them do well in all the ﬁve scalability requirements. Previ-
ous schemes pipeline tries by mapping trie levels to pipeline stages.
We make the fundamental observation that because this mapping is
static and oblivious of the preﬁx distribution, the schemes do not
scale well when worst-case preﬁx distributions are considered. This
paper is the ﬁrst to meet all the ﬁve requirements in the worst case.
We propose scalable dynamic pipelining (SDP) which includes three
key innovations: (1) We map trie nodes to pipeline stages based on
the node height. Because the node height is directly determined by
the preﬁx distribution, the node height succinctly provides sufﬁcient
information about the distribution. Our mapping enables us to prove
a worst-case per-stage memory bound which is signiﬁcantly tighter
than those of previous schemes. (2) We exploit our mapping to pro-
pose a novel scheme for incremental route-updates. In our scheme a
route-update requires exactly and only one write dispatched into the
pipeline. This route-update cost is obviously the optimum and our
scheme achieves the optimum in the worst case. (3) We achieve scal-
ability in throughput by simultaneously pipelining at the data-struc-
ture level and the hardware level. SDP naturally scales in power and
implementation cost. We not only present a theoretical analysis but
also evaluate SDP and a number of previous schemes using detailed
hardware simulation. Compared to previous schemes, we show that
SDP is the only scheme that scales well in all the ﬁve requirements.
Categories & Subject Descriptors
C.2.6 [Internetworking]: Routers
General Terms
Algorithms, Design, Performance
Keywords
IP-lookup, Scalable, Pipelined, Tries, Longest Preﬁx Matching
1 Introduction
The pervasive use of the Internet and advances in ﬁber optics
enabling high line-rates are resulting in an explosion in total trafﬁc
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’05, August 22–26, 2005, Philadelphia, Pennsylvania, USA.
Copyright 2005 ACM 1-59593-009-4/05/0008...$5.00.
and in the number of hosts on the Internet. Studies have shown that
the trafﬁc is doubling almost every three months [18] and the num-
ber of hosts is tripling every two years [5]. These trends translate
into two major problems for IP-lookup mechanisms in core routers.
First, routers will soon need to look up their routing tables at the rate
of about 2 ns per packet (for a 160 Gbps line-rate and minimum
packet size of 40 bytes). Second, routers will have to search through
a large number of preﬁxes in their routing tables (e.g., routers today
hold a few hundred thousands of preﬁxes).
While the demand has been increasing, the supply has not been
scaling up. The key component in IP-lookup is the routing-table
memory which is used to search through the preﬁxes to locate the
one that matches the incoming packet. The IP-lookup scheme has to
satisfy ﬁve key scaling requirements: (1) Because memory size
directly affects system cost, lookup speed and power dissipation, the
total memory required should grow slowly with the number of pre-
ﬁxes. IP-lookup should scale well in memory size and be efﬁcient in
storing the ever-increasing number of preﬁxes. (2) IP-lookup must
scale in throughput, forwarding packets at
increasingly higher
speeds to keep up with the ever-increasing line-rates. (3) To keep the
complexity of heat removal and the cost of cooling reasonable,
power dissipation of IP-lookup must scale well. The power should
grow slowly with line-rates and number of preﬁxes, and avoid
becoming prohibitive. (4) Because a routing table is typically
unavailable for lookups during the time that it is being updated,
applying updates should be quick and efﬁcient. Though updates may
be infrequent, a router with slow updates will require partial or full
duplication of routing-table memory in order to avoid dropping an
increasing number of packets as line-rates grow. Therefore, IP-
lookup must allow simple, incremental, and fast updates indepen-
dent of the routing table size. (5) IP-lookup must scale well in
implementation cost and complexity to remain feasible for future
table sizes and line-rates. Accordingly, the chip area should grow
slowly with line-rates and number of preﬁxes. Because routers must
provide worst-case guarantees for all the ﬁve aspects, meeting these
requirements is especially hard.
We propose an IP-lookup mechanism which meets all the ﬁve
scalability requirements in the worst case. The problem of scalable
IP lookup is not new; there have been several papers on the topic
[1][3][4][11][13][14][15][17][19] which may lead one to believe
that the problem is well-researched, and satisfactorily solved. How-
ever, all previous schemes satisfy only two or three of the require-
ments but not all ﬁve. The unsatisﬁed requirements will likely render
the schemes infeasible in the future. Meeting all the ﬁve require-
ments with worst-case guarantees for the ﬁrst time is the key contri-
bution of this paper. We not only present a theoretical analysis but
also evaluate our scheme and a number of previous schemes using
detailed hardware simulation.
Previous IP-lookup schemes can be classiﬁed into two catego-
ries: TCAMs and trie-based. We list their shortcomings here and
explain the detailed reasons for the shortcomings in Sections 3.1,
2053.2, and 4. TCAMs do not scale well in power and implementation
cost at high line-rates. Tries scale well in power but they do not scale
well in throughput if they are not pipelined. Two approaches for
pipelining tries are hardware-level pipelining (HLP) [15], and data-
structure-level pipelining (DLP) [1]. HLP pipelines the routing-table
memory at the hardware level. However, HLP does not scale well in
power and implementation cost because it requires extremely deep
pipelines at high line-rates. DLP pipelines the trie at the data struc-
ture level by placing each trie level in a different memory, so that
different packets simultaneously probe different levels. Because
DLP does not require HLP’s deep hardware pipelining, DLP scales
well in power and implementation cost.
However, DLP has three shortcomings: (1) DLP does not scale
well in size due to large worst-case memory. (2) DLP’s route-update
cost can be made O(1) by using Tree Bitmap [4], if memory man-
agement overhead is ignored. However, Tree Bitmap almost doubles
the worst-case memory size due to its inability to use leaf-pushing,
and requires over 100 memory accesses, in the worst case, for a
route-update if memory management overhead is considered. (3)
DLP scales for throughput by partitioning the trie into pipeline
stages. However, a trie cannot be partitioned into more stages than
its total height. Hence, DLP’s scalability in throughput is limited by
the maximum height of the trie (i.e., the maximum preﬁx length),
which is constant.
DLP pipelines the trie by mapping a speciﬁc preﬁx bit (i.e., a
speciﬁc trie level), to a speciﬁc pipeline stage (e.g., the 12th bit is
mapped to the 2nd stage). We make the fundamental observation that
DLP incurs its ﬁrst shortcoming because this mapping is completely
static and oblivious of the preﬁx distribution. For instance, a trie
node examining the 12th bit remains mapped to the same stage irre-
spective of changes to the distribution caused by route updates.
Depending on the distribution, as many nodes as all the preﬁxes may
fall into the same level (or equivalently, same stage). Unfortunately,
providing worst-case guarantees for any preﬁx distribution implies
that most stages have to be large enough to hold as many nodes as
all the preﬁxes. Thus the static mapping’s obliviousness of the preﬁx
distribution results in large per-stage memory which limits scalabil-
ity in size and throughput.
To solve DLP’s problems, we propose scalable dynamic pipelin-
ing (SDP) which takes preﬁx distribution into consideration. We
map a trie node to its pipeline stage based on the node height (e.g.,
nodes of height 3 are mapped to the 8th stage). Because the node
height is directly determined by the preﬁx distribution, the node
height succinctly provides sufﬁcient information about the distribu-
tion. Node heights change when the preﬁx distribution changes upon
route updates, causing our mapping to be dynamic. In contrast to the
node height, the node level provides no information about the distri-
bution. This dichotomy exists because the level is measured from the
root whose position remains ﬁxed whereas the height is measured
from the leaves whose positions reﬂect the distribution. For instance,
a trie node at height 3 is guaranteed to have at least three preﬁxes in
its subtree as long as the subtree uses path compression to address a
peculiar feature of tries. We exploit this guarantee to prove a signiﬁ-
cantly tighter bound on the worst-case per-stage memory than that
of DLP. The height-to-stage mapping is our ﬁrst innovation which
addresses DLP’s ﬁrst shortcoming of scalability in size.
The above-mentioned peculiar feature of tries distorts the corre-
lation between node height and distribution: In general, an internal
trie node examines a few preﬁx bits (e.g., 4). Depending on the
length of a given preﬁx, many internal nodes are traversed to match
the preﬁx. If a subtree contains only one preﬁx, every node in the
subtree has only one child, and the traversal goes through a string of
such one-child nodes. Such strings artiﬁcially increase the height of
the subtree’s nodes, distorting the correlation. To remove this distor-
tion, we propose a loss-less adaptation of path compression pro-
posed by PATRICIA tries [9]. We collapse each string of one-child
nodes into a jump node, which examines as many bits as the string
length. Thus jump nodes restore the node’s true height.
Upon route updates, the nodes whose heights are affected need to
be migrated to the correct stage based on their new height. It may
seem that our dynamic mapping would incur high route-update cost
due to such migrations. Surprisingly, while our height-to-stage map-
ping causes this problem we exploit the very same mapping to solve
the problem via a novel scheme for incremental route-updates. In
our scheme, a route-update requires exactly and only one write dis-
patched into the pipeline (at every stage at most one memory write is
done). This route-update cost is obviously the optimum for any pipe-
lined scheme, and our scheme achieves the optimum in the worst
case. In addition, our memory management overhead is exactly one
operation. Though Tree Bitmap’s [4] route-update cost including
memory management is O(1), the constant factor is 2largest stride +
100; whereas SDP’s cost is exactly 1. SDP employs leaf-pushing
and therefore, does not incur the size and throughput penalties of
Tree Bitmap. The route-update scheme is our second innovation
which addresses DLP’s second shortcoming of scalability in route-
update cost.
To attack DLP’s third shortcoming, we make the key observation
that each stage of a data-structure pipeline can be hardware-pipe-
lined further (similar to [15]). We hardware-pipeline each SDP stage
into a different number of hardware stages as per the desired access
rate (e.g., SDP stage 2 has three hardware stages, SDP stage 3 has
ﬁve hardware stages, and so on). Once we internally pipeline the
data-structure stages at the hardware level, the throughput can con-
tinue to scale irrespective of the maximum height of the trie (i.e., the
maximum preﬁx length). By combining hardware-level and data-
structure-level pipelining, we avoid [15]’s high implementation cost
and [1]’s lack of throughput scalability. This combining is our third
innovation.
Using hardware simulation, we show that for 1 million preﬁxes
at 160 Gbps line-rate, TCAM requires 6 MB, dissipates 174 W, and
takes up 8.9 cm2 (chip area is a measure of implementation cost);
HLP requires 75 MB, dissipates 146 W, and takes up more than 200
cm2; DLP requires 88 MB, dissipates 10 W, takes up 27 cm2 and
fails to work beyond 40 Gbps; In contrast, SDP requires only 22
MB, dissipates 22 W, and takes up 14.9 cm2. Thus, SDP achieves the
four goals of scalability in size, throughput, power and implementa-
tion cost. SDP’s route-update cost, which is the remaining goal, is
the theoretical minimum of one write.
The rest of the paper is organized as follows: In Section 2 we
provide some background on IP-lookup mechanisms. We describe
the details of HLP, DLP and SDP in Section 3. In Section 4 we
brieﬂy review TCAM-based schemes. We describe our evaluation
methodology in Section 5. In Section 6 we present our results, and
ﬁnally, we conclude in Section 7.
2 Background
Because we design our IP-lookup scheme based on tries, we ﬁrst
present some background details on IP-lookup and trie-based
schemes.
(a)
0*
1*
101*
P1
P2
P3
0
0
0
1
1
1
P3
P2
(b)
P1
P2
0*
1*
101*
P1
P2
P3
00*
01*
10*
11*
1010*
1011*
P1
P1
P2
P2
P3
P3
Fig. 1.  (a) The prefixes in a routing table (b) a trie constructed
from the given prefixes
(a)
(b)
P1
00
01
11
10
00
01
P1
P2
P2
11
10
P3
P2
P3
(c)
The IP-lookup mechanism accepts an IP-address, performs a
search-and-match through a routing table, and upon a match, returns
the appropriate link identiﬁer. The IP-lookup task is complicated by
a number of requirements: (1) To avoid denial-of-service attacks and
instabilities in the network [7], a router must sustain a worst-case IP-
lookup throughput that can handle minimum sized packets stream-
ing in at full line-rate. (2) Given the number of preﬁxes to design for,
the IP-lookup mechanism must provide enough memory to hold all
the preﬁxes regardless of their distribution. (3) Because of wildcard
bits in preﬁxes, a given destination IP-address may match with mul-
tiple preﬁxes. IP routing protocols require that the lookup must
choose the preﬁx with the longest match.
2.1 Trie-Based IP-lookup Schemes
One of the approaches to matching a destination IP-address
against a set of given preﬁxes is to match it one bit at a time, narrow-
ing the ﬁeld of search with each successive bit. A trie is a tree-like
data-structure designed speciﬁcally for such bit-by-bit searching.
For example, given the set of preﬁxes shown in Figure 1(a) we can
construct the trie shown in Figure 1(b). Each leaf contains the long-
est matching preﬁx corresponding to the bits encountered along the
path from the root to that leaf. We perform an IP-lookup by starting
at the root and traversing down the trie. At any internal node of level
k (root being level 0), the kth bit (bit 0 being the most signiﬁcant) in
the destination IP-address determines whether to follow the left
child or the right child. The trie traversal eventually ends at a leaf.
Starting from the shaded node in Figure 1(b), any path that corre-
sponds to a mismatch with preﬁx P3 must be terminated with a leaf
containing preﬁx P2 (i.e., the longest preﬁx that has already been
entirely matched). This method of constructing the trie is called leaf
pushing. Unfortunately, updating a leaf-pushed trie may be compli-
cated (e.g., if P2 is deleted or modiﬁed).
It is possible to construct tries without leaf pushing by placing
preﬁx information inside internal nodes. However, such schemes
almost double the trie node size, resulting in considerably larger
worst-case memory. The bandwidth demand on the memory is also
increased as the lookup process must read both a preﬁx and pointer
at each node. In addition, as we traverse down the trie, we must
explicitly check for and remember the longest matching preﬁx at
each internal node.
2.2 Multiple-bit Stride Tries
When the IP-lookup is not pipelined in any manner, the total
delay for one IP-lookup determines the maximum lookup rate. The
worst-case delay for one lookup is proportional to the trie depth. IP-
lookup rate can be improved by reducing the trie depth which in turn
can be reduced by striding more than one bit at each internal node. If
the stride is 2 bits at each internal node, the worst case depth of the
trie is reduced by a factor of 2 (i.e., 16), and so on.
Fig. 2.  (a) The routing table after controlled prefix expansion (b)
The 2-bit stride trie constructed from the table in (a)
In Figure 2(a), preﬁx P1 has a length of only one bit. If we wish
to stride 2 bits at the root, P1 must be expanded into all the 2-bit
combinations implied by the original preﬁx P1. The process of
expanding preﬁxes in order to align them with stride boundaries is
called controlled preﬁx expansion [17]. Figure 2(b) shows the rout-
ing table of Figure 2(a) after controlled preﬁx expansion for a 2-bit
stride at each node.
Unfortunately, controlled preﬁx expansion causes a non-deter-
ministic increase in the routing-table size due to replication of point-