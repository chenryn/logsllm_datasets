which appear on 72.6% and 59.6% of the apps’ policies, respectively.
Fig. 2 shows the distribution of privacy statements’ purposes.
Using the models developed in Section 5, 701,427 unique data
flows from 16,362 apps were extracted. Each data flow comprises a
single key–value pair in the captured traffic of each app. 432,078
(61.2%) have a non-Other purpose and 282,984 (40.3%) have both
non-Other purpose and data type. The Other class is for data types
or purposes which our classifier was unable to infer such informa-
tion as a key–value of encrypted data. The most common data types
are Device Information and Identifiers which appear in 95.7% and
87.3% of the apps, respectively. Marketing Analytics and Provide Ad
are the most frequent purposes found in 94.1% and 78.7% of apps’
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2833Figure 2: Distribution of purpose classes in the privacy state-
ments and data flows of mobile apps.
create their policies than less popular ones. In particular, we ran-
domly select 36 apps in each of the 3 segments based on the number
of app installs: greater than 1M (3,144 apps), from 10k to 1M (10,482
apps), and less than 10k (3,176 apps). To have diverse document
structures, we exclude similar policies created from templates. They
are detected by a high TF-IDF cosine similarity [45] (greater than
0.95) and then manual verification that they have no significant
differences other than the company/developer names. Documents
that are not a valid privacy policy (e.g., terms of service or home
pages), due to errors in data collection and pre-processing, are also
excluded from the selection process.
Each privacy policy is independently annotated by 2 co-authors:
an advanced PhD student and a researcher at a major global com-
pany, both with more than 3 years of experience in privacy research.
We carefully read the policies and interpret the policy sentences
as fully as possible to identify pairs of contradictory data-practice
statements (detailed steps are described in Appendix K.1). Any dis-
agreements were then resolved during follow-up discussions after
every 10 policies were annotated. The annotation took two annota-
tors 108 hours in total (30 minutes/policy/analyst on average).
There are 189 pairs of contradictory sentences in 47 (43.5%)
policies. Of these policies, 32 (68.1%) contain 1–3, 12 (25.5%) contain
4–9, and 3 (6.4%) contain more than 9 sentence pairs. The dataset
has 5,911 sentences where each policy has an average of 110.8 (96.4
standard deviation) sentences. The selected apps and their statistics
are provided in Table 18 (Appendix K.2).
Figure 3: Distribution of data types in apps’ data flows.
data flows, respectively. These results indicate that apps commonly
collect both identifiable and anonymous information of devices to
deliver relevant advertisements and perform data analytics. The
distributions of the purposes and data types are shown in Figs. 2
and 3, respectively.
There is a mismatch between the distribution of purposes of data
flows and that of privacy statements. Although the most common
data-flow purposes are advertising and marketing analytics that are
present in more than 78.7% of the apps, these purposes are found in
privacy statements of only 56.5% and 33.4% of the apps, respectively.
The significantly lower presence of the purposes in privacy policies
indicates that declarations of data-usage purposes for advertising
and analytics are frequently omitted in apps’ policies.
8.3 End-to-end Detection of Contradictions
Evaluation Metrics. We evaluate PurPliance’s end-to-end de-
tection of contradictory sentence pairs in privacy policies. Testing
the performance at the sentence level assesses the usability of the
system better than at the low-level privacy statement tuples. A
human analyst would need to read whole sentences to understand
the context of a detected contradiction so that s/he can verify and
fix it. Therefore, a low false positive rate will help human analysts
reduce their effort of reviewing many non-contradictory sentences.
Dataset Creation. We create a ground-truth dataset of 108 policies
selected from the privacy policy corpus (Section 8.1). To increase
the diversity of the policies, we select policies of apps with different
levels of popularity as popular apps may have more resources to
Experimental Configurations. To comparatively analyze the ef-
fects of the main components of PurPliance, we introduce the
following configurations. PurPliance-PA is a purpose-agnostic ver-
sion that does not extract purpose clauses and, thus, uses only non-
purpose transformation rules T1, T3, T4 in Table 4. PurPliance-SRL
is PurPliance-PA with PolicyLint’s ontologies and data-practice
verb list. Based on PolicyLint that uses the default parameters in its
open-source repository [6], PolicyLint-PO leverages PurPliance’s
more complete SCoU verb list and data-object/entity ontologies.
Evaluation Results. PurPliance has 95% precision and 50% re-
call, which are significantly higher than 19% precision and 10%
recall of PolicyLint. There are three main sources of PurPliance’s
improvements over PolicyLint. First, the semantic argument anal-
ysis improves the extraction of privacy statement tuples and in-
creases both precision and recall so PurPliance-SRL improves F1
score from 20% to 32% compared to PolicyLint-PO. Second, a more
complete data-practice verb list and data-object/entity ontologies
improve the coverage of sentences so F1 of PurPliance-PA in-
creases from 32% to 50% compared to PurPliance-SRL. The more
complete verb list and ontologies also increase the performance of
PolicyLint-PO from 13% to 20% F1 score compared to PolicyLint.
Third, the analysis of data-usage purposes improves the detection
of contradictions and increases the precision of PurPliance from
60% to 95% while recall is also enhanced from 43% to 50% compared
to PurPliance-PA. The results are listed in Table 10.
The analysis of data-usage purposes improves PurPliance’s F1
from 50% to 65% compared to PurPliance-PA. First, false positives
are reduced because of the inclusion of purposes in interpreting sen-
tences and more accurate interpretation of data-selling practices
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2834Config
PolicyLint
PolicyLint-PO
PurPliance-SRL
PurPliance-PA
PurPliance
Precision
0.19
0.23
0.46
0.60
0.95
Recall
0.10
0.18
0.24
0.43
0.50
F1
0.13
0.20
0.32
0.50
0.65
Table 10: Detection of contradictory sentence pairs.
Config
PolicyLint
PurPliance
Precision
# Statements
# Sentences
0.82
0.91
85
160
47
68
Table 11: Performance of privacy statement extraction.
Figure 4: Distribution of potential purpose contradictions.
(e.g., sell and rent). For example, PurPliance does not flag sen-
tences "we do not sell personal data" and "we may disclose personal
data to comply with the law" because of different sharing purposes
(Marketing vs. Legality), while purpose-agnostic approaches do.
Second, the recall rate is enhanced because purpose-contradiction
sentence pairs, such as "we use your personal data only for provid-
ing services" and "advertisers may collect personal data to deliver
advertising", cannot be detected without purposes analysis.
The low precision of PolicyLint configuration is due mainly to
the fundamental change of the interpretation of privacy statements.
PolicyLint ignores purposes in statements and thus creates many
false positives. For example, PolicyLint’s interpretation of "we do
not share your personal data for marketing" as "we do not share your
personal data" contradicts many other data collection/sharing state-
ments in the policy. Moreover, our metrics are more fine-grained
than those used in PolicyLint [7], signifying the impact of Poli-
cyLint’s incorrect extraction. To characterize contradiction types in
policies, PolicyLint [7] measured the accuracy of detecting contra-
dictions between pairs of sets of sentences where sentences in a set
generate the same privacy statement tuples. However, a sentence
set may include both true-contradictory and false-positive ones.
The recall rate of PurPliance is still limited for three main rea-
sons: complex sentences (29.5%), cross-sentence references (25.3%),
and incompleteness of data-object ontologies (11.6%). In complex
sentences, data-practice statements are often buried among other
unrelated clauses (such as conditions and means of collection). The
complex meaning of multiple clauses makes the separation of data-
collection statements challenging. For example, "in the event of
a corporate merger, your personal data is part of the transferred
assets," implies the transfer of personal data without using any
data-practice verb. In addition, the sentence-level analysis cannot
resolve data types or entities that are defined in other sentences,
such as "this information" in "we do not collect this information."
In-depth Analysis. We compare the performance of extracting
privacy statement tuples, which is an important intermediate step
of PurPliance and PolicyLint. As shown in Table 11, the results
on 300 randomly selected sentences from the privacy policy corpus
demonstrate that PurPliance significantly outperforms PolicyLint
in extracting the privacy statement tuples. PurPliance has a 9%
higher precision (increased from 82% to 91%), extracts 88% more
privacy statements and covers 45% more sentences than PolicyLint.
Note that the precision at this step is lower than the final contra-
dictory detection because of further filtering in the later steps of
contradiction analysis. The detailed experimental procedures are
described in Appendix K.3.
8.4 Analysis of Policy Contradictions and
Flow-to-Policy Inconsistencies
PurPliance detected 29,521 potentially contradictory sentence
pairs in 3,049 (18.14%) privacy policies. Of these sentence pairs,
2,350 (7.97%) are purpose-specific, i.e., purpose-agnostic systems
will miss them. For flow-to-policy inconsistencies, PurPliance
detected 95,083 (13.56%) potentially inconsistent flows between
the actual behavior and privacy policies in 11,399 (69.66%) of the
apps with data flows extracted. Fig. 4 shows the distribution of the
purpose-specific contradiction types.
The most common contradiction types are 𝐶1 and 𝑁1, indicating
the problematic discussion of broad data-object and purpose terms
in purpose-negated statements. For example, many apps state the
collected personal data is not used for third parties’ marketing pur-
poses but also mention other contradicting usage purposes. The
contradictions show that privacy policies frequently contain am-
biguous descriptions of their data-usage purposes. Similarly, the
high number of apps containing detected flow-to-policy inconsis-
tencies indicates a prevalence of inconsistencies in mobile apps.
8.5 Findings
Finding 1. We found an issue with statements about the collec-
tion of personal data for internal purposes only in 28 apps, many of
which have 100k-10M installs. Their policies state that "your Per-
sonal information we collected is used for internal purposes only."
However, it contradicts with "we do not rent or sell your Personal
information to third parties outside without your consent," because
the exception clause "without your consent" indicates the sharing of
personal data with third parties for third parties’ purposes. On the
other hand, the apps transferred a unique id and geographical loca-
tion to a third-party domain with a path client/v2/ads-service/ads.
Therefore, such data flows were inconsistent with the policies.
Finding 2. A common privacy policy template, used in 211 (0.92%)
apps in our corpus, contains contradictory statements. The policy
claims that their "agents and contractors may not use your personal
data for their own marketing purposes." However, the policy states
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2835later that the app employs "3rd party ad serving systems" which
"allow user data to be utilized for advertising communication pur-
poses displayed in the form of banners and other advertisements
on [app name] apps, possibly based on user interests." While ad-
serving systems are one of their contractors, they use the personal
data for their advertising purposes (which is subsumed under mar-
keting purposes), and user data includes user personal data, hence
these statements are contradictory with respect to the purposes of
marketing and advertising.
Finding 3. Apps promise that the sharing is not for marketing
but later say they will. For example, a popular education app with
10M+ installs states "we do not share your personal data with third
parties or corporate affiliates for their direct marketing purposes."
However, the policy also states "we allow our service providers
(including analytics vendors and advertising networks) to collect
information about your online activities through cookies. These
third parties may use this information to display advertisements
on our application and elsewhere online tailored to your interests."
However, displaying targeted advertisements are direct marketing
and online activities (such as browsing history) that can uniquely
identify a person and can thus be considered as personal data [13].
Therefore, the latter statement is contradictory to the first statement
of no direct marketing purpose.
provides evidence of the presence of data-usage purposes which
is useful for our goal of detecting inconsistencies.Determining the
exact usage purposes of data requires knowledge of server-side
processing since usage information is lost once the data is received
by the servers. Therefore, the detection needs to be verified by hu-
mans such as regulators and service lawyers. Since the data purpose
classification has already been discussed at length and evaluated
in MobiPurpose [33], developing more sophisticated and accurate
data-purpose extraction is beyond the scope of PurPliance.
10 CONCLUSION
We have presented a novel analysis of data purposes in privacy
policies and the actual execution of mobile apps. We have developed
PurPliance, a system for automatic detection of contradictions
and inconsistencies in purposes between privacy policies and apps’
data transfer. Our evaluation results have shown PurPliance to
significantly outperform a state-of-the-art method and detect con-
tradictions/inconsistencies in a large number of Android apps.
ACKNOWLEDGEMENTS
The work reported in this paper was supported in part by Sam-
sung Research and the US National Science Foundation under
Grant No. CNS-1646130 and Army Research Office under Grant
No. W911NF-21-1-0057.
9 DISCUSSION
While PurPliance is designed to have low false positives with rea-
sonable coverage, systematic evaluation of its recall rate is challeng-
ing because labeling privacy policies is very complex and expensive.
SRL still remains a challenging task in NLP [28]. State-of-the-art
SRL models [51] achieved only 87% F1 score with 85.5% recall rates.
Furthermore, the SRL model used in PurPliance was trained on a
generic dataset [57] and has not yet been adapted to the privacy-
policy domain. Thus, its performance may be limited. However,
creating a domain-adapted SRL model requires a significant effort
due to the complexity of the semantic arguments [57] and large
model sizes [4]; this is part of our future inquiry.
PurPliance’s extraction of data flows from network traffic has
two limitations. First, it cannot decode certificate-pinned traffic
which, however, constitutes only < 5% of the traffic generated by
top free apps [33]. Second, the input generator used in PurPliance
also cannot exercise login-required apps that use external verifica-
tion information. Using advanced techniques to exercise certificate-
pinned and login apps will improve the coverage of an app’s execu-
tion paths, thus enhancing PurPliance’s recall rate. For example,
recently available TextExerciser [29] can be used to generate inputs
for the analysis of apps requiring a login. Although PurPliance
does not capture the traffic of certificate-pinned and login-required
apps, this limitation does not increase false positives, that we aim
to minimize. Therefore, we leave this as our future work.
Our analysis is based on client-side information only, so it has
limitations in detecting the ultimate purpose of processing on the
servers. Although the analysis assumes meaningful names of app
resources such as package names and URL hosts/paths, they do
not always reveal the true purposes of data flows, so the extraction
cannot determine purposes of certain data flows (i.e., increase false
negatives). However, predicting the purposes of app behavior still
REFERENCES
[1]
42matters AG. 2020. Google Play Categories | 42matters.
Retrieved 06/27/2020 from https://42matters.com/docs/app-
market-data/android/apps/google-play-categories.
[2] AdGuard. 2021. AdGuard ad filters | AdGuard knowledgebase.
Retrieved 03/12/2021 from https://kb.adguard.com/en/
general/adguard-ad-filters.
[3] Charu C. Aggarwal and ChengXiang Zhai. 2012. A survey
of text clustering algorithms. In Mining Text Data. Charu
C. Aggarwal and ChengXiang Zhai, editors. Springer US,
Boston, MA, 77–128. doi: 10.1007/978-1-4614-3223-4_4.
[4] AllenAI. 2020. AllenNLP - Semantic Role Labeling. (2020).
[5] Ben Andow. 2020. HtmlToPlaintext. Retrieved 07/24/2020
from https://github.com/benandow/HtmlToPlaintext.
[6] Ben Andow. 2020. PrivacyPolicyAnalysis. Retrieved 04/10/2021
from https://github.com/benandow/PrivacyPolicyAnalysis.
[7] Benjamin Andow, Samin Yaseer Mahmud, Wenyu Wang,
Justin Whitaker, William Enck, Bradley Reaves, Kapil Singh,