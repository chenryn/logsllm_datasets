optimized our implementation at the instruction level, so it has an
overall slower access time because there are, on average, more can-
didate memory locations for each item5; the difference is greatest
for lookups that find no match, because these result in searching
all candidate memory locations.
In summary, our experimental evaluation shows that it is possi-
ble to have a NAT network function that both offers competitive
performance and is formally verified.
7 DISCUSSION
Developing and verifying VigNAT is a first step toward the broader
goal of verified, high-performance NFs. The Vigor approach and
prototype have several limitations, and also offer opportunities for
future research. In this section, we describe some of these.
Vigor will not produce an incorrect proof, but it may fail to prove
a property that actually holds for a given NF, because of an invalid
model. For example, in §3, if Vigor uses model (b) from Fig. 4, it
cannot prove that the given NF correctly implements the discard
protocol—even though that is the case—because the model is too
abstract. So, if a proof fails, and the reported reason for the failure
does not lead the NF developer to a bug in their code, it may be
that the given NF exercises libVig functionality that is not properly
5This is the case for the particular implementations used, respectively, by the Unverified
and Verified NAT, not for separate chaining and open addressing in general.
0246110203040506064Number of background ﬂows (thousands)Latency of probe ﬂows, (µsec)No-op  Unveriﬁed NAT  Veriﬁed NAT  0.00.20.40.60.81.0µsecLatency CCDFVeriﬁed NATUnveriﬁed NATNo-op4.555.566.53000123110203040506064Number of ﬂows (thousands)Throughput (Mpps)No-op  Unveriﬁed NAT  Veriﬁed NAT  A Formally Verified NAT
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
captured by a symbolic model; in this case, the NF developer can
request from the libVig developers a more detailed model.
The current version of Vigor cannot verify concurrent code.
We expect that the biggest challenge will be the development and
verification of useful concurrent data structures.
We do not have yet experience with applying Vigor to mature,
legacy code. Such software often has state handling code sprinkled
throughout, so refactoring it to put all state in libVig data structures
could be challenging. It would also require annotating loops and
extracting loop invariants, but we expect to be able to automate
these tasks using known techniques [23, 47].
More generally, we hope to reduce the human effort needed to
expand libVig and use Vigor by automating most tasks: (a) Many
lemmas needed for Step 3 are boiler-plate, and we should be able to
generate them automatically. (b) For proving low-level properties,
it may be enough to use simple over-approximate models that leave
outputs unconstrained, and such models can be generated automat-
ically. We can also leverage techniques that learn invariants from
traces [13, 45] to refine symbolic models or produce initial drafts for
libVig contracts. (c) Much of the effort in verifying libVig goes into
writing intermediate lemmas in order to bridge logical leaps that
VeriFast cannot make on its own. Using a proof assistant or a more
powerful theorem prover would reduce this effort [4, 12]. (d) Gen-
erating formal contracts that specify standards (e.g., as described in
an RFC) will always require manual effort; however, standards often
have structure that is amenable to natural language processing, so
we could, perhaps, employ automated techniques [37] to generate
first drafts, which can then be refined by humans.
8 RELATED WORK
As described in §2, our work on VigNAT falls in the area of NF
verification, under the broader umbrella of “data-plane verification.”
The closest work to ours is that of Dobrescu et al. [19], which
verified NFs written in Click [35], including a NAT. They proved the
low-level properties of crash freedom and bounded execution for
these NFs. Their approach relies on exhaustive symbolic execution
of individual Click elements and on-demand composition of the
resulting analyses to reason about Click pipelines. Like Vigor, their
approach puts all state in special data structures, however, it does
not verify the data structures themselves nor that the NF uses
them correctly. It is for this reason that Dobrescu et al.’s work
cannot prove semantic properties—the step forward that enables
such proofs is the “lazy proofs” technique we described here.
Orthogonally to NF verification is what we refer to as network
verification: verify network properties (reachability, loops, etc.) of
a combination of modeled network devices. There is a lot of prior
work in this area [24, 25, 30–32, 38, 39, 46, 52, 55, 59].
Stoenescu et al. [55] focus on network verification, but neverthe-
less rely on more detailed NF models than other work in this area,
and they test their models for faithfulness to the corresponding NF
implementations. Vigor also relies on models (not of entire NFs but
of state-accessing functions), but it does not rely on testing to gain
trust in the models’ faithfulness. Instead, Vigor formally verifies
that the models are guaranteed to be valid for the proof, which
means that replacing a model with the corresponding implementa-
tion preserves the proof, modulo the assumptions in §5.4.
Many before us have applied verification techniques to net-
worked and distributed systems. We share tools and techniques with
this work—symbolic execution, formal contracts, proof checkers—
but the approach of using different verification techniques for dif-
ferent parts of the code, and combining the results through lazy
proofs, is novel. Musuvathi et al. [41] tested the Linux TCP im-
plementation for conformance to a formal specification. Bishop et
al. [3] tested several implementations of TCP/IP and the sockets
API for conformance to a formal specification. Kuzniar et al. [36]
tested OpenFlow switches for interoperability with reference im-
plementations. Hawblitzel et al. [26] verified network applications
written in Dafny—a high-level language with built-in verification
support. Beringer et al. [2] verified an OpenSSL implementation,
proving functional correctness and cryptographic properties.
There exists a body of prior work that has made significant
progress in verifying properties of systems software. Much of
this work can be applied to NF verification as well. For example,
seL4 [34], CompCert [54], and FSCQ [11] show how to prove se-
mantic properties of systems. Unfortunately, they all require the
use of high-level (sometimes esoteric) programming languages and
deep expertise in verification, which we consider a high barrier to
adoption. The motivation behind Vigor is to make the verification
of network functions accessible to most (ideally all) NF developers.
9 CONCLUSION
We presented a NAT box along with a technique and toolchain
for proving that it is semantically correct according to a formal
interpretation of RFC 3022. Our main contribution is exploiting the
specifics of NF structure to propose a new verification technique
that stitches together exhaustive symbolic execution with formal
proof checking based on separation logic. This technique, called
“lazy proofs,” can scalably prove both low-level and semantic prop-
erties of our NF. Experimental results demonstrate the practicality
of our approach: the verified NAT box performs as well as an un-
verified DPDK NAT and outperforms the standard Linux NAT. We
hope our technique will eventually generalize to proving properties
of many other software NFs, thereby amortizing the tedious work
that has gone into building a library of verified NF data structures.
ACKNOWLEDGMENTS
We thank Jonas Fietz, Vova Kuznetsov, Christian Maciocco, Mia
Primorac, Martin Vassor, and Jonas Wagner for insightful technical
discussions, as well as the members of the Network Architecture
Lab and Dependable Systems Lab at EPFL for valuable feedback. We
thank our shepherd, Arjun Guha, and the anonymous reviewers for
their comments and guidance. This work is supported by a Swiss
National Science Foundation Starting Grant and an Intel grant.
REFERENCES
[1] Barnett, M., Chang, B.-Y. E., DeLine, R., Jacobs, B., and Leino, K. R. M. Boogie:
A Modular Reusable Verifier for Object-Oriented Programs. In Formal Methods
for Components and Objects (2005).
[2] Beringer, L., Petcher, A., Katherine, Q. Y., and Appel, A. W. Verified Correct-
ness and Security of OpenSSL HMAC. In USENIX Security Symp. (2015).
[3] Bishop, S., Fairbairn, M., Norrish, M., Sewell, P., Smith, M., and Wans-
brough, K. Rigorous Specification and Conformance Testing Techniques for
Network Protocols, as applied to TCP, UDP, and Sockets. SIGCOMM Computer
Communication Review 35, 4 (2005).
13
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Zaostrovnykh, S. Pirelli, L. Pedrosa, K. Argyraki, and G. Candea
[4] Blanchette, J., Bulwahn, L., and Nipkow, T. Automatic Proof and Disproof in
Isabelle/HOL. Frontiers of Combining Systems (2011).
[5] Boye, M. Netfilter Connection Tracking and NAT Implementation. In Seminar
on Network Protocols in Operating Systems (2013).
[6] Boyer, R. S., Elspas, B., and Levitt, K. N. SELECT – A Formal System for
Testing and Debugging Programs by Symbolic Execution. SIGPLAN Notices 10
(1975).
[7] Bradner, S., and McQuaid, J. Benchmarking Methodology for Network Inter-
connect Devices. RFC 2544, RFC Editor, 1999.
[8] Brocade Vyatta Network OS. http://www.brocade.com/en/products-services/
software-networking/network-functions-virtualization/vyatta-network-os.
html. Accessed: 2017-01-24.
[13] Clarke, E., Grumberg, O., Jha, S., Lu, Y., and Veith, H. Counterexample-Guided
[12] Chlipala, A. Mostly-Automated Verification of Low-Level Programs in Compu-
[9] Cadar, C., Dunbar, D., Engler, D. R., et al. KLEE: Unassisted and Automatic
Generation of High-Coverage Tests for Complex Systems Programs. In Symp. on
Operating Sys. Design and Implem. (2008).
[10] Canini, M., Venzano, D., Perešíni, P., Kostić, D., and Rexford, J. A NICE
Way to Test OpenFlow Applications. In Symp. on Networked Systems Design and
Implem. (2012).
[11] Chen, H., Ziegler, D., Chajed, T., Chlipala, A., Kaashoek, M. F., and Zel-
dovich, N. Using Crash Hoare Logic for Certifying the FSCQ File System. In
Symp. on Operating Systems Principles (2015).
tational Separation Logic. SIGPLAN Notices 46, 6 (2011).
Abstraction Refinement. In Intl. Conf. on Computer Aided Verification (2000).
[14] Clarke, L. A. A Program Testing System. In Annual ACM Conf. (1976).
[15] CVE-2013-1138. Available from CVE Details, CVE-ID CVE-2013-1138., 2013.
[16] CVE-2014-3817. Available from CVE Details, CVE-ID CVE-2014-3817., 2014.
[17] CVE-2015-6271. Available from CVE Details, CVE-ID CVE-2015-6271., 2015.
[18] CVE-2014-9715. Available from CVE Details, CVE-ID CVE-2014-9715., 2014.
[19] Dobrescu, M., and Argyraki, K. Software Dataplane Verification. In Symp. on
Networked Systems Design and Implem. (2014).
[20] Dobrescu, M., Egi, N., Argyraki, K., Chun, B.-G., Fall, K., Iannaccone, G.,
Knies, A., Manesh, M., and Ratnasamy, S. RouteBricks: Exploiting Parallelism
To Scale Software Routers. In Symp. on Operating Systems Principles (2009).
[21] Data Plane Development Kit. http://dpdk.org. Accessed: 2017-06-16.
[22] Emmerich, P., Gallenmüller, S., Raumer, D., Wohlfart, F., and Carle, G.
MoonGen: A Scriptable High-Speed Packet Generator. In Internet Measurement
Conference (2015).
[23] Ernst, M. D., Cockrell, J., Griswold, W. G., and Notkin, D. Dynamically
Discovering Likely Program Invariants to Support Program Evolution.
IEEE
Transactions on Software Engineering 27, 2 (2001).
[24] Fayaz, S. K., Yu, T., Tobioka, Y., Chaki, S., and Sekar, V. BUZZ: Testing Context-
Dependent Policies in Stateful Networks. In Symp. on Networked Systems Design
and Implem. (2016).
[25] Fogel, A., Fung, S., Pedrosa, L., Walraed-Sullivan, M., Govindan, R., Ma-
hajan, R., and Millstein, T. A General Approach to Network Configuration
Analysis. In Symp. on Networked Systems Design and Implem. (2015).
[26] Hawblitzel, C., Howell, J., Kapritsos, M., Lorch, J. R., Parno, B., Roberts,
M. L., Setty, S., and Zill, B. IronFleet: Proving Practical Distributed Systems
Correct. In Symp. on Operating Systems Principles (2015).
IEEE Transactions on Software Engineering 3, 4 (1977).
Department of Computer Science, KU Leuven, 2008.
[27] Howden, W. Symbolic Testing and the DISSECT Symbolic Evaluation System.
[28] Jacobs, B., and Piessens, F. The VeriFast Program Verifier. Tech. Rep. CW-520,
[29] Kadlecsik, J., and Pásztor, G. Netfilter Performance Testing. Tech. rep., Netfilter
Project, Berlin, Germany, 2004.
[30] Kazemian, P., Chan, M., Zeng, H., Varghese, G., McKeown, N., and Whyte, S.
Real Time Network Policy Checking Using Header Space Analysis. In Symp. on
Networked Systems Design and Implem. (2013).
[31] Kazemian, P., Varghese, G., and McKeown, N. Header Space Analysis: Static
Checking For Networks. In Symp. on Networked Systems Design and Implem.
(2012).
[32] Khurshid, A., Zou, X., Zhou, W., Caesar, M., and Godfrey, P. B. VeriFlow:
Verifying Network-Wide Invariants in Real Time. In Symp. on Networked Systems
Design and Implem. (2013).
[35] Kohler, E., Morris, R., Chen, B., Jannotti, J., and Kaashoek, M. F. The Click
[33] King, J. C. Symbolic Execution and Program Testing. J. ACM 19, 7 (1976).
[34] Klein, G., Elphinstone, K., Heiser, G., Andronick, J., Cock, D., Derrin, P.,
Elkaduwe, D., Engelhardt, K., Kolanski, R., Norrish, M., et al. seL4: Formal
Verification of an OS Kernel. In Symp. on Operating Systems Principles (2009).
Modular Router. ACM Transactions on Computer Systems 18, 3 (2000).
[36] Kuzniar, M., Peresini, P., Canini, M., Venzano, D., and Kostic, D. A SOFT
Way for OpenFlow Switch Interoperability Testing. In Proceedings of the 8th
international conference on Emerging networking experiments and technologies
(2012).
[37] Lee, B.-S., and Bryant, B. R. Automated Conversion from Requirements Docu-
mentation to an Object-oriented Formal Specification Language. In Symposium
on Applied Computing (2002).
[38] Lopes, N. P., Bjørner, N., Godefroid, P., Jayaraman, K., and Varghese, G.
Checking Beliefs in Dynamic Networks. In Symp. on Networked Systems Design
and Implem. (2015).
[39] Mai, H., Khurshid, A., Agarwal, R., Caesar, M., Godfrey, P. B., and King, S. T.
Debugging the Data Plane with Anteater. In ACM SIGCOMM Conf. (2011).
[44] O’Hearn, P. W., Reynolds, J. C., and Yang, H. Local Reasoning about Programs
[40] MS13-064. Available from CVE Details, CVE-ID MS13-064., 2013.
[41] Musuvathi, M., Engler, D. R., et al. Model Checking Large Network Protocol
Implementations. In Symp. on Networked Systems Design and Implem. (2004),
vol. 4.
[42] Nagarakatte, S., Zhao, J., Martin, M. M., and Zdancewic, S. SoftBound:
Highly Compatible and Complete Spatial Memory Safety for C. SIGPLAN Notices
44, 6 (2009).
[43] Nagarakatte, S., Zhao, J., Martin, M. M., and Zdancewic, S. CETS: Compiler
Enforced Temporal Safety for C. SIGPLAN Notices 45, 8 (2010).
that Alter Data Structures. In CSL (2001).
[45] Padhi, S., Sharma, R., and Millstein, T. Data-Driven Precondition Inference
In Intl. Conf. on Programming Language Design and
with Learned Features.
Implem. (2016).
[46] Panda, A., Lahav, O., Argyraki, K., Sagiv, M., and Shenker, S. Verifying
In Symp. on Networked
Reachability in Networks with Mutable Datapaths.
Systems Design and Implem. (2017).
[47] Perkins, J. H., and Ernst, M. D. Efficient Incremental Algorithms for Dynamic
Detection of Likely Invariants. ACM SIGSOFT Software Engineering Notes 29, 6
(2004).
[51] Reynolds, J. C. Separation Logic: A Logic for Shared Mutable Data Structures.
[48] Postel, J. Discard Protocol. RFC 863, RFC Editor, 1983.
[49] Primorac, M., Argyraki, K., and Bugnion, E. How to Measure the Killer
Microsecond. In ACM SIGCOMM Workshop on Kernel-Bypass Networks (2017).
[50] Ramamoorthy, C., Ho, S.-B., and Chen, W. On the Automated Generation of
Program Test Data. IEEE Transactions on Software Engineering 2, 4 (1976).
In IEEE-LCS (2002).
[52] Ryzhyk, L., Bjørner, N., Canini, M., Jeannin, J.-B., Schlesinger, C., Terry,
D. B., and Varghese, G. Correct by Construction Networks Using Stepwise
Refinement. In Symp. on Networked Systems Design and Implem. (2017).
(Traditional NAT). RFC 3022, RFC Editor, 2001.
[53] Srisuresh, P., and Egevang, K. Traditional IP Network Address Translator
[54] Stewart, G., Beringer, L., Cuellar, S., and Appel, A. W. Compositional
[55] Stoenescu, R., Popovici, M., Negreanu, L., and Raiciu, C. SymNet: scalable
CompCert. SIGPLAN Notices 50, 1 (2015).
symbolic execution for modern networks. In ACM SIGCOMM Conf. (2016).
[56] Tange, O. GNU Parallel - The Command-Line Power Tool. ;login: The USENIX
Magazine 36, 1 (2011).
[57] Clang 5 documentation - UndefinedBehaviorSanitizer - Available checks. https:
//clang.llvm.org/docs/UndefinedBehaviorSanitizer.html#available-checks. Ac-
cessed: 2017-01-24.
[58] VigNAT Project Repository. https://vignat.github.io, 2017.
[59] Xie, G. G., Zhan, J., Maltz, D. A., Zhang, H., Greenberg, A., Hjalmtysson, G.,
and Rexford, J. On Static Reachability Analysis of IP Networks. In Intl. Conf.
on Computer Communications (INFOCOM) (2005).
[60] Zaostrovnykh, A., Argyraki, K., and Candea, G. Nework software verification
survey. https://vignat.github.io/survey, Feb. 2016.
14