rules, one for each mail server. Finally, eXpose found the port used
by most of the exchange servers and our admin was particularly in-
terested in the exceptions.
∗.∗ : M ail1.135 ⇒ ∗.∗ : M ail1.49155
Most mail servers were running out of a default con(cid:12)guration (cid:12)le
that created exchange server processes on port ʇ@ʄʈʈ. ˆe few ex-
ceptions were probably due to legacy servers that had out-of-date
con(cid:12)gurations. Ability to detect such (cid:12)ne-grained con(cid:12)guration
helps to understand and debug problems.
File-Servers: For Windows SMB (think NFS for Windows), eXpose
discovers that
SM BServer.445 : ∗.∗ ⇒ SM BServer.139 : ∗. ∗ .
ˆe rule indicates that clients ask which server has the (cid:12)le by query-
ing the server (at port ʄʆ@) which in turn responds with a (cid:12)le-handle
that the clients use to fetch the (cid:12)le (at port ʇʇʈ). Of course, the (cid:12)le-
handle may point to another server.
OtherSM BServer.445 : ∗.∗ ⇒ SM BServer.139 : ∗.∗
Such SMB redirection is common practice so that logical names
share a hierarchy, while the (cid:12)les are distributed across servers. eX-
pose uncovered this tangle of re-directions.
Video Lecture Broadcasts: eXpose discovered multiple cliques cor-
responding to video lecture broadcasts.
V ideo.rtsp : Client1.∗ ⇔ V ideo.rtsp : Client2. ∗
V ideo.rtsp : Client1.∗ ⇔ V ideo.rtsp : Client3. ∗
V ideo.rtsp : Client2.∗ ⇔ V ideo.rtsp : Client3. ∗
. . .
and so on.
It turns out that the Video server live-streamed talks and other
events within the enterprise over Real Time Streaming Proto-
col (rtsp). Each clique corresponded to the audience of a particu-
lar talk. Rules linking a pair of clients who were tuned in for a long
overlapping period had higher scores than those involving clients
who tuned out of the broadcast quickly.
Mailing List Servers: eXpose discovered multiple instances of
cliques involving email servers.
ListServ.∗ : Client1.∗ ⇔ ListServ.∗ : Client2. ∗
ListServ.∗ : Client1.∗ ⇔ ListServ.∗ : Client3. ∗
ListServ.∗ : Client2.∗ ⇔ ListServ.∗ : Client3. ∗
. . .
and so on.
It turns out that each of these mail servers was responsible for a
particular mailing list. Whenever a mail would be sent to the list,
the mail server forwards the mail onto all the participants of that list.
eXpose discovered when each list was active and the participants for
each list.
DHCP: Clients that were searching for a DHCP server by broad-
casting on the network caused the pattern,
N etworkBroadcast.137 : ∗.∗ ⇒ DHCP Server.137 : ∗. ∗ .
Printers: eXpose found a clique involving print spoolers:
IP1.161 : P rintServ.∗ ⇔ IP2.161 : P rintServ. ∗
IP1.161 : P rintServ.∗ ⇔ IP3.161 : P rintServ. ∗
IP2.161 : P rintServ.∗ ⇔ IP3.161 : P rintServ. ∗
. . .
and so on.
It turns out that each of the IP’s corresponded to the network in-
terfaces of the printers throughout the Enterprise. ˆe print-server
appears to periodically poll the SNMP (ʄʉʄ) port of these printers
for usage and other information.
Workload Clusters: eXpose found cliques for (cid:12)le-servers.
P ool1.∗ : F ileServ.445 ⇔ P ool2.∗ : F ileServ.445
P ool1.∗ : F ileServ.445 ⇔ P ool3.∗ : F ileServ.445
P ool2.∗ : F ileServ.445 ⇔ P ool3.∗ : F ileServ.445
. . .
and so on.
File-servers showing up in the above rules are centralized data
stores for various groups. ˆey were accessed by pools of clients
who were crunching the data in parallel.
Presence Server: eXpose found many such rules,
P resence.5601 : Client1.∗ ⇔ P resence.5601 : Client2.∗
It turns out that a presence server is part of Windows O(cid:14)ce Com-
municator. Whenever a client logs in, logs out or goes idle, his ma-
chine sends an update to the presence server which forwards the
information onto the user’s friends. So, each rule above links users
who are in each others friend list.
Oddities: eXpose found that a particular user Bob’s machine does:
Bob.∗ : Home1.∗ ⇔ Bob.∗ : Home2
Bob.∗ : Home1.∗ ⇔ Bob.∗ : Home3
Bob.∗ : Home2.∗ ⇔ Bob.∗ : Home3
. . .
and so on.
It turns out that Bob’s machine was talking to many IPs belonging to
DSL users and cable-modem pools (Verizon DSL Customers, Road
Runner Customers). eXpose found this pattern because the accesses
to these home machines were periodic and synchronized. Either
Bob is running an experiment that probes many home machines, or
he is part of a zombie bot-net and the communication here is keep-
alive messages between bots in the botnet.
4.7 Case: Patterns in the LabEnterprise
We have deployed eXpose within the CSAIL lab at MIT such that
eXpose sees tra(cid:14)c to and from the major internal servers, includ-
ing web, email, Kerberos, AFS, NFS servers, managed user work-
stations and a Debian/Fedora mirror (see Fig. ʄc). Here we report
rules learnt by eXpose on a ʇʃʃ minute trace.
Fig. @ depicts rules learnt by eXpose. Graph nodes correspond to
a (cid:8)ow or a generic in the trace, an edge between nodes indicates that
eXpose discovered a rule linking the two nodes. ˆe graph on the
le(cid:13) plots the output of eXpose’s rule-mining algorithm–i.e., all the
statistically signi(cid:12)cant rules, whereas the graph on the right plots the
output of our recursive spectral partitioning algorithm. We found
in practice that eliminating weak rules which strangle otherwise dis-
connected node groups reduces false positives.
syslogd clique: All the managed workstations within the lab and
most of the internal servers ran the same version of Debian with
similar con(cid:12)guration. In particular, these machines export their sys-
tem log (cid:12)les to a server via port ʈʄʇ. Cron jobs in the con(cid:12)g ran at
speci(cid:12)c times causing synchronized updates to the individual sys-
tem logs and synchronized updates to the syslogd server.
IP1.514 : Syslogd.514 ⇔ IP2.514 : Syslogd.514
IP1.514 : Syslogd.514 ⇔ IP3.514 : Syslogd.514
IP2.514 : Syslogd.514 ⇔ IP3.514 : Syslogd.514
. . .
and so on.
As eXpose could list all the machines uploading logs, the admin
could chase down machines that were supposed to be managed but
were not uploading syslog (cid:12)les synchronously.
AFS accesses: ˆe following pattern repeated across many clients:
Client.7001 : AF SRoot.7003 ⇒ Client.7001 : ∗
Client.7001 : AF S1.7000 ⇒ Client.7001 : AF S2.7000
Client.7001 : AF S1.7000 ⇒ AF S1.7000 : AF SRoot.7002
ˆese rules show that a client talks to the root server (at port @ʃʃʆ)
to (cid:12)nd which of the many volume servers have the (cid:12)les he needs and
then talks to the appropriate volume server. ˆe lab has a single root
server but many tens of volume servers with (cid:12)les distributed across
the volume servers. A user’s content is o(cid:13)en spread across more than
one servers, causing simultaneous accesses from his cache man-
ager (at port @ʃʃʄ) to the AFS servers (at port @ʃʃʃ). Finally, creat-
ing new (cid:12)les, or browsing into di(cid:11)erent parts of the AFS tree initiate
connections to a permissions server (at port @ʃʃʅ).
Besides identifying the underlying structure of AFS tra(cid:14)c, eX-
pose’s rules let us understand where each user’s content was located
and how the accesses were spread across the various servers. ˆe
lab admins were happy to see that the accesses to volume servers
(a) Rules Discovered By Mining
(b) A(cid:13)er Spectral Partitioning
Figure @: E(cid:14)cacy of Rule Pruning. ˆe graph of le(cid:13) represents rules dis-
covered by mining. Each edge corresponds to a discovered rule and joins
nodes corresponding to the  pair involved in the rule. ˆe
graph on the right depicts the same rule-set a(cid:13)er recursive spectral parti-
tioning. Eliminating low-scored rules between node-groups that are oth-
erwise strongly connected breaks the rule-set into understandable pieces.
matched up with the hard-disk sizes on the servers (larger servers
got more accesses).
File-Server as the Backend: ˆe fact that many users have all their
data on AFS leads to many neat patterns:
Remote1 : W ebServ.80 ⇒ W ebServ.7000 : AF S.7001
Client : LoginServ.22 ⇒ LoginServ.7000 : AF S.7001
Compute1 : AF S.7001 ⇔ Compute2 : AF S.7001
Compute1 : AF S.7001 ⇔ Compute3 : AF S.7001
Compute2 : AF S.7001 ⇔ Compute3 : AF S.7001
ˆe (cid:12)rst shows that accesses to home pages on the lab’s web-server
cause the web-server to fetch content from the AFS servers. While
the home-pages are stored on the web-server, it is likely that users
link to content (papers, cgi-scripts) present elsewhere in their AFS
share. Also, ssh connections into the lab’s login server caused the lo-
gin server to mount directories from AFS. Finally, compute clusters
synchronously accessed many volume servers, most likely because
a data-intensive job was parallelized across the cluster.
E-Mail Flow: eXpose discovered how email (cid:8)ows through the lab.
Incoming.25 : ∗.∗ ⇒ W ebmail.2003 : Incoming.X
∗.∗ : W ebmail.143 ⇒ ∗.∗ : W ebmail.993
∗.∗ : W ebmail.110 ⇒ ∗.∗ : W ebmail.995
ˆe (cid:12)rst rule shows that whenever a connection is made on port
ʅʈ (SMTP) to a mail server in the lab, the mail server connects with
di(cid:11)erent server. It turns out, the lab has one Incoming SMTP server
that receives mail from outside but does not store the mail. Instead,
mail is forwarded on to a webmail server via LMTP (enterprise ver-
sion of SMTP, port ʅʃʃʆ). ˆe webmail server in turn provides an
interface for users to read their mail. ˆe next two rules show the
webmail server responding to both IMAP (port ʄʇʆ) and POP (port
ʄʄʃ) connections only to force clients to use the corresponding se-
cure versions IMAPS (port @@ʆ) and POPS (port @@ʈ).
Outgoing E-mail: eXpose tracks patterns in outgoing mail.
OutM ail.∗ : dns0.mtu.ru.53 ⇔ OutM ail.∗ : dns1.mtu.ru.53 (many)
OutM ail.∗ : M ail1.25 ⇔ OutM ail.∗ : M ail2.25 (clique)
ˆe (cid:12)rst rule above shows that whenever the outgoing server does
a DNS (port ʈʆ) lookup at one remote name server, it does a name
lookup on another redundant name server in the same domain. It
turns out that the lab’s mail server implementation simultaneously
looks up multiple DNS servers in the hope that at least one of them
would respond. Further, the second rule shows the outgoing mail
server simultaneously connecting to multiple mail servers in do-
mains like messagelabs.com. Apparently, messagelabs (and many
others) are out-sourced email providers who receive email for com-
panies(e.g., Akamai). Several lab mailing lists have employees of
these companies, and hence a mail to such lists makes the outgo-
ing server deliver mail simultaneously to multiple SMTP servers at
the outsourced email provider.
Nagios Monitor: ˆe Lab admins use a Nagios machine to monitor
the health of the lab’s key servers.
N agios.7001 : AF S1.7000 ⇒ N agios.7001 : AF S2.7000 (AFS clique)
N agios.∗ : M ail1.25 ⇒ N agios.∗ : M ail2.25 (mail clique)
N agios.∗ : AD.139 ⇒ N agios.∗ : AD.389 (active directory)
ˆese rules show that the Nagios machine periodically connects to
the servers of each type as a client and probes their health. ˆe (cid:12)rst
rule is part of a full clique, the Nagios machine connected as an AFS
client with every one of the AFS servers in the lab. Similarly, the
second rule is part of a clique wherein the Nagios box sent mail out
through each of the SMTP servers in the lab. ˆe third rule shows
Nagios checking the windows server for both its name (netbios, port
ʄʆ@) and LDAP services (directory lookup, port ʆ@@).
Discovered Bugs: eXpose discovered many instances of con(cid:12)gura-
tion problems and malicious users that the administrators were able
to act upon. Here are examples of two such rules.
(ʄ) IDENT Dependency: eXpose found many instances when a
client’s connection to a server is followed by the server initiating a
connection at port ʄʄʆ on the client. For example,
Client.∗ : M ailServer.25 ⇔ Client.113 : M ailServer.∗
It turns out that port ʄʄʆ is IDENT tra(cid:14)c.
Despite the fact that IDENT was never very useful, even
today …UNIX servers, most commonly IRC Chat, but
some eMail servers as well, still have this IDENT pro-
tocol built into them. Any time someone attempts to
establish a connection with them, that connection at-
tempt is completely put on hold while the remote server
attempts to use IDENT to connect back to the user’s
port ʄʄʆ for identi(cid:12)cation [@].
Shown this rule, an admin changed con(cid:12)guration to disable IDENT.
(ʅ) Legacy Con(cid:12)guration: A legacy DHCP server was active and
responding to requests on a subnet. Even more, the legacy server
was accessing the lab’s authoritative name server that was supposed
to be used only by the front-end DNS servers, which pull the mas-
ter name table and respond to clients. Shown this rule, an admin
disabled the legacy server.
N etworkBroadcast.68 : Legacy.67 ⇒ Legacy.∗ : M asterN S.53.
False Positives: We were able to corroborate ʇʈ out of the ʆʇʉ
rule groups that eXpose discovered on the labEnterprise trace,
for a false positive rate of ʄʆʂ. Some of the false positives were
due to dependencies that appeared true but we could not explain
such as pairs of (cid:8)ows that always happened together in ʈʂ of time
windows. Others were due to high volume servers, such as a debian
mirror hosted within the lab. ˆere are so many connections to
the debian mirror server from so many di(cid:11)erent IPs that invariably
we (cid:12)nd many connections overlapping with each other. We do
know that many operating systems have so(cid:13)ware that automatically
touches the distribution mirrors at (cid:12)xed times of day to check for
updates, yet it is hard to say for sure why a pair of IPs access the
debian mirror synchronously. A natural improvement to eXpose
is to automatically scale the score threshold per server, i.e., high
volume servers that are at a risk of false positives are reported only
if they are in rules that have much higher scores.
False Negatives: Backend dependencies at the web-server hosting
personal web-pages were too (cid:12)ne-grained for eXpose. We discov-
ered the bulk dependency, i.e., that most web-page requests cause
the web-server to fetch content from AFS. But, we were unable to
isolate the more complex dependencies of individual web-pages
carrying dynamic content. We believe that this is because of too
few connections to such dynamic content and readily admit that
eXpose is likely to miss speci(cid:12)c dependencies while looking for the
broad ones. But, one can white-list servers that eXpose should pay
closer attention to by adding that server’s (cid:8)ows to the list of (cid:8)ows
that eXpose builds rules for.
4.8 Case: Patterns on the Lab’s Access Link
mySQL worm: We found an instance of the mySQL worm; the host
pD@E@Dʉʇʄ.dip.t-dialin.net performed a port scan throughout the
network on port ʆʆʃʉ which is the default mySQL port on Unix.
ˆe mySQL Bot scans this port looking for mySQL
servers with weak passwords and if it is successful in
logging in as root . . . uses an exploit to install the bot
on the system. [ʄʆ].
Unlike other worms such as the SQL Sapphire Worm [ʅʃ], this
worm causes little tra(cid:14)c and may not show up in tools that de-
tect heavy-hitters. Yet, eXpose detects it because the remote host
scanned many lab hosts simultaneously.
Cloudmark: ˆe mail server of a group in the lab was involved in
an interesting pattern. Whenever the server received mail (at port
ʅʈ), the server would connect to one of a bunch of servers owned by
cloudmark.com at port ʅ@ʃʆ:
M ailServer.25 : ∗.∗ ⇒ M ailServer.∗ : CloudM ark.2703.
Apparently, cloudmark is a (cid:12)ltering service for spam, phishing and
virus-bearing mails to which this group subscribes.
NTP synchs: ˆe lab runs a major Network Time Protocol (NTP)
server. To keep system clocks up-to-date, clients periodically probe
an NTP server, and adjust clocks based on the server’s response