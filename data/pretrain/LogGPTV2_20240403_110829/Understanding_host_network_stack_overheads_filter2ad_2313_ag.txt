at both the sender and the receiver. We run a single long flow, and
mix it with a variable number of short flows. We set the RPC size
of short flows to 4KB.
DCA does not help much when workloads comprise of
extremely short flows. Fig. 10(a) shows that, as expected,
throughput-per-core increases with increase in flow sizes. We make
several observations. First, as shown in Fig. 10(b), data copy is no
longer the prominent consumer of CPU cycles for extremely small
flows (e.g., 4KB)—TCP/IP processing overhead is higher due to low
GRO effectiveness (small flow sizes make it hard to batch skbs),
and scheduling overhead is higher due to ping-pong nature of the
workload causing applications to repeatedly block while waiting
for data. Second, data copy not being the dominant consumer of
CPU cycles for extremely short flows results in DCA not contribut-
ing to the overall performance as much as it did in the long-flow
case: as shown in Fig. 10(c), while NIC-local NUMA nodes achieve
significantly lower cache miss rates when compared to NIC-remote
NUMA nodes, the difference in throughput-per-core is only mar-
ginal. Third, while DCA benefits reduce for extremely short flows,
other cache locality benefits of aRFS still apply: for example, skb
accesses during packet processing benefit from cache hits. However,
these benefits are independent of the NUMA node on which the
applications runs. The above three observations suggest interesting
opportunities for orchestrating host resources between long and
short flows: while executing on NIC-local NUMA nodes helps long
flows significantly, short flows can be scheduled on NIC-remote
NUMA nodes without any significant impact on performance; in
addition, carefully scheduling the core across short flows sharing
the core can lead to further improvements in throughput-per-core.
(cid:10)(cid:11)
(cid:9)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
l
s
e
c
y
C
U
P
C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:14)
(cid:16)
(cid:15) (cid:14)
(cid:1)
(cid:2)
(cid:8)
(cid:5)
(cid:12)(cid:13)
(cid:2)(cid:7)
(a) Throughput-per-core (Gbps)
0 short ow
1 short ow
4 short ows
16 short ows
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c
n l o
d
e
h
s c
g
u li n
e t c .
(b) Server CPU breakdown
Figure 11: Linux network stack performance for workloads that
mix long and short flows on a single core. (a) Each column shows
throughput-per-core achieved for different number of short flows colocated
with a long flow. Throughput-per-core decreases with increasing number of
short flows. (b) Even with 16 flows colocated with a long flows, data copy
overheads dominate, but TCP/IP processing and scheduling overheads start
to consume significant CPU cycles. The server-side CPU was completely
utilized for all scenarios.; refer to [7] for client-side CPU breakdown. See
§3.7 for description.
We note that all the observations above become relatively obso-
lete even with slight increase in flow sizes—with just 16KB RPCs,
data copy becomes the dominant factor and with 64KB RPCs, the
CPU breakdown becomes very similar to the case of long flows.
Mixing long and short flows considered harmful. Fig. 11(a)
shows that, as expected, the overall throughput-per-core drops by
∼43% as the number of short flows colocated with the long flow is
increased from 0 to 16. More importantly, while throughput-per-
core for a single long flow and 16 short flows is ∼42Gbps (§3.1) and
∼6.15Gbps in isolation (no mixing), it drops to ∼20Gbps and ∼2.6
Gbps, respectively when the two are mixed (48% and 42% reduction
for long and short flows). This suggests that CPU-efficient network
stacks should avoid mixing long and short flows on the same core.
74
(cid:16)
(cid:15)
(cid:7)
(cid:14)
(cid:13)
(cid:12)
(cid:10)
(cid:3)
(cid:4)
(cid:11)
(cid:3)
(cid:10)
(cid:9)
(cid:8)
(cid:5)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:1)(cid:2) (cid:3)(cid:4)(cid:5)(cid:6)
(cid:7)(cid:8)(cid:3)(cid:9)(cid:10)(cid:11)(cid:3)
(cid:12)(cid:13)(cid:14)(cid:15)(cid:2)
(cid:16)(cid:11)(cid:17)(cid:8)
(cid:7)(cid:2)(cid:5)(cid:16)(cid:18) (cid:7)(cid:19)(cid:4)(cid:5)
(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)
(cid:8)(cid:15)(cid:16) (cid:8)(cid:17)(cid:18)(cid:11)(cid:19)(cid:13)(cid:9)(cid:20) (cid:21)(cid:22)(cid:23)(cid:23)(cid:24) (cid:25)(cid:26)(cid:11)(cid:19)(cid:13)(cid:9)(cid:20)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:16)
(cid:15)
(cid:7)
(cid:14)
(cid:13)
(cid:12)
(cid:8)
(cid:5)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:18)
(cid:17)
(cid:8)
(cid:4)
(cid:1)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
(cid:17)(cid:20)(cid:11) (cid:21)
(cid:22)(cid:23)(cid:24)(cid:24)(cid:25)
(cid:12)(cid:19) (cid:17) (cid:18) (cid:14)
(cid:16) (cid:13)
(cid:16) (cid:20) (cid:9)(cid:13)(cid:14)
(cid:2)
(cid:10)
(cid:13)(cid:11)
(cid:15)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
(cid:17)(cid:20)(cid:11) (cid:21)
(cid:22)(cid:23)(cid:24)(cid:24)(cid:25)
(cid:12)(cid:19) (cid:17) (cid:18) (cid:14)
(cid:16) (cid:13)
(cid:16) (cid:20) (cid:9)(cid:13)(cid:14)
(cid:2)
(cid:10)
(cid:13)(cid:11)
(cid:15)
(a) Throughput-per-core (Gbps)
(b) Sender CPU breakdown
(c) Receiver CPU breakdown
Figure 12: Impact of DCA and IOMMU on Linux network stack performance. (a) Each column shows throughput-per-core achieved for different
DCA and IOMMU configurations: Default has DCA enabled and IOMMU disabled. Either of disabling DCA or enabling IOMMU leads to decrease in
throughput-per-core. (b, c) Disabling DCA does not cause a significant shift in CPU breakdown. Enabling IOMMU causes a significant increase in memory
management overheads at both the sender and the recever. See §3.8 and §3.9 for description.
(cid:16)
(cid:15)
(cid:7)
(cid:14)
(cid:13)
(cid:12)
(cid:10)
(cid:3)
(cid:4)
(cid:11)
(cid:3)
(cid:10)
(cid:9)
(cid:8)
(cid:5)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:1)(cid:2) (cid:3)(cid:4)(cid:5)(cid:6)
(cid:7)(cid:8)(cid:3)(cid:9)(cid:10)(cid:11)(cid:3)
(cid:12)(cid:13)(cid:14)(cid:15)(cid:2)
(cid:16)(cid:11)(cid:17)(cid:8)
(cid:7)(cid:2)(cid:5)(cid:16)(cid:18) (cid:7)(cid:19)(cid:4)(cid:5)
(cid:8)(cid:9)(cid:10)(cid:11)(cid:8)
(cid:10)(cid:10)(cid:12)
(cid:13)(cid:8)(cid:14)(cid:8)(cid:15)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:16)
(cid:15)
(cid:7)
(cid:14)
(cid:13)
(cid:12)
(cid:8)
(cid:5)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:18)
(cid:17)
(cid:8)
(cid:4)
(cid:1)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:14)
(cid:16)
(cid:15)
(cid:4)
(cid:13)
(cid:10)
(cid:12)
(cid:11)
(cid:10)
(cid:9)
(cid:7)
(cid:8)
(cid:7)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:1)
(cid:6) (cid:7)
(cid:1) (cid:2)(cid:3)(cid:2) (cid:4) (cid:5)
(cid:3)(cid:4) (cid:6)(cid:8)(cid:9)(cid:6)
(cid:18)
(cid:6) (cid:10) (cid:5) (cid:4) (cid:11) (cid:12)(cid:12)(cid:9)(cid:13) (cid:14)
(cid:13) (cid:11) (cid:3) (cid:1) (cid:11) (cid:15)(cid:9)(cid:4) (cid:11) (cid:12) (cid:16) (cid:17) (cid:12) (cid:7) (cid:12)(cid:3)(cid:11)