# Quantifying Overheads and Interference in RLI

## Figure 10: Overheads and Interference Analysis

### (a) Bandwidth Overhead
- **CHIC and SANJ Traces:**
  - Utilization: 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9
  - CHIC-PI, CHIC-NI, SANJ-PI, SANJ-NI

### (b) Interference with Regular Traffic
- **Per-flow Delay Interference (seconds):**
  - 10^-5, 10^-4, 10^-3, 10^-2, 10^-1
  - Cumulative fraction of flows experiencing additional delay at high utilization.

### (c) Impact on Packet Losses
- **Packet Loss Rate:**
  - 0.001% increase at 80% utilization.
  - SANJ trace experienced slightly more losses due to bursty arrivals.

## Analysis of Interference and Overheads

In Figure 10(b), we show the cumulative fraction of flows that experienced a particular amount of additional delay under high utilization. As expected, many flows experienced positive additional delay (curves x-PI, where PI means positive interference). However, approximately 10% of flows experienced a decrease in average delay (curves x-NI, where NI is negative interference).

From the distribution of both types of flows (PI and NI), we observe significant variations in delay. Some flows experienced an increase in delay by 20-30 ms, while others saw a similar reduction. Further investigation revealed that the number of dropped packets was significantly different. Flows with reduced packet delay had more packets dropped in the presence of reference packet traffic, whereas flows with increased delay had fewer dropped packets but with a higher delay. This suggests that the queue's proximity to being full is a critical factor. Reference packets cause small perturbations, converting some dropped packets into high-delay packets and vice versa, resulting in minimal overall interference.

In terms of overall loss, our architecture introduces a very small increase in the loss rate, as shown in Figure 10(c). The packet loss rate differs by at most 0.001% even at almost 80% utilization for either trace. The SANJ trace experienced slightly more losses due to its more bursty arrival pattern.

## Implementation

Our architecture can be implemented in routers as follows:
- **Packet Generator Component:**
  - Can be implemented in software, but requires precise timestamps, which are better handled by hardware.
  - Adaptive reference packet generation maintains a small state using utilization counters within line-card ASICs.
  - Reference packets do not need IP headers but require interface identifiers for transmission between interfaces.
  - No extra time synchronization is needed since the interfaces operate in the same time domain.

- **Receiver Side:**
  - Requires three hardware counters per flow for flows of interest.
  - High-speed counter updates can be managed using SRAM, with periodic flushing to cheaper DRAM.
  - Per-flow measurements can be reported for a subset of flows through sampling or other mechanisms like ProgME [38].

- **Alternative Solutions:**
  - Mirroring all packets from the receiver to a PC or network appliance for processing.
  - FPGA-based solutions (e.g., NetFPGA [5] handling 4 Gbps) or high-speed network processor boards (e.g., Intel IXP 2800) can be used for high-speed counter updates.
  - These solutions provide an easy path to deployment without waiting for router vendors to adopt the architecture.

## Related Work

While passive measurement solutions for routers are well-established, fine-grained latency estimation is a relatively new area, driven by applications like data centers and algorithmic trading. Existing literature includes tomography techniques for inferring hop and link characteristics, but these provide aggregate measurements, not per-flow.

In the context of flow measurement, various solutions have been proposed, such as trajectory sampling [14], MultiFlow estimator [25], and LDA [23, 24]. Our approach should work seamlessly with many sampling frameworks, and we compare our results with other solutions, particularly in the context of random packet sampling used by sampled NetFlow.

## Conclusion

Applications like algorithmic trading and data center operations demand low end-to-end latency. While special-purpose measurement devices can help, their high cost is a barrier to widespread deployment. LDA provides a scalable and low-cost alternative for delay measurement but only for aggregate traffic, making it insufficient for isolating problems affecting specific flows.

We propose RLI, a scalable architecture for obtaining per-flow latency measurements across router interfaces. RLI injects periodic reference packets with timestamps, allowing the receiver to use them as reference latency samples. Simulations show that RLI achieves a median relative error of 10-12%, one to two orders of magnitude lower than previous solutions. RLI is simple to implement, cost-effective, and practical for ubiquitous deployment, offering a compelling alternative to high-end monitoring boxes.

## Acknowledgments

We thank the anonymous reviewers, Sue Moon, Joel Sommers, and the support from NSF Award CNS 0831647 and Cisco Systems.

## References

[References listed here as in the original text]

---

This revised version organizes the content more clearly, enhances readability, and ensures a professional tone.