c
a
r
f
e
v
i
t
a
u
m
u
C
l
10-1
10-2
10-3
10-5
CHIC
SANJ
 0.2
 0.3
 0.4
 0.5
 0.6
Utilization
 0.7
 0.8
 0.9
10-4
10-7
8x10-5
6x10-5
4x10-5
2x10-5
e
c
n
e
r
e
f
f
i
d
e
t
a
r
s
s
o
L
0x100
10-1
CHIC
SANJ
 0.2
 0.3
 0.4
 0.5
 0.6
Utilization
 0.7
 0.8
 0.9
CHIC-PI
CHIC-NI
SANJ-PI
SANJ-NI
10-3
10-6
10-2
Per-flow delay interference (seconds)
10-5
10-4
(a) Bandwidth overhead
(b) Interference with regular trafﬁc
(c) Impact to packet losses
Figure 10: Quantifying overheads in RLI and the interference of reference packet packets on regular ﬂows.
by a ﬂow with and without our architecture. In Figure 10(b), we
show the cumulative fraction of ﬂows which experienced a partic-
ular amount of additional delay for the high utilization case, where
interference is (expectedly) the most predominant. It is natural to
expect that ﬂows will potentially experience a positive additional
delay (curves x-PI, PI means positive interference). However, we
also found about 10% ﬂows for which the average delay went down
(curves x-NI, NI is negative interference).
From the distribution of both these types of ﬂows (PI and NI),
we observe that there exist ﬂows that appear to have a signiﬁcantly
high (by 20-30ms in one case!) as well signiﬁcantly low (by almost
the same amount) delay in the presence of reference packet trafﬁc
compared to when they were absent. Further investigation revealed
a signiﬁcant difference in the number of packets that were dropped;
for ﬂows with reduced packet delay, a lot of packets that were not
dropped before were getting dropped in the presence of reference
packet trafﬁc. Similarly, for ﬂows that experienced increase in de-
lay, we observed the opposite phenomenon, where packets that
were getting dropped before somehow survived, although with a
huge delay. Put differently, both these incidents—a packet getting
dropped or getting delayed signiﬁcantly—are really related to how
close to being full the queue is. Reference packets cause a small
perturbance to only the packets at the fringe, with some dropped
packets getting converted to high delay and vice-versa; thus, the
interference is therefore quite minimal.
In terms of overall loss, we ﬁnd that our architecture introduces
very little increase in the loss rate as can be seen in the Figure 10(c).
On the whole, we can ﬁnd that the packet loss rate differs by at most
0.001% even at almost 80% utilization for either traces. SANJ trace
experienced slightly more losses than the other because the arrivals
are a little bit more bursty in that trace.
6.
IMPLEMENTATION
We now outline how our architecture can be implemented in
routers. While the packet generator component itself can be imple-
mented in software as the reference packet rate is not too high, it
needs a precise timestamp at the sender side for which hardware is
preferable. Our adaptive reference packet generation scheme main-
tains a little bit of state in the form of a few utilization counters that
can be accommodated within the line-card ASICs. While the ref-
erence packets need not be routable (IP headers are not necessary),
they need to be transmitted from each interface to every other inter-
face and therefore need interface identiﬁers. (Typical routers main-
tain a small internal header anyway that can be overloaded for this
purpose.) Because the two interfaces are operating within the same
time-domain we may not need extra time synchronization (e.g., us-
ing GPS clocks). On the receiver side, we mainly require three
hardware counters on a per-ﬂow basis for ﬂows of interest. Given
the high line rates, counters need to be in SRAM. One can also
leverage the hybrid SRAM-DRAM architecture commonly used in
managing counters [30] and packet buffers [21], to ensure that high
speed counter updates happen in SRAM that are ﬂushed periodi-
cally to cheaper DRAMs. Another solution is to report per-ﬂow
measurements only for a subset of ﬂows, by sampling or with the
help of other mechanisms (e.g., ProgME [38]).
We could potentially compute the per-ﬂow measurements by mir-
roring all the packets from the receiver to a PC (using span ports
available on routers and switches or, using OpenFlow [7]), or to a
network appliance capable of processing them (dedicated for these
ﬁne-grained measurements). Of course, since the line rates are
high, we still need the counter-update functionality within hard-
ware, for which we could leverage FPGA-based solutions (e.g.,
NetFPGA [5] handles 4 Gbps), or high-speed network processor
boards (e.g., Intel IXP 2800). The main advantage of these solu-
tions is that they provide an easy path to deployment today, with-
out having to wait for router vendors to adopt the architecture to
facilitate per-ﬂow measurements, although, we believe, that would
be the ultimately preferable. A big win for our architecture com-
pared to commercial boxes such as those by Corvil [4] is that we do
not require both senders and receivers to obtain measurements; our
solution allows obtaining measurements directly at the receiver.
7. RELATED WORK
While designing router-based passive measurement solutions is a
well-established area of research, designing solutions for ﬁne-grain
latency estimation is a relatively new line of research. This is in part
due to the relatively recent advent of applications such as data cen-
ter and algorithmic trading applications that demand sophisticated
tools to assist in debugging and troubleshooting their performance.
There exists a lot of literature (e.g., [37, 12, 39]) on tomogra-
phy techniques to infer hop and link characteristics from end-to-
end measurements (conducted using tools such as [34]) and topol-
ogy information. They provide aggregate measurements, but not
on a per-ﬂow basis.
In this context of ﬂow measurement, there
have been a wide variety of solutions proposed (e.g., [38, 17, 16,
22]) that employ sampling to control ﬂow selection. Our latency
measurement approach proposed in this paper should, for the most
part, work seamlessly in many of the sampling frameworks; we
have shown how our results compare with other solutions in the
context of random packet sampling used by sampled NetFlow.
Dufﬁeld et al. proposed trajectory sampling in [14], and using
them for inferring loss and delay in [40, 13], which we have dis-
cussed before. Lee et al. [25] exploit the two timestamps already
stored on a per-ﬂow basis within NetFlow to obtain a crude estima-
tor called Multiﬂow estimator for per-ﬂow latency estimates. Since
it is related to our goal, we compare the efﬁcacy of this solution
37with our architecture in our evaluation. Researchers have in the
past conducted measurement studies to understand single-hop de-
lays [27, 19] and delays across PoPs in [10]. They do not propose
any architecture for measuring per-ﬂow delays however.
A very recent and one of the most relevant efforts is the design
of a data structure called LDA by Kompella et al. in [23] and an
architecture that encompasses LDA in [24]. While LDA enables
ﬁne-grained network latency measurements in the context of new
and emerging class of data center and algorithmic trading networks,
it only provides aggregate measurements that, as we argued in this
paper, is not sufﬁcient since concurrent ﬂows tend to exhibit sig-
niﬁcant performance diversity.
It would be an interesting future
direction to design new data structures more similar to LDAs that
can achieve per-ﬂow measurements in a scalable fashion.
8. CONCLUSION
Many new applications such as algorithmic trading and data cen-
ter applications demand low end-to-end latency in the order of mi-
croseconds. While special purpose measurement devices can help
network operators detect latency spikes and other performance anoma-
lies, the high cost of such devices is a strong inhibitor to ubiqui-
tous deployment. A recent solution, LDA, provides a scalable and
low-cost alternative to delay measurement, but only for aggregate
trafﬁc, and is hence insufﬁcient for isolating problems that affect
speciﬁc applications or ﬂows.
We propose a scalable architecture called RLI for obtaining per-
ﬂow latency measurements across interfaces within routers. Our
architecture is based on two key ideas. First, packets within a given
burst encounter similar queueing and other behavior and hence, ex-
hibiting similar delays. Thus, we inject periodic reference packets
at the sender with a timestamp that the receiver can use as a refer-
ence latency sample. Second, the delay experienced by packets that
arrive between multiple reference packets can be approximated by
linearly interpolating the delays of the two reference packets. Using
simulations on packet traces, we ﬁnd that RLI achieves a median
relative error of 10-12% (§5.1), and one-two orders of magnitude
lower relative error compared to previous solutions (§5.2). An-
other big win for RLI comes from the fact that measurements are
obtained directly at the receiver without the need for sender-side
packet timestamps for all the regular data packets, in contrast to
solutions that require correlating large numbers of packet times-
tamps collected from multiple points. Our architecture is simple to
implement and is cost effective making it practical for ubiquitous
deployment. We believe that it offers a compelling alternative to
high-end expensive monitoring boxes for network operators.
Acknowledgments
The authors are indebted to the anonymous reviewers and Sue Moon,
our shepherd, for comments on previous versions of this manuscript.
We also thank Joel Sommers for providing us with real router traces.
This work was supported in part by NSF Award CNS 0831647 and
a grant from Cisco Systems.
9. REFERENCES
[1] Cisco trading ﬂoor architecture. http://www.ciscocapital.info/en/US/
docs/solutions/Verticals/Trading_Floor_Architecture-E.html.
[2] Corvil tool minimises latency. http://www.computerworlduk.com/technology/
networking/networking/news/index.cfm?newsid=5797.
[3] Hp expands high-performance computing offering with inﬁniband solutions
from cisco. http://www.hp.com/hpinfo/newsroom/press/2007/070524xa.html.
[4] The london stock exchange uses corvil technology to improve on latency and
time to market. http://www.corvil.com/products/casestudies/
case_study_london_stock_exchange/.
[5] Netfpga. http://www.netfpga.org.
[6] Next-generation routers: A comprehensive product analysis.
http://www.heavyreading.com/details.asp?sku_id=662&skuitem_itemid=673
&promo_code=&aff_code=&next_url=%2Fdefault.asp%3F.
[7] Openﬂow. http://www.openﬂow.org.
[8] YAF: Yet Another Flowmeter. http://tools.netsa.cert.org/yaf/.
[9] ARISTA NETWORKS, INC. 7100 series datasheet.
http://www.aristanetworks.com, 2008.
[10] CHOI, B., MOON, S., ZHANG, Z.-L., PAPAGIANNAKI, K., AND DIOT, C.
Analysis of point-to-point packet delay in an operational network. In IEEE
INFOCOM (2004).
[11] COPAS, J. Regression, prediction and shrinkage. Journal of the Royal
Statistical Society. Series B (Methodological) 45, 2 (1983), 311–354.
[12] DUFFIELD, N. Simple network performance tomography. In ACM/USENIX
IMC (2003).
[13] DUFFIELD, N., GERBER, A., AND GROSSGLAUSER, M. Trajectory engine: A
backend for trajectory sampling. In IEEE NOMS (2002).
[14] DUFFIELD, N. G., AND GROSSGLAUSER, M. Trajectory sampling for direct
trafﬁc observation. In IEEE/ACM Transactions on Networking (2000).
[15] EIDSON, J., AND LEE, K. IEEE 1588 standard for a precision clock
synchronization protocol for networked measurement and control systems. In
Sensors for Industry Conference, 2002. 2nd ISA/IEEE (2002).
[16] ESTAN, C., KEYS, K., MOORE, D., AND VARGHESE, G. Building a Better
NetFlow. In ACM SIGCOMM (2004).
[17] ESTAN, C., AND VARGHESE, G. New directions in trafﬁc measurement and
accounting. In ACM SIGCOMM (2002).
[18] FLOYD, S., AND JACOBSON, V. Random Early Detection Gateways for
Congestion Avoidance. IEEE/ACM Transactions on Networking (1993).
[19] HOHN, N., VEITCH, D., PAPAGIANNAKI, K., AND DIOT, C. Bridging router
[20]
[21]
performance and queuing theory. In ACM SIGMETRICS (2004).
INCITS. Fibre channel backbone-5 (FC-BB-5), Oct. 2008. Ver. 1.03.
IYER, S., KOMPELLA, R. R., AND MCKEOWN, N. Designing Buffers for
Router Line Cards. IEEE/ACM Transactions on Networking (ToN) 16, 3 (2008).
[22] KOMPELLA, R. R., AND ESTAN, C. The power of slicing in internet ﬂow
measurement. In ACM/USENIX IMC (2005).
[23] KOMPELLA, R. R., LEVCHENKO, K., SNOEREN, A. C., AND VARGHESE, G.
Every MicroSecond Counts: Tracking Fine-grain Latencies Using Lossy
Difference Aggregator. In ACM SIGCOMM (2009).
[24] KOMPELLA, R. R., SNOEREN, A. C., AND VARGHESE, G. mPlane: An
architecture for scalable fault localization. In ACM ReARCH (2009).
[25] LEE, M., DUFFIELD, N., AND KOMPELLA, R. R. Two Samples are Enough:
Opportunistic Flow-level latency estimation using Netﬂow. In IEEE Infocom
(2010).
[26] MARTIN, R. Wall street’s quest to process data at the speed of light.
http://www.informationweek.com/news/infrastructure/
showArticle.jhtml?articleID=199200297.
[27] PAPAGIANNAKI, K., MOON, S., FRALEIGH, C., THIRAN, P., TOBAGI, F.,
AND DIOT, C. Analaysis of measured single-hop delay from an operational
backbone network. IEEE JSAC 21, 6 (2003).
[28] PAXSON, V. End-to-end internet packet dynamics. In ACM SIGCOMM (1997).
[29] PAXSON, V., AND FLOYD, S. Wide-area trafﬁc: The failure of poisson
modeling. IEEE/ACM Transactions on Networking (1995).
[30] RAMABHADRAN, S., AND VARGHESE, G. Efﬁcient implementation of a
statistics counter architecture. In ACM SIGMETRICS (2003).
[31] SHANNON, C., ABEN, E., KC CLAFFY, AND ANDERSEN, D. E. CAIDA
Anonymized 2008 Internet Traces Dataset (collection).
[32] SOMMERS, J., AND BARFORD, P. Self-Conﬁguring Network Trafﬁc
Generation. In ACM/USENIX IMC (2004).
[33] SOMMERS, J., BARFORD, P., DUFFIELD, N., AND RON, A. Improving
accuracy in end-to-end packet loss measurement. In ACM SIGCOMM (2005).
[34] SOMMERS, J., BARFORD, P., DUFFIELD, N., AND RON, A. Accurate and
efﬁcient SLA compliance monitoring. In ACM SIGCOMM (2007).
[35] VASUDEVAN, V., PHANISHAYEE, A., SHAH, H., KREVAT, E., ANDERSEN,
D. G., GANGER, G. R., GIBSON, G. A., AND MUELLER, B. Safe and
effective ﬁne-grained TCP retransmissions for datacenter communication. In
ACM SIGCOMM (2009).
[36] WOVEN SYSTEMS, INC. EFX switch series overview.
http://www.wovensystems.com.
[37] YAN, N. M., CHEN, Y., BINDEL, D., SONG, H., AND KATZ, R. H. An
Algebraic Approach to Practical and Scalable Overlay. In ACM SIGCOMM
(2004).
[38] YUAN, L., CHUAH, C.-N., AND MOHAPATRA, P. ProgME: towards
programmable network measurement. In ACM SIGCOMM (2007).
[39] ZHAO, Y., CHEN, Y., AND BINDEL, D. Towards unbiased end-to-end network
diagnosis. In ACM SIGCOMM (2006).
[40] ZSEBY, T., ZANDER, S., AND CARLE, G. Evaluation of building blocks for
passive one-way-delay measurements. In Proceedings of Passive and Active
Measurement Workshop (PAM 2001) (2001).
38