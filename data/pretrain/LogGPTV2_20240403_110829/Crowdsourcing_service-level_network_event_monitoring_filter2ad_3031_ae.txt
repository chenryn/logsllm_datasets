S=1
S=2
S=3
 1
Likelihood Ratio
 10
S=1
S=2
S=3
 1
Likelihood Ratio
 10
(a) Dev.=1.5σ, Win.=10
(b) Dev.=2.2σ, Win.=20
Figure 8: Concurrent online peers in this
study.
Figure 9: Likelihood ratios are robust to various parameter settings; they detect
network problems at most 2.15% of the time for small deviations and window sizes
(a) and at most 0.75% of the time for larger ones (b).
ones. As such, the number of network events detected using an
LR threshold should not signiﬁcantly change with different local
detection settings.
Fig. 9 plots CDFs of LR values for BT Yahoo during one month.
In Fig. 9(a), we plot LR values for W = 10 and σ = 1.5 and
Fig. 9(b) plots the same for W = 20 and σ = 2.2. Settings for
small deviations and small window sizes logically yield a larger
number of ratio values greater than one (2.15% of the time) whereas
larger deviations and windows yield a smaller number (0.75%).
Generally, such cases (where concurrent events occur more often
than chance) are rare for different detection parameters, suggesting
that LRs are indeed robust to detection settings.
6. DEPLOYMENT DETAILS
The NEWS plugin for Vuze is written in Java and the core
classes for event detection comprise ≈1,000 LOC. Released under
an open-source (GPL) license, our plugin has been installed over
45,000 times since its release in March, 2008. In the rest of this
section, we discuss details of our NEWS implementation in its
current deployment. In addition to providing speciﬁc algorithms
and settings that we use for event detection, our discussion includes
several lessons learned through deployment experience.
Local detection. NEWS detects local events using the moving
average technique discussed in Sec. 4.3.1, which uses the window
size (w) and standard-deviation multiplier (t) parameters to identify
edges in BitTorrent transfer rate signals. NEWS currently uses
w = 10, 20 samples and t = 2.0, 2.5, 3.0 in parallel, dynamically
conﬁgurable settings that have shown to be most effective.
In practice, we found that BitTorrent often saturates a user’s
access link, leading to stable transfer rates and small σ. As a result,
a moving-average technique may detect events in the through-
put signals even when there are negligible relative performance
changes. We address this issue in NEWS by including a secondary
detection threshold that requires a signal value to change by at least
10% before detecting an event.
Throughput signals also undergo phase changes, during which a
moving average detects consecutive events. NEWS treats these as
one event; if enough consecutive events occur, we assume that the
signal has undergone a phase change, and reset the moving average
using only signal values after the phase change.
After detecting a local event, NEWS generates a report con-
taining the user’s per-session ID, w, t, a bitmap indicating the
performance signals generating events, the current event detection
rate (Lh), the time period for the observed detection rate, the
current time (in UTC) and the version number for the report layout.
The current report format consumes 38 bytes.
The plugin disseminates these reports using Vuze’s built-in
Kademlia-based DHT [24], a key-value store that maintains mul-
tiple values for each key. To facilitate group corroboration of
locally detected events, we use network locations as keys and the
corresponding event reports as values.
In our deployment we found variable delays between event
detection and reporting, in addition to signiﬁcant clock skew. To
address these issues, NEWS uses NTP servers to synchronize
clocks once per hour, reports event times using UTC timestamps
and considers any events that occurred within a ﬁve-minute window
when determining the likelihood of a network event occurring.
Group corroboration. After a NEWS peer detects a local event,
it performs corroboration by searching the DHT for other event
reported in each of the host’s regions – currently its BGP preﬁx
and ASN.6 Before using a report from the DHT for corroboration,
NEWS ensures that: (1) the report was not generated by this
peer; (2) the report was generated recently; and (3) the standard-
deviation multiplier and window size for detecting the event match
a local detection setting.
If these conditions are met, the report’s ID is added to the set
of recently reported events for the corresponding detection setting.
If a peer ﬁnds events from at least three other concurrent peers (a
conﬁgurable threshold), it uses Eq. 3 to determine the likelihood
of these events happening by coincidence. Using the information
gathered from events published to the DHT over time, the peer
can calculate the likelihood ratio described in Sec. 4.3.2.
If the
likelihood ratio is greater than two (also conﬁgurable), the monitor
issues a notiﬁcation about the event.
NEWS peers read from the DHT after detecting a local event in
order to corroborate their ﬁnding. To account for delays between
starting a DHT write and its value being available for reading,
NEWS sets a timer and periodically rechecks the DHT for events
during a conﬁgurable period of interest (currently one hour).
Last, our likelihood ratio calculation requires access to the local
detection rate for each online peer. To ensure it is available, each
peer writes its local detection rate to distributed storage at least
once per hour, regardless of whether it has yet detected a local event
during its current session.
Third-party interface. Beyond end-users, network operators
should be notiﬁed to handle service-level events. With this in mind,
we have implemented a DHT crawler (NEWS Collector) that any
third party can run to gather in and analyze local event reports.
To demonstrate its effectiveness, we built NEWSight – a system
that accesses live event information gathered from NEWS Collector
and publishes detected events through a public Web interface.
NEWSight allows network operators to search for events and
register for event notiﬁcations. Operators responsible for affected
networks can conﬁrm/explain detected events.
6Vuze already collects the host’s preﬁx and ASN; we are currently adding
support for whois information.
396Whereas NEWS crowdsources event detection, NEWSight can
be viewed as an attempt at crowdsourcing network event labeling.
Conﬁrmed events can help to improve the effectiveness of ours
and similar approaches – addressing the paucity of labeled data
available in this domain [28]. We are currently beta-testing this
interface with ISPs; the interface and its data are publicly available.
Overhead for participating hosts NEWS passively monitors
performance and uses low-cost event-detection techniques, so
there is negligible overhead for detecting local events.
The
primary sources of overhead are calculating the union probability
(CPU/memory) and sharing locally detected events (network). We
now demonstrate that these overheads are reasonably low.
For determining the union probability, the formula in Eq. (3)
speciﬁes nCn/2 (n choose n/2) operations, where n is the number
of hosts in the network having a nonzero probability of detecting
an event.7 We use Miller’s algorithm [25], an optimal trade-off
between memory, O(n), and computation, O(n3). While a sub-
stantial improvement over a naïve implementation, its processing
overhead can still be signiﬁcant for a large n (e.g., n > 50). To
bound this, we limit the number of hosts used in the computation
to the H hosts with the largest Lh. In this way, we conservatively
estimate an upper bound for Pu for the full set of n hosts.
The other source of overhead is using distributed storage to
share locally detected events. While this overhead is variable and
dependent on factors including the target network and detection
settings, we found it to be reasonably low for many settings. For
example, our analysis shows that read and write operations are
performed by each host with average frequencies on the order of
several minutes, and in the worst case once every 30 seconds (less
than 4 B/s for each peer in the BT Yahoo network).
7. RELATED WORK
The problem of detecting network events (or anomalies) has
attracted a large number of research efforts. In this context, CEM
is a framework for online detection of network events that impact
performance for applications running on end systems. This section
classiﬁes key properties of event detection systems and describes
how CEM relates to previous work in these areas.
Crowdsourcing. Central to our approach is the idea of crowd-
sourcing event detection to ensure good coverage and accuracy
at the scale of hundreds of thousands of users. This model has
successfully enabled projects that include solving intractable [33]
or otherwise prohibitively expensive problems [1] using human
computation. Unlike these examples, our system passively mon-
itors network activity from each member of a crowd, but it does
not require human input. Dash et al. [10] use a similar model to
improve the quality of intrusion detection systems in an enterprise
network and demonstrate its effectiveness through simulation using
trafﬁc data from 37 hosts inside their enterprise network.
Event types. A class of previous work focuses on detecting
network events in or near backbone links, using data gathered
from layer-3 and below [13, 18, 19, 21, 29]. While these monitors
can accurately detect a variety of events, they may miss silent
failures (e.g., incompatible QoS/ACL settings) and their impact
on performance. Other work focuses on detecting network events
from a distributed platform [2, 16, 20, 36]. These solutions do not
correlate these events with user-perceived performance, and their
detection is limited by their network visibility and/or the overhead
for probing large numbers of networks. The goal of CEM is to
7When Lh = 0 for a host, it does not contribute to the union probability.
Thus n is the number of hosts seeing at least one event.
detect service-level network events and correlate their impact on
application performance from the perspective of end users.
Monitoring location. CEM targets events that impact user-
perceived application performance, by running on the end systems
themselves. While several researchers have proposed using end-
host probing to identify routing disruptions and their effect on end-
to-end services [12,16,35,37], they have focused on GREN [20,36]
or enterprise [10, 15, 26] environments and have not looked at the
impact of network events on application performance nor addressed
the issues of scalability when running on end systems. Some
commercial network monitoring tools generate ﬂows that simulate
protocols used by edge systems (e.g., Keynote and IneoQuest8).
While these can indeed detect end-to-end performance problems,
these tools require controllable, dedicated infrastructure and are
inherently limited to relatively small deployments in PoPs. Our
CEM approach does not require any new infrastructure, nor control
of end systems, and thus can be installed on systems at the edge
of the network. Several research efforts have investigated the idea
of active and passive network measurement from end users, e.g.,
DIMES [31] and Neti@home [32], but have not explored the use
of their monitoring information for online network event detection.
Measurement technique. CEM focuses on passive monitoring
of popular applications to detect events, which allows our approach
to scale to the vast numbers of users at the edge of the network
while still detecting events quickly.
In a similar vein, previous
work has suggested that the volume and breadth of P2P systems’
natural trafﬁc could be sufﬁcient to reveal information about the
used network paths without requiring any additional measurement
overhead [9,36]. PlanetSeer [36] uses passive monitoring of a CDN
deployed on PlanetLab, but relies on active probes to characterize
the scope of the detected events. Casado et al. [3] and Isdal et
al. [14] use opportunistic measurement to reach these edges of the
network, by leveraging spurious trafﬁc or free-riding in BitTorrent.
Unlike these efforts, CEM takes advantage of the steady stream
of natural, (generally) benign trafﬁc generated by applications.
Approaches that use active monitoring (e.g.,
[2, 16]) are limited
by the overhead for detection, which grows with the number of
monitored networks and services. While CEM could be combined
with limited active probes to assist in characterizing and localizing
network events, it does not require them.
8. CONCLUSION
The user experience for networked applications is becoming an
important benchmark for customers and network providers. To
assist operators with resolving such issues in a timely manner,
we argued that the most appropriate place for monitoring service-
level events is at the end systems where the services are used. We
proposed a new approach, called CEM for Crowdsourcing Event
Monitoring, based on pushing end-to-end performance monitoring
and event detection to the end systems themselves. We presented
a general framework for CEM systems and demonstrated its effec-
tiveness using a large dataset of diagnostic information gathered
from peers in the BitTorrent system, along with conﬁrmed network
events from two different ISPs. We showed that our crowdsourcing
approach enables worldwide event detection,
including events
spanning multiple networks. Finally, we designed, implemented
and deployed a BitTorrent extension that performs online event
detection using our approach – currently installed more than 45,000
times. Having demonstrated the feasibility and effectiveness of this
approach, we are investigating opportunities for porting it to other
host applications such as VoIP and streaming video.
8http://www.keynote.com and http://www.ineoquest.com/
397Acknowledgments
We thank our shepherd Kevin Jeffay and the anonymous reviewers
for their insightful comments. We also thank Kobus van der
Merwe, Jennifer Yates, Lorenzo Alvisi, Yan Chen and Peter Dinda
for their valuable advice and John Otto and Zach Bischof for
early feedback on the paper. We are especially grateful to the
users/adopters of our software for their invaluable data. This work
was supported in part by the National Science Foundation under
grants CNS 0644062 and CNS 0917233.
9. REFERENCES
[1] AMAZON. Amazon mechanical turk.
http://www.mturk.com/.
[2] ANDERSEN, D., BALAKRISHNAN, H., KAASHOEK, F.,
AND MORRIS, R. Resilient overlay networks. In Proc. ACM
SOSP (2001).
[3] CASADO, M., GARFINKEL, T., CUI, W., PAXSON, V., AND
SAVAGE, S. Opportunistic measurement: Extracting insight
from spurious trafﬁc. In Proc. HotNets (2005).
[4] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON,
A., AND WALLACH, D. Secure routing for structured
peer-to-peer overlay networks. In Proc. USENIX OSDI
(2002).
[5] CHEN, K., CHOFFNES, D., POTHARAJU, R., CHEN, Y.,
BUSTAMANTE, F., AND ZHAO, Y. Where the sidewalk ends:
Extending the Internet AS graph using traceroutes from P2P
users. In Proc. ACM CoNEXT (2009).
[6] CHEN, X., ZHANG, M., MAO, Z. M., AND BAHL, P.
Automating network application dependency discovery:
Experiences, limitations, and new solutions. In Proc.
USENIX OSDI (2008).
[7] CHOFFNES, D., AND BUSTAMANTE, F. Pitfalls for testbed
evaluations of Internet systems. SIGCOMM Comput.
Commun. Rev. 40, 2 (April 2010).
[8] CHOFFNES, D. R., AND BUSTAMANTE, F. E. Taming the
torrent: A practical approach to reducing cross-ISP trafﬁc in
peer-to-peer systems. In Proc. ACM SIGCOMM (2008).
[9] COOKE, E., MORTIER, R., DONNELLY, A., BARHAM, P.,
AND ISAACS, R. Reclaiming network-wide visibility using
ubiquitous endsystem monitors. In Proc. USENIX ATC
(2006).
[10] DASH, D., KVETON, B., AGOSTA, J. M., SCHOOLER, E.,
CHANDRASHEKAR, J., BACHARCH, A., AND NEWMAN,
A. When gossip is good: Distributed probabilistic inference
for detection of slow network intrusions. In Proc. AAAI
(2006).
[11] DISCHINGER, M., MARCON, M., GUHA, S., GUMMADI,
K. P., MAHAJAN, R., AND SAROIU, S. Glasnost: Enabling
end users to detect trafﬁc differentiation. In Proc. USENIX
NSDI (2010).
[12] FEAMSTER, N., ANDERSEN, D., BALAKRISHNAN, H.,
AND KAASHOEK, M. F. Measuring the effect of Internet
path faults on reactive routing. In Proc. ACM SIGMETRICS
(2003).
[13] IANNACCONE, G., NEE CHUAH, C., MORTIER, R.,
BHATTACHARYYA, S., AND DIOT, C. Analysis of link
failures in an IP backbone. In Proc. ACM IMW (2002).
[14] ISDAL, T., PIATEK, M., KRISHNAMURTHY, A., AND
ANDERSON, T. Leveraging BitTorrent for end host
measurements. In Proc. PAM (2007).
[15] KANDULA, S., MAHAJAN, R., VERKAIK, P., AGARWAL,
S., PADHYE, J., AND BAHL, P. Detailed diagnosis in
enterprise networks. In Proc. ACM SIGCOMM (2009).
[16] KATZ-BASSETT, E., MADHYASTHA, H. V., JOHN, J. P.,
KRISHNAMURTHY, A., WETHERALL, D., AND
ANDERSON, T. Studying black holes in the Internet with
Hubble. In Proc. USENIX NSDI (2008).
[17] KEYNOTE. http://www.keynote.com/.
[18] LABOVITZ, C., AHUJA, A., AND JAHANIAN, F.
Experimental study of Internet stability and wide-area
backbone failure. Tech. Rep. CSE-TR-382-98, U. of
Michigan, 1998.
[19] LAKHINA, A., CROVELLA, M., AND DIOT, C. Diagnosing
network-wide trafﬁc anomalies. In Proc. ACM SIGCOMM
(2004).
[20] MADHYASTHA, H. V., ISDAL, T., PIATEK, M., DIXON,
C., ANDERSON, T., KIRSHNAMURTHY, A., AND
VENKATARAMANI, A. iPlane: an information plane for
distributed systems. In Proc. USENIX OSDI (2006).
[21] MAHAJAN, R., WETHERALL, D., AND ANDERSON, T.
Understanding BGP misconﬁguration. In Proc. ACM
SIGCOMM (2002).
[22] MAHAJAN, R., ZHANG, M., POOLE, L., AND PAI, V.
Uncovering performance differences among backbone ISPs
with Netdiff. In Proc. USENIX NSDI (2008).
[23] MAHIMKAR, A., GE, Z., SHAIKH, A., WANG, J., YATES,
J., ZHANG, Y., AND ZHAO, Q. Towards automated
performance diagnosis in a large IPTV network. In Proc.
ACM SIGCOMM (2009).
[24] MAYMOUNKOV, P., AND MAZIERES, D. Kademlia: A
peer-to-peer information system based on the XOR metric.
In Proc. IPTPS (2002).
[25] MILLER, G. D. Programming techniques: An algorithm for
the probability of the union of a large number of events.
Commun. ACM 11, 9 (1968).
[26] PADMANABHAN, V. N., RAMABHADRAN, S., AND
PADHYE, J. NetProﬁler: proﬁling wide-area networks using
peer cooperation. In Proc. IPTPS (2005).
[27] RABINOVICH, M., TRIUKOSE, S., WEN, Z., AND WANG,
L. Dipzoom: The internet measurements marketplace. In
Proc. IEEE INFOCOM (2006).
[28] RINGBERG, H., SOULE, A., AND REXFORD, J. WebClass:
adding rigor to manual labeling of trafﬁc anomalies.
SIGCOMM Comput. Commun. Rev. 38, 1 (2008).
[29] ROUGHAN, M., GRIFFIN, T., MAO, Z. M., GREENBERG,
A., AND FREEMAN, B. IP forwarding anomalies and
improving their detection using multiple data sources. In
Proc. of the ACM SIGCOMM workshop on Network
troubleshooting (2004).
[30] SCHULZE, H., AND MOCHALSKI, K. Internet study
2008/2009, 2009.
[31] SHAVITT, Y., AND SHIR, E. DIMES: let the Internet
measure itself. SIGCOMM Comput. Commun. Rev. 35, 5
(2005).
[32] SIMPSON, JR, C. R., AND RILEY, G. F. Neti@home: A
distributed approach to collecting end-to-end network
performance measurements. In Proc. PAM (2004).
[33] VON AHN, L. Human computation. In Proc. DAC (2009).
[34] VUZE, INC. Vuze. http://www.vuze.com.
[35] WU, J., MAO, Z. M., REXFORD, J., AND WANG, J.
Finding a needle in a haystack: Pinpointing signiﬁcant BGP
routing changes in an IP network. In Proc. USENIX NSDI
(2005).
[36] ZHANG, M., ZHANG, C., PAI, V., PETERSON, L., AND
WANG, R. PlanetSeer: Internet path failure monitoring and
characterization in wide-area services. In Proc. USENIX
OSDI (2004).
[37] ZHANG, Y., MAO, Z. M., AND ZHANG, M. Effective
diagnosis of routing disruptions from end systems. In Proc.
USENIX NSDI (2008).
398