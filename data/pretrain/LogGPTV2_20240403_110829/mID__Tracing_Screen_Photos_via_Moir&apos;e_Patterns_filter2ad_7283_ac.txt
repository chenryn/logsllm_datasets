and (2) fewer details of the current page will be lost and
less vision disparity will be caused to users. In addition, we
design to embed one bit of mID in each ROE. It is because that
embedding the whole mID in one ROE may require a large ﬂat
region. Separating the mID into several ROEs helps to reduce
the size requirement of ROEs.
Therefore, we search for N rectangular regions close to
the ROI center, where N is the number of bits of mID. Each
embedding region has a size of p×q, where p and q represent
the height and width of a 1-bit grating, respectively. The width
q can be further calculated as q = 2k× n. The height p can
be any value theoretically but a minimum one is required to
ensure the distinguishability of Moiré patterns in the screen
photos. In practice, we suggest that p > 50. Note that the
embedding region can be any shape. We employ rectangle
here for the ease of encoding and decoding.
We utilize a sliding window with a size of p× q and a step
of wm to scan through the current page for ROE searching.
For each image window B(x,y) with (x,y) as the centroid
coordinate, we evaluate its ﬁtness F(x,y) in consideration of
both evenness and location:
D(x,y) =
∑
ch={r,g,b}
σ(ch[x− p
1
2 : x + p
2 ,y− q
2 : y + q
2 ])
(11)
1
−Cy)
abs( x
hB
−Cx) + abs( y
wB
L(x,y) =
F(x,y) = wD · D(x,y) + wL · L(x,y)
where σ(ch) refers to the standard deviation of channel
ch = {r,g,b} of the current page. hB and wB are the height
and width of the current page, (Cx,Cy) is the centroid coordi-
nate of ROI, and wD and wL are the weights of the deviation
D(x,y) and location L(x,y) functions, respectively. In our
implementation, we set wD = wL = 0.5.
We employ the ﬁrst N image windows in the descending
Figure 7: Illustration of ROI (red box) and ROE (black box)
of the current page [26]. The red dot is the center of ROI.
order of ﬁtness ranking as our ROE, and rearrange them ac-
cording to the horizontal coordinates (in an ascending order)*.
As thus, we obtain N image windows in the horizontal direc-
tion. With the obtained regions, we embed the corresponding
mID bits by replacing the pixels of the original page with that
of the generated gratings. As thus, we embed the generated
mID gratings into the current page of the screen, under the
premise of non-obvious visual impact to users.
4.5 mID Extraction
The image captured by smartphones contains Moiré patterns
as well as other elements. To obtain the embedded mID, we
ﬁrst locate the regions of Moiré patterns in the smartphone-
captured image, which we call the Moiré areas.
Image Rectiﬁcation. Smartphone-captured images usually
suffer from geometric distortion due to the unparalleled cam-
era and screen planes, i.e., an angle exists between them. As
a result, the captured screen is no longer a regular rectangle
but a distorted quadrilateral. To address it, we ﬁrst rectify
the distorted image with the commonly-used projection trans-
formation under the homogeneous coordinates [8, 53], and
then extract the rectiﬁed rectangle that contains the screen for
further Moiré area extraction.
Moiré Area Extraction. One intuitive method to extract
the Moiré areas is to search for the red-green fringes. How-
ever, as Moiré patterns may appear as various colors on dif-
ferent backgrounds and blur due to the noise introduced by
the screen-camera channel, simply searching for fringes of a
speciﬁc color may not sufﬁce. Therefore, we turn to the trans-
verse coding style we employ for mID encoding, because of
which the Moiré area is likely to have larger color variations
in the horizontal direction compared to the vertical one.
To extract the Moiré areas with robustness, we use a 2-
dimension (2D) window Wm with a size of hm×wm and a step
of tm to scan through the rectiﬁed rectangle image. Speciﬁ-
cally, we calculate the average color variation Varh and Varv
in both the horizontal and vertical directions, and determine
whether the current window belongs to the Moiré area with
the following in-equation:
Varv > r·Varh
(12)
*Arrange by vertical coordinates If equal horizontal coordinates.
USENIX Association
30th USENIX Security Symposium    2975
513.246Centroid of ROI(a) Saturation of exposure-unbalanced JMA.
(c) Saturation of difference-enlarged JMA.
Figure 8: After pre-processing, the saturation of JMA is balanced and the difference between bits “0” and “1” is enlarged.
(b) Saturation of exposure-balanced JMA.
Figure 9: The JMA saturation distribution
is roughly a Gaussian distribution.
(a) Saturation curve of the pre-processed Moiré area.
Figure 10: An illustration of mID recovered by k-means clustering with check codes.
(b) Bit sequence recovered by bit clustering.
where r is the ratio threshold and the window with signiﬁ-
cantly larger horizontal variation will be regarded as a part
of the Moiré area. To achieve high extraction precision, the
window size and step are usually supposed to be in ﬁne gran-
ularity. In practice, we set hm = wm = sm = 10 pixels, and
r = 1.5. After scanning, we obtain a number of Moiré win-
dows in several clusters with possibly a few outliers. The
number of clusters, i.e., the number of Moiré areas contained
in the photo, is usually less than or equal to the number of mID
bits N since two adjacent embedding regions appear as one
Moiré area in the photos. To locate the Moiré areas, we ﬁrst
cluster those Moiré windows with mean shift clustering [6],
which obtains the center of each Moiré area roughly. Then,
we utilize Random Sample Consensus (RANSAC) [11] to
discriminate outliers and search for the minimum rectangle
that contains the rest of clustered Moiré windows for each
Moiré area. We gradually iterate their boundaries until conver-
gent, with which we extract the Moiré areas for further mID
decoding.
4.6 mID Decoding
After extracting the Moiré areas, we perform mID decoding
to recover the embedded mID. To ease burden of decoding,
we arrange and connect the obtained Moiré areas together
according to their horizontal coordinates. In this way, we
obtain a joint Moiré area (JMA) for decoding.
4.6.1 Image Pre-processing
The ﬁrst set of decoding procedures is image pre-processing
that includes (1) Color Space Transformation that makes the
decoding algorithm robust across different colors, (2) Satu-
ration Balance that reduces the impact of focus position and
the ambient light, and (3) Saturation Difference Enlargement
that enlarges the saturation difference between bit “0” and bit
“1” to help decoding.
Color Space Transformation. The colors of the mID-
related Moiré patterns depend on the screen backgrounds.
For instance, a white background will produce Moiré patterns
with red and green stripes. To make the decoding algorithm ro-
bust across different RGB colors, we transform the joint Moiré
area into the HSV (hue, saturation, value) color space [49].
Speciﬁcally, as we utilize the Moiré pattern intensity to en-
code bits and high intensity results in high color saturation,
we perform mID decoding in the saturation dimension.
Saturation Balance. When taking a picture towards a
screen, people tend to focus on the center of the screen to
capture the whole screen. As a result, the Moiré areas close to
the focus may be better exposed compared to the remote ones,
as shown in Fig. 8(a). To reduce the impact of focus position
as well as the ambient light, we balance the saturation of the
joint Moiré area with the help of check codes.
Speciﬁcally, we focus on the horizontal saturation balance
since we encode mID in a transverse way and thus horizontal
saturation unbalance has a larger impact on decoding com-
pared to that in the vertical direction. To address it, we divide
the joint Moiré area into N splits by width, where the 1st and
2nd splits correspond to the bit “0” and bit “1” of the front
check code, and the (N − 1)th and Nth splits correspond to
that of the end check code. We compare the average satura-
tion of the 2nd and Nth splits and enhance the side with lower
saturation. The image enhancement algorithm we employ ma-
nipulates every pixel of the image and balances the saturation
in a horizontal and linear way, as shown in Algorithm 1 in
Appendix 11.2. For exposure-imbalanced images, saturation
balance is able to restore the actual Moiré patterns as shown
in Fig. 8(b), and thus can improve the decoding accuracy.
Saturation Difference Enlargement. After saturation bal-
ance, we enlarge the saturation difference between bit “0” and
bit “1” to improve the decoding efﬁciency. In general, the
Moiré area of bit “1” is likely to have more pixels with large
saturation values compared with that of bit “0”. However,
the noise (e.g., the Gaussian and salt-and-pepper noise [4],
which are common in photos) introduced during the process
of photographing may blur the image and thus increase the
difﬁculty of decoding. To ease the burden, we perform satu-
ration difference enlargement. Speciﬁcally, we assume that
the saturation values of pixels in the joint Moiré area are in
concordance with the Gaussian distribution based on the Cen-
2976    30th USENIX Security Symposium
USENIX Association
050100150200250300350400Image Transverse Axis203040Avg. Saturation01011010100101050100150200250300350400Image Transverse Axis506070Avg. Saturation01011010100101050100150200250300350400Image Transverse Axis01020Avg. Saturation01011010100101020406080100120140160Saturation0.000.010.02PDFμ+1.6σ050100150200250300350400Image Transverse Axis02Avg. Saturation01011010100101normalizedhanning050100150200250300350400Image Transverse AxisBit Sequence01011010100101clusteringhanningtral Limit Theorem (CLT) [10], i.e., S ∼ N (µ,σ2), as shown
in Fig. 9. Based on it, we enhance the discrepancy between
bit “0” and bit “1” as follows:
(cid:26)
s(x,y) =
0
s(x,y)
s(x,y) < µ + α· σ
s(x,y) ≥ µ + α· σ
(13)
where α is the ampliﬁcation factor. An appropriate α is able
to reduce the saturation intensity of bit “0” while maintain
that of bit “1”, and thus can help enlarge their differences, as
shown in Fig. 8(c). In practice, we set α to be 1.6.
4.6.2 ID Recovery
After image pre-processing, we recover IDs via k-means clus-
tering with the assistance of check codes.
Saturation Curving. With the enhanced joint Moiré area,
we ﬁrst calculate the histogram of each column and obtain a
1×W saturation matrix, where W is the length of the joint
Moiré area. It is based on the transverse encoding we employ,
which means that pixels of the same column are supposed to
be identical. We then perform normalization on the matrix
and utilize a Hanning window to reduce the noise introduced
during photographing and improve the SNR (signal-to-noise
ratio). As thus, we obtain a horizontal saturation curve for
further decoding, as shown in Fig. 10(a).
Bit Clustering. For an N-bit mID, we further divide its sat-
uration curve into N splits and calculate the saturation sum
of each split as the value of the corresponding bit, denoted
as {P0,P1, ...,PN−1}. Since the processed saturation sequence
may have outliers (abnormally large values in our case) that
are likely to affect the clustering threshold, we reduce their
impacts by suppressing data points with large values. Speciﬁ-
cally, for an N-bit mID, we decrease the largest K data points
as follows:
=
(14)
where σ(cid:48) is the standard deviation of the saturation sequence
{P0,P1, ...,PN−1}, and β is the decreasing factor. A larger β
suppresses outliers more heavily. In practice, we set β = 0.1.
After that, we employ k-means clustering [50] to group
the same bit into the same class and utilize the check codes
to identify each class, i.e., bit “0” or bit “1”, as shown in
Fig. 10(b). In this way, we recover mIDs from screen photos
and the whole decoding process is shown in Algorithm 2 in
Appendix 11.2.
(cid:48)
P
P
σ(cid:48)β
5 Implementation
We implement mID scheme at both the OS and application
levels in Windows, where mID runs as a background applica-
tion or a script after a user logs in, receptively. For the OS
level, mID employs the entire screen as the display window
and creates a rendering context using the Windows API func-
tions GetDC() and wglCreateContext. For the application
level, mID employs the application window as the display win-
dow and uses its own rendering context. Then, mID captures
the current page of the screen or application in real-time us-
ing the function glReadPixels() under the OpenGL (Open
Graphics Library) framework [19]. After that, it searches for
the ROI and ROE with the methods proposed in the mID
Embedding module. With the obtained ROE, mID replaces
the pixels of ROE with the gratings generated by the mID
Generation module, passes the new mID-embedded screen
(application) frame to the function glBufferData(), and
ﬁnally renders it on the display.
6 Evaluation
In this section, we evaluate the performance of the mID
scheme. We conduct experiments under various settings and
collect over 5000 photos with 5 display devices and 6 smart-
phones over 3 months. In particular, we evaluate the impact of
(1) IDs, (2) display devices, (3) capturing devices, (4) ambient
lights, (5) shooting distances, and (6) shooting angles with
the metrics of bit error rate (BER) and identity number error
rate (NER). In addition, we evaluate the performance of mID
against several photo processing attacks. The performance of
the mID scheme is summarized below:
• mID achieves an average BER of 0.6% and an average
NER of 4.0%, which demonstrates promises towards
screen photo forensics.
• mID performs well with little inﬂuence from the type of
display devices, cameras, IDs, and ambient lights.
• mID performs well at a
shooting distance of
(60cm,80cm) and a shooting angle of (−20◦,20◦),
which are within the possible attack distances and angles
adopted by adversaries as suggested by the theoretical
calculation (in Appendix 11.1).
6.1 Experiment Setup
We evaluate mID scheme in a laboratory setting with various
display and capturing devices. The detailed settings are as
follows.
Display Device. We use a BenQ EW Series LCD screen
as the default display device. To evaluate the impact of dis-
play devices, we use 2 other LCD displays and 2 laptops of
different brands and models. Throughout the experiments, the
display devices remain in the default settings with normal
color mode and 50% screen brightness. The detailed informa-
tion of each display device is shown in Tab. 1.
Capturing Device. We use an LG Nexus 5X smartphone
as the default capturing device. In addition, we use 5 other
smartphones of various brands and models to evaluate the
impact of capturing devices. Throughout the experiments, the
capturing device is clamped on a tripod with a height of 30 cm
from the desk and alighted with the center point of the display
screen, as shown in Fig. 11. The shooting distance and angle
are set to 70 cm and 0◦ respectively. We use the main camera
of each device in the default settings, with Auto-focusing
USENIX Association
30th USENIX Security Symposium    2977
Table 1: Summary of display devices.
Model
Display Size Aspect Ratio
EW2440ZC
No. Manuf.
BenQ
1
HP
2
AOC
3
4
Lenovo
5
ASUS
MVA: Multi-domain Vertical Alignment.
IdeaPad Y700
LV243XIP
FX50J
24w
24"
23.8"
23.8"
15.6"
15.6"
16:9
16:9
16:9
16:9
16:9
Viewing Area
53.1 cm × 29.9 cm
52.7 cm × 29.6 cm
52.7 cm × 29.6 cm
34.5 cm × 19.4 cm
34.5 cm × 19.4 cm
Native Resolution
1920× 1080
1920× 1080
1920× 1080