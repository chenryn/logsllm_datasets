**Title: Misleading Authorship Attribution of Source Code using Adversarial Learning**

**Authors: Erwin Quiring, Alwin Maier, and Konrad Rieck**

**Affiliation: Technische Universität Braunschweig, Germany**

**Conference: 28th USENIX Security Symposium, August 14–16, 2019, Santa Clara, CA, USA**

**Proceedings: Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.**

---

### Abstract

In this paper, we introduce a novel attack against authorship attribution of source code. We exploit the fact that recent attribution methods rely on machine learning, which can be deceived by adversarial examples. Our attack performs a series of semantics-preserving code transformations that mislead learning-based attribution while appearing plausible to a developer. The attack is guided by Monte-Carlo tree search, enabling us to operate in the discrete domain of source code. In an empirical evaluation with source code from 204 programmers, we demonstrate that our attack significantly reduces the accuracy of two recent attribution methods, dropping their accuracy from over 88% to 1%. Additionally, we show that our attack can imitate the coding style of developers with high accuracy, leading to false attributions. We conclude that current approaches for authorship attribution are not suitable for practical application and highlight the need for more resilient analysis techniques.

### 1. Introduction

Source code often contains peculiarities that reflect individual coding styles, which can be used to identify the programmer. These stylistic patterns range from simple artifacts in comments and code layout to subtle habits in syntax and control flow. For example, a programmer might prefer while-loops even when for-loops would be more appropriate. The task of identifying a programmer based on these patterns is known as authorship attribution, and several methods have been proposed to recognize the authors of both source code [1, 4, 9, 13] and compiled programs [3, 10, 17, 22].

While authorship attribution techniques have made significant progress, their robustness against attacks has received limited attention. Most work has focused on achieving high accuracy. However, a recent study by Simko et al. [25] shows that developers can manually tamper with the attribution of source code, necessitating the consideration of attacks that can forge stylistic patterns and mislead attribution methods.

In this paper, we present the first black-box attack against authorship attribution of source code. Our attack exploits the vulnerability of recent attribution methods to adversarial examples [see 20]. We combine concepts from adversarial learning and compiler engineering to create adversarial examples in the space of semantically-equivalent programs.

Our attack iteratively transforms the source code, altering stylistic patterns while preserving the underlying semantics. To determine these transformations, we interpret the attack as a game against the attribution method and develop a variant of Monte-Carlo tree search [24] to construct a sequence of adversarial but plausible transformations. This black-box strategy enables us to construct both untargeted attacks, which thwart correct attribution, and targeted attacks, which imitate the stylistic patterns of a specific developer.

As an example, Figure 1 shows two transformations performed by our attack on a code snippet from the Google Code Jam competition. The first transformation changes the for-loop to a while-loop, and the second replaces the C++ operator `<<` with `printf` to imitate the stylistic patterns of another author.

**Figure 1: Two iterations of our attack: Transformation (a) changes the control statement `for` → `while`, and transformation (b) manipulates the API usage `ostream` → `printf` to imitate the stylistic patterns of author B.**

We conduct a series of experiments to evaluate the efficacy of our attack using the source code of 204 programmers from the Google Code Jam competition. As targets, we consider the recent attribution methods by Caliskan et al. [9] and Abuhamad et al. [1], which provide superior performance compared to related approaches. In our first experiment, we demonstrate that our attack significantly affects both attribution methods, reducing their accuracy from over 88% to 1%, indicating that authorship attribution can be automatically thwarted at a large scale. In our second experiment, we investigate the effect of targeted attacks. We show that, on average, each individual in a group of programmers can be impersonated by 77% to 81% of the other developers. Finally, we demonstrate in a study with 15 participants that code transformed by our attack is plausible and difficult to distinguish from unmodified source code.

Our work has implications for the practical applicability of authorship attribution. We find that both untargeted and targeted attacks are effective, making reliable identification of programmers questionable. Although our approach builds on a fixed set of code transformations, we conclude that features regularly manipulated by compilers, such as specific syntax and control flow, are not reliable for constructing attribution methods. Consequently, we suggest moving away from these features and seeking more reliable means for identifying authors in source code.

### Contributions

In summary, we make the following major contributions in this paper:

- **Adversarial Learning on Source Code:** We present the first automatic attack against authorship attribution of source code, considering both targeted and untargeted attacks.
- **Monte-Carlo Tree Search:** We introduce Monte-Carlo tree search as a novel approach to guide the creation of adversarial examples, ensuring feasibility constraints in the domain of source code.
- **Black-Box Attack Strategy:** The devised attack does not require internal knowledge of the attribution method, making it applicable to any learning algorithm and suitable for evading a wide range of attribution methods.
- **Large-Scale Evaluation:** We empirically evaluate our attack on a dataset of 204 programmers, demonstrating that manipulating the attribution of source code is possible in the majority of cases.

The remainder of this paper is organized as follows: We review the basics of program authorship attribution in Section 2. The design of our attack is laid out in Section 3, while Sections 4 and 5 discuss technical details on code transformation and adversarial learning, respectively. An empirical evaluation of our attack is presented in Section 6, along with a discussion of limitations in Section 7. Section 8 discusses related work, and Section 9 concludes the paper.

### 2. Authorship Attribution of Source Code

Before introducing our attack, we briefly review the design of methods for authorship attribution. We denote the source code of a program as \( x \) and refer to the set of all possible source codes as \( X \). We also define a finite set of authors \( Y \). Authorship attribution is then the task of identifying the author \( y \in Y \) of a given source code \( x \in X \) using a classification function \( f \) such that \( f(x) = y \). In line with most previous work, we assume that the programs in \( X \) can be attributed to a single author, as the identification of multiple authors is an ongoing research effort [see 12, 17].

Equipped with this basic notation, we proceed to discuss the two main building blocks of current methods for authorship attribution: (a) the extraction of features from source code and (b) the application of machine learning for constructing the classification function.

#### 2.1 Feature Extraction

The coding habits of a programmer can manifest in various stylistic patterns. Therefore, methods for authorship attribution need to extract an expressive set of features from source code that serve as a basis for inferring these patterns. In the following, we discuss the major types of these features and use the code sample in Figure 2 as a running example throughout the paper.

**Figure 2: Exemplary code sample (see Figures 3, 5, and 6)**

**Layout Features:**
Individual preferences of a programmer often manifest in the layout of the code, making corresponding features a simple tool for characterizing coding style. Examples include indentation, the form of comments, and the use of brackets. In Figure 2, for instance, the indentation width is 2, comments are provided in C++ style, and curly braces are opened on the same line.

Layout features are trivial to forge, as they can be easily modified using tools for code formatting, such as GNU indent. Moreover, many integrated development editors automatically normalize source code, unifying stylistic patterns in the layout.

**Lexical Features:**
A more advanced type of feature can be derived from the lexical analysis of source code. In this analysis stage, the source code is partitioned into so-called lexemes, tokens that are matched against the terminal symbols of the language grammar. These lexemes give rise to a strong set of string-based features jointly covering keywords and symbols. For example, in Figure 2, the frequency of the lexeme `int` is 3, while it is 2 for the lexeme `foo`.

Unlike code layout, lexical features cannot be easily manipulated, as they implicitly describe the syntax and semantics of the source code. While the lexeme `foo` in the running example could be easily replaced by another string, adapting the lexeme `int` requires a more involved code transformation that introduces a semantically equivalent data type. We introduce such a transformation in Section 4.

**Syntactic Features:**
The use of syntax and control flow also reveals individual stylistic patterns of programmers. These patterns are typically accessed using the abstract syntax tree (AST), a basic data structure of compiler design [2]. As an example, Figure 3 shows a simplified AST of the code snippet from Figure 2. The AST provides the basis for constructing an extensive set of syntactic features. These features can range from the specific use of syntactic constructs, such as unary and ternary operators, to generic features characterizing the tree structure, such as the frequency of adjacent nodes. In Figure 3, there exist 21 pairs of adjacent nodes, including, for example, `(func foo) → (arg int)` and `(return) → (1)`.

Manipulating features derived from an AST is challenging, as even minor tweaks in the tree structure can fundamentally change the program semantics. Transformations to the AST need to be carefully designed to preserve the original semantics and avoid unintentional side effects. For example, removing the node pair `(decl int) → (b)` from the AST in Figure 3 requires either replacing the type or the name of the variable without interfering with the remaining code. In practice, such transformations are often non-trivial, and we discuss the details of manipulating the AST in Section 4.

**Figure 3: Abstract syntax tree (AST) for code sample in Figure 2.**

#### 2.2 Machine Learning

The three feature types (layout, lexical, syntactic) provide a broad view of the characteristics of source code and are used by many attribution methods as the basis for applying machine-learning techniques [e.g., 1, 4, 9, 21].

### 3. Misleading Authorship Attribution

With a basic understanding of authorship attribution, we are ready to investigate the robustness of attribution methods and develop a corresponding black-box attack. To this end, we first define our threat model and attack scenario before discussing technical details in the following sections.

#### 3.1 Threat Model

For our attack, we assume an adversary who has black-box access to an attribution method. The adversary can send an arbitrary source code \( x \) to the method and retrieve the corresponding prediction \( f(x) \) along with prediction scores \( g(x) \). The training data, extracted features, and employed learning algorithm are unknown to the adversary, and thus the attack can only be guided by iteratively probing the attribution method and analyzing the returned prediction scores. This setting resembles a classic black-box attack as studied by Tramèr et al. [26] and Papernot et al. [19].

As part of our threat model, we consider two types of attacks—untargeted and targeted attacks—that require different capabilities of the adversary and have distinct implications for the involved programmers.

**Untargeted Attacks:**
In this setting, the adversary tries to mislead the attribution of source code by changing the classification into any other programmer. This attack, also denoted as dodging [23], impacts the correctness of the attribution. For example, a benign programmer might use this attack strategy to conceal her identity before publishing the source code of a program.

**Targeted Attacks:**
The adversary tries to change the classification into a chosen target programmer. This attack resembles an impersonation and is technically more advanced, as it requires transferring the stylistic patterns from one developer to another. A targeted attack has more severe implications: for instance, a malware developer could systematically change her source code to blame a benign developer.

Furthermore, we consider two scenarios for targeted attacks:
- **Scenario 1:** The adversary has no access to source code from the target programmer, and thus certain features, such as variable names and custom types, can only be guessed.
- **Scenario 2:** The adversary has access to source code from the target programmer, allowing for more precise imitation of the target's coding style.

---

This revised version aims to improve the clarity, coherence, and professionalism of the text, making it more accessible and engaging for the reader.