title:Misleading Authorship Attribution of Source Code using Adversarial
Learning
author:Erwin Quiring and
Alwin Maier and
Konrad Rieck
Misleading Authorship Attribution of 
Source Code using Adversarial Learning
Erwin Quiring, Alwin Maier, and Konrad Rieck, TU Braunschweig
https://www.usenix.org/conference/usenixsecurity19/presentation/quiring
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.Misleading Authorship Attribution of Source Code
using Adversarial Learning
Erwin Quiring, Alwin Maier and Konrad Rieck
Technische Universit¨at Braunschweig, Germany
Abstract
In this paper, we present a novel attack against authorship
attribution of source code. We exploit that recent attribution
methods rest on machine learning and thus can be deceived
by adversarial examples of source code. Our attack performs
a series of semantics-preserving code transformations that
mislead learning-based attribution but appear plausible to a de-
veloper. The attack is guided by Monte-Carlo tree search that
enables us to operate in the discrete domain of source code.
In an empirical evaluation with source code from 204 pro-
grammers, we demonstrate that our attack has a substantial
effect on two recent attribution methods, whose accuracy
drops from over 88% to 1% under attack. Furthermore, we
show that our attack can imitate the coding style of developers
with high accuracy and thereby induce false attributions. We
conclude that current approaches for authorship attribution
are inappropriate for practical application and there is a need
for resilient analysis techniques.
1 Introduction
The source code of a program often contains peculiarities that
reﬂect individual coding style and can be used for identifying
the programmer. These peculiarities—or stylistic patterns—
range from simple artifacts in comments and code layout to
subtle habits in the use of syntax and control ﬂow. A pro-
grammer might, for example, favor while-loops even though
the use of for-loops would be more appropriate. The task of
identifying a programmer based on these stylistic patterns
is denoted as authorship attribution, and several methods
have been proposed to recognize the authors of source code
[1, 4, 9, 13] and compiled programs [3, 10, 17, 22].
While techniques for authorship attribution have made
great progress in the last years, their robustness against at-
tacks has received only little attention so far, and the majority
of work has focused on achieving high accuracy. The recent
study by Simko et al. [25], however, shows that developers
can manually tamper with the attribution of source code and
thus it becomes necessary to reason about attacks that can
forge stylistic patterns and mislead attribution methods.
In this paper, we present the ﬁrst black-box attack against
authorship attribution of source code. Our attack exploits
that recent attribution methods employ machine learning and
thus can be vulnerable to adversarial examples [see 20]. We
combine concepts from adversarial learning and compiler
engineering, and create adversarial examples in the space of
semantically-equivalent programs.
Our attack proceeds by iteratively transforming the source
code of a program, such that stylistic patterns are changed
while the underlying semantics are preserved. To deter-
mine these transformations, we interpret the attack as a
game against the attribution method and develop a variant
of Monte-Carlo tree search [24] for constructing a sequence
of adversarial but plausible transformations. This black-box
strategy enables us to construct untargeted attacks that thwart
a correct attribution as well as targeted attacks that imitate
the stylistic patterns of a developer.
As an example, Figure 1 shows two transformations per-
formed by our attack on a code snippet from the Google
Code Jam competition. The ﬁrst transformation changes the
for-loop to a while-loop, while the second replaces the C++
operator  0; i - -)
cout  0) {
printf ( " % lld " , ans [ i ]) ;
i - -;
}
Figure 1: Two iterations of our attack: Transformation  changes the control
statement for → while and transformation  manipulates the API usage
ostream → printf to imitate the stylistic patterns of author B.
USENIX Association
28th USENIX Security Symposium    479
We conduct a series of experiments to evaluate the efﬁ-
cacy of our attack using the source code of 204 programmers
from the Google Code Jam competition. As targets we con-
sider the recent attribution methods by Caliskan et al. [9] and
Abuhamad et al. [1], which provide superior performance
compared to related approaches. In our ﬁrst experiment, we
demonstrate that our attack considerably affects both attribu-
tion methods [1, 9], whose accuracy drops from over 88%
to 1% under attack, indicating that authorship attribution can
be automatically thwarted at large scale. In our second ex-
periment, we investigate the effect of targeted attacks. We
show that in a group of programmers, each individual can be
impersonated by 77% to 81% of the other developers on aver-
age. Finally, we demonstrate in a study with 15 participants
that code transformed by our attack is plausible and hard to
discriminate from unmodiﬁed source code.
Our work has implications on the applicability of author-
ship attribution in practice: We ﬁnd that both, untargeted and
targeted attacks, are effective, rendering the reliable identiﬁ-
cation of programmers questionable. Although our approach
builds on a ﬁxed set of code transformations, we conclude
that features regularly manipulated by compilers, such as spe-
ciﬁc syntax and control ﬂow, are not reliable for constructing
attribution methods. As a consequence, we suggest to move
away from these features and seek for more reliable means
for identifying authors in source code.
Contributions. In summary, we make the following major
contributions in this paper:
• Adversarial learning on source code. We present the
ﬁrst automatic attack against authorship attribution of
source code. We consider targeted as well as untargeted
attacks of the attribution method.
• Monte-Carlo tree search. We introduce Monte-Carlo
tree search as a novel approach to guide the creation of
adversarial examples, such that feasibility constraints in
the domain of source code are satisﬁed.
• Black-box attack strategy. The devised attack does not
require internal knowledge of the attribution method,
so that it is applicable to any learning algorithm and
suitable for evading a wide range of attribution methods.
• Large-scale evaluation. We empirically evaluate our
attack on a dataset of 204 programmers and demonstrate
that manipulating the attribution of source code is possi-
ble in the majority of the considered cases.
The remainder of this paper is organized as follows: We
review the basics of program authorship attribution in Section
2. The design of our attack is lay out in Section 3, while
Section 4 and 5 discuss technical details on code transfor-
mation and adversarial learning, respectively. An empirical
evaluation of our attack is presented in Section 6 along with
a discussion of limitations in Section 7. Section 8 discusses
related work and Section 9 concludes the paper.
2 Authorship Attribution of Source Code
Before introducing our attack, we brieﬂy review the design
of methods for authorship attribution. To this end, we denote
the source code of a program as x and refer to the set of all
possible source codes by X . Moreover, we deﬁne a ﬁnite set
of authors Y. Authorship attribution is then the task of identi-
fying the author y ∈ Y of a given source code x ∈ X using a
classiﬁcation function f such that f (x) = y. In line with the
majority of previous work, we assume that the programs in
X can be attributed to a single author, as the identiﬁcation of
multiple authors is an ongoing research effort [see 12, 17].
Equipped with this basic notation, we proceed to discuss
the two main building blocks of current methods for author-
ship attribution: (a) the extraction of features from source
code and (b) the application of machine learning for construct-
ing the classiﬁcation function.
2.1 Feature Extraction
The coding habits of a programmer can manifest in a variety
of stylistic patterns. Consequently, methods for authorship
attribution need to extract an expressive set of features from
source code that serve as basis for inferring these patterns. In
the following, we discuss the major types of these features
and use the code sample in Figure 2 as a running example
throughout the paper.
1
2
3
4
5
6
7
int foo ( int a ) {
int b ;
if ( a < 2)
return 1;
// base case
b = foo ( a - 1) ; // r e c u r s i o n
return a * b ;
}
Figure 2: Exemplary code sample (see Figure 3, 5, and 6)
Layout features.
Individual preferences of a programmer
often manifest in the layout of the code and thus correspond-
ing features are a simple tool for characterizing coding style.
Examples for such features are the indentation, the form of
comments and the use of brackets. In Figure 2, for instance,
the indentation width is 2, comments are provided in C++
style, and curly braces are opened on the same line.
Layout features are trivial to forge, as they can be eas-
ily modiﬁed using tools for code formatting, such as GNU
indent. Moreover, many integrated development editors auto-
matically normalize source code, such that stylistic patterns
in the layout are uniﬁed.
Lexical features. A more advanced type of features can
be derived from the lexical analysis of source code. In this
analysis stage, the source code is partitioned into so-called
lexems, tokens that are matched against the terminal symbols
of the language grammar. These lexems give rise to a strong
480    28th USENIX Security Symposium
USENIX Association
From code to vectors. Most learning algorithms are de-
signed to operate on vectorial data and hence the ﬁrst step
for application of machine learning is the mapping of code
to a vector space using the extracted features. Formally, this
mapping can be expressed as φ : X −→ F = Rd where F
is a d dimensional vector space describing properties of the
extracted features. Different techniques can be applied for
constructing this map, which may include the computation
of speciﬁc metrics as well as generic embeddings of features
and their relations, such as a TF-IDF weighting [1, 9].
Surprisingly, the feature map φ introduces a non-trivial
hurdle for the construction of attacks. The map φ is usually
not bijective, that is, we can map a given source code x to a
feature space but are unable to automatically construct the
source code x(cid:48) for a given point φ (x(cid:48)). Similarly, it is dif-
ﬁcult to predict how a code transformation x (cid:55)→ x(cid:48) changes
the position in feature space φ (x) (cid:55)→ φ (x(cid:48)). We refer to this
problem as the problem-feature space dilemma and discuss
its implications in Section 3.
Multiclass classiﬁcation. Using a feature map φ, we can
apply machine learning for identifying the author of a source
code. Typically, this is done by training a multiclass classiﬁer
g : X −→ R|Y| that returns scores for all authors Y. An
attribution is obtained by simply computing
f (x) = arg max
y∈Y
gy(x).
This setting has different advantages: First, one can inves-
tigate all top-ranked authors. Second, one can interpret the
returned scores for determining the conﬁdence of an attribu-
tion. We make use of the latter property for guiding our attack
strategy and generating adversarial examples of source code
(see Section 5)
Different learning algorithms have been used for construct-
ing the multiclass classiﬁer g, as for example, support vector
machines [21], random forests [9], and recurrent neural net-
works [1, 4]. Attacking each of these learning algorithms
individually is a tedious task and thus we resort to a black-
box attack for misleading authorship attribution. This attack
does not require any knowledge of the employed learning
algorithm and operates with the output g(x) only. Conse-
quently, our approach is agnostic to the learning algorithm as
we demonstrate in the evaluation in Section 6.
Figure 3: Abstract syntax tree (AST) for code sample in Figure 2.
set of string-based features jointly covering keywords and
symbols. For example, in Figure 2, the frequency of the
lexem int is 3, while it is 2 for the lexem foo.
In contrast to code layout, lexical features cannot be eas-
ily manipulated, as they implicitly describe the syntax and
semantics of the source code. While the lexem foo in the
running example could be easily replaced by another string,
adapting the lexem int requires a more involved code trans-
formation that introduces a semantically equivalent data type.
We introduce such a transformation in Section 4.
Syntactic features. The use of syntax and control ﬂow also
reveals individual stylistic patterns of programmers. These
patterns are typically accessed using the abstract syntax tree
(AST), a basic data structure of compiler design [2]. As an
example, Figure 3 shows a simpliﬁed AST of the code snippet
from Figure 2. The AST provides the basis for constructing
an extensive set of syntactic features. These features can
range from the speciﬁc use of syntactic constructs, such as
unary and ternary operators, to generic features characterizing
the tree structure, such as the frequency of adjacent nodes. In
Figure 3, there exist 21 pairs of adjacent nodes including, for
example, (func foo)→(arg int) and (return)→(1).
Manipulating features derived from an AST is challenging,
as even minor tweaks in the tree structure can fundamentally
change the program semantics. As a consequence, transforma-
tions to the AST need to be carefully designed to preserve the
original semantics and to avoid unintentional side effects. For
example, removing the node pair (decl int)→(b) from the
AST in Figure 3 requires either replacing the type or the name
of the variable without interfering with the remaining code.
In practice, such transformations are often non-trivial and we
discuss the details of manipulating the AST in Section 4.
2.2 Machine Learning
The three feature types (layout, lexical, syntactic) provide
a broad view on the characteristics of source code and are
used by many attribution methods as the basis for applying
machine-learning techniques [e.g., 1, 4, 9, 21]
3 Misleading Authorship Attribution
With a basic understanding of authorship attribution, we are
ready to investigate the robustness of attribution methods and
to develop a corresponding black-box attack. To this end,
we ﬁrst deﬁne our threat model and attack scenario before
discussing technical details in the following sections.
USENIX Association
28th USENIX Security Symposium    481
func fooarg intabodydecl intbifoper <a2return1assignreturnacall foooper -a1oper *abfunc fooarg intacompdecl intbifoper <a2return1assignreturnbcall foooper -a1oper *ab3.1 Threat Model
For our attack, we assume an adversary who has black-box
access to an attribution method. That is, she can send an
arbitrary source code x to the method and retrieve the corre-
sponding prediction f (x) along with prediction scores g(x).
The training data, the extracted features, and the employed
learning algorithm, however, are unknown to the adversary,
and hence the attack can only be guided by iteratively probing
the attribution method and analyzing the returned prediction
scores. This setting resembles a classic black-box attack as
studied by Tram`er et al. [26] and Papernot et al. [19]. As
part of our threat model, we consider two types of attacks—
untargeted and targeted attacks—that require different capa-
bilities of the adversary and have distinct implications for the
involved programmers.
Untargeted attacks.
In this setting, the adversary tries
to mislead the attribution of source code by changing the
classiﬁcation into any other programmer. This attack is also
denoted as dodging [23] and impacts the correctness of the
attribution. As an example, a benign programmer might
use this attack strategy for concealing her identity before
publishing the source code of a program.
Targeted attacks. The adversary tries to change the classiﬁ-
cation into a chosen target programmer. This attack resembles
an impersonation and is technically more advanced, as we
need to transfer the stylistic patterns from one developer to
another. A targeted attack has more severe implications: A
malware developer, for instance, could systematically change
her source code to blame a benign developer.
Furthermore, we consider two scenarios for targeted at-
tacks: In the ﬁrst scenario, the adversary has no access to
source code from the target programmer and thus certain fea-
tures, such as variable names and custom types, can only be
guessed. In the second scenario, we assume that the adver-