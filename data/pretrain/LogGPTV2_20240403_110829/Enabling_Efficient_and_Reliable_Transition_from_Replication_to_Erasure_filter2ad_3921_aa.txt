title:Enabling Efficient and Reliable Transition from Replication to Erasure
Coding for Clustered File Systems
author:Runhui Li and
Yuchong Hu and
Patrick P. C. Lee
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Enabling Efﬁcient and Reliable Transition from Replication to Erasure Coding for
Clustered File Systems
Runhui Li, Yuchong Hu, Patrick P. C. Lee
Department of Computer Science and Engineering, The Chinese University of Hong Kong
{rhli, pclee}@cse.cuhk.edu.hk, PI:EMAIL
Abstract—To balance performance and storage efﬁciency,
modern clustered ﬁle systems (CFSes) often ﬁrst store data with
random replication (i.e., distributing replicas across randomly
selected nodes), followed by encoding the replicated data with
erasure coding. We argue that random replication, while being
commonly used, does not take into account erasure coding
and hence will raise both performance and availability issues
to the subsequent encoding operation. We propose encoding-
aware replication, which carefully places the replicas so as to
(i) avoid cross-rack downloads of data blocks during encoding,
(ii) preserve availability without data relocation after encoding,
and (iii) maintain load balancing as in random replication. We
implement encoding-aware replication on HDFS, and show
via testbed experiments that it achieves signiﬁcant encoding
throughput gains over random replication. We also show
via discrete-event simulations that encoding-aware replication
remains effective under various parameter choices in a large-
scale setting. We further show that encoding-aware replication
evenly distributes replicas as in random replication.
I. INTRODUCTION
Clustered ﬁle systems (CFSes) ensure data availability by
striping data with redundancy across different nodes in dif-
ferent racks. Two redundancy schemes are commonly used:
(i) replication, which creates identical replicas for each data
block, and (ii) erasure coding, which transforms original
data blocks into an expanded set of encoded blocks, such that
any subset with a sufﬁcient number of encoded blocks can
reconstruct the original data blocks. Replication improves
read performance by load-balancing read requests across
multiple replicas. On the other hand, erasure coding provably
achieves higher fault tolerance than replication, while using
much less redundancy [31]. For example, traditional designs
of CFSes deploy 3-way replication [5, 14, 28], which incurs
3× storage overhead. Azure reportedly uses erasure coding
to reduce the storage overhead to 1.33×, leading to over
50% of operational cost saving for storage [17].
Recent studies [12, 17, 27] demonstrate the feasibility of
adopting erasure coding in production CFSes. To balance the
trade-off between performance and storage efﬁciency, CFSes
often perform asynchronous encoding [12]: data blocks are
ﬁrst replicated when being stored, and are later encoded with
erasure coding in the background. Asynchronous encoding
maintains high read performance for new data via replication
and minimizes storage overhead for old data via erasure
coding. It simpliﬁes deployment and error handling, and
hides performance degradation [12].
In this paper, we argue that the encoding operation (i.e.,
transforming replicas to erasure-coded blocks) is subject to
both performance and availability challenges. First, it may
need to retrieve data blocks stored in different racks to
generate encoded blocks. This will consume a substantial
amount of bandwidth across racks. Cross-rack bandwidth
is considered to be a scarce resource in CFSes [6, 9],
and is often over-subscribed by many nodes [1, 15]. Thus,
intensive cross-rack data transfers will degrade the perfor-
mance of normal foreground operations. Second, relocation
of encoded blocks may be needed to ensure the availability
requirement (e.g., rack-level fault
tolerance) is fulﬁlled.
Although such relocation is rare in production [21], it is
still undesirable, since it not only introduces additional
cross-rack trafﬁc, but also leaves a vulnerable period before
relocation is done.
Our observation is that when data blocks are ﬁrst stored
with replication, replica placement plays a critical role in
determining both performance and availability of the sub-
sequent encoding operation. One replica placement policy
is random replication (RR) [7], whose idea is to store
replicas across randomly chosen nodes. RR is simple to
realize and has been used by HDFS [28], Azure [5], and the
DRAM-based storage system RAMCloud [22]. However, it
does not take into account the relations among the replicas
when encoding is performed. As we later show, RR brings
both performance and availability issues to the subsequent
encoding operation.
To this end, we propose encoding-aware replication
(EAR), which carefully determines the replica placements
of the data blocks that will later be encoded. The main
idea of EAR is that for each group of data blocks to be
encoded together, EAR keeps one replica of each data block
in the same rack, while storing the remaining replicas in
other racks by equivalently solving a maximum matching
problem. By doing so, EAR avoids downloading data blocks
from other racks for encoding, and avoids relocation of
encoded blocks after encoding. Thus, EAR reduces cross-
rack trafﬁc due to the encoding operation. In addition, EAR
tries to randomly distribute replicas as in RR to maintain
load balancing.
In summary, we make the following contributions:
• We present EAR, a new replica placement algorithm
that addresses both performance and availability issues
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.24
DOI 10.1109/DSN.2015.24
148
148
of the encoding operation.
• We implement EAR on Facebook’s HDFS implemen-
tation [11], with only few modiﬁcations to the source
code of HDFS.
• We conduct testbed experiments on a 13-machine clus-
ter. We observe signiﬁcant encoding throughput gains
of EAR over RR in different settings, and the gains can
reach over 100% in some cases. Also, EAR improves
write throughput by reducing network trafﬁc. Further-
more, based on synthetic MapReduce workloads, we
ﬁnd that the replica placement of EAR does not com-
promise performance before encoding.
• We
conduct discrete-event
simulations based on
CSIM 20 [8], and compare RR and EAR for various
parameter choices in a 400-node CFS. We show that
EAR can improve the encoding throughput of RR by
70% in many cases.
• We examine the replica distribution of EAR, and show
that it maintains load balancing in storage and read
requests as in RR.
The rest of the paper proceeds as follows. Section II
presents the problem setting and issues of RR. Section III
describes the design of EAR. Section IV presents the im-
plementation details of EAR on HDFS. Section V presents
our evaluation results. Section VI reviews related work, and
ﬁnally Section VII concludes the paper.
II. PROBLEM
In this section, we formalize the scope of the encoding
problem. We also motivate our work via an example.
A. System Model
Clustered ﬁle system (CFS) architecture: We consider
a CFS architecture, as shown in Figure 1, that stores ﬁles
over multiple storage nodes (or servers). We group the nodes
into racks (let R be the number of racks), such that different
nodes within the same rack are connected via the same top-
of-rack switch, while different racks are connected via a
network core. Cross-rack bandwidth is a scarce resource
[6, 9] and often over-subscribed [1, 15], so we assume that
cross-rack data transfer is the performance bottleneck in a
CFS architecture. We consider a CFS that uses append-only
writes and stores ﬁles as a collection of ﬁxed-size blocks,
which form the basic read/write data units. Examples of such
a CFS includes GFS [14], HDFS [28], and Azure [5].
We motivate our study by examining the open-source
HDFS implementation by Facebook [11], which supports
erasure-coded storage based on HDFS-RAID [16]. Never-
theless, our discussion can be generalized for other CFSes.
Replication: Traditional CFS designs use r-way replica-
tion by storing r replicas for each block in different nodes,
where r = 3 is commonly used [5, 14, 28]. One common
replica placement policy is collectively called random repli-
cation (RR) [7], which is used by HDFS [28], Azure [5],
149149
Network
Core
Node
Rack 1
Rack 2
...
...
...
Rack R
Figure 1. Example of a CFS architecture.
and RAMCloud [22]. While the implementation of RR may
slightly vary across different CFSes, the main idea of RR
is to place replicas across randomly chosen nodes and racks
for load balancing, and meanwhile ensure node-level and
rack-level fault tolerance. In this paper, we assume that RR
follows the default replica placement policy of HDFS [28]:
it uses 3-way replication, such that the ﬁrst replica is placed
on a node in a randomly chosen rack and the two other
replicas are replaced on different randomly chosen nodes
in a different rack. This protects against either a two-node
failure or a single-rack failure.
Erasure coding: Erasure coding is a redundancy al-
ternative that provably incurs less storage overhead than
replication under the same fault tolerance [31]. We consider
(n, k) erasure coding deﬁned by two parameters n and k
(where k < n). It transforms k original uncoded blocks
(which we call data blocks) to create n − k additional coded
blocks (which we call parity blocks), such that any k out of
the n data and parity blocks can reconstruct all k original
data blocks. We call the collection of n data and parity
blocks to be a stripe, and typical erasure coding schemes
operate on each stripe independently. We assume systematic
erasure coding, which keeps the k data blocks in a stripe.
Examples of erasure coding schemes include Reed-Solomon
codes [26] and Cauchy Reed-Solomon codes [3].
Asynchronous encoding: Erasure-coded data is usually
generated asynchronously in the background (i.e., off the
write path) [12, 17, 27],
in which all blocks are ﬁrst
replicated when being written to a CFS, and the CFS later
transforms the replicas into erasure-coded data. We call the
transformation from replicas to erasure-coded data to be the
encoding operation. The CFS randomly selects a node to
perform the encoding operation for a stripe. The encoding
operation comprises three steps: (i) the node downloads one
replica of each of the k data blocks; (ii) it transforms the
downloaded blocks into n − k parity blocks and uploads the
parity blocks to other nodes; and (iii) it keeps one replica
of each data block and deletes other replicas.
Facebook’s HDFS implementation [11] performs asyn-
chronous encoding via a map-only MapReduce job, in which
multiple map tasks run on different nodes simultaneously,
and each map task performs encoding for a subset of stripes.
Thus, the encoding operation is parallelized at the stripe
level. We provide more implementation details in Section IV.
Blocks: 1
2 3 4
B. Issues of Random Replication (RR)
We elaborate how RR potentially harms both perfor-
mance and availability of the subsequent encoding opera-
tion. First, encoding may incur a lot of cross-rack trafﬁc.
Facebook’s HDFS computes parity blocks for each stripe
by downloading and encoding a group of k data blocks
from HDFS. However, if the blocks are randomly placed
during replication,
the encoding operation may have to
download data blocks from different racks. Second, encoding
may require block relocation to fulﬁll the fault-tolerance
requirement. For example, Facebook’s HDFS distributes n
blocks of each stripe across n racks to tolerate n − k rack
failures [21] (and we verify this feature in Facebook’s HDFS
implementation [11]). It periodically checks for the stripes
that violate the rack-level fault tolerance requirement (using
the PlacementMonitor module), and relocates the blocks if
needed (using the BlockMover module). We emphasize that
block relocation is rare in production CFSes [21], but if it
happens, it introduces additional cross-rack trafﬁc. It also
leaves a vulnerable period before relocation is completed.
We illustrate the issues of RR via a motivating example.
Consider a CFS with 30 nodes evenly grouped into ﬁve racks
(i.e., six nodes per rack). Suppose that the CFS writes four
blocks, denoted by Blocks 1, 2, 3, and 4, with the default
3-way replication. It then encodes the ﬁle with (5, 4) erasure
coding, such that the erasure-coded stripe can tolerate a
single-node failure or a single-rack failure. Figure 2(a) shows
a possible replica layout of the four data blocks with RR
and the subsequent encoding operation. To encode the four
data blocks, suppose that a node in Rack 3 is chosen for
performing the encoding operation. The chosen node can
download Blocks 2, 3, and 4 from other nodes within the
same rack, but it needs to download Block 1 from either
Rack 1 or Rack 2 to compute parity block P . We call the
cross-rack transfer of a data block a cross-rack download.
We can check that even if we choose a node in another rack,
we cannot avoid a cross-rack download.
We further show via simple analysis that it is almost
inevitable to have cross-rack downloads in the encoding
operation. Suppose that RR uses 3-way replication and
places the replicas of each data block in two randomly
chosen racks. Thus, the probability that Rack i (1 ≤ i ≤ R)
contains a replica of a particular data block is 2
R . Given that
the replicas of k data blocks to be encoded into a stripe
are placed in the same way, the expected number of data
blocks stored in Rack i is 2×k
R . If we pick a random node
to perform encoding, the expected number of data blocks to
be downloaded from different racks is k − 2×k
R , which is
almost k if R is large.
The same example also shows the availability issue. After
we remove the remaining replicas (i.e., those crossed away
1
3 3
1
1 2
4
4
2 2
3
4
P
4
Rack 1
Rack 2
Rack 3
Rack 4
Rack 5
Relocation!!
(a) Random replication (RR)
Blocks: 1 2 3 4
4
1 1
3
4
3
2
1
3
4
2
2
P
Rack 1
Rack 2
Rack 3
Rack 4
Rack 5
(b) Encoding-aware replication (EAR)
Figure 2. Encoding of four data blocks under RR and EAR.
in Figure 2(a)), the failure of Rack 2 will result in data
loss. Either Block 2 or Block 4 in Rack 2 needs to be
relocated to Rack 5 to provide single-rack fault tolerance.
Such an availability issue is less likely to occur if R is larger,
because k data blocks are more likely to be scattered across
k different racks, yet it remains possible.
takes into account
To summarize, this example shows that RR potentially
harms performance (i.e., a data block is downloaded from
a different rack) and availability (i.e., blocks need to be
relocated). The primary reason is that the replica layout of
each data block is independently determined, while the data
blocks are actually related when they are encoded together.
This motivates us to explore a different replica placement
policy that
the subsequent encoding
operation. Figure 2(b) provides insights into the potential
gain of the revised replica placement policy, which we call
encoding-aware replication (EAR). When the CFS writes
the four data blocks with 3-way replication, we always keep
one replica in one of the racks (Rack 3 in this case). Thus, if
we choose a node in Rack 3 to perform encoding, we avoid
any cross-rack download. Also, after encoding, the erasure-
coded stripe provides single-rack fault tolerance without the
need of relocation. We elaborate the design of EAR in
Section III.
III. DESIGN
In this section, we present the design of EAR. EAR
imposes constraints on replica placement, so as to address
both performance and availability issues of the encoding