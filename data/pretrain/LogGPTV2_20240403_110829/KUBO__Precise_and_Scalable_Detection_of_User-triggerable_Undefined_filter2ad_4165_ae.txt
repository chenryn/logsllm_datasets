6
7
8
9
10
11
12
13
Modules
block
fs
sound
kernel
drivers:inﬁniband
drivers:media
drivers:rapidio
drivers:staging:gasket
drivers:misc
drivers:soc
drivers:message
drivers:usb
drivers:video
Testable?
y
y
y
y
y
n
n
n
n
n
n
y
y
Why (not)?
built-in
built-in
built-in
built-in
virtual driver
AM4x Cortex-A9 speciﬁc
physical device required
physical device required
physical device required
ASpeed BMC SoC speciﬁc
physical device required
usbfuzzer
virtual driver
TABLE VII: Summarization of testability of syzkaller for all modules
where KUBO found bugs. 7 modules are testable while other 6 are not.
For testable modules, built-in means this module can be tested in the
default setting; virtual driver means it’s a driver for software e.g. rxe
for inﬁniband; usbfuzzer means syzkaller emulated usb stack, thus
making usb driver testable. For untestable modules, it can be either it’s
unique to certain chip e.g. 6 and 10, or a physical device is required.
Modules
block
fs
sound
kernel
drivers:inﬁniband
drivers:usb
drivers:video
Found UB
No
No
shift
shift
s*
s*
No
No
s+
snd_timer_user_ccallback
ext4_ﬁll_super
yura_hash
__ntfs_write_inode
NA
NA
NA
NA
__v4l2_ﬁnd_nearest_size
post-bug check
buggy function
Why not?
True bug?
NA
NA
n
n
y
n
NA
NA
n
post-bug check
post-bug check
non-reproducible
NA
NA
NA
NA
NA
syzkaller’s emulation of the hardware stack e.g. usbfuzzer [8] or
2) testing a virtual driver (driver communicating with software).
2) Syzkaller reported bugs: We ran each instance of
syzkaller testing individual kernel module for 48 hours and
the UBSan reports are summarized in Table VIII. As shown
in this table, ﬁve UBs were detected in total across 3 different
kernel modules. However, after manual validation, four of them
were determined to be false bugs. For these four false bugs,
three of them are because they are checked right after the UB
takes place, another one of them is because it was decided as
un-reproducible by syzkaller.
There is one true bug in yura_hash, as shown in the ﬁgure
below, which is a signed overﬂow. However, according to
our previous communication with the developers, all signed-
overﬂow are converted to 2’s complement wrap-around in the
kernel module, making this overﬂow purely benign. Question
has been submitted to the developer in regard to whether this
overﬂow is harmful or not.
TABLE VIII: UB Bugs found by running syzkaller for each testable
modules for 48 hours. 5 UB bugs were reported for these 7 modules.
For each found bug, we recorded their UB type, the corresponding
function, whether if it’s a true bug, and the reason why if we validate
it to be a false bug.
it actually happens. This challenges all UB detectors to not
only ensure the UB is triggerable but also to go beyond the
UB instruction to make sure the triggered UB is unhandled and
can cause unintended consequences. For discussion of a better
post-bug analysis, please see Section III-G. In our experiment,
if without post bug analysis, the false detection rate will be
increased from 27.5% to 68.3%.
C. Comparing with syzkaller
1) Setting up syzkaller: In this experiment of comparing
with syzkaller, the kernel image of the same version where
KUBO found unknown bugs was compiled with the default rec-
ommended conﬁguration from syzkaller and additional UBSan
instrumentation enabled. For each subsystem where KUBO has
found bugs, we launched a syzkaller instance where only the
relevant syscalls are enabled so that syzkaller can focus on that
speciﬁc subsystem e.g., for sound subsystem, we only enabled
syscalls speciﬁed in dev_snd_seq.txt and other related syscalls
description ﬁles.
There are two exceptions for the above procedure, one
is for the instance launched for kernel subsystem where we
did not specify any syscall, so every syscall can be tested;
Another one is for the device drivers since most of the device
drivers are not testable in the syzkaller VM as they need to
communicate with a real physical device; Worse still some of
the device drivers are unique to certain chip or architecture,
thus cannot be compiled into the kernel image on a regular
server environment. We summarized the testability situation
in Table VII. The untestability problem is solved either by 1)
11
Based on the above reported results, KUBO has two ad-
vantages over syzkaller: 1) directly applicable to (any) kernel
code despite the lack of customized hardware. It is also worth
noting that it takes non-trivial effort to set up syzkaller to test
against different drivers, for example, conﬁguring the kernel
with driver-speciﬁc KConﬁg entries and installing extra user-
space libraries for virtual drivers e.g., RDMA Core Userspace
Libraries for rxe driver are usually required. and 2) lower false
detection rates mainly thanks to post-bug analysis.
However, we note that there are bugs easily found by
syzkaller but difﬁcult for a static analysis tool (e.g., the bug
in yura_hash requires unrolling the loop for at least 10 times).
This makes KUBO and the dynamic testing approach a good
complement for each other.
D. Triaging efforts
Since static analysis reports usually take a huge amount
of manual efforts to triage, in this section, we measure the
time spent on manually verifying the bug reports. We invited
a graduate student to validate all the bugs detected by KUBO.
In this process, the validator needs to check if the bug can
be triggered via user-controlled data, and the triggered UB
has a real system impact. Thanks to the modest number of
reports, and well-marked attack vector, it only costs the student
5 hours to validate all the reports. Compared to the traditional
symbolic execution based static approach like KINT, where two
bug review marathons were used to validate only 0.6% of the
125,172 generated reports, we believe our userspace focused
detection approach is more actionable and usable in practice.
Out of the 23 submitted reports, despite the fact that we
were unable to provide a PoC program to cause any sensible
consequence. Only via the description of the data sources
and the UB found by KUBO, the developers were able to
1u32yura_hash(constsignedchar*msg,intlen)2{3intj,pow;4u32a,c;5inti;6for(pow=1,i=1;i<len;i++)7pow=pow*10;//tooverflowthis,thisloop8//shouldbeunrolledatleast10times.9...is generally required. Last but not least, the data sent from
the device to the driver can be pre-processed by the device
ﬁrmware, this requires the analysis tool to understand the full
software stack including the ﬁrmware, in order to complete the
constraints. Given that there has been works studying the driver-
device interaction like PeriScope [32] which tries to identify
the untrusted input from the devices, it would be interesting to
see how to incorporate these two lines of works to facilitate the
detection of more bugs in the kernel, we leave this to future
exploration.
B. Improve for less false alarms
As we can see from the evaluation § V-A1, 80% of
KUBO’s false positives are caused by the incomplete post-
bug analysis due to its limitation of being intra-procedural.
In addition, understanding the semantics at a UB sink site is
also important in order to precisely screen the non-impactful
UBs, since ultimately it is the developer’s intention that needs
to be understood by the analysis [33], [29], [10]. And such
an intention is expressed in various ways and can be hard to
enumerate.
Existing works [29], [31], [33] trying to distinguish between
harmful UBs and benign ones generally hinge on the idea
that the undeﬁned/overﬂowed values being used in sensitive
functions is deﬁnitely not intended by the developer. This poses
advantages for false alarms but is theoretically unsound in the
sense that sensitive functions are hard to enumerate and the
sinks (UB impacting site) apart from the sensitive functions
can be also critical. KUBO takes the ﬁrst step (towards a
generic approach) to model the constraints of the UB’s use sites,
however, the post-bug analysis is still bounded by being intra-
procedural for performance concerns. As a result, understanding
more about the possible ways of how an undeﬁned value can be
used and a (possibly) efﬁcient inter-procedural analysis to locate
the dangerous sinks can be interesting for future exploration
and warrants a more extensive study.
V I I . R E L AT E D W O R K
A. Undeﬁned behavior detection/ﬁxing
Previous works reason very well about why undeﬁned
behavior exists and the conditions to trigger each of them. Wang
et al. [36] study the causes and consequences of all kinds of
undeﬁned behaviors. STACK [38] [39] investigates the unstable
code which might be optimized out due to the compiler’s
false assumption of all user’s programs being well-deﬁned.
D’Silva et al. [18] studied UB, among other issues, introduced
by compiler optimizations. Chris et al. [21] systematically study
the causes and consequences of UB in C and propose extra
semantics to handle the undeﬁnedness in C. Lee et al. [22]
tries to address the undeﬁned behavior in the design of LLVM
IR instruction set by introducing new instruction.
This line of works, in terms of efforts to detect UB, typically
uses limited intra-procedural or local analysis and does not
consider UB triggerability by user inputs. As a result, they
usually report overwhelmingly high volumes of bug reports
and false positives, which are extremely difﬁcult to vet in
practice. As for the UB ﬁxing methods, they generally rely on
new security features implemented through program rewriting.
These techniques are generally not applicable to programs as
Fig. 8: Total time spent in each analysis phase. Although processing
time varies between different modules due to their varying complexity,
and all tasks were put into a thread pool, most of the modules can be
ﬁnished within 16 hours (0.5h+4m+15h) (blue bar), a few extremely
complex modules take much longer with the worse case scenario being
33 hours (2h+6m+30h) (red bar) detailed in § V-E.
quickly acknowledged 14 of them, thanks to the inherent
security implications that they can be directly triggered by
user-controlled data. The average turn-around time is 44 hours.
E. Performance
In this subsection, we report the time cost in each analysis
stage, the result is presented in Figure 8. Note that, for each
analysis stage, instead of analyzing each module one by one,
all tasks were put into a thread pool, and we measured the time
of an individual module from the moment it entered the thread
pool until it ﬁnished.
For taint analysis, most of the subsystems take half an hour
to generate the per-function taint summary while drivers/net
drivers/inﬁniband drivers/gpu and drivers/net take about 2 hours.
The call graph analysis takes all modules for less than 6
minutes.
As for KUBO, the main analysis, 161 subsystems are ﬁnished
within the ﬁrst 15 hours since the analysis starts. Only 5 subsys-
tems take extra 15 hours due to the high volume of annotated
sanitizers and complexity of the codebase, these subsystems are
fs, drivers:video, drivers:gpu, net and drivers:scsi. In conclusion,
KUBO can produce high-quality bug reports without introducing
too much overhead, the whole analysis can be ﬁnished in a
reasonable human time.
V I . D I S C U S S I O N & F U T U R E W O R K
A. Model hardware input
KUBO does not handle hardware input sources at
the
moment, and this contributes to 5 of the missing bugs out
of Ssurvey. Hence, we identiﬁed several unique challenges to
model hardware inputs.
First of all, hardware interfaces are much more diverse due
to various hardware speciﬁcations (e.g., DMA, MMIO). Thus
when identifying untrusted input coming from hardware, it
usually needs to be analyzed case by case. Also, the data from
the device are often structurized thus a ﬁeld-sensitive analysis
12
0.50.061520.13005101520253035Taint analysisCall Graph AnalysisKUBO main analysisHoursPerformance of each analysis stage for different modules95% of the modules5% of the moduleslarge and complex as the OS kernel. In comparison, KUBO
focuses on detecting critical UB triggered by userspace input
with a much lower false detection rate, thanks to its scalable
and accurate data and call chain analysis.
B. Integer overﬂow (IO) detection
Integer overﬂow/error is a common type of UB, which
attracts a lot of research attention. KINT [37] is a static
integer integrity checker applicable to the Linux kernel. It
relies on range analysis and user-annotated taint input to infer
if an integer can overﬂow. In comparison, KUBO expands the
problem scope to all types of UBs. Moreover, unlike KINT,
whose context-insensitive range analysis and intra-procedural
symbolic execution together result in extremely high false
detection rate, KUBO achieves scalability i.e. ﬁnish analysis
within 33 hours, to the entire Linux kernel without sacriﬁcing
much the accuracy i.e. FP rate is 27.5%.
SIFT [23] generates input ﬁlters for programs to prevent
integer errors during runtime. However, its approach is not
directly applicable to OS kernels or complex software whose
input channels are numerous and diverse. DIODE [31] uses a
targeted branch enforcement strategy to ﬁnd a path that can
trigger IO at memory allocation sites. Despite being relatively