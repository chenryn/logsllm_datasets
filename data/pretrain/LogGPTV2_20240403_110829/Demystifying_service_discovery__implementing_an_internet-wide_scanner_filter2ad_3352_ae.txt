2.7M
6.8
4.6
1.8M
3.7
1.5M
2.3
929K
287K
0.7
39.6M
Table 7: Summary of ﬁngerprinted devices.
OS Class
Windows
Linux
BSD/Unix
Mac
Found % of GP
16.3M
13.0M
2.2M
862K
50.2
40.2
6.7
2.7
Table 8: General purpose (GP) devices.
lays in the Internet. To make the technique more useful, we
processed almost 7K responsive hosts at a large university
to manually verify and increase the database to 98 signa-
tures, including the latest Windows versions (e.g., Vista, 7,
Server 2008, Server 2003 SP2), webcams, switches, printers,
and various other devices.
We applied the modiﬁed Snacktime technique to HTTP2,
which consisted of |O| = 44.3M web servers that responded
with at least one SYN-ACK on port 80. We achieved a
Snacktime ﬁngerprint for 39.6M hosts, with 2.3M being ex-
cluded due to insuﬃcient retransmissions (i.e., none) and the
remaining diﬀerence attributable to gaps in our signature
database (the algorithm, signatures, and dataset are avail-
able [21]). According to Snacktime, the top 5 proﬁled OSes
are given in Table 6, with Linux contributing 32.9% of the to-
tal and various Windows implementations consisting of the
next several slots, which is indicative of their co-dominance
in the web-server market. We provide more detail in Table
7, where we classiﬁed each signature into one of six cate-
gories and calculated summary statistics. Note that general-
purpose systems (e.g., Linux, Windows) are responsible for
nearly 82% of the total, with network devices (e.g., switches,
routers, NAT boxes), networked storage (e.g., NAS, tape
drives), and printers showing up with more than 1M devices
each. The media category is comprised mainly of webcams
and presentation devices (e.g., TVs, DVRs, projectors).
To ﬁnish this section, we present in Table 8 the total num-
ber of devices and their percentage attributed to each class
of OS in the general-purpose category. Snacktime results
suggest that approximately half of the total consists of Mi-
crosoft OSes (5.6% of which belong to Windows 2000 or
older), which is likely due at least partially to individuals
hosting personal web-sites on their home machines. It also
shows that Linux hosts are responsible for 40%, which com-
bined with the various related forms of BSD (e.g., OpenBSD,
FreeBSD), SunOS, and Unix results in nearly 47% of the to-
tal and rivals Microsoft.
4.6 Service Lifetime
Another interesting property of Internet services is their
average lifetime (uptime) E[L], which is the mean duration
of time a port stays active on a given IP. One technique [17]
is to ﬁrst estimate the CDF of lifetime L and then compute
its mean E[L]. However, avoiding round-oﬀ errors and CDF
118Service
DNS
Echo
Ping
HTTP
SMTP
EPMAP
Total
7
1
1
8
2
2
21
Scans Emails Avg
6.4
22
4
3.4
3
1
45
22
4
27
6
2
106
5.05
IPs excluded
3.7M
752K
1K
459K
262K
65K
5.3M
Avg
530K
752K
1K
57K
131K
32K
250K
Source
Cease
FYI
Total
Human
Script Human
Script
Individual
Government
Corporation
University
Total
14
6
11
6
37
14
2
5
3
24
7
6
8
10
31
8
2
0
4
14
43
16
24
23
106
Table 10: Email notices by complainant type.
Table 9: Emails and IPs excluded by service.
tail cut-oﬀ often requires monitoring the pool of target IPs
at frequent intervals (i.e., minutes) and for extended periods
of time (i.e., days), all which contributes not only to higher
bandwidth overhead, but also to more likely aggravation of
remote network administrators.
We oﬀer an alternative method that can estimate E[L]
using much lower overhead and overall delay. Modeling
each host as an alternating ON/OFF process [56], a set
K of uniformly selected live hosts exhibits a departure rate
λ = |K|/E[L] hosts/sec (a similar result follows from Little’s
Theorem). Thus, by probing K twice at time t and t + ∆,
one can estimate λ as p(∆)|K|/∆, where p(∆) is the frac-
tion of hosts that have disappeared in this interval. Solving
p(∆)|K|/∆ = |K|/E[L], we obtain E[L] = ∆/p(∆).
The key to this technique is to uniformly randomly se-
lect K and simultaneously ensure maximal politeness of the
scan. Leveraging the ﬁndings of Section 3.4 aimed exactly at
this issue, we ﬁrst use RLCG to scan the Internet for ∆ time
units at some constant rate r. We then re-generate the same
sequence of IPs at the same rate, but actually send packets
only to those targets that have responded in the ﬁrst scan.
Due to limited space, we omit simulations conﬁrming the
accuracy of this method and discuss only one extrapolation
using port 80 and ∆ = 45 seconds. This experiment cov-
ered 1M targets, found |K| = 23.7K live hosts, and yielded
E[L] = 50 minutes (i.e., p(∆) = 1.5%).
5. ANALYSIS
While it would be ideal to scan the Internet using dif-
ferent techniques (e.g., IP-sequential, uniform, GIW) and
then assess the collected feedback as a measure of intru-
siveness of each scan, certain practical limitations typically
prevent one from doing so (e.g., our network administra-
tors have explicitly prohibited scanning activity using cer-
tain non-optimal permutations). Thus, comparison is often
only possible through feedback analysis exposed in publi-
cations, which unfortunately is very scarce in the existing
literature. To overcome this limitation, this section intro-
duces a number of novel metrics related to the perceived
intrusiveness of Internet-wide scans, studies them in detail,
and unveils certain simple, yet eﬀective, techniques for re-
ducing the blowback.
5.1 Email Complaints
One of the uncertainties we encountered when initially
considering a service discovery project was the number of
complaints to expect, particularly as they related to serious
threats or resulted in widespread blacklisting of the scanner
to the point of making Internet-wide measurements impos-
sible.
In this section, we attempt to clarify the issue by
detailing the complaints we received and the eﬀect they had
on our measurements.
Table 9 contains a summary of email complaints broken
down by service type. Over all 21 scans, we received 106
complaints for an average of 5.05 per scan. Our initial run
(i.e., DNS1) resulted in 10 complaints and more than 2.5M
IP addresses blocked, which is nearly half the total of 5.3M
blacklisted over the course of the project. Most of this initial
number came from a single large ISP asking us to block sev-
eral /16 residential networks. However, even with the initial
burst removed from the calculation, DNS scans resulted in
an average of 172K blacklisted IPs per scan. The most sig-
niﬁcant backlash we received was for the ECHO scan (UDP
port 7), which led to 22 complaints and more than 750K
blocked IP addresses.
In the next section we provide an
explanation for this signiﬁcant increase, but note here that
UDP scans account for 65% of all complaints, while being
responsible for only 40% of the packets sent.
In contrast to the experience of [17], where the authors
received 30 times more complaints for a TCP scan than
ICMP pings, our TCP measurements produced a total of
35 complaints over 12 scans, or about three per scan. This
is an even more remarkable result given that we scanned
two sensitive ports, used ACK packets that penetrate state-
less ﬁrewalls, and clustered six scans in less than a month.
While we cannot explain this discrepancy, our numbers do
not support the notion that TCP scans are more invasive
than the other protocols.
We next categorize the received emails in Table 10 to show
the severity and type of each complaint. Out of 106 com-
plaints, 61 were demands to cease the activity, while the
other 45 were FYI notiﬁcations about a possible virus with
no expectation that the measurement stop. The ﬁrst row of
the table shows that individual users who monitor a single IP
address with a personal ﬁrewall (e.g., ZoneAlarm, Norton)
represented 41% of the total complaints (i.e., 43 out of 106),
which indicates that a large portion of these emails cannot
be avoided by any means. The remaining three rows of the
table represent complaints received from large network enti-
ties, with universities being the most likely to send an FYI
notiﬁcation and worldwide government entities comprising
only 15% of the total complaints.
In contrast to [13], we received only four cease demands
from U.S. Federal Government entities, none of which were
defense-related. Another point of interest is the number of
threats to pursue legal action, though of the three received
none of them turned out to be legitimate. Finally, analysis
of emails generated by an automated script suggests that a
large chunk of all received complaints (i.e., 36% in our case)
are seldom reviewed by an actual human given the large
amount of background scan traﬃc their networks receive
[41].
We now determine the impact of email complaints on
the scope of subsequent measurements (i.e., size of S af-
ter removing blacklisted networks) by studying the progres-
1196
x 10
6
5
4
3
2
1
d
e
k
c
o
b
l
s
P
I
−3
x 10
3
2
1
P
G
B
f
o
n
o
i
t
c
a
r
f
4
x 10
10
s
t
r
o
p
e
r
7.5
5
2.5
4
x 10
10
s
t
r
o
p
e
r
7.5
5
2.5
0
0
5
10
scan #
(a) total
15
20
0
0
5
10
scan #
15
20
(b) fraction of BGP
0
1
9
17
day
25
1
0
20
28
5
day
13
20
(a) HTTP (July 08)
(b) EPMAP (July-Aug 08)
Figure 4: Progression of blacklisted IPs.
sion of blacklisted IP addresses in Figure 4, where scans
are assigned numbers in chronological order. Note that the
complaints for the two simultaneous scans (i.e., SMTP and
EPMAP) are encompassed in a single data point due to our
inability to tell whether the SYN or the ACK portion caused
the complaint. Part (a) of the ﬁgure contains the raw num-
ber of blacklisted addresses, which did not increase signiﬁ-
cantly after we stopped scanning UDP. Part (b) shows the
blocked addresses as a percentage of BGP, where the total
number of 5.3M represents only 0.23% of the current space
(the curve is non-monotonic due to the constant expansion
of BGP).
5.2 Firewall Log Correlation
To gain a broader view of the Internet and decrease the
amount of time required to detect large-scale attacks, on-
line collaborative systems [36], [47] have been developed to
pool data from strategically placed Internet sensors and ﬁre-
wall/IDS logs of various networks. We focus on the SANS
Internet Storm Center (ISC) [47] due to its relatively large
size of 500K monitored IP addresses and detailed publicly
available data. An ISC report consists of an IP address
detected as a scanner, its source port, and the target’s (IP,
port) pair. These reports are often shared publicly, although
certain ﬁelds (e.g., destination IP) are obscured to protect
the identity of subnets that submit their logs. Given that
these reports represent information about unwanted traﬃc,
they can be used to gain insight into how our scans are per-