B.16 Prog Languages
B.17 Filename Edit Distance
B.18 Avg Text Coverage
B.19-22 CTS Time
B.23 Compile time Flags
B.24 Connect back
B.25 Dev time
C: Sandbox Features
C.1 Sandbox Only
C.2 Short Exec
C.4-6 Exec Time
C.7 Net Activity
C.7 Time Window
C.8 Num Crashes
D: Antivirus Features
D.1-3 Malicious Events
D.4-5 VT detection
D.6 VT Conﬁdence
D.7 Min VT detection
D.8 Max VT detection
D.9 AV Labels
E: Submitter Features
E.1 Num IPs
E.2 Num E-Mails
E.3 Accept Languages
The ID of the cluster
The number of samples in the cluster
An approximation of the cluster shape (GROUP—MIX—CHAIN)
Min, Max, Avg, and Variance of the samples ﬁlesize
Min, Max, Avg, and Variance of the number of sections
Min, Max, Avg, and Variance of the number of functions
Average number of different functions
Average number of different sections
One of: Data, Code, Both, None
List of programming languages used during the development
The Average edit distance of the samples’s ﬁlenames
Avg text coverage of the .text sections
Min, Max, Avg, and Variance of the difference between compile and the submission time
Booleans to ﬂag NULL or constant compile times
True if any ﬁle in the cluster contacts back the submitter’s /24 network
Average time between each submission (in seconds)
Numer of samples seen only by the sandbox (and not from external sources)
Number of samples terminating the analysis in less than 60s
Min, Max, and Avg execution time of the samples within the sandbox
The number of samples with network activity
Time difference between ﬁrst and last sample in the cluster (in days)
Number of samples crashing during their execution inside the sandbox
Min, Max, Avg numbers of behavioral ﬂags exibited by the samples
Average and Variance of VirusTotal detection of the samples in the cluster
Conﬁdence of the VirusTotal score
The score for the sample with the minimum VirusTotal Detection
The score for the sample with the maximum VirusTotal Detection
All the AV labels for the identiﬁed pieces of malware in the cluster
Number of unique IP addresses used by the submitter
Number of e-mail addresses used by the submitter
Accepted Languages from the submitter’s browser
Table 2: List of Features associated to each cluster
AUC
Det. Rate
False Pos.
Full data
10-folds Cross-Validation
70% Percentage Split
0.999
0.988
0.998
98.7%
97.4%
100%
0%
3.7%
11.1%
Table 3: Classiﬁcation accuracy, including detection and
false positive rates, and the Area Under the ROC Curve
(AUC)
number of machine learning techniques applied to the set
of features we presented in detail in the previous section.
Among the large number of machine learning algo-
rithms we have tested our training data with, we have
obtained the best results by using the logistic model
tree (LMT). LMT combines the logistic regression and
decision tree classiﬁers by building a decision tree whose
leaves have linear regression models [41].
5 Machine Learning
Machine learning provides a very powerful set of tech-
niques to conduct automated data analysis. As the goal
of this paper is to automatically distinguishing malware
developments from other submissions, we tested with a
Training Set
The most essential phase of machine learning is the train-
ing phase where the algorithm learns how to distinguish
the characteristics of different classes. The success of
the training phase strictly depends on a carefully pre-
pared labeled data.
If the labeled data is not prepared
USENIX Association  
24th USENIX Security Symposium  1063
7
carefully, the outcome of machine learning can be mis-
leading. To avoid this problem, we manually labeled a
number of clusters that were randomly chosen between
the ones created at the end of our analysis phase. Manual
labeling was carried out by an expert that performed a
manual static analysis of the binaries to identify the type
and objective of each modiﬁcation. With this manual ef-
fort, we ﬂagged 91 clusters as non-development and 66
as development. To estimate the accuracy of the LMT
classiﬁer, we conducted a 10-fold cross validation and a
70% percentage split evaluation on the training data.
Feature Selection
In the previous section, we have presented a comprehen-
sive set of features that we believe can be related to the
evolution of samples and to distinguish malware devel-
opments from ordinary malware samples. However, not
all the features contribute in the same way to the ﬁnal
classiﬁcation, and some works well only when used in
combination with other classes.
To ﬁnd the subset of features that achieves the opti-
mal classiﬁcation accuracy while helping us to obtain the
list of features that contribute the most to it, we lever-
aged a number of features selection algorithms that are
widely used in machine learning literature: Chi-Square,
Gain Ratio and Relief-F attribute evaluation. Chi-square
attribute evaluation computes the chi-square statistics of
each feature with respect to the class, which in our case
is the fact of being a malware development or not. The
Gain Ratio evaluation, on the other hand, evaluates the
effect of the feature by measuring its gain ratio. Fi-
nally, the Relief-F attribute evaluation methodology as-
signs particular weights to each feature according to how
much they are successful to distinguish the classes from
each other. This weight computation is based on the
comparison of the probabilities of two nearest neighbors
having the same class and the same feature value.
While the order slightly differs, the ten most effective
features for the accuracy of the classiﬁer for all three fea-
ture selection algorithms are the same. As also the com-
mon sense suggests, the features we extract from the bi-
nary similarity and the analysis of the samples are the
most successful. For example, the connect back feature
that checks if the sample connects back to the same IP
address of the submitter, the average edit distance of the
ﬁlenames of the samples, the binary function similar-
ity, and the sample compile time features are constantly
ranked on the top of the list. The submitter features and
the sandbox features are following the sample features in
the list. All of the features except the number of sand-
box evasions, the VirusTotal results, and the features we
extracted from the differences on the ﬁle sizes in the clus-
ters had a contribution to the accuracy. After removing
those features, we performed a number of experiments
on the training set to visualize the contribution of the
different feature sub-sets to the classiﬁcation accuracy.
Figure 1 shows (in log-scale) the impact of each class
and combination of classes. Among all the classes the
samples-based features produced the best combination
of detection and false positive rates (i.e. 88.2% detection
rate with 7.4% false positives).
In particular, the ones
based on the static and dynamic analysis of the binaries
seem to be the core of the detection ability of the sys-
tem.
Interestingly, the cluster-based features alone are
the worst between all sets, but they increase the accuracy
of the ﬁnal results when combined with other features.
The results of the ﬁnal classiﬁer are reported in Ta-
ble 3: 97.4% detection with of 3.7% false positives, ac-
cording to 10-folds cross validation experiment. Note
that we decided to tune the classiﬁer to favor detection
over false positives, since the goal of our system is only
to tag suspicious submissions that would still need to be
manually veriﬁed by a malware analyst.
6 Results
Our prototype implementation was able to collect sub-
stantial evidences related to a large number of malware
developments.
In total, our system ﬂagged as potential development
3038 clusters over a six years period. While this number
was too large for us to perform a manual veriﬁcation of
each case, if such a system would be deployed we es-
timate between two and three alerts generated per day.
Therefore, we believe our tool could be used as part of
an early warning mechanism to automatically collect in-
formation about suspicious submissions and report them
to human experts for further investigation.
In addition to the 157 clusters already manually la-
beled to prepare the training set for the machine learning
component, we also manually veriﬁed 20 random clus-
ters automatically ﬂagged as suspicious by our system.
Although according to the 10-fold cross validation exper-
iments the false positive rate is 3.7%, we have not found
any false positives on the clusters we randomly selected
for our manual validation.
Our system automatically detected the development of
a diversiﬁed group of real-world malware, ranging from
generic trojans to advanced rootkits. To better under-
stand the distribution of the different malware families,
we veriﬁed the AV labels assigned to each reported clus-
ter. According to them, 1474 clusters were classiﬁed as
malicious, out of which our system detected the develop-
ment of 45 botnets, 1082 trojans, 83 backdoors, 4 key-
loggers, 65 worms, and 21 malware development tools
(note that each development contained several different
samples modeling intermediate steps). A large fraction
1064  24th USENIX Security Symposium 
USENIX Association
8
128	
64	
32	
16	
8	
4	
2	
1	
  submi-er	
  (sub)	
sample	
  (samp)	
sandbox	
  (sand)	
cluster	
  (clus)	
Wrong	
  Classiﬁca;ons	
Detec;on	
  Rate	
False	
  Posi;ves	
sample+av	
sample+cluster+submi-er	
samp+clus+sub+sand_selected	
sample+cluster+av	
samp+clusr+sub+av	
sample+cluster+sandbox	
an;virus	
sample+submi-er	
sample+sandbox	
sample+cluster	
Figure 1: Classiﬁcation success of different feature combinations.
Campaign
Operation Aurora
Red October
APT1
Stuxnet
Beebus
LuckyCat
BrutePOS
NetTraveller
Paciﬁc PlugX
Pitty Tiger
Regin
Equation
Early Submission Time Before Public Disclosure
Submitted by












4 months
8 months
43 months
1 months
22 months
3 months
5 months
14 months
12 months
42 months
44 months
23 months
Germany
US
France
Romania
US
US
US
US
US
US
UK
US
Table 4: Popular campaigns of targeted attacks in the sandbox database
of the clusters that were not identiﬁed by the AV sig-
natures contained the development of probes, i.e., small
programs whose goal is only to collect and transmit in-
formation about the system where they run. Finally,
some clusters also contained the development or testing
of offensive tools, such as packers and binders.
6.1 Targeted Attacks Campaigns
Before looking at some of the malware development
cases detected by our system, we wanted to verify our
initial hypothesis that even very sophisticated malware
used in targeted attacks are often submitted to public
sandboxes months before the real attacks are discovered.
For this reason, we created a list of hashes of known and
famous APT campaigns, such as the ones used in op-
eration Aurora and Red October. In total, we collected
1271 MD5s belonging to twelve different campaigns. As
summarized in Table 4, in all cases we found at least one
sample in our database before the campaign was publicly
discovered (Early Submission column). For example, for
Red October the ﬁrst sample was submitted in February
2012, while the campaign was later detected in October
USENIX Association  
24th USENIX Security Symposium  1065
9
2012. The sample of Regin was collected a record 44
months before the public discovery.
Finally, we checked from whom those samples were
submitted to the system. Interestingly, several samples
were ﬁrst submitted by large US universities. A possi-
ble explanation is that those samples were automatically
collected as part of a network-based monitoring infras-
tructure maintained by security researchers. Other were
instead ﬁrst submitted by individual users (for whom
we do not have much information) from several differ-
ent countries, including US, France, Germany, UK, and
Romania. Even more interesting, some were ﬁrst sub-