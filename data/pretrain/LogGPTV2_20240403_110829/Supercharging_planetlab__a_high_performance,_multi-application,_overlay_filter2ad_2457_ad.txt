of the standard IO libraries. Users do need to link their applica-
tions against these libraries, but are otherwise unaffected. 
To allow outgoing TCP connections to be handled correctly, 
the system must implement Network Address Translation (NAT). 
This  is  handled  by  the  Management  Processor  in  the  Line  Card 
which  intercepts  outgoing  TCP  control  packets  and  maps  the 
source  port  number  to  an  available  value  from  a  pre-allocated 
range.  It  then  configures  tables  in  the  Line  Card  so  that  IP  ad-
dresses  and  port  numbers  are  appropriately  re-mapped  for  all 
subsequent  data  packets.  NAT  processing  is  also  performed  for 
other protocols that require it (e.g. ICMP echo packets). 
PLC
PLC
GNM myPLC
GNM myPLC
CP
CP
GRM
GRM
MP
MP
LC
LC
control net
control net
SC
SC
h
h
c
c
t
t
i
i
w
w
S
S
LNM
LNM
LRM
LRM
VSVSVS
VSVSVS
GPE
GPE
...
...
MP
MP
NPE
NPE
...
...
Figure 6. Control architecture showing global and local Node 
Managers (GNM, RNM) and Resource Managers 
(GRM, LRM). 
A slice that elects to use an NPE for its fast path must request 
an  NP  slice  from  the  Local  Resource  Manager  (LRM),  which 
forwards the request to the GRM. The request specifies the NPE 
required code  option  and  various  resource  parameters,  including 
the  number  of  required  filter  table  entries,  queues  and  packet 
buffers,  the  amount  of  SRAM  space  it  needs,  and  the  total  re-
served bandwidth it requires. The GRM selects the most appropri-
ate NPE to host the slice and returns its id to the LRM. The LRM 
then interacts with the MP to complete the initial configuration. 
Once the initial setup is complete, the application running in 
its vServer can perform additional configuration steps. In particu-
lar, it can request that UDP ports on any of the system’s external 
interfaces be configured to forward packets between the external 
interface  and  its  NPE  fast  path.  As  part  of  this  process  it  may 
request a specified share of the external interface bandwidth (both 
incoming and outgoing). Once a slice has a configured UDP port 
on a physical interface it can associate one or more of its fast path 
queues to a packet scheduler for that interface (in the NPE queue 
manager). It can also configure the lengths of individual queues 
and their individual shares of the interface bandwidth, as well as 
filters in the lookup table. In addition, it can write whatever slice-
specific  configuration  data  is  appropriate  in  its  SRAM  memory 
space. This is accomplished through a generic memory read/write 
mechanism provided by the NPE’s MP. All these control interac-
tions take place through the LRM, which serves as an intermedi-
ary between the vServers running on the GPE and the NPEs. 
As mentioned above, users login to their vServers to configure 
their application, in much the same way that they do on PlanetLab 
today.  However,  this  is  complicated  by  the  fact  that  there  are 
multiple  GPEs  and  each  user’s  session  must  be  directed  to  the 
appropriate GPE. In an ordinary PlanetLab node, the remote client 
opens a connection to the node’s SSH server, which goes through 
the  authentication  process,  and  if  that  succeeds,  forks  a  process 
running in the appropriate vServer, which transparently acquires 
the connection, along with its associated TCP kernel state. 
In  order  to  emulate  this  process,  without  requiring  major 
changes to the PlanetLab OS kernel, we redirect all incoming SSH 
connections to the CP (using filters in the Line Card), which does 
the login authentication. The ideal solution at this point would be 
to fork a process, and then migrate that process to the GPE host-
ing  the  correct  vServer.  However,  in  the  absence  of  a  general 
process  migration  mechanism,  we  have  chosen  instead  to  use  a 
relay process that runs in the CP on behalf of the slice. This relay 
process opens a second SSH connection to the SSH server on the 
selected GPE and forwards traffic at the application level. 
We recognize the obvious performance drawbacks of this ap-
proach, but consider them acceptable given the relatively limited 
amount  of  traffic  that  must  be  relayed  on  behalf  of  users’  login 
sessions. While there are alternative approaches that avoid appli-
cation level forwarding, they are not transparent to users, and we 
consider user transparency the more important consideration here. 
7. EVALUATION 
To  evaluate  the  system,  we  implemented  two  different  applica-
tions  and  studied  their  performance,  relative  to  conventional 
PlanetLab  implementations.  The  first  is  an  IPv4  router  that  we 
developed from scratch. The second is a port of the Internet Indi-
rection Infrastructure (I3). In this case, we restructured the system 
into  fast  path  and  slow  path  sections  and  mapped  the  fast  path 
onto an NPE.   
7.1. IPv4 router 
Our  IPv4  router  uses  the  NPE  to  implement  normal  packet  for-
warding  and  uses  a  vServer  running  on  one  of  the  GPEs  to  im-
plement control functions and exception handling. The TCAM in 
the NPE allows us to implement both conventional IP routing and 
general packet filtering, allowing arbitrary subsets of packets to be 
mapped to different queues. We compare the performance of the 
NPE-based forwarder to a forwarder using Click [KO00], running 
in a vServer. 
For the experiments reported here, the IPv4 router is config-
ured with five externally visible UDP ports, which are mapped to 
five different physical interfaces on the LC. The LC demultiplexes 
received packets based on their UDP port numbers and forwards 
packets for the router to the NPE. The NPE’s packet schedulers 
are rate controlled to limit their sending rate to 800 Mb/s. Figure 7 
shows the results from a basic operational test that demonstrates 
the  operation  of  all  the  major  subsystems  and  verifies  the  fair 
queueing  mechanisms.  Each  of  the  five  inputs  sends  traffic  into 
the router at times that are offset from one another. All the traffic 
is destined for the same output, and the traffic from each input is 
mapped to a different queue at that output, with each queue get-
ting a different share of the output bandwidth (the shares are 30%, 
25%,  20%,  15%  and  10%).  The  top  chart  shows  the  rates  from 
each input as solid lines and the output rates from each queue in 
incremental  form  as  dashed  lines  (the  incremental  form  means 
that  the  top  dashed  line  represents  the  sum  of  the  rates  coming 
Figure 8.  Basic IPv4 router throughput demonstration showing 
input rates (solid) and output rates (dashed) 
The  bandwidths  are  the  actual  link  bandwidths,  including  all 
overheads.  Note  that  the  chart  uses  a  log-log  scale.  We  observe 
that for 0 byte payloads, the maximum throughput that the Click 
router is able to achieve is under 50 Mb/s and that its performance 
deteriorates  dramatically  as  loads  increase  beyond  its  maximum 
capacity. The NPE-based router is able to keep up with the input 
up to a rate of 3.7 Gb/s. For larger payload sizes, the NPE router 
can sustain the full 5 Gb/s. For Click, the maximum packet proc-
essing  rate  is  about  59  Kp/s,  while  for  the  NPE  router,  it  ap-
proaches  4,800  Kp/s,  an  80  times  improvement.  The  highest 
throughput for the Click router is 540 Mb/s.  
Most of the Click results are for a single processor core, even 
though  the  server  blade  has  two  dual-core  processors.  We  also 
show results for 400 byte packets using all four cores, with traffic 
being distributed across four vServers, each running its own Click 
router. While one might expect this configuration to achieve four 
times  the  throughput  of  the  single  core,  in  fact  it  only  achieves 
roughly twice the throughput of the single core. There are several 
possible explanations for this deficiency. We think that the most 
likely explanation is that at high loads, a large share of the proc-
essing capacity is being used by the operating system, and there is 
insufficient parallelism in the OS to take full advantage of all four 
10,000
i
)
s
/
b
M
(
h
t
d
w
d
n
a
b
t
u
p
t
u
o
1,000
100
120, 400, 800, 1400
NPE
0
400/4 cores
1400
800
400
10
10
0
100
120
Click
1,000
10,000
input bandwidth (Mb/s)
Figure 9. Comparison of throughput for various payload sizes 
(add 98 bytes to include all overheads); most Click re-
sults are for single core; ones uses 4 cores.  
Figure 7. IPv4 router basic operational test showing bandwidth 
used at inputs and outputs (top) and queue lengths 
from  all  the  inputs).  The  bottom  chart  shows  the  lengths  of  the 
queues. Since the scheduler is work-conserving, the output rates 
change as new inputs turn on and  as queues drain. These charts 
are real-time performance measurements obtained by polling the 
statistics  counters  maintained  in  the  LC  and  NPE  and  accessed 
through the MP.  
Figure 8 shows a somewhat more complex scenario involving 
traffic to all five outputs. The five traffic sources send the same 
mix of traffic at all times. In each phase, one of the outputs re-
ceives 1.6 Gb/s of traffic, one receives no traffic and the remain-
ing three receive 800 Mb/s of traffic (with each output receiving 
an  equal  amount  of  traffic  from  each  of  the  five  inputs).  Each 
output accumulates a backlog during the period that its input rate 
is 1.6 Gb/s, and this backlog is cleared when the input traffic to 
that  output  turns  off.  The  chart  shows  the  input  rates  (in  incre-
mental form) as solid lines. The output rates (also in incremental 
form) are shown as dashed lines. Note that for a short period after 
the second and third input rate transitions, the output rate briefly 
rises  to  4  Gb/s  as  the  previously  accumulated  backlog  for  the 
newly idle output is forwarded, along with the traffic to the other 
four outputs. These periods show up as brief blips on the display, 
which is sampling the traffic counters every 200 ms. 
Figure 9 shows the results from a large collection of through-
put  measurements  on  both  the  NPE-based  router  and  the  Click 
router.  Both  routers  are  configured  with  five  externally  visible 
UDP ports mapped to five different physical interfaces, and input 
traffic is distributed uniformly across the five ports. The numbers 
labeling  the  curves  are  the  sizes  of  the  payloads  carried  by  the 
packets. The lengths of the frames carried on the external link are 
98 bytes longer (this includes two UDP/IP headers and Ethernet 
overhead,  including  preamble,  VLAN  tag  and  inter-packet  gap). 
1000
100
)
s
m
(
s
y
a
l
e
d
g
n
P
i
10
1
0.1
10
400/1
mean+
3(std. dev.)
1400/1
Click
400/4
NPE
1400
  400
100
1,000
input bandwidth (Mb/s)
10,000
Figure 10.  Comparison of latency for payloads of size 400 and 
1400; results for 1 and 4 cores for 400 byte payloads; 
mean+3(standard deviation) shown for 400/1 
cores.  Note  that  since  the  NP  blade  contains  two  independent 
NPEs, it can achieve a maximum packet processing rate of 9,600 
Kp/s,  while  the  server  blade  has  a  maximum  packet  processing 
rate  of  about  120  Kp/s,  when  using  all  four  cores  to  process 
minimum size packets. 
Figure  10  shows  the  results  of  a  series  of  latency  measure-
ments.  We  are  particularly  interested  in  understanding  how  the 
sharing of a component (GPE or NPE) among multiple PlanetLab 
slices  affects  the  latency.  Consequently,  we  configured  eight 
instances of the router application on an NPE and compared this 
with  the  eight  Click  routers  running  in  separate  vServers  on  a 
GPE. For each data point in Figure 10, the routers were supplied 
with a background traffic load with the total input rate and pay-
load sizes shown in the chart (with each of the eight routers re-
ceiving one eighth of the input traffic). We then sent ping packets 
through the loaded routers using separate logical interfaces (so the 
ping packets were not subjected to queueing delays in the routers). 
Each data point on the solid curves is the average of 2000 meas-
urements. We show results for 400 and 1400 byte payloads, and 
for the Click router, we show results using just a single core and 
using all four cores for the case of 400 byte payloads. For the 400 
byte case with a single core, we also show the mean plus 3 times 
the standard deviation of the delay, in order to show the variability 
of  the  delay.  These  data  indicate  that  there  is  a  non-negligible 
fraction of the traffic that experiences delays that are as much as 5 
times  the  mean  delay.  For  the  NPE-based  routers,  the  average 
round-trip  ping  delays  never  exceeded  0.2  ms  and  the  standard 
deviation was generally a small fraction of the mean. In the NPE 
case,  the  ping  traffic  shares  queues  with  the  background  traffic 
within the fast path. This is why we observe larger delays when 
the  background  traffic  has  1400  byte  payloads.  For  the  Click 
routers, the average ping delays remain small until the input rate 
starts to approach the maximum rate that can be sustained. It then 
rises sharply, with average delays well above 10 ms. We note that 
while  four  cores  does  lead  to  better  throughput,  it  does  little  to 
limit the latency, once the throughput limit has been reached. 
The data in Figure 10 use the standard PlanetLab scheduling 
parameters. We also experimented with different choices, expect-
ing  that  as  we  reduced  the  number  of  tokens  allocated  to  each 
vServer, the latency would drop as the cycle time of the scheduler 
dropped. We found, to our surprise, that the scheduling parame-
ters had a negligible effect on latency (or throughput), under the 
traffic conditions used in this experiment. While we have not been 
able to confirm it, it appears that when the system enters overload, 
the Click router is not appropriately balancing the time devoted to 
input  processing  with  the  time  devoted  to  output  processing.  In 
addition,  a  significant  fraction  of  the  processing  time  is  being 
taken  up  by  the  IP  stack  in  the  operating  system,  which  must 
move arriving data from the network device driver queues into the 
socket  buffers  used  by  the  vServers.  Since  ping  packets  can  get 
delayed behind the packets that make up the background load in 
the device driver queues, they are directly affected by the back-
ground  load,  even  though  they have  separate  socket  buffers  and 
pass through separate queues within the Click routers. 
7.2. Internet Indirection Infrastructure 
The  Internet  Indirection  Infrastructure  (I3)  [ST02]  is  a  novel 
network  architecture  that  explores  the  use  of  indirection  as  an 
underlying  mechanism  for  giving  users  greater  control  over  the 
traffic  they  receive.  Instead  of  traffic  being  sent  directly  to  a 
destination address, it is sent to a user-defined identifier, called a 
trigger. Triggers are defined within a flat identifier space, and the 
responsibility for handling packets labeled by different triggers is 
distributed  over  the  I3  routers.  The  I3  routers  use  Chord-style 
forwarding [ST01] to deliver each packet to the node responsible 
for  its  trigger.  In  the  simplest  (and  presumably  most  common) 
case, when a packet reaches the router responsible for its trigger it 
finds a single filter matching its trigger value, which specifies the 
address of the destination to receive the packet. By having their 
packets  sent  indirectly  through  triggers,  users  can  more  easily 
shield  themselves  from  unwanted  traffic  and  can  shield  their 
communicating peers from changes in their actual address, mak-
ing  support  for  mobility  very  straightforward.  I3  also  allows 
packets  to  match  multiple  filters,  facilitating  multicast;  it  also 
supports more complex trigger processing, including packets with 
“stacks”  of  triggers  and  user-defined  “remapping”  of  trigger 
identifiers. 
I3 has been implemented on PlanetLab and we used the pub-
licly  available  I3  implementation  as  the  basis  for  the  results  re-
ported  here.  We  first  installed,  configured  and  verified  the 
operation of the standard I3 implementation on our GPE, and took 
a set of baseline performance measurements of this configuration 
for comparison purposes. We then created a hybrid implementa-
tion, with the I3 fast path running on the NPE, and the slow path 
on the GPE. The fast path does all the Chord-level forwarding. So, 
if  a  router  receives  a  packet  with  a  trigger  that  lies  outside  the 