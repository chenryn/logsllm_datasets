### 1. Introduction to the Object Directory Service

We implement the object directory service using a set of gRPC [17] server processes distributed across multiple nodes. Each directory server can push location notifications directly to an object store node. In Hoplite, each object store node is a gRPC server with locally buffered objects. When a remote node requests a transfer (e.g., during a `Get` operation), the node establishes a direct TCP connection to the remote node and pushes the object buffer through this connection.

### 2. Tree Reduce Algorithm

In our experiments, we found that setting \( d \) to 1, 2, or \( n \) in the tree reduce algorithm is sufficient for our applications. When a task calls `Reduce`, Hoplite selects \( d \) from 1, 2, and \( n \) to minimize the estimated total latency based on network latency \( L \), bandwidth \( B \), and object size \( S \). Appendix B provides a detailed analysis of the effects of different choices of \( d \).

### 3. Evaluation

#### 3.1 Microbenchmarks

We first microbenchmark Hoplite on a set of popular traditional network primitives, such as broadcast, reduce, and allreduce. We then evaluate Hoplite using real applications on Ray [30], including asynchronous SGD, reinforcement learning, and serving an ensemble of ML models. Additionally, we test Hoplite with synchronous data-parallel training workloads to estimate the performance loss if static and synchronous workloads are run on task-based distributed systems. Each application requires fewer than 100 lines of code changes, primarily for object serialization. All experiments were conducted on AWS EC2 using a cluster of 16 m5.4xlarge nodes (16 vCPUs, 64GB memory, 10 Gbps network) running Linux (version 4.15). Each test was run 10 times, and standard deviations are shown as error bars.

##### 3.1.1 Point-to-Point Data Communication

We benchmarked direct point-to-point transfers. On our testbed, writing object locations to the object directory service takes 167 µs (standard deviation = 12 µs), and retrieving object locations from the service takes 177 µs (standard deviation = 14 µs).

Hoplite's point-to-point communication is efficient. We tested round-trip times for different object sizes using OpenMPI, Ray, Dask, and Hoplite. Figure 6 shows the results, including the theoretical optimal RTT, calculated as \( \text{object\_size} / \text{bandwidth} \times 2 \).

For 1 KB and 1 MB objects, OpenMPI is 1.8x and 2.3x faster than Hoplite, respectively. For 1 GB objects, Hoplite is only 0.2% slower than OpenMPI. Ray and Dask are significantly slower. OpenMPI is the fastest because MPI has knowledge of the process locations for communication. Ray, Dask, and Hoplite need to locate objects through an object directory service. Hoplite outperforms Ray and Dask because:
1. Hoplite stores object contents in the object directory service for objects smaller than 64 KB (§3.2).
2. Hoplite uses pipelining (§3.3) to reduce end-to-end latency. Ray does not support pipelining, leading to additional memory copy latency in the object store. Our pipelining block size is 4 MB, providing better benefits for larger objects (1 GB). On 1 GB objects, Hoplite achieves similar performance to the underlying network bandwidth despite additional memory copies, thanks to fine-grained pipelining that overlaps memory copying and data transfer.

##### 3.1.2 Collective Communication

Next, we measured the performance of collective communication on OpenMPI, Ray, Dask, Gloo, and Hoplite, using arrays of 32-bit floats and addition as the reduce operation (if applicable). We measured the time from when the input objects are ready to when the last process finishes. For both Hoplite and Ray, we assume the application uses a read-only `Get` to avoid memory copy from the object store to the receiver task (§3.3). Gloo only implements broadcast and allreduce. For allreduce, Gloo supports several algorithms; we evaluated all and present the two best-performing ones: ring-chunked allreduce and halving doubling allreduce.

Figure 7 shows the results for medium (1MB) to large (1GB) objects. Results for small objects (1KB, 64KB) are in Appendix A, as they are cached in the object directory service, eliminating the need for collective communication. In summary, Hoplite achieves similar performance to traditional collective communication libraries like OpenMPI and Gloo. Hoplite significantly outperforms Ray and Dask, which do not support efficient collective communication. Gloo’s ring-chunked allreduce is the fastest allreduce implementation for large objects in our tests.

**Broadcast:** One node first `Put`s an object, and after the `Put` succeeds, other nodes simultaneously `Get` the object. Broadcast latency is calculated from the time all nodes call `Get` to the time the last receiver finishes. Hoplite and OpenMPI achieve the best performance for all object sizes and node configurations. This is because Ray, Dask, and Gloo lack collective communication optimization for broadcast. Hoplite slightly outperforms OpenMPI due to fine-grained pipelining.

**Gather:** Each node first `Put`s an object, and after all nodes' `Put`s succeed, one node `Get`s all objects via their `ObjectIDs`. Gather latency is the `Get` duration. OpenMPI and Hoplite outperform the rest for all object sizes and node configurations. This is because Ray and Dask require additional memory copying between workers and the object store. Hoplite also needs additional memory copying, but the latency is masked by fine-grained pipelining between workers and the object store.

**Reduce:** Each node first `Put`s an object, and after all nodes' `Put`s succeed, one node `Reduce`s the objects via their `ObjectIDs` to create a new `ObjectID` for the result. The node then calls `Get` to retrieve the resulting object buffer. Reduce latency is calculated from the time the node calls `Reduce` to the time it has a copy of the reduce result. OpenMPI and Hoplite consistently outperform the rest for all object sizes and node configurations since Ray and Dask do not support collective communication. Hoplite can slightly outperform OpenMPI due to fine-grained pipelining.

**AllReduce:** In Hoplite, we concatenate `Reduce` and `Broadcast` to implement `AllReduce`. AllReduce latency is calculated from the time a node starts to `Reduce` all objects to the time the last node `Get`s the reduce result. We divide the results into two groups in Figure 7. Hoplite significantly outperforms Ray and Dask due to its support for broadcast and reduce. Efficient allreduce is not our design goal, as it is a static and synchronous collective communication operation. However, Hoplite still achieves comparable performance with static collective communication libraries like OpenMPI and Gloo.

##### 3.1.3 Asynchrony

Hoplite’s performance is robust even when processes are not synchronized, typical in task-based distributed systems. We measured broadcast, reduce, and allreduce latencies when participating tasks arrive sequentially with a fixed arrival interval. For broadcast (Figure 8a), OpenMPI makes some progress before the last receiver arrives (§7). However, the algorithm is static (i.e., based on process rank [16]), while Hoplite achieves lower latency with a dynamic algorithm that does not depend on the particular arrival order. We do not include Gloo because it does not optimize its broadcast performance (Figure 7).

For reduce (Figure 8b) and allreduce (Figure 8c), both OpenMPI and Gloo must wait until all processes are ready, while Hoplite can make significant progress before the last object is ready. This allows Hoplite to outperform Gloo’s ring-chunked allreduce when objects do not arrive at the same time.

### 4. Real-World Applications

#### 4.1 Asynchronous SGD

Asynchronous stochastic gradient descent (SGD) is an efficient way to train deep neural networks, often using a parameter server framework [11, 25, 25, 26, 26]. Clients fetch parameters from a centralized server, evaluate them on their portion of data (e.g., forward and backward propagation on a neural network), and send updates (e.g., gradients) back to the server independently. The parameter server needs to broadcast parameters to and reduce from an uncertain set of workers.

We evaluated Hoplite with Ray’s example implementation of an asynchronous parameter server [41]. We used three widely-used standard deep neural networks: AlexNet [23] (model size = 233 MB), VGG-16 [48] (model size = 528 MB), and ResNet-50 [18] (model size = 97 MB). We tested two cluster configurations: 8 p3.2xlarge nodes and 16 p3.2xlarge nodes on AWS. p3.2xlarge instances have the same network performance as m5.4xlarge instances but include an NVIDIA V100 GPU to accelerate neural network execution. The asynchronous parameter server collects and reduces updates from the first half of worker nodes that finish the update and broadcasts the new weights back to these nodes.

Figure 9 shows the results. Hoplite improves the training throughput of the asynchronous parameter server. Compared to Ray, Hoplite speeds up training on the asynchronous parameter server for 16 nodes by 7.8x, 7.0x, and 5.0x for AlexNet, VGG-16, and ResNet-50, respectively. Ray is slow because the parameter server must receive gradients from each worker and send the updated model to each worker one by one, creating a bandwidth bottleneck. In Hoplite, these operations are optimized by our broadcast and reduce algorithms.

#### 4.2 Reinforcement Learning

Reinforcement learning (RL) algorithms involve deeply nested, irregular distributed computation patterns, making task-based distributed systems a perfect fit. We evaluated Hoplite with RLlib [27], a popular and comprehensive RL library on Ray. Distributed RL algorithms can be divided into two classes: sample optimization (e.g., IMPALA [13], Asynchronous PPO [46]) and gradient optimization (e.g., A3C [29]). In sample optimization, a centralized trainer periodically broadcasts a policy to a set of workers and gathers the rollouts generated by the workers to update the model. In gradient optimization, workers compute gradients with their rollouts, and the trainer updates the model with the reduced gradients from the workers.

We evaluated two popular RL algorithms, IMPALA [13] and A3C [29], one from each class. We tested two cluster configurations: 8 nodes (1 trainer + 7 workers) and 16 nodes (1 trainer + 15 workers). The trainer broadcasts a model to the first half of workers that have finished a round of simulation (in IMPALA) or gradient computation (in A3C). We used a two-layer feed-forward neural network with 64 MB of parameters. Figure 10 shows the training throughput, calculated as the number of simulation traces (in sample optimization) or gradients (in gradient optimization) processed per second.

Hoplite significantly improves the training throughput of both IMPALA and A3C. Hoplite improves the training throughput of IMPALA by 1.9x on an 8-node cluster and 1.8x on a 16-node cluster. The reason Hoplite outperforms Ray is that IMPALA frequently broadcasts a 64 MB model to the workers. We expect more improvement with a higher number of nodes, but we already achieve the maximum training throughput: IMPALA is bottlenecked by computation rather than communication using Hoplite with 16 nodes (15 workers). For A3C, Hoplite improves the training throughput by 1.5x on an 8-node cluster and 1.4x on a 16-node cluster.