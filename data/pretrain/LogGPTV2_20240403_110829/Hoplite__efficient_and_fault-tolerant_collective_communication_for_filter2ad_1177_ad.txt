ble 1).
We implement the object directory service using a set of gRPC [17]
server processes distributed across nodes. Each directory server can
push location notifications directly to an object store node. Each
object store node in Hoplite is a gRPC server with locally buffered
(cid:53)(cid:20)(cid:53)(cid:22)(cid:53)(cid:21)(cid:53)(cid:23)(cid:53)(cid:25)(cid:53)(cid:24)(cid:20)(cid:21)(cid:22)(cid:21)(cid:19)(cid:20)(cid:21)(cid:21)(cid:22)(cid:20)(cid:21)(cid:20)(cid:23)(cid:23)(cid:24)(cid:19)(cid:20)(cid:20)(cid:21)(cid:22)(cid:23)(cid:19)(cid:20)(cid:19)(cid:25)(cid:27)(cid:28)(cid:53)(cid:20)(cid:53)(cid:22)(cid:53)(cid:21)(cid:53)(cid:23)(cid:53)(cid:25)(cid:53)(cid:24)(cid:20)(cid:21)(cid:22)(cid:21)(cid:19)(cid:20)(cid:21)(cid:21)(cid:22)(cid:20)(cid:21)(cid:20)(cid:23)(cid:23)(cid:24)(cid:19)(cid:20)(cid:20)(cid:21)(cid:22)(cid:23)(cid:19)(cid:20)(cid:19)(cid:26)(cid:26)(cid:27)(cid:53)(cid:26)(cid:21)(cid:20)(cid:19)(cid:24)(cid:22)(cid:23)SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
(a) 1 KB
(b) 1 MB
(c) 1 GB
Figure 6: Round trip latency for point-to-point data communication on
Hoplite, OpenMPI, Ray, and Dask. We also include the theoretical optimal
RTT (i.e. total bytes transferred divided by the bandwidth).
objects. Upon a transfer request from a remote node (e.g., during
Get), the node sets up a direct TCP connection to the remote node
and pushes the object buffer through the TCP connection.
In our experiments, we observe that setting 𝑑 to 1, 2, or 𝑛 in the
tree reduce algorithm is enough for our applications. When a task
calls Reduce, Hoplite picks 𝑑 from 1, 2 and 𝑛 that minimizes the
estimated total latency based on the network latency 𝐿, bandwidth
𝐵, and the object size 𝑆. Appendix B shows the effect of different
choices of 𝑑.
5 EVALUATION
We first microbenchmark Hoplite on a set of popular traditional
network primitives (e.g., broadcast, reduce, allreduce). We then eval-
uate Hoplite using real applications on Ray [30], including asyn-
chronous SGD, reinforcement learning, and serving an ensemble
of ML models. We also test Hoplite with synchronous data-parallel
training workloads to estimate how much performance is lost if
people choose to run these static and synchronous workloads on
task-based distributed systems. Each application requires <100 lines
of code changes, most of which are for object serialization. All ex-
periments are done on AWS EC2. We use a cluster of 16 m5.4xlarge
nodes (16 vCPUs, 64GB memory, 10 Gbps network) with Linux
(version 4.15). We run every test 10 times, and we show standard
deviations as error bars.
5.1 Microbenchmarks
We use two popular task-based distributed systems, Ray [30] (ver-
sion 0.8.6) and Dask [44] (version 2.25), as our baselines. In addition,
we compare Hoplite with OpenMPI [16] (version 3.3) and Gloo [14].
We chose OpenMPI because OpenMPI is the collective communi-
cation library recommended by AWS. We did not choose Horovod
because Horovod has three backends: OpenMPI, Gloo, and NCCL.
We have already tested OpenMPI and Gloo individually. We cur-
rently do not support GPU, so we do not test NCCL.
5.1.1 Point-to-Point Data Communication. We first benchmark di-
rect point-to-point transfer. On our testbed, writing object locations
to the object directory service takes 167 µs (standard deviation =
12 µs), and getting object location from the object directory service
takes 177 µs (standard deviation = 14 µs).
Hoplite’s point-to-point communication is efficient. We test
round-trip time for different object sizes using OpenMPI, Ray, Dask,
and Hoplite. Figure 6 shows the result. We also include the optimal
RTT, which is calculated by object_size/bandwidth × 2.
For 1 KB and 1 MB object, OpenMPI is 1.8x and 2.3x faster than
Hoplite. For 1 GB objects, Hoplite is 0.2% slower than OpenMPI. Ray
and Dask are significantly slower. OpenMPI is the fastest because
MPI has the knowledge of the locations of the processes to commu-
nicate. Ray, Dask, and Hoplite need to locate the object through an
object directory service. Hoplite outperforms Ray and Dask because
(1) Hoplite stores object contents in object directory service for ob-
jects smaller than 64 KB (§3.2) and (2) Hoplite uses pipelining (§3.3)
to reduce end-to-end latency. Ray does not support pipelining, so
it suffers from the extra memory copy latency in the object store.
Our pipelining block size is 4 MB, and thus larger object (1 GB) has
better pipelining benefits. On 1 GB object, Hoplite achieves similar
performance as the underlying network bandwidth despite it has
additional memory copies. This is because fine-grained pipelining
successfully overlaps memory copying and data transfer.
5.1.2 Collective Communication. Next, we measure the perfor-
mance of collective communication on OpenMPI, Ray, Dask, Gloo,
and Hoplite, with arrays of 32-bit floats and addition as the reduce
operation (if applicable). We measure the time between when the
input objects are ready and when the last process finishes. For
both Hoplite and Ray, we assume that the application uses a read-
only Get to avoid the memory copy from the object store to the
receiver task (§3.3). Gloo only implements broadcast and allreduce.
For allreduce, Gloo supports several algorithms. We evaluated the
performance for all of them, and for presentation simplicity, we only
show the two algorithms with the best performance on our setup:
(1) ring-chunked allreduce and (2) halving doubling allreduce.
Figure 7 shows the results for medium (1MB) to large (1GB)
objects.2 We present the results for small objects (1KB, 64KB) in
Appendix A because small objects are cached in object directory
service in Hoplite, and there is thus no collective communication
to begin with. In summary, Hoplite achieves a similar level of per-
formance as traditional collective communication libraries, such
has OpenMPI and Gloo. Hoplite significantly outperforms Ray and
Dask, because Ray and Dask do not support efficient collective com-
munication. Gloo’s ring-chunked allreduce is the fastest allreduce
implementation for large objects in our tests.
Broadcast. We let one node first Put an object, and after the Put
succeeds, other nodes Get the object simultaneously. The latency of
broadcast is calculated from the time all nodes call Get to the time
when the last receiver finishes. Hoplite and OpenMPI achieve the
best performance for all object size and node configurations. This is
because Ray, Dask, and Gloo do not have collective communication
optimization for broadcast. Hoplite slightly outperforms OpenMPI
because of fine-grained pipelining.
Gather. We let every node first Put an object, and after every
node’s Put succeeds, one of the nodes Get all the object via their
ObjectIDs. The latency of gather is the Get duration. OpenMPI
and Hoplite outperforms the rest for all object size and node con-
figurations. This is because both Ray and Dask need additional
memory copying between workers and the object store. Hoplite
also needs additional memory copying, but the latency is masked
by fine-grained pipelining between workers and the object store.
2OpenMPI’s latency does not increase monotonically because OpenMPI chooses dif-
ferent algorithms on different conditions (e.g., number of nodes, whether the number
of nodes is a power of two, object size).
OptimalHopliteOpenMPIRayDask0.00.51.01.52.02.53.0RTT (ms)1.7 μsOptimalHopliteOpenMPIRayDask0246810RTT (ms)OptimalHopliteOpenMPIRayDask0123456RTT (s)Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Figure 7: Latency comparison of Hoplite, OpenMPI, Ray, Dask, and Gloo on standard collective communication primitives (e.g., broadcast, gather, reduce,
allreduce). To show the results more clearly, we split the results of Allreduce into two groups: group (i) includes Hoplite, Ray, and Dask, and group (ii) includes
Hoplite, OpenMPI, and two different allreduce algorithms in Gloo.
(a) Broadcast
(b) Reduce
(c) AllReduce
Figure 8: Latency of 1 GB object broadcast/reduce/allreduce on 16 nodes when tasks start sequentially with a fixed arrival interval. Arrival interval equals to
0 means that all the tasks start at the same time. The dashed lines denote the time the last task arrives.
Reduce. We let every node first Put an object, and after every
node’s Put succeeds, one of the nodes Reduce the objects via their
ObjectIDs to create a new ObjectID for the result. The node then
calls Get to get the resulting object buffer. The latency of reduce is
calculated from the time the node calls Reduce to the time the node
has a copy of the reduce result. OpenMPI and Hoplite consistently
outperform the rest for all object size and node configurations since
Ray and Dask do not support collective communication. Hoplite can
slightly outperform OpenMPI because of fine-grained pipelining.
AllReduce. In Hoplite, we simply concatenate reduce and broad-
cast to implement allreduce. The latency of allreduce is calculated
from the time a node starts to Reduce all the objects to the last
node Get the reduce result. We divide the results into two groups in
Figure 7. Hoplite significantly outperforms Ray and Dask because
of the collective communication support of broadcast and reduce
in Hoplite. Note that efficient allreduce is not our design goal since
allreduce is a static and synchronous collective communication
operation. However, Hoplite still achieves comparable performance
with static collective communication libraries such as OpenMPI
and Gloo.
5.1.3 Asynchrony. Hoplite’s performance is robust even when pro-
cesses are not synchronized, which is typical in task-based dis-
tributed systems. We measure broadcast, reduce, and allreduce
latencies when the participating tasks arrive sequentially with a
fixed arrival interval. For broadcast (Figure 8a), OpenMPI makes
some progress before the last receiver arrives (§7). However, the
algorithm is static (i.e. based on process rank [16]), while Hoplite
achieves a lower latency with a dynamic algorithm that does not
depend on the particular arrival order. We do not include Gloo
because it does not optimize its broadcast performance (Figure 7).
For reduce (Figure 8b) and allreduce (Figure 8c), both OpenMPI
and Gloo have to wait until all processes are ready, while Hoplite
can make significant progress before the last object is ready. This
allows Hoplite to even outperform Gloo’s ring-chunked allreduce
when objects do not arrive at the same time.
0.00.51.0Latency (s)×10−2Broadcast 1MB0.00.51.0×10−1Gather 1MB0.02.55.07.5×10−2Reduce 1MB0.02.55.07.5×10−2Allreduce(i) 1MB0.00.51.01.5×10−2Allreduce(ii) 1MB024Latency (s)×10−1Broadcast 32MB0246×10−1Gather 32MB0246×10−1Reduce 32MB0.00.51.01.5Allreduce(i) 32MB0.02.55.07.5×10−2Allreduce(ii) 32MB481216Number of Nodes0.00.51.01.5Latency (s)×101Broadcast 1GB481216Number of Nodes0123×101Gather 1GB481216Number of Nodes0123×101Reduce 1GB481216Number of Nodes0.02.55.07.5×101Allreduce(i) 1GB481216Number of Nodes0123Allreduce(ii) 1GBHopliteOpenMPIRayDaskGloo (Broadcast)Gloo (Ring Chunked)Gloo (Halving Doubling)00.10.20.3Arrival Interval (s)0123456Latency (s)HopliteOpenMPI00.10.20.3Arrival Interval (s)012345678Latency (s)HopliteOpenMPI00.10.20.3Arrival Interval (s)012345678Latency (s)HopliteOpenMPIGlooSIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
(a) Number of Nodes = 8
(b) Number of Nodes = 16
(a) IMPALA
(b) A3C
Figure 9: Training throughput (number of training samples per second)
for asynchronous SGD.
Figure 10: RLlib’s training throughput (number of training samples per
second) on Ray and Hoplite.
5.2 Asynchronous SGD
Asynchronous stochastic gradient descent (SGD) is one way to train
deep neural networks efficiently, and it usually uses a parameter
server framework [11, 25, 25, 26, 26]: clients fetch the parameters
from a centralized server, evaluate the parameters on its own por-
tion of data (e.g., performing forward and backward propagation
on a neural network), and send the updates (e.g., gradients) back to
the server independently. The parameter server needs to broadcast
parameters to and reduce from an uncertain set of workers.
Here we evaluate Hoplite with Ray’s example implementation of
an asynchronous parameter server [41]. We use three widely-used
standard deep neural networks, AlexNet [23] (model size = 233 MB),
VGG-16 [48] (model size = 528 MB), and ResNet-50 [18] (model
size = 97 MB). We test two cluster configurations: 8 p3.2xlarge
nodes and 16 p3.2xlarge nodes on AWS. p3.2xlarge instance has
the same network performance as m5.4xlarge instance but with an
additional NVIDIA V100 GPU to accelerate the execution of the
neural networks. The asynchronous parameter server collects and
reduces the updates from the first half of worker nodes that finish
the update and broadcast the new weights back to these nodes.
We show the results in Figure 9. Hoplite improves the training
throughput of the asynchronous parameter server. Comparing to
Ray, it speedups training on asynchronous parameter server for
16 nodes by 7.8x, 7.0x, and 5.0x, for AlexNet, VGG-16, and ResNet-
50, respectively. Ray is slow because the parameter server has to
receive gradients from each worker and send the updated model
to each worker one by one. This creates a bandwidth bottleneck at
the parameter server. In Hoplite, these operations are optimized by
our broadcast and reduce algorithms.
5.3 Reinforcement Learning
RL algorithms involve the deep nesting of irregular distributed
computation patterns, so task-based distributed systems are a per-
fect fit for these algorithms. We evaluate Hoplite with RLlib [27],
a popular and comprehensive RL library on Ray. Distributed RL
algorithms can be divided into two classes: In samples optimization
(e.g., IMPALA [13], Asynchronous PPO [46]), a centralized trainer
periodically broadcasts a policy to a set of workers and gather the
rollouts generated by the workers to update the model. In gradients
optimization (e.g., A3C [29]), the workers compute the gradient
with their rollouts, and the trainer updates the model with the
reduced gradients from the workers.
We evaluate two popular RL algorithms, IMPALA [13] and A3C [29],
one from each class. We test two cluster configurations: 8 nodes
(1 trainer + 7 workers) and 16 nodes (1 trainer + 15 workers). The
trainer broadcast a model to the first half workers that have finished
a round of simulation (in IMPALA) or gradient computation (in
A3C). We use a two-layer feed-forward neural network with 64 MB
of parameters. Figure 10 shows the training throughput. Training
throughput is calculated by the number of simulation traces (in
samples optimization) or gradients (in gradients optimization) the
RL algorithm can process in a second.
Hoplite significantly improves the training throughput of both
IMPALA and A3C. Hoplite improves the training throughput of
IMPALA by 1.9x on an 8-node cluster and 1.8x on a 16-node cluster.
The reason Hoplite outperforms Ray is because IMPALA has to
broadcast a model of 64 MB frequently to the workers. We expect
more improvement when the number of nodes is higher, but we
already achieve the maximum training throughput: IMPALA is
bottlenecked by computation rather than communication using
Hoplite with 16 nodes (15 workers). For A3C, Hoplite improves the