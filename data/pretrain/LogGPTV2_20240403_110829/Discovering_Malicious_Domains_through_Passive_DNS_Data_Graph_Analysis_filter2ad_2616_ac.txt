IPs in I belong to. Then we redeﬁne the weight between
two domains d1 and d2 in a domain graph as
w(d1, d2) = 1 −
1
1 + |asn(ip(d1) ∩ ip(d2))|
if d1 (cid:54)= d2
Though two unrelated domains may be hosted in the same
pool of public IPs of one service provider (e.g., AWS), it is
unlikely that they are both hosted at public IPs from two
or more service providers (e.g., both AWS and CloudFare).
Here we use ASNs of IPs to approximately identify IPs from
diﬀerent service providers. In practice it is certainly possible
that a service provider owns IPs from multiple ASNs (e.g.,
both AS16509 and AS14618 belong to Amazon). Therefore,
two unrelated domains may still be associated even if they
only use services from a single provider. Our experimental
results show that such cases are rare and have limited impact
on the eﬀectiveness of our approach. Besides using ASNs,
we could also use WHOIS records of IPs to identify those
belonging to the same provider. However, WHOIS records
are well-known to be noisy often with conﬂicting informa-
tion due to the lack of standard formats and heterogeneous
information sources.
Another practical concern is performance and scalability.
The performance bottleneck may come from two steps. The
ﬁrst is to generate domain graphs. In the worst case, if there
are n domains in a domain resolution graph, each IP hosts all
the domains, and hence, it may take O(n2|I|) steps to build
the corresponding domain graph, where |I| is the number
of IPs in the a domain resolution graph . Though in prac-
tice a domain graph tends to be sparse, signiﬁcant number
of edges will be generated if an IP hosts a huge number of
domains (for example, an IP of Amazon may host hundreds
of thousands of domains). This is because an edge needs to
be created for each pair of domains hosted at that IP. For-
tunately, our previous step of public IP pruning (excluding
IPs with degrees larger than t from the domain resolution
graph) also helps alleviate this problem, because now the
worst case number of steps to establish the domain graph is
bounded by O(t2|I|). t2 can be a large constant. However,
due to the power law distribution of the degrees of IPs in
domain resolution graphs (which will be shown in section 4),
the actual size of domain graphs is much smaller than the
theoretical bound O(t2|I|), which means it is very manage-
able with moderate computing resources or with distributed
computing platforms like Hadoop.
Compared with the huge number of domains a public IP
may host, the number of IPs that a domain may resolve to
is relatively small (at most several thousands). Therefore,
we do not perform any ﬁltering of domains based on their
degrees in the domain resolution graph, which means we will
not miss domains involved in fast-ﬂuxing.
1
1
The second potential performance bottleneck is to com-
pute the strongest paths from domains to seeds. It is easy
to see that the strongest path problem can be mapped to
the classical weighted shortest path problem. Speciﬁcally,
given a domain graph G(D, E), we construct another graph
G(cid:48)(D, E) , such that for any edge e = {d1, d2} in G, the
weight of e in G(cid:48) is ln(
w(d1,d2) ). As w(d1, d2) is between 0
and 1, ln(
w(d1,d2) ) is positive. Then a path P = (d1, . . . , dn)
is the strongest path between d1 and dn in G if and only if
P is the shortest weighted path from d1 to dn in G(cid:48). Thus,
standard shortest path algorithms can be easily adapted to
compute the malicious scores of domains. With the Dijk-
stra’s algorithm using a min-priority queue, the worst-case
complexity of this step would be O(|S|(|E| + |D|log|D|)),
where S is the set of seeds. Usually S is much smaller com-
pared to the scale of a domain graph. Therefore, with mod-
erate computing resources, the computation cost of this step
is acceptable in practice. In particular, domain graphs tend
to be composed of multiple connected components. The al-
gorithm for malicious score computation can be performed
on each component instead of the whole graph.
It also
allows us to easily speed up through parallel computation
with multi-core or GPU processors or Hadoop. In our ex-
periments, malicious score computation is done by a GPU
processor, which is not a performance bottleneck for us.
Given the above practical considerations, Algorithm 1 shows
the pseudocode of our approach that we will evaluate exper-
imentally in section 4.
4. EXPERIMENTS
As mentioned in section 1, our proposed technique is not
a general classiﬁcation scheme like Notos [1] and EXPO-
SURE [3]. That is, our technique cannot take an arbitrary
given domain and decide whether it is potentially malicious
or not. For example, if a domain is not resolved by any host,
it will not appear in the passive DNS database, which will
then be irrelevant to our technique. Similarly, if a domain
never shares IPs with other domains, it will not appear in
the domain graph, and our technique is not applicable to
such domain either. What we propose is a discovery tech-
nique which tries to ﬁnd previously unknown malicious do-
mains from known ones. Therefore, its eﬀectiveness should
be evaluated in the scope of domains where our scheme ap-
plies. In other words, it could be seen as a complementary
technique to existing classiﬁcation techniques. Speciﬁcally,
our evaluation focuses on the following three metrics:
6
Algorithm 1: Algorithm to discover malicious domains
through passive DNS data
Input : G(I, D, E), a domain resolution graph
t, degree threshold
S, a set of known malicious domains
m, malicious score threshold
Output: M , a set of potential malicious domains
1 for each IP i in I do
2
if degree(i) > t then
remove i from G;
3
end
4
5 end
6 Denote the remaining graph RG(cid:48);
7 Let DG be an empty graph;
8 for domains d1 and d2 in RG(cid:48) with common
9
10
neighboring IPs do
if |asn(ip(d1) ∩ ip(d2))| > 1 then
Add edge {d1, d2} to DG;
w(d1, d2) = 1 −
1
1+|asn(ip(d1)∩ip(d2))| ;
end
11
12 end
13 M = ∅;
14 Let CC be the set of connected components in DG;
15 for each C in CC do
if C ∩ S (cid:54)= ∅ then
16
17
18
19
20
21
22
for each d in CC do
compute mal(d, S);
if mal(d, S) >= m then
add d to M ;
end
end
end
23
24 end
25 return M
• True positive rate: Given a malicious domain in the do-
main graph, the probability that it will be labeled as po-
tentially malicious.
• False positive rate: Given a benign domain in the domain
graph, the probability that it will be labeled as potentially
malicious.
• Expansion: From a set of known malicious domains, how
many more domains will be discovered as potentially ma-
licious, in other words, how much can our scheme expand
the set of malicious domains beyond those in the seeds.
Since our scheme focuses on discovering unknown mali-
cious domains, expansion is an important metric that re-
ﬂects the usefulness of our scheme. To better illustrate,
consider conceptually another scheme which, for example,
builds a graph only with domains whose names possess pat-
terns typical to domain generation algorithms (DGAs). A
scheme designed for such a graph may show a very high true
positive rate and a very low false positive rate, according to
the above deﬁnitions. But it may have a very low expansion,
as it can only discover DGA-generated domains, which may
not be quite useful in practice. Our scheme meanwhile does
not rely on any other features when building the domain
graph, which will yield a high expansion.
Our technique has two parameters, the malicious score
7
Figure 3: Degree distribution of IP nodes in domain reso-
lution graphs for the two datasets. Only the 5000 IPs with
the highest degrees are shown in the ﬁgure
threshold and the seeds set size, both of which will impact
the tradeoﬀ of the above three metrics. Intuitively, the lower
the threshold is, or the larger the set of the seeds are, the
higher the true positive rate and the expansion, but the
higher the false positive rate as well.
4.1 Datasets
Passive DNS data. We downloaded the passive DNS
database from www.dnsdb.info using the website’s API. As
the database is updated constantly, the snapshot we use
is the one obtained in the middle of December 2014. The
database contains various types of DNS records. We choose
to work on A records to ensure the actual mapping between
domains and IPs. As mentioned before, for each domain-
to-IP resolution, the database keeps timestamps regarding
when this resolution is ﬁrst and last seen by the passive DNS
sensors. A resolution is said to belong to a period of time if
its ﬁrst-seen timestamp falls into that period.
In this section, we report experimental results on two
datasets. One is for the ﬁrst week of November 2014, and
the other is for the ﬁrst two weeks of November 2014. We
have also run the same set of experiments on datasets of
other period of times. The experimental results are consis-
tent with that of the above-mentioned two period of times.
To avoid redundancy, we omit them in the paper. The rea-
son to choose datasets for periods of diﬀerent length is to
check whether the scale of data would have any impact on
the eﬀectiveness of our approach.
We mentioned in section 3.4 that we do not consider public
IPs in which anybody can host their domains if they choose
to do so. We use a heuristic that if an IP hosts more than t
domains, we treat it as a public IP. Figure 3 shows the degree
distribution of IPs in the domain resolution graphs of both
datasets, where x axis are IPs sorted based on their degrees
and y axis are their corresponding degrees. We see that the
distribution seems to follow a power law distribution, where
a small set of IPs have degrees signiﬁcantly higher than that
of others. Based on the above ﬁgures, we empirically set t to
be 2000, where only less than 500 and 900 IPs respectively
are removed from the domain resolution graphs of the one-
week and the two-week datasets, which is a very negligible
percentage of the original set of IPs.
0	
  1000	
  2000	
  3000	
  4000	
  5000	
  6000	
  7000	
  8000	
  9000	
  10000	
  0	
  500	
  1000	
  1500	
  2000	
  2500	
  3000	
  3500	
  4000	
  4500	
  5000	
  Degree	
  two-­‐week	
  one-­‐week	
  Table 1 shows the statistics of the domain graphs (DG
in Algorithm 1) constructed from the two datasets. We see
domain graphs contain much fewer domains compared to
domain resolution graphs. Indeed, most of the domains in
the domain resolution graph do not share more than one
IP from diﬀerent ASNs with other domains, and these do-
mains will not appear in the domain graph. An edge in
the domain graph thus reveals a beyond-random connection
between two domains, which allows reliable inferences from
known malicious domains.
Dataset
One-week
Two-week
Domains
54K
98K
Edges
65.3M
120.4M
Table 1: Statistics of domain graphs constructed from the
two passive DNS datasets
Figure 4: Distribution of connected component sizes in do-
main graphs for the two datasets. Only the 50 connected
components with the largest sizes are shown in the ﬁgure
The cost of malicious score computation is largely deter-
mined by the sizes of the connected components in domain
graphs. Figure 4 shows the distribution of the number of
nodes of connected components in the domain graphs of
both datasets. Note that the y-axis is in logarithmic scale.
Clearly they also follow a power-law like distribution.
Ground truth. There are many commercial as well as
public domain blacklists, which can be combined to get a
list of malicious domains. Though each such blacklist may
have false positives, generally there is strong evidence if a
domain is blacklisted, as long as the blacklist is reputable.
Thus it is relatively easy to build a ground truth of malicious
domains. In this work, we use VirusTotal (www.virustotal.
com), which, when given a domain, queries it over more than
60 well-known blacklists. We submit each domain in a do-
main graph to VirusTotal using its public API1, and those
listed by at least one of the blacklists form our ground truth
of malicious domains.
Obtaining ground truth of benign domains is more chal-
lenging. No blacklist is exhaustive. We cannot simply con-
sider a domain to be benign if it is not blacklisted by any of
the blacklists. It may be that the domain has been scanned
1www.virustotal.com/en/documentation/public-api/
8
and no malicious content is found, or it may be because
that domain has never been scanned before.
In this pa-
per, we follow a common practice used in many past eﬀorts
in the literature [7, 8], which builds benign domain ground
truth using Alexa top ranked domains. Speciﬁcally, we treat
a domain as benign if its top-level domain is one of the
Alexa Top 20K domains (http://www.alexa.com). We do
not include domains with ranks lower than 20K, as mali-
cious domains are known to exist in the Alexa top domain
list, especially those with relatively low ranks [9]. On the
other hand, we note that past eﬀorts often perform certain
ﬁltering of Alexa top domains when building benign ground
truth (e.g., only consider domains consistently appearing in
the top domain lists for a period of time, or remove dynamic
DNS service domains such as no-ip.com). As a contrast, we
take a more conservative approach, and do not do any ﬁlter-
ing of the Alexa Top 20K domains. It is more conservative
in the sense that it is more likely to work against us when
measuring false positives. For example, an attacker may
register a subdomain under a dynamic DNS service (e.g.,
malicious.no-ip.com). Even if our scheme successfully dis-
covers it as a malicious domain, we will treat it as a false
positive, as no-ip.com is one of Alexa Top 20K domains.
The ground truth for the one-week dataset contains around
6.5K malicious domains and 6.5K benign domains. That
for the two-week dataset is approximately double the size
(with around 11.5K malicious domains and 12.1K benign
domains).Table 2 shows the statistics of the ground truth for
the domain graphs of the one-week and two-week datasets.
Dataset
One-week
Two-week
Domains Malicious Benign
54K
98K
6.5K
11.6K
6.5K
12.1K
Table 2: Statistics of the ground truth of the two datasets