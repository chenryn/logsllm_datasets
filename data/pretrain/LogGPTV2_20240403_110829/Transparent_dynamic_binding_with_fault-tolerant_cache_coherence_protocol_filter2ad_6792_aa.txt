title:Transparent dynamic binding with fault-tolerant cache coherence protocol
for chip multiprocessors
author:Shuchang Shan and
Yu Hu and
Xiaowei Li
Transparent Dynamic Binding with Fault-Tolerant Cache Coherence Protocol for 
Chip Multiprocessors
Shuchang Shan†‡ 
†Key Laboratory of Computer System and Architecture, 
Institute of Computing Technology, CAS 
‡Graduate University of CAS, 
Beijing, P.R. China 
shanshuchang@ ict.ac.cn 
Abstract—Aggressive  technology  scaling  causes  chip 
multiprocessors  increasingly  error-prone.  Core-level  fault-
tolerant approaches bind two cores to implement redundant 
execution  and  error  detection.  However,  along  with  more 
cores  integrated  into  one  chip,  existing  static  and  dynamic 
binding  schemes  suffer  from  the  scalability  problem  when 
considering  the  violation  effects  caused  by  external  write 
operations.  
In  this  paper,  we  present  a  transparent  dynamic 
binding  (TDB)  mechanism  to  address  the  issue.  Learning 
from static binding schemes, we involve the private caches to 
hold identical data blocks, thus we reduce the global master-
slave  consistency  maintenance  to  the  scale  of  the  private 
caches.  With  our  fault-tolerant  cache  coherence  protocol, 
TDB  satisfies  the  objective  of  private  cache  consistency, 
therefore  provides  excellent  scalability  and  flexibility. 
Experimental  results  show  that,  for  a  set  of  parallel 
workloads,  the  overall  performance  of  our  TDB  scheme  is 
very  close  to  that  of  baseline  fault-tolerant  systems, 
outperforming dynamic core coupling by 9.2%, 10.4%, 18% 
and  37.1%  when  considering  4,  8,  16  and  32  cores 
respectively. 
Keywords-Chip 
tolerance; 
transparent  dynamic  binding  (TDB);  master-slave  memory 
consistency; cache coherence protocol. 
multiprocessor; 
fault 
I.    INTRODUCTION  
technology, 
Profit  from  the  continual  development  of  the  semi-
large  numbers  of 
conductor  fabrication 
transistors are available within a single chip, which offers 
abundant  hardware  resources  for  performance  gains. 
Unfortunately,  shrinking  feature  size,  dropping  supply 
voltage  and  increasing  frequency  make  transistors  more 
susceptible  to  various  internal  failures  and  external 
interferences, such as process variation [1-3], wear-out [4] 
and  particle  attack  [5-6],  which,  seriously  threatens  the 
system reliability. 
(CMPs) 
Nowadays,  Chip  Multiprocessors 
is 
considered  as  a  promising  approach  to  gain  scalable 
performance  as  well  as  to  reduce  power  consumption. 
Industrial  research  has  indicated  that  CMP  will  scale  to 
tens  or  hundreds  of  cores  by  the  year  of  2015  [7]. 
Therefore, how to design fault-tolerant architecture within 
the  CMP  framework  has  attracted  a  lot  of  attention, 
mainly  in  the  context  of  core-level  redundancy  that  the 
Yu Hu† and  Xiaowei Li† 
†Key Laboratory of Computer System and Architecture 
Institute of Computing Technology,  
Chinese Academy of Sciences (CAS), 
Beijing, P.R. China 
{huyu, lxw}@ ict.ac.cn
lacks 
coupled  cores  verify  each  other’s  execution  for  error 
detection  and  recovery  [8-13].  Existing  core-level  fault-
tolerant  schemes  can  be  classified  into  two  categories: 
static  binding  and  dynamic  binding.  The  former  binds 
core  pairs  by  dedicated  channels,  then  exchanges  and 
verifies  the  execution  results  through  the  dedicated 
channels.  Such  a  fine-grained  data  communication 
mechanism simplifies the design, but needs to modify the 
highly  optimized  pipeline.  What’s  more,  the  tightly 
coupled  manner 
flexibility.  For  example,  a 
permanent fault in a single core will cause the pair to be 
abandoned  as  the  pair  is  not  reliable  any  more.  On  the 
contrary,  the  dynamic  binding  schemes  argue  that  the 
master-slave  pair  should  exchange  data  through  shared 
resources, e.g., the system network, so that the system can 
flexibly  adjust  the  master-slave  core  coupling  based  on 
the real-time information. To amortize the network traffic 
overhead,  dynamic  binding  schemes  usually  employ 
coarse-grained data comparison. 
A  key  issue  related  to  the  core-level  fault-tolerant 
design  is  how  to  maintain  the  master-slave  memory 
consistency,  i.e.  the  coupled  master-slave  pair  gets  the 
same  memory  values. 
the  parallel  processing 
environment,  all  threads  share  the  same  memory  space. 
As  the  master  thread  and  the  slave  thread  execute 
asynchronously, an external write operation caused by an 
external  master  thread  between  the  master-slave  load 
operations  may  violate  the  memory  consistency.  For  the 
strict  input  replication  scheme,  two  Load  Value  Queue 
(LVQ) structures residing in the master and the slave are 
employed  to  hold  identical  data  blocks  for  the  master-
slave  pair.  Such  a  strict  data  sharing  mode  adequately 
satisfies  the  master-slave  memory  consistency.  For  the 
relaxed input replication scheme, the master and the slave 
can  independently  access  the  memory.  Therefore,  the 
tightly  shared  LVQ  structures  are  removed.  However, 
violations caused by external write operations need to be 
considered.  Two  studies  have  been  proposed  to  address 
the 
input 
incoherence  induced  by  external  write  operations  rarely 
occurs.  It  employs  the  same  mechanism  for  soft  error 
detection  and  recovery  to  correct  the  infrequent  master-
slave  inconsistency. The other one is a dynamic binding 
proposal called Dynamic Core Coupling (DCC) [12-13].  
issue.  Reunion 
[10]  observes 
that 
the 
In 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
978-1-4244-9233-6/11/$26.00 ©2011 IEEE291n
w
o
d
k
a
e
r
b
l
a
v
r
e
t
n
i
e
t
i
r
w
l
a
n
r
e
t
x
e
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
 500
4
8
lu
16
4
16
8
fft
8
4
16
ocean-con
4
8
16
barnes
8
4
16
cholesky
4
16
8
radix
4
8
16
radiosity
4
8
16
average
Figure 1.   Probability of external writes occurring within certain slacks 
It  constructs  the  master-slave  consistency  window  to 
prevent  external  writes  from  violating  the  memory 
consistency.  The  DCC  mechanism  stalls  external  write 
requests  until  both  the  master  and  the  slave  cores  have 
finished the data accesses. 
However,  as  the  CMP  system  scales,  previously 
proposed schemes are no more cost-effective to maintain 
the  memory  consistency,  which  we  will  explain  below. 
Figure  1  shows  the  probabilities  of  external  write 
operations  occurring  within  certain  slacks.  (The  system 
parameter configuration is described in Section V.) When 
considering  4-,  8-  and  16-core  CMP  systems,  28.3%, 
37.5% and 43.0% of all the external write requests occurs 
within the slack of 100 cycles. The probabilities grow to 
37.2%,  47.1%  and  55.4%  when  the  slack  is  500  cycles. 
According to previous works, the average execution slack 
of the master-slave pair in static binding schemes is on the 
order  of  100  instructions  [9-10]  and  about  200~500 
instructions in dynamic binding schemes [12-13]. We also 
observe that, the number of external write operations per 
1000 cycles grows from 0.33 in a 4-core system to 3.25 in 
a  16-core  system.  As  the  number  of  external  write 
the  probability  of  master-slave 
operations 
inconsistency  grows  correspondingly.  Therefore, 
the 
overhead  induced  by  handling  the  master-slave  memory 
inconsistency grows rapidly. The situation becomes even 
worse  for  dynamic  binding  schemes.  Since  a  significant 
portion of external  writes occurs just  hundreds of cycles 
after  a  core’s  data  access,  the  “consistency  window” 
design  significantly  degrades  the  performance  of  such 
fault-tolerant system [13]. 
increase, 
To solve the scalability  issues of existing solutions, 
we  propose  the  Transparent  Dynamic  Binding  (TDB) 
technique.  The  TDB  takes  the  advantage  of  strict  input 
replication to construct dynamic binding schemes, where 
the private caches are employed to play the role of LVQs 
as in the strict input replication scheme. We introduce the 
the  relaxed 
the  scope  of 
concept  of  Sphere  of  Consistency  (SoC),  which  aids  in 
the  master-slave  memory 
identifying 
input 
consistency  maintenance.  Unlike 
replication  schemes  where  the  SoC  includes  the  whole 
memory hierarchy, we limit the SoC to the private caches 
of  the  master-slave  pair.  In  this  way  we  reduce  the 
consistency maintenance to the scale of the private caches. 
Meanwhile, in case of cache misses, we ask for the data 
provider  to  simultaneously  bring  forward  data  for  the 
master-slave  pair.  What’s  more,  as  the  wrong  path  data 
references  may  pollute  the  private  caches,  we  insert  the 
victim buffer structure between the private cache and the 
global memory to filter the wrong path data blocks. With 
the  help  of  our  fault-tolerant  cache  coherence  protocol, 
TDB  manages 
to  provide  excellent  scalability  and 
flexibility.  Experimental  results  shows  that,  the  overall 
runtime of TDB proposal is very close to that of baseline 
fault-tolerant  systems,  outperforming  DCC  by  9.2%, 
10.4%,  18%  and  37.1% 
tiled-CMP 
architecture  when  considering  4,  8,  16  and  32  cores 
respectively. 
in  a  scalable 
This paper makes the following contributions: 
•  We  demonstrate  that  the  increasing  frequency  of 
external  write  operation  limits  the  scalability  of 
conventional 
fault-tolerant  schemes.  Thus  a 
scalable  fault-tolerant  scheme  needs  to  decouple 
the  complexity  of 
the  master-slave  memory 
consistency maintenance from the system scale. 
•  We introduce the objective of master-slave private 
cache consistency. The private caches of the pair 
are  responsible  for  providing  identical  memory 
values  to  the  pipeline  modules.  In  this  way,  the 
maintenance  of 
the  master-slave  memory 
consistency  is  limited  to  the  scale  of  the  private 
caches. 
•  We  propose  a  core-level  fault-tolerant  execution 
model namely TDB. Through modifying the cache 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
292coherence  protocol,  the  TDB  execution  model 
manages 
scalable 
architecture. 
flexible  and 
to  provide 
The rest of the paper is organized as follows: Section 
II  introduces  the  objective  of  master-slave  private  cache 
consistency  and  discusses  the  key  issues  related  to  it. 
Section III describes the TDB execution model in detail. 
Section  IV  presents  some  implementation  considerations 
related  to  the  TDB  proposal.  Experimental  setup  and 
discussion of the evaluation results are given in Section V 
and  Section  VI.  Section  VII reviews  some  related  work. 
Finally, Section VIII concludes the paper. 
II.  OBJECTIVE: MASTER-SLAVE PRIVATE CACHE 
CONSISTENCY 
A  main  challenge  with  respect  to  core-level  fault-
tolerant  schemes  is  that  the  master-slave  pair  should  get 
identical  memory  values.  However,  external  write 
operations  between  the  read  operations  of  the  pair  may 
update the memory values. To prevent the intervention, the 
simplest way is to bypass the load values from the masters 
and preserve the data in shared buffers in the slaves. The 