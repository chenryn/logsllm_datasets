Table 4: Partitioning results for Tomcat, measured in classes
(#C), methods (#M), and lines of code (LoC). RSACipher
and RSAKeyPairGenerator are expliclity included for dy-
namic loading.
However, this has little impact on execution time because
our design does not synchronously context switch between
enclave and non-enclave execution; rather, Civet follows an
exitless pattern. There is a cost of additional CPU cycles (off
the critical path) to this design, which batching could reduce.
Hadoop determines the number of mappers and reducers
for a given workload based on how many “splits” the data
is divided into inside HDFS. We experiment with split sizes
ranging from 256MB to 4MB. We observe that, as the number
of splits increases well beyond the number of actual cores,
the overhead of scheduling degrades performance more than
any SGX-speciﬁc factor. Civet adds only 16–22% to the
end-to-end latency when running with SGX and deep input
type checks but without taint-tracking. The overhead of type
checks is marginal because of the integration with the class
instantiation of Fast-serialization. If taint-tracking is enabled
with only explicit ﬂow tracking, the overhead is 70–80%.
Furthermore, running a Hadoop task partitioned with Civet is
generally as scalable as native.
9.2 Tomcat
Tomcat [63] is a web server for hosting Java servlets in a
multi-tenant environment. A servlet is usually written to parse
HTTP requests, and can be a building block for microservices.
We partition an “echo” servlet into an enclave, which signs
Figure 12: Average HTTP response time of a request-signing
Tomcat servlet partitioned and executed by Civet, with SGX
and shielded by type checks (TC), compared to native. Lower
is better. The HTTP requests are issued by ab (ApacheBench),
with HTTP request concurrency up to 64.
the HTTP requests from the users using RSA and returns a
certiﬁcate in the response. This is another good ﬁt for Civet,
because the servlet needs to access a secret key to sign the
certiﬁcate. Thus, tenants do not need to expose their secret
keys to the web server or other servlets. Table 4 shows the
partitioning efﬁciency for Tomcat.
Figure 12 shows the average latency to sign requests in a
servlet, as a function of the number of concurrent requests.
In the Tomcat use case, we observe that the overhead of
introducing an enclave in Civet is nearly negligible. The
overheads are not SGX-speciﬁc, and can be improved by
selecting a more scalable conﬁguration for Tomcat.
9.3 GhaphChi
We use GraphChi [79] as more challenging case to partition.
We use the page rank program in GraphChi as a running ex-
ample. GraphChi is an in-memory framework for processing
large graphs, by sharding vertex and edge data of a graph. The
framework includes extensible interfaces for plugging graph
algorithms. The core engine, GraphChiEngine, is tuned for
parallel computing with multiple threads that reuse the graph
data cached in the DRAM. We demonstrate the sensitivity
to the effectiveness of partitioning using three case studies
shown in Figure 13 and evaluated in Table 5.
The simplest, most coarse-grained choice ( 1(cid:13)) is partition-
ing at the main function, Pagerank.main. This choice will
result in a relatively large TCB and the entire program will
run inside the enclave throughout the execution. Although
this choice does not provide any beneﬁt of partitioning, Civet
can still help identify the required classes and methods, and
shrink the class libraries.
A ﬁner-grained choice ( 2(cid:13)) is to partition at each graph
operation, e.g, Pagerank.update(). This method updates
the global GraphChiContext with the pagerank contribution
of each vertex. This approach will only process one vertex per
enclave transition, and is arguably too ﬁne-grained. Worse
yet, the input to Pagerank.update() is a ChiVertex object,
which only contains a pointer to the data blocks; this will
require copying the entire data blocks into the enclave for
the pointer to be valid. Although this choice is ﬁne-grained
in terms of the TCB, the enclave memory footprint is just
USENIX Association
29th USENIX Security Symposium    517
025507510012548163264128256Exec. Time (s)Number of Splits in HDFSNativeCivet+SGXCivet+SGX+TCCivet+SGX+TC+TT01231248163264Resp. Time (ms)# of Concurrent RequestsNativeCivet+SGX+TCFigure 13: A simpliﬁed call graph for the GraphChi page rank
program. Execution starts with Pagerank.main(), followed
by GraphChiEngine.run(). GraphChiEngine eventually
submits multiple jobs of running the Pagerank.update()
by the worker threads. We show three possible choices of
partition boundary in GraphChi.
Selected entry methods
Before partitioning
1(cid:13) Pagerank.main
2(cid:13) Pagerank.update
Shredding
#C
#M
47.1K 419.5K
8.7K 72.5K
LoC ∆%
4.6M
Class
1.1M 75%
Method 3.0K 14.5K 280.2K 94%
Class
1.1M 75%
Method 2.3K 12.2K 250.2K 95%
1.1M 75%
Method 2.3K 12.2K 250.2K 95%
8.7K 72.5K
8.7K 72.5K
GraphChiEngine$2.* Class
3(cid:13) GraphChiEngine$3.*
Table 5: Partitioning results for GraphChi Pagerank, parti-
tioned with three boundaries and measured in classes (#C),
methods (#M), and lines of code (LoC). For all three cases,
AESCipher is explicitly included for dynamic loading.
as large as does not reduce the memory footprint compared
to coarser-grained choices. Note that with only class-level
shredding, the TCB is the same as 1(cid:13) because the same set of
classes are referenced from the entry classes. With method-
level shredding, Civet further reduces ∼30K LoC in 2(cid:13).
A third option ( 3(cid:13)) is to partition at the granularity of
a batch of work, with enough inputs to amortize the en-
clave transition cost.
In the case of GraphChi, chunks
of graph data are submitted as Runnables to the work-
ers. These Runnables are deﬁned as inner classes called
GraphChiEngine$2 and GraphChiEngine$3. As shown
in Table 5, partitioning at these classes seemingly generates
the same TCB as partitioning at Pagerank.update, but per-
forms strictly better at run-time.
Figure 14 shows the execution time processing the page
Figure 14: Execution time of the GraphChi page rank pro-
gram. For Civet, we tested 2 different choices of partition-
ing boundary, one with Pagerank.update and one with
GraphChiEngine$2/$3.*. Both encrypt the graph states
and are shielded by type checks (TC).
Processing time
Workloads
DRAM
cost
Packaging
Points-to Shredding Phosphor & signing
analysis
46s
11s
21s
(Entry methods)
(a) Hadoop ( 2(cid:13))
4.5 GB
(b) Tomcat ( 1(cid:13))
2.5 GB
(c) GraphChi ( 3(cid:13)) 3.4 GB
Table 6: DRAM cost and processing time (for points-to anal-
ysis, shredding, Phosphor instrumentation, packaging, and
class signing) of Civet’s partition tool. Lower is better.
17s
11s
11s
6s
6s
6s
4s
4s
4s
ranks of the LiveJournal social network [80]. The data set is
∼1.1GB, with 4 million vertices and 69 million edges. Our
example shields the partition by encrypting the intermediate
graph states (e.g., in and out edges) cached in ChiVertex
objects. The graph itself is loaded through the ﬁle system and
can be shielded by the library OS.
We partition the page rank program with the two ﬁner-
grained options. We observe that the GraphChi program
caches the vertex data and edge data inside the DRAM, using
32768 raw blocks. GraphChi also assigns a memory budget
for each job, which decides the range of vertex data to be pro-
cessed. We reduce the conﬁguration to using 1024 raw blocks
and 16MB budget per job, to reduce the memory footprint and
RPC overhead. When partitioned with Pagerank.update,
the overhead can be up to 8.2–12.8× compared to native. Par-
titioning at GraphChiEngine$2/$3 lowers the overhead to
1.6–2.5×, due to fewer enclave RPCs.
Performance is generally insensitive to the number of
shards, except at very high numbers. Although fewer shards
implies fewer RPCs, any savings here are offset by the cost
of marshalling a larger data set. Thus, execution time is rela-
tively ﬂat until 64 shards, at which point the cost of additional
RPCs dominates and drives up execution time.
518    29th USENIX Security Symposium
USENIX Association
class GraphChiEngine{  void run(GraphChiProgram prog){    ...    ChiVertex[] vertices      = new ChiVertex[nvertices];    ...    }}ThreadPoolExecutorJob Queue(LinkedBlockingQueue)Worker Threadsnew Runnable(){  void run() {    for(ChiVertex v: vertices)      prog.update(v, ctx);  }} class Pagerank    extends GraphChiProgram{  void main(String[] args)}ChiVertex.getValue()BlockManager.dereference(ChiPointer ptr)➀➁➂submit()run()update()run()poll()void update(  ChiVertex       vertex,  GraphChiContext ctx)}Class boundary}Partition boundaryCall graph03006009001200150048163264128256Exec. Time (s)Number of Shards in GraphChiNativeCivet+SGX+TC (Pagerank.update)Civet+SGX+TC (GraphChiEngine$2/$3.*)Native
Civet
Compute
Input
Output
O
O FFT +/-
RSA +/-
AES +/-
.7
.3 .0
1.6 .1
475.9
.7 0.5× 14.8 .1
5.3 .0 15.5× 713.8
.5 0.4× 7.4 .1
4.0 .4 11.4× 671.3
0.4 .5
19.1
.3
2.8 .0
1.1
1.0 .0
4.5 .1
23.3
8.6 .1 27.7× 1161.5
9.1 1.3× 16.9 .3
O
8.0×
3.5×
9.6×
w/Phosphor
w/ Implicit ﬂow 22.7 .2 67.8× 4050.6 28.2 7.5× 19.6 .5 11.2×
Table 7: Execution time (in microseconds) of each method
and the breakdown of latency in Civet.
9.4 Static Analysis
Table 6 reports the DRAM cost and the processing time for
partitioning a Java application. We implement the Civet par-
titioning tool with Soot 3.3.0 and Apache Byte Code Engi-
neering Library (BCEL) 6.2. Partitioning millions lines of
Java code takes up to ∼1 minute and 4.5GB of DRAM in
our examples. A signiﬁcant portion of the partitioning time
is spent on whole-program points-to analysis. Our Spark
conﬁguration includes both application and library classes,
and uses on-the-ﬂy call graph analysis and a worklist-based
propagation algorithm.
9.5 Microbenchmarks
Table 7 shows the execution time of several microbenchmarks:
AES, RSA, and FFT, each of which demonstrate a different
performance pattern for partitioned enclave execution. For
each of the workloads, we break down the overheads into the
computation inside an enclave, and the latency of moving
inputs and outputs across the enclave boundary. We note that
Native does not incur the cost of moving inputs and outputs.
RSA has the lowest overhead among the three, as the work-
load is the most computation-intensive. For AES, the inputs
and outputs are also small, yet the computation itself suf-
fers up to 11.4× overhead. The difference is that execution
outside the enclave can make better use of the AES-NI instruc-
tions. FFT demonstrates a relatively data-intensive pattern,
and the overhead of transitioning the inputs and outputs is
4.5× in total. Phosphor incurs overhead because of the ad-
ditional instrumentation and runtime tracing.
It performs
worst in the AES benchmark (27.7× and 67.8×, without and
with implicit ﬂow tracking, respectively), which is the least
compute-intensive among the three, showing that the over-
head of taint-tracking (with Phosphor) dominates the running
time. In contrast, the taint-tracking incurs lower overheads in
the more compute-intensive RSA and FFT benchmarks.
9.6 Discussion
The three case studies show the challenges to creating a se-
cure and efﬁcient partition: one must consider not just points
to divide the code, but also the data ﬂow and the optimal
granularity for moving data in and out of an enclave. Our
results show that Civet is very effective at reducing the code
footprint for an enclave partition—removing 75% of the code
even in the coarsest partition.
In general, Civet introduces an acceptable overhead, end-
to-end, for applications. That said, our microbenchmarks
indicate up to an 15.5.× overhead on a short computation
(AES); thus, optimization such as batching inputs are impor-
tant to overall performance. Finally, adding dynamic tracking
of implicit ﬂows effected by the control ﬂow is considerably
more expensive than the rest of Civet. We leave exploration
of more efﬁcient implicit ﬂow tracking for future work.
10 Conclusion
This paper presents an enclave-aware JVM variant and a
framework for partitioning a large application onto enclaves.
Civet leverages language features to help developers reason
about the code that is and is not in the enclave. Simply drop-
ping a managed language runtime in SGX incurs an order-of-
magnitude slowdown. Civet also minimizes the code footprint
in the enclave, as well as adapting the garbage collector to the
hardware peculiarities of SGX.
Acknowledgments
We thank the anonymous reviewers, Mike Bond, and our
shepherd, Tuba Yavuz, for insightful comments on earlier
versions of this work. This work was supported in part by NSF
grants CNS-1228839, CNS-1405641, CNS-1700512, NSF
CISE Expeditions Award CCF- 1730628, as well as gifts from
the Sloan Foundation, Alibaba, Amazon Web Services, Ant
Financial, Arm, Capital One, Ericsson, Facebook, Google,
Intel, Microsoft, Scotiabank, Splunk and VMware. Bhushan
Jain was supported in part by an IBM Ph.D. Fellowship. Part
of this work was done while Tsai, Jain, and Porter were at
Stony Brook University, and while Tsai was at UC Berkeley.
McAvey’s current afﬁliation is with Apple; his contributions
were primarily made while a student at Hendrix college. We
thank Bozhen Liu for the help with the Soot framework.
References
[1] Frank McKeen, Ilya Alexandrovich, Alex Berenzon,
Carlos V. Rozas, Hisham Shaﬁ, Vedvyas Shanbhogue,
and Uday R. Savagaonkar. Innovative instructions and
software model for isolated execution. In HASP, 2013.
[2] AMD secure encrypted virtualization.
https:
//developer.amd.com/amd-secure-memory-
encryption-sme-amd-secure-encrypted-
virtualization-sev/.
USENIX Association
29th USENIX Security Symposium    519