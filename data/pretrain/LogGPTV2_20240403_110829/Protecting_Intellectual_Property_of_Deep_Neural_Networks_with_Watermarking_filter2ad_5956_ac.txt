CNN model LeNet [15] with 60k parameters to the recent model
VGG-16 [48] with 138M parameters. Such a large number of model
parameters make the deep learning computation expensive, but
also leave the space for pruning. The goal of model pruning is to
reduce redundant parameters, but still keep the performance of
original deep neural networks [8, 21, 41, 49, 51].
We adopt the same pruning algorithm used in [54], which prunes
the parameters whose absolute values are very small. The intuition
here is that small weights usually represent unimportant connec-
tions between neurons, and elimination of such connections incur
little impact on final classification results. During the pruning, for
all the models with watermark embedded, we remove the p% (from
10% to 90%) of parameters which has the lowest absolute values
by setting them to zero. Then we compare both the accuracy with
the normal testing dataset to evaluate impacts on the original func-
tionality of the model, and the accuracy of different watermarks
to evaluate impacts on our watermarking framework. Ideally, after
the model pruning, the plagiarizer who steals the models still wants
to keep the model accuracy.
Table 3 and Table 4 show the accuracy of clean testing data and
accuracy of watermarks for different models and datasets. For the
MNIST dataset, even 90% parameters are pruned, our embedded
models still have high accuracy (only drop 0.5% in the worst case)
for different watermarks while the accuracy of testing data drops
around 6%. For CIFAR10 dataset, even if we prune 80% parameters,
the accuracy of watermark is still much higher than accuracy of
testing data. We also notice that when 90% parameters are pruned,
the accuracy for W Munr elated drops to 10.93%. However, in this
case, removing our watermarks through model pruning also leads to
significant accuracy drop (16%) for the stolen model, which makes
the stolen model useless. Therefore, if the plagiarizer still wants to
keep the performance of the stolen models (e.g., 5% accuracy drop
at most), our watermarking is robust to such pruning modifications.
However, the plagiarizer can further disrupt our watermarks at the
expense of dramatically degrading the performance of the models.
Fine-tuning. As we discussed in Section 2, training a well-
designed deep neural network from scratch requires a large training
dataset, while insufficient data can greatly affect the DNNs’ perfor-
mance. Therefore, more often in practice, it will be easy to fine-tune
existing start-of-the-art models when sufficient training data is not
available [43, 56]. In general, if the dataset is not dramatically dif-
ferent in context from the dataset which the pre-trained model is
trained on, fine-tuning is a good choice. Therefore, fine-tuning can
be a very effective approach for plagiarizer to train a new model
on top of the stolen model with only fewer new training data. In
this way, the new model can inherit the performance of the stolen
model, but also looks different from the stolen model.
In this experiment, for each dataset, we split the testing dataset
into two halves. The first half is used for fine-tuning previously
trained DNNs while the second half is used for evaluating new
models. Then we still use testing accuracy and watermark accuracy
of new models to measure the robustness of our watermarking
framework for modifications caused by fine-tuning.
Table 5 shows the accuracy of clean testing data and accuracy
of watermarks for new models after fine-tuning. For the MNIST
dataset, fine-tuning does not reduce too much on the accuracy of
watermarks. This is because that there are too many redundant neu-
rons in the MNIST deep neural networks, which makes them robust
to such fine-tuning based modifications. For CIFAR10 dataset, it
seems thatW Mnoise is very sensitive to fine-tuning, while other em-
bedded watermarks are still robust to fine-tuning. Compare to em-
bedding methods W Mcontent and W Munr elated, noise generated
by W Mnoise is much more complicated. Fine-tuning for W Mnoise
essentially means adapting it to a different domain. Therefore, it
reduces a lot, but still have a relative high accuracy (69.13%).
5.5 Security
The goal of security is to measure whether our embedded water-
marks can be easily identified or modified by unauthorized parties.
In our design, the watermark space for all three watermark gen-
eration algorithms is almost infinite, therefore, those watermarks
should be robust to brute-force attacks. However, recently Fredrik-
son et al. [16] introduced the model inversion attack that can recover
images in the training dataset from deep neural networks. It follows
the gradient of prediction loss to modify the input image in order
to reverse-engineer representative samples in the target class. We
tend to test whether such model inversion attacks can reveal the
embedded watermarks.
We launch such attacks over all the models with watermarks
embedded. We start model inversion attacks from three types of
inputs: images from categories that we embedded watermarks,
blank image, and randomized image. Then we calculate the gradient
of prediction loss to the pre-defined category of watermarks 3. Such
3In practice, the category we embedded watermarks and the pre-defined categories of
watermarks should be unknown to attacks. Here we assume attackers know this and
try to recover our embedded watermarks through model inversion attacks.
8
Table 3: Robustness for model pruning: accuracy of clean testing data and accuracy of watermarks (MNIST)
W Mcontent
W Munr elated
Testing Acc. Watermark Acc. Testing Acc. Watermark Acc. Testing Acc. Watermark Acc.
W Mnoise
Table 4: Robustness for model pruning: accuracy of clean testing data and accuracy of watermarks (CIFAR10)
W Mcontent
W Munr elated
Testing Acc. Watermark Acc. Testing Acc. Watermark Acc. Testing Acc. Watermark Acc.
W Mnoise
Pruning rate
10%
20%
30%
40%
50%
60%
70%
80%
90%
Pruning rate
10%
20%
30%
40%
50%
60%
70%
80%
90%
99.44%
99.45%
99.43%
99.4%
99.29%
99.27%
99.18%
98.92%
97.03%
78.37%
78.42%
78.2%
78.24%
78.16%
77.87%
76.7%
74.59%
64.9%
100%
100%
100%
100%
100%
100%
100%
100%
99.95%
99.93%
99.93%
99.93%
99.93%
99.93%
99.86%
99.86%
99.8%
99.47%
99.43%
99.45%
99.41%
99.31%
99.19%
99.24%
98.82%
97.79%
93.55%
78.06%
78.08%
78.05%
77.78%
77.75%
77.44%
76.71%
74.57%
62.15%
100%
100%
100%
100%
100%
100%
100%
100%
99.9%
100%
100%
100%
100%
100%
100%
100%
96.39%
10.93%
99.4%
99.41%
99.41%
99.42%
99.41%
99.3%
99.22%
99.04%
95.19%
78.45%
78.5%
78.33%
78.31%
78.02%
77.87%
77.01%
73.09%
59.29%
100%
100%
100%
100%
100%
99.9%
99.9%
99.9%
99.55%
99.86%
99.86%
99.93%
99.93%
99.8%
99.6%
98.46%
92.8%
65.13%
Table 5: Robustness for model fine-tuning: accuracy of clean testing data and accuracy of watermarks
Dataset
MNIST
CIFAR10
W Mcontent
W Munr elated
Testing Acc. Watermark Acc. Testing Acc. Watermark Acc. Testing Acc. Watermark Acc.
W Mnoise
99.6%
77.55%
99.95%
98.33%
99.64%
76.75%
100%
95.33%
99.68%
78.43%
99.85%
69.13%
gradients are used further to modify the image toward pre-defined
category.
Figure 8 shows the recovery results for MNIST. Due to the page
limitation, the results for the CIFAR10 dataset in shown in Figure 9
of Appendix A. Starting from blank images or randomized images,
model inversion attack produces a random looking image that is
classified as an airplane. We cannot see anything related to our em-
bedded watermarks. However, when starting from training image
“1”, we can see some blur objects: Figure 8b shows something near
our embedded watermark “TEST”. Although such blur objects are
related to our embedded watermarks based on location, however,
adversaries cannot observe anything useful from such recovery.
Figure 8f shows something similar to “0”, which reflects that the
gradient does not drift towards our embedded watermarks, but to
the original image “0”. Therefore, this demonstrates that our three
embedding algorithms are robust to model inversion attacks.
Such result is expected since the recovered images from model
inversion attacks are usually the prototypical image in that class.
Consistent with the results as shown in [27], our experiments also
show that model inversion attacks cannot recover clear training
data for convolutional neural networks. Hitaj et al. [27] propose
a new attack using generative adversarial networks (GANs) to re-
cover training data for collaborative training. However, such attack
require to train a generative model together with a discriminative
model during the training process, which is not applicable for our
setting. Adversaries in our threat model can only get a pre-trained
model with watermarks, but are not able to intervene the training
process.
5.6 Comparison of different watermarks
In this section, we compare the trade-off among different water-
marks and summarize the insights we learned for DNN watermarks.
Functionality. All of our proposed watermarks can support
both white-box and black-box based ownership verification, since
they only require to access normal APIs for the verification.
Usability. W Mcontent is the best choice in terms of usability.
The original image can always get correct predictions and only
images with watermarks embedded get pre-defined predictions.
W Munr elated may cause false positives if the unrelated images
happen to be used as inputs, similar to W Mnoise.
9
(a) W Mcont ent watermark
(b) recover from image “1”
(c) recover from blank image
(d) recover from random noise
(e) W Munr el at ed watermark
(f) recover from image “1”
(g) recover from blank image
(h) recover from random noise
(i) W Mnoise watermark
(j) recover from image “1”
(k) recover from blank image
(l) recover from random noise
Figure 8: Model inversion attacks on MNIST
cations for both datasets.
recovered, it is still difficult to distinguish it from normal noise.
Security. W Mnoise is the most safe watermark, even it was
Robustness. W Mcontent is robust to all the evaluated modifi-
In summary, to make a good watermark for DNNs, one important
thing needs to be considered is the generality (“ generalization”
vs “overfitting”) of the watermark. “ Generalization” means that
any input follows the watermark patterns can trigger the model
with watermark embedded. For example, in our W Munr elated, any
forms of “1” can trigger the models to pre-defined prediction for
CIFAR10 data. “Overfitting” means that only specified image in the
training data can trigger watermark. For example, only one specified
“1” can trigger the model while other “1” cannot. “ Generalization”
makes watermarks robust to different modifications while it may
cause usability issues, since any input follows the same pattern can
trigger the model. “Overfitting” can reduce the usability issues, but
are more vulnerable to modification attacks. Therefore, for each
method, if we want to use an overfitted watermark, we need to train
the model with exactly the same watermark. However, if we want
to adopt a generalized watermark, we can train the model with
more diverse watermarks, e.g., training with data augmentation on
watermarks.
6 DISCUSSION
In this section, we discuss possible limitations and evasion of our
watermarking framework.
Limitation. Our evaluation has shown great performance of
the watermarking framework to protect the intellectual property of
deep neural networks once those models are leaked and deployed
as online services. However, if the leaked model is not deployed as
an on-line service but used as an internal service, then we cannot
detect that. In this way, the plagiarizer cannot directly monetize the
stolen models. In addition, our current watermarking framework
cannot protect the DNN models from being stolen through pre-
diction APIs [53]. In this attack, attackers can exploit the tension
between query access and confidentiality in the results to learn
the parameters of machine learning models. However, such attacks
work well for conversion machine learning algorithms such as de-
cision trees and logistic regressions. It needs more queries 100k,
10
where k is the number of model parameters for a two-layered neural
network, which makes it less effective for more complicated DNN
models (VGG-16 has 138M parameters). In addition, as discussed
in [53], such attacks can be prevented by changing APIs by not
returning confidences and not responding to incomplete queries. It