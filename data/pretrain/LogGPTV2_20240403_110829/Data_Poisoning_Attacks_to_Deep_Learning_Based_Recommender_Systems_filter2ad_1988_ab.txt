the attacker ﬁrst needs to register a number of fake users in a
web service associated with the recommender system. Each
fake user generates well-crafted rating scores for a chosen
subset of items. These fake data will be included in the training
dataset of the target recommender system and then poisons the
training process. According to whether data poisoning attacks
are focused on a speciﬁc type of recommender system, we
can divide them into two categories: algorithm-agnostic and
algorithm-speciﬁc. The former (e.g., types of shilling attacks
like random attacks [25], [31] and bandwagon attacks [25],
[35]) does not consider the algorithm used by the recommender
system and therefore often has limited effectiveness. For
instance, random attacks just choose rated items at random
from the whole item set for fake users, and bandwagon attacks
tend to select certain items with high popularity in the dataset
for fake users. The algorithm-speciﬁc data poisoning attacks
are optimized to a speciﬁc type of recommender systems
and have been developed for graph-based recommender sys-
tems [13], association-rule-based recommender systems [46],
matrix-factorization-based recommender systems [12], [28],
and neighborhood-based recommender systems [4]. As these
attacks are optimized, they often are more effective. However,
there is no study on algorithm-speciﬁc data poisoning attacks
to deep learning based recommender systems. We bridge this
gap in this paper.
Proﬁle Pollution Attacks. The key idea of proﬁle pollution
attacks is to pollute a users proﬁle (e.g., historical behavior)
0100···0MF User VectorMLP Layer 1MLP Layer 2MLP Layer XbyuiAAAB+XicbVBNS8NAEN34WetX1KOXxSJ4KokIeix68VjBfkAbwmazaZduNmF3Ugkh/8SLB0W8+k+8+W/ctjlo64OBx3szzMwLUsE1OM63tba+sbm1Xdup7+7tHxzaR8ddnWSKsg5NRKL6AdFMcMk6wEGwfqoYiQPBesHkbub3pkxpnshHyFPmxWQkecQpASP5tj184iEbEyjy0i8yXvp2w2k6c+BV4lakgSq0fftrGCY0i5kEKojWA9dJwSuIAk4FK+vDTLOU0AkZsYGhksRMe8X88hKfGyXEUaJMScBz9fdEQWKt8zgwnTGBsV72ZuJ/3iCD6MYruEwzYJIuFkWZwJDgWQw45IpRELkhhCpubsV0TBShYMKqmxDc5ZdXSfey6TpN9+Gq0bqt4qihU3SGLpCLrlEL3aM26iCKpugZvaI3q7BerHfrY9G6ZlUzJ+gPrM8ffP2UMw==AAAB+XicbVBNS8NAEN34WetX1KOXxSJ4KokIeix68VjBfkAbwmazaZduNmF3Ugkh/8SLB0W8+k+8+W/ctjlo64OBx3szzMwLUsE1OM63tba+sbm1Xdup7+7tHxzaR8ddnWSKsg5NRKL6AdFMcMk6wEGwfqoYiQPBesHkbub3pkxpnshHyFPmxWQkecQpASP5tj184iEbEyjy0i8yXvp2w2k6c+BV4lakgSq0fftrGCY0i5kEKojWA9dJwSuIAk4FK+vDTLOU0AkZsYGhksRMe8X88hKfGyXEUaJMScBz9fdEQWKt8zgwnTGBsV72ZuJ/3iCD6MYruEwzYJIuFkWZwJDgWQw45IpRELkhhCpubsV0TBShYMKqmxDc5ZdXSfey6TpN9+Gq0bqt4qihU3SGLpCLrlEL3aM26iCKpugZvaI3q7BerHfrY9G6ZlUzJ+gPrM8ffP2UMw==AAAB+XicbVBNS8NAEN34WetX1KOXxSJ4KokIeix68VjBfkAbwmazaZduNmF3Ugkh/8SLB0W8+k+8+W/ctjlo64OBx3szzMwLUsE1OM63tba+sbm1Xdup7+7tHxzaR8ddnWSKsg5NRKL6AdFMcMk6wEGwfqoYiQPBesHkbub3pkxpnshHyFPmxWQkecQpASP5tj184iEbEyjy0i8yXvp2w2k6c+BV4lakgSq0fftrGCY0i5kEKojWA9dJwSuIAk4FK+vDTLOU0AkZsYGhksRMe8X88hKfGyXEUaJMScBz9fdEQWKt8zgwnTGBsV72ZuJ/3iCD6MYruEwzYJIuFkWZwJDgWQw45IpRELkhhCpubsV0TBShYMKqmxDc5ZdXSfey6TpN9+Gq0bqt4qihU3SGLpCLrlEL3aM26iCKpugZvaI3q7BerHfrY9G6ZlUzJ+gPrM8ffP2UMw==yuiAAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOZpMxszPLPIRlyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtLa0itEUkl6obYU05E7RlmOG0myqKk4jTTjS5nfmdJ6o0k+LBZCkNEzwSLGYEGye1s0Fu2XRQrfl1fw60SoKC1KBAc1D96g8lsQkVhnCsdS/wUxPmWBlGOJ1W+lbTFJMJHtGeowInVIf5/NopOnPKEMVSuRIGzdXfEzlOtM6SyHUm2Iz1sjcT//N61sTXYc5Eag0VZLEothwZiWavoyFTlBieOYKJYu5WRMZYYWJcQBUXQrD88ippX9QDvx7cX9YaN0UcZTiBUziHAK6gAXfQhBYQeIRneIU3T3ov3rv3sWgtecXMMfyB9/kD/3KPZA==AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOZpMxszPLPIRlyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtLa0itEUkl6obYU05E7RlmOG0myqKk4jTTjS5nfmdJ6o0k+LBZCkNEzwSLGYEGye1s0Fu2XRQrfl1fw60SoKC1KBAc1D96g8lsQkVhnCsdS/wUxPmWBlGOJ1W+lbTFJMJHtGeowInVIf5/NopOnPKEMVSuRIGzdXfEzlOtM6SyHUm2Iz1sjcT//N61sTXYc5Eag0VZLEothwZiWavoyFTlBieOYKJYu5WRMZYYWJcQBUXQrD88ippX9QDvx7cX9YaN0UcZTiBUziHAK6gAXfQhBYQeIRneIU3T3ov3rv3sWgtecXMMfyB9/kD/3KPZA==AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzAOSJcxOZpMxszPLPIRlyT948aCIV//Hm3/jJNmDJhY0FFXddHdFKWfa+P63V1pb39jcKm9Xdnb39g+qh0dtLa0itEUkl6obYU05E7RlmOG0myqKk4jTTjS5nfmdJ6o0k+LBZCkNEzwSLGYEGye1s0Fu2XRQrfl1fw60SoKC1KBAc1D96g8lsQkVhnCsdS/wUxPmWBlGOJ1W+lbTFJMJHtGeowInVIf5/NopOnPKEMVSuRIGzdXfEzlOtM6SyHUm2Iz1sjcT//N61sTXYc5Eag0VZLEothwZiWavoyFTlBieOYKJYu5WRMZYYWJcQBUXQrD88ippX9QDvx7cX9YaN0UcZTiBUziHAK6gAXfQhBYQeIRneIU3T3ov3rv3sWgtecXMMfyB9/kD/3KPZA==MLP User Vector0010···0MF Item VectorMLP Item VectorMF LayerNeuMF Layer……ReLUReLUElement-wise ProductConcatenationConcatenationTrainingScoreUseruAAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipmQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryv12zyOIpzBOVyCBzWowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A4a2M+Q==AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipmQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryv12zyOIpzBOVyCBzWowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A4a2M+Q==AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipmQ7KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGNn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryv12zyOIpzBOVyCBzWowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A4a2M+Q==iAAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipyQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/PfYztAAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipyQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/PfYztAAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipyQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/PfYztItemvia cross-site request forgery (CSRF) [49]. For instance, Xing
et.al. [45] proposed proﬁle pollution attacks to recommender
systems in web services, e.g., YouTube, Amazon, and Google.
Their study shows that all these services are vulnerable to
their attacks. However, proﬁle pollution attacks have two key
limitations: i) proﬁle pollution attacks rely on CSRF, which
makes it hard to perform the attacks at a large scale, and ii)
proﬁle pollution attacks can not be applied to item-to-item
recommender systems because the attackers are not able to
pollute the proﬁle of an item [46].
III. PROBLEM FORMULATION
In this section, we ﬁrst present our threat model and then
we formulate our poisoning attack as an optimization problem.
A. Threat Model
Attacker’s Goal. We consider an attacker’s goal is to promote
a target item. Speciﬁcally, suppose a recommender system
recommends top-K items for each user. An attacker’s goal is to
make its target item appear in the top-K recommendation lists
of as many normal users as possible. We note that an attacker
could also aim to demote a target item, making it appear in
the top-K recommendation lists of as few normal users as
possible. For instance, an attacker may demote its competitor’s
items. Since demoting a target item can be implemented by
promoting other items [46], we focus on promotion in this
work.
Attacker’s Background Knowledge. We assume an attacker
has access to the user-item interaction matrix Y. In many
recommender systems such as Amazon and Yelp, users’ ratings
are public. Therefore, an attacker can write a crawler to collect
users’ ratings. However, in our experiments, we will also show
that our attack is still effective when the attacker has access to
a partial user-item interaction matrix. The attacker may or may
not have access to the internal neural network architecture of
the target deep learning based recommender system. When
the attacker does not have access to the neural network
architecture of the target recommender system, the attacker
performs attacks by assuming a neural network architecture. As
we will show in experiments, our attack can transfer between
different neural networks, i.e., our attack constructed based on
one neural network architecture based recommender system is
also effective for other recommender systems that use different
neural network architectures.
Attacker’s Capabilities. We assume that an attacker has
limited resources, so the attacker can only inject a limited
number of fake users. We use m to denote the upper bound of
the number of fake users. In addition to the target item, each
fake user can rate up to n other items to evade trivial detection.
We call these items ﬁller items. Speciﬁcally, normal users often
rate a small number of items, and thus fake users who rate
a large number of items are suspicious and can be detected
easily. We assume the attacker can inject the fake users’ ratings
into the training dataset of the target recommender system to
manipulate the training process of the deep learning model.
B. Formulating Attacks as an Optimization Problem
We deﬁne the hit ratio of an item t, denoted as HRt, as
the fraction of normal users who would receive the item t
in their top-K recommendation lists. In other words, the hit
ratio of t indicates the probability that t is recommended to
a normal user. An attacker’s goal is to maximize the hit ratio
of a target item t. Let y(v) denote the rating score vector of
the fake user v, and yvi denote the rating score that the fake
user v gives to item i. A rating score is an element in a set
of integers {0, 1, . . . , rmax}, where yvi = 0 means that the
fake user v has not rated item i and yvi > 0 represents the
preference score fake user v gives to item i. For instance,
rmax = 5 in many recommender systems. Our goal is to craft
the ratings for the fake users such that the hit ratio of the target
item is maximized. Formally, following previous work [13], we
formulate crafting the ratings for the fake users as solving the
following optimization problem:
max HRt
yvi ∈ {0, 1, . . . , rmax},
subject to ||y(v)(cid:107)0 ≤ n + 1,∀v ∈ {v1, v2, . . . , vm},
(1)
where (cid:107)y(v)(cid:107)0 is the number of non-zero entries in fake user
v’s rating score vector y(v), n is the maximum number of
ﬁller items, m is the maximum number of fake users, and
{v1, v2, . . . , vm} is the set of m fake users.
IV. ATTACK CONSTRUCTION: SOLVING THE
OPTIMIZATION PROBLEM
A. Overview of Our Proposed Attacks
A data poisoning attack is essentially to solve the optimiza-
tion problem in Eq. (1). However, the optimization problem
is computationally intractable as it is a non-convex integer
programming problem. To address the challenge, we develop
multiple heuristics to approximately solve the optimization
problem. Our heuristics are inspired by previous work [13]
on attacking graph-based recommender systems. Figure 2
shows the overview of our data poisoning attack. First, we
approximate the hit ratio using a loss function, where a smaller
loss roughly corresponds to a higher hit ratio. Given the
loss function, we transform the optimization problem into a
tractable one. Second, based on our designed loss function,
we construct a poison model to simulate a compromised deep
learning based recommender system. In particular, we ﬁrst pre-
train the poison model to ensure that it can correctly predict
the preferences of users by using the validation dataset, and
then we update the poison model using a loss function, which
is derived by extracting the attack related parts in the loss
function obtained in the ﬁrst step, to approach the expected
state of the compromised target recommender system. Third,
we select ﬁller items for a fake user according to its rating
score vector predicted by the poison model and a selection
probability vector, where the selection probability vector of
items is periodically updated to choose ﬁller items for the next
fake user. We repeat the second and third steps until m fake
users are generated for the poisoning attack.
B. Approximating the Hit Ratio
The optimization problem we formulated in Eq. (1) is
computationally intractable because the rating scores are in-
teger variables in the domain {0, 1, . . . , rmax} and the hit
ratio is a highly non-linear non-differentiable function of
the rating scores due to the complexity of the recommender
4
Fig. 2: An overview of our data poisoning attack. We ﬁrst use approximation methods to transform the optimization problem
into a tractable one and obtain a loss function. Second, according to the obtained loss function, the algorithm used in the target
recommender system, and the training dataset, we train a poison model that simulates the compromised target recommender
system. Third, we select ﬁller items according to the predicted rankings generated by the poison model and the selection
probability. Note that, we will repeat the second and third steps until enough fake users are generated to construct the attack,
and the selection probability will be updated in each iteration.
system. To address the computational challenge, we design
multiple techniques to convert the optimization problem into
a computationally tractable one.
Relaxing Rating Scores to Obtain Continuous Variables.
As for the rating scores in Y and (cid:98)Y, we can treat them as
continuous variables in our attacking process. Speciﬁcally, the
predicted rating scores, which range from 0.0 to 1.0 in the
recommender systems built upon implicit datasets, can be seen
as correlations between users and items. After acquiring ﬁnal
rating scores from the target recommender system, we can
project them into discrete integer numbers if necessary.
Approximating the Hit Ratio. The hit ratio HRt
is the
proportion of normal users whose top-K recommendation lists
include the target item t. Since HRt is a highly non-linear non-
differentiable function of the users’ rating scores, we propose
to use a loss function to approximate it. In particular, a smaller
loss roughly corresponds to a higher hit ratio. Normally, a
recommender system uses the predicted user-item interaction
matrix (cid:98)Y to make recommendations for users. Therefore, we
propose to use the following steps to convert the optimization
problem as shown in Eq. (1).
1)
Loss Function for Each User. We leverage a loss
function lu over the predicted score vector for each
user to increase the hit ratio for target item t. Intu-
itively, if the target item t is already included in the
recommendation list Lu of user u, it is not necessary
to further improve this recommendation. Otherwise,
we should reﬂect the requirement of the user u in lu
and promote the target item t to get a better ranking
among all items. We apply the following loss function
for user u:
lu = max{min
i∈Lu
log[(cid:98)yui] − log[(cid:98)yut],−κ},
(2)
where κ ≥ 0 is a tunable parameter that can be
used to enhance the robustness and transferability
of our attack. The use of the log operator lessens
the dominance effect, while preserving the order of
conﬁdence scores due to the monotonicity. As we can
see, if a target item t is in Lu, lu will be 0 when
κ = 0. Otherwise, larger (cid:98)yut that is smaller than the
2)
3)
minimum value of(cid:98)yui in Lu, larger the positive value
of lu will be. κ can ensure the target item t keep a
distance from the item with the lowest rating in Lu.
Thus, we can have a higher probability to include the
target item t in the recommendation list of user u by
minimizing the loss function lu.
Loss Function for All Users. Now we build a loss
function for all users. Since our attack goal is to
promote the target item to as many users as possible,
we design a loss function over all users according to
Eq. (2) as follows:
lu,
(3)
l
(cid:48) =(cid:88)u∈S
where S is the set of all normal users who have not
rated target item t yet.
Converting the Optimization Problem. After re-
laxing discrete variables to continuous variables and
approximating the hit ratio, we can approximate the
optimization problem as follows:
min G[y(v)] = (cid:107)y(v)(cid:107)2
2 + η · l
subject to yvi ∈ [0, rmax],
(4)
(cid:48)
where η > 0 is a coefﬁcient to achieve the objective
of promoting the target item t with a limited number
of ratings from fake users. Here, we use the (cid:96)2
norm to replace the (cid:96)0 norm in Eq. (1), in order to
facilitate the calculation of gradients and the stepwise
approximation of global optimal values because the
(cid:96)0 norm can only compare a limited number of ﬁller
item combinations and cannot continuously change,
while (cid:96)1 regularization generates sparse rating score
vectors, which will reduce the diversity of the se-
lected ﬁller items for fake users. As for the constraint
on the number of ﬁller items, we can achieve it by
choosing only a limited number of items for fake user
v based on his ﬁnal rating score vector y(v). Thus, we
can generate fake users by solving the optimization
problem above.
As the users and items in deep learning based recommender
systems are completely with discrete labels, gradients will
5
Predictedratings(2) Constructing The Poison Model(3) Selecting Filler ItemsLoss functionSelectionProbabilityAlgorithmRating data(1) Approximating The Hit RatioTraining DatasetSelecting Filler Items for UsersUpdated rating dataFake UserPoison TrainingPre-trainingTarget SystemG[y(v), w∗]
min
y(v)
subject to w∗ = arg min
L[w, Y ∪ y(v)],
(5)
w
disappear when they back-propagate to the input layer. Thus,
it is infeasible to directly adopt the back-gradient optimization
method that has been applied to attack image classiﬁers [33].
A natural idea is to treat the rating score vector y(v) of fake
user v as the independent variable and formulate the poisoning
attack as follows:
where w∗ represents model parameters, and L is the original
loss function for training the target recommender system. This
is a bilevel optimization problem as the lower-level constraint
for w∗ also depends on y(v). It is quite challenging to solve
this optimization problem for deep learning models because the
model parameters w∗ need to be updated through re-training
the model once y(v) changes. The process would be time-
consuming because it needs to generate enough fake users
if we directly compute high order gradients w.r.t. y(v) and
repeat the training process with the whole dataset in each
iteration when we gradually update the rating score vector
y(v). In particular, we require a large number of of iterations,
even thousands of iterations, to accumulate enough changes
on the randomly initialized rating score vector for each fake
user, which is not practical for large recommender systems
in the real world. Moreover, the rating score matrix used by
the recommender systems is usually sparse, and the neural
network trained on it might generate predicted rating scores
that vary within a certain range, which will be misleading
for gradient-based optimization algorithms since they are with
small
learning rate and can be easily interfered with the
randomness of model training.
C. Constructing the Poison Model
Speciﬁcally in this step, we construct the poison model to
guide the selection of ﬁller items for each fake user according
to the obtained loss functions so that we can efﬁciently
construct
the attack. Here, we investigate and utilize the
characteristics of a recommender system itself from a new
perspective. For a deep learning based recommender system,
as a special type of neural network, it tries to reduce the
entropy between users’ predicted score vectors and real rating
score vectors during the training process. Intuitively, items
with higher scores in user u’s predicted rating score vector
are more likely to have been rated by user u in reality with
high scores than other items. If we can successfully construct
a poison model to simulate the expected state of the original
recommender system after a successful poisoning attack, we
can infer what kind of fake users in the training dataset
can contribute most to the current recommender system. The
poison model, derived from the initial target recommender
system, periodically updates during the attack to approach our
attack goal gradually. We can then use the poison model to give
predictions on fake users’ preferences and choose the items