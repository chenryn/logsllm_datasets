Current Hand: 139
In this example, which has been shortened for space reasons, there are multiple driver-
associated timers, due to expire shortly, associated with the Netbt.sys and Tcpip.sys drivers (both 
related to networking), as well as Ntfs, the storage controller driver drivers. There are also back-
ground housekeeping timers due to expire, such as those related to power management, ETW, 
kernel-mode timers that are used for wait dispatching. You can use !thread on the thread point-
ers to verify this. 
that checks for Daylight Savings Time time-zone changes, the timer that checks for the arrival 
of the upcoming year, and the timer that checks for entry into the next century. One can easily 
locate them based on their typically distant expiration time, unless this experiment is performed 
on the eve of one of these events.
76 
CHAPTER 8 System mechanisms
appropriately program the interrupt controller, as well as determine to which processors it will send an 
IPI to initiate timer processing.
Time Interrupt
Time
Software Timer Expiration
Processor 1
Processor 0
Time
FIGURE 8-22 Intelligent timer tick distribution applied to processor 1.
Leaving as large a gap as possible is important due to the way power management works in proces-
sors: as the processor detects that the workload is going lower and lower, it decreases its power consump-
and enter deeper and deeper idle/sleep states, such as turning off caches. However, if the processor has 
to wake again, it will consume energy and take time to power up; for this reason, processor designers will 
risk entering these lower idle/sleep states (C-states) only if the time spent in a given state outweighs the 
time and energy it takes to enter and exit the state. Obviously, it makes no sense to spend 10 ms to enter a 
sleep state that will last only 1 ms. By preventing clock interrupts from waking sleeping processors unless 
needed (due to timers), they can enter deeper C-states and stay there longer.
Timer coalescing
Although minimizing clock interrupts to sleeping processors during periods of no timer expiration 
gives a big boost to longer C-state intervals, with a timer granularity of 15 ms, many timers likely will 
be queued at any given hand and expire often, even if just on processor 0. Reducing the amount 
of software timer-expiration work would both help to decrease latency (by requiring less work at 
DISPATCH_LEVEL) as well as allow other processors to stay in their sleep states even longer. (Because 
result in longer sleep times.) In truth, it is not just the number of expiring timers that really affects sleep 
state (it does affect latency), but the periodicity of these timer expirations—six timers all expiring at the 
same hand is a better option than six timers expiring at six different hands. Therefore, to fully optimize 
idle-time duration, the kernel needs to employ a coalescing mechanism to combine separate timer 
hands into an individual hand with multiple expirations.
Timer coalescing works on the assumption that most drivers and user-mode applications do not 
-
-
while a driver polling every second could probably poll every second plus or minus 50 ms without too 
CHAPTER 8 System mechanisms
77
and other times at half a second. Even so, not all timers are ready to be coalesced into coarser granu-
larities, so Windows enables this mechanism only for timers that have marked themselves as coales-
cable, either through the KeSetCoalescableTimer kernel API or through its user-mode counterpart, 
SetWaitableTimerEx.
With these APIs, driver and application developers are free to provide the kernel with the maximum 
tolerance
time past the requested period at which the timer will still function correctly. (In the previous ex-
ample, the 1-second timer had a tolerance of 50 ms.) The recommended minimum tolerance is 32 ms, 
any coalescing because the expiring timer could not be moved even from one clock tick to the next. 
preferred coalesc-
ing intervals: 1 second, 250 ms, 100 ms, or 50 ms.
When a tolerable delay is set for a periodic timer, Windows uses a process called shifting, which 
causes the timer to drift between periods until it gets aligned to the most optimal multiple of the 
is scanned, and a preferred expiration time is generated based on the closest acceptable coalescing 
always pushed out as far as possible past their real expiration point, which spreads out timers as far as 
possible and creates longer sleep times on the processors.
and are thus coalescable. In one scenario, Windows could decide to coalesce the timers as shown in 
do for some of the clock interrupts on processor 0, possibly removing the latency of requiring a drop 
to DISPATCH_LEVEL at each clock interrupt.
Time Interrupt
Time
Software Timer Expiration
Processor 1
Processor 0
Time
FIGURE 8-23 Timer coalescing.
78 
CHAPTER 8 System mechanisms
Enhanced timers
Enhanced timers were introduced to satisfy a long list of requirements that previous timer system 
also made timers have inconsistent expiration times, even when there was no need to reduce power (in 
other words, coalescing was an all-or-nothing proposition). Second, the only mechanism in Windows 
for high-resolution timers was for applications and drivers to lower the clock tick globally, which, as 
these timers was now higher, they were not necessarily more precise because regular time expiration 
can happen before
added features such as timer virtualization and the Desktop Activity Moderator (DAM), which actively de-
lay the expiration of timers during the resiliency phase of Modern Standby to simulate S3 sleep. However, 
some key system timer activity must still be permitted to periodically run even during this phase.
These three requirements led to the creation of enhanced timers, which are also internally known as 
Timer2 objects, and the creation of new system calls such as NtCreateTimer2 and NtSetTimer2, as well 
as driver APIs such as ExAllocateTimer and ExSetTimer. Enhanced timers support four modes of behav-
ior, some of which are mutually exclusive:
I 
No-wake This type of enhanced timer is an improvement over timer coalescing because it
provides for a tolerable delay that is only used in periods of sleep.
I 
High-resolution This type of enhanced timer corresponds to a high-resolution timer with a
precise clock rate that is dedicated to it. The clock rate will only need to run at this speed when
approaching the expiration of the timer.
I 
Idle-resilient This type of enhanced timer is still active even during deep sleep, such as the
resiliency phase of modern standby.
I 
Finite This is the type for enhanced timers that do not share one of the previously described
properties.
“special” behavior, why create them at all? It turns out that since the new Timer2 infrastructure was a 
I 
It uses self-balancing red-black binary trees instead of the linked lists that form the timer table.
I 
It allows drivers to specify an enable and disable callback without worrying about manually
creating DPCs.
I 
It includes new, clean, ETW tracing entries for each operation, aiding in troubleshooting.
I 
It provides additional security-in-depth through certain pointer obfuscation techniques and
additional assertions, hardening against data-only exploits and corruption.
CHAPTER 8 System mechanisms
79
Therefore, driver developers that are only targeting Windows 8.1 and later are highly recommended 
to use the new enhanced timer infrastructure, even if they do not require the additional capabilities.
Note The documented ExAllocateTimer API does not allow drivers to create idle-resilient 
timers. In fact, such an attempt crashes the system. Only Microsoft inbox drivers can 
create such timers through the ExAllocateTimerInternal API. Readers are discouraged from 
attempting to use this API because the kernel maintains a static, hard-coded list of every 
has knowledge of how many such timers the component is allowed to create. Any violations 
result in a system crash (blue screen of death).
Enhanced timers also have a more complex set of expiration rules than regular timers because they 
end up having two possible due timesminimum due time-
tem clock time at which point the timer is allowed to expire. The second, maximum due time, is the lat-
est system clock time at which the timer should ever expire. Windows guarantees that the timer will ex-
pire somewhere between these two points in time, either because of a regular clock tick every interval 
(such as 15 ms), or because of an ad-hoc check for timer expiration (such as the one that the idle thread 
does upon waking up from an interrupt). This interval is computed by taking the expected expiration 
time passed in by the developer and adjusting for the possible “no wake tolerance” that was passed in. 
As such, a Timer2 object lives in potentially up to two red-black tree nodes—node 0, for the mini-
mum due time checks, and node 1, for the maximum due time checks. No-wake and high-resolution 
two nodes? Instead of a single red-black tree, the system obviously needs to have more, which are 
called collections
depending on the rules and combinations shown in Table 8-11.
TABLE 8-11 Timer types and node collection indices
Timer type
Node 0 collection index
Node 1 collection index
No-wake
NoWake, if it has a tolerance
NoWake, if it has a non-unlimited or no tolerance
Never inserted in this node
Finite
High-resolution
Hr, always
Finite, if it has a non-unlimited or no tolerance
Idle-resilient
NoWake, if it has a tolerance
Ir, if it has a non-unlimited or no tolerance
High-resolution & Idle-resilient
Hr, always
Ir, if it has a non-unlimited or no tolerance
Think of node 1 as the one that mirrors the default legacy timer behavior—every clock tick, check if 
implies that its minimum due time is the same as its maximum due time. If it has unlimited tolerance; 
80 
CHAPTER 8 System mechanisms
sleeping forever. 
-
posed to expire and never earlier, so node 0 is used for them. However, if their precise expiration time is 
“too early” for the check in node 0, they might be in node 1 as well, at which point they are treated like 
caller provided a tolerance, the system is idle, and there is an opportunity to coalesce the timer.
NoWake collec-
Hr collection other-
wise. However, on the clock tick, which checks node 1, it must be in the special Ir collection to recognize 
that the timer needs to execute even though the system is in deep sleep.
-
ers to behave correctly when checked either at the system clock tick (node 1—enforcing a maximum 
due time) or at the next closest due time computation (node 0—enforcing a minimum due time).
As each timer is inserted into the appropriate collection (KTIMER2_COLLECTION) and associated 
next due time is updated to be the earliest due time of any timer 
in the collection, whereas a global variable (KiNextTimer2Due)
timer in any collection.
EXPERIMENT: Listing enhanced system timers
which are shown at the bottom of the output:
KTIMER2s: 
Address,
Due time,
Exp. Type   Callback, Attributes, 
ffffa4840f6070b0   1825b8f1f4 [11/30/2020 20:50:16.089] (Interrupt) [None] NWF (1826ea1ef4 
[11/30/2020 20:50:18.089]) 
ffffa483ff903e48   1825c45674 [11/30/2020 20:50:16.164] (Interrupt) [None] NW P (27ef6380) 
ffffa483fd824960   1825dd19e8 [11/30/2020 20:50:16.326] (Interrupt) [None] NWF (1828d80a68 
[11/30/2020 20:50:21.326]) 
ffffa48410c07eb8   1825e2d9c6 [11/30/2020 20:50:16.364] (Interrupt) [None] NW P (27ef6380) 
ffffa483f75bde38   1825e6f8c4 [11/30/2020 20:50:16.391] (Interrupt) [None] NW P (27ef6380) 
ffffa48407108e60   1825ec5ae8 [11/30/2020 20:50:16.426] (Interrupt) [None] NWF (1828e74b68 
[11/30/2020 20:50:21.426]) 
ffffa483f7a194a0   1825fe1d10 [11/30/2020 20:50:16.543] (Interrupt) [None] NWF (18272f4a10 
[11/30/2020 20:50:18.543]) 
ffffa483fd29a8f8   18261691e3 [11/30/2020 20:50:16.703] (Interrupt) [None] NW P (11e1a300) 
ffffa483ffcc2660   18261707d3 [11/30/2020 20:50:16.706] (Interrupt) [None] NWF (18265bd903 
[11/30/2020 20:50:17.157]) 
ffffa483f7a19e30   182619f439 [11/30/2020 20:50:16.725] (Interrupt) [None] NWF (182914e4b9 
[11/30/2020 20:50:21.725]) 
ffffa483ff9cfe48   182745de01 [11/30/2020 20:50:18.691] (Interrupt) [None] NW P (11e1a300) 
ffffa483f3cfe740   18276567a9 [11/30/2020 20:50:18.897] (Interrupt) 
Wdf01000!FxTimer::_FxTimerExtCallbackThunk (Context @ ffffa483f3db7360) NWF 
(1827fdfe29 [11/30/2020 20:50:19.897]) P (02faf080) 
EXPERIMENT: Listing enhanced system timers
which are shown at the bottom of the output:
KTIMER2s:
Address,
Due time,
Exp. Type   Callback, Attributes,
ffffa4840f6070b0   1825b8f1f4 [11/30/2020 20:50:16.089] (Interrupt) [None] NWF (1826ea1ef4
[11/30/2020 20:50:18.089])
ffffa483ff903e48   1825c45674 [11/30/2020 20:50:16.164] (Interrupt) [None] NW P (27ef6380)
ffffa483fd824960   1825dd19e8 [11/30/2020 20:50:16.326] (Interrupt) [None] NWF (1828d80a68 
[11/30/2020 20:50:21.326])
ffffa48410c07eb8   1825e2d9c6 [11/30/2020 20:50:16.364] (Interrupt) [None] NW P (27ef6380)
ffffa483f75bde38   1825e6f8c4 [11/30/2020 20:50:16.391] (Interrupt) [None] NW P (27ef6380)
ffffa48407108e60   1825ec5ae8 [11/30/2020 20:50:16.426] (Interrupt) [None] NWF (1828e74b68 
[11/30/2020 20:50:21.426])
ffffa483f7a194a0   1825fe1d10 [11/30/2020 20:50:16.543] (Interrupt) [None] NWF (18272f4a10 
[11/30/2020 20:50:18.543])
ffffa483fd29a8f8   18261691e3 [11/30/2020 20:50:16.703] (Interrupt) [None] NW P (11e1a300)
ffffa483ffcc2660   18261707d3 [11/30/2020 20:50:16.706] (Interrupt) [None] NWF (18265bd903 
[11/30/2020 20:50:17.157])
ffffa483f7a19e30   182619f439 [11/30/2020 20:50:16.725] (Interrupt) [None] NWF (182914e4b9 
[11/30/2020 20:50:21.725])
ffffa483ff9cfe48   182745de01 [11/30/2020 20:50:18.691] (Interrupt) [None] NW P (11e1a300)
ffffa483f3cfe740   18276567a9 [11/30/2020 20:50:18.897] (Interrupt) 
Wdf01000!FxTimer::_FxTimerExtCallbackThunk (Context @ ffffa483f3db7360) NWF
(1827fdfe29 [11/30/2020 20:50:19.897]) P (02faf080)
CHAPTER 8 System mechanisms
81
ffffa48404c02938   18276c5890 [11/30/2020 20:50:18.943] (Interrupt) [None] NW P (27ef6380) 
ffffa483fde8e300   1827a0f6b5 [11/30/2020 20:50:19.288] (Interrupt) [None] NWF (183091c835 
[11/30/2020 20:50:34.288]) 
ffffa483fde88580   1827d4fcb5 [11/30/2020 20:50:19.628] (Interrupt) [None] NWF (18290629b5 
[11/30/2020 20:50:21.628])
In this example, you can mostly see No-wake (NW) enhanced timers, with their minimum due 
time shown. Some are periodic (P) and will keep being reinserted at expiration time. A few also 
System worker threads
During system initialization, Windows creates several threads in the System process, called system 
worker threads, which exist solely to perform work on behalf of other threads. In many cases, threads 
executing at DPC/dispatch level need to execute functions that can be performed only at a lower IRQL. 
can usurp any thread in the system) at DPC/dispatch level IRQL, might need to access paged pool or 
wait for a dispatcher object used to synchronize execution with an application thread. Because a DPC 
DPC/dispatch level.
Some device drivers and executive components create their own threads dedicated to processing 
work at passive level; however, most use system worker threads instead, which avoids the unneces-
sary scheduling and memory overhead associated with having additional threads in the system. An 
ExQueueWorkItem or IoQueueWorkItem. Device drivers should use only the latter (because this as-
sociates the work item with a Device object, allowing for greater accountability and the handling of 
scenarios in which a driver unloads while its work item is active). These functions place a work item on 
a queue dispatcher object where the threads look for work. (Queue dispatcher objects are described in 
more detail in the section “I/O completion ports” in Chapter 6 in Part 1.) 
The IoQueueWorkItemEx, IoSizeofWorkItem, IoInitializeWorkItem, and IoUninitializeWorkItem APIs 
Work items include a pointer to a routine and a parameter that the thread passes to the routine 
when it processes the work item. The device driver or executive component that requires passive-level 
can initialize a work item that points to the routine in the driver that waits for the dispatcher object. At 
ffffa48404c02938   18276c5890 [11/30/2020 20:50:18.943] (Interrupt) [None] NW P (27ef6380)
ffffa483fde8e300   1827a0f6b5 [11/30/2020 20:50:19.288] (Interrupt) [None] NWF (183091c835
[11/30/2020 20:50:34.288])
ffffa483fde88580   1827d4fcb5 [11/30/2020 20:50:19.628] (Interrupt) [None] NWF (18290629b5 
[11/30/2020 20:50:21.628])
In this example, you can mostly see No-wake (NW) enhanced timers, with their minimum due 
time shown. Some are periodic (P) and will keep being reinserted at expiration time. A few also 
82 
CHAPTER 8 System mechanisms
worker thread processes its work item. 
There are many types of system worker threads:
I 
Normal worker threads execute at priority 8 but otherwise behave like delayed worker threads.
I 
Background worker threads execute at priority 7 and inherit the same behaviors as normal
worker threads.
I 
Delayed worker threads
time-critical.
I 
Critical worker threads execute at priority 13 and are meant to process time-critical work items.
I 
Super-critical worker threads execute at priority 14, otherwise mirroring their critical counterparts.
I 
Hyper-critical worker threads execute at priority 15 and are otherwise just like other critical threads.
I 
Real-time worker threads execute at priority 18, which gives them the distinction of operating in
the real-time scheduling range (see Chapter 4 of Part 1 for more information), meaning they are
not subject to priority boosting nor regular time slicing.
Because the naming of all of these worker queues started becoming confusing, recent versions of 
Windows introduced custom priority worker threads, which are now recommended for all driver devel-
opers and allow the driver to pass in their own priority level.
A special kernel function, ExpLegacyWorkerInitialization, which is called early in the boot process, 
optional registry parameters. You may even have seen these details in an earlier edition of this book. 
Note, however, that these variables are there only for compatibility with external instrumentation tools 
and are not actually utilized by any part of the kernel on modern Windows 10 systems and later. This is 
because recent kernels implemented a new kernel dispatcher object, the priority queue (KPRIQUEUE), 
coupled it with a fully dynamic number of kernel worker threads, and further split what used to be a 
single queue of worker threads into per-NUMA node worker threads.
On Windows 10 and later, the kernel dynamically creates additional worker threads as needed, 
with a default maximum limit of 4096 (see ExpMaximumKernelWorkerThreads-
ured through the registry up to a maximum of 16,384 threads and down to a minimum of 32. You 
can set this using the MaximumKernelWorkerThreads