### Error Propagation and Performance Impact in Single-Threaded Workloads

Error propagation can affect even single-threaded workloads, especially when data is evicted from the L1 cache. Figure 8 presents the same single-threaded benchmarks as in Figure 6, with additional bars for timestamps and data blocking in the L1 cache (L1 Timestamps). These mechanisms are necessary to prevent error propagation, as discussed in Section IV-A. Additionally, the figure includes the variable-length checkpointing (L1 AIMD Timestamps) presented in Section IV-D, which is used to reduce overheads caused by conflict misses that trigger early checkpoints and pause the main core execution.

#### Benchmarks Affected by Data Blocking

Three benchmarks are particularly impacted by blocking data from being evicted from the L1 based on timestamp data:

1. **randacc**: This benchmark suffers due to its highly random memory-access pattern, which has little temporal or spatial locality. This results in a large number of both conflict and capacity evictions. However, since the program is extremely memory-bound even without error detection or correction, the overheads can be eliminated by dynamically setting checkpoint lengths using AIMD timestamps. The additional overhead of checkpointing is negligible because the workload is not compute-bound, and reducing the checkpoint length decreases the number of stores concurrently buffered in the L1 cache.

2. **freqmine** and **swaptions**: These benchmarks are also affected, but their performance is less optimal. For freqmine, the overhead increases to 14% with L1 timestamps and reduces to 10% with variable-length checkpointing. Both workloads suffer from frequent conflict evictions, but unlike randacc, the relevant data is typically temporally local and available in the L2 cache. Therefore, these workloads are not as memory-bound. Reducing the size of checkpoints is less effective for conflict misses than for capacity misses, so variable-length checkpointing does not entirely solve the problem of overheads from using the low-associativity L1 cache as a buffer. A higher-associativity cache or victim buffer could mitigate this issue considerably.

### Delays and Checkpoint Lengths

Figure 9 shows the delays observed when using AIMD variable timestamps. We see that typical delays are further reduced compared to the schemes in Figure 7, particularly for randacc, where smaller checkpoints result in significantly reduced average delays.

Figure 10 illustrates that while most benchmarks spend over 90% of their execution time with a maximal instruction window of 5,000 instructions, randacc, freqmine, and swaptions spend the majority of their execution with much smaller checkpoint lengths. Freqmine spends only 21% of its time at this maximum, while randacc spends just 2.4% of its time there, and swaptions less than a thousandth of a percent. The capacity and associativity of the L1 are too low for maximal performance on these benchmarks when the L1 is used as a buffer for checked data with a fixed checkpoint size. However, performance is still relatively high if we dynamically alter the checkpoint size.

### Multithreaded Benchmarks

While single-threaded benchmarks may suffer from using the L1 cache as a buffer for unchecked results due to cache evictions, multithreaded shared-memory workloads introduce the additional problem of shared data. As discussed in Section IV-C, other cores may force data to be flushed from an L1 cache or directly shared with another core.

Figure 11 shows the performance of Parsec [13] benchmarks running with two threads using our schemes and detection alone. With the addition of timestamps in the L1 cache to prevent inter-thread communication before error checking, some slowdown is observed. However, this slowdown is surprisingly slight and almost entirely mitigated with the use of AIMD timestamps to vary checkpoint lengths based on the amount of communication between cores.

The Parsec benchmarks are designed to scale well to multiple threads, and even on current systems, frequent communication causes programs to scale poorly. Therefore, for programs with high thread-level parallelism, the additional performance loss from delaying communication before error checking to prevent error propagation and allow recovery from errors is minimal compared to just providing error detection.

### Other Overheads

Our prior work [8] estimates other overheads at 16% and 24% for power and area, respectively, for their heterogeneous detection system. ParaMedic should have similar overheads: the additional overheads include L1 data cache tag timestamps (a fraction of a percent of core area), along with the hard error table, AIMD unit, and commit order tracker, which are small units insignificant in overall area and power.