error propagation to separately checked code, even for single-
threaded workloads.
Figure 8 presents the same single-threaded benchmarks as
ﬁgure 6, but with additional bars for timestamps and data
blocking in the L1 cache (L1 Timestamps) necessary for
preventing error propagation, as presented in section IV-A,
along with the variable length checkpointing (L1 AIMD
Timestamps) presented in section IV-D used to reduce the
overheads in cases of conﬂict misses causing the triggering
of early checkpoints and paused main core execution.
Three benchmarks are particularly impacted by blocking
data from being evicted from the L1 based on timestamp data.
The ﬁrst of these, randacc, suffers because of its highly random
memory-access pattern, with little temporal or spatial locality,
causing a large number of both conﬂict and capacity evictions.
However, since this results in the program being extremely
memory bound even without error detection or correction, we
can eliminate the overheads entirely by dynamically setting
checkpoint lengths with AIMD timestamps. The overhead of
additional checkpointing is negligible as the workload is not
compute bound, and by shrinking the checkpoint length we
have fewer stores concurrently buffered in the L1 cache.
However, the performance for the other two benchmarks
affected, freqmine and swaptions, is less optimal. Freqmine
is particularly impacted, increasing overhead to 14% with
L1 timestamps and reducing to 10% with variable length
checkpointing. While both workloads suffer from frequent
conﬂict evictions, as with randacc, the relevant data is typically
temporally local, and available in the L2 cache, unlike with
randacc, and thus the code is not as memory bound. Reducing
the size of checkpoints is less effective for conﬂict
than
capacity misses, therefore variable-length checkpointing does
not entirely solve the problem of overheads from using the
low-associativity L1 cache as a buffer. A higher-associativity
cache or victim buffer could mitigate this considerably.
Figure 9 shows the delays observed once we add in AIMD
variable timestamps. Again, we see that typical delays are
reduced further with respect to both schemes in ﬁgure 7. This
is particularly true for randacc because the checkpoints are
smaller, so average delays are reduced considerably.
Indeed, in ﬁgure 10 we see that, while most benchmarks
spend over 90% of their execution time with a maximal
instruction window of 5,000 instructions, randacc, freqmine
and swaptions spend the majority of their execution with much
smaller checkpoint lengths. Freqmine spends only 21% of
its time at this maximum, while randacc spends 2.4% of its
209
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
n
w
o
d
w
o
S
d
e
s
l
i
l
a
m
r
o
N
 1.16
 1.14
 1.12
 1.1
 1.08
 1.06
 1.04
 1.02
 1
h
c
s
k
c
b l a
Detection
Correction
L1 Timestamps
L1 AIMD Timestamps
o l e
s
r a
n
c
c
a
d
fl u i d
a
n i m a t e
b
o
y tr a
d
c
k
fr e
e
q m i n
b it c
u
o
n t
w
s
s
n
p ti o
a
s tr e
a m
e r a
v
a
e
g
Fig. 8: Normalised slowdown for each single core benchmark,
with the schemes from ﬁgure 6, along with the L1 timestamp
scheme necessary to ensure correctness in multicore shared
memory environments, and the AIMD timestamp length scheme
to improve its performance.
y
t
i
s
n
e
D
bitcount
freqmine
stream
fluidanimate
swaptions
bodytrack
blackscholes
randacc
 0
 1000
 2000
 3000
 4000
 5000
Time (ns)
Fig. 9: Density plot to show the delays observed between a
store committing and being checked, with data being buffered
in the L1 and a dynamic AIMD timestamp length scheme.
time there, and swaptions less than a thousandth of a percent.
The capacity and associativity of the L1 are too low for
maximal performance on these benchmarks when the L1 is
used as a buffer for checked data, with a ﬁxed checkpoint
size. However, performance is still relatively high provided
we alter the checkpoint size dynamically.
2) Multithreaded Benchmarks: While the single-threaded
benchmarks may suffer from using the L1 cache as a buffer
for unchecked results due to cache evictions, with true mul-
tithreaded shared-memory workloads the additional problem
of shared data emerges. As we discuss in section IV-C, other
cores may force data to be ﬂushed from an L1 cache, or be
directly shared with another core.
Figure 11 shows the performance of Parsec [13] benchmarks
running with two threads with our schemes and detection
alone. With the addition of timestamps in the L1 cache, to
prevent inter-thread communication before error checking, we
do observe some slowdown. However,
this is surprisingly
slight, and almost entirely mitigated with the use of AIMD
timestamps, to vary checkpoint lengths based on the amount
of communication between cores.
y
t
i
s
n
e
D
y
t
i
s
n
e
D
 1
 0.98
 0.96
 0.94
 0.92
 0.9
 0.88
 0.86
 0.84
 0.82
 0.8
 0.07
 0.06
 0.05
 0.04
 0.03
 0.02
 0.01
 0
bitcount
freqmine
stream
fluidanimate
swaptions
bodytrack
blackscholes
randacc
other benchmarks
randacc
freqmine
swaptions
 0
 1000
 2000
 3000
 4000
 5000
Checkpoint Length (Instructions)
Fig. 10: Density plot of timestamp lengths during execution
when using variable length AIMD timestamps (section IV-D).
n
w
o
d
w
o
S
d
e
s
l
i
l
a
m
r
o
N
 1.035
 1.03
 1.025
 1.02
 1.015
 1.01
 1.005
 1
h
c
s
k
c
b l a
Detection
Correction
L1 Timestamps
L1 AIMD Timestamps
s
o l e
a l
e
n
n
a
c
a
fl u i d
n i m a t e
s
n
p ti o
a
w
s
e
g
e r a
v
a
Fig. 11: Normalised slowdown for Parsec workloads running
on two threads, with techniques from ﬁgure 6, along with the
L1 timestamp scheme necessary for correctness in multicore
shared memory environments, and the AIMD timestamp length
scheme to improve its performance.
Still, in many ways this should be expected. The Parsec
benchmarks are designed to scale well to multiple threads,
and even on current systems, frequent communication causes
programs to scale poorly. Because of this, we can say more
generally that, for programs with high thread-level parallelism,
the additional performance loss from delaying communication
before error checking, to prevent error propagation and there-
fore allowing recovery from errors, is slight when compared
with just providing error detection.
C. Other Overheads
Our prior work [8] places other overheads at 16% and 24%
for power and area respectively for their heterogeneous de-
tection system. We should expect ParaMedic to be similar:
the additional overheads we are L1 data cache tag timestamps
(a fraction of a percent of core area), along with the hard
error table, AIMD unit and commit order tracker, which are
small units that are insigniﬁcant in overall area and power
210
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
Loads
Stores
Register File
Masked
VII. COMPARISON WITH TRANSACTIONAL MEMORY
n
o
i
t
r
o
p
o
r
P
s
o l e
h
c
s
d
n
r a
a
c
c
fl u i d
a
k
c
b l a
n i m a t e
d
o
b