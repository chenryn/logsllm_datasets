architecture, as shown in Figure 2. The log module schedules
51
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
write requests and works with the coding module for parity
computations. The data chunks are issued to the main array
through the SSD write module, while the log chunks are issued
to the log devices through the log write module. To tolerate
the same number of device failures, we require the number
of log devices in EPLOG be equal to the number of tolerable
device failures in the main array. For example, if the main array
assumes RAID-6 (which can tolerate two device failures), two
log devices are needed. We elaborate how EPLOG constructs
log chunks in Section III-B.
The commit module regularly performs parity commit to
ensure that the data and parity chunks in the main array reﬂect
the latest updates. We elaborate the parity commit operation
in Section III-C.
To further reduce parity trafﬁc, we introduce two types of
buffers in the log module, namely a stripe buffer and multiple
device buffers, to batch-process write requests in memory. The
use of buffers is optional, and does not affect the correctness of
our design. We elaborate the caching design in Section III-D.
We carefully implement EPLOG to optimize its per-
formance. In particular, our implementation ensures persis-
tent metadata management. We elaborate the details in Sec-
tion III-E.
Limitations: Before presenting the design of EPLOG, we
discuss its design limitations. First, EPLOG requires additional
storage footprints to keep log chunks, although we employ
HDDs as log devices to limit the extra system cost. Second,
EPLOG keeps multiple versions of data chunks during updates
before parity commit, so we need to provision extra space
in SSDs. Third, if a failure happens before parity commit,
recovery performance may hurt due to the need of accessing
log chunks, especially when we use HDDs as log devices.
Finally, parity commit may create additional performance
overhead. Our design rationale is that if we perform parity
commit regularly on every ﬁxed number of write requests in
batch, we can limit parity commit overhead and the drawbacks
as described above. Caching also helps to reduce the writes
to SSDs and the amount of log chunks, so it can further
mitigate parity commit overheads. We study these issues in
our experiments (see Section V).
B. Write Processing
We ﬁrst describe how EPLOG processes a single write
request and constructs log chunks; in Section III-D, we extend
the design for processing multiple write requests via caching.
Note that read requests under no device failures are processed
in the same way as in traditional RAID. Thus, we omit the
read details.
EPLOG distinguishes (in the log module) write requests
into two types. If the incoming write request is a new write
and spans a full stripe in the main array, we directly write the
data and parity chunks to the main array as in conventional
RAID; otherwise, if the request is a new partial-stripe write
or an update, then we write data chunks to the main array
and the computed parity logs (i.e., log chunks) to log devices.
The rationale is that both types of writes do not pre-read data
chunks from the main array for parity computation. By issuing
new full-stripe writes directly to the main array, we save the
subsequent parity commit overhead.
Recall from Section III-A that EPLOG ﬁrst stores data
chunks in the main array and log chunks in log devices; after
parity commit, it stores both data and parity chunks in the
main array. For ease of presentation, we call a stripe that has
data chunks stored in the main array and log chunks stored in
the log devices a log stripe, and call a stripe that has both data
and parity chunks stored in the main array a data stripe.
Stripe generation: We ﬁrst explain how we generate a data
stripe, followed by how we generate a log stripe. For a data
stripe, EPLOG applies (in the coding module) k-of-n erasure
coding (where k < n) to encode the k data chunks into
additional n − k parity chunks, such that any k out of n data
and parity chunks can reconstruct the data chunks in the data
stripe. We conﬁgure n to be the number of SSDs in the main
array, and conﬁgure k such that n − k is the tolerable number
of device failures. For example, if we construct an SSD RAID-
5 array, we set n − k = 1; for an SSD RAID-6 array, we set
n − k = 2.
To generate a log stripe, we ﬁrst require that the data
chunks of a log stripe belong to different SSDs. To achieve
this, we ﬁrst identify the destined SSD for each data chunk
included in a write request, and then group the data chunks
written to different SSDs to form a log stripe. In particular,
for a new partial-stripe write, since the data chunks can be
written to any SSD, we combine them into a single log stripe
and distribute them across SSDs. For an update request, since
the destination of each data chunk included in the request is
given, if multiple data chunks belong to the same SSD, then we
separate them into different log stripes to ensure that each log
stripe only contain at most one data chunk belonging to each
SSD. We still use the example in Figure 1(b) to illustrate the
idea. Since the data chunks B0, C0, and A1 belong to different
SSDs, we can combine the newly updated data chunks B0’,
C0’, and A1’ into a single log stripe. We generate only one
log chunk B0’+C0’+A1’ and write it to the log device.
Suppose now that a log stripe contains k(cid:2) data chunks to
be stored in k(cid:2) different SSDs, where k(cid:2) is less than or equal
to the number of SSDs in the main array. EPLOG then applies
(in the coding module) k(cid:2)-of-n(cid:2) erasure coding to generate
additional n(cid:2) − k(cid:2) log chunks, such that n(cid:2) − k(cid:2) = n − k,
or equivalently, n(cid:2) − k(cid:2) equals the tolerable number of device
failures. For example, referring to the example in Figure 1, we
can group k(cid:2) = 3 data chunks {B0’, C0’, A1’} into a log
stripe. We then set n(cid:2) = 4 and generate n(cid:2) − k(cid:2) = 1 log chunk.
EPLOG can tolerate the same number of device failures
(including SSD failures and log device failures) as we deploy
conventional RAID directly in the main array. Note that data
chunks in EPLOG are now protected by either the parity
chunks in the main array or the log chunks in the log devices.
Speciﬁcally, if a failed data chunk is not updated since the last
parity commit, then it can be recovered from other data and
parity chunks of the same data stripe in the main array. On the
other hand, if a failed data chunk is updated before the next
parity commit, then it can be recovered by the log chunks in
log devices and other data chunks of the same log stripe. The
same argument applies when either a parity chunk or a log
52
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
chunk fails. In Section IV, we conduct mathematical analysis
to investigate how EPLOG affects the system reliability.
Chunk writes: EPLOG writes both data and parity chunks of
a data stripe, as well as the data chunks of a log stripe, to
the main array via the SSD write module, while writing the
log chunks of a log stripe to the log devices via the log write
module. The two modules use different write policies. First,
the SSD write module uses the no-overwrite policy. When
it updates a data chunk in an SSD, it writes the new data
chunk to a new logical address instead of overwriting the old
one, and maintains a pointer to refer to the old data chunk.
This makes both the old and new data chunks accessible after
the update request. Since the parity chunks in the main array
are not yet updated, keeping both the old and new versions
of data chunks is necessary to preserve fault tolerance (see
Section II-B for example). When the parity chunks in the main
array are updated after parity commit, the old versions of the
data chunks can be removed. On the other hand, the log write
module uses the append-only policy, so as to ensure sequential
writes of log chunks to the log devices and hence preserve
performance.
C. Parity Commit
EPLOG regularly performs the parity commit operation to
ensure all data and parity chunks of data stripes are based
on the latest updates. It can trigger parity commit in one of
the following scenarios: (i) the system is idle, (ii) there is no
available space in any SSD and log device, (iii) an upper-layer
application issues a parity commit, and (iv) after every ﬁxed
number of write requests.
In each parity commit operation, we identify the data
stripes, via the metadata structure (see Section III-E), whose
data chunks have been updated since the last parity commit.
For each identiﬁed data stripe, we read the latest data chunks
from SSDs, compute the corresponding parity chunks, and
write back the updated parity chunks to SSDs in the main
array. Finally, we update the metadata and release the space
occupied by both the obsolete data chunks from the main array
and log chunks from the log devices.
We emphasize that parity commit does not need to access
any log chunks in the log devices in normal mode when there
is no SSD failure. The reason is that all up-to-date data chunks,
which will be used for computing parities, are kept in SSDs.
This guarantees that the log devices can be always accessed in
a sequential order when writing log chunks with the append-
only policy (see Section III-B). In case of SSD failures, we
scan and commit log chunks in batches, in which the batch
size presents a trade-off between memory usage and parity
commit overhead. We point out that parity commit introduces
extra writes to the main array, yet we show that the write
trafﬁc remains limited compared to conventional RAID (see
Section V). The reason is that we only need to perform parity
commit on the latest data chunks in the main array to construct
the corresponding parity chunks, while a data chunk may have
received multiple updates before parity commit.
We may explore the use of TRIM to explicitly remove the
obsolete data chunks during parity commit and further remove
GC overhead. On the other hand, the use of TRIM can be
Incoming requests: 
{A4, B4}, {B0’, C0’, A1’}, {C1’, A2’}
Stripe buffer
Device buffers
Buffers
A4 B4
A2’
A1’
B0’
C0’
C1’
SSD0
SSD1 SSD2
SSD3
Stripe 0
Stripe 1
Stripe 2
Stripe 3
A0
A1
A2
P3
B0
B1
P2
A3
C0
P1
B2
B3
P0
C1
C2
C3
Storage
Devices
A1’+B0’+C0’+C1’
B0’
C0’
A1’
C1’
SSD0 SSD1 SSD2 SSD3
SSD RAID-5
Log Device
Fig. 3: Illustration of buffers in EPLOG.
tricky and require special handling in SSD RAID arrays [18].
We pose the use of TRIM as future work.
D. Caching
To further reduce parity trafﬁc, EPLOG supports an op-
tional caching feature to batch-process multiple write requests
in memory. It includes two types of buffers in the log module:
a stripe buffer and multiple device buffers, which process new
writes and updates, respectively.
The stripe buffer is used to cache new writes, which are
directed to the main array, so as to increase the chance of full-
stripe writes when generating data stripes. We set the size of
the stripe buffer to be multiples of data stripes. Speciﬁcally,
when a new write request arrives, the data chunks contained in
the write request are appended to the stripe buffer. If the stripe
buffer is full, all cached data chunks are grouped together to
generate full data stripes and written to the main array in batch.
In addition, there are multiple device buffers, each of which
is associated with an SSD in the main array. Each device
buffer is used to cache update requests. The rationale is that
real-world workloads often exhibit high locality both spatially
and temporally [34], [43], [46], such that recently updated
chunks and their nearby chunks tend to be updated more
frequently. Thus, the device buffers can potentially absorb
multiple updates for the same data chunk, thereby reducing
both data chunks and log chunks written to the main array
and the log devices, respectively. Speciﬁcally, when an update
request arrives, each of the data chunks in the request is cached
in the corresponding device buffer, according to the destined
SSDs of these data chunks. If the same data chunk is found in
the device buffer, it is directly updated in place. When one of
the device buffers is full, we extract one data chunk from the
head of each non-empty device buffer to form a log stripe.
We further illustrate via an example how the buffers work
in EPLOG, as shown in Figure 3. We consider a stream of
write requests issued to an SSD RAID-5 array. Speciﬁcally,
when the new write request {A4, B4} arrives, we add the data
chunks to the stripe buffer. For the subsequent update requests
{B0’, C0’, A1’} and {C1’, A2’}, we add them to the device
53
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
buffers. We add the data chunks A1’ and A2’ to the device
buffer of SSD0, since both their original data chunks A1 and
A2 belong to SSD0. Similarly, we add the data chunks B0’,
C0’, C1’ to the device buffers of SSD1, SSD2, and SSD3,
respectively. Suppose that the size of each device buffer is
conﬁgured to hold at most two data chunks. Now the device
buffer of SSD0 becomes full. Thus, we construct a log stripe
using the set of data chunks {A1’, B0’, C0’, C1’}. Finally,
we write the new data chunks A1’, B0’, C0’, and C1’ to the
main array by using the no-overwrite policy, and append the
generated log chunks to the log devices as shown in Figure 3.
E. Implementation Details
We build EPLOG as a user-level block device that
is
compatible with commodity hardware conﬁgurations. We im-
plement the EPLOG prototype in C++ on Linux. It exports
the basic block device interface, which operates on logical
addresses on underlying physical devices, as a client API to
allow upper-layer applications to access the storage devices.
For parity computations, EPLOG implements erasure coding
based on Cauchy Reed-Solomon codes [4] using the Jerasure
2.0 library [42].
EPLOG is designed to provide persistent metadata manage-
ment, and it supports two metadata checkpoint operations: full
checkpoint and incremental checkpoint. The full checkpoint
ﬂushes all metadata, while the incremental checkpoint ﬂushes
any modiﬁed metadata since the last full/incremental check-
point. Both checkpoint operations can be triggered regularly
in the background, or by the upper-layer applications.
EPLOG maintains a ﬂat namespace and comprises two
types of metadata: data stripe metadata and log stripe meta-
data. The data stripe metadata describes the mapping of each
data stripe to data chunks, including both the latest and stale
ones. It includes the stripe ID and chunk locations. The log
stripe metadata describes the mapping of each log stripe to
data chunks, referenced by data stripes, and log chunks on the
log devices. It contains stripe ID, number of chunks, and a list
of chunk locations.
EPLOG provides persistent metadata storage on SSDs. It
creates a separate metadata volume from the main array to
keep the metadata checkpoints. The metadata volume com-
prises three areas: super block area, full checkpoint area, and
incremental checkpoint area. The superblock area is located
at the front of the metadata partition, and keeps the essential
information of the metadata layout. The full checkpoint area
follows the super block area, and keeps the full checkpoints.
It has two sub-areas [24], which hold the latest and previous
full checkpoints. The intuition is to write the full checkpoints
alternately to one of the sub-areas, so as to ensure that
there always exists a consistent copy of the full checkpoint
and hence survive any unexpected system failure during the
checkpoint operation. The incremental checkpoint area follows
the full checkpoint area. It stores all incremental checkpoints
in append-only mode.
To create the metadata volume, we ﬁrst create two parti-
tions in each SSD in the main array, one for data and another
for metadata. We then mount a RAID-10 volume on the
metadata partitions of all SSDs using mdadm [37], and EPLOG
directly accesses the metadata on the volume. In addition,
EPLOG directly accesses the data partitions of SSDs and the
log devices as raw block devices in JBOD mode. To maintain
I/O performance, EPLOG uses multi-threading to read/write
data via the devices in parallel.
IV. RELIABILITY ANALYSIS
In this section, we analyze the system reliability of EPLOG
and compare it with that of conventional RAID (i.e., we deploy
RAID directly in the main array without using any log device).
At ﬁrst glance, the impact of EPLOG on the system reliability
is debatable. EPLOG reduces write trafﬁc to the main array via
elastic parity logging. This slows down the wearing of ﬂash
memory, and potentially decreases the failure rates of SSDs as
well [12], [26]. On the other hand, EPLOG adds log devices,
while still tolerating the same number of device failures. This
degrades the system reliability.
We resolve this debate as follows. Speciﬁcally, we measure
the system reliability of EPLOG and conventional RAID in
terms of mean-time-to-data-loss (MTTDL) (i.e., the expected
time until data loss happens) through a simpliﬁed setting.
Suppose that a storage system (either EPLOG or conventional
RAID) reaches a certain system state after processing some
workload. We ﬁx the current system state, which implies that
the corresponding error and recovery rates are ﬁxed. Then
under the same system state, we analyze how much longer the
storage system continues to survive without any data loss based
on MTTDL. Note that our simpliﬁed reliability analysis does
not consider the time-varying bit error rate of ﬂash memory
[29]. Also, the correctness of MTTDL remains a concern [11].
Nevertheless, our analysis only serves to provide reliability
comparisons between EPLOG and conventional RAID, and by
no means do we use the absolute values to provide accurate
quantiﬁcations.
A. MTTDL Computation
We ﬁrst deﬁne the notations. Let n be the number of SSDs