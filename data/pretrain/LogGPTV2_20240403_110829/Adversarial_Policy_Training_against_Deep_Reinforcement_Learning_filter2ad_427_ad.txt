the following equation
v ,a(t)
v ,a(t)
argminθh(cid:107)H(o(t)
v ,a(t)
v ,a(t)
α ;θh)− o(t+1)
v
(cid:107)∞ ,
(10)
where θh denotes the parameters of the neural network H. It
should be noted that (cid:107)·(cid:107)∞ is non-differentiable. Therefore,
we adopt the approximation technique introduced in [7], and
use the alternative objective function
Lst = minimizeθh(cid:107)(|H(o(t)
v ,a(t)
v ,a(t)
α )− o(t+1)
v
|− εs)+(cid:107)2
2 ,
(11)
v
v
v ,a(t)
v ,a(t)
v ,a(t)
v ,a(t)
α ) and o(t+1)
to train the approximated state-transition model H. In the
εs is a
equation above, (·)+ is equivalent to max(·,0).
hyperparameter, which controls the maximum l∞ between
H(o(t)
. To solve this objective function,
we collect the ground truth training data (o(t)
α ,o(t+1)
)
by using trajectory rollouts. Then, we utilize the ADAM op-
timization method [20] to minimize this objection function.
More speciﬁcally, as is shown in Algorithm 1 (step 7), the
state-transition model is trained jointly with the policy net-
work of the adversarial agent. At each iteration, we ﬁrst
collect a set of trajectories by using current adversarial policy
to play against the opponent agent. The information contained
in the collect trajectories includes the opponent agent’s ac-
tions and observations. Using these actions and observations
as the ground truth, we can update the surrogate networks
by minimizing the loss functions above. It should be noted
that, while the state transition model H should be obtained
based on the old adversarial policy, we predict the state tran-
sition under the new adversarial policy. We argue this does
not introduce negative effect to our training process because
the PPO objective function guarantees a minor change in the
adversarial policy at each iteration.
Opponent policy network approximation. As is shown in
Equation (9), computing action deviation requires a(t+1)
and
v
ˆa(t+1)
. In addition, as is mentioned earlier, our attack relies
v
upon the capability of knowing how a victim agent weights
the importance of the adversarial actions. To do that, as
we will elaborate in Section 5.4, we leverage gradient-based
explanation AI techniques, which need to take as input the
policy network of the victim agent. As such, in addition to the
state transition approximation, we use a deep neural network
F to approximate the policy network of the opponent agent.
In this work, to learn the victim’s policy network, we fol-
low existing imitation learning methods [52] and design the
following objective function
v ;θ f )− a(t)
v |− εa)+(cid:107)2
2 .
Lop = minimizeθ f (cid:107)(|F(o(t)
(12)
Here, θ f represents the parameters of the deep neural net-
work F. As we can observe from the equation above, we
also use the approximated l∞ loss to train F. Similar to the
method above, we also collect the training samples (o(t)
v ,a(t)
v )
through trajectory rollouts and then apply the ADAM algorithm
to minimize the loss. As we will empirically illustrate in
Section 6, the network trained with l∞ norm usually exhibits
better performance than those trained with l2 and l1.
Note that, in MDP, both the state transition and the policy
network should be in the form of stochastic. This means that
the most typical way of approximating P and π should be
density estimation [14]. In this work, we, however, conduct
point estimations to reduce the computational cost. As we will
show in Section 6, while point estimate ignores the variance
of the original distribution and may introduce a bias, our
attack is still able to achieve decent performance in terms of
beating the opponent in the two-agent competitive game.
After obtaining the approximated models H and F, we
can predict the observation of the opponent agent ˆo(t+1)
through H(o(t)
through
F(H(o(t)
α )). With these predictions, we can rewrite
Equation (9) as
α ), and its action ˆa(t+1)
v ,a(t)
v ,a(t)
v , ˆa(t)
v , ˆa(t)
v
v
Lad = maximizeθ((cid:107)F(H(o(t)
−(cid:107)H(o(t)
v ,a(t)
v , ˆa(t)
v
Here, it should be noted that ˆa(t)
from the adversarial policy πα.
v ,a(t)
α ))− a(t+1)
v , ˆa(t)
α )− o(t+1)
(cid:107)1) .
v
(cid:107)1
(13)
α is the new action derived
5.4 Hyperparameter adjustment
As is mentioned in Section 4, we introduce a hyperparameter
to balance the weight of the newly added loss term. In this
work, we automatically adjust λ by using an explainable AI
technique. More speciﬁcally, by using the gradient saliency
methods (e.g., [46]) at the time step t, we ﬁrst compute g(t) =
(cid:79)
v ) which indicates the importance of each element
o(t)
v
in the opponent agent’s observation.5 In this equation, F(o(t)
v )
5Note that we do not have the access to the opponent policy network and,
F(o(t)
therefore, we compute the gradient on the basis of its approximation F.
USENIX Association
30th USENIX Security Symposium    1891
Algorithm 1: Adversarial policy training algorithm.
1 Input: the adversarial agent’s policy πα parameterized by
θα, the adversarial agent’s value function network
Vα with parameter vα, the state transition model H with
parameter θh, the opponent’s policy approximation
model F with parameter θ f , and the pretrained
opponent agent’s policy πv.
α , θ(0)
, and v(0)
α .
h , θ(0)
f
2 Initialization: Initialize θ(0)
3 for k = 0,1,2, ...,K do
4
Collect a set of trajectories Dk = {τi} by using adversarial
policy πk
α to play against the opponent agent πv, where
i = 1,2, ....,|Dk| and each trajectory contains T time step.
Obtain the reward of the time t in each trajectory τi: ri(t)
α .
Compute the estimated advantage of each time in each
trajectory: Ai(t) based on the current value function Vαk :
Ai(t) = ri(t)
Update the state transition approximation function H and
the opponent policy approximation function F using the
current trajectories according to the following objective
function
)−Vαk (oi(t)
α ).
α + γVαk (oi(t+1)
α
θk+1
h = argminθh
θk+1
f = argminθ f
1
|Dk|T ∑
τ∈Dk
|Dk|T ∑
τ∈Dk
1
T
∑
t=0
T
∑
t=0
Lst ,
Lop .
(15)
, ai(t)
v
in Dk, and Fθk+1
Based on the updated oi(t)
, compute
v
the penalty term for each time t in each trajectory i: λi(t)
according to Equation (14).
Update the policy by maximizing the following objective
function
f
5
6
7
8
9
F(o(t)
denotes the action of the opponent agent a(t)
v ∈ Rp×1 and F(o(t)
v predicted by F.
v ) ∈ Rq×1, the gradi-
Supposing o(t)
ent g(t) ∈ Rp×q is a matrix, in which each element g(t)
i j =
(cid:79)
v ) j indicates the importance of the i-th element in
(o(t)
v )i
o(t)
to the j-th element in F(o(t)
v ). To assess the overall impor-
v
tance of each element in o(t)
to F(o(t)
v ), we sum the elements
v
in each row of g(t) and transform it into a normalized vector
i j . Here, ˜g(t) ∈ Rp×1 indicates the importance
˜g(t) = ∑ j=1:q g(t)
of the i-th element in o(t)
v
After obtaining ˜g(t), we then calculate the importance of
the adversarial agent’s action to the opponent agent’s action at
the time t. Recall that the observation of the opponent agent
o(t)
contains three components – environment, the action
v
of the opponent agent, and that of the adversarial agent –
and we focus only on the action of the adversarial agent.
Therefore, we eliminate the feature importance tied to the
environment and the action of the opponent agent. To do this,
we ﬁrst perform an element-wise multiplication between ˜g(t)
and a mask M ∈ Rp×1. Then, we borrow the idea of an early
research work [9], through which we compute λ as follows
to F(o(t)
v ).
I(t) = (cid:107)F(o(t)
v )− F(o(t)
v (cid:12) ( ˜g(t) (cid:12) M))(cid:107)∞ , λ(t) =
1
1 + I(t)
.
(14)
v . In o(t)
Here, the vector o(t)
is a vector, indicating the observation at
v
time t. M is a vector with the same dimensionality as the vec-
tor o(t)
v , if the corresponding observation dimensions
indicate the actions of adversarial agent, we assign 1 to the
corresponding element in M. Otherwise, we assign 0 accord-
ingly. For example, assuming the kth ∼ (k + N)th dimensions
of o(t)
indicate the actions of the adversarial agent. Then, we
v
assign 1 to the kth to (k + N)th dimensions of M, and the rest
is assigned to 0. In this work, we normalize λ(t) to [0,1]. 6
From this equation, we can easily discover that, the higher
value of I(t) indicates a lower importance score, resulting in
a lower value of λ(t). In Algorithm 1, we illustrate how to
combine λ with our extended loss function, and thus train an
adversarial agent with the ability to attack its opponent.
6 Evaluation
In this section, we evaluate our proposed attack technique
from various aspects, compare it with the state-of-the-art
method, and demonstrate its effectiveness and efﬁciency by
using representative two-agent competitive games. Below,
we ﬁrst present our experiment setup. Then, we discuss the
design of our experiment, followed by our experiment results.
6Normalization could capture temporal changes and prevent the inﬂuence
of its extreme values upon the PPO learning process.
θk+1
α = argmaxθα
1
|Dk|T
|Dk|∑