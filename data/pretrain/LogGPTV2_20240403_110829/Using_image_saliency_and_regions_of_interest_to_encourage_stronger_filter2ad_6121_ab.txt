3.1 Data
Training data for both stages consisted of the images and
click points provided by Zhao et al. [37]. They selected 15
images originally from the PASCAL Visual Object Classes
(VOC) Challenge 2007 dataset [14]. The images (see Fig-
ure 1 for samples) contain a variety of scenes with varying
complexity. Zhao et al. collected more than 10,000 pass-
words from 762 subjects that we use as actual click points
(ACP).
3.2 Stage 1: Visual Saliency Determination
We computed the saliency maps of our training images
using the MATLAB implementation of GBVS [18, 19]. The
generated saliency maps were converted to binary by com-
paring gray level values of individual pixels to diﬀerent
129
Figure 1: Sample of the ﬁfteen images of the training
dataset from [37], including computed saliency maps
and ACP. Original images from [14].
(a) Img 2
(b)
Img
GBVS 30%
2
(c) Img 2 ACP
(d) Img 5
(e)
Img
GBVS 30%
5
(f) Img 5 ACP
(g) Img 11
Img
(h)
GBVS 30%
11
(i)
ACP
Img
11
thresholds ranging from 0% to 90% of the full image saliency.
We used a threshold of 30% based on work by Mayron [21]
that showed that thresholds greater than 30% provided fewer
predictions and thresholds lower than 30% did not provide
more predictions. Figures 2b, 2e and 2h show samples of
computed saliency maps at the 30% threshold.
It can be
seen from these ﬁgures that the more visually complex im-
ages (e.g., Image 11) have a higher proportion of the image
that is considered salient.
Image Suitability Measurement
3.2.1
To support the idea that using guiding images with less
saliency results in weaker passwords, we measured the en-
tropy of passwords created using the 15 images in Zhao et
al.’s original study. Our image suitability measurements fol-
low an intuition similar to that of Wang et al. [35]: that
a higher per-click-point entropy relates to the strength of
a given image in providing click points suitable for a strong
graphical password. A higher entropy for a click point means
a higher theoretical password space, which means the resul-
tant passwords are harder to guess or otherwise crack. En-
tropy is deﬁned as measuring the amount of uncertainty in
the composition of a password [4] and is measured in bits.
For example, for a set of characters b and password length k,
the entropy of the standard textual password is bk bits. For
a graphical password, characters can be related to salient
regions, so the entropy is bk bits where b is the set of de-
tected salient regions and k is the number of click points
chosen for the password. We segmented the salient regions
of each image into squares of 19x19 pixels (and discarded
non-salient regions) and counted the number of regions that
130
contained ACP. We chose 19x19 as a size of a region be-
cause it represents the allowed size of the tolerance area for
a ﬁnger-selected click point that is used in other studies [32,
27]. Table 1 shows the total number of detected salient re-
gions and the proportion of those salient regions that were
selected by users. This shows that users do choose salient
regions frequently when creating graphical passwords, which
provides support for discounting images with small propor-
tions of salient regions.
Table 1: Proportion of salient 19x19 pixel regions
detected by GBVS at 30% threshold and selected
by users, with resultant practical and theoretical en-
tropy per click point. SR = salient regions. Bolded
rows represent the images that were considered suit-
able.
Image # SR % Image salient % selected SR Theor. entropy Prac. entropy
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Avg.
20
15
146
242
84
236
176
153
242
270
279
116
187
98
197
164.1
2%
2%
24%
45%
14%
42%
32%
23%
45%
45%
47%
14%
28%
13%
30%
27.1%
100%
100%
84.9%
82.2%
88.1%
82.2%
85.2%
85.6%
73.6%
78.5%
71.7%
83.6%
85.6%
74.5%
75.6%
83.4%
4.32
3.91
7.19
7.92
6.39
7.88
7.46
7.26
7.92
8.08
8.12
6.86
7.55
6.61
7.62
7.01
4.32
3.91
6.95
7.64
6.21
7.60
7.23
7.03
7.48
7.73
7.64
6.60
7.32
6.19
7.22
6.74
We then used Equation 1 as deﬁned in [4] to compute the
entropy per click point of the user-created passwords, where
b represents the number of salient 19x19 regions, and k is
the length of the password. In this case, the entropy was
computed for each actual click point in turn, so k = 1.
H(I) = Log2(bk)
(1)
Our hypothesis is that the larger the proportion of the
image that is considered salient, the larger the number of
salient regions users may choose, and thus the higher the
practical password space for that image. We calculated en-
tropy of the theoretical password space using the total num-
ber of salient regions in an image, and the entropy of the
practical password space that considers the number of salient
regions that were selected by users in the same image. The
results of computing the entropy are shown in Table 1; the
results show that some images such as images 10, 11, 4, and
6 have high entropy per click point because of the large pro-
portion of the images that are covered by saliency maps,
thereby providing a large number of 19x19 regions that can
contain click points. For instance, Image 2 (see Figure 2a)
has a small salient proportion, and also a lower theoretical
and practical password space.
We examined whether there is a correlation between the
detected number of salient regions and those regions selected
as part of a password. We computed the Pearson correlation
between the entropy of the two variables in Table 1. The re-
sult indicated that there was a strong, positive, statistically
signiﬁcant correlation between the entropy per click point of
total salient regions and the entropy per click point of the
selected salient regions (r(15) = 1, ρ = 0.0). This implies
that users tend to choose diﬀerent salient regions when there
is a large proportion of salient regions that cover the image.
The high degree of correlation between the entropy of total
and selected salient regions indicated an image with more
salient regions has more prominent hotspots and a larger
selection of appropriate graphical password click points. As
a result, such images with highly salient regions can lead to
more secure password selections, which provides support for
our hypothesis in the previous paragraph.
We formalize this result into a decision model: if an image
A has entropy value EA and an image B has entropy value
EB, and EA > EB, then image A is more suitable than
image B as a guiding image for graphical passwords. The
result of this stage is that the higher the image saliency pro-
portion for the guiding image, the higher the entropy of the
resulting password. Therefore, discarding the images with
the lowest proportion of saliency is a promising way to en-
courage users to create stronger passwords. We determined
empirically that a total image saliency percentage threshold
of 23% (see the % image salient column in Table 1) produced
the best results in terms of determining whether or not an
image was suitable for use with graphical passwords. We
deﬁne ‘best’ here as having balance between image saliency
proportion and the resulting theoretical and practical pass-
word entropy, as well as conﬁrming visual determinations of
saliency and thus suitability. This threshold depends on the
input images, and therefore will change depending on the
images chosen.
3.3 Stage 2: Detecting Image Objects
After removing the least suitable images using the method
described in Stage 1, we further reﬁned image suitability by
relating the salient regions to actual objects in the image
(which we call Regions of Interest, or RoI) that drew the
participant’s eye when creating their graphical passwords.
Our rationale for exploring this relationship is that simple
saliency does not determine whether an image region will be
selected by a user, and thus there must be some other fac-
tor that encourages such a selection. We hypothesize that
objects, which are likely to have meaning to a user and thus
be perceived as more memorable, may be this factor in en-
couraging selection. As an example, salient regions may
to be those that have changes in contrast, such as a “salt
and pepper” image of random white and black pixel regions.
However, a simple change in contrast does not necessarily
have meaning to a user and therefore may not be memo-
rable in terms of point selection for a graphical password.
To relate saliency to objects, we segmented the images into
19x19 pixel squares and detected objects within each square
(see Figure 2 for examples). If a square contained at least
one detected object, we consider it an RoI.
We then computed the likelihood that a user will select
the detected region based on how frequently it was selected
and use this likelihood to compute the entropy of the result-
ing passwords. We evaluate our model using real graphical
passwords chosen by users in the study by Zhao et al. [37].
We based our RoI detection method on the way users cre-
ate graphical passwords in Windows 8TM [23]. Here, users
choose from three actions as part of their password: a single
tap, a line, or a circle. Given these actions, we expect that
users will choose regions on the guiding image that corre-
spond to the available actions. For instance, objects such
as faces, eyes, noses, and mouths may be the target of taps,
or the user may draw a line from one of these objects to
another. Moreover, a detected circle in the guiding image
Figure 2: Output samples of the detected objects
and circles and the ACP chosen by users in Zhao
et al.’s study [37]. Background images originally
from [14].
(a) Img 2: De-
tected face ob-
jects
Img
2:
(b)
Detected circle
objects
(c) Img 2 ACP
(d) Img 5: De-
tected face ob-
jects
Img
(e)
5:
Detected circle
objects
(f) Img 5 ACP
Img
(g)
11:
Detected face
objects
Img
(h)
11:
Detected circle
objects
(i)
ACP
Img
11
could encourage users to trace a circle over the object, or to
tap inside the circle. These objects have been shown to be
attractive to users when creating graphical passwords [37].
3.3.1 Face object detection
We used the Viola-Jones algorithm [34] with Haar cas-
cades [1] to detect six diﬀerent object classes:
face, eye,
nose, mouth, left ear and right ear. Any object that is de-
tected within the boundary of the face is considered part
of the face. However, any object that is detected as one of
the face parts (eye, nose, etc.), but is located outside the
face boundary is considered a false positive, so we called it
a “Generic” object. Figures 3a, 3d, and 3g show examples of
detected objects for a sample of the training images.
3.3.2 Circle object detection
We used the Java implementation of the Hough Transform
Circle detection algorithm [3, 25] to detect the location of
the circle’s center points and radii lengths. Figures 3b, 3e,
and 3h show examples of circles detected by this method.
3.3.3 Image corners
As can be seen in Figures 3c, 3f, and 3i, many users clicked
on the image corners themselves (top left, top right, bottom
left and bottom right, as opposed to corners that appear in
the subject of the image, such as a building) even though
there may not have been a detected region of interest. We
hypothesize that this is because these points are memorable
131
Table 2: Proportion of actual and detected click
points and their related entropy per click point in