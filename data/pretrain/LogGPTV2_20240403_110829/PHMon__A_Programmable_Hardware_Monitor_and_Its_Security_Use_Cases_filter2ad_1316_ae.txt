3.05%
Table 3: Performance overhead of PHMon-based shadow
stack compared to that of HDFI-based (as reported in [76])
and LLVM-based shadow stacks.
Table 3 (the ﬁrst three columns) provides a head-to-head
comparison for the performance overhead of PHMon-based
and HDFI-based shadow stacks. For both PHMon and HDFI,
the evaluation baseline is the RISC-V Rocket processor. Un-
fortunately, HDFI only provides the shadow stack overhead
numbers for four SPECint2000 benchmarks [76]. These four
benchmarks are cross-compiled for RISC-V using the GCC
toolchain. On average, for these four benchmarks, PHMon
has a 1.0% performance overhead compared to a 2.1% perfor-
mance overhead of HDFI.
In the last column of Table 3, we reported the performance
overhead of our front-end pass LLVM implementation of a
shadow stack. Our LLVM pass instruments the prologue and
epilogue of each function to push the original return address
and pop the shadow return address, respectively. We used
Clang to compile four SPECint2000 benchmarks and used
the reference input for our evaluations. We only compiled
the main executable of SPEC benchmarks (without libraries
such as glibc) using Clang. Hence, the implemented front-
end pass only protects the main executable. On average, our
gzip
mcf
gap
bzip2
LLVM Plugin
2.24%(cid:63)
8.42%†
12.30%±
3.66%(cid:63)
(cid:63) Similar to HDFI, due to the memory limitations of our evaluation board, we had to reduce the
buffer size of the reference input to 3 MB for gzip and bzip2 benchmarks.
± We used -O0 for PHMon and -O2 for LLVM and an input buffer size of 96 MB to run gap.
† Due to memory limitation of our evaluation board, we used test input for mcf benchmark.
LLVM plugin has a 5.4% performance overhead.
The main source of performance overhead for PHMon is
an increase in the number of memory accesses. Unlike our
Rocket processor conﬁguration, in a realistic deployment, the
processor would at least include an L2 data cache. Hence, we
expect PHMon’s performance overhead to be lower in a real-
istic deployment, which alleviates the signiﬁcant performance
overhead caused by a cache miss.
To put PHMon’s performance overhead into perspective,
Table 4 compares PHMon’s overhead with that of other state-
of-the-art software and hardware shadow stack implementa-
tions. To facilitate this comparison, we have only listed the
implementations that measure their performance overhead
on SPEC benchmarks. As an overall criterion, the average
overhead of a technique should be less than 5% for getting
adopted by industry [79], which PHMon’s shadow stack im-
USENIX Association
29th USENIX Security Symposium    817
mcf†gzip⋆twolfbzip⋆vprgcccraftygap±parserGeometricMeanbzip2libquantumgobmkhmmerastarh264refxalancbmkgccGeometricMeanFFTsusanblowfish (dec)GSM (enc)IFFTblowfish (enc)GSM (dec)shaADPCM (enc)basicmathjpeg (enc)qsortADPCM (dec)jpeg (dec)bitcountdijkstrapatriciarijndael (enc)stringsearchGeometricMean0.00.51.01.52.02.53.03.54.04.55.0Performance Overhead (%)0.41.11.11.21.31.41.41.94.81.40.30.51.11.11.22.62.73.41.20.00.10.10.10.20.20.30.40.50.60.80.90.91.31.51.81.93.15.10.5SPECint2000SPECint2006MiBenchTable 4: Performance overhead of previous software and hard-
ware implementations of shadow stack compared with PH-
Mon.
Mechanism
Methodology
[79]
[1]
[17]
[22]
[75]
[86]
[20]
[56]
[49]
[76]
PHMon
Software (LLVM plugin)
Software (binary rewriting)
Software (binary rewriting)
Software (Pin tool)
Software (DynamoRIO)
Software (static binary instrumentation)
Software
Hardware
Hardware
Hardware
Hardware
Performance Overhead
5% on SPEC2006
21% on SPEC2000 (CFI + ID check)
20.53% on SPEC2000 (encoding)
53.60% on SPEC2000 (memory isolation)
2.17× on SPEC2006
18.21% on SPEC2000
18% on SPEC2006
3.5% on SPEC2006
∼0.5%-∼2.4% on SPEC2000
0.24% on SPEC2006
2.1% on SPEC2000
1.4% on SPEC2000, 1.2% on SPEC2006
Figure 6: Performance improvement of PHMon over the base-
line AFL compared to fork server AFL. The numbers below
the “Baseline AFL” bars show the number of executions per
second for the baseline AFL.
plementation satisﬁes.
Hardware-Accelerated Fuzzing. To fuzz RISC-V programs,
we integrated AFL into the user-mode RISC-V QEMU ver-
sion 2.7.5. We fuzzed each of the 6 vulnerable programs for
24 hours using QEMU on the Zedboard FPGA. To provide
a fair comparison, for the PHMon-based AFL experiments,
we fuzzed each of these programs for the same number of
executions as in the QEMU experiments. Similar to other
works in fuzzing [71, 77], we used the number of executions
per second as our performance metric. We fuzzed each vul-
nerable program three times and calculated the average value
of performance (all standard deviations were below 1%).
For performance evaluation, we used the user-mode
QEMU-based AFL running on the FPGA as our baseline.
We also ran the QEMU-based fork server version of AFL
as a comparison point for PHMon. Figure 6 shows the
performance improvement of the PHMon-based AFL over
our baseline compared to the performance improvement of
the fork server version of AFL. On average, PHMon improves
AFL’s performance by 16× and 3× over the baseline and
fork server version, respectively. Similar to the baseline AFL,
we can integrate PHMon with the fork server version of
AFL. We expect this integration to further enhance PHMon’s
performance improvement of AFL. We validated the correct
functionality of the PHMon-based AFL by examining the
found crashes. On average, for the 6 evaluated vulnerable
programs, PHMon-based AFL and the baseline AFL detected
12 and 11 crashes, respectively, for the same number of
executions. The mismatch between the two approaches is
due to the probabilistic nature of AFL-based fuzzing. Since
PHMon improves the performance of AFL, it increases the
probability of ﬁnding more unique crashes compared to the
baseline.
Detecting Information Leakage. To validate that PHMon
detects and prevents conﬁdential information leakage, speciﬁ-
cally private key of a server, we reproduced the Heartbleed
attack on the FPGA by using OpenSSL version 1.0.1f. We ini-
tially sent non-malicious heartbeat messages to the server. As
expected, none of these messages resulted in false positives.
Next, we sent malicious heartbeat messages to the server to
leak information. PHMon successfully detected the informa-
tion leakage attempt and triggered an interrupt; and then, the
OS terminated the process. For the non-malicious heartbeat
messages, PHMon has virtually no performance overhead
(only once a key is accessed, PHMon performs a few ALU
operations).
Watchpoints and Accelerated Debugger. We have used
and validated the watchpoint capability of PHMon as part of
the information leak prevention use case. Also, we evaluated
PHMon’s capability in accelerating a conditional breakpoint
in a loop. Once the program execution reaches the breakpoint,
PHMon triggers an interrupt. We evaluated two scenarios
for handling the interrupt, trapping into GDB (PHMon_GDB)
and terminating the process by generating the core dump ﬁle
(PHMon_CoreDump). Figure 7 shows the activation time of the
breakpoint over the loop index value for GDB compared to
two PHMon-accelerated scenarios. In case of GDB, which
uses software breakpoints, each loop iteration results in two
context switches to/from GDB, where GDB compares the
current value of the loop index with the target value.
For the PHMon_GDB case, since PHMon monitors and eval-
uates the conditional breakpoint, GDB can omit the software
breakpoints used in the previous case. Due to the initial over-
head of running GDB, PHMon_GDB has a similar execution
time as GDB for the ﬁrst breakpoint index (i = 0). By in-
creasing the breakpoint index, PHMon_GDB’s execution time
virtually stays the same while GDB’s execution time increases
linearly. For the PHMon_CoreDump case, since PHMon mon-
itors the conditional breakpoint and generates a core dump
(without running GDB), the performance overhead is neg-
ligible (i.e., virtually 0). This experiment clearly indicates
PHMon’s advantage as an accelerated debugger.
Context Switch Performance Overhead. We measured the
performance overhead of maintaining PHMon’s conﬁguration
(including the conﬁguration of MUs and CFUs, the counter
and threshold of each MU, and local registers) across con-
text switches for mcf benchmark with test input. On aver-
age, over three runs, PHMon increases the execution time
818    29th USENIX Security Symposium
USENIX Association
sleuthkitzstdunaceindentnasmpcreGeometricMeanBenchmarks0510152025Performance improvement (X)11.313.716.417.818.920.616.13.77.64.26.15.26.35.41.01.01.01.01.01.01.0(0.11)(0.18)(0.15)(0.14)(0.13)(0.12)(0.14)Baseline AFLPHMonFork ServerFigure 7: The performance overhead of PHMon compared to
GDB for a loop conditional breakpoint.
Figure 8: The power and area overheads of PHMon compo-
nents compared to the baseline Rocket processor.
of a context switch by 4.01%. In total, for mcf benchmark,
maintaining PHMon’s conﬁguration during context switches
takes 0.14 ns, while overall context switches on the baseline
processor take 23.80 ns (the total execution time of the pro-
cess is 5.93 s, where on average 175 context switches happen).
The required operation to maintain PHMon’s conﬁguration
during a context switch is constant. Hence, we expect the per-
formance overhead of PHMon during context switches to be
the same for other benchmarks. According to our evaluations
for the shadow stack use case, the activation queue is empty
before each context switch and there is no need to delay a con-
text switch to complete the remaining actions. However, for
different use cases depending on the actions, we might need
to delay a context switch to perform the remaining actions.
6.3 Power and Area Results
We measured the post-extraction power and area consumption
of PHMon and the Rocket processor using the Cadence Genus
and Innovus tools (at 1 GHz clock frequency). In this mea-
surement, we used black box SRAMs for all of the memory
components; then, we used CACTI 6.5 to estimate the leak-
age power and energy/access of memory components. Rocket
contains an L1 data cache and L1 instruction cache while PH-
Mon includes a Match Queue and Action Config Table
as the main memory components. In our implementation,
the Match Queue and each Action Config Table consist
of 2,048 and 16 elements, respectively. Each Match Queue
element is 129-bit wide (for a conﬁguration with 2 MUs),
while each Action Config Table is 79-bit wide. Due to
the small size of the Action Config Table, its power and
area overheads are negligible.
To estimate the dynamic power of the Rocket’s L1 caches
and PHMon’s Match Queue, we determined the average
Table 5: The power and area of PHMon’s AU and RISC-V
Rocket core determined using 45nm NanGate.
Power (µW /MHz)
Description @1 GHz @180 MHz Area (mm2)
Rocket core
PHMon’s AU
0.359
0.048
534.3
43.8
556.7
25.0
memory access rate of these components using PHMon and
CSR cycle address. We estimated the access rate of the
Match Queue for two of our use cases,4 i.e., the shadow stack
and the hardware-accelerated AFL, by leveraging PHMon (2
MUs with threshold=max) to count the number of calls
and rets, jumps and branches, and call and branches,
respectively. We averaged the access rates of our two use
cases and determined the average dynamic power consump-
tion based on this metric. Figure 8 depicts the total area over-
head as well as the power overhead of the main components
of PHMon compared to the baseline Rocket processor. There
is a trade-off between the number of MUs and the power and
area overheads of PHMon. For the number of MUs ranging
from 1 to 6, PHMon incurs a power overhead ranging from
3.6% to 10.4%. Similarly, area overhead ranges from 11%
to 19.9% as we increase the MU count from 1 to 6. For all
of our use cases in this paper, we used a design with only 2
MUs. This design has a 5% power overhead and it incurs a
13.5% area overhead. Table 5 lists the absolute power and
area consumed by PHMon’s AU and the Rocket core.5 Our
FPGA evaluation shows that a PHMon conﬁguration with 2
MUs increases the number of logic Slice LUTs by 16%.
7 Discussion and Future Work
In this Section, we address some of undiscussed aspects of
PHMon and present our future work.
7.1 Architecture Aspect
As discussed in Section 4, PHMon maintains the incoming
match packets in a queue prior to performing follow-up ac-
tions. The size of this queue is a design decision, which affects
the number of match packets that PHMon can have in ﬂight.
We envision that when the queue is full, PHMon can take
one of the following actions: 1) PHMon may opt to drop the
incoming match packets; 2) PHMon could stall the instruction
fetch stage of Rocket’s pipeline; 3) PHMon could raise an
interrupt, then the OS stays in a sleep state, until a certain
number of empty slots are available. In our current prototype,
PHMon stalls the pipeline once the queue gets full. For all our
experiments, a size of 2KB entries for the queue was sufﬁcient
to avoid any stalling.
PHMon performs actions in a blocking manner, i.e., it only
performs one action at a time. Although the L1 data cache
4The access rate for the other two use cases is negligible.
5Note that in 40GPLUS TSMC process, Rocket processor has 0.034
mW/MHz dynamic power consumption and its area is 0.39 mm2 [44]. Here,
we use a non-optimized but publicly available process (45nm NanGate) for
power and area measurements.
USENIX Association
29th USENIX Security Symposium    819
0200400600800100012001400160018002000Conditional Break Loop Index050100150200250300350Time (s)GDBPHMon_GDBPHMon_CoreDump1MU2MUs3MUs4MUs5MUs6MUsNumber of Matching Units02468101214[%] Power OverheadAreaMatch QueueMUALUCU + other logic0510152025[%] Area Overheadin Rocket is non-blocking, PHMon blocks the rest of the
actions while waiting to receive a memory response. This can
increase the run time for performing actions. The evaluation
results presented in the paper include the effect of blocking
actions. Potentially, we can modify PHMon such that it can
perform non-blocking actions. Although such a design will
improve the performance, it will increase the complexity and
power/area overheads of PHMon.
In this paper, we interface our PHMon with an in-order
RISC-V processor. We implement the AU of PHMon as a mi-
crocontroller with restricted I/O, which implements a limited
hand-crafted 16-bit ISA and provides a safe and restricted
domain to take actions. Our developed ISA does not include
branches/jumps, i.e., our AU is not Turing complete. This
limited processing implementation is useful for preventing se-
curity threats. However, if a user requires actions that cannot
be implemented by our restricted ISA, the option of trigger-
ing an interrupt provides the user with ﬂexibility of executing
actions in form of arbitrary programs. Then, PHMon can