could cover the size of a human vocal tract. Each pair is
synchronized to measure the TDoA of the sound origin to
the two microphones. These pairs produce three indepen-
dent TDoA values, which could uniquely locate the sound
origin in a 3D space. We measure the TDoA in terms of the
2Note that the sectional view of the human vocal tract in
Figure 5 (b) is on the Y-Z plane.
1082Dœ≠
œ±ƒê≈µ
DœÆ
DœÆ
œ±ƒê≈µ
œ±ƒê≈µ
DœÆ
Dœ≠
œ≥Õòœ≤ƒê≈µ
œ≠œ¨ƒê≈µ
Dœ≠
ŒÄ≈µŒÅ
ŒÄ‡°öÕ¨‡°©ŒÅ
ŒÄ≈©ŒÅ
ŒÄ∆µŒÅ
ŒÄ∆êŒÅ
ŒÄ…ΩŒÅ
ŒÄ«ÅŒÅ


≈Ω

d
ŒÄ«åŒÅŒÄ∆öŒÅ
ŒÄ«ÄŒÅ
ŒÄ‡°ÅŒÅ
ŒÄƒû‡°ÅŒÅ
ŒÄ‡†≥ŒÅ ŒÄ‡°°ŒÅ
ŒÄ‘•ŒÅ
ŒÄ≈ùŒÅ
ŒÄ‹≠ŒÅ
ŒÄ‡†ß‡°ÅŒÅ
ŒÄƒçŒÅ
ŒÄ‡†Ø‡°°ŒÅ
ŒÄ‡†™ŒÅ
ŒÄ‡†®ŒÅ
ŒÄ‡†ß‡•§ŒÅ

z

y
y
D/D\RXWRI0LFURSKRQHV
z
E/RFDOL]HGSKRQHPHVRXQGV
Figure 5: Phonemes localization using microphone
array.
number of delayed samples to the two microphones. As we
use 192kHz for recording, the TDoA ranging resolution is
1.77mm. Before the phoneme localization, we test the local-
ization accuracy by emitting chirp sounds at diÔ¨Äerent Ô¨Åxed
locations in front of the microphones. We observe that it
produces an averaged localization error within 2mm.
We recruit two participants to pronounce each phoneme
sound in front of the microphone array multiple trials. Fig-
ure 5 (b) illustrates the localized phoneme sound origins for
one participant. It shows the sectional view of human vocal
tract on Y-Z plane. The red dots show the localized vowel
sound origins, whereas the green dots show these of conso-
nant ones. We obtain several important observations from
Figure 5. First, the located sound origins of vowels match
the tongue positions very well. For example, the vowels con-
nected by the dotted lines in Figure 5 (b) have similar rel-
ative positions and overall shape as that of the vowel chart
in Figure 2 (b). This is because the tongue position is the
deterministic factor of vowel production. Second, some of
the consonants have the origins close to the place of articu-
lation, while others are signiÔ¨Åcantly aÔ¨Äected by the manner
of articulation. For instance, [s],[z] and [t] have the local-
ized sound origins close to alveolar, which is the place of
articulation of these sounds, whereas [m] is located in the
nasal cavity where the airÔ¨Çows out (i.e., manner of articu-
lation). Moreover, we observe the located phoneme origins
are mainly distributed within the mouth and nasal cavities
with the size of about 4cm by 4cm, and they show little
changes in X axis (i.e., lateral direction of mouth). We also
Ô¨Ånd that diÔ¨Äerent participants produce diÔ¨Äerent localized
sound origins for the same phoneme due to the individual
diversity in the human vocal tract (e.g., shape and size) and
the habitual way of pronouncing phonemes.
3. SYSTEM DESIGN
In this section, we introduce our system design and its
core components and algorithms.
3.1 Approach Overview
The key idea underlying our liveness detection system is
to perform TDoA ranging for a sequence of phoneme sounds
at the two microphones on the phone. As illustrated in
Figure 6, a user Ô¨Årst speaks an utterance, say ‚Äúvoice‚Äù to
the phone that closely placed to the user‚Äôs mouth. Each
phoneme sound (i.e., [v] [A] [I] [s] in the example) is then
emitted from the user‚Äôs vocal system and picked up by the
œ∞Õò«Ü∆ö∆åƒÇƒê∆ö≈ù≈∂≈êd≈Ωƒö«á≈∂ƒÇ≈µ≈ùƒê≈Ωƒ®∆â≈ö≈Ω≈∂ƒû≈µƒû∆ê
ƒ®≈Ω∆å≈Ø≈ù«Äƒû≈∂ƒû∆ê∆êƒöƒû∆öƒûƒê∆ö≈ù≈Ω≈∂Õò
œ≠Õòh∆êƒû∆å∆ê∆âƒûƒÇ≈¨∆êƒÇ≈∂∆µ∆ö∆öƒû∆åƒÇ≈∂ƒêƒûÕïƒûÕò≈êÕòÕïÕû«Ä≈Ω≈ùƒêƒûÕü

«Å≈ù∆ö≈ö∆â≈ö≈Ω≈∂ƒû≈µƒû∆êÕóŒÄ«ÄŒÅŒÄ‡†™ŒÅŒÄ/ŒÅŒÄ∆êŒÅÕò
ŒÄ«ÄŒÅ
ŒÄ‡†™ŒÅ
ŒÄ‡°ÅŒÅ
ŒÄ∆êŒÅ
d≈ΩŒÄ∆êŒÅ
d≈ΩŒÄ/ŒÅ
ŒÄ∆êŒÅ
ŒÄ‡°ÅŒÅ
ŒÄ‡†™ŒÅ
D≈ùƒêœÆ
d≈ΩŒÄ«ÄŒÅ d≈ΩŒÄ‡†™ŒÅ
D≈ùƒêœ≠
œÆÕòƒÇƒê≈ö∆â≈ö≈Ω≈∂ƒû≈µƒû∆ê≈Ω∆µ≈∂ƒö∆â∆å≈Ω∆âƒÇ≈êƒÇ∆öƒû∆ê
∆ö≈Ω∆ö≈öƒû∆ö«Å≈Ω≈µ≈ùƒê∆ê ≈Ωƒ®∆ö≈öƒû∆â≈ö≈Ω≈∂ƒûÕò
ŒÄ«ÄŒÅ
œØÕòW≈ö≈Ω≈∂ƒû≈Ω∆åƒÇ∆µ∆ö≈öƒû≈∂∆ö≈ùƒêƒÇ∆ö≈ù≈Ω≈∂∆ê«á∆ê∆öƒû≈µƒöƒûƒö∆µƒêƒû∆êd≈Ω
≈Ωƒ®ƒûƒÇƒê≈ö∆â≈ö≈Ω≈∂ƒû≈µƒû∆ö≈Ω∆ö≈öƒû∆ö«Å≈Ω≈µ≈ùƒê∆å≈Ω∆â≈ö≈Ω≈∂ƒû∆êÕò
Figure 6: Illustration of phoneme localization using
a single phone.
two microphones of the phone with stereo recording. The
phone processes the recorded sound to deduce the TDoA
of each phoneme sound to the two microphones. As most
phoneme sounds have measurable TDoA diÔ¨Äerences to the
two microphones, a sequence of phonemes will produce series
of TDoA with various values, as shown in Figure 6. We refer
to the changes in TDoA measurements as ‚ÄúTDoA dynamic‚Äù,
which is then used for liveness detection.
In particular, the measured TDoA dynamic will be com-
pared with the one extracted when the user enrolled in the
system. A live user is detected if the similarity score exceeds
the pre-deÔ¨Åned threshold. Under playback attacks, the mea-
sured TDoA dynamic will be very diÔ¨Äerent from that of a live
user due to diÔ¨Äerent sound production systems (i.e., loud-
speaker v.s. human vocal system). Under replace attacks,
it is extremely unlikely, if not impossible, for an adversary
to place a stereo recorder (e.g., smartphone) very close, say
5cm, to the victim‚Äôs mouth to collect voice samples. Due
to the origins of the phoneme sounds are crowded in the
mouth and nasal cavities as shown in Figure 5 (b), the TDoA
dynamic diminishes rapidly with the increased distance be-
tween the recorder and the user‚Äôs mouth. For example, if
the phone is placed 30cm away from the user‚Äôs mouth, the
maximum achievable TDoA range among all phonemes is
less than 1cm. With such a small range, most phonemes
have the same TDoA measurement to the two microphones
of the phone. The measured TDoAs under replace attack
thus cannot match the one extracted when the user enrolled
in the system.
Virtually all smartphones are equipped with two micro-
phones and are capable of stereo recording. By leveraging
a sequence of phoneme sounds in an utterance/passphrase,
our approach relaxes the problem of locating each phoneme
sound to tracking TDoA dynamic for live user detection.
We thus enable the phoneme localization based liveness de-
tection on a single phone without requiring any additional
hardware.
Our system does require the user to place the smartphone
close to the mouth with the same pose in both enrollment
and authentication processes. The eÔ¨Äects of diÔ¨Äerent phones
and phone displacement are studied in experiment evalua-
tion. Moreover, data protection mechanisms or secure com-
munication protocols should be in place to prevent an at-
tacker from obtaining the plain-text of TDoA dynamic and
the dual-channel audio samples [18]. For example, TDoA
dynamic could be extracted locally without storing the dual-
channel audio sample, and only the encrypted one-channel
1083d≈Ω
«á≈∂ƒÇ≈µ≈ùƒê
W∆å≈Ωƒ®≈ù≈Øƒû∆ê
5000
5000
4000
AU
E
T
C
TI
N
s≈Ω≈ùƒêƒû
^ƒÇ≈µ∆â≈Øƒû
W≈ö≈Ω≈∂ƒû≈µƒû
^ƒû≈ê≈µƒû≈∂∆öƒÇ∆ö≈ù≈Ω≈∂
d≈Ω
ƒÇ≈Øƒê∆µ≈ØƒÇ∆ö≈ù≈Ω≈∂
^≈ù≈µ≈ù≈ØƒÇ∆å≈ù∆ö«á
≈Ω≈µ∆âƒÇ∆å≈ù∆ê≈Ω≈∂
ƒû∆öƒûƒê∆ö≈ù≈Ω≈∂


>≈ù«Äƒûh∆êƒû∆å
≈Ω∆å
Zƒû∆â≈ØƒÇ«á∆ö∆öƒÇƒê≈¨
Figure 7: The Ô¨Çow of our liveness detection system.










 







 
Figure 8: Example: spectrogram of phonemes.
audio sample together with the encrypted TDoA dynamic
are transmitted or used for veriÔ¨Åcation and liveness detec-
tion.
3.2 System Flow
Realizing our system requires four major components: Pho-
neme Segmentation, TDOA Calculation, Similarity Compar-
ison, and Detection. As shown in Figure 7, the voice sample
acquired by two microphones Ô¨Årst passes through phoneme
segmentation, which extracts phonemes existing in the voice
sample. In particular, we combine Hidden Markov Model-
ing techniques to perform forced alignment on the words
recognized from the voice sample to identify each phoneme
sound. The words in the voice sample are recognized by
acoustic modeling and language modeling algorithms.
Next, the TDOA calculation component is used to calcu-
late the number of delayed samples of each phoneme sound
to the two microphones. As acoustic signals can be eas-
ily distorted due to multipath propagation, simply corre-
lating phonemes between two channels will result in large
error. To address this challenge, we adopt generalized cross-
correlation and heuristic-based phase transform weighting
approaches for accurate TDoA estimation.
After that, the similarity comparison component measures
the similarity of the calculated TDoA dynamic to the one
stored in the system. It results in a similarity score, which is
then compared with a pre-deÔ¨Åned threshold. If the score is
larger than the threshold, a live user is detected, otherwise
a replay attack is declared. The detection result can be then
combined with the traditional voice authentication system
to verify the claimed identity of a user.
3.3 Phoneme Segmentation
The underlying principle for phoneme segmentation is that
the sound of a phoneme contains a number of diÔ¨Äerent over-
tone pitches simultaneously, known as formants [23]. By
analyzing the sound spectrogram, we are able to discover
these overtone pitches or formats to identify each individual
phoneme sound. Although the most informative formants
are the Ô¨Årst three formants, the two Ô¨Årst formants, F1 and
TH
N
I
A
O
)
z
H
(
y
c
n
e
u
q
e
r
F
3000
2000
1000
0
0
0
0
0.3
0.6
Time (s)
0.9
1.2
1.401
Figure 9: Example of segmented phonemes.
F2, are enough to disambiguate the vowel. As illustrated in
Figure 8, it is easy to observe the Ô¨Årst two formants, F1 and
F2, which contribute to the overtone of each vowel most.
It is thus feasible to segment diÔ¨Äerent vowels by looking at
the F1 and F2 in the spectrogram. Unlike vowels, conso-
nants‚Äô spectrograms display as random mixture of diÔ¨Äerent
frequencies, as showed in Figure 8. This static noise-like
sound makes it diÔ¨Écult to accurately identify each conso-
nant by simply utilizing formants. We thus adopt forced
alignment by using HMM (Hidden Markrov Models), which
aligns the input voice spectrogram with existing voice sam-
ples to distinguish diÔ¨Äerent consonants [20].
In particular, we Ô¨Årst recognize the words existing in the
voice sample, which could be done by using automatic speech
recognition (ASR). We use advanced CMUSphinx [29] to au-
tomatically recognize each word in the user‚Äôs voice sample.
More speciÔ¨Åcally, the voice sample is Ô¨Årst parsed into fea-
tures, which are a set of mel-frequency cepstrum coeÔ¨Écients
(MFCC) that model the human auditory system. Then, the
MFCCs are combined together with the dictionary, acoustic
model, and language model to recognize the words in the
voice sample [29].
Given the recognized words, we utilize MAUS as primary
method for phoneme segmentation and labeling [21]. In par-
ticular, the recognized words are Ô¨Årst transferred into ex-
pected pronunciation based on standard pronunciation model
(i.e., SAMPA phonetic alphabet). Then, the generated canon-
ical pronunciation together with the millions of possible ac-
cents of users yield a probabilistic graph including all possi-
ble hypotheses and the corresponding probabilities. At last,
the system searches the graph space for the path of pho-
netic units that have been spoken with highest probability
using a Hidden Markrov Model. Outcomes of the search are
segmented and labeled phonetic units. Figure 9 illustrates
one example of the resulted phoneme segmentation when
one user pronounces the word ‚Äúauthentication‚Äù. We observe
that the segmentation accurately captures both the vowels
and the consonants.
3.4 TDOA Calculation
The basic idea of TDoA calculation is to count the num-
ber of delayed samples to the two microphones by correlating
each segmented phoneme sound between smartphone‚Äôs two
channels. Let‚Äôs denote mic1 and mic2 as the two micro-
phones/channels of the phone, and Œît as the TDoA of one
phoneme sound to the two microphones. Given the phoneme
sound mic1(t) recorded at mic1, we correlate such phoneme
sound to the sound signal mic2(t + d) recorded at the mic2,
with d varying from 0 to N ‚àí 1. Once the best match is
found, the corresponding d value is the number of delayed
samples between mic1 and mic2. In particular, such correla-
1084-30
-35
l
s
e
p
m
a
S
data1
data2
data3
data4
data5
data6
data7
data8
data9
data10
-40
-45
f
o
s
m
r
e
T
n
i
A
o
D
T
-50
O s car di dn t
li ke s wee
p day
Phoneme Sound
Figure 10: TDoAs of one passphrase for 10 trials.
tion can be done by using a cross-correlation technique [25],