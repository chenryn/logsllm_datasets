### Bandwidth Cost Reduction and Network Exception Handlers

The cost of bandwidth during a typical charging period, usually a month, is calculated by the provider based on the 95th percentile of the corresponding cumulative distribution function (CDF). 

Assume that the operator of our monitored network aims to reduce this bandwidth cost by lowering the 95th percentile of upstream traffic, while minimizing the impact on application performance and user experience. One approach to achieve this is through network exception handlers, which can rate-limit low-priority applications when traffic exceeds a threshold approaching the 95th percentile. For example, this threshold could be set at the 90th percentile of last month's upstream traffic, assuming that overall traffic demands remain relatively stable over such periods.

Low-priority applications may include long file transfers and non-interactive applications like backups, where user behavior is less affected compared to rate-limiting HTTP traffic. An example of an exception handler implementing this policy is shown in Figure 8.

### Evaluation of the Exception Handler

To evaluate the effectiveness of this exception handler in our enterprise network, we replayed captured traffic using the specified exception handler. We first extracted the traffic destined outside the local building from the trace using topology information. The exception handler was then applied in all 5-minute intervals where traffic exceeded the specified threshold.

Figure 9 (left) shows the upstream traffic timeseries before and after applying the exception handler, with the handler activated when traffic crosses the 90th percentile of the total trace. The exception handler successfully eliminates most large spikes in the traffic timeseries by smoothing heavy periods caused by "low-priority" applications. Figure 9 (right) further illustrates how the traffic distribution is affected, shifting traffic from heavier periods to periods below the 95th percentile. Overall, by applying this simple exception handler, traffic never rises above the 96th percentile of the original traffic.

### Network Exception Handler: Server under SYN Attack

```plaintext
boolean Exception(NetworkState)
begin
    return NetworkState.SYNAttackedMachines is not empty set
end

void Handler (NetworkState, Hoststate)
begin
    if not Exception(NetworkState) then
        DeRegister(Handler); return
    foreach host in NetworkState.SYNAttackedMachines do
        PID := SYNPktRateToDest(host, threshold)
        if PID != NULL then TerminateProcess(PID)
    end
end
```

Figure 10: A network exception handler to terminate a process if the rate of SYN packets it generates to a particular destination exceeds a threshold.

### Cooperation and Complexity

While it might be attractive for multiple applications to cooperate to divide up capacity, the complexity of implementing dynamic control over hundreds of thousands of geographically spread hosts is high.

### Routing Decisions

Since the network topology is distributed across the network, hosts could potentially make routing decisions and implement mechanisms such as multipath and loose source-routing [19], or load balancing techniques like ECMP (equal-cost multipath). However, we chose not to allow hosts to make such routing decisions because it could lead to traffic oscillations across links. For instance, if load-balancing is implemented at the host level, it is difficult to ensure that all hosts will not simultaneously shift their traffic from a congested link to another, leading to frequent congestion shifts and oscillations. This is another scenario where host cooperation would be beneficial, allowing a set of hosts to coordinate and reduce the likelihood of oscillations. Generally, hosts are better suited for per-application traffic management, while the network is better suited for routing and load-balancing across links.

### Closing the Loop

In our current design, information flows from the network to the hosts, but no information flows back from the hosts to the network. Closing this loop would provide a mechanism to facilitate better network management. For example, it could enable network administrators to gather context about certain exceptions, including both HostState and NetworkState, to aid in troubleshooting and management. However, challenges such as handling potential report implosions must be addressed.

### Security

Network exception handlers can expose powerful operations, such as terminating a local process. They can also be used in conjunction with network Intrusion Detection Systems (IDSs). For example, in a SYN attack scenario, the IDS detects the attack and inserts the IP addresses of the attacked machines into the NetworkState. Each host can then run an exception handler that triggers when this set is not empty. When triggered, the process generating the SYN packets is terminated if the rate exceeds a threshold. Figure 10 shows an example of such an exception handler. While this is a simplified case, it highlights the power and possibilities of providing hosts with network state information. Allowing feedback to network administrators would be valuable, but exposing information to hosts could be a mixed blessing if a host becomes compromised. Therefore, it is important to filter NetworkState information exposed to hosts and ensure that network operators sign all network exception handlers, and that CCs only execute signed handlers.

### Deployment

Network exception handlers can be partially deployed without affecting the performance of the enterprise network, provided that CCs have access to annotated topology information. Some scenarios may require deployment on all hosts in a domain for efficient handling. However, even partial deployment is feasible and allows for localized management, as local host decisions do not affect the performance of other remote hosts, and CCs can operate in isolation. In very small partial deployments, some policies may have limited impact. Network exception handlers do not rely on new network capabilities that add to the complexity of network management or modify host applications.

### Related Work

Network exception handlers allow hosts to participate directly in network management, enabling some traditional in-network functionality to be migrated to the host. Bellovin [3] proposed migrating firewall functionality to hosts, creating a distributed firewall where firewall policy is pushed to the hosts. This has been widely adopted in enterprises. The mechanisms used to implement network exception handlers could potentially be used to implement a distributed firewall, but enabling network management requires exposing more dynamic information about the network.

Several host-based congestion control schemes have been proposed, such as PCP [1] and endpoint-based admission control [4]. These rely on probe packet sequences to determine transmission rates. Bandwidth brokers use selected hosts to maintain QoS management state, providing admission control on a path-basis by conditioning at selected points on the network’s edge. Using network exception handlers potentially allows every host to condition the traffic they introduce, but with finer-grained control.

There are many proposals to monitor and measure network performance, such as loss rate, throughput, or round-trip-time, for management purposes [7, 8, 21]. Network exception handlers allow policies to specify desired reactions when particular behavior or performance is detected. They are almost the inverse of distributed network triggers [11, 10], which collect host measurements at a coordinator and raise an alarm when a policy violation is detected. Network exception handlers push the policy towards the hosts and, by exposing information about the network’s behavior, allow the hosts to implement the policy as required.

Several proposed network architectures improve network management by simplifying the network’s configuration and providing more control. Ethane [6] uses three high-level principles: policies declared over high-level names, policies determining the path packets follow, and strong binding of packets to their origin. Network exception handlers effectively implement the first and third principles but do so for performance rather than configuration. Tesseract [20] implements the 4D [16] control plane, enabling direct network control under a single administrative domain. Tesseract configures all network switch nodes to impose defined policies, making it host-independent. Network exception handlers enable policies to be imposed on the network without directly impacting network devices, supporting richer policies using information not exposed to the network.

CONMan [2] is an architecture for simplifying network device configuration and management. Each device is associated with a network manager, which maps high-level policy goals down to the capabilities of the devices. Network exception handlers allow the specification of a policy associated with a trigger, rather than a way to reconfigure the network to match overall policy requirements.

### Conclusion

In this paper, we argue that within a single administrative domain, such as an enterprise network, hosts should be more directly involved in network management. We introduced the concept of network exception handlers, where information is shared between the network and hosts so that when exceptional conditions are detected, hosts can react according to policies imposed by the operator. We described a simple programming model for specifying such policies and provided several examples of its potential use. Finally, we demonstrated the feasibility and effectiveness of our design using data from our global enterprise network. Network exception handlers are a simple yet powerful abstraction, enabling enterprise network operators to gain significant control over their network’s behavior. Our analysis suggests that changing our mindset about the architecture of enterprise networks is attractive and demonstrates the feasibility of one possible such change.

### References

[1] T. Anderson, A. Collins, A. Krishnamurthy, and J. Zahorjan. PCP: Efficient Endpoint Congestion Control. In Proc. ACM/USENIX NSDI 2006, pages 197–210, San Jose, CA, May 2006.

[2] H. Ballani and P. Francis. CONMan: A Step Towards Network Manageability. In Proc. ACM SIGCOMM, pages 205–216, New York, NY, 2007.

[3] S. M. Bellovin. Distributed firewalls. ;login:, pages 37–39, Nov. 1999.

[4] L. Breslau, E. W. Knightly, S. Shenker, I. Stoica, and H. Zhang. Endpoint Admission Control: Architectural Issues and Performance. In Proc. ACM SIGCOMM 2000, pages 57–69, New York, NY, 2000.

[5] R. Callon. Use of OSI IS-IS for routing in TCP/IP and dual environments. RFC 1195, IETF, Dec. 1990.

[6] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown, and S. Shenker. Ethane: Taking Control of the Enterprise. In Proc. ACM SIGCOMM, pages 1–12, New York, NY, 2007.

[7] Y. Chen, D. Bindel, H. Song, and R. H. Katz. An Algebraic Approach to Practical and Scalable Overlay Network Monitoring. In Proc. ACM SIGCOMM 2004, pages 55–66, New York, NY, 2004.

[8] E. Cooke, R. Mortier, A. Donnelly, P. Barham, and R. Isaacs. Reclaiming Network-wide Visibility Using Ubiquitous End System Monitors. In Proc. USENIX 2006 Annual Technical Conference, June 2006.

[9] S. Hanks, T. Li, D. Farinacci, and P. Traina. Generic Routing Encapsulation (GRE). RFC 1701, IETF, Oct. 1994.

[10] L. Huang, M. Garofalakis, J. Hellerstein, A. Joseph, and N. Taft. Toward Sophisticated Detection with Distributed Triggers. In MineNet’06, pages 311–316, New York, NY, 2006.

[11] A. Jain, J. M. Hellerstein, S. Ratnasamy, and D. Wetherall. A Wakeup Call for Internet Monitoring Systems: The Case for Distributed Triggers. In Proc. HotNets-III, San Diego, CA, November 2004.

[12] A. W. Moore and K. Papagiannaki. Toward the Accurate Identification of Network Applications. In Sixth Passive and Active Measurement Workshop (PAM), Boston, MA, 2005.

[13] J. Moy. OSPF Version 2. RFC 2328, IETF, Apr. 1998.

[14] D. Oran. OSI IS-IS Intra-domain Routing Protocol. RFC 1142, IETF, Feb. 1990.

[15] K. Ramakrishnan, S. Floyd, and D. Black. The Addition of Explicit Congestion Notification (ECN) to IP. RFC 3168, IETF, Sept. 2001.

[16] J. Rexford, A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, G. Xie, J. Zhan, and H. Zhang. Network-wide Decision Making: Toward a Wafer-thin Control Plane. In Proc. HotNets-III, San Diego, CA, Nov. 2004.

[17] E. Rosen and Y. Rekhter. BGP/MPLS IP Virtual Private Networks (VPNs). RFC 4364, IETF, Feb. 2006.

[18] A. Shaikh and A. Greenberg. OSPF Monitoring: Architecture, Design and Deployment Experience. In Proc. ACM/USENIX NSDI 2004, pages 57–70, San Francisco, CA, Mar. 2004.

[19] A. Snoeren and B. Raghavan. Decoupling Policy from Mechanism in Internet Routing. In Proc. HotNets-II, pages 81–86, Cambridge, MA, Nov. 2003.

[20] H. Yan, D. A. Maltz, T. E. Ng, H. Gogineni, H. Zhang, and Z. Cai. Tesseract: A 4D Network Control Plane. In Proc. ACM/USENIX NSDI 2007, pages 369–382, Cambridge, MA, May 2007.

[21] Y. Zhao, Y. Chen, and D. Bindel. Towards Unbiased End-to-End Network Diagnosis. In Proc. ACM SIGCOMM, pages 219–230, New York, NY, 2006.