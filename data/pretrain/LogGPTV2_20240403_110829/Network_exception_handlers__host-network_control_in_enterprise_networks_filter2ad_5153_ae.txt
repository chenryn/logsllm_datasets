of the charging period which is usually a month, the cost is cal-
culated by the provider at the 95th percentile of the corresponding
cumulative distribution function (CDF).
Let’s now assume that the operator of our monitored network
aims at reducing this bandwidth cost by scaling down the 95th per-
centile of upstream trafﬁc, but at the same time wishes to affect ap-
plication performance and user experience as little as possible. A
way to achieve this through network exception handlers would be
to rate limit low-priority applications at time instances where trafﬁc
is over a threshold approaching the 95th percentile. This threshold
could for example reﬂect the 90th percentile of last month’s up-
stream trafﬁc, as the overall trafﬁc demands should be more or less
stable over such periods. Low priority applications may correspond
here to long ﬁle transfers and non-interactive applications such as
backups, where users’ behavior is not signiﬁcantly affected, as it
would be when rate-limiting HTTP trafﬁc. An exception handler to
implement such a policy is presented in Fig. 8.
To evaluate the effect of this exception handler in our enterprise
network, we replayed our captured trafﬁc employing the speciﬁed
exception handler. We ﬁrst extracted the trafﬁc targeted out of
the local building from the trace using the topology information,
and then applied the exception handler in all 5 minute intervals
for which trafﬁc crosses over the speciﬁed threshold. Fig. 9(left)
presents the upstream trafﬁc timeseries before and after the use of
the exception handler, where the handler is applied when trafﬁc
crosses over the 90th percentile of the total trace. The exception
handler succeeds in eliminating most of the large spikes present in
the trafﬁc timeseries, by being able to smooth heavy periods caused
by “low-priority” applications. Fig. 9(right) further highlights how
the trafﬁc distribution is affected by the exception handler by shift-
ing trafﬁc from heavier periods to periods where trafﬁc is below the
95th percentile. Overall, by applying this simple exception han-
dler, trafﬁc never rises above the 96th percentile of the original
Network Exception Handler: Server under SYN attack
boolean Exception(NetworkState)
begin
return NetworkState.SYNAttackedMachines is not empty set
end
void Handler (NetworkState, Hoststate)
begin
if not Exception(NetworkState) then
DeRegister(Handler); return
foreach host in NetworkState.SYNAttackedMachines do
PID := SYNPktRateToDest(host, threshold)
if PID != NULL then TerminateProcess(PID)
end
Figure 10: Network exception handler to terminate a process if
the rate of SYN packets it generates to a particular destination is
larger than a threshold.
application X to cooperate in order to potentially divide up that
capacity. While attractive, the complexity of implementing this dy-
namic control over hundreds of thousands of geographically spread
hosts is high.
Routing. Since the network topology is distributed across the
network, hosts could potentially make routing decisions, and re-
alize mechanisms such as multipath and loose source-routing [19],
or load balancing techniques such as ECMP (equal-cost multipath).
However, we chose explicitly not to allow hosts to make such rout-
ing decisions, because attempting to perform such operations at the
host may lead to trafﬁc oscillations across links. For example, if
load-balancing is implemented at the host, it is hard to guarantee
that all hosts will not shift their trafﬁc from a congested link to a
second link simultaneously, thus leading eventually to trafﬁc oscil-
lations due to congestion frequently shifting to new links. This is
another instance where cooperation between hosts would be use-
ful, so that a set of hosts would be able to coordinate to reduce the
likelihood of oscillations. In general, hosts are better placed to per-
form per-application trafﬁc management, and the network is better
placed to perform routing or load-balancing across links.
Closing the loop. In our current design, information ﬂows from
the network to the hosts, but no information ﬂows from the hosts
to the network. If the loop were closed, then this would also pro-
vide a mechanism to facilitate better network management. For
example, this could be a good mechanism to express and expose
information to network administrators from hosts. At the moment,
most host monitoring is simplistic, with a small set of attributes
proactively monitored on each host, and aggregated into a single
database. In our architecture, as described, when a network excep-
tion occurs, the user(s) on the host can be explicitly informed. This
could be extended, so that the network administrator could also
be informed. Therefore, the network administrator could gather
the context of certain exceptions, i.e., both the HostState and Net-
workState, when the exception triggers, in order to facilitate trou-
bleshooting and management. Of course, there are a number of
challenges with such an approach, such as handling the potential
implosions of reports.
Security. Network exception handlers can expose powerful op-
erations. An extreme scenario would even be to terminate a lo-
cal process! Further, such a policy could be used in conjunction
with network Intrusion Detection Systems (IDSs). For example,
consider a SYN attack scenario. When the IDS detects an attack,
all the IP addresses of the machines under attack are inserted in
the NetworkState by the IDS. Each host then could run a network
exception handler that triggers when this set is not empty. When
triggered, the process producing the SYN packets would be termi-
nated, if the rate of SYN packets to a particular destination were
over a threshold. Fig. 10 shows an example of such an exception
handler. Of course, this is a simpliﬁed case, since we assume for
instance that the misbehaving process can actually be terminated,
it will not restart once terminated, etc. Yet, this example clearly
highlights the power and possibilities of providing the hosts with
network state information. This is also another example where al-
lowing feedback to the network administrators would be valuable.
However, exposing information to the hosts might be a mixed-
blessing, should a host become compromised. Having all topology
and trafﬁc information pushed to the host, may enable informed
attacks concentrated at speciﬁc “weak” points in the network. Hav-
ing the CC ﬁlter the NetworkState information exposed to the hosts
is therefore advantageous, and ensuring that the network operator
signs all network exception handlers, and that CCs only execute
signed network exception handlers is also important. In general,
we expect certain levels of trust to already exist within an enter-
prise network, and that hosts will authenticate at least with the CC.
Services such as Network Access Protection10 may further ensure
that systems are fully patched and are updated with the latest virus
deﬁnitions before gaining access to the corporate network. How-
ever, protection from zero-day exploits, or detection and handling
of compromised CCs are issues for future research.
Deployment. Network exception handlers may be partially de-
ployed without affecting the performance of the enterprise network
provided that CCs have access to annotated topology information.
Some of the scenarios described throughout the paper may require
that network exception handlers are deployed on all hosts in a do-
main for efﬁcient handling of the speciﬁc scenario. In general how-
ever, even partial deployment is feasible and allows for localized
management. This is possible as all local host decisions do not
affect the performance of other remote hosts, and CCs may oper-
ate in isolation. In very small partial deployments some policies
may have only limited impact. Network exception handlers do not
rely on the deployment of new network capabilities that add to the
complexity of network management (e.g., loose source routing) or
modiﬁcation of host applications.
9. RELATED WORK
Network exception handlers allow hosts to participate directly
in the management of the network; they allow some traditional
in-network functionality to be migrated to the host. Bellovin [3]
proposed migrating ﬁrewall functionality to the hosts, to create a
distributed ﬁrewall where ﬁrewall policy is pushed to the hosts
to implement. This has now been widely adopted in enterprises.
The mechanisms used to implement network exception handlers
could potentially be used to implement a distributed ﬁrewall. How-
ever, enabling network management requires exposing of further
dynamic information about the network.
Several host-based congestion control schemes have been pro-
posed, e.g., PCP [1] and endpoint-based admission control [4]. These
rely on probe packet sequences to determine the rate at which they
should transmit. Bandwidth brokers use selected hosts in the net-
work to maintain QoS management state so as to provide guaran-
teed services, essentially providing admission control on a path-
basis by conditioning at selected points on the network’s edge. Us-
ing network exception handlers potentially allows every host to be
able to condition the trafﬁc they introduce, but with much ﬁner
grained control.
10http://technet.microsoft.com/en-us/network/
bb545879.aspx
There are many proposals to monitor/measure the network’s per-
formance, e.g., loss rate, throughput, or round-trip-time, for man-
agement purposes [7, 8, 21]. Network exception handlers allow
policies to specify the desired reactions when particular behavior
or performance is detected. Network exception handlers are al-
most the inverse of distributed network triggers [11, 10] which col-
lect host measurements at a coordinator that raises an alarm when
policy violation is detected. Network exception handlers push the
policy towards the hosts and, by exposing information about the
network’s behavior, allow the hosts to implement the policy as re-
quired.
Several proposed network architectures improve network man-
agement by simplifying the network’s conﬁguration by providing
more control over the network. Ethane [6] uses three high-level
principles: policies are declared over high-level names, policies
should determine the path packets follow, and packets are strongly
bound to their origin. Network exception handlers effectively im-
plement the ﬁrst and third of these, but do so for performance rather
than conﬁguration. Tesseract [20] implements the 4D [16] control
plane enabling direct network control under a single administrative
domain. Tesseract conﬁgures all network switch nodes to impose
deﬁned policies on the network, so it is host independent. Net-
work exception handlers enable policies to be imposed on the net-
work without directly impacting network devices. Therefore, they
can support richer policy, using information not exposed to the net-
work.
CONMan [2] is an architecture for simplifying network device
conﬁguration and management. Each device is associated with a
network manager, which can map high-level policy goals down to
the capabilities of the devices. Network exception handlers allow
the speciﬁcation of a policy associated with a trigger, rather than
a way to reconﬁgure the network to match overall policy require-
ments.
10. CONCLUSION
In this paper, we argued that within a single administrative do-
main such as an enterprise network, hosts should be more directly
involved in the management of the network. To this end we intro-
duced the concept of network exception handlers, where informa-
tion is shared between the network and hosts so that when excep-
tional conditions are detected, hosts can be made to react subject to
policies imposed by the operator. We described a simple program-
ming model against which operators can specify such policies, giv-
ing several examples of its potential use. Finally, we described a
concrete design and demonstrated its feasibility and effectiveness
using data gathered from our own global enterprise network. Net-
work exception handlers are a simple, yet powerful abstraction en-
abling enterprise network operators to gain signiﬁcant control of
their network’s behavior. Our analysis suggests that changing our
mindset about the architecture of enterprise networks is attractive,
and demonstrates the feasibility of one possible such change.
11. REFERENCES
[1] T. Anderson, A. Collins, A. Krishnamurthy, and J. Zahorjan.
PCP: Efﬁcient Endpoint Congestion Control. In
Proc. ACM/USENIX NSDI 2006, pages 197–210, San Jose,
CA, May 2006.
[2] H. Ballani and P. Francis. CONMan: A Step Towards
Network Manageability. In Proc. ACM SIGCOMM, pages
205–216, New York, NY, 2007.
[3] S. M. Bellovin. Distributed ﬁrewalls. ;login:, pages 37–39,
Nov. 1999.
[4] L. Breslau, E. W. Knightly, S. Shenker, I. Stoica, and
H. Zhang. Endpoint Admission Control: Architectural Issues
and Performance. In Proc. ACM SIGCOMM 2000, pages
57–69, New York, NY, 2000.
[5] R. Callon. Use of OSI IS-IS for routing in TCP/IP and dual
environments. RFC 1195, IETF, Dec. 1990.
[6] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown,
and S. Shenker. Ethane: Taking Control of the Enterprise. In
Proc. ACM SIGCOMM, pages 1–12, New York, NY, 2007.
[7] Y. Chen, D. Bindel, H. Song, and R. H. Katz. An Algebraic
Approach to Practical and Scalable Overlay Network
Monitoring. In Proc. ACM SIGCOMM 2004, pages 55–66,
New York, NY, 2004.
[8] E. Cooke, R. Mortier, A. Donnelly, P. Barham, and R. Isaacs.
Reclaiming Network-wide Visibility Using Ubiquitous End
System Monitors. In Proc. USENIX 2006 Annual Technical
Conference, June 2006.
[9] S. Hanks, T. Li, D. Farinacci, and P. Traina. Generic Routing
Encapsulation (GRE). RFC 1701, IETF, Oct. 1994.
[10] L. Huang, M. Garofalakis, J. Hellerstein, A. Joseph, and
N. Taft. Toward Sophisticated Detection with Distributed
Triggers. In MineNet’06, pages 311–316, New York, NY,
2006.
[11] A. Jain, J. M. Hellerstein, S. Ratnasamy, and D. Wetherall. A
Wakeup Call for Internet Monitoring Systems: The Case for
Distributed Triggers. In Proc. HotNets-III, San Diego, CA,
November 2004.
[12] A. W. Moore and K. Papagiannaki. Toward the Accurate
Identiﬁcation of Network Applications. In Sixth Passive and
Active Measurement Workshop (PAM), Boston, MA, 2005.
[13] J. Moy. OSPF Version 2. RFC 2328, IETF, Apr. 1998.
[14] D. Oran. OSI IS-IS Intra-domain Routing Protocol. RFC
1142, IETF, Feb. 1990.
[15] K. Ramakrishnan, S. Floyd, and D. Black. The Addition of
Explicit Congestion Notiﬁcation (ECN) to IP. RFC 3168,
IETF, Sept. 2001.
[16] J. Rexford, A. Greenberg, G. Hjalmtysson, D. A. Maltz,
A. Myers, G. Xie, J. Zhan, and H. Zhang. Network-wide
Decision Making: Toward a Wafer-thin Control Plane. In
Proc. HotNets-III, San Diego, CA, Nov. 2004.
[17] E. Rosen and Y. Rekhter. BGP/MPLS IP Virtual Private
Networks (VPNs). RFC 4364, IETF, Feb. 2006.
[18] A. Shaikh and A. Greenberg. OSPF Monitoring:
Architecture, Design and Deployment Experience. In
Proc. ACM/USENIX NSDI 2004, pages 57–70, San
Francisco, CA, Mar. 2004.
[19] A. Snoeren and B. Raghavan. Decoupling Policy from
Mechanism in Internet Routing. In Proc. HotNets-II, pages
81–86, Cambridge, MA, Nov. 2003.
[20] H. Yan, D. A. Maltz, T. E. Ng, H. Gogineni, H. Zhang, and
Z. Cai. Tesseract: A 4D Network Control Plane. In
Proc. ACM/USENIX NSDI 2007, pages 369–382,
Cambridge, MA, May 2007.
[21] Y. Zhao, Y. Chen, and D. Bindel. Towards Unbiased
End-to-End Network Diagnosis. In Proc. ACM SIGCOMM,
pages 219–230, New York, NY, 2006.