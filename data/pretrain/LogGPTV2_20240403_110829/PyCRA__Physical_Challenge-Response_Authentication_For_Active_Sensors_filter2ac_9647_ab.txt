with. For example, automotive systems are equipped with
sensor failure detection systems which can detect whether
all sensor subsystems are correctly connected and alert the
driver if any of them fails [8].
Because of this, any attack must be carried out from a distance,
without direct access to any sensor hardware. In short, an adversary
is assumed to have access only to the physical/analog medium used
by the sensor—magnetic waves, optics, acoustics, etc.
Additionally, it is important to distinguish these sensors from
sensor nodes (which appear in the literature of sensor networks);
the attacks and countermeasures in this work target sensors them-
selves. Sensors are simple subsystems designed to perform only
one simple task; sensing the physical world. Because of this, many
sensors do not support remote ﬁrmware updates and do not typi-
cally receive commands from a remote operator, making such at-
tack vectors uncommon as many sensors do not have such capabil-
ities.
A2 Trusted Measured Entity We assume that the physical entity
to be measured by the sensor is trusted and incapable of be-
ing compromised.
Similar to the sensor hardware itself, the entity that the sensor aims
to measure is typically difﬁcult to access or alter directly while
maintaining Goals G1–G3. For example, in RFID systems the tag
itself is often encased in tamper-proof packaging [31, 17]; for ul-
trasonic ranging and active radar, maliciously altering the measured
entity (often the entire surrounding environment) is impractical in
time & effort and undoubtedly violates Goal G1; for airplane en-
gine speed sensors, the engines cannot easily be modiﬁed or re-
placed; for heart monitors, the heart cannot (we hope) be modiﬁed
[19], and so forth.
A3 Physical Delays (τattack): Adversaries require physical hard-
ware with inherent physical delays. This delay, though vari-
able in duration, is fundamental to all physical actuation and
sensing hardware.
These same analog/physical signals cannot be manipulated or even
observed (i.e. sniffed) without physical hardware. That is, to tam-
per with magnetic waves, an attacker needs hardware that is able to
generate magnetic waves, optical signals need physical hardware
that generates optical signals, and so on. Furthermore, this hard-
ware has to obey fundamental physics imposed by nature; the un-
derlying physics dictate that the response of any physical element
is governed by a dynamical model (mathematically modeled using
differential/difference equations) [9, ch. 2], [2, chs. 8–9]. This
dynamical model describes the output response for each physical
element in response to their inputs, e.g., the time for a voltage to
drop from a certain value to zero and so on. Although from a sys-
tem point of view, we often assume that analog signals like those in
Figure 2 take on logical values of 0 and 1, the underlying physics
is always different from this “system” point of view. For example,
Figure 2 shows how hardware that generates clock waveforms and
optical pulse signals behaves quite differently from the desired, log-
ical signals used to control them. In general, no physical signal can
arbitrarily jump from one state to another without suffering from
delays imposed by physics [9, ch. 2].
Furthermore, these physical delays are lower bounded by a non-
zero, fundamental limit. For example, the time response of an elec-
tromagnetic sensor/actuator is a multiple of physical constants like
magnetic permeability [2, chs. 8–9] or permitivity and electric con-
stants for capacitive sensors [9, ch. 4]. In general, the time response
of any sensor or actuator can never be below certain fundamental
thresholds controlled by physical constants. We refer to this physi-
cal delay as τattack for the remainder of this paper.
A4 Computational Delays: PyCRA is designed and analyzed with
a focus on physical delays. We make no assumption regard-
ing the computational power of a potential adversary.
We assume that an adversary has knowledge of the underlying se-
curity mechanism, attempting to conceal an attack by reacting to
each physical challenge or probe from the PyCRA-secured active
sensor. In practice, such an adversary would suffer from compu-
tational delays in addition to the physical delays addressed above.
These delays would make it even more difﬁcult for an adversary to
respond to these challenges in a timely manner. PyCRA is designed
00.20.40.60.811.21.41.61.82·10−70246Time[s]Voltage[volt]Physical/Analogsignal“Logical”representation00.20.40.60.81·10−40510Time[s]Voltage[volt]Physical/Analogsignal“Logical”representation1006(a)
Figure 3: An illustration of three physical attack types: (a) a passive eavesdropping attack, (b) a simple spooﬁng attack where a malicious
actuator blindly injects a disruptive signal, and (c) an advanced spooﬁng attack where an adversary uses a sensor to measure the original
signal and an actuator to actively cancel the original signal and inject a malicious one.
(b)
(c)
to leverage only the physical delays addressed above, but additional
computational delays would make it even easier to detect the pres-
ence of an attack.
2.5 Physical Attack Types for Sensors
Attacks can be classiﬁed as either passive (eavesdropping) or ac-
tive (spooﬁng). While we consider only physical/analog attacks in
accordance with assumptions A1–A4, the passivity of an attack is
decided by whether or not the attacker is manipulating (or spoof-
ing) the physical signal or merely listening to it. Active attacks
themselves can be classiﬁed once more into simple spooﬁng or ad-
vanced spooﬁng attacks. In short, physical sensor attacks in accor-
dance with assumptions A1–A4 can be broadly divided into three
categories (Types):
T1 Eavesdropping Attacks: In an eavesdropping attack, an ad-
versary uses a malicious sensor in order to listen to the active
sensor’s “communication” with the measured entity (Figure
3a).
T2 Simple Spooﬁng Attacks: In a simple spooﬁng attack, an ad-
versary uses a malicious actuator to blindly inject a mali-
cious signal in order to alter the signal observed by the sen-
sor. These attacks are simple in that the malicious signal is
not a function of the original, true signal (Figure 3b).
T3 Advanced Spooﬁng Attacks In an advanced spooﬁng attack,
an adversary uses a sensor in order to gain full knowledge
of the original signal and then uses a malicious actuator to
inject a malicious signal accordingly. This enables an at-
tacker to suppress the original signal or otherwise alter it in
addition to injecting a malicious signal (Figure 3c).
We argue that these attack types span all possible modes of attacks
that abide by Assumptions A1–A4 with those goals outlined in G1–
G3. For example, jamming or Denial of service (DoS) attacks falls
in category T2 where the attacker’s actuator is used to blindly gen-
erate high amplitude, wide bandwidth signals to interfere with the
physical signal before it reaches the sensors; replay attacks fall in
either category T2 or T3 based on whether the attacker is blindly
replaying a physical signal or destructing the original physical sig-
nal before inserting the replay signal; spooﬁng attacks like those
demonstrated in [19] fall in category T2; and attacks described in
[33] fall within both T2 and T3.
At ﬁrst glance, attacks of type T1 may not seem important espe-
cially if the sensor under attack measures a physical signal that is
publicly accessible (e.g., room temperature, car speed, etc.).
In
such cases, an adversary can measure the same physical signal
without the need to “listen” to the interaction between the active
sensor and the environment. However, this may not always be the
case. For example, an attacker might measure magnetic waves dur-
ing an exchange between an RFID reader and an RFID tag, learning
potentially sensitive information about the tag. These attacks are
passive, meaning that the attacker does not inject any energy into
the system. Sections 5 describes methods for detecting attack types
T2 and T3, leaving attack type T1 for later discussion in Section 6.
3. PYCRA AUTHENTICATION SCHEME
The core concept behind PyCRA is that of physical challenge-
response authentication. In traditional challenge-response authen-
tication schemes, one party requires another party to prove their
trustworthiness by correctly answering a question or challenge. This
challenge-response pair could be a simple password query, a ran-
dom challenge to a known hash function, or other similar mecha-
nisms. In the proposed physical challenge-response authentication,
the challenge comes in the form of a physical stimulus placed on the
environment by an active sensor. Unlike traditional schemes, the
proposed physical challenge operates in the analog domain and is
designed so that an adversary cannot issue the correct response be-
cause of immutable physical constraints rather than computational
or combinatorial challenges.
We begin by modeling the problem of detecting physical sensor
attacks as an authentication problem. To draw this analogy, let us
consider the communication system shown in Figure 4a. This ﬁg-
ure shows two ‘parties’: (1) an active sensor composed of actuation
and sensing subsystems and (2) the measured entity which responds
to signals emitted by the actuator contained within the active sen-
sor. The ﬁrst party—the active sensor—is responsible for initiating
the “communication” by generating some physical signal such as a
magnetic, acoustic, or optical wave. The second party—the mea-
sured entity—responds to this “communication” by modulating this
signal and reﬂecting it back to the sensing subsystem of the active
sensor. With this analogy in mind, the problem of detecting physi-
cal attacks can be posed as that of ensuring that the “message” seen
by the sensor has originated from a trusted party (the true entity
to be measured). This is akin to identity authentication in the the
literature of computer security but applied to the analog domain.
3.1 Simple PyCRA Attack Detector
Using the communication analogy shown in Figure 4a and re-
calling that we are interested only in active sensors as described in
Section 2.1, we notice that the measured entity, as a participating
party in this communication, is strictly passive, i.e. it cannot initi-
ate communication; it responds only when the sensor generates an
appropriate physical signal.
PyCRA exploits this “passivity” in order to facilitate the detec-
tion of attacks. Without PyCRA, an active sensor’s actuator would
probe the measured entity in a normal fashion using a determin-
ActuatorMeasured EntityActive SensorproberesponseSensorSensorMaliciousActuatorMeasured EntityActive SensorproberesponseSensorActuatorMaliciousActuatorMeasured EntityActive SensorproberesponseSensorSensorMaliciousActuatorMaliciousShield1007A (t)
t
u(t)
y(t)
B(t)
t
u(t)
y(t)
B(t)
t
u(t)
y(t)
a(t)
a(t)
a(t)
t
t
t
t
t
t
(a)
Figure 4: An illustration of the PyCRA architecture and attack detection scheme: (a) During normal operation, the active sensor generates a
signal A (t). This signal passes through environmental dynamics and is reﬂected back to the sensor as y(t); (b) Using the proposed PyCRA
scheme, the sensor generates a modulated signal B(t). If there is no attack present, the reﬂected signal diminishes if the active sensor’s
actuator is driven to zero; (c) Using the proposed PyCRA scheme while the sensor is under attack (by signal a(t)), a malicious signal is
detected during the period when the actuator is disabled.
(b)
(c)
istic signal denoted by A (t). We embed in this signal a physical
challenge through pseudo-random binary modulation of the form:
B(t) = u(t)A (t), u(t) ∈ {0,1}
(1)
where u(t) is the binary modulation term and B(t) is the modulated
output of the actuator. The output of the active sensor is denoted by
y(t) as shown in Figure 4. In the absence of an attacker and from the
passivity of the measured entity, setting u(t) = 0 (and consequently
B(t) = 0) at time tchallenge will cause y(t) to go to zero.
Potential attackers must actively emit a signal a(t) to overpower
or mask y(t) (Goals G2–G3). A naïve attacker might continue to
emit this signal even when B(t) = 0 as shown in Figure 4c. In this
case, the attack can be easily detected, since any nonzero y(t) while
u(t) = 0 can be attributed to the existence of an attacker.
More advanced attackers might attempt to conceal their attacks
when they sense the absence of B(t) as in Goal G1. Due to As-
sumption A3, an attacker could drive a(t) to zero only after a delay
of τattack, where τattack ≥ τphysical limit > 0 is the unavoidable phys-
ical delay inherent in the attacker’s hardware. Therefore, the mech-
anism described above can still detect the presence of an attack
within this unavoidable time delay. Furthermore, an attacker can-
not learn and compensate for this inherent delay preemptively due
to the randomness of the modulation term u(t). Again, any nonzero
y(t) sensed while u(t) = 0 can be attributed to the existence of an
attacker. The simple PyCRA attack detector can be summarized as
follows:
[Step 1] Select a random time, tchallenge
[Step 2] Issue a physical challenge by setting u(tchallenge) = 0
[Step 3] If y(tchallenge) > 0, declare an attack
Note that the previous process needs to happen within small amount
of time (e.g., in the order of milliseconds) such that it does not af-
fect the normal operation of the system.
3.2 The Confusion Phase
Every physical signal is subject to random perturbations known
as noise. A fundamental characteristic of this noise is the signal to
noise ratio (SNR). This SNR determines the ability of any sensor to
distinguish between changes in a signal of interest and the random
noise. As with the physical delay, this SNR is fundamental, and
it is never equal to zero. As a result, if a signal is within the noise
ﬂoor (less than the noise amplitude), it is fundamentally impossible
to detect any change in the physical signal [36].
As with the physical time delay τattack, we use this fundamental
limit in order to enhance PyCRA and introduce additional security.
To do so, we modify the physical challenge by introducing an in-
termediate step—between the active phase (e.g., u(t) = 1) and the
silent phase (e.g., u(t) = 0)—called the confusion phase. In this
phase, the active sensor uses its actuator to generate a signal u(t)
that is small enough to barely exceed the noise level. Next, we wait
in this confusion phase for a random time tcon f usion before entering
the silent time. This process is summarized in Figure 5.
Recall that one of the attacker’s goals is to remain stealthy (Goal
G1). If the attacker is unable to instantaneously detect the changes
in the physical challenge, he or she will reveal themselves. Due
to the existence of noise, no attacker—whether using software or