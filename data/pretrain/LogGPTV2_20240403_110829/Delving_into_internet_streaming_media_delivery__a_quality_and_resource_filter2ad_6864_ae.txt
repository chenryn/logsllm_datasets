Stream thinning works in a similar way as stream switch.
To characterize the quality degradation due to stream thin-
ning, we deﬁne the thinning duration as the time duration
from the “thinning” command to the “un-thinning” com-
mand or the “stop playing” command, which reﬂects the
quality degradation time that a user suﬀers. We also deﬁne
the thinning interval as the interval between two consecutive
stream thinning events, which reﬂects the frequency of such
quality degradations. Figure 15(a) and Figure 15(b) show
the thinning duration and the interval for video sessions
longer than 30 seconds in the home and business user work-
loads, respectively. As shown in Figure 15(a), more than
70% of the thinning durations are shorter than 30 seconds.
Figure 15(b) shows most (70% in the home user workload
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0.1
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 1000
 0
 10
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 10000
 0
 10
All Sessions
Degradation Sessions
Smooth Sessions
 100
 1000
Playback duration (sec)
All Sessions
Degradation Sessions
Smooth Sessions
 100
 1000
Playback duration (sec)
 10000
Fast Cache
Normal TCP
 1
Rebuffering Duration (sec)
 10
 100
(Home user workload only)
(a) Home user workload
(b) Business user workload
Figure 16: Rebuﬀering duration for
Fast Cache and normal TCP stream-
ing sessions with rebuﬀering
Figure 17: Comparison of playback durations for sessions with and
without quality degradations
Table 5: Summary of streaming quality
biz
streaming
quality
smooth playback
rebuﬀering only
stream switch
stream thinning
video cancellation
biz
duration > 300 sec
duration > 30 sec
home
home
87.06% 59.96% 56.82%
8.65%
18.91% 32.87%
1.40%
16.30%
0.83%
4.73%
3.42%
1.94%
1.52%
1.41%
4.18%
19.73%
31.97%
37.42%
6.80%
4.08%
and 82% in the business user workload) thinning intervals
are longer than 30 seconds.
When bandwidth is too low to transmit the key frame of
video stream, the client may send a TEARDOWN command to
cancel the video stream, then the server sends audio only.
When the bandwidth increases, the client may set up and
request the video stream again.
6.3 Summary of Internet streaming quality
According to our extensive trace analysis and real ex-
periments, Fast Cache does not support rate adaptation in
practice. In a streaming session with Fast Cache enabled,
the client never requests to switch streams after the ini-
tial stream selection in the SETUP command, even if there
is a more suitable stream matching the decreased/increased
bandwidth during playback. Thinning and video cancel-
lation are also disabled when Fast Cache is enabled. As
a result, when the bandwidth drops below the encoding
rate, Fast Cache supported streaming performs like pseudo
streaming [17]: the player stops to buﬀer data for a while,
then continues to play the media for about ﬁve seconds (the
play-out buﬀer size), and this procedure repeats. With such
a conﬁguration, if a sudden network congestion happens and
lasts for a long time, the streaming quality of Fast Cache
supported streaming could be even worse than that of nor-
mal TCP streaming. Figure 16 shows that when rebuﬀering
happens, the rebuﬀering duration of Fast Cache supported
streaming is much longer than that of normal TCP stream-
ing in the home user workload, because it cannot switch to
a lower rate stream upon network congestion.
Figure 17(a) and Figure 17(b) show the CDF of play-
back duration of TCP-based video streaming sessions that
are longer than 30 seconds in the home and business user
workloads, respectively. The three curves in each ﬁgure
denote all sessions, sessions without quality degradations,
and sessions with quality degradations (including rebuﬀer-
ing, stream switch, stream thinning, and video cancellation),
respectively. We can see that for sessions with longer dura-
tions, degradation happens with a higher probability. For
example, in the business user workload, 88% of the sessions
with quality degradations have a duration longer than 100
seconds, while 58% of the sessions without quality degra-
dations have a duration longer than 100 seconds. Table
5 further shows the breakdowns of sessions with and with-
out quality degradations for TCP-based video streaming ses-
sions that are longer than 30 seconds and longer than 300
seconds, in the home and business user workloads, respec-
tively. We can see that quality degradation happens less
frequently in the home user workload than in the business
user workload, which may be due to the longer playback
duration of business users as shown in Figure 17. For ses-
sions longer than 30 seconds, 13%–40% of the video sessions
still have quality degradation due to the rebuﬀering, stream
switch, stream thinning, and video cancellation. For ses-
sions longer than 300 seconds, the quality is getting worse.
Further investigation shows that in a signiﬁcant amount of
video sessions with rebuﬀering, the requested media objects
are MBR encoded, and the lack of stream switch is largely
due to the usage of Fast Cache, which disables rate adapta-
tion.
In conclusion, the quality of media streaming on the Inter-
net leaves much to be improved, especially for those sessions
with longer durations.
7. DISCUSSION: COORDINATING
CACHING AND RATE ADAPTATION
Fast Cache and rate adaptation are two commonly and
practically used techniques that improve the experience of
streaming media users from diﬀerent perspectives. Fast
Cache aggressively buﬀers media data in advance at a rate
higher than the media encoding rate, aiming to absorb the
streaming jitter due to network congestion. In contrast, rate
adaptation conservatively switches to a lower bit rate stream
upon network congestion. As shown in our analysis, both
techniques have their merits and limits. Fast Cache has its
problems such as increasing server load and producing extra
traﬃc. On the other hand, the latency of stream switch is
non-trivial in most sessions, due to the small size of play-out
buﬀer.
Combining the merits of both techniques, in this section,
we discuss Coordinated Streaming, a mechanism that coor-
dinates caching and rate adaptation.
In this scheme, an
 1
 0.95
F
D
C
 0.9
 0.85
 0.8
 0
Fast Cache
Normal TCP
Coordinated Streaming
 0.2
 0.4
 0.6
 0.8
 1
Rebuffer Ratio (rebuffer time / play time)
(a) Rebuﬀering ratio
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0
 1
F
D
C
 0.8
 0.6
Fast Cache
Normal TCP
Coordinated Streaming
 1
 2
 3
 4
 5
Over-transferred bytes/total played bytes
(b) Over-supplied data
 0.4
 0.1
Normal TCP
Coordinated Streaming
 1
 10
 100
Stream Switch Handoff Latency (sec)
(c) Switch handoﬀ latency
Figure 18: Evaluation of Coordinated Streaming
upper bound and a lower bound are applied to the play-out
buﬀer of the client player. The upper bound setting prevents
aggressive data buﬀering while the lower bound setting elim-
inates the stream switch latency. When a streaming session
starts, the server transmits data to the client as fast as possi-
ble until the lower bound is reached. Then the playback be-
gins and the client continues to buﬀer data with the highest
possible rate until the buﬀer reaches its upper bound. With
a full buﬀer, the client buﬀers data at the media encoding
rate, and the buﬀer is kept full. When network congestion
occurs, the client may receive data at a rate lower than the
object encoding rate, and the buﬀer is drained oﬀ. If the
network bandwidth increases before the buﬀer drops below
its lower bound, the client will request data at a higher rate
to ﬁll the buﬀer. Otherwise, the client will switch to a lower
rate stream. The selection of the lower rate stream should be
based on the following:
in a typical bandwidth ﬂuctuation
period, the current bandwidth should be able to maintain
normal playback of this lower rate stream and transmit ex-
tra data to ﬁll the buﬀer to its upper bound. When the
network bandwidth is increased, the client may switch to a
higher encoding rate stream.
We conducted an ideal experiment as a proof of concept of
this scheme. We set the lower bound of the buﬀer size as 5
seconds to cover the normal stream switch latency as well as
the initial buﬀering duration, as the default play-out buﬀer
size is 5 seconds, and the average stream switch latency is
also about 5 seconds. The upper bound of the buﬀer size is
set to 30 seconds, considering the typical network ﬂuctua-
tion periods that may aﬀect streaming quality, such as low
quality duration, thinning duration, and thinning interval
(Figures 14(b), 15(a), and 15(b)). In a practical system, the
lower and upper bound of buﬀer size should be adaptively
tunable based on these quality degradation events. However,
we will show that even with the above simple conﬁguration,
the streaming quality can be eﬀectively improved and the
over-supplied traﬃc can be signiﬁcantly reduced.
We simulated the Coordinated Streaming scheme based
on the packet level information of Fast Cache supported
streaming sessions, and compared the quality and band-
width usage of this scheme with that of Fast Cache sup-
ported streaming and normal TCP-based streaming. To
have a fair comparison, we only consider video sessions that
request objects with 200–400 Kbps encoding rates for a du-
ration longer than 30 seconds in the home user workload.
Figure 18(a) shows the rebuﬀering ratio in Fast Cache sup-
ported streaming, normal TCP streaming, and Coordinated
Streaming. As shown in this ﬁgure, the rebuﬀering ratio of
Coordinated Streaming is close to zero. The fraction of nor-
mal TCP streaming sessions with large rebuﬀering ratios is
close to or even smaller than that of Fast Cache supported
streaming sessions because many of them use rate adap-
tation to avoid rebuﬀering. Figure 18(b) shows the over-
transfered data in the above three schemes, and we can see
that Coordinated Streaming reduces 77% over-supplied traf-
ﬁc produced by Fast Cache, although not as good as normal
TCP streaming. Figure 18(c) shows that the switch hand-
oﬀ latency of Coordinated Streaming is nearly zero, much
less than that of normal TCP streaming. Furthermore, the
number of stream switches in our scheme is only 33.4% of
that in normal TCP-based streaming.
8. RELATED WORK
Existing measurement studies have analyzed the Internet
streaming traﬃc in diﬀerent environments and from diﬀer-
ent perspectives. Li et al. [20] characterized the streaming
media stored on the Web, while Mena et al. [21] and Wang
et al. [29] presented an empirical study of Real audio and
Real video traﬃc on the Internet, respectively. These stud-
ies characterized the packet size, data rate, and frame rate
patterns of streaming media objects. Almeida et al. [10] and
Chesire et al. [13] studied the client session duration, object
popularities and sharing patterns based on the workload col-
lected from an educational media server and an university
campus network, respectively. Cherkasova et al. [12] char-
acterized the locality, evolution, and life span of accesses
in enterprise media workloads. Yu et al. [31] studied the
user behavior of large scale video-on-demand systems. Pad-
hye et al. [23] and Costa et al. [15] characterized the client
interactivity in educational and entertainment media sites,
while Guo et al. [18] analyzed the delay of jump accesses
for video playing on the Internet. Live streaming media
workloads have also been studied in recent years. Veloso
et al. [27] characterized a live streaming media workload in
three increasingly granular levels, named clients, sessions,
and transfers. Sripanidkulchai et al. [26] analyzed a live
streaming workload in a large content delivery network.
However, these on-demand and live streaming media mea-
surements mainly concentrated on the characterization of
media content, access pattern, and user activities, etc. So
far, few studies have focused on the mechanism, quality, and
resource utilization of streaming media delivery on the In-
ternet. Chung et al. [14] and Nichols et al. [22] conducted
an experimental study in a lab environment on the respon-
siveness of RealNetworks and Windows streaming media, re-
spectively. Wang et al. [28] proposed a model to study the
TCP-based streaming. In contrast to these studies, we ana-
lyzed the delivery quality and resource utilization of stream-
ing techniques based on a large scale Internet streaming me-
dia workload.
9. CONCLUSION
In this study, we have collected a 12-day streaming me-
dia workload from a large ISP, including both live and on-
demand streaming for both audio and video media. We
have characterized the streaming traﬃc requested by dif-
ferent user communities (home users and business users),
served by diﬀerent hosting services (third-party hosting and
self-hosting). We have further analyzed several commonly
used techniques in modern streaming media services,
in-
cluding protocol rollover, Fast Streaming, MBR, and rate
adaptation. Our analysis shows that with these techniques,
current streaming services tend to over-utilize the CPU and
bandwidth resources to provide better services to end users,
which may not be a desirable and eﬀective way to im-
prove the quality of streaming media delivery. A coordina-
tion mechanism that combines the advantages of both Fast
Streaming and rate adaptation techniques is proposed to ef-
fectively utilize the server and Internet resources for building
a high quality streaming service. Our trace-driven simula-
tion study demonstrates its eﬀectiveness.
Acknowledgments
We thank the appreciations and constructive comments
from the anonymous referees. William Bynum and Matti
Hiltunen made helpful suggestions on an early draft of this
paper. This work is partially supported by the National
Science Foundation under grants CNS-0405909 and CNS-
0509054/0509061.
10. REFERENCES
[1] Buﬀer settings in Windows media player.
http://support.microsoft.com/?scid=kb;en-
us;q257535.
[2] Fast Streaming with Windows Media 9 Series.
http://www.microsoft.com/.
[3] HTTP streaming protocol. http://sdp.ppona.com.
[4] Microsoft Windows media - Intelligent Streaming.
http://www.microsoft.com/.
[5] MMS streaming protocol. http://sdp.ppona.com.
[6] Real data transport (RDT).
http://protocol.helixcommunity.org/.
[7] RealProducer 10 user guide. http://www.real.com/.
[8] Windows media load simulator.
http://www.microsoft.com/.
[9] YouTube - broadcast yourself.
http://www.youtube.com/.
[10] J. M. Almeida, J. Krueger, D. L. Eager, and M. K.
Vernon. Analysis of educational media server
workloads. In Proc. of ACM NOSSDAV, June 2001.
[11] S. Chen, B. Shen, S. Wee, and X. Zhang. Designs of
high quality streaming proxy systems. In Proc. of
IEEE INFOCOM, Mar. 2004.
[12] L. Cherkasova and M. Gupta. Characterizing locality,
evolution, and life span of accesses in enterprise media
server workloads. In Proc. of ACM NOSSDAV, May
2002.
[13] M. Chesire, A. Wolman, G. Voelker, and H. Levy.
Measurement and analysis of a streaming media
workload. In Proc. of USITS, Mar. 2001.
[14] J. Chung, M. Claypool, and Y. Zhu. Measurement of
the congestion responsiveness of RealPlayer streaming
video over UDP. In Proc. of the Packet Video
Workshop, Apr. 2003.
[15] C. Costa, I. Cunha, A. Borges, C. Ramos, M. Rocha,
J. Almeida, and B. Ribeiro-Neto. Analyzing client
interactivity in streaming media. In Proc. of WWW,
May 2004.
[16] C. Cranor, T. Johnson, and O. Spatscheck. Gigascope:
a stream database for network applications. In Proc.
of ACM SIGMOD, June 2003.
[17] L. Guo, S. Chen, Z. Xiao, and X. Zhang. Analysis of
multimedia workloads with implications for Internet
streaming. In Proc. of WWW, May 2005.
[18] L. Guo, S. Chen, Z. Xiao, and X. Zhang. DISC:
Dynamic interleaved segment caching for interactive
streaming. In Proc. of IEEE ICDCS, June 2005.
[19] M. Handley and V. Jacobsen. SDP: Session
description protocol. RFC 2327, Apr. 1998.
[20] M. Li, M. Claypool, R. Kinicki, and J. Nichols.
Characteristics of streaming media stored on the Web.
Nov. 2005.
[21] A. Mena and J. Heidemann. An empirical study of
Real audio traﬃc. In Proc. of IEEE INFOCOM, Mar.
2000.
[22] J. Nichols, M. Claypool, R. Kinicki, and M. Li.
Measurements of the congestion responsiveness of
Windows streaming media. In Proc. of ACM
NOSSDAV, June 2004.
[23] J. Padhye and J. Kurose. An empirical study of client
interactions with a continuous media courseware
server. In Proc. of ACM NOSSDAV, July 1998.
[24] H. Schulzrinne, S. Casner, R. Frederick, and
V. Jacobson. RTP: A transport protocol for real-time
applications. RFC 1889, Jan. 1996.
[25] H. Schulzrinne, A. Rao, and R. Lanphier. Real time
streaming protocol (RTSP). RFC 2326, Apr. 1998.
[26] K. Sripanidkulchai, B. Maggs, and H. Zhang. An
analysis of live streaming workloads on the Internet.
In Proc. of ACM SIGCOMM IMC, Oct. 2004.
[27] E. Veloso, V. Almeida, W. Meira, A. Bestravos, and
S. Jin. A hierarchical characterization of a live
streaming media workload. IEEE/ACM Transactions
on Networking, Sept. 2004.
[28] B. Wang, J. Kurose, P. Shenoy, and D. Towsley.
Multimedia streaming via TCP: An analytic
performance study. In Proc. of ACM Multimedia, Oct.
2004.
[29] Y. Wang, M. Claypool, and Z. Zuo. An empirical
study of RealVideo performance across the Internet.
In Proc. of the ACM SIGCOMM IMW, Nov. 2001.
[30] K. Wu, P. S. Yu, and J. Wolf. Segment-based proxy
caching of multimedia streams. In Proc. of WWW,
May 2001.
[31] H. Yu, D. Zheng, B. Y. Zhao, and W. Zheng.
Understanding user behavior in large scale video-on
-demand systems. In Proc. of EuroSys, Apr. 2006.