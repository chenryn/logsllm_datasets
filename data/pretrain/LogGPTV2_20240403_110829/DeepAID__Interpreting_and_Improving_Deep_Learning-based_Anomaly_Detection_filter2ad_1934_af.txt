0.8196
0.8204
0.8822
0.9001
f1-macro*
0.7848
0.8101
0.7780
0.8441
0.8728
Table 5: Reducing FPs by Distiller.
# classes = 10
FPR
TPR
0.7504
0.9990
0.9998
0.3319
0.0845
0.9717
0.9999
0.0002
0.0046
1.0
0.0220
1.0
# classes = 5
FPR
TPR
0.9719
0.9989
1.0
0.0026
0.0816
0.9755
0.0
1.0
1.0
0.0
1.0
0.0
Kitsune+
DeepAID
K=5
K=10
K=15
Method
RF
MLP
Kitsune
UACC
N/A(0.)
N/A(0.)
0.5217
0.9357
0.9801
f1-micro and f1-macro are evaluated under training data, and f1-micro* and f1-macro* under test data;
Kitsune (w/o. DeepAID): UACC=0.28 (not evaluated for f1-micro(*) and f1-macro(*) since it cannot classify multi-class data).
(Higher is better for all five metrics.)
Figure 9: Reducing FPs by retraining.
(i.e., C in Figure 2). Here explicit means human-factor errors that
are easy to repair. (In contrast to the implicit errors caused by
deviant model learning, which will be discussed in ¬ß6.6).
In Kitsune, we find some false positives (FP) with extremely
high reconstruction error (RMSE). From interpretations (a represen-
tative one is listed in Table 3(c)), we find that these FPs are caused by
morbidly large features related to the covariance (cov.) or Pearson
correlation coefficient (pcc.). After analyzing the original traffic, we
find the relevant features should not be so large. Therefore, we infer
that there may be bugs or bias in the implementation of feature
extraction algorithm in Kitsune. By examining the open-source
implementation of Kitsune [35], we note that they claimed that
they used approximation algorithms to compute related features
instead of counting real values. Therefore, we modified the imple-
mentation of extracting cov./pcc. related features to calculate their
real value. As shown in Figure 8 (with Mirai botnet traffic), by fixing
this error, we reduce the FPs (94.92%‚Üì) in Kitsune without much
affecting the detection of anomalies (only 0.17%‚Üì).
lar threats that do not appear in existing rules?
6.5 Human-in-the-Loop Reliable Detection
Next we evaluate the performance of Distiller as an extension to In-
terpreter to perform human-in-the-loop detection (¬ß5). Specifically,
we design experiments to answer the following questions:
‚Ä¢ RQ1: Can Distiller accurately match the existing rules?
‚Ä¢ RQ2: Whether Distiller has generalization ability to detect simi-
‚Ä¢ RQ3: Can Distiller preserve the ability to detect unknown threats?
Experimental Setting. To evaluate Distiller, expert feedback to
anomaly interpretations is required as rules for updating Distiller.
However, human feedback is experience-dependent and very sub-
jective. For fairness and simplicity, we select two well-labeled multi-
class datasets, Kitsune dataset [35] and CIC-IDS2017 [43]. As men-
tioned in ¬ß5, we use two ML/DL classifiers as the baseline: Random
Forest (RF) and Multi-Layer Perceptron (MLP). We also evaluate
the impact of ùêæ for Distiller with ùêæ = 5, 10, 15. We respectively
use 5 and 10 different classes/feedback states of attack traffic and
200 typical data(/rules) in each class for training/updating (thus
totally 1000 and 2000). Note that, we update Distiller with the same
training set for RQ1/RQ2/RQ3 while test on training set (RQ1)
and test set (RQ2). Particularly for RQ3, we test Distiller on an-
other class of anomalies not in the training set. As for metrics, we
evaluate f1-micro and f1-macro (f1-score in multi-classification) for
RQ1/RQ2, and accuracy rate of unknown attack detection (UACC)
for RQ3. Details about the datasets/metrics are in Appendix C.
Results and Conclusions. Results are shown in Table 5. From
f1-micro/f1-macro, we find DeepAID can match existing rules very
accurately (RQ1). From f1-micro*/f1-macro*, we find DeepAID sig-
nificantly outperforms baselines on generalization ability (RQ2).
Especially for ùêæ=15, f1-micro*/f1-macro* of DeepAID exceeds RF
and MLP by 10% and 20%. We find MLP is even worse than RF, which
may be because the training set is relatively small, while Distiller
performs very well with fewer rules. The choice of ùêæ can be viewed
as a trade-off. That is, a larger ùêæ ensures that the rules are more
detailed (hence more accurate) but will lose the conciseness. From
the results of UACC, we find that DeepAID not only retains the
ability to detect unknown threats, but also significantly improves
the original 28% to >98% (for ùêæ=15). This is because DL models
failed to report unknown threats that are similar to normal data,
while Distiller has explicit rules to match known anomalies, thus
can judge unknown threats more accurately (RQ3). This also shows
that DeepAID can also reduce false negatives (FN) incidentally. This
experiment confirms DeepAID can help security systems to handle
thousands of rules with superior performance on detecting stored
(RQ1), similar (RQ2), and unknown (RQ3) anomalies.
6.6 Reducing FPs
We evaluate two methods mentioned in ¬ß5 to reduce FPs in security
systems with Distiller (also D and E in Figure 2).
Modifying Distiller. We select only 10 rules from 10 FPs to a
new feedback state representing normal data, and use the same
baselines and datasets in ¬ß6.5. Here we consider all abnormal classes
as one class and use two well-known metrics for evaluation: TPR
and FPR (True/False Positive Rate). The results are shown in Table
5. DeepAID demonstrates extremely superior performance with
nearly no FP and 100% TPR. It significantly reduces FPR of Kitsune
(from 8% to 0%) while improving its TPR. The performance of
baselines are poor with respect to FPR. Here we find a small number
of FPs appears when ùêæ=15. This is because higher ùêæ is more likely
to induce more ‚Äúconflicts‚Äù, which means that one state in the first
FSM transits to multiple feedback states. We provide the detailed
theoretical analysis of conflicts in Appendix B.
Retraining the DL model. Another method to reduce FPs men-
tioned in ¬ß5 is to retrain DL models with concept drift FPs found by
Distiller. Here we use a simple baseline by randomly selecting FPs
for retraining, in order to evaluate whether DeepAID can find the
more optimal retraining set. As shown in Figure 9, we conclude that
DeepAID can help to save the cost of retraining and achieve high
performance (e.g., with DeepAID, retraining 30% samples reduces
more FPs than 60% in the random method).
10%20%30%40%50%60%Retraining Ratio (%)0255075100Œî FP (‚Üì)RandomDeepAIDSession 12A: Applications and Privacy of ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea32087 DISCUSSIONS
Below, we discuss design choices, limitations, and future works.
Why only Interpret Anomalies? In practical security applica-
tions, operators are more concerned about anomalies. Actually,
they cannot even handle all anomalies, let alone the normal data.
Why White-box Interpretation? Besides the taxonomy in ¬ß2.1,
local DL interpretations can also be separated as white-box and
black-box. White-box methods require interpreters to have full ac-
cess to the DL model, while black-box methods treat the DL model
as a black box without detailed knowledge. DeepAID and DeepLIFT
are white-box for using model gradients, while other baseline in-
terpreters are black-box. Indeed, black-box methods are more con-
venient for operators. However, as justified in related works and
our experiments (¬ß6.2), they are difficult to use in practice due to
the relatively poor performance. On the other hand, the white-box
condition makes sense when the one seeking interpretations is
the model owner. [20] thinks white-box condition is unreasonable
when using pre-trained models, but this is not the case for unsu-
pervised DL models since they do not need pre-training. In fact,
DeepAID only requires the back-propagated gradient information
with no need for fully understanding or modifying the original
model, which avoids burdening the operators.
Limitations and Future Work. First, the adversarial robustness
evaluation and claim of DeepAID are mainly against optimization-
based and distance-based attacks (in ¬ß6.2 and Appendix D). There
are other target attacks that may fail DeepAID, such as poisoning the
original models (as DeepAID is highly faithful to model decisions),
hijacking the search process to generate false references, and craft-
ing anomalies extremely or equally far from the decision boundary
in many dimensions to force interpretations to use more features
(spoiling the conciseness). Future work can investigate the robust-
ness against more attacks. Second, hyper-parameters of DeepAID
are configured empirically. We have evaluated the sensitivity of
hyper-parameters in Appendix F, and provided brief guidelines
about how to configure them in the absence of anomalies. Future
work can develop more systematic strategies to configure hyper-
parameters. Third, we implement Distiller only for tabular data.
Future work will focus on extending Distiller to other data types.
Fourth, the practical effectiveness of DeepAID may depend on the
expert knowledge of operators, since they do not have prior knowl-
edge of anomalies in the unsupervised setting. Future work can
investigate the impact of knowledge level and look into approaches
to relax the requirement of expert knowledge for operators.
8 RELATED WORK
Most related works have been introduced in ¬ß2. Below, we briefly
discuss other related works from two aspects.
Interpreting Unsupervised Learning & Anomaly Detection.
Recently, there are a few studies discussing interpretation methods
on unsupervised learning or anomaly detection. First, some studies
develop global interpretation through model distillation [49] and
rule extraction [5] as well as interpretation for clustering models
[25], which are beyond the scope of this study. As for local interpre-
tation of anomaly detection, in addition to COIN [31] and CADE [58]
which are used as baselines, most of other works simply modify the
existing supervised methods to unsupervised learning. For example,
SHAP interpretation [32] is adapted to (single) Autoencoder in [2],
which cannot be applied to Kitsune (ensemble Autoencoders) or
other DL models. Deep Taylor Decomposition [36] is leveraged
for interpreting one-class SVMs in [26], which cannot be applied
to DNNs. In [7], a dedicated interpretation for Isolation Forest is
developed which also cannot be applied to DNNs. An interpretation
method for convolutional models and image analysis is proposed
in [29] by visualizing abnormal region, which is unadaptable for
other DL models in security domains. There are also works convert-
ing anomaly detection to supervised tasks and leverage existing
supervised interpretations [1, 38]. Other works [4, 24] for interpret-
ing anomaly in big-data streaming systems use the knowledge of
many other anomalies (time-series intervals), which is hard to be
generalized into the unsupervised setting in security domains.
Improving DL-based security systems. Recently, several works
have been proposed for improving the practicality of (unsupervised)
DL-based security systems, such as improving the robustness of DL-
based systems [17, 21], performing lifelong learning [11], detecting
concept drift [58], learning from low-quality labeled data [30]. They
are orthogonal to our work and could be adopted together for
developing more practical security systems.
9 CONCLUSIONS
In this paper, we propose DeepAID, a general framework to interpret
and improve DL-based anomaly detection in security applications.
We design DeepAID Interpreter by optimizing well-designed ob-
jective functions with special constraints for security domains to
provide high-fidelity, human-readable, stable, robust, and efficient
interpretations. Several applications based on our interpretations
and the model-based extension Distiller are proposed to improve
the practicality of security systems, including understanding model
decisions, discovering new knowledge, diagnosing mistakes, and
reducing false positives. By applying and evaluating DeepAID over
three types of security-related anomaly detection systems, we show
that DeepAID can provide high-quality interpretations for DL-based
anomaly detection in security applications.
ACKNOWLEDGMENTS
We are grateful to the anonymous reviewers for their insightful
comments. We also thank for the suggestions from Shize Zhang, Bin
Xiong, Kai Wang, as well as all other members from NMGroup and
CNPT-Lab in Tsinghua University. This work was supported in part
by the National Key Research and Development Program of China
under Grant 2018YFB1800200. Zhiliang Wang is the corresponding
author of this paper.
REFERENCES
[1] Kasun Amarasinghe, Kevin Kenney, and Milos Manic. 2018. Toward explain-
able deep neural network based anomaly detection. In 2018 11th International
Conference on Human System Interaction (HSI). IEEE, 311‚Äì317.
[2] Liat Antwarg, Bracha Shapira, and Lior Rokach. 2019. Explaining anomalies
detected by autoencoders using SHAP. arXiv preprint arXiv:1903.02407 (2019).
[3] Sebastian Bach, Alexander Binder, Gr√©goire Montavon, Frederick Klauschen,
Klaus-Robert M√ºller, and Wojciech Samek. 2015. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PloS one 10,
7 (2015), e0130140.
[4] Peter Bailis, Edward Gan, Samuel Madden, Deepak Narayanan, Kexin Rong, and
Sahaana Suri. 2017. MacroBase: Prioritizing Attention in Fast Data. In SIGMOD
Conference. ACM, 541‚Äì556.
Session 12A: Applications and Privacy of ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3209[5] Alberto Barbado and √ìscar Corcho. 2019. Rule Extraction in Unsupervised
Anomaly Detection for Model Explainability: Application to OneClass SVM.
CoRR abs/1911.09315 (2019).
[6] Benjamin Bowman, Craig Laprade, Yuede Ji, and H Howie Huang. 2020. Detecting
Lateral Movement in Enterprise Computer Networks with Unsupervised Graph
AI. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses