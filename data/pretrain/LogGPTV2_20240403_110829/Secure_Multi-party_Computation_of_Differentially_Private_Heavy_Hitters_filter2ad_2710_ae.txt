We evaluated the running time and communication of the entire
protocol, i.e., offline as well as online phase, in a real-world WAN
for 𝑚 = 3 parties. We split the parties into two AWS regions, Ohio
(us-east-2) and Frankfurt (eu-central-1), and measured an inter-
region round time trip (RTT) of approx. 100 ms with 100 Mbits/s
bandwidth. The computation parties already received and com-
bined secret-shared inputs from the input users (Section 4.5). We
present the average of 10 runs for running time and communica-
tion (except MP-SPDZ for HHthreads with 3 runs) and 20 runs for
accuracy with 95% confidence intervals, but omit the intervals in
most cases, as the results are very stable. We used modest hardware,
t2.medium AWS instances (2 GB RAM, 4vCPUs) [6], to show that
the computational overhead of modern MPC is acceptable. (More
powerful hardware did not provide significant improvements.) Re-
call, HHthreads is a parallelized version of HH (Section 4.1), which
required c4.2xlarge instances (15 GB RAM, 8vCPUs) to leverage 8
threads. Also, t2.large (8 GB RAM, 4vCPUs) instances were used for
PEM in two settings – MP-SPDZ with 𝜂 = 5, 𝑘 = 16, and SCALE-
MAMBA with 𝜂 = 4, 𝑘 = 16 – as more memory was required for
these larger programs. To evaluate running time and communica-
tion of HH, we set map size 𝑡 = 𝑘, and fix it to 16 in our accuracy
evaluation (Section 5). We stress that we evaluated a worst-case sce-
nario for PEM: Each round assumes that the maximum of 𝑘 prefix
candidates are output after thresholding. Fewer outputs decrease
computation and communication due to smaller candidate sets for
the next round. We securely sort the candidates. However, if one is
not interested in the order, i.e., which value is the 𝑖-th most frequent,
the sorting step can be replaced by linear scan (to find the minimum
4See Ács et al. [2, Section 8.3] (technical report version) for a detailed analysis of the
required noise increase.
5Uniform random numbers can be generated in a distributed manner even in a ma-
licious setting, e.g., by XORing random inputs from each party (which is random as
long as a single party provides actual randomness) [51, Supplementary Material], or
by using the randomness generated in the offline phase [4].
(a) MP-SPDZ: HH
(b) MP-SPDZ: HHthreads
(c) SCALE-MAMBA: HH
(d) SCALE-MAMBA:
HHthreads
Figure 5: Running time of HH, HHthreads.
(a) MP-SPDZ: PEM, 𝑏 = 32
(b) MP-SPDZ: PEM,𝑏 =64
(c) SCALE-MAMBA: PEM, 𝑏 = 32
Figure 6: Running time of PEM.
count for the threshold). Next, we evaluate accuracy, running time
and communication of our protocols in a real-world WAN. Evalua-
tion of AWS cost is given in Appendix G, further comparisons of
SCALE-MAMBA with MP-SPDZ are detailed in Appendix K.
Running Time: Figures 5, 6 show the running times for HH,
PEM implemented with MP-SPDZ as well as SCALE-MAMBA with
data sizes 𝑛𝑝 ∈ {10, 30, 100} per computation party 𝑝 (i.e., |𝐷| ∈
{30, 90, 300}). To show the difference between HH and HHthreads,
we used the same scale for MP-SPDZ (Figures 5a, 5b) and SCALE-
MAMBA (Figures 5c, 5d). For MP-SPDZ, the running time with
8 threads increases, whereas it decreases with SCALE-MAMBA.
Overall, for HH, and especially HHthreads, SCALE-MAMBA is faster
than MP-SPDZ, requiring at most 11 minutes for HHthreads, and
less than 16 for HH. The opposite is the case for PEM: MP-SPDZ is
much faster, taking less than 6 minutes for 𝜂 = 5, whereas SCALE-
MAMBA requires almost half an hour for 𝜂 = 4. Note that we used
smaller values of 𝜂 for SCALE-MAMBA (i.e., 𝜂 ∈ {2, 3, 4}) since the
differences to MP-SPDZ are already sufficiently pronounced here.
48162505007501,0001,250kSecondsnp=100np=30np=1048162505007501,0001,250kSeconds4816200400600800kSecondsnp=100np=30np=104816200400600800kSeconds4816150175200225250275300kSecondsη=5η=4η=34816300400500600700kSeconds48162505007501,0001,2501,5001,750kSecondsη=4η=3η=2Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2369Client Communication: For HH, a client (input party) sends
her secret-shared value to each of the 𝑚 servers (computation par-
ties). In total, a client sends 𝑚 · 128 bits (our evaluated share size is
128 bits). For PEM, a client sends 2⌈log 𝑘⌉+𝜂 secret-shared counts,
i.e., at most 𝑚 · 8 𝐾𝐵 (our largest evaluation with 𝜂 = 5, 𝑘 = 16).
Server Communication: As is to be expected, semi-honest
MP-SPDZ sends less than maliciously secure SCALE-MAMBA. We
briefly evaluated MP-SPDZ with malicious security (Appendix K),
and found it to be still more communication-efficient, albeit slower,
than SCALE-MAMBA. Next, we report the average communication
of HH and PEM per party for 𝑘 = 16. Further evaluations are
provided in Appendix H. For HH communication increases linearly
with data size. We consider data size 𝑛𝑝 per computation party 𝑝 ∈
{1, 2, 3}, and MP-SPDZ requires ≈13/38/122 MB for 𝑛𝑝 10/30/100.
While SCALE-MAMBA provides better running times than MP-
SPDZ for HHthreads, MP-SPDZ requires much less communication,
e.g., roughly 45 times less for HHthreads with 𝑘 = 16, 𝑛𝑝 = 100
(125 MB vs 5.6 GB), suggesting superior communication batching
and parallelization from SCALE-MAMBA compared to MP-SPDZ.
For PEM and 𝑏 = 32, MP-SPDZ sends ≈130/258 MB and SCALE-
MAMBA sends ≈989/1884 MB for 𝜂 3/4. Doubling the domain bit-
length to 64 also roughly doubles the communication. Note that
PEM, unlike HH, is independent of the data size, as we now consider
aggregated candidate counts and not single values.
Comparing different DP notions: We use the same value for
𝜖 to compare our approach to state-of-the-art PEMorig for heavy
hitter detection in the local model. Our protocols, however, operate
in the central model realized with MPC and approximate differ-
ential privacy (𝛿 > 0), whereas PEMorig is a local model protocol
with pure differential privacy (𝛿 = 0). The main benefit of approxi-
mate DP is improved composition [37, Section 3.5], i.e., running 𝑔
mechanisms on the same data requires a smaller privacy budget of
≈ √𝑔𝜖 instead of 𝑔𝜖 for large enough 𝑔. However, we run PEM once
per disjoint subsets of the data and not multiple times on the same
data. Thus, we gain no significant advantage over PEMorig from
using approximate DP. Furthermore, for an advantage to become
noticeable one requires 𝑔 >
2 log(1/𝛿)
(2−exp(𝜖)2 (Appendix D, [64]).
Accuracy: Next, we evaluate accuracy via NCR score (Defini-
tion 2), and provide a comparision to F1 in Appendix J. For the
accuracy evaluation, we set Δ = 1, 𝛿 = 10−7, assume domain bit-
length 𝑏 = 32, and report the average of 20 runs with 95% confidence
intervals. Like Wang et al. [78], we use a synthetic data set sam-
pled from the Zipf distribution with parameter 1.5, i.e., the 𝑗-th
most frequent value appears with probability proportional to 1/𝑗1.5.
We also used prices from an Online retail data set [75]. Note that
we use small data sizes (few thousand values) on purpose, as it is
the most challenging regime for DP, i.e., the ratio of “signal” (ac-
tual counts) to noise is small. We compare PEM and PEMorig for
different values of 𝜂 ∈ {4, 5}, where 𝜂 is given in brackets (e.g.,
“PEM(𝜂 = 4)”), as well as with PEMorig with query limit count of
220 (denoted as “PEMorig”), where 𝜂 is set to the largest integer
satisfying 𝑔2𝛾+𝜂  4. Altogether,
the empirical evaluation confirms our analysis in Section 3.3: HH
provides better accuracy for small data sizes with modest values
for 𝑡. Also, PEM provides better accuracy than PEMorig for larger
data sizes 𝑛 ∈ 105, 2 · 105, 5 · 105 as we detail in Appendix I.
6 RELATED WORK
Non-private top-𝑘: Algorithms for heavy hitter detection are
roughly grouped into three classes [7, 27]: Quantile algorithms,
which uses estimated quantiles of range endpoints to approximate
3001,0003,0005,00000.20.40.60.81nNCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig3001,0003,0005,00000.20.40.60.81nNCR481600.20.40.60.81kNCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig481600.20.40.60.81kNCRSession 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2370(a) 𝑘 = 4
(b) 𝑘 = 8
(c) 𝑘 = 16
Figure 9: NCR of PEM variants and HH for Zipf with fixed 𝑛 = 1000, and varying 𝑘 ∈ {4, 8, 16}, 𝜖 ∈ {0.1, 0.25, 0.5, 1, 2}.
(a) 𝑘 = 4
(b) 𝑘 = 8
(c) 𝑘 = 16
Figure 10: NCR of PEM variants and HH for Zipf with fixed 𝑛 = 5000, and varying 𝑘 ∈ {4, 8, 16}, 𝜖 ∈ {0.1, 0.25, 0.5, 1, 2}.
(a) 𝑘 = 4
(b) 𝑘 = 8
(c) 𝑘 = 16
Figure 11: NCR of PEM variants and HH for retail data [75] with fixed 𝑛 = 1000, varying 𝑘 ∈ {4, 8, 16}, 𝜖 ∈ {0.1, 0.25, 0.5, 1, 2}.
(a) 𝑘 = 4
(b) 𝑘 = 8
(c) 𝑘 = 16
Figure 12: NCR of PEM variants and HH for retail data [75] with fixed 𝑛 = 5000, varying 𝑘 ∈ {4, 8, 16}, 𝜖 ∈ {0.1, 0.25, 0.5, 1, 2}.
frequencies of range elements; hash-based sketches, which provide
a space-efficient frequency estimation, and counter-based sketches,
where a set of counters are updated when new data arrives. The
latter are the best with regards to space, speed and accuracy [7, 27],
thus, we selected [27, Alg. 1] as basis for HH. HH provides differ-
entially privacy unlike related work [7, 27]. While recent improve-
ments achieve better performance [7] (amortized over the control
flow), we cannot leverage them in HH due to our use of MPC (which
hides the control flow).
Local DP top-𝑘: LDP heavy hitter approaches [11, 12, 40, 42, 78,
81] mainly differ in how they encode and reconstruct the candi-
dates, for which counts are estimated. Such encoding (in the form
of domain reduction, e.g., Bloom filters [40], matrix projection [12])
incurs information loss, which can exceed the loss due to DP ran-
domization [78]. Notably, some encodings already provide some
form of DP, e.g., [81] (or [26] for distinct counts), but only with
large 𝜖 or for large data sizes. Wang et al. [78] carefully analyze
related work [11, 12, 40, 42], which mainly utilize non-overlapping
segments (e.g., report single bits or sets of bits), present a state-of-
the-art protocol by leveraging overlapping prefixes, and show that
0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorig0.10.250.51200.20.40.60.81NCRHHPEM(η=5)PEM(η=4)PEMorig(η=5)PEMorig(η=4)PEMorigSession 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2371it provides better accuracy than other approaches. We use [78] as
basis for our central model protocol PEM, which does not suffer
from information loss due to encoding and local randomization, and
allows (central model) 𝑂(1) count accuracy instead of (local model)
Ω(√𝑛) for 𝑛 users [23]. Also, we directly output heavy hitters (as
our sketches contain their values or bit representation); unlike re-
lated work, where costly reconstructions are required to find heavy
hitters given an encoded representation (e.g, hash), which has to
be mapped to potential candidates from the domain [40, 42, 78].
Central DP top-𝑘: An alternative to approximate DP with thresh-
olding is probabilistic selection with pure DP, i.e., via exponential
mechanism [60] or report noisy max [37]. These alternatives can
be applied in a peeling fashion to find the most frequent value from
a known domain, remove it from the domain, and repeat until 𝑘
values are found. More computationally efficient one-shot meth-
ods [38, 70] release 𝑘 values in one go. We choose thresholding
as it is preferable, especially for small data, for two reasons: First,
selection requires considering all elements from a known domain
and sampling an output from the entire domain with probability
proportional to an element’s utility. With thresholding, on the other
hand, focusing on data elements (from an unknown domain) suf-
fices – leveraged by our protocol HH. Second, for large domains
(e.g., of size 232) and small data (e.g., few hundred elements) the
probability mass of elements with count zero (i.e., not in the data
but in the domain) can exceed the selection probability of even
the most frequent element, which destroys accuracy (especially
using disjoint groups that split the counts among them). Durfee
and Rogers [33] first compute the actual top-𝑘′, where 𝑘′ > 𝑘, and
use (𝜖, 0)-DP noise and 𝛿-based thresholding to release (𝜖, 𝛿)-DP
top-𝑘. All central DP approaches assume access to the raw data or a
trusted third party. We, on the other hand, securely discover top-𝑘′
without such assumptions, and apply thresholding [33] to release
DP top-𝑘 in PEM. Vadhan [76, Theorem 3.5] presents a stability-
based algorithm for central DP histograms and HH can be seen as
a space-efficient version in the distributed setting.
MPC DP top-𝑘: Melis et al. [61] combine count-min and count
sketches as follows: parties evaluate multiple hash functions on
their input, set the counters indexed by the hash functions to 1, and
securely aggregate the counters. They mention heavy hitters as
an application but do not evaluate. However, reconstructing heavy
hitters from such sketches is linear in the domain size (as each
candidate is mapped to sketch entries by evaluating multiple hash
functions), whereas our protocols are linear in the data size (HH) or
linear in the domain bit-length (PEM) and efficiently handle large
and even unknown domains. Additional data structures, e.g., using
multiple sketches [20], reduce the reconstruction complexity at the
cost of increasing communication and aggregation overhead.
Boneh et al. [20] securely compute heavy hitters in a malicious
setting with two computation servers. They focus on novel cryp-
tographic primitives, i.e., incremental distributed point functions,
allowing secret shares of size 𝑂(𝑚) to represent a vector of 2𝑚
values with only one non-zero element. They consider DP only
optionally to bound their protocol’s information leakage. In con-
trast, DP with high accuracy is at the heart of our design, whereas
they require large noise addition from each server, prohibiting any
meaningful DP statistics for small number of clients and overall
provide less accuracy than our DP-focused protocols. They require
millions of clients to achieve an absolute error of 16% for 𝜖 < 1 [20,