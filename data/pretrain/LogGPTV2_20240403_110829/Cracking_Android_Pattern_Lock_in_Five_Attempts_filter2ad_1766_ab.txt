our participants drew patterns. During video recording, our
participants ﬁrstly performed some on-screen activities such
as web browsing and gaming for a period of time as they
wished; they then opened up a pattern lock screen to draw a
pattern and continued to perform other on-screen operations
afterwards. For each video stream, we then analyzed frames
that are associated with pattern drawing and those are not.
Figure 3 shows that all our participants paused at least
1.5 seconds before or after pattern drawing due to delay of
the user or the device. We also found that identical on-screen
activities often follow closely. For example, on several occa-
sions our participants had to swipe several times to locate a
program from the application list. These consecutive on-screen
operations have some spatial-temporal motion characteristics
that are different from pattern drawing. Figure 4 shows the
spatial-temporal motion structure for two gestures, swiping and
zooming, when they are performed once (a, c, e) and twice (b,
d, f). This diagram suggests that the spatial-temporal motion of
two identical on-screen activities contains one or more looping
structures for which pattern drawing does not have.
Our heuristic for identifying the pattern drawing process
is described in Algorithm 1. The input to the algorithm is
a video capturing the unlocking process, and the output of
the algorithm is a time-stamp tuple, , which
marks the start and the end of a video segment. To locate
the video segment of pattern drawing, we ﬁrst ﬁlter out on-
screen activities where the ﬁngertip location does not change
within a timeframe of 1.5 seconds (lines 4 and 11). This
allows us to exclude some basic on-screen activities such as
clicking. We use the number of video frames, frameCount, as
a proxy to estimate the time interval between two on-screen
operations. Here, a time interval of 1.5s translates to 45 frames
or 90 frames when the video was shot at 30 or 60 frames per
second (FPS) respectively. We also use the spatial-temporal
characteristics described above to exclude two consecutive
swiping or zooming gestures (line 8). Finally, we exploit the
observation that users typically pause at least 1.5s before or
after unlocking to locate the start and end points of pattern
drawing (line 19).
Limitations Our heuristic is not perfect. It is likely to fail if
the user was typing using a Swype-like method (i.e. entering
words by sliding a ﬁnger from the ﬁrst letter of a word to its
last letter) during video recording. In this case, our method will
identify multiple video segments of which one may contain the
pattern unlock process. If multiple segments are detected, the
algorithm will ask the user to conﬁrm which video segment
Figure 3.
between pattern drawing and other on-screen activities.
The cumulative distribution function (CDF) of the time interval
(a) a horizontal-swiping
gesture
(b) two consecutively
horizontal-swiping gestures
(c) a vertical-swiping gesture
(d) two consecutively
vertical-swiping gestures
(e) a zooming gesture
(f)two consecutive zooming
gestures
Figure 4. Spatial-temporal characteristics for performing an on-screen gesture
once (a, c, e) and twice (b, d, f).
to use. In this scenario, the ﬁrst identiﬁed segment is likely to
be the correct one. In practice, an experienced attacker would
wait patiently to avoid this complicated situation by ﬁnding the
right time for ﬁlming (e.g. for a screen lock, the time is just
after the device is retrieved). The attacker could also watch
the video to manually cut it to ensure the obtain the correct
video segment. It is worthwhile to mention that automatically
identifying the pattern unlocking process is not central to our
attack because an attacker often can obtain a quality video
input used the manual methods described above. Despite its
limitations, our algorithm can reduce the efforts involved in
some common scenarios.
B. Track ﬁngertip locations
After cutting out the video segment of pattern drawing,
we need to track the ﬁnger motions from the video segment.
We achieve this by employing a video tracking algorithm
called Tracking-Learning-Detection (TLD) [15]. This algo-
rithm automatically detects objects deﬁned by a boundary
box. In our case, the objects to be tracked are the user’s
4
1.51.61.71.800.20.40.60.81The time interval (s)CDF−40−2002040−20−1001020−50−2502550−30−20−1001020−20−1001020−60−3003060−40−2002040−60−3003060−40−2002040−30−1501530−40−200204030150−15−30(a) The ﬁrst video frame
(b) A middle video frame
(c) The last video frame
(d) Fingertip movement trajectory
Figure 5. Tracking the ﬁngertip movement trajectory. For each video frame, the system tracks two areas: one surrounds the ﬁngertip and the other covers the
edge of the device. The ﬁngertip position is determined by computing the relative coordinates of the central points of the two areas. The red points highlighted
in the ﬁnal results (d) are the touching points tracked from the three video frames.
Algorithm 1 Unlocking process identiﬁcation heuristic
Input:
IV : Video footage
f rameCount: Pause threshold before or after unlocking
: Start and end of the unlocking video segment
Output:
1: f rames[] ← getV ideoF rames(IV )
2: LEN ← getF ramesLen(f rames[])
3: for i = 1 : LEN − f rameCount do
4:
←
sL
i + f rameCount])
if !sL then
hasF ingertipChanged(f rames[i
:
5:
6:
7:
8:
9:
10:
11:
sN o = i + f rameCount
for j = sN o : LEN do
if checkLoop(f rames[j : LEN ]) then
eN o = i
break;
else if
f rameCount]) then
!hasF ingertipChanged(f rames[j
:
j +
eN o = i
break;
end if
end for
break;
12:
13:
14:
15:
16:
end if
17:
18: end for
19: ← getT argetV ideo(f rames[], sN o, eN o)
ﬁngertip and an area of the device. These are supplied to the
algorithm by simply highlighting two areas on the ﬁrst frame
of the video segment (see Figure 2 b). The algorithm tries to
localize the ﬁngertip from each video frame and aggregates the
successfully tracked locations to produce a ﬁngertip movement
trajectory as an output (see Figure 2 c).
1) Generate The Fingertip Movement Trajectory: The TLD
algorithm automatically detects objects based on the examples
seen from previous frames. For each tracked object, the algo-
rithm generates a conﬁdence between 0 and 1. A tracking is
considered to be successfully if the conﬁdence is greater than
a threshold. We set this threshold to 0.5 which is found to give
good performance in our initial design experiments using 20
patterns2. TLD has three modules: (1) a tracker that follows
objects across consecutive frames under the assumption that
the frame-to-frame motion is limited and objects are visible;
(2) a detector to fully scan each individual frame to localize
all appearances of the objects; and (3) a learner that estimates
2To provide a fair evaluation, the patterns used in all our initial test runs in
the design phase are different from the ones used later in evaluation.
5
errors of the detector and updates the detector to avoid these
errors in future frames.
The TLD learner automatically extracts features from the
area of interest to build a K-Nearest Neighbor classiﬁer [13]
which is a part of the detector. In the following frames, the
learner estimates the detection errors and generates new train-
ing examples (i.e. new appearances of the object) arose from
object motion to re-train the classiﬁer to avoid these errors.
For each video frame, TLD calculates the tracking conﬁdence
and if the conﬁdence is lower than the predeﬁned threshold,
the result of this particular frame will be discarded. This
allows the algorithm to tolerate a certain degree of detection
errors. Finally, the successfully detected object locations will
be put onto a single image as the output. Detailed discussion
of TLD can be found at [15]. Sometimes the algorithm may
fail to detect the objects in many video frames due to poor
selections of interesting areas. If this happens, our system will
ask the user to re-select the areas of interest. We have also
extended TLD to report when a ﬁngertip position is seen on the
footage. This temporal information is recorded as the number
of video frames seen with respect to the ﬁrst frame of the video
segment. This is used to separate two possibly overlapping line
segments described in Section IV-D.
2) Camera Shake Calibration: By default, the TLD algo-
rithm reports the position of a tracked object with respect to
the top-left pixel of the video frame. However, videos recorded
by a hand-held device is not always perfectly steady due to
camera shake. As a result, the top-left pixel of a video frame
may appear in a different location in later frames. This can
drastically affect the precision of ﬁngertip localization, leading
to misidentiﬁcation of patterns.
Our approach to cancel camera shake is to record the
ﬁngertip location with respect to a ﬁxed point of the target
device. To do so, we track two areas from each video frame.
One area is an edge of the device and the other is the ﬁngertip.
Both areas are highlighted on the ﬁrst frame by the user. The
location of a successfully tracked ﬁngertip is reported as the
the relative coordinates of the two center points of the marked
areas. This approach can also be used to calibrate the minor
motions of the target device during pattern drawing.
Example: To illustrate how our camera-shake calibration
method works, considering Figure 5 where two areas are ﬁrstly
marked by two bounding boxes in subﬁgure (a). Both areas
will then be automatically detected by the TLD algorithm in
following video frames as shown in subﬁgures (b) and (c). The
coordinates of the two center points of each box are the values
x=265.00  y=364.00x=156.00  y=454.00 x=109.00   y= -90.00x=275.62  y=324.86x=156.22  y=456.98 x= -119.40   y=132.12x=310.70  y=278.00x=157.40  y=437.94 x= -153.30   y=159.94-60-3003060-60-3003060(a) w/o camera shake calibration
(b) w/ camera shake calibration
(c) correct pattern
Figure 6. The resulting ﬁngertip movement trajectories without (a) and with (b) camera-shake calibration. The correct pattern is shown in (c). To aid clarity
we have transformed (a) and (b) to the user’s perspective.
Figure 7. Filming angle calculation. The ﬁlming angle, θ, is the angle between
the edge line of the device and a vertical line.
of x and y, and their relative positions are represented by (cid:52)X
and (cid:52)Y . For each frame where both areas are successfully
tracked, we compute the relative coordinates, ((cid:52)X, (cid:52)Y ),
which are reported as the location of the tracked ﬁngertip.
Figure 6 shows the results when using TLD to process
a video that was ﬁlmed with some camera shake effects.
Figure 6 illustrates the tracking results without (a) and with (b)
camera-shake calibration. To aid clarity, we have converted the
trajectories into the user’s perspective. Without camera-shake
calibration, the resulting trajectory is signiﬁcantly different
from the actual pattern shown in Figure 6 (c). Because of this
great difference, using Figure 6 (a) will lead to misidentiﬁca-
tion of candidate patterns. By contrast, Figure 6 (b) generated
with camera-shake calibration is more alike the correct pattern.
C. Filming angle transformation
In practice, the ﬁlming camera will not directly face the
target device to avoid raising suspicion by the target user. As
a result, the ﬁngertip movement trajectory generated by the
tracking algorithm will look differently from the actual pattern.
For example, for the pattern presented in Figure 2 (a), if the
video is ﬁlmed from the attacker’s front-left to the target device
(i.e. with a ﬁlming angle of approximate 45 degrees), we get
the trajectory shown in Figure 2 (c). Using this trajectory
without any postprocessing will lead to misidentiﬁcation of
candidate patterns. Therefore, we must transform the resulting
trajectory to the user’s view point. To do so, we need to
estimate the angle between the ﬁlming camera and the target
device. Our approach is described as follows.
We use an edge detection algorithm called Line Segment
Detector (LSD) [12] to detect the longer edge of the device.
The ﬁlming angle is the angle between the detected edge
line and a vertical line. This is illustrated in Figure 7. In
Section VI-E, we show that a minor estimation error of the
ﬁlming angle has little impact on the attacking success rate.
By default, we assume that the pattern grid is presented in
the portrait mode3. If this is not the case, i.e. the pattern grid
is shown in the landscape mode, we need to use the shorter
edge of the device to calculate the ﬁlming angle. We believe
that an attacker interested in a particular target device would
have some knowledge of how the pattern grid is presented
under different orientation modes and be able to identify the
device orientation by watching the video. There are also other
methods to be used to identify the ﬁlming angle [28].
Based on the estimated ﬁlming angle, θ, we use the
following formula to transform the tracked ﬁngertip movement
trajectory from the camera’s view point to the user’s:
(cid:48)
S = T S
,
T =
(cid:20)cos θ − sin θ
(cid:21)
sin θ
cos θ
(1)
(cid:48)
where T is a Transformation Matrix, S
is the coordinate
of a point of the tracked trajectory, and S is the resulting
coordinate after the transformation. For each video frame, our
algorithm individually calculates the ﬁlming angle and perform
the transformation, because the ﬁlming angle may change
across video frames.
D. Identify and rank candidate patterns
In this step,
the ﬁngertip movement
trajectory will be
mapped to a number of candidate patterns to be tested on
the target device. The goal of the attack is to exclude as many
patterns as possible and only leave the most-likely patterns to
be tried out on the target device. Our approach is to use the
geometry information of the ﬁngertip movement trajectory, i.e.
the length and direction of line segments and the number of
turning points, to reject patterns that do not satisfy certain
criteria. In this section, we ﬁrst describe how to identify
overlapping line segments and extract length and direction
information before presenting how to use the extracted infor-
mation to identify and rank candidate patterns.
1) Extracting Structure Information: A pattern can be
deﬁned as a collection of line segments where each line
segment has two properties: the length of the line, l, and
the direction of the line, d. We deﬁne a pattern, P , as a
3The pattern grid of the Android native pattern lock is always presented in
the portrait mode regardless of the orientation of the device.
6
−100−50050100−100−50050100−100−50050100−100−5005010010020030040015010050(a) tracked ﬁngertip movement
(b) pattern example
Figure 8. This ﬁgure shows the tracked ﬁngertip movement trajectory (a) of
a pattern (b). Point S on (a) is the the starting point and points A, B, C, and
D on (b) represent four turning points.
(a) line direction number
(b) numbering line segment of the
tracked trajectory
Figure 10. All possible line directions for a 3 × 3 Android pattern grid.
struct T []: Temporal information of each tracked location
timeT h: Threshold of whether two line segments are overlapping
Output:
tp[] Turning points of ﬁngertip movement.
Algorithm 2 Line Segment Identiﬁcation
Input:
tpN um = 0;
struct lines[] ← getLines(T [])
lN um ← getLinesN umber(lines[])
for i = 1 : lN um do
if checkOverlap(lines[i], timeT h) then
1: for each ﬁngertip movement with temporal sequences T [] do
2:
3:
4:
5:
6: