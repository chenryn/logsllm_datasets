to maximize rewards from the environment. Referring again to
Figure 4, the environment defines the action space A and state
space S. The agent starts with an initial policy, evaluates it using
multiple rollouts, and then updates it based on the results (rewards)
of these rollouts. Then, it repeats this process until satisfied with
the reward.
We first consider a strawman formulation of decision tree gener-
ation as a single Markov Decision Process (MDP). In this framing,
a rollout begins with a tree consisting of a single node. This is the
initial state, s0 ∈ S. At each step t, the agent executes an action
at ∈ A and receives a reward rt ; the environment transitions from
the current state st ∈ S to the next state st +1 ∈ S (i.e., the updated
tree and next node to process). The goal is to maximize the total
t γ t rt where γ is a discounting
reward received by the agent, i.e.,
factor used to prioritize more recent rewards.

Design challenges. While at a high level this RL formulation
seems straightforward, there are three key challenges we need
to address before we have a realizable implementation. The first
is how to encode the variable-length decision tree state st as an
input to the neural network policy. While it is possible to flatten
the tree, say, into an 1-dimensional vector, the size of such a vector
would be very large (i.e., hundreds of thousands of units). This will
require both a very large network model to process such input, and
a prohibitively large number of samples.
While recent work has proposed leveraging recurrent neural
networks (RNNs) and graph embedding techniques [58, 60, 61] to
reduce the input size, these solutions are brittle in the face of large or
dynamically growing graph structures [66]. Rather than attempting
to solve the state representation problem to deal with large inputs,
in NeuroCuts we instead take advantage of the underlying structure
of packet classification trees to design a simple and compact state
representation. This means that when the agent is deciding how to
split a node, it only observes a fixed-length representation of the
node. All needed state is encoded in the representation; no other
information about the rest of the tree is observed.
The second challenge is how to deal with the sparse and delayed
rewards incurred by the node-by-node process of building the deci-
sion tree. While we could in principle return a single reward to the
agent when the tree is complete, it would be very difficult to train
an agent in such an environment. Due to the long length of tree
rollouts (i.e., many thousands of steps), learning is only practical if
we can compute meaningful dense rewards.1 Such a dense reward
for an action would be based on the statistics of the subtree it leads
to (i.e., its depth or size).2 This effectively reduces the delay of
the rewards from O(tree size) to O(loд(tree size)). Unfortunately,
it is not possible to compute this until the subtree is complete. To
1Note that just returning -1 or -cut Size for each step would be a dense reward but
not particularly useful.
2The rewards for NeuroCuts correspond to the true problem objective; we do not do
"reward engineering" since that would bias the solution.
260
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica
(cid:22)(cid:20)(cid:20)
(cid:21)(cid:25)(cid:20)
(cid:21)(cid:20)(cid:20)
(cid:25)(cid:20)
(cid:20)
(cid:52)(cid:86)(cid:83)(cid:88)(cid:83)(cid:71)(cid:83)(cid:80)
(cid:40)(cid:87)(cid:88)(cid:52)(cid:83)(cid:86)(cid:88)
(cid:55)(cid:86)(cid:71)(cid:52)(cid:83)(cid:86)(cid:88)
(cid:40)(cid:87)(cid:88)(cid:45)(cid:52)
(cid:55)(cid:86)(cid:71)(cid:45)(cid:52)
(cid:28)(cid:20)(cid:20)
(cid:26)(cid:20)(cid:20)
(cid:24)(cid:20)(cid:20)
(cid:22)(cid:20)(cid:20)
(cid:20)
(cid:21)(cid:20)(cid:20)(cid:20)(cid:20)
(cid:27)(cid:25)(cid:20)(cid:20)
(cid:25)(cid:20)(cid:20)(cid:20)
(cid:22)(cid:25)(cid:20)(cid:20)
(cid:20)
(cid:24)(cid:20)(cid:20)(cid:20)(cid:20)
(cid:23)(cid:20)(cid:20)(cid:20)(cid:20)
(cid:22)(cid:20)(cid:20)(cid:20)(cid:20)
(cid:21)(cid:20)(cid:20)(cid:20)(cid:20)
(cid:20)
(a) NeuroCuts starts with a randomly initialized policy that generates poorly shaped trees (left, truncated). Over time, it learns to
reduce the tree depth and develops a more coherent strategy (center). The policy converges to a compact depth-12 tree (right) that
specializes in cutting SrcIP, SrcPort, and DstPort.
(b) In comparison, HiCuts produces a
depth-29 tree for this rule set that is 15×
larger and 3× slower in classification
time.
Figure 5: Visualization of NeuroCuts learning to split the fw5_1k ClassBench rule set. The x-axis denotes the tree level, and
the y-axis the number of nodes at the level. The distribution of cut dimensions per level of the tree is shown in color.
handle this, we take the somewhat unusual step of only computing
rewards for the rollout when the tree is completed, and setting
γ = 0, effectively creating a series of 1-step decision problems simi-
lar to contextual bandits [24]. However, unlike the bandit setting,
where an agent only makes a decision once per environment, these
1-step decisions are connected through the dynamics of the tree
building process. For instance, this makes NeuroCuts amenable to
techniques from the Deep RL literature such as GAE [42].
Another way of looking at the dense reward problem is that
the process of building a decision tree is not really sequential but
tree-structured (i.e., it is more accurately modeled as a branching
decision process [8, 18, 39]), and we need to account for the reward
calculations accordingly. In such a "branching" formulation, γ > 0,
but the rewards of an action are computed as an aggregation over
multiple child states produced by an action. For example, cutting a
node produces multiple child sub-nodes, and the reward calcula-
tion may involve a sum or a min over each child’s future rewards,
depending on whether we are optimizing for tree size or depth. The
1-step decision problem and branching decision process formula-
tions of NeuroCuts are roughly equivalent; in the implementation
section we describe how we adapt standard RL algorithms to run
NeuroCuts.
The final challenge is how to scale the solution to large packet
classifiers. The decision tree for a packet classifier with 100K rules
can have hundreds of thousands of nodes. The size of the tree
impedes training along several dimensions. Not only does it take
more steps to finish building a tree, but the execution time of each
action increases as there are more rules to process. The space of
trees to explore is also larger, requiring the use of larger network
models and generating more rollouts to train.
State representation. One key observation is that the action on
a tree node only depends on the node itself, so it is not necessary
to encode the entire decision tree in the environment state. Our
goal to optimize a global performance objective over the entire tree
suggests that we would need to make decisions based on the global
state. However, this does not mean that the state representation
needs to encode the entire decision tree. Given a tree node, the
action on that node only needs to make the best decision to optimize
the sub-tree rooted at that node. It does not need to consider other
tree nodes in the decision tree.
261
Figure 6: The NeuroCuts policy is stochastic, which enables
it to effectively explore many different tree variations dur-
ing training. Here we visualize four random tree variations
drawn from a single policy trained on the acl4_1k Class-
Bench rule set.
Formally, given tree node n, let tn and sn denote n’s classification
time and memory footprint, respectively, and Tn and Sn be the
classification time and memory footprint of the entire sub-tree
rooted at node n, respectively. Then, for a cut action, we have the
following equations:
Tn = tn + maxi ∈childr en(n)Ti
Sn = sn + sumi ∈childr en(n)Si
(1)
(2)
Similarly, for a partition action, we have as an upper bound on cost,
assuming serial execution:
Tn = tn + sumi ∈childr en(n)Ti
Sn = sn + sumi ∈childr en(n)Si
(3)
(4)
An action, a, taken on node n only needs to optimize the sub-tree
rooted at n according to the following expression,
Vn = argmaxa ∈A − (c · Tn + (1 − c) · Sn ),
(5)
where c is a coefficient capturing the tradeoff between classifica-
tion time and memory footprint. The negation is needed since we
want to minimize time and space complexities. We note that these
values can be computed after the tree is fully built, regardless of
the traversal order taken building the tree.
When c ∈ {0, 1}, it is easy to see that if at every tree node n we
take the action that optimizes Vn , then, by induction, we end up
optimizing Vr , where r is the root of the tree. In other words, we end
up optimizing the global objective (reward) for the entire decision
tree. For 0 < c < 1 this optimization becomes approximate, but
we find empirically that c can still be used to interpolate between
the two objectives. It is important to note here that while the state
Neural Packet Classification
SIGCOMM ’19, August 19–23, 2019, Beijing, China
representation only encodes current node n, action a taken for node
n is not local, as it optimizes the entire sub-tree rooted at n.
In summary, we only need to encode the current node as the
input state of the agent. This is because the environment builds
the tree node-by-node, node actions need only consider their own
state, and each node contains a subset of the rules of its parent (i.e.,
rules contained in some subspace of its parent space). Therefore,
nodes in the tree can be completely defined by the ranges they
occupy in each dimension. Given d dimensions, we use 2d numbers
to encode a tree node, which indicate the left and right boundaries
of each dimension for this node. The state also needs to describe
the partitioning at the node, which can be handled in a similar
way. We note that the set of rules for the packet classifier are not
present in the observation space. NeuroCuts learns to account for
packet classifier rules implicitly through the rewards it gets from
the environment. A full description of the NeuroCuts state and
action representations can be found in Table 1.
Training algorithm. We use an actor-critic algorithm to train the
agent’s policy [19]. This class of algorithms have been shown to
provide state-of-the-art results in many use cases [5, 35, 43], and can
be easily scaled to the distributed setting [7]. We also experimented
with Q-learning [37] based approaches, but found they did not
perform as well.
Algorithm 1 shows the pseudocode of the NeuroCuts algorithm,
which executes as follows. NeuroCuts starts with the root node of
the decision tree, s∗
. The end goal is to learn an optimized stochastic
policy function π (a|s; θ ) (i.e., the actor). NeuroCuts first initializes
all the parameters (line 1-6), and then runs for N rollouts to train
the policy and the value function (line 7-23). After each rollout,
it reinitializes the decision tree to the root node (line 9). It then
incrementally builds the tree by repeatedly selecting and applying
an action on each non-terminal leaf node (line 11-13) according
to the current policy. A terminal leaf node is a node in which the
number of rules is below a given threshold.
More specifically, NeuroCuts traverses the tree nodes in depth-
first-search (DFS) order (line 13), i.e., it recursively cuts the child of
the current node until the node becomes a terminal leaf. Note that
the DFS order is not essential. It is used to give a way for the agent
to find a tree node to cut. Other orders, such as the breadth-first-
search (BFS), can be used as well. After the decision tree is built, the
gradients are reset (line 14), and then the algorithm iterates over
all the tree nodes to aggregate the gradients (line 15-21). Finally,
NeuroCuts uses the gradients to update the parameters of the actor
and critic networks (line 22), and proceeds to the next rollout (line
23).
The first gradient computation (line 19) corresponds to that
for the policy gradient loss. This loss defines the direction to up-
date θ to improve the expected reward. An estimation of the state
value V (s; θv ) is subtracted from the rollout reward R to reduce
the gradient variance [19]. V is trained concurrently to minimize
its prediction error (line 21). Figure 5 visualizes the learning pro-
cess of NeuroCuts to build a decision tree. The NeuroCuts policy
is stochastic, enabling it to effectively explore many different tree
variations during training, as illustrated in Figure 6.
Algorithm 1 Learning a tree-generation policy using an actor-
critic algorithm.
∗
Input: The root node s
where a tree always grows from.
Output: A stochastic policy function π (a |s; θ ) that outputs a branching action
a ∈ A given a node state s, and a value function V (s; θv ) that outputs a value
estimate for a node state.
Main routine:
1: // Initialization
2: Randomly initialize the model parameters θ , θv
3: Maximum number of rollouts N
4: Coefficient c ∈ [0, 1] that trades off classification time vs. space
5: Reward scaling function f (x ) ∈ {x, log(x )}
6: n ← 0
7: // Training
8: while n < N do
∗)
s ← Reset(s
9:
10:
// Build a tree using the current policy
while s (cid:2) Null do
11:
a ← π (a |s; θ )
12:
s ← GrowTreeDFS(s, a)
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
// Compute the future rewards for the given action
R ← −(c · f (Time(s )) + (1 − c ) · f (Space(s )))
// Accumulate gradients wrt. policy gradient loss
dθ ← dθ + ∇θ log π (a |s; θ )(R − V (s; θv ))
// Accumulate gradients wrt. value function loss
dθv ← dθv + ∂(R − V (s; θv ))2/∂θv
Perform update of θ using dθ and θv using dθv .
n ← n + 1
Reset gradients dθ ← 0 and dθv ← 0
for (s, a) ∈ TreeIterator(s
∗) do
Subroutines:
• Reset(s): Reset the tree s to its initial state.
• GrowTreeDFS(s, a): Apply action a to tree node s , and return the next
non-terminal leaf node in the tree in depth-first traversal order.
• TreeIterator(s): Non-terminal tree nodes of the subtree s and their taken