uous, with a Lipschitz constant L := 1. We see, for any
input interval X:
w(ReluI(X)) = max(X,0)− max(X,0)
≤ max(X,0)− X ≤ X − X = w(X)
Thus, the interval extension ReluI of ReLU is Lipschitz
continuous. As the NN is a ﬁnite composition of Lips-
chitz continuous functions, its interval extension F is still
Lipschitz continuous as well [45].
Now we demonstrate that by splitting input X into N
smaller pieces and taking the union of their corresponding
outputs, we can achieve at least a N times smaller overes-
timation error. We deﬁne an N-split uniform subdivision
of input X = (X1, ...,Xd) as a collection of sets Xi, j:
Xi, j := [Xi + ( j− 1) w(Xi)
N
w(Xi)
]
,Xi + j
N
where i ∈ 1, . . . ,d and j ∈ 1, . . .N. We note that this is
exactly a partition of each Xi into N pieces of equiva-
lent width such that ∀i, j, w(Xi, j) = w(Xi)/N and Xi =
(cid:3)
N
j=1 Xi, j. We then deﬁne a reﬁnement of F over X with
N splits as:
F (N)(X) :=
N(cid:2)
i=1
F(X1,i, . . . ,Xd,i)
1604    27th USENIX Security Symposium
USENIX Association
Finally, we deﬁne the range of overestimated error
created by naive interval extension on an NN after N-split
reﬁnement as w(E(N)(X)):
w(E(N)(X)) := w(F (N)(X))− w( f (X))
Because F is Lipschitz continuous, Theorem 6.1 in
[45] gives us the following result:
w(E(N)(X)) ≤ 2L· w(X)/N
(2)
Equation 2 shows the error width of the N-split reﬁne-
ment w(E(N)(X)) converges to 0 linearly as we increase
N. That is, we can achieve arbitrary accuracy when using
N-split reﬁnement to approximate f (X) with sufﬁciently
large N.
5 Methodology
Figure 4 shows the main workﬂow along with the differ-
ent components of ReluVal. Speciﬁcally, ReluVal uses
symbolic interval analysis to get a tight estimation of the
output range based on the input ranges. It declares a secu-
rity property as veriﬁed If the estimated output interval is
tight enough to satisfy the property. If the output interval
shows potential existence of violations, ReluVal randomly
samples a few points from the interval and check for vio-
lations. If any adversarial case is detected, i.e., a concrete
input violating the security property, it outputs this as
a counterexample. Otherwise, ReluVal uses iterative in-
terval reﬁnement to further tighten the output interval to
approach the theoretically tightest bound and repeats the
same process described above. Once the number of itera-
tions reaches a preset threshold, ReluVal outputs timeout
denoting it cannot verify the security property.
pendency problem. Below, we describe the details of the
optimizations we propose to further tighten the bounds.
5.1 Symbolic Interval Propagation
Symbolic Interval propagation is one of our core contri-
butions to mitigate the input dependency problem and
tighten the output interval estimation. If a DNN would
only consist of linear transformations, keeping symbolic
equation throughout the intermediate computations of a
DNN can perfectly eliminate the input dependency errors.
However, as shown in Section 3, while passing an equa-
tion through a ReLU node essentially involves dropping
the equation and replacing it with 0 if the equation can
evaluate to a negative value for the given input range.
Therefore, we keep the lower and upper bound equations
(Equp,Eqlow) for as many neurons as we can and only
concretize as needed.
Algorithm 1 Forward symbolic interval analysis
Inputs:
network ← tested neural network
input ← input interval
(cid:4)
eq;
for i = 1 to layerSize[layer] do
// matmal equations with weights as interval;
eq= weight
// update the output ranges for each node
if layer != lastLayer then
if equp[i] ≤ 0 then
// Update to 0
R[layer][i]=[0,0];
equp[i] = eqlow[i] = 0;
else if eqlow[i] ≥0 then
// Keep dependency
R[layer][i]=[1,1];
1: Initialize eq = (equp,eqlow);
2: // cache mask matrix needed in backward propagation
3: R[numLayer][layerSize];
4: // loops for each layer
5: for layer = 1 to numlayer do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26: return R, output;
// Concretization
R[layer][i]=[0,1];
eqlow[i] = 0
if equp[i] ≤ 0 then
equp[i] = equp[i];
output = {lower, upper};
(cid:2) d(relu(x))
dx
else
else
= [0,0]
(cid:2) d(relu(x))
dx
= [1,1]
(cid:2) d(relu(x))
dx
= [0,1]
Figure 4: Workﬂow of ReluVal in checking security prop-
erty of DNN.
As discussed in Section 3, simple interval extension
only obtains loose/conservative intervals due to input de-
Algorithm 1 elaborates the procedure of propagating
symbolic intervals/equations during the interval computa-
tion of a DNN. We describe the core components and the
details of this technique below.
Constructing symbolic intervals. Given a particular
neuron A, (1) If A is in the ﬁrst layer, we can compute the
USENIX Association
27th USENIX Security Symposium    1605
symbolic bounds as:
EqA
up(X) = EqA
low(X) = w1x1 + ... + wdxd
where x1, ...,xd are the inputs and w1, ...,wd is the weights
of the corresponding edges. (2) If A belongs to the inter-
mediate layer, we initialize the symbolic intervals of A’s
output as:
EqA
up(X) = W+EqAprev
up
low(X) = W+EqAprev
low
(X) +W−EqAprev
low
(X) +W−EqAprev
up
(X)
(X)
EqA
and EqAprev
where EqAprev
low are the equations from last layer.
up
W+ and W− denote the positive and negative weights of
current layer respectively. The output will be [w+a,w+b]
for multiplying positive weight parameter w+ with an
interval [a,b]. For the negative weight parameters, the
output will be ﬂipped in terms of a and b, i.e., [w−b,w−a].
Concretization. While passing a symbolic equation
through the ReLU nodes, we evaluate the concrete value
of the equation’s upper and lower bounds Equp(X) and
Eqlow(X). If Eqlow(X) > 0, then we pass the lower equa-
tion on to the next layer. Otherwise, we concretize it to
be 0. Similarly, if Equp(X) > 0, we pass the upper equa-
tion on to the next layer. Otherwise, we concretize it as
Equp(X).
Correctness. We ﬁrst clarify three different output in-
tervals: (1) theoretically tightest bound f (X), (2) naive
interval extension bound F(X), and (3) symbolic bound
[Eqlow(X),Equp(X)]. We prove that the symbolic bound
is a superset of theoretically tightest bound and a subset
of output naive interval extension:
f (X) ⊆ [Eqlow(X),Equp(X)] ⊆ F(X)
(3)
For a given input range propagated to the output layer,
it will involve both computing linear transformations and
applying ReLUs. symbolic interval analysis keeps the
accurate bounds for linear transformations and uses con-
cretization to handle non-linearity. Compared to theoreti-
cally tightest bound, the only approximation introduced
during the symbolic propagation process is due to con-
cretization while handling ReLU nodes, which is an over-
approximation as shown before. Naive interval extension,
on the other hand, is a degenerate version of symbolic
interval analysis where it does not keep any symbolic
constraints. Therefore, symbolic interval analysis over-
approximates the theoretically tightest bound and, in turn,
is over-approximated by naive interval extension as shown
in Equation 3.
Iterative Interval Reﬁnement
5.2
While symbolic interval analysis helps in computing rel-
atively tight bounds, the estimated output intervals for
complex networks may still not be tight enough for veri-
fying properties, especially when the input intervals are
comparably large and thus result in many concretizations.
As discussed above in Section 5, for such cases, we re-
sort to another technique, iterative interval reﬁnement.
In addition, we also propose two other optimizations, in-
ﬂuence analysis and monotonicity, which further reﬁnes
the estimated output ranges based on iterative interval
reﬁnement.
Baseline iterative reﬁnement. In Section 4, we have
proved that theoretically tightest bound could be ap-
proached by repeatedly splitting the input intervals. There-
fore, we perform iterative bisection of each input interval
X1, ...,Xn until the output interval is tight enough to meet
the security property, or time out, as shown in Figure 4.
The iterative bisection process can be represented as a
bisection tree as shown in Figure 5. Each bisection on one
input yields two children denoting two consecutive sub-
intervals, the union of which computes the output bound
for their parent. Here, X (i) j means the jth input interval
with split depth i. After one bisection on X (i) j, it creates
two children: X (i+1)2 j−1 = {X1, ..., [Xi, Xi+Xi
], ...,Xd} and
X (i+1)2 j = {X1, ..., [ Xi+Xi
To identify the existence of any adversarial example in
the bisected input ranges, we sample a few input points
(the current default is the middle point of each range) and
verify if the concrete output leads to any property viola-
tions. If so, we output the adversarial example, mark this
sub-interval as deﬁnitely containing adversarial examples,
and conclude the analysis for this speciﬁc sub-interval.
Otherwise, we repeat the symbolic interval analysis pro-
cess for the sub-interval. This default conﬁguration is
tailored towards deriving a conclusive answer of “secure”
or “insecure” for the entire input interval. Users of Re-
luVal can conﬁgure it to further split an insecure interval
to potentially discover secure sub-intervals within the
insecure interval.
Optimizing iterative reﬁnement. We develop two other
optimizations, namely inﬂuence analysis and monotonic-
ity, to further cut the average bisection depths.
,Xi], ...,Xd}.
2
2
(1) Inﬂuence analysis. When deciding which input
intervals to bisect ﬁrst, instead of following a random
strategy, we compute the gradient or Jacobian of the out-
put with respect to each input feature and pick the largest
one as the ﬁrst to bisect. The high-level intuition is that
gradient approximates the inﬂuence of the input on the
output, which essentially measures the sensitivity of the
output to each input feature.
Algorithm 2 shows the steps for backward computation
of the input feature inﬂuence. Note that instead of work-
ing on concrete values, this version works with intervals.
The basic idea is to approximate the inﬂuence caused
by ReLUs. If there is no ReLU in the target DNN, the
1606    27th USENIX Security Symposium
USENIX Association
Algorithm 3 Using inﬂuence analysis to choose the most
inﬂuential feature to split
network ← tested neural network
input ← input interval
g ← gradient interval calculated by backward propagation
Inputs:
1: for i = 1 to input.length do
2:
3:
4:
5:
6:
7:
8:
9: return splitFeature;
// r is the range of each input interval
r = w(input[i]);
// e is the inﬂuence from each input to output
e = gup[i]∗ r;
if e > largest then
largest = e;
splitFeature = i;
(cid:2) most effective feature
violation. Our empirical results in Section 7 also indicate
that such monotonicity checking can help decrease the
number of splits required for checking different security
properties.
6
Implementation