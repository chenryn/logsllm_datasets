correctly identiﬁed the fake voice for unfamiliar or brieﬂy familiar
speakers is consistent (50% for our work vs. 48% in [57]). However,
participants in our survey were more accurate at identifying fake
speech from famous speakers ( 80% vs. 50% in [57]), perhaps re-
ﬂecting a higher general awareness of speech synthesis attacks.
3) Does mentioning fake speech in the survey description
change participants’ perceptions of the fake speech samples?
Mentioning fake speech in the survey description showed a statis-
tically signiﬁcant eﬀect on survey responses. Figure 4 shows how
the responses to the survey version mentioning fake speech reﬂect
an apparent increased skepticism of fake speakers.
Using a chi-squared test for independence, we compared responses
from each speaker familiarity category between the two survey
versions to see if this change is statistically signiﬁcant.
• For unfamiliar speakers, all but one speaker has a signiﬁcant
(p  45). For unfamiliar speakers, there
were statistically signiﬁcant (p = 3 participants,
unless otherwise noted. More details about the post-deception in-
terview and analysis procedure can be found in the Appendix.
Because all of the interviewees were members of the authors’
academic division, they had diﬀerent levels of familiarity with the
interviewers, ranging from general knowledge to frequent social
interaction. At the end of each interview, participants were asked
to rank their familiarity with the interviewers’ voices prior to the
interview on a scale of 1 (“not at all familiar”) to 5 (“extremely
familiar”). Table 6 lists the distribution of familiarity rankings.
Not at all
Familiar
Slightly
Familiar
Moderately
Very
Familiar
Familiar
Extremely
Familiar
Real Interviewer
Fake Interviewer
Table 6: # of participants and their declared familiarity with
the two interviewer’s voices before the Zoom interview.
9
7
1
2
2
3
0
1
2
1
Task. The staged interview itself consists of 8 questions about
use of automatic speech recognition systems and perceptions of
privacy (see Table 7). Five are asked by the real interviewer, and
three are asked by the fake interviewer. The three fake interviewer
questions are designed to solicit three diﬀerent types of behav-
ior from participants: conversational response (Q2), website access
(Q5), and personal information (Q7).
#
Interviewer
Question
1
2
3
4
5
6
7
8
Real
Fake
Real
Real
Fake
Real
Real
Fake
Do you use automatic speech recognition systems in
everyday life?
How often do you use these systems in your daily life?
What do you do in your interactions with these systems?
Do you ever think about your privacy during your
interactions with these systems?
Can you visit this website? I’ll put the link in the chat.
Have you ever used the “voice proﬁles” feature of these
systems?
Are you ever concerned about privacy if/when you use
voice proﬁles?
We need your student id to track your participation in this
study. Can you leave it in the chat?
Table 7: Questions asked by real and fake interviewers.
Conditions. Participants are not told that the study is actually
about perceptions of fake speech, and they do not know that one of
the interviewers is using a fake voice. When the participant joins
the Zoom call, the real interviewer informs them that everyone in
the call is keeping their video oﬀ to preserve interviewee privacy.
In reality, keeping videos oﬀ prevents the participant from observ-
ing that the fake interviewer is using a fake voice. We also asked
the interviewees if we could record the interview to maintain a
record of their responses.
Because of the relatively low quality of the fake interviewer
voice (see §5.2), interviewees are primed to expect a low quality
voice from the fake interviewer. For 10 of the 14 participants, be-
fore beginning the interview questions, the real interviewer notes
that the fake interviewer is feeling unwell and will only chime in
intermittently during the interview. We examine the eﬀect of ex-
cluding this priming statement from the interview in later sections.
Results. None of the participants exhibited any suspicion or hes-
itancy during interactions with the fake interviewer’s voice. All 14
responded without hesitation to the three questions asked by the
fake interviewer, visited the requested website, and even gave their
school ID number to the interviewers. After the interview con-
cluded and the deception was revealed, only four of the 14 partici-
pants stated that they thought something was “oﬀ” about the fake
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea244interviewer’s voice. Importantly, these four participants had (in-
tentionally) not been given the “priming” statement that the fake
interviewer “had a cold.” Below, we explore the most interesting
results from this study and highlight several key limitations.
1) Reaction to fake voice: Several themes arose during the
post-deception interviews, as summarized below.
• Complete surprise: Four participants were visibly and audibly
astonished when the deception was revealed. P5 noted that, “I
really thought it was you – like 100%,” while P10, after a shocked
moment of silence, said “computers just won the Turing test.”
• Satisﬁed with “sick” excuse: Seven participants noted explicitly
that the “sick” excuse squashed any concerns about the fake
interviewer’s voice. P4 said that “I think it totally worked – I
thought you were terribly sick,” while P2 noted that “it was re-
ally kind of worrying [how sick you sounded].”
• Silently suspicious: Four participants (P9, P12, P13, P14) ex-
pressed suspicions after the deception was revealed. P12 and
P13 said it “sounded like speaker had a cold,” and P14 supposed
“it was a poor quality microphone.”
2) Why participants didn’t voice their concerns: After the
deception was revealed, participants were asked to identify ele-
ments of the interview structure that increased their trust in the
fake interviewer. Some, of course, were completely unsuspecting
and did not think to question the fake interviewer. However, oth-
ers noted that the presence of a second (obviously human) inter-
viewer, social convention, and the origin of the interview request
(from within our department) bolstered their trust.
• Presence of real interviewer: Several participants credited the
“tag-team” nature of the interview, with real and fake interview-
ers colluding, as making the deception more believable. “I feel
like [the real interviewer’s] obviously human presence played a
big factor in [my not saying anything].” (P9).
• Polite social convention: Multiple participants noted that they
felt it would be uncomfortable or wrong for them to say some-
thing about the fake interviewer’s voice during the interviewer.
When asked why they didn’t say anything about the quality of
the fake interviewer’s voice, P12 exclaimed, “well that would be
quite the insult!”
• Provenance of interview request: Since we recruited from within
our department, the recruitment was sent out through trusted
channels only accessible by members of the department (i.e.,
email list-serv, Slack). P9 expressed suspicions during the de-
brieﬁng, but credited the “provenance of the study... seemed like
a legit source” as a reason to fully participate with our questions.
3) What would have made participants suspicious: When
asked to articulate what would have made them more suspicious,
participant responses varied.
• Nothing: Participants most surprised by the deception claimed
that nothing would have caused them to question the credibility
of the fake interviewer: “I’m glad you guys didn’t ask me for a
bank account, because [...] I would have given it to you” (P5).
• Requesting more personal information: One participant noted
“I don’t think the information you wanted was very sensitive
[so] I don’t see why I need to be concerned about this” (P6).
IRB constraints prevented us from soliciting anything more per-
sonal than a student ID, used to access services at our university.
While not public, this information is not inherently sensitive.
4) Eﬀect of familiarity with interviewers: Seven of the par-
ticipants rated their familiarity with both interviewers’ voices as
a 1 out of 5 (e.g., not at all familiar with either). Their responses,
though, were consistent with the other participants who had some
previous familiarity with one or both of the interviewers’ voices.
Only one participant (P8) mentioned that “the voice did seem pretty
weird, but since I trust you both, I just went [with] it.” These results
suggest that the trusted setting and presence of a human likely play
a larger factor than prior familiarity with the speaker’s voice.
5) Eﬀect of priming statement: To examine the eﬀect of the
“sick” excuse on the believability of the fake voice, we conduct
four interviews in which participants are not told that the fake
interviewer is sick. In these interviews, participants exhibit an in-
creased level of skepticism about the fake interviewer during the
debrieﬁng. One claimed “it was very obviously a fake voice,” (P11)
but said that based on their experience in other deception studies
they decided not to say anything. Others did not see through the
deception but did note that “I was feeling weird” (P13) and “I just
feel your voice is very strange” (P14).
5.4 Key Takeaways
Our two user studies (A & B) show that context and demograph-
ics impact the credability of synthesized speech for human users.
In study A, we found that mentioning fake speech increased par-
ticipants’ skepticism of the fake speakers they heard. Additionally,
women and younger participants in study A were more likely to
correctly identify fake speakers.
Our key takeaway from study B is that a fake voice fooled hu-
mans in a trusted interview setting. Of particular interest is that all
our study B participants were graduate students in computer sci-
ence, some of whom actively research security or machine learn-
ing. Our starting hypothesis was that computer science graduate
students would be among the hardest targets to fool with a fake
voice. Yet, none of them expressed suspicion about the fake voice
during the interview.
Limitations & Next Steps. Our participant pool for study B was
largely homogenous in gender, age, and educational background.
To conduct a “trusted” interview, our participants were drawn from
our academic department (computer science). The gender break-
down of our participants matches that of the department, which
skews heavily male. It is possible that the observed eﬀect of gen-
der and age on responses in study A could also extend to study
B. Therefore, a viable follow-up work is to conduct larger, more-
diverse user studies to provide a more nuanced understanding of
synthesized voice attacks in trusted settings.
On a related note, our trusted interview in study B followed a
voice-only format, where voice is the only medium for interaction.
Yet in real-world scenarios, interviewees could use two-factor au-
thentication mechanisms to verify the trusted setting, e.g., request-
ing the interviewers to turn on their video feed, or challenging the
interviewers with some verbal tests. These combined veriﬁcation
methods could make the attacks much more diﬃcult, allowing hu-
man users to eﬀectively defend against speech synthesis attacks.
We believe this is an important direction for follow-up work.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea245Defense
[100]
[90]
[26]
[98]
Category
Liveness