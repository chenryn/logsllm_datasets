average of 5 runs.
11020∞SamplingFrequency2345678910TCPgoodput(Gb/s)1ﬂows10ﬂows20ﬂows11020∞SamplingFrequency2345678910Throughput(Gb/s)12last rule, and (iii) ‘all’ means there is at least one ﬂow that matches
each rule. In ‘ﬁrst’ and ‘last,’ there are 10 TCP ﬂows. In ‘all,’ there
are as many ﬂows as there are number of rules (with at least 10
ﬂows). Each rule matches on a TCP destination port. As we can
see, there is little loss in throughput up to 10 rules. With more rules,
throughput does drop, but there is no difference between matching
on the ﬁrst (best case) and last rule (worst case) in the ﬁlter chain.
With 1000 ﬂows, other overheads (context switches) result in much
lower throughput.
7 Limitations
Though TPPs help in a wide variety of tasks that were discussed
in §2, they are not a panacea to implement any arbitrary function-
ality due to two reasons: (i) the restricted instruction set, and (ii)
restricted programming model in which end-hosts initiate tasks. As
we have not presented a formal theory of “network tasks,” the clas-
siﬁcation below is neither complete nor mutually exclusive; it is
only meant to be an illustration.
Tasks that require per-packet computation: The read and write
instructions in TPPs limit end-hosts to high throughput network up-
dates, but not arbitrary network computation. As an example, con-
sider the task of implementing an active queue management scheme
such as Stochastic Fair Queueing, static priority, dynamic priority
queueing (e.g. pFabric [2]), and fair queueing. These tasks require
ﬁne-grained control over per-packet transmit and drop schedules,
which is better realized using dedicated hardware or FPGAs [34].
In a similar vein, TPPs are not expressive enough to scan packets
for speciﬁc signatures (e.g., payload analysis using deep packet in-
spection). Such tasks are better served by other approaches (e.g.,
middlebox software, or custom packet processors).
Tasks that are event-driven: In the examples we discussed, all
TPPs originate at end-hosts. This limits end-hosts from implement-
ing tasks that require precisely timed notiﬁcations whenever there is
some state change within the network. For instance, TPPs by them-
selves cannot be used to implement ﬂow control mechanisms (e.g.,
priority ﬂow control, or PFC [15]), or reactive congestion notiﬁ-
cations such as Quantized Congestion Notiﬁcation [30] and Fast-
Lane [38]. Such tasks require the network to send special packets
when the queue occupancy reaches a certain threshold. However,
this isn’t a show-stopper for TPPs, as end-hosts can proactively in-
ject TPPs on a subset of packets and be notiﬁed quickly of network
congestion.
8 Discussion
In §2, we showed how TPPs enable end-hosts to access network
state with low-latency, which can then act on this state to achieve
a certain functionality. This is attractive as it enables interesting
functionality to be deployed at software-development timescales.
We now discuss a number of important concerns that we haven’t
covered.
Handling Device Heterogeneity: There are two issues here: in-
struction encoding, and statistics addressing. First, instructions are
unlikely to be implemented in an ASIC as hardwired logic, but us-
ing microcodes, adding a layer of indirection for platform speciﬁc
designs. Second, we recommend having two address spaces: (i)
a standardized address space where a majority of the important
statistics are preloaded at known locations, such as those identi-
ﬁed by the OpenFlow standard [29], and (ii) a platform-speciﬁc ad-
dress space through which additional statistics, speciﬁc to vendors
and switch generations can be accessed. For dealing with multiple
vendors, TPPs can support an indirect addressing scheme, so that
the the compiler can preload packet memory with platform spe-
ciﬁc addresses. For example, to load queue sizes from a Broadcom
ASIC at hop 1, and an Intel ASIC at hop 2, the compiler generates
the TPP below, loading the word from 0xff00 for Broadcom, and
0xfe00 for Intel, obtained out-of-band. For safety, the entire TPP
is wrapped around a CEXEC as follows:
CEXEC [Switch:VendorID], [Packet:Hop[0]]
LOAD [[Packet:Hop[1]], [Packet:Hop[1]]
PacketMemory:
Hop1: $BroadcomVersionID, 0xff00 (* overwritten *)
Hop2: $IntelVersionID, 0xfe00
The TPP compiler can query the ASIC vendor IDs from time to
time and change the addresses if the devices at a particular hop
suddenly change. However, indirect addressing limits the extent to
which a TPP can be statically analyzed.
MTU issues: Piggy-backed TPPs are attached to packets at the
edge of a network (end-host or a border router). Thus, if the in-
coming packet is already at the MTU size, there would be no room
to add a TPP. This is fortunately not a big issue, as many switches
support MTUs up to 9000 bytes. This is already being done today
in overlay networks to add headers for network virtualization [1].
9 Related Work
TPPs represent a point in the broad design space of programmable
networks, ranging from essentially arbitrary in-band programs as
formulated by Active Network proposals [33, 36], to switch-centric
programmable dataplane pipelines [4, 7, 17, 25], to controller-
centric out-of-band proposals such as OpenFlow [27] and Simple
Network Management Protocol (SNMP). We do not claim that the
TPP approach is a fundamentally novel idea, as it is a speciﬁc re-
alization of Active Networks. However, we have been ruthless in
simplifying the interface between the end-hosts and switches to a
bare minimum. We believe TPPs strike a delicate balance between
what is possible in switch hardware at line rate, and what is sufﬁ-
ciently expressive for end-hosts to perform a variety of useful tasks.
TPPs superﬁcially resemble Sprocket, the assembly language in
Smart Packets [33]. However, Sprocket represents a far more ex-
pressive point in the design space. It allows loops and larger pro-
grams that would be hard to realize in hardware at line rate. By
contrast, a TPP is a straight-line program whose execution latency
is deterministic, small, and known at compile time. TPPs fully
execute on the fast-path (i.e., router ASIC), whereas Sprocket ex-
ercises the slow-path (router CPU), which has orders of magnitude
lower bandwidth. TPPs also resemble the read/write in-band con-
trol mechanism for ATM networks as described in a patent [5];
however, we also focus extensively on how to refactor useful data-
plane tasks, and a security policy to safeguard the network against
malicious TPPs. Wolf et al. [37] focus on designing a high per-
formance Active Network router that supports general purpose in-
structions. It is unclear whether their model allows end-hosts to
obtain a consistent view of network state. Moreover, it is unlikely
that ASICs can take on general purpose computations at today’s
switching capacities at a reasonable cost. Furthermore, out-of-band
control mechanisms such as OpenFlow and Simple Network Man-
agement Protocol (SNMP) neither meet the performance require-
ments for dataplane tasks, nor provide a packet-consistent view of
network state.
There have been numerous efforts to expose switch statistics
through the dataplane, particularly to improve congestion manage-
ment and network monitoring. One example is Explicit Congestion
13Notiﬁcation in which a router stamps a bit in the IP header when-
ever the egress queue occupancy exceeds a conﬁgurable thresh-
old. Another example is IP Record Route, an IP option that en-
ables routers to insert the interface IP address on the packet. Yet
another example is Cisco’s Embedded Logic Analyzer Module
(ELAM) [10] that traces the packet’s path inside the ASIC at layer 2
and layer 3 stages, and generates a summary to the network control
plane. Instead of anticipating future requirements and designing
speciﬁc solutions, we adopt a more generic, protocol-independent
approach to accessing switch state.
10 Conclusion
We set out with a goal to rapidly introduce new dataplane func-
tionality into the network. We showed how, by presenting a pro-
grammatic interface, using which end-hosts can query and manip-
ulate network state directly using tiny packet programs. TPPs sup-
port both a distributed programming model in which every end-
host participates in a task (e.g., RCP* congestion control), and a
logically centralized model in which a central controller can mon-
itor and program the network. We demonstrated that TPPs enable
a whole new breed of useful applications at end-hosts: ones that
can work with the network, have unprecedented visibility nearly
instantly, with the ability to tie dataplane events to actual pack-
ets, umambiguously isolate performance issues, and act on network
view without being limited by the control plane’s ability to provide
such state in a timely manner.
Acknowledgments
Vimalkumar thanks Brandon Heller, Kok-Kiong Yap, Sarang
Dharmapurikar, Srinivas Narayana, Vivek Seshadri, Yiannis Yi-
akoumis, Patrick Bosshart, Glen Gibb, Swarun Kumar, Lavanya
Jose, Michael Chan, Nick McKeown, Balaji Prabhakar, and Navin-
dra Yadav for helpful feedback and discussions that shaped this
work. The authors also thank our shepherd John Wroclawski and
the anonymous SIGCOMM reviewers for their thoughtful reviews.
The work at Stanford was funded by NSF FIA award CNS–
1040190. Opinions, ﬁndings, and conclusions do not necessarily
reﬂect the views of NSF or other sponsors.
References
[1] Mohammad Alizadeh, Tom Essall, Sarang Dharmapurikar,
Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Terry Lam,
Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. “CONGA:
Distributed Congestion-Aware Load Balancing for Datacenters”. In:
SIGCOMM (2014).
[2] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti,
Nick McKeown, Balaji Prabhakar, and Scott Shenker. “pFabric: Minimal
Near-Optimal Datacenter Transport”. In: SIGCOMM (2013).
Arista Networks – 7100 Series Performance Results.
http://www.aristanetworks.com/media/system/pdf/7148sx-rfc
2889-broadcast-with-latency.pdf, Retrieved January 23, 2014.
Eric A Baden, Mohan Kalkunte, John J Dull, and Venkateshwar Buduma.
Field processor for a network device. US Patent 7,787,471. 2010.
[3]
[4]
[6]
[7]
[5] A.D. Berenbaum, Alexander Gibson Fraser, and Hubert Rae McLellan Jr.
In-band device conﬁguration protocol for ATM transmission convergence
devices. US Patent 08/939,746. 2001.
Pat Bosshart and Glen Gibb. Personal communication, 2014-01-27.
Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown,
Martin Izzard, Fernando Mujica, and Mark Horowitz. “Forwarding
Metamorphosis: Fast Programmable Match-Action Processing in Hardware
for SDN”. In: SIGCOMM (2013).
Sarang Dharmapurikar. Insieme Networks, Personal communication,
2013-07-18.
[9] Nandita Dukkipati and Nick McKeown. “Why Flow-Completion Time is the
[8]
Right metric for Congestion Control”. In: SIGCOMM CCR (2006).
ELAM Overview.
http://www.cisco.com/c/en/us/support/docs/switches/nexus-
7000-series-switches/116648-technote-product-00.html,
Retrieved March 13, 2014.
[10]
[11] Dongsu Han, Robert Grandl, Aditya Akella, and Srinivasan Seshan. “FCP: a
ﬂexible transport framework for accommodating diversity”. In: SIGCOMM
(2013).
[12] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, Bob Lantz, and
Nick McKeown. “Reproducible network experiments using container-based
emulation”. In: CoNEXT (2012).
[13] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, David Mazières,
and Nick McKeown. “I Know What Your Packet Did Last Hop: Using
Packet Histories to Troubleshoot Networks”. In: NSDI (2014).
[14] Danny Yuxing Huang, Kenneth Yocum, and Alex C Snoeren. “High-Fidelity
[15]
[16]
[17]
[18]
Switch Models for Software-Deﬁned Network Emulation”. In: HotSDN
(2013).
IEEE 802.1Qbb – Priority-based Flow Control.
http://www.ieee802.org/1/pages/802.1bb.html, Retrieved April 1
2014.
Intel Fulcrum FM4000 ASIC.
http://www.intel.com/content/dam/www/public/us/en/document
s/datasheets/ethernet-switch-fm4000-datasheet.pdf, Retrieved
July 1, 2013.
Intel Fulcrum FM6000 ASIC. http:
//www.ethernetsummit.com/English/Collaterals/Proceedings/2
013/20130404_S23_Ozdag.pdf, Retrieved July 1, 2013.
Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski,
Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, et al.
“B4: Experience with a globally-deployed software deﬁned WAN”. In:
SIGCOMM (2013).
[19] Vimalkumar Jeyakumar, Mohammad Alizadeh, Changhoon Kim, and
David Mazières. “Tiny Packet Programs for low-latency network control and
monitoring”. In: HotNets (2013).
[20] Dina Katabi, Mark Handley, and Charlie Rohrs. “Congestion control for high
[21]
[22]
bandwidth-delay product networks”. In: SIGCOMM (2002).
Peyman Kazemian, Michael Chang, Hongyi Zeng, George Varghese,
Nick McKeown, and Scott Whyte. “Real Time Network Policy Checking
using Header Space Analysis”. In: NSDI (2013).
Frank Kelly, Gaurav Raina, and Thomas Voice. “Stability and fairness of
explicit congestion control with small buffers”. In: SIGCOMM CCR (2008).
[23] Ahmed Khurshid, Xuan Zou, Wenxuan Zhou, Matthew Caesar, and
P Brighten Godfrey. “VeriFlow: Verifying Network-Wide Invariants in Real
Time”. In: NSDI (2013).
[24] Changhoon Kim. Windows Azure, Personal communication, 2014-01-26.
[25]
Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, and
M Frans Kaashoek. “The Click modular router”. In: TOCS (2000).
[26] Guohan Lu, Chuanxiong Guo, Yulong Li, Zhiqiang Zhou, Tong Yuan,
Haitao Wu, Yongqiang Xiong, Rui Gao, and Yongguang Zhang.
“ServerSwitch: a programmable and high performance platform for data
center networks”. In: NSDI (2011).
[27] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar,
Larry Peterson, Jennifer Rexford, Scott Shenker, and Jonathan Turner.
“OpenFlow: Enabling Innovation in Campus Networks”. In: SIGCOMM
CCR (2008).
[28] Millions of Little Minions: Using Packets for Low Latency Network
Programming and Visibility (extended version).
http://arxiv.org/abs/1405.7143. 2014.
[29] OpenFlow Switch Speciﬁcation, version 1.4.
https://www.opennetworking.org/images/stories/downloads/sd
n-resources/onf-specifications/openflow/openflow-spec-v1.
4.0.pdf, Retrieved April 1, 2014.
[30] Rong Pan, Balaji Prabhakar, and Ashvin Laxmikantha. “QCN: Quantized
congestion notiﬁcation”. In: IEEE802 1 (2007).
[31] Ben Pfaff, Justin Pettit, Keith Amidon, Martin Casado, Teemu Koponen, and
Scott Shenker. “Extending Networking into the Virtualization Layer.” In:
HotNets (2009).
[32] Mark Reitblatt, Nate Foster, Jennifer Rexford, Cole Schlesinger, and
David Walker. “Abstractions for Network Update”. In: SIGCOMM (2012).
[33] Beverly Schwartz, Alden W Jackson, W Timothy Strayer, Wenyi Zhou,
R Dennis Rockwell, and Craig Partridge. “Smart packets for active
networks”. In: Open Architectures and Network Programming Proceedings
(1999).
[34] Anirudh Sivaraman, Keith Winstein, Suvinay Subramanian, and
Hari Balakrishnan. “No silver bullet: extending SDN to the data plane”. In:
HotNets (2013).
[35] Ao Tang, Jiantao Wang, Steven H Low, and Mung Chiang. “Equilibrium of
heterogeneous congestion control: Existence and uniqueness”. In: IEEE TON
(2007).
[36] David L Tennenhouse and David J Wetherall. “Towards an Active Network
[37]
Architecture”. In: DARPA Active Nets. Conf. and Exposition (2002).
Tilman Wolf and Jonathan S Turner. “Design Issues for High Performance
Active Routers”. In: IEEE Journal on Sel. Areas in Comm. (2001).
[38] David Zats, Anand Padmanabha Iyer, Randy H Katz, Ion Stoica, and
Amin Vahdat. “FastLane: An Agile Congestion Signaling Mechanism for
Improving Datacenter Performance”. In: Technical Report
UCB/EECS-2013-113 (2013).
14