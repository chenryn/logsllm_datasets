### Average of 5 Runs

| Sampling Frequency | TCP Goodput (Gb/s) | Throughput (Gb/s) |
|--------------------|--------------------|-------------------|
| 1                  | 1 flow             | 1 flow            |
| 2                  | 10 flows           | 10 flows          |
| 3                  | 20 flows           |                   |
| 4                  |                    |                   |
| 5                  |                    |                   |
| 6                  |                    |                   |
| 7                  |                    |                   |
| 8                  |                    |                   |
| 9                  |                    |                   |
| 10                 |                    |                   |

- **'First'** and **'Last'**: There are 10 TCP flows.
- **'All'**: There are as many flows as there are rules, with a minimum of 10 flows. Each rule matches on a TCP destination port.

From the results, we observe that throughput remains stable up to 10 rules. Beyond this, throughput decreases, but there is no significant difference between matching the first (best case) and last (worst case) rules in the filter chain. With 1000 flows, other overheads, such as context switches, result in much lower throughput.

### Limitations

Though Tiny Packet Programs (TPPs) are useful for a wide range of tasks discussed in §2, they are not a panacea for implementing arbitrary functionality due to two main reasons:

1. **Restricted Instruction Set**: TPPs have a limited set of instructions, which restricts their ability to perform complex computations.
2. **Restricted Programming Model**: End-hosts initiate tasks, limiting the ability to implement event-driven or precisely timed notifications.

#### Tasks Requiring Per-Packet Computation

The read and write instructions in TPPs are designed for high-throughput network updates but not for arbitrary network computations. For example, implementing active queue management schemes like Stochastic Fair Queueing, static priority, dynamic priority queueing (e.g., pFabric [2]), and fair queueing requires fine-grained control over per-packet transmit and drop schedules. Such tasks are better realized using dedicated hardware or FPGAs [34]. Similarly, TPPs are not expressive enough for deep packet inspection, which is better served by middlebox software or custom packet processors.

#### Event-Driven Tasks

In our examples, all TPPs originate at end-hosts, which limits their ability to implement tasks requiring precisely timed notifications based on state changes within the network. For instance, TPPs alone cannot implement flow control mechanisms (e.g., Priority Flow Control, PFC [15]) or reactive congestion notifications (e.g., Quantized Congestion Notification [30] and FastLane [38]). These tasks require the network to send special packets when queue occupancy reaches a certain threshold. However, end-hosts can proactively inject TPPs on a subset of packets to be quickly notified of network congestion.

### Discussion

In §2, we demonstrated how TPPs enable end-hosts to access network state with low latency, allowing them to act on this state to achieve specific functionalities. This is attractive because it enables the deployment of interesting functionalities at software-development timescales. Here, we discuss some important concerns not previously covered.

#### Handling Device Heterogeneity

There are two primary issues: instruction encoding and statistics addressing.

1. **Instruction Encoding**: Instructions are likely to be implemented using microcodes rather than hardwired logic, adding a layer of indirection for platform-specific designs.
2. **Statistics Addressing**: We recommend having two address spaces:
   - A standardized address space where important statistics are preloaded at known locations, such as those identified by the OpenFlow standard [29].
   - A platform-specific address space for additional statistics, specific to vendors and switch generations.

For multiple vendors, TPPs can support an indirect addressing scheme, allowing the compiler to preload packet memory with platform-specific addresses. For example, to load queue sizes from a Broadcom ASIC at hop 1 and an Intel ASIC at hop 2, the compiler generates the following TPP:

```plaintext
CEXEC [Switch:VendorID], [Packet:Hop[0]]
LOAD [[Packet:Hop[1]], [Packet:Hop[1]]
PacketMemory:
Hop1: $BroadcomVersionID, 0xff00 (* overwritten *)
Hop2: $IntelVersionID, 0xfe00
```

The TPP compiler can query the ASIC vendor IDs and change the addresses if the devices at a particular hop change. However, indirect addressing limits the extent to which a TPP can be statically analyzed.

#### MTU Issues

Piggy-backed TPPs are attached to packets at the edge of a network (end-host or border router). If the incoming packet is already at the MTU size, there would be no room to add a TPP. Fortunately, many switches support MTUs up to 9000 bytes, making this less of an issue. This is already being done in overlay networks to add headers for network virtualization [1].

### Related Work

TPPs represent a point in the broad design space of programmable networks, ranging from essentially arbitrary in-band programs as formulated by Active Network proposals [33, 36] to switch-centric programmable dataplane pipelines [4, 7, 17, 25] and controller-centric out-of-band proposals such as OpenFlow [27] and Simple Network Management Protocol (SNMP).

While TPPs are not fundamentally novel, they simplify the interface between end-hosts and switches to a bare minimum, striking a balance between what is possible in switch hardware at line rate and what is sufficiently expressive for end-hosts to perform various useful tasks.

TPPs superficially resemble Sprocket, the assembly language in Smart Packets [33]. However, Sprocket is more expressive, allowing loops and larger programs that are hard to realize in hardware at line rate. By contrast, a TPP is a straight-line program with deterministic, small, and known execution latency. TPPs fully execute on the fast-path (router ASIC), whereas Sprocket exercises the slow-path (router CPU), which has significantly lower bandwidth.

TPPs also resemble the read/write in-band control mechanism for ATM networks described in a patent [5], but we focus extensively on refactoring useful dataplane tasks and a security policy to safeguard the network against malicious TPPs. Wolf et al. [37] focus on designing a high-performance Active Network router that supports general-purpose instructions, but it is unclear whether their model allows end-hosts to obtain a consistent view of network state. Additionally, it is unlikely that ASICs can handle general-purpose computations at today’s switching capacities at a reasonable cost.

Out-of-band control mechanisms such as OpenFlow and SNMP do not meet the performance requirements for dataplane tasks and do not provide a packet-consistent view of network state.

### Conclusion

We aimed to rapidly introduce new dataplane functionality into the network. By presenting a programmatic interface, end-hosts can query and manipulate network state directly using tiny packet programs. TPPs support both a distributed programming model, where every end-host participates in a task (e.g., RCP* congestion control), and a logically centralized model, where a central controller can monitor and program the network. We demonstrated that TPPs enable a new breed of useful applications at end-hosts, providing unprecedented visibility and the ability to tie dataplane events to actual packets, unambiguously isolate performance issues, and act on network views without being limited by the control plane's ability to provide such state in a timely manner.

### Acknowledgments

Vimalkumar thanks Brandon Heller, Kok-Kiong Yap, Sarang Dharmapurikar, Srinivas Narayana, Vivek Seshadri, Yiannis Yiakoumis, Patrick Bosshart, Glen Gibb, Swarun Kumar, Lavanya Jose, Michael Chan, Nick McKeown, Balaji Prabhakar, and Navindra Yadav for their feedback and discussions. The authors also thank our shepherd John Wroclawski and the anonymous SIGCOMM reviewers for their thoughtful reviews. The work at Stanford was funded by NSF FIA award CNS–1040190. Opinions, findings, and conclusions do not necessarily reflect the views of NSF or other sponsors.

### References

[1] Mohammad Alizadeh, Tom Essall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Terry Lam, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. “CONGA: Distributed Congestion-Aware Load Balancing for Datacenters”. In: SIGCOMM (2014).

[2] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. “pFabric: Minimal Near-Optimal Datacenter Transport”. In: SIGCOMM (2013).

[3] Arista Networks – 7100 Series Performance Results. http://www.aristanetworks.com/media/system/pdf/7148sx-rfc2889-broadcast-with-latency.pdf, Retrieved January 23, 2014.

[4] Eric A Baden, Mohan Kalkunte, John J Dull, and Venkateshwar Buduma. Field processor for a network device. US Patent 7,787,471. 2010.

[5] A.D. Berenbaum, Alexander Gibson Fraser, and Hubert Rae McLellan Jr. In-band device configuration protocol for ATM transmission convergence devices. US Patent 08/939,746. 2001.

[6] Pat Bosshart and Glen Gibb. Personal communication, 2014-01-27.

[7] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin Izzard, Fernando Mujica, and Mark Horowitz. “Forwarding Metamorphosis: Fast Programmable Match-Action Processing in Hardware for SDN”. In: SIGCOMM (2013).

[8] Sarang Dharmapurikar. Insieme Networks, Personal communication, 2013-07-18.

[9] Nandita Dukkipati and Nick McKeown. “Why Flow-Completion Time is the Right Metric for Congestion Control”. In: SIGCOMM CCR (2006).

[10] ELAM Overview. http://www.cisco.com/c/en/us/support/docs/switches/nexus-7000-series-switches/116648-technote-product-00.html, Retrieved March 13, 2014.

[11] Dongsu Han, Robert Grandl, Aditya Akella, and Srinivasan Seshan. “FCP: a flexible transport framework for accommodating diversity”. In: SIGCOMM (2013).

[12] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, Bob Lantz, and Nick McKeown. “Reproducible network experiments using container-based emulation”. In: CoNEXT (2012).

[13] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, David Mazières, and Nick McKeown. “I Know What Your Packet Did Last Hop: Using Packet Histories to Troubleshoot Networks”. In: NSDI (2014).

[14] Danny Yuxing Huang, Kenneth Yocum, and Alex C Snoeren. “High-Fidelity Switch Models for Software-Defined Network Emulation”. In: HotSDN (2013).

[15] IEEE 802.1Qbb – Priority-based Flow Control. http://www.ieee802.org/1/pages/802.1bb.html, Retrieved April 1, 2014.

[16] Intel Fulcrum FM4000 ASIC. http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-switch-fm4000-datasheet.pdf, Retrieved July 1, 2013.

[17] Intel Fulcrum FM6000 ASIC. http://www.ethernetsummit.com/English/Collaterals/Proceedings/2013/20130404_S23_Ozdag.pdf, Retrieved July 1, 2013.

[18] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, et al. “B4: Experience with a globally-deployed software defined WAN”. In: SIGCOMM (2013).

[19] Vimalkumar Jeyakumar, Mohammad Alizadeh, Changhoon Kim, and David Mazières. “Tiny Packet Programs for low-latency network control and monitoring”. In: HotNets (2013).

[20] Dina Katabi, Mark Handley, and Charlie Rohrs. “Congestion control for high bandwidth-delay product networks”. In: SIGCOMM (2002).

[21] Peyman Kazemian, Michael Chang, Hongyi Zeng, George Varghese, Nick McKeown, and Scott Whyte. “Real Time Network Policy Checking using Header Space Analysis”. In: NSDI (2013).

[22] Frank Kelly, Gaurav Raina, and Thomas Voice. “Stability and fairness of explicit congestion control with small buffers”. In: SIGCOMM CCR (2008).

[23] Ahmed Khurshid, Xuan Zou, Wenxuan Zhou, Matthew Caesar, and P Brighten Godfrey. “VeriFlow: Verifying Network-Wide Invariants in Real Time”. In: NSDI (2013).

[24] Changhoon Kim. Windows Azure, Personal communication, 2014-01-26.

[25] Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, and M Frans Kaashoek. “The Click modular router”. In: TOCS (2000).

[26] Guohan Lu, Chuanxiong Guo, Yulong Li, Zhiqiang Zhou, Tong Yuan, Haitao Wu, Yongqiang Xiong, Rui Gao, and Yongguang Zhang. “ServerSwitch: a programmable and high performance platform for data center networks”. In: NSDI (2011).

[27] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott Shenker, and Jonathan Turner. “OpenFlow: Enabling Innovation in Campus Networks”. In: SIGCOMM CCR (2008).

[28] Millions of Little Minions: Using Packets for Low Latency Network Programming and Visibility (extended version). http://arxiv.org/abs/1405.7143. 2014.

[29] OpenFlow Switch Specification, version 1.4. https://www.opennetworking.org/images/stories/downloads/sdn-resources/onf-specifications/openflow/openflow-spec-v1.4.0.pdf, Retrieved April 1, 2014.

[30] Rong Pan, Balaji Prabhakar, and Ashvin Laxmikantha. “QCN: Quantized congestion notification”. In: IEEE802 1 (2007).

[31] Ben Pfaff, Justin Pettit, Keith Amidon, Martin Casado, Teemu Koponen, and Scott Shenker. “Extending Networking into the Virtualization Layer.” In: HotNets (2009).

[32] Mark Reitblatt, Nate Foster, Jennifer Rexford, Cole Schlesinger, and David Walker. “Abstractions for Network Update”. In: SIGCOMM (2012).

[33] Beverly Schwartz, Alden W Jackson, W Timothy Strayer, Wenyi Zhou, R Dennis Rockwell, and Craig Partridge. “Smart packets for active networks”. In: Open Architectures and Network Programming Proceedings (1999).

[34] Anirudh Sivaraman, Keith Winstein, Suvinay Subramanian, and Hari Balakrishnan. “No silver bullet: extending SDN to the data plane”. In: HotNets (2013).

[35] Ao Tang, Jiantao Wang, Steven H Low, and Mung Chiang. “Equilibrium of heterogeneous congestion control: Existence and uniqueness”. In: IEEE TON (2007).

[36] David L Tennenhouse and David J Wetherall. “Towards an Active Network Architecture”. In: DARPA Active Nets. Conf. and Exposition (2002).

[37] Tilman Wolf and Jonathan S Turner. “Design Issues for High Performance Active Routers”. In: IEEE Journal on Sel. Areas in Comm. (2001).

[38] David Zats, Anand Padmanabha Iyer, Randy H Katz, Ion Stoica, and Amin Vahdat. “FastLane: An Agile Congestion Signaling Mechanism for Improving Datacenter Performance”. In: Technical Report UCB/EECS-2013-113 (2013).