O 
D 
0 
0 
I 
B 
1 
1 
0 
O 
B 
1 
0 
O 
C 
0 
I 
C 
3 
I 
D 
0 
1 
3 
O 
D 
1 
1 
I 
B 
1 
0 
O 
B 
1 
O 
C 
3 
0 
I 
C 
3 
I 
D 
0 
O 
D 
0 
Src:S 
O 
C 
3 
0 
I 
C 
0 
3 
I 
D 
0 
O 
D 
0 
I 
B 
1 
O 
C 
3 
I 
D 
1 
0 
O 
B 
1 
0 
0 
I 
C 
0 
0 
1 
3 
O 
D 
0 
Dst:T 
Dst:U 
Src:T 
Src:S 
Dst:U 
(b) Abstract representation for the control plane (ARC): it contains
one digraph for every pair of source and destination subnets; ver-
tices correspond to routing processes; edges represent the possible
ﬂow of trafﬁc enabled by the exchange of routing information be-
tween the connected processes
Figure 2: Example network with three endpoint groups and
three routers participating in a single OSPF instance
straction that operates at a higher level than today’s control
plane veriﬁers and enables more direct analysis.
3. A NEW ABSTRACTION: ARC
To avoid modeling a network’s data plane, which depends
on the current state of network links, and instead analyze the
network at a higher-level, we present an abstract represen-
tation for control planes (ARC).
A network’s ARC is a data structure that contains a collec-
tion of weighted digraphs, one for each “trafﬁc class”, i.e.,
a source-destination subnet pair. As an example, Figure 2b
shows the ARC for the simple control plane in Figure 2a;
the ARC has six graphs: one for each possible combina-
tion of source and destination subnets. Each digraph models
the behavior of the routing instances/protocols in the control
plane, and the interactions among them, with respect to the
corresponding trafﬁc class. Vertices correspond to routing
processes—each has an in (I) and out (O) vertex for reasons
described in §4.2. Directed edges represent the possible ﬂow
of data trafﬁc enabled by the exchange of routing informa-
tion between the connected processes.
For an ARC to be useful for veriﬁcation and equivalence
checking, its constituent digraphs must satisfy two key at-
tributes: pathset-equivalence and path-equivalence. We de-
scribe these next.
is pathset-equivalent, because it encodes all possible and
no impossible forwarding behaviors, respectively. In §4.2,
we describe how to construct provably pathset-equivalent
graphs for networks that use OSPF, RIP, eBGP, static routes,
ACLs, route ﬁlters, and route redistribution.
One of the main beneﬁts of a pathset-equivalent ARC is
that verifying invariants I1–I4 in Table 1 for arbitrary link
failures boils down to checking simple graph attributes. For
example, suppose we want to verify that “subnet T can never
send trafﬁc to subnets S or U under any link failures” in the
network shown in Figure 2a. Assuming the graphs in Fig-
ure 2b are pathset-equivalent, this can be done by checking
if T and S (or T and U ) are in separate connected com-
ponents of the graphs for the corresponding trafﬁc classes
(center graphs in Figure 2b). Because T and U are in the
same connected component in the lower-center graph, there
is some link failure scenario where the invariant is violated
and T can send trafﬁc to U (e.g., when the B–D link fails).
Path-equivalent graphs. To aid operators in debugging vi-
olations, and allow for fast equivalence testing, the edge
weights in each digraph are assigned such that, after remov-
ing edges corresponding to failed links, the min-cost path
in the digraph between the source and destination vertices
is the exact path taken in the real network. We say such a
graph is path-equivalent, because it encodes the network’s
actual forwarding behavior under arbitrary link failures.
For example, when there are no link failures in the net-
work in Figure 2a, trafﬁc from S to U takes the path S →
B → C → U , which is the min-cost path in the lower-right
graph in Figure 2b. When the B − C link fails, the actual
and min-cost path is S → B → D → C → U . While in this
example, edge weights are the same as OSPF cost metrics,
in a real ARC the weights are a function of the relative rank
of speciﬁc routing protocols, AS paths, and network links.
In §4.3, we describe how to construct provably path-equiv-
alent graphs for networks under some restrictions, i.e., the
route redistribution policy is acyclic and the costs assigned
to redistributed routes are congruent with each process’s ad-
ministrative distance (AD).
When the digraph is path-equivalent, we can produce all
min-cost paths from T to U as counter-examples to the afore-
mentioned invariant. The operator can use this to add the
missing ACL to C and prevent T and U from ever commu-
nicating. Additionally, we can check the equivalence of two
control planes by directly comparing the graphs contained
in their ARC. If each graph in each control plane’s ARC has
the same vertices and edges, and the edge weights are pro-
portional, then the control planes are equivalent.2
The main challenge in constructing ARCs is determining
the appropriate vertices, edges, and weights to use for the
graphs to be pathset- and path-equivalent.
Pathset-equivalent graphs. Each digraph in an ARC is
constructed such that it contains every path between the source
and destination endpoints that is used in the real network,
and does not contain any path that is infeasible in the real
network, under arbitrary failures. We say such a digraph
2An equivalent ordering of edges by weight does not guar-
antee the ordering of paths is the same: e.g., changing the
weight of the B–C edge in Figure 2a to 2.5 results in the
same ordering of edges but causes the path D → C to be
preferred over the path D → B → C.
303
4. GENERATING A NETWORK’S ARC
We start by discussing the practical challenges in design-
ing the ARC. Then, we motivate and present our approach
for constructing a pathset-equivalent ARC. Finally, we de-
scribe our algorithms for deriving edge weights to get a path-
equivalent ARC.
For simplicity, we focus on networks that use OSPF, RIP,
eBGP, static routes, AD-based route selection, route redistri-
bution, data plane ACLs, and/or route ﬁlters. These are the
constructs we ﬁnd in our campus network and hundreds of
data center networks operated by a large OSP (§7.1). How-
ever, other protocols (e.g., EIGRP) can be accommodated
through extensions to our algorithms.
4.1 Opportunities and Challenges
Modeling the collective behavior of multiple routing in-
stances in a series of weighted digraphs in the ARC is en-
abled by the fact that most routing protocols in use today
employ a cost-based path selection algorithm. For exam-
ple, OSPF uses Dijkstra’s algorithm to compute min-cost
paths from a source to all destinations; RIP computes short-
est paths using the Bellman-Ford algorithm. If multiple min-
cost paths are available and ECMP is enabled, then trafﬁc
is evenly divided among the paths using multi-path routing.
BGP associates cost labels with paths based on numeric met-
rics: e.g., operator-deﬁned local preference, path length, and
multi-exit discriminator (MED) [7, 14]. These have similar
properties to link costs used in IGPs, except BGP costs are
per-path rather than per-link.
While these similarities allow us to use weighted digraphs
to model routing behavior, differences between protocols in-
troduce at least two challenges:
1. In the actual control plane, interior and exterior gateway
protocols (IGPs and EGPs, respectively) compute routes
at different granularities. An IGP treats each router as
a node, while an EGP views each AS as a node. For-
tunately, enterprise and data center networks tend to use
EGPs in restricted ways (§4.3.3) that mirror IGPs’ view.
2. Each routing protocol uses a different currency for ex-
pressing link and path costs/preferences: e.g., a link with
an OSPF cost of 1 may be less desirable than an AS path
whose local preference is 1, or vice versa. Thus, we
cannot directly add or compare costs between protocols.
However, in real network control planes, redistributed
routes are assigned ﬁxed costs (§4.3.4); this masks the
costs used in other routing instances and provides an av-
enue for reconciling differences in currency.
There are other subtle aspects of network routing that also
impact our modeling:
• Trafﬁc-class-speciﬁc policies. Only certain classes of
trafﬁc are blocked by data plan ACLs and route ﬁlters.
• Redistribution of routes between routing instances. A
routing process may advertise routes computed by an-
other routing instance, allowing trafﬁc to traverse a path
composed of segments selected by different protocols.
• Selection of routes based on AD. When multiple routing
processes on the same device identify a route to a desti-
nation, only the route from the process with the lowest
administrative distance (AD) is installed in the device’s
global routing information base (RIB) [19].
We next describe how we select ARC vertices, edges, and
weights to accommodate the above issues.
4.2 ARC Vertices and Edges
A network’s physical topology may seem like a natural
starting point for the ARC’s graphs. By having a vertex for
each router and an edge for each physical link, we can as-
sign edge weights based on the per-interface cost metrics
deﬁned for IGPs (e.g., OSPF and RIP) and the AS prefer-
ences deﬁned for BGP. However, this is too coarse to express
route selection and redistribution policies between routing
processes running on the same device.
4.2.1 Extended Topology Graph
To accommodate these features, we introduce an abstrac-
tion we call an extended topology graph (ETG). Figure 3
shows the ETG for the example control plane depicted in
Figure 1b. Vertices in the ETG correspond to individual
routing processes.3 Directed edges represent inter- and intra-
device communication paths between routing processes, in-
cluding: hardware paths—a single physical link or multiple
physical links that form a layer-2 network—and software
paths—inter-process communication channels used to ex-
change information between processes on the same device.
Some aspects of a network’s control plane only apply to
speciﬁc trafﬁc classes: e.g., data plane ACLs, route ﬁlters,
and static routes. To accommodate these features, an ARC
includes a customized ETG for each trafﬁc class. As men-
tioned earlier, a trafﬁc class represents the set of trafﬁc ﬂow-
ing from one endpoint group—a set of related hosts, sub-
nets, etc.—to another. We use the network preﬁxes in device
conﬁgurations, including preﬁxes assigned to interfaces, ad-
vertised by routing processes, and referenced in ACLs, as
the basis for determining a network’s endpoint groups. Be-
cause some preﬁxes may overlap, we use standard ﬁrewall
rule optimization algorithms [10] to compute a set of non-
overlapping preﬁxes. We generate a list of trafﬁc classes by
enumerating all possible pairings of preﬁxes.
Modeling forwarding behavior at the level of routing pro-
cesses results in an ARC that is not protocol-independent.
This model is nevertheless useful to answer control plane
veriﬁcation questions. In §5.2, we show, under restricted as-
sumptions, how to transform an ETG from a process-based
to an interface-based model, resulting in a protocol-independent
ARC that can be useful for equivalence testing.
4.2.2 Constructing ETGs
We now describe how to construct ETGs from device con-
ﬁgurations. The complexity of constructing an ETG is
O(maxr |Ir |2 + maxr |Ir | ∗ maxi |Ri |), where maxr |Ir |
is the maximum number of routing processes running on a
single device, and maxi |Ri | is the maximum number of de-
vices participating in a single routing instance.
3Static routes are also viewed as a routing process.
304
4 
0.0096 
0.5 
5 
0.0024 
B.2O
1 
C.2I
B.3I
0.0024 
1 
0.0096 
0 
0.0024 
0 
0 
1 
0.1 
A.2O
4 
B.2I
1 
C.2O
B.3O
Y.3O
0 
A.2I
0 
1 
0.0024 
DST:T 
1 
0.00041 
0.0024 
1 
0.1 
1 
0 
1 
0.1 
0.0024 
D.2I
Y.3I
1 
0 
1 
0.0024 
0.1 
1 
0.0024 
1 
D.2O
OSPF2 
Z.3I
0 
Z.3O
0 
Intra-device, 
inter-instance 
0 
A.0I
0 
A.0O
D.1I
1 
E.1O
1 
F.1I
1 
Z.1O
Intra-device, 
intra-instance 
0 
0 
0 
BGP1 
0 
Inter-device 
D.1O
1 
E.1I
1 
F.1O
1 
Z.1I
100 
100 
0.041 
0.041 
H.0I
0 
100 
0.041 
G.0O
100 
0.041 
0 
0.041 
0.041 
E.0I
0 
1.25 
0 
SRC:S 
H.0O
100 
G.0I
100 
E.0O
OSPF0 
Figure 3: ETG for the control plane in Figure 1b (sans
ACLs) for the S → T trafﬁc class:
light shaded regions
indicate routing instances; the structure and weights are the
same for all trafﬁc classes, with the exception of endpoint
edges, edges removed due to ACLs, and static route ver-
tices; weights in dashed boxes are assigned by our scaling
algorithm to model route redistribution and selection
Vertices. The ETG contains two vertices (in and out) for
each routing process, including static routes. For example,
the processes on routers B, Y , and Z for routing instance
OSPF3 in Figure 1b are represented by vertices B.3I, B.3O,
Y.3I, Y.3O, Z.3I, and Z.3O in Figure 3. We use two ver-
tices per process in order to accommodate route selection
and redistribution (described in detail below). We identify
a network’s routing processes from the router stanzas in
device conﬁgurations [21].
We also add special source and destination vertices (SRC
and DST, respectively) to the ETG to represent the source
and destination endpoints associated with the trafﬁc class.
Inter-device edges. The out vertex for a routing process
on one device is connected to the in vertex for a process
on another device if: (1) the two devices are connected by a
(sequence of) physical link(s),4 and (2) the routing processes
participate in the same routing instance. Such an inter-device
edge thus represents two things. First, it represents the di-
rect exchange of routing information (e.g., link-state updates
or AS-level path advertisements) within a routing instance.
Second, it represents a possible physical path over which
data trafﬁc may be forwarded due to the RIB entries result-
ing from the aforementioned exchange of routing informa-
tion. Inter-device edges always go from an out vertex to an
in vertex and point in the direction data trafﬁc ﬂows, which
is the inverse of the direction routing information ﬂows.
For example, in the network shown in Figure 1b, the
4We assume two devices are connected if they each have an
interface that participates in the same subnet [21].
305
BGP1 routing process on router E may compute a route to
the subnet S via router F as a result of routing information