depicted in Table 3, where CMX denotes the results for MX.
Although the boundaries were derived from the 80th and
95th percentile, the results (especially for M2) are not evenly
distributed. This stems from the fact that the boundaries
are a ﬁxed value which denotes that at least 80% or 95% of
M1
M2
M3
M4
M5 R1, R2 R3, R4
MC HC
≤ 22
>22
≤ 10
>10
3
>3
≤ 394 >394
R5
LC
≤ 9
≤ 4
≤ 2
≤ 75
Table 2: Classiﬁcation boundaries
05101520250%20%40%60%80%100%0510150%20%40%60%80%100%1426CM1
CM2
CM3
CM4
CM5
Combined
LC MC
HC
1,079
1,161
1,035
920
1,094
134
85
178
179
120
60
27
60
51
59
813
199
63.9% 20.5% 15.6%
261
Table 3: Classiﬁcation by applied metrics
the ﬂows had a lower ranking, not necessarily exactly that
number. Note also that M4 can only be applied to subset
of 1.150 of the ﬂows in our data set, as only in these cases
source and sink access were contained within the same ﬁle.
By design, each of our metrics assigns at least 80% of the
ﬂaws to lowest complexity class. The results of the combina-
tion of all metrics is also shown in Table 3: we observe that
in combining the result, less than two thirds of the ﬂows are
categorized as having an overall low complexity, i.e., that
for each metric their value was below the 80th percentile.
This highlights the fact that while a ﬂow might be simple in
terms of a single metric, ﬂows are actually more complex if
all metrics are evaluated, putting emphasis on the diﬀerent
proposed metrics.
5.5 Summary of Our Findings
In summary, by grouping the results from each of the met-
rics, separated by the 80th and 95th percentile, and combin-
ing the resulting classiﬁcations into either low, medium or
high complexity, about two thirds of our data set is still la-
beled as having a low complexity, whereas 20% and 15% of
the ﬂows are labeled as having a medium or high complex-
ity, respectively. This shows that taking into account only
a single metric is not suﬃcient to ascertain the complexity
of a vulnerability, but that rather all dimensions must be
analyzed. Given the large fraction of simple ﬂows, which
consist of at most nine operations on the tainted data (in-
cluding source and sink access) and span no more than two
contexts, we ascertain that Client-Side Cross-Site Scripting
is often caused by developers who are unaware of the risks
and pitfalls of using attacker-controllable data in an unﬁl-
tered manner.
Although the fraction of straight-forward vulnerabilities
is very high, the unawareness of developers is only one root
cause of Client-Side Cross-Site Scripting. As this section has
shown, developers may also be overwhelmed by the sheer
amount code they have to understand, potentially passing
ﬁrst- and third-party code on the way. In addition, we found
273 cases in which the vulnerability was contained strictly
inside third-party code and, thus, the developer of the then
vulnerable application was not at fault.
5.6 Comparison to Randomly-Sampled Flows
From our data set, we gathered a large number of ﬂows
which appeared to be vulnerable, but did not trigger our
payload. We can, however, not state with certainty that
these ﬂows were indeed secure. For instance, the applied
ﬁltering might be incomplete, a condition that is not covered
by our exploit generator. Therefore, to put the results of
our study into perspective, we randomly sampled 1.273 ﬂows
80th
≤ 20
≤ 9
≤ 2
95th
≤ 44
≤ 19
3
100th
M1
M2
M3
M4
M5 R1, R2, R3
>44
>19
>3
≤ 189 ≤ 1,208 >1,208
R5
R4
Table 4: Percentiles for randomly-sampled ﬂows
from an attacker-controllable sink to a direct execution sink.
Table 4 shows the results for the percentiles of these ﬂows.
Interestingly, the percentile values are higher for each of the
metrics compared to vulnerable ﬂows. This shows that the
complexity of such ﬂows alone can not be the causing factor
for the widespread occurrence of client-side XSS.
6. ADDITIONAL INSIGHTS
Our analysis so far uncovered that the biggest fraction
of vulnerabilities are caused by programming errors which,
according to the results of our metrics, should be easy to
spot and correct. We conducted a more detailed analysis
into several low complexity ﬂaws as well as those ﬂaws which
were ranked as having a high complexity and found a number
of interesting cases, which shed additional light on the issues
that cause client-side XSS. Therefore, in the following, we
highlight four diﬀerent insights we gained in our analysis.
6.1 Involving Third Parties
In our analysis, we found that vulnerabilities were also
caused when involving code from third parties, either be-
cause ﬁrst- and third-party code were incompatible or be-
cause a vulnerable library introduced a ﬂaw.
Incompatible First- and Third-Party Code: A more
complex vulnerability, which was rated as being of medium
complexity for M3 and high complexity by M5, utilized meta
tags as a temporary sink/source. Listing 2 shows the code,
which extracts the URL fragment and stores it into a newly
created meta element called keywords. Since this code was
found in an inline script, we believe that it was put there
with intend by the page’s programmer.
var parts = window.location.href.split("#");
if (parts.length > 1) {
var kw = decodeURIComponent(parts.pop());
var meta = document.createElement(’meta’);
meta.setAttribute(’name’, ’keywords’);
meta.setAttribute(’content’, kw);
document.head.appendChild(meta);
}
Listing 2: Creating meta tags using JavaScript
This page also included a third-party script, which for the
most part consisted of the code shown in Listing 3. This code
extracts data from the meta tag and uses it to construct a
URL to advertisement. In this case, however, this data is
attacker-controllable (originating from the URL fragment)
and thus this constitutes a client-side XSS vulnerability.
This code is an example for a vulnerability which is caused
by the combination of two independent snippets, highlight-
ing the fact that the combined use of own and third-party
code can signiﬁcantly increase complexity and the potential
1427for an exploitable data ﬂow. In this concrete case, the Web
application’s programmer wanted to utilize the dynamic na-
ture of the DOM to generate keywords from user input, while
the third-party code provider reckoned that meta tags would
only be controllable by the site owner.
function getKwds() {
var th_metadata = document.getElementsByTagName("meta");
...
}
var kwds = getKwds();
document.write(’’);
Listing 3: Third-party code extracting previously set meta
tags
Vulnerable Libraries: An example for a vulnerability
which was rated as being of low complexity is related to
a vulnerable version of jQuery. The popular library jQuery
provides a programmer with the $ selector to ease the access
to a number of functions inside jQuery, such as the selection
by id (using the # tag) as well as the generation of a new
element in the DOM when passing HTML content to it. Up
until version 1.9.0b1 of jQuery, this selector was vulnerable
to client-side XSS attacks [10], if attacker-controllable con-
tent was passed to the function—even if a # tag was hard-
coded in at the beginning of that string. Listing 4 shows
an example of such a scenario, where the intended use case
is to call the fadeIn function for a section whose name is
provided via the hash. This ﬂaw could be exploited by an
attacker by simply putting his payload into the hash.
var section = location.href.slice(1);
$("#" + section + "_section").fadeIn();
Listing 4: Vulnerable code if used with jQuery before 1.9.0b1
In our study, we found that 25 vulnerabilities were caused
by this bug, although the vulnerability had been ﬁxed for
over three years at time of writing this paper. In total, we
discovered that 472 of the exploited Web pages contained
outdated and vulnerable versions of jQuery, albeit only a
fraction contained calls to the vulnerable functions. jQuery
was accompanied by a number of vulnerable plugins, such
as jquery-ui-autocomplete (92 times). Second to jQuery
came the YUI library, of which vulnerable versions were
included in 39 exploited documents. This highlights that
programmers should regularly check third-party libraries for
security updates or only include the latest version of the li-
brary into their pages.
6.2 Erroneous Patterns
Apart from the involvement of third-party code which
caused vulnerabilities, we found two additional patterns that
highlight issues related to client-side XSS, namely the im-
proper usage of browser-provided APIs and the explicit de-
coding of user-provided data.
Improper API Usage: In our data set, we found a vul-
nerability in the snippet shown in Listing 5, which was as-
signed the lowest complexity score by any of our metrics. In
this case, the user-provided data is passed to the outlined
function, which apparently aims at removing all script tags
inside this data. The author of this snippet, however, made
a grave error. Even though the newly created div element
is not yet attached to the DOM, assigning innerHTML will
invoke the HTML parser. While any script tag is not exe-
cuted when passed to innerHTML [9], the attacker can pass a
payload containing an img with an error handler [15]. The
HTML parser will subsequently try to download the refer-
enced image and in the case of a failure, will execute the
attacker-provided JavaScript code. While the eﬀort by the
programmer is commendable, this ﬁltering function ended
up being a vulnerability by itself. Next to this ﬂaw, we
found examples of the improper use of built-in functions,
such as parseInt and replace.
function escapeHtml(s) {
var div = document.createElement(’div’);
div.innerHTML = s;
var scripts = div.getElementsByTagName(’script’);
for (var i = 0; i < scripts.length; ++i) {
scripts[i].parentNode.removeChild(scripts[i]);
}
return div.innerHtml;
};
Listing 5: Improper use of innerHTML for sanitization
Explicit Decoding of Otherwise Safe Data: As out-
lined in Section 4.3, the automatic encoding behavior of
data retrieved from the document.location source varies
between browsers: Firefox will automatically escape all com-
ponents of the URL, while Chrome does not encode the frag-
ment, and IE does not encode any parts of the URL. In con-
sequence, some insecure data ﬂows may not be exploitable
in all browsers, with Firefox being the least susceptible of
the three, thanks to its automatic encoding.
The data set underlying our study was validated to be
exploitable if Chrome’s escaping behavior is present, which
leaves the fragment portion of the URL unaltered. Nev-
ertheless, we wanted to investigate how many vulnerabili-
ties would actually work in any browser, i.e., in how many
cases data was intentionally decoded before use in a security-
sensitive sink. Using an unmodiﬁed version of Firefox, we
crawled the persisted vulnerabilities again and found that
109 URLs still triggered our payload. This highlights the
fact that programmers are aware of such automatic encod-
ing, but simply decode user-provided data for convenience
without being aware of the security implications.
6.3 Summary of Our Insights
In summary, we ﬁnd that although a large fraction of
all vulnerabilities are proverbial facepalms, developers are
often confronted with additional obstacles even in cases,
where the complexity is relatively low.
In our work, we