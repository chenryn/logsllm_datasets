86.9
99.9
98.16
3.4.3 Confidence of the Combined Predictions. Last but not least,
we focus on the JavaScript samples for which all three of our com-
bined modules made the same predictions, and on the contrary,
those for which they had different classification results. On aver-
age, ngrams AST, value tokens and value PDG labeled 234,875.8
JavaScript inputs the same way (92.76%). On these samples specif-
ically, they have both an extremely high TPR of 99.55% and TNR
of 99.9% (standing for an overall detection accuracy of 99.73%, Ta-
ble 6), meaning that their predictions are more trustworthy than on
the whole dataset, which we expected since the modules perform
better combined than separately. Finally, we classified the remain-
ing 18,340.2 samples (over 80% of which are benign), which can
also be seen as samples that may try to evade detection (similarly
to Section 3.3.4). Still, we retain a high TNR of 98.16% with the
majority voting system (Table 6) on such samples, meaning that
we accurately detect malicious JavaScript over 98% of the time.
In turn, we have a TPR of 86.9%. All in all, we retain over 96%
accuracy on samples for which our combined detectors predict
conflicting labels, which we consider to still be relatively high.8
Nevertheless, the overall detection accuracy of JStap should not
be evaluated only on such samples, but on our whole dataset (also
containing these samples), where we retain an accuracy of almost
99.5% (Section 3.4.2).
3.5 Run-Time Performance
We tested JStap’s run-time performance on several CPUs, each
Intel(R) Xeon(R) Platinum 8160 CPU at 2.10GHz. Even though we
parallelized our implementation to generate the results for this
paper, the run-time of our system was tested on one CPU only.
Table 7 presents the average, median, minimum, and maximum
duration to generate each of our considered code representations.
The tokenizing and parsing with Esprima [23] are relatively fast,
with an average time of respectively 17 and 35 ms per file. The most
time-consuming operation is the PDG generation, which highly
depends on the AST size, since we have to traverse it, pushing and
popping the variables encountered all the way down to the leaves.
Once we generated all the PDGs of all the files from our dataset,
we stored them so as not to have to produce them for each module
again. Therefore, we did not take into consideration the PDGs (and
tokens, for comparison purpose) generation time in Table 8.
This table presents the duration times to generate the features
considered by each module, for one file. The last two columns
stand for the run-time to leverage the previous features to build a
model (averaged for one file) and to classify one unknown input.
In overall, more complicated code representations (e.g., PDG, CFG
compared to tokens or AST) lead to a higher overhead, since we
follow more edges in the graphs and consider more features. The
value approach also is slower than the ngrams one, as we fetch
8We further discuss this point in Section 4.3
Code representations Mean (ms) Median (ms) Min (ms) Max (s)
0.175
Tokenizer
Parser
0.311
4.103
AST from parser
1.114
CFG from AST
PDG from CFG
27.27
9.0
19.0
11.487
4.635
8.71
16.894
34.921
97.711
39.085
369.49
0.0
1.0
0.038
0.004
0.125
Table 8: JStap’s run-time per module
Modules
ngrams_tokens
ngrams_ast
ngrams_cfg
ngrams_pdg-dfg
ngrams_pdg
value_tokens
value_ast
value_cfg
value_pdg-dfg
value_pdg
Mean (ms) Median (ms) Min (ms) Max (s)
0.203
0.722
0.778
1.111
1.243
1.397
86.619
207.255
107.432
247.253
2.344
9.683
18.288
19.412
34.745
13.251
112.036
129.77
101.83
216.895
1.42
2.592
3.781
3.736
5.544
3.743
11.131
12.138
9.707
21.44
0.65
0.635
0.762
0.723
0.799
0.947
1.085
0.875
0.99
1.003
Learner (ms) Classifier (ms)
0.715
1.427
1.667
2.685
2.763
1.127
1.37
1.174
1.279
1.311
0.162
0.19
0.252
0.228
0.241
0.187
0.227
0.187
0.195
0.173
a value for each unit, thereby traversing sub-ASTs down to the
leaves.
Specifically, classifying a JavaScript sample with the ngrams
tokens module takes on average 19 ms for the features generation
(including tokens production) and, 0.71 ms for the classification. For
the value AST-based approach, it takes 112 ms to produce features,
with an AST previously generated, and 1.4 ms for the classification.
Based on the number of features each module considers (Table 2)
and an average size of 23 KB per file, we consider the overhead to be
reasonable. Also, JStap is fully parallelized to leverage all available
CPU cores for a faster analysis for a deployment in the wild.
4 DISCUSSION
In this section, we first analyze the limitations JStap might have,
focussing on the static analysis of JavaScript. We then discuss tech-
niques that might evade our system in theory but are not specifically
used in practice. Finally, we introduce new strategies to classify
more JavaScript instances accurately.
4.1 Limitations
JStap is based on a static analysis of JavaScript to build both the
control and data flow of a given script. Therefore, it provides a
complete code coverage based on the proportion of source code
analyzed. In turn, it is subject to the traditional flaws induced by the
high dynamic of the language [1, 19, 25, 26]. Specifically, JavaScript
models inheritance with prototype chaining [38], where properties
can be added or removed during the execution, and property names
may be dynamically computed. Also, JavaScript can generate code
at run-time, e.g., with the eval function, a dynamically constructed
string can be interpreted as a program fragment and executed in
the current scope. Still, to partially mitigate this limitation, we au-
tomatically (and statically) rewrote eval calls to a string into the
corresponding code that would have been generated dynamically.
This way, we increased JStap’s code coverage by not merely con-
sidering a CallExpression node anymore, but the actual content
of the string along with possible control/data flow (Section 3.1.1).
JStap: A Static Pre-Filter for Malicious JavaScript Detection
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Still, our approach would not work on eval calls with a variable
as parameter or nested evals. Nevertheless, JStap aims at working
directly at the code level to detect malicious JavaScript, by analyzing
the traces left in the syntax of malicious files, e.g., due to the specific
malicious obfuscation techniques used by attackers. As long as all
the code is not dynamically generated, which we encountered only
for conditional compilation and solved by automatically generating
the actual code (Section 3.1.1), JStap will leverage the existing code
to classify the JavaScript inputs considered.
4.2 Evasion Techniques
All learning-based malware detection systems will fail to detect
some attacks, e.g., if the considered malicious instances do not con-
tain any features present in the training set, given that machine
learning relies on statistical assumptions about the distribution of
the training data to classify unknown inputs [4]. Therefore, ad-
versaries could modify their malicious samples by adding benign
features (not to mention copy their malicious file into a signifi-
cantly bigger benign one), to statistically increase the proportion of
benign features in a malicious file and have a more benign-looking
structure [28]. This way, they would mislead our detector into clas-
sifying the considered sample as benign. Even though related work
effectively added, deleted or replaced specific features of a given
file [13, 22, 51, 55] and injected malicious content into bigger be-
nign samples [37], we observed very few such samples. Specifically
and out of the 19,942 malicious samples we manually analyzed, we
found such evasion techniques less than 50 times. For this reason,
we believe that malicious actors rather use obfuscation to hide their
attack. The rarity of such malicious samples could also be a limita-
tion of our dataset. In this case, it would mean that our malware
providers did not detect these samples, which should not happen
after a dynamic analysis.
Another class of attacks against JStap are samples with the
same structure but different ground truths. In particular, Fass et
al. showed with HideNoSeek [16] that malicious samples can be
rewritten, so that they have exactly the same AST as an existing
benign file. Yet, because their variables have different values, they
perform distinct actions after execution. While this attack would by
construction impact our token- and AST-based modules, we believe
that our PDG-DFG module might be able to recognize such samples,
because of the specific changes induced by the attack at the data
flow level.
Nevertheless, as presented in Section 3.4, our system makes more
accurate predictions when we combine the labels given by several
modules. This fact also holds for samples which might be trying
to evade detection (for example when several modules classified
them differently). In this specific case, we suggest to use JStap as a
pre-filtering system before sending samples for which the modules
predicted conflicting labels to more costly dynamic components
(Section 4.3). This way, JStap is more resilient to evasive samples
than any of its modules alone.
4.3 Improving the Detection With Pre-Filtering
Layers
To detect malicious JavaScript, we specifically chose a static ap-
proach, which is by construction fast, while still making accurate
predictions. Dynamic detectors may perform better, in particular, if
they visit all possible execution paths [30, 31], but at the same time,
they are more costly, e.g., they require specific instrumentations,
they introduce overhead inherently depending on the code’s exe-
cution, also the necessary amount of time to observe a malicious
behavior is not defined [51]. Besides, such analyses can be defeated
if the sample notices that it is running in a sandboxed environ-
ment [6, 7]. To this end and to maximize the detection accuracy
while at the same time minimizing the run-time performance, we
rather envision that JStap could be used to pre-filter JavaScript
samples, sending, e.g., only those with conflicting labels to much
slower dynamic components. In the context of Section 3.4.3, the
18,340.2 samples (on average) for which ngrams AST, value tokens
and value PDG made different predictions, could be sent to such
components. For this purpose, we could also consider a second
pre-filtering layer to limit the number of inputs to be executed in a
sandboxed environment. Similar to the combination of Cujo [45],
JaSt [17] and Zozzle [12], we combined ngrams tokens, ngrams
AST and value AST to classify the previous 18,340.2 samples. These
three detectors predicted the same labels for 16,469.4 of them, with
an accuracy over 99%, meaning that only the resulting 1,870.8 could
be sent to dynamic components. Naturally, it depends on the reli-
ability a user would like to have. Still, out of our 253,216 sample
set, JStap correctly classified 234,875.8 instances (92.76%) with an
accuracy of 99.73% in a first pre-filtering step (Section 3.4.3). Then,
it correctly labeled 16,469.4 additional samples (6.5% of the ini-
tial dataset) with an accuracy over 99% in a second pre-filtering
step, meaning that only 0.74% of the original dataset would be
outsourced to more costly components, while having a detection
accuracy significantly over 99% for the majority of the considered