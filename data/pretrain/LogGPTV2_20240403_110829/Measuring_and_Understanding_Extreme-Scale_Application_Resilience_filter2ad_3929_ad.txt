U
S
Y
S
Resiliency data 
data (6 fields) 
270,166,713
Tot. Entries
e
c
n
e
u
q
e
S
G = GROUP
C = CATEGORY
g
a
T
r
o
r
r
i
t
t
r
a
t
S
i
t
t
r
a
t
S
… 
… 
E
E
/
Workload data 
jobs/apps data 
(30 fields) 
Size dataset: 5,116,766 (Num. application runs) x 213 fields  [6GB] 
Fig. 3. LogDiver: (a) data processing workﬂow, (b) extracted error templates and count of entries, (c) schema of the main output.
A message template is a combination of a ﬁxed text portion
and a variable text portion in each raw log entry. The ﬁxed
text indicates the type of event that generated the syslog entry;
the variable text contains the information speciﬁc to the event
logged. For example, “* client was evicted by * in *” is ﬁxed
text, and “NODE ID client was evicted by LUSTRE OST
NODE ID in PARTITION ID” is variable text (see 3.(a)).
Here LUSTRE OST NODE ID, and PARTITION ID specify
respectively the id of the node evicted, the evicting Lustre node,
and the partition to which the eviction refers. We equipped
LogDiver with a set of functionalities to identify all the ﬁxed
and variable items in the logs and to substitute the variable
items (such as IP, MAC, node id, hardware address, dates, user
names, and numbers) with wildcard symbols in the ﬁxed text
(such as the * in the example in Figure 3.(a)).
The categorization consists of assigning a speciﬁc unique
numerical template ID, tag, category, and group to each er-
ror template. The tag is a textual description of the event
of interest (e.g., GPU DOUBLE BIT EXCEPTION or LUS-
TRE CLIENT EVICT), the category refers to the subsystem
generating the event (i.e., NVIDIA GPU or LUSTRE), and
the group corresponds to the type of the subsystem involved
in the event (e.g., NODE HW or STORAGE). For instance,
we assigned the tag CPU MACHINE CHECK and category
NODE HW to the template with ID 56724 (see the bottom left
corner in Figure 3.(a)).
That step is the only semi-automated part of the tool; the
other steps are fully automated. The categorization of error
templates requires interactions with technical personnel for val-
idation purposes. At the end of this step, all the entries matching
the obtained templates identiﬁed at step 2 are retrieved from
the data and tagged according to the tag, category, and group
in the template list.At the end of step 2, LogDiver identiﬁed
22,082 different templates. Those entries were further reduced
using a set of 92 keywords identiﬁed by Blue Waters personnel,
to 5,127. These we manually screened and ﬁnally reduced to
398 templates of error entries generated by events potentially
affecting system and user operations.
Figure 3.(b) shows the classiﬁcation of the error messages
obtained from the considered datasets in Blue Waters. We cat-
egorized the 398 templates into 148 error tags (i.e., error types)
generated by 10 error categories (relating the error tag to the
speciﬁc involved subsystem, e.g., Gemini), further categorized
into ﬁve error groups: i) Network, which includes Gemini (e.g.,
routing or Gemini hardware errors) and LNet errors (e.g., packet
dropped or endpoint shutdown), ii) Scheduling, which includes
errors encountered by Moab/Torque (e.g., impossibility of allo-
cating/deallocating resources for a job) and ALPS (e.g., crash
of the ALPS processes), iii) Resiliency Architecture, which
encompasses errors detected by the resiliency mechanisms in
Blue Waters, including errors generated by the GPU, memory,
and processor (e.g., GPU MMU errors and node warm-swap
failure), iv) System Software, which includes errors generated
by the Cray software, including the services and OS, and v)
Storage, which includes errors generated by Lustre (e.g., client
eviction) and the Sonexion cluster (e.g., I/O errors).
To reduce the ﬁltering time, LogDiver is equipped with a
data-parallelization framework that i) splits the input data set
into smaller sets, ii) produces a parallel batch script assigning
iii) orchestrates the
batches of 32-64 data ﬁles per node,
2929
submission of a job to the target computing system, and iv)
collects and merges the results gathered by the nodes, creating
a new, single data set.
Step 3: Workload Consolidation. The required application
data (i.e., executed aprun commands, see Figure 1) are scattered
over several nonconsecutive entries in the ALPS logs and need
to be retrieved and assembled for each user application. The
output of the step is an extended data set of user applica-
tions (referred to as “application data” in Figure 3.(c)), which
contains one entry for each application. The entry includes i)
start and end time, ii) reservation ID, job ID, user, group, and
application name, iii) resources data, e.g., number, ID, and type
of nodes, memory, and virtual memory, iv) application exit code
and job exit code, v) job- and application-required wall time
and used wall time, and vi) the command used to launch the
application.
Step 4: Workload-Error Matching. The goals of this step
are to collate relevant error data with the consolidated workload
data. The ﬁrst operation consists of executing a change point
detection analysis [6] to determine changes in the mean and
variance of different estimators that LogDiver evaluates from
the error data. Given that software and hardware sensors
generate events using different sampling periods and applying
different generation rules (e.g., periodic or impromptu), esti-
mators are computed on a uniform representation of the data
in the form of (stochastic) point processes. That is, a set of
events is generated at random points in time Ti using different
representations and transformations based on both the event
inter-arrival process and the count of events in speciﬁc time
intervals. We ﬁnally group all the error tags that occur with
statistically high correlation (estimated using Pearson’s lagged
cross-correlation coefﬁcients among estimators) and that are
generated by the nodes executing the same application.
Decoding Application Exit Reasons. There are more than
256 possible application exit codes, many of which are am-
biguous or application dependent. An example is that of the
exit code 143 (i.e., application terminated by issuing a TERM
signal). This exit code can be issued when the application
is killed either by system errors or by the user. LogDiver is
able to disambiguate and categorize an application exit reason
by matching error data with the application exit code data.
Exit reasons are classiﬁed into the following categories: (i)
success, for applications completing successfully, ii) walltime,
for applications not completing within the allocated wall clock
time, iii) user, for abnormal terminations caused by user-related
problems, including compiler/linking/job script and command
errors, missing module/ﬁle/directory, wrong permissions, and
user-initiated actions, such as a control-C signal or termina-
tion/kill command, iv) system, when an application is termi-
nated due to system-related issues caused by any of the con-
sidered system errors, and v) user/system, when an application
is terminated for reasons that can be related to both user and
system events, such as errors detected by the applications (e.g.,
through assertions) and forcing a legit termination.
Estimated Metrics. LogDiver estimates various metrics of
interest with respect to applications that fail because of system-
related issues. Metrics used in this study include:
Fig. 4. Breakdown of the application exit reasons generated by (a) all
applications, (b) XE applications, (c) XK applications decoded by LogDiver.
• Mean Node Hours Between Failures (MNBF) computed
as a ratio of the total number of production node hours 1
to the total number of application failures;
• Mean Time Between Interrupt (MTBI) computed as a ratio
of the total number of production hours to the total number
of application failures;
• Probability of an application failure.
V. DISSECTING APPLICATION EXIT STATUS
In this section, we provide a ﬁrst breakdown of how XE and
XK applications terminate. Figure 4 gives the breakdown of
the application exit reasons. Figure 4.(a) shows that about two
thirds of the applications (66.47%) terminated with success. The
remaining applications failed for several reasons, including: i)
the application execution time exceeded the time limit (4.71%,
category “walltime”); ii) user-related problems (18.75%, cate-
gory “user”); iii) system-related problems (1.53%) caused by
hardware, software, conﬁguration, or network issues at
the
system or node levels that occur with an MTBI (production
hours / total application interrupts) of 15 minutes; and iv) a
combination of user- and system-related causes, e.g., excep-
tions raised because of issues with Gemini rerouting (8.53%,
category “user/systems”). Compared to earlier Cray systems
(for which only job success data are publicly available), Blue
Waters shows a lower percentage of application failures [7]–[9].
For instance, for Franklin, the Cray XT4 100 TF machine at
NERSC, 61% of applications complete successfully, whereas
11.5% are terminated because of the wall-time limit, 25%
are terminated because of user problems, and 2.7% are killed
because of system problems. Athena (166 TF, 46 XT4 cabinets)
shows that 82% of applications were successful. The percentage
of application failures due to system issues is not reported and
is believed to be 3%, according to the technical staff.
XK applications show a higher percentage of application
failures due to system errors (1.83%) than XE nodes (1.36%).
Figures 4.(b) and (c) reveal that 61.27% of XE applications are
successful against 76.60% for XK applications. The difference
in the percentage of failed applications is due to the higher
number of XE applications failing because of user causes (cat-
egory “user”, 22.24% for XE applications and 12.20% for XK).
The reason behind that is that XE nodes are often preferred by
users to develop and debug their applications before deploying
1A unit of work equal to one node computing for one hour, e.g., 8 nodes
computing for 0.25 h = 2 node hours. Only actual production hours are used in
this study, i.e., burn-in test, upgrade and maintenance hours are not considered.
3030
16% 
14% 
12% 
10% 
8% 
6% 
4% 
2% 
0% 
xe 
xk 
12.62% 
14.67% 
13.84% 
12.50% 
11.72% 
10.27% 
8.46% 
4.21% 
2.48% 
5.53% 
4.36% 
1.48% 
single 
nano 
low 
med 
high 
full 
Fig. 5. Percentage of failed applications (system problems) estimated with
respect to the total number of successful applications ([#failed]/[#failed +
#successful]) for different application scales.
Fig. 7. Example of error trace generated by LogDiver for an application
impacted by the ﬁle system failover (only a reduced number of ﬁelds is shown).
substantial and destined to grow for larger machines.
Fig. 6. Breakdown of the application node hours corresponding to the exit
reasons generated by LogDiver for XE and XK applications.
the code on the XK nodes because the debugging support is
generally better on XE nodes, and because the wait time for
executing applications on XK7 blades is typically longer than
that for accessing XE6 blades.
In further analysis conducted in this paper, we remove the
contribution of the categories “user”, “user/system”, and “wall-
time” and focus on characterization of application failures due
to system-related problems (the category “system”). First, we
compute the ratio #f ailed/[#f ailed + #successf ul], where
#f ailed corresponds to the percentage of applications failed
due to system issues and #successf ul represents applications
that completed successfully.
Figure 5 shows the results obtained for the different applica-
tion scales (as deﬁned in Table III). We observe that the larger
the scale, the higher the percentage of applications that fail
because of system problems. For example, for XE applications
the ratio changes from 4% for the code running on 1 to 4 nodes
(i.e., a single blade) to 14% for the same code running on more
than 50% of the XE partition. Section VII discusses some of
the reasons for this behavior.
9% of the total used node hours is consumed by applica-
tions that eventually failed because of system-related issues.
While 1.53% of applications fail due to system problems, they
contribute to about 9% (see Figure 6) of the total production
node hours. Those applications run for 17,952,261 node hours
(i.e., the equivalent of about 28 days of continuous 100% use
of the system) and eventually had to be relaunched or executed
multiple times. Considering an average power consumption of
2KW/blade [1], re-executing the applications that failed because
of system errors (when a checkpoint is not available) would
theoretically add up to $421,878 to the Blue Waters’ energy bill
(based on a cost of 0.047c/KW and without considering other
costs for cooling and infrastructures). Therefore, the impact
of system errors on applicationss and costs of ownership is
37% of failed applications fail during/after failover oper-
ations. During the measured period, Blue Waters executed 392
failover operations.2 Our analysis shows that 37% of the appli-
cation failures are because of system issues occurred during or
just after (±15min in our analysis) a failover operation. 3 The
distribution of the time to recovery is a long-tail distribution,
with the tail containing 10% of the samples, starting around 15
minutes. Therefore, we chose a 15min interval in our analysis.
Figure 7 shows the error trace of an application terminated
during a ﬁle system failover. The sequence of tags in the trace
shows that one OSS (Object Storage Server) failed because
of an interrupted system call that was detected by the LBUG
assertion in one of the Lustre ﬁle system daemons. This caused
a failure of the OSS that triggered the failover. During the
failover, the portion of the ﬁle system served by the OSS was
not available to the application, and each ﬁle system operation
was paused (or dropped). After 3.6 h (“error duration” ﬁeld
in Figure 7), the application reached the wall-time limit and
was terminated by the scheduler with the SIGTERM signal.
Usually the recovery method available at the application level is
the checkpoint/restart. However, in the example, the application
affected by the failed OSS could not take a stable checkpoint
because of the unavailability of the ﬁle system.
An implication of this observation is that system-level re-
siliency mechanisms should interact with the workload manage-
ment system and with application-level resiliency mechanisms
in order to avoid workload disruptions and reduce the impact
of system outages on users’ activities. Many cases of failure of
the failover mechanisms are indeed related to a high load level
on components playing a critical role for the system recovery
process, as shown by the example in Section III. Providing
an interface at
to attach application-
speciﬁc error detectors is a possible way to orchestrate runtime
resiliency solutions working at different levels of the software
stack (e.g., on-demand checkpointing cooperating with system-
level failover). Generalized proactive response methods may be
built on top of such an architecture to improve the overheads
involved in the overall checkpoint-restart architecture.
the application level
2As discussed in [2], more than 75% of the time the failover is successful.
3We performed the same measurements for the interval of ±1min (minimum
failover time for the Gemini interconnect) and ±30min (maximum failover
time for the Lustre ﬁle system before declaring a system-wide outage), and the
results were only affected marginally.
3131
4.71% of all the applications were interrupted because they
reached the allocated wall-time limits. Causes of application
termination due to exceeding the wall-time limit include: i) the
wall-time limit deﬁned by the user in the job script did not meet
the actual execution time of the application, ii) the user did not
invoke the “exit” command at the end of the job/interactive
session, and iii) one or more applications within the job hang,
e.g., because of a deadlock in MPI communication or a long-
lasting system failover.
Although some of these problems may be caused by system-
related issues, the current event logging system does not pro-
vide enough information to identify the system-induced hangs.
Nevertheless, LogDiver is able to detect application hangs due
to the failover by looking at the application that terminated
because of wall-time and that were quiesced for a failover
operation to complete at the time of termination (error tag
“WAITING FAILOVER END” in Figure 3.(b)).
VI. APPLICATION FAILURE STATISTICS
In this section, we describe the failure statistics estimated
by LogDiver (see Section IV) for the most intensively utilized
applications in Blue Waters. For each application, we evaluate
MNBF and MTBI metrics (see metric deﬁnitions in Section IV).
Note that (i) MNBF characterizes the time between application
failures in terms of node hours and hence takes into account
the application scale,
time and number
of nodes; (ii) MTBI characterizes the time between failures
without encompassing the application scale.
the used wall
i.e.,
Considered Applications. In order to obtain statistically
meaningful measurements, we selected XE and XK applications
considering the following criteria: (i) total number of runs: the
number of an application runs is greater than the 75th percentile
of the distribution of the number of runs per application;
(ii) total number of users: the number of application users
is greater than the 75th percentile of the distribution of the
number of users per application; (iii) node hours: the number
of node hours an application executes is greater than the
75th percentile of the distribution of node hours executed per
application. Eighteen applications satisﬁed the selection criteria,
out of which we selected the top 9. Selected applications were
executed 3,326,467 times, accounting for more than 41% of
the total production node hours in the measured period. The
list includes the following applications:
• chroma, for lattice Quantum Chromdynamics (QCD) sim-
ulations implemented using SciDAC QDP++ [10];
• namd, a N-body parallel molecular dynamics code based
on Charm++ parallel objects;
• enzo, a parallel adaptive mesh reﬁnement application for
computational astrophysics based on MPI/OpenMP;
• pmemd, the Particle Mesh Ewald Molecular Dynamics
component of the AMBER suite of molecular dynamics
packages application;
• rhmd, a Rational Hybrid Monte Carlo algorithm for lattice
QCD simulations based on Charm++;
• pmcl3d, an application to solve the elastic/anelastic seismic
wave equation to simulate 3D seismic propagation models
based on MPI-2 and (intensive IO load);
FAILURE STATISTICS REPORTED FOR THE TOP 9 APPLICATIONS (IN TERMS
OF NODE HOURS). FIGURES IN BOLD REFER TO APPLICATIONS USING
CHECKPOINT/RESTART.
TABLE V
Node Hours 
Max used nodes MNBF (node hours) 
Code 
namd2 
chroma 
spectrum 
rhmd 
enzo 
rmg 
pmemd 
pmcl3d 
vmd 