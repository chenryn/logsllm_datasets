the options following the existing works.
For consistency, we conducted all the experiments on machines
equipped with Intel Core i5-7260U and 8 GB RAM running 64-bit
Ubuntu 14.04-LTS. To minimize the effect of randomness intro-
duced during software fuzzing, we ran each fuzzing test 5 times
and reported the average results with standard deviation.
5.2 Execution Speed Evaluation
Figure 7: Normalized dry-run duration for different fuzzing
techniques. Shorter is better.
To show how fast is PTrix, we compared its execution speed
with QEMU-AFL, Edge-PT, and PTFuzzer. To be specific, we ran
these fuzzers with an identical input corpus and examined their
execution time. In this evaluation, different inputs could trigger
different types of fuzzing operations/decisions. For example, an
input that results in no new coverage will be discarded without
further processing. To avoid such difference in fuzzing runs, we
selected inputs which make all the fuzzers to go through the entire
fuzzing procedure. For this, we ran QEMU-AFL with the settings
shown in Table 1 for 24 hours and only kept inputs that led to new
coverage. Note that these inputs also resulted in new coverage in
PTrix due to its highly sensitive feedback.
In this test, we utilized the dry-run mode of AFL. It allows the
fuzzers to repeatedly process the above input corpus. In Figure 7,
we show the evaluation results that have been normalized with
PTrix as baseline. On average, PTrix ran 4.3x, 25.8x, and 54.9x
faster than QEMU-AFL, Edge-PT, and PTFuzzer 3, respectively. In ad-
dition, we observed that Edge-PT ran 6.0x slower than QEMU-AFL
with all our optimization enabled (i. e., parallel decoding, optimized
communication and caching instruction trace), and PTFuzzer ran
13.5x slower than QEMU-AFL. Considering that Edge-PT, PTFuzzer
and AFL-QEMU share the identical feedback, this observation indi-
cates that the design with control flow reconstruction cannot truly
expose the potential of PT in improving fuzzing efficiency.
PTrix optimization breakdown: To better understand how PTrix
achieves the high execution speed, we inspected the improvement
that each of our optimization introduces. We first re-ran PTrix
without our new feedback scheme, parallel trace decoding and
3Our evaluation on PTFuzzer shows much worse performance than the results re-
ported in [38]. We believe this is mainly because our benchmarks have higher com-
plexities and the seeds we use trigger deeper execution.
PTrix: Efficient Hardware-Assisted Fuzzing for COTS Binary
ASIACCS ’19, July 07–12, 2019, Auckland, NZ
Program
Name
New Feedback
Optimization
libpng
libjpeg
libxml
c++filt
nm
objdump
exif+libexif
perl
mupdf
Average
Bitmap
Scheme
1038.16%
1412.96%
2856.08%
1208.11%
1145.90%
393.58%
2695.03%
845.71%
1426.82%
1446.93%
9.51%
14.04%
5.80%
10.58%
6.61%
4.40%
8.60%
9.37%
3.68%
8.07%
Table 2: PTrix system optimization breakdown
Optimization
Parallel
Parsing
22.19%
36.54%
51.02%
28.70%
18.22%
6.63%
41.64%
63.14%
47.61%
35.08%
bitmap optimization. Then we enabled the optimization one by
one and measured the increase of execution speed independently.
The results are shown in Table 2. On average, our new coverage
scheme increases the execution speed by over 14X. The major rea-
son, we believe, is that the new scheme avoids the time-consuming
instruction reconstruction. In addition, the parallel parsing intro-
duces 35% increase in execution speed and our bitmap optimization
contributes around 8% to the speedup.
5.3 Code Coverage Measurements
As above shown, the design of PTrix substantially accelerates the
fuzzing process. Next, we show that PTrix is not just faster but
also covers more code. In fact, code coverage is the most widely
acknowledged metric [11, 23, 24, 27, 34, 35] for evaluating fuzzers.
We run PTrix and AFL for 72 hours or until QEMU-AFL saturates4,
whichever comes first. This long-term evaluation reduces potential
random noise in results and gives a more comprehensive view of
the coverage efficiency across time. Note that in this evaluation, we
excluded Edge-pt and PTFuzzer. The reason is that Edge-pt and
PTFuzzer explore code even slower than QEMU-AFL, as echoed by
our observations on the above 24 hour tests.
In the following, we first present the efficiency comparison be-
tween PTrix and QEMU-AFL. Then we examine the difference be-
tween code covered by the two fuzzers and discuss the possible
reasons.
Code exploration efficiency: We calculated the code coverage
using a representative quantification — number of edges between
basic blocks [23] and summarize the results in Figure 8.
As is shown in the Figure, PTrix generally explored code space
quicker than QEMU-AFL across the timeline. Only in the case of
c++filt, PTrix fell behind QEMU-AFL from the 24th hour to the
48th hours. We believe this was mainly because PTrix spent more
time on a local code region, which is reflected by its increased pace
after 48 hours. For all the 9 programs, PTrix covered more edges
than QEMU-AFL at the end. In particular for objdump and libpng,
PTrix significantly increased the code coverage for over 5%. In the
cases of c++filt, nm and mupdf, PTrix covered a similar amount
of edges as QEMU-AFL after 72 hours. A possible reason for PTrix
not achieving an obvious increase is that the fuzzers were reaching
the first code coverage plateau, as their new edge discovering rate
drops almost to 0.
4When QEMU-AFL finishes all inputs that lead to new coverage, we consider it has
saturated. The rationale is after that, QEMU-AFL may only discover new coverage
through random attempts instead of strategic exploration.
PTRIX only
QEMU-AFL only
Code coverage
Program
Name
libpng
libjpeg
c++filt
mupdf
Average
Table 3: Edge coverage comparison.
overlap
95.60%
89.50%
89.93%
96.51%
92.89%
2.80%
10.00%
5.85%
2.12%
5.19%
1.60%
0.50%
4.22%
1.37%
1.92%
PTrix uses a feedback scheme with higher sensitivity, which
tends to explore localized code more thoroughly. By theory, this
will make PTrix move slowly around code regions. However, our
evaluation shows an opposite conclusion. We believe this is largely
attributable to the high execution speed of PTrix. This fast exe-
cution not only offsets the delay by localizing into code regions
but also accelerates the travel between different regions. Also note
that the comprehensive exploration by PTrix is not running in
vain. It gains new opportunities to reach new code regions and
vulnerabilities. We will shortly discuss this with evaluation results.
Code exploration effectiveness: PTrix and QEMU-AFL use differ-
ent feedback schemes. Intuition suggests that the two fuzzers may
explore code in different favors. To explore this intuition, we com-
pared the difference of edges discovered by PTrix and QEMU-AFL.
Essentially, we took the union of edges from the two fuzzers as the
baseline. Then we calculated the proportion that was covered by
both PTrix and QEMU-AFL, by PTrix only, and by QEMU-AFL only.
The average results are organized in Table 3. We only included the
cases where QEMU-AFL has saturated. In those cases, QEMU-AFL has
sufficiently expressed its exploration capability following the strate-
gic approach, which enables us to better inspect whether PTrix
can really outperform QEMU-AFL.
As shown in the table, the two fuzzers were mostly covering
the same set of edges, but they indeed explored different code re-
gions. For instance, in the case of cxxfilt, over 10% of code edges
were individually discovered. Taking a closer look, PTrix missed
significantly fewer edges than QEMU-AFL. Particularly in the case
of libpng, PTrix nearly covered all the edges by QEMU-AFL. This
indicates the path-sensitive feedback improves the code exploration
of PTrix. More importantly, during the long-term running, PTrix
never saturated. For example, when we ended the tests on c++filt,
PTrix’s pending favorite metric was still about 1,000. This demon-
strated the potential of PTrix to cover all edges that have been
explored by QEMU-AFL.
We have also manually inspected the different edges covered by
PTrix and QEMU-AFL. Due to limited time, we have only analyzed
a subset of them. We have identified two code regions which we
believe shall only be covered by PTrix. We have explained one
case from libpng in Section 3 and will present the other case from
objdump in Section 5.4.
Code exploration comprehensiveness: As shown above, PTrix
and QEMU-AFL may cover different code given the same amount of
time. Presumably, this is due to their different feedback schemes.
To verify this intuition, we performed an additional analysis named
call chain analysis. This analysis takes as inputs the corpus from
PTrix and QEMU-AFL in the long-term run. It re-executed each test
case and collected the call chains. A call chain is defined as follows
— When the execution reaches a leaf node on the program’s call
ASIACCS ’19, July 07–12, 2019, Auckland, NZ
Chen, et al.
(a) c++filt*
(b) exif
(c) perl
(d) libjpeg*
(e) libpng*
(f) libxml
(g) mupdf*
(h) nm
(i) objdump
Figure 8: Edge coverage results of different fuzzing techniques for 72 hours. The star (*) besides a program name indicates that
fuzzing on that program has saturated.
graph, the sequence of functions on the stack is deemed as a call
chain. The length of a call chain represents a “locally maximal”
execution depth.
To give an overview of the call chains, we aggregated them by
their lengths and present the cumulative distribution in Figure 9.
Generally speaking, PTrix produced higher proportion of shorter
call chains than QEMU-AFL. We also observed that PTrix usually
generates the shorter call chains before the longer ones. This shows
that PTrix spends more efforts in the beginning on shorter call
chains and then later moves onto longer ones, which is consistent
with our expectation — PTrix explores local code more compre-
hensively and does not easily skip code paths or regions.
5.4 Discovery of Real-world Vulnerabilities
Going beyond evaluation on fuzzing efficiency and code coverage,
we further applied PTrix to hunt unknown bugs in the wild. We
selected a set of programs as shown in Table 1 and four other well-
tested programs including gnu-ld, curl, nasm, and tcpdump. Due
to constraints of computation resources and time, we only ran each
program for 24 hours.
Program
Name
objdump
c++filt
perl
nm
gifview
gdk-pixbuf
nasm
glibc ld
libxml
tcpdump
unrtf
libjpeg
Total
Vulnerability Type
DOS
Memory Error
1
2
0
1
0
0
2
0
2
0
0
1
10
4
3
3
4
1
1
7
1
0
1