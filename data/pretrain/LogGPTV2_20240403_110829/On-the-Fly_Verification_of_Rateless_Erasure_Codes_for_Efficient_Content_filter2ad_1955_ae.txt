was specially chosen due to its fast veriﬁcation time, as shown.
Surprisingly, verifying a check block using a SHA1 binary tree
is more than twice as slow as using our homomorphic hashing
protocol, due to the height of the tree.
E. Discussion
For encoding rates such as r = 1=16, each of the three
ﬁxed-rate schemes has important strengths and weaknesses.
Though the sign-every-block scheme is bandwidth-efﬁcient,
requires no up-front hash transfer, and has good veriﬁcation
performance, its hash generation costs are prohibitive and its
storage costs are higher. Similarly, though the binary hash
tree method has no up-front transfer, its bandwidth, storage
and veriﬁcation costs make it less attractive than hash trees
with larger fan-out. The homomorphic hashing scheme entails
no such tradeoffs, as it performs well across all categories
considered. Homomorphic hashing ranks less favorably when
considering veriﬁcation throughput, but as argued in Sec-
tion V-C, tuning batch size allows throughput to scale with
available bandwidth.
Scheme
Homomorphic Hashing
Big-Degree SHA1 Hash Tree
Binary SHA1 Hash Tree
Sign Every Block
Up-Front
Predicted (bits)
(cid:21)pn(cid:12)=((cid:12) (cid:0) (cid:21)p)
160n(cid:12)=((cid:12) (cid:0) 160)
0
0
TABLE V
BANDWIDTH
Per-Block
e.g. (MB)
Predicted (bits)
e.g. (KB)
8.06
20.02
0
0
(cid:12) + m
(cid:12)
(cid:12) + 160 log2(n=r)
(cid:12) + (cid:21)(cid:27)
16.06
16.00
16.39
16.13
Total (GB)
1.0118
1.0196
1.0244
1.0078
Total w/ Penalty (GB)
1.0118
1.0528
1.0578
1.0407
TABLE VI
PER-BLOCK VERIFICATION PERFORMANCE
Scheme
Homomorphic Hash
Big-Degree Tree
Binary Tree
Sign Every Block
Batch
SHA1
Rabin
1
0
0
0
0
1
log2(n=r)
0
0
0
0
1
Total (msec)
2.05
0.28
5.60
0.29
VI. SECURITY
In modern real-world P2P-CDNs, an honest receiver who
wishes to obtain the ﬁle F from the network communicates
almost exclusively with untrusted parties. As mentioned in
Section III, a crucial stage of the ﬁle transmission protocol—
mapping ﬁle names to ﬁle hashes—is beyond the scope of
this paper. In our analysis, we assume that the receiver can
reliably resolve N (F ) ! JG(F ) through a trusted, out-of-band
channel. We wish to prove, however, that given JG(F ), the
downloader can recover F from the network while recognizing
certain types of dishonest behavior almost immediately.
A. Collision-Resistant Hash Functions
First, we formally deﬁne a collision-resistant hash function
(CRHF) in the manner of [21]. Recall
that a family of
hash functions is given by a pair of PPT algorithms F =
(HGen; H). HGen denotes a hash generator function, taking
an input of security parameters ((cid:21)p; (cid:21)q; m) and outputting a
description of a member of the hash family, G. HG will hash
inputs of size m(cid:21)q to outputs of size (cid:21)p, exactly as we have
seen thus far. A hash adversary A is a probabilistic algorithm
that attempts to ﬁnd collisions for the given function family.
Deﬁnition 1: For any CRHF family F, any probabilistic
algorithm A, and security parameter (cid:21) = ((cid:21)p; (cid:21)q; m) where
(cid:21)q  0, any batch size t > 1, let:
Advenc-atk
H;V;m;n;t(A) =
Pr(cid:2)G   HGen(H); (F; X; C)   A(G; m; n; t);
F 0   P(F ); b   V(HG(F 0); G; X; C) :
F is m (cid:2) n ^ F 0 is m (cid:2) n0 ^ X is n0 (cid:2) t
(5)
^ C is m (cid:2) t ^ F 0X 6= C ^ b = Accept(cid:3)
The encoding veriﬁer V is ((cid:28); ")-secure if, 8 m; n > 0, t >
1 and PPT adversaries A with time-complexity (cid:28) (m; n; t),
Advenc-atk
H;V;m;n;t(A)  1. Thus, a protocol that uses a secure encoding veriﬁer
can tune t as desired to trade computational efﬁciency for
communication overhead. From here, we can prove that the
batch veriﬁcation procedure presented previously is secure.
See Appendix II.
Theorem 1: Given security parameters l; (cid:21)p; (cid:21)q, batch size
t, number of generators m, and the ((cid:28); ")-secure hash family
h generated by ((cid:21)q; (cid:21)p; m), the batched veriﬁcation procedure
V given above is a ((cid:28) 0; "0)-secure encoding veriﬁer, where
(cid:28) 0 = (cid:28) (cid:0) mt(MultCost(q) + MultCost(p)) and "0 = " + 2(cid:0)l.
We do not state or prove the corresponding theorem for
the na¨ıve veriﬁer, but it is straightforward to check that it
has equivalent or stronger properties than that of the batch
veriﬁer. The security of the recursive hashing scheme outlined
in Section IV-E follows from an inductive application of
Theorem 1.
C. Future Work and End-To-End Security
These security guarantees, while necessary, are not sufﬁcient
for all multicast settings. In Section III, we proposed both the
bogus-encoding attack and the distribution attack. While we
have solved the former, one can imagine malicious encoders
who thwart the decoding process through an incorrect distri-
bution of well-formed check blocks. Because Tornado, Raptor,
Online, and LT Codes are all based on irregular graphs, their
output symbols are not interchangeable. Bad encoders could
corrupt degree distributions; they could also purposefully avoid
outputting check blocks derived from some particular set of
message blocks. Indeed, the homomorphic hashing scheme
and the three ﬁxed-rate schemes discussed in Section V-D are
all vulnerable to the distribution attack.
In future work, we hope to satisfy a truly end-to-end
deﬁnition of security for encoding schemes. For the end-to-end
model, we envision an experiment in which the adversary can
chose to supply the recipient with either its own check blocks,
or those from an honest encoder. The X and C parameters of
the veriﬁer function now correspond to the entire download
history, not just the most recent batch of blocks. The veriﬁer
outputs Reject if it believes it is talking to a malicious encoder,
in which case the experiment discards the batch of blocks
just received. In the end, the experiment runs the decoder
on all retained check blocks after receiving (1 + (cid:15))n0 + aB
total blocks, where a is a constant allowance for wasted
bandwidth per bad encoder, and B is the number of times the
veriﬁer correctly output Reject after receiving blocks from the
adversary. The adversary succeeds if this decoding fails with
non-negligible probability.
One approach toward satisfying such a deﬁnition might be
to require a sender to commit to a pseudo-random sequence
determined by a succinct seed, and then send check blocks
whose xi portions are entirely determined by the pseudo-
random sequence. But in the context of non-reliable network
transport or multicast “downsampling,” a malicious sender can
drop particular blocks in the sequence and place the blame on
congestion. If, for example, the sender drops all degree-one
blocks, or drops all check blocks that mention a particular
message block, decoding will never succeed.
A more promising approach involves validating an exist-
ing set of check blocks by simulating the receipt of fu-
ture check blocks. Given an existing set of check blocks
hx1; c1i; hx2; c2i; : : : ; hxQ; cQi, the veriﬁer can run the en-
coder (without the contents of F ) to generate a stream of
block descriptions xQ+1; xQ+2; : : :. If the ﬁle would not be
recoverable given cQ+1; cQ+2; : : :, this is evidence that the
distribution of x1; : : : ; xQ has been skewed. If the ﬁle would
be recoverable, the veriﬁer can repeat the experiment several
times to amplify its conﬁdence in x1; : : : ; xQ. To be of any
use, such a veriﬁer can do no more than O(log n) operations
per check block received. Thus simulated streams should be
re-used for efﬁciency, with the effects of the ﬁrst simulated
block xQ+1 replaced by those of the next real block received.
The feasibility of efﬁciently “undoing” encoding remains an
open question; therefore we leave the description and analysis
of an exact algorithm to future work.
VII. RELATED WORKS
Multicast source-authentication is well-studied problem in
the recent literature; for a taxonomy of security concerns and
some schemes, see [22]. Preexisting solutions fall into two
broad categories: (1) sharing secret keys among all participants
and MACing each block, or (2) using asymmetric cryptogra-
phy to authenticate each block sent. Unfortunately, the former
lacks any source authentication, while the latter is costly with
respect to both computation resources and bandwidth.
A number of papers have looked at providing source authen-
tication via public key cryptography, yet amortizing asymmet-
ric operations over several blocks. Gennaro and Rohatgi [23]
propose a protocol for stream signatures, which follows an
initial public-key signature with a chain of efﬁcient one-time
signatures, although it does not handle block erasures (e.g.,
from packet loss). Wong and Lam [24] delay consecutive
packets into a pool, then form an authentication hash and sign
the tree’s root. Rohatgi [25] uses reduced-size online/ofﬂine k-
time signatures instead of hashes. Recent tree-based [26] and
graph-based [27] approaches reduce the time/space overheads
and are designed for bursty communication and random packet
loss. More recent work [28], [29] makes use of trusted erasure
encoding in order to authenticate blocks, while most schemes,
including our own,
try to authenticate blocks in spite of
untrusted erasure encoding.
Another body of work is based solely on symmetric key
operations or hash functions for real-time applications. Several
protocols used the delayed disclosure of symmetric keys to
provide source authentication, including Chueng [30], the Guy
Fawkes protocol [31], and more recently TESLA [32], [33],
by relying on loose time synchronization between senders
and recipients. The recent BiBa [34] protocol exploits the
birthday paradox to generate one-time signatures from k-wise
hash collisions. The latter two can withstand arbitrary packet
loss; indeed, they were explicitly developed for Digital Foun-
tain’s content distribution system [6], [7] to support video-on-
demand and other similar applications. Unfortunately, these
delayed-disclosure key schemes require that publishers remain
online during transmission.
In the traditional settings considered above, the publisher
and the encoder are one in the same. In our P2P-CDN setting,
untrusted mirrors generate the check blocks; moreover a
trusted publisher cannot explicitly authenticate every possible
check block, since their number grows exponentially with ﬁle
size. Thus, a publisher must generate its authentication tokens
on the initial message blocks, and we require a hash function
that preserves the group structure of the encoding process.
Our basic homomorphic hashing scheme is complementary
to existing threads of work that make use of homomorphic
group operations. One-way accumulators [35], [36] and in-
cremental hashing [21], based on RSA and DL constructions
respectively, examine commutative hash functions that yield
an output independent of the operations’ order. Improvements
to the schemes’ efﬁciency [37], [38], [39], however, largely
focus on dynamic or incremental changes to the elements
being hashed/authenticated, e.g., the modiﬁcation of an entry
of an authenticated dictionary. More recent work has inves-
tigated homomorphic signature schemes for speciﬁc applica-
tions: undirected transitive signatures [40], authenticated preﬁx
aggregation [41], redactable signatures [42], and set-union
signatures via accumulators [42]. We use similar techniques to
maintain group structure across applications of cryptographic
functions, but
to different ends. Composing homomorphic
signatures with traditional hash functions such as SHA1 [13]
would not solve our problem, as the application of the tra-
ditional hash function would destroy the group structure we
hope to preserve.
VIII. CONCLUSIONS
Current peer-to-peer content distribution networks, such as
the widely popular ﬁle-sharing systems, suffer from unveri-
ﬁed downloads. A participant may download an entire ﬁle,
increasingly in the hundreds of megabytes, before determining
that the ﬁle is corrupted or mislabeled. Current downloading
techniques can use simple cryptographic primitives such as
signatures and hash trees to authenticate data. However, these
approaches are not efﬁcient for low encoding rates, and are
not possible for rateless codes.
To our knowledge, this paper is the ﬁrst to consider non-
interactive, on-the-ﬂy veriﬁcation of rateless erasure codes.
We present a discrete-log-based hash scheme that provides
useful homomorphic properties for verifying the integrity of
downloaded content. Because recipients can compose hashes
just as encoders compose message blocks,
they can ver-
ify any possible check block. Using batching techniques to