the log. The interface is based on the declarative Pandas API
for python. With these queries, we search for contention in the
code or dependencies of calls, which result in a high overhead.
Selective code proﬁling. We also support selective code
proﬁling. In particular, by selecting parts of the code, where
our tool injects the measurements it is possible to only measure
parts of the application. Therefore, we provide a systematic
knob to reduce the log size and selective code proﬁling.
III. IMPLEMENTATION
We next present the implementation details of TEE-PERF.
a
calls
function
respectively. Another
__cyg_profiler_func_enter
Compiler pass. We use compiler ﬂags which are available
in gcc and clang for
injecting proﬁling code in the
application. These ﬂags are -finstrument-function,
and
adding
__cyg_profiler_func_exit to
or
return instruction,
feature we use
is the --include ﬂag, allowing us to include a ﬁle in
every compilation unit. The included ﬁle has the code
necessary to write to the log. Therefore,
the compiler
should be able to inline these methods,
and/or
reducing their overhead further. The ﬁnal
to
the library containing the setup and tear
link against
down code. A full compiler call
follows: gcc
-finstrument-function --include=profiler.h
test.c -o test -lprofiler.
step is
linker
is as
Recorder. The two parts of the recorder are the wrapper
and the code injected into the application. Since the recorder
should be platform agnostic it does not use any special
libraries. Therefore, it only depends on the libc. For compat-
ibility to most environments, it is written in C. Thereby, 389
LoC will be injected into the application, and the wrapper
consist of 230 LoC.
Importantly,
the injected code has
to be
in an inﬁnity
from happening by adding the
measured itself,
loop. We
avert
__attribute__((no_instrument_function))
to all injected methods.
this would result
to prevent
as
that
Analyzer. The analyzer runs ofﬂine and potentially on a dif-
ferent system, thus it is not constraint by the same portability
considerations as the recorder. Therefore, we implemented the
analyzer in Python 3 and used numpy and pandas for the
analysis of the log. Further, to reduce the implementation effort
the analyzer depends on UNIX tools: addr2line, readelf and
c++ﬁlt. We implemented the analyzer with 370 LoC.
In addition, we provide a query interface for analyzing the
logs. The query feature was implemented by starting the script
in interactive mode. After the script run through the user
can issue pandas queries on our data structures to ﬁnd the
information relevant to the user.
Visualizer. We further provide visualization mechanism for
the developers to track the performance bottlenecks. Therefore,
we provide support for ﬂamegraph for visualization. This
is directly implemented in the analyzer. Due to the already
existing analysis of the log structure, the Flame Graph output
could be implemented with as little as 15 LoC. We suspect
that other tooling support for visualization should be similarly
easy to port.
IV. EVALUATION
Our evaluation answers the following questions:
• What are the proﬁling overheads of TEE-PERF compared
to perf? (§IV-B)
• Does TEE-PERF detect performance optimization oppor-
tunities for applications running in the TEEs? (§IV-C)
417
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:15:59 UTC from IEEE Xplore.  Restrictions apply. 
A. Experimental Setup
Experimental testbed. We used a machine with Intel Xeon
E3-1270 v5 (3.60 GHz, 4 cores, 8 hyper-threads) with 64 GiB
RAM running Linux kernel 4.9. Each core has private 32 KiB
L1 and 256 KiB L2 caches, and all cores share a 8 MiB
L3 cache. For the storage device our testbed uses a Intel
DC P3700 SSD. The SSD has a capacity of 400 GB and is
connected over PCIe x4.
Applications and compiler. We evaluated TEE-PERF with
applications from the Phoenix 2.0 multithreaded benchmark
suite [25]. In addition, we have evaluated TEE-PERF using
two real-world applications: (i) RocksDB [27] persistent key-
value storage: we evaluated RocksDB using the RocksDB
benchmark [7]; and (ii) Intel SPDK library [18] version for
high-performance direct I/O storage operation.
All native applications were compiled using gcc (Debian
6.3.0-18+deb9u1) compiler, and to compile the applications
with the proﬁler we used x86 64-linux-musl-gcc (GCC) 7.3.0.
We used compilers with the -O3 optimization ﬂag.
Methodology. We used the Fex [23] framework to run the
experiments. For all measurements, we report the geometric
mean over 10 runs across all benchmarks.
B. Performance Overheads
We ﬁrst present
the proﬁling overheads of TEE-PERF
compared to perf. To measure the overheads, we used the
the Phoenix multithreaded benchmark suite running inside the
Intel SGX enclave using SCONE [10].
Figure 4 shows that
the performance overheads of the
TEE-PERF vary signiﬁcantly across benchmarks. The mean
overhead of our tool compared to perf is 1.9×. TEE-PERF
is in nearly all benchmarks slower than perf as the in-
ject code has to run on each method call and return. In
linear_regression TEE-PERF is around 8 % faster than
perf. This is expected as this particular application is issuing
a low number of function calls, and is mainly performing
computation work inside one function, therefore, the injected
code is not executed often. However, perf still has to perform
context switches to sample the data periodically. The other
outlier is string_match, where we suffer a 5.7× overhead
compared to perf. This is due to a high number of function
calls, as a result the injected code in our tool is executed often
and producing a higher overhead.
We note that the performance debugging is not done in a
production environment, but in a development environment.
Therefore, even though the performance overheads are on the
slightly higher side, they are still quite reasonable considering
the useful insights provided for the application programmers.
We also measured the performance of RocksDB with
our tool and plotted it using the Flame Graph. Figure 5
shows the runtime of different method with the db_bench
tool. We run a random read write benchmark with 80 %
reads. The Flame Graph shows
the benchmark
spent most of
timestamp
(rocksdb::Stats::Now) and generating random numbers
its time in getting a current
that
(cid:73)
(cid:85)
(cid:72)
(cid:83)
(cid:3)
(cid:82)
(cid:87)
(cid:3)
(cid:71)
(cid:68)
(cid:72)
(cid:75)
(cid:85)
(cid:72)
(cid:89)
(cid:82)
(cid:3)
(cid:72)
(cid:89)
(cid:76)
(cid:87)
(cid:68)
(cid:72)
(cid:53)
(cid:79)
(cid:25)
(cid:24)
(cid:23)
(cid:22)
(cid:21)
(cid:20)
(cid:19)
(cid:80) (cid:68)(cid:87)(cid:85)(cid:76)(cid:91)(cid:66) (cid:80) (cid:88)(cid:79)
(cid:90) (cid:82)(cid:85)(cid:71)(cid:66)(cid:70)(cid:82) (cid:88) (cid:81)(cid:87)
(cid:79)(cid:76)(cid:81) (cid:72) (cid:68)(cid:85)(cid:66)(cid:85)(cid:72) (cid:74)
(cid:75)(cid:76)(cid:86)(cid:87)
(cid:86)(cid:87)(cid:85)(cid:76)(cid:81) (cid:74)(cid:66) (cid:80) (cid:68)(cid:87) (cid:83) (cid:70)(cid:68)
(cid:80) (cid:72) (cid:68) (cid:81)
Figure 4: Overhead of TEE-PERF compared to perf for the
Phoenix benchmark suite running in the Intel SGX TEE.
(rocksdb::RandomGenerator::RandomGenerator).
To increase the performance of RocksDB in the enclave, these
two functions either have to be removed from the critical
path, or have to be replaced.
C. Case-study: Performance Optimization of Intel SPDK
We next present a case-study of Intel SPDK [18], where we
used TEE-PERF to optimize the performance in the context
of Intel SGX. In particular, entering and exiting a TEE is
performance expensive because the hardware needs to perform
a secure context switch, e.g. TLB ﬂush. This results in TEE
having huge overheads in tasks in which such operations are
frequent. One of these operations is I/O access since it requires
to make system calls, which cannot be made from within the
TEE since it could leak information to the outside. Due to