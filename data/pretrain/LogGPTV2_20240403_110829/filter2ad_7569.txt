title:Enabling near real-time central control for live video delivery in
CDNs
author:Matthew K. Mukerjee and
JungAh Hong and
Junchen Jiang and
David Naylor and
Dongsu Han and
Srinivasan Seshan and
Hui Zhang
Enabling Near Real-time Central Control for Live Video
Delivery in CDNs
Matthew K. Mukerjee(cid:63), JungAh Hong†, Junchen Jiang(cid:63), David Naylor(cid:63),
Dongsu Han†, Srinivasan Seshan(cid:63), Hui Zhang(cid:63)
(cid:63)Carnegie Mellon University
†KAIST
{mukerjee, junchenj, dnaylor, srini, hzhang}@cs.cmu.edu
{hja2508, dongsu_han}@kaist.ac.kr
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems—Distributed applications
Keywords
live video; CDNs; central optimization; hybrid control
1.
INTRODUCTION
Internet video delivery is experiencing a dramatic change.
Live video dominates on-demand video in user engagement—
PC viewers watch live video 11x more than VoD [2]. Live
streaming services like Twitch, YouTube Live, and Ustream
support many workloads: from mega-events (e.g., sporting
events) viewed by tens of millions of users, to thousands
of popular channels (>1,000 viewers), to many more less
popular channels in the “long-tail”. However, the addition
of user-created live content is causing a fundamental shift
towards unpredictable popularity dynamics.
Provisioning a video delivery network for such workloads
is challenging. The CDN needs to maximize video-speciﬁc
quality metrics (e.g., high bitrate and low join time) for both
popular and unpopular streams while simultaneously coping
with unexpected shifts in popularity/network conditions.
Failures in addressing these challenges result in diﬃculties
broadcasting live TV shows [3, 8] and eﬀective blackouts
during mega-events such as the Oscars [11]. These service
disruptions, which are apparent to end-users, will only get
worse as user-created live channels with rapidly shifting
popularity (virality) increase in number.
These issues are a direct result of a few underlying prob-
lems. Today, we rely on CDNs that were originally designed
to support Web page delivery to deliver most Internet video
traﬃc [1]. Thus, CDNs face many challenges in live video
delivery: 1) CDNs are already responsible for a signiﬁcant
fraction of video quality problems for both live and VoD
(including 20% of join failures and 22% of bitrate degra-
dation) [6], 2) unlike VoD, live video cannot beneﬁt from
prefetching or caching, requiring content to be sent through
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631444.
the internal delivery network in real-time, 3) live video is
shifting to viral user-created streams, and 4) CDNs are shift-
ing to higher bitrate streams (e.g., 15+Mbps 4K streams [10]).
With multiple encodings for diﬀerent devices (e.g., desktop,
mobile, set-top boxes) and a variety of bitrates, a single video
channel could easily consume 150+Mbps.
Many techniques exist to improve video delivery from
CDNs’ edge servers to users, including placing edge servers
closer to the users, bolstering last mile links, and choosing
better CDNs and edge servers (or multiplexing them) [7].
However, we argue that dynamic ﬁne-grained control over
the internal delivery network is needed to overcome these
underlying problems, rather than traditional DNS-based
redirection [9]. To be successful, a live video delivery system
must provide:
• Quality: It must optimize for application-speciﬁc quality
metrics (e.g., high bitrate). Note that more throughput
does not always mean higher bitrate and may even reduce
other metrics.
• Dynamic provisioning and scalability: It must sup-
port the workload of today’s largest CDNs by scaling
to thousands of live video streams and millions of users,
using thousands of CDN distribution clusters, while pro-
visioning for individual videos on-the-ﬂy.
• Responsiveness: It must respond to user- and network-
induced events with low latency (less than a second).
Existing solutions do not satisfy all three requirements.
Traﬃc engineering schemes (e.g., SWAN [4], B4 [5]) do not
optimize for video-speciﬁc metrics and traditional DNS-based
solutions are too coarse to react to quick shifts in popularity
at a per-video granularity, especially in long-lived live video
sessions [9].
For this, signiﬁcant innovations in the CDN’s control plane
are required. We present Video Delivery Network (VDN),
a new CDN architecture designed speciﬁcally for live video.
We believe that the two fundamental challenges to achieve
these goals are: (1) video-speciﬁc global optimization and (2)
a scalable and robust control plane that operates at a very
ﬁne timescale.
2. SYSTEM OVERVIEW AND RESULTS
VDN reuses the current CDN infrastructure as much as
possible but redesigns its control plane to overcome the
challenges of live video distribution by using application-
speciﬁc global coordination and local adaptation. We make
two major contributions:
343Figure 1: VDN System Overview
• A centralized coordination algorithm that optimizes
for live-video-speciﬁc objectives at the stream level using
an approximate linear optimization. It carefully picks
the most eﬃcient combination of distribution trees out of
many possible trees to maximize video-speciﬁc metrics
(e.g., highest supported bitrate). The algorithm can
scale to the workload of today’s largest CDNs (1000s
of concurrent live video streams delivered via 1000-2000
clusters) while delivering close-to-optimal results.
• A hybrid control approach where individual clusters
safely deviate from the central algorithm to respond to
changes in network conditions and/or viewership at the
timescale of 100s of milliseconds. We show that this
approach meets the strict responsiveness requirements of
live video while providing sustained quality at Internet
scale.
Figure 1 shows our design. A local agent in each cluster
discovers link and viewership information 1 which it for-
wards to the central controller as a network graph 2 . The
central controller computes the globally optimal distribution
trees for the current video channels over this network graph
using a linear program 3 , the result of which it sends to
the clusters 4 . This is the global control loop.
If the local
agent discovers a new user-/network-event (e.g., new channel
request, link failure, etc.) 1 , then it responds to this event
in a purely local fashion 2 (subject to certain restrictions).
We refer to this as the local control loop. We omit the details
due to space, but we take care to ensure these two control
loops do not work against each other.
Summary of results: In Figure 2(a), we compare dif-
ferent algorithms for building distribution trees. Local is a
greedy algorithm using local information to construct distri-
bution trees by recursively looking for parents that have the
best available bandwidth. Best-Tree greedily constructs dis-
tribution trees using a global view of the remaining network
bandwidth. Our application-speciﬁc centralized coordina-
tion algorithm VDN provides a 2x improvement in average
video bitrate over traditional CDN deployments while si-
multaneously scaling to 600 video channels and 1,000 CDN
distribution clusters.
Figure 2(b) shows VDN’s response time in a 10 machine
deployment on Amazon EC2. Our hybrid control plane
allows VDN to react to network and viewership changes at a
timescale of 100s of milliseconds and with much less variation
than a purely local approach.
(a) Quality comparison
(b) Join time
Figure 2: VDN’s quality and responsiveness
Acknowledgments
This work is supported in part by NSF Grant #1040801,
NDSEG Fellowship 32 CFR 168a, the National Research
Foundation of Korea (NRF-2013R1A1A1076024), and the
ICT R&D Program 2014 of MSIP, Korea.
3. REFERENCES
[1] Cisco visual networking index: Forecast and methodology,
2012-2017. http://www.cisco.com/en/US/solutions/collateral/
ns341/ns525/ns537/ns705/ns827/white_paper_c11-
481360_ns827_Networking_Solutions_White_Paper.html.
[2] Ooyala global video index q3 2013.
http://go.ooyala.com/rs/OOYALA/images/Ooyala-Global-Video-
Index-Q3-2013.pdf.
[3] Forbes. Game of thrones premier brings hbo more customers
despite episode crash.
http://www.forbes.com/sites/brandindex/2014/05/06/game-of-
thrones-premier-brings-hbo-more-customers-despite-episode-
crash/, May 2014.
[4] Hong, C.-Y., Kandula, S., Mahajan, R., Zhang, M., Gill, V.,
Nanduri, M., and Wattenhofer, R. Achieving high utilization
with software-driven wan. In Proc. ACM SIGCOMM (2013).
[5] Jain, S., Kumar, A., Mandal, S., Ong, J., Poutievski, L., Singh,
A., Venkata, S., Wanderer, J., Zhou, J., Zhu, M., et al. B4:
Experience with a globally-deployed software deﬁned wan. In
Proc. ACM SIGCOMM (2013), pp. 3–14.
[6] Jiang, J., Sekar, V., Stoica, I., and Zhang, H. Shedding light
on the structure of internet video quality problems in the wild.
In Proc. of the ACM CoNEXT (2013).
[7] Liu, X., Dobrian, F., Milner, H., Jiang, J., Sekar, V., Stoica,
I., and Zhang, H. A case for a coordinated internet video
control plane. In Proc. ACM SIGCOMM (2012), pp. 359–370.
[8] Reporter, T. H. Hbo go crashes during ’true dective’ ﬁnale.
http://www.hollywoodreporter.com/live-feed/hbo-go-crashes-
true-detective-687087, March 2014.
[9] Su, A.-J., and Kuzmanovic, A. Thinning akamai. In Proc. ACM
IMC (2008).
[10] Time. Netﬂix rolls out 4k tv support, for the extreme minority
that have it. http://time.com/53882/netflix-rolls-out-4k-tv-
support-for-the-extreme-minority-that-have-it/, April 2014.
[11] Variety. Abc’s live oscar internet stream suﬀers nationwide
outage.
http://variety.com/2014/digital/news/abcs-live-internet-
oscar-stream-suffers-nationwide-outage-1201124215/, March
2014.
     Local control Local discovery Global control Global discovery Apache Forwarding Table Controller Local Agent 1 1 2 4 2 3 # # Global Control Loop Local Control Loop Cluster A Controller CDN Local  Agent     Cluster B Output%m1: {S} -> {B} m2: {S} -> {A,B} Input%0100200300400500600NumberofVideoChannels0123456AverageBitrate(Mbps)VDNBest-TreeLocal050100150200#ofchannels0510152025JoinTime(Seconds)LightLoadMed.LoadHvy.LoadVDNFullyCentralizedFullyLocal344