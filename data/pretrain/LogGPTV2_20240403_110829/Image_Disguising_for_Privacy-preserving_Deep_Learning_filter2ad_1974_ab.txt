train the network for 1000 iterations. For CIFAR-10, we adapt the
learning rate from 0.1 to 0.001 as the model was trained for 350
iterations. Both models are implemented with TensorFlow.
Table 2: Results of applying image disguising mechanisms.
Model Accuracy
9.2%
36.8%
94.4%
89.7%
95.6%
89.3%
96.7%
93.4%
Visual Privacy Model Misuability
Datasets With Disguise Without Disguise
MNIST
CIFAR-10
Table 2 shows that the models trained on disguised images per-
form very close to the optimum accuracy attained by the models
trained on undisguised images. Furthermore, we observe high vi-
sual privacy for both the datasets and low model mis-usability for
MNIST. The model mis-usability of 36.8% for CIFAR-10 36.8% is
significantly higher and implies potential risk of model misuse.
The per-record disguising cost for the MNIST dataset with the
above setting was less than 1 ms and resulted in images of size 8
KB whereas for the CIFAR-10 the costs were 13 ms and 33 KB.
Figure 5 shows RMT with permutation (w. perm.) over larger
block sizes improve model accuracy. Moreover, higher additive
noises result in lower quality models. Note: We see a slight im-
provement in accuracy for MNIST dataset with increasing block
sizes and without permutation (w/o perm.).
Figure 6 shows the result of applying a “DNN examiner“ on
disguised images to assess the preserved visual privacy. The DNN
examiners perform consistently low across the board for all block
sizes and noise levels for classifying the transformed images, result-
ing in around 90% preserved visual privacy. Finally, Figure 7 shows
that the model mis-usability reduces when the block sizes and the
noise level increase and when permutation is applied.
6 CONCLUSION
We propose several image disguising mechanisms to attain practical
privacy-preserving deep learning in the outsourced setting. Our
preliminary evaluation show highly encouraging results and some
y
c
a
r
u
c
c
A
.
g
v
A
100%
90%
80%
70%
60%
50%
N=0 w/o perm.
N=0 w. perm.
N=50 w/o perm.
N=50 w. perm.
2 × 2
4 × 4
8 × 8
16 × 16 32 × 32
y
c
a
r
u
c
c
A
.
g
v
A
100%
99%
98%
97%
96%
95%
N=0 w/o perm.
N=0 w. perm.
N=50 w/o perm.
N=50 w. perm.
2 × 2
4 × 4
7 × 7
14 × 14 28 × 28
Block sizes
Block sizes
Figure 5: Model quality of DNN models for different block
sizes and noise levels (N). CIFAR-10 left, MNIST right
N=50 w/o perm.
N=50 w. perm.
N=50 w/o. perm.
N=50 w. perm.
N=0 w/o perm.
N=0 w. perm.
N=0 w/o perm.
N=0 w. perm.
100%
94%
y
c
a
v
i
r
P
l
a
u
s
i
V
92%
90%
88%
86%
y
c
a
v
i
r
P
l
a
u
s
i
V
95%
90%
85%
2 × 2
4 × 4
8 × 8
16 × 16 32 × 32
80%
2 × 2
4 × 4
7 × 7
14 × 14 28 × 28
Block sizes
Block sizes
Figure 6: Visual privacy for different block sizes and noise
levels (N). CIFAR-10 left, MNIST right
60%
50%
40%
30%
20%
10%
y
t
i
l
i
b
a
s
u
-
s
i
M
l
e
d
o
M
N=0 w/o perm.
N=0 w. perm.
N=50 w/o perm.
N=50 w. perm.
N=0 w/o perm.
N=0 w. perm.
N=50 w/o perm.
N=50 w. perm.
60%
50%
40%
30%
20%
10%
y
t
i
l
i
b
a
s
u
-
s
i
M
l
e
d
o
M
2 × 2
4 × 4
8 × 8
16 × 16 32 × 32
2 × 2
4 × 4
7 × 7
14 × 14 28 × 28
Block sizes
Block sizes
Figure 7: Model mis-usability for different block sizes and
noise levels (N). CIFAR-10 left, MNIST right
interesting patterns that are to be further explored. We will extend
the evaluation to more datasets, include more image disguising
techniques, consider more stringent threat and attack models, and
finally establish a theoretical justification of the preserved privacy.
ACKNOWLEDGMENTS
This work is partially supported by the National Science Foundation
under Grant 1245847.
REFERENCES
[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang. Deep learning with differential privacy. 2016.
[2] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit
confidence information and basic countermeasures. Conference on Computer and
Communications Security, page 1322, 2015.
[3] T. Graepel, K. Lauter, and M. Naehrig. Ml confidential: Machine learning on
encrypted data. In Proceedings of the 15th International Conference on Information
Security and Cryptology, ICISC’12, Berlin, Heidelberg, 2013. Springer-Verlag.
[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.
CoRR, abs/1512.03385, 2015.
[5] B. Hitaj, G. Ateniese, and F. Pérez-Cruz. Deep models under the GAN: information
leakage from collaborative deep learning. CoRR, abs/1702.07464, 2017.
[6] T. Jeon. Classifying mnist dataset using cnn. http://yann.lecun.com/exdb/mnist/.
[7] M. Li, L. Lai, N. Suda, V. Chandra, and D. Z. Pan. Privynet: A flexible framework
for privacy-preserving deep neural network training with A fine-grained privacy
control. CoRR, abs/1709.06161, 2017.
[8] P. Mohassel and Y. Zhang. Secureml: A system for scalable privacy-preserving
machine learning. In 2017 IEEE Symposium on Security and Privacy (SP), 2017.
[9] A. Narayanan. Data privacy: The story of a paradigm shift, 2010.
[10] V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft. Privacy-
preserving ridge regression on hundreds of millions of records. In Proceedings of
the 2013 IEEE Symposium on Security and Privacy. IEEE Computer Society, 2013.
[11] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In Proceedings
of the 22nd ACM SIGSAC Conference on Computer and Communications Security,
2015.
[12] S. S. Vempala. The Random Projection Method. American Mathematical Society,
2005.