### Training and Implementation

The network is trained for 1000 iterations. For the CIFAR-10 dataset, the learning rate is adjusted from 0.1 to 0.001 after 350 iterations. Both models are implemented using TensorFlow.

### Table 2: Results of Applying Image Disguising Mechanisms

| Model Accuracy | MNIST | CIFAR-10 |
|----------------|-------|----------|
| With Disguise  | 94.4% | 89.7%    |
| Without Disguise | 96.7% | 93.4%    |

| Visual Privacy | MNIST | CIFAR-10 |
|----------------|-------|----------|
| With Disguise  | 95.6% | 89.3%    |
| Without Disguise | 95.6% | 89.3%    |

| Model Misusability | MNIST | CIFAR-10 |
|--------------------|-------|----------|
| With Disguise      | 9.2%  | 36.8%    |
| Without Disguise   | 9.2%  | 36.8%    |

**Table 2** shows that models trained on disguised images achieve accuracy very close to those trained on undisguised images. Additionally, high visual privacy is observed for both datasets, with low model misusability for MNIST. The model misusability of 36.8% for CIFAR-10 indicates a significant risk of potential misuse.

### Performance Metrics

- **MNIST Dataset:**
  - Per-record disguising cost: < 1 ms
  - Image size: 8 KB

- **CIFAR-10 Dataset:**
  - Per-record disguising cost: 13 ms
  - Image size: 33 KB

### Figure 5: Model Quality for Different Block Sizes and Noise Levels

- **CIFAR-10 (Left):**
  - Increasing block sizes with permutation (w. perm.) improves model accuracy.
  - Higher additive noise results in lower model quality.

- **MNIST (Right):**
  - Slight improvement in accuracy with increasing block sizes without permutation (w/o perm.).

### Figure 6: Visual Privacy for Different Block Sizes and Noise Levels

- **DNN Examiner:**
  - Consistently low performance across all block sizes and noise levels for classifying transformed images.
  - Approximately 90% visual privacy is preserved.

### Figure 7: Model Misusability for Different Block Sizes and Noise Levels

- **Reduction in Misusability:**
  - Increases in block sizes and noise levels, along with permutation, reduce model misusability.

### Conclusion

We propose several image disguising mechanisms to achieve practical privacy-preserving deep learning in an outsourced setting. Our preliminary evaluation shows highly encouraging results, and we plan to extend the evaluation to more datasets, include additional image disguising techniques, consider more stringent threat and attack models, and establish a theoretical justification for the preserved privacy.

### Acknowledgments

This work is partially supported by the National Science Foundation under Grant 1245847.

### References

1. M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. 2016.
2. M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. Conference on Computer and Communications Security, page 1322, 2015.
3. T. Graepel, K. Lauter, and M. Naehrig. ML confidential: Machine learning on encrypted data. In Proceedings of the 15th International Conference on Information Security and Cryptology, ICISC’12, Berlin, Heidelberg, 2013. Springer-Verlag.
4. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.
5. B. Hitaj, G. Ateniese, and F. Pérez-Cruz. Deep models under the GAN: information leakage from collaborative deep learning. CoRR, abs/1702.07464, 2017.
6. T. Jeon. Classifying MNIST dataset using CNN. http://yann.lecun.com/exdb/mnist/.
7. M. Li, L. Lai, N. Suda, V. Chandra, and D. Z. Pan. Privynet: A flexible framework for privacy-preserving deep neural network training with fine-grained privacy control. CoRR, abs/1709.06161, 2017.
8. P. Mohassel and Y. Zhang. SecureML: A system for scalable privacy-preserving machine learning. In 2017 IEEE Symposium on Security and Privacy (SP), 2017.
9. A. Narayanan. Data privacy: The story of a paradigm shift, 2010.
10. V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft. Privacy-preserving ridge regression on hundreds of millions of records. In Proceedings of the 2013 IEEE Symposium on Security and Privacy. IEEE Computer Society, 2013.
11. R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015.
12. S. S. Vempala. The Random Projection Method. American Mathematical Society, 2005.