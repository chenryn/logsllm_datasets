detailed database, only keeping track of allocated blocks
and not the owners. In this case, we take the last alloca-
tion that contains our sample IP address – note a single
IP address might be reallocated several times, as part of
different IP blocks – i.e., the smallest block, as its owner.
2.3.1 Mapping Process
In the following paragraphs we explain in detail the ma-
nual process of (1): (1a) extracting sample IP addresses
through a number of examples, and (1b) identifying the
aggregation unit using the sample IP address. The ge-
neral outline of the process for (1a) is that we ﬁrst read
the report concerning each incident, and extract the web-
site of the company involved. If the website is the in-
trusion point in the breach, or indicative of the compro-
mised network, then we take the address of this website
to be our sample IP address. The website is determined
to be indicative of the compromised network when the
owner ID for the sample IP address matches the reported
name of the victim network. Occasionally the victim net-
work can be identiﬁed separately regardless of the web-
site address, but in most cases this is found to be an ef-
fective way of quickly obtaining the owner ID.
Our ﬁrst example [21] is a website defacement target-
ing the ofﬁcial website of the City of Mansﬁeld, Ohio.
Since the point of intrusion is clearly the website, we
take its address as our sample IP address for this inci-
dent. Note that in this case the website might be ma-
naged by a 3rd party hosting company, a possibility dis-
cussed further when we explain the process to address
(1b). The second example [6] is on Evernote resetting all
user passwords following an attack on its online system.
For this incident we identify said domain (evernote.com),
1012  24th USENIX Security Symposium 
USENIX Association
and trace it to an IP block in ARIN’s database registered
to Evernote Corporation. Since this network is main-
tained by Evernote itself, we take evernote.com to be our
sample IP address. Our ﬁnal example [10] involves the
defacement of Google Kenya and Google Burundi web-
sites. As the report suggests, the hackers altered the DNS
records of the domains by hacking into the Kenya and
Burundi NICs. Since the attack was not through directly
compromising the defaced websites, we excluded this
incident – the victim in this incident is neither Google
Kenya nor Google Burundi, but the networks owned by
the NICs.
The above examples provide insight into the manual
process of mapping incident to a network address. For
a large portion of the reports the incident descriptor is
unique and should therefore be treated as such; this is
the main reason that such a mapping is primarily done
manually. For a signiﬁcant portion (∼ 95%) of the re-
ports we are able to identify the compromised network
with a high level of conﬁdence – in such cases either the
report explicitly cites the website as the intrusion point
(ﬁrst example), or the network identiﬁed by the website
is registered under the victim organization (second exam-
ple). When neither of these conditions is satisﬁed, this
incident is excluded unless we can identify the victim
network through alternative means; such cases are few.
Overall our process is a conservative one: we only in-
clude an incident when there is zero or minimal ambigu-
ity. Finally, we also remove duplicate owner IDs in order
to avoid a bias against commonly used hosting compa-
nies (e.g. Amazon, GoDaddy) in our training and testing
process.
We now explain the process used in (1b) to map an ob-
tained sample IP address (as well as the identiﬁed owner
ID) to network(s) operated by a single entity. The ge-
neral outline of this process is as follows: we take all
the IP blocks that have the same owner ID listed in the
RIR databases, excluding sub-blocks that have been re-
allocated to other organizations, as our aggregation unit.
Continuing with the same set of examples, in the case
of Evernote (second example) we reverse search ARIN’s
database and extract all IP blocks registered to Evernote
Corporation, giving us a total of 520 IP addresses. For
the case of the City of Mansﬁeld website, using records
kept by ARIN we see that its web address belongs to Lin-
ode, a cloud hosting company. Obviously Linode is also
hosting other entities on its network without reported in-
cidents. Nonetheless, in this case we take the network
owned by Linode as our aggregation unit, since we can-
not further differentiate the source IP address(es) more
closely associated with the city. The inclusion of such
cases is a tradeoff as excluding them would have left
us with too few samples to perform a meaningful study.
More on this is discussed in Section 2.4.
2.3.2 A global table of aggregation units
The above explains how we process the incident reports
to identify network units that should be given a label
of “1”, i.e., victim organizations. For training and test-
ing purposes we also need to identify network units that
should be given a label of “0”, i.e., non-victim organi-
zations. To accomplish this, we built a global table us-
ing information gathered from the RIRs that provides us
with a global aggregation rule, containing both victim
and non-victim organizations. Our global table contains
4.4 million preﬁxes listed under 2.6 million owner IDs.
Note that the number of preﬁxes in the RIR databases
is considerably larger than the global BGP routing ta-
ble size, which includes roughly 550,000 unique preﬁxes
[41]. This is partly due to the fact that the preﬁxes in
our table can overlap for those that have been reallocated
multiple times. In other words, the RIR databases can
be viewed as a tree indicating all the ownership alloca-
tions and reallocations over the IP address space. On the
other hand, the BGP table tends to combine preﬁxes that
are located within the same Autonomous System (AS),
in order to reduce routing table sizes. Therefore, the RIR
databases provide us with a ﬁner-grained look into the IP
address space. By taking all the IP addresses that have
been allocated to an organization, and have not been fur-
ther reallocated, we can break the IP address space into
mutually exclusive sets, each owned and/or maintained
by a single organization. Out of the 4.4 million preﬁxes,
300,000 of them are assigned by LACNIC and therefore
have no owner ID. Combined with the 2.6 million owner
IDs from the other registries, the IP address space is bro-
ken, by ownership (or LACNIC preﬁxes), into 2.9 mil-
lion sets. Each set constitutes an aggregation unit that is
given a label of “0”, except for those already identiﬁed
and labeled as “1” by the previous process.
2.3.3 Aggregation Process
Once these aggregation units are identiﬁed, the second
step (2) is relatively straightforward. For each misma-
nagement symptom we simply calculate the fraction of
symptomatic IPs within such a unit. For malicious acti-
vities, we count the number of unique IP addresses listed
on a given day (by a single blacklist, or by blacklists
monitoring the same type of malicious activities) that be-
long to this unit; this results in one or more time series
for each unit. This step is carried out in the same way for
both victim and non-victim organizations.
2.4 A Few Caveats
As already alluded to, our data processing consists of a
series of rules of thumb that we follow to make the data
useable, some perhaps less clear-cut than others. Below
USENIX Association  
24th USENIX Security Symposium  1013
we summarize the typical challenges we encounter in this
process and their possible implications on the prediction
performance.
As described in Section 2.3, the aggregation units
are deﬁned using ownership information from RIR
databases. One issue with the use of ownership infor-
mation is that big corporations tend to register their IP
address blocks under multiple owner IDs, and in our pro-
cessing these IDs are treated as separate organizations.
In principle, as long as each of the aggregation units is
non-trivial in size, each can have its own security pos-
ture assessed. Furthermore, in some cases it is more ac-
curate to treat such IDs separately, since they might rep-
resent different sections of an organization under differ-
ent management. The opposite issue also exists, where
it may be impossible to distinguish between the network
assets of multiple organizations; recall, e.g. our ﬁrst ex-
ample where multiple organizations are hosted on the
same network. As mentioned before, we have chosen
in such cases to use the owner ID as the aggregation unit.
While this mapping process is clearly non-ideal, it is a
best-effort attempt at the problem, and will instead pro-
vide the classiﬁer with the average value of the features
over all organizations hosted on the identiﬁed network.
The labels for our classiﬁer are extracted from real in-
cident reports, and we can safely assume that the amount
of false positives in these reports, if any, is negligible.
However data breach incidents are only reported when
an external source detects the data breach (e.g. website
defacements), or an organization is obligated to report
the incident due to private customer information getting
compromised. In general, organizations tend not to an-
nounce incidents publicly, and security incidents remain
largely under-reported. This will affect our classiﬁer in
two ways: First, by failing to incorporate all incidents in
our training set, we may fail to identify all of the factors
that might affect an organization’s likelihood of suffer-
ing a breach. Second, when choosing non-victim organi-
zations, it is possible that we select some of them from
unreported victims, which could further impact the ac-
curacy of our classiﬁer. We have tried to overcome this
challenge by using three independently maintained in-
cident datasets. Ultimately, however, this can only be
addressed when timely incident reporting becomes the
norm; more on this is discussed in Section 5.
Last but not least, all the raw security posture data
(mismanagement symptoms and blacklists) could con-
tain error, which we have no easy way of calibrating.
However, two aspects of the present study help mitigate
the potential impact of these noises. Firstly, we use many
different datasets from independent sources; the diver-
sity and the total volume generally have a dampening
effect on the impact of the noise contained in any sin-
gle source. Secondly and perhaps more importantly, our
ultimate veriﬁcation and evaluation of the prediction per-
formance are not based on the security posture data, but
on the incident reports (with their own issues as noted
above). In this sense, as long as the prediction perfor-
mance is satisfactory, the noise in the input data becomes
less relevant.
3 Forecasting Methodology
The key to our prediction framework is the construction
of a good classiﬁer. We will primarily focus on the Ran-
dom Forest (RF) method [37], which is an ensemble cla-
ssiﬁer and an enhancement to the classical random de-
cision tree method.
It uses randomly selected subsets
of samples to construct different decision trees to form
a forest, and is generally considered to work well with
large and diverse feature sets. In particularly, it has been
observed to work well in several Internet measurement
studies, see e.g., [57]. As a reference, we will also pro-
vide performance comparison by using the Support Vec-
tor Machine (SVM) [27], one of the earliest and most
common classiﬁers. To train a classiﬁer, we need to iden-
tify a set of features from the measurement data. Below,
we ﬁrst detail the set of features used, and then present
the training and testing procedures.
3.1 Feature Set
We shall use two types of features, a primary set and a
secondary set. The primary set of features consists of the
raw data, while the secondary set is derived or extracted
from the raw data, i.e., in the form of various statistics.
In all, 258 features are used, including 5 mismanagement
features, 180 primary features, 72 secondary features,
and a last feature on the organization size.
3.1.1 Primary Features (186)
Mismanagement symptoms (5). There are ﬁve symp-
toms; each is measured by the ratio between the number
of misconﬁgured systems and the total number of sys-
tems in an organization. For instance, for the untrusted
HTTPS certiﬁcates, this ratio is between the number of
misconﬁgured certiﬁcates over the total number of cer-
tiﬁcates discovered in an organization. Similarly, for
open SMTP mail relay this ratio is between the num-
ber of misconﬁgured mail servers and the total number
of mail servers. The only exception is in the case of open
recursive resolver: since we do not know the total num-
ber of open resolvers, this ratio is between the number
of misconﬁgured open DNS resolvers and the total num-
ber of IPs in an organization. These ratios are denoted as
mi ∈ [0,1]5 for organization i.
1014  24th USENIX Security Symposium 
USENIX Association
Malicious activity time series (60 × 3). For each or-
ganization we collect three separate time series, one for
each malicious activity type, namely spam, phishing, and
scan. Accordingly, for organization i, its time series data
are denoted by rSP
. These time series data are
i
directly fed in their entirety into the classiﬁer. Several
examples of rSP
are given in Fig. 1; these are collected
i
over a two-month (60 days) period and show the total
number of unique IPs blacklisted on each day over all
spam blacklists in our dataset.
,rSC
i
,rPH
i
within the noise inherent in the blacklists), while Exam-
ples 2 and 3 show much higher levels of activity in gen-
eral. These two, however, differ in how persistent they
are at those high levels. Example 2 shows a network
with high levels throughout this period, while Example
3 shows a network that ﬂuctuates much more wildly. In-
tuitively, such dynamic behavior reﬂects to a large degree
how responsive the network operators are to blacklisting,
i.e., time to clean up, time to resurfacing of malicious ac-
tivities, and so on.
4
3
2
1
0
1k
800
600
400
10
20
30
Days
40
50
60
10
20
30
Days
40
50
60
10k
8k
6k
4k
2k
10
20
30
Days
40
50
60
(a) Org. 1
(b) Org. 2
(c) Org. 3
Figure 1: Examples of malicious activity time series of
three organizations; Y-axis is the number of unique IP
addresses listed on all spam blacklists in each day over a
60-day period.
Size (1). This refers to the size of an organization in
terms of the number of IP addresses identiﬁed within that
organization’s aggregation unit as outlined in the previ-
ous section. For organization i, this is denoted by si.
The relevance of these symptoms to an organization’s
security posture is examined more closely by comparing
their distributions among the victim and the non-victim
populations, as shown in Fig. 2. We see a clear difference
between the two populations in their untrusted HTTPS
and Openresolver distributions. This difference suggests
that these symptoms are meaningful distinguishers, and
thus hold predictive power. This is indeed veriﬁed later
when these two symptoms emerge as the most indicative
of the ﬁve. By contrast, the other three mismanagement
symptoms appear much less powerful.
The relevance of the malicious activity time series will
be examined more closely in the next section, within the
context of their secondary features. Lastly, the organi-
zation size can to some extent capture the likelihood of
an organization becoming a target of intentional attacks,
and is therefore included in the feature set.
3.1.2 Secondary Features (72)
In determining what type of statistics to extract to serve
as secondary features, we aim to capture distinct beha-
vioral patterns in an organization’s malicious activities,
particularly concerning their dynamic changes. To illu-
strate, the three examples given in Fig. 1 show drastically
different behavior: Org. 1 shows a network with consis-
tently low level of observed malicious IPs (and possibly
d
e
t
s
9k
8k
7k
6k
5k
4k
3k
i
l
s
P
I
f
o
#
Persistency
10
20
30
Days
40
50
60
Figure 3: Extracting secondary features. The solid red
line indicates time-average of the signal while the two
dotted lines denote the boundary of different regions.
The region above is “bad” with higher-than-average ma-
licious activities, while the region below is “good” with
lower-than-average activities. Persistency refers to the
duration the time series persist in the same region.
These observed differences motivate us to collect
statistics summarizing such behavioral patterns by mea-
suring their persistence and change, e.g., how big is the
change in the magnitude of malicious activities over time
and how frequently does it change. To balance the ex-
pressiveness of the features and their complexity, we
shall do so by ﬁrst value-quantizing a time series into
three regions relative to its time average: “good”, “nor-
mal” and “bad”. An illustration is given in Fig. 3 using
one of the examples shown earlier (Org. 3). The solid
line marks the average magnitude of the time series over
the observation period; the dotted lines then outline the
“normal” region, i.e., a range of magnitude values that
are relatively close (either from above or below) to its
time-average. The region above the top dotted line is a-
ccordingly referred to as the “bad” region, showing large
number of malicious IPs, and the region below the bot-
tom dotted line the “good” region, with a smaller number
of malicious IPs, both relative to its average1.
An additional motivation behind this quantization step
is to capture certain onset and departure of “events”, such
as a wide-area infection, or scheduled patching and soft-
ware update, etc. Viewed this way, the duration an orga-
1The choice on the size of the normal region may lead to differences
in classiﬁer performance, which is discussed in more detail in Section
5.3. In most of our experiments ±20% of the time average is used.