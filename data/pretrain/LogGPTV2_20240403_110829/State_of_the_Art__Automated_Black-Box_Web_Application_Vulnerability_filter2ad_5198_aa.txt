title:State of the Art: Automated Black-Box Web Application Vulnerability
Testing
author:Jason Bau and
Elie Bursztein and
Divij Gupta and
John C. Mitchell
2010 IEEE Symposium on Security and Privacy
State of the Art: Automated Black-Box Web Application Vulnerability Testing
Jason Bau, Elie Bursztein, Divij Gupta, John Mitchell
Stanford University
{jbau, divijg}@stanford.edu, {elie, mitchell}@cs.stanford.edu
Stanford, CA
Abstract—Black-box web application vulnerability scanners
are automated tools that probe web applications for security
vulnerabilities. In order to assess the current state of the art, we
obtained access to eight leading tools and carried out a study
of: (i) the class of vulnerabilities tested by these scanners, (ii)
their effectiveness against target vulnerabilities, and (iii) the
relevance of the target vulnerabilities to vulnerabilities found
in the wild. To conduct our study we used a custom web
application vulnerable to known and projected vulnerabilities,
and previous versions of widely used web applications con-
taining known vulnerabilities. Our results show the promise
and effectiveness of automated tools, as a group, and also
some limitations. In particular, “stored” forms of Cross Site
Scripting (XSS) and SQL Injection (SQLI) vulnerabilities are
not currently found by many tools. Because our goal is to
assess the potential of future research, not to evaluate speciﬁc
vendors, we do not report comparative data or make any
recommendations about purchase of speciﬁc tools.
Keywords-Web Application Security; Black Box Testing;
Vulnerability Detection; Security Standards Compliance;
I. INTRODUCTION
Black-box web application vulnerability scanners are au-
tomated tools that probe web applications for security vul-
nerabilities, without access to source code used to build the
applications. While there are intrinsic limitations of black-
box tools, in comparison with code walkthrough, automated
source code analysis tools, and procedures carried out by
red teams, automated black-box tools also have advantages.
Black-box scanners mimic external attacks from hackers,
provide cost-effective methods for detecting a range of im-
portant vulnerabilities, and may conﬁgure and test defenses
such as web application ﬁrewalls. Since the usefulness of
black-box web scanners is directly related to their ability
to detect vulnerabilities of interest to web developers, we
undertook a study to determine the effectiveness of leading
tools. Our goal in this paper is to report test results and
identify the strengths of current tools, their limitations, and
strategic directions for future research on web application
scanning methods. Because this is an anonymized confer-
ence submission, we note that the authors of this study are
university researchers.
Web application security vulnerabilities such as cross-site
scripting, SQL injection, and cross-site request forgeries are
acknowledged problems with thousands of vulnerabilities
reported each year. These vulnerabilities allow attackers to
1081-6011/10 $26.00 © 2010 IEEE
DOI 10.1109/SP.2010.27
332
perform malevolent actions that range from gaining unau-
thorized account access [1] to obtaining sensitive data such
as credit card numbers [2]. In the extreme case, these vulner-
abilities may reveal the identities of intelligence personnel
[3]. Because of these risks, web application vulnerability
remediation has been integrated into the compliance pro-
cess of major commercial and governmental standards, e.g.
the Payment Card Industry Data Security Standard (PCI
DSS), Health Insurance Portability and Accountability Act
(HIPAA), and the Sarbanes-Oxley Act. To meet these man-
dates, web application scanners that detect vulnerabilities,
offer remediation advice, and generate compliance reports.
Over the last few years, the web vulnerability scanner market
as become a very active commercial space, with, for exam-
ple, more than 50 products approved for PCI compliance
[4].
This paper reports a study of current automated black-
box web application vulnerability scanners, with the aim of
providing the background needed to evaluate and identify
the potential value of future research in this area. To the
best of our knowledge this paper is the most comprehensive
research on any group of web scanners to date. Because we
were unable to ﬁnd competitive open-source tools in this
area (see Section VII), we contacted the vendors of eight
well-known commercial vulnerabilities scanners and tested
their scanners against a common set of sample applications.
The eight scanners are listed in Table I. Our study aims to
answer these three questions:
1) What vulnerabilities are tested by the scanners?
2) How representative are the scanner tests of vulnera-
bility populations in the wild?
3) How effective are the scanners?
Because our goal is to assess the potential impact of
future research, we report aggregate data about all scanners,
and some data indicating the performance of the best-
performing scanner on each of several measures. Because
this is not a commercial study or comparative evaluation of
individual scanners, we do not report comparative detection
data or provide recommendations of speciﬁc tools. No single
scanner is consistently top-ranked across all vulnerability
categories.
We now outline our study methodology and summarize
our most signiﬁcant ﬁndings. We began by evaluating the
set of vulnerabilities tested by the scanners. Since most
of the scanners provide visibility into the way that target
vulnerability categories are scanned, including details of
the distribution of their test vector sets by vulnerability
classiﬁcation, we use this and other measures to compare the
scanner target vulnerability distribution with the distribution
of in-the-wild web application vulnerabilities. We mine the
latter from incidence rate data as recorded by VUPEN
security [5], an aggregator and validator of vulnerabilities
reported by various databases such as the National Vul-
nerability Database (NVD) provided by NIST [6]. Using
database results, we also compare the incidence rates of web
application vulnerability as a group against incidence rates
for system vulnerabilities (e.g. buffer overﬂows) as group.
In the ﬁrst phase of our experiments, we evaluate scan-
ner detection performance on established web applications,
using previous versions of Drupal, phpBB, and Wordpress,
released around January 2006, all of which include well-
known vulnerabilities. In the second phase of our experi-
ments, we construct a custom testbed application containing
an extensive set of contemporary vulnerabilities in pro-
portion with the vulnerability population in the wild. Our
testbed checks all of the vulnerabilities in the NIST Web
Application Scanner Functional Speciﬁcation [7] and tests
37 of the 41 scanner vulnerability detection capabililities
in the Web Application Security Consortium [8] evaluation
guide for web application scanners. (See Section VII).
Our testbed application also measures scanner ability to
understand and crawl links written in various encodings and
content technologies.
We use our custom application to measure elapsed scan-
ning time and scanner-generated network trafﬁc, and most
importantly, we tested the scanners for vulnerability detec-
tion and false positive performance.
Our most signiﬁcant ﬁndings include:
1) The vulnerabilities for which the scanners test most
extensively are,
Information Disclosure,
Cross Site Scripting (XSS), SQL Injection, and other
forms of Cross Channel Scripting (XCS). This testing
distribution is roughly consistent with the vulnerability
population in the wild.
in order,
2) Although many scanners are effective at following
links whose targets are textually present in served
pages, most are not effective at following links through
active content technologies such as Java applets, Sil-
verLight, and Flash.
3) The scanners as a group are effective at detecting
well-known vulnerabilities. They performed capably
at detecting vulnerabilities already reported to VuPen
from historical application versions. Also, the scanners
detected basic “reﬂected” cross-site scripting well,
with an average detection rate of over 60%.
4) The scanner performed particularly poorly at detect-
ing “stored” vulnerabilities. For example, no scanner
Table I
STUDIED VULNERABILITY SCANNERS
Product
Company
Acunetix WVS
Cenzic
HailStorm Pro
Version
6.5
6.0
HP
IBM
McAfee
N-Stalker
Qualys
Rapid7
8.0
WebInspect
Rational AppScan
7.9
McAfee SECURE Web
QA Edition
7.0.0
QualysGuard PCI Web
4.8.0
NeXpose
Scanning Proﬁles Used
Default and Stored XSS
Best Practices, PCI
Infrastructure, and Session
All Checks
Complete
Hack Simulation and DoS
Everything
N/A
PCI
detected any of our constructed second-order SQLI
vulnerabilities, and the stored XSS detection rate was
only 15%. Other limitations are discussed further in
this paper.
Our analysis suggests room for improvement in detecting
vulnerabilities inserted in our testbed, and we propose po-
tential areas of research in Section VIII. However, we have
made no attempt to measure the ﬁnancial value of these tools
to potential users. Scanners performing as shown may have
signiﬁcant value to customers, when used systematically as
part of an overall security program. In addition, we did not
quantify the relative importance of detecting speciﬁc vulner-
abilities. In principle, a scanner with a lower detection rate
may be more useful if the smaller number of vulnerabilities
it detects are individually more important to customers.
Section II of this paper discusses the black box scanners
and their vulnerability test vectors. Section III establishes
the population of reported web vulnerabilities. Section IV
presents scanner results on Wordress, phpBB, and Drupal
versions released around January 2006. Section V discusses
testbed results by vulnerability category for the aggregated
scanner set and also false positives. Section VI contains
some remarks by scanner, on individual scanner performance
as well as user experience. Section VII discusses related
work and section VIII concludes by highlighting research
opportunities resultant from this work.
II. BLACK BOX SCANNERS
We begin by describing the general usage scenario and
software architecture of the black-box web vulnerability
scanners. We then discuss the vulnerability categories which
they aim to detect, including test vector statistics where
available. Table I lists the eight scanners incorporated in
our study, which include products from several of the
most-established security companies in the industry. All the
scanners in the study are approved for PCI Compliance
testing [4]. The prices of the scanners in our study range
from hundreds to tens-of-thousands of dollars. Given such
a wide price range and also variations in usability, potential
customers of the scanners would likely not make a purchase
decision on detection performance alone.
333
Info leaks
Conﬁguration
CSRF
Session
XCS
SQLI
XSS
0
10
20
30
40
50
Figure 1. Scanner Test Vector Percentage Distribution
A. Usage Scenario
To begin a scanning session using a typical scanner, the
user must enter the entry URL of the web application as
well as provide a single set of user login credentials for
this application. The user then must specify options for the
scanner’s page crawler, in order to maximize page scanning
coverage. Most scanners tested allow a “crawl-only” mode,
so that the user can verify that the provided login and the
crawler options are working as expected. After setting the
crawler, the user then speciﬁes the the scanning proﬁle, or
test vector set, to be used in the vulnerability detection
run, before launching the scan. All scanners can proceed
automatically with the scan after proﬁle selection, and most
include interactive modes where the user may direct the
scanner to scan each page. In our testbed experiments,
we always set
in automated mode,
the most comprehensive set of tests available, to maximize
vulnerability detection capability.
the scanner to run,
B. Software Architecture Descriptions
We ran two of the tested scanners, McAfee and Qualys, as
remote services whereby the user conﬁgures the scanner via
a web-interface before launching the scan from a vendor-
run server farm. The other six scanners were tested as
software packages running on a local computer, although
the NeXpose scanner runs as a network service accessed by
browser via an IP port (thus naturally supporting multiple
scanner instances run by one interface). All scanners, as
would be expected of black box web-application testers,
generate http requests as test vectors and analyze the http
response sent by the web server for vulnerabilities. All local
scanner engines seem to run in a single process, except for
the Cenzic scanner, which runs a separate browser process
that appears to actually render the http response in order to
ﬁnd potential vulnerabilities therein.
334
CONSENSUS VULNERABILITY CLASSIFICATION ACROSS SCANNERS
Table II
Classiﬁcation
Cross-Site Scripting (XSS)
SQL Injection (SQLI)
Cross Channel Scripting
Session Management
Cross-Site Request Forgery
SSL/Server Conﬁguration
Information Leakage
Example Vulnerability
Cross-Site Scripting
SQL Injection
Arbitrary File Upload
Remote File Inclusion
OS Command Injection
Code Injection
Session Fixation
Session Prediction
Authentication Bypass
Cross Site Request Forgery
SSL Misconﬁguration
Insecure HTTP Methods
Insecure Temp File
Path Traversal
Source Code Disclosure
Error Message Disclosure
C. Vulnerability Categories Targeted by Scanners
As each scanner in our study is qualiﬁed for PCI com-
pliance, they are mandated to test for each of the Open
Web Application Security Project (OWASP) Top Ten 2007
[9] vulnerability categories. We also examine the scanning
proﬁle customization features of each scanner for further
insight into their target vulnerability categories. All scanners
except Rapid7 and Qualys allow views of the scanning
proﬁle by target vulnerability category, which are often
direct from the OWASP Top Ten 2007 and 2010rc1, Web
Application Security Consortium (WASC) Threat Classiﬁca-
tion version 1 [10], or the Common Weakness Enumeration
(CWE) top 25 [11]. In fact, each of the six allow very
ﬁne-grained test customization, resulting in a set of over
100 different targeted vulnerability categories, too numerous
to list here. However, when related vulnerability categories
were combined into more general classiﬁcations, we were
able to ﬁnd a set of consensus classiﬁcations for which all
tools test. Table II presents this list of consensus classiﬁ-
cations, along with some example vulnerabilities from each
classiﬁcation. We have kept Cross-Site Scripting and SQL
Injection as their own vulnerability classiﬁcations due to
their preponderant rate of occurrence (supported by “in the
wild” data in the next section) and their targeting by all
scanners. The Cross Channel Scripting classiﬁcation [12]
includes all vulnerabilities,
including those listed in the
table, allowing the user to inject code “across a channel”
onto the web server that executes on the server or a client
browser, aside from XSS and SQLI.
D. Test Vector Statistics
We were able to obtain detailed enough test proﬁle infor-
mation for four scanners (McAfee, IBM, HP, and Acunetix)
to evaluate how many test vectors target each vulnerabilities
classiﬁcation, a rough measure of how much “attention”
scanner vendors devote to each classiﬁcation. Figure 1 plots
the percentage of vectors targeting each classiﬁcation aggre-
gated over the four scanners. The results show that scanners
devote most testing to information leakage vulnerabilities,
followed by XSS and SQLI vulnerabilities.
III. VULNERABILITY POPULATION FROM
VUPEN-VERIFIED NVD
In order to evaluate how well the vulnerability categories
tested by the scanners represent the web application vulner-
ability population “in the wild”, we took all of the web vul-
nerability categories forming the consensus classiﬁcations
from Table II and performed queries against the VUPEN
Security Vulnerability Notiﬁcation Service database for the
years 2005 through 2009. We chose this particular database
as our reference as it aggregates vulnerabilities, veriﬁes them
through the generation of successful attack vectors, and
reports them to sources such as the Common Vulnerabilities
and Exposures (CVE) [13] feed of the National Vulnerability
Database.
We collected from the VUPEN database the relative
incidence rate trends of the web application vulnerability
classes, which are plotted in Figure 2. Figure 3 plots
incidences of web application vulnerabilities against
in-
cidences of system vulnerabilities, e.g. Buffer Overﬂow,
Integer Overﬂow, Format String, Memory Corruption, and
Race Conditions, again collected by us using data from
VUPEN.
Figure 2 demonstrates that Cross-Site Scripting, SQL
Injection, and other forms of Cross-Channel Scripting have
consistently counted as three of the top four reported web
application vulnerability classes, with Information Leak be-
ing the other top vulnerability. These are also the top four
vulnerability classes by scanner test vector count. Within
these four, scanner test vectors for Information Leak amount
to twice that of any other vulnerability class, but the Infor-
mation Leak incidence rates in the wild are generally lower
than that of XSS, SQLI, and XCS. We speculate that perhaps
test vectors for detecting information leakage, which may
be as simple as checking for accessible common default
pathnames, are easier to create than other test types. Overall,
however, it does appear that the testing emphasis for black-
box scanners as a group is reasonably proportional to the