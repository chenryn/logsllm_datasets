callee will be smaller. We apply a factor Φ(CN ) = ϕ·CN +1
to
ϕ·CN
denote this effect, where ϕ is a constant value (usually, ϕ = 2).
(2) The number of basic blocks CB in the caller that contains at
least one call site of the callee. The rationale is that, with more
branches that have a call site, more different execution traces
will include the callee. The factor function Ψ(CB) = ψ ·CB +1
ψ ·CB
denotes this effect, and ψ is a constant value (usually, ψ = 2).
Note that both factor functions are monotone decreasing func-
tions; also, Φ converges to 1 when CN → ∞ and Ψ converges to 1
when CB → 1. Given a (direct or indirect) immediate function call
pair (f1, f2) where f1 is the caller and f2 is the callee, the original
distance between f1 and f2 is 1 (see AFLGo [6]). Now, with the two
metrics mentioned above, we can define the augmented distance
between the function pairs that holds an immediate call relation.
The final adjustment factor will be a multiplication of Φ and Ψ, and
the augmented adjacent-function distance is
′
f (f1, f2) = Ψ(f1, f2) · Φ(f1, f2)
(1)
where d′
f (f1, f2) refers to the augmented direct function distance.
As an example, in Fig. 4a, for fb, CN (fa, fb) = 2, CB(fa, fb) = 2;
and for fc, CN (fa, fc) = 1, CB(fa, fc) = 1. Assume ϕ = 2 and
ψ = 2 and assume the original distance df (fa, fb) = df (fa, fc) = 1,
d
4 · 5
2 · 3
4 = 1.56.
2 = 2.25, and
f (fa, fc) = 3
the augmented distances will be d′
d′
f (fa, fb) = 5
A special case not shown in the above examples is that some
branches form cycles (i.e., loops). Indeed, these functions may be
called multiple times at runtime. However, it is uncertain that how
many times they will be executed across different runs when fed
with different seeds. Fortunately, actual execution on one call site
of a callee inside one loop typically has similar effect — the loop
explores the similar program states and benefits less in covering new
paths. Hence, the function call inside the loop does not bring many
execution trace diversities like the scenario where the same callee
occurs in multiples branches with significantly different parameters.
The applied approach aims to make a trade-off between the
efficiency and the utility of the static analysis. Therefore, we do
not consider the solution space of any branch condition that may
affect the runtime reachability in the CFG. For example, in Fig. 4a, if
we change the condition check i>0 to be i==0, the true branch will
be executed only when the input value of i is 0. It is tempting to
assign a smaller distance to the code segments in the false branch.
However, since the PUT is usually nontrivial, it is impractical to
statically formulate the exact constraint set of the preconditions
before reaching function fa and predicate the branches’s actual
execution probabilities. One common scenario is that the branch
condition i==0 is used for checking the return status code of an
external function call, at runtime it may actually execute the true
branch more often than the false branch.
(cid:40)undefined.
4.3 Directedness Utility Computation
In §4.2, the augmented function distance is calculated on two ad-
jacent functions according to their call patterns. By assigning the
adjacent-function distance as the weight of the edges in the call
graph, we can calculate the function level distance for any two
functions with the Dijkstra shortest path algorithm, beyond which
we can further derive the basic block level distance. Besides, we
also compute the target function trace closure which will be used
to calculate the covered function similarity in §4.4.
Function Level Distance. This distance is calculated according
to CG. It tells the (average) distance from the current function
to target functions. Given a function n, its distance to the target
function set Tf is defined as:
if R(n,Tf ) = ∅
df (n,Tf ) =
where R(n,Tf ) =(cid:8)tf |reachable(n, tf )(cid:9), which is the set of tar-
[Σtf ∈R(n,Tf )df (n, tf )−1]−1
otherwise
(2)
get functions that can be statically reached from n in CG, and
df (n, tf ) is the dijkstra shortest path based on augmented function
distance from n to a given target function tf in CG.
Basic Block Level Distance. Given a function n and two basic
b(m1, m2) is
blocks m1 and m2 inside, the basic block level distance do
defined as the minimal number of edges from m1 to m2 in the CFG
G(n). The set of functions called inside basic block m is denoted as
Cf (m), then CT
m|∃G(n), m ∈ G(n), n ∈ F , CT
, where F is the set of all
functions. Given a basic block m, its distance to the target basic
f (m) =(cid:8)n|R(n,Tf ) (cid:44) ∅, n ∈ Cf (m)(cid:9), and Transb =
f (m) (cid:44) ∅(cid:111)
(cid:110)
blocks Tb are defined as:
db(m,Tb) =
0
c · minn∈CT
[Σt ∈T r ansb(do
f (m)(df (n,Tf ))
b(m, t) + db(t,Tb))−1]−1
if m ∈ Tb
if m ∈ Transb
otherwise
(3)
where c is a constant that magnifies function level distance.
Note that Equation 2 and 3, on their own, are the same as those
in AFLGo [6]. However, df (n, tf ) for these equations in AFLGo is
simply the Dijkstra shortest distance on a CG where the weight of
edges (i.e., adjacent function distance) is 1.
Target Function Trace Closure. This utility, ξf (Tf ), is calcu-
lated by collecting all the predecessors that can statically lead to the
target functions Tf , until the entry function main has been reached.
We choose not to exclude those that are considered unreachable
from entry function due to the limitations of static analysis. In the
example in Fig. 2, ξf (Tf ) = {a, b, c, d, e,T}.
4.4 Power Scheduling
During dynamic fuzzing, we apply power scheduling on a selected
seed based on two dynamically-computed metrics: basic block trace
distance and target function trace similarity.
Basic Block Trace Distance. The distance between the seed s
to the target basic blocks Tb is defined as:
Σm∈ξb(s)db(m,Tb)
ds(s,Tb) =
|ξb(s)|
(4)
where ξb(s) is the execution trace of a seed s and contains all the
basic blocks that are executed. Hence, the basic idea of Equation 4 is
that: for all the basic blocks in the execution trace of s, we calculate
the average basic block level distance to the target basic blocks Tb.
Note that Equation 4 is also the same as the one in AFLGo [6].
It then applies a feature scaling normalization to get the final
distance ˜ds(s,Tb) = ds(s,Tb)−minD
max D−minD where minD (or maxD) is the
smallest (or largest) distances ever met.
Covered Function Similarity. This metric measures the simi-
larity between the execution trace of the seed and the target exe-
cution trace on the function level. We do not track the basic block
level trace similarity since that would introduce considerable over-
heads. The similarity is calculated based on the intuition that seeds
covering more functions in the “expected traces” will have more
chances to be mutated to reach the targets. This similarity is cal-
culated by tracking the function sets the current seed covered (de-
noted as ξf (s)) and comparing it with the target function trace clo-
sure ξf (Tf ). In the example in Fig. 2, ξf (abcdT Z) = {a, b, c, d,T},
ξf (aeT Z) = {a, e,T} and ξf (ae f Z) = {a, e}.
The covered function similarity is then determined by the fol-
lowing formula:
Σf ∈ξf (s)∩ξf (Tf )df (f ,Tf )−1
cs(s,Tf ) =
|ξf (s) ∪ ξf (Tf )|
(5)
df (f ,Tf ) is the function level distance calculated with Equation 2.
Similar to ds, a feature scaling normalization is also applied and
the final similarity is denoted as ˜cs. Note that this similarity metric
is uniquely proposed in our approach.
Scheduling. Scheduling deals with the problem how many mu-
tation chances will be assigned to the given seed. The intuition is
that if the trace that the current seed executes is “closer” to any of
the expected traces that can reach the target site in the program,
more mutations on that seed should be more beneficial for gener-
ating expected seeds. A scheduling purely based on trace distance
may favor certain patterns of traces. For AFLGo, as mentioned in
§2.2.2, the shorter paths will be assigned more energy, which may
starve longer paths that are still reachable to the target sites. To mit-
igate this, we propose the power function that considers both trace
distance (based on basic block level distance) and trace similarities
(based on covered function similarity):
p(s,Tb) = ˜cs(s,Tf ) · (1 − ˜ds(s,Tb))
(6)
It is obvious that the value of p(s,Tb) fits into [0, 1] since both
the multipliers are in [0, 1].
Compared to AFLGo’s approach, which only considers basic
block trace distance (ds, or ˜ds), our power function balances the
effect of shorter paths and the longer paths that can reach the target.
Logically, there are some differences between cs and ds:
(1) ds considers both the effects of CG and the CFGs; cs considers
only the effects of CG. For ds, the major effect is still CG due to
the magnification factor c used in Equation 2.
(2) ds does not penalize traces that do not lead to the targets, while
cs penalizes them via a union of ξf (s) (by tracking function
level traces) and ξf (Tf ).
(3) Given multiple traces that can lead to the targets, ds favors
those that have short lengths, but cs favors those with longer
lengths of common functions in expected trace.
In this sense, p(s,Tb) strives a balance between shorter traces and
longer traces that can reach the target sites with two heterogeneous
metrics. Admittedly, there may still exist some bias. One of the
scenarios is that the power function may assign more energy to a
seed that covers many functions in the target function trace closure.
For example, assume two traces that can reach the target function
T : ⟨a, b, c, d,T , Z⟩ and ⟨a, e, f , д,T , Z⟩; and the target function trace
closure is ⟨a, b, c, d, e, f , д,T⟩. The power function may assign much
energy to a seed with trace ⟨a, b, c, d, e, f , д, Z⟩ which does not
reach the target function T . This is not an issue in our opinion:
since this seed has covered many “expected” functions, it has high
chance to be “close” to the target; with proper mutations, it is likely
to be flipped to mutants that can indeed touch the target.
In Hawkeye, the power function determines the number of mu-
tation chances to be applied on the current seed (i.e., energy); it is
also used during the seed prioritization to determine whether the
mutated seeds should be favored.
4.5 Adaptive Mutation
In §4.4, for each seed, the output of power scheduling is the energy
(a.k.a. the times of applied mutations), which will be the input of
the step of our adaptive mutation. The problem is that, given the
total energy available for a seed, we still need to assign the number
of mutations for each type of mutators.
In general, two categories of mutators are used in GFs. Some are
coarse-grained in the sense that they change bulks of bytes during
the mutations. Others are fine-grained since they only involve a few
Algorithm 1: adaptiveMutate(): Adaptive Mutation
input :s, the seed to be fuzzed after power scheduling
output: Ms , the map to store the new mutated seed, whose key is
const.
const.
1 Ms = ∅;
2 p ← s .дetScor e();
3 if r eachT arдet(s) == f alse then
the seed and whole value is the energy of the seed
:γ , the constant ratio to do fine-grained mutation
:δ, the constant ratio to be adjusted
4
5
6
7
8
9
S′ ← coarseMutate(s, p ∗ (1 − γ));
for s′ in S′ do
S′′ ← f ineMutate(s, p ∗ γ);
for s′′ in S′′ do
Ms ← Ms ∪ {(s′, s′.дetScor e())}
Ms ← Ms ∪ {(s′′, s′′.дetScor e())}
10 else
11
12
13
14
15
16
S′ ← coarseMutate(s, p ∗ (1 − γ − δ));
for s′ in S′ do
Ms ← Ms ∪ {(s′, s′.дetScor e())}
S′′ ← f ineMutate(s, p ∗ (γ + δ));
for s′′ in S′′ do
Ms ← Ms ∪ {(s′′, s′′.дetScor e())}
byte-level modifications, insertions or deletions. For coarse-grained
mutations, we consider them to be:
(1) Mixed havoc. This includes several bulk mutations, namely
deleting a chunk of bytes, overwriting the given chunk with
other bytes in the buffer, deleting a certain lines, duplicating
certain lines multiple times, etc. The actual mutation involves
their combinations.
(2) Semantic mutation. This is used when the target program is
known to process semantic relevant input files such as javascript,
xml, css, etc. In detail, this follows Skyfire [43], which includes
three meta mutations, inserting another subtree into a random
AST position, deleting a given AST, and replacing the given
position with another AST.
(3) Splice. This includes a crossover between two seeds in the
queue and subsequent mixed havocs.
Algo. 1 shows the workflow of our adaptive mutation, given
a seed s. The basic idea is to give less chance of coarse-grained
mutations when the seed s can reach the target functions (at line
10 in Algo. 1). Once the seed reaches targets, the times of doing
fine-grained mutations increase from p ∗γ (line 7) to p ∗(γ +δ) (line
14), but the times of doing coarse-grained mutation decrease from
p ∗ (1 − γ) (at line 4) to p ∗ (1 − γ − δ) (line 11). Here, s.дetScore()
at line 2 is to get the energy assigned to the seed according to the
the power function value calculated in Equation 6.
f ineMutate() in Algo. 1 simply applies a random fine-grained
mutation (e.g., bit/byte flippings, arithmetics on some bytes) for the
seed. Algo. 2 shows the details for coarse-grained mutation strategy
coarseMutate(). Given a seed s and the iteration times of mutations
i, the basic idea is to apply semantic mutations (line 2) only when it
is necessary (line 3). The constraints needSemMutation(s) returns
true if the following conditions are satisfied: 1) our fuzzer detects
that the input file is a semantic-relevant input file such as javascript,
xml, css, etc; 2) The previous semantic mutations have not failed. If
not necessary (line 6), mixed havoc mutations will get more times
Algorithm 2: coarseMutate(): Coarse-Grained Mutation
input :s, the seed to be fuzzed after power scheduling
input :i, the number of iterations to do mutation on the seed
output: S, the set to store the new mutated seed
const.
const.
1 S = ∅;
2 if needSemMutation(s) == true then
: σ , the constant ratio to do semantic mutations
:ζ , the constant ratio to do mixed havoc mutations
3
4
5
6 else
7
8
S ← S ∪ semMutate(s, i ∗ σ) ;
S ← S ∪ coarseH avoc(s, i ∗ (1 − σ) ∗ ζ );
S ← S ∪ splice(s, i ∗ (1 − σ) ∗ (1 − ζ )) ;
S ← S ∪ coarseH avoc(s, i ∗ ζ );
S ← S ∪ splice(s, i ∗ (1 − ζ )) ;
Algorithm 3: seedPrioritize(): Seed Prioritization
input :s, the seed to be processed
output: Q1, the tier 1 queue to store the most important seeds
output: Q2, the tier 2 queue to store the important seeds
output: Q3, the tier 3 queue to store the least important seeds
const.
1 Q1 = Q2 = Q3 = ∅;
2 if seedIs N ew(s) == true then
:η, the threshold of energy value for accepting important
seeds
3
4
5
6
7
8
9