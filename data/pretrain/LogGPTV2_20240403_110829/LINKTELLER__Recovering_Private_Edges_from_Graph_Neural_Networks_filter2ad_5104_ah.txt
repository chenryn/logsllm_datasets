the twitch datasets and micro-averaged F1 score for PPI and
Flickr datasets. The rationale is put in Appendix F2.
2) Evaluation Results: The ﬁgures for the model utility are
presented in Figure 3(a). We plot the change of the utility
with the increase of privacy budget of two DP mechanisms
EDGERAND and LAPGRAPH, as well as the utility of two
baseline models independent of the privacy budget. For each
privacy budget ε, the reported results are averaged over 10 runs
for different random seeds. We examine Figure 3(a) to see how
the model utility of DP methods compares with the baselines.
We ﬁrst compare the performance of two baseline models:
GCN (the black horizontal line) and MLP (the red line). We
note that GCN is almost always better than MLP except on
the PPI dataset. The observation is well suited to our intuition
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2015
twitch-RU
twitch-DE
twitch-FR
twitch-ENGB
twitch-PTBR
PPI
Flickr
twitch-RU
twitch-DE
twitch-FR
twitch-ENGB
twitch-PTBR
PPI
Flickr
(a) Model utility
w
o
l
d
e
n
i
a
r
t
s
n
o
c
n
u
h
g
i
h
(b) Attack effectiveness on different models and node degree distributions (low, unconstrained, and high)
Fig. 3: (a) Model utility and (b) attack effectiveness on different models (ˆk = k). Each column corresponds to a dataset. We consider four
types of models: EDGERAND, LAPGRAPH, vanilla GCN, and MLP, with the ﬁrst two satisfying DP guarantees. In each ﬁgure, the vertical
bar represents the standard deviation.
twitch-RU
twitch-DE
twitch-FR
twitch-ENGB
twitch-PTBR
PPI
Flickr
D
N
A
R
E
G
D
E
H
P
A
R
G
P
A
L
Fig. 4: F1 score of nodes with different degrees under privacy budget ε ∈ {1, 5, 10} for two DP mechanisms (EDGERAND and LAPGRAPH).
Each bar group represents a degree range, e.g., 1-5, 6-10, 50-. Each bar within a group corresponds to one privacy budget.
that the knowledge of the graph structure can beneﬁt learning.
For most datasets, the model utility increases with the growth
of the privacy budget, since the privacy protection to the graph
structure becomes weaker. However, twitch-ENGB is an ex-
ception that achieves slightly higher utility when more privacy
noise is added. This may be due to the following reason. For
twitch datasets, the model is trained on the graph of twitch-ES
and tested on graphs of other ﬁve countries. When the training
and testing graph distribution distance is too large, there is no
guarantee that the performance on the training graph can be
transferred to the testing graph. This is the case for twitch-
ENGB, which is an extremely sparse graph (see Table IV(a))
compared with the training graph. Thus, with slightly more
random noise added, its generalization may be improved. In
addition to the evaluation over a range of ε, we also evaluate
such tradeoff of model utility and privacy resiliency by select-
ing appropriate ε based on a validation dataset. The detailed
setups and results are omitted to Appendix G6.
D. Tradeoff between Model Utility and Privacy
We analyze the tradeoff between model utility and the attack
performance: models with high utility tend to be more vulnera-
ble to LINKTELLER. The sweet spot differs across datasets and
scenarios. Summarizing the observations, we derive a series
of conclusions on how to protect privacy given the gap of the
model utility between the vanilla GCN and the MLP model.
First, if the utility of the vanilla GCN is much higher than the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2016
EdgeRandLapGraphvanilla GCNMLP123456789100.180.200.220.240.260.280.300.32F1 score123456789100.480.500.520.54123456789100.230.250.280.300.330.350.380.40123456789100.580.600.620.640.660.68123456789100.360.380.400.420.44123456789100.420.440.460.480.500.52123456789100.440.460.480.50EdgeRandLapGraphvanilla GCN123456789100.00.20.40.60.81.0F1 score123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.01234567891000.10.20.30.40.50.6123456789100.00.20.40.60.81.0F1 score123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.01234567891000.10.20.30.40.50.6123456789100.00.20.40.60.81.0F1 score123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.0123456789100.00.20.40.60.81.01234567891000.10.20.30.40.50.6=1=5=101-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.4F1 score1-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.40.50.61-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.41-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.40.50.60.71-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.40.51-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.40.51-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-0.00.10.20.30.40.51-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.20.30.4F1 score1-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.20.30.40.50.61-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.10.20.20.20.30.40.41-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.20.30.40.50.60.71-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.20.30.40.51-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.20.30.40.51-56-1011-1516-2021-2526-3031-3536-4041-4546-5050-Degree0.00.10.20.30.40.5MLP model, then there is space for performance degradation
caused by ensuring privacy. We do observe a few cases where
the utility of the DP model is above the MLP baseline, and
the attack success rate at that point is relatively low, especially
under the low degree case, e.g., the DP model on twitch-RU
when ε = 7. In such cases, carefully choosing an ε will give
the practitioner fairly good utility and a certain level of privacy
guarantee simultaneously. Second, when the performance of
the vanilla GCN only exceeds MLP by a small margin, almost
all DP models that can effectively defend against the attack
suffer tremendous utility loss. We point out that most scenarios
fall under this category, where either privacy or utility will
be sacriﬁced. This further substantiates the power of LINK-
TELLER. Third, when the graph structure hurts learning (e.g.,
PPI), we may avoid using the graph structure in training by
using MLP. There might exist other graph neural networks that
can achieve better performance on datasets like PPI, and ap-
plying LINKTELLER to these models are exciting future work.
low-degree nodes. As noted
in Section VI-B, DP GCN offers better protection to nodes of
low degree. A natural question is then: does better protection
imply a degradation of utility of these nodes? To answer this
question, we separate the nodes into bins by degree (e.g., 1-5,
6-10, . . ., 46-50, 50-), and investigate the F1 score of nodes
in each individual bin. The results on all datasets, two DP
mechanisms, with three privacy budgets are presented in Fig-
ure 4. We can see that the utility for low-degree nodes does not
drop faster than high-degree nodes when the privacy budget
decreases, which indicates that DP GCN does not sacriﬁce the
utility of low-degree nodes particularly.
Utility and privacy of
Discussion: EDGERAND or LAPGRAPH. We further
compare the results of the two mechanisms regarding model
utility and attack success rate. When ε is small, the utility of
EDGERAND and LAPGRAPH do not differ much (especially
on PPI). When ε is large, EDGERAND generally has better
model utility, while LAPGRAPH is more robust to LINK-
TELLER. The results for EDGERAND are incomplete for the
large scale dataset Flickr under tight privacy budgets (ε ∈ {1,
2, 3, 4}) using EDGERAND. Under these cases, the graphs
become much denser after perturbation of large magnitudes,
and we experience an OOM error using an 11 GB GPU. In
comparison, LAPGRAPH does not suffer such an issue.
VII. RELATED WORK
1) Privacy Attack on Graphs: This topic was widely
studied [32]–[35] before graph neural networks came into
play. There are mainly three types of privacy attacks on
graphs: identity disclosure, attribute disclosure, and link re-
identiﬁcation [32], corresponding to different components
(nodes, node attributes, and edges) of a graph. In this paper, we
focus on edge privacy. Previous endeavors have illustrated the
feasibility of the link re-identiﬁcation attack, whilst relying on
strong prior knowledge and information that arguably might
not always hold or accessible. For example, when prior knowl-
edge about the graph is available—e.g., nodes with similar
attributes or predictions are likely connected—He et al. [11]
claim that an attacker could infer links in the training graph by
applying methods such as clustering to predict connections for
nodes within the same cluster. Duddu et al. [36] show that with
access to the node embeddings trained to preserve the graph
structure, one can recover edges by analyzing predictions
based on the embeddings. Apart from the privacy attacks, there
exist other adversarial attacks on GNNs, e.g., against node
embeddings [37] and graph- and node-level classiﬁers [38].
Despite the promising attacks illustrated by these early en-
deavors, there is a clear need to weaken the assumptions for
more reliable, practical settings. In this paper, we thus answer:
to what extent can we recover private edges of a graph by
probing a trained blackbox GNN model without strong prior
knowledge? Could we leverage the property of inﬂuence prop-
agation among nodes in GNNs to design an effective attack?
2) Differential Privacy for Graphs: Differential pri-
vacy [18] is a notion of privacy that entails that the outputs
of the model on neighboring inputs are close. This privacy
requirement ends up obscuring the inﬂuence of any individual
training instance on the model output. There are a series of
works that examine the theoretical guarantee or the practi-
cal performance of models under differential privacy guaran-
tees [39]–[44]. Depending on the properties of the datasets,
e.g.,
the distinction between the distribution of members
and non-members and the underlying correlations within the
datasets, there is no trivial answer to this problem.
The extension of differential privacy to the graph setting
was ﬁrst conducted in Hay et al. [45]. Since then, there has
been extensive research on computing graph statistics such as
degree distribution [45], cut queries [46], and sub-graph count-
ing queries [47] under edge or node differential privacy. These
statistics are useful for graph analysis but insufﬁcient for train-
ing a GCN model. Thus, in this paper, to evaluate the strength
of the LINKTELLER attack, we adapt one existing algorithm
EDGERAND and propose a Laplacian mechanism LAPGRAPH
for training DP GCN models as evaluation baselines.
VIII. CONCLUSIONS
We propose the ﬁrst edge re-identiﬁcation attack LINK-
TELLER via inﬂuence analysis against GNNs. We also evalu-
ate LINKTELLER against differentially private GNNs trained
using an existing and a proposed DP mechanisms EDGERAND
and LAPGRAPH to understand the capability of the attack.
Extensive experiments on real-world datasets (8 for inductive
and 3 for transductive setting) demonstrate the effectiveness
of LINKTELLER in revealing private edge information, even
when there are certain privacy guarantees provided by a DP
mechanism.
We believe this work will inspire a range of future re-
search opportunities and lay down a foundation for future
explorations by providing a clear data isolation problem setup,
analysis of edge privacy, together with extensive empirical
observations and conclusions.
Acknowledgement. This work is partially supported by the
NSF grant No.1910100, NSF CNS 20-46726 CAR, and Ama-
zon Research Award.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2017
REFERENCES
[1] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015, pp. 3431–3440.
[2] Y. Wei, X. Wang, L. Nie, X. He, R. Hong, and T.-S. Chua, “Mmgcn:
Multi-modal graph convolution network for personalized recommenda-
tion of micro-video,” in Proceedings of the 27th ACM International
Conference on Multimedia, 2019, pp. 1437–1445.
[3] J. Zhou, C. Chen, L. Zheng, X. Zheng, B. Wu, Z. Liu, and L. Wang,
“Privacy-preserving graph neural network for node classiﬁcation,” arXiv
preprint arXiv:2005.11903, 2020.
[4] L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin, M. Deng,
and H. Li, “T-gcn: A temporal graph convolutional network for
trafﬁc prediction,” IEEE Transactions on Intelligent Transportation
Systems, vol. 21, no. 9, p. 3848–3858, Sep 2020. [Online]. Available:
http://dx.doi.org/10.1109/TITS.2019.2935152
[5] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and
J. Leskovec, “Graph convolutional neural networks for web-scale
recommender systems.” in KDD, Y. Guo and F. Farooq, Eds. ACM,
2018, pp. 974–983. [Online]. Available: http://dblp.uni-trier.de/db/conf/
kdd/kdd2018.html#YingHCEHL18
[6] J. Jiang, J. Chen, T. Gu, K. R. Choo, C. Liu, M. Yu, W. Huang, and
P. Mohapatra, “Anomaly detection with graph convolutional networks
for insider threat and fraud detection,” in MILCOM 2019 - 2019 IEEE
Military Communications Conference (MILCOM), 2019, pp. 109–114.
[7] “Vertex ai — google cloud,” https://cloud.google.com/vertex-ai, (Ac-
cessed on 06/22/2021).
[8] “Parlai,” https://ai.facebook.com/tools/parlai, (Accessed on 06/22/2021).
https://www.ibm.
[9] “Infosphere
(Accessed
on
pipeline — ibm,”
com/products/ibm-infosphere-virtual-data-pipeline,
06/22/2021).
virtual
data
[10] J. Klicpera, A. Bojchevski, and S. G¨unnemann, “Predict then propagate:
Graph neural networks meet personalized pagerank,” arXiv preprint
arXiv:1810.05997, 2018.
[11] X. He,
J.
Jia, M. Backes, N. Z. Gong,
and Y. Zhang,
from graph neural networks,” in 30th USENIX
“Stealing links
Security Symposium (USENIX Security 21). USENIX Association,
Aug. 2021.
[Online]. Available: https://www.usenix.org/conference/
usenixsecurity21/presentation/he-xinlei
[12] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A
comprehensive survey on graph neural networks,” IEEE transactions on
neural networks and learning systems, 2020.
[13] T. N. Kipf and M. Welling, “Semi-Supervised Classiﬁcation with
Graph Convolutional Networks,” in Proceedings of the 5th International
Conference on Learning Representations, ser. ICLR ’17, 2017. [Online].
Available: https://openreview.net/forum?id=SJU4ayYgl
[14] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs.” in NIPS,
I. Guyon, U. von Luxburg,
S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan,
and R. Garnett, Eds., 2017, pp. 1024–1034.
[Online]. Available:
http://dblp.uni-trier.de/db/conf/nips/nips2017.html#HamiltonYL17
[15] Y. Rong, W. Huang, T. Xu, and J. Huang, “Dropedge: Towards
deep graph convolutional networks on node classiﬁcation.” in ICLR.
OpenReview.net, 2020. [Online]. Available: http://dblp.uni-trier.de/db/
conf/iclr/iclr2020.html#RongHXH20