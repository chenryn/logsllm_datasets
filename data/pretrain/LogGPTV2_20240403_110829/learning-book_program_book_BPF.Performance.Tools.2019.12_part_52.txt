19118
[128K, 256K)
0 1
[512K, 1M}
[256K, 512K)
 1
01
[1M, 2H)
11
[2M, 4M)
840188988
[4H, 8K)
887 188088
[8M, 168)
441189
[16N, 328]
1241
(K9*N761
22018
[64N, 128H)
20718
[1288, 25 68)
20518
[256K, 5128)
31
[512%, 16]
28618
: [pp]s3oq,s§
[0]
29908 1eee88e8ee88e 88ee8ee8ee8ee8ee8ee88e88e 88ee8ee8ee8ee8e1
[1]
01
[..-]
(K9*N761
0 1
[64N, 128H)
11
This output shows that processes named *dd° usually did not request any seeking: an offset of O was
requested 29,908 times while tracing. This is expected, as I was running a dd(1) sequential workload.
I also ran a tar(1) file system backup, which generated a mixed workload: some sequential, some
random.
The source to seeksize(8) is:
#1/usx/local/bin/bpttrace
BEGIX
printf (*Txaclng block I/0 requested seeks .
Hit Ctr1-C to end.\n")
tracepoint:block:bloc
---
## Page 403
366
Chapter 9 Disk I/0
1[nepdev] ;
$dist = (args=>sector - $last) > 0 ?
args->sector - $last : $last - args->sector,
// store details
esectors[args=>corn]- hist ($dist] 
/ / save last requested position of disk head
last [args=>dev] = args=>sectoz + args=>nz_sector;
END
clear (91ast) :
This works by looking at the requested sector offset for each device I/O and comparing it to a
recorded previous location. If the script is changed to use the block_rq_completion tracepoint,
it will show the actual seeks encountered by the disk But instead it uses the block_rq_issue trace
point to answer a different question: how random is the workload the application is requesting?
This randomness may change after the I/O is processed by the Linux I/O scheduler and by the
on-disk scheduler. I first wrote this to prove which applications were causing ranxdom workloads,
so I chose to measure the workload on requests.
The following tool, biopattern(8), measures randomness on I/O completion instead.
9.3.6
biopattern
biopattern(8)a is a bpftrace tool to identify the pattern of I/O: random or sequential. For example:
Attach.ing
s9q03d
TIME
SRND
5SE0
COUNT
S3±.8X
00 : 05 : 54
83
1 6
2960
13312
55150=00
82
17
3881
15524
00 : 05 : 56
78
3059
12232
.5150=00
EL
26
2770
14204
00 : 05 : 58
0
100
1
0
had sent me (which also had more columns). I crested this bpftrace version for this book on 19-Mar-201.9)
---
## Page 404
9.3  BPF Tools
367
00:05: 59
0
0
0
00:90=00
196360
0
D 
99
1536
00 : 06 : 01
100
13444
1720704
00 :06 : 02
D
99
13864
1771876
00 : 06 : 03
00T
13129
1680640
00 :06 : 04
D 
13532
1731484
[. .-]
This examples begins with a file system backup workload, which caused mostly random I/O. At
6:00 I switched to a sequential disk read, which was 99 or 100% sequential, and delivered a much
higher throughput (KBYTES).
The source to biopattern(8) is:
#1/usx/1ocal/bin/bpftrace
BEGIN
printf(*s=8s 15a 45a V8s 10sn", "rIME*, *RND*, *ssEQ", *co0NT*,
*KBYTES*) 
tracepoint:block:block_rq_conplete
1f dev] == axgs=>sector)(
sequentia]++;
| else (
random++
Bbytes = Bbytes + args->nr_sector * 512
lastsector [args=>dev] = args=>sector + args=>nx_sector,
interval:s11
$count = Brandom + Bsequential;
sdiv = $count
if (div == 0) ↑
$d1v = 1;
time ("%B: SH:S ") ;
printf (*&5d 45d eBd $10dn*, Brandon * 100 / $div,
esequential + 100 / $div, $count, @bytes / 1024}
clear (Brandom) = clear (8sequential) : clesr (@bytes) 
---
## Page 405
368
Chapter 9 Disk I/0
END
1
clear (8lastsector)
clear (Brandon) : clear (8seqoentia1) : cleaz (@bytes)
This works by instrumenting block I/O completion and remembering the last sector (disk address)
used for each device, so that it can be compared with the following I/O to see if it carried on from
the previous address (sequential) or did not (random).°
This tool can be changed to instrument tracepoint:block:block_rq_insert, which will show the
randomness of the workload applied (similar to seeksize(8)).
9.3.7 biostacks
biostacks(8) is a bpftrace tool that traces fullI/O latency (from OS enqueue to device completion)
with the I/O initialization stack trace, For example:
+biostacks.bt
Attach.ing 5 pzobes...
Tracing block I/0 vith init stacks. Hit Ctrl-C to end.
[...]
| eoa2n6
blk_account_io_start+1
blk_nq_nake_request+1069
genexic_nake_request+292 
submi t_bio+115
DTE+ebedpeea-dexs
read_sxap_cache_async+64
svapls_readahead+614
980T+abed dexsop
handle_pte_fault+T25
_handle_mn_fault+1144
LLT+tneg"uu"etpve
265+“obed"op-
do_page_fault+46
9 Prior to the tracing ers, I would identify random/seuential worklosds by interpreting iostat(1) output and looking for
(eguanbas) sazs 0/ u sgm saug eouas mo ao (uopueu) sazs 0/1eus g(m sau aoes lu4
talk in 2018, and for the first time sam initialization stacks associsted with I/0 completion times,
10 Origin: 1 created it for this book on 19-Mar-2019. 1 had constructed a similar tool live during an internal Facebool
---
## Page 406
9.3 BPF Tools
369
page_fault+69
] :
[16K,32K)
11
[32K,64K)
321
[64K, 128K)
3362 8 e88e88e88e 8ee8ee8ee8ee8e88e88e88 88ee8e88e8e 1
[128K, 256K)
381
[256K, 512K]
0 1
[512K, 18]
[1M, 2M)
1 1
[2M, 4N)
11
[4M, 8M)
11
b1k_account_io_start+1
b1k_nq_nake_request+1069
Z62+snbx"exeu"otxeue6
submi t_bio+115
subml t_bh_vbc+384
11_rx_block+173
_breadabead+68
_ext4_get_inode_loc+914
ext4_sget+146
ext4_iget_norna1+48
ext4_1ookup+240
lookup_s1ov+171
xa1k_conponent+451
path_lookupat+132
f1lename_lookup+182
user_path_at_enpty+54
vfs_statx+118
SYSC_nevfstatat+53
sys_nexfatatat+14
do_sysca11_64+115
entzy_SYsCALL_64_after_hvfxame+61
] :
[8K, 16K)
18 18ee88e88e88
[16K,32K)
8886886886881 07
[323, 64K)
10 1889889
[64K, 128K)
56 1869889889886 88698698698698698898898
[128K, 256K]
[256K, 512K)
718898
---
## Page 407
Chapter 9 Disk I/0
I have seen cases where there was mysterious disk I/O without any application causing it. The
reason turned out to be background file system tasks. (In one case it was ZFS's background scrub.
ber, which perioxdically verifies checksums.) biostacks(8) can identify the real reason for disk I/O
by showing the kernel stack trace.
The above output has two interesting stacks. The first was triggered by a page fault that became a
swap in: this is swapping.1 The second was a newfstatat() syscallthat became a readahead.
The source to biostacks(8) is:
#1/usx/1ocal/bin/bpEtrace
BEGIN
printf (*Tracing block I/0 vith Init stacks. Bit Ctx1-C to end.\n*) 
kprobe:blk_account_io_start
Breqstack[arg0] - kstack,
fsoesu = [obze]eqbexg
kprobe:b1k_start_request,
kprobe:blk_nq_atart_request
/Breqts [a.rg0] /
Busecs [@regstack[arg0]] - hist (nsecs - reqgts |arg0] 
delete (@regstack[azg0]) 
delete (@reqts [arg0]1
END
clear (Breqstack) ; cleax (Bxeqts) ;
This works by saving the kernel stack and a timestamp when the I/O was initiated and retriev
ing that saved stack and timestamp when the I/O completed. These are saved in a map keyed by
the struct request pointer, which is arg0 to the traced kernel functions. The kernel stack trace is
recorded using the kstack builtin. You can change this to ustack to record the user-level stack trace
or add them both.
11 Linux terminology, where this means switching pages with the swsp device. Swspping for other kermels can mean
moving entire processes.
---
## Page 408
9.3 BPF Tools
371
With the Linux 5.0 switch to multi-queue only, the blk_start_request() function was removed
from the kernel. On that and later kernels, this tool prints a warning:
Warning: could not attach probe kprobe:blk_start_request, skipping-
This can be ignored, or that kprobe can be deleted from the tool. The tool could also be rewriten
to use tracepoints. See the "Tracepoints" subsection of Section 9.3.1.
9.3.8 bioerr
bioerr(8)2 traces block I/O errors and prints details. For example, running bioerr(8) on my laptop:
+ bioerr.bt
Attaching 2 pzobes.
Tracing block I/0 errors. Hit Ctel=C to end.
00:31:52 devlce: 0,0, sector: =1, bytea: 0, Clags: N, exrox: -5
00:31:54 device: 0,0, sectort -1, bytes: 0, flags: N, error: -5
00:31:56 device: 0,0, sector: -1, bytes: 0, [lags: x, exrox: -5
00:31:58 device: 0,0, sector: -1, bytes: 0, Elags: K, error: -5
00:32:00 devlce: 0,0, sector: -1, bytes: 0, [lags: N, exrox: -5
[...]
This output is far more interesting than I was expecting. (I wasn’t expecting any errors, but ran it
just in case.) Every two seconds there is a zero-byte request to device 0,0, which seems bogus, and
which returns with a -5 error (EIO).
The previous tool, biostacks(8), was created to investigate this kind of issue. In this case I
don’t need to see the latency, and I only want to see stacks for the device 0,0 I/O. I can tweak
biostacks(8) to do this, although it can also be done as a bpftrace one-liner (in this case, Pll check
that the stack trace is still meaningful by the time this tracepoint is hit; if it were not still mean-
ingful, Fd need to switch back to a kprobe of blk_account_io_start() to really catch the initializa
tion of this I/O):
(++[xoe,sx]g 1 /0 == aperror != 0/
time I**B:M:s *)
printf(*device: id, ld, sector: Id, bytes: 5d, flags: ls, error: Id’n",
args=>dev >> 20, args=>der 6 ((1 sector,
axgs=>nr_sectoz * 512, axgs=>rvbs, azgs=>ezroz) 
The logic for mapping the device identifier (args->dev) to the major and minor numbers comes
from the format file for this tracepoint:
 cat /sys/kerne1/debug/tracing/events/block/block_rq_complete/format
nane: block_rq_conplete
[..-]
[(oz dev) ≤ ((1u rvbs, get_str (cmd) , (unsigned
long long REC>sectox, REC->nz_sectox, REC->exrox
---
## Page 410
9.3 BPF Tools
373
While bioerr(8) is a handy tool, note that perf(1) can be used for similar functionality by filtering
on error. The output includes the format string as defined by the /sys format file. For example:
18446744073709551615 + 0 [-5]
ksoftirqd/2
22 [002] 2289450.691041: block:block_rq_complete: 0,0  (]
...]
The BPF tool can be customized to include more information, going beyond the standard capa
bilities of perf(1).
For example, the error returned, in this case -5 for EIO, has been mapped from a block error code.
It may be interesting to see the original block error code, which can be traced from functions that
handle it, for example:
.1 ++[06ize] g1/obae/ ouzxa“oqnqeqstq:aqoxdx, o- aoexdq +
Attaching 1 prcbe...
^C
9[10] : 2
It's really block I/O status 10, which is BLK_STS_IOERR. These are defined in linux/blk_types.h:
define BLK_STS_OK 0
define BLK_STS_NOrSUPP
T(a"snesxtq oxog))
define BLK_STS_TIMEOT
((force blk_status_t)21
define BLK_STS_NOSPC
(E ()
define BLK_STS_TARGET