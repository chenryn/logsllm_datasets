i
s
n
a
r
T
Figure 8: RTT by link type and metro.
4.3.3
Serving the world from different metros. For this ex-
ample, we consider all of Akamai’s deployments in six metro
areas around the world and show again only results for the
2017-09-17 snapshot of ViewA. EU-1 and EU-2 are two metro
areas in Europe that are considered to be major Internet hubs
and have large IXPs; US-1 and US-2 are major metros on the
east- and west-coast of the US, respectively, with substantial
Internet infrastructure; and AS-1 and AS-2 are major com-
mercial centers and Internet hubs in Asia. In all six metro
areas, all four link- and deployment options are used, but as
Figure 8 shows, to a varying degree. As in the previous figure,
the different widths of the boxes encode the traffic volume
and show that in all metro areas, onnet and PNI combined
serve most of the traffic. Except for US-2, there are noticeable
differences in traffic volumes between onnet and PNI, and
they can be explained partly by differences in population size,
partly by differences in availability, access, and cost of PNIs,
and partly by how large ISPs in the different metros view
Akamai or other large CPs as peering partners. For example,
even though EU-1 and US-1 are Internet hubs with a high
density of data centers, their population compared either to
EU-2 and US-2, respectively, or to the total population in the
two respective countries is not that large to justify a large
number of Type 1 deployments.
Overall, onnet and PNI achieve the best performance (in
terms of RTT), except for AS-1 where performance-wise,
217
transit has a slight edge. Although transit is across the board
a very small fraction of the traffic, its relative performance
differs markedly for the three continents. In the US with its
remaining Tier-1 ASes, transit performs similar to PNI, and
in Asia, it performs good due to a strong reliance in transit
providers in that region. To explain the wide ranges in RTT
for IXP in EU-1 and EU-2 note that Akamai’s deployments
at European IXPs serve both local end users and end users
in remote locations i.e., networks from other countries that
connect to the IXPs (e.g., see [13]).
Summary: When examining how traffic associated with ac-
tual user requests traverses Akamai’s dense connectivity fabric,
we find that some 90% of the overall traffic is coming from
just 1% of the paths. This extreme skewness also holds for all
those paths seen from explicit-only, implicit-only, and com-
bined explicit-implicit peerings. Considering different scenarios
around the world, we observe that different providers make
different decisions about how to connect with Akamai and that
these decision matter for performance.
5 RELATED WORK
Two recent papers [42, 47] have contributed to a renewed
interest in the actual structure and operations of the serving
infrastructures of large CPs such as Facebook and Google and
have demonstrated that the principles of SDN are applicable
to public-facing networks. Complementing these systems-
focused studies that offer only a few details about the actual
connectivity fabric component of Facebook’s or Google’s
serving infrastructures, our work provides a first-of-its kind
in-depth account of this very connectivity fabric of Akamai,
a large, global-scale CDN.
Our work is also related to prior research efforts on (i)
mapping the footprints of different large CP infrastructures
(e.g., see [3, 5, 25, 43, 45] and also [2, 7, 9]); (ii) providing new
insights into the structure and evolution of the AS-level Inter-
net (e.g., see [17, 33, 35, 41] and references therein), including
intricate interconnectivity fabrics at the large IXPs across the
world (e.g., see [4, 8, 11, 13, 40]); (iii) studying the flattening
of the Internet (e.g., see [15, 16, 22, 30, 48]); (iv) exploring
Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
CDN-ISP collaborations (e.g., see [21, 26, 38, 39]); and (v)
optimizing content delivery to end users in a rapidly chang-
ing Internet (e.g., see [10, 29, 36, 44, 46, 49] and references
therein). Our study is complementary to these and similar ef-
forts; it provides a detailed account of the connectivity fabric
of Akamai’s serving infrastructure and illustrates how this
large CP leverages this fabric to optimize the performance
of content delivery as experienced by the end users.
6 DISCUSSION
Realizing that the serving infrastructures of today’s large
CPs come in different shapes and sizes and change in re-
sponse to emerging technologies, new business models, and
a constantly evolving Internet edge, we consider our detailed
account of Akamai’s current serving infrastructure and the
breakdown of its connectivity fabric into its various compo-
nents as a valuable reference point for examining its own
evolution in time. For example, as part of our future work, we
plan to study the evolution of this large CDN’s serving infras-
tructure as a whole and of its connectivity fabric in particular
as it leverages and expands its own multi-service backbone
to transport its traffic between its own server clusters in a
performance-aware and cost-effective manner [27, 28] and
at the same time expands its business model to include more
service offerings.
We also view our work to be an important step towards
future efforts on understanding the serving infrastructures of
other large CPs in general and on quantifying the advantages
or disadvantages of one serving infrastructure design over
another in particular. For example, there is some resemblance
between the designs of Akamai’s, Google’s and Facebook’s
current serving infrastructures in the sense that they all uti-
lize, in one form or another, highly-distributed collections
of deployments (including deployments in third-party net-
works) that are organized in some hierarchical fashion on
top of some specialized private backbone network. This ob-
servation begs the question about the optimality properties
of this particular design choice over alternative designs (no
deployments in third-party networks, no private backbone)
when the all-important underlying objective of these large
CPs is the delivery of content to end users in a cost-effective,
performance-optimal, reliable and scalable manner. What
makes studying this problem especially challenging is that it
requires examining largely opaque and at times fast moving
targets. That is, the large CPs view details about their serving
infrastructures (including factors such as types of customers,
services, and workload) as proprietary information, and even
if publicly available, these details change over time.
Finally, by piecing together some of the interconnection
options that are available in today’s Internet to the large
CPs and are utilized by them in practice (e.g., see Section
3.4), the following scenario describes an all-to-realistic use
case. Take a large content owner/producer that utilizes the
services of a large cloud provider to store/process its content.
In turn, this cloud-based content is accessed by a large CDN
that transports it across its private backbone for delivery
to end users serviced by a large ISP. This content will typi-
cally traverse different PNIs all the way from the where it
is produced to the large ISP’s network and thus none of the
associated voluminous traffic will be visible in the public
Internet. This shift of traffic from the łpublicž Internet as
we know it to the łprivatež Internet is real and massive and
well-known among network operators (e.g., see [19]). How-
ever, by their very nature, the publicly available datasets that
network researchers commonly use to study the evolution
of the Internet’s interconnection fabric and its traffic pat-
terns say little if anything about the portion of the private
Internet that can be expected to see (or already sees) most
of the łactionž. The development of new methodologies that
allow third parties to study different facets of the Internet’s
evolution looms as an important open problem for closing
the gap between what network operators know based on
empirical evidence and what network researchers can study
and quantify based on relevant measurements.
7 CONCLUSIONS
Complementing recent studies that focus largely on the de-
sign of new SDN-based Internet peering edge architectures
that enable today’s large CPs to route their traffic at scale
and in a performance-aware manner, our work provides the
first account of the actual scale of the peering edge of such a
large CP. By examining the actual connectivity fabric of the
serving infrastructure of a large global-scale CDN, we show
that it consists of about 6,100 explicit peerings and some
28,500 implicit peerings. The latter refer to existing intercon-
nections between a third-party network and its downstreams
that this CDN has access to and can utilize for its content
delivery service simply by virtue of operating deployments
in such third-party networks. We further illustrate how this
CDN leverages this dense connectivity fabric for serving its
content łto the ISPs of the world.ž
ACKNOWLEDGMENTS
We want to thank Christian Kaufmann for his detailed com-
ments and valuable suggestions on earlier versions of this
paper, Steve McManus for sharing his insights and know-
how about Akamai’s control-plane data and BGP collectors,
and Rick Weber for providing us access to his group’s data
collection and computational resources. We also thank Carl
Princi, Jim Wyllie and Pat Larkin for their support of this
project and our shepherd Stefano Vissicchio and the anony-
mous reviewers for their helpful and constructive feedback.
218
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, W. Willinger
REFERENCES
[1] IPv4 & IPv6 CIDR Report. http://www.cidr-report.org/as2.0. Accessed:
Jan. 2017.
[2] Vijay Kumar Adhikari, Yang Guo, Fang Hao, Volker Hilt, Zhi-Li Zhang,
Matteo Varvello, and Moritz Steiner. Measurement Study of Netflix,
Hulu, and a Tale of Three CDNs. IEEE/ACM TON, 23(6), 2015.
[3] Vijay Kumar Adhikari, Sourabh Jain, Yingying Chen, and Zhi-Li Zhang.
Vivisecting YouTube: An Active Measurement Study. In IEEE INFO-
COM, 2012.
[4] Bernhard Ager, Nikolaos Chatzis, Anja Feldmann, Nadi Sarrar, Steve
Uhlig, and Walter Willinger. Anatomy of a Large European IXP. In
ACM SIGCOMM, 2012.
[5] Bernhard Ager, Wolfgang Mühlbauer, Georgios Smaragdakis, and Steve
Uhlig. Web Content Cartography. In ACM IMC, 2011.
[6] Seth Bennet.
Facebook Scalable Interconnection.
https:
//www.peering-forum.eu/system/documents/124/original/09.
30_-_Facebook_-_Seth_Bennet.pdf.
[7] Timm Böttger, Félix Cuadrado, Gareth Tyson, Ignacio Castro, and
Steve Uhlig. A Hypergiant’s View of the Internet. ACM CCR, 2017.
[8] Samuel Henrique Bucke Brito, Mateus A. S. Santos, Ramon dos
Reis Fontes, Danny Alex Lachos Perez, and Christian Esteve Rothen-
berg. Dissecting the Largest National Ecosystem of Public Internet
eXchange Points in Brazil. In PAM, 2016.
[9] Matt Calder, Xun Fan, Zi Hu, Ethan Katz-Bassett, John S. Heidemann,
and Ramesh Govindan. Mapping the Expansion of Google’s Serving
Infrastructure. In ACM IMC, 2013.
[10] Matt Calder, Ashley Flavel, Ethan Katz-Bassett, Ratul Mahajan, and
Jitendra Padhye. Analyzing the Performance of an Anycast CDN. In
ACM IMC, 2015.
[11] Ignacio Castro, Juan Camilo Cardona, Sergey Gorinsky, and Pierre
François. Remote Peering: More Peering without Internet Flattening.
In ACM CoNEXT, 2014.
[12] H. Chang, S. Jamin, and W. Willinger. Internet Connectivity at the
AS-Level: An Optimization-Driven Modeling Approach. In ACM SIG-
COMM Workshop on Models, Methods, and Tools for Reproducible Net-
work Research, 2003.
[13] Nikolaos Chatzis, Georgios Smaragdakis, Jan Böttger, Thomas Krenc,
and Anja Feldmann. On the Benefits of Using a Large IXP As an
Internet Vantage Point. In ACM IMC, 2013.
[14] Fangfei Chen, Ramesh K. Sitaraman, and Marcelo Torres. End-User
Mapping: Next Generation Request Routing for Content Delivery. In
ACM SIGCOMM, 2015.
[15] Yi-Ching Chiu, Brandon Schlinker, Abhishek Balaji Radhakrishnan,
Ethan Katz-Bassett, and Ramesh Govindan. Are We One Hop Away
from a Better Internet? In ACM IMC, 2015.
[16] Amogh Dhamdhere and Constantine Dovrolis. The Internet is Flat:
Modeling the Transition from a Transit Hierarchy to a Peering Mesh.
In ACM CoNEXT, 2010.
[17] Amogh Dhamdhere and Constantine Dovrolis. Twelve Years in the
Evolution of the Internet Ecosystem. IEEE/ACM TON, 19(5), 2011.
[18] Hurricane Electric.
Internet Statistics. https://bgp.he.net/report/
netstats. Accessed: Jan. 2017.
[19] Equinix. Private Data Exchange Between Businesses Forecasted
to Outpace the Public Internet by Nearly 2x in Growth and 6x in
Volume by 2020. https://www.equinix.com/newsroom/press-releases/
pr/123570/private-data-exchange-between-businesses-forecasted-
to-outpace-the-public-internet-by-nearly-2x-in-growth-and-6x-in-
volume-by-2020.
[20] Facebook. Building Express Backbone: Facebook’s new long-haul net-
work. https://code.facebook.com/posts/1782709872057497/building-
express-backbone-facebook-s-new-long-haul-network.
[21] Benjamin Frank, Ingmar Poese, Yin Lin, Georgios Smaragdakis, Anja
Feldmann, Bruce M. Maggs, Jannis Rake, Steve Uhlig, and Rick Weber.
Pushing CDN-ISP Collaboration to the Limit. ACM CCR, 2013.
[22] Phillipa Gill, Martin F. Arlitt, Zongpeng Li, and Anirban Mahanti. The
Flattening Internet Topology: Natural Evolution, Unsightly Barnacles
or Contrived Collapse? In PAM, 2008.
[23] Google. Google Edge Network. https://peering.google.com.
[24] Packet Clearing House. Daily Routing Snapshots. https://www.pch.
net/resources/Routing_Data.
[25] Cheng Huang, Angela Wang, Jin Li, and Keith W. Ross. Measuring
and Evaluating Large-Scale CDNs. In ACM IMC, 2008.
[26] Wenjie Jiang, Rui Zhang-Shen, Jennifer Rexford, and Mung Chiang.
Cooperative Content Distribution and Traffic Engineering in an ISP
network. In ACM SIGMETRICS/Performance, 2009.
[27] Christian Kaufmann.
Akamai
ICN.
https://pc.nanog.org/
static/published/meetings/NANOG71/1532/20171003_Kaufmann_
Lightning_Talk_Akamai_v1.pdf. NANOG 71 Light. Talk, 2017.
[28] Christian Kaufmann. ICN - Akamai’s Backbone. https://www.linx.net/
wp-content/uploads/LINX101-Akamai-ICN-ChristianKaufmann.pdf.
LINX Meeting 101, 2018.
[29] Rupa Krishnan, Harsha V. Madhyastha, Sridhar Srinivasan, Sushant
Jain, Arvind Krishnamurthy, Thomas E. Anderson, and Jie Gao. Moving
Beyond End-to-End Path Information to Optimize CDN Performance.
In ACM IMC, 2009.
[30] Craig Labovitz, Scott Iekel-Johnson, Danny McPherson, Jon Ober-
heide, and Farnam Jahanian. Internet Inter-Domain Traffic. In ACM
SIGCOMM, 2010.
[31] RIPE NCC. RIS Raw Data. https://www.ripe.net/analyse/internet-
measurements/routing-information-service-ris/ris-raw-data.
[32] University of Oregon. Route Views Archive Project. http://archive.
routeviews.org.
[33] Ricardo V. Oliveira, Dan Pei, Walter Willinger, Beichuan Zhang, and
Lixia Zhang. In Search of the Elusive Ground Truth: the Internet’s
AS-level Connectivity Structure. In ACM SIGMETRICS, 2008.
[34] Ricardo V. Oliveira, Dan Pei, Walter Willinger, Beichuan Zhang, and
Lixia Zhang. The (in)Completeness of the Observed Internet AS-level
Structure. IEEE/ACM TON, 18(1), 2010.
[35] Ricardo V. Oliveira, Beichuan Zhang, and Lixia Zhang. Observing the
Evolution of Internet as Topology. In ACM SIGCOMM, 2007.
[36] John S. Otto, Mario A. Sánchez, John P. Rula, and Fabián E. Bustamante.
Content Delivery and the Natural Evolution of DNS: Remote DNS
Trends, Performance Issues and Alternative Solutions. In ACM IMC,
2012.
[37] Vern Paxson. End-to-end Routing Behavior in the Internet. In ACM
SIGCOMM, 1996.
[38] Ingmar Poese, Benjamin Frank, Bernhard Ager, Georgios Smaragdakis,
and Anja Feldmann. Improving Content Delivery Using Provider-aided
Distance Information. In ACM IMC, 2010.
[39] Ingmar Poese, Benjamin Frank, Bernhard Ager, Georgios Smaragdakis,
Steve Uhlig, and Anja Feldmann. Improving Content Delivery with
PaDIS. IEEE Internet Computing, 16(3), 2012.
[40] Philipp Richter, Georgios Smaragdakis, Anja Feldmann, Nikolaos
Chatzis, Jan Böttger, and Walter Willinger. Peering at Peerings: On
the Role of IXP Route Servers. In ACM IMC, 2014.
[41] Matthew Roughan, Walter Willinger, Olaf Maennel, Debbie Perouli,
and Randy Bush. 10 Lessons from 10 Years of Measuring and Modeling
the Internet’s Autonomous Systems. IEEE Journal on Selected Areas in
Communications, 29(9), 2011.
[42] Brandon Schlinker, Hyojeong Kim, Timothy Cui, Ethan Katz-Bassett,
Harsha V. Madhyastha, Ítalo Cunha, James Quinn, Saif Hasan, Petr
Lapukhov, and Hongyi Zeng. Engineering Egress with Edge Fabric:
Steering Oceans of Content to the World. In ACM SIGCOMM, 2017.
219
Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
[43] Florian Streibelt, Jan Böttger, Nikolaos Chatzis, Georgios Smaragdakis,
and Anja Feldmann. Exploring EDNS-Client-Subnet Adopters in Your
Free Time. In ACM IMC, 2013.
[44] Ao-Jan Su, David R. Choffnes, Aleksandar Kuzmanovic, and Fabián E.
Bustamante. Drafting Behind Akamai (Travelocity-based Detouring).
In ACM SIGCOMM, 2006.
[45] Ruben Torres, Alessandro Finamore, Jin Ryong Kim, Marco Mellia,
Maurizio M. Munafò, and Sanjay G. Rao. Dissecting Video Server
Selection Strategies in the YouTube CDN. In IEEE ICDCS, 2011.
[46] Marc Anthony Warrior, Uri Klarman, Marcel Flores, and Aleksandar
Kuzmanovic. Drongo: Speeding Up CDNs with Subnet Assimilation
from the Client. In ACM CoNEXT, 2017.
[47] Kok-Kiong Yap, Murtaza Motiwala, Jeremy Rahe, Steve Padgett,
Matthew J. Holliman, Gary Baldus, Marcus Hines, Taeeun Kim, Ashok
Narayanan, Ankur Jain, Victor Lin, Colin Rice, Brian Rogan, Arjun
Singh, Bert Tanaka, Manish Verma, Puneet Sood, Muhammad Mukar-
ram Bin Tariq, Matt Tierney, Dzevad Trumic, Vytautas Valancius,
Calvin Ying, Mahesh Kallahalla, Bikash Koley, and Amin Vahdat. Tak-
ing the Edge off with Espresso: Scale, Reliability and Programmability
for Global Internet Peering. In ACM SIGCOMM, 2017.
[48] Bahador Yeganeh, Reza Rejaie, and Walter Willinger. A View From The
Edge: A Stub-AS Perspective of Traffic Localization and Its Implications.
In IEEE TMA, 2017.
[49] Hyunho Yeo, Sunghyun Do, and Dongsu Han. How will Deep Learning
Change Internet Video Delivery? In ACM HotNets, 2017.
220