LengthParity segment is the number of sectors in each par(cid:173)
ity segment. The performance of the policy to sched(cid:173)
ule background requests during idle intervals determines
RTupdate and consequently affects the MTTDL.
In the following, we present results for traces T1 and T2.
Traces T2 and T3 yield similar results because both have
high variability in idle times and also because for the finite
work generated by parity updates exploiting burstiness does
not yield any further improvement. The following four met(cid:173)
rics are monitored: (a) the MTTDL improvement via intra(cid:173)
disk parity, (b) the ratio of completed parity updates to the
total trace WRITE traffic, (c) the average time of parity up(cid:173)
dates which is the time interval between the completion of
a user-issued WRITE operation and the update of the par(cid:173)
ity, and (d) the overall (foreground + background) system
utilization.
7.2 Parity Updates under Trace Tl
Assuming that the disk capacity is 40GB, the relative
MTTDL improvement estimated for parity updates under
trace Tl, which has nearly 40% user WRITEs, is given in
Table 5. Recall that the relative MTTDL improvement is
defined as the difference between MTTDL under the cases
with and without intra-disk parity. Here, such an improve(cid:173)
ment attributed to intra-disk parity is only two orders of
magnitude - recall that those attributed to scrubbing are
as high as five orders of magnitude. The important result
of Table 5 is that there is almost no difference between
the MTTDL improvement achieved via instantaneous par(cid:173)
ity (IP) updates and the delayed parity updates evaluated in
this paper, which strongly argues in favor of delayed intra(cid:173)
disk parity. Furthermore, for finite background activities
(e.g., parity updates) under trace T1, the tail-based policy
achieves slightly better improvement in system reliability
than the body-based policy.
c=J
~ body
I TI
I
II 0.481 x 102 I 0.484 X 102 I 0.484 X 102 I
Policy
tail
IP
I
I
Table 5. MTTDL improvement for trace Tl via intra-disk
data redundancy, where IP is instantaneous parity update
without delaying.
Table 6. Parity update performance for trace Tl (low vari(cid:173)
ability).
1-4244-2398-9/08/$20.00 ©2008 IEEE
497
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Table 6 further shows that for this experiment the tail(cid:173)
based scheduling perfonns better than the body-based pol(cid:173)
icy across all metrics. Most importantly, the tail-based
policy updates parities almost by two orders of magnitude
faster than the body-based policy. Quick parity update times
are particularly desirable because the average parity update
time is the metric that affects data reliability. Note that sys(cid:173)
tem utilization is higher under the body-based than under
the tail-based policy. Under the body-based policy, there
are more cases where a user request preempts a parity up(cid:173)
date. Unfortunately, this results in wasted work. Under the
tail-based policy, only long idle intervals are used to update
the finite parities which results in only few of them being
preempted by foreground traffic.
100 ~-.--rr-r-~T'""""""""T'"""TT"-.:~T"""""T""'""""T"""1T"""""--"""
90
80
70
~ 60
ia'
50
40
30
20
10o L.......L...-...............""'"'----'o........&.-!o-.&..-....L.................----'--L"'---'--......................................
0.01
body-based -
tail-based
0.1
o
.
1
10
100
parity update time (s)
1000 10000
Figure 3. CDF of parity updates time for trace Tl (low
variability).
Figure 3 shows the distribution of the parity update
times. While about 68% of parity updates under the body(cid:173)
based policy are faster than under the tail-based policy, the
tail ofparity update times is longer and dominates the aver(cid:173)
age parity update time. This causes a two orders of magni(cid:173)
tude advantage for the average tail-based performance, see
the mean parity update times in Table 6.
7.3 Parity Updates under '!race T2
User issued WRITE traffic in T2 represents only 1% of
the total requests. To experiment with traces with more
WRITE traffic, we generate three additional traces that have
10%, 50%, and 90% WRITEs, respectively. These traces
are generated based on T2, by probabilistically selecting an
entry in the trace to be a READ or a WRITE.
Table 7 presents the relative MTTDL improvement via
parity updates under four variants of trace T2 using the
body-based and tail-based policies to schedule work in idle
times. This table also shows two different performances
for the tail-based policy (marked as "tail-S" and "tail-L").
Although both tail-based policies utilize the tail of the idle
times, under "tail-S" the idle wait is (approximately 40%)
shorter than under "tail-L". Table 7 shows that similar to
the results for trace TI (see Table 5), all scheduling poli(cid:173)
cies achieve two orders of magnitude MTTDL improve-
ment attributed to intra-disk parity. The difference between
the MTTDL improvement achieved via instantaneous par(cid:173)
ity (IP) updates and the delayed parity updates is negligi(cid:173)
ble, especially under the tail-based policy. Recall that IP
updates dirty parities instantaneously upon the completion
of each user-issued WRITE and is not preemptable. When
the amount of parity updates is large (e.g., cases with 50%
and 90% WRITEs), the body-based policy obtains the worst
MTTDL improvement.
Table 8 shows that since T2 has highly variable idle
times,
the tail-based policy outperforms the body-based
one. For example, the body-based policy performs at least
seven times worse than the tail-based policy with respect to
the average parity update time. The differences in perfor(cid:173)
mance between the body-based and the tail-based policies
increase as the amount of parity updates increases. Among
the tail-based policies, "tail-L" achieves better update time,
while "tail-S" complets more parity updates. The two tail(cid:173)
based policies perform exactly the same when the amount
of parity updates is small (e.g., cases with 1% WRITEs).
Timely updates are critical for MTTDL, we elaborate more
on this later in this section.
Trace
Policy
Parity Update
Parity Update
Ratio (%)
Time (s)
System
Util (%)
T2
(lOA»
T2
(10%)
T2
(50%)
T2
(90%)
body
tail-S
tail-L
body
tail-S
tail-L
body
tail-S
tail-L
body
tail-S
tail-L
28.40
66.40
66.40
13.39
33.20
21.88
3.96
21.57
17.44
2.41
18.36
16.90
141.4
25.4
25.4
7,272.9
194.3
62.9
9,721.2
69.2
31.4
8,142.6
48.6
33.4
4.68
4.57
4.57
5.23
5.03
4.75
5.42
6.36
5.88
5.44
7.36
7.03
Table 8. Performance of parity updates for trace T2 (high
variability) and four different user WRITE traffic, i.e., 1%,
10%, 50% and 90% (numbers in parenthesis indicate the
absolute number of user WRITEs).
The overall system utilization in Table 8 is not as high as
the 80% utilization level under scrubbing in Table 4 because
parity updates represent a finite amount of work. Similarly
to the results of trace T1, if the amount of parity updates
is small (cases with 1% and 10% WRITEs), then the body(cid:173)
based policy utilizes the system more than the tail-based
policy because of the preempted updates. As the amount
of parity updates increases, the· effect of this phenomenon
diminishes.
Figure 4 plots the CDFs of parity update times for all
four variants of trace T2. Consistently with results in Ta(cid:173)
ble 8, under the body-based policy, the distribution has
1-4244-2398-9/08/$20.00 ©2008 IEEE
498
DSN 2008: Mi et a!.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Policy
body
tail-S
tail-L
IP
T2
1% WRITEs
0.484 x lO~
0.484 x lO~
0.484 xlO:t
0.484 xlO:t
10% WRITEs
0.466 xlO:t
0.483 x lO:t
0.484 x lO~
0.484 x lO:t
50% WRITEs
0.386 xlO~
0.482 X lO:l
0.483 x lO~
0.484 x lO~
90% WRITEs
0.351 x lO:t
0.482 xlO:t
0.483 X lO:l
0.484 xlO:l
Table 7. MTTDL improvement for trace T2 via intra-disk data redundancy.
(a) 1% (5,000)
(b) 10% (50,000)
100 ,.............,'""'T"""""T""~~~~..........,....:r---r"T'I
90
80
70
~ 60
;;:: 50
"8 40
30
20
10
body-based -
tail-based
O~~--'-'-'-~"""""""''''''''''-''''...o.....oJ....'''''''''''''''
0.01
0.1
1
10
100 100010000
parity update time (s)
/.,
100 .......-.-TT""'T""""'I~.,....".......-.-.r-"T~.............-r~
~~
70
'#- 60
;;:: 50
"8 40
30
20
10
O~~"""""""'~"""""""''''''''''''''''''''''''''''''''''''''''''''''
0.01 0.1
·········.·.r".£..:.::.::.....
;body-based -
'tail-S
tail-L
~ 60
~ 50
"8 40
30
20
10
O~.o..L...J.-,... ........................................................................."""--'-"
0.01 0.1
10 100 10001e+41e+5
1
-
parity update time (s)
(d) 90% (450,000)
100 ........--.............
...-rr--=~............,.."""""7"""'I
::::=i
(Z::~::·=-
/1
.f,!
::
::
If
~
~ ~~
;;:: 50
"8 40
30
i~
~.1I:::::l:0I:......;.OoL....1.............J.1..............L1.......0 .......1OOL-o--.o-10.oL-.oO........0l.........e+........4.-...J1e+5
.
parity update time (s)
Figure 4. CDF of parity update time for trace T2 (high variability) and four different user WRITE traffic, i.e., 1%, 10%,50% and
90% (numbers in parenthesis indicate the absolute number of user WRITEs).
longer tail than under the tail-based policy. The "tail-L"
variant has the shortest tail indicating that the best average
perfonnance comes from the policy that results in a shorter
tail ofupdate times. The "tail-L" variant has also the longest
idle waiting, which indicates that it uses the smallest num(cid:173)
ber of idle intervals among all policies evaluated, i.e., it
waits for the very long idle intervals to arrive. Neverthe(cid:173)
less, it results in the shortest mean parity update time and
distribution tail. As parity updates increase in number, the
differences in the distribution ofupdate times between "tail(cid:173)
S" and "tail-L" decrease.
abled. Differently from the MTTDL estimation in Sec(cid:173)
tion 7, the MTTDLML(l) and MTTDLML (2) in Equa(cid:173)
tion (3) are computed using Equation (2). The average time
for a complete disk scrubbing when it runs concurrently
with parity updates is used in Equation (2) to estimate both
MTTDLML(l) and MTTDLML (2) , i.e., ML(l) = 0.5 x