useragent are correlated, and thus the independence assumption
that allows us to derive Eq. (7) from Eq. (6) does not hold in
practice; for example, mobile useragents are more likely to
show up on a phone carrier’s network. However, as we will
see in Sect. V, this correlation is not strong enough to prevent
the two features from providing complementary signals to the
model.
In production the model will be periodically trained to
incorporate changing trends in attack behavior. The training
phase in our case amounts to estimating the parameters of the
probability distributions involved in the computation of ˆgu(x)
(Eq. 13) from the available data and computing the feature
weights via logistic regression. This last phase uses labeled
data from a team that manually investigates reports of account
takeover. The output of the training phase is a set of parameters
used to update the scoring model.
Our code for model evaluation and experiments was written
in R [47] and executed on a single 2.8 GHz MacBook Pro.
Once all the data was loaded into memory, the (amoritized)
cost for scoring was less than 10 milliseconds per user. In
practice the time taken to score will be dominated by the time
required to fetch user history and feature attributes from online
data stores.
V. EXPERIMENTAL ANALYSIS
A. Experimental Setup
We built a prototype implementation of the model de-
scribed in Sect. II using one year of login data from LinkedIn,
the popular professional network. We acquired labeled data
in the form of three classes: legitimate logins, compromised
accounts, and login attempts from a single botnet attack. We
distinguish the two types of compromise events because they
have very different characteristics: compromised accounts by
deﬁnition got past LinkedIn’s existing login defenses, while
the vast majority of the botnet attempts were blocked by said
defenses.3 Our success criterion in measuring our new model is
to maintain a high level of protection against the botnet while
improving protection against the known instances of account
compromise.
For simplicity, our experiments focused on two features,
IP address and useragent; see Sect. IV for a discussion of
this choice. In practice LinkedIn’s present and future login
scoring models incorporate more than just these two features.
Our experiments were conducted ofﬂine and at no point were
live members scored by the particular models described below.
B. Dataset and Ground Truth
For our historical dataset we computed statistics on all suc-
cessful logins to LinkedIn for the six months July–December
3We unfortunately have no labeled data on attempts other than the botnet
attack that were blocked by the existing defenses.
8
2014. For every login attempt made on the site, we maintain
data available in the HTTP headers like IP address and
useragent along with password validation, timestamp, etc. We
extracted global and per-member tables for IP address and
useragent. For each IP address x1 and member u, we use
the login frequencies from the table to compute p(x1) and
p(x1|u, L). Similarly, for each useragent x2 and member u,
we compute p(x2) and p(x2|u, L). We also used LinkedIn’s
internal reputation scoring system to compute “risk scores”
for IP addresses and useragents seen over the same six-month
period.
Our training and validation data came from logins in the
six months January–June 2015. There were two types of
positive cases: (1) a single botnet attack from January 2015
in which passwords were compromised but nearly all attempts
were blocked by LinkedIn’s existing defense, and (2) a set of
accounts positively identiﬁed by LinkedIn’s Security team as
being compromised during the time frame. For negative cases,
we sampled roughly 300,000 legitimate logins from this time
period. We constructed our sample so that all accounts had at
least one login during the six-month historical period, as our
model is not designed to protect accounts that have no login
history. (One could argue that all logins after a long dormant
period should be treated as suspicious.)
Since LinkedIn was running some form of login-scoring
system during the data collection period, our labeled data may
not be representative of the real-life distribution of account-
takeover attempts. In particular, beyond the single botnet
incident (which LinkedIn identiﬁed from signals in internal
data) we have no way of identifying with high conﬁdence
account-takeover attempts that were already blocked by this
system. We could simply mark all blocked login attempts as
account-takeover attempts, but since we have reason to believe
that such attempts are a relatively small proportion of the total
blocked login trafﬁc, marking all blocks as positive samples
would pollute our labeled dataset.
C. Baseline and Performance Metrics
Our baseline is one of the simplest rules used to protect
login: ﬂag the login as suspicious if it comes from a country
not in the member’s prior login history. We evaluated our data
set against this criterion and found the following:
Class
Legitimate
Compromise
Botnet
Country History Match
96.3%
93.3%
1.0%
The disparity in history match between the compromised
accounts and the botnet victims reﬂects the fact that country
history match is a component of LinkedIn’s existing login
defenses.
Since the simple country-mismatch rule is already strong
enough to stop 99% of (this particular) botnet attack, our goal
in developing and tuning our model will be to maintain a
high level of botnet protection while improving the coverage
of known compromises, without signiﬁcantly increasing false
positives on legitimate accounts. To turn this goal into mea-
surable statistics, we will assess the performance of our model
against different types of attacks (either observed or simulated)
by computing the True Positive Rate (TPR, i.e., the fraction
of correctly classiﬁed account-takeover attempts) at 10% False
Positive Rate (FPR, i.e., the fraction of legitimate attempts
misclassiﬁed as attacks). Since false positives translate into
extra steps required for good users to log in (see Sect. IV), the
choice of an acceptable FPR is entirely a business decision to
be made on a per-implementation basis; we choose 10% here
as an arbitrary yet reasonable baseline.
To compare different choices of model (e.g., different
smoothing techniques) we plot the Receiver Operating Char-
acteristic (ROC) curve, which shows how the TPR varies
as a function of the FPR for different decision thresholds,
and compute the Area Under the ROC Curve (AUC). Notice
that using AUC as a performance metric allows us to avoid
choosing a speciﬁc decision threshold for classiﬁcation. We
also remark that such metrics are insensitive to class imbalance
in the training data, which is necessary as we signiﬁcantly
downsampled login data to create our training set.
D. System Performance and Model Variations
Using the LinkedIn data made available to us, we computed
conﬁdence scores gu(x) using Eqs. (7) and (13). We tried
several different combinations of parameters and used area
under the ROC curve as our metric for comparison. The
dimensions along which we varied parameters were as follows:
Smoothing. We evaluated both backoff and linear interpolation
smoothing, as described in Sect. II-C. We also evaluated
two different choices for the parameter µhk representing the
number of unseen IP addresses for each entity hk. Our ﬁrst
choice was µhk = 1 for all hk, as represented in Fig. 1.
However, we encountered two problems with this choice:
• First, µhk = 1 does not properly penalize a country
mismatch. For example,
in the scenario developed in
Fig. 1, a login from an IP in the most common ISP
in the most common country is assigned a probability
1/9, while a login from a completely new country is
assigned a probability 1/18. From our experience we feel
that the relative risk ratio of these two events should be
much greater than 2, so we want to assign a much lower
probability to the new country.
• Second, we found that Algorithm 1 did not converge when
run on our data set with smoothing µhk = 1, while it did
converge when we set µhk to be much larger.
We therefore recomputed the features with µhk = |hk|, the
total number of IP addresses seen in entity hk, to obtain three
different choices for smoothing: backoff with µhk = 1, backoff
with µhk = |hk|, and interpolation with µhk = |hk|.
We reserved 40% of our training set to train the inter-
polation coefﬁcients, and evaluated all of our models on the
remaining 60% of the data.
Features and feature weights. For comparison purposes we
considered four different ways to combine features:
• IP address only, using Eq. (7),
• Useragent (UA) only, using Eq. (7),
• IP address and useragent, using Eq. (7),
9
(a) Backoff smoothing, µhk = 1.
(b) Backoff Smoothing, µhk = |hk|.
Fig. 3: ROC curves for the two backoff smoothing techniques with (cid:96) = 4. AUC values are reported in parentheses.
(a) Interpolation smoothing.
(b) Evasion attacks.
Fig. 4: ROC curves for interpolation smoothing with (cid:96) = 4 and evasion attacks. AUC values are reported in parentheses.
• IP and useragent features weighted by a logistic regres-
sion model, using Eq. (13) as described in Sect. II-D.
For the logistic regression model, we trained the classiﬁer
on 60% of the data and evaluated it on the remaining 40%.4
Levels of granularity. Both IP addresses and useragents can
be aggregated into groupings of increasing size. We wanted
to determine the effect of changing the number of levels (cid:96) of
the hierarchy, so we considered two such hierarchies for each
feature:
4Since we already reserved 40% of the data to train interpolation coefﬁ-
cients, as a proportion of all entire dataset we trained on 36% and evaluated
on 24%.
• For IP addresses, we considered hierarchies of (IP ad-
dress, organization, autonomous system (AS), country,
world) ((cid:96) = 4); and (IP, AS, world) ((cid:96) = 2).
• For useragents, we considered nested hierarchies of (user-
agent, browser family, operating system, app, world)
((cid:96) = 4); and (useragent, OS, world) ((cid:96) = 2). “App”
refers to the user experience: desktop, mobile web, native
mobile app, or unknown. By a “nested hierarchy” we
mean that
is app + OS +
browser, then app + OS, etc.
the level above useragent
Results. We computed 24 different sets of scores (3 smoothing
methods × 4 feature combinations × 2 sets of feature hier-
10
  0.1  0.5    1     2     5     10    20    40    60    80    90    95    98  0102030405060708090100False Positive Rate (%)True Positive Rate (%)  IP (0.624)UA (0.612)IP + UA (0.7)Weighted IP + UA (0.819)  0.1  0.5    1     2     5     10    20    40    60    80    90    95    98  0102030405060708090100False Positive Rate (%)True Positive Rate (%)  IP (0.868)UA (0.786)IP + UA (0.912)Weighted IP + UA (0.952)  0.1  0.5    1     2     5     10    20    40    60    80    90    95    98  0102030405060708090100False Positive Rate (%)True Positive Rate (%)  IP (0.872)UA (0.789)IP + UA (0.913)Weighted IP + UA (0.955)  0.1  0.5    1     2     5     10    20    40    60    80    90    95    98  0102030405060708090100False Positive Rate (%)True Positive Rate (%)  Password−only (0.999)Botnet (sim.) (0.992)Researching (0.985)Phishing (0.924)Botnet (real) (0.969)Compromised (0.934)Features
IP
UA
IP + UA
Weighted IP + UA
Backoff, µhk = 1
(cid:96) = 4
(cid:96) = 2
0.56
0.624
0.612
0.67
0.7
0.68
0.819
0.82
Backoff, µhk = |hk|
(cid:96) = 2
0.82
0.76
0.89
0.94
(cid:96) = 4
0.868
0.786
0.912
0.952
Interpolation, µhk = |hk|
(cid:96) = 2
0.82
0.75
0.88
0.94
(cid:96) = 4
0.872
0.789
0.913
0.955
TABLE I: AUC values for various feature combinations, smoothing models, and number of levels (cid:96) in feature hierarchies.
Features
IP
UA
IP + UA
Weighted IP + UA
Backoff, µhk = 1
(cid:96) = 4
(cid:96) = 2
0.19
0.19
0.32
0.29
0.15
0.14
0.43
0.43
Backoff, µhk = |hk|
(cid:96) = 2
0.28
0.41
0.58
0.83
(cid:96) = 4
0.39
0.46
0.74
0.88
Interpolation, µhk = |hk|
(cid:96) = 2
0.28
0.40
0.56
0.83
(cid:96) = 4
0.44
0.47
0.75
0.89
TABLE II: TPR at 10% FPR for various feature combinations, smoothing models, and number of levels (cid:96) in feature hierarchies.
archies) for each login in our test set. Table I gives AUC for
each combination. Table II shows the TPR at 10% FPR for
each set of scores. ROC curves with ((cid:96) = 4) and 4 feature
combinations for the 3 smoothing techniques are plotted in
Fig. 3 and Fig. 4.
sampled IP addresses from a known hosting provider weighted
by their “risk score” as calculated by LinkedIn, and we give
all attempts the useragent “Python-httplib2/0.7.2 (gzip)”. Our
best model easily identiﬁed this attack, with AUC 0.999 and
99% TPR at just a 1% FPR.