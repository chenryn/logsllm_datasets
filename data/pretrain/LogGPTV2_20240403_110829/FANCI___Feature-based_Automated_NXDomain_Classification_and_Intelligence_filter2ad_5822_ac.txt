classiﬁcation module generalizes very well.
5.2.1 Experimental Setup
Siemens. As a second source for bNXDs we obtained
data from the DNS infrastructure of Siemens. Note that
we only obtained NXDs and not full NXD responses
as this is entirely sufﬁcient for FANCI’s classiﬁcation
7Education Roaming—WLAN infrastructure for students and em-
ployees, https://eduroam.org
Due to the considerable size of our data set, we per-
formed random sampling to generate sets for our eval-
uations. Each data set is composed of as many bNXDs
as mAGDs, and is created by performing fresh uniform
random sampling for each single set from our benign
data sets. Depending on the corresponding experiment,
the malicious data is either drawn uniformly at random
USENIX Association
27th USENIX Security Symposium    1171
from the unique mAGDs of all DGAs or from the unique
mAGDs of a single DGA. For sets considering all DGAs,
we strive a uniform representation of all DGAs as far as
possible. The size of a set here denotes the number of
samples in total, that is, the sum of bNXDs and mAGDs.
Depending on the experiment we perform either a
5-fold cross validation (CV) or a leave-one-group-out
(LOGO) CV. In a 5-fold CV the data set is divided into
5 equally sized folds using 4 for training and 1 for pre-
diction. Each fold is used exactly once for prediction.
Resulting statistical metrics are averaged over all 5 runs.
An LOGO CV is in its basic procedure similar to a k-fold
CV, but instead of building k random folds, the folds are
deﬁned regarding a predeﬁned grouping, for example, by
seeds or DGAs.
We determined the optimal parameter settings for the
ML algorithms for two different scenarios with the help
of extensive grid searches on data sets independent of
the ones used for evaluation. The ﬁrst scenario considers
single-DGA detection, (i.e., one classiﬁer targeting one
speciﬁc DGA), where the second targets multi-DGA de-
tection (i.e., one classiﬁer trained to detect all DGAs).
We ﬁxed the resulting parameters and used them in all
subsequent evaluation scenarios including the one done
in the wild. For an excerpt of the results of the grid
searches see Appendix B.
All computations were carried out on the RWTH Com-
pute Cluster8.
In all experiments, we consider accuracy (ACC) as pri-
mary metric to characterize a classiﬁer’s performance de-
ﬁned as ACC = |T P| +|T N|/|population|, where |T P| is
the amount of true positives and |T N| the amount of true
negatives. This means that ACC indicates the fraction
of correctly predicted samples. However, for each ex-
periment we additionally present statistics of the follow-
ing four metrics: true positive rate (TPR), true negative
rate (TNR), false negative rate (FNR), and false positive
rate (FPR). For each metric we consider the arithmetic
mean x, the standard deviation σ, the minimum xmin, the
median ˜x, and the maximum xmax.
5.2.2 Classiﬁer Selection
In this section, the presented experiments reﬂect the pro-
cedure to select the best performing classiﬁers for a real-
world application. For the following experiments we
consider benign data from RWTH Aachen exclusively.
We performed each experiment for SVMs and RFs. As it
is our goal to ﬁnd the best performing classiﬁer and RFs
perform marginally better than SVMs in most scenarios,
we present results for RFs in the following in detail. Re-
sults for SVMs can be found in Appendix A.
8https://doc.itc.rwth-aachen.de/display/CC
ACC
TPR
TNR
FNR
FPR
0.99936
0.00190
0.98600
0.99988
1.00000
0.99989
0.00050
0.99400
1.00000
1.00000
0.99883
0.00351
0.97267
0.99978
1.00000
0.00011
0.00050
0.00000
0.00000
0.00600
0.00117
0.00351
0.00000
0.00022
0.02733
x
σ
xmin
˜x
xmax
Table 4: Results for classifying bNXDs and mAGDs of
single DGAs with RFs. In total, 295 sets of 59 DGAs
were considered each evaluated by 5 repetitions of a 5-
fold CV.
Single DGAs. The ﬁrst experiment covers the detec-
tion of a certain single DGA using a dedicated classiﬁer.
We considered all 59 DGAs and created 5 different sets
per DGA of a maximum set size of 100,000 following
the procedure presented in Section 5.2.1. This means
that each data set always contains an equal number of
mAGDs and bNXDs. Depending on the DGA less than
50,000 unique mAGDs may be available. In these cases
the set size is adjusted accordingly.
In summary, this
yields 295 sets of a maximum size of 100,000. For each
set we performed 5-fold CVs, which we repeated 5 times
with fresh, random folds.
Table 4 presents a statistical description of an RF’s ca-
pabilities in the detection of single DGAs. The mean
ACC is 0.99936 with a small standard deviation of
0.00190. The minimal ACC of 0.98600 is reached in the
detection of Bobax, which is the only outlier. RFs de-
tect 6 out of 59 DGAs (Bamital, Blackhole, Dyre, Sisron,
Tofsee, and UD2) with 100 percent ACC.
Unknown Seeds.
In this experiment, we focus on eval-
uating the detection of mAGDs generated by a DGA with
a new seed, where the model is trained with mAGDs gen-
erated by the same DGA using known seeds.
To evaluate this scenario we perform an LOGO CV,
that is, we perform training with mAGDs of all but one
seed of a certain single DGA, perform prediction on the
skipped one, and repeat this procedure for each seed and
DGA. Again, we use data sets with a maximum size of
100,000 and use 5 distinct sets per DGA. We consider
all DGAs with at least two known seeds, which yields
30 DGAs with 550 seeds overall.
In total, this results
in 5 · 550 = 2750 iterations for all available seeds and
DGAs.
A statistical summary of the evaluation results for this
experiment for RFs is depicted in Table 5. The mean of
the ACC is 0.95319 showing a notable standard deviation
of 0.12499. ACC values are between 0.49900 and 1.0,
where 75 percent of all measures show a higher ACC
than 0.98193. As only 6 DGAs are related to an ACC
lower than 98 percent, the wide range of the ACC can be
1172    27th USENIX Security Symposium
USENIX Association
ACC
TPR
TNR
FNR
FPR
ACC
TPR
TNR
FNR
FPR
x
σ
xmin
˜x
xmax
0.95319
0.12499
0.49900
0.99965
1.00000
0.90689
0.25005
0.00000
0.99991
1.00000
0.99947
0.00075
0.99570
0.99960
1.00000
0.09330
0.25059
0.00000
0.00011
1.00000
0.00053
0.00075
0.00000
0.00040
0.00430
x
σ
xmin
˜x
xmax
0.98073
0.00034
0.97972
0.98078
0.98119
0.96389
0.00065
0.96182
0.96397
0.96468
0.99756
0.00015
0.99726
0.99759
0.99779
0.02424
0.00072
0.02339
0.02416
0.02649
0.00244
0.00015
0.00221
0.00241
0.00274
Table 5: Results for LOGO CV for mAGDs of single
DGAs grouped by seed using RFs. In total, 150 sets of
30 DGAs were considered.
Table 7: Results for LOGO CV for sets of mAGDs of
mixed DGAs grouped by DGA using RFs. In total, 20
sets were considered.
ACC
TPR
TNR
FNR
FPR
0.99759
0.00009
0.99745
0.99758
0.99776
0.99764
0.00013
0.99739
0.99762
0.99783
0.99753
0.00012
0.99733
0.99752
0.99772
0.00236
0.00013
0.00217
0.00238
0.00261
0.00247
0.00012
0.00228
0.00248
0.00267
x
σ
xmin
˜x
xmax
Table 6: Results for detecting mAGDs with RFs of arbi-
trary mixed DGAs using 5 repetitions of 5-fold CV for
each set. In total, 20 sets were considered.
explained by outliers.
This experiment is the only experiment, where SVMs
perform slightly better than RFs. SVMs achieve a mean
ACC of 0.98315 with a much smaller standard deviation
of 0.06166, but with a similar wide range from 0.49850
to 1.0. Detailed results of this experiments for SVMs are
presented in Table 14. SVMs are also affected by the
same outliers (i.e., the same DGAs cause problems) as
RFs. In contrast to RFs, SVMs do not consistently miss
all new seeds of these certain DGAs and hence yield a
slightly higher ACC in the mean.
Mixed DGAs. Next, we examine how well a single
classiﬁer trained on some mAGDs of the known DGAs
is able to detect other mAGDs generated by one of these
known DGAs.
We created 20 sets of a targeted size of 100,000 con-
taining an equal number of mAGDs of each of the 59
DGAs. For DGAs with a too small amount (i.e., less
than 50000/59 ≈ 847) of unique mAGDs we included
all available mAGDs of such DGAs, which results in an
effective set size of 92,102. For each of these 20 sets we
performed 5 repetitions of a 5-fold CV.
In its trend, results for detecting mAGDs in sets con-
taining mAGDs of multiple DGAs are similar to the
detection of using dedicated classiﬁers for each sin-
gle DGAs as presented previously. Table 6 illustrates
measurement results for RFs. The ACC’s mean is
0.99759 with a very small standard deviation of 0.00009.
Minimum and maximum ACC values are 0.99745 and
0.99776 respectively.
In summary, we state a single classiﬁer trained with
mAGDs of multiple DGAs achieves a very high and sta-
ble ACC in detecting arbitrary mAGDs.
Unknown DGAs. This experiment conﬁrms capabili-
ties in detecting mAGDs of unknown DGAs. To verify
that our classiﬁers are able to generalize to mAGDs of
unknown DGAs we performed LOGO CV regarding a
grouping by DGA, that is, mAGDs of all but one DGA
are used for training and mAGDs of the left out DGA are
predicted. Sets considered in this experiment are equiva-
lent to sets from the previous experiment, that is, we con-
sider 20 sets with equal numbers of mAGDs per DGA.
This means that for each of the 20 sets we performed 59
iterations of training and prediction leaving one DGA out
at once.
Table 7 depicts a statistical summary of results for RFs
in detecting mAGDs of unknown DGAs. The ACC is be-
tween 0.97972 and 0.98119 and the mean of the ACC is
0.98073 with a very small standard deviation of 0.00034.
RFs detect 55 out of 59 left out DGAs with an ACC
comparable to the previously presented experiment. We
conclude that we are able to detect mAGDs of unknown
DGAs.
Classiﬁer Selection.
In real-world applications, we
aim at reliably detecting known DGAs as well as un-
known seeds and DGAs.
Furthermore, we want to
achieve maximum classiﬁcation accuracy. Hence, we
have to choose the best performing classiﬁer or ensem-
ble of classiﬁers to achieve these goals. For this reason,
we additionally evaluated several logical combinations
of classiﬁers dedicated to single DGAs. In particular, we
tested several or and and combinations, threshold vot-
ing with different thresholds, majority voting, even with
combinations of RFs and SVMs. However, a single RF
classiﬁer trained with all known DGAs outperforms any
of the above ensembles. That is why FANCI uses a single
RF classiﬁer trained with mAGDs of all known DGAs.
5.2.3 Generalization
Up to now, we performed all experiments with test sets
containing bNXDs from RWTH Aachen University. In
USENIX Association
27th USENIX Security Symposium    1173
ACC
TPR
TNR
FNR
FPR
ACC
TPR
TNR
FNR
FPR
x
σ
xmin
˜x
xmax
0.99699
0.00015
0.99681
0.99697
0.99730
0.99815
0.00018
0.99787
0.99812
0.99868
0.99582
0.00022
0.99540
0.99581
0.99628
0.00185
0.00018
0.00132
0.00188
0.00213
0.00418
0.00022
0.00372
0.00419
0.00460
x
σ
xmin
˜x
xmax
0.99534
0.00018
0.99511
0.99530
0.99565
0.99937
0.00007
0.99920
0.99939
0.99949
0.99132
0.00034
0.99083
0.99125
0.99201
0.00063
0.00007
0.00051
0.00061
0.00080
0.00868
0.00034
0.00799
0.00875
0.00917