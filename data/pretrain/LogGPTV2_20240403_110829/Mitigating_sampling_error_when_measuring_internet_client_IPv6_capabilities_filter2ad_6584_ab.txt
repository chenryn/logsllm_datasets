within 24 hours is low (see Section 3.1)
2.4 User impact
The JS-test is only executed after the web page has loaded com-
pletely. Hence it does not delay the loading of the page. The JS-test
script is 12 kB large (including the Google Analytics code). The
loading of the FA-test is controlled by Google. Our ad is only 16 kB
large, not larger than many other ads and well below Google’s size
limit of 50 kB. The test images are only 157 bytes large and in-
visible (loaded but not displayed). The total size of the test data
transferred is well under 20 kB, less than 6% of the average web
page size of 320 kB (as determined in [21]).
Our test opens six diﬀerent TCP connections to download the
test images (ﬁve for the sub-tests plus one for the summary). The
download is done in the background by the browsers. The max-
imum number of concurrent open connections is at least 30 for
most browsers [22]. Whether the test connections will cause ex-
isting connections to be closed depends on how many connections
are already open. But in any case the test does not aﬀect the load-
ing of the current page and the next page will not load slower than
when loaded for the ﬁrst time.
We argue that overall our test does not have a signiﬁcant impact
on the user’s browsing experience. We conducted a few simple tests
in which test users did not experience any noticeable impact. Also,
our test does not trick users into revealing any sensitive informa-
tion. Instead, we utilise information that web clients normally send
to every visited web server.
3. DATASET
We use the data collected between 16th of May 2011 and 19th
of February 2012. We discarded 1% of tests as “invalid”, such as
tests with malformed or truncated URLs in the server logs. We also
discarded tests of clients referred by three large web sites that only
Figure 2: Total number of tests and number of completed tests
per day
Figure 3: Number of tests per day per IP address for JS-test
and FA-test
participated for one or two days. We analyse the data in blocks of
days (24 hours). However, we will present most of the statistics
averaged over weekly periods to reduce the error of the estimates.
We now describe the properties of our dataset.
3.1 Number of tests
Figure 2 shows the weekly averages of the number of total and
completed (with test summary) tests per day. Apart from the initial
period the total number of tests per day was 250 000–300 000 and
the number of completed tests was 180 000–200 000 per day (of
which 30 000–35 000 were FA-tests and the rest were JS-tests).
(cid:113)
n
ˆp(1− ˆp)
The statistics we present in Section 5 are proportions. If the sam-
ples are independent, then the standard error for proportion esti-
, where ˆp is the estimated proportion and n
mates is SE =
is the number of samples. Generally, for our statistics the relative
error is below 1% and hence we do not plot error bars. However,
due to the way we sample the clients there is an additional sampling
error, which we discuss in Section 4.
Figure 3 shows the CDFs of the number of tests per day for each
IPv4 address. FA-tests have a higher repeat rate (FA-tests cannot
use cookies), but it is only slightly higher since Google distributed
the ads well. Less than 8% of IPs performed the test more than
once per day and less than 0.1% (JS-test) or 0.3% (FA-test) of IPs
performed the test more than 10 times per day. This suggests that
single IPs do not dominate our statistics.
>E^ĞƌǀĞƌ/ŶƚĞƌŶĞƚdĞƌĞĚŽͬϲƚŽϰůŝĞŶƚE^tĞďƚĐƉĚƵŵƉ>ŽŐŐŝŶŐ͙dWŚĂŶĚƐŚĂŬĞ;ƐŝŵƉůŝĨŝĞĚͿTotal number of testsDate16May201125Jun201104Aug201113Sep201123Oct201102Dec201111Jan201220Feb20120100200300400Tests [k]GGGGGGGTotalComplete05101520250.940.960.981.00Number of tests per IP addressNumber of tests per IP addressCDFGGJSFA89address’ last octet to zero). The dashed lines show the maxima
(unique equals total). Since the IPv6 capability of clients depends
on the clients themselves and on the clients’ ISPs, a good spread
over diﬀerent /24 is also desirable.
The FA-test reaches a much larger proportion of unique IP ad-
dresses. The chance of the same IP address being tested multiple
times is much lower than for the JS-test. It appears that Google tries
to distribute the ads to a wide audience (at least in our case where
the ad is not clicked often). For /24 networks the FA-test initially
also tested unique /24 networks at a higher rate, but over time has
slowed to a rate only slightly higher than that of the JS-test. The
FA/JS-test sampled 0.40%/0.48% of the routed IP addresses and
22.2%/21.7% of the routed /24 networks (the total number of routed
addresses divided by 256). Overall both tests sampled 0.75% of
the routed IP addresses and 28.7% of the routed /24 networks. The
percentages are based on Routeviews data [26] (2.52 billion routed
addresses in February 2012).
Figure 7 plots the percentages of /24 networks from each /8 pre-
ﬁx where at least one IP was sampled for JS-test and FA-test versus
each other. We classiﬁed the /8 preﬁxes based on which RIR mainly
assigned them, and use the category “Several” for preﬁxes assigned
by multiple RIRs. The ﬁgure shows that the JS-test covers signiﬁ-
cantly more /24s in many preﬁxes assigned by ARIN, and slightly
more /24s in some preﬁxes assigned by RIPE or several RIRs (the
latter mainly in the old class B space). On the other hand, the FA-
test covers signiﬁcantly more /24s in preﬁxes assigned by APNIC
and a few more /24s in some preﬁxes assigned by LACNIC.
4. SAMPLING ERROR MITIGATION
Now we discuss sources of potential bias and present methods
to mitigate the error. Our main method is based on re-weighting
the data. We compare our OS and browser statistics from the raw
and re-weighted data with reference statistics and show that the re-
weighted statistics are a much better match than the raw statistics.
4.1 Error analysis and data re-weighting
Our data collection can be modelled as probability sampling,
since random clients are tested. However, our client sample is
not uniform. Diﬀerent sites hosting the JS-test have diﬀerent au-
diences. Hence, the JS-test error has two parts: a sampling error
due to the random choice of sites and the bias of results conditional
on a given site. We view the total error as sampling error since,
averaged over all possible random choices of sites, the average per-
site bias would be zero. The FA-test has an actual bias towards
low-revenue sites showing our Google ad, and we abuse terminol-
ogy by referring to this also as sampling error.
Bias may be caused by clients that do not respond to the test.
The JS-test uses JavaScript, but all major browsers are JavaScript-
capable and JavaScript is usually enabled as many web pages rely
on it. According to [20] only 1–2% of clients have JavaScript dis-
abled. Also, disabled JavaScript may be uncorrelated with IPv6
capability. We do not expect signiﬁcant error to be introduced by
this non-response.
The FA-test requires Flash, which may not be present on all
clients. Most notably during the time of our experiments iOS
(iPhone, iPad) did not support Flash, and on some Unix OS Flash
also does not always work out-of-the-box. Since IPv6 capabil-
ity depends on the OS, it is likely that the lack of Flash in non-
Windows OS introduces bias. Furthermore, bias could be intro-
duced by clients that use ad-blockers, which prevent the execution
of ads. However, it is unclear what percentage of clients use ad-
blockers and whether their use is correlated with IPv6 capability.
Figure 4: Minimum number of tests per day for the top-10,
top-30 and top-60 traﬃc generating countries [24,25]
Figure 5: Number of domains with at least 50, 100 and 500 tests
per day
3.2 Country and domain coverage
According to MaxMind’s GeoIP [23] our dataset includes clients
from 95% of all country codes including all 196 sovereign countries
(the missing 5% are dependent territories). For about one third of
the sovereign countries we observed at least 100 tests per day (with
a few outliers), and these countries accounted for almost 100% of
the Internet’s traﬃc according to [24,25]. Figure 4 shows the mini-
mum number of samples per day per country for the top-10, top-30
and top-60 countries, which generate 76%, 90%, 98% of the Inter-
net’s traﬃc according to Cisco and Wikipedia statistics [24,25] (we
use the 5% quantile as minimum to exclude a few outliers). Apart
from the initial period the minimum number of samples per day
was higher than 750 for any of the top-10 countries, higher than
200 for any of the top-30 countries and at least 100–150 for any of
the top-60 countries.
Figure 5 shows the number of domains sampled that generated
at least 50, 100 or 500 tests per day (based on HTTP referrer in-
formation for JS-tests and Google ad placement information for
FA-tests). Apart from the initial period, there always were 30–35
domains that each generated at least 500 tests a day and 55–75 do-
mains that each generated at least 100 tests per day.
3.3 Client and subnet coverage
Figure 6 shows the number of unique versus total tested IPv4
addresses (left) and unique versus total tested /24 networks (right)
for the measurement period (a /24 is obtained by setting an IPv4
Minimum number of tests per country per dayDate16May201125Jun201104Aug201113Sep201123Oct201102Dec201111Jan201220Feb2012050010001500Minimum number of tests per dayGGGGGGGTop 10Top 30Top 60Average number of tests for domainsDate16May201125Jun201104Aug201113Sep201123Oct201102Dec201111Jan201220Feb2012020406080100Number of domains sampledGGGGGGG50+ tests100+ tests500+ tests90Figure 6: Unique vs. total tested IPv4 addresses (left) and unique vs. total tested /24 networks (right). The dashed lines show the
maxima (all observed IPs or /24s are unique).
ranks of the 16 countries in both datasets are broadly consistent.
Spearman’s rank correlation is 0.54 (1.0 indicates perfect positive
correlation) and a hypothesis test indicates correlation (at 99% sig-
niﬁcance level). However, the Wikipedia data appears biased to-
wards non-Asian countries. Hence, we use the Cisco estimates for
the 16 countries covered and the Wikipedia statistics for the re-
maining countries.
We use MaxMind’s GeoLite country database [23] to map IPv4
client addresses to countries, which has a claimed accuracy of
99.5%. Note that the ISO 3166-1 country codes represent not only
sovereign countries, but also dependent territories and special areas
of geographical interest.
Similar re-weighting approaches were used to mitigate bias be-
fore. For example, to deal with non-response the US Current Pop-
ulation Survey divided all sampled households into 254 adjustment
cells and for each cell the sampling weight of non-respondents was
redistributed to the other households in the cell [27].
We interpret the measurement results as statistics of connections3
and not of clients. Interpreting the measurements as statistics of
clients would result in an “observation bias”. Also, IP addresses
are not very good identiﬁers for clients (even when combined with
browser ID strings). Using statistics of connections we avoid poten-
tial bias caused by multiple clients behind web proxies or NATs, or
clients that change their IP addresses more often (home users, mo-
bile users, frequently oﬄine clients). Furthermore, we avoid poten-
tial bias because we are more likely to observe clients that are more
likely to use the Internet, which potentially have more up-to-date
systems and are more up to date with IPv6.
4.2 Effectiveness of re-weighting
To determine the possible bias in the raw data and the eﬀective-
ness of our re-weighting we compare the clients’ OS, web browser,
and Windows version distributions from our data with reference
“population statistics”. Since nobody has true population statistics
we use the data of multiple statistics providers as reference.
We used the two-sample t-test (statistical hypothesis test) to
compare the JS-test and FA-test combined raw and re-weighted
data with the reference. The test determines whether the diﬀerence
of the means of two samples is signiﬁcant or due to random chance.
The null hypothesis is that the diﬀerence between the two means is
zero. If the null hypothesis is rejected, then there is a statistically
signiﬁcant diﬀerence.
3Here “connections” refers to instances of end-host connectivity
and not to transport-layer connections.
Figure 7: Percentage of /24 networks from each /8 preﬁx where
at least one IP was sampled for JS-test (x-axis) and FA-test (y-
axis)
Another source of potential bias are clients that do not perform
some sub-tests, not because they lack the capabilities, but because
of other reasons. For example, if a user moves to another web
page quickly, this eﬀectively aborts a test. To eliminate this bias
we analyse only completed tests (with test summary). For these we
can be sure that the clients attempted all sub-tests.
Our sample of tested clients is biased towards the web sites acti-
vating the JS-tests and FA-tests. For example, Indonesian JS-test
connections are well over-represented due to a large participat-
ing Indonesian web site. Furthermore, the participating web sites
change over time. To mitigate this bias we weight each test based
on the tested client’s country.2 Let Pc be the weight of country c
((cid:80) Pi = 1), T be the total number of tests, and Tc be the number of
tests of country c . Then the weight for a test Wt is:
Wt = Pc
T
Tc
.
(1)
Our weights Pc are based on traﬃc statistics estimated by Cisco
[24] for 16 countries that generate 79% of the Internet’s traﬃc
and Wikipedia country traﬃc statistics [25]. The traﬃc proportion
2For <1% of JS-tests, where the IPs were proxies located in dif-
ferent countries than the clients (e.g. BlackBerry connections), we
assume a client’s country was the referring web sites’ country (our
data shows that many sites had relatively high locality).
010203040024681012Total IP addresses [M]Unique IP addresses [M]GGJSFA0510150.00.51.01.52.0Total /24 networks [M]Unique /24 networks [M]GGJSFAG020406080100020406080100JS−test /24s sampled in each /8 [%]FA−test /24s sampled in each /8 [%]GARINRIPEAPNICLACNICAFRINICSeveralGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG91Table 1: OS percentage of connections for raw (R) and re-weighted (W) data (mean±standard deviation)
Windows
MacOS X
iOS
Linux
Reference [%]
81.5±4.5
8.8±3.1
4.4±1.4
1.2±0.4
JS R [%]
89.1±1.8