objects on QUIC multiplexing. We leave investigating the effect of
dynamic pages on performance for future work.
In addition, we evaluate video streaming performance for content
retrieved from YouTube. Specifically, we use the YouTube iFrame
API to collect QoE metrics such as time to start, buffering time, and
number of rebuffers.
Performance Metrics. We measure throughput, “page load
time” (i.e., the time to download all objects on a page), and video
quality metrics that include time to start, rebuffering events, and
rebuffering time. For web content, we use Chrome’s remote debug-
ging protocol [2] to load a page and then extract HARs [34] that
include all resource timings and the protocol used (which allows
us to ensure that the correct protocol was used for downloading an
object9). For video streaming, we use a one-hour YouTube video
that is encoded in all quality levels (i.e., from “tiny” to 4K HD).
9Chrome “races” TCP and QUIC connections for the same server and uses the one that establishes
a connection first. As such, the protocol used may vary from the intended behavior.
 0 500 1000 1500 2000Our server unadjustedGAEOur server adjustedTime (ms)ReceiveWaitIMC ’17, November 1–3, 2017, London, United Kingdom
A. Molavi Kakhki et al.
4 EVALUATION FRAMEWORK
Our testbed provides a platform for running experiments, but does
not alone ensure that our comparisons between QUIC and TCP are
sound, nor does it explain any performance differences we see. We
face two key challenges in addressing this.
First, Google states that the public version of QUIC is not “per-
formant" and is for integration testing purposes only [8]. To ensure
our findings are applicable to Google’s deployment environment,
we must configure our QUIC servers to match the performance of
Google’s QUIC servers.
Second, QUIC is instrumented with a copious amount of de-
bugging information but no framework that maps these logs into
actionable information that explains performance. While there is a
design document, there is no state machine to compare with.
We now describe how we address these challenges in our evalu-
ation framework.
4.1 Calibration
At first glance, a simple approach to experimenting with QUIC
as configured by Google would simply be to use Google’s servers.
While this intuition is appealing, it exhibits two major issues. First,
running on Google servers, or any other servers that we do not
control, prevents us from instrumenting and altering the protocol
to explain why performance varies under different network en-
vironments. Second, our experience shows that doing so leads to
highly variable results and incorrect conclusions.
For example, consider the case of Google App Engine (GAE),
which supports QUIC and allows us to host our own content for
testing. While the latency to GAE frontends was low and constant
over time, we found a variable wait time between connection estab-
lishment and content being served (Fig. 2, middle bar). We do not
know the origins for these variable delays, but we suspect that the
constant RTT is due to proxying at the frontend, and the variable
delay component is due to GAE’s shared environment without re-
source guarantees. The variability was present regardless of time
of day, and did not improve when requesting the same content
sequentially (thus making it unlikely that the GAE instance was
spun down for inactivity). Such variable delay can dominate PLT
measurements for small web pages, and cannot reliably be isolated
when multiplexing requests.
To avoid these issues, we opted instead to run our own QUIC
servers. This raises the question of how to configure QUIC pa-
rameters to match those used in deployment. We use a two-phase
approach. First, we extract all parameters that are exchanged be-
tween client and server (e.g., window sizes) and ensure that our
QUIC server uses the same ones observed from Google.
For parameters not exposed by the QUIC server to the client,
we use grey-box testing to infer the likely parameters being used.
Specifically, we vary server-side parameters until we obtain perfor-
mance that matches QUIC from Google servers.
The end result is shown in Fig. 2. The left bar shows that QUIC
as configured in the public code release takes twice as long to
download a large file when compared to the configuration that
most closely matches Google’s QUIC performance (right bar)10.
State
Init
Slow Start
Congestion Avoidance (CA) Normal congestion avoidance
CA-Maxed
Application Limited
Description
Initial connection establishment
Slow start phase
Max allowed win. size is reached
Current cong. win. is not being uti-
lized, hence window will not be
increased
Loss detected due to timeout for
ACK
Proportional rate reduction fast re-
covery
Recover tail losses
Retransmission Timeout
Recovery
Tail Loss Probe [22]
Table 3: QUIC states (Cubic CC) and their meanings.
We made two changes to achieve parity with Google’s QUIC
servers. First, we increased the maximum allowed congestion win-
dow size. At the time of our experiments, this value was 107 by
default in Chrome. We increased this value to 430, which matched
the maximum allowed congestion window in Chromium’s devel-
opment channel. Second, we found and fixed a bug in QUIC that
prevented the slow start threshold from being updated using the
receiver-advertised buffer size. Failure to do so caused poor perfor-
mance due to early exit from slow start.11
Prior work did no such calibration. This explains why they ob-
served poor QUIC performance in high bandwidth environments
or when downloading large web pages [16, 20, 30].
4.2 Instrumentation
While our tests can tell us how QUIC and TCP compare to each
other under different circumstances, it is not clear what exactly
causes these differences in performance. To shed light on this, we
compile QUIC clients and servers from source (using QUIC versions
25 and 34) and instrument them to gain access to the inner workings
of the protocol.
QUIC implements TCP-like congestion control. To reason about
QUIC’s behavior, we instrumented our QUIC server to collect logs
that allow us to infer QUIC’s state machine from execution traces,
and to track congestion window size and packet loss detection.
Table 3 lists QUIC’s congestion control states.
We use statistics about state transitions and the frequency of
visiting each state to understand the root causes behind good or
bad performance for QUIC. For example, we found that the reason
QUIC’s performance suffers in the face of packet re-ordering is
that re-ordered packets cause QUIC’s loss-detection mechanism to
report high numbers of false losses.
Note that we evaluate QUIC as a whole, in lieu of isolating the im-
pact of protocol components (e.g., congestion avoidance, TLP, etc.).
We found that disentangling and changing other (non-modular)
parts of QUIC (e.g., to change loss recovery techniques, add HOL
blocking, change how packets are ACKed) requires rewriting sub-
stantial amount of code, and it is not always clear how to replace
them. This is an interesting topic to explore in future work.
10We focus on PLT because it is the metric we use for end-to-end performance comparisons through-
out the paper.
11We confirmed our changes with a member of the QUIC team at Google. He also confirmed our
bug report.
Taking a Long Look at QUIC
IMC ’17, November 1–3, 2017, London, United Kingdom
(a) QUIC’s Cubic CC
(b) QUIC’s BBR CC
Figure 3: State transition diagram for QUIC’s CC.
(a) QUIC vs. TCP
(b) QUIC vs. two TCP flows
Figure 4: Timeline showing unfairness between QUIC and
TCP when transferring data over the same 5 Mbps bottle-
neck link (RTT=36ms, buffer=30 KB).
5 ANALYSIS
In this section, we conduct extensive measurements and analysis
to understand and explain QUIC performance. We begin by focus-
ing on the protocol-layer behavior, QUIC’s state machine, and its
fairness to TCP. We then evaluate QUIC’s application-layer per-
formance, using both page load times (PLT) and video streaming
as example application metrics. Finally, we examine the evolution
of QUIC’s performance and evaluate the performance that QUIC
“leaves on the table” by encrypting transport-layer headers that
prevent transparent proxying commonly used in cellular (and other
high-delay) networks.
5.1 State Machine and Fairness
In this section, we analyze high-level properties of the QUIC proto-
col using our framework.
QUIC has only a draft formal specification and
State machine.
no state machine diagram or formal model; however, the source
code is made publicly available. Absent such a model, we took an
empirical approach and used traces of QUIC execution to infer the
Flow Avg. throughput
Scenario
QUIC vs. TCP
QUIC vs. TCPx2
QUIC vs. TCPx4
QUIC
TCP
QUIC
TCP 1
TCP 2
QUIC
TCP 1
TCP 2
TCP 3
TCP 4
(std. dev.)
2.71 (0.46)
1.62 (1.27)
2.8 (1.16)
0.7 (0.21)
0.96 (0.3)
2.75 (1.2)
0.45 (0.14)
0.36 (0.09)
0.41 (0.11)
0.45 (0.13)
Table 4: Average throughput (5 Mbps link, buffer=30 KB, av-
eraged over 10 runs) allocated to QUIC and TCP flows when
competing with each other. Despite the fact that both pro-
tocols use Cubic congestion control, QUIC consumes nearly
twice the bottleneck bandwidth than TCP flows combined,
resulting in substantial unfairness.
state machine to better understand the dynamics of QUIC and their
impact on performance.
Specifically, we use Synoptic [15] for automatic generation of
QUIC state machine. While static analysis might generate a more
complete state machine, a complete model is not necessary for
understanding performance changes. Rather, as we show in Sec-
tion 5.2, we only need to investigate the states visited and transitions
between them at runtime.
Fig. 3a shows the QUIC state machine automatically generated
using traces from executing QUIC across all of our experiment
configurations. The diagram reveals behaviors that are common
to standard TCP implementations, such as connection start (Init,
SlowStart), congestion avoidance (CongestionAvoidance), and
receiver-limited connections (ApplicationLimited). QUIC also
includes states that are non-standard, such as a maximum sending
rate (CongestionAvoidanceMaxed), tail loss probes, and propor-
tional rate reduction during recovery.
Note that capturing the empirical state machine requires in-
strumenting QUIC’s source code with log messages that capture
transitions between states. In total, this required adding 23 lines of
 0 1 2 3 4 5 0 20 40 60 80 100Throughput (Mbps)Time (s)QUICTCP 0 1 2 3 4 5 0 20 40 60 80 100Throughput (Mbps)Time (s)QUICTCP1TCP2IMC ’17, November 1–3, 2017, London, United Kingdom
A. Molavi Kakhki et al.
(a) QUIC vs. TCP
(a) Varying object size
(b) Varying object count
Figure 6: QUIC (version 34) vs. TCP with different rate lim-
its for (a) different object sizes and (b) with different num-
bers of objects. Each heatmap shows the percent difference
between QUIC over TCP. Positive numbers—colored red—
mean QUIC outperforms TCP and has smaller page-load
time. Negative numbers—colored blue—means the opposite.
White cells indicate no statistically significant difference.
(b) 5-second zoom of above figure
Figure 5: Timeline showing congestion window sizes for
QUIC and TCP when transferring data over the same 5 Mbps
bottleneck link (RTT=36ms, buffer=30 KB).
code in 5 files. While the initial instrumentation required approxi-
mately 10 hours, applying the instrumentation to subsequent QUIC
versions required only about 30 minutes. To further demonstrate
how our approach applies to other congestion control implementa-
tions, we instrumented QUIC’s experimental BBR implementation
and present its state transition diagram in Fig. 3b. This instrumen-
tation took approximately 5 hours. Thus, our experience shows
that our approach is able to adapt to evolving protocol versions and
implementations with low additional effort.
We used inferred state machines for root cause analysis of per-
formance issues. In later sections, we demonstrate how they helped
us understand QUIC’s poor performance on mobile devices and in
the presence of deep packet reordering.
An essential property of transport-layer protocols is
Fairness.
that they do not consume more than their fair share of bottleneck
bandwidth resources. Absent this property, an unfair protocol may
cause performance degradation for competing flows. We evaluated
whether this is the case for the following scenarios, and present
aggregate results over 10 runs in Table 4. We expect that QUIC and
TCP should be relatively fair to each other because they both use
the Cubic congestion control protocol. However, we find this is not
the case at all.
• QUIC vs. QUIC. We find that two QUIC flows are fair to each
other. We also found similar behavior for two TCP flows.
• QUIC vs. TCP. QUIC multiplexes requests over a single con-
nection, so its designers attempted to set Cubic congestion
control parameters so that one QUIC connection emulates N
TCP connections (with a default of N = 2 in QUIC 34, and
N = 1 in QUIC 37). We found that N had little impact on fair-
ness. As Fig. 4a shows, QUIC is unfair to TCP as predicted, and
consumes approximately twice the bottleneck bandwidth of
TCP even with N = 1. We repeated these tests using different
buffer sizes, including those used by Carlucci et al. [17], but
did not observe any significant effect on fairness. This directly
Figure 7: QUIC with and without 0-RTT. Positive numbers—
colored red—show the performance gain achieved by 0-RTT.
The gain is more significant for small objects, but becomes
insignificant as the bandwidth decreases and/or objects be-
come larger, where connection establishment is a tiny frac-
tion of total PLT.
contradicts their finding that larger buffer sizes allow TCP and
QUIC to fairly share available bandwidth.
• QUIC vs. multiple TCP connections. When competing
with M TCP connections, one QUIC flow should consume
2/(M + 1) of the bottleneck bandwidth. However, as shown in
Table 4 and Fig. 4b, QUIC still consumes more than 50% of the
bottleneck bandwidth even with 2 and 4 competing TCP flows.
Thus, QUIC is not fair to TCP even assuming 2-connection
emulation.
To ensure fairness results were not an artifact of our testbed, we
repeated these tests against Google servers. The unfairness results
were similar.
We further investigate why QUIC is unfair to TCP by instrument-
ing the QUIC source code, and using tcpprobe [13] for TCP, to
extract the congestion window sizes. Fig. 5a shows the congestion
window over time for the two protocols. When competing with
TCP, QUIC is able to achieve a larger congestion window. Taking
a closer look at the congestion window changes (Fig. 5b), we find
that while both protocols use Cubic congestion control scheme,
QUIC increases its window more aggressively (both in terms of
slope, and in terms of more frequent window size increases). As a
result, QUIC is able to grab available bandwidth faster than TCP
does, leaving TCP unable to acquire its fair share of the bandwidth.
020406080 0 10 20 30 40 50 60 70 80 90 100Cong. Win. (KB)QUICTCP020406080 20 21 22 23 24 25Cong. Win. (KB)Time (s)Taking a Long Look at QUIC
IMC ’17, November 1–3, 2017, London, United Kingdom
(a) Varying object size,1% Loss
(b) Varying object size, 112 ms RTT