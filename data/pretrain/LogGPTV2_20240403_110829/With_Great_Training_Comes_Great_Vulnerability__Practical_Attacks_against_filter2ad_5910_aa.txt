title:With Great Training Comes Great Vulnerability: Practical Attacks against
Transfer Learning
author:Bolun Wang and
Yuanshun Yao and
Bimal Viswanath and
Haitao Zheng and
Ben Y. Zhao
With Great Training Comes Great Vulnerability: 
Practical Attacks against Transfer Learning
Bolun Wang, UC Santa Barbara; Yuanshun Yao, University of Chicago;  
Bimal Viswanath, Virginia Tech; Haitao Zheng and Ben Y. Zhao, University of Chicago
https://www.usenix.org/conference/usenixsecurity18/presentation/wang-bolun
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.With Great Training Comes Great Vulnerability:
Practical Attacks against Transfer Learning
Bolun Wang
Yuanshun Yao
Bimal Viswanath
Haitao Zheng
UC Santa Barbara
University of Chicago
Virginia Tech
University of Chicago
Ben Y. Zhao
University of Chicago
Abstract
Transfer learning is a powerful approach that allows
users to quickly build accurate deep-learning (Student)
models by “learning” from centralized (Teacher) mod-
els pretrained with large datasets, e.g. Google’s In-
ceptionV3. We hypothesize that the centralization of
model training increases their vulnerability to misclas-
siﬁcation attacks leveraging knowledge of publicly ac-
cessible Teacher models. In this paper, we describe our
efforts to understand and experimentally validate such at-
tacks in the context of image recognition. We identify
techniques that allow attackers to associate Student mod-
els with their Teacher counterparts, and launch highly
effective misclassiﬁcation attacks on black-box Student
models. We validate this on widely used Teacher mod-
els in the wild. Finally, we propose and evaluate multi-
ple approaches for defense, including a neuron-distance
technique that successfully defends against these attacks
while also obfuscates the link between Teacher and Stu-
dent models.
1 Introduction
Deep learning using neural networks has transformed
computing as we know it. From image and face recog-
nition, to self-driving cars, knowledge extraction and re-
trieval, and natural language processing and translation,
deep learning has produced game-changing applications
in every ﬁeld it has touched.
While advances in deep learning seem to arrive on a
daily basis, one constraint has remained: deep learning
can only build accurate models by training using large
datasets. This thirst for data severely constrains the num-
ber of different models that can be independently trained.
In addition, the process of training large, accurate mod-
els (often with millions of parameters) requires compu-
tational resources that can be prohibitive for individuals
or small companies. For example, Google’s InceptionV3
model is based on a sophisticated architecture with 48
layers, trained on ∼1.28M labeled images over a period
of 2 weeks on 8 GPUs.
The prevailing consensus is to address the data and
training resource problem using transfer learning, where
a small number of highly tuned and complex centralized
models are shared with the general community, and in-
dividual users or companies further customize the model
for a given application with additional training. By us-
ing the pretrained teacher model as a launching point,
users can generate accurate student models for their ap-
plication using only limited training on their smaller
domain-speciﬁc datasets. Today, transfer learning is rec-
ommended by most major deep learning frameworks, in-
cluding Google Cloud ML, Microsoft Cognitive Toolkit,
and PyTorch from Facebook.
Despite its appeal as a solution to the data scarcity
problem, the centralized nature of transfer learning cre-
ates a more attractive and vulnerable target for attackers.
Lack of diversity has ampliﬁed the power of targeted at-
tacks in other contexts, i.e. increasing the impact of tar-
geted attacks on network hubs [21], supernodes in over-
lay networks [54], and the impact of software vulnerabil-
ities in popular libraries [71, 22].
In this paper, we study the possible negative implica-
tions of deriving models from a small number of cen-
tralized teacher models. Our hypothesis is that bound-
ary conditions that can be discovered in the white box
teacher models can be used to perform targeted misclas-
siﬁcation attacks against its associated student models,
even if the student models themselves are closed, i.e.
black-box. Through detailed experimentation and test-
ing, we ﬁnd that this vulnerability does in fact exist in
a variety of the most popular image classiﬁcation con-
texts, including facial and iris recognition, and the iden-
tiﬁcation of trafﬁc signs and ﬂowers. Unlike prior work
on black-box adversarial attacks, this attack does not re-
quire repeated queries of the student model, and can in-
stead prepare the attack image based on knowledge of
the teacher model and any target image(s).
USENIX Association
27th USENIX Security Symposium    1281
Layer copied from Teacher
Layer newly added for classiﬁcation
Layer trained by Student
Teacher
    Student
Initialization
     Student
After Training
t
u
p
n
I
t
u
p
n
I
t
u
p
n
I
t
u
p
t
u
O
t
u
p
t
u
O
t
u
p
t
u
O
TK
SN-K
Figure 1: Transfer learning. A student model is initial-
ized by copying the ﬁrst N-1 layers from a teacher model,
with a new dense layer added for classiﬁcation. The
model is further trained by only updating the last N-K
layers.
This paper describes several key contributions:
• We identify and extensively evaluate the practicality
of misclassiﬁcation attacks against student models in
multiple transfer-learning applications.
• We identify techniques to reliably identify teacher
models given a student model, and show its effective-
ness using known student models in the wild.
• We perform tests to evaluate and conﬁrm the effective-
ness of these attacks on popular deep learning frame-
works, including Google Cloud ML, Microsoft Cog-
nitive Toolkit (CNTK), and the PyTorch open source
framework initially developed by Facebook.
• We explore and develop multiple defense techniques
against attacks on transfer learning models, including
defenses that alter the student model training process,
that alter inputs prior to classiﬁcation, and techniques
that introduce redundancy using multiple models.
Transfer learning is a powerful approach that ad-
dresses one of the fundamental challenges facing the
widespread deployment of deep learning. To the best of
our knowledge, our work is the ﬁrst to extensively study
the inheritance of vulnerabilities between transfer learn-
ing models. Our goal is to bring attention to fundamen-
tal weaknesses in these models, and to advocate for the
evaluation and adoption of defensive measures against
adversarial attacks in the future.
2 Background
We begin by providing some background information on
transfer learning and adversarial attacks on deep learning
frameworks.
2.1 Transfer Learning
The high level idea of transfer learning is to transfer
the “knowledge” from a pre-trained Teacher model to
a new Student model, where the student model’s task
shares signiﬁcant similarity to the teacher model’s. This
“knowledge” typically includes the model architecture
and weights associated with the layers. Transfer learning
enables organizations without access to massive datasets
or GPU clusters to quickly build accurate models cus-
tomized to their application context.
How Transfer Learning Works.
Figure 1 illustrates
transfer learning at a high level. The student model is ini-
tialized by copying the ﬁrst N − 1 layers of the Teacher.
A new dense layer is added for classiﬁcation.
Its size
matches the number of classes in the student task. Then
the student model is trained using its own dataset, while
the ﬁrst K layers are “frozen”, i.e. their weights are ﬁxed,
and only weights in the last N − K layers are updated.
The ﬁrst K layers (referred to as shallow layers) are
frozen during training because outputs of those layers al-
ready represent meaningful features for the student task.
The student model can reuse these features directly, and
freezing them lowers both training cost and amount of
required training data.
Based on the number of layers being frozen (K) during
the training process, transfer learning is categorized into
the following three approaches.
• Deep-layer Feature Extractor: N −1 layers are frozen
during training, and only the last classiﬁcation layer is
updated. This is preferred when the student task is
very similar to the teacher task, and requires minimal
training cost (the cost of training a single-layer DNN).
• Mid-layer Feature Extractor: The ﬁrst K layers are
frozen, where K < N − 1. Allowing more layers to be
updated helps the student perform more optimization
for its own task. Mid-layer Feature Extractor typically
outperforms Deep-layer Feature Extractor in scenar-
ios where the student task is more dissimilar to the
teacher task, and more training data is available.
• Full Model Fine-tuning: All layers are unfrozen and
ﬁne-tuned during student training (K = 0). This re-
quires more training data, and is appropriate when the
student task differs signiﬁcantly from the teacher task.
Bootstrapping using pre-trained model weights helps
the student converge faster and potentially achieve bet-
ter performance than training from scratch [23].
We run a simple experiment to demonstrate the impact
of transfer learning. We target facial recognition, where
the student task is to recognize a set of 65 faces, and uses
a well-performing face recognition model called VGG-
Face [11] as teacher model. Using only 10 images per
class to train the student model, we achieve 93.47% clas-
siﬁcation accuracy. Training the student with the same
architecture but with random weights (no pre-trained
weights) produces accuracy close to random guessing.
1282    27th USENIX Security Symposium
USENIX Association
2.2 Adversarial Attacks in Deep Learning
The goal of adversarial attacks against deep learning net-
works is to modify input images so that they are misclas-
siﬁed in the DNN. Given a source image, the attacker
applies a small perturbation so that it is misclassiﬁed by
the victim DNN into either a speciﬁc target class, or any
class other than the real class. Existing attacks fall into
two categories, based on their assumptions on how much
information attacker has about the classiﬁer.
White-box Attacks.
These attacks assume the at-
tacker knows the full internals of the classiﬁer DNN,
including its architecture and all weights. It allows the
attacker to run unlimited queries on the model, until a
success adversarial sample is found [17, 36, 47, 41, 55].
These attacks often achieve close to 100% success with
minimal perturbations, since full access to the DNN al-
lows them to ﬁnd the minimal amount of perturbations
required for misclassiﬁcation. The white-box scenario is
often considered impractical, however, since few systems
reveal internals of their model publicly.
Black-box Attacks.
Here attackers do not have
knowledge of the internals of the victim DNN, i.e. it
remains a black-box. The attacker is allowed to query
the victim model as an Oracle [46, 55]. Most black-
box attacks either use queries to test intermediate ad-
versarial samples and improve iteratively [55], or try to
reverse-engineer decision boundaries of the DNN and
build a replica, which can be used to craft adversarial
samples [46]. Black-box attacks often achieve lower suc-
cess than white-box attacks, and require a large number
of queries to the target classiﬁer [55].
Adversarial attacks can also be categorized into tar-
geted and non-targeted attacks. A targeted attack aims
to misclassify the adversarial image into a speciﬁc tar-
get class, whereas a non-targeted attack focuses on trig-
gering misclassiﬁcation into any class other than the real
class. We consider and evaluate both targeted and non-
targeted attacks in this paper.
3 Attacks on Transfer Learning
Here, we describe our attack on transfer learning, begin-
ning with the attack model.
Attack Model.
In the context of our deﬁnitions
in Section 2, our attack assumes white-box access to
teacher models (consistent with common practice today)
and black-box access to student models. We consider a
given attacker looking to trigger a misclassiﬁcation from
a Student model S, which has been customized through
transfer learning from a Teacher model T .
• White-box Teacher Model. We assume that T is a
white-box, meaning the attacker knows its model ar-
chitecture and weights. Most or all popular models
Figure 2: Illustration of our attack. Given images of a cat
and a dog, attacker computes perturbations that mimic
the internal representation of the dog image at layer K. If
the calculations are perfect, the adversarial sample will
be classiﬁed as dog, regardless of unknown layers in
SN−K.
today have been made publicly available to increase
adoption. Even if Teacher models became proprietary
in the future, an attacker targeting a single Teacher
model could obtain it by posing as a Student to gain
access to the Teacher model.
• Black-box Student Model. We assume S is black-box,
and all weights remain hidden from the attacker. We
also assume the attacker does not know the Student
training dataset, and can use only limited queries (e.g.,
1) to S. Apart from a single adversarial sample to trig-
ger misclassiﬁcation, we expect no additional queries
to be made during the pre-attack process.
• Transfer Learning Parameters. We assume the at-
tacker knows that S was trained using T as a Teacher,
and which layers were frozen during the Student train-
ing. This information is not hard to learn, as many ser-
vice providers, e.g., Google Cloud ML, release such
information in their ofﬁcial tutorials. We further relax
this assumption in Sections 4 and 5, and consider sce-
narios where such information is unknown. We will
discuss the impact on performance, and propose tech-
niques to extract such information from the Student
using a few additional queries.
Insight and Attack Methodology.
Figure 2 illustrates
the main idea behind our attack. Consider the scenario
where the attacker knows that the ﬁrst K layers of the
Student model are copied from the Teacher and frozen
during training. Attacker perturbs the source image so
it could be misclassiﬁed as the same class of a speciﬁc
target image. Using the Teacher model, attacker com-
putes perturbations that mimic the internal representa-
tion of the target image at layer K. Internal representa-
tion is captured by passing the target image as input to
the Teacher, and using the values of the corresponding
neuron outputs at layer K.
USENIX Association
27th USENIX Security Symposium    1283
Our key insight:
is that (in feedforward networks)
since each layer can only observe what is passed on from
the previous layer, if our adversarial sample’s internal
representation at layer K perfectly matches that of the
target image, it must be misclassiﬁed into the same la-
bel as the target image, regardless of the weights of any
layers that follow K.
This means that in the common case of feature ex-
tractor training, if we can mimic a target in the Teacher
model, then misclassiﬁcation will occur regardless of
how much the Student model trains with local data.
We also note that some models like InceptionV3 and
ResNet50, where “shortcut” layers can skip several lay-
ers, are not strictly feedforward. However, the same prin-
ciple applies, because a block (consisting of several lay-
ers) only takes information from the previous block. Fi-
nally, it is hard in practice to perfectly mimic the internal
representation, since we are limited in our level of pos-
sible perturbation, in order to keep adversarial changes
indistinguishable by humans. The attacker’s goal, there-
fore, is to minimize the dissimilarity between internal
representations, given a ﬁxed level of perturbation.
Targeted vs. Non-targeted Attacks.
We consider
both targeted and non-targeted attacks. The goal in tar-
geted attacks is to misclassify a source image xs into the
class of a target image xt . The attacker focuses on a spe-
ciﬁc layer K of the Teacher model, and tries to mimic
the target image’s internal representation (neuron values)
at layer K. Let TK(.) be the function (associated with
Teacher) transforming an input image to the internal rep-
resentation at layer K. A perturbation budget P is used
to control the amount of perturbation added to the source
image. The following optimization problem is solved to
craft an adversarial sample x′
s.
min D(TK(x′
s.t.
s), TK(xt ))
d(x′
(1)
s, xs) < P
The above optimization tries to minimize dissimilarity
D(.) between the two internal representations, under a
constraint to limit perturbation within a budget P. We
use L2 distance to compute D(.). d(x′, xs) is a distance
function measuring the amount of perturbation added to
xs. We discuss d(.) later in this section.
In non-targeted attacks, the goal is to misclassify xs
into any class different from the source class. To do this,
we need to identify a “direction” to push the source im-
age outside its decision boundary. In our case, it is hard
to estimate such a direction without having a target im-
age in hand, as we rely on mimicking hidden represen-
tations. Therefore, we perform a non-targeted attack by
evaluating multiple targeted attacks, and choose the one
that achieves the minimum dissimilarity between the in-
ternal representations. We assume that the attacker has
access to a set of target images I (each belonging to a
distinct class). Note that the source image can be mis-
classiﬁed to even classes outside the set I. The set of
images I merely serves as a guide for the optimization
process. Empirically, we ﬁnd that even small sizes of set
I (just 5 images) can achieve high attack success. The
optimization problem is formulated as follows.
min mini∈I{D(TK(x′
s.t. d(x′
s, xs) < P
s), TK(xti))}
(2)
Measuring Adversarial Perturbations.
As men-
tioned before, d(x′
s, xs) is the distance function used to
measure the amount of perturbation added to the image.
Most prior work used the Lp distance family, e.g., L0, L2,
and L∞ [17]. While a helpful way to quantify perturba-
tion, Lp distance fails to capture what humans perceive
as image distortion. Therefore, we use another metric,
called DSSIM, which is an objective image quality as-
sessment metric that closely matches with the perceived
quality of an image (i.e. subjective assessment) [65, 66].
The key idea is that humans are sensitive to structural
changes in an image, which strongly correlates with their
subjective evaluation of image quality. To infer structural
changes, DSSIM captures patterns in pixel intensities, es-
pecially among neighboring pixels. The metric also cap-
tures luminance, and contrast measures of an image, that
would also impact perceived image quality. DSSIM val-
ues fall in the range [0, 1], where 0 means the image is
identical to the original image, and a higher value means
the perceived distortion will be higher. We include the
mathematical formulation of DSSIM in the Appendix.
We also refer interested readers to the original papers for
more details [65, 66].
Solving the Optimization Function. To solve the op-
timization in Equation 1, we use the penalty method [43]
to reformulate the optimization as follows.
min D(TK(x′
s), TK(xt )) + λ·(max(d(x′