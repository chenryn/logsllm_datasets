debug. You can either run the application directly under the debugger from the command
line or attach the debugger to an already-running process based on its process ID. Table
10-1 shows the various commands you need for running the three debuggers.
Table 10-1: Commands for Running Debuggers on Windows, Linux, and macOS
Debugger
New process
Attach process
CDB
cdb application.exe [arguments]
cdb -p PID
GDB
gdb --args application [arguments]
gdb -p PID
LLDB
lldb -- application [arguments]
lldb -p -PID
Because the debugger will suspend execution of the process after you’ve created or
attached the debugger, you’ll need to run the process again. You can issue the commands
in Table 10-2 in the debugger’s shell to start the process execution or resume execution if
attaching. The table provides some simple names for such commands, separated by
commas where applicable.
Table 10-2: Simplified Application Execution Commands
Debugger
Start execution
Resume execution
CDB
g
g
GDB
run, r
continue, c
LLDB
process launch, run, r
thread continue, c
When a new process creates a child process, it might be the child process that crashes
rather than the process you’re debugging. This is especially common on Unix-like
platforms, because some network servers will fork the current process to handle the new
connection by creating a copy of the process. In these cases, you need to ensure you can
follow the child process, not the parent process. You can use the commands in Table 10-3
to debug the child processes.
Table 10-3: Debugging the Child Processes
Technet24
||||||||||||||||||||
||||||||||||||||||||
Debugger
Enable child process debugging
Disable child process debugging
CDB
.childdbg 1
.childdbg 0
GDB
set follow-fork-mode child
set follow-fork-mode parent
LLDB
process attach --name NAME --waitfor
exit debugger
There are some caveats to using these commands. On Windows with CDB, you can
debug all processes from one debugger. However, with GDB, setting the debugger to
follow the child will stop the debugging of the parent. You can work around this somewhat
on Linux by using the set detach-on-fork off command. This command suspends debugging
of the parent process while continuing to debug the child and then reattaches to the parent
once the child exits. However, if the child runs for a long time, the parent might never be
able to accept any new connections.
LLDB does not have an option to follow child processes. Instead, you need to start a
new instance of LLDB and use the attachment syntax shown in Table 10-3 to
automatically attach to new processes by the process name. You should replace the NAME in
the process LLDB command with the process name to follow.
Analyzing the Crash
After debugging, you can run the application while fuzzing and wait for the program to
crash. You should look for crashes that indicate corrupted memory—for example, crashes
that occur when trying to read or write to invalid addresses, or trying to execute code at an
invalid address. When you’ve identified an appropriate crash, inspect the state of the
application to work out the reason for the crash, such as a memory corruption or an array-
indexing error.
First, determine the type of crash that has occurred from the print out to the command
window. For example, CDB on Windows typically prints the crash type, which will be
something like Access violation, and the debugger will try to print the instruction at the
current program location where the application crashed. For GDB and LLDB on Unix-
like systems, you’ll instead see the signal type: the most common type is SIGSEGV for
segmentation fault, which indicates that the application tried to access an invalid memory
location.
As an example, Listing 10-2 shows what you’d see in CDB if the application tried to
execute an invalid memory address.
(2228.1b44): Access violation - code c0000005 (first chance)
First chance exceptions are reported before any exception handling.
This exception may be expected and handled.
00000000`41414141 ??              ???
Listing 10-2: An example crash in CDB showing invalid memory address
||||||||||||||||||||
||||||||||||||||||||
After you’ve determined the type of crash, the next step is to determine which
instruction caused the application to crash so you’ll know what in the process state you
need to look up. Notice in Listing 10-2 that the debugger tried to print the instruction at
which the crash occurred, but the memory location was invalid, so it returns a series of
question marks. When the crash occurs due to reading or writing invalid memory, you’ll
get a full instruction instead of the question marks. If the debugger shows that you’re
executing valid instructions, you can disassemble the instructions surrounding the crash
location using the commands in Table 10-4.
Table 10-4: Instruction Disassembly Commands
Debugger
Disassemble from crash location
Disassemble a specific location
CDB
u
u ADDR
GDB
disassemble
disassemble ADDR
LLDB
disassemble –frame
disassemble --start-address ADDR
To display the processor’s register state at the point of the crash, you can use the
commands in Table 10-5.
Table 10-5: Displaying and Setting the Processor Register State
Debugger
Show general purpose
registers
Show specific
register
Set specific
register
CDB
r
r @rcx
r @rcx = NEWVALUE
GDB
info registers
info registers rcx
set $rcx = NEWVALUE
LLDB
register read
register read rcx
register write rcx
NEWVALUE
You can also use these commands to set the value of a register, which allows you to
keep the application running by fixing the immediate crash and restarting execution. For
example, if the crash occurred because the value of RCX was pointing to invalid reference
memory, it’s possible to reset RCX to a valid memory location and continue execution.
However, this might not continue successfully for very long if the application is already
corrupted.
One important detail to note is how the registers are specified. In CDB, you use the
syntax @NAME to specify a register in an expression (for example, when building up a memory
address). For GDB and LLDB, you typically use $NAME instead. GDB and LLDB, also have
a couple of pseudo registers: $pc, which refers to the memory location of the instruction
currently executing (which would map to RIP for x64), and $sp, which refers to the current
stack pointer.
Technet24
||||||||||||||||||||
||||||||||||||||||||
When the application you’re debugging crashes, you’ll want to display how the current
function in the application was called, because this provides important context to
determine what part of the application triggered the crash. Using this context, you can
narrow down which parts of the protocol you need to focus on to reproduce the crash.
You can get this context by generating a stack trace, which displays the functions that
were called prior to the execution of the vulnerable function, including, in some cases,
local variables and arguments passed to those functions. Table 10-6 lists commands to
create a stack trace.
Table 10-6: Creating a Stack Trace
Debugger
Display stack trace
Display stack trace with arguments
CDB
K
Kb
GDB
backtrace
backtrace full
LLDB
backtrace
You can also inspect memory locations to determine what caused the current
instruction to crash; use the commands in Table 10-7.
Table 10-7: Displaying Memory Values
Debugger
Display bytes/words, dwords, qwords
Display ten 1-byte values
CDB
db, dw, dd, dq ADDR
db ADDR L10
GDB
x/b, x/h, x/w, x/g ADDR
x/10b ADDR
LLDB
memory read --size 1,2,4,8
memory read --size 1 --count 10
Each debugger allows you to control how to display the values in memory, such as the
size of the memory read (like 1 byte to 4 bytes) as well as the amount of data to print.
Another useful command determines what type of memory an address corresponds to,
such as heap memory, stack memory, or a mapped executable. Knowing the type of
memory helps narrow down the type of vulnerability. For example, if a memory value
corruption has occurred, you can distinguish whether you’re dealing with a stack memory
or heap memory corruption. You can use the commands in Table 10-8 to determine the
layout of the process memory and then look up what type of memory an address
corresponds to.
Table 10-8: Commands for Displaying the Process Memory Map
Debugger
Display process memory map
CDB
!address
||||||||||||||||||||
||||||||||||||||||||
GDB
info proc mappings
LLDB
No direct equivalent
Of course, there’s a lot more to the debugger that you might need to use in your triage,
but the commands provided in this section should cover the basics of triaging a crash.
Example Crashes
Now let’s look at some examples of crashes so you’ll know what they look like for different
types of vulnerabilities. I’ll just show Linux crashes in GDB, but the crash information
you’ll see on different platforms and debuggers should be fairly similar. Listing 10-3 shows
an example crash from a typical stack buffer overflow.
   GNU gdb 7.7.1
   (gdb) r
   Starting program: /home/user/triage/stack_overflow
   Program received signal SIGSEGV, Segmentation fault.
➊ 0x41414141 in ?? ()
➋ (gdb) x/i $pc
   => 0x41414141:  Cannot access memory at address 0x41414141
➌ (gdb) x/16xw $sp-16
   0xbffff620:     0x41414141      0x41414141      0x41414141      0x41414141
   0xbffff630:     0x41414141      0x41414141      0x41414141      0x41414141
   0xbffff640:     0x41414141      0x41414141      0x41414141      0x41414141
   0xbffff650:     0x41414141      0x41414141      0x41414141      0x41414141
Listing 10-3: An example crash from a stack buffer overflow
The input data was a series of repeating A characters, shown here as the hex value 0x41.
At ➊, the program has crashed trying to execute the memory address 0x41414141. The
fact that the address contains repeated copies of our input data is indicative of memory
corruption, because the memory values should reflect the current execution state (such as
pointers into the stack or heap)and are very unlikely to be the same value repeated. We
double-check that the reason it crashed is that there’s no executable code at 0x41414141 by
requesting GDB to disassemble instructions at the location of the program crash ➋. GDB
then indicates that it cannot access memory at that location. The crash doesn’t necessarily
mean a stack overflow has occured, so to confirm we dump the current stack location ➌.
By also moving the stack pointer back 16 bytes at this point, we can see that our input data
has definitely corrupted the stack.
The problem with this crash is that it’s difficult to determine which part is the
vulnerable code. We crashed it by calling an invalid location, meaning the function that
was executing the return instruction is no longer directly referenced and the stack is
corrupted, making it difficult to extract calling information. In this case, you could look at
the stack memory below the corruption to search for a return address left on the stack by
the vulnerable function, which can be used to track down the culprit. Listing 10-4 shows a
Technet24
||||||||||||||||||||
||||||||||||||||||||
crash resulting from heap buffer overflow, which is considerably more involved than the
stack memory corruption.
   user@debian:~/triage$ gdb ./heap_overflow
   GNU gdb 7.7.1
   (gdb) r
   Starting program: /home/user/triage/heap_overflow
   Program received signal SIGSEGV, Segmentation fault.
   0x0804862b in main ()
➊ (gdb) x/i $pc
   => 0x804862b :        mov    (%eax),%eax
➋ (gdb) info registers $eax
   eax            0x41414141       1094795585
   (gdb) x/5i $pc
   => 0x804862b :        mov    (%eax),%eax
      0x804862d :        sub    $0xc,%esp
      0x8048630 :        pushl  -0x10(%ebp)
    ➌ 0x8048633 :        call   *%eax
      0x8048635 :        add    $0x10,%esp
   (gdb) disassemble
   Dump of assembler code for function main:
      ...
    ➍ 0x08048626 :   mov    -0x10(%ebp),%eax
      0x08048629 :   mov    (%eax),%eax
   => 0x0804862b :   mov    (%eax),%eax
      0x0804862d :   sub    $0xc,%esp
      0x08048630 :   pushl  -0x10(%ebp)
      0x08048633 :   call   *%eax
   (gdb) x/w $ebp-0x10
   0xbffff708:     0x0804a030
➎ (gdb) x/4w 0x0804a030
   0x804a030:      0x41414141      0x41414141      0x41414141      0x41414141
   (gdb) info proc mappings
   process 4578
   Mapped address spaces:
       Start Addr    End Addr       Size  Offset  objfile
        0x8048000   0x8049000     0x1000     0x0  /home/user/triage/heap_overflow
        0x8049000   0x804a000     0x1000     0x0  /home/user/triage/heap_overflow
      ➏ 0x804a000   0x806b000    0x21000     0x0  [heap]
       0xb7cce000  0xb7cd0000     0x2000     0x0
       0xb7cd0000  0xb7e77000   0x1a7000     0x0  /lib/libc-2.19.so
Listing 10-4: An example crash from a heap buffer overflow
Again we get a crash, but it’s at a valid instruction that copies a value from the memory
location pointed to by EAX back into EAX ➊. It’s likely that the crash occurred because EAX
points to invalid memory. Printing the register ➋ shows that the value of EAX is just our
overflow character repeated, which is a sign of corruption.
We disassemble a little further and find that the value of EAX is being used as a memory
address of a function that the instruction at ➌ will call. Dereferencing a value from another
value indicates that the code being executed is a virtual function lookup from a Virtual
||||||||||||||||||||
||||||||||||||||||||
Function Table (VTable). We confirm this by disassembling a few instructions prior to the
crashing instruction ➍. We see that a value is being read from memory, then that value is
dereferenced (this would be reading the VTable pointer), and finally it is dereferenced
again causing the crash.
Although analysis showing that the crash occurs when dereferencing a VTable pointer
doesn’t immediately verify the corruption of a heap object, it’s a good indicator. To verify
a heap corruption, we extract the value from memory and check whether it’s corrupted
using the 0x41414141 pattern, which was our input value during testing ➎. Finally, to
check whether the memory is in the heap, we use the info proc mappings command to dump
the process memory map; from that, we can see that the value 0x0804a030, which we
extracted for ➍, is within the heap region ➏. Correlating the memory address with the
mappings indicates that the memory corruption is isolated to this heap region.
Finding that the corruption is isolated to the heap doesn’t necessarily point to the root
cause of the vulnerability, but we can at least find information on the stack to determine
what functions were called to get to this point. Knowing what functions were called would
narrow down the range of functions you would need to reverse engineer to determine the
culprit.
Improving Your Chances of Finding the Root Cause of a Crash
Tracking down the root cause of a crash can be difficult. If the stack memory is corrupted,
you lose the information on which function was being called at the time of the crash. For a
number of other types of vulnerabilities, such as heap buffer overflows or use-after-free,
it’s possible the crash will never occur at the location of the vulnerability. It’s also possible
that the corrupted memory is set to a value that doesn’t cause the application to crash at
all, leading to a change of application behavior that cannot easily be observed through a
debugger.
Ideally, you want to improve your chances of identifying the exact point in the
application that’s vulnerable without exerting a significant amount of effort. I’ll present a
few ways of improving your chances of narrowing down the vulnerable point.
Rebuilding Applications with Address Sanitizer
If you’re testing an application on a Unix-like OS, there’s a reasonable chance you have
the source code for the application. This alone provides you with many advantages, such as
full debug information, but it also means you can rebuild the application and add improved
memory error detection to improve your chances of discovering vulnerabilities.