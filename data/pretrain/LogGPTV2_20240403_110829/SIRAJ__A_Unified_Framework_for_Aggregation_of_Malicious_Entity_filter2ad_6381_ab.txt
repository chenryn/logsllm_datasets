Paper Outline. We introduce the relevant preliminaries in
Section II. We provide an overview of the components of
SIRAJ in Section III. The generative model is introduced in
Section IV while the pretext tasks for self-supervised learning
are introduced in Section V. We describe how to use SIRAJ to
develop supervised, semi-supervised and unsupervised classi-
ﬁers for diverse downstream tasks in Section VI. We describe
experimental results in Section VII, and conclude with some
parting thoughts in Section IX.
II. PRELIMINARIES
SIRAJ is designed to work with the intelligence of diverse
Internet threats. In this paper, we use the generic term entity to
refer to objects that could be either benign or malicious and be
used in attacks, e.g. binaries, domains, URLs and IPs. We use
the term scanner to represent a source of threat intelligence of
a certain type of entity, e.g. an antivirus engine, an IP blacklist
or a domain reputation system. When queried about an entity
e, a scanner would report its own assessment of e, which,
without loss of generality, could be malicious, benign or no
information. The assessment from all scanners together form
the scan report of the entity e. We use the term label to denote
the maliciousness or benignness of an entity. Formally, let E =
{e1, e2, . . . , em} be the set of m entities. Suppose there are n
scanners S = {s1, s2, . . . , sn}. We represent the input dataset
for intelligence aggregation by a matrix X = (Xij) 1 ≤ j ≤
m and 1 ≤ j ≤ n with
if sj detected ei as benign
if sj detected ei as malicious
if sj did not scan ei
1,
−1,
0,
Xij =
The scan report Re for an entity e is a vector of dimension
n containing the assessment from each scanner s ∈ S for e.
For a collection of entities e ∈ E(cid:48) where E(cid:48)
⊆ E we also
collect a time series of scan reports over a period of time
T = {T, T + δ, T + 2δ, . . .} where T represents the starting
time and δ represents the periodicity of data collection such
as 6 hours, 1 day, etc. Rt
e is the scan report for entity e at
time t. We omit t when the context is clear. We use the time
series to understand the temporal dynamics of scan reports.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
2508
about
the scanner dynamics for accurate aggregation. We
achieve this by employing a self-supervised learning approach
with careful design of three inter-related pretext tasks that
imbue the embedding model with knowledge about depen-
dencies between scanners and their temporal dynamics. The
ﬁrst task seeks to learn the dependencies between the scanners
when generating the scan report for an entity. While the ﬁrst
task superﬁcially looks similar to the generative model, they
both learn the scanner dependencies at different granularities.
For example, GM might learn that scanners Si and Sj have
high correlation in their reports. On the other hand, task 1
model could learn that the output of Sj for a particular entity
could be predicted from the output of Si, Sk and Sl. Later,
we conduct experiments to showcase the complementarity
of the generative model and the ﬁrst task. The second task
tackles the disconnect between early and late scan reports of
a new entity (e.g., phishing or malware URL) by modeling
the temporal dynamics of scanner responses. The goal of the
third task is to learn embeddings that are temporally consistent.
SIRAJ uses a multi-task approach to learn embeddings that
are simultaneously well suited for all pretext tasks.
3. Fine-Tuning. Once the encoder is trained from pretext tasks,
it can take any scan report and generate an embedding that
could be used for various downstream tasks such as detection
and clustering. We consider two ﬁne-tuning scenarios. First,
when there is a limited number of labeled data available (as
little as 100 scan reports), we use a semi-supervised approach
to train an effective discriminative model that achieves similar
performance as a supervised model
trained over a much
larger labeled corpus. Second, when there are no labeled data
available, we leverage the properties of the embedding space
to design an unsupervised classiﬁer that outperforms a wide
variety of other unsupervised approaches such as heuristic
thresholds, generative models, and weak supervision.
IV. GENERATIVE MODELS FOR AGGREGATION
In this section, we describe our approach to learn the
dependency structure of a generative model and use it to model
the latent variables of the scanners.
Generative Modeling for Aggregation. A principled ap-
proach for aggregating unlabeled data is generative models.
The key insight is to model the process by which the unlabeled
data (i.e., scan reports) was generated. We treat the true label
of a scan report (malicious or benign) as a latent variable that
generates the observed and possibly noisy scan report. This
contrasts with traditional approaches that try to model the
true label given the noisy scan report. Once the parameters
of an appropriate generative model have been ﬁtted using the
unlabeled data, we could use it to estimate the latent true label
of a scan report. This approach is counter intuitive - yet yields
accurate results if the generative model is appropriate.
Learned vs Speciﬁed Generative Models. Prior works that
applied generative models for VirusTotal aggregation [10],
[23] assume that generative models are pre-speciﬁed by the
domain expert and only focus on estimating the latent vari-
ables. However, the structure of the generative model has
Fig. 1: Overview of Our Approach
III. OVERVIEW OF OUR APPROACH
SIRAJ synthesizes diverse ideas from machine learning
and we provide a high level overview of its key components
in this section. Figure 1 illustrates the general overview of
our approach. Our solution can be decomposed into three key
steps: (1) Generative models, (2) Pre-training, and (3) Fine-
Tuning. We provide a rationale for each of the components
and how all these components ﬁt together in the following.
Then, we provide additional details in Section IV, Section V,
and Section VI.
Solution Framework. Our goal is to learn an appropriate
intermediate embedding representation for scan reports, which
could be used for downstream tasks such as detecting ma-
licious entities without any labeled data or with minimal
labeling. To the best of our knowledge, we are not aware of
any prior work that seeks to learn such an embedding. We
emphasize that our approach is generic and customizable. The
same approach could be used for diverse malicious entities
such as malware, URLs, and IPs through VirusTotal scan
reports or IP blacklists.
1. Generative Models. Prior work such as [10] has shown that
understanding the dependencies between the scanners (using
a generative model) is useful for aggregating the scanner
outputs. However, directly using generative models often fails
due to the mismatch between modeling assumptions and the
real-world. Hence, instead of using the generative model for
prediction, we use it to learn the dependencies and dynamics
exhibited by the scanners and their labels. The learned model
is used to inject domain knowledge into the self-supervised
learning tasks.
2. Pre-Training using Pretext Tasks. While the generative
model is a promising start, it is insufﬁcient on its own for
accurate aggregation of scan reports. The generative model is
an unsupervised approach that can learn the dependencies at
the corpus level. It is necessary to learn more granular details
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
3509
a signiﬁcant impact on its accuracy. The biggest source of
inaccuracy is ignoring the statistical dependencies between the
scanners. For example, scanner s1 and s2 could have highly
correlated scan reports for different entities as they use the
same algorithm in the back end [8]. Not taking that into
account could seriously affect the accuracy of aggregation.
It is unreasonable to expect even a domain expert to specify
the generative models with accurate dependencies.
In this work, we advocate for a novel two-step approach.
First, we leverage recent innovations in weak supervision [24]
to efﬁciently learn the dependencies. Second, we specify the
generative model using the learned dependencies and use
it to estimate the latent variables corresponding to scanner
accuracies. Our approach differs from prior work in two
aspects: (a) our generative model is constructed using a data-
driven approach; (b) we do not directly use the estimated latent
variables for aggregation – instead, we use them to infuse the
pretext tasks with appropriate domain knowledge.
Learning Dependency Structures. Our generative model
for learning the dependencies between scanners is based on
a factor graph [25]. Such graphs consist of two types of
nodes – variable nodes corresponding to scanners and their
outputs and factor nodes that deﬁne the relationships between
variable nodes. For example, the algorithm could designate
two scanners si and sj as related by creating a new factor node
fk connecting to both si and sj. This simple graph structure is
expressive enough to model arbitrary relationships, including
high order dependencies. Similar to [26], we consider pairwise
and conjunctive dependencies. The key challenge is that the
number of dependencies explodes with the increasing number
of scanners. For example, even if we limit ourselves to
pairwise dependencies between any pair of scanners, there are
more than 2500 such dependencies given VirusTotal’s over
70 scanners. One needs a huge amount of data to identify
which of these potential dependencies are irrelevant reliably.
We rely on prior works for efﬁciently learning the generative
model. First, we learn the dependency structure by leverag-
ing the techniques from [26] that proposed an optimization
formulation for estimating the log marginal pseudo-likelihood
of the output of a single scanner conditioned on the outputs
of all other scanners. Second, once the dependency structure
is known, we use [27] that breaks the generative model into
smaller sub-problems and learns the parameter values through
closed-form solutions. This approach allows one to learn the
parameters in time linear in the size of the data, which is
appealing in a domain where the unlabeled data is abundant.
V. SELF SUPERVISED LEARNING BASED APPROACH
Self supervised learning (SSL) is a recently emerged tech-
nique that builds a supervised learning task (i.e., a pretext
task) from the dataset itself, not from the manually annotated
data [28]. SSL automatically generates pseudo labels based on
the attributes in the data such that pseudo labels are correct
for a speciﬁc task. Then model is trained in a supervised
manner using the generated pseudo labels. Essentially, SSL
works by designing pretext tasks to be solved during a pre-
training step. The model trained to solve those pretext tasks
learns representations of data that will be used for downstream
tasks.
SSL has been extensively studied for computer vision and
natural language processing. These domains are well suited for
SSL due to the spatial and semantic structure present in image
or language data. However, such properties do not exist in
tabular domain data such as scan reports, making the problem
of data augmentation and SSL challenging [28]. To the best
of our knowledge, we are not aware of any work on SSL
for aggregating multi-source intelligence such as VirusTotal.
We introduce a novel SSL approach to learn domain and time
invariant representations that are useful for diverse downstream
tasks.
Desiderata for Pretext Task Design. A key challenge in
SSL is the design of an appropriate pretext task such that
(a) the knowledge/embeddings learned is relevant for the
downstream task; and (b) the pseudo-labels for pretext tasks
can be generated automatically and efﬁciently. We desire that
the embeddings are generic enough to be used for training
ML models for a broad spectrum of downstream tasks such as
(early) detection of malicious entities and detection of attack
types. This could only be achieved by learning some intrinsic
properties of the scan reports.
Transferable Properties of Scan Reports. There has been
extensive work on analyzing the properties of VirusTotal scan
reports along different dimensions such as relative accuracy,
stability, and convergence [29], [8], [9]. Our analysis of
these works identiﬁed some generic properties across various
domains and multi-source intelligence aggregators.
• Not all scanners are equal. They vary in their accuracy,
expertise, and consistency of their responses.
• Scanners have sophisticated dependencies between them.
Some scanners have a higher degree of overlap in their
responses than others due to various reasons such as
shared expertise. Some scanners could have specialized
expertise and respond only to a speciﬁc class of entities
such as Phishing URLs.
• Complex temporal dependencies exist between scanners.
Even for a ﬁxed entity e, individual scanners could have
disparate behavior over a period of time. Some scanners
could produce a detection result (malicious or benign) and
stick to it. Alternatively, some scanners could ﬂip their
detection results intermittently. Scanners for malware
often exhibit hazardous ﬂips [8] where the label ﬂips to
a different value twice in a short period of time (such
as malicious to benign and back to malicious). Similarly,
some scanners are conservative and lag behind others,
while other scanners could be bellwethers.
Our goal is to design pretext tasks that are cognizant of
these behaviors. It is challenging to design a single pretext
task that can address all the relevant desiderata. Instead, we
propose three intuitive tasks that jointly learn the necessary
dependencies using a multi-task learning framework. Figure 2
illustrates our approach where each row represents each task.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
4510
Fig. 2: Illustration of Self Supervised Approach.
Given a scan report Re, we pass it to pretext generators for
three pretext tasks, respectively. Pretext Generator 1 corrupts
the scan report based on the masks, while Pretext Generator 2
and Pretext Generator 3 function as identity functions by pass-
ing the input without modiﬁcation. The corrupted and original
scan reports are passed to an encoder to generate embeddings
e that are then fed to the task speciﬁc predictors.
z1
e , z2
We then apply the pretext task-speciﬁc loss function on the
predicted output and apply back propagation to improve upon
both the encoders and the predictors. In the following, we
describe each of three tasks in detail.
e , z3
e to an encoder that outputs an embedding z1
pass Rc
e. Next,
we train a predictor (Mask Output Predictor) that takes z1
e
as input and outputs Re. We perform joint learning where
the back-propagation improves both components. We can see
that to do well on this pretext task, the SSL has to design an
appropriate encoder and also learn the various dependencies
between the scanners. For example, assume that scanners
si, sj, sk exhibit high correlation between their responses and
Re[i] is corrupted. Then the predictor could learn to ﬁx it
through the dependencies with Re[j] and Re[k].
B. Task 2: Temporal Scanner Dependencies
e
A. Task 1: Scanner Dependencies
Given an unlabeled dataset of scan reports for various
entities, the goal of the ﬁrst pretext task is to learn the high-
level dependencies between scanners. A simple but intractable
approach would be to collect some summary statistics such
as correlations between scanners. For example, even if we
limit ourselves to pairwise correlations, there are more than
2500 such values for the 70-odd scanners from VirusTotal’s
URL scanning service. Including higher-order correlations