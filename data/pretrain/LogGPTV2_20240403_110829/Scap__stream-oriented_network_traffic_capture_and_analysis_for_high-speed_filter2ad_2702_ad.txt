FDIR ﬁlters are not used, because Scap does not copy the incom-
ing packets around: as soon as a packet arrives, the kernel module
accesses only the needed information from its headers, updates the
respective stream_t, and just drops it. In contrast, Libnids and YAF
receive all packets to user space, resulting in much higher overhead.
YAF performs better than Libnids because it receives only the ﬁrst
96 bytes of each packet and it does not perform stream reassembly.
When Scap uses FDIR ﬁlters to discard the majority of the pack-
ets at NIC layer it achieves even better performance. Figure 3(c)
shows that the software interrupt load is signiﬁcantly lower with
FDIR ﬁlters: as little as 2% for 6 Gbit/s. Indeed, Scap with FDIR
brings into main memory as little as 3% of the total packets—just
the packets involved in TCP session creation and termination.
6.3 Delivering Streams to User Level: The Cost
of an Extra Memory Copy
In this experiment, we explore the performance of Scap, Snort,
and Libnids when delivering reassembled streams without any fur-
ther processing. The Scap application receives all data from all
streams with no cutoff, and runs as a single thread. Snort is con-
ﬁgured with only the Stream5 preprocessor enabled, without any
rules. Figure 4(a) shows the percentage of dropped packets as a
function of the trafﬁc rate. Scap delivers all steams without any
packet loss for rates up to 5.5 Gbit/s. On the other hand, Libnids
starts dropping packets at 2.5 Gbit/s (drop rate: 1.4%) and Snort
at 2.75 Gbit/s (drop rate: 0.7%). Thus, Scap is able to deliver re-
assembled streams to the monitoring applications for more than two
times higher trafﬁc rates. When the input trafﬁc reaches 6 Gbit/s,
Libnids drops 81.2% and Snort 79.5% of the total packets received.
The reason for this performance difference lies in the extra mem-
ory copy operations needed for stream reassembly at user level.
When a packet arrives for Libnids and Snort, the kernel writes it
in the next available location in a common ring buffer. When per-
forming stream reassembly, Libnids and Snort may have to copy
each packet’s payload from the ring buffer to a memory buffer al-
located speciﬁcally for this packet’s stream. Scap avoids this extra
copy operation because the kernel module copies the packet’s data
not to a common buffer, but directly to a memory buffer allocated
speciﬁcally for this packet’s stream. Figure 4(b) shows that the
CPU utilization of the Scap user-level application is considerably
lower than the utilization of Libnids and Snort, which at 3 Gbit/s
exceeds 90%, saturating the processor. In contrast, the CPU utiliza-
tion for the Scap application is less then 60% even for speeds up to
6 Gbit/s, as the user application does very little work: all the stream
reassembly is performed in the kernel module, which increases the
software interrupt load, as can be seen in Figure 4(c).
6.4 Concurrent Streams
An attacker could try to saturate the ﬂow table of a stream re-
assembly library by creating a large number of established TCP
ﬂows, so that a subsequent malicious ﬂow cannot be stored. In this
experiment, we evaluate the ability of Scap, Libnids, and Snort to
handle such cases while increasing the number of concurrent TCP
streams up to 10 million.
Each stream consists of 100 packets
with the maximum TCP payload, and streams are multiplexed so
that the desirable number of concurrent streams is achieved. For
each case, we create a respective packet trace and then replay it
at a constant rate of 1 Gbit/s, as we want to evaluate the effect of
concurrent streams without increasing the trafﬁc rate. As in the pre-
vious experiment, the application uses a single thread and receives
all streams at user level, without performing any further processing.
Figure 5 shows that Scap scales well with the number of concur-
rent streams: as we see in Figure 5(a), no stream is lost even for
10 million concurrent TCP streams. Also, Figures 5(b) and 5(c)
show that the CPU utilization and software interrupt load of Scap
100
)
%
(
d
e
p
p
o
r
d
s
t
e
k
c
a
P
80
60
40
20
0
0
Libnids
Snort
Scap
Scap with packets
1
2
3
4
5
6
Traffic rate (Gbit/s)
(a) Packet loss
100
)
%
(
d
e
h
c
t
a
M
s
n
r
e
t
t
a
P
80
60
40
20
0
0
Libnids
Snort
Scap
Scap with packets
1
2
3
4
5
6
Traffic rate (Gbit/s)
(b) Patterns Successfully Matched
100
)
%
(
t
s
o
l
s
m
a
e
r
t
S
80
60
40
20
0
0
Libnids
Snort
Scap
Scap with packets
1
2
3
4
5
6
Traffic rate (Gbit/s)
(c) Lost streams
Figure 6: Performance comparison of pattern matching for Snort, Libnids, and Scap, for varying trafﬁc rates.
slightly increase with the number of concurrent streams, as the traf-
ﬁc rate remains constant. On the other hand, Snort and Libnids can-
not handle more than one million concurrent streams, even though
they can handle 1 Gbit/s trafﬁc with less than 60% CPU utiliza-
tion. This is due to internal limits that these libraries have for the
number of ﬂows they can store in their data structures. In contrast,
Scap does not have to set such limits because it uses a dynamic
memory management approach: when more memory is needed for
storing stream_t records, Scap allocates dynamically the nec-
essary memory pools to capture all streams.
In case an attacker
tries to overwhelm the Scap ﬂow table, Scap will use all the avail-
able memory for stream_t records. When there is no more free
memory, Scap’s policy is to always store newer streams by remov-
ing from the ﬂow table the older ones, i.e., streams with the highest
inactivity time based on the access list.
6.5 Pattern Matching
In the following experiments, we measure the performance of
Scap with an application that receives all streams and searches for a
set of patterns. We do not apply any cutoff so that all trafﬁc is deliv-
ered to the application, and a single worker thread is used. Pattern
matching is performed using the Aho-Corasik algorithm [5]. We
extracted 2,120 strings based on the content ﬁeld of the “web
attack” rules from the ofﬁcial VRT Snort rule set [4], and use them
as our patterns. These strings resulted in 223,514 matches in our
trace. We compare Scap with Snort and Libnids using the same
string matching algorithm and set of patterns in all three cases. To
ensure a fair comparison, Snort is conﬁgured only with the Stream5
preprocessor enabled, using a separate Snort rule for each of the
2,120 patterns, applied to all trafﬁc, so that all tools end up us-
ing the same automaton. The Scap and Libnids programs load the
2,120 patterns from a ﬁle, build the respective DFA, and start re-
ceiving streams. We use the same chunk size of 16KB for all tools.
Figure 6(a) shows the percentage of dropped packets for each ap-
plication as a function of the trafﬁc rate. We see that Snort and Lib-
nids process trafﬁc rates of up to 750 Mbit/s without dropping any
packets, while Scap processes up to 1 Gbit/s trafﬁc with no packet
loss with one worker thread. The main reasons for the improved
performance of Scap are the improved cache locality when group-
ing multiple packets into their respective transport-layer streams,
and the reduced memory copies during stream reassembly.
Moreover, Scap drops signiﬁcantly fewer packets than Snort and
Libnids, e.g., at 6 Gbit/s it processes three times more trafﬁc. This
behavior has a positive effect on the number of matches. As shown
in Figure 6(b), under the high load of 6 Gbit/s, Snort and Libnids
match less than 10% of the patterns, while Scap matches ﬁve times
as many: 50.34%. Although the percentage of missed matches
for Snort and Libnids is proportional to the percentage of dropped
packets, the accuracy of the Scap application is affected less from
i
s
e
s
s
m
2
L
Libnids
Snort
Scap
 30
 25
 20
 15
 10
 5
 0
0
.
2
5
0
.
5
1 2 3 4 5 6
Traffic Rate (Gbps)
Figure 7: L2 cache misses of pattern matching using Snort, Lib-
nids, and Scap, for varying trafﬁc rates.
high packet loss rates. This is because Scap under overload tends
to retain more packets towards the beginning of each stream. As
we use patterns from web attack signatures, they are usually found
within the ﬁrst few bytes of HTTP requests or responses. Also,
Scap tries to deliver contiguous chunks, which improves the detec-
tion abilities compared to delivery of chunks with random holes.
6.5.1 Favoring Recent and Short Streams
We turn our attention now to see how dropped packets affect the
different stream reassembly approaches followed by Scap, Libnids,
and Snort. While Libnids and Snort drop packets randomly under
overload, Scap is able to (i) assign more memory to new or small
streams, (ii) cut the long tails of large streams, and (iii) deliver more
streams intact when the available memory is limited. Moreover, the
Scap kernel module always receives and processes all important
protocol packets during the TCP handshake. In contrast, when a
packet capture library drops these packets due to overload, the user-
level stream reassembly library will not be able to reassemble the
respective streams. Indeed, Figure 6(c) shows that the percentage
of lost streams in Snort and Libnids is proportional to the packet
loss rate. In contrast, Scap loses signiﬁcantly less streams than the
corresponding packet loss ratio. Even for 81.2% packet loss at 6
Gbit/s, only 14% of the total streams are lost.
6.5.2 Locality
Let’s now turn our attention to see how different choices made
by different tools impact locality of reference and, in the end, de-
termine application performance. For the previous experiment, we
also measure the number of L2 cache misses as a function of the
trafﬁc rate (Figure 7), using the processor’s performance coun-
ters [3]. We see that when the input trafﬁc is about 0.25 Gbit/s,
100
)
%
(
d
e
p
p
o
r
d
s
t
e
k
c
a
P
80
60
40
20
0
Libnids
Snort
Scap w/o FDIR
Scap with FDIR
0
0.01 0.1
1
10
100 1000 10   
    4 10   
    5
Stream size cutoff (KB)
(a) Packet loss
100
)
%
(
n
o
i
t
a
z
i
l
i
t
u
U
P
C
80
60
40
20
0
Libnids
Snort
Scap w/o FDIR
Scap with FDIR
0
0.01 0.1
1
10
100 1000 10   
    4 10   
    5
Stream size cutoff (KB)
(b) CPU utilization
)
%
(
s
t
p
u
r
r
e
t
n
i
e
r
a
w
t
f
o
S
20
15
10
5
0
Libnids
Snort
Scap w/o FDIR
Scap with FDIR
0
0.01 0.1
1
10
100 1000 10   
    4 10   
    5
Stream size cutoff (KB)
(c) Software interrupt load
Figure 8: Performance comparison of Snort, Libnids, and Scap, for varying stream size cutoff values at 4 Gbit/s rate.
Snort experiences about 25 misses per packet, Libnids about 21,
while Scap experiences half of them: just 10.2 misses per packet.
We have to underline that at this low trafﬁc rate none of the three
tools misses any packets, and we know that none of the tools is
stressed, so they all operate in their comfort zone. The reason that
Lidnids and Snort have twice as many cache misses as Scap can
be traced to the better locality of reference of the Scap approach.
By reassembling packets into streams from the moment they arrive,
packets are not copied around: consecutive segments are stored to-
gether, and are consumed together. On the contrary, Libnids and
Snort perform packet reassembly too late: the segments have been
stored in (practically) random locations all over the main memory.
6.5.3 Packet Delivery