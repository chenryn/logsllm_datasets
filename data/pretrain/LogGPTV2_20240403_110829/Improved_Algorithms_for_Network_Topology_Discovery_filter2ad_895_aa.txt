title:Improved Algorithms for Network Topology Discovery
author:Benoit Donnet and
Timur Friedman and
Mark Crovella
Improved Algorithms for
Network Topology Discovery
Benoit Donnet1, Timur Friedman1, and Mark Crovella2,(cid:1)
1 Universit´e Pierre & Marie Curie, Laboratoire LiP6-CNRS
2 Boston University Department of Computer Science
Abstract. Topology discovery systems are starting to be introduced in
the form of easily and widely deployed software. However, little consid-
eration has been given as to how to perform large-scale topology dis-
covery eﬃciently and in a network-friendly manner. In prior work, we
have described how large numbers of traceroute monitors can coordinate
their eﬀorts to map the network while reducing their impact on routers
and end-systems. The key is for them to share information regarding the
paths they have explored. However, such sharing introduces considerable
communication overhead. Here, we show how to improve the communi-
cation scaling properties through the use of Bloom ﬁlters to encode a
probing stop set. Also, any system in which every monitor traces routes
towards every destination has inherent scaling problems. We propose cap-
ping the number of monitors per destination, and dividing the monitors
into clusters, each cluster focusing on a diﬀerent destination list.
1
Introduction
We are starting to see the wide scale deployment of tools based on traceroute [1]
that discover the Internet topology at the IP interface level. Today’s most exten-
sive tracing system, skitter [2], uses 24 monitors, each targeting on the order of
one million destinations. Other well known systems, such as RIPE NCC TTM [3]
and NLANR AMP [4], conduct a full mesh of traceroutes between on the order
of one- to two-hundred monitors. An attempt to scale either of these approaches
to thousands of monitors would encounter problems from the signiﬁcantly higher
traﬃc levels it would generate and from the explosion in the data it would collect.
However, larger scale systems are now coming on line.
If a traceroute monitor were incorporated into screen saver software, following
an idea ﬁrst suggested by J¨org Nonnenmacher (see Cheswick et al. [5]), it could
(cid:1) The authors are participants in the traceroute@home project. Mr. Donnet and Mr.
Friedman are members of the Networks and Performance Analysis research group
at LiP6. This work was supported by: the RNRT’s Metropolis project, NSF grants
ANI-9986397 and CCR-0325701, the e-NEXT European Network of Excellence, and
LiP6 2004 project funds. Mr. Donnet’s work is supported by a SATIN European
Doctoral Research Foundation grant. Mr. Crovella’s work at LiP6 is supported by
the CNRS and Sprint Labs.
C. Dovrolis (Ed.): PAM 2005, LNCS 3431, pp. 149–162, 2005.
c(cid:1) Springer-Verlag Berlin Heidelberg 2005
150
B. Donnet, T. Friedman, and M. Crovella
lead instantaneously to a topology discovery infrastructure of considerable size,
as demonstrated by the success of other software distributed in this manner,
most notably SETI@home [6]. Some network measurement tools have already
been released to the general public as screen savers or daemons. Grenouille [7]
was perhaps the ﬁrst, and appears to be the most widely used. More recently
we have seen the introduction of NETI@home [8], and, in September 2004, the
ﬁrst freely available tracerouting monitor, DIMES [9].
In our prior work [10], described in Sec. 2 of this paper, we found that stan-
dard traceroute-based topology discovery methods are quite ineﬃcient, repeat-
edly probing the same interfaces. This is a concern because, when scaled up, such
methods will generate so much traﬃc that they will begin to resemble distributed
denial of service (DDoS) attacks. To avoid this eventuality, responsibly designed
large scale systems need to maintain probing rates far below that which they
could potentially obtain. Thus, skitter maintains a low impact by maintaining a
relatively small number of monitors, and DIMES does so by maintaining a low
probing rate. The internet measurement community has an interest in seeing
systems like these scale more eﬃciently. It would also be wise, before the more
widespread introduction of similar systems, to better deﬁne what constitutes
responsible probing.
Our prior work described a way to make such systems more eﬃcient and less
liable to appear like DDoS attacks. We introduced an algorithm called Double-
tree that can guide a skitter-like system, allowing it to reduce its impact on
routers and ﬁnal destinations while still achieving a coverage of nodes and links
that is comparable to classic skitter. The key to Doubletree is that monitors
share information regarding the paths that they have explored. If one monitor
has already probed a given path to a destination then another monitor should
avoid that path. We have found that probing in this manner can signiﬁcantly
reduce load on routers and destinations while maintaining high node and link
coverage.
This paper makes two contributions that build on Doubletree, to improve the
eﬃciency and reduce the impact of probing. First, a potential obstacle to Dou-
bletree’s implementation is the considerable communication overhead entailed in
sharing path information. Sec. 3 shows how the overhead can be reduced through
the use of Bloom ﬁlters [11]. Second, any system in which every monitor traces
routes towards every destination has inherent scaling problems. Sec. 4 examines
those problems, and shows how capping the number of monitors per destination
and dividing the monitors into clusters, each cluster focusing on a diﬀerent des-
tination list, enables a skitter-like system to avoid appearing to destinations like
a DDoS attack. We discuss related and future work in Sec. 5.
2 Prior Work
Our prior work [10] described the ineﬃciency of the classic topology prob-
ing technique of tracing routes hop by hop outwards from a set of monitors
Improved Algorithms for Network Topology Discovery
151
towards a set of destinations. It also introduced Doubletree, an improved probing
algorithm.
Data for our prior work, and also for this paper, were produced by 24 skit-
ter [2] monitors on August 1st through 3rd, 2004. Of the 971,080 destinations
towards which all of these monitors traced routes on those days, we randomly
selected a manageable 50,000 for each of our experiments.
Only 10.4% of the probes from a typical monitor serve to discover an interface
that the monitor has not previously seen. An additional 2.0% of the probes
return invalid addresses or do not result in a response. The remaining 87.6% of
probes are redundant, visiting interfaces that the monitor has already discovered.
Such redundancy for a single monitor, termed intra-monitor redundancy, is much
higher close to the monitor, as can be expected given the tree-like structure of
routes emanating from a single source. In addition, most interfaces, especially
those close to destinations, are visited by all monitors. This redundancy from
multiple monitors is termed inter-monitor redundancy.
While this ineﬃciency is of little consequence to skitter itself, it poses an
obstacle to scaling far beyond skitter’s current 24 monitors. In particular, inter-
monitor redundancy, which grows in proportion to the number of monitors, is
the greater threat. Reducing it requires coordination among monitors.
Doubletree is the key component of a coordinated probing system that signif-
icantly reduces both kinds of redundancy while discovering nearly the same set
of nodes and links. It takes advantage of the tree-like structure of routes in the
internet. Routes leading out from a monitor towards multiple destinations form
a tree-like structure rooted at the monitor. Similarly, routes converging towards
a destination from multiple monitors form a tree-like structure, but rooted at
the destination. A monitor probes hop by hop so long as it encounters previously
unknown interfaces. However, once it encounters a known interface, it stops, as-
suming that it has touched a tree and the rest of the path to the root is also
known.
Both backwards and forwards probing use stop sets. The one for backwards
probing, called the local stop set, consists of all interfaces already seen by that
monitor. Forwards probing uses the global stop set of (interface, destination) pairs
accumulated from all monitors. A pair enters the stop set if a monitor visited
the interface while sending probes with the corresponding destination address.
A monitor that implements Doubletree starts probing for a destination at
some number of hops h from itself. It will probe forwards at h + 1, h + 2,
etc., adding to the global stop set at each hop, until it encounters either the
destination or a member of the global stop set. It will then probe backwards at
h − 1, h − 2, etc., adding to both the local and global stop sets at each hop,
until it either has reached a distance of one hop or it encounters a member of
the local stop set. It then proceeds to probe for the next destination. When it
has completed probing for all destinations, the global stop set is communicated
to the next monitor.
The choice of initial probing distance h is crucial. Too close, and intra-monitor
redundancy will approach the high levels seen by classic forward probing tech-
152
B. Donnet, T. Friedman, and M. Crovella
niques. Too far, and there will be high inter-monitor redundancy on destinations.
The choice must be guided primarily by this latter consideration to avoid having
probing look like a DDoS attack.
While Doubletree largely limits redundancy on destinations once hop-by-hop
probing is underway, its global stop set cannot prevent the initial probe from
reaching a destination if h is set too high. Therefore, we recommend that each
monitor set its own value for h in terms of the probability p that a probe sent h
hops towards a randomly selected destination will actually hit that destination.
Fig. 1 shows the cumulative mass function for this probability for skitter monitor
apan-jp. For example, in order to restrict hits on destinations to just 10% of
s
s
a
m
e
v
i
t
l
a
u
m
u
c
  1.00
  0.90
  0.80
  0.70
  0.60
  0.50
  0.40
  0.30
  0.20
  0.10
  0.00
 0
 5
 10
 15
 20
 25
path length
 30
 35
 40
Fig. 1. Cumulative mass plot of path lengths from skitter monitor apan-jp
initial probes, this monitor should start probing at h = 10 hops. This distance
can easily be estimated by sending a small number of probes to randomly chosen
destinations.
For a range of p values, Doubletree is able to reduce measurement load by
approximately 70% while maintaining interface and link coverage above 90%.
3 Bloom Filters
One possible obstacle to successful deployment of Doubletree concerns the com-
munication overhead from sharing the global stop set among monitors. Tracing
from 24 monitors to just 50,000 destinations with p = 0.05 produces a set of
2.7 million (interface, destination) pairs. As 64 bits are used to express a pair of
IPv4 addresses, an uncompressed stop set based on these parameters requires
20.6 MB. This section shows that encoding the stop set into a Bloom ﬁlter [11]
can reduce the size by a factor of 17.3 with very little loss in node and link cover-
age. Some additional savings are possible by applying the compression techniques
that Mitzenmacher describes [12]. Since skitter traces to many more than 50,000
destinations, a skitter that applied Doubletree would employ a larger stop set.
Exactly how large is diﬃcult to project, but we could still expect to reduce the
communication overhead by a factor of roughly 17.3 by using Bloom ﬁlters.
A Bloom ﬁlter encodes information concerning a set into a bit vector that
can then be tested for set membership. An empty Bloom ﬁlter is a vector of all
Improved Algorithms for Network Topology Discovery
153
zeroes. A key is registered in the ﬁlter by hashing it to a position in the vector
and setting the bit at that position to one. Multiple hash functions may be used,
setting several bits set to one. Membership of a key in the ﬁlter is tested by
checking if all hash positions are set to one. A Bloom ﬁlter will never falsely
return a negative result for set membership. It might, however, return a false
positive. For a given number of keys, the larger the Bloom ﬁlter, the less likely
is a false positive. The number of hash functions also plays a role.
To evaluate the use of Bloom ﬁlters for encoding the global stop set, we
simulate a system that applies Doubletree as described in Sec. 2. The ﬁrst mon-
itor initializes a Bloom ﬁlter of a ﬁxed size. As each subsequent monitor applies
Doubletree, it sets some of the bits in the ﬁlter to one. A ﬁxed size is necessary
because, with a Bloom ﬁlter, the monitors do not know the membership of the
stop set, and so are unable to reencode the set as it grows.
Our aim is to determine the performance of Doubletree when using Bloom
ﬁlters, testing ﬁlters of diﬀerent sizes and numbers of hash functions. We use the
skitter data described in Sec. 2. A single experiment uses traceroutes from all
24 monitors to a common set of 50,000 destinations chosen at random. Hashing
is emulated with random numbers. We simulate randomness with the Mersenne
Twister MT19937 pseudorandom number generator [13]. Each data point rep-
resents the average value over ﬁfteen runs of the experiment, each run using a
diﬀerent set of 50,000 destinations. No destination is used more than once over
the ﬁfteen runs. We determine 95% conﬁdence intervals for the mean based, since
the sample size is relatively small, on the Student t distribution. These intervals
are typically, though not in all cases, too tight to appear on the plots.
We ﬁrst test p values from p = 0 to p = 0.19, a range which our prior work
identiﬁed as providing a variety of compromises between coverage quality and
redundancy reduction. For these parameters, the stop set size varies from a low
of 1.7 million pairs (p = 0.19) to a high of 9.2 million (p = 0). We investigate
ten diﬀerent Bloom ﬁlter sizes: 1 bit (the smallest possible size), 10, 100, 1,000,
10,000, 100,000, 131,780 (the average number of nodes in the graphs), 279,799
(the average number of links), 1,000,000, 10,000,000 and, ﬁnally, 27,017,990.
This last size corresponds to ten times the average ﬁnal global stop set size
when p = 0.05. We test Bloom ﬁlters with one, two, three, four, and ﬁve hash
functions. The aim is to study Bloom ﬁlters up to a suﬃcient size and with a
suﬃcient number of hash functions to produce a low false positive rate. A stop
set of 2.7 million pairs encoded in a Bloom ﬁlter of 27 million bits using ﬁve hash
functions, should theoretically, following the analysis of Fan et al. [14–Sec. V.D],
produce a false positive rate of 0.004.
Bloom Filter Results
3.1
The plots shown here are for p = 0.05, a typical value. Each plot in this section
shows variation as a function of Bloom ﬁlter size, with separate curves for varying
numbers of hash functions. The abscissa is in log scale, running from 100,000
to 30 million. Smaller Bloom ﬁlter sizes are not shown because the results are
identical to those for size 100,000. Curves are plotted for one, two, three, four,
154
B. Donnet, T. Friedman, and M. Crovella
and ﬁve hash functions. Error bars show the 95% conﬁdence intervals for the
mean, but are often too tight to be visible.
Fig. 2 shows how the false positive rate varies as a function of Bloom ﬁlter
size. Ordinate values are shown on a log scale, and range from a low of 0.01% to
a high of 100%. The ﬁgure displays two sets of curves. The upper bound is the
upper-bound - 1 hash
upper-bound - 2 hash
upper-bound - 2 hash
upper-bound - 4 hash
upper-bound - 5 hash
experimental - 1 hash
experimental - 2 hash
experimental - 3 hash
experimental - 4 hash
experimental - 5 hash
e
t
a
r
e
v
i
t
i
s
o
p
e
s
a
l
f
100
10-1
10-2
10-3
10-4
105
106
Bloom filter size
107
Fig. 2. Bloom ﬁlter false positive rate
false positive rate that one would obtain from a Bloom ﬁlter of the given size,
with the given number of hash functions, encoding the global stop set at its ﬁnal
size, and presuming that any possible key is equally likely to be tested. This is
an upper bound because the number of elements actually in the stop set varies.
The ﬁrst monitor’s Bloom ﬁlter is empty, and it conducts the most extensive ex-
ploration, never obtaining a false positive. Successive monitors encounter higher
false positive rates, and the value that results from an experiment is the rate
over all monitors. The false positive rate should also diﬀer from the theoretic
bound because all keys are not equally likely. We would expect a disproportion-
ate number of set membership tests for interfaces that have high betweenness
(Dall’Asta et al. work [15] point out the importance of this parameter for topol-
ogy exploration).
Looking at the upper bounds, we see that false positives are virtually guar-
anteed for smaller Bloom ﬁlters, up to a threshold, at which point false positive
rates start to drop. Because of the abscissa’s log scale, the falloﬀ is less dramatic
than it might at ﬁrst appear. In fact, rates drop from near one hundred percent
to the single digit percentiles over a two order of magnitude change in the size
of the Bloom ﬁlter, between 2.8 × 105 and 2.7 × 107. The drop starts to occur
sooner for a smaller number of hash functions, but then is steeper for a larger
number of hash functions.
Based on an average of 2.7 × 106 (interface, destination) pairs in a stop set,
we ﬁnd that the decline in the false positive rate starts to be perceptible at a
Bloom ﬁlter size of approximately 1/10 bit per pair encoded, and that it drops
into the single digit percentiles at approximately ten bits per pair encoded. This
translates to a range of compression ratios from 640:1 to 6.4:1.
Improved Algorithms for Network Topology Discovery
155
Looking at the experimental results, we ﬁnd, as we would expect, that the
false positive rates are systematically lower. The experimental curves parallel
the corresponding upper bounds, with the false positive rate starting to decline
noticeably beyond a somewhat smaller Bloom ﬁlter size, 1.3 × 105, rather than
2.8×105. False positive rates are below one percent for a Bloom ﬁlter of 2.7×107
bits. We would expect to ﬁnd variation in performance over the same range, as
the subsequent ﬁgures bear out.
The main measure of performance for a probing system is the extent to which
it discovers what it should. Fig. 3 shows how the node and link coverage varies as
a function of Bloom ﬁlter size. The ordinate values are shown on linear scales, and
represent coverage proportional to that discovered by skitter. A value 1.0, not
r
e