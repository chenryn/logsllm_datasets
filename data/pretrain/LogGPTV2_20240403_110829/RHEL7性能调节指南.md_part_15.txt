    尝试自动设置合适的日志带状单元，但这取决于输出该信息的 RAID 设备。
    :::
    ::: para
    如果您的工作负载过于频繁地触发同步事件，设置大日志带状单元会降低性能。这是因为小的写操作需要填充至日志带状单元，而这会增加延迟。如果您的工作负载受到日志写操作延迟的约束，红帽推荐将日志带状单元设置为
    1 个块，从而尽可能地触发非对齐日志写操作。
    :::
    ::: para
    支持的最大日志带状单元为最大日志缓存的大小（256 KB）。因此底层存储器可能拥有更大的带状单元，且该带状单元能在日志中配置。在这种情况下，`mkfs.xfs`{.command}
    会发出警告，并设置一个大小为 32 KB 的日志带状单元。
    :::
    ::: para
    使用以下选项之一配置日志带状单元，其中 *N*
    是被用于带状单元的块的数量，*size* 是以 KB 为单位的带状单元的大小。
    :::
    ``` screen
    mkfs.xfs -l sunit=Nb
    mkfs.xfs -l su=size
    ```
    ::: para
    更多详细信息参见 `mkfs.xfs`{.command} 手册页：
    :::
    ``` screen
    $ man mkfs.xfs
    ```
:::
:::
::: section
::: titlepage
#### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_XFS-Mount_options}5.3.7.1.2. 挂载选项 {.title}
:::
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Mount_options-Inode_allocation}[Inode 分配]{.term}
:   ::: para
    强烈推荐文件系统大于 1 TB。*`inode64`* 参数配置
    XFS，从而在文件系统中分配 inode 和数据。这样能保证 inode
    不会被大量分配到文件系统的起始位置，数据也不会被大量分配到文件系统的结束位置，从而提高大文件系统的性能表现。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Mount_options-Log_buffer_size_and_number}[日志缓存和数量]{.term}
:   ::: para
    日志缓存越大，将所有变更写入日志的 I/O 操作越少
    。大日志缓存能提高有大量 I/O
    密集型工作负载的系统性能表现，而该工作负载没有非易变的写缓存。
    :::
    ::: para
    通过 *`logbsize`*
    挂载选项配置日志缓存大小，并确定日志缓存中信息存储的最大数量。如果未设置日志带状单元，缓存写操作可小于最大值，因此不需要减少大量同步工作负载中的日志缓存大小。默认的日志缓存大小为
    32 KB。最大值为 256 KB， 也支持 64 KB、128 KB 或以 2
    的倍数为幂的介于 32 KB 和 256 KB 之间的日志带状单元。
    :::
    ::: para
    日志缓存的数量是由 *`logbufs`* 挂载选项确定的。日志缓存的默认值为 8
    （最大值），但能配置的日志缓存最小值为2。通常不需要减少日志缓存的数量，除非内存受限的系统不能为额外的日志缓存分配内存。减少日志缓存的数量会降低日志的性能，尤其在工作负载对
    I/O 延迟敏感的时候。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Mount_options-Delay_change_logging}[延迟变更日志]{.term}
:   ::: para
    内存的变更写入日志前，XFS 的选项能集成这些改变。 *`delaylog`*
    参数允许将频繁更改的元数据周期性地写入日志，而非每次改变都要记录到日志中。此选项会增加故障中操作丢失的潜在数量，也会增加用于跟踪元数据的内存大小。但是，它能通过按量级排序增加元数据的更改速度和可扩展性，为确保数据和元数据写入硬盘而使用
    `fsync`{.methodname}、`fdatasync`{.methodname} 或`sync`{.methodname}
    时，此选项不能减少数据或元数据的完整性。
    :::
:::
:::
:::
::: section
::: titlepage
### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_file_systems_for_performance-Tuning_ext4}5.3.7.2. 调整 ext4 {.title}
:::
::: para
本章节涵盖格式化和挂载时ext4 文件系统可用的一些调整参数。
:::
::: section
::: titlepage
#### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_ext4-Formatting_options}5.3.7.2.1. 格式化选项 {.title}
:::
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Inode_table_initialization}[inode 表初始化]{.term}
:   ::: para
    在很大的文件系统上初始化文件系统中所有 inode
    会耗时很久。默认设置下会推迟初始化过程（启用迟缓 inode
    表初始化）。但是，如果您的系统没有 ext4 驱动，默认设置下会禁用迟缓
    inode 表初始化。可通过设置 *`lazy_itable_init`* 为 `1`{.literal}
    启用。那么在挂载后，内核进程继续初始化文件系统。
    :::
:::
::: para
本章节仅描述了在格式化时可用的一些选项。更多格式化参数，参见`mkfs.ext4`{.command}手册页：
:::
``` screen
$ man mkfs.ext4
```
:::
::: section
::: titlepage
#### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_ext4-Mount_options}5.3.7.2.2. 挂载选项 {.title}
:::
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Mount_options-Inode_table_initialization_rate}[inode表初始化率]{.term}
:   ::: para
    启用迟缓 inode 表初始化时，您可通过指定 *`init_itable`*
    参数值控制初始化发生的速率。执行后台初始化的时间约等于 1
    除以该参数值。默认值为 `10`{.literal}。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Mount_options-Automatic_file_synchronization}[自动文件同步]{.term}
:   ::: para
    对现有文件重命名、截断或重写后，一些应用程序无法正确执行
    `fsync`{.methodname}。默认设置下，执行这些操作之后，ext4
    会自动同步文件。但这样会比较耗时。
    :::
    ::: para
    如果不需要此级别的同步，您可在挂载时通过指定
    `noauto_da_alloc`{.option} 选项禁用该行为。如果
    `noauto_da_alloc`{.option} 已设置，应用程序必须明确使用 fsync
    以确保数据的持久化。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Mount_options-Journal_IO_priority}[日志 I/O 优先级]{.term}
:   ::: para
    默认设置下日志 I/O 优先级为 `3`{.literal}，该值比常规 I/O
    的优先级略高。您可在挂载时使用 *`journal_ioprio`* 参数控制日志 I/O
    的优先级。*`journal_ioprio`* 的有效值范围为从 `0`{.literal} 到
    `7`{.literal}，其中 `0`{.literal} 表示具有最高优先级的 I/O。
    :::
:::
::: para
本章节仅描述挂载时可用的一些选项。更多选项，参见 `mount`{.command}
手册页：
:::
``` screen
$ man mount
```
:::
:::
::: section
::: titlepage
### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_file_systems_for_performance-Tuning_btrfs}5.3.7.3. 调整 btrfs {.title}
:::
::: para
自红帽企业版 Linux 7.0 起，btrfs 作为技术预览而为用户提供。如果 btrfs
受到全面支持，本章节将在未来更新。
:::
:::
::: section
::: titlepage
### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_file_systems_for_performance-Tuning_GFS2}5.3.7.4. 调整 GFS2 {.title}
:::
::: para
本章节涵盖 GFS2 文件系统在格式化和挂载时可用的调整参数。
:::
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_GFS2-Directory_spacing}[目录间距]{.term}
:   ::: para
    GFS2
    挂载点的顶层目录中创建的所有目录都是自动间隔，以减少目录中的碎片并提高写速度。为像顶层目录间隔其他目录，用
    *`T`* 属性标注该目录，如示，用您想间隔该目录的路径替代 *dirname*。
    :::
    ``` screen
    # chattr +T dirname
    ```
    ::: para
    `chattr`{.command} 作为 [e2fsprogs]{.package}
    软件包中的一部份为用户提供。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_GFS2-Reduce_contention}[减少争用]{.term}
:   ::: para
    GFS2
    使用全域锁机制，该机制需要簇中节点之间的通信。多节点之间的文件和目录争用会降低性能。通过最小化多节点间共享的文件系统区域，您可将缓存失效的风险最小化。
    :::
:::
:::
:::
:::
:::
[]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html}
::: chapter
::: titlepage
# [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking}第 6 章 网络 {.title}
:::
::: para
网络子系统由很多敏感连接的不同部分构成。红帽企业版 Linux 7
网络因此旨在为大多数工作负载提供最佳性能，并且自动优化其性能。因此，不需要时常手动调节网络性能。本章探讨了可以对功能网络系统做的进一步优化。
:::
::: para
网络性能问题有时是硬件故障或基础结构层故障造成的。解决这些问题超出了本文的范畴。
:::
::: section
::: titlepage
# [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking-Considerations}6.1. 注意事项 {.title}
:::
::: para
要决定调优，用户需要对红帽企业版 Linux
包的接收有充分的认识。本章节解释了如何接收和处理网络数据包，以及潜在瓶颈会出现的地方。
:::
::: para
发送至红帽企业版 Linux 系统的数据包是由
NIC（网络接口卡）接收的，数据包置于内核硬件缓冲区或是循环缓冲区中。NIC
之后会发送一个硬件中断请求，促使生成一个软件中断操作来处理该中断请求。
:::
::: para
作为软件中断操作的一部分，数据包会由缓冲区转移到网络堆栈。根据数据包及用户的网络配置，数据包之后会被转发、删除或转移到一个应用程序的
socket接收队列，并将从网络堆栈中删除。这一进程会持续进行，直到 NIC
硬件缓冲区中没有数据包或一定数量的数据包（在
`/proc/sys/net/core/dev_weight`{.filename} 中指定）被转移。
:::
::: section
::: titlepage
## [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Networking.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Considerations-Before_you_tune}6.1.1. 调节前 {.title}