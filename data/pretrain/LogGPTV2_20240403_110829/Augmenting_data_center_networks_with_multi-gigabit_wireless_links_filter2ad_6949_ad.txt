set of racks in a cluster, each equipped with one or more wireless
devices that can be used to construct ﬂyways as needed. Our design
is independent of the speciﬁc topology used in the oversubscribed
core, which could be the typical tree structure, or recent proposals
for non-oversubscribed networks [1, 8, 9] with proportionally fewer
switches and links.
Our goal is to conﬁgure the ﬂyway links and the routing to im-
prove the time to satisfy trafﬁc demands. The metric of interest is
the completion time of the demands (CTD), deﬁned as the time it
takes for the last ﬂow to complete.
The system has three tasks: (i) measure and estimate trafﬁc de-
mands, (ii) decide which ﬂyways to instantiate, and (iii) make ap-
propriate routing changes to route trafﬁc over ﬂyways. Inputs to the
system include the measured 60 GHz channel model, antenna char-
acteristics, device locations and trafﬁc demands if available. We
focus on ﬂyways instantiation, and discuss trafﬁc estimation and
routing only brieﬂy (§5.3).
Computing an optimal choice of ﬂyways is challenging since
wireless constraints such as range and interference are hard to incor-
porate into a max-ﬂow formulation. Hence, our design decomposes
the problem into two sub-parts. A ‘ﬂyway picker’ (§5.1) proposes
ﬂyways that will improve the completion time of demands. A mea-
surement and channel-model driven ‘ﬂyway validator’ (§5.2) con-
ﬁrms or rejects this proposal. The validator ensures that the system
only adds feasible, non-interfering ﬂyways. In addition, the val-
idator also predicts how much capacity the ﬂyways will have. This
allows the picker to add the “approved” ﬂyway and propose ﬂyways
for subsequent hotspots. The process repeats until no more ﬂyways
can be added. This decomposition is not optimal and there is room
to improve. However, it ﬁnishes quickly, scales well and provides
impressive gains, as we will show.
5.1 Choosing ﬂyways
In this section, we will assume that trafﬁc demands are known.
We begin with an example. Consider the network in Figure 14(b).
Six ToR switches A and C–G have trafﬁc to send to ToR B. A
has 100 units to send, whereas the rest each send 80 units. Each
ToR has one wireless device connected to it. Wired link capacity in
and out of the ToRs is 10 units/sec and for simplicity assume that
these are the only potential bottlenecks. The downlink into B is the
bottleneck here. It carries 500 units of trafﬁc in total and takes 50 s
to do so. Hence, completion time (CTD) is 50 s.
Suppose we add a ﬂyway (capacity 3) from A to B to improve
performance of the straggler, i.e., the ToR pair that sends the most
amount of trafﬁc on the bottleneck link and completes last, by by-
passing the bottleneck. As Figure 14(c) shows, trafﬁc on the bottle-
neck drops to 400 units, and time to complete drops to 40 s. How-
ever, as our trafﬁc analysis shows, the straggler often contributes
only a small proportion of the total demand on that link (in this
case 100/500). Alleviating the straggler provides only 20% gains,
reducing CTD to 40 s.
Note that there is room to spare on the ﬂyway; the demand from
A to B completes after 33.3 s, 6.7 s before trafﬁc from C–G. Our
datasets indicate that this is quite common; very few of the ToR
pairs on hot links require substantial capacity. Hence, we also allow
43Demand A(cid:1)B
100 units
C,D,E,F,G(cid:1)B 80 units
Capacity wired links
10 units/s
flyway: A(cid:1)B 3 units/s
flyway: C(cid:1)B 6 units/s
100
500
80
4 * 80
A
B C
(a) Setup
(b) Baseline: CTD=50s
Agg. Switch
Agg. Switch
Agg. Switch
Agg. Switch
400
B C
A
100
(c) Direct ﬂyway for the
straggler: CTD=40s
385
B C
15
A
115
(d) Allowing transit traf-
ﬁc: CTD=38.5s
A
312
108
B C
188
(e) Greedy choice of ﬂy-
ways: CTD=31.2s
Figure 14: A motivating example: Greedy choice of ﬂyways to add and allowing transit trafﬁc through ﬂyways are crucial.
indirect transit trafﬁc to use the ﬂyway, i.e., as Figure 14(d) shows,
trafﬁc from other sources to B bypasses the bottleneck by ﬂowing
via node A and the ﬂyway. This improves CTD to 115
10 =
38.5 s.
3 = 385
Often the ﬂyway to the straggler is infeasible or an inferior choice,
the devices at either ends might be used up in earlier ﬂyways or the
link may interfere with an existing ﬂyway or the ToR pairs might
be too far apart. Allowing transit trafﬁc ensures that any ﬂyway that
can ofﬂoad trafﬁc on the bottleneck will be of use, even if it is not
between the straggler pair. In this case, it is more effective to en-
able the ﬂyway from C to B, with twice the capacity of the ﬂyway
from A. This decision allows more trafﬁc to be ofﬂoaded results in
a CTD of 312
6 = 31.2 s.
10 = 188
Proposed algorithm: Our approach formalizes these two insights.
By allowing transit trafﬁc on a ﬂyway, via indirection, we skirt the
problem of high fan-in (or fan-out) that we saw to be correlated with
congestion. Further, doing so opens up the space of potentially use-
ful ﬂyways, greedy choice among this set adds substantial value. In
particular, at each step, we choose the ﬂyway that diverts the most
trafﬁc away from the bottleneck link. For a congested downlink to
ToR p, the best ﬂyway will be from the ToR that has a high capacity
ﬂyway and sufﬁcient available bandwidth on its downlink to allow
transit trafﬁc through, i.e.,
arg max
ToR i
min (Ci→p, Di→p + downi) .
The ﬁrst term Ci→p denotes the capacity of the ﬂyway. The amount
of transit trafﬁc is capped by downi the available bandwidth on the
downlink to i and Di→p is i’s demand to p. Together, the second
term indicates the maximum possible trafﬁc that i can send to p.
The corresponding expression of the best ﬂyway for a congested
uplink to ToR is similar,
arg max
ToR i
min (Cp→i, Dp→i + upi) .
5.2 Validating ﬂyway choice
The ﬂyways validator determines whether a speciﬁed set of ﬂy-
ways can operate together — it computes the effects of interference
and what capacity each link is likely to provide. It operates using
the same principle as DIRC’s conﬂict graph [17]: If we know how
much signal is delivered between all pairs of nodes in all transmit
and receive antenna orientations, we can combine these measure-
ments with knowledge of which links are active, and how the an-
tennas are oriented, to compute the SINR for all nodes. We can then
use the simple SINR-based auto-rate algorithm (§B) to select rates.
Our SINR model (§B) is very conservative: we compute interfer-
ence assuming all nodes from all other ﬂyways send concurrently,
and add an additional 3 dB. Hence, we disable carrier sense on our
ﬂyways links, managing contention between sender and receiver
with other types of coordination (§C). Recall that both the SINR
model and rate selection are appropriate for our data center envi-
ronment because of the high directionality (§3 and [17]).
ceived signal strengths. How can we generate this very large table?
In the simulator, we compute delivered signal power using the mod-
els of antennas and signal propagation developed in §3. In a real
DC deployment, we can measure it—a data center provides a sta-
ble, line-of-sight environment (§3.3) and a ﬁxed set of nodes with
known geographic coordinates. Hence, unlike DIRC’s dynamic,
non-line-of-sight environment with unknown client locations, we
can afford to measure this table only once when the data center is
conﬁgured, and measurements will remain valid over time. We can
refresh entries in the table opportunistically, without disrupting on-
going wireless trafﬁc, by having idle nodes measure signal strength
from active senders at various receive antenna orientations and shar-
ing these measurements, along with transmitter antenna orientation,
over the wired network.
The table can be used not just to compute interference, but also
to determine the best antenna orientation for two ToRs to commu-
nicate with each other, and the complex antenna orientation mech-
anisms prescribed in 802.11ad are no longer needed. In this pa-
per, we evaluate antennas that use purely directional radiation pat-
terns and point directly at their intended receivers. Advanced, more
powerful antenna methods such as null-steering to avoid interfer-
ence [20] could increase ﬂyway concurrency, but we defer these to
future work. Our results (§6) will show that even this simple an-
tenna model is effective at improving data center performance.
5.3 Trafﬁc estimation and routing
Trafﬁc estimation and routing are not the main focus of this pa-
per, and our system design in these areas is largely similar to prior
work [30, 7, 8]. We describe it brieﬂy for the sake of completeness.
Estimating trafﬁc demands: Trafﬁc demand can be estimated in
one of two ways. First, for clusters that are orchestrated by cluster-
wide schedulers (e.g., map-reduce schedulers such as Quincy [11]),
logically co-locating our system with such a scheduler makes trafﬁc
demands visible. In this mode, our system can pick ﬂyways appro-
priate for these demands. C-Through [30] takes a somewhat similar
approach: it assumes that applications hint at their trafﬁc demands.
Second, in clusters that have predictable trafﬁc patterns, such
as the HPC datasets we analyzed, we can use instrumentation to
estimate current trafﬁc demands and pick ﬂyways appropriate for
demands predicted based on these estimates. Such distributed, end-
host based, trafﬁc measurement instrumentation is already used, for
e.g., at EC2 and Windows Azure, for billing and accounting, and
can provide up-to-date inputs for our system as well.
We have designed a simple trafﬁc estimation scheme that uses a
shim layer (an NDIS ﬁlter driver) on servers to collect trafﬁc statis-
tics, in a manner similar to prior work [6, 14]. We use a simple
moving average of estimates from the recent past [13]. This estima-
tor works as well with our traces as the more complex alternatives
that we tried. Micro-benchmarks show this estimator to be feasible
at line rate with negligible increase in server load.
In future work, we will address trafﬁc that is neither predictable
nor orchestrated. See (§7) for some ideas.
Obtaining the conﬂict graph: If there are N racks and K antenna
orientations, the input to the validator is an (N K)2-size table of re-
Routing: We present a simple mechanism that routes trafﬁc across
the potentially multiple paths that are made feasible with ﬂyways.
44Our approach is straightforward and similar to prior work [8, 7]. We
treat ﬂyways as point-to-point links. Note that every path on the ﬂy-
way transits through exactly one ﬂyway link, so all that the routing
does is to encapsulate packets to the appropriate interface address.
For example, to send trafﬁc via A → Core → C → B, the servers
underneath A encapsulate packets with the address of C’s ﬂyway
interface to B. The ﬂyway picker computes the fraction of trafﬁc to
ﬂow on each path and relays these decisions to the servers. We have
built this functionality into the aforementioned NDIS ﬁlter driver.
Our micro-benchmarks tests on standard DC servers equipped with
1 Gbps NICs indicate that these operations can be performed at line
speed with negligible additional load.
When changing the ﬂyway setup, we simply disable encapsula-
tion, and remove the added routes. The default routes on the ToR
and Agg switches are never changed, and direct trafﬁc on the wired
network. Thus, as soon as we remove the ﬂyway route, the trafﬁc
ﬂows over wired links. Thus, during ﬂyway changes (and ﬂyway
failures, if any), packets are simply sent over wired network.
6. EVALUATING FLYWAYS
In this section we combine our measurement- and standard-driven
wireless models with our traces from real data centers to evaluate
the practical beneﬁts to data center workloads in oversubscribed
networks that come from our wireless ﬂyways system.
6.1 Methodology
Demands: We replay the trafﬁc described in §4, which is measured
from four different clusters and includes workloads from latency-
and throughput-sensitive applications and highly tuned HPC appli-
cations.
Wireless models: We use the wireless physical and MAC layers
and channel models described in §B. Here, we recall a few salient
speciﬁcs: We use the three channels deﬁned in 802.11ad to increase
the number of concurrent links. Devices use a uniform 10 mW
transmit power. The system uses the interference model and rate
selection algorithms described in §B and the ﬂyways validator de-
scribed in §5.2. We use the 802.11ad OFDM rates, which peak at
6.76 Gbps, only about 85% of which is usable for trafﬁc (§C).
Geography: We mimic the geographical layout of racks as per
measurements from an open ﬂoor-plan data center (see Figure 9).
We assume that each ToR is equipped with K wireless devices,
often 1, which are mounted atop the rack. ToR switches in the ob-
served data centers have a few unused ports for occasional network
management tasks.
We compare these variants of our system:
Straggler is the simplest alternative, in which the picker pro-
poses a ﬂyway between the pair of ToRs taking the longest time
to complete. If the validator accepts this proposal as safe then the
ﬂyway is added, and if not then the process terminates — the CTD
cannot be further improved.
Transit augments Straggler by allowing for transit trafﬁc on the
added ﬂyways. As we saw in §5, doing so improves performance
by ofﬂoading more trafﬁc from the bottleneck link, and potentially
changes which link will next be the bottleneck.
Greedy augments Transit by preferentially picking, in each iter-
ation, the ﬂyway that ofﬂoads the most trafﬁc from the bottleneck
link.
In practice, this results in using ﬂyways between close-by
nodes that have high capacity. As a side-effect, this process tends
to add shorter links and thus results in more feasible ﬂyways than
Straggler.
Metric: Our primary metric of goodness is the completion time of
s
e
c
i
r
t
a
M
d
n
a
m
e
D
r
e
v
o
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
No OverSubscription
Greedy
1:2 OverSubscribed
 1
 1.2
 1.4
 1.6
 1.8
 2
Normalized CTD
Figure 15: With just one device/ToR with NB antennas, the
greedy trafﬁc-aware choice of ﬂyways provides signiﬁcant im-
provements for demands observed in data centers.
s
e
c
i
r
t
a
M
d
n
a
m
e
D
r
e
v
o
F
D
C
 1
 0.8