beneﬁts between ZCBPF and normal BPF in FreeBSD are
expected to be smaller than in Linux with shared memory
support.
TNAPI is very recent development by Luca Deri [5]. It im-
proves standard Linux network card drivers and can be used
in conjunction with PF RING. Usually, Linux drivers assign
new memory pages to network cards for DMA after the card
copied new packets to old memory pages. The driver allo-
208that today’s oﬀ-the-shelf hardware is able to do, whereas it
remains a very challenging task in 10 GE networks [4]. Apart
from the ten-fold increased throughput, the diﬃculties also
lie in the ten-fold increased packet rate, as the number of
packets to be captured per time unit is a factor that is even
more important than the overall bandwidth. As an exam-
ple, capturing a 1 GE stream that consists entirely of large
packets, e.g., 1500 bytes, is easy; whereas capturing a 1 GE
stream consisting entirely of small packets, e.g., 64 bytes,
is a very diﬃcult task [8, 4]. This is due to the fact that
each packet, regardless of its size, introduces a signiﬁcant
handling overhead.
Driver issues that arise with a high number of packets
have been studied very well [7, 10, 11]. Problems concern-
ing interrupt storms are well understood and most network
cards and drivers support mechanisms to avoid them. Such
mechanisms include polling or interrupt moderation.
In 2007, Schneider et al. compared FreeBSD and Linux on
Intel and AMD platforms [4]. They determined that device
polling on Linux reduces the CPU cycles within the ker-
nel and therefore helps to improve capturing performance.
On FreeBSD however, device polling actually reduced the
performance of the capturing and furthermore reduced the
stability of the system. Hence, they recommend using the
interrupt moderation facilities of the cards instead of polling
on FreeBSD. In their comparison, FreeBSD using BPF and
no device polling had a better capturing performance than
Linux with PF PACKET and device polling. This trend
is enforced if multiple capturing processes are simultane-
ously deployed; in this case, the performance of Linux drops
dramatically due to the additional load. Schneider et al.
ﬁnd that capturing 1 GE in their setup is possible with the
standard facilities because they capture traﬃc that contains
packets originated from a realistic size distribution. How-
ever, they do not capture the maximum possible packet rate,
which is about 1.488 million packets per second with 64 bytes
packets [5]. Another important aspect about the workload of
Schneider et al. is that during each measurement, they send
only one million packets (repeating each measurement for 7
times). This is a very low number of packets, considering
that using 64 byte packets, it is possible to send 1.488 mil-
lion packets within one second. Some of the eﬀects we could
observe are only visible if more packets are captured. Based
on their measurement results, they recommend a huge buﬀer
size in the kernel buﬀers, e.g., for the HOLD and STORE
buﬀers in FreeBSD, to achieve good capturing performance.
We can see a clear correlation between their recommenda-
tion and the number of packets they send per measurement
and will come back to this in Section 4.2.
Deri validated his PF RING capturing system against the
standard Linux capturing PF PACKET in 2004 [8]. He
ﬁnds PF RING to really improve the capturing of small (64
bytes) and medium (512 bytes) packets compared to the cap-
turing with PF PACKET and libpcap-mmap. His ﬁndings
were reproduced by Cascallana and Lizarrondo in 2006 [6]
who also found signiﬁcant performance improvements with
PF RING. In contrast to Schneider et al., neither of the
PF RING comparisons consider more than one capturing
process.
Using TNAPI and the PF RING extensions, Deri claims
to be able to capture 1 GE packet streams with small packet
sizes at wire-speed (1.488 million packets) [5].
Figure 3: Berkeley Packet Filter and Zero Copy
Berkeley Packet Filter
cates new pages and assign them to the card for DMA. Deri
changed this behaviour in two ways: Non-TNAPI drivers re-
ceive an interrupt for received packets and schedule a kernel
thread to perform the packet processing. The processing is
performed by a kernel thread that is not busy doing other
work and is chosen according to the scheduling rules of the
operating system. Deri’s driver creates a separate kernel
thread that is only used to perform this processing. Hence,
there is always a free kernel thread that can continue, and
packet processing can almost immediately start.
The second major change is the memory handling within
the driver: As soon as the thread is notiﬁed about a new
packet arrival, the new packet is copied from the DMA mem-
ory area into the PF RING ring buﬀer. Usually, network
drivers would allocate new memory for DMA transfer for
new packets. Deri’s drivers tell the card to reuse the old
memory page, thus eliminating the necessity of allocating
and assigning new memory.
Furthermore, his drivers may take advantage of multiple
RX queues and multi-core systems. This is done by creating
a kernel thread for each RX queue of the card. Each kernel
thread is bound to a speciﬁc CPU core in the case of multi-
core systems. The RX queues are made accessible to the
user space, so that users can run monitoring applications
that read from the RX queues in parallel.
A rather non-standard solution for wire-speed packet cap-
ture is ncap. Instead of using the operating system’s stan-
dard packet processing software, it uses special drivers and a
special capturing library. The library allows an application
to read packets from the network card directly [3].
2.2 Previous Comparisons
Previous work compared the diﬀerent approaches to each
other. We will now summarise the previous ﬁndings and
determine which experiments we need to repeat with our
hardware platforms and new software versions. Results from
related work can also give hints on further experiments we
need to conduct in order to achieve a better understanding
of the involved processes.
Capturing traﬃc in 1 GE networks is seen as something
209and a Broadcom BCM5701 all connected via a PCI-X bus.
In our experiments, we only use one NIC at a time.
The second capturing PC has an AMD Athlon 64 X2
5200+ CPU and is also equipped with several network cards,
including an Intel Pro/1000 (82541PI) and a nVidia Corpo-
ration CK804 onboard controller. Both cards are connected
via a slow PCI bus to the system. Furthermore, there are
two PCI-E based network cards connected to the system.
One is an Intel Pro/1000 PT (82572EI), the other is a Dual
Port Intel Pro/1000 ET(82576). It should be noted that the
AMD platform is signiﬁcantly faster than the Xeon platform.
Using diﬀerent network cards and diﬀerent architectures,
we can check if an observed phenomenon emerges due to
a general software problem or if it is caused by a speciﬁc
hardware or driver. Both machines are built from a few years
old and therefore cheap hardware, thus our machines are
not high end systems. We decided not to use more modern
hardware for our testing because of the following reasons:
• We did not have 10 GE hardware.
• We want to identify problems in the software stack
that appear when the hardware is fully utilised. This
is quite diﬃcult to achieve with modern hardware on
a 1 GE stream.
• Software problems that exist on old hardware which
monitors 1 GE packet streams still exist on newer hard-
ware that monitors a 10 GE packet stream.
Both machines are installed with Ubuntu Jaunty Jacka-
lope (9.04) with a vanilla Linux kernel version 2.6.32. Addi-
tionally, they have an installation of FreeBSD 8.0-RELEASE
for comparison with Linux.
We perform tests with varying load at the capturing ma-
chines’ application layer in order to simulate the CPU load
of diﬀerent capturing applications during monitoring. Two
tools are used for our experiments:
First, we use tcpdump 4.0.0 [15] for capturing packets and
writing them to /dev/null. This scenario emulates a sim-
ple one-threaded capturing process with very simple com-
putations, which thus poses almost no load on the applica-
tion layer. Similar load can be expected on the capturing
thread of multi-threaded applications that have a separate
thread that performs capturing only. Examples for such
multi-threaded applications are the Time Machine [16, 17]
or the network monitor VERMONT [18].
The second application was developed by us and is called
packzip.
It poses variable load onto the thread that per-
forms the capturing. Every captured packet is copied once
within the application and is then passed to libz [19] for
compression. The compression mode can be conﬁgured by
the user from 0 (no compression) to 9 (highest compression
level). Increasing the compression level increases CPU us-
age and thus can be used to emulate an increased CPU load
for the application processing the packets. Such packet han-
dling has been performed before in [2] and [4]. We used this
tool in order to make our results comparable to the results
presented in this related work.
4. EVALUATION
This section presents our analysis results of various packet
capture setups involving Linux and FreeBSD, including a
performance analysis of our own proposed improvements
Figure 4: Evaluation Setup
3. TEST SETUP
In this section, we describe the hardware and software
setup that we used for our evaluation. Our test setup (see
Figure 4) consists of three PCs, one for traﬃc generation,
the other two for capturing.
The traﬃc generator is equipped with several network in-
terface cards (NICs) and uses the Linux Kernel Packet Gen-
erator [14] to generate a uniform 1 GE packet stream.
It
was necessary to generate the traﬃc using several NICs si-
multaneously because the deployed cards were not able to
send out packets at maximum speed when generating small
packets. Even this setup was only able to generate 1.27
million packets per seconds (pps) with the smallest packet
size. However, this packet rate was suﬃcient to show the
bottlenecks of all analysed systems. We had to deploy a sec-
ond generator for our experiments in Section 4.4, where we
actually needed wire-speed packet generation (1.488 million
pps).
In contrast to [4], we where did not produce a packet
stream with a packet size distribution that is common in
real networks. Our goal is explicitly to study the behaviour
of the capturing software stacks at high packet rates. As we
do not have 10 GE hardware for our tests available, we have
to create 64 bytes packets in order to achieve a high packet
rate for our 1 GE setup.
Each of our test runs is conﬁgured to produce a packet
stream with a ﬁxed number of packets per seconds and runs
over a time period of 100 seconds and repeated for ﬁve times.
As our traﬃc generator is software based and has to han-
dle several NICs, the number of packets per second is not
completely stable and can vary to some extend. Hence, we
measure the variations in our measurements and plot them
where appropriate.
Capturing is performed with two diﬀerent machines with
diﬀerent hardware in order to check whether we can re-
produce any special events we may observe with diﬀerent
processor, bus systems and network card, too. The ﬁrst
capturing PC is operated by two Intel Xeon CPUs with
2.8 GHz each.
It has several network cards including an
Intel Pro/1000 (82540EM), an Intel Pro /1000 (82546EB)
210to the Linux capturing stack. As multi-core and multi-
processor architectures are common trends, we focus in par-
ticular on this kind of architecture. On these hardware plat-
forms, scheduling issues arise when multiple processes in-
volved in packed capturing need to be distributed over sev-
eral processors or cores. We discuss this topic in Section 4.1.
Afterwards, we focus on capturing with low application load
in Section 4.2 and see if we can reproduce the eﬀects that
have been observed in the related work.
In Section 4.3,
we proceed to capturing with higher application load. We
identify bottlenecks and provide solutions that lead to im-
provements to the capturing process. Finally, in Section 4.4
we present issues and improvements at the driver level and
discuss how driver improvements can inﬂuence and improve
the capturing performance.
4.1 Scheduling and packet capturing perfor-
mance
On multi-core systems, scheduling is an important fac-
tor for packet capturing: If several threads, such as kernel
threads of the operating system and a multi-threaded cap-
turing application in user space are involved, distributing
them among the available cores in a clever way is crucial for
the performance.
Obviously, if two processes are scheduled to run on a sin-
gle core, they have to share the available CPU time. This
can lead to shortage of CPU time in one or both of the pro-
cesses, and results in packet loss if one of them cannot cope
with the network speed. Additionally, the processes then
do not run simultaneously but alternately. As Schneider et
al. [4] already found in their analysis, packet loss occurs if
the CPU processing limit is reached. If the kernel capturing
and user space analysis are performed on the same CPU,
the following eﬀect can be observed: The kernel thread that
handles the network card dominates the user space applica-
tion because it has a higher priority. The application has
therefore only little time to process the packets; this leads
to the kernel buﬀers ﬁlling up. If the buﬀers are ﬁlled, the
kernel thread will take captured packets from the card and
will throw them away because there is no more space to
store them. Hence, the CPU gets busy capturing packets
that will be immediately thrown away instead of being busy
processing the already captured packets, which would empty
the buﬀers.
Conversely, if the processes are scheduled to run on two
cores, they have more available CPU power to each one of
them and furthermore can truly run in parallel, instead of
interleaving the CPU. However, sharing data between two
cores or CPUs requires memory synchronisation between
both of the threads, which can lead to severe performance
penalties.
Scheduling can be done in two ways: Processes can either
be scheduled dynamically by the operating system’s sched-
uler, or they can be statically pinned to a speciﬁc core by
the user. Manual pinning involves two necessary operations,
as described in [5, 20]:
• Interrupt aﬃnity of the network card interrupts have
to be bound to one core.
• The application process must be bound to another
core.
We check the inﬂuence of automatic vs. manually pinned
scheduling in nearly all our experiments.
Figure 5: Scheduling eﬀects
Figure 5 presents a measurement with 64 byte packets
with varying numbers of packets per seconds and low appli-
cation load.
Experiments were run where the kernel and user space ap-
plication are processed on the same core (A:A), are pinned
to run on diﬀerent cores (A:B), or are scheduled automat-
ically by the operating system’s scheduler. Figure 5 shows
that scheduling both processes on a single CPU results in
more captured packets compared to running both processes
on diﬀerent cores, when packet rates are low. This can be
explained by the small application and kernel load at these
packet rates. Here, the penalties of cache invalidation and
synchronisation are worse than the penalties of the involved
threads being run alternately instead of parallel. With in-
creasing packet rate, the capturing performance in case A:A
drops signiﬁcantly below that of A:B.
Another interesting observation can be made if automatic
scheduling is used. One would expect the scheduler to place
a process on the core where it performs best, depending on
system load and other factors. However, the scheduler is not
informed about the load on the application and is therefore
not able to make the right decision. As can be seen on the
error bars in Figure 5, the decision is not consistent over the
repetitions of our experiments in all cases, as the scheduler
tries to move the processes to diﬀerent cores and sometimes