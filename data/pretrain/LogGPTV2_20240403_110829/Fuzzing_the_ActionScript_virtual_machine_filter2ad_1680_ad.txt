by only Tamarin (in red), only ScriptGene (in blue) or both
Tamarin and ScriptGene (in light green). The improvement
of block coverage by ScriptGene is seen to be very signiﬁ-
cant. The complexity of grammar structures produced by
ScriptGene is certainly not less than that oﬀered by the
Tamarin test suite. In addition, with runtime class muta-
tions, ScriptGene is able to create more interactions between
various structures. We reason that both of these aspects
have enabled us to achieve the higher block coverage.
3.4 Examples of bugs
Next, we would like to explain the generation process of
the test cases that triggered some AVM bugs. These bugs
were found by ScriptGene and missed by Tamarin. The o-
riginal test cases can be found on our website [11]. Because
our test cases contain many lines and each line consists of
multiple structures of AS instructions, it is not immediate-
ly obvious which line or structure triggered a speciﬁc bug.
Thus, for the following passages of this section, only the
essential parts pertaining to each bug are maintained and
simpliﬁed. ScriptGene found four bugs in version 10.0.45.2,
one in version 10.2.152.32 and one in version 11.3.300.265.
Although these bugs do not exist in the newest version of
AVM at the time of writing, we have not yet found any pub-
lished documentations on them. It is possible that Adobe
adjusted the code of newer versions of AVMs, without re-
porting these bugs. Of the six bugs, three will be brieﬂy
detailed here, to illustrate the eﬀectiveness of our strategies
in code generation, runtime class mutation and testing.
3.4.1 Complex grammar structure generation
1: for each(var o in new 
[1,2,3,"hello",’out’,"there",true,false,3.14159])
{str+=o;}
2: for each(var _vardeclarationname_ in new
[1,2,3,"hello",’out’,"there",true,false,3.14159])
{_varname_+=_varname_;}
3: for each(var var2:Number in new 
[1,2,3,"hello",’out’,"there",true,false,3.14159])
{var2+=var2;}
A bug leading to memory corruption is caused by mal-
formed enumerations against the vector type. A number
0x1000dfbd0x100c2df00x1014c7a30x101800c10x101a20850x1020ee800x1024108f465tillation of the test cases. Thus, we are uncertain if these
access violations are due to the same reason as in version
10.0.45.2.
It is possible that these memory leak problems
have existed in Flash for many versions. Each subsequent
revision reduced the eﬀect of memory leaks somewhat, pos-
sibly through counter-measures, such that newer versions
gradually required more test cases to trigger the memory
leaks.
4. RELATED WORK
Fuzz testing was introduced in 1972 by Purdom [25]. Pur-
dom used a syntax-directed method to generate test sen-
tences for a parser. He gave an eﬃcient algorithm for gener-
ating short sentences from a context-free grammar such that
each production of the grammar was used at least once and
tested LR(1) parsers using this technique. It is one of the
ﬁrst attempts to automatically test a parser using its gram-
matical structure. In 1990, Miller et al.
[21] were among
the ﬁrst to apply fuzz testing to real world applications. In
their study, the authors used randomly generated program
inputs to test various UNIX utilities. Since then, the tech-
nique of fuzz testing has been used in many diﬀerent areas,
such as protocol testing [13, 26], ﬁle format testing [28, 29],
or mutations of valid inputs [22, 29].
In 2011, Yang et al.
The most relevant studies for this paper are recent ones
on grammar-based fuzzing and test generations for compil-
ers and interpreters.
[32] presented
CSmith, a language-speciﬁc fuzzer operating on the C pro-
gramming language grammar. CSmith is a pure generator-
based fuzzer, generating C programs for testing compilers
and is based on earlier works of the same authors and on
the random C program generator published by Turner [30].
Drawing a parallel to our work, they have used the built-in
grammar to create compilable programs. Furthermore, they
introduced semantic rules during their generation process by
using ﬁlter functions, which allow or disallow certain pro-
ductions depending on the context. In contrast to our work,
ScriptGene takes less control over the instruction ﬂow (we
only restrict the behaviour of breaks and continues) during
the generation phase, and leave the compiler-pass task to
the runtime class mutation phase. This approach allows us
to generate a greater diversity of grammar structures during
the generation phase.
In 2012, Holler at al. [19] proposed LangFuzz, a grammar-
based fuzzing framework that has been proven to be versatile
in discovering vulnerabilities in JavaScript engines. We es-
pecially adapted LangFuzz ’s idea of the “Shortest Terminal
String Algorithm” for ScriptGene. Similar to LangFuzz, we
have used grammar instances extracted from a test suite of
a certain language to terminate the code generation branch.
In contrast, LangFuzz mainly deals with JavaScript that re-
quires only line-validity, while ScriptGene must maintain
full-text validation with the help of modiﬁed grammar rules
and code templates. Furthermore, ScriptGene adds runtime
class mutations, legalizes the code and covers more branches
in tested AVMs simultaneously.
As shown by Godefroid et al.
[17] in 2008, a grammar-
based fuzzing framework that produces JavaScript test cas-
es can increase coverage when linked to a constraint solver
and coverage measurement tools. They present a dynamic
test generation algorithm where symbolic execution direct-
ly generates grammar-based constraints whose satisﬁability
is veriﬁed using a custom grammar-based constraint solver.
Unfortunately their work in its current form only suits well
on JavaScript VMs, since the feedback adjusts the input di-
rectly. However, AVMs only accept bytecodes, which are
the outputs of the compiler. This additional layer prevents
the direct feedback, rendering the symbolic execution inef-
fective.
Fuzzing web browsers and their components is a promising
ﬁeld, particularly in the case of Adobe Flash, which are also
used in some hybrid documents. To date, several approaches
have been taken to ﬁnd bugs/vulnerabilities in Flash.
Flash as a particular format of documents could be fuzzed
using a variety of ﬁle format fuzzing frameworks such as
Peach [9] and SPIKE [14]. One of the methods of ﬁle for-
mat fuzzing called Dumb Fuzz bitﬂips every bit in the entire
ﬁle. Despite its name, Dumb Fuzz was found to be able to
test AVMs to a certain extent. Since AS code is stored as
bytecodes in a SWF, single bit change in these bytecodes
would in general completely alter the original meaning of
the AS code. Several vulnerabilities were conﬁrmed to be
found by Dumb Fuzz recently [20]. Most of these bugs are
due to type confusion. Late in 2011, Google used large scale
computing resources to fuzz AVMs, relying solely on Dumb
Fuzz and found tens of bugs [18]. Nevertheless, this type
of method is mutation based and highly dependant on the
source of the parent SWF. The source for Google was 20T-
B of downloaded SWF ﬁles. While 20TB cannot possibly
include all the structures of AS, mutations on the bytecode
level is a good way to explore additional code paths of AVMs
by bypassing the compiler and alter the inputs directly, when
computational resources permit.
There are other fuzzing approaches targeting Flash. How-
ever, most of them mainly focus on whether AS functions
give enough validation to their parameters. In our previous
work [31], we have attempted to fuzz the Regular Expression
functions of AS. Using grammar-based mutation fuzzing,
we have constructed much more sophisticated expressions
to test the Regular Expression interpreter of AVMs, where
several vulnerabilities were subsequently found.
5. DISCUSSION
Approximate grammars Other than the additional con-
trols to the AS grammar rules as mentioned in Section 2.2,
several other structures have been simpliﬁed. Package =>
Class => Function => Statement is essentially the gener-
ated code in a macro-view. Serialization of Package =>
Function, Class => Expression. . . is impossible due to the
modiﬁcations of the grammar rules. Fixed serialization of
code structures helps us keep track of the properties and
functions and enables us to build them valid contexts. We
need more domain knowledge of a particular language and
its grammar, if we want to build a valid context for all com-
plex grammar rules.
Resource limitations For the code generation phase,
only several hours was needed to generate around ten thou-
sand nearly-valid code snippets with diﬀerent initializations
of sub-rule selections. During each batch of runtime class
mutations, only two of the classes from the pool were chosen.
About ten days of mutation time yields only (cid:24)10% of the
mutations possible under our current strategies. Consider-
ing all possible combinations and the depth of the grammar,
these code snippets explore only a tiny subset of AVM execu-
tion paths, since the mutation phase requires far more time
than the generation phase. This is practically inevitable
466with the blackbox fuzzing method. Additional information
from whitebox analysis should be helpful to constrain the
possibilities for future researches. Distributed computing
environments such as cluster computing or cloud computing
would be helpful for larger mutation diversities.
Finding new vulnerabilities We have used the com-
piler to generate complex bytecodes from AS source code,
then execute the resulting ﬁles on the AVMs. Our approach
is seen to be eﬀective in achieving a good code coverage,
as demonstrated in Section 3. However, we have found less
bugs than expected, from our comparatively greater code
coverage (compared to the Tamarin oﬃcial test suite). N-
evertheless, this is reasonable, since the discovery of bugs
from random fuzzing is probabilistic. Evaluating a given
fuzzing approach by the amount of bugs found is not appro-
priate, unless computational resources permit a large and
statistically valid amount of runs. Therefore, code coverage
and comparisons are better indicators of the eﬀectiveness
of ScriptGene. The discovery of bugs should be considered
secondary to code coverage testing. In our future works, we
would like to add bytecode mutations to the test cases pro-
duced by ScriptGene. Based on current code coverage, we
deduce that bytecode mutations will be capable of identify-
ing even more bugs by extending the code coverage. Byte-
code mutations would also be a more direct method to test
AVMs, since it bypasses the compiler-check.
6. CONCLUSION
ScriptGene is a novel approach to fuzz testing Action-
Script Virtual Machines, where compilers are involved. We
have extended and expanded the ideas of LangFuzz from
JavaScript to ActionScript, to generate code snippets, then
produced nearly-valid ActionScript code with additional con-
trols. Finally, using runtime class mutations, we produce
grammatically-complex compilable code that are rich in run-
time class interactions to ultimately test a few AVMs. Our
evaluation shows that our approach, ScriptGene explores
deeper execution paths and is capable of nearly twice the
code coverage compared to oﬃcial tests (Tamarin). Our
code coverage and discovery of unreported bugs found by
ScriptGene in three diﬀerent versions of AVMs demonstrate
the eﬀectiveness, validity and novelty of our approach.
7. ACKNOWLEDGMENTS
We thank Steven Ergong Zhang from University of Ot-
tawa for his support and suggestions. This work is support-
ed by National Natural Science Foundation of China (NS-
FC) under Grant 60970140, Beijing Natural Science Founda-
tion (4122089) and China Postdoctoral Science Foundation
Project (2011M500416, 2012T50152).
8. REFERENCES
[1] http://help.adobe.com/en_US/FlashPlatform/
reference/actionscript/3/flash/utils/package.
html#describeType().
[2] About ﬂex sdk. http:
//sourceforge.net/adobe/flexsdk/wiki/About/.
[3] Actionscript acceptance tests.
https://developer.mozilla.org/en-US/docs/
Tamarin/Tamarin_Acceptance_Testing/
Actionscript_Acceptance_Tests.
[4] Adobe ﬂash professional cs6.
http://www.adobe.com/en/products/flash.html.
[5] Cygwin. http://www.cygwin.com.
[6] Ecmascript.
http://www.adobe.com/devnet/actionscript/
articles/actionscript3_overview.html.
[7] Flex formatter. http:
//flexformatter.cvs.sourceforge.net/viewvc/
flexformatter/ActionscriptInfoCollector/
ASCollector.g3?view=log.
[8] Ida:about. http:
//www.hex-rays.com/products/ida/index.shtml.
[9] Peach fuzzing platform.
http://peachfuzz.sourceforge.net/.
[10] Running tamarin acceptance tests.
https://developer.mozilla.org/en-US/docs/
Tamarin/Tamarin_Acceptance_Testing/Running_
Tamarin_acceptance_tests.
[11] Scriptgene project.
http://www.nipc.org.cn/project/ScriptGene.
[12] Tamarin. https:
//developer.mozilla.org/en-US/docs/Tamarin.
[13] D. Aitel. The advantages of block-based protocol
analysis for security testing. Immunity Inc., February,
2002.
[14] D. Aitel. An introduction to spike, the fuzzer creation
kit. immunity inc. white paper, 2004.
[15] B. Binde, R. McRee, and T. O ,a´rConnor. Assessing
outbound traﬃc to uncover advanced persistent
threat. SANS Institute. Whitepaper, 2011.
[16] D. Blazakis. Interpreter exploitation. In Proceedings of
the USENIX Workshop on Oﬀensive Technologies,
2010.
[17] P. Godefroid, A. Kiezun, and M. Levin.
Grammar-based whitebox fuzzing. In ACM SIGPLAN
Notices, volume 43, pages 206–215. ACM, 2008.
[18] Google. Fuzzing at scale, 2011.
http://googleonlinesecurity.blogspot.nl/2011/
08/fuzzing-at-scale.html.
[19] C. Holler, K. Herzig, and A. Zeller. Fuzzing with code
fragments. In Proceedings of the 21st USENIX
conference on Security symposium, Security’12, pages
38–38, Berkeley, CA, USA, 2012. USENIX
Association.
[20] H. Li. Understanding and exploiting ﬂash actionscript
vulnerabilities, 2011.
[21] B. Miller, L. Fredriksen, and B. So. An empirical
study of the reliability of unix utilities.
Communications of the ACM, 33(12):32–44, 1990.
[22] P. Oehlert. Violating assumptions with fuzzing.
Security & Privacy, IEEE, 3(2):58–62, 2005.
[23] T. Parr and R. Quong. Antlr: A predicated-ll (k)
parser generator. Software: Practice and Experience,
25(7):789–810, 1995.
[24] pedramamini. Paimei.
http://pedramamini.com/PaiMei/docs/.
[25] P. Purdom. A sentence generator for testing parsers.
BIT Numerical Mathematics, 12(3):366–375, 1972.
[26] G. Shu, Y. Hsu, and D. Lee. Detecting communication
protocol security ﬂaws by formal fuzz testing and
machine learning. Formal Techniques for Networked
467and Distributed Systems–FORTE 2008, pages 299–304,
2008.
[27] J. Smith and R. Nair. Virtual machines: versatile
platforms for systems and processes. Morgan
Kaufmann, 2005.
[28] M. Sutton and A. Greene. The art of ﬁle format
fuzzing. In Blackhat USA Conference, 2005.
[29] M. Sutton, A. Greene, and P. Amini. Fuzzing: brute
force vulnerabilty discovery. Addison-Wesley
Professional, 2007.
[30] B. TURNER. Random c program generator, 2007.
http://sites.google.com/site/brturn2/
randomcprogramgenerator.
[31] D. Yang, Y. Zhang, and Q. Liu. Blendfuzz: A
model-based framework for fuzz testing programs with
grammatical inputs. In Trust, Security and Privacy in
Computing and Communications (TrustCom), 2012
IEEE 11th International Conference on, pages
1070–1076. IEEE, 2012.
[32] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding
and understanding bugs in c compilers. ACM
SIGPLAN Notices, 47(6):283–294, 2012.
468