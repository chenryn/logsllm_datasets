### Figure 6: Classification Accuracy on Test and Trigger Sets

Figure 6 presents the results for both the PRE-TRAINED and FROM-SCRATCH models on the test set and trigger set, after applying four different fine-tuning techniques. The results indicate that while both models achieve nearly the same accuracy on the test set, the FROM-SCRATCH models generally outperform or match the PRE-TRAINED models across all fine-tuning methods. Specifically, the FROM-SCRATCH models reach similar accuracy on the trigger set when each of the four fine-tuning approaches is applied.

This observation holds for both the CIFAR-10 and CIFAR-100 datasets. For CIFAR-100, it appears to be easier to remove the trigger set using the PRE-TRAINED models. 

### Robustness of Trigger Sets After Fine-Tuning

To ensure a fair comparison of the robustness of the trigger sets after fine-tuning, we used the same number of epochs to embed the new trigger set as we did for the original one. The FROM-SCRATCH models were trained using a different trigger set, denoted as TS-ORIG, and then fine-tuned using RTLL and RTAL methods.

Figure 7 summarizes the results on the test set, TS-NEW, and TS-ORIG. We report results for both the FTAL and RTAL methods, along with the baseline results of no fine-tuning (we did not report the results of FTLL and RTLL, as they are considered the easier cases in our setting). The red bars represent the model with no fine-tuning, the yellow bars represent the FTAL method, and the blue bars represent the RTAL method.

The results suggest that the original trigger set, TS-ORIG, remains embedded in the model, as demonstrated by the right columns, and the accuracy of classifying it even improves after fine-tuning. This implies that the model embeds the trigger set in a way that closely aligns with the training data distribution. However, for the new trigger set, TS-NEW, there is a significant drop in accuracy. Embedding TS-NEW can be seen as embedding a watermark using the PRE-TRAINED approach, so this accuracy drop is not surprising and aligns with the results observed in Figure 6.

### Ownership Piracy

In Section 3, we explored the scenario where an adversary attempts to claim ownership of a model that has already been watermarked. For this purpose, we collected a new trigger set of 100 different images, denoted as TS-NEW, and embedded it into the FROM-SCRATCH model. According to the fine-tuning experiments, removing this new trigger set using the above fine-tuning approaches will not affect the original trigger set but will significantly decrease the results on the new trigger set. Given that FROM-SCRATCH models are more robust than PRE-TRAINED models, we report the results for these models only in the rest of the paper.

### Transfer Learning

In transfer learning, we aim to use knowledge gained from solving one problem and apply it to a different problem. For example, we use a model trained on one dataset (source dataset) and fine-tune it on a new dataset (target dataset). We fine-tuned the FROM-SCRATCH model (trained on either CIFAR-10 or CIFAR-100) for another 20 epochs using the labeled part of the STL-10 dataset [12].

Since our watermarking scheme is based on the model's outputs, fine-tuning on a different dataset often changes the number of classes, potentially breaking our method. To address this, we save the original output layer, so during verification, we use the model’s original output layer instead of the new one. This approach makes FTLL and RTLL ineffective, as they update only the parameters of the output layer. FTAL is useful in specific settings where the source and target datasets have related classes, which holds for CIFAR-10 but not for CIFAR-100. Therefore, we report the results only for the RTAL method.

Table 2 summarizes the classification accuracy on the test set of STL-10 and the trigger set after transferring from CIFAR-10 and CIFAR-100. Both models, with and without the watermark, achieve similar accuracy in terms of Prec@1 and Prec@5, while the model without the watermark scores 0% on the trigger set, and the watermarked model scores 100%.

### ImageNet - Large Scale Visual Recognition Dataset

For the final set of experiments, we explored the robustness of our watermarking method on a large-scale dataset, ImageNet, which contains about 1.3 million training images with over 1000 categories. Table 3 summarizes the results for the functionality-preserving tests. Both models, with and without the watermark, achieve similar accuracy in terms of Prec@1 and Prec@5, while the model without the watermark scores 0% on the trigger set, and the watermarked model scores 100%.

Table 4 reports the results of transfer learning from ImageNet to ImageNet (considered as FTAL) and from ImageNet to CIFAR-10 (considered as RTAL or transfer learning). After fine-tuning on ImageNet, the trigger set results remain high, indicating a strong presence in the model. When transferring to CIFAR-10, there is a drop in Prec@1 and Prec@5, but these results are still significant given the 1000 target classes in ImageNet.

### Technical Details

All models were implemented using the PyTorch package [33]. In all experiments, we used a ResNet-18 model, a convolutional neural network with 18 layers [20, 21]. Each model was optimized using Stochastic Gradient Descent (SGD) with a learning rate of 0.1. For CIFAR-10 and CIFAR-100, we trained the models for 60 epochs, halving the learning rate every 20 epochs. For ImageNet, we trained the models for 30 epochs, halving the learning rate every 10 epochs. The batch size was set to 100 for CIFAR-10 and CIFAR-100, and to 256 for ImageNet. For fine-tuning tasks, we used the last learning rate from the training phase.

### Conclusion and Future Work

In this work, we analyzed the practicality of watermarking a neural network using random training instances and random labels. We presented possible black-box and grey-box attacks and showed the robustness of our watermarking approach. We also outlined a theoretical connection to previous work on backdooring such models.

For future work, we aim to define a theoretical boundary for the amount of change a party must apply to a model before claiming ownership. Additionally, we leave open the construction of a practically efficient zero-knowledge proof for our publicly verifiable watermarking construction.

### Acknowledgments

This work was supported by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Directorate in the Prime Minister’s Office.

### References

[References listed here]