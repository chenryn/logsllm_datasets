subtrees rooted in n, using r as the ﬁrst rule, applying
l rules. This information is needed for the Uniform
Generation (Section IV-A).
p(n, l) For each non-terminal n, for each possible length
l, the number of possible subtrees. This represents how
many possible subtrees can be generated when applying
l rules starting from the non-terminal n, or, in other
words, the number of derivation trees with root n and
l edges. This information is also needed for the Uniform
Generation (Section IV-A).
The algorithm for calculating the minimum length for non-
terminals is very similar to the one proposed by Purdom [42].
The other values are calculated using the algorithm proposed
by McKenzie [39].
D. Fuzzing Phase
Figure 2 shows the workﬂow of NAUTILUS during the
fuzzing phase. After generating some initial inputs, the Sched-
uler decides which input should be tried next: either (i) mutate
an existing input with a certain mutation, or (ii) generate
a new input from scratch. The scheduler controls a queue
which contains all generated or mutated inputs that are still
considered interesting, i.e., each of them triggers at least one
transition between basic blocks in the application that no other
input triggers.
The scheduler processes every item in the queue sequen-
tially. Each item in the queue has a state which indicates how
it will be processed when taken from the queue. The state can
be one of these values:
init If an input triggered a new transition, it is saved in
the queue with the init state. When the scheduler
selects an item in the init state, the item is minimized
6
START
Generate
initial inputs
Select next input
i from queue
Scheduler
random
detafl
det
init
state of i?
Minimize i
Run Rules,
Splice, or Random
Mutator on i
Run AFL,
Splice, or Random
Mutator on i
Run Random
Mutator on i
Check for
duplicates
Execute input
in forkserver
New
transitions
No new
transitions
Crash
Add input
to queue
Save input
Discard
input
Fig. 2: Workﬂow of the fuzzing phase.
(Section IV-B). After ﬁnishing the minimization of an
item, its state is set to det.
det Items in the det are mutated using the Rules Mutator, the
Random (Recursive) Mutator, and the Splice Mutator (see
Section IV-C). When the Rules Mutator is done with an
item because no more mutations of that type are possible,
the item moves to the detafl state.
detafl Items in the det are mutated using the AFL Mutator,
the Random (Recursive) Mutator, and the Splice Mutator.
When the AFL Mutator is done with an item, it moves
to the random state.
random This is the ﬁnal state of each entry. Only the Ran-
dom Mutation, Random Recursive Mutation, and Splice
Mutation are applied on this entry. In contrast to AFL,
we do not ﬁnish each stage before we continue with
the next input. Instead NAUTILUS only spends a short
amount of time (typically a few seconds) on each input,
before we continue with the next one. Therefore, we
quickly explore those inputs that are very likely to yield
new coverage while not spending too much time on
unproductive inputs. This allows us to achieve an effect
similar to AFLFast [25].
After an input tree is selected for execution, it is unparsed
to an input string. Then, the target program is run with this
input using a fork server similar to the one used by AFL,
which can start
the target application in a highly-efﬁcient
way. There are three possible states that can follow: (i) the
target application crashes during the execution, then, the binary
representation of the input that caused the crash is saved in a
separate folder, (ii) the input caused the target application to
take a new path, then, the tree representation of the input is
added to the queue, or (iii) the input did not trigger any new
transition and the input is discarded.
VI. EVALUATION
We tested NAUTILUS on four real-world applications and
it found vulnerabilities in all of them. We chose four pro-
gramming language interpreters as targets since these had well
documented grammars. The efﬁciency of our prototype was
also evaluated against other state of the art fuzzers.
Our evaluation aimed at answering the following research
questions:
RQ 1 Can NAUTILUS identify new bugs in real-life applica-
RQ 2 Is NAUTILUS more efﬁcient than other state-of-the-art
tions?
fuzzers?
RQ 3 How much does the use of grammars improve the
fuzzing efﬁciency for target applications with highly
structured inputs?
RQ 4 When using grammars, how much does the use of
feedback increase the fuzzing performance?
RQ 5 Does our complex generation method, which requires
more computational power than a naive approach,
actually increase fuzzing performance, and if so, how
much?
RQ 6 How much does each of the mutation methods used
contribute to ﬁnd new paths?
Section VI-B describes the bugs NAUTILUS found and dis-
cusses RQ1. Section VI-C evaluates the efﬁciency of NAU-
7
QueueTarget
mruby
PHP
Type
Use after free caused by integer overﬂow
Use after free in initialize_copy
Use of uninitialized pointer in hash.c
Segmentation fault in mrb_class_real
Segmentation fault in cfree
Heap buffer overﬂow caused by Fiber::transfer
Stack overﬂow (not ﬁxed yet)
Division by Zero triggered by range() caused by a type conversion.
Segmentation fault in zend_mm_alloc_small
Stack overﬂow caused by using too many parameters in a function call.
ChakraCore Wrong number of arguments emitted in JIT-compiled code
Segmentation fault in out-of-memory conditions
Type confusion
Lua
CVE
CVE-2018-10191
CVE-2018-10199
CVE-2018-11743
CVE-2018-12247
CVE-2018-12248
CVE-2018-12249
none yet
-
-
-
-
-
-
TABLE I: Vulnerabilities found by NAUTILUS in our targets
TILUS and discusses RQ2, RQ3, and RQ4. Section VI-D
evaluates our generation method and discusses RQ5, while
Section VI-E evaluates our mutation methods and discusses
RQ6.
A. Experimental Setup
For our evaluation we chose four widely-used scripting
languages: Ruby, Lua, PHP, and JavaScript:
• For Ruby, we chose the mruby implementation [8] since
it is used by Shopify in their infrastructure and they
have an open bug bounty program (see Section VI-B
for details). We fuzzed various versions of mruby during
the ﬁrst half of 2018. The performance experiments were
performed using the version from Git commit 14c2179
on the ofﬁcial mruby repository [9].
• For Lua, we used version 5.3.4 from the ofﬁcial site [13].
• For PHP, we used version 7.2.6 from the ofﬁcial distri-
• For JavaScript, we chose the ChakraCore implementa-
tion [2], since it was made public more recently. We
used the code from Git commit cf87c70 for performance
measurements and the code from commit 550d2ea for our
fuzzing campaign.
bution network [11].
For our performance evaluation we used 14 identical ma-
chines, each with an Intel Core i5-650 CPU clocked at 3.2
GHz, 4 GB of RAM, and Ubuntu 16.04.4 LTS. Each fuzzer
was only allowed to use one core; we only ran one fuzzer on
any machine at any given time to avoid interferences. Each test
was performed 20 times (12 times for IFuzzer [47]) to enable
a statistical analysis of the results.
We based our grammars on existing ANTLR grammars [6].
We performed a set of changes to improve the performance: as
mentioned earlier, some cases required adding whitespaces, as
they are typically discarded during tokenization and not part of
the grammar. Additionally, we replaced the rules to generate
identiﬁers with a list of strings retrieved from the documenta-
tion or the program itself. Lastly, we often signiﬁcantly shrunk
the grammar for strings and number literals; otherwise, the
fuzzer would spend a lot of time exploring random literals
that add very little interesting information.
B. Vulnerabilities Identiﬁed
To answer RQ1 we evaluated our prototype by fuzzing our
four target applications. Our fuzzer was able to ﬁnd new bugs
in all four, while none of the other fuzzers did during the
evaluation period. All bugs were reported and acknowledged
by the various vendors. The vulnerabilities are summarized in
Table I and described below.
mruby: Mruby is an interpreter for the Ruby programming
language with the intention of being lightweight and easily
embeddable [8]. In total, we found 7 bugs in mruby, including
two use-after-free vulnerabilities2,
two segmentation faults,
one use of an uninitialized pointer, one heap buffer overﬂow,
and a stack overﬂow (see Table I). 6 CVEs were assigned so
far. Reporting these bugs was awarded with a sum of 2,600
USD from the shopify-scripts bug bounty program [16].
Case Study: Finding CVEs. Given the bug bounty program
and the ease of the reporting process, we performed a more
thorough analysis of mruby. We started by inspecting previous
security issues and we noticed that nearly all bugs did not
rely on special strings or non-trivial integers. Therefore, we
built a grammar that only contains a small set of identiﬁers,
integers and strings. We also signiﬁcantly reduced the variance
in the language, e.g., by including only one of the multiple
ways to invoke methods. Using this grammar allowed us to
ﬁnd multiple CVEs.
PHP: PHP is a popular general-purpose scripting language
that is especially suited to web development [11]. NAUTILUS
found three bugs in PHP: a division by zero, a segmentation
fault, and a stack overﬂow (see Table I). The bugs were not
considered security bugs by the PHP developers since they
require “obviously malicious” code. For this reason, no CVEs
where obtained for the three identiﬁed bugs. However, those
bugs could be triggered in a sandboxed PHP environment and
all lead to a crash.
2 In a use-after-free vulnerability, a program calls free on a pointer, then
dereferences the pointer and uses the memory again. An adversary can force
the application to allocate some other data at that address and then run the
faulty code on data of the adversary’s choosing.
8
Target
Baseline
Coverage
Fuzzer
Mean Median
Median New
Coverage Found
ChakraCore
14.7%
mruby
25.7%
PHP
Lua
2.2%
39.4%
NAUTILUS
NAUTILUS - No Feedback
AFL
IFuzzer
NAUTILUS
NAUTILUS - No Feedback
AFL
NAUTILUS
NAUTILUS - No Feedback
AFL
NAUTILUS
NAUTILUS - No Feedback
AFL
34.0% 34.1%
18.6% 18.5%
15.8% 15.8%
15.9% 16.0%
53.7% 53.8%
37.7% 37.8%
28.0% 27.6%
11.7% 12.3%
6.1%
6.1%
2.2%
2.2%
66.7% 66.6%
47.9% 47.8%
54.4% 54.6%
19.4 pp
3.8 pp
1.1 pp
1.3 pp
28.1 pp
12.1 pp
1.9 pp
10.0 pp
3.9 pp
0.0 pp
27.2 pp
8.4 pp
15.2 pp
Std
Deviation
0.60 pp
0.24 pp
0.27 pp
0.20 pp
1.60 pp
0.34 pp
1.28 pp
2.17 pp
0.09 pp
0.00 pp
1.33 pp
1.02 pp
0.54 pp
Skewness Kurtosis
−0.29
−0.44
1.42
0.53
−0.58
0.10
−1.08
0.35
−0.16
−0.38
−1.01
−0.81
4.20
2.36
−0.65
−0.43
−1.69
0.08
−1.40
0.61
−0.11
−0.72
−1.80
0.11
−1.80
2.42