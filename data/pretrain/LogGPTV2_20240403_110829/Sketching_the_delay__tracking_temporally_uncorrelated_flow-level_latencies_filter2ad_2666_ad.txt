ﬂow lengths, we approximate the probability of a small num-
ber of packets (at most x/k) due to i colliding ﬂows at some
site ℓ as Si = Pr[Vℓ  x] under the (ﬁtted) Pareto distribution.
Thus, under our simpliﬁcations, the probability of small
collisions is Q(Rk) = 1 − (nkRP/C)Rk. The optimization
and dimensioning strategy is then immediate by compari-
son with Section 3.1: (i) choose a value k based on targets
for loss resilience; (ii) for a given small target survivabil-
ity rate 1 − ε, calculate the number of rows by rounding
max{1, − log(ε)/k} to the nearest integer.
4. PRACTICAL ENHANCEMENTS
In this section, we introduce a series of enhancements to
the LDS data structure that make it more practical for real
deployment. We start by deﬁning a mechanism to boost
the accuracy for a selected subset of ﬂows of particular in-
terest to network operators. We then investigate how to
parametrize the sampling rates to support a wide range of
loss ratios. Finally, we note that the data structure contains
additional information that can be used to mine other in-
teresting metrics, including per-ﬂow packet loss and heavy
hitter detection.
4.1 Weighting of Flows
As formally analyzed in Sec. 3, the LDS intrinsically pro-
duces better estimates for large ﬂows. This is due to the
fact that, when ﬂows collide, estimates are average delays of
said ﬂows, weighted by the amount of packets.
However, often times, small ﬂows are of interest (a no-
table example are DNS ﬂows, which usually consist of only
one packet per direction). On the other hand, operators
490might be particularly interested in measuring a particular
set of ﬂows with higher accuracy. For example, one could
increase the accuracy for certain subnetworks where critical
services are hosted, or where troubleshooting activities call
for precise examination of network delays (a practical use
case for ﬂow weighting is presented in Sec. 5).
We provide a mechanism to raise the accuracy of ﬂows
at will. This can be very simply accomplished by weighting
ﬂows according to some pre-deﬁned policies driven by the
operator’s desires. Such policies can deﬁne a ﬂow’s weight
according to any information present on packet headers. The
default weight for non-policed ﬂows is deﬁned as 1 for sim-
plicity.
These weights are taken into consideration straightfor-
wardly by slightly modifying the update procedure. When
a packet of a ﬂow f arrives, its headers are examined and
a weight w is determined according the existing weighting
policies. Then, a cell of each row is selected as explained in
Sec. 2.3.4. For each of the cells, their s value is increased by
w times the packet timestamp, while n values are increased
by w (recall that, previously, s values were increased by the
timestamp and n values by 1).
No modiﬁcation is required to the estimation procedure.
When all ﬂow weights are equal, the estimates are identical
to those of Sec. 2.3.4. Otherwise, ﬂows are weighted by their
number of packets times their weight. It shall be noted, how-
ever, that this extra accuracy will always come at expense
of the accuracy of the estimates for ﬂows with lesser weight.
Thus, it is not advisable to heavily increase the weight of
a large percentage of ﬂows, as it will dramatically reduce
the accuracy for all the others. We will analyze the eﬀect of
weighting from a practical standpoint in Sec. 5.
4.2 Multi-Bank LDS
As explained in Sec. 2.3.3, to maximize the collection of
delay samples in front of packet loss, each vLDA should
sample the incoming packet stream at rate L/k, where L
corresponds to its associated number of losses and k to its
length. However, in a real scenario, the absolute number of
losses that each ﬂow will experience is unpredictable, which
raises the question of how to set the sampling rate. On the
one hand, a reasonable amount of loss has to be supported,
which calls for low sampling rates. On the other, aggressive
sampling will miss small ﬂows and fail to collect enough
packets for those that do not experience loss.
Inspired by [17], we propose dividing the counters of the
LDS in several banks, and have each bank sample the in-
coming packets at a diﬀerent rate. This way, at least one of
the banks will suit the actual loss rate of each ﬂow.
A natural way to divide counters in banks is to set a
diﬀerent sampling rate on a per-row basis. In the evalua-
tion provided in Sec. 5, we show that conﬁguring one row
for worst-case loss scenarios, and maintaining increasingly
higher sampling rates in the other rows, does not sacriﬁce
accuracy in normal scenarios with low loss, while still provid-
ing protection against high loss. When a ﬂow surpasses the
target worst-case threshold, the LDS will be unable to pro-
vide delay estimates for that ﬂow. In such a case, however,
the data structure can provide an estimate for its number of
lost packets, as will be explained in Sec. 4.3.
This variant of the LDS requires very few changes to the
algorithms detailed in Sec. 2.3. Now each row has an as-
sociated sampling stage, which is also implemented using
pseudo-random hashing to coordinate measurement nodes,
while the estimation procedure only needs to be modiﬁed
to be aware of the sampling rate that each cell has applied.
In particular, packet counts need to be inverted before de-
ciding which LDS cell will be selected to produce a ﬁnal
estimate. After the cell selection procedure, delay estimates
are produced normally.
4.3 Mining Other Estimates
The LDS data structure can be mined to extract addi-
tional information of practical interest to network opera-
tors. Firstly, the data structure can provide per-ﬂow delay
variance estimates by examining the diﬀerences across the
delays recorded in buckets dedicated to a given ﬂow. The
procedure to obtain this estimate was originally proposed
and is thoroughly described in [17]. Our data structure has
a comparatively smaller number of buckets per ﬂow, but the
same method could be applied to obtain rough delay vari-
ance estimates. This additional estimate can be extremely
useful in practice to detect unexpected delay variations, such
as jitter or delay peaks.
If we ignore the s ﬁelds and focus only on the n ﬁelds of
each cell, the data structure behaves similarly to a Count-
Min Sketch [9]. Consequently, an estimate of the length
of a given ﬂow can be obtained as follows. For each row,
aggregate all vLDA cells to obtain a packet count. Then,
take the minimum of such values. This is the ﬁnal estimate.
It can be easily shown that this estimate is, at best, error
free, and, in the presence of collisions, it can only be greater
than the actual value. This is an interesting property for
certain problems and, especially, for heavy hitter detection.
The accuracy of this technique is analyzed in greater detail
in [9].
Simply by attaching another counter to each cell, where
packet sizes are aggregated, we can also estimate ﬂow sizes
in bytes. Both these new counters and the existing could be
used for heavy hitter detection in terms of bytes or packets
respectively. Additionally, one could obtain crude estimates
for the average packet sizes.
In this paper we divide the
aggregate delay over the number of packets to estimate av-
erage ﬂow delays. Likewise, total ﬂow sizes could be used
to obtain a per-ﬂow average packet size. Finally, we note
that per-ﬂow packet loss can be obtained by comparing the
n ﬁelds of our data structure as collected in sender and re-
ceiver .
5. EVALUATION
With the objective of evaluating the LDS data structure in
a realistic scenario, we deployed two network monitors in an
operational network. For the sake of reproducibility, we col-
lected a packet delay trace, rather than directly processing
live traﬃc. We then ran a series of experiments using LDS
and two state of the art techniques that will be described in
Sec. 5.1. We note, however, that all the traﬃc measurement
procedures, including LDS and both reference techniques,
were fast enough to run on-line and, therefore, the results
we present are completely equivalent to live traﬃc analysis.
The ﬁrst monitor was deployed in a 10 Gb/s link that
connects a large research and educational networking con-
sortium to the rest of the Internet. The second was lo-
cated in the 1 Gb/s access link of one University part of
this consortium. We obtained a copy of the traﬃc that tra-
versed both links in both directions and used Endace DAG
491)
s
m
(
y
a
e
d
l
0
2
.
0
0
1
.
0
0
0
.
0
0
1
.
0
−
0
50
100
150
200
250
300
time (s)
Figure 7: Timeseries of a sample of the packet de-
lays, with outbound delays portrayed as negative
values.
)
x
(
n
F
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
inbound
outbound
1e−05
2e−05
5e−05
1e−04
2e−04
5e−04
delay (s)
Figure 8: CDF of the delays for inbound and out-
bound packets.
cards [12] to simultaneously capture packets in both mea-
surement points. We synchronized DAG clocks using the
PTP protocol, which reportedly provides sub-microsecond
accuracies [10] and thus is accurate enough for ﬁne grained
delay measurement.
We then wrote a CoMo module [4] to analyze the trace
and extract, for each packet, its ﬂow identiﬁer, packet identi-
ﬁer, and exact one-way delay. The trace averages 27Kpkts/s
and contains around 7.76 million packets that belong to ap-
proximately 146000 ﬂows.
Figure 7 shows a time series of a sample of the packet
delays for each traﬃc direction. Two features are appar-
ent from this ﬁgure that make inbound traﬃc (i.e., destined
towards the University network) more interesting. First, in-
bound packet delays present higher variability. Second, two
delay modes are clearly appreciable in the inbound traﬃc, as
can be also conﬁrmed in the CDF of the packet delays pre-
sented in Figure 8. Therefore, unless otherwise noted, the
experiments presented next in the evaluation are performed
on the inbound traﬃc.
5.1 Comparison with Existing Methods
The objective of this section is to compare the accuracy
of LDS with the state-of-the-art on per-ﬂow delay measure-
ment. We choose the NetFlow Multi-Point Estimator (MPE)
[19] and the Reference Latency Interpolation (RLI) [18] as
representatives of a recently introduced class of techniques
that exploit temporal delay correlation to reﬁne measure-
ments from a few samples.
The Multi-Point Estimator is conceived as an extension
to NetFlow, and requires routers to use coordinated sam-
pling. Under this assumption, for each sampled ﬂow, there
exist two delay samples from which to estimate the ﬂow de-
lay (NetFlow records include a timestamp of the ﬁrst and
last packet). Additionally, based on the empirical observa-
tion that packets that travel close in time experience similar
delays, the method can interpolate the delay between these
two samples using the NetFlow records of other ﬂows that
start or end within the duration of the measured ﬂow. The
main diﬀerence between MPE and RLI is that, while MPE
relies on a modiﬁed version of NetFlow, RLI injects active
probes to obtain the necessary delay samples and assumes
that packets between two probes experience the same delay.
We evaluate LDS with three diﬀerent conﬁgurations: one
that provisions half as many counters as ﬂows (n/C ≈ 2)
(to obtain a conﬁguration that, as will be discussed, is com-
parable with MPE), while the other two are 10 times larger
and smaller than this reference LDS. Consistently with the
example in Sec. 3, we structure the sketch in 4 rows, which
yields a sketch of 17500 × 4 counters for the ﬁrst conﬁgu-
ration. Given that loss is negligible in our scenario, we set
the vLDA length k = 1, and the sampling rate p = 1. We
analyze the impact of loss in detail in Sec. 5.2.
Figure 9 plots the CDF of the relative error obtained by
each ﬂow in the traﬃc, for diﬀerent ﬂow sizes. The ﬁgure in-
cludes the accuracy of MPE with a sampling rate of 1% and
10%,2 RLI with 1KHz probing, and a simple method that
estimates the delay of each ﬂow to be the average delay of
all packets. The ﬁgure shows that LDS greatly outperforms
both MPE and RLI. The increase in accuracy compared to
MPE can be explained by two primary causes. First, MPE
completely misses a large number of small ﬂows (e.g., more
than 50% with 10% sampling). For these ﬂows, we estimate
their delay as the average delay of all packets, instead of
simply assigning an error of 1. In contrast, LDS can always
obtain an estimate for all ﬂows. Second, for the ﬂows it
does collect, it interpolates the delays using other ﬂows, but
in our case these are not necessarily correlated, as can be
observed in Figure 8. While RLI outperforms MPE, its ac-
curacy is also far from LDS, especially for large ﬂows, and
requires signiﬁcantly more memory and state maintenance.
Figure 9 (left) shows that, as predicted by the analysis,
large ﬂows are very accurately measured. A still notable ac-
curacy for ﬂows of 100 or more packets is also observed in the
middle plot. Figure 9 (right) shows the per-ﬂow accuracy
for all ﬂows, including also those with less than 100 pack-
ets. According to the analysis in Sec. 3, these ﬂows are not
considered to be survivable, since they tend to experience
large collisions. These ﬂows however only account for 20%
of the packets in our trace. Even in this case, the accuracy
of LDS is consistently above the state of the art. This result
shows that the estimate of LDS for unsurvivable ﬂows is in
practice more accurate than just using the average delay of
all packets.
LDS also features better memory usage. For example,
with 10% sampling, MPE captures around 70000 ﬂows, so
(generously disregarding the fact that NetFlow stores ﬂow
keys) it consumes roughly as much memory as the 17500 × 4
LDS. Thus, with the same memory budget, LDS clearly out-
performs MPE in terms of measurement accuracy. Note also
that, even when LDS uses 10 times less memory than MPE
(C = 1750×4), it obtains signiﬁcantly higher accuracy, espe-
cially for medium sized to large ﬂows. LDS also outperforms
2Note that MPE uses sampling to control the memory usage,
while for LDS sampling is only a measure against packet loss.
492)
x
(
n
F
0
.
1
8
.
0
6
.
0