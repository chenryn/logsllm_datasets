title:Machine learning attacks against the Asirra CAPTCHA
author:Philippe Golle
Machine Learning Attacks
Against the Asirra CAPTCHA
Philippe Golle
Palo Alto Research Center
Palo Alto, CA 94304, USA
PI:EMAIL
ABSTRACT
The Asirra CAPTCHA [7], proposed at ACM CCS 2007,
relies on the problem of distinguishing images of cats and
dogs (a task that humans are very good at). The security
of Asirra is based on the presumed diﬃculty of classifying
these images automatically.
In this paper, we describe a classiﬁer which is 82.7% ac-
curate in telling apart the images of cats and dogs used in
Asirra. This classiﬁer is a combination of support-vector
machine classiﬁers trained on color and texture features ex-
tracted from images. Our classiﬁer allows us to solve a
12-image Asirra challenge automatically with probability
10.3%. This probability of success is signiﬁcantly higher
than the estimate of 0.2% given in [7] for machine vision at-
tacks. Our results suggest caution against deploying Asirra
without safeguards.
We also investigate the impact of our attacks on the partial
credit and token bucket algorithms proposed in [7]. The
partial credit algorithm weakens Asirra considerably and we
recommend against its use. The token bucket algorithm
helps mitigate the impact of our attacks and allows Asirra
to be deployed in a way that maintains an appealing balance
between usability and security. One contribution of our work
is to inform the choice of safeguard parameters in Asirra
deployments.
Categories and Subject Descriptors
K.6.5 [Computing Milieux]: Management of Computing
and Information Systems—Security and Protection
General Terms
Security
Keywords
CAPTCHA, reverse Turing test, machine learning, support
vector machine, classiﬁer.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’08, October 27–31, 2008, Alexandria, Virginia, USA.
Copyright 2008 ACM 978-1-59593-810-7/08/10 ...$5.00.
1.
INTRODUCTION
The Asirra CAPTCHA [7], proposed at ACM CCS 2007,
relies on the problem of distinguishing images of cats and
dogs (Asirra stands for “Animal Species Image Recognition
for Restricting Access”). An Asirra challenge consists of 12
images, each of which is of either a cat or a dog. To solve
the CAPTCHA, the user must select all the cat images,
and none of the dog images. This is a task that humans
are very good at. According to [7], Asirra “can be solved
by humans 99.6% of the time in under 30 seconds”. The
usability of Asirra is a signiﬁcant advantage compared to
CAPTCHAs [9] based on recognizing distorted strings of
letters and numbers.
The security of Asirra is based on the presumed diﬃculty
of classifying images of cats and dogs automatically. As re-
ported in [7], evidence from the 2006 PASCAL Visual Object
Classes Challenge suggests that cats and dogs are particu-
larly diﬃcult to tell apart algorithmically. A classiﬁer based
on color features, described in [7], is only 56.9% accurate.
The authors of [7] conjecture that “based on a survey of ma-
chine vision literature and vision experts at Microsoft Re-
search, we believe classiﬁcation accuracy of better than 60%
will be diﬃcult without a signiﬁcant advance in the state of
the art”. With a 60% accurate classiﬁer, the probability of
solving a 12-image Asirra challenge is only about 0.2%.
In this paper, we describe a classiﬁer which is 82.7% ac-
curate in telling apart the images of cats and dogs used in
Asirra. This classiﬁer allows us to solve a 12-image Asirra
challenge with probability 10.3%. This success probability
is signiﬁcantly higher than the 0.2% estimate given in [7] for
machine vision attacks. While our success rate may appear
low in absolute terms, it nevertheless poses a serious threat
to Asirra if additional safeguards are not deployed to pre-
vent machine adversaries from requesting, and attempting
to solve (at virtually no cost), too many CAPTCHAs.
Our classiﬁer is a combination of two support-vector ma-
chine [5] (SVM) classiﬁers trained on color and texture fea-
tures of images. The classiﬁer is entirely automatic, and
requires no manual input other than the one-time labelling
of training images. Using 15,760 color features, and 5,000
texture features per image, our classiﬁer is 82.7% accurate.
The classiﬁer was trained on a commodity PC, using 13,000
labelled images of cats and dogs downloaded from the Asirra
website [1].
We also investigate the impact of our attacks on the partial
credit and token bucket algorithms proposed in [7]. The
partial credit algorithm weakens Asirra considerably and we
recommend against its use. The token bucket algorithm
helps mitigate the impact of our attacks and allows Asirra
to be deployed in a way that maintains an appealing balance
between usability and security. One contribution of our work
is to inform the choice of safeguard parameters in Asirra
deployments.
Beyond this immediate contribution, we also hope that
our paper will contribute to the popularization of machine
learning techniques, for both oﬀensive and defensive pur-
poses, in the security community. Machine learning and
other artiﬁcial intelligence techniques have not so far been
widely used in cryptographic attacks. Yet recent work sug-
gests that these techniques are powerful tools for the crypt-
analyst’s arsenal. SAT solvers, for example, have been used
to ﬁnd collisions in hash functions [13] and defeat authenti-
cation schemes [8]. Object recognition algorithms [14] were
used in very successful breaks of the text-based Gimpy and
EZ-Gimpy CAPTCHAs. The machine learning classiﬁers
of the type used in this paper, likewise, will hopefully ﬁnd
broader applications in computer security.
Organization. We describe our SVM classiﬁers in sec-
tion 2, using color features (section 2.1), texture features
(section 2.2) and both in combination (section 2.3). We
discuss the use of these classiﬁers in attacking Asirra in sec-
tion 3. Section 3.1 investigates the impact of our attacks
on the partial credit and token bucket algorithms of [7]. We
brieﬂy discuss other counter-measures that may help mit-
igate our attack in section 3.2. Finally, we review related
work in section 4 and conclude in section 5.
2. SUPPORT VECTOR MACHINE CLASSI-
FIERS FOR ASIRRA IMAGES
Asirra relies on a large and growing database of some
3,000,000 images of cats and dogs licensed from the adoption
service Petﬁnder.com. The images displayed by Asirra are
250-by-250 pixels. In the majority of images, there is either
a single cat or a single dog. Some images contain multiple
cats or multiple dogs. In a very few images, there is no rec-
ognizable animal, or else there is both a cat and a dog (these
images cannot be classiﬁed according to the rules of Asirra).
Image collection. We collected 13,000 distinct images
from the Asirra implementation publicly available on the
Asirra website [1]. The website serves Asirra CAPTCHAs
that consist of 12 images selected at random (according
to [6]) from the entire Asirra image database. We wrote
a script to automatically refresh the website and download
the 12 images in the new Asirra CAPTCHA obtained after
each refresh. Over the course of a night, our script refreshed
the website approximately 1,100 times and downloaded just
over 13,000 images. To avoid duplicates, every image was
saved in a ﬁle named after a hash of its pixels (we detected
and discarded 6 duplicate images). Other than duplicates,
no images were deleted, ﬁltered or otherwise selected.
The collection of 13,000 images thus obtained is a rep-
resentative, unbiased sample of Asirra images, since “the
Asirra service selects images randomly from [Asirra’s] entire
image database for each challenge” [6]. The Asirra authors
conjecture that “Photos have a wide variety of backgrounds,
angles, poses, lighting, and so forth – factors that make ac-
curate automatic classiﬁcation diﬃcult” [7]. We have every
reason to believe that our subset of 13,000 images oﬀers a
similar diversity of factors.
Manual classiﬁcation. The next step was to manually
classify the 13,000 images in our collection into 3 classes:
Cat, Dog and Other. The Cat and Dog classes are self-
explanatory. The Other class was for images which either
contained no recognizable animal, or contained both a cat
and a dog. Manual classiﬁcation was followed by a manual
veriﬁcation step, in which 159 misclassiﬁed images (1.2% of
the total) were detected and moved to the correct category.
After veriﬁcation, we obtained 6,403 images of cats (49.3%),
6,466 images of dogs (49.7%) and 131 other images (1.0% of
the total). In the rest of our work, we kept only the images
of cats and dogs and discarded the other images.
Building a classiﬁer. We experimented with diﬀerent
color and texture features computed from images. These
features are described in the rest of this section. We trained
a support vector machine (SVM) classiﬁer [5] with each set
of features. SVM classiﬁers were selected for their ability
to extract linear combination of features, their predictive
power, and their computational scalability. We refer the
reader to [10] for an excellent introduction to SVM (chapter
12), and a comparison of the characteristics of SVMs and
other learning methods (page 313). In short, a SVM is a su-
pervised learning method which constructs an optimal linear
boundary (or separating hyperplane) between two classes.
This hyperplane is optimal in the sense that it maximizes the
distance, or margins, between the hyperplane and the two
classes on each side of it (an error penalty accounts for mis-
classiﬁed points, when the two classes are not perfectly lin-
early separable). The power of SVM classiﬁers comes from
the fact that the linear boundary is not computed directly in
feature space, but in a transformed, higher-dimensional ver-
sion of the feature space. The transformation is represented,
loosely speaking, by a kernel function. Linear boundaries in
the transformed space produce non-linear boundaries when
mapped back to the original feature space.
Measuring accuracy. We measured the accuracy of our
SVM classiﬁers using 5-fold cross-validation on random sub-
sets of our image collection. Cross-validation operates by
dividing a subset of images into 5 randomly chosen parti-
tions; 4 of these partitions are used for training while the
remaining one is used for validation. We report results us-
ing subsets of various sizes (5,000 and 10,000 images), to
show the inﬂuence of the size of the training sample on the
accuracy of our classiﬁer. The accuracy reported for our
classiﬁers in the following sections is the average accuracy
(and its standard deviation) over the 5 experiments of 5-fold
cross-validation. We note that all our subsets of images, and
all the partitions used for cross-validation were generated at
random to avoid any bias that might aﬀect our results.
SVM implementation. We trained our SVM with a radial
basis kernel. This kernel deﬁnes the inner product of two
feature vectors v and v(cid:48) as
K(v, v
(cid:48)
) = exp (−γ|v − v
(cid:48)|2).
The parameter γ was tuned with 5-fold cross-validation to
approximately achieve the best test error performance. We
found that γ = 10−3 worked well for color features and
γ = 10−1 worked well for texture features. We used the LIB-
SVM [3] Java implementation of SVM. We rewrote parts of
Feature set
F1
F2
F3
F3
Color features
N
1
3
5
5
Ch Cs Cv
10
10
8
10
6
10
10
6
10
8
6
6
# Images
# features
1,000
5,760
9,000
9,000
Total
5,000
5,000
5,000
10,000
Training set
4,000
4,000
4,000
8,000
mean
67.3 %
74.6 %
74.6 %
75.7 %
Classiﬁer accuracy
stdev
1.6
1.1
0.6
0.7
Table 1: Accuracy of SVM classiﬁers trained on color features extracted from Asirra images. The color
features are described in section 2.1. The accuracy of the classiﬁer is the fraction of cat and dog images
classiﬁed correctly in the test set.
Color features
# Images
Feature set
F1 ∪ F2 ∪ F3
F1 ∪ F2 ∪ F3
# features
15,760
15,760
Total
5,000
10,000
Training set
4,000
8,000
Classiﬁer accuracy
stdev
mean
76.3 %
77.1 %
0.9
0.6
Table 2: Accuracy of SVM classiﬁers trained on a combination of color features.
the LIBSVM library to make more economical use of mem-
ory for vectors of boolean features. All computations were