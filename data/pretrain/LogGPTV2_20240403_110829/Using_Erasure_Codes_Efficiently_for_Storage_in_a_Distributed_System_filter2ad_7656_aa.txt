title:Using Erasure Codes Efficiently for Storage in a Distributed System
author:Marcos Kawazoe Aguilera and
Ramaprabhu Janakiraman and
Lihao Xu
Using Erasure Codes Efﬁciently for Storage
in a Distributed System∗
Marcos K. Aguilera
HP Laboratories
1501 Page Mill Road MS 1250
Palo Alto, CA, USA
Ramaprabhu Janakiraman
Washington University
Saint Louis, MO 63130, USA
Dept. of Computer Science and Engineering
Dept. of Computer Science and Engineering
Lihao Xu
Washington University
Saint Louis, MO 63130, USA
Abstract
Erasure codes provide space-optimal data redundancy to protect
against data loss. A common use is to reliably store data in a
distributed system, where erasure-coded data are kept in different
nodes to tolerate node failures without losing data. In this paper,
we propose a new approach to maintain ensure-encoded data in a
distributed system. The approach allows the use of space efﬁcient k-
of-n erasure codes where n and k are large and the overhead n−k is
small. Concurrent updates and accesses to data are highly optimized:
in common cases, they require no locks, no two-phase commits, and
no logs of old versions of data. We evaluate our approach using an
implementation and simulations for larger systems.
1. Introduction
Storage systems are quickly growing in size through the use of
more and bigger disks, and through distribution over a network. With
larger systems, the chance of component failure also increases, so
techniques to protect data become more important. Single parity used
in RAID systems no longer provides sufﬁcient protection in all cases
[1], and k-way replication is much too wasteful in storage space,
even for small k. Therefore, new schemes are needed to protect data
against multiple failures in a distributed storage system.
Erasure codes [2] have been used traditionally in communication
systems, and more recently in storage systems as an alternative
to replication (e.g., [3], [4], [5], [6]). Proper use of erasure codes
provides greater space efﬁciency and ﬁne tunable levels of protection,
at the cost of greater complexity. An (n, k) MDS erasure code, or
simply k-of-n code, encodes k blocks of data into n > k blocks—
which we call a stripe—such that any k blocks in the stripe can
recover the original k blocks. By storing each block in a separate
node, data are protected against the simultaneous failure of up to
n − k nodes.
A general challenge of distributed storage is to provide data consis-
tency while allowing failures and concurrent access. At the same time,
one would like to get reasonable performance, to scale with number
of clients, and to allow expansion of storage capacity at low cost.
These difﬁculties are well-recognized, understood, and reasonably
addressed for replication-based storage. For erasure-coded storage,
however, different schemes are still being proposed (e.g., [5], [6]), as
researchers explore new ways to deal with the additional complexity
created by erasure codes. Roughly speaking, this complexity is caused
by an inherent coupling of data in erasure codes, as we explain deeper
in the paper.
∗This work is partially supported by NSF grants CCR-0208975, CNS-
0322615, and IIS-0430224.
This paper proposes a new protocol and scheme to use erasure
codes for distributed storage. Our scheme has the following features:
• High concurrency: It allows concurrent updates of blocks, includ-
ing blocks coupled by the erasure code.
• Consistency: It ensures a strong type of consistency despite
concurrent updates, and crashes of both storage nodes and clients.
• Optimized for common cases: It
is highly optimized for the
common cases where no failures occur; in such cases, it does not
require use of expensive synchronization. A read requires only a
round-trip to a storage node, and a write requires only a round-trip
to each storage location that needs to be changed according to the
erasure code; this is true even when there are concurrent operations.
• Good performance with highly-efﬁcient erasure codes: The
scheme performs well with k-of-n Reed-Solomon codes where k and
n are large and n − k is small—these are the codes with the best
space efﬁciency for a given fault resiliency.
• Online recovery: When failures occur, recovery does not require
to suspend read and write operations.
• Small space overhead: It keeps a small amount of overhead data
at storage nodes—a goal consistent with using erasure codes to save
space.
• Thin servers: It uses thin storage nodes that implement very simple
functionality, thus following the principle of moving functionality
to clients. This implies better scalability and lower costs to expand
storage capacity when new servers are added.
To evaluate our approach, we built a prototype of a distributed and
reliable storage service. The service comprises a set of storage nodes
accessible to clients via a network, where clients read and write data
using our protocols. We also use simulations to study the performance
of larger systems.
Limitations of our approach
As a trade-off for its features, our scheme has four limitations:
• It is tailored for linear erasure codes, like Reed-Solomon codes,
where redundant blocks are updated with commutative operations.
• It uses the redundancy of erasure codes solely for fault tolerance,
not to improve read performance. For instance, our read performance
is very similar to that of a system with no data redundancy. This
is consistent with our motivation of supporting highly-efﬁcient codes
where n − k ≤ k (number of redundant blocks no greater than data
blocks). Systems that use erasure code to improve read performance
tend to have n − k >> k, and provide weak consistency or assume
data are immutable.
• The write throughput of clients decreases as n−k grows. However,
this can be avoided if broadcasts are available.
• It can tolerate at most tp client failures, where tp is a chosen
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
failure threshold. If there are tp +1 client failures and a storage crash,
data may be lost. For example, consider the following scenario: (a)
tp + 1 clients are simultaneously writing to the same stripe S, and (b)
a network partition, such as caused by a switch failure, causes those
tp + 1 clients to be permanently disconnected. This results in tp + 1
client partial writes that make the system vulnerable: a subsequent
storage crash in this conﬁguration cannot be tolerated. We mitigate
this problem by using a monitoring mechanism that efﬁciently detects
and ﬁxes partial writes, to restore full recoverability and reduce the
window of vulnerability. After any number of client failures, if this
mechanism executes before a storage crash, data are safe again.
Related work and protocol comparison
The closest related work are the distributed protocols proposed
by FAB [5] and Goodson et al [6]. FAB uses erasure codes in
a distributed disk array built from low-cost commodity computers.
The FAB protocol tolerates crash failures, ensures a strong form of
consistency, and allows concurrent writes and reads. Concurrent writes
to blocks in the same erasure code stripe return an exception. Servers
have non-volatile memory and keep a log with old versions of data,
which is periodically garbage collected.
Goodson et al also propose a protocol (GWGR) for distributed
storage using erasure codes. The GWGR protocol tolerates Byzantine
clients and storage nodes, ensures a strong form of consistency, and
allows concurrent writes and reads. GWGR keeps a log with old
versions of data for recovery, which is periodically garbage collected.
GWGR only allows writes to modify the entire erasure code stripe at
once; to modify individual blocks, it is necessary to read its stripe,
and write it back. Doing so, however, has a performance cost and
does not ensure consistency of concurrent updates to blocks in the
same stripe.
Fig. 1 shows a performance comparison between our protocol
(AJX-*), FAB and GWGR. Our protocol has at least as good latency,
number of messages, and bandwidth. With FAB and GWGR, every
write needs to contact all storage nodes in the erasure code stripe,
and so these protocols perform poorly for random I/O, especially
with highly-efﬁcient erasure codes that have large k and n, and small
p = n − k. These are the codes with best space efﬁciency for a given
fault resiliency. For sequential I/O, all protocols allow pipelining of
requests; with the optimizations of Section 6, we believe that our
protocol is competitive, as shown by the results of Section 6. Thus, the
advantages of our protocol over FAB and GWGR are (1) it supports
well highly-efﬁcient erasure codes, (2) it does not keep old versions
of data at storage nodes (less space overhead), and (3) it allows for
thin storage servers. An advantage of FAB and GWGR is that they
can tolerate any number of client failures by using the log of old
versions of data.∗ Our protocol keeps no log and tolerates only a
chosen number of client crashes.
Myriad [4] proposes erasure codes for disaster tolerance. Updates
of redundant disks do not happen during a write, but are instead de-
ferred and done in batches using two-phase commit. Zhang and Lian
[7] also propose a general scheme to use erasure codes for distributed
storage. However, this scheme does not handle concurrent updates;
instead, it assumes some external mechanism, like a transaction for
each operation. This appears to be expensive, but no performance data
is provided. LH ∗
RS [8] use erasure codes to implement an expandable
and distributed data structure, where redundancy increases with the
amount of data, to ensure a minimum availability. However, the paper
∗In FAB, because client and storage nodes are colocated, this requires some
fraction of the nodes to restart after failing.
=client node
=storage node
=disk or other
storage device
separated client and
storage nodes
colocated client and
storage nodes
Fig. 2. System with client nodes and storage nodes that
communicate via a network. Storage nodes may be
thin or powerful devices; client nodes are computers
running applications. We support both separated or
colocated storage and client nodes.
does not address failures of clients or recoveries concurrent with
client updates. In some systems, erasure cores are used for archival
of immutable data (e.g., [3]). When data is immutable, there are few
concurrency issues, and so much simpler protocols are needed than
what we propose.
The rest of this paper is organized as follows. In Section 2 we
explain our assumptions and goals. Section 3 explains our design and
protocols. We consider the protocol’s failure resilience in Section 4. In
Section 5 we validate our approach in two ways: An implementation
is described in Section 5.1 and simulations of larger systems are
described in Section 5.2. We give results in Section 6. All protocol
correctness proofs are omitted due to space limitations; they can be
found in [9].
2. Assumptions and Goals
We consider a distributed system where client nodes wish to store
data at a set of storage nodes that are reachable through a fast network,
like a local area network. A storage node may be a dedicated server
with lots of memory, many processors, and a set of one or more disks
or other storage connected to it. Or it could be a very thin passive
device with a network interface, a storage interface, some memory,
and some storage connected to it (and not much more beyond that).
Storage devices have a standard ﬁxed block size (e.g., 512 bytes) used
as the minimum quantum of data transfer. A client node is a computer
running applications that need to read and write data stored at the
storage nodes. Client nodes have reasonable computational power and
a network bandwidth that is not extremely limited. A client node
may be colocated with a storage node, but the client node may need
to access storage nodes not colocated with it. This might occur if
a machine is powerful enough to host both applications and shared
storage.
Each node has a network identity, like an IP address, used to
communicate with other nodes. We assume that each client node
can obtain the identities of the nodes providing the storage service.
However, client nodes may not know about each other. As a result,
two or more client nodes may issue storage operations concurrently.
Most likely, those operations are on different locations most of the
time or always (e.g., [5]). However, in some rare cases, two concurrent
operations may try to access the same data. In those cases, the result
should not be garbage.
Client and storage nodes are subject to fail-stop failures [10], which
causes a node to halt its execution, and the node’s halted state can
be detected by other nodes if necessary. If a storage node fails, it
may never recover, in which case the data that it stored is lost. We
assume that failures are not extremely frequent, and if they occur,
it is acceptable for the system performance to temporarily degrade a
little.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Scheme
min r/w granularity
read lat. (round trips)
write lat. (round trips)
# msgs for read
# msgs for write
read bandwidth
write bandwidth
1
2
2
1
2
2
AJX-par
1 block
AJX-bcast
1 block
2(p + 1)
p + 3
B
(p + 2)B
B
3B
AJX-ser
1 block
1
p + 1
2
2(p + 1)
B
FAB
1 block
1
2
2k
4n
B
(p + 2)B (2n + 1)B
GWGR
k blocks
1
2
2n
4n
nB
nB
notes
k determined by erasure code
p much smaller than n
smaller is better
(B = block size)
Fig. 1. Performance comparison in most likely (failure-free) executions using k-of-n erasure code (p = n − k). AJX-*
are the protocols in this paper: -par uses parallel updates, -bcast uses broadcast (if available), and -ser uses serial
updates (cf Section 4).
Our goal is to use erasure codes to provide redundancy to protect
stored data against the above failures, while keeping space overhead
small. Our goal is not to use redundancy to increase read throughput—
this goal is often at odds with having small space overhead and
supporting concurrent updates with strong consistency. We would like
to support a wide variety of k-of-n erasure codes, especially highly
efﬁcient ones where n and k are large, but the overhead n − k is
small.
We also want
to hide from applications the intrinsics of how
erasure codes are used. Target applications include operating systems,
databases, distributed ﬁle servers, or other higher-level services that
require block storage. These applications access data through a block
interface that support read-block and write-block operations. We
prefer that the block size be one of the values commonly used; in
fact, we prefer that all peculiarities of erasure codes be hidden from
applications. However, these peculiarities may be known by some
low-level module running at the client nodes below the application
level.
3. Design and Algorithms
While designing our scheme, we chose to follow three well-known
principles. Shift functionality to clients: Client nodes do active work,
while storage nodes are thin, passive servers. This choice tends to
provide better scalability, simplify crash recovery, and decrease the
cost of adding more storage nodes to grow storage. Optimize for
common cases, simplify rare cases: We avoid expensive mechanisms
like locks or two-phase commits, in the common cases when there
are no failures. In rare cases when failures do occur, we simplify
the design using strong coordination via locks. Hide intrinsics of
mechanism being implemented: The choice of erasure code does not
affect the service interface provided to applications. For example,
larger erasure-code stripes does not require applications to use larger
block sizes.
3.1. Consistency
In the presence of concurrency and failures, we provide a reason-
ably strong consistency guarantee—the same as provided by regular
registers [11] generalized to multiple writers [12]. Roughly speaking,
it ensures that a read never returns a value that was never written, or
a value that was overwritten by another write. If a write is concurrent
with a read,
the read may return the value of the write or the
previously written value. If multiple writes are concurrent with a read,
the read may return the value of any of the writes or the previously
written value.
3.2. Organization
Our scheme is physically organized in two parts: (1) storage nodes
are conﬁgured to serve simple requests from client nodes, and (2)
client nodes orchestrate the storage nodes to store, retrieve, and
recover data. Logically, the scheme has four components: (1) failure
detection and node remap, (2) read and write algorithm, (3) recovery
algorithm, and (4) garbage collection algorithm. We cover these
components in later sections.
3.3. Brief primer on erasure codes
Roughly speaking, a k-of-n systematic maximum distance separa-
ble (MDS) erasure code [2] takes k data blocks and produces n − k
redundant blocks, such that any subset of k blocks (data or redundant
or mixed) can reconstruct the k data blocks. We consider codes where
the redundant blocks are linear combinations of the data blocks. We
call stripe the combination of the k data blocks and n − k redundant
blocks.
For example, if a and b are data blocks, then we could produce two
redundant blocks a+b and a−b.† Given a stripe consisting of the four
blocks (a, b, a+b, a−b), any subset of two blocks can reconstruct a
and b. For instance, given a+b and b, we can obtain a by subtracting
b from a+b. Therefore, we have a 2-of-4 erasure code, which can
tolerate the loss of any 2 blocks in the stripe. Note how this is more
powerful than 2-way replication with the same space overhead: if we
simply replicate a and b, we get (a, b, a, b); if we later lose both
replicas of a, we cannot reconstruct a.
More technically if b1, . . . , bk are data blocks then,
in a k-
of-n code, each redundant block bk+1, . . . , bn is given by bj =
(cid:1)k
αjibi for j = k + 1, . . . , n, where αji are carefully chosen
constants, and arithmetic is over some ﬁnite ﬁeld, usually GF(2h).
3.4. Challenges of erasure-coded distributed storage
i=1
There are two main reasons why known solutions for replicated
storage cannot be used with erasure codes: (1) erasure codes couple
together different blocks of data, while replicated storage only couples
together replicas; and (2) divergence of erasure-coded data is harder
to detect and correct than divergence of copies in replicated storage.
To illustrate the challenges, suppose that we use the 2-of-4 code of
Section 3.3 to store blocks a and b: each of (a, b, a+b, a−b) are each
kept in a separate storage node. Now suppose client node c1 wishes
to change a to c, while another client node c2 wishes to change b
to d. Here, the updates are to different data, but because the erasure
code couples together c and d, some care is needed with concurrency.
The end result must be (c, d, c+d, c−d), but how do we keep c1
and c2 from clashing? This is easy if we use locks: c1 locks all
four blocks, reads b, and then overwrites the blocks; and c2 proceeds
analogously. But locks are very expensive, and we want to avoid using
them. Moreover, even with locks, if c1 fails before completion then
†Technically,
in this case + and − must be taken over a ﬁeld with
characteristic (cid:1)= 2.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Code for client p:
To READ(i) do { 1 ≤ i ≤ k }
v ← Si.read()
return v
// RPC
To WRITE (i, v) do { 1 ≤ i ≤ k }
w ← Si.swap(v)
pfor j ← k + 1 . . . n do
Sj .add (αji.(v − w))
// RPC
// RPC
return