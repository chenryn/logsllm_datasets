ing roughly m = n1/2 range queries and with L = n1/4 local
client storage. That is,  = 0.25 in these experiments. The size of
each range being queried was randomly selected from a geometric
distribution with mean 100; that is, each range query returned on
average 100 results.
Network. Our main experiments were performed in a local setup,
but with careful measurement of communication and under the as-
sumption that network bandwidth (i.e., amortized communication
size) and latency (i.e., round complexity) would be the bottlenecks
of a remote cloud implementation. In particular, in our network
experiments, we used the tc “trafﬁc control” utility to add speciﬁc
latency durations as well as bandwidth limitations. This allowed
us to test the behavior under controlled but realistic network set-
tings, when we throttled the network slower than 5ms of latency
and 20Mbps bandwidth.
Comparison with Popa et al. We compared our construction ex-
perimentally to that of Popa et al. [35], who had a setting most
similar to ours. Further comparison benchmarks, such as to [30] or
even to ORAMs, might provide further insight, and we leave this
as future work.
For a fair comparison to prior work, we also implemented the
mOPE scheme of [35] in Python3 along with our implementation
of POPE. We followed the description in their work, using a B-tree
with at most 4 items per node to store the encryptions. To get a
fair comparison, we used the same framework as our POPE experi-
ments, with the client that receives sorting and partitioning requests
from the server. In the case of mOPE, each round of communica-
tion consisted of sending a single B-tree node’s worth of cipher-
texts, along with one additional ciphertext to be encoded, and re-
ceiving the index of the result within that sorted order. We acknowl-
edge that our implementation is likely less tuned for efﬁciency than
that of the original authors, but it gives a fair comparison to our
own implementation of POPE. It is also important to note that our
communication cost measurements depend only on the algorithm
and not on the efﬁciency of the implementation.
Measuring communication and running time. When our tests
measured communication (in terms of rounds and total ciphertexts
transferred) and running time, we did not include the cost of the
server sending the search results; this is inherent in the operation
being performed and would be the same for any alternative imple-
mentation.
5.2 Experimental workloads
Local setting: huge data, various search patterns.
In our main
experiments, where we wanted to scale the number of database en-
tries from one thousand up to 100 million entries, we used synthetic
data consisting of random pairs of English words for both label and
payload values. For these experiments we also did not actually
transfer the data over a network, but merely measured the theo-
retical communication cost. This allowed us to test a much wider
range of experimental sizes, as we found a roughly 10x slowdown
in performance when running over a network, even with no throt-
tling applied.
We were able to run experiments with POPE up to 100 million
entries, limited only by the storage space available on our test ma-
chine. We observed no signiﬁcant change in per-operation perfor-
mance after one million entries, indicating our construction should
scale well to even larger datasets.
5.3 Local Setting
Experimental communication costs. Figures 6 and 7 show the
communication costs, the total number of rounds of communica-
tion, and the average number of ciphertexts transferred per opera-
tion. The number of insertions n is shown in the plots, and for each
n searches allowing L = n1/4
experiment we performed m =
entries stored in temporary memory on the client.
√
The actual size of each range being searched was, on average,
100 database entries. While the distribution of searches does not
affect the running time of mOPE, for POPE we varied among three
distributions of the random range queries: (i) uniformly distributed
queries, (ii) search queries all “bunched” at the end after all in-
sertions, (iii) a single, repeated query, performed at random inter-
vals among the insertions. According to our theoretical analysis,
the “bunched” distribution should be the worst-case scenario and
the repeated query should be the best-case. In practice we did not
see much difference in performance between bunched or random
queries, though as expected, we observed improved performance
for the repeated query case.
Networked setting: real salary data. To test performance over a
realistic network with latency and bandwidth restrictions, we used
the California public employee payroll data from 2014, available
from [1], as a real dataset on which to perform additional experi-
ments. This dataset lists payroll information for roughly 2.3 mil-
lion public employees. We used the total salary ﬁeld as our “label”
value (on which range queries are performed), and the names as the
payload values.
We were not able to complete any test runs of the mOPE using
actual network communication over the salary dataset; based on
partial progress in our experiment we estimate it would take several
days to complete just one test run of this experiment using mOPE
and actual network communication with our Python implementa-
tion.
Figure 6: Total rounds of communication for POPE and mOPE, plotted in
log/log scale according to total number of insertions n. Lower is better. The
number of range queries in all cases was
√
n.
Figure 7: Amortized communication costs for POPE and mOPE, according
to total number of insertions n. Lower is better. The number of range
queries in all cases was
√
n.
As these ﬁgures demonstrate, the round complexity for POPE,
which is constant per range query, is several orders of magnitude
less than that of mOPE. Furthermore, when averaged over all op-
erations, the number of ciphertexts transferred per operation for
POPE is roughly 7 in the worst case, whereas for mOPE this in-
creases logarithmically with the database size.
√
Experimental running time. The per-second operations counts,
n range queries,
for our main experiments with n insertions, m =
and L = n1/4 client-side storage, are presented in Figure 8. For
POPE, the performance increases until roughly 1 million entries,
after which the per-operation performance holds steadily between
50,000 operations per second with random, distinct queries, and
110,000 operations per second with a single, repeated query.
For one million entries and using our Python implementation
without parallelization, we achieved over 55,000 operations per
second with POPE vs.
less than 2,000 operations per second for
mOPE, without even accounting for the network communication.
Our POPE construction is well-suited particularly for problems
with many more insertions than range queries; indeed, the O(1)
theoretical performance guarantees hold only when mL < n. Fig-
ure 9 shows the effects of varying numbers of range queries on
POPE performance. Although the performance of POPE clearly
degrades with increasing numbers of queries performed, this ex-
periment shows competitive performance compared to mOPE even
when m = n.
Figure 8: Operations performed per second for POPE and mOPE. Higher
is better. The number of range queries in all cases was
n.
√
Figure 9: Degradation in POPE performance with increasing number of
queries, measured in operations per second. Higher is better. In all experi-
storage at L = 32. For these choices, 210 ≈ √
ments, the number of insertions n was ﬁxed at 1 million, and the client-side
n queries is as shown in
prior ﬁgures, and our O(1)-cost analysis holds up to m ≈ 215.
5.4 Experimental Network Effects
We tested the effects of varying network latency and bandwidth
using the California public employees payroll data as described
above. Our workload consisted of all 2,351,103 insertions as well
as 1,000 random range queries at random points throughout the in-
sertions. Each range query result size was ﬁxed at 100 entries.
Figure 10 shows the effects of latency on the POPE implemen-
tation. With less than 5ms of latency, the cost is dominated by that
of the POPE computation and other overhead. Beyond this level,
the total runtime scales linearly with the latency. Note that 10ms
to 30ms represents typical server response times within the same
continent over the Internet.
Figure 11 shows the effects of bandwidth limitations on our con-
struction. Without any latency restriction, we limited the band-
width between 1 and 20 megabits per second (Mbps), which is the
typical range for 4G (on the low end) and home broadband Inter-
net (on the high end) connections. We can see that, past roughly 10
Mbps, the other overhead of the implementation begins to dominate
and there is no more signiﬁcant gain in speed.
6. RELATED WORK
Order-Preserving and Order-Revealing Encryption. Order-
preserving encryption (OPE) [3, 7, 8] guarantees that enc(x) <
enc(y) iff x < y. Thus, range queries can be performed directly
over the ciphertexts in the same way that such a query would be
101102103104105106107108109103104105106107108total rounds of communicationnumber of entriesmOPEPOPE, random queriesPOPE, bunched queriesPOPE, repeated queries 0 10 20 30 40 50 60103104105106107108ciphertexts transfered per opnumber of entriesmOPEPOPE, random queriesPOPE, bunched queriesPOPE, repeated queries 0 20 40 60 80 100 120 140 160103104105106107108thousands of ops per secondnumber of entriesPOPE, repeated queriesPOPE, bunched queriesPOPE, random queriesmOPE 0 10 20 30 40 50 60210212214216218220thousands of ops per secondnumber of queriesPOPEmOPESymmetric searchable encryption (SSE) was ﬁrst proposed by
Song, Wagner, and Perrig [39] who showed how to search over
encrypted data for keyword matches in sub-linear time. The ﬁrst
formal security deﬁnition for SSE was given by Goh [23], Curt-
mola et al. [16] showed the ﬁrst SSE scheme with sublinear search
time and compact space, while Cash et al. [13] showed the ﬁrst SSE
scheme with sublinear search time for conjunctive queries. Recent
works [34, 13, 19] achieve performance within a couple orders of
magnitude of unencrypted databases for rich classes of queries in-
cluding boolean formulas over keyword, and range queries. Of par-
ticular interest is the work of Hahn and Kerschbaum [27] who show
how to use lazy techniques to build SSE with quick updates. We
refer interested readers to the survey by Bösch et al. [12] for an
excellent overview of this area.
Oblivious RAM [24, 38, 40, 44] and oblivious storage schemes
[26, 4, 18, 32] can be used for the same applications as OPE and
POPE, but achieve a stronger security deﬁnition that additionally
hides the access pattern, and therefore incur a larger performance
cost than our approach.
Finally, we note that techniques such as fully-homomorphic en-
cryption [22], public-key searchable encryption [9, 11, 37], and
secure multi-party computation [46, 6, 25] can enable searching
over encrypted data while achieving the strongest possible security.
However, these approaches would require performing expensive
cryptographic operations over the entire database on each query
and are thus prohibitively expensive. Very recently cryptographic
primitives such as order-revealing encryption [10], as well as gar-
bled random-access memory [21], have offered the potential to
achieve this level of security for sub-linear time search. However,
all constructions of these primitive either rely on very non-standard
assumptions or are prohibitively slow.
Lazy data structures and I/O complexity. Our POPE tree is is
similar in concept to the Buffer Tree of [5]. Their data structure de-
lays insertions and searches in a buffer stored at each node, which
are cleared (thus executing the actual operations) when they be-
come sufﬁciently full. The main difference here is that our buffers
contain only insertions, and they are cleared only when a search
operation passes through that node.
We also point out an interesting connection to I/O complexity
regarding the size of local storage. In our construction, as in [36],
the client is treated as an oracle to perform comparisons of cipher-
texts. If we think of the client’s memory as a “working space” of
size L, and the server’s memory as external disk, then from [2] it
can be seen that performing m range queries on a database of size
n ≥ m requires a total transfer bandwidth of at least Ω(m logL m)
ciphertexts. (This is due to the lower bound on the I/O complexity
of sorting, and the fact that m range queries can reveal the order
of a size-m subset.) In particular, this means that the mOPE con-
struction from [36] cannot be improved without either limiting the
number of queries, or increasing the client-side storage, both of
which we do for POPE.
Acknowledgments We thank Jonathan Katz and David Cash for
recommending the importance of the improved security of POPE.
We also thank the anonymous reviewers for their useful comments.
Daniel S. Roche’s work is supported in part by Ofﬁce of Naval
Research (ONR) award N0001416WX01489 and National Science
Foundation (NSF) awards #1319994 and #1618269. Daniel Apon’s
work is supported in part by NSF awards #1111599, #1223623, and
#1514261. Seung Geol Choi’s work is supported in part by ONR
awards N0001416WX01489 and N0001416WX01645, and NSF
award #1618269. Arkady Yerukhimovich’s work is sponsored by
the Assistant Secretary of Defense for Research and Engineering
under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-
Figure 10: Total running time for 2.3 million insertions and 1000 random
range queries, running over a network with varying artiﬁcially-induced la-
tency times.
Figure 11: Total running time for 2.3 million insertions and 1000 random
range queries, running over a network with varying bandwidth limitations.
performed over the plaintext data. However, OPE comes with a
security cost. None of the original schemes [3, 7] achieve the
ideal security goal for OPE of IND-OCPA (indistinguishability un-
der ordered chosen-plaintext attack) [7] in which ciphertexts reveal
no additional information beyond the order of the plaintexts.
In
fact Boldyreva et al. [7] prove that achieving a stateless encryp-
tion scheme with this security goal is impossible under reasonable
assumptions. The existing schemes, instead, either lack formal
analysis or strive for weaker notions of security which have been
shown to reveal signiﬁcant amount of information about the plain-
text [8]. The ﬁrst scheme to achieve IND-OCPA security was the
order-preserving encoding scheme of Popa et al. [35].
A related primitive to OPE is order-revealing encryption (ORE)
[7], which provides a public mechanism for comparing two en-
crypted values and thus also enables range searches over encrypted
data. (Note, OPE is the special case where this mechanism is lex-
icographic comparison,) The ﬁrst construction of ORE satisfying
ideal security [10] was based on multi-linear maps [20] and is thus
unlikely to be practical in the near future. An alternative scheme
based only on pseudorandom functions [15], however, has addi-
tional leakage that weakens the achieved security.
OPE alternatives. In addition to OPE there are several other lines
of work that enable searching over encrypted data. Typically, these
works provide stronger security than provided by OPE; in particu-
lar they do not reveal the full order of the underlying data as hap-
pens with OPE. However, the additional security guarantees come
at a signiﬁcant performance cost with even the latest schemes being
one to two orders of magnitude slower than the latest OPE-based
implementations [34, 19].