### Priming and Starring Zeros in the Hungarian Algorithm

In Step 4, let \( Z_0 \) represent the uncovered primed zero found. Let \( Z_1 \) denote the starred zero in the column of \( Z_0 \) (if any). Let \( Z_2 \) denote the primed zero in the row of \( Z_1 \) (there will always be one). Continue this process until the series terminates at a primed zero that has no starred zero in its column. Unstar each starred zero in the series, star each primed zero in the series, erase all primes, and uncover every line in the matrix. Then, return to Step 3.

### Step 6: Adjusting the Cost Matrix

Add the value found in Step 4 to every element of each covered row, and subtract it from every element of each uncovered column. Return to Step 4 without altering any stars, primes, or covered lines.

### The Hungarian Algorithm

Figure 1 outlines the six steps of the Hungarian Algorithm as described in [23]. The algorithm takes the cost matrix as input and manipulates rows and columns through addition and subtraction to find a set of starred zero elements that represent the optimal assignment. During the algorithm, rows and columns are covered, and zeroes are starred and primed to indicate special status. Upon completion, there are \( N \) starred zeros. A starred zero at location \((i, j)\) means that the optimal solution to the Assignment Problem schedules application \( i \) to run on core \( j \). The Hungarian scheduler then uses the best assignment for the simplified problem as the schedule for the steady-state phase.

### Scheduling Algorithms

| Algorithm | Exploration Phase Complexity |
|-----------|------------------------------|
| Randomized | None                         |
| Round Robin | None                        |
| Hungarian | \( O(N^3) \)                 |
| Global Search | \( O(N) \)                |
| Local Search | \( O(N) \)                 |

### Iterative Optimization Algorithms

Our other approach involves using iterative optimization algorithms inspired by artificial intelligence research [25, 27]. These algorithms are highly suited to this scheduling task because they are generally simple to implement, have low computational requirements, and are extremely effective in practice. The simplest search algorithms are greedy: they avoid searching in directions that initially appear to have performance slowdowns or power inefficiencies, even if they may hold promise in the future. Therefore, these greedy algorithms may get stuck in local minima. However, in practice, greedy algorithms are quite effective in certain problem domains and are often used due to their simplicity. In this paper, we study global search and local search.

### Global Search Algorithm

In the global search (Figure 2), the processor is configured into a new random schedule in each interval of the algorithm. The operating system keeps track of the best configuration thus far and employs this configuration during the longer, steady-state phase. Figure 2 illustrates how global search operates on a sample four-core chip multiprocessor. Global search has the advantage of rapidly exploring a broad range of configurations in a large search space such as a CMP with many cores. However, it may not arrive at a near-optimal solution.

### Local Search Algorithm

Local search defines a neighborhood of assignment options related to the current configuration. During each exploration interval, a member in the neighborhood of the current assignment is selected as the next assignment. If this new assignment performs better than the original, it is kept; otherwise, local search reverts to searching further in the neighborhood of the original solution. We define the neighborhood of a scheduling configuration as all schedules that can be derived from the original schedule through some fixed number of pair-wise swaps.

### One-Swap and Two-Swap Local Search

In our results, we explore how many pair-wise swaps the algorithm should make per interval to determine the best setting. The advantage of selecting among a neighborhood of configurations derived from a few or even just one swap is that assignments in close proximity to the original are likely to have similar performance. This leads to a more gradual search that steadily improves the solution and avoids large changes that could lead to poor results. On the other hand, increased swapping more rapidly explores the search space of assignments. Figure 3 demonstrates how local search works when one swap is performed per iteration. Figure 4 shows a two-swap version of local search and highlights a key improvement in our algorithm, which allows some of the swaps from an interval to be retained while others are discarded. We also implemented a version of local search that uses hill climbing [27] to escape local minima. However, we found that the improvements over greedy search were minimal, indicating that the algorithms were not greatly impacted by local minima.

### Methodology

Our simulation infrastructure is based on the SESC simulator [26]. We improved the power and thermal modeling by augmenting SESC with Cacti 4.0 [35], an improved version of Wattch [6], the block model of Hotspot 3.0 [31], and an extended version of HotLeakage [36] to model the dynamic and static power of all units not addressed in Cacti 4.0, including logic structures such as the decoder, dependency check logic, issue queue selection logic, and ALUs. We assume a nominal clock frequency of 4.0 GHz and a supply voltage of 1.0V.

To efficiently simulate large multi-core architectures, we developed a parallel simulation framework. For this study, we focus on workloads of single-threaded applications chosen from the SPEC CPU2000 benchmarks. Multi-threaded workloads will present unique challenges when run on a heterogeneous CMP, and we leave this added dimension to future work.

With these workloads, direct interaction among applications executing on different cores is limited. While heat from one core conducted across the silicon die can cause inter-core heating effects, in our design, private L2 caches surround each core. These large caches have low and relatively uniform activity and thus act as heat sinks, preventing much of the heating from another core from affecting its neighbors. The second major interaction among cores is contention for off-chip memory bandwidth. We assume the bandwidth is statically partitioned among the cores, avoiding further complications in our already large search space of scheduling and core configuration options.

With these assumptions, we simulate a multi-core processor using single-core simulations to obtain performance, power, and thermal statistics, which are then combined by a higher-level chip-wide simulator that performs the role of the operating system scheduler. The chip-wide simulator is responsible for setting up the proper application assignments for each interval in the sampling phase, gathering and interpreting the individual core results, and applying the algorithms to determine the best schedule for the steady-state phase. A major advantage of this approach is its scalability to CMPs with a large number of cores.

### Core Architectural Parameters

#### Front-End Parameters
- **Branch Predictor**: Hybrid of gshare and bimodal with 4K entries in the bimodal, gshare 2nd level, and meta predictor.
- **Branch Target Buffer**: 512 entries, 4-way associative.
- **Return Address Stack**: 64 entries, fully associative.
- **Front-End Width**: 3-way.
- **Fetch Queue Size**: 18 entries.
- **Re-Order Buffer**: 100 entries, 3-way associative.
- **Retire Width**: 3-way.

#### Back-End Parameters
- **Integer Issue Queue**: 32 entries, 2-way issue.
- **Integer Register File**: 80 registers.
- **Integer Execution Units**: 2 ALUs and 1 mult/div unit.
- **FP Issue Queue**: 24 entries, 1-way issue.
- **FP Register File**: 80 registers.
- **FP Execution Units**: 1 adder and 1 mult/div unit.

#### Memory Hierarchy
- **L1 Instruction Cache**: 8KB, 2-way associative, 1 port, 1 cycle latency.
- **Instruction TLB**: 32 entry, fully associative, 1 port.
- **Load Queue**: 32 entries, 2 ports.
- **Store Queue**: 16 entries, 2 ports.
- **L1 Data Cache**: 8KB, 2-way associative, 2 ports, 1 cycle latency.
- **Data TLB**: 32 entry, fully associative, 2 ports.
- **L2 Cache**: 1MB, 8-way associative, 1 port, 10 cycle latency.
- **Main Memory**: 1 port, 200 cycle latency.

### Baseline Architecture

Our baseline architecture consists of an eight-core homogeneous chip multiprocessor with no degradation due to hard failures or variations. Each core is a single-threaded, 3-way superscalar, out-of-order processor. The main architectural parameters are listed in Table 2. To model temperature-dependent leakage power, we created a core floor plan. Each core is surrounded by its L2 cache, modeled as four banks and illustrated in Figure 5.

### Modeling Faults and Variations

Modeling faults and variations in an architectural simulation is challenging. Much of the effect from errors and variability on a chip is highly device and circuit dependent, and such low-level details are not available at the time of initial architectural design. In this work, we focus on the architecturally visible effects of faults and variations. We study processor configurations that have become degraded through manufacturing inconsistencies and wear-out over the lifetime of the device. For this study, the specific source of the degradation—manufacturing or wear-out—is not important because we focus on adapting the OS thread scheduling and core configuration ex post facto.

We focus on three forms of processor degradation:
1. **Errors Causing Pipeline Component Disabling**: We model errors that cause the system to disable part of a pipeline component, such as an ALU, load queue port, or set of ROB entries. We focus on large granularity errors that damage significant portions of the structure. Prior work has shown that when only a few entries in structures, such as an issue queue or register file, are damaged, the performance impact (assuming graceful degradation) is negligible, and adaptation is unnecessary [17].
2. **Frequency Degradation Due to Manufacturing Variations**: We assume manufacturing process variations that result in slow transistors in critical circuit paths [17, 22]. Prior work has found that these variations can increase processor cycle time by as much as 30%, eliminating an entire technology generation’s worth of frequency improvement [4].
3. **Leakage Current Variations**: We assume leakage current variations caused by process variations that diminish the quality of the transistors, magnifying sub-threshold and gate leakage currents. Past research concluded that excessive leakage currents will be a very serious problem, with some [4] saying that leakage variability across dies could be as high as 20X. Others [10] suggest that even at 45nm, within-die variations alone could cause leakage differences among cores of as much as 45%. Following the arguments of [10, 11], we focus on leakage variations that can be attributed to systematic variability. Thus, we consider leakage variations that affect an entire core as well as those that affect a group of architectural blocks in close proximity.

### Degraded CMP Configuration

| Core | Structural Faults | Frequency Degradation |
|------|-------------------|-----------------------|
| 1    | 2x normal memory latency (100 ns) | - |
| 2    | Half the nominal size integer issue queue (16) | - |
| 3    | Half the nominal size load queue (16) | 20% (3.2 GHz) |
| 4    | - | 10% (3.6 GHz) |
| 5    | One integer ALU disabled | 20% (3.2 GHz) |
| 6    | Integer issue queue can only issue one instruction per cycle | - |
| 7    | Half the L2 cache broken, leaving 500KB | - |
| 8    | - | 10% (3.6 GHz) |