primed  and  starred  zeros  as  follows.  Let 
Z0  represent  the  uncovered  primed  zero 
found  in  Step  4.  Let  Z1  denote  the 
starred  zero  in  the  column  of  Z0  (if 
any).  Let  Z2  denote  the  primed  zero  in 
the row of Z1 (there will always be one). 
Continue until the series terminates at a 
primed  zero  that  has  no  starred  zero  in 
its  column.  Unstar  each  starred  zero  of 
the  series, star each primed zero of the 
series,  erase  all  primes,  and  uncover 
every  line in the matrix. Return to Step 
3. 
Step 6: Add the value found in Step 4 to 
every  element  of  each  covered  row,  and 
subtract  it  from  every  element  of  each 
uncovered  column.  Return  to  Step  4 
without  altering  any  stars,  primes,  or 
covered lines.
Figure 1: The six step Hungarian Algorithm
Figure  1  outlines  the  six  steps  of  the  Hungarian 
Algorithm as described in [23]. The algorithm takes the 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE44DSN 2008: Winter & Albonesicost matrix as input and proceeds by manipulating rows 
and columns through addition and subtraction to find a 
set of starred zero elements that represent the optimal 
assignment.  During  the  algorithm,  rows  and  columns 
are  covered  and  zeroes  are  starred  and  primed  to 
indicate special status. When the algorithm completes, 
there  are  N  starred  zeroes.  A  starred  zero  at  location 
(i,j) means that the optimal solution to the Assignment 
Problem  schedules  application  i  to  run  on  core  j.  The 
Hungarian scheduler then uses the best assignment for 
the  simplified  problem  as  the  schedule for the steady-
state phase.
Table 1: Scheduling algorithms
Algorithm
Exploration Phase 
Complexity
Randomized
Round Robin
Hungarian
Global Search
Local Search
(in cycles)
none
none
8 intervals of 12.5M
25 intervals of 4M
25 intervals of 4M
O(N)
O(N)
O(N3)
O(N)
O(N)
3.2. Iterative Optimization Algorithms
Our  other  approach  is  to  use  iterative  optimization 
algorithms  inspired  by  artificial  intelligence  research 
[25,27].  These  algorithms  are  highly  suited  to  this 
scheduling  task  because  they  are  generally  simple  to 
implement,  have  low  computational  requirements,  and 
yet  are  extremely  effective  in  practice.  The  simplest 
search  algorithms  are  greedy:  they  avoid  searching  in 
directions  that  initially  appear  to  have  performance 
slowdowns  or  power  inefficiencies  even  if  they  may 
hold  promise  in  the  future.  Therefore,  these  greedy 
algorithms may get stuck in local minima. However, in 
practice,  greedy  algorithms  are  quite  effective  in 
certain problem domains and are often used due to their 
simplicity.  In  this  paper,  we  study  global  search  and 
local search. 
Figure 2: The global search algorithm
the  exploration  phase  of 
In  global  search  (Figure  2),  the  processor  is 
configured into a new random schedule in each interval 
of 
the  algorithm.  The 
operating system keeps track of the best configuration 
thus  far  and  employs  this  configuration  during  the 
longer,  steady-state  phase.  Figure  2  illustrates  how 
global  search  operates  on  a  sample  four  core  chip 
multiprocessor.  Global  search  has  the  advantage  of 
rapidly  exploring a broad range of configurations in a 
large  search  space  such  as  a  CMP  with  many  cores. 
However, it may not arrive at a near-optimal solution.
that  are  closely  related 
to 
Local search defines a neighborhood of assignment 
options 
the  current 
configuration.  During  each  exploration  interval,  a 
member in the neighborhood of the current assignment 
is  selected  as  the  next  assignment.  If  this  new 
assignment performs better than the original, then it is 
kept  and 
this  new 
configuration. If the new assignment does not function 
as  well  as  the  original,  then  local  search  reverts  to 
searching  further  in  the  neighborhood  of  the  original 
solution.  We  define  the  neighborhood  of  a scheduling 
configuration as all schedules that can be derived from 
the  original  schedule  through  some  fixed  number  of 
pair-wise swaps. 
local  search  proceeds  from 
Figure 3: The one swap local search algorithm
In  our  results,  we  explore  how  many  pair-wise 
swaps  the  algorithm  should  make  per  interval  to 
determine  the  best  setting.  The  advantage  of  selecting 
among  a  neighborhood  of  configurations  that  are 
derived  from  a  few  or  even  just  one  swap  is  that 
assignments in close proximity to the original are likely 
to  have  quite  similar  performance.  This  will  lead  to  a 
more gradual search that steadily improves the solution 
and avoids the large changes which could lead to poor 
results.  On  the  other  hand,  increased  swapping  more 
rapidly  explores  the  search  space  of  assignments. 
Figure  3  demonstrates  how  local  search  works  when 
one swap is performed per iteration. Figure 4 shows a 
two  swap version of local search and highlights a key 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE45DSN 2008: Winter & Albonesiimprovement  in  our  algorithm  which  allows  some  of 
the swaps from an interval to be retained while others 
are discarded. We also implemented a version of local 
search  that  uses  hill  climbing  [27]  to  escape  local 
minima.  We  found,  however,  that  the  improvements 
over  greedy  search  were  minimal,  indicating  that  the 
algorithms were not greatly impacted by local minima.
Figure 4: The two swap local search algorithm
4. Methodology
Our simulation infrastructure is based on the SESC 
simulator  [26].  We  improved  the  power  and  thermal 
modeling by augmenting SESC with Cacti 4.0 [35], an 
improved  version  of  Wattch  [6],  the  block  model  of 
Hotspot  3.0  [31],  and  an 
improved  version  of 
HotLeakage 
and 
HotLeakage to model the dynamic and static power of 
all the units not addressed in Cacti 4.0, including logic 
structures such as the decoder, dependency check logic, 
issue  queue  selection  logic,  and  ALUs.  We  assume  a 
nominal  clock  frequency  of  4.0  GHz  and  a  supply 
voltage of 1.0V. 
extended  Wattch 
[36].  We 
In  order  to  efficiently  simulate  large  multi-core 
architectures,  we  developed  a  parallel  simulation 
framework.  For  this  study,  we  focus  on  workloads  of 
single-threaded  applications  chosen  from  the  SPEC 
CPU2000  benchmarks.  Multi-threaded  workloads  will 
present a unique set of additional challenges when run 
on  a  heterogeneous  CMP  and  we  leave  this  added 
dimension to future work. 
With  these  workloads,  direct  interaction  among 
applications  executing  on  different  cores  is  limited. 
While heat from one core conducted across the silicon 
die can cause inter-core heating effects, in our design, 
private  L2  caches  surround  each  core.  These  large 
caches  have  low  and  relatively  uniform  activity  and 
thus  act  as  heat  sinks  preventing  much  of  the  heating 
from  another  core  from  affecting  its  neighbors.  The 
second  major 
their 
interaction  among  cores 
is 
contention for off-chip memory bandwidth. We assume 
the bandwidth is statically partitioned among the cores. 
This  avoids  further  complicating  our  already  large 
search 
scheduling  and  core 
configuration options.
space  of 
thread 
With  these  assumptions,  we  simulate  a  multi-core 
processor  using  single-core  simulations 
to  obtain 
performance, power, and thermal statistics that are then 
combined  by  a  higher  level  chip-wide  simulator  that 
performs  the  role  of  the  operating  system  scheduler. 
The  chip-wide  simulator  is  responsible  for  setting  up 
the proper application assignments for each interval in 
the  sampling  phase,  gathering  and  interpreting  the 
individual core results, and applying the algorithms to 
determine the best schedule for the steady-state phase. 
A major advantage of this approach is its scalability to 
CMPs with a large number of cores.
Table 2: Core architectural parameters
Front-End Parameters
Branch Predictor
Branch Target Buffer
Return Address Stack
Front-End Width
Fetch Queue Size
Re-Order Buffer
Retire Width
Hybrid of gshare and 
bimodal with 4K entries in 
the bimodal, gshare 2nd
level, and meta predictor
512 entries, 4-way assoc.
64 entries, fully assoc.
3-way
18 entries
100 entries
3-way
Back-End Parameters
Integer Issue Queue
Integer Register File
Integer Execution 
Units
FP Issue Queue
FP Register File
FP Execution Units
32 entries, 2-way issue
80 registers
2 ALUs and 1 mult/div unit
24 entries, 1-way issue
80 registers
1 adder and 1 mult/div unit
Memory Hierarchy
L1 Instruction Cache
Instruction TLB
Load Queue
Store Queue
L1 Data Cache
Data TLB
L2 Cache
Main memory
8KB, 2-way assoc., 1 port, 
1 cycle latency
32 entry, fully assoc., 1 port
32 entries, 2 ports
16 entries, 2 ports
8KB, 2-way assoc., 2 ports, 
1 cycle latency
32 entry, fully assoc., 2 ports
1MB, 8-way assoc., 1 port, 
10 cycle latency
1 port, 200 cycle latency
Our  baseline  architecture  consists  of  an  eight  core 
homogeneous chip multiprocessor with no degradation 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE46DSN 2008: Winter & Albonesidue to hard failures or variations. Each core is a single-
threaded,  3-way  superscalar,  out-of-order  processor. 
The main architectural parameters are listed in Table 2. 
In  order  to  model  temperature-dependent  leakage 
power,  we  created  a  core  floor  plan.  Each  core  is 
surrounded by its L2 cache modeled as four banks and 
illustrated in Figure 5. 
Figure 5: Processor core floor plan
The  task  of  modeling  faults  and  variations  in  an 
architectural  simulation  is  quite  challenging.  Much  of 
the effect from errors and variability on a chip is highly 
device and circuit dependent and such low level details 
are  not  available  at  the  time  of  initial  architectural 
design.  In  this  work,  we  focus  on  the  architecturally 
visible  effects  of  faults  and  variations.  We  study 
processor  configurations  that  have  become  degraded 
from 
through  manufacturing 
inconsistencies as well as wear-out over the lifetime of 
the  device.  For  this  study,  the  specific  source  of  the 
degradation  –  manufacturing  or  wear-out  –  is  not 
important because we focus on adapting the OS thread 
scheduling and core configuration ex post facto.
the  nominal  design 
We  focus  on  three forms of processor degradation. 
First, we model errors that cause the system to disable 
part  of  a  pipeline  component  such  as  an  ALU,  load 
queue  port,  or  set  of  ROB  entries.  We  focus  on  large 
granularity  errors  that  damage  significant  portions  of 
the  structure.  Prior  work  has  shown  that  when  only  a 
few  entries  in  structures,  such  as  an  issue  queue  or 
register  file,  are  damaged,  the  performance  impact 
(assuming graceful degradation) is negligible, and thus 
adaptation  is  unnecessary  [17].  Second,  we  assume 
from  manufacturing 
core 
frequency  degradation 
process  variations  that  result  in  slow  transistors  in 
critical circuit paths [17,22]. Prior work has found that 
these variations can increase processor cycle time by as 
much  as  30%,  eliminating  an  entire 
technology 
generation’s  worth  of  frequency  improvement  [4]. 
Third, we assume leakage current variations, which are 
also  caused  by  process  variations  that  diminish  the 
quality of the transistors, magnifying sub-threshold and 
gate leakage currents. 
Past  research  concluded  that  excessive  leakage 
currents will be a very serious problem, with some [4] 
saying  that  leakage  variability  across  dies  could  be  as 
high  as  20X.  Others  [10]  suggest  that  even  at  45nm 
within-die  variations  alone  could  cause 
leakage 
differences among cores of as much as 45%. Following 
the  arguments  of  [10,11],  we  focus  on  leakage 
variations 
to  systematic 
variability.  Thus,  we  consider  leakage  variations  that 
affect  an  entire  core  as  in  [10]  as  well  as  those  that 
affect  a  group  of  architectural  blocks 
in  close 
proximity.
that  can  be  attributed 
Table 3: Degraded CMP configuration
Core Structural Faults
Frequency 
Degradation
1
2
3
4
5
6
7
8
2x normal memory 
–
latency (100 ns) 
half the nominal 
size integer 
issue queue (16)
half the nominal 
size load queue (16)
20% 
 (3.2 GHz)
10% 
(3.6 GHz)
one integer ALU is 
20% 
disabled
(3.2 GHz)
integer issue queue 
can only issue one 
instruction per cycle
half the L2 cache is 
–
10%
broken leaving 
(3.6 GHz)
500KB 