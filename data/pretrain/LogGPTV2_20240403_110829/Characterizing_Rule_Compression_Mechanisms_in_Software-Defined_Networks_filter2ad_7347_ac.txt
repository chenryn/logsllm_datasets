gation methods built for IP-based rules. Evaluating these approaches within the
scope of OpenFlow is subject to future work.
5.1 Simple Aggregation
To reduce the memory footprint of the conﬁguration installed on a switch, we
automatically aggregate similar rules into a single rule. A network controller
can accomplish this by intercepting all OpenFlow control messages and storing
the state of all switches in-memory. On a rule install to a switch, the controller
adds the rule to its in-memory state for the switch and checks for aggregation. If
aggregation is not possible, the controller simply installs the rule into the switch.
Otherwise, it sends an aggregated rule and deletes all rules that are covered by
it. Similarly, on a rule removal, the controller checks to see if it is part of any
aggregated ruleset and appropriately reinserts rules as necessary.
To build a proof of concept implementation of rule reduction and demon-
strate its eﬀectiveness, we use binary trees [26] to store and aggregate rules on a
particular switch. Because we use binary trees, we are limited to only IP-based
rules. We are currently exploring other possibilities that can accommodate more
header ﬁelds.
For every switch, we maintain two binary trees: one based on source IP
addresses and the other based on destination IP addresses. Every node corre-
sponds to a source or destination address preﬁx. When the controller wants to
install a rule r to a switch, it adds the rule action to both the source and des-
tination trees at the nodes corresponding to the source and destination preﬁx
included in r.
Given this binary tree based representation of rules installed at a switch,
we aggregate rules as follows. Consider a new rule r added at nodes s and d in
the source and destination trees, respectively. We can potentially aggregate if
r has the same action as another rule r(cid:3) and if r(cid:3) satisﬁes one of the following
conditions in both the source and destination trees: (1) r and r(cid:3) are at the same
node in the tree, (2) r(cid:3) is r’s parent, or (3) r and r(cid:3) are siblings. Moreover,
in the case that r is aggregated up to its parent in either tree, we recursively
continue checking upwards in the source and destination trees to see if further
opportunities for aggregation exist.
312
C. Yu et al.
Figures 2 and 3 show a three-level sub-tree representing the last two bits
of the IP space, along with example rules. Diﬀerent colors represent diﬀerent
rule actions. First, rules r2 and r3 are aggregated into r6 because they (a) have
the same associated action (blue), (b) are at the same node in the destination
tree, and (c) are siblings in the source tree. Thereafter, recursive checks for
aggregation ﬁnd that r1 and r6 can be aggregated into r7. On the other hand,
though r4 and r5 have the same action (red) and are siblings in the source tree,
they cannot be aggregated since they do not satisfy any one of criteria (1), (2),
and (3) mentioned above.
5.2 Aggressive Aggregation
As described so far, we can aggregate a rule r up to its parent node only if
there exists another rule with the same action at r’s sibling. This limits the
ability to aggregate similar rules when two rules are not at the same node or
share a parent, but share a common ancestor. For example, in Fig. 2, although
r4 and r5 could not be aggregated because they do not have a common parent in
the destination tree, they could potentially be aggregated up to their common
grandparent.
However, unless we place any restrictions, aggregating rules with common
ancestors could result in the aggregation of very dissimilar rules. For example,
two rules that are at the leftmost and rightmost nodes in either tree (as dissimilar
as they can get), can be aggregated up to their common ancestor—the root. In
such cases, the aggregated rule will span a very large part of the IP address
space, and matched packets will be associated with an action that is perhaps
not intended by the application policy.
To limit the aggressiveness of aggregation with common ancestors, we use a
threshold T . We install an aggregated rule at a node in the source or destination
tree only if the controller has already inserted rules that are associated with at
least T % of the leaves in the subtree rooted at the node. For example, in Fig. 2,
we could aggregate r4 and r5 into the root of the destination tree if T ≥ 50 %.
One of the side-eﬀects of aggressive aggregation is that it can violate appli-
cation policies. When threshold-based aggregation is used, an aggregated rule
may match packets that are not covered by rules previously installed by the
controller. In the absence of the aggregated rule, these packets would trigger a
PacketIn message sent to the controller, to which the controller may have cho-
sen to insert a rule with a diﬀerent action than the aggregated rule. Later, we
evaluate the extent to which policy violations occur and the trade-oﬀs involved
in eliminating them.
5.3 Evaluation
Table 2 shows the results of our measurement.
Rule Savings of Simple Aggregation. Figure 4 shows how the rule savings
vary with the use of wildcards i.e., reducing the IP preﬁx size (ignore the lines
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
313
 4500
 4000
 3500
 3000
 2500
 2000
 1500
 1000
 500
l
s
e
u
r
t
n
e
r
r
u
c
n
o
c
f
o
r
e
b
m
u
n
x
a
M
Baseline
T = 100%
T = 75%
T = 50%
T = 25%
 60000
 50000
 40000
 30000
 20000
 10000
l
s
e
u
r
t
n
e
r
r
u
c
n
o
c
f
o
r
e
b
m
u
n
x
a
M
Baseline
T = 100%
T = 75%
T = 50%
T = 25%
 0
 32
 30
 28
 26
 24
 22
 20
IP prefix size
)a(
 0
 20
 18
 14
 16
IP prefix size
 12
 10
)b(
Fig. 4. Maximum number of concurrent rules needed to cover the (a) Campus and (b)
Abilene ﬂows, as we vary the value of T and use wildcards.
for T < 100 % for now). In the Abilene dataset, as we decrease the preﬁx size, the
potential for aggregation increases. Without aggregation, specifying rules at /16
granularity (rather than /21) reduces their number to only around 40 K (com-
pared to slightly over 50 K). In contrast, when using aggregation, the maximum
number of rules is further reduced by third (to around 25 K). The savings are
even bigger for the Campus data set: up to 62 % savings when aggregating at
/28 preﬁx).
Overhead of Simple Aggregation. Aggregation may increase the number
of switch operations, because one rule addition or deletion performed by the
controller can translate to several operations at the switch. This is reﬂected
in the Abilene data where the operation rate increases slightly by 5 % (see
Table 2). However, when we have many aggregations, we may also save opera-
tions because we delete an aggregated rule from the switch only when all rules it
aggregates are deleted. Since the Campus data has more rule savings (and implic-
itly more higher-in-the-tree aggregations), the number of operations decreases
slightly by 3 %.
Is Aggressive Aggregation Eﬀective? Table 2 and Fig. 4 show that aggres-
sive aggregation can reduce dramatically the number of rules (by 62 % for Cam-
pus and 24 % for Abilene) and the rate of switch operations (45 % for Campus
and 2 % for Abilene). Using a threshold has only limited eﬀect on the wildcarded
Campus rules. When the preﬁx size is big, the savings are signiﬁcant (up to
62 % with /28 preﬁx and 75 % threshold). However, because the IPs in the Cam-
pus data are more similar, most rules are already aggregated when the preﬁx
size decreases enough (less than /24) and using a threshold cannot yield further
savings.
We measure policy violations as the percentage of ﬂows that are forwarded
with a diﬀerent action when we aggregate rules compared to a deployment where
there is no aggregation. The fraction of ﬂows for which rule aggregation leads
to an incorrect output action is low. When the threshold is 25 % i.e., we install
an aggregate rule in a node even when only a quarter of the leafs in its subtree
314
C. Yu et al.
have an associated rule, less than 1 % of the Abilene ﬂows could be misdirected.
The number of policy violations decreases with higher thresholds. There are no
violations for Campus, as the set of output actions is less varied than for Abilene.
5.4 Summary
Automatically aggregating similar rules reduces their number by up to 20 % com-
pared to IP-only rules with 60 s timeout at negligible changes in control channel
overhead. Operators or programmers can further increase eﬃciency (up to 62 %
rule reduction) if they allow a small part of the traﬃc (under 1 %) to be directed
to other destinations. While this is unacceptable for most applications, it may
be a solution for dedicated network deployments where any of a set of destina-
tions is acceptable (e.g., load balancers, ﬁrewalls, anycast). As Table 2 shows,
for many cases, it is more eﬀective to use small timeouts than any automatic
aggregation.
6 Conclusions and Future Work
Our real-world traces study shows that simple OpenFlow-based mechanisms,
such as lowering rule expiration timeouts, are eﬀective in managing the conﬁgu-
ration size on OpenFlow switches although may increase (sometimes unaccept-
ably) the utilization of the switch-to-controller channel. Other manual (using
wildcards) or automatic (aggregating similar rules) mechanisms may reduce the
size of the rule set even higher but curtail the expressiveness of the high-level pol-
icy and may, in a small number of cases, misdirect some packets. Understanding
these trade-oﬀs is important to SDN operators and programmers that must write
network policies that satisfy both infrastructure and application constraints.
Our ongoing and future work spans two directions. On one hand, we are
studying the adaptability of existing IP-based rule compression mechanisms [17]
to OpenFlow. We are exploring the use of R-trees [9] to extend our ability
to identify and aggregate rule similar in ﬁelds other than IP addresses (e.g.,
protocol).
References
1. NEC OpenFlow switches. http://www.openﬂow.org/wp/switch-NEC/
2. Pronto OpenFlow switches. http://www.openﬂow.org/wp/switch-Pronto/
3. Appelman, M., Boer, M.D.: Performance analysis of OpenFlow hardware. Techni-
cal report, University of Amsterdam (2012)
4. Benson, T., Akella, A., Maltz, D.: Network traﬃc characteristics of data centers
in the wild. In: IMC (2010)
5. Curtis, A.R., Mogul, J.C., Tourrilhes, J., Yalag, P., Sharma, P., Banerjee, S.:
Devoﬂow: scaling ﬂow management for high-performance networks. In: SIGCOMM
(2011)
6. Dong, Q., Banerjee, S., Wang, J., Agrawal, D., Shukla, A.: Packet classiﬁers in
ternary CAMs can be smaller. In: ACM Sigmetrics (2006)
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
315
7. Foster, N., Harrison, R., Freedman, M.J., Monsanto, C., Rexford, J., Story, A.,
Walker, D.: Frenetic: a netowrk programming language. In: ACM IFIP (2011)
8. Freguson, A.D., Guha, A., Liang, C., Fonseca, R., Krishnamurthi, S., Networking,
P.: An API for application control in SDNs. In: SIGCOMM (2013)
9. Guttman, A.: R-trees: a dynamic index structure for spatial searching. In: SIG-
MOD (1984)
10. HP 3800. http://h17007.www1.hp.com/us/en/networking/products/switches/HP
3800 Switch Series/index.aspx
11. IBM OpenFlow switches. http://www.openﬂow.org/wp/ibm-switch/
12. Kandula, S., Sengupta, S., Greenberg, A., Patel, P., Chaiken, R.: The nature of
datacenter traﬃc: measurement and analysis. In: IMC (2009)
13. Katta, N., Alipourfad, O., Rexford, J., Walker, D.: Inﬁnite CacheFlow in software-
deﬁned networks. In: HotSDN (2014)
14. Kogan, K., Nikolenko, S., Culhane, W., Eugster, P., Ruan, E.: Towards eﬃcient
implementation of packet classiﬁers in SDN/OpenFlow. In: HotSDN (2013)
15. Liu, H.: Routing table compaction in ternary CAM. IEEE Micro 22(1), 55–64
(2002)
16. McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L., Rexford,
J., Shenker, S., Turner, J.: OpenFlow: enabling innovation in campus networks.
ACM SIGCOMM CCR 38, 69–74 (2008)
17. Meiners, C.R., Liu, A.X., Torng, E., Razor, T.: A systematic approach towards
minimizing packet classiﬁers in TCAMs. IEEE/ACM Trans. Netw. 18(2), 490–500
(2010)
18. Monsanto, C., Foster, N., Harrison, R., Walker, D.: A compiler and run-time system
for network programs. In: ACM POPL (2012)
19. Monsanto, C., Reich, J., Foster, N., Rexford, J., Walker, D.: Composing software-
deﬁned networks. In: NSDI (2013)
20. Moshref, M., Yu, M., Sharma, A., Govindan, R.: Scalable rule management for
data centers. In: NSDI (2013)
21. Openﬂow multipath proposal. http://www.openﬂow.org/wk/index.php/Multipath
Proposal
22. Openﬂow switch speciﬁcation, 1.0.0. http://www.openﬂow.org/documents/open
ﬂow-spec-v1.0.0.pdf
23. Ravikumar, V.C., Mahapatra, R.N.: TCAM architecture for IP lookup using preﬁx
properties. IEEE Micro 24(2), 60–69 (2004)
24. Rotsos, C., Sarrar, N., Uhlig, S., Sherwood, R., Moore, A.W.: OFLOPS: an open
framework for OpenFlow switch evaluation. In: Taft, N., Ricciato, F. (eds.) PAM
2012. LNCS, vol. 7192, pp. 85–95. Springer, Heidelberg (2012)
25. Sarrar, N., Wuttke, R., Schmid, S., Bienkowski, M., Uhlig, S.: Leveraging locality
for FIB aggregation. In: IEEE Globecom (2014)
26. Wang, R., Butnariu, D., Rexford, J.: OpenFlow-based server load balancing gone
wild. In: Hot-ICE (2011)
27. Yu, M., Rexford, J., Freedman, M.J., Wang, J.: Scalable ﬂow-based networking
with DIFANE. In: ACM SIGCOMM (2010)