reported in multiple papers across populations and/or provided
understandable illustrations of how risk factors could combine
to yield new risks or amplify each other.
Prominence added focused targeting to other risk factors.
When added to other risk factors, prominence appeared to
make it more likely that an at-risk user would experience
focused targeting that ampliﬁed their other risk factors. For
example, political campaign workers had access to a sensi-
tive resource, but prominent politicians were more likely to
be subjected to focused targeting to access those sensitive
resources [21]. Similarly, while many transgender people re-
ported experiencing marginalization, the prominence of trans-
gender activists resulted in highly targeted hate and harassment
attacks [56].
Resource or time constraints made it harder to cope with
other risk factors. Populations who are resource or time
constrained were more likely to experience worse outcomes
pertaining to their other risk factors, because they did not
have the time, money, or other resources to eﬀectively protect
themselves or recover from attacks. For example, the primary
risk factor for survivors of intimate partner abuse was typically
their relationship with the attacker, but prior research has em-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
62349
phasized that survivors with low SES had particular diﬃculty
protecting their (and their children’s) digital lives [63].
Underserved accessibility needs and reliance on a third party
often combined. In our dataset, at-risk users with underserved
accessibility needs also tended to rely on a third party for care
or help with technology, at least sometimes. This included
children, teens, older adults, people with visual impairments,
and refugees, as shown in Table I. For example, older adults
experiencing mild cognitive impairment often forgot important
passwords and information (related to underserved accessibil-
ity needs), which reinforced the need to share these passwords
and information with caregivers (subjecting them to risks
associated with reliance on a third party) [76].
V. PROTECTIVE PRACTICES
At-risk users employed practices they perceived would help
them prevent, mitigate, or respond to digital-safety risks.
These protective practices form the second part of our at-
risk framework. The risk factors presented above helped
drive user decisions about which protective practices to use,
but these practices were not always ideal or even eﬀective.
Protective practices involved tradeoﬀs and highlight barriers
to technology use (which we explore in SectionVI). Diﬀerent
users weighed the pros and cons diﬀerently, sometimes leading
to seemingly contradictory choices. We catalog these imperfect
practices to show what at-risk users currently do given their
risks, and to set the stage for a discussion of barriers to
protections in Section VI, both of which provide context for
how to design technologies intended to support at-risk users.
Our meta-analysis identiﬁed three categories of protective
practices: social strategies where at-risk users relied on their
social connections to respond to threats; distancing behaviors
where at-risk users distanced themselves from, or entirely
abandoned, certain accounts and technologies; and technical
solutions that involved leveraging technical tools and mech-
anisms to prevent or respond to threats. We found these
strategies were not mutually exclusive, with at-risk users
commonly relying on multiple strategies simultaneously.
A. Social strategies
At-risk users frequently relied on social connections (in-
person or online) to overcome digital-safety threats. This
included relying on family or peers for trusted advice and
support, vetting the identities of people they interacted with
online, and controlling social interactions to minimize harms.
Informal help from trusted family and peers. A popu-
lar protective practice among at-risk users experiencing the
marginalization, social norms, or underserved accessibility
needs factors was to informally seek help from trusted family
and peers. The at-risk user may be seeking direct help from
someone perceived to be more knowledgeable or resourced,
or may simply be seeking emotional support. For example,
children and teenagers reportedly sought help from parents
to understand security warnings [47] or deal with strange,
scary, or confusing internet experiences [119, 120, 124]. Older
adults who self-identiﬁed as having low technical understand-
ing sought assistance from family members when addressing
digital-safety concerns [36, 78]. At times, women in South
Asia relied on family members for emotional support when
harassed online [88, 89]. Informal support sometimes also
came from anonymous peers, such as transgender individuals
relying on social media to connect with other LGBTQ+ peers
for emotional support when they experienced hate and harass-
ment [56, 91], or people in armed conﬂict zones connecting
to share critical information [95].
Formal help from trusted organizations. Public recognition
of digital-safety threats facing at-risk users—particularly with
respect to the legal or political, marginalization, relation-
ship with the attacker, or resource or time constrained risk
factors—has prompted trusted organizations to oﬀer support.
For example, organizations that assist survivors of traﬃck-
ing and intimate partner abuse, including NGOs [19] and
government agencies [33, 35, 42], have provided assistance
setting up digital-safety protections that are interwoven with
continuous care [42]. Similarly, NGOs have assisted refugees
with technology-required tasks, such as applying for work
or submitting legal documents [96]. Public institutions, like
libraries, have provided assistance and access to computers
and connectivity to people with low SES [110]. While at-risk
users may rely on these institutions for a broad variety of
technical assistance [96, 110], digital-safety support was often
explicitly reported as a key beneﬁt. This assistance could incur
additional digital-safety risks due to reliance on a third party,
but the information and aid provided was often critical, both
for digital safety and other basic needs.
Formal support may also stem from dedicated professional
resources, particularly for those who have access to a sensitive
resource or access to other at-risk users. For example, journal-
ists have beneﬁted from institutional support and accumulated
best practices, as exempliﬁed by the International Consortium
of Investigative Journalists’ (ICIJ) tools for digital safety in the
Panama Papers investigation [69]. Elementary school teachers
using technology in classrooms—who have a responsibility
for ensuring the digital safety of their students—have sought
assistance from school media specialists or librarians [53].
Political campaign workers in the U.S. have received training
from organizations such as Defending Digital Campaigns and
the Center for Democracy & Technology [21].
Vetting identities to avoid potential attackers. At-risk users
who experience marginalization or legal or political risk
factors have a documented need to verify that new people they
encounter online are safe to associate with or to admit into a
private space. For example, social media community modera-
tors for transgender communities of color asked potential new
members questions about topics relating to race and LGBTQ+
issues to help protect their spaces from potential attackers [56].
Women from non-Western cultures scrutinized online proﬁles
to prevent men from inﬁltrating their private discussion spaces
and subjecting them to sexual harassment [88]. Sex workers
informally shared lists of abusive clients and aggressors to
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
72350
be avoided [13]. Undocumented immigrants restricted com-
munication with untrusted parties until suﬃciently vetted, due
to concerns of impersonation by the police or immigration
enforcement [40]. Activists used physical in-person meetings
or mutually trusted peers to vet new individuals before adding
them to sensitive group chats [25]. Unifying these practices
is a lack of dependable digital signals of authenticity or
identity, resulting in at-risk users relying on ad-hoc techniques
to establish trust.
Preemptive disclosure for control. At-risk users experiencing
the marginalization or legal or political risk factors some-
times proactively revealed sensitive personal information in
a controlled manner to disempower attackers and preempt
future emotional harm. For example, some men seeking men
on dating apps opted to publicly disclose their HIV-positive
status, both to simplify navigating sexual negotiations and as
a path toward destigmatization [113, 114]. Some transgender
people chose to “out” themselves to avoid the emotional stress
of maintaining a secret identity and the risk of coercion or
extortion [56]. Some people with disabilities—especially those
with “visible” disabilities—chose to disclose these disabilities
on dating apps, to reduce the chance of undesirable connec-
tions [82]. Some activists engaged in an “anti-surveillance
tactic of openness” by collapsing public and private personas,
rendering information they shared—like political beliefs—
unusable against them in a separate context if leaked [90]. This
practice is not applicable to all at-risk users, as it requires a
willingness to be hyper-public, which carries potential risks
akin to those that prominent at-risk users face.
Social pleas. Some at-risk users who experience the marginal-
ization or social norms risk factors reported reaching out
directly to attackers with a plea to cease activities, such
as content leaks or harassment. For example, some women
in South Asia either independently, or with the assistance
of family, asked attackers within their community to cease
online sexual harassment [88, 109]. Similarly, some teens
engaged with peers [120] or parents [77] to have embarrassing
content removed from social media. This practice leverages the
attacker’s empathy to help resolve digital-safety threats.
B. Distancing behaviors
The second category of protective practices in our frame-
work involves distancing, or limiting use of technology. Dis-
tancing behaviors were prevalent in our dataset, used by most
populations. The fact that limited or non-participation often
felt like the safer choice may perhaps be a troubling signal to
technology creators. Here we highlight two common themes.
Censoring online sharing. Some at-risk users carefully self-
censored personal content they shared online. This was espe-
cially common for users experiencing the marginalization risk
factor, who often did not feel safe sharing personal information
or did not want to reveal stigmatized aspects of their lives to
broad online audiences. For example, some LGBTQ+ parents
refrained from sharing family or personal photos to try to
avoid “outing” themselves beyond speciﬁc social circles [16].
In another case, some HIV-positive men seeking men reported
not sharing their HIV status on dating apps to avoid unwanted
stigmatization [113]. (As noted in the previous section, some
members of this population did preemptively disclose their
HIV status, demonstrating that protective practices within a
population can vary.)
Reducing one’s digital footprint. Some at-risk users dramati-
cally reduced technology use to avoid attackers with extensive
access, knowledge, and power, as was often the case with
the relationship with the attacker and legal or political risk
factors. For example, Chen et al. [19] found that survivors
of human traﬃcking took relatively extreme measures to try
to protect their new location, including abandoning devices,
deleting social media accounts, and severely restricting online
sharing. Survivors of intimate partner abuse employed similar
measures when they suspected devices or accounts may have
been compromised by their abuser [34]. As another example,
activists in the Sudanese Revolution, concerned about
the
government conﬁscating their devices, used coded communi-
cations and regularly deleted sensitive data [25].
C. Technical solutions
At-risk users also employed technical solutions to protect
their digital safety, such as specialized software or settings
in common apps and services. Across these strategies, at-
risk users did their best to protect themselves based on their
knowledge and experience; however, the technical solutions
they chose did not always provide the desired protections.
Secure communication and encryption. At-risk users expe-
riencing the legal or political risk factor, such as journalists,
activists, or politicians, commonly reported using encrypted
messaging platforms. These at-risk users frequently wished to
secure their communications against monitoring by a nation-
state (or similarly resourced) attacker. For example, journal-
ists working together to report on the Panama Papers used
PGP as part of organizational security policies established
by the ICIJ [69]. Similarly, journalists and activists in varied
international contexts reported using encrypted chat apps to
communicate over networks directly controlled by their nation-
state adversaries [25, 90]. Transgender activists in the U.S.,
particularly those engaged in more prominent activities, simi-
larly used encrypted chat to protect their communications [56].
NGOs, who had access to other at-risk users, sought
to
protect those at-risk users by using encrypted email for internal
communications [19].
Beyond encrypted messaging, some at-risk users experi-
encing the legal or political risk factor reported encrypting
ﬁles or entire devices to protect content from attackers—
including nation-states and other
focused attackers—who
might gain physical access. At-risk users employing this
technique included journalists, human rights organizations, and
activists [55, 59, 67].
Strong(er) authentication. Strong authentication is com-
monly advised for internet users in general [80, 85], but can
be especially important for users at higher risk of device
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
82351
or account compromises. While truly strong authentication
practices were rare in our dataset, we identiﬁed a few au-
thentication trends in subsets of select populations.
While frequently changing passwords is no longer consid-
ered a best practice generally [80], some at-risk users did
this to address speciﬁc practical concerns. For example, at-
risk users who have a relationship with the attacker, such
as survivors of intimate partner abuse or traﬃcking, coped
with attackers who may have ready access to their passwords
(e.g., through coercion, co-presence leaks, or remote surveil-
lance [33, 63]) by regularly changing passwords on accounts
known to the attacker [19, 33]. Some users with visual
impairments also chose to change passwords frequently in case
others might have observed them inputting their passwords [5].
While not mentioned frequently in our dataset, our analysis
revealed some cases of successful adoption of two-factor
authentication (2FA). For example, in Matthews et al. [63],
only a small number of survivors of intimate partner abuse
in their study reported using 2FA to protect their accounts,
despite highly motivated attackers. Only a minority of par-
ticipants involved with political campaigns reported adopting
the strongest form of 2FA, despite commonly experiencing
phishing attacks [21]. Journalists collaborating on the Panama
Papers (access to a sensitive resource) were required by the
ICIJ to use 2FA [69], though journalists are more broadly
described as having limited awareness of 2FA’s beneﬁts [68].5
Privacy settings and access control. We identiﬁed three
categories of privacy settings (within widely used apps and
services) that were discussed by multiple papers across a vari-
ety of at-risk populations and risk factors: location privacy [19,
28, 36, 63], social media visibility settings [19, 28, 36, 46, 63,
124], and blocking undesired contacts [28, 36, 63, 86, 88]. For
users with a relationship with the attacker or access to other
at-risk users, these privacy settings were commonly discussed
as protecting highly sensitive information that, if obtained
by an attacker, could compromise safety [19, 35, 63]. For
example, some women in South Asia used platform controls
to block unwanted contact from abusers on social media [88].
For at-risk users in professional settings with access to
a sensitive resource—such as NGO staﬀ, political campaign
workers, and journalists—access control settings were also
important for limiting who could access sensitive digital
resources [21, 55, 69].
Online identity management. Some at-risk users reported
attempting to hide their identity online via careful account
management, including multiple and pseudonymous accounts.
Some at-risk users used multiple accounts or devices to
maintain boundaries between diﬀerent facets of their identities.
Marginalization was one driver for this behavior; for example,
sex workers (whose profession was stigmatized) reported keep-
ing separate work and personal accounts [13]. Alternatively,
survivors of sexual assault used throwaway accounts to seek
support online without revealing their identities [10].
5We note that 2FA has become more commonplace since some of these
papers were published.
At-risk users also used multiple accounts or devices in
response to potential account hijacking or surveillance. Sur-
vivors of intimate partner abuse, whose relationship with the
attacker tended to give the attacker opportunities to access
their accounts or devices, reported creating new accounts
or purchasing new devices to avoid revictimization after es-
cape [63]. Users experiencing legal or political risks, such
as activists, reported using multiple accounts and devices to
protect themselves from potential nation-state attackers. For
example, activists in the Sudanese Revolution reported using
SIM cards from other countries, creating fake U.S. phone
numbers online, and asking relatives and friends overseas to
verify social media accounts in an eﬀort to thwart government
surveillance [25]. Marczak et al. found similar evidence of
an activist using multiple SIM cards to try to prevent gov-
ernments from linking calls made in diﬀerent countries and
contexts [60].
In some cases—particularly cases of marginalization—
rather than use entirely new accounts, at-risk users reported
ad-hoc pseudonymity strategies for existing accounts. For
example,
in order to prevent personal photos being used
for digital abuse, some women in South Asia chose neutral
images, like ﬂowers, as proﬁle photos [88]. This strategy was
shared by some transgender people, particularly activists (who
also experienced elevated risk due to prominence) [56]. Low-
income Black Americans similarly reported sometimes using
emojis rather than contact names to protect their contacts’
identities on devices they thought might be compromised [31].
Network security. Only two at-risk populations in our review
were reported to have used network security tools—like Tor or
virtual private networks—to disguise their web traﬃc. Certain
activists [25, 59] and journalists [67] were aware of and used
these technologies, but these tools did not appear elsewhere
in our dataset—and not every user in those populations found
these tools equally useful or usable [59, 68]. This could mean
that other at-risk populations are largely unaware of these