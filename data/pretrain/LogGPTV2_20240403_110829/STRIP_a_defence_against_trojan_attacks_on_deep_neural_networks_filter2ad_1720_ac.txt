B. Case Studies
1) MNIST: For MNIST dataset, the square trigger shown
in Fig. 2 and heart trigger in Fig. 7 (a) are used. The square
trigger occupies nine pixels—trigger size is 1.15% of the
image, while the heart shape is resized to be the same size,
28 × 28, of the digit image.
We have tested 2000 clean digits and 2000 trojaned digits.
Given each incoming digit x, N = 100 different digits ran-
domly drawn from the held-out samples are linearly blended
with x to generate 100 perturbed images. Then entropy of
input x is calculated according to Eq 4 after feeding all
100 perturbed images to the deployed model. The entropy
distribution of tested 2000 benign and 2000 trojaned digits
Figure 9. Entropy distribution of benign and trojaned inputs. Dataset is
GTSRB, model is ResNet 20, and trigger b is used.
Table II summarizes the attack success rate and classiﬁca-
tion accuracy of trojan attacks on tested tasks. We can see that
backdoored models have been successfully inserted because it
maintains the accuracy on clean inputs and classiﬁes trojaned
inputs to the attacker’s targeted label with high accuracy, 100%
in most tested cases.
C. Detection Capability: FAR and FRR
To evaluate FAR and FRR, we assume that we have access
to trojaned inputs in order to estimate their corresponding en-
tropy values (pretend to be an attacker). However, in practice,
the defender is not supposed to have access to any trojaned
samples under our threat model, see Section IV-B. So one may
ask:
How the user is going to determine the detection
boundary by only relying on benign inputs?
( a )( b )( c )( d )( e )( f )( b )trigger heart( c )trigger b( d )trigger c( a )trigger squareATTACK SUCCESS RATE AND CLASSIFICATION ACCURACY OF TROJAN
ATTACKS ON TESTED TASKS.
Table II
Dataset
MNIST
MNIST
CIFAR10
CIFAR10
GTSRB
Trigger
type
square
(Fig. 2)
trigger a
(Fig. 7 (a))
trigger b
(Fig. 7 (b))
trigger c
(Fig. 7 (c))
trigger b
(Fig. 7 (b))
Trojaned model
Classiﬁcation rate1
Attack success rate2
98.86%
98.86%
87.23%
87.34%
96.22%
99.86%
100%
100%
100%
100%
Origin clean model
classiﬁcation rate
98.62%
98.62%
88.27%
88.27%
96.38%
1 The trojaned model predication accuracy of clean inputs.
2 The trojaned model predication accuracy of trojaned inputs.
FAR AND FRR OF STRIP TROJAN DETECTION SYSTEM.
Table III
Dataset
MNIST
MNIST
CIFAR10
CIFAR10
GTSRB
Trigger
type
square,
Fig. 2
trigger a,
Fig. 7 (a)
trigger b,
Fig. 7 (b)
trigger c,
Fig. 7 (c)
trigger b,
Fig. 7 (b)
N
Mean
Standard
variation
100
0.196
0.074
100
0.189
0.071
100
0.97
0.30
100
1.11
0.31
100
0.53
0.19
FRR
3%
2%
1%1
2%
1%
0.5%
2%
1%
0.5%
2%
1%
0.5%
2%
1%
0.5%
Detection
boundary
FAR
0.058
0.046
0.026
0.055
0.0235
0.0057
0.36
0.28
0.20
0.46
0.38
0.30
0.133
0.081
0.034
0.75%
1.1%
1.85%
0%
0%
1.5%
0%
0%
0%
0%
0%
0%
0%
0%
0%
1 When FRR is set to be 0.05%, the detection boundary value
becomes a negative value. Therefore, the FRR given FAR of
0.05% does not make sense, which is not evaluated.
Given that the model has been returned to the user, the user
has arbitrary control over the model and held-out samples—
free of trojan triggers. The user can estimate the entropy
distribution of benign inputs. It is reasonable to assume that
such a distribution is a normal distribution, which has been
afﬁrmed in Fig. 8. Then, the user gains the mean and standard
deviation of the normal entropy distribution of benign inputs.
Firstly, FRR, e.g., 1%, of a detection system is determined.
Then the percentile of the normal distribution is calculated.
This percentile is chosen as the detection boundary. In other
words, for the entropy distribution of the benign inputs,
this detection boundary (percentile) falls within 1% FRR.
Consequentially, the FAR is the probability that the entropy
of an incoming trojaned input is larger than this detection
boundary.
Table III summarises the detection capability for four dif-
ferent triggers on MNIST, CIFAR10 and GTSRB datasets. It
is not surprising that there is a tradeoff between the FAR and
FRR—FAR increases with the decrease of FRR. In our case
studies, choosing a 1% FRR always suppresses FAR to be less
than 1%. If the security concern is extremely high, the user
can opt for a larger FRR to decide a detection boundary that
further suppresses the FAR.
For CIFAR10 and GTSRB datasets with the trigger (either
7
trigger b or c), we empirically observed 0% FAR. Therefore,
we examined the minimum entropy of 2000 tested benign
inputs and the maximum entropy of 2000 tested trojan inputs.
We found that the former is larger than the latter. For instance,
with regards to CIFAR10, 0.029 minimum clean input entropy
and 7.74 × 10−9 maximum trojan input entropy are observed
when trigger b is used. When the trigger c is used, we
observer a 0.092 minimum clean input entropy and 0.005
maximum trojaned input entropy. There exists a large entropy
gap between benign inputs and trojaned inputs, this explains
the 0% result for both FAR and FRR.
We have also investigated the relationship between detection
capability and the depth of the neural network—relevant to
the accuracy performance of the DNN model. Results can be
found in Appendix B.
Figure 10. Detection time overhead vs N.
D. Detection Time Overhead
To evaluate STRIP run-time overhead, we choose a com-
plex model architecture, speciﬁcally, ResNet20. In addition,
GTSRB dataset and trigger b are used.
We investigate the relationship between the detection time
latency and N—number of perturbed inputs—by varying N
from 2 to 100 to observe the detection capability, depicted in
Fig. 10. Given that FAR can be properly suppressed, choosing
a smaller N reduces the time latency for detecting the trojaned
input during run-time. This is imperative for many real-time
applications such as trafﬁc sign recognition. Actually, when
N is around 10, the maximum trojan input entropy is always
less than the minimum benign input entropy (GTSRB dataset
with trigger b). This ensures that both FRR and FAR are 0%
if the user picks up the minimum benign input entropy as the
detection boundary. To this end, one may rise the following
question:
How to determine N by only relying on the normal
distribution of benign inputs’ entropy?
We propose to observe the change of the standard variation
of the benign input entropy distribution as a function of N.
One example is shown in Fig. 11. The user can gradually
increase N. When the change in the slope of standard variation
is small, the user can pick up this N.
According to our empirical evaluations on GTSRB dataset,
setting N = 10 is sufﬁcient, which is in line with the above
N selection methodology as shown in Fig. 11. Without opti-
mization, STRIP is 1.32 times longer than the original default
8
trojaned samples. FRR is preset to 0.5%. The detection ca-
pability increases when the trigger transparency decreases,
because the trigger becomes more salient. Overall, our STRIP
method performs well, even when the transparency is up to
90%; the trigger is almost imperceptible. Speciﬁcally, given a
preset of 0.5% FRR, STRIP achieves FAR of 0.10%. Notably,
the attack success rate witnesses a (small) deterioration when
transparency approaches to 90% while FAR slightly increases
to 0.10%. In other words,
lowering the chance of being
detected by STRIP sacriﬁces an attacker’s success rate.
B. Large Trigger
We use the Hello Kitty trigger—an attack method reported
in [6] and shown in Fig. 1—with the CIFAR10 dataset to
further evaluate STRIP insensibility to large triggers. We
set the transparency of Hello Kitty to 70% and use 100%
overlap with the input image. For the trojaned model, its
classiﬁcation rate of clean images is 86%, similar to a clean
model, and the attack success rate of the trojaned images is
99.98%—meaning a successful backdoor insertion. Given this
large trigger, the evaluated min entropy of clean images is
0.0035 and the max entropy of trojaned images is 0.0024.
Therefore, STRIP achieves 0% FAR and FRR under our
empirical evaluation. In contrast, large triggers are reported
to evade Neural Cleanse [17] and Sentinet [11].
C. Multiple Infected Labels with Separate Triggers
We consider a scenario where multiple backdoors targeting
distinct labels are inserted into a single model [17]. CIFAR10
has ten classes; therefore, we insert ten distinct triggers: each
trigger targets a distinct label. We create unique triggers via
10 digit patterns—zero to nine. Given the trojaned model,
the classiﬁcation rate for clean images is 87.17%. As for all
triggers, their attack success rates are all 100%. Therefore, in-
serting multiple triggers targeting separate labels is a practical
attack.
STRIP can effectively detect all of these triggers. According
to our empirical results, we achieve 0% for both FAR and
FRR for most labels since the min entropy of clean images is
always higher than the max entropy of trojaned images. Given
a preset FRR of 0.5%, the worst-case is a FAR of 0.1% found
for the ‘airplane’ label.
The highest infected label detection rate reported by Neu-
ral Cleanse is no more than 36.9% of infected labels on
the PubFig dataset. Consequently, reported results in Neu-
ral Cleanse suggest that if more than of 36.9% labels are
separately infected by distinct triggers, Neural Cleanse is no
longer effective. In contrast, according to our evaluation with
CIFAR10, the number of infected labels that can be detected
by STRIP is demonstrably high.
D. Multiple Input-agnostic Triggers
This attack considers a scenario where multiple distinc-
tive triggers hijack the model to classify any input image
stamped with any one of these triggers to the same target
label. We aggressively insert ten distinct triggers—crafted in
Figure 11. The relationship between the standard variation of the benign input
entropy distribution and N, with N being the number of perturbed replicas.
inference time. To be speciﬁc, processing time—generating
N = 10 perturbed images—takes 0.1ms, while predicting 10
images takes 6.025ms 3. In total, STRIP detection overhead
is 6.125ms, whereas the original inference time without im-
plementing STRIP is 4.63ms. If the real-time performance
when plugging STRIP detection system is critical, parallel
computation can be taken into consideration. Noting the 0.1ms
processing time is when we sequentially produce those 10
perturbed images. This generation can be paralleled. Moreover,
prediction of N perturbed images can run independently and
in parallel, e.g., through N separated model replicas.
VI. ROBUSTNESS AGAINST BACKDOOR VARIANTS AND
ADAPTIVE ATTACKS
them. To some extent,
In line with the Oakland 2019 study [17], we implement ﬁve
advanced backdoor attack methods and evaluate the robustness
of STRIP against
those backdoor
variants can be viewed as adaptive attacks that are general
to backdoor defences. Besides those ﬁve backdoor variants,
we identify an adaptive attack that is speciﬁc to STRIP and
evaluate it. To expedite evaluations,
in the following, we
choose the CIFAR10 dataset and 8-layer model as summarized
in Table I.
A. Trigger Transparency
In above experimental studies, the trigger transparency used
in the backdoor attacks are set to be 0%. In other words, the
trigger is opaque, which facilitates the attacker who can simply