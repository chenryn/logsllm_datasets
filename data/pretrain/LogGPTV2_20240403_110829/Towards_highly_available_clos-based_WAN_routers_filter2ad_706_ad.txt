### Proportionality of Multipath Table Entries

The number of multipath table entries at an L2 switch, \( b \), for a trunk pair \((i, j)\) is proportional to the number of flow counts. To illustrate, consider a trunk pair \((i, j)\) with a value of 10. If switch \( b \) forwards these 10 flow counts to L1 switches, any split with integer weights of these flow counts can be at most 10. Additionally, L2 switches require more WCMP (Weighted Cost Multi-Path) entries than L1 switches for internal links. Overall, L1 switches need more entries because they also handle egress links (see §5.4). An L1 switch may not see traffic for all trunk pairs \((i, j)\), but an L2 switch may see traffic for all such pairs with non-zero upflow. This is because, in general, an L1 switch must distribute the upflow across L2 switches to ensure the effective capacity constraint.

### Optimization Objective

These observations motivate our optimization objective: to find flow count assignments \( x \) that minimize the maximum total flow count at an L2 switch, across all possible traffic matrices, for a given failure pattern. The optimization formulation is presented in §A.9. In this section, we also discuss an important detail: computing WCMP weights for egress links in L1 switches (our formulation focuses on weight assignments for internal links).

### Scaling Techniques

To this formulation, we apply two scaling techniques discussed in previous sections:
1. **Replacement of Infinite Traffic Matrix Set**: We replace every occurrence of the infinite traffic matrix set \( T \) with the finite extreme traffic matrix set \( E \), resulting in an MILP (Mixed-Integer Linear Programming) problem solvable by an off-the-shelf solver.
2. **Optimization for Canonical Failure Patterns**: We run the optimization only for canonical failure patterns.

### Improved Scaling with Approximate Solution

Even with these techniques, our formulation does not scale well to 512-port routers. To address this, we use the same approximation technique as in §2: instead of iterating over all extreme traffic matrices, we evaluate for one matrix whose elements are the element-wise maximum across all extreme matrices in \( E \).

## Evaluation

In this section, we compare the resilience of our approach to other approaches for both 128 and 512-port switches, explore the efficacy of our routing table compaction, and quantify the benefits of our techniques to scale the computations.

### 5.1 Methodology

**Goal**: We compare our approach against a baseline wiring scheme that sequentially assigns external ports to each trunk one by one. For example, in a 128-port WAN router with 16 L1 switches each with 8 egress ports, a trunk set (8, 32, 32, 56) would be assigned as follows: the first trunk connects to all the egress ports on the leftmost L1 switch, the next trunk connects to egress ports in the next 4 L1 switches, and so on. For this baseline wiring, we evaluate both ECMP (Equal-Cost Multi-Path, which splits traffic from L1 to L2 switches evenly) and WCMP (which weights the traffic split using the algorithm in §4). We also compare against a random wiring that randomly assigns external ports to trunks. For this strategy as well, we evaluate ECMP and WCMP-based routing. Overall, we explore a space defined by two dimensions: a routing strategy dimension consisting of two alternatives (ECMP and WCMP), and a wiring dimension with three alternatives (our approach, baseline wiring, and random wiring).

**Metrics and Methodology**: To understand the efficacy of our approach, we use three metrics: upflow (§2), effective capacity (§3), and table size (§4). We also quantify the benefits of our optimizations: finding extreme traffic matrices, the upflow approximation, failure pattern canonicalization, and routing approximation. Our evaluations use two sizes of WAN routers. In a 128-port WAN router, each switch has 16 ports, and there are 16 L1 and 8 L2 switches. For this router, we evaluate all possible trunk sets with four trunks where the number of links in each trunk is divisible by 8. There are 34 such trunk sets, shown on the x-axis in Figure 11. In a 512-port WAN router, each switch has 32 ports, and there are 32 L1 switches and 16 L2 switches. For this router, we evaluate all possible trunk sets with 5 trunks (there are 480 such sets), where the number of links in each trunk is divisible by 16.

**Implementation**: Our experiments use the cdd [16] library to generate the extreme traffic set. We use Gurobi [28] to solve all LP and MILP problems, and Open MPI [33] to parallelize our computations. Our code is available at https://github.com/USC-NSL/Highly-Available-WAN-Router.

### 5.2 Resilience: 128-port WAN Router

Figure 12 (left) quantifies resilience by showing the effective capacity for our approach over different trunk sets, under link failure and L2-switch failure (§3) and under L1-switch failure (§A.7).

**Upflow**: To understand the results in Figure 12, it is important to first understand the efficacy of minimal-upflow trunk wiring. Figure 11 shows the upflow across all the trunk sets in a 128-port router. We use Equation 5 to compute upflow for our approach, random wiring, and baseline wiring. Baseline wiring does not employ early forwarding, but random wiring does. Our optimal wiring approach leads to the lowest upflow rate in all scenarios because it spreads links from the same trunk across L1 switches and maximizes early routing opportunities to minimize upflow. Trunk sets where each trunk has a multiple-of-16 links have no upflow, a consequence of Theorem A.2. By comparison, baseline wiring has an upflow that is sometimes 10× higher. With baseline wiring, the upflow can vary with trunk set. Some trunk sets have more constrained traffic matrices than others: for example, in the trunk set (8, 8, 8, 104) the largest trunk can send at most 24 (normalized) units of traffic even though the trunk capacity is 104. Finally, random wiring yields upflow (averaged over 100 random wirings for each trunk set) that is 2-3× worse than optimal.

**Link Failures**: The upper left plot of Figure 12 plots the resilience of trunks across link failures. The resilience varies by trunk set, but instead of plotting resilience across all trunk sets, we group them into 4 classes by their upflow: these classes demonstrate qualitatively different behaviors. Our approach is able to completely mask up to six concurrent link failures across all trunk sets, and the effective capacity only drops below one after the 7th failure. This is because the maximum upflow across all trunk sets, in Figure 11, is well below 32. In a 128-port WAN router, there are 16 L1 switches and 8 internal links between a pair of L1 and L2 switches. Each switch has at most 2 units of upflow (since the total upflow is less than 32), which requires 2 links to carry the upflow (per L1 switch), so the router can tolerate up to 6 failures. There are 4 classes with respect to link failures. Trunk sets with zero upflow always have an effective capacity of 1 under any link failure. Any trunk set with upflow in (0, 16] starts to degrade after 7 link failures. Similarly, any trunk set with upflow in (16, 32] starts to degrade after 6 link failures. These trunk sets differ slightly in the drop in effective capacity resulting from the 7th failure because of the way the trunk sets are configured. Finally, every trunk set with non-zero upflow has an effective capacity of 0 under 8 failures: no 128-port WAN router can ensure non-blocking behavior under the worst-case failure pattern with 8 concurrent link failures (which occurs when all uplinks on an L1 switch fail).

By comparison, baseline wiring with WCMP cannot mask a single failure (top right figure in Figure 12). The effective capacity in this case is independent of the trunk set. The WAN router capacity degrades gracefully under failure: every link failure drops capacity by 1/8th. Baseline wiring with ECMP performs worse than baseline wiring with WCMP. Table 1 shows the resilience of baseline wiring with ECMP for up to four failures: effective capacity drops by 50% with a single failure, and by nearly 3/4th with 4 failures. While it might be tempting to conclude that routers should implement WCMP to increase failure resilience, we note that an alternative strategy which treats each link failure as the failure of the corresponding L2 switch has the same resilience as baseline wiring with WCMP, so there is really no incentive to deploy WCMP for link failure resilience.

**L2 Switch Failures**: Resilience to L2-switch failures (center left of Figure 12) is identical to that for link failures. An L2 failure removes 1 link from each L1 switch, so our upflow-based categorization still applies. Here too, there are four classes categorized by the value of upflow, and baseline wiring with WCMP (middle right of Figure 12) and ECMP (Figure 13) provide no masking and have identical behavior: with each L2 failure, capacity drops by 1/8th. Random wiring with WCMP (Figure 14), is more resilient than baseline wiring, but has lower effective capacity than minimal-upflow wiring across all failure configurations. For example, minimum upflow wiring ensures full effective capacity with 6 concurrent failures when total upflow is 32. However, random wiring, for the same setting, only has an effective capacity of 0.3. The effective capacity of random wiring with link failure is identical to L2 switch failure, so we have omitted a description of the former.

**L1 Switch Failures**: No approach can mask L1 failures, since these reduce trunk capacity in addition to internal capacity. However, our approach degrades much more gracefully than competing approaches, but the behavior depends on the trunk set configuration. Our 34 trunk sets fall into four classes (lower left of Figure 12) with qualitatively different behavior. These sets depend on two factors: the size of the smallest trunk in the trunk set (either 8, or larger), and whether the minimal-upflow wiring (§2) wires the trunks uniformly across the L1 switches or not. Non-uniform wiring introduces a little asymmetry with a slightly different resilience.

When the minimum trunk size is 8, because our approach spreads the links of these trunks across 8 L1 switches, they can tolerate up to 8 L1 switch failures, with each failure degrading capacity by 1/8th, as shown by the line “Trunk size = 8”. With non-uniform wiring with a minimum trunk size of 8, our approach can tolerate up to 7 failures resulting from non-uniform spreading of the wires (only 7 L1 switches connect to the 8-wire trunk). For a similar reason, trunk sets with a minimum of 16 links in each trunk can tolerate up to 15 L1 switch failures, with capacity drops of around 1/16th at each step (there are slight variations resulting from non-uniformity described earlier).

By contrast, baseline wiring with ECMP (Figure 13) or WCMP (bottom right of Figure 12) can only tolerate up to 4 L1 switch failures, and, for some trunk configurations, may have zero capacity even with a single L1 switch failure. Finally, random wiring with WCMP (Figure 14), performs generally worse than minimal-upflow wiring: the latter generally degrades gracefully, but the capacity degradation in the former is more dramatic (e.g., for the uniform wiring case).

**Simultaneous L1 and L2 Switch Failures**: Our approach can handle simultaneous failures of links, L1 switches, and L2 switches (§A.8). To demonstrate this, Figure 15 shows effective capacity resulting from simultaneous failure of L1 and L2 switches for a specific trunk set in the L2-category “Upflow in (16, 32)” and L1-category “Non-uniform, Trunk size > 8”. The effective capacity is a combination of the results from those categories: for instance, the dip in effective capacity for the 7th L2 switch failure in Figure 12 is also visible in this plot.

### 5.3 Resilience: 512-port WAN Router

Figure 16 shows the effective capacity of 512-port router trunk sets under L1 and L2 switch failures. For this router, there are 480 trunk sets. In computing the effective capacity, we use the approximation formulation (§2) to compute the minimal-upflow wiring. That we are able to obtain these results demonstrates the scalability of our approximation: in §5.5, we quantify the optimality gap introduced by the approximation. We have also computed effective capacity for random wiring for a 512-port router. These results are qualitatively similar to those for the 128-port router, so we omit them for brevity.

**L1 Switch Failures**: Our trunk sets exhibit three qualitatively different classes of behavior. These classes depend on two factors: the minimum trunk size in a trunk set, and whether the optimal wiring spreads a trunk’s links uniformly across L1 switches or not. When trunks are uniform, degradation is graceful, but, for obvious reasons, when the minimum trunk set size is 16, the trunk set can only tolerate up to 16 failures. With non-uniform wiring, the degradation is steeper for the first few failures as a result of asymmetry.

**L2 Switch Failures**: As with the 128-port case, we observe four qualitatively different types of behavior with L2 switch failures. Every trunk set with zero upflow can mask all L2 switch failures. Every trunk set having upflow in (0, 32] requires one internal link per L1 switch to carry traffic, so can sustain 15 L2 switch failures. Every trunk set with 64 units of upflow requires 2 L2 switches to carry traffic, so the effective capacity becomes 0.5 when 15 L2 switches fail. For trunk sets with upflow in (32, 64), the effective capacity is slightly better than the 64-unit upflow case due to slightly lower upflow.

**Link Failures**: As with the 128-port router, we find that the resilience under link failure is similar to that under L2 switch failures, so we omit this graph for brevity.

### 5.4 Compact Routing Tables

We now show that our routing table compaction technique (§4) results in tables that do not exceed hardware routing table limits. To do this, we compute, for the two sizes of routers, the largest table size at any switch, across all trunk sets, for any combination of L1 failures, and (separately) for any combination of L2 failures that are completely masked. For the 512-port router, we compute the tables using the scaling approximation (§4).

Table 2 shows the table sizes. For calibration, the hardware limit on the multipath table in modern switches is 65K [32]. Our approach (last column) uses at most 388 and 64 entries for a 128-port router and at most 3310 and 346 entries for a 512-port router under L1 and L2 failures. We also see that the routing table sizes are relatively insensitive to L2 failures because the optimization assigns WCMP weights sparsely to minimize table sizes: every L1 switch sends traffic over a few links to L2 switches, and other links that do not carry traffic are not assigned WCMP weights. So, when a link with non-zero weight fails, our algorithm moves, to another active link, the weights assigned to that link without increasing table sizes.

ECMP with baseline wiring in the absence of failures (first column) uses fewer entries, because links of the same trunk connect to the same L1 switch, which permits grouping of entries (an L2 switch only needs 1 entry per cross-trunk pair and per L1 switch). The grouping is less effective for arbitrary wiring (two middle columns). Random wiring requires 186 and 640 entries, while the minimal-upflow wiring requires 192 and 640 entries for 128-port and 512-port routers. Our routing optimization compacts the entries effectively.