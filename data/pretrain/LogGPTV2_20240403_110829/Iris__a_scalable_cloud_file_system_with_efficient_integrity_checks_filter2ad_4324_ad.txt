chronous and never holds a lock while waiting for network or disk
I/O operations. The code runs in user space as a transparent layer
that can take advantage of any existing storage system at the cloud
provider. Our implementation uses the open-source .NET frame-
work Mono, which is advantageously platform-independent: Iris
can run on Linux, Windows, and MAC OS.
Implementation
Our implementation includes the Portal, a simple Cloud storage
server, and clients that run traces and benchmarks, as depicted in
the detailed system architecture in Figure 1.
6.1 Cloud
The cloud stores not only regular ﬁle system data, but also au-
thenticating meta-data, including MAC ﬁles and our Merkle tree
authenticated data structure, as well as checkpointed parities needed
for recovery. The repositories for these data types are shown at the
top of Figure 1.
The portal performs reads and writes to the various data reposi-
tories by invoking their respective cloud-side services. The Cloud
File System Service handles requests for ﬁle blocks, MAC ﬁles, and
the Merkle tree (stored in our implementation in an NTFS ﬁle sys-
tem). Operations on ﬁle blocks are executed asynchronously at the
portal. Sequential access operations to the same ﬁle can potentially
arrive out of order at the cloud. (Re-ordering can occur in transit on
the network, as our portal and cloud machines are each equipped
with three network cards.) To reduce disk spinning, the Cloud File
System Service orders requests to the same ﬁle in increasing order
by block offset.
6.2 Portal
The portal interacts with multiple clients which issue ﬁle sys-
tem calls to the Portal Service. The portal executes client opera-
tions in parallel: Each operation is executed in a thread pool as a
user-scheduled task with asynchronous steps. When an operation
is waiting for a long running step such as disk or network I/O, the
task is paused and the current thread switches to another task. This
allows thousands of simultaneously active operations to be handled
by the thread pool with a small number of threads. In our setup,
the thread pool had 16 threads—one for each virtual CPU core, for
maximum parallelism.
Operations don’t interact directly with the cloud, but instead with
the Merkle Tree and Block Caches. All data and meta-data re-
quested by the caches is downloaded from the cloud via the Storage
Interface in the portal. While in use by an active operation, blocks
and nodes are retained in the cache. Prior to being cached, how-
ever, blocks and nodes downloaded from the cloud are checked for
integrity by the Integrity Checker components.
Our implementation beneﬁts from multi-core functionality avail-
able in most modern computers. Operations performed on active
blocks in the cache are split into atomic operations (e.g., hash up-
date for a tree node, check MAC for a data block or compact nodes
in ﬁle version trees). These are inserted into various priority queues
maintained at the portal. Multiple threads seize blocks from these
queues, lock them and execute the atomic operations. Operations
are always started in order, but may complete out of order. How-
ever, our implementation ensures that the effect of the operations on
the system is the same as if they were executed by a single thread
in order.
If multiple clients issue conﬂicting operations simulta-
neously, they are executed in the order in which they arrive to the
Portal. It is the responsibility of the clients to perform locking out-
of-band.
Distributing the Portal. For scalability, the portal can be dis-
tributed across multiple machines. Each portal machine would then
be responsible for a subtree of the ﬁle system. When clients ﬁrst
mount the ﬁle system, they can contact any one of the portals to
get the assignment of portal machine to subtrees. As the ﬁle sys-
tem changes over time, a subtree may grow or shrink substantially,
and then the subtree assignment will need to be rebalanced by split-
ting a subtree and copying one part of it onto another portal. Our
current implementation does not support this, but we would like to
point out that even with a single portal, Iris can achieve a through-
put of up to 260 MB/s, which already exceeds the bandwidth to the
cloud for many enterprises. Additional challenges (e.g., caching to
reduce latency) arise when the portal is geographically distributed,
but these are out of the scope of this paper.
6.2.1 Merkle tree cache
The Merkle Tree Cache in the portal is Iris’s most complex com-
ponent. Much of the design effort and complexity of Iris lies in
the caching strategy for recently accessed portions of the tree. We
designed a generic, efﬁcient Merkle Tree Cache that ensures con-
sistency across thousands of simultaneous asynchronous client op-
erations.
When an operation accesses the cache, it ﬁrst locks it using a mu-
tex and unlocks it when it’s done. All of the operations are designed
such that they access the cache for a very short period of time for
tasks such as changing the value of a few ﬁelds of a Merkle tree
node. To ensure a high degree of parallelism, the Merkle tree mu-
tex is never locked while an operation waits for a long running step
such as network or disk I/O.
When executing operations in parallel, a real challenge is to han-
dle dependencies among tree nodes and maintain data structure
235
consistency and integrity. We do this by imposing several order-
ings of operations. Nodes are brought into the cache in a top-down
order and are evicted in a bottom-up order. The top-down ordering
is necessary because when a node is read from the untrusted stor-
age, it can only be veriﬁed once all of its ancestors have also been
cached in and veriﬁed. Likewise, a node can only be written out
to the untrusted storage after the hash of its subtree has been com-
puted. If multiple nodes in a sub-tree are modiﬁed, the Merkle Tree
Cache will only hash the shared path to the root once, thereby sig-
niﬁcantly reducing the number of hashes that need to be performed.
Phases. To enforce the ordering, each node is always in one of the
following phases: Reading, Verifying, Neutral, Compacting, Up-
datingHash, or Writing. A node always traverses these phases in
order and only after its parent or children have reached a certain
phase. For example, a node only enters the verifying phase after its
parent has completed the verifying phase. The Reading and Veri-
fying phases are applied top-down and the Compacting, Updating-
Hash, and Writing phases are applied bottom-up. When a node is
in the Neutral phase, it is in the cache and available to be used by
operations.
Pinning. Operations oftentimes need to access multiple nodes. For
example, a WriteFile operation needs to access the path in the ver-
sion tree that descends all the way to the version node correspond-
ing to a speciﬁc block. The operations ﬁrst pin all of the nodes
they need and then proceed to execute. If a node is needed by an
operation and is not currently in the cache, the operation is paused
and resumed when all of its pinned nodes have been loaded into
the cache. Once a node is pinned, it is not cached out until it is
unpinned (e.g., when the operation completes). A node may be
pinned multiple times, in which case it must be unpinned the same
number of times until it is considered in the unpinned state and may
be cached out.
If a node is pinned, its ancestors, sibling, and siblings of the
ancestors are automatically indirectly pinned. This is necessary be-
cause if the node is modiﬁed, the indirectly pinned nodes will be
needed when updating the hashes of the path to that node.
Eviction. When the cache reaches its maximum allowed size, it
repeatedly evicts least-recently-used (LRU) leaf nodes, causing a
bottom-up wave of evictions. Evicting a node consists of transi-
tioning its phase from the Neutral to Compacting. The node then
goes through the UpdatingHash and Writing phases until it is ﬁ-
nally removed from the cache. If a node and its subtree were not
modiﬁed, then the UpdatingHash and Writing phases are skipped.
If all of the nodes are pinned, then new operations block until the
currently executing operations complete and unpin more nodes.
6.2.2 Other components
The Block Cache functions much like the Merkle Tree Cache
except that blocks don’t have parents/children so there are no de-
pendencies between blocks.
The Merkle Tree and Block Caches keep track of two items per
node/block: The old and new data. The old data is the value of the
node/block when it was fetched from the cloud. The new data is
its value after it was (possibly) modiﬁed by an operation. When a
node/block is evicted, the portal computes the difference of the byte
representations of the old and new data and updates the parities.
Another component of the portal is the auditing module. This
service, periodically invoked by the portal, transmits a PoR chal-
lenge to the cloud and receives and veriﬁes the response, consisting
simply of a set of randomly selected data blocks in the ﬁle system
and their associated Merkle tree paths. The portal also maintains
a repository of Parities to recover from ﬁle system corruptions de-
236
tected in a PoR, seen in the portal cache module in Figure 1. Parities
undergo frequent modiﬁcation: Multiple parities are updated with
every ﬁle-block write. Thus, the Parities repository sits in the main
memory of the portal.
The portal can include a checkpointing service that backs up data
stored in the main memory at the portal to local permanent storage.
To enable recovery in the event of a portal crash, checkpointed data
can be periodically transmitted to the cloud (with a MAC for in-
tegrity). While we have not implemented this component, it can
rely on well-known checkpointing techniques.
7. Experimental evaluation
We ran several experiments to test different aspects of Iris. We
ﬁrst describe our setup and then present our results. Two machines
ran the full end-to-end system implementation described in Sec-
tion 6: The Portal and the Cloud.
Portal Computer. The Portal computer has an Intel Core i7 pro-
cessor and 12 GB of RAM. The experiments were run on Windows
7 64-bit installed on a rotational disk, but no data was written to the
Portal’s hard drive for the purpose of our experiments.
Cloud Computer. The Cloud computer has seven rotational hard
drives with 1TB of storage each. The ﬁle system and MAC ﬁles
reside on these disks. The disks are used as separate devices and
are not conﬁgured as a RAID array. This conﬁguration mimics a
cloud where each disk could potentially be on a separate physical
machine. The operating system (Windows 7 64-bit) runs on an sep-
arate additional hard drive to avoid interfering with our experiment.
Networking. Because our ﬁle system can handle very large through-
put, we used three 1Gbps cables to connect the two computers.
Each computer had one network port on the motherboard and two
additional network cards. After accounting for networking over-
head, the 3 Gbps combined connections between the two comput-
ers can handle about 280 MB/s of data transfer as our experiments
show.
Conﬁguration.
In our conﬁguration, write operations originate
from clients (simulated as threads on the Portal). Then they are
processed by the Portal and multiplexed over the three network con-
nections. Finally, data reaches the Cloud computer and is written
to the appropriate disk. Reads are similarly processed, but the data
ﬂow is in the opposite direction (from the Cloud machine to the
Portal).
Simulated Latency. To obtain more realistic results, we deliber-
ately simulated 20ms round-trip time (RTT) latency between the
clients and Portal, and 100ms RTT latency between the Portal and
Cloud. This setting aims to resemble the scenario where the clients
and Portal are both part of the same corporate network and the
Cloud is a data center located elsewhere on the same continent.
7.1 Workloads
To evaluate Iris, we used the following workloads. Each work-
load was recorded as a trace and played back exactly under different
parameterizations of our system.
• Tar/Untar (directory structure): Benchmarks access and mod-
ify operations on a tarball consisting of the entire Linux ker-
nel source (420 MB, 37,000 ﬁles, and 2,300 directories).
• IOZone (various ﬁle access patterns): IOZone [2] bench-
mark of combining various operations (reread/rewrite, ran-
dom read/write, backwards read, and strided read).
• Sequential Read/Write (throughput): Measures the perfor-
mance of sequentially reading/writing ten ﬁles simultane-
ously, each of size 10 GB.
• Random Read/Write (seeks): Measures the performance of
Figure 5: Avg sequential read &
write speed.
Figure 4: Workloads under different Merkle Tree Cache sizes. Time for the
workload to complete vs the Merkle Tree Cache size.
Figure 6: PoR Encoding Rate.
Portal Cache:
Create ﬁle in directory of depth 0
Create ﬁle in directory of depth 1
Create ﬁle in directory of depth 2
Create ﬁle in directory of depth 3
List directory with 10 ﬁles at depth 1
Write 1 MB ﬁle at depth 1, wait completed
Read 1 MB ﬁle at depth 1
Total Latency (ms)
Hot
20.0
20.0
20.0
20.0
27.7
24.8
20.0
Cold
20.0
144.0
254.0
363.0
678.9
138.8
284.2
Network I/O