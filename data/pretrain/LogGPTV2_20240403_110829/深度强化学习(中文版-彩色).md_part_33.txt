### 5.2 减少方差的方法

在策略梯度方法中，从奖励函数中减去一个基准函数是一种常见的降低方差的技术。假设我们需要估计随机变量 \(X\) 的期望 \(E[X]\)。对于任意一个期望为0的随机变量 \(Y\)，我们知道 \(X - Y\) 仍然是 \(E[X]\) 的无偏估计。此外，\(X - Y\) 的方差为：

\[
V(X - Y) = V(X) + V(Y) - 2 \text{cov}(X, Y)
\]

其中，\(V\) 表示方差，\(\text{cov}(X, Y)\) 表示 \(X\) 和 \(Y\) 的协方差。如果 \(Y\) 本身的方差较小，并且与 \(X\) 高度正相关，那么 \(X - Y\) 将是一个方差较小的关于 \(E[X]\) 的无偏估计。

在策略梯度方法中，基准函数的常见选择是状态价值函数 \(V(S_t)\)。下一节将介绍，这种算法与初版 Actor-Critic 算法非常相似。最近的一些研究工作提出了其他不同的基准函数选择，感兴趣的读者可以参考文献 (Li et al., 2018; Liu et al., 2017; Wu et al., 2018) 了解更多细节。

### 5.3 Actor-Critic

**Actor-Critic 算法**（Konda et al., 2000; Sutton et al., 2000）是一种既基于策略也基于价值的方法。在初版策略梯度方法中，可以用状态价值函数作为基准函数来降低梯度估计的方差。Actor-Critic 算法沿用了这一思想，同时学习行动者（Actor）函数（即智能体的策略函数 \(\pi(a|s)\)）和批判者（Critic）函数（即状态价值函数 \(V^\pi(s)\)）。此外，Actor-Critic 算法还采用了自举法（Bootstrapping）的思想来估计 Q 值函数。REINFORCE 中的误差项 \(\sum_{t=i}^{\infty} \gamma^{t-i} R_t - b(S_i)\) 被时间差分误差取代，即 \(R_i + \gamma V^\pi(S_{i+1}) - V^\pi(S_i)\)。

我们采用 L 步的时间差分误差，并通过最小化该误差的平方来学习批判者函数 \(V^\pi_\theta(s)\)，即：

\[
\psi \leftarrow \psi - \eta_\psi \nabla_\psi J(V^\pi_\theta(\psi))
\]

其中，\(\psi\) 表示学习批判者函数的参数，\(\eta_\psi\) 是学习步长，并且：

\[
J(V^\pi_\theta(\psi)) = \frac{1}{2} \sum_{t=i}^{i+L-1} \left( \gamma^{t-i} R_t + \gamma^L V^\pi_\theta(S'_{i+L-1}) - V^\pi_\theta(S_i) \right)^2
\]

\[
\nabla_\psi J(V^\pi_\theta(\psi)) = \left( V^\pi_\theta(S_i) - \sum_{t=i}^{i+L-1} \gamma^{t-i} R_t - \gamma^L V^\pi_\theta(S'_{i+L-1}) \right) \nabla_\psi V^\pi_\theta(S_i)
\]

类似地，行动者函数 \(\pi_\theta(a|s)\) 决定每个状态 \(s\) 上所采取的动作或动作空间上的概率分布。我们采用类似于初版策略梯度的方法来学习这个策略函数：

\[
\theta \leftarrow \theta + \eta_\theta \nabla_\theta J(\theta)
\]

其中，\(\theta\) 表示行动者函数的参数，\(\eta_\theta\) 是学习步长，并且：

\[
\nabla_\theta J(\theta) = E_{\tau, \theta} \left[ \sum_{i=0}^\infty \nabla \log \pi_\theta(A_i | S_i) \left( \sum_{t=i}^{i+L-1} \gamma^{t-i} R_t + \gamma^L V^\pi_\theta(S'_{i+L-1}) - V^\pi_\theta(S_i) \right) \right]
\]

注意到，我们分别用 \(\theta\) 和 \(\psi\) 来表示策略函数和价值函数的参数。在实际应用中，当我们选择用神经网络来表示这两个函数时，通常会让两个网络共享一些底层的网络层作为共同的状态表征（State Representation）。此外，AC 算法中的 \(L\) 值经常设为 1，即 TD(0) 误差。AC 算法的具体步骤如算法 5.19 所示。

#### 算法 5.19: Actor-Critic 算法

**超参数**: 步长 \(\eta_\theta\) 和 \(\eta_\psi\)，奖励折扣因子 \(\gamma\)。

**输入**: 初始策略函数参数 \(\theta_0\)、初始价值函数参数 \(\psi_0\)。

1. 初始化 \(\theta = \theta_0\) 和 \(\psi = \psi_0\)。
2. 对于 \(t = 0, 1, 2, \ldots\)：
   1. 执行一步策略 \(\pi_\theta\)，保存 \(\{S_t, A_t, R_t, S_{t+1}\}\)。
   2. 估计优势函数 \(\hat{A}_t = R_t + \gamma V^\pi_\theta(S_{t+1}) - V^\pi_\theta(S_t)\)。
   3. 更新策略函数：\(\theta \leftarrow \theta + \eta_\theta \nabla_\theta J(\theta)\)。
   4. 更新价值函数：\(\psi \leftarrow \psi - \eta_\psi \nabla_\psi J(V^\pi_\theta(\psi))\)。
3. 返回 \((\theta, \psi)\)。

值得注意的是，AC 算法也可以使用 Q 值函数作为其批判者。在这种情况下，优势函数可以用以下式子估计：

\[
Q(s, a) - V(s) = Q(s, a) - \sum_a \pi(a|s) Q(s, a)
\]

用来学习 Q 值函数这个批判者的损失函数为：

\[
J_Q = \left( R_t + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right)^2
\]

或者

\[
J_Q = \left( R_t + \gamma \sum_a \pi_\theta(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right)^2
\]

这里动作 \(A_{t+1}\) 由当前策略 \(\pi_\theta\) 在状态 \(S_{t+1}\) 下采样而得。

### 5.4 生成对抗网络和 Actor-Critic

乍看之下，生成对抗网络（Generative Adversarial Networks, GAN）(Goodfellow et al., 2014) 和 Actor-Critic 似乎是截然不同的算法，用于不同的机器学习领域，一个是生成模型，而另一个是强化学习算法。但实际上，它们的结构非常相似。对于 GAN，有两个部分：用于根据某些输入生成对象的生成网络，以及紧接生成网络的用于判断生成对象真实与否的判别网络。对于 Actor-Critic 方法，也有两部分：根据状态输入生成动作的动作网络，以及一个紧接动作网络之后用价值函数（比如下一个动作的价值或 Q 值）评估动作好坏的批判网络。

因此，GAN 和 Actor-Critic 基本遵循相同的结构。在这个结构中有两个相继的部分：一个用于生成物体，第二个用一个分数来评估生成物体的好坏；随后选择一个优化过程来使第二部分能够准确评估，并通过第二部分反向传播梯度到第一部分来保证它生成我们想要的内容，通过一个定义为损失函数的标准，也就是一个来自结构第二部分的分数或价值函数来实现。

GAN 和 Actor-Critic 的结构详细比较如图 5.1 所示。

**图 5.1** 对比 GAN 和 Actor-Critic 的结构。在 GAN 中，\(z\) 是输入的噪声变量，可以从如正态分布中采样，而 \(x\) 是从真实目标中采集的数据样本。在 Actor-Critic 中，\(s\) 和 \(a\) 分别表示状态和动作。

- **对于第一个生成物体的部分**：GAN 中的生成器和 Actor-Critic 中的行动者基本一致，包括其前向推理过程和反向梯度优化过程。对于前向过程，生成器采用随机变量做输入，并输出生成的对象；对于方向优化过程，它的目标是最大化对生成对象的判别分数。行动者用状态作为输入并输出动作，对于优化来说，它的目标是最大化状态-动作对的评估值。
- **对于第二个评估物体的部分**：判别器和批判者由于其功能不同而优化公式也不同，但遵循相同的目标。判别器有来自真实对象额外输入。它的优化规则是最大化真实对象的判别值而最小化生成对象的判别值，这与我们的需要相符。对于批判者，它使用时间差分（Temporal Difference, TD）误差作为强化学习中的一种自举方法来按照最优贝尔曼方程优化价值函数。

也有一些其他模型彼此非常接近。例如，自动编码器（Auto-Encoder, AE）和 GAN 可以是彼此的相反结构等。注意到，不同深度学习框架中的相似性可以帮助你获取关于现有不同领域方法共性的认识，而这有助于为未解决的问题提出新的方法。

### 5.5 同步优势 Actor-Critic

同步优势 Actor-Critic（Synchronous Advantage Actor-Critic, A2C）(Mnih et al., 2016) 与上一节讨论的 Actor-Critic 算法非常相似，只是在 Actor-Critic 算法的基础上增加了并行计算的设计。如图 5.2 所示，全局行动者和全局批判者在 Master 节点维护。每个 Worker 节点的增强学习智能体通过协调器和全局行动者、全局批判者对话。在这个设计中，协调器负责收集各个 Worker 节点上与环境交互的经验（Experience），然后根据收集到的轨迹执行一步更新。更新之后，全局行动者被同步到各个 Worker 上继续和环境交互。在 Master 节点上，全局行动者和全局批判者的学习方法和 Actor-Critic 算法中行动者和批判者的学习方法一致，都是使用 TD 平方误差作为批判者的损失函数，以及 TD 误差的策略梯度来更新行动者的。

**图 5.2** A2C 基本框架

在这种设计下，Worker 节点只负责和环境交互。所有的计算和更新都发生在 Master 节点。实际应用中，如果希望降低 Master 节点的计算负担，一些计算也可以转交给 Worker 节点，比如说，每个 Worker 节点保存了当前全局批判者（Critic）。收集了一个轨迹之后，Worker 节点直接在本地计算给出全局行动者（Actor）和全局批判者的梯度。这些梯度信息继而被传送回 Master 节点。最后，协调器负责收集和汇总从各个 Worker 节点收集到的梯度信息，并更新全局模型。同样地，更新后的全局行动者和全局批判者被同步到各个 Worker 节点。A2C 算法的基本框架如算法 5.20 所示。

**算法 5.20: A2C**

**Master:**
- **超参数**: 步长 \(\eta_\psi\) 和 \(\eta_\theta\)，Worker 节点集 \(W\)。
- **输入**: 初始策略函数参数 \(\theta_0\)、初始价值函数参数 \(\psi_0\)。
- **初始化**: \(\theta = \theta_0\) 和 \(\psi = \psi_0\)。
- 对于 \(k = 0, 1, 2, \ldots\):
  1. \((g_\psi, g_\theta) = 0\)
  2. 对于 \(W\) 里的每一个 Worker 节点:
     1. \((g_\psi, g_\theta) = (g_\psi, g_\theta) + \text{worker}(V^\pi_\theta, \pi_\theta)\)
  3. \(\psi \leftarrow \psi - \eta_\psi g_\psi\); \(\theta \leftarrow \theta + \eta_\theta g_\theta\)。

**Worker:**
- **超参数**: 奖励折扣因子 \(\gamma\)、轨迹长度 \(L\)。
- **输入**: 价值函数 \(V^\pi_\theta\)、策略函数 \(\pi_\theta\)。
- 执行 \(L\) 步策略 \(\pi_\theta\)，保存 \(\{S_t, A_t, R_t, S_{t+1}\}\)。
- 估计优势函数 \(\hat{A}_t = R_t + \gamma V^\pi_\theta(S_{t+1}) - V^\pi_\theta(S_t)\)。

### 5.6 异步优势 Actor-Critic

异步优势 Actor-Critic（Asynchronous Advantage Actor-Critic, A3C）是在 A2C 的基础上进一步改进的算法，主要区别在于 A3C 使用异步更新机制。在 A3C 中，多个 Worker 节点独立地与环境交互，并在各自的本地副本上进行更新，然后将更新后的参数发送给 Master 节点。Master 节点则负责汇总这些更新，并定期将最新的参数同步回各个 Worker 节点。这种异步更新机制使得 A3C 更加高效，尤其是在大规模分布式系统中。

A3C 的基本框架与 A2C 类似，但主要区别在于 Worker 节点的更新方式。具体来说，A3C 的 Worker 节点在本地执行多步策略，并在每一步结束后立即更新本地参数，而不是等待整个轨迹完成后再更新。这样可以显著提高训练速度和效率。

**算法 5.21: A3C**

**Master:**
- **超参数**: 步长 \(\eta_\psi\) 和 \(\eta_\theta\)，Worker 节点集 \(W\)。
- **输入**: 初始策略函数参数 \(\theta_0\)、初始价值函数参数 \(\psi_0\)。
- **初始化**: \(\theta = \theta_0\) 和 \(\psi = \psi_0\)。
- 对于 \(k = 0, 1, 2, \ldots\):
  1. 接收来自 Worker 节点的更新 \((g_\psi, g_\theta)\)。
  2. \(\psi \leftarrow \psi - \eta_\psi g_\psi\); \(\theta \leftarrow \theta + \eta_\theta g_\theta\)。
  3. 将更新后的 \(\psi\) 和 \(\theta\) 发送给所有 Worker 节点。

**Worker:**
- **超参数**: 奖励折扣因子 \(\gamma\)、轨迹长度 \(L\)。
- **输入**: 价值函数 \(V^\pi_\theta\)、策略函数 \(\pi_\theta\)。
- 对于 \(t = 0, 1, 2, \ldots\):
  1. 执行一步策略 \(\pi_\theta\)，保存 \(\{S_t, A_t, R_t, S_{t+1}\}\)。
  2. 估计优势函数 \(\hat{A}_t = R_t + \gamma V^\pi_\theta(S_{t+1}) - V^\pi_\theta(S_t)\)。
  3. 更新策略函数：\(\theta \leftarrow \theta + \eta_\theta \nabla_\theta J(\theta)\)。
  4. 更新价值函数：\(\psi \leftarrow \psi - \eta_\psi \nabla_\psi J(V^\pi_\theta(\psi))\)。
  5. 将更新后的 \(\psi\) 和 \(\theta\) 发送给 Master 节点。

通过这种方式，A3C 实现了高效的并行训练，从而在大规模环境中表现出色。