B t=1 ℓ=0 θ t,ℓ t,ℓ t,ℓ
θ =θ+η ∇J(θ)
θ
用{S ,A ,R }更新b(S )
t,ℓ t,ℓ t,ℓ t,ℓ
endfor
返回θ
直观来讲，从奖励函数中减去一个基准函数这个方法是一个常见的降低方差的方法。假设需
要估计一个随机变量X 的期望E[X]。对于任意一个期望为0的随机变量Y，我们知道X −Y
依然是E[X]的一个无偏估计。而且，X−Y 的方差为
V(X−Y)=V(X)+V(Y)−2cov(X,Y). (5.5)
式子中的 V 表示方差，cov(X,Y) 表示 X 和 Y 的协方差。所以如果 Y 本身的方差较小，而且
和 X 高度正相关，那么 X −Y 会是一个方差较小的关于 E[X] 的无偏估计。在策略梯度方法
中，基准函数的常见选择是状态价值函数V(S )。在下一节中我们可以看到，这个算法和初版的
i
Actor-Critic算法很相像。最近的一些研究工作也提出了其他不同的基准函数的选择，感兴趣的读
者可以从文献(Lietal.,2018;Liuetal.,2017;Wuetal.,2018)中了解更多的细节。
148
5.3 Actor-Critic
5.3 Actor-Critic
Actor-Critic算法(Kondaetal.,2000;Suttonetal.,2000)是一个既基于策略也基于价值的方法。
在上一节我们提到，在初版策略梯度方法中可以用状态价值函数作为基准函数来降低梯度估计的
方差。Actor-Critic算法也沿用了相同的想法，同时学习行动者（Actor）函数（也就是智能体的策略
函数π(·|s)）和批判者（Critic）函数（也就是状态价值函数Vπ(s)）。此外，Actor-Critic算法还沿用
P
了自举法（Bootstrapping）的思想来估计Q值函数。REINFORCE中的误差项 ∞ γt−iR −b(S )
t=i t i
被时间差分误差取代了，即R +γVπ(S )−Vπ(S )。
i i+1 i
我们这里采用L步的时间差分误差，并通过最小化该误差的平方来学习批判者函数Vπθ(s)，
ψ
即
ψ ←ψ−η ψ∇J Vπθ(ψ). (5.6)
ψ
式子中ψ表示学习批判者函数的参数，η 是学习步长，并且
ψ
0 1
2
i+XL−1
J ψπθ(ψ)= 21@ γt−iR t+γLV ψπθ(S′ )−V ψπθ(S i)A , (5.7)
V
t=i
S′是智能体在π 下L步之后到达的状态，所以
θ
0 1
i+XL−1
∇J Vπθ(ψ)=@ V ψπθ(S i)− γt−iR t−γLV ψπθ(S′ )A∇V ψπθ(S i). (5.8)
ψ
t=i
类似地，行动者函数π (·|s)决定每个状态s上所采取的动作或者动作空间上的一个概率分
θ
布。我们采用和初版策略梯度相似的方法来学习这个策略函数。
θ =θ+η ∇J (θ), (5.9)
θ πθ
这里θ表示行动者函数的参数，η 是学习步长，并且
θ
2 0 13
6X∞ i+XL−1
7
∇J(θ)=E 4 ∇logπ (A |S )@ γt−iR +γLVπθ(S′ )−Vπθ(S )A5. (5.10)
τ,θ θ i i t ψ ψ i
i=0 t=i
注意到，我们这里分别用了θ和ψ来表示策略函数和价值函数的参数。在实际应用中，当我
们选择用神经网络来表示这两个函数的时候，经常会让两个网络共享一些底层的网络层作为共同
的状态表征（StateRepresentation）。此外，AC算法中的L值经常设为1,也就是TD(0)误差。AC
149
第5章 策略梯度
算法的具体步骤如算法5.19所示。
算法5.19Actor-Critic算法
超参数: 步长η 和η ,奖励折扣因子γ。
θ ψ
输入: 初始策略函数参数θ 、初始价值函数参数ψ 。
0 0
初始化θ =θ 和ψ =ψ 。
0 0
fort=0,1,2,··· do
执行一步策略π ,保存{S ,A ,R ,S }。
θ t t t t+1
估计优势函数Aˆ =R +γVπθ(S )−Vπθ(S )。
P t t ψ t+1 ψ t
J(θ)= logπ (A |S )Aˆ
t P θ t t t
J Vπθ(ψ)= tAˆ2
t
ψ
ψ =ψ+η ψ∇J Vπθ(ψ),θ =θ+η θ∇J(θ)
ψ
endfor
返回(θ,ψ)
值得注意的是，AC算法也可以使用Q值函数作为其批判者。在这种情况下，优势函数可以
用以下式子估计。
X
Q(s,a)−V(s)=Q(s,a)− π(a|s)Q(s,a). (5.11)
a
用来学习Q值函数这个批判者的损失函数为
J = R +γQ(S ,A )−Q(S ,A ) 2 , (5.12)
Q t t+1 t+1 t t
或者
!
X 2
J = R +γ π (a|S )Q(S ,a)−Q(S ,A ) . (5.13)
Q t θ t+1 t+1 t t
a
这里动作A 由当前策略π 在状态S 下取样而得。
t+1 θ t+1
5.4 生成对抗网络和 Actor-Critic
初看上去，生成对抗网络（GenerativeAdversarialNetworks，GAN）(Goodfellowetal.,2014)和
Actor-Critic应该是截然不同的算法，用于不同的机器学习领域，一个是生成模型，而另一个是强
化学习算法。但是实际上它们的结构十分类似。对于GAN，有两个部分：用于根据某些输入生成
对象的生成网络，以及紧接生成网络的用于判断生成对象真实与否的判别网络。对于Actor-Critic
方法，也有两部分：根据状态输入生成动作的动作网络，以及一个紧接动作网络之后用价值函数
150
5.4 生成对抗网络和Actor-Critic
（比如下一个动作的价值或Q值）评估动作好坏的批判网络。
因此，GAN和Actor-Critic基本遵循相同的结构。在这个结构中有两个相继的部分：一个用
于生成物体，第二个用一个分数来评估生成物体的好坏；随后选择一个优化过程来使第二部分能
够准确评估，并通过第二部分反向传播梯度到第一部分来保证它生成我们想要的内容，通过一个
定义为损失函数的标准，也就是一个来自结构第二部分的分数或价值函数来实现。
GAN和Actor-Critic的结构详细比较如图5.1所示。
N
图5.1 对比GAN和Actor-Critic的结构。在GAN中，z是输入的噪声变量，它可以从如正态分布
中采样，而x是从真实目标中采集的数据样本。在Actor-Critic中，s和a分别表示状态和
动作
• 对第一个生成物体的部分：GAN中的生成器和Actor-Critic中的行动者基本一致，包括其前
向推理过程和反向梯度优化过程。对于前向过程，生成器采用随机变量做输入，并输出生
成的对象；对于方向优化过程，它的目标是最大化对生成对象的判别分数。行动者用状态
作为输入并输出动作，对于优化来说，它的目标是最大化状态-动作对的评估值。
• 对于第二个评估物体的部分：判别器和批判者由于其功能不同而优化公式也不同，但是遵循
相同的目标。判别器有来自真实对象额外输入。它的优化规则是最大化真实对象的判别值而
最小化生成对象的判别值，这与我们的需要相符。对于批判者，它使用时间差分（Temporal
Difference，TD）误差作为强化学习中的一种自举方法来按照最优贝尔曼方程优化价值函数。
也有一些其他模型彼此非常接近。举例来说，自动编码器（Auto-Encoder，AE）和GAN可
151
第5章 策略梯度
以是彼此的相反结构等。注意到，不同深度学习框架中的相似性可以帮助你获取关于现有不同领
域方法共性的认识，而这有助于为未解决的问题提出新的方法。
5.5 同步优势 Actor-Critic
同步优势Actor-Critic（SynchronousAdvantageActor-Critic，A2C）(Mnihetal.,2016)和上一
节讨论的Actor-Critic算法非常相似，只是在Actor-Critic算法的基础上增加了并行计算的设计。
如图5.2所示，全局行动者和全局批判者在Master节点维护。每个Worker节点的增强学习
智能体通过协调器和全局行动者、全局批判者对话。在这个设计中，协调器负责收集各个Worker
节点上与环境交互的经验（Experience），然后根据收集到的轨迹执行一步更新。更新之后，全局
行动者被同步到各个Worker上继续和环境交互。在Master节点上，全局行动者和全局批判者的
学习方法和 Actor-Critic 算法中行动者和批判者的学习方法一致，都是使用 TD 平方误差作为批
判者的损失函数，以及TD误差的策略梯度来更新行动者的。
图5.2 A2C基本框架
在这种设计下，Worker节点只负责和环境交互。所有的计算和更新都发生在Master节点。实
际应用中，如果希望降低Master节点的计算负担，一些计算也可以转交给Worker节点4，比如说，
每个Worker节点保存了当前全局批判者（Critic）。收集了一个轨迹之后，Worker节点直接在本
地计算给出全局行动者（Actor）和全局批判者的梯度。这些梯度信息继而被传送回Master节点。
最后，协调器负责收集和汇总从各个Worker节点收集到的梯度信息，并更新全局模型。同样地，
更新后的全局行动者和全局批判者被同步到各个Worker节点。A2C算法的基本框架如算法5.20
所示。
4这经常取决于每个Worker节点的计算能力，比如是否有GPU计算能力，等等。
152
5.6 异步优势Actor-Critic
算法5.20A2C
Master:
超参数: 步长η 和η ,Worker节点集W。
ψ θ
输入: 初始策略函数参数θ 、初始价值函数参数ψ 。
0 0
初始化θ =θ 和ψ =ψ
0 0
fork =0,1,2,··· do
(g ,g )=0
ψ θ
forW 里每一个Worker节点do
(g ,g )=(g ,g )+worker(Vπθ,π )
ψ θ ψ θ ψ θ
endfor
ψ =ψ−η g ;θ =θ+η g 。
ψ ψ θ θ
endfor
Worker:
超参数: 奖励折扣因子γ、轨迹长度L。
输入: 价值函数Vπθ、策略函数π 。
ψ θ
执行L步策略π ,保存{S ,A ,R ,S }。
θ t t t t+1
估计优势函数Aˆ =R +γVπθ(S )−Vπθ(S )。
P t t ψ t+1 ψ t