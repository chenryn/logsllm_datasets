Once the traces have been separated into phases, we apply
a set of feature extraction functions. For any particular feature,
we use Shannon entropy to estimate the amount of information
an attacker can gain by making side-channel observations
about that feature in network traces. We ﬁnd the features that
leak information about a benign user’s secret values, and rank
them to identify the most informative trace features.
A. Feature extraction
Feature extraction is commonly used in leakage quantiﬁ-
cation and machine learning to extract information from data
[47]. In our approach, we process full network traces to obtain
subtraces by splitting the full trace using the phase alignment
and use the subtraces and full trace to extract features of
interest. Each subtrace t (including the full trace) is converted
to three subtraces t↑, t↓ and t(cid:108) according to the direction
of packets as explained in Section III. We deﬁne a feature
set F = {f 1, f 2, . . . , f n} such that each feature function f j
extracts a statistic from a packet or all packets from a subtrace.
7
We deﬁne aggregate features, which compute the sum
of packet sizes and sum of timing differences between the
ﬁrst and last packets in a subtrace. We deﬁne ﬁne-grained
packet-level features, consisting of the size of packet pi and
timing differences between consecutive packets pi and pi+1.
To ensure that we have the same number of features for each
sampled trace, we align each subtrace on the left side and
remove packets that are not fully aligned when computing
per-packet features. Aggregate features are not affected by
this change and use the entire subtrace. The features are
summarized in Table I. Applying a feature function f j to
each packet series obtained from traces results in a feature
n)(cid:105), which is used
proﬁle P i = (cid:104) (s1, vj
to compute information leakage.
2), . . . , (sn, vj
1), (s2, vj
TABLE I: Deﬁnition of network trace features.
Feature Function
f sum-size(t)
(cid:80)
Deﬁnition
p∈t p.size
f size((cid:104) p1, . . . , pn(cid:105), i)
f total-time((cid:104) p1, . . . , pn(cid:105), i)
f ∆time((cid:104) p1, . . . , pn(cid:105), i)
pi.size
pn.time − p1.time
pi+1.time − pi.time
Description
Sum of sizes
of packets in
sub-trace t.
Size of packet i.
Total time of
subtrace.
Time diff. of
packets i & i + 1.
B. Information theory for quantifying leakage
In our threat model, an attacker who is observing the
network communication can record a network trace, extract
features from that trace, and make an inference about the value
of an unknown secret. Our goal in this section is to describe
how to measure the strength of this inference process. The
ultimate goal is to compare and rank individual features in
terms of their usefulness in determining the value of the secret.
Here, we ﬁx our attention on the relationship between secrets
and a single particular feature of interest, f j, and so we omit
the superscript j for the current discussion and refer simply to
f as the feature of interest, and vi as the ith sampled value of
feature f in the given feature proﬁle.
Quantitative information ﬂow: Before observing a run
of the system, an outside observer has some amount of initial
uncertainty about the value of the secret. Benign users of
the system perform interactions and, meanwhile, an attacker
observes the network traces and computes the value of feature
f. In our scenario, observing a trace feature results in some
amount of information gain. In other words, measuring f
reduces an observer’s remaining uncertainty about the secret
s. Our goal is to measure the strength of ﬂow of information
from s to f, which is called the mutual information between
the feature and the secret. This intuitive concept can be
formalized in the language of quantitative information ﬂow
(QIF) using information theory [45]. Speciﬁcally, we make use
of Shannon’s information entropy which can be considered a
measurement of uncertainty [19], [43].
Given a random variable S which can take values in S
with probability function p(s), the information entropy of S,
denoted H(S), which we interpret as the observer’s initial
uncertainty, is given by
p(s) log2 p(s)
(1)
H(S) = −(cid:88)
s∈S
Given another random variable, V , denoting the value of
the feature of interest, and a conditional distribution for the
probability of a secret given the observed feature value, p(s|v),
the conditional entropy of S given V , which we interpret as
the observer’s remaining uncertainty about S, is
H(S|V ) = −(cid:88)
p(s|v) log2 p(s|v)
(2)
(cid:88)
s∈S
p(v)
v∈V
Given these two deﬁnitions, we can compute the expected
amount of information gained about S by observing V . The
mutual information between V and S, denoted I(S; V ) is
deﬁned as the difference between the initial entropy of S and
the conditional entropy of S given V :
I(S; V ) = H(S) − H(S|V )
(3)
Probability estimation via proﬁle samples. The preceding
discussion assumes that the probabilistic relationships between
i.e. p(s|v).
the secret and the feature values are known,
However, since we do not know this relationship in advance,
we estimate the conditional probability distribution using the
samples generated via proﬁling.
We begin with a generic discussion of estimating prob-
ability distributions from a ﬁnite sample set. Let V be a
sample space, V be a random variable that ranges over V,
v represent a particular element of V, and v = (cid:104)v1, . . . , vn(cid:105)
be a ﬁnite list of n random samples from V. We estimate the
probability of any v ∈ V in two ways. Each method relies on
a choice of “resolution” parameter, which we make explicit in
the following descriptions. The reader may refer to Figure 9.
Histogram estimation. We choose a discretization which
partitions the sample set v into m intervals or “bins” where ci
is the count of the samples in bin i. The bins are represented
by intervals of length ∆v = m/(max v − min v). Then for
any v, p(v) is estimated by the number of samples that
are contained in the same interval as v divided by the total
number of samples. The resolution parameter is m and the
probability estimator for v which falls in bin i is given by
ˆp(v) = ci/n. This estimation of probability is straightforward
and commonly used. However, our experiments indicate that,
due to the huge search space, our sampling is extremely
sparse. Hence, histogram-based probability estimation fails to
generalize well to predict the probability of unseen samples.
Gaussian estimation. We can estimate the probability of
any v ∈ V by assuming the sample set comes from a Gaussian
distribution. We compute the mean, µ, and standard deviation
σ from the set of samples v. We then have an estimate ˆp(v)
assuming v comes from the normal distribution N (µ, σ). This
allows us to more smoothly interpolate the probability of
feature values for any v that was not observed during proﬁling.
Information gain estimation via proﬁle. We make use
of the proﬁle for the current feature of interest f to esti-
mate the expected information gain. We consider a proﬁle
P that consists of n pairs of secrets and feature values,
P = (cid:104)(s1, v1), (s2, v2), . . . , (sn, vn)(cid:105). For any particular secret
s ∈ S let vs = (cid:104)vi : si = s(cid:105) be the list of feature value samples
that correspond to s. We use vs to estimate the probability
distribution of the feature value given the secret, ˆp(v|s), using
either the histogram- or Gaussian-based method. We then
Fig. 9: Estimating a probability distribution from samples using
histograms or Gaussian estimates.
compute the probability of a secret value given a feature
value, ˆp(s|v), using a straightforward application of Bayes’
rule. We assume a uniform probability distribution for p(s).
Using ˆp(s|v), we apply equations 1, 2, and 3 to compute the
estimated information gain (leakage) for the secret given the
current feature of interest, ˆI(S, V ). In later sections, we refer
to the value that Proﬁt computes for ˆI as LeakH (histogram-
based estimation) or LeakG (Gaussian-based estimation).
Example. Consider a scenario in which we have two
possible equally likely secrets, s1 and s2. Thus, we have
1 bit of secret information. After conducting proﬁling for a
feature f, we can compute the estimate for the probability of
the feature values given the secret values ˆp(v|s1) and ˆp(v|s2)
using either histogram-based estimation or Gaussian estimation
as depicted in Figure 9. Using histogram-based estimation
with a bin-width of ∆v = 0.5 as shown, we observe that
the only sample collisions occur at v = 17 and v = 20.5.
Since we observe very few collisions this way, we expect that
histogram-based estimation will tell us that there is a high
degree of information leakage since most observable feature
values correspond to distinct secrets. Indeed, the estimated
information gain is 0.8145 bits out of 1 bit.
On the other hand, we have sparsely sampled the feature
value space, and if we were able to perform more sampling,
we would “ﬁll in” the gaps in the histogram. Hence, using
Gaussian distributions to interpolate the density, as shown in
Figure 9, we see that we are much better able to capture the
probability of observable feature value collisions. Using the
Gaussian probability estimates, we compute that the expected
information leakage is 0.4150 bits out of 1 bit, much less than
when estimating with the histogram method. We say that the
histogram overﬁts the sampled data. Estimating probabilities
from a sparse set of features without overﬁtting is addressed
in multiple works [8], [27], [30], [44]. Our experimental
evaluation (Section VI) indicates that Gaussian ﬁtting works
well for estimating entropy in network trafﬁc features.
8
0510152025300.00.10.20.30.40.50.6Featurevalue,vp(vs)HistogramEstimations1s20510152025300.000.020.040.060.080.100.12Featurevalue,vp(vs)GaussianEstimations1s2VI. EXPERIMENTAL EVALUATION
In this section we present the experimental evaluation of
Proﬁt on the DARPA STAC benchmark.
A. DARPA STAC systems and vulnerabilities
The applications in our benchmark are from the DARPA
Space/Time Analysis for Cybersecurity program [20], which
seeks to push the state of the art in both side-channel and
algorithmic-complexity vulnerability detection. Algorithmic
complexity attacks are beyond the scope of this paper; we fo-
cus on STAC’s side-channel-related applications. These STAC
applications [21] include a collection of realistic Java systems,
many of which contain side-channel leaks in time or in space,
and certain secrets of interest. Some of the systems come in
multiple variants, some of which may leak more than others,
or have a particular vulnerability added or removed. All the
systems are network-based (web-based, client-server, peer-to-
peer), and most of the vulnerabilities are based on proﬁling
network trafﬁc and eavesdropping. We have omitted some
applications whose side channels are based on other media,
such as interception of ﬁle I/O, or whose vulnerabilities are
exclusively about cryptography.
AIRPLAN is a Web-based client-server system for airlines.
It allows uploading, editing, and analyzing ﬂight routes by
metrics like cost, ﬂight time, passenger and crew capacities.
One secret of interest is the number of cities in a route map
uploaded by a user; the challenge is to guess this using a side
channel in space. AIRPLAN 2 has a vulnerability by which
the cells of the table shown on the View passenger capacity
matrix page are padded with spaces to a ﬁxed width. Thus, the
HTML code for the table looks neatly laid out. This is easily
overlooked by the end-user, as multiple spaces are rendered as
one space by Web browsers, but it does inﬂuence the number of
bytes transmitted. Thus, the download size of this particular
page becomes proportional to the number of cities squared.
In AIRPLAN 5, the HTML cell padding is randomized rather
than ﬁxed, which dilutes the leakage but does not eliminate
it. AIRPLAN 3 does not pad the cells, and is thus much more
resilient to this kind of attack; there is still a correlation, but
it is a much weaker one.
Another secret of interest in AIRPLAN is the strong connec-
tivity of a route map uploaded by an airline. Both AIRPLAN 3
and AIRPLAN 4 have a Get properties page that shows various
attributes of a route map. AIRPLAN 3 has a vulnerability
that causes a slight variation in the byte size of this page
depending on whether the route map in question, viewed as a
graph, is strongly connected. AIRPLAN 4 does not have this
vulnerability. The fault can be exploited to fully leak the secret
in the former, while the latter does not leak at all.
BIDPAL is a peer-to-peer system that allows users to buy
and sell items via a single-round, highest-bidder-wins auction
with secret bids. It allows users to create auctions, bid on an
auction, ﬁnd auctions, etc. The secret of interest is the value
of the secret bid placed by a user. BIDPAL 2 contains a timing
vulnerability whereby a certain loop is executed a number
of times proportional to the maximum possible bid, and a
counter is increased; after the counter exceeds the victim’s
offered bid, a different action is performed per iteration which
takes slightly longer. Thus, the total execution time of the loop
correlates with the secret.
GABFEED, as explained in Section II, is a Web-based
forum where users can post messages, search posted messages,
and chat. In GABFEED 2, an authentication mechanism is
affected by a timing vulnerability in a modPow() method,
where a branch is only taken when the i-th bit of the server’s
private key is 1. Thus, the delay between two network packets
involved in this authentication is proportional to the Hamming
weight of the binary representation of the private key. In
GABFEED 1, the modPow() method is securely implemented
and the vulnerability is not present.
SNAPBUDDY is a Web application for image sharing; it
allows users to upload photos from different locations, share
them with friends, and ﬁnd out who is online nearby. The
secret is the physical location of the victim user. During the
execution of the Change user location operation, a few network
messages are sent, including one whose size correlates with
the destination location. By careful manual inspection one can
conﬁrm that each one of the 294 known locations has a unique
associated message size, thus providing a unique signature
for each location. However, the crucial message may impact
the size of one, two, three, or up to four adjacent packets
depending on its total size. Thus, one should pay attention to
the sum of those packets.
POWERBROKER is a peer-to-peer system used by power
suppliers to exchange power. Power plants with excess supply
try to sell power, whereas those with a shortfall try to purchase
it. The secret of interest is the value offered by one of the
participating power plants. POWERBROKER 1 has a vulnera-
bility in time whereby a certain loop is executed a number of
times that is proportional to the amount of the price, in dollars,
offered for the power. This induces a time execution difference
that ends up affecting network traces. In POWERBROKER 2
and POWERBROKER 4, this loop is always executed a constant
number of times, which removes the vulnerability. In addition
to this, in POWERBROKER 2 as in BIDPAL 2, the behavior
of the program changes when loop counter reaches the bid.
However unlike BIDPAL 2, this change in behavior does not
impact the time taken for a loop iteration so the program
remains non-vulnerable.
TOURPLANNER is a client-server system that, given a list
of places that the user would like to visit, calculates a tour
plan that is optimal with respect to certain travel costs. It
is essentially a variation of the traveling salesman problem.
The secret of interest is the user-given list of places. The
TOURPLANNER system has a subtle timing vulnerability. The
computation can take a while, so the server sends periodic
progress-report packets to the client. Their precise timing ex-
poses the duration of certain internal stages of the computation.
There are ﬁve consecutive packets of which the four time-
deltas in between (i.e.,
the time differences between each
packet and the following one) are particularly relevant. Each
of these deltas, by itself, leaks just a little information about
the secret. Their sum leaks more information than each of
them separately. And when interpreted as a vector in R4, they
constitute a signature for the secret list of places with a high
level of leakage.
9
B. Experimental setup
Proﬁling-input suite generation: In many real-world con-
texts, one can leverage existing input suites and/or existing
input generators that might be available for the system. If no
input suite is available, we will need to generate inputs to run
the system. Generating complex structured inputs for black-
box execution of a system is a nontrivial task, and its full
automation is beyond the scope of this work.
Designing a proﬁling-input suite compels us to consider
the following goals:
1) Secret domain coverage: We want to exercise the system
for many different secrets, i.e., choose a secret set S that
is reasonably representative of the secret domain S.
2) Input domain coverage: We want to choose an input set
I that is reasonably representative of the input domain
I. Typically, for each secret s ∈ S we may need many
different inputs i ∈ I such that ζ(i) = s. Since such
inputs may differ from each other in various different ways,
we may want to sweep several dimensions to capture a
representative subset.
3) Sampling for noise resilience: We want to run each input
i ∈ I multiple times so that system noise can be modeled
and accounted for, especially if we know or suspect that
the system may have a strong degree of nondeterminism.