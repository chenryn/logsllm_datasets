> **引言**
随着互联网与数字化信息技术的飞速发展，新的数据源不断涌现，各种应用所产生的数据呈指数级增长。根据互联网数据中心监测，全球数据量大约每两年翻一番，预计到2020年，全球的数据量将达到35
ZB。据Gartner
Group统计，如今80％的数据为非结构化数据，并且仍保持高速增长态势，非结构化数据已逐渐成为大数据的主体。
> **1 非结构化机器数据利用难题**
计算机信息化系统中的数据分为结构化数据和非结构化数据。非结构化数据是数据结构不规则或不完整，没有预定义的数据模型。包括所有格式的办公文档、文本、图片、XML,
HTML、各类报表、图像和音频/视频信息等等。
非结构化数据其格式非常多样，标准也是多样性的，而且在技术上非结构化信息比结构化信息更难标准化和理解。所以存储、检索、发布以及利用需要更加智能化的IT技术，比如海量存储、智能检索、知识挖掘、内容保护、信息的增值开发利用等。
对比结构化数据，非结构化数据数量巨大、产生速度快，又缺乏规律性，价值密度较低，之前因缺乏有效的技术手段进行处理和分析，往往被丢弃和忽视。随着大数据浪潮的来临，海量数据处理技术的日渐成熟，数据存储成本下降，数据分析应用方向逐渐也向非结构化数据转移。
非结构化数据包含复杂的内容，并具有不同的结构特点，传统关系数据库无论从描述能力上还是从管理数据的规模上，都无法应对非结构化数据管理的要求。因此，如何对非结构化数据进行范式化处理，已成为非结构化机器数据处理相关研究中的一个核心问题。
> **2 非结构化机器数据范式化处理技术研究**
一般情况下，对非结构化数据的处理通过范式化匹配，提取字段后建立字段索引，则可形成新的结构化数据形式，提高搜索效率。但是非结构化机器数据格式是经常改变，对于没有提取到字段信息的数据，如果需要进行字段统计，则需要实现搜索时提取字段功能。
非结构化机器数据范式化处理流程如下：
![解析](media/image1.png){width="5.833333333333333in"
height="4.861111111111111in"}
-   收到一条原始非结构化数据raw_event
-   提取关键信息key(token +
    appname（appname不存在则使用hostname）)，从ParserCache中找对应的解析规则EventParser
-   如果找到对应的解析规则并且处理成功 ，则返回structured event
-   如果没找到对应的解析规则EventParser 或解析 EventParser
    处理不成功，进入ParserContainer处理
-   看token+appname是否有对应的用户Custom配置
-   如果有则使用按照其配置产生的Parser来处理，如果处理成功则产生event_parser和structured
    event,更新cache，若不成功则使用DefaultParser只保留raw_message，不提取任何字段
-   如果没有，则使用Common配置处理，需要依次使用各种类型的Parser去尝试处理，如果处理成功则产生event_parser和结构化数据structured
    event，更新cache，若不成功则使用DefaultParser只保留raw_message，不提取任何字段
-   返回结构化数据structured event
在运维过程中，非结构化数据主要来源于大量的设备日志、服务日志、应用日志，普遍缺乏统一的数据采集、格式和存储规范。
针对常见的日志数据，采用内置相应的日志解析规则，能够自动识别和解析常见的日志格式，如Linux、Apache、JSON等，对于非常用的日志格式，通过个性化配置日志格式解析规则，抽取自定义字段达到数据的范式化处理。
日志解析的主要作用是抽取重要的字段，则通过字段名称可以进行统计与分析，根据机器数据格式提供常见自定义解析方法：
**针对机器数据的字段抽取规则**
**字段抽取类**
\- 正则解析： 通过配置正则解析出匹配的字段,
支持命名分组、多行正则、grok语法
\- KeyValue解析 适用于日志中包含字段名，分隔符比较明确的日志,
配置kv对分隔符和kv之间的分隔符来抽取字段，
\- KeyValue正则解析 适用于分隔符不确定, kv对不连续的日志, 通过配置key,
value, 分隔符的正则来抽取字段，
\- Json解析 适用于Json日志格式, 抽取出来的字段结构和Json中定义的结构一致
\- XML解析 适用于Xml日志解析，抽取出来的字段结构和Xml中定义的结构一致
\- CSV解析 适用于列顺序固定，分隔符固定的日志,
配置分隔符和列名来解析字段，
\- 结构体解析 适用于按固定字节长度写入的日志, 配置字节格式的解析
**字段转换类**
\- UserAgent解析： 从UserAgent抽取出操作系统，浏览器，设备信息
\- geo解析：
从IP地址中抽取出国家、省、市、互联网服务提供商、经度、经度、纬度信息
\- 手机号码解析：
从手机号码中抽取出国家、省、市、互联网服务提供商、经度、纬度信息
\- 固定号码解析： 从手机号码中抽取出国家、省、市、经度、纬度信息
\- 自定义字典： 查自定义字典表，从一个字段扩展到多个字段
\- syslog_prio解析： 从syslog的日志抽取出来设备、日志级别
**字段操作类**
\- 格式化处理： 用于把多个字段按指定格式, 产生新的字段
\- 删除字段： 用于删除不用的字段
\- 重命名字段： 用于重命名字段
\- 内容替换： 用于替换字段的内容
\- hex转换： 用于把16进制度的hex dump转换成可读度字符格式
\- URL转换： 用于把url转码的字段转换到转码之前的格式
\- ip格式转换： 用于把数值类型的ipv4地址转换成字符串ip地址
\- 数值字段转换： 用于把字符串类型的字段转换成数值类型
其他
\- 时间戳解析： 配置时间戳抽取格式,
来抽取日志的时间戳，抽取出来时间戳决定日志的存储位置和搜索时间范围
> **3 非结构化机器数据的范式化处理能力试验**
>
> **自动解析能力试验**
**规则匹配流程**
\- 配置的规则可以指定appname，tags,
匹配上原始日志中断appname，tags的规则执行这些规则。
\- 没有规则的日志或者配置的规则没有解析成功的日志，尝试默认规则
**默认规则**
\- apache access： 匹配apache access日志，同时适用于nginx access日志
\- apache error： 匹配apache error日志, 同时适用于nginx error日志
\- json： 匹配json格式的日志