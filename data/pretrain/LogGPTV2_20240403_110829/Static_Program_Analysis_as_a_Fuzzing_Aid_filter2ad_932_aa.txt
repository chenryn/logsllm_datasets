title:Static Program Analysis as a Fuzzing Aid
author:Bhargava Shastry and
Markus Leutner and
Tobias Fiebig and
Kashyap Thimmaraju and
Fabian Yamaguchi and
Konrad Rieck and
Stefan Schmid and
Jean-Pierre Seifert and
Anja Feldmann
Static Program Analysis as a Fuzzing Aid
Bhargava Shastry1(B), Markus Leutner1, Tobias Fiebig1,
Kashyap Thimmaraju1, Fabian Yamaguchi2, Konrad Rieck2, Stefan Schmid3,
Jean-Pierre Seifert1, and Anja Feldmann1
1 TU Berlin, Berlin, Germany
PI:EMAIL
2 TU Braunschweig, Braunschweig, Germany
3 Aalborg University, Aalborg, Denmark
Abstract. Fuzz testing is an eﬀective and scalable technique to perform
software security assessments. Yet, contemporary fuzzers fall short of
thoroughly testing applications with a high degree of control-ﬂow diver-
sity, such as ﬁrewalls and network packet analyzers. In this paper, we
demonstrate how static program analysis can guide fuzzing by augment-
ing existing program models maintained by the fuzzer. Based on the
insight that code patterns reﬂect the data format of inputs processed
by a program, we automatically construct an input dictionary by stati-
cally analyzing program control and data ﬂow. Our analysis is performed
before fuzzing commences, and the input dictionary is supplied to an
oﬀ-the-shelf fuzzer to inﬂuence input generation. Evaluations show that
our technique not only increases test coverage by 10–15% over baseline
fuzzers such as aﬂ but also reduces the time required to expose vul-
nerabilities by up to an order of magnitude. As a case study, we have
evaluated our approach on two classes of network applications: nDPI, a
deep packet inspection library, and tcpdump, a network packet analyzer.
Using our approach, we have uncovered 15 zero-day vulnerabilities in
the evaluated software that were not found by stand-alone fuzzers. Our
work not only provides a practical method to conduct security evalua-
tions more eﬀectively but also demonstrates that the synergy between
program analysis and testing can be exploited for a better outcome.
Keywords: Program analysis · Fuzzing · Protocol parsers
1 Introduction
Software has grown in both complexity and dynamism over the years. For exam-
ple, the Chromium browser receives over 100 commits every day. Evidently, the
scale of present-day software development puts an enormous pressure on pro-
gram testing. Evaluating the security of large applications that are under active
Electronic supplementary material The online version of this chapter (doi:10.
1007/978-3-319-66332-6 2) contains supplementary material, which is available to
authorized users.
c(cid:2) Springer International Publishing AG 2017
M. Dacier et al. (Eds.): RAID 2017, LNCS 10453, pp. 26–47, 2017.
DOI: 10.1007/978-3-319-66332-6 2
Static Program Analysis as a Fuzzing Aid
27
development is a daunting task. Fuzz testing is one of the few techniques that
not only scale up to large programs but are also eﬀective at discovering program
vulnerabilities.
Unfortunately, contemporary fuzzers are less eﬀective at testing complex net-
work applications that handle diverse yet highly structured input. Examples
of such applications are protocol analyzers, deep packet inspection modules,
and ﬁrewalls. These applications process input in multiple stages: The input
is ﬁrst tokenized, then parsed syntactically, and ﬁnally analyzed semantically.
The application logic (e.g., intrusion detection, network monitoring etc.) usu-
ally resides in the ﬁnal stage. There are two problems that these applications
pose. First, the highly structured nature of program input begets a vast number
of control ﬂow paths in the portion of application code where packet parsing
takes place. Coping with diverse program paths in the early stages of the packet
processing pipeline, and exploring the depths of program code where the core
application logic resides is taxing even for state-of-the-art fuzzers. Second, the
diversity of program input not only ampliﬁes the number of control ﬂows but also
demands tests in breadth. For example, the deep packet inspection library, nDPI,
analyzes close to 200 diﬀerent network protocols [27]. In the face of such diversity,
generating inputs that eﬃciently test application logic is a hard problem.
Although prior work on grammar-based fuzzing [13,16,29] partly address the
problem of fuzz testing parser applications, they cannot be applied to testing
complex third-party network software for two reasons. First, existing grammar-
based fuzzers rely on a user-supplied data model or language grammar speci-
ﬁcation that describes the input data format. A fundamental problem with a
speciﬁcation-based approach to fuzzing is that the formal grammar of program
input might not be available to begin with. Indeed, few network protocols have a
readily usable formal speciﬁcation. Therefore, grammar-based fuzzing at present,
is contingent upon a data model that is—most often—manually created by an
expert. Although proposals such as Prospex [7] that automatically create gram-
mar speciﬁcations from network traces are promising, they are designed with
a single protocol in mind. Automatic speciﬁcation generation for diverse gram-
mars has not been attempted. A second problem with certain grammar-based
approaches that use whitebox testing is that they require signiﬁcant software
alterations, and rely on implementation knowledge. For example, to conduct
grammar-based whitebox testing, parsing functions must be manually identiﬁed
in source code, and detokenization functions must be written. Although man-
ual fallbacks may be inevitable in the face of implementation diversity, prior
approaches demand signiﬁcant software revisions, making them ill-suited for
security evaluation of third-party software.
In this paper, we demonstrate how the stated challenges can be addressed by
augmenting fuzzing with static program analysis. Being program centric, static
analysis can examine control ﬂow throughout an application’s codebase, per-
mitting it to analyze parsing code in its entirety. This design choice makes our
approach well-suited for testing complex network applications. Our approach has
two key steps. First, we automatically generate a dictionary of protocol message
28
B. Shastry et al.
constructs and their conjunctions by analyzing application source code. Our
key insight is that code patterns signal the use of program input, and therefore
suﬃcient cues about program input may be gathered by analyzing the source
code. To this end, we develop a static analyzer that performs data and control-
ﬂow analysis to obtain a dictionary of input constructs. Second, the dictionary
obtained from the ﬁrst step is supplied to an oﬀ-the-shelf fuzzer. The fuzzer
uses the message fragments (constructs and conjunctions) present in the sup-
plied dictionary toward input generation. Although anecdotal evidence suggests
that a carefully constructed dictionary can dramatically improve a fuzzer’s eﬀec-
tiveness [35], program dictionaries at present are created by a domain-speciﬁc
expert. To make our analysis and test framework easily deployable on real-world
code, we have developed a plugin to the Clang/LLVM compiler that can (i) Be
automatically invoked at code compilation time, and (ii) Produce input dictio-
naries that are readily usable with oﬀ-the-shelf fuzzers such as aﬂ. Indeed, our
work makes security evaluations accessible to non-domain-experts e.g., audit of
third-party code in the government sector.
We have prototyped our approach in a tool that we call Orthrus, and eval-
uated it in both controlled and uncontrolled environments. We ﬁnd that our
analysis helps reduce the time to vulnerability exposure by an order of magni-
tude for the libxml2 benchmark of the fuzzer test suite [15]. Furthermore, we
use Orthrus to conduct security evaluations of nDPI (deep packet inspection
library), and tcpdump (network packet analyzer). Input dictionaries generated
via static code analysis increase test coverage in nDPI, and tcpdump by 15%,
and 10% respectively. More signiﬁcantly, input dictionaries have helped uncover
15 zero-day vulnerabilities in the packet processing code of 14 diﬀerent proto-
cols in the evaluated applications that were not found by stand-alone fuzzers
such as aﬂ, and the Peach fuzzer. These results lend credence to the eﬃcacy
of our approach in carrying out security evaluations of complex third-party net-
work software. Our prototype, Orthrus, is available at https://www.github.com/
test-pipeline/Orthrus.
Contributions
– To address the challenges of fuzzing complex network software, we propose
a static analysis framework to infer the data format of program inputs from
source code.
– We propose a novel approach—the use of static program analysis—to augment
fuzzing. To this end, we couple our analysis framework with an oﬀ-the-shelf
fuzzer.
– Finally, we prototype our approach and extensively evaluate its impact. Our
prototype achieves an improvement of up to 15% in test coverage over state-
of-the-art fuzzers such as aﬂ, expedites vulnerability discovery by an order
of magnitude, and exposes 15 zero-day vulnerabilities in popular networking
software1. These results validate our proposition that static analysis can serve
as a useful fuzzing aid.
1 Ethical Considerations: Vulnerabilities found during our case studies have been
responsibly disclosed to the concerned vendors who have subsequently patched them.
Static Program Analysis as a Fuzzing Aid
29
2 Background
In this section, we provide a brief overview of static analysis, and fuzz testing
that is relevant to our work.
Static Analysis. Our application of static analysis is closer to the notion of
static analysis as a program-centric checker [10]: Tools that encapsulate a notion
of program behavior and check that the implementation conforms to this notion.
Historically, static analysis tools aimed at ﬁnding programming errors encode a
description of correct (error-free) program behavior and check if the analyzed
software meets this description. In contrast, our analyses encode input-processing
properties of a program in order to extract features of the input message format.
Static analysis helps in analyzing the breadth of a program without concrete
test inputs. However, because static analysis usually encapsulates an approxi-
mate view of the program, its analysis output (bugs) has to be manually val-
idated. The analysis logic of a static analyzer may be catered to diﬀerent use
cases, such as ﬁnding insecure API usages, erroneous code patterns etc. This
analysis logic is usually encoded as a set of rules (checking rules), while the
analysis itself is carried out by a static analyzer’s core engine.
Static program analysis includes, among other types of analyses, program
data-ﬂow and control-ﬂow analyses [1]. Data-ﬂow analysis inspects the ﬂow of
data between program variables; likewise control-ﬂow analysis inspects the ﬂow
of control in the program. While data-ﬂow analysis may be used to understand
how program input interacts with program variables, control-ﬂow analysis may
be used to understand how control is transferred from one program routine to
another. In practice, both data and control ﬂow analyses are essential compo-
nents of a static analyzer.
Program data and control-ﬂow may be analyzed at diﬀerent program abstrac-
tions. In our work, we focus on syntactic as well as semantic analysis, using the
program abstract syntax tree (AST), and control ﬂow graph (CFG) respectively.
At the syntactic level, our analysis is performed on the program’s AST, and at
the semantic level, on the program’s CFG. A program’s AST representation
comprises syntactic elements of a program, such as the If, For, While state-
ments, program variables and their data types etc. Each syntactic element is
represented as an AST node. All AST nodes, with the exception of the root and
the leaf nodes, are connected by edges that denote a parent-child relationship.
The CFG of a program unit represents its semantic elements, such as the control
ﬂow between blocks of program statements. The CFG nodes are basic blocks:
Group of program statements without a branching instruction. The CFG edges
connect basic blocks that comprise a possible program path. The infrastructure
to obtain program AST, CFG, and perform analysis on them is available in
modern compiler toolchains.
Fuzz Testing. Fuzzing is one of the most common dynamic analysis techniques
used in security assessments. It was introduced by Miller et al. to evaluate the
robustness of UNIX utilities [22]. Ever since, fuzzing has seen widespread adop-
tion owing to its eﬀectiveness in eliciting faulty program behavior. The ﬁrst
30
B. Shastry et al.
fuzzer functioned without any program knowledge: It simply fed random inputs
to the program. In other words, it was a blackbox (program agnostic) fuzzer.
Blackbox fuzzers paved the way for modern fuzzers that are program aware.
State-of-the-art fuzzers build a model of the analyzed program as it is tested.
This model is used to guide testing more optimally, i.e., expend resources for
teasing out unexplored program paths. Techniques used to build a model of
the program under test may vary from coverage tracing (aﬂ) [34], to constraint
solving (SAGE) [14]. Fuzzers may also expect the user to deﬁne a grammar
underlying the message format being tested. Examples of such fuzzers are the
Peach Fuzzer [29] and Sulley [28], both of which generate inputs based on a user
speciﬁed grammar. Fuzzers such as aﬂ support the use of message constructs for
fuzzer guidance. However, unlike Peach, aﬂ does not require a formal grammar
speciﬁcation; it simply uses pre-deﬁned constructs in the input dictionary toward
input mutation.
3 Program Analysis Guided Fuzzing
In this section, we ﬁrst brieﬂy outline our speciﬁc problem scope with regard to
protocol speciﬁcation inference, then provide an overview of our approach, and
ﬁnally describe our methodology.
Problem Scope. An application protocol speciﬁcation usually comprises a state
machine that deﬁnes valid sequences of protocol messages, and a message format
that deﬁnes the protocol message. In our work, we focus on inferring the pro-
tocol message format only, leaving the inference of the state machine for future
work. Since ﬁle formats are stateless speciﬁcations, our work is applicable for
conducting security evaluations of ﬁle format parsers as well.
Approach Overview. We demonstrate how fuzz testing of network applications
can be signiﬁcantly improved by leveraging static analysis for test guidance. It
has already been suggested in non-academic circles that a carefully constructed
dictionary of parser input can dramatically improve a fuzzer’s eﬀectiveness [35].
However, creating input dictionaries still requires domain expertise. We automat-
ically generate input dictionaries by performing static program analysis, supply-
ing it to an oﬀ-the-shelf fuzzer toward input generation. Indeed, our prototype
builds on legacy fuzzers to demonstrate the eﬀectiveness of our approach.
Figure 1 illustrates our analysis and test workﬂow. First, we statically ana-
lyze application source code and obtain a dictionary of protocol message con-
structs and conjunctions. Each item in the dictionary is an independent message
fragment: It is either a simple message construct, or a conjunction of multiple
constructs. For example, a constant string SIP/2.0 in the source code is inferred
as a message construct, while usages of another construct, say the constant string
INVITE, that are contingent on SIP/2.0 are inferred to be a conjunction of the
form INVITE SIP/2.0. Second, we supply the input dictionary obtained in the
ﬁrst step to a fuzzer toward input generation. The fuzzer uses the supplied dic-
tionary together with an initial set of program inputs (seeds) toward fuzzing
Static Program Analysis as a Fuzzing Aid
31
Fig. 1. Work-ﬂow for program analysis guided fuzzing.
an application test case. In contrast to prior work, our analysis is automatic,
and requires neither a hand-written grammar speciﬁcation, nor manual software
alterations. Furthermore, the input dictionary obtained through our analysis
may be supplied as is to existing fuzzers such as aﬂ, aﬂfast, and libFuzzer, mak-
ing our approach legacy compliant.
3.1 Input Dictionary Generation
The use of static program analysis for inferring program properties is a long-
standing ﬁeld of research. However, the main challenge underlying our approach
is that our analysis must infer properties of the program input from applica-
tion source code. Although Rice’s theorem [17] states that all semantic program
properties are undecidable in general, we aim to make an informed judgement.
Program Slicing. The ﬁrst problem we encounter is an instance of the classical
forward slicing problem [12]: determining the subset of program statements, or
variables that process, or contain program input. Although existing forward
slicing techniques obtain precise inter-procedural slices of small programs, they
do not scale up to complex network parsers that exhibit a high degree of control
as well as data-ﬂow diversity.
As a remedy, we obtain a backward program slice with respect to a pre-
determined set of program statements that are deemed to process program input.
These program statements are called taint sinks, since program input (taint)
ﬂows into them. Since our analysis is localized to a set of taint sinks, it is tractable
and scales up to large programs. Naturally, the selection criteria for taint sinks
inﬂuence analysis precision, and ultimately decide the quality of inferred input
fragments. Therefore, we employ useful heuristics and follow reasonable design
guidelines so that taint sink selection is not only well-informed by default, but
can also beneﬁt from domain expertise when required. We explain our heuristics
and design guidelines for taint sink selection in the next paragraph.
Taint Sinks. We select a program statement as a taint sink if it satisﬁes one or
more of the following conditions:
32
B. Shastry et al.
1. It is a potentially data-dependent control ﬂow instruction, such as switch, if
2. It is a well-known data sink API (e.g., strcmp), or an API that accepts const
3. It contains a constant assignment that contains a literal character, string, or
statements.
qualiﬁed arguments as input.
integer on the right hand side, such as
const char *sip = ‘‘SIP/2.0’’
Although these heuristics are simple, they are eﬀective, and have two useful
properties that are crucial to generating an eﬀective fuzzer dictionary. First,
they capture a handful of potential input fragments of high relevance by focusing
on program data and control ﬂow. In contrast, a na¨ıve textual search for string
literals in the program will inevitably mix-up interesting and uninteresting use of
data, e.g., strings used in print statements will also be returned. Second, although
our heuristics are straightforward, they capture a wide array of code patterns
that are commonly found in parsing applications. Thus, they constitute a good
default speciﬁcation that is applicable to a large class of parsing applications. The
defaults that are built-in to our analysis framework make our solution accessible
for conducting security assessments of third-party network software.
Naturally, our heuristics may miss application-speciﬁc taint sinks. A promi-
nent example is the use of application speciﬁc APIs for input processing. As
a remedy, we permit the security analyst to specify additional taint sinks as
an analysis parameter. In summary, we facilitate entirely automatic analysis of
third-party software using a default taint speciﬁcation, while opportunistically
beneﬁting from application-speciﬁc knowledge where possible. This makes our
analysis framework ﬂexible in practice.
Analysis Queries. In order to infer protocol message constructs, we need to
analyze data and control-ﬂow around taint sinks. To facilitate fast and scalable