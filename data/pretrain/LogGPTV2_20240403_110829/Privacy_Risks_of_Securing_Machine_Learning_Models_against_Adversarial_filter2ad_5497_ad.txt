72.43%
75.67%
We performed the robust training [33] for three CIFAR10 classi-
fiers with varying adversarial perturbation budgets, and show the
result in Table 6. Note that a model trained with a larger ϵ is more
robust since it can defend against larger adversarial perturbations.
From Table 6, we can see that more robust models leak more
information about the training data. With a larger ϵ value, the
robust model relies on a larger l∞ ball around each training point,
leading to a higher membership inference attack accuracy.
5.2.4 Privacy risk with model capacity. Madry et al. [33] have ob-
served that compared with natural training, robust training requires
a significantly larger model capacity (e.g., deeper neural network
architectures and more convolution filters) to obtain high robust-
ness. In fact, we can think of the robust training approach as adding
more “virtual training points”, which are within the l∞ ball around
original training points. Thus the model capacity needs to be large
enough to fit well on the larger “virtual training set”.
Here we investigate the influence of model capacity by varying
the capacity scale of wide ResNet architecture [65] used in CIFAR10
training, which is proportional to the output channel numbers of
residual layers. We perform membership inference attacks for the
robust models, and show the results in Figure 3. The attacks are
based on benign inputs’ predictions (strategy IB) and the gray line
measures the privacy leakage for the natural models as a baseline.
First, we can see that as the model capacity increases, the
model has a higher membership inference accuracy, along
with a higher adversarial train accuracy. Second, when using a
larger adversarial perturbation budget ϵ, a larger model ca-
pacity is also needed. When ϵ = 2/255, a capacity scale of 2 is
(a) Membership inference attacks against models with different
model capacities.
(b) Adversarial train accuracy for models with different model
capacities.
Figure 3: Membership inference accuracy and adversarial
train accuracy for CIFAR10 classifiers [33] with varying
model capacities. The model with a capacity scale of s con-
tains 3 groups of residual layers with output channel num-
bers (16s, 32s, 64s), as described in Section 4.
enough to fit the training data, while for ϵ = 8/255, a capacity scale
of 8 is needed.
Inference attacks using targeted adversarial examples. Next,
5.2.5
we investigate membership inference attacks using targeted adver-
sarial examples. For each input, we compute 9 targeted adversarial
examples with each of the 9 incorrect labels as targets using Equa-
tion (19). We then compute the output prediction vectors for all
adversarial examples and use the shadow-training inference method
proposed by Shokri et al. [47] to perform membership inference
attacks. Specifically, for each class label, we learn a dedicated in-
ference model (binary classifier) by using the output predictions of
targeted adversarial examples from 500 training points and 500 test
points as the training set for the membership inference. We then
test the inference model on the remaining CIFAR10 training and
test examples from the same class label. In our experiments, we
use a 3-layer fully connected neural network with size of hidden
neurons equal to 200, 20, and 2 respectively. We call this method
“model-infer (targeted)”.
For untargeted adversarial examples or benign examples, a sim-
ilar class label-dependent inference model can also be obtained
by using either untargeted adversarial example’s prediction vec-
tor or benign example’s prediction vector as features of the infer-
ence model. We call these methods “model-infer (untargeted)” and
“model-infer (benign)”. We use the same 3-layer fully connected
neural network as the inference classifier.
Finally, we also adapt our confidence-thresholding inference
strategy to be class-label dependent by choosing the confidence
threshold value according to prediction confidence values from 500
training points and 500 test points, and then testing on remaining
CIFAR10 points from the same class label. Based on whether the
confidence value is from the untargeted adversarial input or the
benign input, we call the method as “confidence-infer (untargeted)”
and “confidence-infer (benign)”.
Table 7: Comparison of membership inference attacks
against the robust CIFAR10 classifier [33]. Inference attack
strategies include combining predictions of targeted adver-
sarial examples, untargeted adversarial examples, and be-
nign examples with either training an inference neural net-
work model or thresholding the prediction confidence.
Class
label
0
1
2
3
4
5
6
7
8
9
confidence-infer model-infer
confidence-infer
(benign)
70.88%
63.57%
80.16%
90.43%
82.30%
81.34%
75.34%
69.54%
69.16%
68.13%
(benign)
71.49%
64.42%
76.74%
90.49%
82.17%
79.84%
70.92%
67.61%
69.57%
66.34%
(untargeted)
72.21%
67.52%
79.71%
87.64%
81.83%
81.57%
77.66%
72.92%
74.36%
71.86%
model-infer
(untargeted)
model-infer
(targeted)
72.70%
67.69%
80.16%
87.83%
81.57%
81.34%
76.97%
72.82%
74.40%
72.06%
74.42%
68.88%
83.58%
90.57%
84.47%
83.02%
79.94%
72.98%
75.33%
73.32%
The membership inference attack results using the above five
strategies are presented in Table 7. We can see that the targeted
adversarial example based inference strategy “model-infer
(targeted)” always has the highest inference accuracy. This
is because the targeted adversarial examples contain information
about distance of the input to each label’s decision boundary, while
untargeted adversarial examples contain information about dis-
tance of the input to only a nearby label’s decision boundary.
Thus targeted adversarial examples leak more membership informa-
tion. As an aside, we also find that our confidence-based inference
methods obtain nearly the same inference results as training neu-
ral network models, showing the effectiveness of the confidence-
thresholding inference strategies.
6 MEMBERSHIP INFERENCE ATTACKS
AGAINST VERIFIABLY ROBUST MODELS
In this section we perform membership inference attacks against 3
verifiable defense methods: duality-based verification (Dual-Based
Verify) [61], abstract interpretation-based verification (Abs-Based
Verify) [34], and interval bound propagation-based verification
(IBP-Based Verify) [16]. We train the verifiably robust models using
the network architectures as described in Section 4 (with minor
modifications for the Dual-Based Verify method [61] as discussed
in Appendix C), the l∞ perturbation budget ϵ is set to be 8/255 for
the Yale Face dataset and 0.1 for the Fashion-MNIST dataset. We
do not evaluate the verifiably robust models for the full CIFAR10
dataset as none of these three defense methods scale to the wide
ResNet architecture.
6.1 Overall Results
The membership inference attack results against natural and veri-
fiably robust models are presented in Table 8 and Table 9, where
“acc” stands for accuracy, “adv-train acc” and “adv-test acc” measure
adversarial accuracy under PGD attacks (Equation (9)), and “ver-
train acc” and “ver-test acc” report the verified worse-case accuracy
under the perturbation constraint Bϵ .
For the Yale Face dataset, all three defense methods leak
more membership information. The IBP-Based Verify method
even leads to an inference accuracy above 75%, higher than the
inference accuracy of empirical defenses shown in Table 2, result-
ing a 4.5× membership inference advantage (Equation (16)) than
the natural model. The inference strategy based on verified predic-
tion confidence (strategy IV) has the highest inference accuracy
as the verification process enlarges prediction confidence between
training data and test data.
On the other hand, for the Fashion-MNIST dataset, we fail to
obtain increased membership inference accuracies on the verifiably
robust models. However, we also observe much reduced benign
train accuracy (below 90%) and verified train accuracy (below 80%),
which means that the model fits the training set poorly. Similar
to our analysis of empirical defenses, we can think the verifiable
defense as adding more “virtual training points” around each train-
ing example to compute its verified robust loss. Since the verified
robust loss is an upper bound on the real robust loss, the added
“virtual training points” are in fact beyond the l∞ ball. Therefore,
the model capacity needed for verifiable defenses is even larger
than that of empirical defense methods.
From the experiment results in Section 5.2.4, we have shown
that if the model capacity is not large enough, the robust model
will not fit the training data well. This explains why membership
inference accuracies for verifiably robust models are limited in
Table 9. However, enlarging the model capacity does not guarantee
that the training points will fit well for verifiable defenses because
the verified upper bound of robust loss is likely to be looser with
a deeper and larger neural network architecture. We validate our
hypothesis in the following two subsections.
6.2 Varying Model Capacities
We use models with varying capacities to robustly train on the Yale
Face dataset with the IBP-Based Verify defense [16] as an example.
We present the results in Figure 4, where model capacity scale of
8 corresponds to the original model architecture, and we perform
membership inference attacks based on verified worst-case predic-
tion confidence IV. We can see that when model capacity increases,
Table 8: Membership inference attacks against natural and verifiably robust models [16, 34, 61] on the Yale Face dataset with
a l∞ perturbation constraint ϵ = 8/255. Based on Equation (16), the natural model has the inference advantage of 11.70%, while
the robust model has the inference advantage up to 52.10%.
Training
method
Natural
Dual-Based
Verify [61]
Abs-Based
Verify [34]
IBP-Based
Verify [16]
train
acc
100%
test
acc
98.25%
adv-train
acc
4.53%
adv-test
acc
2.92%
ver-train
acc
N.A.
ver-test
acc
N.A.
inference
acc (IB)
55.85%
inference
acc (IA)
54.27%
inference
acc (IV)
N.A.
98.89%
92.80%
98.53%
83.66%
96.37%
68.87%
55.90%
99.26%
83.27%
85.68%
50.39%
43.32%
18.09%
65.11%
99.16%
85.80%
94.42%
69.68%
89.58%
36.77%
60.45%
60.40%
65.64%
66.28%
64.48%
67.05%
76.05%
Table 9: Membership inference attacks against natural and verifiably robust models [16, 34, 61] on the Fashion-MNIST dataset
with a l∞ perturbation constraint ϵ = 0.1.
Training
method
Natural
Dual-Based
Verify [61]
Abs-Based
Verify [34]
IBP-Based
Verify [16]
train
acc
100%
test
acc
92.18%
adv-train
acc
4.35%
adv-test
acc
4.14%
ver-train
acc
N.A.
ver-test
acc
N.A.
75.13%
74.29%
65.77%
65.36%
61.77%
61.45%
86.44%
85.47%
74.12%
73.28%
69.69%
68.89%
inference
acc (IB)
57.12%
50.58%
50.79%
89.85%
86.26%
82.60%
78.44%
79.20%
74.17%
52.13%
inference
acc (IA)
50.95%
inference
acc (IV)
N.A.
50.42%
50.69%
52.06%
50.45%
50.59%
52.67%
Table 10: Membership inference attacks against natural and verifiably robust CIFAR10 classifiers [61] trained on a subset (20%)
of the training data with varying l∞ perturbation budgets.
Training
method
Natural
Dual-Based
Verify [61]
Dual-Based
Verify [61]
Dual-Based
Verify [61]
Dual-Based
Verify [61]
Dual-Based
Verify [61]
Dual-Based
Verify [61]
Perturbation
budgets (ϵ)
train
acc
test
acc
N.A.
99.83%