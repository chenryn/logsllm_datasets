title:NetLord: a scalable multi-tenant network architecture for virtualized
datacenters
author:Jayaram Mudigonda and
Praveen Yalagandula and
Jeffrey C. Mogul and
Bryan Stiekes and
Yanick Pouffary
NetLord: A Scalable Multi-Tenant Network Architecture for
Virtualized Datacenters
Jayaram Mudigonda
Praveen Yalagandula
Jeff Mogul
HP Labs, Palo Alto, CA
ABSTRACT
Providers of “Infrastructure-as-a-Service” need datacenter net-
works that support multi-tenancy, scale, and ease of operation, at
low cost. Most existing network architectures cannot meet all of
these needs simultaneously.
In this paper we present NetLord, a novel multi-tenant network
architecture. NetLord provides tenants with simple and ﬂexible
network abstractions, by fully and efﬁciently virtualizing the ad-
dress space at both L2 and L3. NetLord can exploit inexpensive
commodity equipment to scale the network to several thousands
of tenants and millions of virtual machines. NetLord requires
only a small amount of ofﬂine, one-time conﬁguration. We im-
plemented NetLord on a testbed, and demonstrated its scalability,
while achieving order-of-magnitude goodput improvements over
previous approaches.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Archi-
tecture and Design
General Terms
Design, Experimentation, Management
Keywords
Datacenter Network, Network Virtualization, Multi-Tenant, Multi-
Pathing, Scalable Ethernet
1.
INTRODUCTION
Cloud datacenters such as Amazon EC2 [1] and Microsoft
Azure [6] are becoming increasingly popular, as they offer com-
puting resources at a very low cost, on an attractive pay-as-you-go
model. Many small and medium businesses are turning to these
cloud computing services, not only for occasional large computa-
tional tasks, but also for their IT jobs. This helps them eliminate
the expensive, and often very complex, task of building and main-
taining their own infrastructure.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15–19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
Bryan Stiekes
Yanick Pouffary
HP
Cloud datacenter operators, on the other hand, can provide cost-
effective Infrastructure as a Service (IaaS), because they can time-
multiplex the physical infrastructure among a large number of ten-
ants. The advent of mature CPU virtualization techniques (e.g.,
VMWare [26] and Xen [28]) makes it possible to convert the ded-
icated, and often extremely underutilized, physical servers in an
enterprise into Virtual Machines (VMs) that run in an IaaS data-
center.
To fully realize the beneﬁts of resource sharing, these datacenters
must scale to huge sizes. The larger the number of tenants, and the
larger the number of VMs, the better the chances for multiplexing,
which in turn achieves better resource efﬁciency and cost savings.
Increasing the scale alone, however, cannot fully minimize the
total cost. Today, a great deal of expensive human effort is required
to conﬁgure the equipment, to operate it optimally, and to provide
ongoing management and maintenance. A good fraction of these
human costs reﬂect the complexity of managing a multi-tenant net-
work; IaaS datacenters cannot become cost-effective at scale unless
we can reduce these costs.
Therefore, IaaS networks must support virtualization and multi-
tenancy, at scales of tens of thousands of tenants and servers, and
hundreds of thousands of VMs. They must keep costs down by ex-
ploiting commodity components and by facilitating automatic con-
ﬁguration and operation. Most existing datacenter network archi-
tectures, however, suffer one or more of the following drawbacks:
They are expensive to scale: Today, scaling the network to the
sizes needed by IaaS datacenters remains very expensive. The
straightforward scaling of existing datacenter networks requires
huge core switches with thousands of ports [8]. Some approaches
require complex new protocols to be implemented in hardware [5,
17], or may work only with speciﬁc features such as IP-in-IP de-
capsulation [13] and MAC-in-MAC encapsulation [5, 17]. Some
approaches that do not require switch modiﬁcations (e.g., [18]) may
require excessive switch resources – in particular, they require very
large forwarding tables, because the MAC address of every VM
is exposed to the switches. None of these architectures can easily
leverage existing, inexpensive commodity switches.
They provide limited support for multi-tenancy:
Ideally, a
multi-tenant network should provide a network abstraction that al-
lows a tenant to design its network as if it were the sole occupant of
a datacenter. That is, a tenant should be able to deﬁne its own layer-
2 (L2) and layer-3 (L3) addresses. Previous multi-tenancy architec-
tures do not provide full address-space virtualization; they either
focus on performance guarantees and performance isolation [14,
24, 25], or only provide IP address space sharing [11, 13].
They require complex conﬁguration: Many existing architec-
tures that might make use of cheaper switches often require careful
62manual conﬁguration: for example, setting up IP subnets and con-
ﬁguring OSPF [8, 13].
In this paper, we present NetLord, a novel multi-tenant virtual-
ized datacenter network architecture. NetLord encapsulates a ten-
ant’s L2 packets, to provide full address-space virtualization. Net-
Lord employs a light-weight agent in the end-host hypervisors to
transparently encapsulate and decapsulate packets from and to the
local VMs. Encapsulated packets are transferred over an underly-
ing, multi-path L2 network, using an unusual combination of IP
and Ethernet packet headers. NetLord leverages SPAIN’s approach
to multi-pathing [18], using VLAN tags to identify paths through
the network.
The encapsulating Ethernet header directs the packet to the des-
tination server’s edge switch, via L2 forwarding. By leveraging
a novel conﬁguration of the edge switch’s IP forwarding table, the
encapsulating IP header then allows the switch to deliver the packet
to the correct server, and also allows the hypervisor on that server
to deliver the packet to the correct tenant.
Because NetLord does not expose any tenant MAC addresses to
the switches (and also hides most of the physical server addresses),
the switches can use very small forwarding tables, thus reducing
capital costs, while scaling to networks with hundreds of thousands
of VMs. Because NetLord uses simple, static switch conﬁgura-
tions, this reduces operational costs.
Our focus in this paper is only on qualitative isolation between
tenants: tenants can design their L2 and L3 address spaces without
any restrictions created by multi-tenancy. NetLord itself provides
no guarantees on performance isolation between tenants [14, 24,
25]. However, because NetLord explicitly exposes tenant identi-
ﬁers in the encapsulation header, it can efﬁciently support various
slicing/QoS mechanisms using commodity switches.
In this paper, we present the NetLord design, and show through
simple calculations how it can scale. We also describe an exper-
imental evaluation, with up to 3000 tenants and 222K emulated
VMs, showing that NetLord can achieve substantial goodput im-
provements over other approaches.
2. PROBLEM AND BACKGROUND
We start by describing the problems we are trying to solve, and
then describe some prior work on similar problems.
2.1 Goals: scalable, cheap, and ﬂexible
Our goal is to provide a network architecture for a multi-tenant
virtualized cloud datacenter. We want to achieve large scale at low
cost, with easy operation and conﬁguration, and we want to provide
tenants with as much ﬂexibility as possible.
Scale at low cost: Cloud providers can offer services at low cost
because they can leverage economies of scale. This implies that
the network for a cloud datacenter must scale, at low cost, to large
numbers of tenants, hosts, and VMs. The network must support
lots of addresses, and provide ample bandwidth between the VMs
of any tenant.
A provider’s costs include both capital expenditures (CAPEX)
and operational expenditures (OPEX). To reduce CAPEX, the net-
work should use commodity, inexpensive components. However,
commodity network switches often have only limited resources and
limited features. For example, typical commercial switches can
hold a few tens of thousands of MAC forwarding information base
(FIB) table entries in their data-plane fast paths (e.g., 64K entries
in the HP ProCurve 6600 series [20] and 55K in the Cisco Catalyst
4500 series [10]).
Small data-plane FIB tables in switches create a scaling problem
for MAC-address learning: if the working set of active MAC ad-
dresses is larger than the data-plane table, some entries will be lost,
and a subsequent packet to those destinations will cause ﬂooding.
(Even when the table is large enough, a rapid arrival rate of new ad-
dresses can lead to ﬂooding, if learning is done in software and the
switch’s local management CPU is too slow to keep up.) Therefore,
we cannot afford to expose the switches to the MAC addresses of
all tenant VMs, or even of all physical servers in a large network,
because the resultant ﬂooding will severely degrade performance.
Section 5.3 demonstrates this effect experimentally.
We also want to be able to support high bisection bandwidth at
low cost. In particular, we would like to allow the cloud provider
to choose an efﬁcient and cost-effective physical wiring topol-
ogy without having to consider whether this choice interferes with
multi-tenancy mechanisms or tenant-visible abstractions.
Easy to conﬁgure and operate: To reduce OPEX, cloud providers
need networks that, as much as possible, can be conﬁgured and
operated automatically. We would like to avoid any per-switch
conﬁguration that is hard to scale, or that is highly dynamic. We
also want to avoid network-related restrictions on the placement of
VMs, to allow the cloud provider to efﬁciently multiplex physical
hosts, without worrying about resource fragmentation.
Cloud providers may wish to offer QoS features to tenants. We
would also like to provide simple mechanisms that support per-
tenant trafﬁc engineering; we do not want to require the provider to
individually manage QoS for each TCP ﬂow for each tenant.
We also want the network to handle switch and link failures au-
tomatically. We would like the unfailed portions of the network to
continue to work without being affected by failed components.
Flexible network abstraction: Different tenants will have differ-
ent network needs. A tenant wishing to run a Map-Reduce job
might simply need a set of VMs that can communicate via TCP. On
the other hand, a tenant running a three-tier Web application might
need three different IP subnets, to provide isolation between tiers.
Or a tenant might want to move VMs or entire applications from
its own datacenter to the cloud, without needing to change the net-
work addresses of the VMs. This ﬂexibility will also allow tenants
to create networks that span VMs both in their own datacenters or
on rented servers in the cloud datacenter [15].
These examples, and others, motivate our desire for a datacenter
network that fully virtualizes the L2 and L3 address spaces for each
tenant, without any restrictions on the tenant’s choice of L2 or L3
addresses.
Also, in certain kinds of cloud environments, tenants might
wish to use non-IP protocols, such as Fibre Channel over Ethernet
(FCoE), ATA over Ethernet (AoE), or HyperSCSI. These proto-
cols, while currently unsupported in public cloud networks, could
be important for tenants trying to move existing applications into
the cloud, and would be impossible to use in a network that did
not support an L2 abstraction. Similarly, these tenants would ben-
eﬁt from a cloud network that supports tenant-level broadcasts or
muliticasts.
2.2 State of the art
In this section, we ﬁrst describe current practices and recent
research proposals for multi-tenant datacenter networking. Also,
since NetLord depends on an underlying large-scale L2 network,
we discuss recent work on scalable network fabrics.
Multi-tenant datacenters: Traditionally, datacenters have em-
ployed VLANs [16] to isolate the machines of different tenants on
a single L2 network. This could be extended to virtualized datacen-
ters, by having the hypervisor encapsulate a VM’s packets with a
63VLAN tag corresponding to the VM’s owner. This simple approach
provides an L2 abstraction to the tenants, and can fully virtualize
the L2 and L3 address spaces. However, to correctly support the
Spanning Tree Protocol, each VLAN needs to be a loop-free sub-
graph of the underlying network, and that limits the bisection band-
width for any given tenant. Also, unless VLANs are carefully laid
out, this approach may expose all VM addresses to the switches,
creating scalability problems. Finally, the VLAN tag is a 12-bit
ﬁeld in the VLAN header, limiting this to at most 4K tenants. (The
IEEE 802.1ad standard on Provider Bridges [3] deﬁnes the “QinQ”
protocol to allow stacking of VLAN tags, which would relieve the
4K limit, but QinQ is not yet widely supported.)
Amazon’s Virtual Private Cloud (VPC) [2] provides an L3 ab-
straction and full IP address space virtualization. Tenants can spec-
ify up to 20 arbitrary IP subnets (at least /28 in size), and the
provider will instantiate a VPC router to connect these IP subnets.
We could not ﬁnd any documentation of how the VPC router is
implemented, and hence cannot comment on its routing efﬁciency.
VPC does not support multicast and broadcast, which implies that
the tenants do not get an L2 abstraction.
Greenberg et al. [13] propose VL2, a scalable and ﬂexible dat-
acenter network. VL2 provides each tenant (termed “service” in
the paper) with a single IP subnet (L3) abstraction, but implements
efﬁcient routing between two different IP subnets without needing
to divert the packets through an explicit router. Services running in
VL2 are expected to use only IP-based protocols. VL2 works with
a speciﬁc topology (Clos) to achieve a full bisection bandwidth
network, and hence does not restrict VM placement. However,
the approach assumes several features not common in commodity
switches, such as IP-in-IP decapsulation support at line rates. VL2
handles a service’s L2 broadcast packets by transmitting them on a
IP multicast address assigned to that service. The VL2 paper does
not explicitly address unicast non-IP packets, but we believe their
approach can be extended, as it encapsulates all packets.
Diverter [11] presents an efﬁcient fully-distributed virtualized
routing system, which accommodates multiple tenants’ logical IP
subnets on a single physical topology. Similar to VL2, Diverter ad-
dresses the problem of efﬁcient routing between subnets. Diverter’s
solution is to overwrite the MAC addresses of inter-subnet packets,
allowing it to relay these packets via a single hop. Diverter pro-
vides an L3 network abstraction to tenants, but it assigns unique
IP addresses to the VMs; that is, it does not provide L3 address
virtualization.
Scalable network fabrics: Many research projects and industry
standards address the limitations of Ethernet’s Spanning Tree Pro-
tocol. Several, including TRILL (an IETF standard) [5], Short-