title:Detecting malicious landing pages in Malware Distribution Networks
author:Gang Wang and
Jack W. Stokes and
Cormac Herley and
David Felstead
Detecting Malicious Landing Pages in Malware
Distribution Networks
Gang Wang
Computer Science
UC Santa Barbara
Santa Barbara, CA 93106
PI:EMAIL
Jack W. Stokes
Cormac Herley
David Felstead
Microsoft Research
One Microsoft Way
Redmond, WA 98052
PI:EMAIL
Microsoft Research
One Microsoft Way
Redmond, WA 98052
PI:EMAIL
Microsoft Corporation
One Microsoft Way
Redmond, WA 98052
PI:EMAIL
Abstract—Drive-by download attacks attempt to compromise
a victim’s computer through browser vulnerabilities. Often they
are launched from Malware Distribution Networks (MDNs) con-
sisting of landing pages to attract trafﬁc, intermediate redirection
servers, and exploit servers which attempt the compromise.
In this paper, we present a novel approach to discovering
the landing pages that lead to drive-by downloads. Starting from
partial knowledge of a given collection of MDNs we identify the
malicious content on their landing pages using multiclass feature
selection. We then query the webpage cache of a commercial
search engine to identify landing pages containing the same or
similar content. In this way we are able to identify previously-
unknown landing pages belonging to already identiﬁed MDNs,
which allows us to expand our understanding of the MDN.
We explore using both a rule-based and classiﬁer approach
to identifying potentially malicious landing pages. We build
both systems and independently verify using a high-interaction
honeypot that the newly identiﬁed landing pages indeed attempt
drive-by downloads. For the rule-based system 57% of the landing
pages predicted as malicious are conﬁrmed, and this success rate
remains constant in two large trials spaced ﬁve months apart. This
extends the known footprint of the MDNs studied by 17%. The
classiﬁer-based system is less successful, and we explore possible
reasons.
Keywords—Drive-by download; malware distribution network;
signature;
I.
INTRODUCTION
The reach and scale of the Internet has fostered a parasitic
industry of those who seek illegal proﬁt. A common strategy is
to infect innocent users’ machines with malicious code which
can then be used to harvest passwords, send spam, retrieve
contact lists, participate in a botnet, etc. A malware author
needs three things [1]: bad code, a way to get it running,
and an introduction to the user. The second and third often
represent
the challenge in running a cybercrime business;
that is, ﬁnding users and getting the code to run on their
machines is more of a challenge than writing the malware.
Social engineering, the process of using false pretence to
lure a user to install
the software himself, has met with
considerable success. Numerous studies have shown that users
can be manipulated into installing malware, ignoring security
warnings, and disabling protection mechanisms [2], [3]. The
introduction to the user in this case is often provided in the
form of a spam campaign.
A second approach attempts to exploit un-patched vul-
nerabilities in the applications on the user’s machine. Large
complex applications such as Adobe Acrobat, Microsoft Excel,
etc, often have vulnerabilities. Opening a malicious document
with a vulnerable application can be enough to give the attacker
the opening to get malicious code running on the user’s
machine. Again, in this case spam is often the introduction
vector. For example, many spam campaigns try to get a user
to open an attachment with lures such as “your tax request has
been denied” or “your package delivery failed” in the subject
line of the email.
A drive-by download is a particular case where the vul-
nerable application is a browser. Some browser vulnerabilities
will allow malicious code to begin running without the user’s
knowledge or consent. A user who visits a malicious webpage
with a vulnerable browser could get
infected. This opens
various possibilities for attackers. An attacker could set up
websites that host malicious content and then wait for vulner-
able browsers to come by. The number of users infected will
then be related to the amount of trafﬁc that the site can gain.
It is certainly possible to employ Search Engine Optimization
(SEO) techniques to maximize trafﬁc to a page that has nothing
except malicious code to offer. However, since even legitimate
websites compete vigorously for visitors, getting trafﬁc is by
no means a trivial proposition.
A more common approach is to infect an innocent website
with code that directs a browser to load malware from a second
site. A particularly attractive aspect of this approach is that it
allows the attacker to piggyback on someone else’s trafﬁc: the
introduction to the user is provided by the web-trafﬁc that
a site is already attracting. Rather than deface, or interfere
with the performance of the infected site the attacker generally
injects a malicious script that eventually redirects the browser
to a server hosting a malicious payload. Thus visitors with
browsers that possess the targeted vulnerabilities will become
infected. The innocent site is called the landing page, and the
site with the malware is called the exploit server. In this way
the attacker gets to infect many clients without having to earn
the trafﬁc.
Often the path from the landing page to the exploit server
contains many redirects. For example, if the attacker succeeds
in infecting the innocent webserver at foo.com he can direct
all trafﬁc to load the malicious content from evil.com. This
can be done indirectly, so that the page at foo.com points
to a.com, which points to b.com, which points to c.com
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:16 UTC from IEEE Xplore.  Restrictions apply. 
Benign
Pages
MDN with 
some landing 
pages detected
MDN with
all landing 
pages detected
Undetected
MDN
Static and Dynamic
Crawlers
Crawl Data
Our System
User Queries
Search
Engine
SERP
Filter
Results
Filtered SERP
Fig. 1: High-level overview. Our system processes webpage content from the static crawler and drive-by download detection
results from the dynamic crawler. It identiﬁes malicious webpages belonging to MDNs based on similar content which can then
be used to ﬁlter the search engine result pages.
and so on, until it eventually reaches evil.com. Often there
will be many landing pages that share a small collection of
exploit servers. They may also share some of the links in
their redirection paths to the exploit servers. The collection
of landing pages, exploit servers and intermediate redirection
pages is collectively known as a Malware Distribution Network
(MDN) [4].
malicious webpages (webpages created by the attacker), le-
gitimate but compromised webpages, or a combination of
both. For malicious webpages, content within an MDN most
likely varies to avoid ﬁltering from search results by duplicate
detection. However like the compromised webpages, a small
amount of common, malicious code is used to initiate the drive-
by download attack.
Addressing this exploitation of users involves a multi-
pronged approach. Browser vendors aggressively seek to iden-
tify and patch vulnerabilities. This is complicated by the ap-
parent reluctance of users to promptly install security updates.
For example, 22% of Internet Explorer users were still using
IE6 more than four years after the launch of IE7 and 18 months
after the launch of IE8, and 80% of users studied by Trustseer
were using un-patched versions of Flash Player [5]. Hav-
ing updates installed automatically by default has improved
this situation somewhat, but machines that are compromised
through vulnerabilities for which available patches have not
been installed remains a serious problem. Anti-virus software,
of course, is the main line of defense for most users.
Search engines actively seek to identify webpages asso-
ciated with malicious content. Such webpages can be low-
ered, or dropped altogether, in the ranked results returned to
users. However, the architecture of the MDN makes the task
of identifying exploit servers very difﬁcult. Search crawlers
typically retrieve the contents of the document at a site, and
do not run any scripts on the page. This is an unavoidable
consequence of scale: large search engines index billions of
pages per day. Rendering a page and running all scripts, as
a browser would do, can take orders of magnitudes more
resources than simply loading the main document. Thus the
malicious actions performed by the scripts on a landing page
are largely invisible to search crawlers. We outline previous
attempts at MDN detection in the Related Work Section.
In this paper, we study drive-by download attacks with
a focus on MDN landing pages. We conjecture that landing
pages within an MDN will exhibit a certain level of similarity
regarding the webpage content. We validate this hypothesis
using a large-scale dataset from a production search engine.
In an MDN, this content maybe be similar across completely
We propose a technique to extract
the MDN speciﬁc
redirection patterns and use them to detect previously-unknown
drive-by download landing pages. Our technique is to start
from a seed set of landing pages from already-known MDNs.
We identify the common malicious content on these pages and
then seek other pages in the search crawler cache that contain
this content. These are good candidates to be previously-
unknown landing pages associated with the MDN in question.
We validate the results by submitting the found pages to a
high-interaction honeypot. For example, 57% of the suspicious
pages found by the rule-based system in Section III are veriﬁed
as pointing to malicious drive-by downloads.
A high-level overview of the system environment is pro-
vided in Figure 1. The static crawler collects content from a
wide range of webpages, most of which are benign, and gives
this to the search engine. During this process, the static crawler
may also fetch content from malicious webpages belonging to
different MDNs. In parallel, the dynamic crawler is scanning
unknown webpages in order to identify malicious pages, and
the dynamic crawler output
is used to identify individual
MDNs. Our system consumes the static and dynamic crawl
data, correlates the web content for pages belonging to indi-
vidual MDNs, and identiﬁes the malicious web content within
each individual MDN. When a user enters a new query into the
search engine, the search engine returns one or more SERPs
(Search Engine Result Pages). Once the MDN content which
is suspected as being malicious has been validated by the
dynamic crawler, the corresponding webpage can be removed
from the SERP producing a ﬁltered SERP for the end user.
The ﬁltered SERP may block all, some, or none of the landing
pages within an MDN.
This paper is organized as follows. We introduce necessary
background material in the next section. In Section III we
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:16 UTC from IEEE Xplore.  Restrictions apply. 
describe and evaluate a rule-based approach to the problem. In
Section IV we present and evaluate a classiﬁer approach to the
same problem. Section V discusses the differences and merits
of the two approaches. We review related work in Section VI.
II. BACKGROUND
In this section, we provide background on three key aspects
related to our system, namely malware distribution networks,
the static search crawler, and the dynamic crawler.
A. Malware Distribution Networks
A malware distribution network (MDN) consists of three
components: landing pages, one or more redirection servers,
and the exploit servers. The attack starts when a browser
requests content from the landing page. Sometimes the landing
page redirects the user to an exploit server directly, but
more often the landing page will redirect the user to other
redirection servers before reaching the ﬁnal exploit server.
Multi-hop redirection usually exists in more sophisticated
drive-by download attacks where the redirection servers in
the middle of the infection process further examine different
conditions (e.g. browser type, version, plugins etc.) to decide
which exploit server the browser should be directed to. For
example, one redirection path might be followed by Firefox
browsers while another would be taken by particular versions
of Internet Explorer. The landing page will trigger the ﬁrst
hop of redirection. For compromised webpages, redirection is
usually caused by injected content. In an MDN, an exploit
server may handle trafﬁc from a large number of landing pages.
This architecture offers several advantages to the attackers.
It separates the functions of trafﬁc acquisition, trafﬁc redirec-
tion and exploit serving. The redirection path can be obfus-
cated. The architecture facilitates management and auditing of
trafﬁc in the redirection layer, which allows for the possibility
that the compromise of landing pages and redirection layers
are controlled by a different party than the one hosting the
exploit servers.
B. Static Search Crawler
Search engines employ crawlers to retrieve content for
indexing. For this project we had access to the crawler of a
large commercial search engine. The crawler runs continuously
visiting new pages as found, and revisiting existing pages
based on a schedule determined by the pages’ changefulness
and ranking. The crawler retrieves the content of a page for
analysis. Some, but not all, of the links identiﬁed in the content
are added to the list of pages to be subsequently crawled. The
crawler does not however fetch embedded images or execute
any scripts on the page, or attempt to render the page as a
browser would. As it retrieves the static, but not dynamic,
content from webpages it is commonly referred to as the static
crawler. For example, in fetching the page www.nytimes.com
(fetched April 16, 2012) in a browser a total of 176 requests
were issued to 31 different domains. Of the 176 requests, 54
were jpeg, 40 gif, 4 png, 7 css, 12 ﬂash, and 33 were javascript
objects. None of these objects would be fetched by the static
crawler, which limits itself to static text and HTML content.
Thus a server that uses javascript to point to a server hosting
TABLE I: An example of the output of the dynamic crawler
or DCTrace.
Landing Page
Redirection URLs
Exploit URL
IPs
DCTrace
www.foo.com/index.html
www.a.com/redirect.js
www.b.com/check.php
www.c.com/hack.js
www.evil.com/malware.exe
www.foo.com (23.21.215.24)
www.a.com (192.168.0.1)
www.b.com (192.168.0.2)
www.c.com (192.168.0.3)
www.evil.com (192.220.74.179)
File Hash
Is Drive-By Successful?
E21AD55HCCSAD7DC21B....74R
True
malicious content does not exhibit suspicious behavior to the
static crawler.
Servers that host malicious content (as opposed to pointing
to other servers that host it) will often attempt to hide their
nature from crawlers by cloaking [6]. This is a technique that
involves delivering malicious content to potentially vulnerable
visitors, but innocent content to crawlers. Since web crawlers
for major search engines operate from easily-identiﬁed blocks
of IP addresses, and strictly obey any crawling policies put in
place by robots.txt, it is simple to offer them different
content from regular web users. Thus, an exploit server which
seeks to evade discovery will not typically be reached via the
links placed in innocent pages and has no difﬁculty showing
an innocent face to any crawler that ﬁnds it by another path.
C. Dynamic Crawler
While the information retrieved by the static crawler suf-
ﬁces for search indexing, it does not reveal if a webpage
is attempting to infect the user’s machine with a drive-by
download or not. Thus, a malicious embedded script which
causes the browser to follow a series of redirects terminating
at the exploit server, will never be followed.
Thus, many search engines,
in addition to the static
crawler, have a second dynamic crawler which examines a web
page more thoroughly. The dynamic crawler can be thought
of as a active, client-side honeypot. It visits a site posing as
a vulnerable browser, and runs all the scripts on the page. In
the www.nytimes.com example above, it would fetch all of
the Flash and javascript objects and execute them. If those
scripts involve fetching other links, these links would also be
followed.
The dynamic crawler uses different vulnerable browsers
and OS components to trigger possible malicious reactions.
If any attempts to exploit known vulnerabilities are detected,
the site will be ﬂagged as potentially malicious. Since all of
this must happen in an isolation environment, it is orders of
magnitude slower than the static crawler. It is simply infeasible
to comprehensively crawl a signiﬁcant fraction of the web
using the dynamic crawler. Thus the dynamic crawler must be
reserved for pages that are suspected of being malicious. We
refer to the output of dynamic crawler as the DCTrace. This
contains all logged activity associated with the page load. Thus
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:16 UTC from IEEE Xplore.  Restrictions apply. 
it has hostnames and IP addresses of all links followed. An
example is shown in Table I.
As shown in Figure 2, our system takes two inputs: the