# Detecting Malicious Landing Pages in Malware Distribution Networks

**Authors:**
- Gang Wang, Computer Science, UC Santa Barbara, Santa Barbara, CA 93106, USA
- Jack W. Stokes, Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA
- Cormac Herley, Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA
- David Felstead, Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA

## Abstract
Drive-by download attacks aim to compromise a victim’s computer by exploiting browser vulnerabilities. These attacks often originate from Malware Distribution Networks (MDNs) that consist of landing pages, intermediate redirection servers, and exploit servers. In this paper, we present a novel approach to discovering the landing pages that lead to drive-by downloads. Starting with partial knowledge of a given MDN, we use multiclass feature selection to identify malicious content on their landing pages. We then query the webpage cache of a commercial search engine to find landing pages containing similar or identical content. This method allows us to identify previously unknown landing pages within known MDNs, thereby expanding our understanding of these networks.

We explore both rule-based and classifier-based approaches to identifying potentially malicious landing pages. Both systems are independently verified using a high-interaction honeypot. For the rule-based system, 57% of the predicted malicious landing pages are confirmed, and this success rate remains consistent over two large trials spaced five months apart, extending the known footprint of the MDNs by 17%. The classifier-based system is less successful, and we discuss possible reasons for this.

**Keywords:** Drive-by download, malware distribution network, signature

## I. Introduction
The vast reach and scale of the Internet have facilitated a parasitic industry aimed at illegal profit. A common strategy involves infecting innocent users’ machines with malicious code, which can be used for various nefarious activities such as harvesting passwords, sending spam, retrieving contact lists, or participating in botnets. A malware author needs three things: malicious code, a way to execute it, and an introduction to the user. The latter two aspects—finding users and executing the code—are often more challenging than writing the malware itself.

Social engineering, which uses deceptive tactics to trick users into installing malware, has been highly effective. Numerous studies have shown that users can be manipulated into installing malware, ignoring security warnings, and disabling protection mechanisms. Spam campaigns are a common introduction vector, luring users with false pretenses.

Another approach exploits unpatched vulnerabilities in applications like Adobe Acrobat, Microsoft Excel, and web browsers. A drive-by download attack specifically targets browser vulnerabilities, allowing malicious code to run without the user's knowledge or consent. Attackers can set up websites hosting malicious content and wait for vulnerable browsers to visit. Alternatively, they can inject malicious scripts into legitimate websites, redirecting visitors to servers hosting malicious payloads. This method allows attackers to piggyback on existing traffic, making it easier to infect many clients.

Malware Distribution Networks (MDNs) often include multiple redirects between the landing page and the exploit server. The collection of landing pages, exploit servers, and intermediate redirection pages is collectively known as an MDN.

Addressing this exploitation requires a multi-faceted approach. Browser vendors work to identify and patch vulnerabilities, but user reluctance to install updates remains a significant issue. Anti-virus software is a primary defense, and search engines actively seek to identify and demote webpages associated with malicious content. However, the architecture of MDNs makes it difficult for search crawlers to detect exploit servers, as they do not execute scripts on the page.

In this paper, we focus on detecting drive-by download attacks through MDN landing pages. We hypothesize that landing pages within an MDN will exhibit content similarities. We validate this hypothesis using a large-scale dataset from a production search engine. Our technique starts with a seed set of known MDN landing pages, identifies common malicious content, and searches for other pages in the search crawler cache containing similar content. We validate the results using a high-interaction honeypot.

Figure 1 provides a high-level overview of our system. The static crawler collects content from a wide range of webpages, while the dynamic crawler scans unknown webpages to identify malicious ones. Our system correlates the web content for pages belonging to individual MDNs, identifies malicious content, and filters search engine result pages (SERPs) to block suspected malicious landing pages.

This paper is organized as follows: Section II introduces necessary background material, Section III describes and evaluates a rule-based approach, Section IV presents and evaluates a classifier-based approach, Section V discusses the differences and merits of the two approaches, and Section VI reviews related work.

## II. Background

### A. Malware Distribution Networks
A Malware Distribution Network (MDN) consists of three components: landing pages, one or more redirection servers, and exploit servers. The attack begins when a browser requests content from a landing page. The landing page may directly redirect the user to an exploit server or to other redirection servers before reaching the final exploit server. Multi-hop redirection is common in sophisticated drive-by download attacks, where intermediate servers examine conditions like browser type and version to direct the user to the appropriate exploit server. This architecture offers several advantages to attackers, including the separation of traffic acquisition, redirection, and exploit serving, and the ability to obfuscate the redirection path.

### B. Static Search Crawler
Search engines use crawlers to retrieve content for indexing. The static crawler continuously visits new and existing pages based on a schedule determined by the pages' changefulness and ranking. It retrieves the static content of a page but does not fetch embedded images or execute scripts. This limits its ability to detect malicious behavior that relies on dynamic content. Servers hosting malicious content often use cloaking techniques to deliver innocent content to crawlers while serving malicious content to regular users.

### C. Dynamic Crawler
To detect drive-by download attempts, search engines use a dynamic crawler that examines web pages more thoroughly. The dynamic crawler poses as a vulnerable browser, runs all scripts, and follows any links. If any attempts to exploit known vulnerabilities are detected, the site is flagged as potentially malicious. Since this process is much slower than the static crawler, the dynamic crawler is reserved for pages suspected of being malicious. The output, known as DCTrace, logs all activity associated with the page load, including hostnames and IP addresses of all links followed.

As shown in Figure 2, our system takes inputs from both the static and dynamic crawlers, correlates the web content for pages belonging to individual MDNs, and identifies malicious content. When a user enters a new query into the search engine, the system filters the SERP to block suspected malicious landing pages.

[Continue with the rest of the sections as needed, following the same structure and style.]