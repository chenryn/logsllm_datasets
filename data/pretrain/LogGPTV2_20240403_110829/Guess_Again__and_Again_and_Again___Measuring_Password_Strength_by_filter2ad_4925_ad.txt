each condition weighted to one-tenth, equal, and ten times
the cumulative size of the public lists. We tested each
weighting on 500 other passwords from each condition.
Overall, we found that weighting had only a minor effect.
There were few signiﬁcant differences at one million, one
billion, or one trillion guesses, with equal weighting occa-
sionally outperforming the other two in some conditions.
From these results, we concluded that the choice of weight-
ing was not particularly important, but we used an equal
weighting in all other experiments that train with passwords
from our dataset because it provides an occasional beneﬁt.
BFM training. We also investigated the effect of training
data on BFM calculator performance, using four training
sets: one with public data only, one that combined public
data with collected passwords across our conditions, and one
each specialized for basic8 and comprehensive8. Because the
BFM algorithm eventually guesses every password, we were
concerned only with efﬁciency, not total cracking. Adding
our cross-condition data had essentially no effect at either
smaller or larger numbers of guesses. Specialized training
for basic8 was similarly unhelpful. Specialized training for
comprehensive8 did increase efﬁciency somewhat, reaching
50% cracked with about 30% fewer guesses.
C. Effects of test-data selection
Researchers typically don’t have access to passwords
created under the password-composition policy they want
to study. To compensate, they start with a larger set of
532
Experiment S1
1E2
1E0
1E4
Experiment S2
1E6
1E8
1E10 1E12
40%
32%
24%
16%
8%
d
e
k
c
a
r
c
s
d
r
o
w
s
s
a
p
f
o
%
40%
32%
24%
16%
8%
comprehensive8
comprehensive
Subset
comprehensive8
comprehensive
Subset
1E2
1E8
1E0
Number of guesses (log scale)
1E4
1E6
1E10 1E12
Figure 6.
Passwords generated under the comprehensive8 condition
proved signiﬁcantly easier to guess than passwords that conform to the
comprehensive8 requirements but are generated under other composition
policies. In experiment S1 (top), the Weir calculator was trained with only
public data; in experiment S2 (bottom), the Weir calculator was trained on
a combination of our data and public data.
passwords (e.g., the RockYou set), and pare it down by dis-
carding passwords that don’t meet the desired composition
policy (e.g., [1], [13]). A critical question, then, is whether
subsets like these are representative of passwords actually
created under a speciﬁc policy. We ﬁnd that such subsets
are not representative, and may in fact contain passwords
that are more resistant to guessing than passwords created
under the policy in question.
In our experiments, we compared the guessability of 1000
comprehensive8 passwords to the guessability of the 206
passwords that meet the comprehensive8 requirements but
were collected across our other seven conditions (the com-
prehensiveSubset set). We performed this comparison with
two different training sets: public data, with an emphasis on
RockYou passwords that meet comprehensive8 requirements
(experiment S1); and the same data enhanced with our other
2000 collected comprehensive8 passwords (experiment S2).
Both experiments show signiﬁcant differences between
the guessability of comprehensive8 and comprehensiveSub-
set
test sets, as shown in Figure 6. In the two experi-
ments, 40.9% of comprehensive8 passwords were cracked
on average, compared to only 25.8% comprehensiveSubset
passwords. The two test sets diverge as early as one billion
guesses (6.8% to 0.5%).
Ignoring comprehensiveSubset passwords that were cre-
ated under basic16 leaves 171 passwords, all created under
less strict conditions than comprehensive8. Only 25.2% of
these are cracked on average, suggesting that subsets drawn
exclusively from less strict conditions are more difﬁcult to
guess than passwords created under stricter requirements.
To understand this result more deeply, we examined the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:23 UTC from IEEE Xplore.  Restrictions apply. 
Passwords cracked
Empirically
estimated entropy
basic16
NIST entropy
comprehensive8
blacklistHard
blacklistMedium
basic8
blacklistEasy
dictionary8
basic8survey
comprehensive8
basic16
blacklistHard
blackListMedium
blackListEasy
dictionary8
basic8
basic8survey
1E9
1E12
5E13     
43
38
33
28
23
18
y
p
o
r
t
n
e
f
o
s
t
i
B
0%
cracked
13%
26%
39%
52%
65%
cracked
1E3 
Number of guesses
1E6
Figure 7. Relationship among the resistance of our collected password sets to heuristic cracking (experiment E); empirical entropy estimates we calculate
from those sets; and NIST entropy estimates for our password conditions.
distribution of structures in the two test sets. There are 618
structures in the 1000-password comprehensive8 set, com-
pared to 913 for comprehensiveSubset (normalized), indi-
cating greater diversity in comprehensiveSubset passwords.
This distribution of structures explains why comprehensive8
is signiﬁcantly easier to guess.
We suspect this difference may be related to comprehen-
siveSubset isolating those users who make the most complex
passwords. Regardless of the reason for this difference,
however, researchers seeking to compare password policies
should be aware that such subsets may not be representative.
D. Guessability and entropy
Historically, Shannon entropy (computed or estimated by
various methods) has provided a convenient single statistic to
summarize password strength. It remains unclear, however,
how well entropy reﬂects the guess resistance of a password
set. While information entropy does provide a theoretical
lower bound on the guessability of a set of passwords [41],
in practice a system administrator may be more concerned
about how many passwords can be cracked in a given num-
ber of guesses than about the average guessability across the
population. Although there is no mathematical relationship
between entropy and this deﬁnition of guess resistance, we
examine whether the two are correlated in practice. To do
this, we consider two independent measures of entropy, as
deﬁned in Section IV-B: an empirically calculated estimate
and a NIST estimate. For both measures, we ﬁnd that en-
tropy estimates roughly indicate which composition policies
provide more guess resistance than others, but provide no
useful information about the magnitude of these differences.
Empirically estimated entropy. We ranked our password
conditions based on the proportion of passwords cracked in
our most complete experiment (E) at one trillion guesses,
and compared this to the rank of conditions based on
empirically estimated entropy. We found these rankings,
shown in Figure 7, to be signiﬁcantly correlated (Kendall’s
τ = 0.71, Holm-corrected p = 0.042). However, at one
million or one billion guesses, the correlation in rankings is
no longer signiﬁcant (Holm-corrected p = 0.275, 0.062). We
found the same pattern, correlation at one trillion guesses
but not one billion or one million, in our largest public-
data experiment (P4). These results indicate entropy might
be useful when considering an adversary who can make a
large number of guesses, but not when considering a smaller
number of guesses.
Further, empirically estimated entropy did not predict the
ranking of dictionary8, even when considering a large num-
ber of guesses. This condition displayed greater resistance to
guessing than basic8, yet its empirically estimated entropy
was lower. This might indicate a ﬂaw in entropy estimation,
a ﬂaw in the guessing algorithm, or an innate shortcoming
of the use of entropy to predict guessability. Since entropy
can only lower-bound the guessability of passwords, it is
possible for the frequency distribution of dictionary8 to have
low entropy but high guess resistance. If this is the case,
Verheul theorized that such a distribution would be optimal
for password policy [51].
NIST entropy.
Computing the NIST entropy of our
password conditions produces three equivalence classes, as
shown in Figure 7, because the heuristics are too coarse to
capture all differences between our conditions. First, NIST
entropy does not take into account the size of a dictionary
or details of its implementation, such as case-insensitivity
or removal of non-alphabetical characters before the check.
All ﬁve of our dictionary and blacklist conditions meet
the NIST requirement of a dictionary with at least 50,000
words [11]. Our results show that these variations lead to
password policies with very different levels of password
strength, which should be considered in a future heuristic.
Second, the NIST entropy scores for basic16 and compre-
hensive8 are the same, even though basic16 appears to be
much more resistant to powerful guessing attacks. This may
533
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:23 UTC from IEEE Xplore.  Restrictions apply. 
suggest that future heuristics should assign greater value to
length than does the NIST heuristic.
Perhaps surprisingly,
the equivalence classes given by
NIST entropy are ordered correctly based on our results
for guessability after 50 trillion guesses. Though it fails to
capture ﬁne-grained differences between similar password
conditions, NIST entropy seems to succeed at its stated
purpose of providing a “rough rule of thumb” [11].
We stress that although both measures of entropy provide
a rough ordering among policies, they do not always cor-
rectly classify guessability (see for example dictionary8),
and they do not effectively measure how much additional
guess resistance one policy provides as compared to another.
These results suggest that a “rough rule of thumb” may be
the limit of entropy’s usefulness as a metric.
VI. DISCUSSION
We next discuss issues regarding ethics, ecological valid-
ity, and the limitations of our methodology.
Ethical considerations.
Most of our results rely on
passwords collected via a user study (approved by our insti-
tution’s IRB). However, we also use the RockYou and MyS-
pace password lists. Although these have collectively been
used by a number of scientiﬁc works that study passwords
(e.g., [2], [13], [25], [30]), this nevertheless creates an ethical
conundrum: Should our research use passwords acquired
illicitly? Since this data has already been made public and is
easily available, using it in our research does not increase the
harm to the victims. We use these passwords only to train
and test guessing algorithms, and not in relationship with
any usernames or other login information. Furthermore, as
attackers are likely to use these password sets as training
sets or cracking dictionaries, our use of them to evaluate
password strength implies our results are more likely to be
of practical relevance to security administrators.
Ecological validity. As with any user study, our results
must be understood in context. As we describe in Sec-
tion I, our participants are somewhat younger and more
educated than the general population, but more diverse than
typical small-sample password studies. The passwords we
collected did not protect high-value accounts, reﬂecting a
long-standing limitation of password research.
To further understand this context, we tested two
password-creation scenarios (Section III-C): a survey sce-
nario directly observing user behavior with a short-term,
low-value account, and an email scenario simulating a
longer-term, higher-value account. In both cases, users knew
they might be asked to return and recall the password. Our
users provided stronger passwords (measured by guessability
and entropy) in the email scenario, a result consistent with
users picking better passwords to protect a (hypothetical)
high-value e-mail account than a low-value survey account.
To get real-world measures of password-related behavior,
we surveyed users of Carnegie Mellon University’s email
system, which uses the comprehensive8 policy [9]. Com-
paring these survey results to the reports of our MTurk
study participants, we ﬁnd that on several measures of
behavior and sentiment, the university responses (n = 280)
are closer to those of our comprehensive8 participants
than those of any other condition. For example, we asked
MTurk participants who returned for the second half of the
study whether they stored the password they had created
(reassuring them they would get paid either way); we
similarly asked university participants whether they store
their login passwords. 59% of the university respondents
report writing down their password, compared with 52%
of comprehensive8 participants and a maximum of 37%
for other MTurk conditions. These results show that study
participants make different decisions based on password-
composition requirements, and that in one condition their
behavior is similar to people using that policy in practice.
We designed our study to minimize the impact of sam-
pling and account-value limitations. All our ﬁndings result
from comparisons between conditions. Behavior differences
caused by the ways in which conditions differ (e.g., us-
ing a different technique to choose longer passwords than
shorter ones) would be correctly captured and appropriately
reﬂected in the results. Thus, we believe it
likely that
our ﬁndings hold in general, for at least some classes of
passwords and users.
Other limitations. We tested all password sets with a
number of password-guessing tools; the one we focus on
(the Weir algorithm) always performed best. There may exist
algorithms or training sets that would be more effective
at guessing passwords than anything we tested. While this
might affect some of our conclusions, we believe that most
of them are robust, partly because many of our results are
supported by multiple experiments and metrics.
In this work, we focused on automated ofﬂine password-
guessing attacks. There are many other real-life threats to
password security, such as phishing and shoulder surﬁng.
Our analyses do not account for these. The password-
composition policies we tested can induce different behav-
iors, e.g., writing down or forgetting passwords or using
password managers, that affect password security. We report
on some of these behaviors in prior work [46], but space
constraints dictate that a comprehensive investigation is
beyond the scope of this paper.
VII. CONCLUSION
Although the number and complexity of password-
composition requirements imposed by systems administra-
tors have been steadily increasing, the actual value added
by these requirements is poorly understood. This work takes
a substantial step forward in understanding not only these
requirements, but also the process of evaluating them.
We introduced a new, efﬁcient technique for evaluating
password strength, which can be implemented for a variety
534
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:23 UTC from IEEE Xplore.  Restrictions apply. 
of password-guessing algorithms and tuned using a variety
of training sets to gain insight into the comparative guess re-
sistance of different sets of passwords. Using this technique,
we performed a more comprehensive password analysis than
had previously been possible.
We found several notable results about the comparative
strength of different composition policies. Although NIST
considers basic16 and comprehensive8 equivalent, we found
that basic16 is superior against large numbers of guesses.
Combined with a prior result that basic16 is also easier for
users [46], this suggests basic16 is the better policy choice.
We also found that the effectiveness of a dictionary check
depends heavily on the choice of dictionary; in particular,
a large blacklist created using state-of-the-art password-
guessing techniques is much more effective than a standard
dictionary at preventing users from choosing easily guessed
passwords.
Our results also reveal important information about con-
ducting guess-resistance analysis. Effective attacks on pass-
words created under complex or rare-in-practice composition
policies require access to abundant, closely matched training
data. In addition, this type of password set cannot be charac-
terized correctly simply by selecting a subset of conforming
passwords from a larger corpus; such a subset is unlikely to
be representative of passwords created under the policy in
question. Finally, we report that Shannon entropy, though
a convenient single-statistic metric of password strength,