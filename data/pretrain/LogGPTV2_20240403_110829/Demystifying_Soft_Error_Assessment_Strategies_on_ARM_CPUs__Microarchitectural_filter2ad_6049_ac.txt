4.0 %
1.7 %
4.0 %
3.7 %
3.8 %
4.0 %
Avg Err
2.9 %
3.0 %
3.7 %
3.7 %
4.0 %
4.0 %
an initial unknown AVF estimation [12], which, as suggested
by [70], is set to p = 0.5 in order to maximize the fault
sample. After the execution of simulation campaign however
we can re-adjust the p variable in the formula with the result
of the estimation, shifted by the maximum error margin. This
gives us a tighter estimation of the error margin for each
combination of workload/component, which in our results
varies between 1.7% and 4% with 99% conﬁdence. Table IV
shows the error margin range for each component in the fault
injection campaigns.
fault
Microarchitecture level
injection offers signiﬁcant
amount of observability, allowing distinction of where exactly
did the fault strike (e.g., whether it was on kernel or user
mode or data, whether the corrupted entry was used or not
etc.) but also detailed information of what was the system
effect. The default fault classiﬁcation is sufﬁcient to match
the beam experiment fault effect classes, and each fault
injection simulation can be characterized as SDC, System
Crash, Application Crash or Masked, depending on the result
of the simulation. That being said, the vulnerability estimation
of GeFIN can be compared against beam experiments, but
requires a conversion, as the original outcome of a fault
injection campaign is vulnerability estimation [12] rather than
failure rate. Details on how AVF can be attributed to FIT rates
are presented in Section VI.
D. Fault Injection vs. Radiation Experiment
To avoid any difference not related to the reliability evalua-
tion that can bias our results we used exactly the same source
code, compiler, compiler options, and input vector (size and
values) for both fault injection and beam experiments. Still,
the setups used for beam experiments and fault injection are
intrinsically different as we are comparing the execution on
actual hardware vs. Gem5 simulation [71]. To evaluate if the
benchmark execution shows any difference when running in
the setups used for beam experiments and fault injection, we
compare the execution of the same code in the two setups
using 7 different counters: CPU cycles, branch misses, L1 data
cache accesses, L1 data cache misses, L1 data TLB misses,
L1 instruction cache misses, and L1 Instruction TLB misses.
About 70% of the counters report acceptable deviations be-
tween the two setups. The biggest difference is observed in
the L1 Instruction TLB counters. Literature actually identiﬁes
certain design differences that exist in the implementation
of TLB of Gem5 and ARM Cortex microarchitectures that
support these observations [71]. Differences are to be expected
31
Fig. 2. Radiation test setup at LANSCE.
it is highly unlikely to see more than a single corruption
during program execution. We have carefully designed the
experiments to maintain this property (observed error rates
were lower than 1 error per 1,000 executions). Experimental
data, then, can be scaled to the natural radioactive environment
without introducing artifacts.
Experiments aim to measure the Failures In Time (FIT)
rate of the device executing a code (failures per 109 hours
of operation). FIT depends only on the kind and amount of
resources required for computation, without considering the
code execution time [20].
C. Fault Injection
The microarchitectural modeling is based on Gem5 simula-
tor, a ﬂexible full system cycle-accurate simulator [69]. Gem5
fully supports ARM ISA and comes along with a detailed out-
of-order core implementation. The CPU core was conﬁgured
to resemble the microarchitecture of Cortex-A9. Among the
various abstraction models available, microarchitecture level
is the only one that comes with sufﬁcient hardware detail
and is capable of running full-system simulation. While RTL
is certainly more accurate, the limited simulation throughput
does not allow running of multiple fault injection simulations
of a full system stack that includes an operating system and
transactions with I/O peripheral devices.
GeFIN fault injection framework [13] was used on top of
Gem5 for the reliability assessment. In order to replicate the
effects of beaming, GeFIN was conﬁgured to inject single-
event transient faults during the simulation of the system.
The faults were injected in six components: L2 Cache, L1
Data and Instruction Caches, Physical Register ﬁle, Data
and Instruction Translation Lookaside Buffers (TLB). These
components cover more than 94% of the available memory
cells modeled inside the CPU.
A set of 1,000 single-bit faults was generated for every
component, resulting in 6,000 fault injection simulations per
program. According to [70] formulation on statistical fault
sampling calculation, this corresponds to 4% error margin
with 99% conﬁdence level. This estimation corresponds to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
discussed in the following, FFT and Qsort have a higher
Application Crash FIT compared to other benchmarks; their
Application Crash FIT is even higher than their System Crash
rate. As shown in [39], System Crashes have a component
which is intrinsic to the particular hardware platform, only.
Even resilient codes (i.e., with very low SDC or Application
Crash rates), then, could experience a relative high number of
System Crashes. This is the case of CRC32, Rijndael D, and
Rijndael E in Figure 3. The benchmarks with the highest Sys-
tem Crashes FIT are Dijkstra, MatMul, StringSearch, and the
three Susans benchmarks. These are actually the benchmarks
with the smallest input size (please refer to Table III), which is
not even sufﬁcient to ﬁll all caches. Since there is space avail-
able in the caches, the Linux kernel code will not be evicted
when the context is switched to the application. When Linux
is in idle, its data is then exposed and vulnerable to radiation.
An error in the kernel code is likely to lead to a System Crash,
exacerbating the System Crash vulnerability. This is aligned
with previous studies that shown a higher operating system
error rate in the absence of cache conﬂicts [36].
There is also a signiﬁcant variation among the Application
Crash FIT of the different benchmarks. The benchmarks with
highest Application Crash rate are MatMul, Qsort, and FFT
while the lowest (about 2 orders of magnitude lower than
MatMul) is CRC32. The codes with the higher Application
Crash FIT are found to be the ones with higher presence of
control-ﬂow operations, higher use of stack for memory or
nested loops. Additionally, these applications are the ones with
non-coherent memory accesses. This requires several accesses
(reads and writes) to the main memory, which is outside the
irradiated chip. To access the main memory it is necessary
to use an interface between the core and the main memory.
It has already been demonstrated that errors during intra-chip
communications are likely to lead to an Application Crash as
an error in the interface or in one of the cores involved in the
communication makes the application to wait indeﬁnitely [39].
We observe as well that while Rijndael has similar Application
Crash FIT rate for both encoding and decoding, for Jpeg C
the coding has a much higher (almost 1 order of magnitude)
Application Crash FIT rate than the decoding. This can be
explained by the fact that in the case of Rijndael the process
to encode and decode is algorithimcally almost identical, but
in Jpeg D the decoding is achieved by doing the reverse
steps from the encoding. This means that the program ﬂow
is different and behaves differently when an error occurs.
For SDC FIT rate,
the difference between the lowest
(Dijkstra) and the highest (Qsort) is around 2 orders of
magnitude. This result conﬁrms previous studies showing that
the SDC rate is strongly application dependent [39]. The codes
with lower SDC FIT rate are the three Susans, StringSearch,
MatMul, Dijkstra, and CRC32. These benchmarks either have
a small input (Susans, StringSearch, MatMul) or have long
memory latency (CRC32). Long memory latency, in fact, has
been shown to reduce the error rate of a device as while
waiting for memory transfer the core is in idle state and, thus,
unlike to produce an error [39]. We also observe that for SDC
Fig. 3. Beam FIT rates for SDCs, Application Crashes and System Crashes.
as the Gem5 model
is not exactly the same as the one
implemented in hardware. The main goal of this paper is
to understand if microarchitectural simulations can provide
accurate insights on the corresponding hardware reliability.
To predict the error rate of a code using fault injection so
to compare it with the one measured with beam experiments
it
is necessary to know the raw probability for faults to
occur. In principle, multiplying the raw fault probability of
each microarchitectural resource by its AVF would provide
the realistic error rate of the code executed on the device.
However, measuring the fault probability for each hardware
resource would require too much time and, when dealing with
COTS, could be unfeasible due to visibility limitations. In this
paper, we decide to use the experimentally measured raw fault
rate of a bit in the L1 cache as a reference for the Cortex-
A9 technology fault probability. This simpliﬁcation is justiﬁed
by the fact
that caches are normally the most vulnerable
resource in a microprocessor and also the targeted components
in GeFIN are implemented all in the same SRAM technology
as the L1 cache.
V. FAULT INJECTION AND BEAM TESTING DATA
In this Section we present and discuss beam experiment
results and fault
injection analysis, highlighting the main
insights on the reliability of the ARM A9 each methodology
provides. In the next Section we will compare the FIT rates
measured with beam experiments and predicted with fault
injection.
A. Beam Experiments Results
Figure 3 shows the FIT rates for the 13 benchmarks tested
at LANSCE following the experimental procedures detailed
in Section IV-B. We report the FIT rate for SDC, Application
Crash, and System Crash. It is worth noting that these three
types of events are uncorrelated and independent, in the sense
that at most one of the three events occurs in one execution
and its occurrence will not affect the probability of errors in
the next executions.
From Figure 3 it is clear that a System Crash is the most
likely event, for all the benchmarks but FFT and Qsort. As
32
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4. Fault injection effects classiﬁcation for all 13 benchmarks in all 6 components. Effects are Masked, AppCrash, SysCrash, and SDC. The AVF
corresponds to the sum of the three non-Masked cases.
rate for both encoding and decoding of Jpeg and Rijndael
benchmarks have similar FIT SDC rate. This can be explained
noting that, for Rijndael, both encoding and decoding have
equal input size and, for Jpeg, the combination of input and
output have equal size (the input of the encoding and the
output of the decoding have almost identical sizes).
B. Fault injection results using GeFIN
Unlike beam experiments, fault injection campaigns were
executed for each hardware component separately. Figure
4 illustrates the AVF estimation reported by GeFIN. With
fault injection we can also count the faults that were benign
(masked) and the fault was either overwritten or did not affect
the execution in any observable way. Figure 4 presents the
AVF distribution of each fault class, SDC, Application Crash
or System Crash, while the summary of all non-masked cases
expresses the vulnerability of the structure (AVF). Notice that
AVF does not only depend on a structure’s size or organization,
and is aggregately related to the signiﬁcance of the stored data
to the correct operation of the system.
As one would intuitively expect, the majority of SDC results
(grey areas) are related with the structures that mostly contain
data, which are the L1 Data Cache and L2 Cache. In contrast,
we can see how most of the abnormalities reported by faults in
the L1 Instruction Cache are crashes. Interestingly, we can see
that most of the benchmarks have higher Application Crashes
than System Crashes, with CRC32, Qsort and StringSearch
being the only outliers.
The TLBs are consistently highly vulnerable. The reported
fault injections refer to the Physical page (target) of the tables
as they mostly lead to either incorrect memory translations
or wrong permission ﬂags. Incorrect translations will lead to
use of wrong data in all references of the particular page. In
contrast, the virtual part (tag) has almost zero vulnerability as
corruption in the tag can mainly result to tag misses and thus
invoke unnecessary page walks, which introduces a perfor-
mance penalty. The Register ﬁle is involved in both control and
data processing and the vulnerability is evenly distributed in all
classes, without particular trends. However, we can see how
both Rijndael benchmarks report high probability of SDCs,
and this can be attributed to the high level of instruction level
parallelism of the algorithms, which results in high utilization
of the register ﬁle for data processing.
Although the AVF measurement is not related only to the
size of a component, the probability that a fault is introduced at
a particular structure (by a particle) is highly related to its size.
Each TLB for instance, has a size of 512 bytes (4,096 bits)
while the L1 Cache memories have a size of 32KB (252,144
bits). Consequently, the probability that a fault will strike the
TLB is only 1/64th of the Cache’s probability. That being said,
the L2 Cache, which covers more than 80% of the modeled
memory cells of the system, suffers the most by the striking
of faults.
VI. COMPARISON OF FAULT INJECTION AND BEAM DATA
In this Section we compare the FIT rate measured with beam
experiments and predicted with fault injection simulations.
The Architectural Vulnerability Factor expresses the prob-
ability that a fault in a speciﬁc hardware structure can result
in a corruption in the execution of a program. The metric is
independent of the components technology or environmental
factors, which are however related to the soft error rate.
There is a direct connection between the failure rate and the
vulnerability of a structure. The soft error rate quantiﬁes how
many faults are introduced in a hardware structure at a given
period of time. As the structures size increases, the error rate is
also increased. If we want to attribute the FIT of a component
to its size, we need to have a per-bit FIT, which is called
raw FIT, or F ITraw. The FIT of a component can then be
expressed as:
F ITcomponent = F ITraw(bit)∗Size(bits)∗AV Fcomponent
The size of each component is known a priori and the AVF
is provided by GeFIN. The only missing attribute is the
33
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 5. Fault Injection FIT rates for SDCs, Application Crashes and System
Crashes.
one that depends on the technology, which is the F ITraw.
To have a perfect emulation of the realistic error rate we
would need the F ITraw for all the resources in the processor,
which is unfeasible mainly for COTS devices, as explained
in Section II. We decide to use the L1 cache bit F ITraw as
representative of the Cortex-A9 technology as implemented in
the Xilinx Zynq. We choose L1 cache because it is among the
most vulnerable resources, while at the same time uses the
same SRAM technology with rest of the CPU components.
The L1 cache bit FIT was used as a common baseline for all
the resources in the ARM CPU.
To experimentally measure the F ITraw, we exposed a
speciﬁc benchmark designed to test the L1 data cache. This is
achieved ﬁlling byte-by-byte the L1 data cache with a known
pattern and read it after a period of time, comparing the read
values with the pattern. This gives us the FIT rate for the
cache and, diving it by the tested cache size, it gives us the
cache FIT per bit. For the L1 cache, the measured error rate is
2.76x10−5 FIT per bit, very close to other publicly available