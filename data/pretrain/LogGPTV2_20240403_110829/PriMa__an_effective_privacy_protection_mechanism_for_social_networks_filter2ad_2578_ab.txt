fcr(Tk(i))
∗ L(i) ∗ α(Tk(i))
321Once a sensitivity value θ is assigned to each trait Tl(i) in a pro-
ﬁle profi = [T1(i), . . . , Tm(i)], we can cluster traits into γ1(i),. . .,
γk(i), k ≥ 2 classes based on traits level of sensitivity computed
according to Deﬁnition 3.1. We employ the k-means clustering
for discrete objects algorithm [16] (discrete k-mean for short) in
order to perform the partitioning. Using the k-means algorithm, the
system administrator can control the granularity of the partitions
and, therefore, the granularity of the privacy policies generated by
varying the value of k. Once the clustering is ﬁnished, the overall
sensitivity of each partition, denoted by θ(γ(i)), is calculated as
the mean of the sensitivity scores of all the traits in that cluster.
4.
P RIM A PRIVACY POLICY GENERATION
One of the key features of P riM a is that the policy protect-
ing the traits in users’ proﬁles is automatically suggested by the
framework based on users privacy inclinations and the actual risk
of exposing the traits to a certain set of users. Such a policy is
speciﬁed by means of a set of access rules that essentially state
who is granted access to the classes of traits generated for a user
proﬁle (positive rules) and who is denied access (negative rules).
A P riM a access rule is represented as a tuple (AccRule) of the
form hP red, U sers, γi where P red is a predicate that can assume
the values Share, and N otShare which denote positive and nega-
tive rules respectively. U sers is the set of users to which the policy
is applied. U sers can be a set of users identiﬁers or a set of rela-
tionships, and γ is the partition of traits protected by the rule. At
implementation level, only positive access rules are needed because
negative rules are complementary to the positive ones. Without loss
of generality, we show how the rules are computed for users who
have a ﬁrst-degree relationship with the proﬁle owner and we as-
sume that the other users, who are not related to the proﬁle owner
by a ﬁrst-degree relationship are denied access. It is straightforward
to apply the same mechanism to connections of higher degree. Ac-
cess rules are generated for each class of traits γn(i) in which a
proﬁle profi has been partitioned, based on the notion of user ac-
cess score.
The user access score, denoted as δ(γn(i), j), is representa-
tive of the adequacy of a given user j to access a given partition
γn(i), n ≤ k, (where k is the number of classes of traits in which
user i proﬁle has been partitioned). We use two metrics to compute
the user access score, the relationship score and the risk. The rela-
tionship score rates the strength of the relationship between user i
and j; the higher their relationship score, the stronger the relation-
ship. The second dimension estimates the risk of disclosing a class
of traits γn(i) to the user j. Even if a user is trusted, it may be
objectively unsafe to disclose certain traits to him. We compute a
score that is an indicator of the strength of the relationship between
two users, which does not solely depend on the users’ input. We
calculate the relationship score of two users i and j as
rel(i, j) = type ∗
#(deg(j) ∩ deg(i))
#deg(i)
∗ repj
where type is a normalized numerical value assigned to each rela-
tionship type, such that the closer the relationship, the higher the
score. Here, we refer to SNs that have a hierarchical structure for
ﬁrst degree relationships. For example for the relationships “Best
Friends”, type = 1, for “Friends”, type = 0.8, F riendsof F riends =
0.6 and so on. If two users share more than one relationship, then
the closest one is taken for assigning the value for type. If only one
relationship type exists, or if the supported relationships are not hi-
erarchically sorted, the value of type is set to 1 by default.
The parameter repj denotes the normalized rating given by user
i to j. This value can be directly input by the user, or calculated
using approaches such as [6]. The ratio #(deg(j)∩deg(i))
expresses
the similarity in terms of common ﬁrst-degree connections. To es-
timate the access score, we also consider the speciﬁc risk level as-
sociated with the disclosure of a trait set γn(i) to a user j. The risk
score of disclosing a class of traits γn(i) of user i to a user j is
calculated as follows:
#deg(i)
risk(γn(i), j) = L(j) ∗ θ(γn(i))
(1)
In the equation, we use L(j), the looseness of user j, to express the
information leakage associated with j, and combine it with actual
sensitivity of the partition under consideration.
We are now ready to deﬁne the user access score, which is cal-
culated as the ratio between the relationship and the risk score.
δ(γn(i), j) =
rel(i, j)
risk(γn(i), j)
(2)
The user access score is directly related to rel(i, j) because the
relationship score indicates the strength of the relationship between
two users. On the other hand, the higher the risk of a user with
respect to γn(i), the lower should be the access score of the user.
Once the adequacy scores are computed, generating access rules
in P riM a is straightforward. For each class γn(i), 1 ≤ n ≤ k
the set of users in deg(i) is partitioned into two smaller sets, U sers
and U sers′′. U sers is the set of users allowed to access γn(i)
while U sers′′ is the set of users who are denied access to γn(i). To
build sets U sers and U sers′′ for a class γn(i), for each and every
user, j, in deg(i), the algorithm checks whether the user has enough
privilege to access the partition under consideration by verifying
that the user access score δ(γn(i), j) is greater than the threshold
ξ.
If this condition is met by the user j, he is added to the set
U sers. Otherwise, he is added to the set U sers′′. Following this
approach, the positive and negative access control rules for γn(i)
are generated using the sets U sers and U sers′′. The algorithm
iterates this process over all the partitions in order to cover all the
traits in the user’s proﬁle. Such approach helps deﬁne the access
rules and the policy at a very ﬁne granularity, i.e. on a per user, per
partition basis. The algorithm is efﬁcient, with a complexity linear
to the number of users connected with the proﬁle owner.
5. RELATED WORK
SNs demand a new approach to access control [8, 4, 9], ﬂexi-
ble and based on interpersonal relationships. A ﬁrst attempt along
this direction has been taken by authors in [9], where a social-
networking based access control scheme suitable for online shar-
ing is presented. In the proposed approach, authors consider iden-
tities as key pairs, and social relationship on the basis of social
attestations.In [8], authors proposed a content-based access con-
trol model, which makes use of relationship information available
in SN for denoting authorized subjects More sophisticated mecha-
nisms have been proposed in [4, 3]. Carminati and colleagues pre-
sented a rule-based access control mechanism for SN. Such an ap-
proach is based on enforcement of sophisticated policies expressed
as constraints on the type, depth, and trust level of existing rela-
tionships. Subsequently, the same group of authors has extended
the previously proposed model[3] to make access control decisions
completely decentralized and collaborative. This work is orthogo-
nal to ours, since PriMa does not only deal with privacy of users’ re-
lationships, but also to ﬁne-grained data protection. In an approach
parallel to ours, Lindamood et al.
[11] have leveraged the data
available on SN including relationship and the effect that chang-
ing a user’s trait has on their privacy. Lindamood et ,al. employ
a Naive Bayes classiﬁer to classify the data gathered from SN.The
322[9] K. Kollu, S. Saroiu, and A. Wolman. A social
networking-based access control scheme for personal
content. In 21st ACM Symposium on Operating Systems
Principles. Work in Progress, October 2007.
[10] S. R. Kruk, A. Gzella, and S. Grzonkowsk. D-FOAF
distributed identity management based on social networks. In
First Asian Semantic Web Conference, pages 140–154, 2006.
[11] J. Lindamood, R. Heatherly, M. Kantarcioglu, and
B. Thuraisingham. Inferring private information using social
network data. In 18th International World Wide Web
Conference (WWW2009), 2009, ACM.
[12] P. Massa and P. Avesani. Controversial users demand local
trust metrics: an experimental study on epinions.com
community. In 25th American Association for Artiﬁcial
Intelligence Conference (AAAI), 2005.
[13] G. McLachlan and T. Krishnan. The EM algorithm and
extensions. Wiley series in probability and statistics.
[14] B. Monahan. Gnosis: HP labs modeling and simulation
framework. Systems Security Lab, 2009.
[15] M. E. J. Newman. The mathematics of networks. In Blume,
L.E., Durlauf, S.N. (eds.), The New Palgrave Encyclopedia of
Economics, 2nd edn. Palgrave Macmillan, Basingstoke,
2008.
[16] D. Pelleg and A. W. Moore. X-means: Extending k-means
with efﬁcient estimation of the number of clusters. In ICML
’00: Proceedings of the Seventeenth International
Conference on Machine Learning, San Francisco, CA, USA,
2000.
[17] P. Resnick and R. Zeckhauser. The value of reputation on
eBay: A controlled experiment. 9(2):79Ð101, 2006.
[18] S. E. Robertson, C. J. van Rijsbergen, and M. F. Porter.
Probabilistic models of indexing and searching. In SIGIR
’80: Proceedings of the 3rd annual ACM conference on
Research and development in information retrieval, pages
35–56, Kent, UK, 1981. Butterworth & Co.
[19] E. Zheleva and L. Getoor. To join or not to join: the illusion
of privacy in social networks with mixed public and private
user proﬁles. In WWW ’09: Proceedings of the 18th
international conference on World wide web, pages 531–540,
2009. ACM.
[20] C.-N. Ziegler and J. Golbeck. Investigating interactions of
trust and interest similarity. Decis. Support Syst.,
43(2):460–475, 2007.
authors explore the effect of sanitizing both traits and link details.
We borrow the idea of representing users’ proﬁle in terms of traits
from Lindamood’s work. However, this work focuses on the effect
of trait and link sanitization on private information leakage rather
than leveraging the data available on the SN to provide an access
control mechanism geared towards preventing the leakage of pri-
vate information.
Regarding the evaluation of users’ relationships, a number of re-
searchers have investigated trust metrics in SN[6, 20, 4]. The trust
metrics are a means to predict the trustworthiness of a user - often
unknown to the focus user. Our work uses a metric, the user ac-
cess score, that elegantly leverages the user’s perceived local trust
-expressed by the reputation value- with supervised SN centric di-
mensions, such as the objective risk associated with certain users.
Finally, Ziegler et al. [20] argue that there is strong evidence for
correlation between user similarity and trust. Our learning ap-
proach for predicting users inadequacy on speciﬁc data sets lever-
ages this idea of similar user proﬁle attributes being an indicator
of their adequacy to access the data. In addition, we go one step
further by leveraging the focus user’ social graph network metrics
as input to predicting the appropriate protection of user’s data.
6. CONCLUSION
This paper explores an adaptive policy generation framework,
P riM a, as a ﬁrst step towards providing ﬂexible, adaptive and
powerful access control to SN users. P riM a access rules are gen-
erated on the basis of users privacy preferences on their proﬁle data,
the sensitivity of the data and the risk of disclosing such data to
other users.
There still exist many shortcomings to be overcome before P riM a
can be regarded to be completely sufﬁcient in protecting the user’s
information. For example, the tuning of the thresholds used for the
rule generation processes will play an essential role in real-world
settings.
7. REFERENCES
[1] A. Acquisti. Privacy in electronic commerce and the
economics of immediate gratiﬁcation. In A. Press, editor,
Proceedings of the 5th ACM Electronic Commerce
Conference, pages 21–29, 2004.
[2] A. Acquisti and J. Grossklags. Privacy and rationality in
decision making. IEEE Security and Privacy
(January/February), pages 26–33, 2005.
[3] B. Carminati and E. Ferrari. Privacy-aware collaborative
access control in web-based social networks. In DBSec,
pages 81–96, 2008.
[4] B. Carminati, E. Ferrari, and A. Perego. Private relationships
in social networks. In ICDE Workshops, pages 163–171,
2007.
[5] Facebook. http://www.facebook.com.
[6] J. A. Golbeck. Computing and applying trust in web-based
social networks. PhD thesis, College Park, MD, USA, 2005.
Chair-Hendler, James.
[7] R. Gross, A. Acquisti, and H. J. Heinz, III. Information
revelation and privacy in online social networks. In WPES
’05: Proceedings of the 2005 ACM workshop on Privacy in
the electronic society, pages 71–80, New York, NY, USA,
2005. ACM.
[8] M. Hart, R. Johnson. A. Stent. More content - Less control:
Access control in the Web 2.0. In IEEE Web 2.0 Privacy and
Security Workshop, 2007.
323