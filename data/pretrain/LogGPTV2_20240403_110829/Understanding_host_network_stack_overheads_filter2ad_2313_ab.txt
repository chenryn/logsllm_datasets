frames are received or a timer expires2. This reduces the number of
IRQs, especially for high-speed networks where incoming data rate
is high. While busy polling, the driver allocates an skb for each
frame, and makes a cross reference between the skb and the kernel
memory where the frame has been DMAed. If the NIC has written
enough data to consume all Rx descriptors, the driver allocates more
DMA memory using the page-pool and creates new descriptors.
The network subsystem then attempts to reduce the number of
skbs by merging them using Generic Receive Offload (GRO), or its
corresponding hardware offload Large Receive Offload (LRO); see
discussion in [7]. Next, TCP/IP processing is scheduled on one of the
CPU cores using the flow steering mechanism enabled in the system
(see Table 2). Importantly, with aRFS enabled, all the processing (the
2These NAPI parameters can be tuned via net.core.netdev_budget and
net.core.netdev_budget_usecs kernel parameters, which are set to 300 and 2ms
by default in our Linux distribution.
67
Mechanism
Description
Receive Packet Steering (RPS)
Receive Flow Steering (RFS)
Receive Side Steering (RSS)
accelerated RFS (aRFS)
Use the 4-tuple hash for core selection.
Find the core that the application is running on.
Hardware version of RPS supported by NICs.
Hardware version of RFS supported by NICs.
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
Table 2: Receiver-side flow steering techniques.
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
(cid:1)(cid:2)(cid:2)
IRQ handler, TCP/IP and application) is performed on the same CPU
core. Once scheduled, the TCP/IP layer processing is performed and
all in-order skbs are appended to the socket’s receive queue. Finally,
the application thread performs data copy of the payload in the
skbs in the socket receive queue to the userspace buffer. Note that
at both the sender-side and the receiver-side, data copy of packet
payloads is performed only once (when the data is transferred
between userspace and kernel space). All other operations within
the kernel are performed using metadata and pointer manipulations
on skbs, and do not require data copy.
2.2 Measurement Methodology
In this subsection, we briefly describe our testbed setup, experimen-
tal scenarios, and measurement methodology.
Testbed setup. To ensure that bottlenecks are at the network
stack, we setup a testbed with two servers directly connected
via a 100Gbps link (without any intervening switches). Both of
our servers have a 4-socket NUMA-enabled Intel Xeon Gold 6128
3.4GHz CPU with 6 cores per socket, 32KB/1MB/20MB L1/L2/L3
caches, 256GB RAM, and a 100Gbps Mellanox ConnectX-5 Ex NIC
connected to one of the sockets. Both servers run Ubuntu 16.04
with Linux kernel 5.4.43. Unless specified otherwise, we enable
DDIO, and disable hyperthreading and IOMMU in our experiments.
Experimental scenarios. We study network stack performance
using five standard traffic patterns (Fig. 2)—single flow, one-to-one,
incast, outcast, and all-to-all—using workloads that comprise long
flows, short flows, and even a mix of long and short flows. For
generating long flows, we use a standard network benchmarking
tool, iPerf [14], which transmits a flow from sender to receiver;
for generating short flows, we use netperf [22] that supports ping-
pong style RPC workloads. Both of these tools perform minimal
application-level processing, which allows us to focus on perfor-
mance bottlenecks in the network stack (rather than those arising
due to complex interactions between applications and the network
stack); many of our results may have different characteristics if
applications were to perform additional processing. We also study
the impact of in-network congestion, impact of DDIO and impact
of IOMMU. We use Linux’s default congestion control algorithm,
TCP Cubic, but also study impact of different congestion control
protocols. For each scenario, we describe the setup inline.
Performance metrics. We measure total throughput, total CPU
utilization across all cores (using sysstat [19], which includes
kernel and application processing), and throughput-per-core—ratio
of total throughput and total CPU utilization at the bottleneck
(sender or receiver). To perform CPU profiling, we use the standard
sampling-based technique to obtain a per-function breakdown of
CPU cycles [20]. We take the top functions that account for ∼95%
of the CPU utilization. By examining the kernel source code, we
classify these functions into 8 categories as described in Table 1.
(a) Single
(b) One-to-one
(c) Incast
(d) Outcast
(e) All-to-all
Figure 2: Traffic patterns used in our study. (a) Single flow from one
sender core to one receiver core. (b) One flow from each sender core to a
unique receiver core. (c) One flow from each sender core, all to a single
receiver core. (d) One flow to each receiver core all from a single sender
core. (e) One flow between every pair of sender and receiver cores.
3 LINUX NETWORK STACK OVERHEADS
We now evaluate the Linux network stack overheads for a variety of
scenarios, and present detailed insights on observed performance.
3.1 Single Flow
We start with the case of a single flow between the two servers, each
running an application on a CPU core in the NIC-local NUMA node.
We find that, unlike the Internet and early incarnations of datacenter
networks where the throughput bottlenecks were primarily in the
core of the network (since a single CPU was sufficient to saturate
the access link bandwidth), high-bandwidth networks introduce
new host bottlenecks even for the simple case of a single flow.
Before diving deeper, we make a note on our experimental con-
figuration for the single flow case. When aRFS is disabled, obtaining
stable and reproducible measurements is difficult since the default
RSS mechanism uses hash of the 4-tuple to determine the core for
IRQ processing (§2.1). Since the 4-tuple can change across runs,
the core that performs IRQ processing could be: (1) the application
core; (2) a core on the same NUMA node; or, (3) a core on a differ-
ent NUMA node. The performance in each of these three cases is
different, resulting in non-determinism. To ensure deterministic
measurements, when aRFS is disabled, we model the worst-case sce-
nario (case 3): we explicitly map the IRQs to a core on a NUMA node
different from the application core. For a more detailed analysis of
other possible IRQ mapping scenarios, see [7].
A single core is no longer sufficient. For 10 − 40Gbps access
link bandwidths, a single thread was able to saturate the network
bandwidth. However, such is no longer the case for high-bandwidth
networks: as shown in Fig. 3(a), even with all optimization enabled,
Linux network stack achieves throughput-per-core of ∼42Gbps3.
Both Jumbo frames4 and TSO/GRO reduce the per-byte processing
overhead as they allow each skb to bring larger payloads (up to
9000B and 64KB respectively). Jumbo frames are useful even when
GRO is enabled, because the number of skbs to merge is reduced
with a larger MTU size, thus reducing the processing overhead for
packet aggregation in software. aRFS, along with DCA, generally
3We observe a maximum throughput-per-core of upto 55Gbps, either by tuning NIC
Rx descriptors and TCP Rx buffer size carefully (See Fig. 3(e)), or using LRO instead
of GRO (See [7]). However, such parameter tuning is very sensitive to the hardware
setup, and so we leave them to their default values for all other experiments. Moreover,
the current implementation of LRO causes problems in some scenarios as it might
discard important header data, and so is often disabled in the real world [10]. Thus we
use GRO as the receive offload mechanism for the rest of our experiments.
4Using larger MTU size (9000 bytes) as opposed to the normal (1500 bytes).
68
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
(cid:25)(cid:16) (cid:10)(cid:11)(cid:12)(cid:13)
(cid:17)(cid:18)(cid:10)(cid:15)(cid:19)(cid:20)(cid:10)
(cid:21)(cid:22)(cid:23)(cid:24)(cid:16)
(cid:26)(cid:20)(cid:27)(cid:18)
(cid:17)(cid:16)(cid:12)(cid:26)(cid:9) (cid:17)(cid:28)(cid:11)(cid:12)
(cid:8)(cid:9)(cid:9) (cid:10)(cid:11)(cid:12)(cid:13)
(cid:14)(cid:15)(cid:16) (cid:17)(cid:18)(cid:10)(cid:15)(cid:19)(cid:20)(cid:10)
(cid:14)(cid:15)(cid:16) (cid:21)(cid:22)(cid:23)(cid:24)(cid:16)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:9)
(cid:17)
(cid:5)(cid:1)(cid:1)
(cid:4)(cid:2)(cid:1)
(cid:4)(cid:1)(cid:1)
(cid:3)(cid:2)(cid:1)
(cid:3)(cid:1)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:14)(cid:26)(cid:27)(cid:28)(cid:26)(cid:29) (cid:30)(cid:31)(cid:32) (cid:32)(cid:10)(cid:33)(cid:34)
(cid:17)(cid:26)(cid:35)(cid:26)(cid:33)(cid:36)(cid:26)(cid:29) (cid:30)(cid:31)(cid:32) (cid:32)(cid:10)(cid:33)(cid:34)
(cid:13)(cid:7)(cid:10)(cid:22)(cid:34) (cid:13)(cid:37)(cid:9)(cid:10)
(cid:6)(cid:7) (cid:8)(cid:9)(cid:10)(cid:11) (cid:12)(cid:13)(cid:14)(cid:8)(cid:15)(cid:16)(cid:17)(cid:8) (cid:12)(cid:18)(cid:19)(cid:20)(cid:21)(cid:7)
(cid:12)(cid:22)(cid:17)(cid:23)(cid:14)
(a) Throughput-per-core (Gbps)
(b) CPU utilization (%)
(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:2)
(cid:3)(cid:9)(cid:10)(cid:11)(cid:12)(cid:1)
(cid:3)(cid:13)(cid:8)(cid:14)(cid:5)
(cid:14)
(cid:12) (cid:19)(cid:17) (cid:18)
(cid:16) (cid:13)
(cid:16)(cid:20) (cid:9)(cid:13) (cid:14)
(cid:2)
(cid:10)
(cid:13) (cid:11)
(cid:15)
(cid:9)(cid:1)
(cid:8)(cid:1)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:17)(cid:18)(cid:19)(cid:20)(cid:12)(cid:21)(cid:18)(cid:22)(cid:12)(cid:14)
(cid:4)(cid:3)(cid:1)(cid:1)
(cid:7)(cid:5)(cid:1)(cid:1)
(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)
(cid:15)(cid:2)(cid:16)(cid:3)(cid:3)(cid:4)(cid:5)
(cid:23)(cid:11)(cid:24)(cid:18)(cid:9) (cid:25)(cid:26)(cid:27)(cid:27) (cid:28)(cid:11)(cid:14)(cid:9)
(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)
(cid:6)(cid:7)(cid:3)(cid:3)(cid:4)(cid:5)
(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)
(cid:15)(cid:2)(cid:16)(cid:3)(cid:3)(cid:4)(cid:5)
(cid:2)(cid:3)(cid:9)
(cid:3)(cid:6)(cid:7)
(cid:6)(cid:2)(cid:3)
(cid:2)(cid:1)(cid:3)(cid:5) (cid:3)(cid:1)(cid:5)(cid:9) (cid:5)(cid:1)(cid:10)(cid:7) (cid:9)(cid:2)(cid:10)(cid:3)
(cid:14)(cid:15) (cid:16)(cid:13)(cid:17)(cid:14)
(cid:25)(cid:1)
(cid:2)(cid:1)
(cid:24)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:1)
(cid:2)(cid:7)(cid:1)
(cid:2)(cid:5)(cid:1)
(cid:2)(cid:3)(cid:1)
(cid:2)(cid:1)(cid:1)
(cid:9)(cid:1)
(cid:7)(cid:1)
(cid:5)(cid:1)
(cid:3)(cid:1)
(cid:1)
(cid:22)
(cid:10)
(cid:7)
(cid:13)
(cid:11)
(cid:13)(cid:11)
(cid:12)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
)
s
u
(
p
p
A
o
t
I
P
A
N
m
o
r
f
y
c
n
e
t
a
L
 3000
 2500
 2000
 1500
 1000
 500
 0
(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:2)
(cid:3)(cid:9)(cid:10)(cid:11)(cid:12)(cid:1)
(cid:3)(cid:13)(cid:8)(cid:14)(cid:5)
(cid:14)
(cid:12) (cid:19)(cid:17) (cid:18)
(cid:16) (cid:13)
(cid:16)(cid:20) (cid:9)(cid:13) (cid:14)
(cid:2)
(cid:10)
(cid:13) (cid:11)
(cid:15)
(c) Sender CPU breakdown
Avg. Latency
Tail (99p) Latency
100
200
400
800
1600
3200
6400 12800
TCP Rx buﬀer size(KB)
(d) Receiver CPU breakdown
(e) Cache miss rate (%)
(f) Latency from NAPI to start of data copy
Figure 3: Linux network stack performance for the case of a single flow. (a) Each column shows throughput-per-core achieved for different combinations
of optimizations. Within each column, optimizations are enabled incrementally, with each colored bar showing the incremental impact of enabling the
corresponding optimization. (b) Sender and Receiver total CPU utilization as all optimizations are enabled incrementally. Independent of the optimizations
enabled, receiver-side CPU is the bottleneck. (c, d) With all optimizations enabled, data copy is the dominant consumer of CPU cycles. (e) Increase in NIC ring
buffer size and increase in TCP Rx buffer size result in increased cache miss rates and reduced throughput. (f) Network stack processing latency from NAPI to
start of data copy increases rapidly beyond certain TCP Rx buffer sizes. See §3.1 for description.
improves throughput by enabling applications on the NIC-local
NUMA node cores to perform data copy directly from L3 cache.
Receiver-side CPU is the bottleneck. Fig. 3(b) shows the overall
CPU utilization at sender and receiver sides. Independent of the
optimizations enabled, receiver-side CPU is the bottleneck. There
are two dominant overheads that create the gap between sender and
receiver CPU utilization: (1) data copy and (2) skb allocation. First,
when aRFS is disabled, frames are DMAed to remote NUMA mem-
ory at the receiver; thus, data copy is performed across different
NUMA nodes, increasing per-byte data copy overhead. This is not
an issue on the sender-side since the local L3 cache is warm with
the application send buffer data. Enabling aRFS alleviates this issue
reducing receiver-side CPU utilization by as much as 2× (right-most
bar in Fig. 3(b)) compared to the case when no optimizations are
enabled; however, CPU utilization at the receiver is still higher than
the sender. Second, when TSO is enabled, the sender is able to allo-
cate large-sized skbs. The receiver, however, allocates MTU-sized