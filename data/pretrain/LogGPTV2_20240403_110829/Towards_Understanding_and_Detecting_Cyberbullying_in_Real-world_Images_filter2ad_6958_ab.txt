users), whereas a relatively smaller number of users reported
themselves as male (54 users). The rest of the users wished
not to report their age or gender.
B. Cyberbullying Keywords Extraction
To extract keywords of cyberbullying in images from the
cyberbullying stories, we used the following method. We ﬁrst
removed all identiﬁers from the cyberbullying stories informa-
tion. Next, we used the Python NLTK library [23] to remove
stop words [45] from all stories. At the end of this process,
we collected 2,648 keywords. Then, we used the sentiment
analyzer of the Python NLTK library to remove neutral and
positive words, followed by manual veriﬁcation of the words,
which left us with 378 words (we used a polarity threshold
of -0.55 4). We used these words as the ﬁnal keywords list
to collect potential images of cyberbullying content for our
dataset. Table I shows some cyberbullying story samples and
the keywords extracted with our methodology.
4Polarity threshold is deﬁned in the interval -1 to +1. More negative words
have a polarity value closer to -1.
3
Stories
The oldest boy’s dad is crazy and
has been sending text containing
verbal harm messages and even a
text holding a gun and a message
to the boyfriend and just wanted to
know what we should do.
I have been threatened that some-
one was going to kill me and told
me to shut the f*ck up here is a
picture.
How does it feel being the fat ugly
outcast of all your pretty skinny
friends why do you take a bazillion
pictures of yourself.
I am keep getting name called such
as f*g, douche bag, small d*ck.
Extracted Keywords
holding, gun,
crazy,
harm
f*ck, kill, threatened
fat, ugly
f*g, douche, d*ck
TABLE I: Samples of cyberbullying stories and the extracted
keywords.
Fig. 2: Image samples that did not have any Regions of
Interest (ROIs).
C. Data Collection and Annotation
The models of cyberbullying detection in images should be
capable of differentiating between images with cyberbullying
content from other benign images. In addition, they should
also distinguish between harmless images that do not intend
to cause cyberbullying, so that false alarms are reduced. To col-
lect a diverse dataset of images that captures important patterns
of cyberbullying in images, we used multiple web sources,
including web search engines (Google, Bing, and Baidu) and
publicly available social media images from multiple online
social media websites (Instagram, Flickr, and Facebook). We
collected images using keywords and phrases compiled from
our ﬁndings in Section III-B. We ﬁnally collected 117, 112
images using our data collection methodology. Next, we used
an object localization tool called YOLO [70] to exclude images
that do not have any regions of interest (ROIs). These are
images that typically do not have any content and hence, do
not convey any meaning. Some samples of images that were
excluded in this step are depicted in Figure 2. After this step,
we were left with 19, 300 images for annotations.
1) Image Annotation: We used MTurk to obtain annota-
tions for the collected images. Our objective was to annotate
whether an image contains cyberbullying content or does not
contain any cyberbullying content. Therefore, we referred to
the deﬁnition of cyberbullying from [58], [68] as guidelines
for annotation. Speciﬁcally, we focused on cyberbullying in
images as “an act of online aggression carried out through
images” for the participants of our study (the interface of
4
our image annotation task can be found in Appendix B).
We displayed a warning to participants about the nature of
the task in both the task title and description according to
MTurk guidelines. We placed a restriction that only allows
participants with an approval rating of 90% or higher and
1000 approved HITs to participate in our annotation task.
We offered a $0.05 reward for each task submission and
recorded an average task completion time of 18 seconds per
task. We allowed each image to be annotated by three distinct
participants and chose the majority voted category as the
ﬁnal annotation. Finally, in our dataset, 4,719 images were
annotated as cyberbullying images and 14,581 images were
annotated as non-cyberbullying images.
We computed the inter-rater agreement [47] using the
Randolph’s κ-measure [69], a statistical measure of agreement
between individuals for qualitative ratings. Note that, κ < 0
corresponds to no agreement, κ = 0 to agreement by chance,
and 0 < κ ≤ 1 to agreement beyond chance. We measured κ
on our cyberbullying images dataset, and obtained κ = 0.80.
IV. MOTIVATION AND OBSERVATION
To illustrate our motivation, we ﬁrst conducted a study
into the detection capability of several popular offensive image
detectors, including Google Cloud Vision API (Google API),
Yahoo Open NSFW, Clarifai NSFW, DeepAI and Amazon
Rekognition, and ran these detectors against images annotated
as cyberbullying in our dataset. We chose these detectors
because they have the ability to detect certain offensive at-
tributes in images. We computed the performance of these
detectors in terms of precision and recall metrics on the
cyberbullying images as shown in Table II. From Table II,
we observed that
those state-of-the-art detectors have low
performance in detecting cyberbullying images. Among those
popular offensive image detectors, Yahoo Open NSFW (preci-
sion = 36.27%, recall = 2.82%) and Clarifai NSFW (precision
= 42.94%, recall = 10.67%) offer overall lowest performance.
DeepAI (precision = 69.43%, recall = 15.92%) and Amazon
Rekognition (precision = 77.44%, recall = 23.55%) offer only
a small improvement over the previous two detectors, although
they consider a higher number of attributes. Among the popular
detectors, Google API (precision = 35.65%, recall = 39.40%)
achieves the best performance, although this detector also
misses a large number of cyberbullying samples (60.59%). A
more startling observation was that 39.32% of the cyberbully-
ing samples could circumvent all ﬁve popular offensive image
detectors.
Detector
Google API
Yahoo Open NSFW
Clarifai NSFW
DeepAI
Amazon Rekognition
Precision
35.65%
36.27%
42.94%
69.43%
77.44%
Recall
39.40%
2.82%
10.67%
15.92%
23.55%
TABLE II: Precision and recall of popular offensive image
detectors.
After an examination of cyberbullying images annotated
by users in our dataset, we found that most of such im-
ages are context-aware. Figure 3 depicts two images without
cyberbullying context (annotated as non-bullying images by
Detector
Google Cloud Vi-
sion API
Categories of Of-
fensive Content
detection,
Object
detection,
face
attributes,
image
web
entities,
content moderation
Yahoo Open NSFW NSFW detection
Clarifai NSFW
NSFW detection,
content moderation
concepts
DeepAI
Moderation API
Content
Content moderation
Amazon
Rekognition
and scene
face
Object
detection,
recognition,
emotion detection,
unsafe
image
detection
Limitations
No offensive image
detection capability
limited
of
(explicit,
gore
Limited to only nu-
dity detection
Only
types
concepts
suggestive,
and drug)
Only limited to a
few objects
(guns
confederate ﬂag)
Limited categories
of unsafe detection
(nudity
and
violence)
TABLE IV: Capabilities of existing detectors and their limita-
tions.
by existing systems as an offensive content category. Secondly,
since the factors of cyberbullying in images are unknown, the
existing detectors are not capable of detecting them. Thus, we
are motivated to shed light on identifying the visual factors
of cyberbullying so that they can be automatically detected in
images.
V. OUR APPROACH
We analyse the cyberbullying images in our dataset in
three steps: (i) understand and identify the factors related to
cyberbullying in images (Section V-B); (ii) extract those factors
from images (Section V-C); and (iii) examine the usage of
those factors in classiﬁer models (Section V-D).
A. Approach Overview
The main components involved in our approach are de-
picted in Figure 4. We ﬁrst collect a large dataset of cyberbul-
lying images to study this phenomenon (Figure 4, Step 1 “Data
Collection and Annotation”). Next, we analyze the collected
data to identify factors in the way participants consider cyber-
bullying in images (Figure 4, Step 2, “Factor Identiﬁcation and
Extraction”, “Factors”). In this step, we identify ﬁve factors
of cyberbullying in images in our dataset: body-pose, facial
emotion, gesture, object and social factors. We then focus on
two processes to study and address cyberbullying in images:
“Factors Identiﬁcation and Extraction”, “Attributes” (Fig-
ure 4, Step 2) and “Classiﬁer Model Measurement” (Figure 4,
Step 3). In Factor Extraction, our primary goal is to extract
the attributes of those factors of cyberbullying in images. We
use several off-the-shelf tools and techniques to extract these
visual factors. In Classiﬁer Model Measurement, we then use
several deep learning-based classiﬁers to demonstrate that the
identiﬁed factors can be used to effectively detect cyberbully-
ing in images. To understand the importance of these factors
and to study their effectiveness in detecting cyberbullying in
images, we train four classiﬁer models: baseline, factors-only,
(a) Without cyberbullying context.
(b) With cyberbullying context.
Fig. 3: Image context in cyberbullying images.
Clarifai Deep
Image #
Figure 3a (i)
Figure 3a (ii)
Figure 3b (i)
Figure 3b (ii)
Google
API
0.2
0.2
0.2
0.2
Yahoo
NSFW
0.17
0.005
0.008
0.004
0
0.05
0.01
0
Amazon
0
0.98
0
0.97
AI
0.17
0.003
0.008
0
TABLE III: Detection scores of existing detectors on image
samples in Figure 3.
participants) and two other images with cyberbullying context
(annotated as bullying images by participants), respectively,
from our dataset. The images in Figure 3a only show a possible
factor (a demeaning hand gesture or a gun), but without any
contextual information. In contrast, Figure 3b shows images
that portray these factors with contextual information, such as
a person deliberately showing the hand gesture in Figure 3b (i)
to the viewer, or the person in Figure 3b (ii) pointing the gun
at the viewer. Table III depicts the scores of each popular of-
fensive image detectors on those image samples. We observed
that the Google API scores all the image samples equally, and
rates them as “unlikely” to be unsafe. Yahoo NSFW, Clarifai
and DeepAI seem to have very small scores for all image
samples, and therefore are unable to differentiate between non-
cyberbullying and cyberbullying content. Amazon Rekognition
seems to only detect guns in Figure 3a (ii) (score = 0.98) and
Figure 3b (ii) (score = 0.97), and naively ﬂags down all such
images. Thus, we note that the existing detectors cannot detect
cyberbullying in images effectively.
We further study the capabilities and limitations of the
ﬁve state-of-the-art offensive image detectors, as depicted in
Table IV. From Table IV, we can ﬁrst observe that none of
state-of-the-art detectors consider cyberbullying in images as a
category of offensive content. Thus, our ﬁrst motivation is that
this important category of offensive content should be included
5
(i)(ii)(i)(ii)Fig. 4: Approach overview.
ﬁne-tuned pre-trained, and multimodal models. During the
evaluation of a new photo, we extract the factors and predict a
score of cyberbullying in images using those classiﬁer models.
We discuss our methodology in more details in the following
sections.
In our work, the context of cyberbullying refers to the
story that an image is conveying, where the intent is to bully
receivers/viewers of the image. For example, a photo with a
person at a gun shop looking at various guns on display has a