### Online Algorithm for Mixture Distributions

We present an online algorithm for mixtures of simple component distributions, starting with a basic version before introducing specific variants for different subclasses of Phase-Type Distributions (PHDs). The approach from [7] is easily applicable to distributions where each component has only a single parameter. We first describe its straightforward application to hyperexponential distributions. For more complex distributions like hyper-Erlang, Acyclic Phase-Type Distributions (APHDs), or general PHDs, the algorithm's application is more intricate and will be discussed later. Additionally, the approach in [7] assumes homogeneous data, and after a certain point, further observations have little to no effect on the parameters. Therefore, we propose a new adaptive variant of the algorithm in Section V, which can handle changing input streams.

#### Notation
Consider an infinite sequence of observations \( T \) and assume that data points \( t_h \) (for \( h = 1, 2, \ldots \)) arrive sequentially. Define \( T_{:h} \) as the sub-sequence of the first \( h \) observations, \( T_{h:} \) as the sub-sequence starting with observation \( h \), and \( T_{h:l} \) for \( h \leq l \) as the finite sub-sequence starting with \( t_h \) and ending with \( t_l \). We typically use \( t_k \) for the current observation. The online EM-algorithm can be interpreted as a stochastic approximation approach [33] in this setting.

### A. Online-EM Algorithm

In the EM-algorithm, we have observed data from \( T \), unobserved data from \( X \), and parameters from \( \Theta \). Let \( f_{\theta}(x, t) \) be the detailed density and \( f_{\theta}(t) = \int f_{\theta}(x, t) \, dx \). Then \( E_{X|t,\theta}[\cdot] \) is the conditional expectation given \( t \) and \( \theta \). An offline EM-algorithm defines the Q-function [5], [7] as:

\[
Q_{\theta}(T_{:k}) = \frac{1}{k} \sum_{h=1}^k E_{X_h|t_h,\theta} [\log f_{\theta}(X_h, t_h)]
\]

In the online EM-algorithm of [7], this is replaced by:

\[
\hat{Q}_k(\theta) = \hat{Q}_{k-1}(\theta) + \gamma_k \left( E_{X_k|t_k,\theta(k-1)} [\log f_{\theta(k)}(X_k, t_k)] - \hat{Q}_{k-1}(\theta) \right)
\]

for \( k > 0 \) and \( \hat{Q}_0(\theta) \) is some initial approximation. Here, \( \gamma_k \) is a decreasing sequence of non-negative values (see [7] and Section V). The parameters \( \theta(k) \) are chosen to maximize \( \hat{Q}_k(\theta) \) over the set \( \Theta \). The approach is applicable whenever \( \hat{Q}_k(\theta) \) can be computed efficiently and maximized with respect to the current parameter \( \theta(k-1) \).

Mixture distributions are often used. Below, we present concrete realizations of the algorithms for different variants of PHDs.

### B. Application to Hyperexponential Distributions

The simplest class of PHDs for online EM fitting are hyperexponential distributions, where the application is straightforward. Hyperexponential distributions are mixtures of exponential components with parameters \( \theta = (\pi_1, \ldots, \pi_n, \lambda_1, \ldots, \lambda_n) \), where \( \pi_i \) are the frequencies of the exponential components with rates \( \lambda_i \). The density function is:

\[
f_{\theta}^{(hexp)}(t) = \sum_{i=1}^n \pi_i \lambda_i e^{-\lambda_i t} \quad \text{for } t \geq 0
\]

For observation \( t_k \) with \( \theta(k-1) \), the following values can be computed in the E-step:

\[
P_{\theta(k-1)}(X_k = i | t_k) = \frac{\pi_i \lambda_i e^{-\lambda_i t_k}}{\sum_{j=1}^n \pi_j \lambda_j e^{-\lambda_j t_k}}
\]

\[
E_{\theta(k-1)}[B_i^{(k)}] = (1 - \gamma_k) E_{\theta(k-1)}[B_i^{(k-1)}] + \gamma_k P_{\theta(k-1)}(X_k = i | t_k)
\]

\[
E_{\theta(k-1)}[S_i^{(k)}] = (1 - \gamma_k) E_{\theta(k-1)}[S_i^{(k-1)}] + \gamma_k t_k P_{\theta(k-1)}(X_k = i | t_k)
\]

The E-step can be evaluated if started with appropriate initial estimates for \( E[B_i^{(0)}] \). The new parameter vector \( \theta(k) \) is then computed in the M-step as:

\[
\pi_i^{(k)} = \frac{E[B_i^{(k)}]}{\sum_{j=1}^n E[B_j^{(k)}]}, \quad \lambda_i^{(k)} = \frac{E[S_i^{(k)}}{E[B_i^{(k)}]}
\]

### C. Application to Hyper-Erlang Distributions

Hyper-Erlang distributions generalize hyperexponential distributions, where the components are Erlang distributions instead of exponential. The parameters are \( \theta = (\pi_1, \ldots, \pi_m, \lambda_1, \ldots, \lambda_m, n_1, \ldots, n_m) \), where \( \pi_i \) are the frequencies of the Erlang components with rates \( \lambda_i \) and \( n_i \) phases. The direct application of the EM-algorithm is challenging due to the interplay between \( \lambda_i \) and \( n_i \). The algorithm in [13] presents an offline EM-algorithm for hyper-Erlang distributions by estimating only \( \lambda_i \) and assuming either given \( n_i \) values or trying all combinations of \( n_i \) values for a given overall number of states \( n = \sum_{i=1}^m n_i \).

The hyper-Erlang distribution has the density function:

\[
f_{\theta}^{(herd,(n_1,...,n_m))}(t) = \sum_{i=1}^m \pi_i g_{\lambda_i}^{(erl,n_i)}(t)
\]

where the Erlang density with rate \( \lambda_i \) and \( n_i \) phases is:

\[
g_{\lambda_i}^{(erl,n_i)}(t) = \frac{\lambda_i^{n_i} t^{n_i-1} e^{-\lambda_i t}}{(n_i-1)!}
\]

To compute \( \pi_i^{(k)} \) and \( \lambda_i^{(k)} \), (9) can be applied after substituting \( g_{\lambda_i}(t) \) by \( g_{\lambda_i}^{(erl,n_i)}(t) \):

\[
\pi_i^{(k)} = \frac{E[B_i^{(k)}]}{\sum_{j=1}^m E[B_j^{(k)}]}, \quad \lambda_i^{(k)} = \frac{E[S_i^{(k)}}{E[B_i^{(k)}]}
\]

### D. Application to Acyclic Phase-Type Distributions

Hyperexponential and hyper-Erlang distributions are specific cases of APHDs. Each APHD can be transformed into the canonical form [14], [17]. In canonical form, the behavior of an APHD is as follows: with probability \( \pi_1 \), the PHD starts in state 1 and moves through all states 2, ..., n, where absorption occurs. The time spent in this state follows a hypoexponential (or generalized Erlang) distribution with \( n \) states and rates \( \lambda_1, \ldots, \lambda_n \). With probability \( \pi_2 \), we have a generalized Erlang distribution with \( n-1 \) states and rates \( \lambda_2, \ldots, \lambda_n \), and so on. Thus, we have a mixture of \( m = n \) generalized Erlang distributions with parameter vector \( \theta = (\pi_1, \ldots, \pi_n, \lambda_1, \ldots, \lambda_n) \).

The density function is:

\[
f_{\theta}^{(aphd)}(t) = \sum_{i=1}^m \pi_i g_{\lambda_i, \ldots, \lambda_m}^{(gerl)}(t)
\]

and the probability densities of the generalized Erlang distributions:

\[
g_{\lambda_i, \ldots, \lambda_m}^{(gerl)}(t) = \sum_{j=i}^m \lambda_j e^{-\lambda_j t} \prod_{h=i, h \neq j} \frac{\lambda_h}{\lambda_h - \lambda_j}
\]

For the computation of the parameters, (9) with the densities \( g_{\lambda_i, \ldots, \lambda_m}^{(gerl)}(t) \) can be applied. The new estimates \( \lambda_i^{(k)} \) are computed iteratively until the APHD is in canonical form.

### E. General PHDs

To realize an online EM-algorithm for general PHDs, we need to formulate online versions of (4) and (5). The E-step becomes:

\[
E_{\theta(k)}[B_i^{(k)}] = (1 - \gamma_k) E_{\theta(k-1)}[B_i^{(k-1)}] + \gamma_k P_{\theta(k-1)}(X_k = i | t_k)
\]

\[
E_{\theta(k)}[Y_i^{(k)}] = (1 - \gamma_k) E_{\theta(k-1)}[Y_i^{(k-1)}] + \gamma_k t_k P_{\theta(k-1)}(X_k = i | t_k)
\]

The M-step updates the parameters accordingly.