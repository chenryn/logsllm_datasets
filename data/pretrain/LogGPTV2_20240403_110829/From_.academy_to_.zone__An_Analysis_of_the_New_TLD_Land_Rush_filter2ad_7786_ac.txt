istrant, we classify the technical data each domain returns when
queried by our DNS or HTTP infrastructure. We perform this clas-
siﬁcation with features of both crawls, including DNS CNAME
records, Web headers, Web contents, and the NS records in the
zone ﬁles.
Domains with invalid DNS or HTTP errors are straightforward
to identify, but in many instances, we need to classify the domains
based on the textual content they return to HTTP queries. We use a
combination of automated machine learning techniques and manual
inspection of Web pages hosted at these domains.
We assign each domain to one of the following seven categories:
No DNS domains do not successfully resolve DNS queries.
HTTP Error domains have valid DNS, but do not return an
HTTP 200 when queried.
Parked domains are owned by an ad network or are for sale
by their owners and typically return Web pages domi-
nated by ads.
Unused domains return HTTP content that is not consumer-
ready, including empty pages, default Web server tem-
plates, or PHP errors.
Free domains include domains given out as part of a pro-
motion that still have the original template, as well as
domains with registry-owned Web templates.
Defensive Redirect domains redirect through one of several
technical mechanisms to a different domain name.
Content domains host valid Web content for users to visit.
We start by presenting high-level content categorizations, includ-
ing domains in the older TLDs as a reference point. Then Sec-
tion 5.2 provides more detail about our clustering methodology,
and Section 5.3 describes the seven categories in more detail.
5.1 Content Summary
To place the new TLD results in context, we present domain clas-
siﬁcations for three data sets. The ﬁrst includes all domains in the
new TLDs as of February 3, 2015. The second includes 3 mil-
Content Category
Results
No DNS
HTTP Error
Parked
Unused
Free
Defensive Redirect
Content
567,390
362,727
1,161,892
504,928
432,323
236,380
372,569
15.6%
10.0%
31.9%
13.9%
11.9%
6.5%
10.2%
Total 3,638,209 100.0%
Table 3: Overall content classiﬁcations for all domains in the zone
ﬁle for the new public TLDs.
lion domains from the old TLDs deﬁned in Section 3.1 chosen uni-
formly at random. The third includes all domains in the same set of
old TLDs that were newly registered during December 2014. (De-
lays in our com processing pipeline prevented us from using a more
recent data set.) Figure 2 summarizes all three data sets. This paper
focuses on the new TLDs, so we focus on those domains. Table 3
shows exact values for the 290 public English TLDs described in
Section 3.3, minus quebec, scot, and gal, the TLDs for which we
did not have zone ﬁle access at the time.
For most categories the classiﬁcation breakdown is comparable
among the three data sets: erroneous domains (No DNS and HTTP
Error) account for about a quarter of all domains, another quarter
utilizes domain parking, and roughly 20% of domains are either
unused or redirect elsewhere. The old and new TLDs differ greatly
in content and promotional domains: the new TLDs show a dearth
of content, but make up for it with a high volume of free domains,
which domain owners do not actively use yet.
Figure 3 shows our content classiﬁcation for the 20 largest TLDs
that allow public registrations. Most TLDs show a typical split
between the major content categories, but other TLDs show very
different registration types, especially those with free domains.
5.2 Content Clustering
Our goal is to cluster Web pages hosted at domains into one of
the content categories. Two key challenges to classifying content
are the sheer size of the data (millions of domains), and the lack of
labeled data for training a classiﬁer. With so many unlabeled Web
pages, we must learn from scratch to classify the domains.
Our ﬁrst step is to cluster Web pages with highly similar content.
This procedure groups together duplicate and near-duplicate Web
pages, which commonly arise when HTML is automatically gen-
erated using a ﬁxed template. Prevalent examples include parked
pages, and default placeholder pages served by a registrar before
the registrant publishes any content.
To map Web pages to inputs for a clustering algorithm, we fol-
low a conventional “bag-of-words” approach which extracts HTML
features from the Web pages.
In particular, we compose a dic-
tionary of all terms that appear in the HTML source code, and
for each Web page, we count the number of times that each term
appears.
In this way, each Web page is represented as a sparse,
high-dimensional vector of feature counts. We implemented a cus-
tom bag-of-words feature extractor which forms tag-attribute-value
triplets from HTML tags, as described in [7].
For reasons of computability and conciseness of results, we be-
gin by clustering roughly one tenth of the crawled Web pages. We
used the k-means clustering algorithm with k = 400 to organize
these Web pages into groups of high similarity (based on the Eu-
clidean distance between their feature vectors). We set k to be in-
Figure 3: Domain classiﬁcations in individual TLDs for the 20
most common. We have sorted TLDs by fraction of “No DNS” to
better highlight the category breakdowns of successful content.
tentionally large because we wished to discover especially cohesive
clusters of replicated Web pages.
Next we manually inspected the resulting clusters using a custom
visualization tool. The tool displays screenshots of how the Web
pages rendered in our crawler and provides a link to the HTML
source next to each screenshot. To facilitate efﬁcient manual re-
view, the tool presents a condensed view of the clusters by show-
ing only a sample of Web pages in each one. Speciﬁcally, it sorts
the Web pages in each cluster by their distance to the cluster cen-
troid, then displays the top and bottom-ranked pages as well as a
random sample of pages in between. If all Web pages in this sam-
ple are visually nearly identical, we can conclude with conﬁdence
that the entirety of Web pages in the cluster have been appropri-
ately grouped. Furthermore, we can classify Web pages in these
perfectly homogenous clusters all together.
By examining the clusters, we placed domains into three broad
categories according to their content: parked, content-free, and
meaningful content. Our clustering approach was particularly ef-
fective at identifying large numbers of parked domains and content-
free (or unused) domains that host a default registration page. The
class of Web pages with meaningful content exhibits the most va-
riety: Web content is highly diverse and unlikely to have the same
degree of replication as the other two classes. Thus at this stage,
we focused only on bulk labeling of clusters that clearly contained
parked or content-free Web pages. If it was not visually obvious
how to label a cluster in bulk, then its pages remained unclassiﬁed
at this point. (In practice, though, we found that Web pages with
content often were grouped together in clusters with wide diame-
ters.)
After this phase of clustering, manual inspection, and labeling,
we then aimed to classify domains that were not included in the
initial subset. Now equipped with a large number of labeled exam-
ples, we used nearest neighbor classiﬁcation to discover many more
candidate Web pages which are likely parked or content-free. First,
we extracted HTML features from the remaining Web pages, then
mapped the pages into the same feature space as the original subset.
Then for each unlabeled Web page, we found its nearest neighbor
by Euclidean distance in the labeled set and, if the distance was less
than a strict threshold, we marked the page as a candidate for its
neighbor’s class. This thresholding minimizes false positives. This
step continues to focus only on parked and content-free pages; no
content pages were classiﬁed in this way. We modiﬁed our visual-
ization tool to display candidates next to their nearest neighbor; if
the Web pages were visually nearly the same, then we were conﬁ-
dent in assigning the appropriate label to the candidates.
In one round of this nearest neighbor method, we were able to
label many of the remaining (non-content) Web pages in our data
set with high conﬁdence. However, since we only clustered about
one tenth of the Web pages at the outset, we likely missed differ-
ent templates that did not appear in the initial subset. Thus, we
iterated this approach to achieve greater coverage. That is, we clus-
tered the remaining unlabeled Web pages, manually inspected and
labeled homogenous clusters, and performed thresholded nearest
neighbor classiﬁcation—now with a larger set of labeled examples.
We iterated this process until there were no more obviously cohe-
sive clusters. Finally, after identifying all parked and content-free
domains, we manually inspected a random sample of the remaining
unlabeled Web pages. The results gave us conﬁdence to conclude
that the remaining Web pages contain legitimate content.
5.3 Content Categories
We use this content clustering methodology to create a cluster
label for each domain. Then, we took any page metadata (e.g., DNS
errors, HTTP status code, the redirect chain, etc.) and combined
these features together to make a ﬁnal classiﬁcation.
The rest of this subsection describes how we combined those fea-
tures to determine a ﬁnal content category. For domains that might
fall into multiple categories, we prioritize categories in the order
listed in Table 3. For example, for parked domains that redirect to