If both nominator and denominator are 0, then by the definition
of DP, we have 0 = Pr [F (x ) ∈ Φ] ≤ exp(ϵ ) Pr [F (x′) ∈ Φ] = 0.
Hence, no matter what ϵ we pick, ϵ-DP will never be violated.
In this case, DP-Finder skips the current triple and continues to
the next iteration, where a new triple is randomly picked. In any
1
AT,{[0,0]}(x):
def check
ρ = 7.5
ν [1] = -23.3
if x[1]+ν [1] ≥ T+ρ:
y[1] = 1
else
y[1] = 0
ν [2] = 24.3
if x[2]+ν [2] ≥ T+ρ:
y[2] = 1
else
y[2] = 0
return y[1] == 0 &&
y[2] == 0
1
AT,{[0,0]}(x):
def dcheck
ρ = 7.5
ν [1] = -23.3
B1 = (1 + e−50·(x[1]+ν [1]−T−ρ ) )−1
y[1] = B1 · 1 + (1 − B1) · 0
ν [2] = 24.3
B2 = (1 + e−50·(x[2]+ν [2]−T−ρ ) )−1
y[2] = B2 · 1 + (1 − B2) · 0
2 ·
return e−502·(y[1]−0)
e−502·(y[2]−0)
2
1
Figure 9: check
AT,{[0,0]} (Fig. 4) and its corresponding differ-
entiable program.
other case, that is, when the denominator is not 0, DP-Finder runs
optimization.
Sources of Imprecision. The surrogate optimization problem
induces two sources of imprecision, which may prevent us from
reaching an optimal solution for the original optimization in Eq. (3).
The sources of imprecision are: (i) the maximum of ˆϵd (x, x′, Φ) may
be different from ϵ (x, x′, Φ), and (ii) the optimizer may overfit to
the random choices fixed in the dchecki
F,Φ (x ) programs. In Sec. 6,
we show empirically that the values for ˆϵd (x, x′, Φ) we find for the
transformed programs are close to the true values of ϵ (x, x′, Φ).
Thus, we view this transformation as a heuristic that allows us to
apply off-the-shelf optimization techniques to a surrogate prob-
lem. To verify the obtained solution, at the end of the execution,
DP-Finder uses a symbolic solver (PSI [16]), or may estimate the
privacy violation with ˆϵ (x, x′, Φ) if the symbolic solver times out.
6 EVALUATION
We now present a detailed evaluation of our approach.
6.1 Implementation
We implemented a prototype of DP-Finder in Python, using the
Sequential Least Squares Programming optimizer (SLSQP) from
TensorFlow [1] for the optimization task. Our prototype supports
algorithms from Rn to Rn or to Dn, for a finite set D. Given an al-
gorithm4, DP-Finder randomly picks a triple and draws n′ = 2000
samples. Then, DP-Finder doubles n′ until the confidence inter-
val’s diameter drops below 4 · 10−3 (which implies that the error
∆ϵ is at most 2 · 10−3). Then, it synthesizes a new counterexample
(x, x′, Φ) by maximizing the violation ˆϵd (x, x′, Φ), using SLSQP,
while satisfying two constraints: (i) x and x′ are neighbors, and
towards counterexamples whose probability is easier to estimate. If
(ii)(cid:68)Pr [F (x ) ∈ Φ] ≥ 10−2. The second constraint directs the search
4Our prototype currently does not support an automated synthesis of dcheckF , Φ ;
instead it assumes to be given dcheckF , Φ as input (in which case the sampling effort is
estimated directly on ˆϵ d (x, x′, Φ)). We note that implementing this is not a technical
challenge, and simply requires to parse the algorithm and apply our rules.
the optimization returns an invalid triple (this can happen, e.g., if
(cid:68)Pr [F (x ) ∈ Φ] = 0, in which case SLSQP fails to enforce constraint
(ii)), DP-Finder returns the randomly picked counterexample. Fi-
nally, DP-Finder returns the new counterexample (x, x′, Φ) and
its estimated violation ˆϵd (x, x′, Φ), and continues to the next iter-
ation. In our experiments, we set the number of iterations to 50
(i.e., DP-Finder computes 50 counterexamples for each evaluated
algorithm, and finally returns the one with the highest privacy
violation).
6.2 Evaluated Algorithms
We evaluated DP-Finder on 9 algorithms from the DP literature,
described next (full implementation is given in App. D).
Above Threshold and Variants. We evaluate DP-Finder on
AboveThreshold (denoted AT), as defined in Fig. 2, and variants
of it. The variants are algorithms 1–5 from [26], which we denote
by AT1–AT5. The variations of AboveThreshold are interesting be-
cause (i) some of them turned out not to be differentially private
and (ii) they obfuscate their input in a non-trivial fashion by adding
noise to multiple variables, making them hard to analyze. In the
experiments, we fix the size of the input array to 4.
We next describe the variants. Unlike AT, which returns all in-
dices above the threshold, AT1–AT4 only report the first c indices
above the threshold, for some meta-parameter c. In our experiments,
we set c = 1 (we could also use other values of c). Additionally,
compared to AT, AT1 uses a different scale for the noise added to the
inputs. Compared to AT1, the main difference of AT2 is resampling
the threshold noise whenever an input is above the threshold. We
note that DP-Finder suggests that this does not increase the pri-
vacy of the algorithm, a hypothesis supported by the known upper
bounds. Compared to AT1, the main difference of AT3 is that it re-
turns the (noisy) entries that are above the threshold (see Fig. 10a).
Hence, AT3 is an algorithm from Rk to Rk (while the rest are algo-
rithms to Boolean arrays, i.e., {0, 1}k). Compared to AT1, AT4 uses
different scales for both the threshold and input noise. Lastly, com-
pared to AT, AT5 does not add noise to the input. This leads to a
non-private algorithm (DP-Finder correctly detects this).
For all these algorithms, two array inputs x and x′ are neighbors
if they differ element-wise by at most one (i.e., Neigh≤1): ∀i ∈
{0, ..., k − 1}. |xi − x′
The known upper bounds on the differential privacy of these
algorithms are: AT is 0.45-DP, AT1 and AT2 are 0.1-DP, AT3 is 0.2-DP,
AT4 is 0.175-DP, and AT5 is ∞-DP. The latter means that there are
two neighboring inputs and an output set which can be returned
for one of the inputs but not for the other. In all our result graphs,
we show these upper bounds in a blue line (to put in context the
lower bound results).
i | ≤ 1.
To select Φ, in all algorithms except AT3, DP-Finder sam-
ples a single output y, uniformly at random, from all possi-
ble outputs of these algorithms, and sets Φ := {y}. For AT3,
this is not possible, because its output is continuous, and thus
Pr [F (x ) ∈ {y}] = 0, for any y. Instead, DP-Finder picks Φ to
be the box Φ := {y ∈ Rn | ∀i ∈ {0, ..., k − 1}. ai ≤ yi ≤ bi}. To sam-
ple (ai , bi ), it first picks an array of indicators Ii, such that Ii = 1
th value lies above the threshold, uniformly at ran-
indicates that the i
dom from all possibilities. Then, it sets (ai , bi ) = (−10 − 3,−10 + 3)
def AT3(x):
ρ = Lap(20)
for i = 1 to k:
ν [i] = Lap(20)
if x[i]+ν [i]≥T+ρ:
y[i] = x[i]+ν [i]
else
y[i] = -10
return y
(a) Variant of AT
def noisyMax(x):
best = 0
r = 0
for i = 1 to k:
d = x[i]+Lap(20)
if d>best or i==0:
r = i
best = d
return r
(b) NoisyMax
Figure 10: Two representative algorithms used for evalua-
tion.
if Ii = 0 (note that −10 is the value returned for entries which are
not above the threshold), and (ai , bi ) = (xi − 3, xi + 3), otherwise.
Noisy Maximum. We also evaluate on two algorithms taken
from [3]. The first is noisyMax (Fig. 10b), which is a noisy imple-
mentation of a function returning the index of the largest element
in an array. Here, the noise is drawn from a Laplace distribution.
The second algorithm is expMech, which is identical to noisyMax
but draws the noise from an exponential distribution. Both algo-
rithms are known to be 0.1-DP. Just as for AT, DP-Finder uses the
neighboring notion of Neigh≤1 and picks Φ := {y}, for y picked
uniformly at random from all possible outputs.
Sum. To illustrate a different notion of neighboring inputs, we
also evaluate on sum [12], which takes an array x, whose entries
are between −1 and 1, and returns its noisy sum. Here, two arrays
x and x′ are neighbors if x′ is x extended with an additional entry.
In this benchmark, we consider a single Φ := {x ∈ R | a ≤ x ≤ b},
where a =(cid:80)k
i =1 xi − 3 and b =(cid:80)k
i =1 xi + 3.
6.3 Evaluation Results
Our evaluation results answer the following questions:
Q1 How precise are the estimated violations ˆϵd (x, x′, Φ), com-
Q2 How efficient is DP-Finder in finding violations compared
pared to ϵ (x, x′, Φ)?
to random search?
Q3 How efficient is DP-Finder in terms of runtime?
We ran all experiments on a machine with 500GB RAM and 128
cores at 1.2GHz, running Ubuntu 16.04.3 LTS with Tensorflow 1.9.0
and Python 3.5.2.
Q1: Precision of estimated violations. To evaluate the preci-
sion of the estimated violations, we compare the estimated violation
ˆϵd (x, x′, Φ) with the actual violation ϵ (x, x′, Φ), as computed by
the exact solver PSI [16]. Fig. 11 shows the boxplots of the estimated
violation ˆϵd (x, x′, Φ) and the actual violation ϵ (x, x′, Φ), obtained
from the 50 counterexamples generated by DP-Finder for each
algorithm. The figure shows that our estimation is very precise,
expect in a few cases (e.g., for AT5). We recall that the imprecision
of ˆϵd (x, x′, Φ) is due to (i) the finite sampling of the randomized
programs checkF,Φ (x ) (presented in Sec. 4) and (ii) the transforma-
tion of the individual samples checki
F,Φ (x ) to differential functions
Figure 13: Execution times of DP-Finder for sampling and
finding counterexamples using SLSQP.
Figure 11: Boxplot comparing the estimated violation
ˆϵd (x, x′, Φ) to the true violation ϵ (x, x′, Φ). The solid blue
lines show known upper bounds of these algorithms. We
omit one counterexample with a violation of ∞ for AT5.
Figure 14: Numbers of samples used by DP-Finder, for ran-
dom triples (rand) and optimized triples (opt).
50 counterexamples produced by DP-Finder, and compared this to
the exact violation of 50 counterexamples obtained by random sam-
pling. Fig. 12 shows the results. Left bars show the privacy violations
of the randomly generated counterexamples, while right show those
produced by DP-Finder. Results indicate that DP-Finder outper-
forms the random approach. Comparing the medians of found viola-
tions for 50 runs, we get an improvement of a factor of 2 (for AT4) to
33 (for sum), depending on the algorithm. Furthermore, DP-Finder
returns counterexamples whose violations are often close to the
known upper bounds. In particular, for expMech, DP-Finder found,
in all iterations, a counterexample whose violation was very close
to the known upper bound. This demonstrates that these are tight
bounds. For the other algorithms, it is unclear which of the bounds
can be tightened (perhaps both).
Q3: Runtime. Finally, we study the runtime of DP-Finder. Fig. 13
displays DP-Finder’s execution times for the 9 algorithms. We split
the runtime into the two steps of DP-Finder: sampling and numeri-
cal optimization. In addition, we report the time spent on confirming
the estimated violations (using PSI).
The results indicate that DP-Finder spends most of its time on
the optimization problem, which is inherently hard. For every algo-
rithm, each iteration of DP-Finder completes within 5 minutes on
average, demonstrating that DP-Finder is efficient in practice.
Figure 12: Boxplot of the true violation ϵ (x, x′, Φ) found by
randomly picking a triple vs. our search. The solid blue lines
show known upper bounds of these algorithms. We omit
one counterexample with a violation of ∞ (for both the ran-
domly picked and the optimized counterexamples) on AT5.
(presented in Sec. 5). In particular, Fig. 11 demonstrates that both
steps do not significantly reduce the quality of the estimates. As the
differentiable estimate ˆϵd (x, x′, Φ) is more imprecise compared to
the estimate ˆϵ (x, x′, Φ) based on sampling, Fig. 11 also demonstrates
the effectiveness of our estimation method (Sec. 4).
Q2: Efficiency of Violation Search. Next, we compare the ef-
ficiency of DP-Finder in finding counterexamples with large vio-
lations compared to random search (where we randomly sample
triples). For each algorithm, we computed the exact violation of the
0.51.01.5ˆdAT10.0000.0250.0500.0750.1000.1250.1500.1750.200ˆdAT2ˆdAT3ˆdAT4ˆdAT5ˆdATˆdexpMechˆdnoisyMaxˆdsum0.51.01.5randoptAT10.0000.0250.0500.0750.1000.1250.1500.1750.200randoptAT2randoptAT3randoptAT4randoptAT5randoptATrandoptexpMechrandoptnoisyMaxrandoptsumAT1AT2AT3AT4AT5ATexpMechnoisyMaxsum101102time[s](logscale)SamplingOptimization(SLSQP)Conﬁrmation(PSI)randoptAT11.0·1041.0·10510.0·1051.0·107numberofsamples(logscale)randoptAT2randoptAT3randoptAT4randoptAT5randoptATrandoptexpMechrandoptnoisyMaxrandoptsumFig. 14 shows the number of samples that DP-Finder selected,
according to the process described in Sec. 4.5. It demonstrates that
the required number of samples varies greatly across algorithms,
and even for a given algorithm. This suggests to adaptively select
the number of samples, which is what DP-Finder does (see Sec. 4.5).
7 RELATED WORK
In this section, we discuss the work closely related to ours.
Proving Differential Privacy. DP-Finder computes counterex-
amples to differential privacy (DP), thereby providing lower bounds
to DP. A complementary problem to this is finding upper bounds,
thereby proving DP. Many works have studied verification of DP.
Some works present languages that, at compile time, determine
the privacy or sensitivity of algorithms (queries) [6, 15, 32, 35]. A
different approach translates probabilistic algorithms to formulas
in Hoare logic to verify privacy [5]. Recently, proofs by couplings
have been shown successful for verifying privacy [3, 4].
Proving Sensitivity. Dwork et al. [12] defined the (global) sen-
sitivity of a function f as the maximum amount that any input to
f can change the output. While determining this sensitivity can
be done analytically for some functions (e.g., noisy sum), in others,
this task is more complex. In Nissim et al. [28], the authors define
the smooth sensitivity of a function for a given database, to avoid
the pessimistic worst-case bound of sensitivity. They also present a
sampling approach to approximate the smooth sensitivity. In Ru-
binstein and Aldà [33], the authors present a sampling approach to
approximate the (global) sensitivity.
Lower Bounds. The study of lower bounds on privacy started
with the work of Dinur and Nissim [11]. While they do not define
privacy in this work, they show how much noise needs to be added
to prevent a gross privacy violation. Since then, differential privacy
has been formally defined in [12] as (ϵ, 0)- and (ϵ, δ )-privacy, with
which lower bounds were proven for certain algorithms. Hardt and
Talwar [18] provide lower bounds on different noise mechanism
using ideas from convex geometry. [10] improve some of their
lower bounds and study additional settings (e.g., the one of [11]).
[25] study lower bounds in the context of how big a database has
to be to guarantee privacy.
Making Algorithms Differentiable. Priya Inala et al. [30] syn-
thesize unknowns in an algorithm that involves discrete and floating
point computation. To search for the unknowns, they make the algo-
rithm differentiable by techniques similar to ours, e.g, they use the
same construction to make the if-then-else primitive differentiable.
8 FUTURE WORK
In this section, we discuss possible future work items.
Extending DP-Finder to Real-world Algorithms. An impor-
tant topic of future research is extending DP-Finder to real-world
algorithms, like [8, 14, 29]. The main gap is that DP-Finder, in its
current form, does not scale to such complex algorithms. We see
several ways to mitigate this issue: (i) exploiting properties of the
search space, e.g., if dense, randomly sampling triples may perform
comparably to optimizing them, (ii) employing other optimization