title:Genoa TIE, Advanced Boundary Controller Experiment
author:Eric Monteith
Genoa TIE, Advanced Boundary Controller Experiment
Eric Monteith
NAI Labs, Network Associates Inc.
PI:EMAIL
Abstract
This 
in 
describes 
document 
two  phases, 
(TIE).  Achieved 
experimentation
performed as part of the Genoa Technology Integration
Experiment 
the
overarching  assertion  of  the  Genoa  TIE  was  that
boundary  controllers,  in  the  form  of  an  automated
guard,  could play  an  important  role  in  the  operational
success  of  Project  Genoa  [1].  Genoa,  an  ongoing
DARPA [2] research program, is focused on developing
a  prototype  decision  support  environment  for  the
National  Command  Authority,  and  is  intended  to
mitigate  potential  international  crises  early  in  their
development. In addition to protection from Information
Warfare  attacks  across  the  Internet  and  other  sources,
boundary  controllers  could  assist  the  Genoa  system  in
managing important aspects of information sharing, by
implementing  access  control  and  content  filtering  for
inter-enclave  transactions.  The  focus  of  this  paper
includes  experimentation  with  syntactic  and  Natural
Language  Processing 
the  Genoa
environment, and the measurement of their effectiveness
in filtering inter-enclave transactions.
filters  within 
Introduction
in 
for 
support  environment 
Genoa  is  focused  on  developing  a  prototype
decision 
the  National
Command  Authority  (NCA)  to  identify  and  mitigate
crisis  situations  early 
their  development.  The
organizations  comprising  the  NCA  span  those  of
intelligence,  operations,  execution,  and  decision/policy
control. Illustrated in Figure 1, each enclave comprising
the  NCA  represents  a  homogeneous  entity.  They  are
often organizationally oriented and can be hierarchically
organized.    However,  for  any  enclave  to  conduct  the
activities  that  it  is  responsible  for,  the  staff  in  that
enclave  will  have  to  make  use  of  information  and
human  resources  external 
  This
information  sharing  and  collaboration  across  enclave
boundaries  is  essential  functionality  that  Genoa  will
depend upon for fully attaining the program’s goals.
that  enclave. 
to 
Figure 1: Conceptual architecture
that 
Transactions 
take  place  across  enclave
boundaries  are  of  concern  to  each  of  the  individual
Genoa  enclaves.  Issues  over  need-to-know,  proper
examination of information, and release  of  information
that  could  be  interpreted  as  official  position  or  policy,
are  of  greatest  concern.  Consequently,  the  proper
control of information flow across enclave boundaries is
critical  to  the  success  of  Genoa,  and  its  acceptance  in
the existing organizational and process structures of the
intelligence  communities.  By  implementing  boundary
controllers  with  content-based  filtering  capabilities,
locally  controlled  discretionary  information  sharing
policies can be enforced, providing heightened security
for Genoa enclaves and inter-enclave transactions.
Motivation
Many  types  of  information  are  tightly  held  by
organizations  for  a  variety  of  reasons.    Policies  that
restrict sharing may be based on any number of security
reasons.  When faced with requests to share information
outside  of 
that
organization must review the information to ensure that
it  may  be  shared  without  violating  the  organization’s
policy.  Performed  by  security  officers,  this  review  can
be both time consuming and error-prone.  The primary
goal of the Genoa TIE was to compare the accuracy and
the  organization,  someone  within 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:08:29 UTC from IEEE Xplore.  Restrictions apply. 
efficiency  of  the  information  sanitization  and  filtering
process conducted by  security officers,  with  automated
filtering  capabilities.  To  perform  this  comparison,  we
needed to establish an experimental data sharing policy,
a  collection  of  information  resources,  and  a  set  of
requests initiating the  transactions.    We  also  needed  to
develop  the  automated  processes  that  would  determine
whether  one  organizations  data  could  be  released  to
another, based on policy.
The scenario used for the TIE was  closely  aligned
with  the  FY-99  Genoa  demonstration  scenario  and
related data.  That scenario involved the development of
structured  arguments  that  assess  the  capability  and  the
intent  of  terrorist  organizations  to  conduct  chemical
weapon  attacks.    The  working  example  is  that  of  the
Aum  Shinrikyo  Japanese  cult  [3]  that  bombed  the
Japanese metro system with an anthrax pathogen.
Experiment design
Within  the  Genoa  TIE,  syntactic  and  Natural
Language Processing (NLP) filters were investigated in
an  attempt  to  evaluate  their  effectiveness  in  terms  of
accuracy  and  performance  against  some  baseline.  A
baseline  was  established  through  manual  (human)
content-based  review  of  transaction  data.  We  expected
that  each  of  the  filtering  methods  (syntactic,  NLP,
manual) had varying strengths and weaknesses, and the
strengths  could  be  combined  to  achieve  more  accurate
and  efficient  filtering  results  than  any  single  filtering
method. The goal of experimentation was to investigate
ideal ways of combining automated, syntactic and NLP
filters.  Through  this  effort,  three  important  aspects  of
our work became apparent. 1) We developed a process
for measuring and comparing the content-based filtering
capabilities  of  an  automated  guard.  2)  We  determined
that  syntactic  and  NLP  filters,  composed  in  a  layered
configuration,  can  provide  better  accuracy  than  either
stand-alone solution. 3) While we found that our manual
review  provided 
the
automated  filters  were  able  to  detect  valid  policy
violations  the  humans  had  not  detected.  For  the
experimentation  described  within  this  document,  the
hypothesis  was  that  the  combination  of  syntactic  and
NLP  filters  in  some  configuration,  would  be  better  in
terms  of  performance  and  accuracy  than  either  filter
method individually.
the  best  accuracy,  both  of 
Experiment data
Data included information resources in the form of
Critical  Information  Packages  (CIPs),  a  set  of  requests
for  those  CIPs,  a  security  policy  governing  their
releasability,  and    “meta-rules”  that  elaborated  on  how
the rules were to be implemented.
(Identity info, time, title, type, format, language, class, etc.)
CIP Metadata
Product Metadata
(UUID, name, source, confidence, creation time, etc)
Product
Figure 2: CIP structure
into 
Created  in  Extensible  Markup  Language  (XML),
CIPs  encapsulated  a  number  of  “products”  and
associated  metadata  (information  pertaining  to  the
product).  Products  were  created  from  over  600  MB  of
raw  data  files  obtained  from  the  Genoa  program  that
were  converted 
text,  HTML,  Word,  Excel,
PowerPoint,  image,  and  Genoa  Virtual  Situation  Book
(VSB) files. Eighty-seven unique products were used to
construct  34  unique  CIPs  with  between  2-11  products
each. This represents approximately 1200 policy-related
metadata  fields  (author,  source,  description,  creation
date,  time  last  modified,  etc.).  The  products  ranged  in
size  from  a  small  number  of  bytes  to  approximately
500K. The structure of a CIP is depicted in Figure 2.
Experiment security policy
The goal of the security policy was to represent an
appropriate set of business rules for sharing data within
a Genoa environment. Several CIP metadata fields were
chosen  for  the  filters  to  examine  against  the  policy,
including  organization,  sub-organization,  user,  author,
CIP  size,  information  type,  information  source,  and
information  topic.  Similarly,  product  metadata  and
product content were examined as well. The policy also
included  a  set  of  “meta-rules”  to  provide  baseline
responses  in  the  absence  of  more  specific  information
and  provide  rule  precedence  in  the  case  of  conflicting
rules.  For  experimentation,  the  policy  was  developed
into an XML-like, Rules Markup Language (RML) that
the automated filters could then interpret. Ultimately, 28
RML rules were developed for the experiment.
Filter technology
Two  types  of  filters  were  implemented  to  perform
content-based review of transaction data. Syntactic-only
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:08:29 UTC from IEEE Xplore.  Restrictions apply. 
filtering  was  performed  by  Felt  [4]  filters  developed
specifically  for  the  TIE,  while  NLP  filter  capabilities
were  provided  by  DataShield,  a  product  developed  by
solutions-united  [5].  These  two  filtering  technologies
are explained throughout the remainder of this section.
3)  Syntactic  analysis  -  phrases  identified;  numeric
concept,  complex  nominal,  proper  noun,  non-
compositional.
4)  Semantic  analysis  -  proper  name  interpretation;
category of proper noun.
Felt  was  originally  developed  as  a  special-purpose
language  for  developing  filter  procedures  on  guards,
allowing  a  wide  class  of  message  formats  to  be
characterized.  Each  time  a  Felt  filter  parses  a  message
from its input, it applies the current policy to determine
whether  the  message  is  releasable.  In  addition,  the
contents of a field may be altered (sanitized) in order to
create  a  releasable  version  of  a  message.  Historically,
Felt had been used for filtering messages of the National
Imagery  Transmission  Format  (NITF),  a  complex
format  used  for  imagery  products.  The  resulting  filters
were highly efficient  yet provided detailed checking of
the many header and sub-header fields contained in the
messages.  The  Felt  filters  implemented  within  the
Genoa  TIE  used  a  list  of  character  strings,  categorized
under  key-topic  areas 
theme  of
experimentation. While  we had prior knowledge of the
CIP  data,  this  knowledge  was  not  used  to  specifically
develop  our  list  of  strings.  For  each  of  the  key-topic
areas  of  the  experiment  theme,  “key-words”  were
chosen  from  related,  open-source  subject  matter.  This
list  of  “key-words”  was  used  to  determine  syntactic
matches throughout the CIP meta, product, and product
meta data.
related 
to 
the 
NLP  filtering  performed  by  DataShield  was  much
more complex than the syntactic only review. There are
two  distinct  activities  involved  with  implementing  a
trainable  text  classification  system  such  as  DataShield.
The  first  is  the  actual  training  of  the  classifier.  The
second  is  its  implementation  in  which  it  performs
content  filtering.  The  first  of  these  tasks  involves
manually  classifying  a  set  of  “training  documents”  in
preparation for feeding them into the automatic system.
Each training document is characterized as being “in” or
“out” of range if it does or does not contain individual
key-topic areas as outlined by their definitions.
The second step is to take these manually classified
documents  and  process  them  with  the  trainable  text
classification  system.  During  this  process,  it  builds  a
“structure” of terms, phrases, and entities extracted from
the  text.  Multi-level  Natural  Language  Processing
(MNLP)  outputs  are  the  basis  for  these  textual  data
feature  representations.  The  four  types  of  analysis
include:
1)  Morphological  analysis  -  words  are  stemmed  to
their root form.
2)  Lexical  analysis  -  words  are  tagged  with  their
and
type  of  noun,  verb, 
part-of-speech, 
determiner.
This collection  of  automatically  generated  features
is  then  used  to  determine  membership  of  new  texts
within  a  particular  key-topic  area.  The  DataShield
system  determines  the  “certainty  of  membership”  for
each of the documents as compared to each of the topic
areas. Consider a range of 0.0 to 1.0, where 1.0 defines
a document as containing a member of a certain class of
key-topics and 0.0 defines a document as not containing
a member of a certain class. Values of 0.0 and 1.0 both
have  a  “certainty  of  membership”  value  of  1.0.  This
means that for either of these cases, it can be concluded
with  great  certainty  that  the  document  either  does  or
does  not  belong  within  a  given  class.    If  DataShield
characterizes  a  document  with  a  value  close  to  0.5,  a
“certainty of membership” value close to 0.0 will result.
For 
these  cases,  DataShield  cannot  automatically
determine whether a given document should be assigned
to  a  given  class.  These  documents  are  considered
valuable  in  refining  the  classification  system  and  its
“knowledge  base”.    By  manually  classifying  these
documents  and  then  feeding  them  back  into  the
automatic system, the system is trained to recognize the
subtle differences that distinguish how these documents
should  be  classified.  During 
the  CIP  verification
process, individual products are classed into predefined
key-topic  areas  specified  within 
the  RML  rules.
Products that  fall  outside  of  these  classes  do  not  cause
any  of  the  rules  to  fire.    If  a  product  falls  into  one  or
more of the predefined classes within a given rule, and
the other conditions of the rule have been met, the rule
will  fire  with  the  specified  action.  For  additional
information about the NLP technology used within this
effort, see [6].
Experiment baseline
Ultimately,  in  order  to  assess  the  accuracy  and
performance  of  the  automated  filters,  a  baseline  for
comparison  was  required.  This  baseline  was  obtained