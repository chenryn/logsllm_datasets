# TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time

**Authors:** Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and Lorenzo Cavallaro  
**Affiliations:** King’s College London, Royal Holloway, University of London, Bundeswehr University Munich

**Publication:** Proceedings of the 28th USENIX Security Symposium, August 14–16, 2019, Santa Clara, CA, USA  
**DOI:** 978-1-939133-06-9  
**Open Access:** Sponsored by USENIX

## Abstract

Is Android malware classification a solved problem? Published F1 scores of up to 0.99 suggest that there is little room for improvement. However, we argue that these results are often inflated due to two pervasive sources of experimental bias: spatial bias, where the distribution of training and testing data does not reflect real-world conditions; and temporal bias, where incorrect time splits of training and testing sets lead to unrealistic configurations. We propose a set of space and time constraints to eliminate both biases. We introduce a new metric, AUT, to summarize the expected robustness of a classifier in real-world settings and present an algorithm to optimize its performance. Finally, we demonstrate how this approach can be used to evaluate mitigation strategies for time decay, such as active learning. Our solutions are implemented in TESSERACT, an open-source evaluation framework for comparing malware classifiers in a realistic setting. Using TESSERACT, we evaluated three Android malware classifiers on a dataset of 129K applications spanning over three years. Our evaluation confirms that earlier published results are biased, reveals counter-intuitive performance, and shows that appropriate tuning can lead to significant improvements.

## 1. Introduction

Machine learning has become a standard tool for malware research in the academic security community, with applications ranging from Windows malware [12, 34, 51] to PDF malware [27, 32], malicious URLs [28, 48], malicious JavaScript [11, 43], and Android malware [4, 22, 33]. Despite high performance figures, malware remains a persistent threat.

Malware classifiers operate in dynamic contexts, and their effectiveness can decay over time as new variants and families emerge. Temporal consistency is crucial for evaluating classifier performance. When the experimental setup allows a classifier to train on future knowledge, the reported results become biased [2, 36].

In this paper, we focus on Android malware and claim that many Android malware classifiers [4, 13, 18, 22, 33, 49, 56, 57] (including our own work) are not evaluated in settings representative of real-world deployments. We chose Android due to the availability of a large, timestamped dataset (AndroZoo [3]) and algorithms that are feasible to reproduce.

We identify experimental bias in two dimensions: space and time. Spatial bias refers to unrealistic assumptions about the ratio of goodware to malware in the data, which must be consistent during the testing phase to mimic a realistic scenario. For example, most Android apps in the wild are goodware [21, 30], whereas most software download events are malicious [31, 41]. Temporal bias refers to temporally inconsistent evaluations that integrate future knowledge into the training phase or create unrealistic settings. This problem is exacerbated by closely related malware families, where including one variant in the training set may allow the algorithm to identify many variants in the testing set.

The pervasiveness of these issues is due to two main reasons: possible sources of evaluation bias are not common knowledge, and accounting for time complicates the evaluation and makes it difficult to compare to other approaches using headline metrics like F1-Score or AUROC. We address these issues by systematizing evaluation bias for Android malware classification, providing new constraints for sound experiment design, and introducing new metrics and tool support.

Prior work has investigated challenges and experimental bias in security evaluations [2, 5, 36, 44, 47, 54]. The base-rate fallacy [5] describes how metrics like TPR and FPR can be misleading in intrusion detection due to class imbalance. In contrast, we identify and address experimental settings that give misleading results regardless of the metrics used. Sommer and Paxson [47], Rossow et al. [44], and van der Kouwe et al. [54] discuss guidelines for sound security evaluations, but none identify temporal and spatial bias or quantify their impact on classifier performance. Allix et al. [2] and Miller et al. [36] identify initial temporal constraints, but even recent work following their guidelines (e.g., [33]) suffers from other temporal and spatial biases (§4.4). To the best of our knowledge, we are the first to identify and address these sources of bias with novel, actionable constraints, metrics, and tool support (§4).

### Contributions

- **Identification of Biases:** We identify temporal bias associated with incorrect train-test splits (§3.2) and spatial bias related to unrealistic assumptions in dataset distribution (§3.3). We experimentally verify on a dataset of 129K apps (with 10% malware) that, due to bias, performance can decrease by up to 50% in practice (§3.1) in two well-known Android malware classifiers, DREBIN [4] and MAMADROID [33], referred to as ALG1 and ALG2, respectively.
- **Robust Evaluation Framework:** We propose a set of spatio-temporal constraints for more robust evaluations (§4.1), a new metric, AUT, that captures a classifier's robustness to time decay (§4.2), and a tuning algorithm that optimizes classification performance when malware is the minority class (§4.3). We compare the performance of ALG1 [4], ALG2 [33], and DL [22] (a deep learning-based approach) and show how removing bias can provide counter-intuitive results on real performance (§4.4).
- **TESSERACT Implementation:** We implement and publicly release TESSERACT, a methodology that assists the research community in producing comparable results, revealing counter-intuitive performance, and assessing a classifier's prediction qualities in industrial deployment (§6). TESSERACT also evaluates the performance-cost trade-offs of solutions to mitigate time decay, such as active learning (§5).

We believe our methodology creates an opportunity to evaluate the extent to which spatio-temporal experimental bias affects other security domains and encourage the security community to adopt its underlying philosophy.

### Use of the Term "Bias"

We use "bias" to refer to details of an experimental setting that depart from real-world conditions and can have a misleading impact on evaluations. This is distinct from the classifier bias/variance trade-off in traditional machine learning terminology [8].

## 2. Android Malware Classification

### 2.1 Reference Algorithms

To assess experimental bias, we consider two high-profile machine learning-driven techniques for Android malware classification, both published in top-tier security conferences:

- **ALG1 [4]:** A linear support vector machine (SVM) on high-dimensional binary feature vectors engineered with lightweight static analysis.
- **ALG2 [33]:** A Random Forest (RF) applied to features engineered by modeling caller-callee relationships over Android API methods as Markov chains.

We choose ALG1 and ALG2 because they build on different types of static analysis to generate feature spaces capturing Android application characteristics at different levels of abstraction and use different machine learning algorithms to learn decision boundaries between benign and malicious apps. This broad design space supports the generality of our methodology for characterizing experimental bias. We reimplemented ALG1 based on the detailed description in the paper and relied on the implementation provided by the authors for ALG2. We replicated the baseline results for both approaches.

After identifying and quantifying the impact of experimental bias (§3), we propose specific constraints and metrics to allow fair and unbiased comparisons (§4). Since ALG1 and ALG2 use traditional ML algorithms, we also consider DL [22], a deep learning-based approach that takes the same features as ALG1. We include DL because the latent feature space of deep learning approaches can capture different representations of the input data, which may affect their robustness to time decay. We replicated the baseline results for DL by re-implementing its neural network architecture and using the same input features as for ALG1.

Details on the hyperparameters of the re-implemented algorithms are provided in §A.1.

### 2.2 Estimating in-the-wild Malware Ratio

The proportion of malware in the dataset significantly affects classifier performance. Unbiased experiments require a dataset with a realistic percentage of malware. For existing datasets, this can be achieved by downsampling the majority class (§3.3). Each malware domain has a unique ratio of malware to goodware. For example, malware is the minority class in network traffic [5] and Android [30], but the majority class in binary download events [41].

Estimating the percentage of malware in the wild for a given domain is non-trivial, but measurement papers, industry telemetry, and publicly-available reports can provide realistic estimates. In the Android landscape, malware represents 6%–18.8% of all apps, according to different sources. A key industrial player reported approximately 6%, while the AndRadar measurement study [30] reports around 8%. Google's 2017 Android security report [21] suggests 6–10% malware, and an analysis of the AndroZoo dataset [3] reveals an incidence of 18.8% malicious apps. Given these estimates, we stabilize the malware percentage to 10% (a de-facto average), with per-month values between 8% and 12%. This ensures a statistically sound number of per-month malware samples without aggressive undersampling or oversampling goodware (§2.3).

### 2.3 Dataset

We use samples from the public AndroZoo [3] dataset, consisting of more than 8.5 million Android apps between 2010 and early 2019. Each app is associated with a timestamp, and most include VirusTotal metadata results. The dataset is constantly updated by crawling from different markets, including Google Play Store, Anzhi, and AppChina.

#### Goodware and Malware

AndroZoo's metadata reports the number \( p \) of positive anti-virus reports on VirusTotal [20] for each app. We classify apps with \( p = 0 \) as goodware and \( p \geq 4 \) as malware, following Miller et al.'s [36] advice for reliable ground truth. About 13% of AndroZoo apps are grayware (0 < \( p \) < 4) and are excluded from the sampling to avoid disadvantaging classifiers designed with different labeling thresholds.

#### Choosing Apps

The number of apps considered in our study is influenced by the feature extraction cost and storage requirements. Extracting features for the entire AndroZoo dataset would take up to three years on our research infrastructure (three high-spec Dell PowerEdge R730 nodes, each with 2 x 14 cores in hyperthreading, 168 vCPU threads, 1.2TB of RAM, and a 100TB NAS). Therefore, we extracted features from 129K apps, which we believe is a large, statistically significant dataset. To evaluate time decay, we sampled 129K AndroZoo apps uniformly from January 2014 to December 2016, enforcing an overall average of 10% malware (with a monthly range of 8% to 12%). This ensures 1,000+ apps per month, except for the last three months, where fewer apps were available. We considered apps up to December 2016 because VirusTotal results for 2017 and 2018 were mostly unavailable, and antivirus detections become stable after approximately one year [36].

**Figure 1:** Details of the dataset used in this paper. The figure shows a stack histogram with the monthly distribution of 129,728 Android applications (with an average 10% malware) from January 2014 to December 2016. The vertical dotted line denotes the split used in all time-aware experiments: training on 2014, testing on 2015 and 2016.