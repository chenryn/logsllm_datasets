title:TESSERACT: Eliminating Experimental Bias in Malware Classification
across Space and Time
author:Feargus Pendlebury and
Fabio Pierazzi and
Roberto Jordaney and
Johannes Kinder and
Lorenzo Cavallaro
TESSERACT: Eliminating Experimental Bias in 
Malware Classification across Space and Time
Feargus Pendlebury, Fabio Pierazzi, and Roberto Jordaney, King’s College London & 
Royal Holloway, University of London; Johannes Kinder, Bundeswehr University Munich; 
Lorenzo Cavallaro, King’s College London
https://www.usenix.org/conference/usenixsecurity19/presentation/pendlebury
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.TESSERACT: Eliminating Experimental Bias in Malware Classiﬁcation
across Space and Time
Feargus Pendlebury∗†‡, Fabio Pierazzi∗†‡, Roberto Jordaney†‡, Johannes Kinder§, Lorenzo Cavallaro†
† King’s College London
‡ Royal Holloway, University of London
§ Bundeswehr University Munich
Abstract
Is Android malware classiﬁcation a solved problem? Pub-
lished F1 scores of up to 0.99 appear to leave very little room
for improvement. In this paper, we argue that results are com-
monly inﬂated due to two pervasive sources of experimental
bias: spatial bias caused by distributions of training and test-
ing data that are not representative of a real-world deployment;
and temporal bias caused by incorrect time splits of training
and testing sets, leading to impossible conﬁgurations. We
propose a set of space and time constraints for experiment de-
sign that eliminates both sources of bias. We introduce a new
metric that summarizes the expected robustness of a classiﬁer
in a real-world setting, and we present an algorithm to tune
its performance. Finally, we demonstrate how this allows us
to evaluate mitigation strategies for time decay such as active
learning. We have implemented our solutions in TESSERACT,
an open source evaluation framework for comparing mal-
ware classiﬁers in a realistic setting. We used TESSERACT
to evaluate three Android malware classiﬁers from the liter-
ature on a dataset of 129K applications spanning over three
years. Our evaluation conﬁrms that earlier published results
are biased, while also revealing counter-intuitive performance
and showing that appropriate tuning can lead to signiﬁcant
improvements.
1 Introduction
Machine learning has become a standard tool for malware re-
search in the academic security community: it has been used
in a wide range of domains including Windows malware [12,
34, 51], PDF malware [27, 32], malicious URLs [28, 48], ma-
licious JavaScript [11, 43], and Android malware [4, 22, 33].
With tantalizingly high performance ﬁgures, it seems malware
should be a problem of the past.
Malware classiﬁers operate in dynamic contexts. As mal-
ware evolves and new variants and families appear over time,
prediction quality decays [26]. Therefore, temporal consis-
tency matters for evaluating the effectiveness of a classiﬁer.
*Equal contribution.
When the experimental setup allows a classiﬁer to train on
what is effectively future knowledge, the reported results be-
come biased [2, 36].
This issue is widespread in the security community and
affects multiple security domains. In this paper, we focus on
Android malware and claim that there is an endemic issue
in that Android malware classiﬁers [4, 13, 18, 22, 33, 49, 56,
57] (including our own work) are not evaluated in settings
representative of real-world deployments. We choose Android
because of the availability of (a) a public, large-scale, and
timestamped dataset (AndroZoo [3]) and (b) algorithms that
are feasible to reproduce (where all [33] or part [4] of the
code has been released).
We identify experimental bias in two dimensions, space and
time. Spatial bias refers to unrealistic assumptions about the
ratio of goodware to malware in the data. The ratio of good-
ware to malware is domain-speciﬁc, but it must be enforced
consistently during the testing phase to mimic a realistic sce-
nario. For example, measurement studies on Android suggest
that most apps in the wild are goodware [21, 30], whereas
for (desktop) software download events most URLs are mali-
cious [31,41]. Temporal bias refers to temporally inconsistent
evaluations which integrate future knowledge about the test-
ing objects into the training phase [2, 36] or create unrealistic
settings. This problem is exacerbated by families of closely
related malware, where including even one variant in the train-
ing set may allow the algorithm to identify many variants in
the testing.
We believe that the pervasiveness of these issues is due to
two main reasons: ﬁrst, possible sources of evaluation bias are
not common knowledge; second, accounting for time com-
plicates the evaluation and does not allow a comparison to
other approaches using headline evaluation metrics such as
the F1-Score or AUROC. We address these issues in this paper
by systematizing evaluation bias for Android malware classi-
ﬁcation and providing new constraints for sound experiment
design along with new metrics and tool support.
Prior work has investigated challenges and experimental
bias in security evaluations [2,5,36,44,47,54]. The base-rate
USENIX Association
28th USENIX Security Symposium    729
fallacy [5] describes how evaluation metrics such as TPR and
FPR are misleading in intrusion detection, due to signiﬁcant
class imbalance (most trafﬁc is benign); in contrast, we iden-
tify and address experimental settings that give misleading
results regardless of the adopted metrics—even when correct
metrics are reported. Sommer and Paxson [47], Rossow et
al. [44], and van der Kouwe et al. [54] discuss possible guide-
lines for sound security evaluations; but none of these works
identify temporal and spatial bias, nor do they quantify the
impact of errors on classiﬁer performance. Allix et al. [2]
and Miller et al. [36] identify an initial temporal constraint in
Android malware classiﬁcation, but we show that even results
of recent work following their guidelines (e.g., [33]) suffer
from other temporal and spatial bias (§4.4). To the best of
our knowledge, we are the ﬁrst to identify and address these
sources of bias with novel, actionable constraints, metrics,
and tool support (§4).
This paper makes the following contributions:
• We identify temporal bias associated with incorrect train-
test splits (§3.2) and spatial bias related to unrealistic
assumptions in dataset distribution (§3.3). We experi-
mentally verify on a dataset of 129K apps (with 10% mal-
ware) that, due to bias, performance can decrease up to
50% in practice (§3.1) in two well-known Android mal-
ware classiﬁers, DREBIN [4] and MAMADROID [33],
which we refer to as ALG1 and ALG2, respectively.
• We propose novel building blocks for more robust eval-
uations of malware classiﬁers: a set of spatio-temporal
constraints to be enforced in experimental settings (§4.1);
a new metric, AUT, that captures a classiﬁer’s robust-
ness to time decay in a single number and allows for
the fair comparison of different algorithms (§4.2); and
a novel tuning algorithm that empirically optimizes the
classiﬁcation performance, when malware represents the
minority class (§4.3). We compare the performance of
ALG1 [4], ALG2 [33] and DL [22] (a deep learning-
based approach), and show how removing bias can pro-
vide counter-intuitive results on real performance (§4.4).
• We implement and publicly release the code of our
methodology (§4), TESSERACT, and we further demon-
strate how our ﬁndings can be used to evaluate
performance-cost trade-offs of solutions to mitigate time
decay such as active learning (§5).
TESSERACT can assist the research community in pro-
ducing comparable results, revealing counter-intuitive perfor-
mance, and assessing a classiﬁer’s prediction qualities in an
industrial deployment (§6).
We believe that our methodology also creates an opportu-
nity to evaluate the extent to which spatio-temporal experi-
mental bias affects security domains other than Android mal-
ware, and we encourage the security community to embrace
its underpinning philosophy.
Use of the term “bias”: We use (experimental) bias to
refer to the details of an experimental setting that depart from
the conditions in a real-world deployment and can have a
misleading impact (bias) on evaluations. We do not intend
it to relate to the classiﬁer bias/variance trade-off [8] from
traditional machine learning terminology.
2 Android Malware Classiﬁcation
We focus on Android malware classiﬁcation. In §2.1 we intro-
duce the reference approaches evaluated, in §2.2 we discuss
the domain-speciﬁc prevalence of malware, and in §2.3 we
introduce the dataset used throughout the paper.
2.1 Reference Algorithms
To assess experimental bias (§3), we consider two high-proﬁle
machine learning-driven techniques for Android malware clas-
siﬁcation, both published recently in top-tier security confer-
ences. The ﬁrst approach is ALG1 [4], a linear support vector
machine (SVM) on high-dimensional binary feature vectors
engineered with a lightweight static analysis. The second
approach is ALG2 [33], a Random Forest (RF) applied to fea-
tures engineered by modeling caller-callee relationships over
Android API methods as Markov chains. We choose ALG1
and ALG2 as they build on different types of static analy-
sis to generate feature spaces capturing Android application
characteristics at different levels of abstraction; furthermore,
they use different machine learning algorithms to learn de-
cision boundaries between benign and malicious Android
apps in the given feature space. Thus, they represent a broad
design space and support the generality of our methodology
for characterizing experimental bias. For a sound experimen-
tal baseline, we reimplemented ALG1 following the detailed
description in the paper; for ALG2, we relied on the imple-
mentation provided by its authors. We replicated the baseline
results for both approaches. After identifying and quantifying
the impact of experimental bias (§3), we propose speciﬁc con-
straints and metrics to allow fair and unbiased comparisons
(§4). Since ALG1 and ALG2 adopt traditional ML algorithms,
in §4 we also consider DL [22], a deep learning-based ap-
proach that takes as input the same features as ALG1 [4]. We
include DL because the latent feature space of deep learning
approaches can capture different representations of the input
data [19], which may affect their robustness to time decay.
We replicate the baseline results for DL reported in [22] by
re-implementing its neural network architecture and by using
the same input features as for ALG1.
It speaks to the scientiﬁc standards of these papers that
we were able to replicate the experiments; indeed, we would
like to emphasize that we do not criticize them speciﬁcally.
We use these approaches for our evaluation because they are
available and offer stable baselines.
We report details on the hyperparameters of the reimple-
mented algorithms in §A.1.
2.2 Estimating in-the-wild Malware Ratio
The proportion of malware in the dataset can greatly affect
the performance of the classiﬁer (§3). Hence, unbiased experi-
730    28th USENIX Security Symposium
USENIX Association
ments require a dataset with a realistic percentage of malware
over goodware; on an already existing dataset, one may en-
force such a ratio by, for instance, downsampling the majority
class (§3.3). Each malware domain has its own, often unique,
ratio of malware to goodware typically encountered in the
wild. First, it is important to know if malware is a minority,
majority, or an equal-size class as goodware. For example,
malware is the minority class in network trafﬁc [5] and An-
droid [30], but it is the majority class in binary download
events [41]. On the one hand, the estimation of the percentage
of malware in the wild for a given domain is a non-trivial task.
On the other hand, measurement papers, industry telemetry,
and publicly-available reports may all be leveraged to obtain
realistic estimates.
In the Android landscape, malware represents 6%–18.8%
of all the apps, according to different sources: a key indus-
trial player1 reported the ratio as approximately 6%, whereas
the AndRadar measurement study [30] reports around 8% of
Android malware in the wild. The 2017 Google’s Android
security report [21] suggests 6–10% malware, whereas an
analysis of the metadata of the AndroZoo dataset [3] totaling
almost 8M Android apps updated regularly, reveals an inci-
dence of 18.8% of malicious apps. The data suggests that, in
the Android domain, malware is the minority class. In this
work, we decide to stabilize its percentage to 10% (a de-facto
average across the various estimates), with per-month values
between 8% and 12%. Settling on an average overall ratio of
10% Android malware also allows us to collect a dataset with
a statistically sound number of per-month malware. An ag-
gressive undersampling would have decreased the statistical
signiﬁcance of the dataset, whereas oversampling goodware
would have been too resource intensive (§2.3).
2.3 Dataset
We consider samples from the public AndroZoo [3] dataset,
consisting of more than 8.5 million Android apps between
2010 and early 2019: each app is associated with a times-
tamp, and most apps include VirusTotal metadata results.
The dataset is constantly updated by crawling from differ-
ent markets (e.g., more than 4 million apps from Google Play
Store, and the remaining from markets such as Anzhi and
AppChina). We choose to refer to this dataset due to its size
and timespan, which allow us to perform realistic space- and
time-aware experiments.
Goodware and malware. AndroZoo’s metadata reports
the number p of positive anti-virus reports on VirusTotal [20]
for applications in the AndroZoo dataset. We chose p = 0 for
goodware and p ≥ 4 for malware, following Miller et al.’s [36]
advice for a reliable ground-truth. About 13% of AndroZoo
apps can be called grayware as they have 0 < p < 4. We
exclude grayware from the sampling as including it as either
goodware or malware could disadvantage classiﬁers whose
features were designed with a different labeling threshold.
1Information obtained through conﬁdential emails with the authors.
Figure 1: Details of the dataset considered throughout this
paper. The ﬁgure reports a stack histogram with the monthly
distribution of apps we collect from AndroZoo: 129,728 An-
droid applications (with average 10% malware), spanning
from Jan 2014 to Dec 2016. The vertical dotted line denotes
the split we use in all time-aware experiments in this paper
(see §4 and §5): training on 2014, testing on 2015 and 2016.
Choosing apps. The number of objects we consider in our
study is affected by the feature extraction cost, and partly
by storage space requirements (as the full AndroZoo dataset,
at the time of writing, is more than 50TB of apps to which
one must add the space required for extracting features). Ex-
tracting features for the whole AndroZoo dataset may take
up to three years on our research infrastructure (three high-
spec Dell PowerEdge R730 nodes, each with 2 x 14 cores in
hyperthreading—in total, 168 vCPU threads, 1.2TB of RAM,
and a 100TB NAS), thus we decided to extract features from
129K apps (§2.2). We believe this represents a large dataset
with enough statistical signiﬁcance. To evaluate time decay,
we decide on a granularity of one month, and we uniformly
sample 129K AndroZoo apps in the period from Jan 2014 to
Dec 2016, but also enforce an overall average of 10% mal-
ware (see §2.2)–with an allowed percentage of malware per
month between 8% and 12%, to ensure some variability. Span-
ning over three years ensures 1,000+ apps per month (except
for the last three months, where AndroZoo had crawled less
applications). We consider apps up to Dec 2016 because the
VirusTotal results for 2017 and 2018 apps were mostly un-
available from AndroZoo at the time of writing; moreover,
Miller et al. [36] empirically evaluated that antivirus detec-
tions become stable after approximately one year—choosing