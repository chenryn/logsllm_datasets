title:HackEd: A Pedagogical Analysis of Online Vulnerability Discovery
Exercises
author:Daniel Votipka and
Eric Zhang and
Michelle L. Mazurek
2
9
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
HackEd: A Pedagogical Analysis of Online Vulnerability Discovery Exercises
Daniel Votipka
Tufts University
PI:EMAIL
Eric Zhang and Michelle L. Mazurek
University of Maryland
PI:EMAIL, PI:EMAIL
Abstract—Hacking exercises are a common tool for security
education, but there is limited investigation of how they teach
security concepts and whether they follow pedagogical best
practices. This paper enumerates the pedagogical practices of
31 popular online hacking exercises. Speciﬁcally, we derive a set
of pedagogical dimensions from the general learning sciences and
educational literature, tailored to hacking exercises, and review
whether and how each exercise implements each pedagogical di-
mension. In addition, we interview the organizers of 15 exercises
to understand challenges and tradeoffs that may occur when
choosing whether and how to implement each dimension.
We found hacking exercises generally were tailored to students’
prior security experience and support learning by limiting extra-
neous load and establishing helpful online communities. Con-
versely, few exercises explicitly provide overarching conceptual
structure or direct support for metacognition to help students
transfer learned knowledge to new contexts. Immediate and
tailored feedback and secure development practice were also
uncommon. Additionally, we observed a tradeoff between provid-
ing realistic challenges and burdening students with extraneous
cognitive load, with beneﬁts and drawbacks at any point on this
axis. Based on our results, we make suggestions for exercise
improvement and future work to support organizers.
I. INTRODUCTION
Historically, the security community has used online hacking
exercises to provide practical education, exposing participants
to a variety of vulnerabilities and security concepts. In
these exercises, participants demonstrate their security concept
understanding by ﬁnding, exploiting, and sometimes ﬁxing
vulnerabilities in programs. Exercises offer discrete practice
sets that can be undertaken in a modular fashion, similarly
to practice problems commonly included at the end of each
chapter in mathematics textbooks. In fact, hacking exercises are
commonly considered useful educational tools, with security
experts often reporting that
they rely on them for their
education [1], bug bounty platforms directing those interested
in security to start with these exercises [2], [3], and a signiﬁcant
amount of recent security-education work focuses on creating
new exercises [4]–[10]. Further, prior work has provided some
evidence that hacking exercises can provide valuable immediate
feedback to learners in academic settings [7], [11]–[14].
However, analysis of hacking exercises as educational tools
is limited. First, many studies only consider a sparse few
exercises [4]–[10], [13], [15], [16], limiting understanding of
the broad set of popular exercises. Prior work also focuses on
a few speciﬁc measures of learning and engagement [5]–[8],
[12], [17], making the evidence narrow. In particular, learning
factors which are difﬁcult to control for and measure are rarely
considered. Overall, exercise organizers have limited guidance
for building effective exercises, educators do not know which
exercises provide the most effective learning, and researchers
do not have a broad view of the landscape of current exercises.
As a step toward expanding this analysis, we review online
hacking exercises to address two main research questions:
• RQ1: Do currently available exercises apply general
pedagogical principles suggested by the learning sciences
literature? If so, how are these principles implemented?
• RQ2: How do exercise organizers consider which princi-
ples to implement?
To answer these questions we performed an in-depth qual-
itative review of 31 popular online hacking exercises (67%
of all online exercises we identiﬁed). As part of our analysis,
we completed a sample of 313 unique challenges from these
31 exercises. We evaluated each exercise against a set of
recommended pedagogical principles grounded in learning
theory [18], [19]. We base our approach on previous curriculum
evaluation efforts [20], tailoring the pedagogical principles we
use for applicability to hacking exercises. Further, we interview
the organizers of 15 exercises to understand how they consider
which principles to implement.
We found that no exercise implemented every pedagogical
principle, but most were implemented by at least some exercises,
some in unique and creative ways. Notable exceptions include
that many exercises do not provide structure to help students
organize knowledge, or feedback to guide their progress
through learning objectives. Few organizers had considered
metacognition, i.e., helping students consider what and how
much they have learned at a high level. We also found that
some pedagogical principles are in tension with each other —
such as balancing difﬁculty with realism — while others are in
tension with the competitive origin of many exercises. Finally,
we ﬁnd that community participation brings many beneﬁts, but
must be carefully managed to ensure educational structures are
maintained. From these results, we distill recommendations for
improving exercises and future work to support organizers.
II. METHODS
To understand the current landscape of online hacking
exercises, we performed a two-phase study: a qualitative review
of popular online exercises and interviews with the organizers
of these exercises. Here, we discuss how we selected exercises
for review, our review process, and our interview protocol.
A. Exercise Selection
There are many kinds of resources available to security stu-
dents, such as vulnerability write-ups, certiﬁcations, academic
coursework, and books. To limit our inquiry’s scope, we focus
© 2021, Daniel Votipka. Under license to IEEE.
DOI 10.1109/SP40001.2021.00092
1268
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
on online educational exercises—commonly recommended by
security experts [1]—, which meet the following criteria:
• Educational – Because we are evaluating the educational
beneﬁt of each exercise, we only include exercises which
explicitly state education as a goal. We do not consider
competitions, such as the DefCon Qualiﬁers, whose goal
is to identify the “best” hackers.
• Hands-on – Exercises must include a hands-on component
requiring students to actively practice security concepts.
This component could be the central focus of the exercise—
as in many CTFs—or auxiliary, e.g., presented after a
series of associated lectures.
• Online and publicly accessible - We focused on online
exercises so we could analyze them by actually partici-
pating, rather than making possibly incorrect assumptions
based on an ofﬂine exercise’s description.
• Popular – We opted to focus on popular exercises students
are most likely to participate in. To estimate a site’s
popularity, we used its Tranco rank—a secure method
for ranking sites based on user visits [21] — . as of
October 15th, 2019. Because Tranco only tracks the top
one million sites, we used Alexa rankings if no Tranco
ranking was available. Each site’s rank is given in Table I.1
Because we focused on explicitly educational and popular
exercises, many of the exercises we reviewed had funding
support, either by offering a paid version of the exercise
(e.g., HackEDU, HackTheBox, Mr. Code, Vulnhub), receiving
funding through a parent company (e.g., Google supports
gCTF and the SANS Institute supports GirlsGo CyberStart),
or through grant funding (e.g., picoCTF, BIBIFI). As a result,
several organizers we interviewed could dedicate time and
resources to improving students’ educational experience, which
is not necessarily common among CTFs run by professionals
in their spare time or university student clubs [22].
1) Exercise Identiﬁcation: To identify exercises meeting our
criteria, we ﬁrst contacted eight security education experts
recruited through one author’s personal and professional
relationships. We asked each to recommend exercises, publicly
available lists of exercises, and possible search keywords. Based
on their recommendations, we performed Google searches
with all possible combinations of “cybersecurity,” “computer
security,” and “security” with “capture the ﬂag,” “CTF,” and
“war games,” as well as “hacking exercises.” We reviewed the
ﬁrst 10 result pages per query for candidates. We also reviewed
curated exercise lists suggested by our experts [23]–[27].
For each exercise and recommendation list identiﬁed, we also
reviewed the top three similar sites identiﬁed by Alexa.com.2
After this search, the security education experts reviewed our
list to identify any missing exercises and add other terms or
lists they had previously mentioned. We continued this process
until no new exercises were identiﬁed, in October 2019.
1This ranking indicates a domain’s popularity, not the exercise’s speciﬁc
sub-domain, introducing some ambiguity (e.g., gCTF beneﬁts in ranking from
its location at withgoogle.com). However, this was not a common problem in
our data, so we believe this popularity ranking is a reasonable approximation.
2https://www.alexa.com/siteinfo
While almost all the exercises we identiﬁed were joinable
year-round, many were initially designed as a live, short-
term competition. We expected the initial participation context
to affect exercise structure, so for comparison purposes, we
assigned each exercise to one of two categories:
• Synchronous (N=13) – Designed for simultaneous partic-
ipation over a short time period (i.e.,a few days or weeks).
This includes most capture-the-ﬂag (CTF) competitions.
Challenges from these exercises are made available after
the competition for more students to try at their own pace.
• Asynchronous (N=18) – Designed for participation at any
time at the student’s pace; often referred to as “wargames.”
2) Sample Selection: We identiﬁed 45 exercises meeting
our criteria (18 Synchronous, 27 Asynchronous). To balance
completeness with manual effort, we sampled about 66% for
in-depth review. To focus on exercises reaching the most
participants, we began with the top 30% (by popularity rank) in
each group. We then randomly sampled the remaining exercises
until we selected about 66% of each group. We include less
visited exercises to account for those still growing in popularity.
The ﬁnal list of exercises is given in Table I. Note, some authors
are afﬁliated with BIBIFI, which was randomly selected during
this phase. We did not exclude it to ensure representation of
attack-defense-style exercises. To expand this category beyond
BIBIFI, we purposively added one more exercise (iCTF), and
worked with its organizers to enable analysis despite its highly
synchronous (not typically joinable at any time) structure,
bringing the total set of reviewed exercises to 31.
B. Pedagogical Review (RQ1)
To identify pedagogical principles, we drew on previous
efforts to synthesize major theoretical and empirical learn-
ing sciences and education research ﬁndings into actionable
principles [18]. This led us to ﬁve core pedagogical prin-
ciples: connecting to learners’ prior knowledge [28], [29],
organizing declarative knowledge [18], active practice and
feedback [19], [30], encouraging metacognitive learning [31],
[32], and establishing a supportive and collaborative learning
environment [19]. These principles are understood to support
human learning generally. While there is little evidence speciﬁc
to security education for these principles [5], [7], [15], they
have been found effective in related domains (i.e., various
STEM ﬁelds including computer science education), so we
expect them to apply to security education as well.
To identify actionable dimensions for each principle, we
started with 24 dimensions used by Kim and Ko [20] in their
similar review of online programming exercises. Two authors
then tailored these dimensions through collaborative open
coding of ﬁve exercises. For example, Kim and Ko considered
whether students wrote code during tutorials as a dimension
of the active practice and feedback principle. We modify this
by asking whether students are required to practice exploiting
programs and writing secure code. Additionally, Kim and Ko
did not consider establishing a supportive and collaborative
learning environment because online programming tutorials
are typically used in isolation. Because we observed that the
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1269
communities around exercises were an important factor, we
added this principle to our review. This process resulted in 30
total pedagogical dimensions, across the 5 core principles. We
discuss each dimension in further detail in Section III.
For each selected exercise, two researchers independently
evaluated each exercise qualitatively coding implementation
of the pedagogical dimensions. For reporting simplicity, we
used three levels: yes ((cid:32)), no ((cid:35)), and partial ((cid:71)(cid:35)), as shown
in Table I. Using an open coding approach, we deﬁned each
dimension’s levels based on analyzed exercises. That is, the
difference between a partial and a complete implementation
emerged from our exercise review. In most cases, “yes”
indicates frequent dimension implementation across challenges.
Conversely, “partial” indicates the dimension was implemented,
but only in one or two of challenges, and “no” means the
dimension was not implemented at all in the exercise. We
used this approach, rather than setting a challenge-percentage
threshold, in order to account for variation across dimensions.
We ensure consistency using a dual-coder agreement process,
described below. We give speciﬁc examples of dimension levels
in Section III when they differ from this general deﬁnition.
For each exercise, we performed a broad review of all
website components (e.g., FAQs, initial instructions, challenge
categories, forums, additional resource pages, etc.) to under-
stand the information students might view. Next, we completed
at least one logical exercise unit (e.g., all challenges in a
category or a single speciﬁed path through the exercise). If
no logical relationship was present, we completed challenges
until we reached saturation: when we observed no additional
pedagogical methods [33, pg. 113-115]. In all cases, we
completed at least ﬁve challenges.3 On average, we completed