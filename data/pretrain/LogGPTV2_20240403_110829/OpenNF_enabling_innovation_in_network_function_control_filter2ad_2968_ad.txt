copy calls. As a convenience, we do provide a function for control
applications to become aware of state updates:
void notify(ﬁlter,inst,enable,callback)
When invoked with enable set to true, the controller calls ena-
bleEvents(ﬁlter, process) on NF instance inst, otherwise it
calls disableEvents(ﬁlter) on inst. For each event the con-
troller receives, it invokes the provided callback function.
5.2.2 Share Operation
Strong and strict consistency are more difﬁcult to achieve be-
cause state reads and updates must occur at each NF instance in
the same global order. For strict consistency this global order must
match the order in which packets are received by sw. For strong
consistency the global order may differ from the order in which
packets were received by sw, but updates for packets received by a
speciﬁc NF instance must occur in the global order in the order the
instance received the packets.
Both cases require synchronizing reads/updates across all NF in-
stances (list) that are using a given piece of state. OpenNF’s
share operation provides this:
void share(list,ﬁlter,scope,consistency)
The ﬁlter and scope arguments are the same as above, while con-
sistency is set to strong or strict.
Events can again be used to keep state strongly consistent. The
controller calls enableEvents(ﬁlter,drop) on each instance,
followed by a sequence of get and put calls to initially synchronize
their state. When events arrive at the controller, they are placed in
a FIFO queue labeled with the ﬂowid for the ﬂow group to which
they pertain; ﬂows are grouped based on the coarsest granularity of
state being shared (e.g., per-host or per-preﬁx).
For each queue, one event at a time is dequeued, and the packet
it contains is marked with a “do-not-drop” ﬂag and forwarded to
the originating NF instance. The NF instance processes the packet
and raises an event, which signals to the controller that all state
reads/updates at the NF are complete. The controller then calls
getMultiflow (or getPerflow, getAllflows) on the orig-
inating NF instance, followed by putMultiflow (or putPer-
flow, putAllflows) on all other instances in list.
Then, the next event is dequeued and the process repeated.
Since events from different NFs may arrive at the controller in
a different order than packets were received by sw, we require a
slightly different approach for strict consistency. The controller
must receive packets directly from the switch to know the global
order in which packets should be processed. We therefore update
all relevant forwarding entries in sw—i.e., entries that both cover
a portion of the ﬂow space covered by ﬁlter and forward to an in-
stance in list—to forward to the controller instead. We
then employ the same methodology as above, except we invoke
enableEvents with action set to process and queue packets
received from sw rather than receiving packets via events.
It is up to control applications to determine the appropriate con-
sistency requirements for the situation, recognizing that strong or
strict consistency comes at a signiﬁcant performance cost (§8.1.1).
Applications should also consider which multi-/all-ﬂows state is re-
quired for accurate packet processing, and, generally, invoke copy
or share operations on this state prior to moving per-ﬂow state.
6. CONTROL APPLICATIONS
Using OpenNF, we have written control applications for several
of the scenarios described in §2. The applications are designed for
the environment shown in Figure 7. In all applications, we use the
Figure 7: The Bro IDS runs on VMs in both a local data center and a
public cloud. An SDN switch in the local data center receives a copy of
all trafﬁc from the Internet gateway for the local network and routes
it to an IDS instance. The local IDS instances monitor for port scans
and HTTP requests from outdated web browsers. The cloud instances
additionally check for malware in HTTP replies.
1 movePreﬁx (preﬁx, oldInst, newInst)
2
3
4
5
6
7
copy (oldInst, newInst, {nw_src: preﬁx}, MULTI)
move (oldInst, newInst, {nw_src: preﬁx}, PER, LOSSFREE)
while true do
sleep (60)
copy (oldInst, newInst, {nw_src: preﬁx}, MULTI)
copy (newInst, oldInst, {nw_src: preﬁx}, MULTI)
Figure 8: Load balanced network monitoring application
Bro IDS, but different applications place different requirements on
both the granularities of state operations and the guarantees needed;
despite these differences, the applications are relatively simple to
implement. We describe them below.
High performance network monitoring. The ﬁrst application
(Figure 8) monitors the CPU load on the local Bro IDS instances
and calculates a new distribution of local network preﬁxes when
load becomes imbalanced.
If a subnet is assigned to a different
IDS instance, the movePrefix function is invoked. This func-
tion calls copy to clone the multi-ﬂow state associated with scan
detection, followed by move to perform a loss-free transfer of the
per-ﬂow state for all active ﬂows in the subnet.
We copy, rather than move, multi-ﬂow state because the coun-
ters for port scan detection are maintained on the basis of hexternal
IP, destination porti pairs, and connections may exist between a
single external host and hosts in multiple local subnets. An order-
preserving move is unnecessary because re-ordering would only
potentially result in the scan detector failing to count some con-
nection attempts, and, in this application, we are willing to tolerate
moderate delay in scan detection. However, to avoid missing scans
completely, we maintain eventual consistency of multi-ﬂow state
by invoking copy in both directions every 60 seconds.
Fast failure recovery. The second application (Figure 9) maintains
a hot standby for each local IDS instance with an eventually consis-
tent copy of all per-ﬂow and multi-ﬂow state. The initStandby
function is invoked to initialize a standby (stbyInst) for an IDS
instance (normInst).
It notes which normInst the standby
is associated with and requests notiﬁcations from normInst for
packets whose corresponding state updates are important for scan
detection and browser identiﬁcation—TCP SYN, SYN+ACK, and
RST packets and HTTP packets sent from a local client to an exter-
nal server. The copy is made eventually consistent when these key
packets are processed, rather than recopying state for every packet.
In particular, events are raised by normInst for these packets and
the controller invokes the updateStandby function. This func-
tion copies the appropriate per-ﬂow state from normInst to the
corresponding stbyInst. When a failure occurs, the forwarding
table in the switch is updated to forward the appropriate preﬁxes to
stbyInst instead of normInst (code not shown).
Selectively invoking advanced remote processing. The third ap-
plication (code not shown) monitors for outdated browser alerts
from each local Bro IDS instance, and uses the cloud to check for
malware in connections triggering such alerts.
1701 standbys ← {}
2 initStandby (normInst, stbyInst)
3
4
standbys[normInst] ← stbyInst
notify ({nw_proto: TCP, tcp_ﬂags: SYN}, normInst, true,
updateStandby)
notify ({nw_proto: TCP, tcp_ﬂags: RST}, normInst, true,
updateStandby)
notify ({nw_src: 10.0.0.0/8, nw_proto: TCP, tp_dst: 80},
normInst, true, updateStandby)
5
6
7 updateStandby (event)
8
9
10
11
normInst ← event.src
stbyInst ← standbys[normInst]
ﬁlter ← extractFlowId (event.pkt)
copy (normInst, stbyInst, ﬁlter, PER)
Figure 9: Fast failure recovery application
When a local IDS instance (locInst) raises an alert for a spe-
the application calls move(locInst,
ciﬁc ﬂow (flowid),
cloudInst,flowid,perflow,lossfree) to transfer the
ﬂow’s per-ﬂow state and forward the ﬂow’s packets to the IDS in-
stance running in the cloud. The move must be loss-free to ensure
all data packets contained in the HTTP reply are received and in-
cluded in the md5sum that is compared against a malware database,
otherwise malware may go undetected. Multi-ﬂow state in this
case, i.e., the set of scan counters at the local IDS instance, does
not matter for the cloud instance’s actions (i.e., malware signature
detection), so it is not moved or copied.
7.
IMPLEMENTATION
Our OpenNF prototype consists of a controller that implements
our northbound API (§5) and several modiﬁed NFs—Bro, PRADS,
Squid, and iptables–that implement our southbound API (§4).
The OpenNF controller is written as a module atop Floodlight [6]
(≈4.7K lines of Java code). The controller listens for connections
from NFs and launches two threads—for handling state operations
and events—for each NF. The controller and NFs exchange JSON
messages to invoke southbound functions, provide function results,
and send events. Packets contained in events are forwarded to NFs
by issuing OpenFlow packet-out control messages [29] to the SDN
switch (sw); ﬂow-mod messages are issued for route updates. The
interface with control applications is event-driven.
We implemented NF-speciﬁc handlers for each southbound API
functions. The NFs use a shared library for communicating with
the controller. We discuss the NF-speciﬁc modiﬁcations below, and
evaluate the extent of these modiﬁcations in §8.2.2.
Bro IDS [31] performs a variety of security analyses deﬁned by
policy scripts. The get/putPerflow handlers for Bro lookup
(using linear search) and insert Connection objects into internal
hash tables for TCP, UDP, and ICMP connections. The key chal-
lenge is serializing these Connection objects and the many other
objects (>100 classes) they refer to; we wrote custom serialization
functions for each of these objects using Boost [2]. We also added a
moved ﬂag to some of these classes—to prevent Bro from logging
errors during delPerflow—and a mutex to the Connection
class—to prevent Bro from modifying the objects associated with
a ﬂow while they are being serialized. Lastly, we added library
calls to Bro’s main packet processing loop to raise events when a
received packet matches a ﬁlter on which events are enabled.
PRADS asset monitor [13] identiﬁes and logs basic information
about active hosts and the services they are running. The get/put-
Perflow and get/putMultiflow handlers for PRADS lookup
and insert connection and asset structures, which store ﬂow
meta data and end-host operating system and service details, re-
spectively, in the appropriate hash tables. If an asset object pro-
vided in a putMultiflow call is associated with the same end-
host as an asset object already in the hash table, then the handler
merges the contents of the two objects. The get/putAllflows
handlers copy and merge, respectively, a global statistics structure.
Squid caching proxy [15] reduces bandwidth consumption by
caching and serving web objects requested by clients. The per-
ﬂow state in Squid includes sockets, making it challenging to write
get/putPerflow handlers. Fortunately, we are able to borrow
code from CRIU [5] to (de)serialize sockets for active client and
server connections. As with Bro, we wrote custom serialization
functions, using Boost [2], for all objects associated with each con-
nection. The get/put/delMultiflow handlers capture, insert,
and remove entries from Squid’s in-memory cache; entries are
(de)serialized individually to allow for ﬁne-grained state control.
iptables [9] is a ﬁrewall and network address translator integrated
into the Linux kernel. The kernel tracks the 5-tuple, TCP state,
security marks, etc. for all active ﬂows; this state is read/written by
iptables. We wrote an agent that uses libnetﬁlter_conntrack [10] to
capture and insert this state when get/putPerflow are invoked.
There is no multi-ﬂow or all-ﬂows state in iptables.
8. EVALUATION
Our evaluation of OpenNF answers the following key questions:
• Can state be moved, copied, and shared efﬁciently even when
guarantees on state or state operations are requested by appli-
cations? What beneﬁts do applications see from the ability
to move, copy, or share state at varying granularities?
• How efﬁciently can NFs export and import state, and do these
operations impact NF performance? How much must NFs be
modiﬁed to support the southbound API?
• How is OpenNF’s efﬁciency impacted by the scale of an NF
deployment?
• To what extent do existing NF control planes hinder the abil-
ity to satisfy a combination of high-level objectives?
The testbed we used for our evaluation consists of an OpenFlow-
enabled HP ProCurve 6600 switch and four mid-range servers
(Quad-core Intel Xeon 2.8GHz, 8GB, 2 x 1Gbps NICs) that run
the OpenNF controller and modiﬁed NFs and generate trafﬁc. We
use a combination of replayed university-to-cloud [24] and data-
center [19] network trafﬁc traces, along with synthetic workloads.
8.1 Northbound Operations
8.1.1 Efﬁciency with Guarantees
We ﬁrst evaluate the efﬁciency of our northbound operations
when guarantees are requested on state or state operations. We use
two PRADS asset monitor instances (P RADS1 and P RADS2)
and replay our university-to-cloud trace at 2500 packets/second.
We initially send all trafﬁc to P RADS1. Once it has created
state for 500 ﬂows (≈80K packets have been processed), we move
all ﬂows and their per-ﬂow state, or copy all multi-ﬂow state, to
P RADS2; we evaluate ﬁner granularity operations in §8.1.2. To
evaluate sharing with strong consistency, we instead call share
(for all multi-ﬂow state) at the beginning of the experiment, and
then replay our trafﬁc trace. During these operations, we measure
the number of dropped packets, the added latency for packets con-
tained in events from P RADS1 or buffered at P RADS2, and the
total operation time (for move and copy only). Although the spe-
ciﬁc values for these metrics vary based on the NF, scope, ﬁlter
granularity (i.e., number of ﬂows/states affected), and packet rate,
the high-level takeaways still apply.
171 500
)
s
m
 400
 300
(
e
m
T
i
l
 200
a
 100
t
o
T
NG
NG PL
LF PL
LF PL+ER
LF+OP PL+ER