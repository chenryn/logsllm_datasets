(A1) and of applicationF
aggregate features indicate how the user responded to previous
prompts associated with that combination. As expected, after
5The application running in the foreground when the permission is re-
quested by another application.
1084
Feature Set Contextuals Defaulters Overall
R1
95.80% 83.71%
95.92% 83.93%
R2 + B
R2 + A
99.20% 92.24%
69.30%
69.48%
75.45%
THE MEDIAN ACCURACY OF THE MACHINE LEARNING MODEL FOR
DIFFERENT FEATURE GROUPS ACROSS DIFFERENT SUB POPULATIONS.
TABLE V
we introduced the aggregate features, the relative importance
of the user ID variable diminished and so we removed it (i.e.,
users no longer needed to be uniquely identiﬁed). We deﬁne
R2 as R1 without the user ID.
B. Inference Based on Behavior
One of our main hypotheses is that passively observing
users’ behaviors helps infer users’ future privacy decisions.
To this end, we instrumented Android to collect a wide
array of behavioral data, listed in Table II. We categorize
our behavioral instrumentation into interaction with Android
privacy/security settings, locking behavior, audio settings and
call habits, web-browsing habits, and application usage habits.
After the feature selection process (§VI-A), we found that
only locking behavior, audio habits, and web-browsing habits
correlated with privacy behaviors. Appendix B contains more
information on feature importance. All the numerical values
under the behavioral group were normalized per day.
We trained an SVM model with an RBF kernel on only the
behavioral and runtime features listed in Table IV, excluding
user ID. The 5-fold cross-validation accuracy (with random
splitting) was 83% across all users. This ﬁrst setup assumes we
have prior knowledge of previous privacy decisions to a certain
extent from each user before inferring their future privacy
decisions, so it is primarily relevant after the user has been
using their phone for a while. However, the biggest advantage
of using behavioral data is that it can be observed passively
without any active user involvement (i.e., no prompting).
We use leave-one-out cross validation to measure the extent
to which we can infer user privacy decisions with absolutely
no user involvement (and without any prior data on a user). In
this second setup, when a new user starts using a smartphone,
we assume there is a ML model which is already trained
with behavioral data and privacy decisions collected from a
selected set of other users. We then measured the efﬁcacy
of such a model to predict the privacy decisions of a new
user, purely based on passively observed behavior and runtime
information on the request, without ever prompting that new
user. This is an even stricter lower bound on user involvement,
which essentially mandates that a user has to make no effort
to indicate privacy preferences, something that no system
currently does.
We performed leave-one-out cross validation for each of
our 131 participants, meaning we predicted a single user’s
privacy decisions using a model trained using the data from
the other 130 users’ privacy decisions and behavioral data.
The only input for each test user was the passively observed
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
behavioral data and runtime data surrounding each request.
The model yielded a median accuracy of 75%, which is a 3×
improvement over AOI. Furthermore, AOI requires users to
make active decisions during the installation of an application,
which our second model does not require.
Examining only behavioral data with leave-one-group-out
cross validation yielded a median accuracy of 56% for Contex-
tuals, while for Defaulters it was 93.01%. Although, prediction
using solely behavioral data fell short of AOFU-AP for Con-
textuals, it yielded a similar median accuracy for Defaulters;
AOFU-AP required 12 prompts to reach this level of accuracy,
whereas our model would not have resulted in any prompts.
This relative success presents the signiﬁcant observation that
behavioral features, observed passively without user involve-
ment, are useful in learning user privacy preferences. This
provides the potential to open entirely new avenues of user
learning and reduce the risk of habituation.
C. Inference Based on Contextual Cues
Our SVM model with an RBF kernel produced the best
accuracy. The results in the remainder of this section are
trained and tested with ﬁve-fold cross validation with random
splitting for an SVM model with an RBF kernel using the ksvm
library in R. In all instances, the training set was bootstrapped
with an equal number of allow and deny data points to
avoid training a biased model. For each feature group, all
hyperparameters were tuned through grid search to achieve
highest accuracy. We used one-hot encoding for categorical
variables. We normalized numerical variables by making each
one a z-score relative to its own average. Table V shows how
the median accuracy changes with different feature groups.
As a minor note, the addition of the mentioned behavioral
features to runtime features performed only marginally better;
this could be due to the fact that those two groups do not
complement each other in predictions. In this setup, we assume
that there is a single model across all the users of Android.
By incorporating user involvement in the form of prompts,
we can use our aggregate features to increase the accuracy
for Contextuals, slightly less so for Defaulters. The aggregate
features primarily capture how consistent users are for particu-
lar combinations (i.e., application:permission:visibility, appli-
cationF :permission:visibility), which greatly affects accuracy
for Contextuals. Defaulters have high accuracy with just run-
time features (R1), as they are likely to stick with a default
allow or deny policy regardless of the context surrounding a
permission. Thus, even without any aggregate features (which
do not impart any new information about this type of user),
the model can predict privacy preferences of Defaulters with
a high degree of accuracy. On the other hand, Contextuals
are more likely to vary their decision for a given permission
request. However, as the accuracy numbers in Table V suggest,
this variance is correlated with some contextual cues. The high
predictive power of aggregate features indicates that they may
be capturing the contextual cues, used by Contextuals to make
decisions, to a greater extent.
The fact
that both application:permission:visibility and
applicationF :permission:visibility are highly predictive (Ap-
pendix A) indicates that user responses for these combina-
tions are consistent. The high consistency could relate to
the notion that the visibility and the foreground application
6) are strong contextual cues people use to make
(applicationF
their privacy decisions; the only previously studied contextual
cue was the visibility of the application requesting the sensitive
data [43]. We offer a hypothesis for why foreground appli-
cation could be signiﬁcant: the sensitivity of the foreground
application (i.e., high-sensitivity applications like banking,
low-sensitivity applications like games) might
impact how
users perceive threats posed by requests. Irrespective of the
application requesting the data, users may be likely to deny
the request because of the elevated sense of risk. We discuss
this further in §IX.
The model trained on feature sets R2, A1, and A2 had
the best accuracy (and the fewest privacy violations). For the
remainder of the paper, we will refer to this model unless
otherwise noted. We now compare AOFU-AP (the status quo
as of Android 6.0 and above, presented in Table III) and our
model (Table V). Across all users, our model reduced the error
rate from 15.38% to 7.76%, nearly a two-fold improvement.
Mispredictions (errors) in the ML model were split between
privacy violations and functionality losses (54% and 46%).
Deciding which error type is more acceptable is subjective
and depends on factors like the usability issues surrounding
functionality losses and gravity of privacy violations. However,
the (approximately) even split between the two error types
shows that the ML is not biased towards one particular deci-
sion (denying vs. allowing a request). Furthermore, the area
under the ROC curve (AUC), a metric used to measure the
fairness of a classiﬁer, is also signiﬁcantly better in the ML
model (0.936 as opposed to 0.796 for AOFU). This indicates
that the ML model is equally good at predicting when to
both allow and deny a permission request, while AOFU tends
to lean more towards one decision. In particular, with the
AOFU policy, users would experience privacy violations for
10.01% of decisions, compared to just 4.2% with the ML
model. Privacy violations are likely more costly to the user
than functionality loss: denied data can always be granted at
a later time, but disclosed data cannot be taken back.
While increasing the number of prompts improves classiﬁer
accuracy, it plateaus after reaching its maximum accuracy, at
a point we call the steady state. For some users, the classiﬁer
might not be able to infer their privacy preferences effectively,
regardless of the number of prompts. As a metric to measure
the effectiveness of the ML model, we measure the conﬁdence
of the model in the decisions it makes, based on prediction
class probabilities.7 In cases where the conﬁdence of the model
6Even when the requesting application is running visible to the user, the
foreground application could still be different from the requesting application
since the only visible cue of the requesting application could be a notiﬁcation
in the notiﬁcation bar.
7To calculate the class probabilities, we used the KSVM library in R. It
employs a technique proposed by Platt et al. [25] to produce a numerical
value for each class’s probability.
1085
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
from a single user. The training set contains all the prompt re-
sponses from 130 users and the test set is the prompt responses
collected from the single remaining user. The model had a
median accuracy of 66.6% (56.2% for Contextuals, 86.4%
for Defaulters). Although this approach does not prompt new
users, it falls short of AOFU. This no-prompt model behaves
close to random guessing for Contextuals and signiﬁcantly
better for Defaulters. Furthermore, Wijesekera et al. found that
individuals’ privacy preferences varied a lot [43], suggesting
that utilizing other users’ decisions to predict decisions for a
new user has limited effectiveness, especially for Contextuals;
some level of prompting is necessary.
There are a few interesting avenues to explore when
determining the optimal way to prompt
the user in the
learning phase. One option would be to follow the same
weighted-reservoir sampling algorithm mentioned in §III-A.
The algorithm is weighted by the frequency of each appli-
cation:permission:visibility combination. The most frequent
combination will have the highest probability of creating a
permission prompt and after the given combination reaches
a maximum of three prompts, the algorithm will no longer
consider that combination for prompting, giving the second
most frequent combination the new highest probability. Due
to frequency-weighting and multiple prompts per combina-
tion, the weighted-reservoir sampling approach requires more
prompts to cover a broader set of combinations. However,
AOFU prompts only once per combination without frequency-
weighting. This may be a useful strategy initially for a new
user since it allows the platform to learn about the users’
privacy preferences for a wide array of combinations with
minimal user interaction.
To simulate such an approach, we extend the aforemen-
tioned no-prompt model (leave-one-out validation). In the no-
prompt model, there was no overlap of users in the train and
test set. In the new approach, the training set includes the
data from other users as well as the new user’s responses to
the ﬁrst occurrence of each unique combination of applica-
tion:permission:visibility. The ﬁrst occurrence of each unique
combination simulates the AOFU-APV policy. That is, this
model is bootstrapped using data from other users and then
adopts the AOFU-APV policy to further learn the current
user’s preferences. The experiment was conducted using the
same set of features mentioned in §VI-A (R2 + A1 + A2 and
an SVM with a RBF kernel). The test set only contained
prompt responses collected after the last AOFU prompt to
ensure chronological consistency.
is below a certain threshold, the system should use a runtime
prompt to ask the user to make an explicit decision. Thus,
we looked into the prevalence of low-conﬁdence predictions
among the current predictions. With a 95% conﬁdence inter-
val, on average across ﬁve folds, low-conﬁdence predictions
accounted for less than 10% of all predictions. The remaining
high-conﬁdence predictions (90% of all predictions) had an
average accuracy of 96.2%, whereas predictions with low
conﬁdence were only predicted with an average accuracy of
72%. §VII-B goes into this aspect in detail and estimates the
rate at which users will see prompts in steady state.
The caveat in our ML model is that AOFU-AP only re-
sulted in 12 prompts on average per user during the study,
while our model averaged 24. The increased prompting stems
from multiple prompts for the same combination of appli-
cation:permission:visibility, whereas in AOFU, prompts are
shown only once for each application:permission combination.
During the study period, users on average saw 2.28 prompts
per unique combination. While multiple prompts per combi-
nation help the ML model to capture user preferences under
different contextual circumstances, it risks habituation, which
may eventually reduce the reliability of the user responses.
The evaluation setup mentioned in the current section does
not have a speciﬁc strategy to select
the training set. It
randomly splits the data set into the 5 folds and picks 4 out
of 5 as the training set. In a real-world setup, the platform
needs a strategy to carefully select the training set so that the
platform can learn most of the user’s privacy preferences with
a minimum number of prompts. The next section presents an
in-depth analysis on possible ways to reduce the number of
prompts needed to train the ML model.
VII. LEARNING STRATEGY
This sections presents a strategy the platform can follow
in the learning phase of a new user. The key objective of
the learning strategy should be to learn the user’s privacy
preferences with minimal user involvement (prompts). Once
the model reaches adequate training, we can use model deci-
sion conﬁdence to analyze how the ML model performs for
different users and examine the tradeoff between user involve-
ment and accuracy. We also utilize the model’s conﬁdence on
decisions to present a strategy that can further reduce model
error through selective permission prompting.
A. Bootstrapping
The bootstrapping phase occurs when the ML model is
presented with a new user about whom the model has no
prior information. In this section, we analyze how the accuracy
improves as we prompt the user. Since the model presented
in §VI is a single model trained with data from all users, the
ML model can still predict a new user’s privacy decisions by
leveraging the data collected on other users’ preferences.
We measured the accuracy of the ML model as if it had
to predict each user’s prompt responses using a model trained
using other users’ data. Formally, this is called leave-one-out
cross-validation, where we remove all the prompt responses
Figure 3 shows how accuracy changes with the varying
number of AOFU prompts for Contextuals and Defaulters.
For each of the 131 users, we ran the experiment varying
the AOFU prompts from 1 to 12. We chose this upper bound
because, on average, a participant saw 12 different unique ap-
plication:permission combinations during the study period—
the current permission model in Android. AOFU relies on user
prompts for each new combination. The proposed ML model,
however, has the advantage of leveraging data collected from
other users to predict a combination not seen by the user; it can
1086
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
1.0
0.9
0.8
y
c
a
r
u
c