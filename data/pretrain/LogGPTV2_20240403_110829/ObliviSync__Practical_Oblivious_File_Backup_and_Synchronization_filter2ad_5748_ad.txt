would require potentially changing many ﬁletable B-tree nodes
at each step, something that should be avoided as writes are
expensive.
Instead, we take a different approach in order to address
the fragmentation problem and minimize the block-id updates
for the non-stale fragments at the same time. Our approach
has two stages: the placement of non-stale fragments and then
clearing the fragments in the buffer.
Placement of non-stale fragments. We use the following rule
when addressing the existing non-stale fragments.
Non-stale full-block fragments stay as they are, but
non-stale small fragments may move to the other
block in the same backend ﬁle.
Recall that blocks are paired in order and share a single
backend ﬁle, and so this ﬂexibility enables small fragments
to be re-packed across two blocks to reduce fragmentation
without having to update the block-id value. Further,
this
solution also avoids a “full-block starvation” issue in which
all blocks contained just a small split-block fragment. After
re-packing, the small fragments in each block pair may be
combined into a single split block, leaving the other block
in the pair empty and ready to store full block fragments
from the buffer. In other words, the re-pack procedure ensures
that existing full-block fragments do not move, but existing
small-block fragments are packed efﬁciently within one of the
blocks in a pair to leave (potentially) more fully empty blocks
available to be rewritten.
Pushing fresh fragments. At this point, the randomly-chosen
blocks are each either: (a) empty, (b) ﬁlled with an existing
full-block fragment, or (c) partially ﬁlled with some small frag-
ments. The block re-packing ﬁrst considers any directory ﬁle
fragments in the buffer, followed by any regular ﬁle fragments,
each in FIFO order. (Giving priority status to directory ﬁles
is important to maintain low latency, as discussed in the next
section.) The synchronization process then proceeds the same
for all fragments in the buffer: for each fragment, it tries to
pack it into the randomly-selected blocks as follows:
available empty block (case (a)), if any remain.
• If it is a full-block fragment, it is placed in the ﬁrst
• If it is a small fragment, it is placed if possible in the ﬁrst
available split block (case (c)) where there is sufﬁcient
room.
• If it is a small fragment but it cannot ﬁt in any existing
split block, then the ﬁrst available empty block (case (a)),
if any, is initialized as a new split block containing just
that fragment.
In this way, every buffer fragment is considered for re-
packing in order of age, but not all may actually be re-packed.
Those that are re-packed will be removed from the buffer and
their ﬁletable entries will be updated according to the chosen
block’s block-id.
A key observation that will be important in our runtime
proof later is that after re-packing, either (1) the buffer is
completely cleared, or (2) all the chosen blocks are nonempty.
Step 4: Re-encrypting the chosen backend ﬁles. After
the re-packing is complete, the sync procedure re-encrypts
the superblock (which always goes at index 0), as well as
all the re-packed blocks, and stages them for writing back to
backend ﬁles. The actual writing is done all at once, on the
timer, immediately before the next sync operations, so as not
to reveal how long each sync procedure took to complete.
F. Frontend FUSE Mounts
The FUSE (ﬁle system in user space) mounts are the
primary entry point for all user applications. FUSE enables
the capture of system calls associated with I/O, and for those
calls to be handled by an identiﬁed process. The result is
that a generic ﬁle system mount is presented to the user,
but all accesses to that ﬁle system are handled by either the
ObliviSync-RW or ObliviSync-RO client that is running in the
background.
The key operations that are captured by the FUSE mount
and translated into ObliviSync-RW or ObliviSync-RO calls are
as follows:
• create(f ilename): create a new (empty) ﬁle in the system
in two steps. First a new ﬁle-id is chosen, and the
corresponding ﬁle-entry is added to the ﬁletable. Then
that ﬁle-id is also stored within the parent directory ﬁle.
• delete(f ilename): remove a ﬁle from the system by
removing it from the current directory ﬁle and removing
the associated ﬁle-entry from the ﬁletable.
• read(f ilename, of f set, length) → data : read data
from the identiﬁed ﬁle by looking up its ﬁle-id in the cur-
rent directory and requesting the backend ObliviSync-RW
or ObliviSync-RO to perform a read operation over the
appropriate blocks.
8
• write(f ilename, of f set, length) : write data to the iden-
tiﬁed ﬁle by looking up its ﬁle-id in the current directory
and then adding the corresponding fragment(s) to the
ObliviSync-RW’s buffer for eventual syncing.
• resize(f ilename, size) : change the size of a ﬁle by
looking up its ﬁle-entry in the current directory and
changing the associated metadata. This may also add or
remove entries from the corresponding block-id list if the
given size represents a change in the number of blocks
for that ﬁle. Any added blocks will have negative block-id
values to indicate that the data is not yet available.
Of course, there are more system calls for ﬁles than these,
such as open() or stat() that are implemented within
the FUSE mount, but these functions succinctly encompass
all major operations between the frontend FUSE mount and
backend ﬁle system maintenance.
As noted before, the FUSE mount also maintains the ﬁle
system’s directory structure whose main purpose is to link ﬁle
names to their ﬁle-id values, as well as store other expected
ﬁle statistics. The directory ﬁles are themselves treated just
like any other ﬁle, except that (1) the root directory always
has ﬁle-id 0 so it can be found on a fresh (re-)mount, and (2)
directory ﬁles are given priority when syncing to the backend.
For a ﬁle to become available to the user, it must both be
present in the backend and have an entry in the directory ﬁle.
Without prioritization of directory ﬁles, it may be the case that
some ﬁle is available without the directory entry update, thus
delaying access to the user. Conversely, the opposite can also
be true: the directory entry shows a ﬁle that is not completely
synchronized. Fortunately, this situation is easy to detect upon
open() and an IO-error can be returned which should be
handled already by application making the system call.
The FUSE module is aware of some backend settings to
improve performance, notably the block size. When a ﬁle is
modiﬁed, it is tracked by the FUSE module, but for large
ﬁles, with knowledge of the block size, the FUSE module
can identify which full fragments of that ﬁle are modiﬁed
and which remain unchanged. Only the fragments with actual
changes are inserted into the buffer to be re-packed and synced
to the backend.
G. Key parameter settings
The tunable parameters for a ObliviSync implementation
consist of:
• B: the size of each backend ﬁle (i.e., block pair)
• N: the total number of backend ﬁles
• n: the total number of frontend ﬁles (i.e., logical ﬁles)
• t: the drip time
• k: the drip rate
Backend ﬁles. The ﬁrst two parameters B and N depend
on the backend cloud service. A typical example of such
parameters can be taken from the popular Dropbox service,
which optimally handles data in ﬁles of size 4MB, so that
B = 222 [11], and the maximal total storage for a paid
“Dropbox Pro” account is 1TB, meaning N = 218 ﬁles would
be the limit. Note that, as our construction always writes blocks
in pairs, each block pair is stored in a single ﬁle and the block
size in ObliviSync will be B/2.
9
Frontend ﬁles. The next parameter n is not really a parameter
per se but rather a limitation, as our construction requires n ≤
B2 in order to ensure the ﬁletable’s B-tree has height at most
1. For B = 222, this means the user is “limited” to roughly
16 trillion ﬁles.
Drip time and drip rate. The drip time and drip rate are
important parameters for the buffer syncing. The drip time is
the length of the epoch, i.e., the time between two consecutive
syncs to the backend. The drip rate refers to how many block
pairs are randomly selected for rewriting on each epoch.
These two parameters provide a trade-off between latency
and throughput. Given a ﬁxed bandwidth limitation of, say, x
bytes per second, (k+1)B bytes will be written every t seconds
for k randomly chosen backend ﬁles and the superblock, so
that we must have (k + 1)B/t ≤ x. Increasing the drip time
and drip rate will increase latency (the delay between a write in
the ObliviSync-RW appearing to ObliviSync-RO clients), but
it will increase throughput as the constant overhead of syncing
the superblock happens less frequently.
We will consider in our experimentation section (see Sec-
tion VI) the throughput, latency, and buffer size of the system
under various drip rate and drip time choices. Our experiment
indicates that for most uses, the smallest possible drip time
t that allows a drip rate of k ≥ 3 ﬁles per epoch should be
chosen.
V. ANALYSIS
A. Time to write all ﬁles
In this subsection we will prove the main Theorem 1 that
shows the relationship between the number of sync operations,
the drip rate, and the size of the buffer. Recall from the
preceding subsection the parameters B (block pair size), N
(number of backend block pairs), and k (drip rate). Speciﬁcally,
we will show that, with high probability, a buffer with size s
is completely cleared and synced to the backend after O(cid:0) s
Bk
sync operations. This is optimal up to constant factors, since
only Bk bytes are actually written during each sync.
Theorem 1. For a running ObliviSync-RW client with param-
eters B, N, k as above, let m be the total size (in bytes) of
all non-stale data currently stored in the backend, and let s
be the total size (in bytes) of pending write operations in the
buffer, and suppose that m + s ≤ N B/4.
(cid:1)
Then the expected number of sync operations until the
buffer is entirely cleared is at most 4s/(Bk).
Moreover, the probability that the buffer is not entirely
Bk + 18r sync operations is at most
cleared after at least 48s
exp(−r).
Before giving the proof, let us summarize what the this
theorem means speciﬁcally.
First,
the condition m + s ≤ N B/4 means that
the
guarantees hold only when at most 25% of the total backend
capacity is utilized. For example, if using Dropbox with 1TB
of available storage, the user should store at most 250GB
of ﬁles in the frontend ﬁlesystem in order to guarantee the
performance speciﬁed in Theorem 1.
Second, as mentioned already, the expected number of sync
operations is optimal (up to constant factors), as the total
amount of data written in the frontend cannot possibly exceed
the amount of data being written to the backend.
In the number of syncs 48s/(Bk) + 18r required to clear
the buffer with high probability, one can think of the parameter
r as the number of “extra” sync operations required to be
very sure that the buffer is cleared. In practice, r will be
set proportionally to the security parameter. A beneﬁt of our
construction compared to many other ORAM schemes is that
the performance degradation in terms of the security parameter
is additive and not multiplicative. Put another way, if it takes
1 extra minute of syncing, after all operations are complete,
in order to ensure high security, that extra minute is ﬁxed
regardless of how long the ObliviSync-RW has been running
or how much total data has been written.
Finally, a key observation of this theorem is that it does not
depend on the distribution of ﬁle sizes stored in the frontend
ﬁlesystem, or their access patterns, but only the total size of
data being stored. The performance guarantees of our system
therefore allow arbitrary workloads by the user, provided they
can tolerate a constant-factor increase in the backend storage
size.
We now proceed with the proof of Theorem 1.
Proof: There are N blocks of backend storage. Each
stores some combination of at most two split blocks and full
blocks. Full blocks have size B
2 each, and split blocks contain
multiple fragments summing to size at most B
2 each.
Suppose some sync operation occurs (selecting k block
pairs from the backend, removing stale data and re-packing
with new fragments from the buffer), and afterwards the buffer
is still not empty. Then it must be that case that the k block
pairs that were written are at least half ﬁlled, i.e., their total
2 . The reason is, if any block pair had
size is now at least kB
size less than B
2 , then it could have ﬁt something more (either
a full block or a fragment) from the buffer. But since the buffer
was not emptied, there were no entirely empty blocks among
the k block pairs.
Furthermore, because m 
3kB
8
≥ 1 − kB/4
3kB/8