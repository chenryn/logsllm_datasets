data center structures: 1) the nodes in the topologies typically have
regular degrees. For example, we show the degree patterns for sev-
eral well-known data center networks in Table 1; 2) the graphs are
sparse, so that our O2 can quickly determine if two graphs are iso-
morphic. These properties are important for us to detect malfunctions
in data centers. In DAC, the ﬁrst property is used to detect malfunc-
tioning devices where there are node degree changes, and the second
one serves as a tool in our malfunction detection scheme for the case
where no degree change occurs.
4.3.1 Malfunction with Node Degree Change
For the aforementioned three types of malfunctions, we discuss
them one by one as follows. Our observation is that most of the cases
may cause the change of degree on devices.
44/* pre-compute the SPLDs of all nodes in Gb, select one node from each
group of nodes with same SPLDs and store it in Cb */
(Ap; Ab) = Anchor_Pair_Selection(Gp; Gb)
1 Ap = a group of selected anchor points in Gp;
2 foreach v ∈ Ap
select a v
3
store v=v
4
Malfunction_Detection(Gp; Gb; Ap; Ab)
5 Deﬁne Sx
′ ∈ Cb that minimizes ∥SPLD(v) − SPLD(v
′ in Ap=Ab;
from node v where v ∈ Gp, and the same as Sx
b (v
);
′
6 foreach pair of nodes v=v
7
′ ∈ Ap=Ab
′)∥;
p (v) as maximal subgraph of Gp with maximal hop length x
′
use binary search to ﬁnd a value x that satisﬁes O2_Mapping(Sx
Sx
b (v
foreach node i ∈ Gp that is x-hop or (x + 1)-hop away from v
)) = true and O2_Mapping(Sx+1
p (v);
)) = f alse;
(v); Sx+1
(v
p
′
b
8
9
10 return a node list sorted by their counter values;
counter(i)=counter(i)+1;
Figure 8: Pseudocode for malfunction detection.
b
′
′
′
p
(v
p (v) in line 5, for each anchor pair v=v
Figure 8. Speciﬁcally, given Gp=Gb, Ap=Ab and deﬁnition of max-
′ ∈ Ap=Ab,
imal subgraph Sx
we search the maximal isomorphic subgraph of graphs Gp=Gb with
hop length x from nodes v=v
respectively. The process to obtain
such subgraph is in line 7. We can use binary search to accelerate
the searching procedure. If we ﬁnd that Sx
) are iso-
morphic while Sx+1
) are not, we assume some mis-
wirings happened between x-hop and (x + 1)-hop away from v and
the nodes in these two hops are suspicious. In line 9, we increase a
counter for each of these nodes to represent this conclusion.
p (v) and Sx
(v) and Sx+1
b (v
After ﬁnishing the detection from all the anchor points, we report a
list to the administrator. The list contains node device IDs and counter
values of each node, ranked in the descending order of the counter
values. Essentially, the larger its counter value, the more likely the
device is miswired. Then the administrator will go through the list
and rectify the miswirings. This process stops when he ﬁnds a node
is not really miswired and ignores the rest of nodes on the list.
The accuracy of our scheme depends on the number of anchor
points we selected for detection versus the number of miswirings in
the network. Our experiments suggest that, with a sufﬁcient number
of anchor points, our algorithm can always ﬁnd all the malfunctions
(i.e., put the miswired devices on top of the output list). According to
the experimental results in Section 6.4, with at most 1:5% of nodes
selected as anchor points we can detect all miswirings on the eval-
uated structures. To be more reliable, we can always conservatively
select a larger percentage of anchor points to start our detection and
most likely we will detect all miswirings (i.e., have all of them on
top of the list). Actually, this can be facilitated by the parallel com-
puting because in our malfunction detection, the calculations from
different anchor points are independent of each other and thus can be
performed in parallel.
After ﬁxing the miswirings, we will run O2 to get the device-to-
logical ID mapping again. Even in the case that not all the miswirings
are on the top of the list and we miss some, O2 will perceive that
quickly. Then we will re-run our detection algorithm until all mis-
wirings are detected and rectiﬁed, and O2 can get the correct device-
to-logical ID mapping ﬁnally.
4.4 Device Locating
Given a detected malfunctioning device, the next practical question
is how to identify the location of the device given only its device ID
(i.e., MAC). In fact, the device locating procedure is not necessar-
ily achieved by an autoconﬁguration algorithm, but also possibly by
some human efforts. In this paper, we argue that it is a practical de-
ployment and maintenance problem in data centers, and thus we seek
a scheme to collect such location information automatically.
Our idea is to sequentially turn on the power of each rack in or-
der to generate a record for the location information. This procedure
Figure 7: Miswirings with and without degree change.
• Node.
If there is a malfunctioning node, the degrees of its
neighboring nodes are decreased by one, and thus it is possi-
ble to identify the malfunction by checking its neighbor nodes.
• Link. If there is a malfunctioning link, the degrees of associ-
ated nodes are decreased by one, making it possible to detect.
• Miswiring. Miswirings are somewhat more complex than the
other two errors. As shown in the left of Figure 7, the mis-
wiring causes its related nodes to increase or decrease their de-
grees and can be detected readily. On the contrary, in the right
of Figure 7, the miswirings of a pair of cables occur coinciden-
tally so that the degree change caused by one miswired cable is
glossed over by another, and thus no node degree change hap-
pens. We discuss this hardest case separately in the following.
Note that for any malfunction caused by the links, i.e., link failure
or miswirings, we report the associated nodes (i.e., malfunctioning
devices) in our malfunction detection.
4.3.2 Malfunction without Node Degree Change
Though in most cases the malfunctions cause detectable node de-
gree change [24], it is still possible to have miswirings with no node
degree change. This case occurs after an administrator has checked
the network and the degree-changing malfunctions have been ﬁxed.
The practical assumptions here are: 1) the number of nodes involved
in such malfunctions is a considerably small amount over all the
nodes; 2) Gp and Gb have the same number of nodes as well as node
degree patterns.
Despite the miswirings, the vast majority part of Gp and Gb are still
the same. We leverage this fact to detect such miswirings. Our basic
idea is that we ﬁrst ﬁnd some nodes that are supposed to be symmetric
between Gp and Gb, then use those nodes as anchor points to check
if the subgraphs deduced from them are isomorphic. Through this we
derive the difference between the two graphs, and correlate the mal-
functioning candidates derived from different anchor points to make a
decision. Basically, our scheme has two parts: anchor point selection
and malfunction detection.
To minimize the human intervention, the ﬁrst challenge is selecting
anchor pairs between the blueprint graph Gb and the physical topol-
ogy graph Gp without human input. Our idea is again to leverage the
SPLD. Considering that the number of nodes involved in miswirings
is small, it is likely that two “symmetric” nodes in two graphs will still
have similar SPLDs. Based on this, we design our heuristics to select
anchor pair points, which is Anchor_Pair_Selection() in Figure 8. In
)∥ is simply the Euclidean dis-
the algorithm, ∥SPLD(v) − SPLD(v
tance. Given that two node with similar SPLDs are not necessarily
a truly symmetric pair, our malfunction detection scheme will take
the potential false positives into account, and handle this issue via
majority voting.
Once the anchor node pairs have been selected, we compare Gb
and Gp from these anchor node pairs and correlate malfunctions via
majority voting. The algorithm for this is Malfunction_Detection() in
′
Miswiring with degree changeDegree changeMiswiring without degree change45is performed only once and the generated record is used by the ad-
ministrator to ﬁnd a mapping between MAC and rack. It works as
follows: 1) To power on the data center for the ﬁrst time, the admin-
istrator turns on the power of server racks one by one sequentially.
We require a time interval between powering each rack so we can dif-
ferentiate devices in different racks. The time interval is a tradeoff:
larger values allow easier rack differentiation while smaller values re-
duce boot time cost on all racks. We think by default it should be 10
seconds. 2) In the physical topology collection stage, when reporting
the topology information to DAC manager, each device also piggy-
backs the boot-up time, from when it had been powered on to its ﬁrst
reporting. 3) When receiving such boot-up time information, DAC
manager groups the devices with similar boot-up times (compared to
the power on time interval between racks). 4) When DAC manager
outputs a malfunctioning device, it also outputs the boot-up time for
that group. Therefore, the administrator can check the rack physical
position accordingly.
To summarize, our malfunction detection and locating designs fo-
cus on how to quickly detect and locate various malfunctions includ-
ing the most difﬁcult miswiring cases. We note that our schemes help
to identify malfunctions, but not repair them. It is our hope that the
detection procedure can help administrators to ﬁx any malfunction
more rapidly during the autoconﬁguration stage.
5.
IMPLEMENTATION AND EXPERIMENT
In this section, we ﬁrst introduce the protocols that are used to do
physical topology collection and logical ID dissemination. Then, we
describe our implementation of DAC.
5.1 Communication Protocols
To achieve reliable physical topology collection and logical ID dis-
semination between all devices and DAC manager, we need a com-
munication channel over the network. We note that the classical span-
ning tree protocol (STP) does not ﬁt our scenario: 1) we have a ﬁxed
root - DAC manager, so network-wide broadcast for root selection is
not necessary; 2) the scale of data center networks can be hundreds
of thousands, making it difﬁcult to guarantee reliability and informa-
tion correctness in the network-wide broadcast. Therefore, we pro-
vide a Communication channel Building Protocol (CBP) to set up a
communication channel over a mega-data center network. Moreover,
we introduce two protocols, namely the Physical topology Collection
Protocol (PCP) and the Logical ID Dissemination Protocol (LDP),
to perform the topology information collection and ID dissemination
over that spanning tree built by CBP.
Building communication channel. In CBP, each network device
sends Channel Building Messages (CBMs) periodically (with a time-
out interval T ), to all of its interfaces. Neighbor nodes are discovered
by receiving CBMs. Each node sends its own CBMs, and does not
relay CBMs received from other nodes. To speed up the information
propagation procedure, a node also sends out a CBM if it observes
changes in neighbor information. A checking interval (c-int) is intro-
duced to reduce the number of CBM messages by limiting the mini-
mal interval between two successive CBMs.
DAC manager sends out its CBM with its level marked as 0, and
its neighbor nodes correspondingly set their levels to 1. This proce-
dure continues until all nodes get their respective levels, representing
the number of hops from that node to DAC manager. A node ran-
domly selects a neighbor node as its parent if that node has the lowest
level among its neighbors, and claims itself as that node’s child by
its next CBM. The communication channel building procedure is ﬁn-
ished once every node has its level and has selected its parent node.
Therefore, the built communication channel is essentially a layered
spanning tree, rooted at DAC manager. We deﬁne a leaf node as one
Figure 9: The testbed topology and blueprint.
that has the largest level among its neighbors and no children node. If
a leaf node observes no neighbor updates for a timeout value, 3 ∗ T ,
it enters the next stage, physical topology information collection.
Physical topology collection and logical ID dissemination. Once
the communication channel has been built by CBP, the physical topol-
ogy collection and logical ID dissemination over the communication
channel can be performed by using PCP and LDP. Essentially, the
topology collection is a bottom-up process that starts from leaf de-
vices and blooms up to DAC manager, while the logical ID dissemi-
nation is a top-down style that initiates from DAC manager and ﬂows
down to the leaf devices.
In PCP, each node reports its node device ID and all its neighbors
to its parent node. After receiving all information from its children,
an intermediate node merges them (including its own neighbor in-
formation) and sends them to its parent node. This procedure con-
tinues until DAC manager receives the node and link information of
the whole network, and then it constructs the physical network topol-
ogy. In LDP, the procedure is reverse to PCP. DAC manager sends the
achieved device-to-logical ID mapping information to all its neighbor
nodes, and each intermediate node delivers the information to its chil-
dren. Since a node knows the descendants from each child via PCP,
it can divide the mapping information on a per-child base and de-
liver the more speciﬁc mapping information to each child. Note that
the messages exchanged in both PCP and LDP are uni-cast messages
which require acknowledgements for reliability.
5.2 Testbed Setup and Experiment
We designed and implemented DAC as an application over the
Windows network stack. This application implements the modules
described in Section 2: including device-to-logical ID mapping, com-
munication channel building, physical topology collection and logical
ID dissemination. We built a testbed using 64 Dell servers and 16 8-
port DLink DGS-1008D Gigabit Ethernet switches. Each server has
an Intel 2GHz dual-core CPU, 2GB DRAM, 160GB disk and an Intel
Pro=1000PT dual-port Ethernet NIC. Each link works at Gigabit.