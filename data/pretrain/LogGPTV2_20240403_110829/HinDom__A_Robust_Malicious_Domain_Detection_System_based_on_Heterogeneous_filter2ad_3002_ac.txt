[24], Malwaredomainlist.com [23], DGArchive [27], etc. We
also refer to integrated services like Google Safe Browsing [9]
and VirusTotal [34]. Besides, we manually investigate dubious
domains which have both benign and malicious information.
According to the graph pruning rules mentioned in Section
4.3, we discard some unhelpful nodes by setting Kd = 25, Ka
= 0.1, Kc = 3. Table 3 shows details about the HIN instance
we construct and Table 4 lists the evaluation metrics used in
the experiments.
Table 4: Metrics for evaluation
Figure 4: ROC for the labeled result of each metapath
Metric
TP
FP
TN
FN
accuracy
precision
recall
F1
ROC
AUC
Description
malicious domains labeled as malicious
benign domains labeled as malicious
benign domains labeled as benign
malicious domains labeled as benign
(T P + T N)/(T P + FP + T N + FN)
T P/(T P + FP)
T P/(T P + FN)
2× (precision· recall)/(precision + recall)
a curve plotting TPR against FPR with various
thresholds
area under the ROC curve
to each meta-path, we need to consider two aspects: coverage
and accuracy. For coverage, it means with this meta-path, we
can fully exploit connections so that few domains will be left
unlabeled. For accuracy, it means among the labeled domains,
few are misclassiﬁed. The Meta-path Combiner assigns a
weight to each meta-path according to Laplacian Scores. To
test its effectiveness, we list the average detection results and
the corresponding weight of each meta-path in Table 6, where
the Unlabeled rate and F1 score reveal coverage and accuracy
respectively.
Table 6: Detection results of each meta-path
5.2 Metepath Combiner
HinDom relies on six meta-paths to formulate the ﬁnal simi-
larity matrix. These meta-paths represent associations among
domains from different perspectives and thus have different
inﬂuences on domain detection. We test the detection perfor-
mance of each meta-path by randomly keep the label infor-
mation for 70% domains, leaving the remained 30% as test
samples and repeat the procedure for 10 times. The accuracy,
precision and recall of labels generated by each meta-path are
shown in Table 6 while the ROCs are shown in Figure 4.
Table 5: Labeled Metrics of each meta-path
PID metapath
1
2
3
4
5
6
S
C
QQT
RRT
QNQT
RDRT
accuarcy
0.9376
0.9999
0.9567
0.9888
0.9571
0.9754
precision
0.8703
0.9999
0.9198
0.9879
0.9251
0.9580
recall
0.8712
0.9998
0.8860
0.9989
0.8710
0.9281
Noting that some meta-paths have low connectivity, which
means the classiﬁer cannot reach plenty of domains if it only
relies on one of the meta-paths. Thus, when assigning weights
PID metapath
S
1
C
2
QQT
3
RRT
4
QNQT
5
RDRT
6
combined path
F1 Score Unlabeled Rate Weight
0.1698
0.8708
0.9996
0.0003
0.1386
0.9026
0.0125
0.9934
0.3826
0.8973
0.9428
0.2962
0.9743
0.0027
0.9133
0.0917
0.5317
0.0049
0.2057
0
-
From Table 5, we can see that HinDom combines these
meta-paths in order: PID 5, PID 6, PID 1, PID 3, PID 4 and
PID 2. The combined path gets the ability to cover the whole
set of domains with high detection accuracy. It is worth noting
that some meta-paths with strong relations in domain detec-
tion (e.g. PID2: d C−→ d ) get extremely high accuracy but very
low coverage. This is consistent with the fact that domains
in a CNAME record tend to belong to the same class, yet not
many domains are in this kind of relationship. With Laplacian
Score, the Meta-path Combiner assigns these meta-paths with
relatively low weights to ensure that HinDom can detect as
many domain names as possible. Take PID 4: d R−→ ip RT−→ d
and PID 6:d R−→ ip D−→ ip RT−→ d for instance, the latter extends
its coverage by utilizing relations among IP addresses, though
introducing some noises, it can reach more domains and thus
plays a more important role in this scenario.
406          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX AssociationTable 7: Detection results with different fraction of labels
Metrics of each method
Initial Label
Fraction
90%
70%
50%
30%
10%
NB
accuarcy F1 Score
0.9499
0.9632
0.9276
0.9429
0.8912
0.9020
0.8235
0.8260
0.7834
0.7929
SVM
accuarcy F1 Score
0.9827
0.9864
0.9550
0.9682
0.9090
0.9286
0.8527
0.8516
0.8031
0.8120
RF
accuarcy F1 Score
0.9803
0.9813
0.9611
0.9700
0.9180
0.9257
0.8613
0.8544
0.7902
0.8141
HinDom
accuarcy F1 Score
0.9905
0.9960
0.9743
0.9880
0.9776
0.9840
0.9698
0.9453
0.9116
0.9626
5.3 Transductive Classiﬁcation
As mentioned in Section 3.5, the public whitelists or blacklists
of domains are not completely reliable an have a number of
subtle issues. In order to reduce the cost of labeling domains
manually, HinDom utilizes a meta-path based transductive
classiﬁcation method to make better use of structural infor-
mation of the unlabeled samples. To test the effectiveness
of the Transductive Classiﬁer, in this section, we compare
HinDom with three inductive classiﬁcation methods: Navie
Bayes (NB), Support Vector Machine (SVM) and Random
Forest (RF),on the situation where 90%, 70%, 50%, 30%, 10%
labels are kept randomly. For the inductive methods, we ex-
tract all details about entities and relations of the HIN instance
as features to learn the classiﬁcation functions. The accuracy
and F1 score of each method are shown in Table 7. As we can
see, HinDom maintains relatively stable performance when
the fraction of initial label information decreases. It yields
accuracy: 0.9960, F1:0.9905 when 90% domains are pre-
labeled and still obtains accuracy: 0.9626, F1: 0.9116 when
only 10% labels are left at the beginning. As for the inductive
methods, they can obtain relatively good performance with
sufﬁcient labels, yet the accuracy drops to around 0.8 as the
size of training dataset decreases. The reason behind is that
by using HIN assisted with a transductive classiﬁer, HinDom
can not only learn from the labeled data but also fully exploit
associations among domains and generally propagates the
initial label information over the whole network.
To ﬁnd the minimum threshold of label fraction for rela-
tively high performance, we gradually reduce the initial label
information and draw curves of accuracy and F1 score in
Figure 5. The breakpoint is around 10%, which means we
need to provide at least 10 percent labeled samples for domain
detection, otherwise, the performance of HinDom will suffer
a dramatic decrease.
5.4 Robutness
Though consuming lots of efforts, the manually labeled train-
ing dataset often contains noises which might be caused by
attackers’ tricks or human mistakes. In this section, to test
Figure 5: Accuracy and F1 score with different initial label
fraction
HinDom’s robustness to label noises, we keep 70% initial
label information, randomly change labels of kd% training
samples and compare the detection results of HinDom with
those traditional methods: NB, SVM and RF. We increase
kd% gradually and stop at 50% where none of these meth-
ods can generate a tolerable detection result. We repeat each
scenario for 10 times. Figure 6 shows the average accuracy
trend and F1-score trend of each method. We can see that the
traditional machine learning methods are susceptible to misla-
beling. Meanwhile, with a better understanding of structural
information, HinDom can hold relatively stable performance
when dealing with some label noises.
Figure 6: Accuracy and F1 score with kd% label noise
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 4075.5 Multi-classiﬁcation
It is observed that malicious domains from the same attacker
tend to follow the same patterns, regardless of character dis-
tributions, the victim groups or the sets of IP addresses they
map to. From the deﬁnition of Transductive Classiﬁer, we can
see that apart from detecting malicious domains, HinDom can
support multi-classiﬁcation and identify which categories the
malicious domains belong to. The domain family informa-
tion provided by multi-classiﬁcation is useful for follow-up
work like reverse engineering and security reports. To test
HinDom’s multi-classiﬁcation ability, we further label the
0.25 million malicious domains mentioned in Section 4.1 into
13 categories based on the malware or cybercrimines they
related to. Note that for those categories with less than F=150
domains, we group their domains together as Class Rare.
Table 8: Multi-classiﬁcation with different fraction of labels
Initial Label
Fraction
90%
70%
50%
30%
10%
accuarcy
0.9814
0.9783
0.9720
0.9644
0.9598
Metrics
precision
0.9786
0.9759
0.9673
0.96178
0.9543
recall
0.9815
0.9765
0.9706
0.9654
0.9585
F1 Score
0.9801
0.9762
0.9689
0.9636
0.95647
Figure 7: Confusion matrix of multi-classiﬁcation with 50%
initial labels
Table 8 lists the detection results with different fractions
of initial labels. Considering the sample imbalance of each
category, we use the weighted-average metrics to evaluate
the multi-classiﬁcation. With this method, the metrics of each
label will be weighted by support to ﬁnd their average. Due
to space limitations, Figure 7 only displays the confusion ma-
trix of multi-classiﬁcation when there are 50% initial labels.
The X axis denotes the ture category of each domain while
the Y axis denotes their predicted labels. The confuse matrix
shows that most misclassiﬁcations are between Class Benign
and some malicious classes with relatively small sample size,
which we suppose is caused by data skew. The additional in-
formation about domain family brings higher TPR yet lead to
worse FPR. To solve this problem, we will separate HinDom
into two stages, ﬁrst distinguish malicious domains from the
benign ones and then multi classify these malicious domains
to identify their families.
5.6 Public Information Only
To test the real-world practicality, we eliminate the inﬂuence
of human decisions by running HinDom with initial labels
only from public whitelists or blacklists. When using lists
with only 2LDs (e.g. Alexa Top List, DGArchive), we set
domains with the same 2LD to the same class. For instance,
agoodm.m.taobao.com and chat.im.taobao.com are in benign
class because their 2LD taobao.com is in Alexa Top 1K. For
those dubious domains appear in both whitelists and blacklists,
we randomly set them to benign or malicious class. In the
end, we label about 24% of the 0.95 million domains with
about 0.04 million confused ones. HinDom yields accuracy:
0.9634, F1: 0.9253 with this kind of initial label information.
Table 9 shows the performance o HinDom with public la-
bels and with 20%, 30% manual labels while Figure 8 displays
their ROC. We can see that HinDom gets similar performance
no matter with only public information or with manual labels
of the same proportion. Besides, by analyzing the detection
results, we ﬁnd that with a relatively small µ = 0.3, which
means we do not ﬁrmly insist on the pre-labeled informa-
tion, HinDom can correct the label of some domains that
are misclassiﬁed at the very beginning. For example, mem-
berprod.alipay.com and hosting.rediff.com were randomly
assigned to the malicious class because they have both be-
nign and malicious information. HinDom adjusts their labels
from 1 to 0 after 11 times iteration because of the strong
associations they have with the benign domains.
Table 9: Detection results with public and manual labels
Metrics
accuracy