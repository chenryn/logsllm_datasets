### 1. Introduction

This section provides an overview of the statistical properties of repair times, including their distribution and variability, based on a dataset collected over a decade at a high-performance computing (HPC) site. The analysis focuses on how repair times vary depending on the root cause of the failure and across different systems.

### 2. Statistical Properties of Repair Times

**Table 2: Statistical Properties of Time to Repair as a Function of Root Cause**

| Root Cause | Mean (min) | Median (min) | Std. Dev. (min) | Variability (C²) |
|------------|------------|--------------|-----------------|------------------|
| Unkn.      | 398        | 32           | 6099            | 234              |
| Hum.       | 163        | 44           | 418             | 6                |
| Env.       | 572        | 269          | 808             | 2                |
| Netw.      | 247        | 70           | 720             | 8                |
| SW         | 369        | 33           | 6316            | 293              |
| HW         | 342        | 64           | 4202            | 151              |
| All        | 355        | 54           | 4854            | 187              |

**Figure 7:**
- **(a)** Empirical CDF of repair times.
- **(b)** Mean repair time for each system.
- **(c)** Median repair time for each system.

### 3. Analysis of Repair Times

#### 3.1. Root Cause and Repair Times

The median and mean time to repair vary significantly depending on the root cause of the failure. For example:
- Human error: Mean time to repair is less than 3 hours.
- Environmental problems: Mean time to repair is nearly 10 hours.
- Other categories (software, hardware, network): Mean time to repair ranges between 4 and 6 hours.

The overall mean repair time across all failures is close to 6 hours, primarily due to the high frequency of hardware and software failures, which have mean repair times around 6 hours.

#### 3.2. Variability in Repair Times

Repair times are highly variable, especially for software and hardware failures. For instance:
- Software failures: Median time to repair is about 10 times lower than the mean.
- Hardware failures: Median time to repair is 4 times lower than the mean.

This high variability is reflected in the extremely high C² values (see Table 2). The diversity of problems causing hardware and software failures contributes to this variability. For example, hardware failures span 99 different categories, compared to only two (power outage and A/C failure) for environmental problems.

#### 3.3. Distribution Fitting

Figure 7(a) shows the empirical CDF for all repair times and four standard distributions fitted to the data:
- **Exponential Distribution:** Poor fit due to high variability.
- **Lognormal Distribution:** Best fit, both visually and by negative log-likelihood.
- **Weibull Distribution and Gamma Distribution:** Weaker fits than the lognormal but better than the exponential.

### 4. System-Specific Repair Times

Figures 7(b) and 7(c) show the mean and median time to repair for each system, respectively. The hardware type has a major effect on repair times, with systems of the same hardware type exhibiting similar mean and median repair times. However, repair times vary significantly across systems of different types.

System size does not significantly affect repair times. For example, type E systems range from 128 to 1024 nodes but exhibit similar repair times. In fact, the largest type E systems (systems 7–8) have among the lowest median repair times.

### 5. Comparison with Related Work

Several studies have analyzed failure data, differing in the type of data used, the number and type of systems under study, and the time of data collection. Key findings include:
- **Root Cause Statistics:** Four studies [4, 13, 16, 7] report that software-related failures make up 20-50% of all failures, hardware 10-30%, and environmental problems 5%. Network problems account for 20-40%.
- **Time Between Failures (TBF):** Several studies use distribution fitting and find the Weibull distribution to be a good fit [5, 24, 9, 15].
- **Correlations:** Some studies report correlations between workload and failure rate [2, 6, 18], and between the type of workload and failure rate [18].

### 6. Summary

- **Failure Rates:** Vary widely across systems, ranging from 20 to more than 1000 failures per year, and are roughly proportional to the number of processors.
- **Workload Correlation:** There is evidence of a correlation between failure rate and the type and intensity of the workload.
- **Failure Rate Over Lifetime:** The curve of the failure rate over the lifetime of an HPC system differs from lifecycle curves reported for individual components.
- **Time Between Failures (TBF):** Not well modeled by an exponential distribution; better fit by a gamma or Weibull distribution.
- **Mean Repair Times:** Vary widely, ranging from 1 hour to more than a day, and depend mostly on the type of the system.
- **Repair Time Variability:** Extremely variable, better modeled by a lognormal distribution than an exponential distribution.

### 7. Acknowledgments

We thank Gary Grider, Laura Davey, and the Computing, Communications, and Networking Division at LANL for their efforts in collecting and clearing the data for public release. We also thank Roy Maxion, Priya Narasimhan, and the participants of the ISSRE’05 “Workshop on Dependability Benchmarking” for their valuable comments and questions. Finally, we thank the members of the PDL Consortium for their interest and support.

### 8. References

[1] The raw data and more information are available at: 
- http://www.pdl.cmu.edu/FailureData/
- http://www.lanl.gov/projects/computerscience/data/, 2006.

[2] X. Castillo and D. Siewiorek. Workload, performance, and reliability of digital computing systems. In FTCS-11, 1981.

[3] J. Gray. Why do computers stop and what can be done about it? In Proc. of the 5th Symp. on Reliability in Distributed Software and Database Systems, 1986.

[4] J. Gray. A census of tandem system availability between 1985 and 1990. IEEE Trans. on Reliability, 39(4), 1990.

[5] T. Heath, R. P. Martin, and T. D. Nguyen. Improving cluster availability using workstation validation. In Proc. of ACM SIGMETRICS, 2002.

[6] R. K. Iyer, D. J. Rossetti, and M. C. Hsueh. Measurement and modeling of computer reliability as affected by system activity. ACM Trans. Comput. Syst., 4(3), 1986.

[7] M. Kalyanakrishnam, Z. Kalbarczyk, and R. Iyer. Failure data analysis of a LAN of Windows NT based computers. In SRDS-18, 1999.

[8] G. P. Kavanaugh and W. H. Sanders. Performance analysis of two time-based coordinated checkpointing protocols. In Proc. Pacific Rim Int. Symp. on Fault-Tolerant Systems, 1997.

[9] T.-T. Y. Lin and D. P. Siewiorek. Error log analysis: Statistical modeling and heuristic trend analysis. IEEE Trans. on Reliability, 39, 1990.

[10] D. Long, A. Muir, and R. Golding. A longitudinal survey of internet host reliability. In SRDS-14, 1995.

[11] J. Meyer and L. Wei. Analysis of workload influence on dependability. In FTCS, 1988.

[12] B. Mullen and D. R. Lifecycle analysis using software defects per million (SWDPM). In 16th international symposium on software reliability (ISSRE’05), 2005.

[13] B. Murphy and T. Gent. Measuring system and software reliability using an automated data collection process. Quality and Reliability Engineering International, 11(5), 1995.

[14] S. Nath, H. Yu, P. B. Gibbons, and S. Seshan. Subtleties in tolerating correlated failures. In Proc. of the Symp. on Networked Systems Design and Implementation (NSDI’06), 2006.

[15] D. Nurmi, J. Brevik, and R. Wolski. Modeling machine availability in enterprise and wide-area distributed computing environments. In Euro-Par’05, 2005.

[16] D. L. Oppenheimer, A. Ganapathi, and D. A. Patterson. Why do internet services fail, and what can be done about it? In USENIX Symp. on Internet Technologies and Systems, 2003.

[17] J. S. Plank and W. R. Elwasif. Experimental assessment of workstation failures and their impact on checkpointing systems. In FTCS’98, 1998.

[18] R. K. Sahoo, R. K., A. Sivasubramaniam, M. S. Squillante, and Y. Zhang. Failure data analysis of a large-scale heterogeneous server environment. In Proc. of DSN’04, 2004.

[19] D. Tang, R. K. Iyer, and S. S. Subramani. Failure analysis and modeling of a VAX cluster system. In FTCS, 1990.

[20] T. Tannenbaum and M. Litzkow. The condor distributed processing system. Dr. Dobbs Journal, 1995.

[21] N. H. Vaidya. A case for two-level distributed recovery schemes. In Proc. of ACM SIGMETRICS, 1995.

[22] W. Willinger, M. S. Taqqu, R. Sherman, and D. V. Wilson. Self-similarity through high-variability: statistical analysis of Ethernet LAN traffic at the source level. IEEE/ACM Trans. on Networking, 5(1):71–86, 1997.

[23] K. F. Wong and M. Franklin. Checkpointing in distributed computing systems. J. Par. Distrib. Comput., 35(1), 1996.

[24] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Networked Windows NT system field failure data analysis. In Proc. of the 1999 Pacific Rim Int. Symp. on Dependable Computing, 1999.

[25] Y. Zhang, M. S. Squillante, A. Sivasubramaniam, and R. K. Sahoo. Performance implications of failures in large-scale cluster scheduling. In Proc. 10th Workshop on Job Scheduling Strategies for Parallel Processing, 2004.

---

This revised version aims to provide a clear, coherent, and professional presentation of the content, with improved structure and readability.