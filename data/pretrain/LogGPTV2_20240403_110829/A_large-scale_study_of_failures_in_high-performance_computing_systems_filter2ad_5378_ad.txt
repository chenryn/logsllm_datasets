1000
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
System ID
)
n
m
i
(
e
m
i
t
r
i
a
p
e
r
n
a
d
e
M
i
350
300
250
200
150
100
50
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
System ID
Figure 7. (a) Empirical CDF of repair times. (b) Mean repair time and (c) median repair time for each system.
(a)
(b)
(c)
Mean (min)
Median (min)
Std. Dev. (min)
Variability (C 2)
Unkn.
398
32
6099
234
Hum.
163
44
418
6
Env.
572
269
808
2
Netw.
247
70
720
8
SW
369
33
6316
293
HW
342
64
4202
151
All
355
54
4854
187
Table 2. Statistical properties of time to repair as a
function of the root cause of the failure.
times. We then study the statistical properties of repair
times, including their distribution and variability.
Table 2 shows the median and mean of time to repair as
a function of the root cause, and as an aggregate across all
failure records. We ﬁnd that both the median and the mean
time to repair vary signiﬁcantly depending on the root cause
of the failure. The mean time to repair ranges from less
than 3 hours for failures caused by human error, to nearly
10 hours for failures due to environmental problems. The
mean time to repair for the other root cause categories varies
between 4 and 6 hours. The mean repair time across all fail-
ures (independent of root cause) is close to 6 hours. The rea-
son is that it’s dominated by hardware and software failures
which are the most frequent types of failures and exhibit
mean repair times around 6 hours.
An important observation is that the time to repair for all
types of failures is extremely variable, except for environ-
mental problems. For example, in the case of software fail-
ures the median time to repair is about 10 times lower than
the mean, and in the case of hardware failures it is 4 times
lower than the mean. This high variability is also reﬂected
in extremely high C2 values (see bottom row of Table 2).
One reason for the high variability in repair times of soft-
ware and hardware failures might be the diverse set of prob-
lems that can cause these failures. For example, the root
cause information for hardware failures spans 99 different
categories, compared to only two (power outage and A/C
failure) for environmental problems. To test this hypothesis
we determined the C2 for several types of hardware prob-
lems. We ﬁnd that even within one type of hardware prob-
lem variability can be high. For example, the C2 for repair
times of CPU, memory, and node interconnect problems is
36, 87, and 154, respectively. This indicates that there are
other factors contributing to the high variability.
Figure 7(a) shows the empirical CDF for all repair times
in the data, and four standard distributions ﬁtted to the data.
The exponential distribution is a very poor ﬁt, which is not
surprising given the high variability in the repair times. The
lognormal distribution is the best ﬁt, both visually as well
as measured by the negative log-likelihood. The Weibull
distribution and the gamma distribution are weaker ﬁts than
the lognormal distribution, but still considerably better than
the exponential distribution.
Finally, we consider how repair times vary across sys-
tems. Figure 7(b) and (c) show the mean and median time
to repair for each system, respectively. The ﬁgure indicates
that the hardware type has a major effect on repair times.
While systems of the same hardware type exhibit similar
mean and median time to repair, repair times vary signiﬁ-
cantly across systems of different type,
Figure 7(b) and (c) also indicate that system size is not
a signiﬁcant factor in repair time. For example, type E sys-
tems range from 128 to 1024 nodes, but exhibit similar re-
pair times. In fact, the largest type E systems (systems 7–8)
are among the ones with the lowest median repair time.
The relatively consistent repair times across systems of
the same hardware type are also reﬂected in the empirical
CDF. We ﬁnd that the CDF of repair times from systems of
the same type is less variable than that across all systems,
which results in an improved (albeit still sub-optimal) ex-
ponential ﬁt5.
7 Comparison with related work
Work on characterizing failures in computer systems dif-
fers in the type of data used; the type and number of systems
under study; the time of data collection; and the number of
failure or error records in the data set. Table 3 gives an
overview of several commonly cited studies of failure data.
Four of the above studies include root cause statistics
[4, 13, 16, 7]. The percentage of software-related failures
is reported to be around 20% [3, 13, 16] to 50% [4, 7].
Hardware is reported to make up 10-30% of all failures
5Graphs omitted for lack of space.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
Study
[3, 4]
[7]
[16]
[13]
[19]
[9]
[6]
[18]
[5]
[24]
[15]
[10]
[2]
Date
1990
1999
2003
1995
1990
1990
1986
2004
2002
1999
2005
1995
1980
Length
3 years
6 months
3-6 months
7 years
8 months
22 months
3 years
1 year
Environment
Tandem systems
70 Windows NT mail server
3000 machines in Internet services
VAX systems
7 VAX systems
13 VICE ﬁle servers
2 IBM 370/169 mainframes
395 nodes in machine room
1-36 months
70 nodes in university and Internet services
503 nodes in corporate envr.
Type of Data
Customer data
Error logs
Error logs
Field data
Error logs
Error logs
Error logs
Error logs
Error logs
Error logs
4 months
6–8 weeks
3 months
1 month
300 university cluster and Condor[20] nodes
Custom monitoring
1170 internet hosts
PDP-10 with KL10 processor
RPC polling
N/A
Table 3. Overview of related studies
# Failures
800
1100
501
N/A
364
300
456
1285
3200
2127
N/A
N/A
N/A
Statistics
Root cause
Root cause
Root cause
Root cause
TBF
TBF
TBF
TBF
TBF
TBF
TBF
TBF,TTR
TBF,Utilization
[4, 13, 16, 7]. Environment problems are reported to ac-
count for around 5% [4]. Network problems are reported
to make up between 20% [16] and 40% [7]. Gray [4] re-
ports 10-15% of problems due to human error, while Op-
penheimer et al. [16] report 14-30%. The main difference
to our results is the lower percentage of human error and
network problems in our data. There are two possible ex-
planations. First, the root cause of 20-30% of failures in our
data is unknown and could lie in the human or network cate-
gory. Second, the LANL environment is an expensive, very
controlled environment with national safety obligations and
priorities, so greater resources may be put into its infras-
tructure than is put into commercial environments.
Several studies analyze the time between failures [18, 19,
5, 24, 15, 10]. Four of the studies use distribution ﬁtting and
ﬁnd the Weibull distribution to be a good ﬁt [5, 24, 9, 15],
which agrees with our results. Several studies also looked at
the hazard rate function, but come to different conclusions.
Some of them [5, 24, 9, 15] ﬁnd decreasing hazard rates
(Weibull shape parameter < 0.5). Others ﬁnd that hazard
rates are ﬂat [19], or increasing [18]. We ﬁnd decreasing
hazard rates with Weibull shape parameter of 0.7–0.8.
Three studies [2, 6, 18] report correlations between
workload and failure rate. Sahoo [18] reports a correlation
between the type of workload and the failure rate, while
Iyer [6] and Castillo [2] report a correlation between the
workload intensity and the failure rate. We ﬁnd evidence for
both correlations, in that we observe different failure rates
for compute, graphics, and front-end nodes, for different
hours of the day and days of the week.
Sahoo et al. [18] also study the correlation of failure rate
with hour of the day and the distribution of failures across
nodes and ﬁnd even stronger correlations than we do. They
report that less than 4% of the nodes in a machine room
experience almost 70% of the failures and ﬁnd failure rates
during the day to be four times higher than during the night.
We are not aware of any studies that report failure rates
over the entire lifetime of large systems. However, there ex-
ist commonly used models for individual software or hard-
ware components. The failures over the lifecycle of hard-
ware components are often assumed to follow a “bathtub
curve” with high failure rates at the beginning (infant mor-
tality) and the end (wear-out) of the lifecycle. The failure
rate curve for software products is often assumed to drop
over time (as more bugs are detected and removed), with
the exception of some spikes caused by the release of new
versions of the software [13, 12]. We ﬁnd that the failure
rate over the lifetime of large-scale HPC systems can differ
signiﬁcantly from the above two patterns (recall Figure 4).
Repair times are studied only by Long et al. [10]. Long et
al. estimate repair times of internet hosts by repeated polling
of those hosts. They, like us, conclude that repair times are
not well modeled by an exponential distribution, but don’t
attempt to ﬁt other distributions to the data.
An interesting question that is beyond the scope of our
work is how system design choices depend on failure char-
acteristics. Plank et al. [17] study how checkpointing strate-
gies are affected by the distribution of time between fail-
ures, and Nath et al. [14] study how correlations between
failures affect data placement in distributed storage systems.
8 Summary
Many researchers have pointed out the importance of an-
alyzing failure data and the need for a public failure data
repository [16]. In this paper we study a large set of fail-
ure data that was collected over the past decade at a high-
performance computing site and has recently been made
publicly available [1]. We hope that this data might serve
as a ﬁrst step towards a public data repository and encour-
age efforts at other sites to collect and clear data for public
release. Below we summarize a few of our ﬁndings.
• Failure rates vary widely across systems, ranging from
20 to more than 1000 failures per year, and depend
mostly on system size and less on the type of hardware.
• Failure rates are roughly proportional to the number
of processors in a system, indicating that failure rates
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
are not growing signiﬁcantly faster than linearly with
system size.
• There is evidence of a correlation between the failure
rate of a machine and the type and intensity of the
workload running on it. This is in agreement with ear-
lier work for other types of systems [2, 6, 18].
• The curve of the failure rate over the lifetime of an
HPC system looks often very different from lifecycle
curves reported in the literature for individual hard-
ware or software components.
• Time between failure is not modeled well by an expo-
nential distribution, which agrees with earlier ﬁndings
for other types of systems [5, 24, 9, 15, 18]. We ﬁnd
that the time between failure at individual nodes, as
well as at an entire system, is ﬁt well by a gamma
or Weibull distribution with decreasing hazard rate
(Weibull shape parameter of 0.7–0.8).
• Mean repair times vary widely across systems, ranging
from 1 hour to more than a day. Repair times depend
mostly on the type of the system, and are relatively
insensitive to the size of a system.
• Repair times are extremely variable, even within one
system, and are much better modeled by a lognormal
distribution than an exponential distribution.
We hope that our ﬁrst step in analyzing the wealth of in-
formation provided by the data, together with the public re-
lease of the raw data [1], will spark interesting future work.
9 Acknowledgments
We thank Gary Grider, Laura Davey, and the Computing,
Communications, and Networking Division at LANL for
their efforts in collecting the data and clearing it for public
release. We also thank Roy Maxion, Priya Narasimhan, and
the participants of the ISSRE’05 “Workshop on depend-
ability benchmarking” for their many comments and ques-
tions. Finally, we thank the members of the PDL Consor-
tium (including APC, EMC, Equallogic, Hewlett-Packard,
Hitachi, IBM, Intel, Microsoft, Network Appliance, Oracle,
Panasas, Seagate, and Sun) for their interest and support.
References
[1] The raw data and more information is available at the follow-
ing two URLs:. http://www.pdl.cmu.edu/FailureData/ and
http://www.lanl.gov/projects/computerscience/data/, 2006.
[2] X. Castillo and D. Siewiorek. Workload, performance, and
reliability of digital computing systems. In FTCS-11, 1981.
[3] J. Gray. Why do computers stop and what can be done about
In Proc. of the 5th Symp. on Reliability in Distributed
it.
Software and Database Systems, 1986.
[4] J. Gray. A census of tandem system availability between
1985 and 1990. IEEE Trans. on Reliability, 39(4), 1990.
[5] T. Heath, R. P. Martin, and T. D. Nguyen. Improving cluster
availability using workstation validation. In Proc. of ACM
SIGMETRICS, 2002.
[6] R. K. Iyer, D. J. Rossetti, and M. C. Hsueh. Measurement
and modeling of computer reliability as affected by system
activity. ACM Trans. Comput. Syst., 4(3), 1986.
[7] M. Kalyanakrishnam, Z. Kalbarczyk, and R. Iyer. Failure
data analysis of a LAN of Windows NT based computers.
In SRDS-18, 1999.
[8] G. P. Kavanaugh and W. H. Sanders. Performance analy-
sis of two time-based coordinated checkpointing protocols.
In Proc. Paciﬁc Rim Int. Symp. on Fault-Tolerant Systems,
1997.
[9] T.-T. Y. Lin and D. P. Siewiorek. Error log analysis: Statis-
tical modeling and heuristic trend analysis. IEEE Trans. on
Reliability, 39, 1990.
[10] D. Long, A. Muir, and R. Golding. A longitudinal survey of
internet host reliability. In SRDS-14, 1995.
[11] J. Meyer and L. Wei. Analysis of workload inﬂuence on
dependability. In FTCS, 1988.
[12] B. Mullen and D. R. Lifecycle analysis using software de-
fects per million (SWDPM). In 16th international sympo-
sium on software reliability (ISSRE’05), 2005.
[13] B. Murphy and T. Gent. Measuring system and software re-
liability using an automated data collection process. Quality
and Reliability Engineering International, 11(5), 1995.
[14] S. Nath, H. Yu, P. B. Gibbons, and S. Seshan. Subtleties
in tolerating correlated failures.
In Proc. of the Symp. on
Networked Systems Design and Implementation (NSDI’06),
2006.
[15] D. Nurmi, J. Brevik, and R. Wolski. Modeling machine
availability in enterprise and wide-area distributed comput-
ing environments. In Euro-Par’05, 2005.
[16] D. L. Oppenheimer, A. Ganapathi, and D. A. Patterson. Why
do internet services fail, and what can be done about it? In
USENIX Symp. on Internet Technologies and Systems, 2003.
[17] J. S. Plank and W. R. Elwasif. Experimental assessment
of workstation failures and their impact on checkpointing
systems. In FTCS’98, 1998.
[18] R. K. Sahoo, R. K., A. Sivasubramaniam, M. S. Squillante,
and Y. Zhang. Failure data analysis of a large-scale hetero-
geneous server environment. In Proc. of DSN’04, 2004.
[19] D. Tang, R. K. Iyer, and S. S. Subramani. Failure analysis
and modelling of a VAX cluster system. In FTCS, 1990.
[20] T. Tannenbaum and M. Litzkow. The condor distributed pro-
cessing system. Dr. Dobbs Journal, 1995.
[21] N. H. Vaidya. A case for two-level distributed recovery
schemes. In Proc. of ACM SIGMETRICS, 1995.
[22] W. Willinger, M. S. Taqqu, R. Sherman, and D. V. Wilson.
Self-similarity through high-variability: statistical analysis
of Ethernet LAN trafﬁc at the source level.
IEEE/ACM
Trans. on Networking, 5(1):71–86, 1997.
[23] K. F. Wong and M. Franklin. Checkpointing in distributed
computing systems. J. Par. Distrib. Comput., 35(1), 1996.
[24] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Networked Windows
NT system ﬁeld failure data analysis. In Proc. of the 1999
Paciﬁc Rim Int. Symp. on Dependable Computing, 1999.
[25] Y. Zhang, M. S. Squillante, A. Sivasubramaniam, and R. K.
Sahoo. Performance implications of failures in large-scale
cluster scheduling. In Proc. 10th Workshop on Job Schedul-
ing Strategies for Parallel Processing, 2004.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply.