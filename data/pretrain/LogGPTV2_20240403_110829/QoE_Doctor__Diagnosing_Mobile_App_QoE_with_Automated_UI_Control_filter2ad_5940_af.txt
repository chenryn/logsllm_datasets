d
e
t
a
m
i
t
s
E
 12000
 10000
 8000
 6000
 4000
 2000
 0
Non-tail
Tail
C1
C2
Figure 22: C2’s RRC state
machine has only two states,
omitting FACH.
Table 4: RRC state transition distribution in the user
study trace. C1 experiences a RRC state transition
23.4% of the time, while C2 only experiences 6.5%.
Figure 23: Tail energy contributes
35.68% of the total energy for C1, but
only 2.88% for C2.
F
D
C
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
 0.8
 0.6
 0.4
 0.2
 0
C1 DCH
C1 FACH->DCH
C1 DCH->FACH->DCH
C1 PCH->FACH->DCH
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
C2 DCH
C2 Disconnected->DCH
C2 DCH->Disconnected->DCH
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
User-perceived latency (s)
Figure 24: RRC state transitions introduce additional delay for
the user-perceived web page loading time. There is only one
data point for C2 DCH→DISCONNECTED→DCH.
wide range of content types, including news, movies, sports, social
networking, google search results, and travel guides. Between the
experiment for C1 and the experiment for C2, the average relative
download data size difference is only 1.94%, which is mainly
caused by changes in the HTTP metadata such as time, encoding
mechanism, etc. instead of the web page content.
Finding 10. The 2-state RRC state machine model has a 16.9%
lower chance to experience RRC state transitions, which causes
22.8% lower web page loading time than the 3-state RRC state
machine model. To understand the impact of different types of
RRC state machine transitions, we classify the loading time based
on the RRC promotion and demotion events that happen inside the
QoE Window. Fig. 24 shows the web page loading time distribution
for different types of transitions for C1 and C2’s 3G networks,
and Table 4 shows the percentage of different types of transitions.
Consistent with previous work [35, 34], the loading time in Fig. 24
for C1 and C2’s 3G networks becomes longer when it involves
RRC state machine transitions. The situation is the worst if the
loading time involves both the state promotion and demotion –
the page loading time at least doubles.
In Table 4, between the
2-state model of C2 and 3-state model of C1, the web page loading
in the 2-state model has a 16.9% lower chance to involve RRC
state transitions. Based on QxDM logs, for C1 the DCH→FACH
and FACH→PCH demotion timers are 3 seconds and 6 seconds,
while for C2 the DCH→Disconnected timer is 10 seconds, which
is roughly equal to the sum of the demotion timers in the 3-state
model. Thus, the main cause for fewer state transitions in C2 is the
simpliﬁed state machine design which removes the middle state:
it stays in DCH most of the time, and once it promotes, it is very
unlikely to demote. This results in 22.8% lower loading time in
the 2-state model of C2 compared to the model of C1. We also
calculate the energy consumption using the technique described
in §5.3. As shown in Fig. 23, although the overall energy is similar,
due to the lower chance of demotion, C2 has 92.5% lower tail
energy compared to C1 according to the deﬁnition of tail energy
in previous work [34]. For these results, we believe that they are
generalizable to other apps besides web browsing apps, since the
root cause lies in the RRC state machine design.
8. RELATED WORK
UI automation. UI automation tools are a common approach
for dynamically analyzing applications for various purposes.
Application bug detection is the most common application.
In
this category, Dynodroid [29], A3E [16], VanarSena [37], and
ContextualFuzzing [27] are designed to uncover application bugs
and crashes by automatically exploring all possible internal states,
and exposing them to various external contexts. Accessibility
policy checking is also a popular target. AMC [26] automatically
explores app UI states to check for violations of UI requirements
for vehicular apps.
DECAF [28] uses UI automation to
detect ad fraud. UI automation is also used for security and
privacy purposes. For example, AppsPlayground [36] provides
a framework using fuzz testing for malware analysis. Finally,
PUMA [21] generalizes the common procedures for all the systems
above, and provides a generic programmable framework. Unlike
PUMA, QoE Doctor in this paper does not aim to expose abnormal
behavior in mobile apps, but instead aims to analyze the QoE for
normal mobile application usage. For example, we have a specially
designed wait component in our UI automation (detailed in §4)
for the accurate measurements of user-perceived latency, which
in previous work was usually implemented using heuristic waiting
timers.
QoE measurement. Previous work have measured application
QoE using subjective evaluations from users such as evaluation
scores. D. Joumblatt et. al. [24], Chen et. al. [20], and Ickin et.
al. [23] deﬁne a target QoE metric based on user satisfactions and
dissatisfactions. Schatz et. al. [42] and Balachandran et. al. [17, 18]
use user engagement as the target QoE to predict. Unlike them, we
focus on objective QoE metrics, which are reproducible and can be
measured repeatedly and automatically. Like us, Prometheus [15],
AppInsight [38], Timecard [39], and Panappticon [47] measure
objective QoE metrics, but they either require access to app source
code, or require instrumentation of the app logic or the underlying
system, which QoE Doctor does not.
162the bit rate,
Rather than directly measuring subjective and objective QoE,
QoE estimation from network trafﬁc is also a popular approach.
D. Joumblatt et. al. [24] predicts the QoE of network applications
from network metrics. Schatz et. al. [42] and Balachandran et.
al. [17, 18] build a predictive framework to ﬁnd the relationships
between measurable user engagement metrics and actionable video
delivery mechanisms in the network (e.g.,
initial
loading time, and buffering ratio).
Prometheus [15] predicts
objective QoE metrics such as the buffering time with passive
network measurements. Unlike them, QoE Doctor does not predict
or estimate QoE metrics, but rather directly measures the ground
truth values of QoE metrics.
QoE improvement. There has also been work on improving the
QoE of mobile apps. Timecard [39] instruments a mobile OS to
ensure that user-perceived delays can meet deadline requirements.
Proteus [46] predicts future network performances over cellular
networks in real time, and increases QoE for RTC applications.
Sprout [45] builds a UDP-based end-to-end protocol for mobile
apps such as video conferencing which requires both low latency
and high throughput. Our work is complementary: QoE Doctor
can be used to automatically and repeatedly collect and analyze
QoE data for validating these systems, and can potentially uncover
root causes of new QoE problems, shedding light on future areas
for QoE improvement.
Cross-layer analysis has been less
Cross-layer analysis.
extensively explored. RILAnalyzer [44] uses cross-layer analysis
approach to uncover how RRC states affect app performances.
Compared to it, QoE Doctor supports automatically collecting
objective QoE values from UI changes directly, instead of relying
on user studies and studying network layer performance metrics
such as TCP operations which are less directly related to user-
perceived latency. ARO [35] analyzes tcpdump traces to uncover
app performance issues. However, compared to QoE Doctor, their
work mainly focuses on radio resource efﬁciency problems rather
than app QoE.
9. CONCLUSION
In this paper, we built a tool, QoE Doctor, which automatically
replays user interaction sequences of interest to measure mobile
app QoE, record relevant QoE metrics, and allow the root causes of
QoE problems to be analyzed across multiple layers, covering both
the system and network stacks. Using this tool, we systematically
study various QoE metrics for popular apps, and quantitatively
evaluate the impact of various QoE-related factors on these QoE
metrics. With QoE Doctor, we uncover several signiﬁcant QoE
problems along with the potential root causes of them for major
applications and carriers.
Acknowledgments
We would like to thank Yihua Ethan Guo, Ashkan Nikravesh,
the anonymous reviewers, and our shepherd, Anmol Sheth, for
providing valuable suggestions and feedback on our work. We
would also like to thank Warren McNeel, John Murphy, Pete
Myron, Isaac Robinson, Samson Kwong, Jeffery Smith, the Device
Development and QoE Lab team at T-Mobile for their assistance.
This research was supported in part by the National Science
Foundation under grants CNS-1059372, CNS-1039657, CNS-
1345226, and CNS-0964545.
10. REFERENCES
[1] 94% Users Skip Pre-roll Ads. https://
econsultancy.com/blog/63277-pre-roll-
video-ads-is-it-any-wonder-why-we-hate-
them#i.9gohovetnegyqcc.
[2] Android Activity Testing. http://developer.
android.com/tools/testing/activity_
testing.html.
[3] Android DDMS. http://developer.android.com/
tools/debugging/ddms.html.
[4] Android Logcat. http://developer.android.com/
tools/help/logcat.html.
[5] Android MediaPlayer. http://developer.android.
com/reference/android/media/MediaPlayer.
html.
[6] Global Internet Phenomena. https://www.sandvine.
com/trends/global-internet-phenomena/.
[7] Monsoon Power Monitor. http://www.msoon.com/
LabEquipment/PowerMonitor/.
[8] Speed Index metric for WebPagetest. https://sites.
google.com/a/webpagetest.org/docs/using-
webpagetest/metrics/speed-index.
[9] TCPDUMP. http://www.tcpdump.org/.
[10] Trafﬁc Control HOWTO. http://tldp.org/HOWTO/
Traffic-Control-HOWTO/.
[11] Comparing Trafﬁc Policing and Trafﬁc Shaping for
Bandwidth Limiting. http://www.cisco.com/c/en/
us/support/docs/quality-of-service-qos/
qos-policing/19645-policevsshape.html,
2005.
[12] QxDM Professional Proven Diagnostic Tool for Evaluating
Handset and Network Performance. http://www.
qualcomm.com/media/documents/files/qxdm-
professional-qualcomm-extensible-
diagnostic-monitor.pdf, 2012.
[13] T-Mobile’s next move: Shame AT&T and Verizon into
ditching data overage fees. http://bgr.com/2014/
04/09/t-mobile-vs-att-vs-verizon-data-
overage-fees, 2014.
[14] T. 3rd Generation Partnership Project. 3GPP TS 25.322:
Radio Link Control (RLC) - UMTS, 2013.
[15] V. Aggarwal, E. Halepovic, J. Pang, S. Venkataraman, and
H. Yan. Prometheus: Toward Quality-of-Experience
Estimation for Mobile Apps from Passive Network
Measurements. In HotMobile, 2014.
[16] T. Azim and I. Neamtiu. Targeted and Depth-ﬁrst
Exploration for Systematic Testing of Android Apps. In
Proc. OOPSLA, 2013.
[17] A. Balachandran, V. Sekar, A. Akella, S. Seshan, I. Stoica,
and H. Zhang. A quest for an internet video
quality-of-experience metric. In ACM HotNet, 2012.
[18] A. Balachandran, V. Sekar, A. Akella, S. Seshan, I. Stoica,
and H. Zhang. Developing a predictive model of quality of
experience for internet video. In ACM SIGCOMM, 2013.
[19] A. Carroll and G. Heiser. An analysis of power consumption
in a smartphone. In USENIX ATC, 2010.
[20] K. Chen, C. Huang, P. Huang, , and C. Lei. Quantifying
Skype user satisfaction. In Proc. ACM SIGCOMM, 2006.
[21] S. Hao, B. Liu, S. Nathy, W. G. Halfond, and R. Govindan.
PUMA: Programmable UI-Automation for Large Scale
Dynamic Analysis of Mobile Apps. In Proc. ACM MobiSys,
2014.
[22] J. Huang, F. Qian, A. Gerber, Z. M. Mao, S. Sen, and
O. Spatscheck. A close examination of performance and
163power characteristics of 4G LTE networks. In Proc. ACM
MobiSys, 2012.
[23] S. Ickin, K. Wac, M. Fiedler, L. Janowski, J.-H. Hong, and
A. K. Dey. Factors inﬂuencing quality of experience of
commonly used mobile applications. Communications
Magazine, IEEE, 50(4):48–56, 2012.
[24] D. Joumblatt, J. Chandrashekar, B. Kveton, N. Taft, and
R. Teixeira. Predicting user dissatisfaction with internet
application performance at end-hosts. In INFOCOM, 2013
Proceedings IEEE, 2013.
[36] V. Rastogi, Y. Chen, and W. Enck. AppsPlayground:
automatic security analysis of smartphone applications. In
CODASPY, 2013.
[37] L. Ravindranath, S. Nath, J. Padhye, and H. Balakrishnan.
Automatic and Scalable Fault Detection for Mobile
Applications. Technical report, Microsoft Research, 2013.
[38] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan,
I. Obermiller, and S. Shayandeh. AppInsight: Mobile App
Performance Monitoring in the Wild. In Proc. Operating
Systems Design and Implementation, 2012.
[25] S. S. Krishnan and R. K. Sitaraman. Understanding the
[39] L. Ravindranath, J. Padhye, R. Mahajan, and
effectiveness of video ads: a measurement study. In Proc.
ACM IMC, 2013.
[26] K. Lee, J. Flinn, T. Giuli, B. Noble, and C. Peplin. AMC:
Verifying User Interface Properties for Vehicular
Applications. In Proc. ACM MobiSys, 2013.
[27] C.-J. M. Liang, N. Lane, N. Brouwers, L. Zhang,
B. Karlsson, R. Chandra, and F. Zhao. Contextual Fuzzing:
Automated Mobile App Testing Under Dynamic Device and
Environment Conditions. Technical report, Microsoft
Research, 2013.
[28] B. Liu, S. Nath, R. Govindan, and J. Liu. DECAF: Detecting
and Characterizing Ad Fraud in Mobile Apps. In Proc.
NSDI, 2014.
[29] A. Machiry, R. Tahiliani, and M. Naik. Dynodroid: An Input
Generation System for Android Apps. In Proc. ACM FSE,
2012.
[30] A. Pathak, Y. C. Hu, and M. Zhang. Where is the energy
spent inside my app?: ﬁne grained energy accounting on
smartphones with eprof. In EuroSys, 2012.
[31] F. Qian, S. Sen, and O. Spatscheck. Characterizing Resource
Usage for Mobile Web Browsing. In MobiSys, 2014.
[32] F. Qian, Z. Wang, Y. Gao, J. Huang, A. Gerber, Z. Mao,
S. Sen, and O. Spatscheck. Periodic transfers in mobile
applications: network-wide origin, impact, and optimization.
In WWW, 2012.
[33] F. Qian, Z. Wang, A. Gerber, Z. M. Mao, S. Sen, and
O. Spatscheck. Characterizing radio resource allocation for
3G networks. In Proc. ACM IMC, 2010.
[34] F. Qian, Z. Wang, A. Gerber, Z. M. Mao, S. Sen, and
O. Spatscheck. Top: Tail optimization protocol for cellular
radio resource allocation. In ICNP, 2010.
[35] F. Qian, Z. Wang, A. Gerber, Z. M. Mao, S. Sen, and
O. Spatscheck. Proﬁling resource usage for mobile
applications: a cross-layer approach. In MobiSys, 2011.
H. Balakrishnan. Timecard: controlling user-perceived
delays in server-based mobile applications. In Proc. ACM
SOSP, 2013.
[40] S. Rosen, H. Luo, Q. A. Chen, Z. M. Mao, J. Hui, A. Drake,
and K. Lau. Discovering Fine-grained RRC State Dynamics
and Performance Impacts in Cellular Networks. In Proc.
ACM MobiCom, 2014.
[41] S. Rosen, H. Luo, Q. A. Chen, Z. M. Mao, J. Hui, A. Drake,
and K. Lau. Understanding RRC State Dynamics through
Client Measurements with Mobilyzer. In ACM MobiCom S3
Workshop, 2014.
[42] R. Schatz, T. Hossfeld, and P. Casas. Passive YouTube QoE
Monitoring for ISPs. In Proc. IEEE IMIS, 2012.
[43] G.-H. Tu, C. Peng, C.-Y. Li, X. Ma, H. Wang, T. Wang, and
S. Lu. Accounting for roaming users on mobile data access:
Issues and root causes. In Proc. ACM MobiSys, 2013.
[44] N. Vallina-Rodriguez, A. Auçinas, M. Almeida,
Y. Grunenberger, K. Papagiannaki, and J. Crowcroft.
RILAnalyzer: a comprehensive 3G monitor on your phone.
In Proc. ACM IMC, 2013.
[45] K. Winstein, A. Sivaraman, H. Balakrishnan, et al. Stochastic
forecasts achieve high throughput and low delay over cellular
networks. In Proc. NSDI, volume 13, 2013.
[46] Q. Xu, S. Mehrotra, Z. Mao, and J. Li. PROTEUS: network
performance forecast for real-time, interactive mobile
applications. In Proc. ACM MobiSys, 2013.
[47] L. Zhang, D. R. Bild, R. P. Dick, Z. M. Mao, and P. Dinda.
Panappticon: event-based tracing to measure mobile
application and platform performance. In CODES+ ISSS,
2013.
[48] L. Zhang, B. Tiwana, Z. Qian, Z. Wang, R. Dick, Z. M. Mao,
and L. Yang. Accurate Online Power Estimation and
Automatic Battery Behavior Based Power Model Generation
for Smartphones. In CODES+ ISSS, 2010.
164