17
18
19 WackoPicko
20 WackoPicko
21 WackoPicko
22 WackoPicko
23 WackoPicko
24 WordPress
25 WordPress
Description
User upload
Review rating
Tax class
Admin ranks
Conﬁguration
Site name
Date
Add session
Comment
Conference name
Edit paper
Edit session
Delete comment
General options
User options
Comment draft
Locale
Title banner
Comment
Multi-step
Picture
Search
SQL error
Comment
Nearby event
Model Workﬂow
ISD
Unique
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
F. Missed by Us
Out of the 25 unique injections found by all scanners,
we also ﬁnd all 25. There was, however, an instance where
Arachni found a vulnerability by injecting a different pa-
rameter than we did. This does not constitute a unique
vulnerability due to our clustering, which we explain in
Section IV-D. On SCARF, input elements can be dynami-
cally generated by adding more users. The input names will
simply be 1_name, 2_name, etc. Arachni managed to add
multiple users by randomizing email addresses. Since our
crawler is focused on consistency, we do not generate valid
random email addresses and could therefore not add more
than one user.
The drawback, as we have discussed is that is it easier to
lose the state if too much randomness is used. A possible
solution to this could be to keep two sets of default values
and always test both when possible. There is still the risk
that using multiple users can result in mixing up the state
between them. It would also introduce a performance penalty
as multiple submissions for each form would be required.
The w3af scanner was able to ﬁnd a reﬂected version
of a vulnerable parameter that we considered to be stored.
In this particular case on SCARF, it was possible to get a
direct reﬂection by submitting the same password and retype
password in the user settings. This is what w3af did. Our
scanner injected unique values into each ﬁeld, resulting in
an error without reﬂection, however, the ﬁelds were still
stored. Inter-state dependency analysis was used to detect
these stored values when revisiting the user settings.
Further possible improvements include updating our
method for determining safe requests and more robust
function hooking. A machine learning approach, such as
Mitch [27], could be used to determine if a request can
be considered safe. The function hooking could be done by
modifying the JavaScript engine instead of instrumenting
JavaScript code.
G. Vulnerability Exploitability
For the six new vulnerabilities, we further investigate the
impact and exploitability. While all of these vulnerabilities
were found using an admin account in the web application,
the attacker does not necessarily need to be an admin. In
fact, XSS payloads executed as the admin gives a higher
impact as the JavaScript runs with admin privileges. What
the attacker needs to do is usually to convince the admin to
click on a link or visit a malicious website, i.e. the attacker
does not require any admin privileges. Although, there might
be an XSS vulnerability in the code, i.e. user input being
reﬂected, there are orthogonal mitigations such as CSRF
tokens and CSP that can decrease the exploitability.
To exploit the HotCRP vulnerability the attacker would
have to guess a CSRF token, which is considered difﬁcult.
Similarly, PrestaShop has a persistent secret in the URL
which would have to be known by the attacker. One of
the WordPress vulnerabilities was a self-XSS, meaning the
admin would need to be convinced to, in this case, input our
payload string, while the other one required a CSRF token.
Finally, osCommerce required no CSRF tokens making it
both high impact and easy to exploit.
H. Coordinated Disclosure
We have reported the vulnerabilities to the affected ven-
dors, following the best practices of coordinated disclo-
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1138
sure [28]. Speciﬁcally, we reported a total of six vulnerabil-
ities to HotCRP, osCommerce, PrestaShop and WordPress.
So far our reports have resulted in HotCRP patching their
vulnerability [29]. A parallel disclosure for the same vul-
nerability was reported to PrestaShop and is now tracked as
CVE-2020-5271 [30]. Due to the difﬁculty of exploitation,
WordPress did not consider them vulnerabilities. However,
the nearby event vulnerability is ﬁxed in the latest version.
We have not received any conﬁrmation from osCommerce
yet.
VI. RELATED WORK
This section discusses related work. Automatic vulnerabil-
ity scanning has been a popular topic due to its complexity
and practical usefulness. This paper focuses on blackbox
scanning, which requires no access to the application’s
source code or any other input from developers. We have
evaluated our approach with respect to both community-
developed open-source tools [18], [16], [17] and academic
blackbox scanners [8], [13]. There are also earlier works on
vulnerability detection and scanning [31], [32], [33], [34],
[11], [12]. While we focus on blackbox testing, there is also
progress on whitebox security testing [35], [36], [37], [38],
[39].
As previous evaluations [15], [14], [40], [41], [42] show,
detecting stored XSS is hard. A common notion is that it is
not the exact payload that is the problem for scanners but
rather crawling deep enough to ﬁnd the injections, as well as,
model the application to ﬁnd the reﬂections. Similar to our
ﬁndings, Parvez et al. [42] note that while some scanners
were able to post comments to pictures in WackoPicko,
something which requires multiple actions in sequence, none
of them was able to inject a payload.
We now discuss work that addresses server-side state,
client-side state, and tracking data dependencies.
Server-side state: Enemy of the State [13] focuses on
inferring the state of the server by using a heuristic method
to compare how requests result in different links on pages.
Black Widow instead takes the approach of analyzing the
navigation methods to infer some state information. For
example, if the previous edge in the navigation graph was a
form submission then we would have to resubmit this form
before continuing. This allows us to execute sequences of
actions without fully inferring the server-side state.
One reason many of the other scanners pay little attention
to server-side state is to prioritize performance from concur-
rent requests. Skipﬁsh [22] is noteworthy for its high perfor-
mance in terms of requests per second. One method they use
to achieve this is making concurrent requests. Concurrent
requests can be useful
in a stateless environment since
the requests will not interfere with each other. ZAP [17],
w3af [16] and Arachni [18] take the same approach as
Skipﬁsh and use concurrent requests in favor of better state
control. Since our traversing method relies on executing a
1139
sequence of possibly state-changing action we need to ensure
that no other state-changing requests are sent concurrently.
For this reason, our approach only performs actions in serial.
Client-side state: j ¨Ak considers client-side events to
improve exploration. The support for events is however
limited, leaving out such events as form submission. While
other scanners like Enemy of the State, w3af, and ZAP
execute JavaScript, they do not model the events. This limits
their ability to explore the client-side state. As modern
applications make heavy use of JavaScript, Black Widow
offers fully-ﬂedged support of client-side events. In contrast
to j ¨Ak, Black Widow models client-side events like any
other navigation method. This means that we do not have to
execute the events in any particular order which allows us to
chain them with other navigations such as form submissions.
Tracking data dependencies: Tracking payloads is an
important part of detecting stored XSS vulnerabilities. Some
scanners, including Arachni, use a session-based ID in each
payload. Since the ID is based on the session this can
lead to false positives as payloads are reused for different
parameters. j ¨Ak and Enemy of the State use unique IDs
for their payload but forgets them on new pages. w3af uses
unique payloads and remembers them across pages. ZAP
uses a combination in which a unique ID is sent together
with a generic payload but in separate requests. This works
if both the ID and payload are stored on a page. In addition
to using unique IDs for all our payloads, Black Widow
incorporates the inter-state dependencies in the application
to ensure that we can fuzz the correct input and output across
different pages.
LigRE [11], and its successor KameleonFuzz [12] use a
blackbox approach to reverse engineering the application
and apply a genetic algorithm to modify the payloads.
While they also use tainting inside the payloads to track
them, we use plaintext tokens to avoid ﬁlters destroying
the taints. While Black Widow works on live applications,
KameleonFuzz requires the ability to reset the application.
Unfortunately, neither LigRE nor KameleonFuzz are open-
source, which has hindered us from their experimental
evaluation.
VII. CONCLUSION
We have put a spotlight on key challenges for crawling
and scanning the modern web. Based on these challenges,
we have identiﬁed three core pillars for deep crawling
and scanning: navigation modeling, traversing, and tracking
inter-state dependencies. We have presented Black Widow,
a novel approach to blackbox web application scanning
that leverages these pillars by developing and combining
augmented navigation graphs, workﬂow traversal, and inter-
state data dependency analysis. To evaluate our approach,
we have implemented it and tested it on 10 different web
applications and against 7 other web application scanners.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
Our approach results in code coverage improvements rang-
ing from 63% to 280% compared to other scanners across
all tested applications. Across all tested web applications,
our approach improved code coverage by between 6% and
62%, compared to the sum of all other scanners. When
deployed to scan for cross-site scripting vulnerabilities, our
approach has featured no false positives while uncovering
more vulnerabilities than the other scanners, both in the
reference applications,
i.e. phpBB, SCARF, Vanilla and
WackoPicko, and in production software, including HotCRP,
osCommerce, PrestaShop and WordPress.
ACKNOWLEDGMENT
We would like to thank Sebastian Lekies for inspiring
discussions on the challenges of web scanning. We would
also like to thank Nick Nikiforakis and the reviewers for
their valuable feedback. This work was partially supported
by the Swedish Foundation for Strategic Research (SSF) and
the Swedish Research Council (VR).
REFERENCES
[1] Google, “Vulnerability Reward Program: 2019 Year in Re-
view,” https://security.googleblog.com/2020/01/vulnerability-
reward-program-2019-year.html, 2020.
[2] Facebook,
“A Look Back
2019 Bug Bounty
https://www.facebook.com/notes/facebook-
Highlights,”
bug-bounty/a-look-back-at-2019-bug-bounty-highlights/
3231769013503969/, 2020.
at
[3] The
2017,”
2017,
OWASP
Foundation,
10
-
https://www.owasp.org/images/7/72/
OWASP Top 10-2017 %28en%29.pdf.pdf. [Online]. Avail-
able:
https://www.owasp.org/images/7/72/OWASP Top 10-
2017 %28en%29.pdf.pdf
“Owasp
top
[4] S. Innovation, “Google Awards $1.2 Million in Bounties Just
for XSS Bugs,” https://blog.securityinnovation.com/google-
awards-1.2-million-in-bounties-just-for-xss-bugs, 2016.
[5] InfoSecurity, “XSS is Most Rewarding Bug Bounty as
CSRF is Revived,” https://www.infosecurity-magazine.com/
news/xss-bug-bounty-csrf-1-1-1-1/, 2019.
[6] Bugcrowd, “The State of Crowdsourced Security in 2019,”
https://www.bugcrowd.com/, 2020.
[7] A. Petukhov and D. Kozlov, “Detecting security vulnera-
bilities in web applications using dynamic analysis with
penetration testing,” Computing Systems Lab, Department
of Computer Science, Moscow State University, pp. 1–120,
2008.
[9] A. Mesbah, E. Bozdag, and A. Van Deursen, “Crawling ajax
by inferring user interface state changes,” in 2008 Eighth
International Conference on Web Engineering.
IEEE, 2008,
pp. 122–134.
[10] C.-P. Bezemer, A. Mesbah, and A. van Deursen, “Automated
security testing of web widget interactions,” in Proceedings
of the the 7th joint meeting of the European software engi-
neering conference and the ACM SIGSOFT symposium on
The foundations of software engineering. ACM, 2009, pp.
81–90.
[11] F. Duchene, S. Rawat, J.-L. Richier, and R. Groz, “Ligre:
Reverse-engineering of control and data ﬂow models for
black-box xss detection,” in 2013 20th Working Conference
on Reverse Engineering (WCRE).
IEEE, 2013, pp. 252–261.
[12] ——, “Kameleonfuzz: evolutionary fuzzing for black-box xss
detection,” in Proceedings of the 4th ACM conference on Data
and application security and privacy, 2014, pp. 37–48.
[13] A. Doup´e, L. Cavedon, C. Kruegel, and G. Vigna, “Enemy of
the state: A state-aware black-box web vulnerability scanner,”
in USENIX Security Symposium 12, 2012, pp. 523–538.
[14] A. Doup´e, M. Cova, and G. Vigna, “Why johnny cant
pentest: An analysis of black-box web vulnerability scanners,”
in International Conference on Detection of Intrusions and
Malware, and Vulnerability Assessment. Springer, 2010, pp.
111–131.
[15] J. Bau, E. Bursztein, D. Gupta, and J. Mitchell, “State of
the art: Automated black-box web application vulnerability
testing,” in 2010 IEEE Symposium on Security and Privacy.
IEEE, 2010, pp. 332–345.
[16] A. Riancho, “w3af - open source web application security
scanner,” 2007. [Online]. Available: https://w3af.org
[17] OWASP, “Owasp zed attack proxy (zap),” 2020. [Online].
Available: https://owasp.org/www-project-zap/
[18] Sarosys LLC, “Framework - arachni
security scanner
https://www.arachni-scanner.com/features/framework/
framework,” 2019.
- web application
[Online]. Available:
[19] R. Fielding and J. Reschke, “Hypertext Transfer Protocol
(HTTP/1.1): Semantics and Content,” Internet Requests for
Comments, RFC Editor, RFC 7231, June 2014. [Online].
Available: https://www.rfc-editor.org/rfc/rfc7231.txt
[20] D. Rethans, “Xdebug - debugger ad proﬁler tool for php,”
2019. [Online]. Available: https://xdebug.org/
[21] H. Niki, “Wget - gnnu project,” 2019. [Online]. Available:
https://www.gnu.org/software/wget/
[22] M. Zalewski, “Skipﬁsh,” 2015. [Online]. Available: https:
//code.google.com/p/skipﬁsh/
[8] G. Pellegrino, C. Tsch¨urtz, E. Bodden, and C. Rossow,
“j ¨Ak: Using Dynamic Analysis to Crawl and Test Modern
Web Applications,” in International Symposium on Recent
Advances in Intrusion Detection. Springer, 2015, pp. 295–
316.
[23] S. Idrissi, N. Berbiche, F. Guerouate, and M. Shibi, “Per-
formance evaluation of web application security scanners
for prevention and protection against vulnerabilities,” Inter-
national Journal of Applied Engineering Research, vol. 12,
no. 21, pp. 11 068–11 076, 2017.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1140
[24] PortSwigger,
“Burp
Scanner
https://portswigger.net/burp/documentation/scanner, 2020.
-
PortSwigger,”
[25] A. Doup´e, “Wackopicko,” 2018. [Online]. Available: https:
//github.com/adamdoupe/WackoPicko
[26] WHATWG,
“Html
[Online]. Avail-
able: https://html.spec.whatwg.org/multipage/forms.html#the-
form-element
standard,”
2019.
[27] S. Calzavara, M. Conti, R. Focardi, A. Rabitti, and
G. Tolomei, “Mitch: A machine learning approach to the
black-box detection of csrf vulnerabilities,” in 2019 IEEE
European Symposium on Security and Privacy (EuroS&P).
IEEE, 2019, pp. 528–543.
[28] Google, “Project zero: Vulnerability disclosure faq,” 2019.
[Online]. Available: https://googleprojectzero.blogspot.com/
p/vulnerability-disclosure-faq.html
[29] E. Kohler, “Correct missing quoting reported by Benjamin
at Chalmers,” https://github.com/kohler/hotcrp/
Eriksson
commit/81b7ffee2c5bd465c82acf139cc064daacca845c, 2020.
[30] “CVE-2020-5271.” Available from MITRE, CVE-ID CVE-
[Online]. Available: http:
2020-5271., Apr. 20 2020.
//cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5271
[31] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic, E. Kirda,
C. Kruegel, and G. Vigna, “Saner: Composing static and
dynamic analysis to validate sanitization in web applications,”
in 2008 IEEE Symposium on Security and Privacy (sp 2008).
IEEE, 2008, pp. 387–401.
[32] W. G. Halfond, S. R. Choudhary, and A. Orso, “Penetration
testing with improved input vector identiﬁcation,” in 2009
International Conference on Software Testing Veriﬁcation and
Validation.
IEEE, 2009, pp. 346–355.
[33] T. S. Rocha and E. Souto, “Etssdetector: A tool to automati-
cally detect cross-site scripting vulnerabilities,” in 2014 IEEE
13th International Symposium on Network Computing and
Applications, Aug 2014, pp. 306–309.
[34] S. Kals, E. Kirda, C. Kruegel, and N. Jovanovic, “Secubat:
a web vulnerability scanner,” in Proceedings of
the 15th
international conference on World Wide Web, 2006, pp. 247–
256.
[35] V. Felmetsger, L. Cavedon, C. Kruegel, and G. Vigna, “To-
ward automated detection of logic vulnerabilities in web
applications,” in USENIX Security Symposium, vol. 58, 2010.
[36] Y.-W. Huang, F. Yu, C. Hang, C.-H. Tsai, D.-T. Lee, and S.-Y.
Kuo, “Securing web application code by static analysis and
runtime protection,” in Proceedings of the 13th international
conference on World Wide Web, 2004, pp. 40–52.
[37] N. Jovanovic, C. Kruegel, and E. Kirda, “Static analysis
for detecting taint-style vulnerabilities in web applications,”
Journal of Computer Security, vol. 18, no. 5, pp. 861–907,
2010.
[38] X. Li, W. Yan, and Y. Xue, “Sentinel: securing database
from logic ﬂaws in web applications,” in Proceedings of the
second ACM conference on Data and Application Security
and Privacy, 2012, pp. 25–36.
[39] A. Vernotte, F. Dadeau, F. Lebeau, B. Legeard, F. Peureux,
and F. Piat, “Efﬁcient detection of multi-step cross-site
scripting vulnerabilities,” in Information Systems Security,
A. Prakash and R. Shyamasundar, Eds.
Cham: Springer
International Publishing, 2014, pp. 358–377.
[40] L. Suto, “Analyzing the accuracy and time costs of web ap-
plication security scanners,” San Francisco, February, 2010.
[41] M. Vieira, N. Antunes, and H. Madeira, “Using web security
scanners to detect vulnerabilities in web services,” in 2009
IEEE/IFIP International Conference on Dependable Systems
& Networks.
IEEE, 2009, pp. 566–571.
[42] M. Parvez, P. Zavarsky, and N. Khoury, “Analysis of effec-
tiveness of black-box web application scanners in detection
of stored sql injection and stored xss vulnerabilities,” in 2015
10th International Conference for Internet Technology and
Secured Transactions (ICITST).
IEEE, 2015, pp. 186–191.
VIII. APPENDIX
A. Scanner conﬁguration
1) Arachni: The following command was used to run
Arachni.
1 arachni [url] --check=xss* --browser-cluster-
pool-size=1 --plugin?autologin:url=[
loginUrl],parameters="[userField]=[username
]&[passField]=[password]",check="[logout
string]}
2) Black Widow: The following command was used to
run Black Widow.
1 python3 crawl.py [url]
3) Enemy of the State: First we changed the username
and password in the web application to scanner1 then we
ran the following command.
1 jython crawler2.py [url]
4) j ¨Ak: We updated the example.py ﬁle with the URL
and user data.
1 url = [url]
2 user = User("[sessionName]", 0, url, login_data
= {"[userField]": "[username]", "[
passField]": "[password]"}, session="ABC")
5) Skipﬁsh: The following command was used to run
Skipﬁsh.
1 skipfish -uv -o [output]
2
3
4
5
6
7
8
--auth-form [loginUrl]
--auth-user-field [userField]
--auth-pass-field [passField]
--auth-user [username]
--auth-pass [password]
--auth-verify-url [verifyUrl]
[url]
6) w3af: For w3af we used the following settings, generic
and xss for the audit plugin, web spider for crawl plugin and
generic (with all credentials) for the auth plugin.
1141
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
7) Wget: The following command was used to run Wget.
1 wget -rp -w 0 waitretry=0 -nd --delete-after --
execute robots=off [url]
8) ZAP: For ZAP we used the automated scan with both
traditional spider and ajax spider. In the Scan Progress
window we deactivated everything that was not XSS. Similar
to Enemy of the State, we changed the credentials in the web
application to the scanner’s default, i.e. ZAP.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1142