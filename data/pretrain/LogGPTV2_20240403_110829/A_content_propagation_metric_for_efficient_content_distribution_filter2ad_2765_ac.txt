rent time minus τ , the time interval over which block propagations
are measured. A processor extracts a propagation bandwidth for
each such edge, prunes those edges from the forest, and records the
new propagation bandwidths in the state layer.
Before extracting propagation bandwidths, the processor adjusts
the timestamps on the forest’s edges such that no edge is time-
stamped later than any edge in its subtree. Such an inconsistency
occurs if a peer deposits its spent tokens before an ancestor in a
block propagation tree deposits its own tokens for the same block.
To make the adjustment, the processor recurses on each of the for-
est’s roots, setting each timestamp to the minimum of its own time-
stamp and the earliest timestamp in its subtree. This makes the
forest reﬂect the constraint that a peer can only upload a block after
it has received that block.
To extract block propagation bandwidths, the processor recurses
on each vertex in a forest, summing up the number of edges in
each subtree. The processor only calculates and reports propaga-
tion bandwidths for edges older than the measurement time interval
τ . It removes the corresponding edges from the forest and appends
each new propagation bandwidth to a list in the state layer of prop-
agation bandwidths for the tracked uploader’s contribution to the
forest’s swarm. Each new propagation bandwidth is timestamped
with the time on the forest’s tracked transfer edge, equal to the time
that the tracked transfer completed.
Web servers compute CPM values for a peer’s swarms by aver-
aging the propagation bandwidths in the list that are timestamped
within a recent time interval π, a global constant set to ﬁve min-
utes in our implementation. In an announce response, web servers
report the most recent CPM value, or, if there are no recent prop-
agation bandwidths, instruct the requester to upload blocks to the
swarm to obtain fresh measurements.
Operating on block exchange forests is a highly parallel task.
Each forest represents exchanges of a single block for a single
swarm, enabling processors to operate on block exchange forests
in isolation. Multiple processors coordinate their behavior through
the state by atomically reading and incrementing the swarm and
block identiﬁers for the next forest to process. Thus, increasing
the number of processor machines linearly increases the supported
processing workload of the coordinator.
If high load renders the coordinator unable to process all block
propagation forests, the coordinator sheds load by decreasing the
fraction of blocks that it tracks. The coordinator maintains a dy-
namically adjusted parameter that dictates the fraction of blocks to
track, enabling web servers and processors to independently deter-
mine whether a particular block should be tracked. Web servers do
not insert forest edges for untracked blocks, and processors do not
iterate over forests of untracked blocks. The coordinator adjusts the
parameter such that for each swarm, some block is processed with
a target frequency.
3315.1.3 Distributed State Layer
V-Formation uses memcached to implement a distributed, shared
state layer for web servers and processors. The coordinator’s state
is linear in the number of swarms it supports and in the number of
peers.
The coordinator maintains data structures for each of the swarms
it supports as well as for each peer in the system. Since this state
is stored in memcached, it is distributed across multiple servers.
Atomic compare and swap operations supported by memcached en-
able nodes to update this state quickly and concurrently. Since all
such state is soft and can be recreated through remeasurement, if
necessary, it need not be stored on disk. All lookups are performed
with a speciﬁc key, so the memcached key-value store sufﬁces, and
an expensive relational database insertion is unnecessary.
For each peer, the state layer maintains its address, port, identi-
ﬁer, credit balance, and the set of swarms to which it belongs. For
each swarm, the state layer keeps a swarm identiﬁer and the set
of peers in the swarm. To make bandwidth allocations, for each
peer the state layer records its current τ for computing block prop-
agation bandwidths, its current CPM value for each swarm, and a
history of block propagation bandwidths for each measured block
over the past time interval π. Recent block transfers for measured
blocks are stored as block propagation forests linear in size to the
number of peers and edges that they contain. Forests are pruned as
block propagation bandwidths are extracted, based on peers’ values
of τ . Lastly, the state layer maintains a single value representing
the next block that a processor should analyze, which processors
advance each time they read it.
5.2 Peers
Peers interact with the coordinator by issuing announce requests
periodically for each swarm, get_token requests when their fresh
tokens are nearly depleted, and deposit_token requests when they
possess spent tokens from other peers. They interact with other
peers through block requests for the rarest blocks among directly
connected neighbors, and they satisfy block requests according to
their CPM values for competing swarms. Upon receiving a block,
a peer responds with a fresh token embedded with the block’s iden-
tiﬁer.
To allocate bandwidth among competing swarms, peers priori-
tize their swarms based on the CPM values contained in announce
responses. Upon receiving a CPM value, a peer updates a local
prioritized list of its swarms. When a peer has received multiple
outstanding block requests from peers in different swarms, it satis-
ﬁes the request from the swarm with the largest CPM value.
If the coordinator lacks current information on recent block ex-
changes for any swarm to which a peer belongs, the coordinator
reports that the peer should probe the swarm. The special probe
ﬂag instructs a peer to upload a small, constant number of blocks
to the swarm for the coordinator to track.
5.3 QoS Guarantees
In some cases, it may be desirable to guarantee a minimum band-
width for particular swarms in order to meet service-level agree-
ments or to provide a certain quality of service to designated swarms.
V-Formation enables system administrators to specify lower bounds
for swarms where necessary, sacriﬁcing overall system performance
in favor of control. V-Formation satisﬁes such requests by diverting
upload bandwidth from members of the swarm whose CPM values
suggest that they should upload to other competing swarms instead.
In order to minimize impact on overall system performance, the co-
ordinator iteratively reassigns bandwidth from peers with low CPM
values for competing swarms until the target service level has been
achieved.
5.4 Security
Securing a peer-assisted content distribution system from ma-
licious users or free-loaders is signiﬁcantly more tractable than
securing pure peer-to-peer systems. The coordinator in a peer-
assisted systems serves as a trusted entity in the system that can
detect attacks and enforce access control when coupled with a se-
cure wire protocol.
The V-Formation wire protocol is an extension of the Antfarm
wire protocol [32] and shares the same security guarantees.
It
makes standard cryptographic assumptions on the infeasibility of
reversing a one-way cryptographic hash function. It also assumes
that packets cannot be read or modiﬁed by untrusted third parties at
the IP level. Such an attack is difﬁcult without collusion from ISPs,
and a successful attack only inﬂuences peers whose packets can be
snooped and spoofed. We support SSL on peer-to-peer and peer-
to-coordinator exchanges, respectively. While a formal treatment
of the security properties of the protocol is beyond the scope of this
paper, prior work [35] has established the feasibility of a secure,
cryptographic wire protocol using a trusted, logically centralized
server.
V-Formation’s token protocol incentivizes peers to report accu-
rate and timely information about block exchanges to the coordi-
nator. Fresh tokens contain unforgeable identiﬁers known only to
the coordinator and the peers for which they are minted. When
a peer receives a token in exchange for a block, the token’s re-
cipient embeds the identiﬁer of its original owner and veriﬁes the
block identiﬁer embedded in the token before depositing the token
at the coordinator. The coordinator veriﬁes that the token’s spender
and depositor are members of the swarm for which the token was
minted, according to its records. In addition, the coordinator checks
that the token was deposited before its expiration time, ensuring
that it represents a recent block exchange. If a check fails, at least
one of the two peers known to have touched the token are at fault,
and the coordinator marks both peers possible culprits.
Peers are incentivized to deposit tokens soon after receiving them
in exchange for blocks, resulting in accurate block propagation
forests at the coordinator. All tokens must be deposited at the coor-
dinator before they expire, placing an upper bound on the deviance
of a block exchange’s timestamp. Peers with small credit balances
are incentivized to deposit tokens earlier than their expiration times
in order to receive fresh tokens to spend. Consequently, when the
coordinator adjusts timestamps on forest edges before extracting
propagation bandwidths, promptly deposited tokens result in early
timestamps that percolate up the forest trees, replacing timestamps
of exchanges whose tokens were deposited signiﬁcantly after the
block exchanges actually occurred.
6. EVALUATION
We have implemented the full V-Formation protocol described
in this paper, both in a deployed system that is actively running
on FlixQ [2], and in a simulator for ﬁne-grain analysis of its per-
formance. Through a deployment on PlanetLab [6] and extensive
simulations, we compare V-Formation’s performance to Antfarm,
BitTorrent, and a BitTorrent-like global rarest policy where peers
request the rarest blocks for which they are interested across all
swarms. We also evaluate secondary features of the CPM, such as
convergence time to a stable allocation and sensitivity to changing
swarm dynamics.
332Figure 9: Performance on PlanetLab. 380 nodes in 200 swarms
download movies from FlixQ using the V-Formation protocol and
the same movies using the Antfarm and BitTorrent protocols.
Figure 10: Scalability. Both memory consumption and bandwidth
at the coordinator scale linearly with the size of the system.
6.1 Live Deployment
We evaluated our live deployment of V-Formation against Ant-
farm and version 5.0.9 of the ofﬁcial BitTorrent implementation.
Our V-Formation deployment uses a distributed coordinator de-
ployed in the Amazon EC2 cloud. In this experiment, 380 Plan-
etLab nodes each download one or more of 200 simulated movies,
where a random 20% of the downloading nodes join two or more
swarms to reﬂect the results of our BitTorrent trace. Two cache
servers running on PlanetLab nodes seed the swarms. We scaled
down the upload capacities of the cache servers to 50 KBytes/s
each to reﬂect our relatively small deployment size. Peer upload
capacities are drawn from the distribution of BitTorrent peer band-
width collected by Pouwelse et al. [25]. This distribution speci-
ﬁes a median and 90th percentile peer upload capacity of 30 and
250 KBytes/s, respectively. The peers’ download capacities are
set 50% higher than their upload capacities to simulate asymmetric
links.
The results of the experiment (Figure 9) show the three systems’
aggregate bandwidths over time. V-Formation exhibits similar ini-
tial behavior as Antfarm, with lower aggregate bandwidth than Bit-
Torrent in the ﬁrst six minutes as peers probe swarms to determine
an efﬁcient allocation of bandwidth. V-Formation transitions to its
steady state more quickly than Antfarm as a result of its lightweight
probes, and it maintains a signiﬁcantly higher steady state aggre-
gate bandwidth than Antfarm and BitTorrent.
V-Formation’s logically centralized coordinator is a potential per-
formance bottleneck, and a poor implementation could limit the
Figure 11: Comparison of protocols. Peers download movies
with lengths and popularities randomly drawn from the Internet
Movie Database. Peers have link capacities drawn from a distri-
bution determined by a BitTorrent measurement study. Error bars
indicate 95% conﬁdence.
system’s scalability. The next experiment examines how our imple-
mentation of the coordinator scales as a function of the size of the
deployment; we found that the coordinator’s bandwidth and mem-
ory requirements scale linearly with the total number of peers (Fig-
ure 10). In this experiment, peers are simulated across hosts in a
computer cluster. Each peer is assigned a random bandwidth drawn
from the same measured BitTorrent distribution as in the PlanetLab
deployment. A peer’s bandwidth is proportional to the rate at which
the peer simulates receiving blocks from random participants in its
swarm. Hosts in the cluster issue realistic deposit_token requests
to the coordinator according to these simulated block transfers, as
well as periodic announce requests. We made minor modiﬁcations
to the coordinator to accept deposited tokens as if they were com-
ing from legitimate peers with different IP addresses. In the ex-
periment, three new peers enter the system every second and join
a swarm for a 1-GByte ﬁle with 256-KByte blocks. The coordina-
tor is distributed over two Amazon EC2 instances, each running a
web server, a processor, and a slice of the memcached shared state
layer. The reported memory usage includes all CPM, swarm, and
peer metadata stored in the state layer, as discussed in Section 5.1.3.
Coordinator bandwidth includes all outgoing tokens, CPM values,
and responses to announce requests.
6.2 Simulations
Simulation experiments provide an in-depth examination of the
CPM and how it affects hosts’ bandwidth allocations in V-Formation.
We ﬁrst show the system-wide aggregate bandwidth of V-Formation,
Antfarm, BitTorrent, and a global rarest policy for a realistic sim-
ulation based on movies in the Internet Movie Database (IMDb).
The experiment is based on the number of votes and lengths of
425,000 movies, scaled down to 500 peers and 300 swarms to make
simulations feasible. Each swarm facilitates the download of a sin-
gle movie ﬁle, and each swarm’s popularity is proportional to the
number of votes that its movie has received on IMDb, resulting in a
power-law distribution of swarm sizes. Each ﬁle’s size is based on
the movie’s length and 1 Mbit/s video compression, common for
480p video. Swarm memberships are assigned iteratively, each of
approximately 670 movie downloads randomly assigned either to a
peer that has already been assigned one or more downloads, or to a
fresh peer with no assigned downloads, the probability of each case
calibrated so that 20% of peers belong to multiple swarms to reﬂect
our BitTorrent trace. As in the PlanetLab deployment, nodes draw
their bandwidth distribution from the measured BitTorrent band-
333can only upload to swarm s1 because it only contains one of the
movies, as depicted by the light gray area. Cache server A, on the
other hand, possesses both movies, so it can upload blocks to ei-
ther swarm. The dark gray and medium gray areas indicate that
swarm s2 receives more bandwidth from cache server A than s1.
Fluctuations in the caches’ bandwidth is due to allocating band-
width to other swarms in the system as measured CPM values vary
over time. Averaged over eight runs of the experiment, cache server A
uploads a majority of its bandwidth to the swarm with only one
source (with an average of 124 peers) and only 12.6 KBytes/s to
the swarm also sourced by cache server B (with an average of
120 peers) in order to offset cache server B’s asymmetric contri-
bution of 42.1 KBytes/s to the swarm sourced by both servers. In-
teractions among swarms similar to the two swarms we have ex-
amined account for V-Formation’s 30% higher system-wide band-
width over Antfarm.
To provide further insight into V-Formation’s bandwidth allo-
cation algorithm, we empirically show how V-Formation allocates
bandwidth to competing swarms. We set up a scenario where peer
p1 possesses content for two swarms s1 and s2 with 25 download-
ers each, and peer p2 possesses content for s1 and a small swarm
s3 with only three downloaders (Figure 6). All peers have upload
and download capacities of 50 KBytes/s. The two peers p1 and p2
converge on an efﬁcient, stable allocation (Figure 13). The left-
hand graph shows p1’s CPM values for its swarms over time; the
right-hand graph shows the same for p2. When the simulation be-
gins, p1 and p2 probe their respective swarms to obtain initial CPM
values. They both measure comparable CPM values for s1, which
are similar to p1’s initial measurement of s2. p2 quickly discov-
ers that s3 receives little beneﬁt from its block uploads, so it allo-
cates its bandwidth to s1. The competition that p2’s uploads create
diminishes p1’s CPM value for s1, causing it to dedicate its band-
width to s2. This sequence of events matches the expected behavior
of the V-Formation protocol, with peer p1 preferentially providing
bandwidth to s2 as s1 can be sourced by both p1 and p2. The peri-
odic ﬂuctuations of measured CPM values are the result of probing;
CPM values go stale after ﬁve minutes of no activity, at which time
peers probe swarms for new block propagation bandwidths.
In order to differentiate peers’ effects on competing swarms, the
coordinator adjusts the block propagation measurement time inter-
val τ for each peer. We measure a single peer p’s block propagation
bandwidths for three competing swarms s1, s2, and s3, as well as
the aggregate bandwidth that results from using each value. Swarm
s1 has 30 downloaders, and s2 and s3 each have 20 download-
ers. Swarm s3 has an additional source of content whose uploads
compete with p’s uploads. All peers have upload and download ca-
pacities of 50 KBytes/s. The coordinator chooses a value for τ that
enables it to differentiate among swarms (Figure 14). The left-hand
graph shows the resulting CPM values as a function of the coordi-
nator’s choice of τ . All three swarms exhibit comparable CPM
values for small τ , but with sufﬁciently large τ , the swarms’ dif-
ferent behaviors become prominent. The right-hand graph shows
the system-wide aggregate bandwidth that results from each value
of τ , with values 30 seconds and above providing approximately
equal aggregate bandwidth. The vertical dashed line in the graph
indicates the coordinator’s dynamic choice of τ for determining p’s
bandwidth allocation. The coordinator chooses the smallest τ such
that it is able to distinguish p’s contribution to the swarms that re-
ceive the most beneﬁt from p’s blocks. The selected τ is safely
above 30 seconds, enabling the system to operate at a high aggre-
gate bandwidth.
The next two experiments evaluate how the CPM enables V-
Formation to converge on a stable allocation of bandwidth in the
Figure 12: Cache bandwidth to a pair of swarms. Two swarms
of similar size achieve comparable aggregate bandwidth even
though cache B does not possess content for swarm s2. Cache A
gives more bandwidth to s2 (medium gray area) than to s1 (dark