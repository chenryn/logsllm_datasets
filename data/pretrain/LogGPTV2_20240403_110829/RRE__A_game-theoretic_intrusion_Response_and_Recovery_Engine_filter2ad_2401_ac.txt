### 4.3. Automatic Conversion: ART to MDP

Using the Attack-Response Trees (ARTs), RRE's local engines automatically construct response decision process models. In these models, security states are defined as binary vectors, where each variable represents a leaf consequence node in the ART, which can be either satisfied (1) or unsatisfied (0). Essentially, each binary string of the security state vector indicates the leaf node consequences that have already been set to 1 based on alerts received from Intrusion Detection Systems (IDS).

For an ART with \( n \) leaf nodes, the security state space consists of \( 2^n \) possible \( n \)-bit state vectors. For ARTs with a large number of leaf nodes, this exponential growth of the security state space often leads to the state space explosion problem. RRE addresses this issue by employing approximation techniques, as discussed in Section 7.

Once the local engines have generated the security state space, the next step is to construct state transitions for each state, i.e., \( A(s) \). In a given security state, there can be two types of actions: responsive \( A_r(s) \) and adversarial \( A_a(s) \), depending on the player making the decision.

1. **Responsive Actions**:
   - When a response action \( m \in M \) is taken in state \( s \), it results in a transition to state \( s' \). The bits in \( s' \) are similar to those in \( s \), except for the bits corresponding to the leaf nodes of the ART subtree rooted at \( m \), which are set to 0.
   - As an example, if the system is in state \( s \in S \) and RRE decides to enforce a response action \( m_r \) connected to the root node in the ART, the system’s next state will be \( s' \), where all bits are 0, representing the most secure state.
   - Although it might be tempting to always take \( m_r \) whenever any leaf node is set to 1, a cost-benefit evaluation (discussed later) usually results in the choice of a cheaper response action or no action at all.

2. **Adversarial Actions**:
   - During the automatic ART-to-CMDP conversion in RRE, each leaf consequence node \( l \) in the ART is mapped to an adversarial action that causes \( l \) to be set to 1.
   - Suppose the system is in a security state \( s \) of the CMDP. For every leaf node \( l \) whose bit in \( s \) is 0, a transition is built to state \( s' \), where all bits are the same as in \( s \), except that the bit related to \( l \) is set to 1.
   - It is observed that there is no adversarial action transition from a state where all bits are already set, i.e., the most insecure state.

The probabilities of state transition arcs \( k \in K \) in the CMDP are assigned based on the success rates of previous actions, computed using reports from local agents (see Section 4.5). Additionally, the reward function \( r: K \to \mathbb{R} \) must be calculated. The reward function \( r(s, a, s') \) is the payoff gained by the player who successfully takes action \( a \) in state \( s \) and causes a transition to \( s' \):

\[ r(s, a, s') = (\delta_g(s) - \delta_g(s')) \tau_1 C(a) \tau_2, \]

where \( 0 \leq \tau_1 \leq 1 \) and \( \tau_2 \leq 0 \) are fixed parameters. \( \delta_g(s) \) denotes the root node compromise probability of the ART graph \( g \) whose leaf nodes’ Boolean variables are set according to the bits in \( s \). This probability is computed using equations (1) and (3). Clearly, \( \delta_g(s') \leq \delta_g(s) \) since \( a \) is a response action. Furthermore, \( C(a) \) is the positive cost function for action \( a \), regardless of the source and destination states. This cost function should be defined specifically depending on the application; for instance, one reasonable option would be the mean-time-to-accomplish measure. Consequently, equation (4) assigns higher rewards to less costly actions that contribute more to the overall security state.

Having automatically generated the CMDP using the ART, RRE can now solve the decision process to find the optimal response action. As discussed earlier, due to a lack of knowledge about the attacker’s cost function, given the system’s current state, RRE uses the maximin approach to find the security strategy for the game. To do so, it must know the exact current state of the system. At every time instant, a reasonable choice (according to the leaf nodes) is state \( s \), where bits are 1 if their corresponding leaf nodes are set, and 0 otherwise. Thus, in local engines, where leaf nodes of ARTs are mapped to subsets of IDS alerts, the system’s current state consists of 1s for bits that represent satisfied leaf nodes according to received alerts, and 0s for other bits in \( s \).

As mentioned earlier, the fact that leaf nodes have been set does not necessarily mean that they are truly positive, as in equation (1); hence, uncertainty in received information prevents RRE from precisely figuring out the current security state of the system. However, the probability of being in each state is calculated as follows:

\[ b(s) = \prod_{l \in L} (1[s_l = 1] \cdot \delta(l) + 1[s_l = 0] \cdot (1 - \delta(l))), \]

where \( L \) is the set of leaf nodes in the ART, \( \delta(l) \) is computed as in equation (1), \( s_l \) is the bit in state \( s \) that corresponds to leaf node \( l \), and \( 1[\text{expr}] \) is the indicator function, which is 1 if expr is true and 0 otherwise. It is worth noting that equation (5) is based on the implicit assumption of independence among leaf nodes. Therefore, to consider uncertainty, instead of determining the exact current state of the system, we obtain a probability distribution \( b(.) \) on state space \( S \) (called the belief state) using equation (5). The axioms of probability require that \( 0 \leq b(s) \leq 1 \) for all \( s \in S \) and that \( \sum_{s \in S} b(s) = 1 \).

Uncertainty in updating inputs, i.e., IDS alerts, converts our Markov decision process into a higher-level model called a partially observable competitive Markov decision process (POCMDP), which is similar to the model described in [11] with the subtle difference that [11] studies simultaneous games, whereas the game here is sequential. Indeed, states \( b \in B \) in this higher-level model are probability distributions over a set of states \( S \) in the underlying Markov decision process model.

### 4.4. Optimal Response Strategy

As the last step in the decision-making process in local engines, RRE solves the POCMDP to find an optimal response action from its action space and sends an action command to its agents, which are in charge of enforcing the received commands. Action optimization in RRE is accomplished by trying to maximize the cumulative long-run reward measure received while taking sequential response actions. To accumulate sequential achieved rewards, we use the infinite-horizon discounted cost technique [12], which gives more weight to nearer future rewards. In other words, in each step, the game value is computed by recursively adding up the immediate reward after both players take their next actions and the discounted expected game value from then on.

To formalize the explanation just given, the solution of a POCMDP consists in computing an optimal policy, which is a function \( \pi^* \) that associates with any belief state \( b \in B \) an optimal action \( \pi^*(b) \), which is an action that maximizes the expected cumulative reward over the remaining temporal horizon of the game. As discussed above, this cumulative reward is defined as the discounted sum of the local rewards \( r \) that are associated with the actual action transitions. The Markov decision process theory assigns to every policy \( \pi \) a value function \( V^\pi \), which associates every belief state \( b \in B \) with an expected global reward \( V^\pi(b) \) obtained by applying \( \pi \) in \( b \). For finite-horizon POMDPs, the optimal value function is piecewise-linear and convex [25], and it can be represented as a finite set of vectors. In the infinite-horizon formulation, a finite vector set can closely approximate the optimal value function \( V^* \), whose shape remains convex. Bellman’s optimality equations (6) characterize in a compact way the unique optimal value function \( V^* \), from which an optimal policy \( \pi^* \), which is discussed later, can be easily derived:

\[ V^*(b) = \max_{a_r \in A_r(b)} \Psi(V^*, b, a_r), \]

where \( A(b) = \bigcup_{s \in S: b(s) \neq 0} A(s) \). \( A(.) \) is partitioned into \( A_r(.) \) and \( A_a(.) \) for response and adversary actions, respectively. \( \Psi \) is defined in (7), in which \( \rho \) is the POCMDP reward function. \( \rho \) is computed using the reward function \( r \) in the inherent CMDP:

\[ \rho(b, a, b') = \sum_{s, s' \in S} b(s) b'(s') r(s, a, s'). \]

Here, \( b'_{b, a, o} \) is the updated next belief state if the current state is \( b \), action \( a \) is taken, and observation \( o \) is received from sensors:

\[ b'_{b, a, o}(s') = P(s' | b, a, o) = \frac{P(o | s') \sum_{s \in S} P(s' | s, a) b(s)}{\sum_{s'' \in S} P(o | s'') \sum_{s \in S} P(s'' | s, a) b(s)}, \]

where, due to the independence assumption among the ART’s leaf nodes, we have:

\[ P(o | s) = \prod_{l \in L} (1[s_l = 1] \cdot P(o | l) + 1[s_l = 0] \cdot P(\bar{o} | l)), \]

where \( P(\bar{o} | l) = 1 - P(o | l) \), and \( P(o | l) \) is simply obtained using equation (2).

Once the partially observable decision process is formulated, the optimal response action is chosen based on the optimal value function. There are different techniques for getting the optimal value function. The decision-making unit in RRE uses a value iteration technique [3]:

\[ V_t(b) = \max_{a_r \in A_r(b)} \Psi(V_{t-1}, b, a_r), \]

which applies dynamic programming updates to gradually improve the value until it converges to the \( \epsilon \)-optimal value function, i.e., \( |V_t(b) - V_{t-1}(b)| < \epsilon \). Through improvement of the value, the policy is implicitly improved as well. Finally, the optimal policy \( \pi^* \) maps the system’s current belief state \( b \) to a response action:

\[ \pi^*(b) = \arg\max_{a_r \in A_r(b)} \Psi(V^*, b, a_r), \]

which, in local engines, is sent to RRE agents that are in charge of carrying out the received response action commands. Agents then send status messages to the decision-making unit, indicating whether the received action command has been accomplished successfully. If it has, the decision-making unit updates the leaf nodes and variables in the corresponding ART.

So far, we have discussed how RRE’s local engine estimates local security state and decides upon and takes local response actions following alerts received from the IDS. Next, we will address how RRE’s server makes use of local information received from local engines to estimate the security status of the whole network and then decide what global response actions to take. The information that is sent by local engines to RRE’s server consists of root probabilities \( \delta_g \), as computed in equation (3), of local ARTs. In the current implementation of RRE, these include three root node probabilities of three ART trees reflecting confidentiality, integrity, and availability of local host systems.

### 4.5. Agents

In the aforementioned security battle between RRE and the adversary, agents play a key role in accomplishing each step of the game. They are in charge of taking response actions decided on by RRE engines. Actually, having received commands from engines, agents try to carry them out successfully and report the result, whether they were successful or not, back to the engine. If the agent’s report indicates that some response action has been taken successfully, the engine updates the corresponding variables in its ART trees, which are leaf node values in the subtree for the successfully taken response action node. Consequently, as explained above, leaf node variables in ART trees are updated by two types of messages: IDS alerts and agents’ reports. Otherwise, if the agent cannot respond successfully (e.g., within a specific amount of time), the second-best action is sent by the engine to carry out.

### 5. Global Response and Recovery

Although host-based intrusion response is taken into account by RRE’s local engines using local ARTs and the IDS rule-set for computing assets, e.g., the SQL server, maintenance of global network-level security requires information about the underlying network topology and a profound understanding of what different combinations of secure assets are necessary to guarantee network security maintenance. In RRE, global network intrusion response is resolved in the global server, where, just as in local engines, ARTs are used for correlating received information, and then the maximin theory is applied to choose the optimal global response action. Such a choice is not possible in local engines due to either their limited local information or their inability to manage cooperation among distributed RRE agents.

In contrast to ARTs in local engines for computing assets that demand one-time design effort for each asset (as in IDS rule-sets), global ARTs in RRE’s server for network security should be designed specifically for each individual network in which RRE is deployed, since these higher-level ARTs depend on network topology, which implicitly affects a network’s global security state. In our current implementation, there is only one global, i.e., network-level, ART in RRE that must be designed by an expert. Generally, global ARTs in RRE’s server have the same structure discussed in Section 4.1, though some clarifying explanations are needed regarding their root and leaf nodes. As discussed in Section 4.4, local engines send their local security estimates, i.e., root node probabilities \( \delta_g \) of their ARTs, to RRE’s server. Indeed, local security estimates contribute to leaf nodes in the global ART in RRE’s server. Furthermore, the top-event node of the ART in the global engine is labeled “network security violation,” and is defined and formulated according to the underlying nodes. In other words, network security is defined by the global ART in RRE’s server using its leaf nodes, which are themselves root nodes of local ARTs in RRE’s local engines. The global ART is employed for the quantitative evaluation of a network’s security state. The correlation and response selection calculations are the same as in local ARTs (Section 4), except that for the global ART, the basic leaf node probability measures are computed as \( \delta(l) = \delta_g(l) \), where \( g(l) \) denotes the local ART corresponding to leaf node \( l \) of the global ART in RRE’s server.

### 6. Case Study: SCADA Networks

In this section, we describe the response selection process for a case study process control network for a power grid. We have chosen supervisory control and data acquisition (SCADA) networks as our case study for two reasons. First, since they control physical assets, they need timely response in the presence of attacks. Second, in contrast to general IT computer networks, they consist of computing assets with predefined specific responsibilities and communication patterns; this simplifies the design process for comprehensive ARTs and IDSes with thorough alert sets.

Figure 3 shows a sample 3-bus power grid and its SCADA network, which is responsible for monitoring and controlling the underlying power system. There are a total of three generators, any one of which is able to provide the power required by customers, i.e., load. To monitor the power system, each bus is attached to a sensor, i.e., a phasor measurement unit (PMU). The sensors send voltage phasors (i.e., magnitudes and phase angles) of the bus and current phasors of transmission lines connected to that particular bus to SCADA. Moreover, to control power generation, having received sensory data, SCADA computes optimal generation setpoints for individual generators. As shown in Figure 3, SCADA consists of different components, among which there are constrained communications. First of all, given noisy sensory data, the state estimation server is responsible for estimating the state of the whole power system. A database stores these states and other information that might be used later by administrators or customers through the web server. The human-machine interface (HMI) and security-constrained optimal power flow (SCOPF) compute control commands using those estimated states. As demonstrated, a hot spare HMI is also active and connected as part of the network.

Figure 4 illustrates a sample brief network-level ART for the process control network described above. The top event is chosen to be “SCADA is compromised,” and its children denote deficiencies in providing loads and report generation, which are two main goals of the supervisory network. For simplicity, leaf nodes here denote the compromise of individual host systems and are updated by local engines. As a case in point, G1, if set to 1, indicates that the controller device for the generator on bus 3 is compromised, and the upper response node “restart” shows that a countermeasure for the compromised controller is to reinstall the control software and restart the device. Details such as action costs, rates, and probabilities are not shown.

The cyber-security state space definition for the above attack-response tree is shown in Figure 5 as a binary vector, where each individual bit is set based on reports from local engines. For instance, the sample state vector in the figure indicates that HMI and G1 are compromised, according to reports from their corresponding local engines, while other hosts are in their normal operational mode. Given the attack-response tree and reports from local engines, RRE starts online construction of its accompanying partially observable competitive Markov decision process. Starting from the current state, i.e., \( s = (000100000010) \), the decision-making process continues.