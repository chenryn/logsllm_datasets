5
4
3
B
12
12
16
16
24
24
28
28
20
20
log n
rc
0.28
0.24
0.2
0.16
0.12
0.08
0.04
c
r
Figure 6: Pool Size versus B and rc
Time Cost of Pool Initialization. We measured the pool initializa-
tion time for several representative pool sizes and give the timings
in Table 3. Roughly, we observe that Pool can garble and “commit”
500K garbled-ANDs/second and check 850K garbled-ANDs/second.
However, we note that each pool size will favor a particular rc.
Depending on the type of the gadget, pool initialization takes only
seconds to dozens of minutes.
Table 3: Timings for Initializing Pool (seconds)
n
rc
AND
MUXCORE-8
10K
12.2%
0.34
1.92
100K
5.5%
2.94
18.82
1M
5.5%
29.5
188.53
10M
26.5%
411.4
2374.3
Pool Storage. To store each AND gate, the garbler needs to re-
member three wires and a garbled-table whereas the evaluator only
need to remember the commitments/i-hashes of the three wires
and a garbled-table. For MUXCORE gadgets, the number of wires and
garbled tables will depend on the width of the gadget. Table 4 shows
the exact number of bytes needed per AND and per 8-bit MUXCORE.
However, when Pool is optimized for storage in the way dis-
cussed in Section 4, the storage requirements drop to 16 bytes (for
the seed) on the garbler side and 32 bytes (for the root hash) on the
evaluator side.
Table 4: Sizes of Garbled Gate/Gadget (bytes)
Garbler’s Side Evaluator’s Side
AND
MUXCORE-8
288
1856
240
1584
7.2 MUX Optimization
Figure 7 shows the savings in computing oblivious multiplexers due
to the use of MUXCORE gadgets. We are able to reduce the overhead
of MUX of various widths by about 30%, which is largely in line with
our theoretical estimation based on the wire reduction rate and the
fraction of overall overhead (60–70%) spent on processing wires.
We have also evaluated the gain of this technique over several
applications that use multiplexers as building blocks. We observe
6–23% improvements (Figure 7b), since the exact savings now will
also depend on the proportion of MUX computation as well as the
widths of the MUXs being used.
7.3 Applications
In this study, our main focus is reactive computations such as
ORAMs that are difficult to realize with existing implementations
of actively-secure protocols. The left half of Table 5 shows the
per ORAM access time and bandwidth costs with either a basic
Circuit-ORAM or a full-blown one with seven recursions. Because
of pooling, we can execute these heavy-weighted computational
tasks without any delay for offline preprocessing. We note that se-
curely computing randomized ORAMs would be feasible with gate-
level BatchedCut protocols like JIMU [38], NST [25], and WRK [36].
However, no prior work of this kind exists, partly due to technical
concerns (such as the lack of well-defined programming interfaces)
and the anticipated high latency due to offline processing.
For comparison purposes, we also evaluated Pool with several
applications, including sort, hamming distance, and edit distance,
scaled at moderate-size circuits. For these applications, we are able
to run implementations provided by WMK [35] and JIMU [38],
whose performance is representative of the state-of-the-art proto-
cols in the malicious adversary model. Compared with the original
JIMU protocols [38], using a smaller pool of 35K garbled gates
already allows to reap almost all the benefit of batched cut-and-
choose. With a bigger pool with 35M garbled gates, we actually ob-
serve 1.6–2.6x improvements in time and bandwidth. The speedup
can also be explained by the significantly reduced memory us-
age thanks to the pool, which indirectly helps to reduce time due
to improved caching and faster memory accesses. Comparing to
WMK [35], Pool-Jimu is more efficient on Hamming Distances be-
cause Hamming Distance is secret-input-intensive while Pool-Jimu
inherits the efficiency of secret-input processing from JIMU [38].
On the other hand, WMK [35] is still more efficient in computation-
intensive applications such as Sort and Edit Distance, though its
cut-and-choose mechanism makes it infeasible to support reactive
computations such as ORAM.
We have also attempted to run these applications with sev-
eral other BatchedCut protocols including NST [25], RR [27], and
LR [19]. Unfortunately, these proof-of-concept implementations
1
1
1
1
0.75
0.73
0.73
0.71
MUX-8
MUX-16
MUX-32
MUX-64
With regular MUX
With optimized MUX
(a) Microbenchmarks
1
0.92
1
0.94
1
0.81
1
0.77
1
0.8
0.6
0.4
0.2
1
0.8
0.6
0.4
0.2
t
s
o
C
d
e
z
i
l
a
m
r
o
N
t
s
o
C
d
e
z
i
l
a
m
r
o
N
Sort
Edit Distance
With regular MUX
Basic
C-ORAM
With optimized MUX
Recursive
C-ORAM
(b) Applications
Figure 7: Efficiency Gain from Optimized MUX. (Applications
were run in the same way as they were measured in Table 5.)
do not provide explicit APIs for users to build applications that
were not already included in their implementation. It is also un-
clear how to efficiently calculate important protocol parameters
such as bucket size and check rates for general circuits. In fact, the
parameter selection procedures suggested in their security analy-
sis requires computing a great number of combinatorial formulas
that involve very big integers, which only worked for determining
parameters for a few specific small-scale scenarios but didn’t scale
up well in general.
Nonetheless, the micro-benchmarks reported in the literature
can still shed some light on the comparison. Researchers [38] have
shown that the performance of NST [25] is very similar to JIMU [38]
which we have included in Table 5. Wang et al. [35] show that
Table 5: Performance of Selected Applications. (Units of the numbers are either seconds or GB.)
Sort3
Hamm. Dist.4
Edit Dist.5
BW
Basic C-ORAM1
Time
BW
Recursive C-ORAM2
Time
BW
Hard to Support
(for its cut-and-choose mechanism)
Never Done Before
(for API and memory scalability issues)
46.6
279.6
31.4
174.7
6.38
3.96
15.4
83.2
11.2
48.0
1.9
1.08
I
J
K LAN
M
W
WAN
U LAN
M
WAN
) n = 35K
(LAN)
n = 35K
(WAN)
n = 35M
(LAN)
n = 35M
(WAN)
k
r
o
w
s
i
h
T
i
J
-
l
o
o
P
(
u
m
Time
46.2
575.3
228.4
1172.3
149.3
1168
87.1
662.6
14.2
BW Time
44.5
381.5
40.7
244.4
28.4
9.2
BW Time
302.3
3781.5
1677.2
8754
6.04
27.0
15.9
36.9
268.9
24.9
160
6.42
3.8
1247
9450
741.2
5333
93.8
203
215
127
When n = 35K, we set B = 5, rc = 12.5%; when n = 35M, we set B = 3, rc = 12.3%. Optimized MUXes are used whenever
possible.
1 Accessing an array of 10000 32-bit blocks using Circuit-ORAM without recursion (1.82M ANDs);
2 Accessing an array of 10000 32-bit blocks using Circuit-ORAM with a recursion factor of 8 and a cutoff threshold
of 256 (resulting in 2 levels of recursion totaling at 665K ANDS);
3 Sort 4096 32-bit numbers (10.2M ANDs);
4 Hamming distance between two 1M-bit strings (2.1M ANDs);
5 Edit distance between two 1024-nucleotide DNA (73.3M ANDs).
LR [19] and RR [27] are about 1.5–3x faster than WMK [35], hence
about 2–5x more efficient than Pool-Jimu. Note that LR and RR
require significant function-dependent offline processing, which
would adversely affect their general applicability. In comparison,
Pool-Jimu is able to run all these six applications (and any other
dynamically defined functions) using the same single pool of AND
gates.
We also note that Wang et al. has recently proposed WRK, a
highly efficient, constant-round protocol [36] based on authenti-
cated multiplicative triples and authenticated garbling. Section 8
discusses how the idea of Pool can be adapted to make WRK more
scalable.
Extreme Scales. To evaluate the scalability of Pool-Jimu, we have
run two single-threaded programs on two LAN-connected servers
(Intel Xeon 2.5 GHz) for executing actively-secure computations
with a pool of 16M gates. The service has been up non-stop for
seven days (until we intentionally shut it down), executing 47.3
billion gates at about 278M logical-ANDs/hour.
8 APPLYING POOL TO WRK
The pool idea can also be combined with WRK [36], an authenticated-
garbling-based protocol that is by far the fastest actively-secure
two-party computation scheme. We call our WRK-based protocol
Pool-WRK.
A Brief Overview of WRK. WRK is a constant-round two-party
computation protocol based on authenticated multiplicative triples,
i.e., (a1 ⊕ a2)∧(b1 ⊕ b2) = c1 ⊕ c2 where a1, a2, b1, b2, c1, c2 ∈ {0, 1}.
The protocol requires a preparation phase to generate a linear (in
the circuit size) number of such triples and distribute them properly
between the two parties. That is, P1 holds three bits a1, b1, c1 and
their authentication tags (127-bit each) ⟨a1⟩, ⟨b1⟩, ⟨c1⟩ that allow
P1 to later prove to P2 the use of authentic values of a1, b1, c1 when
needed; while similarly P2 holds a2, b2, c2 and ⟨a2⟩, ⟨b2⟩, ⟨c2⟩. Using
each authenticated multiplicative triple, the parties will collabora-
tively garble a binary AND gate such that (1) the permutation of
the garbled entries are determined by the authenticated random
bits from both parties; and (2) a honestly garbled entry in the ta-
ble can always be verified upon decryption. As a result, although
a malicious garbler could render some garbled entries invalid, it
cannot gain any information by observing failed gate evaluations
because in the adversary’s perspective the failures always happen
at random places.