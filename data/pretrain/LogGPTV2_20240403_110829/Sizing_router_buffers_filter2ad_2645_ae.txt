0.5 x
400
400
1 x
2 x
400
400
3 x
Link Utilization (%)
Pkts RAM Model Sim. Exp.
96.9% 94.7% 94.9%
99.9% 99.3% 98.1%
100% 99.9% 99.8%
100% 99.8% 99.7%
98.8% 97.0% 98.6%
99.9% 99.2% 99.7%
100% 99.8% 99.8%
100% 100% 99.8%
99.5% 98.6% 99.6%
100% 99.3% 99.8%
100% 99.9% 99.8%
100% 100% 100%
99.7% 99.2% 99.5%
100% 99.8% 100%
100% 100% 100%
100% 100% 99.9%
1 Mbit
2 Mbit
4 Mbit
8 Mbit
1 Mbit
2 Mbit
4 Mbit
4 Mbit
512 kb
1 Mbit
2 Mbit
4 Mbit
512 kb
1 Mbit
2 Mbit
4 Mbit
64
129
258
387
46
91
182
273
37
74
148
222
32
64
128
192
Figure 14: Comparison of our model, ns2 simulation
and experimental results for buﬀer requirements of
a Cisco GSR 12410 OC3 linecard.
5.2.1 Long Flows
n
Figure 14 shows the results of measurements from the
GSR 12410 router. The router memory was adjusted by
limiting the length of the interface queue on the outgoing
interface. The buﬀer size is given as a multiple of RT T×C√
,
the number of packets and the size of the RAM device that
would be needed. We subtracted the size of the internal
FIFO on the line-card (see Section 5.2.2). Model is the lower-
bound on the utilization predicted by the model. Sim. and
Exp. are the utilization as measured by a simulation with
ns2 and on the physical router respectively. For 100 and 200
ﬂows there is, as we expect, some synchronization. Above
that the model predicts the utilization correctly within the
measurement accuracy of about ±0.1%. ns2 sometimes pre-
dicts a lower utilization than we found in practice. We at-
tribute this to more synchronization between ﬂows in the
simulations than in the real network.
The key result here is that model, simulation and experi-
ment all agree that a router buﬀer should have a size equal
, as opposed to RT T × C (which
to approximately RT T×C√
in this case would be 1291 packets).
n
 1
 0.1
 0.01
 0.001
Exp. Cisco GSR if-queue
Exp. Cisco GSR buffers
Model M/G/1 PS
Model M/G/1 FIFO
FIFO
 0
 50
 100
 150
 200
 250
 300
 350
 400
Queue length [pkts]
Figure 15: Experimental, Simulation and Model
prediction of a router’s queue occupancy for a Cisco
GSR 12410 router.
5.2.2
Short Flows
In Section 4 we used an M/G/1 model to predict the buﬀer
size we would need for short-lived, bursty TCP ﬂows. To
verify our model, we generated lots of short-lived ﬂows and
measured the probability distribution of the queue length of
the GSR 12410 router. Figure 15 shows the results and the
comparison with the model, which match remarkably well.9
5.3 Scope of our Results and Future Work
The results we present in this paper assume only a single
point of congestion on a ﬂow’s path. We don’t believe our
9The results match very closely if we assume the router
under-reports the queue length by 43 packets. We learned
from the manufacturer that the line-card has an undocu-
mented 128kByte transmit FIFO. In our setup, 64 kBytes
are used to queue packets in an internal FIFO which, with
an MTU of 1500 bytes, accounts exactly for the 43 packet
diﬀerence.
results would change much if a percentage of the ﬂows ex-
perienced congestion on multiple links, however we have not
investigated this. A single point of congestion means there
is no reverse path congestion, which would likely have an
eﬀect on TCP-buﬀer interactions [28]. With these assump-
tions, our simpliﬁed network topology is fairly general. In
an arbitrary network, ﬂows may pass through other routers
before and after the bottleneck link. However, as we assume
only a single point of congestion, no packet loss and little
traﬃc shaping will occur on previous links in the network.
We focus on TCP as it is the main traﬃc type on the
internet today. Constant rate UDP sources (e.g. online
games) or single packet sources with Poisson arrivals (e.g.
DNS) can be modelled using our short ﬂow model and the
results for mixes of ﬂows still hold. But to understand traﬃc
composed mostly of non-TCP packets would require further
study.
Our model assumes there is no upper bound on the con-
gestion window. In reality, TCP implementations have max-
imum window sizes as low as 6 packets [29]. Window sizes
above 64kByte require use of a scaling option [30] which
is rarely used. Our results still hold as ﬂows with limited
window sizes require even smaller router buﬀers [1].
We did run some simulations using Random Early De-
tection [12] and this had an eﬀect on ﬂow synchronization
for a small number of ﬂows. Aggregates of a large number
(> 500) of ﬂows with varying RTTs are not synchronized
and RED tends to have little or no eﬀect on buﬀer require-
ments. However early drop can slightly increase the required
buﬀer since it uses buﬀers less eﬃciently.
There was no visible impact of varying the latency other
than its direct eﬀect of varying the bandwidth-delay prod-
uct.
Congestion can also be caused by denial of service (DOS)
that attempt to ﬂood hosts or routers with large amounts
of network traﬃc. Understanding how to make routers ro-
bust against DOS attacks is beyond the scope of this paper,
however we did not ﬁnd any direct beneﬁt of larger buﬀers
for resistance to DOS attacks.
6. RELATED WORK
Villamizar and Song report the RT T × BW rule in [1],
in which the authors measure link utilization of a 40 Mb/s
network with 1, 4 and 8 long-lived TCP ﬂows for diﬀerent
buﬀer sizes. They ﬁnd that for FIFO dropping discipline and
very large maximum advertised TCP congestion windows it
is necessary to have buﬀers of RT T ×C to guarantee full link
utilization. We reproduced their results using ns2 and can
conﬁrm them for the same setup. With such a small number
of ﬂows, and large congestion windows, the ﬂows are almost
fully synchronized and have the same buﬀer requirement as
a single ﬂow.
Morris [31] investigates buﬀer requirements for up to 1500
long-lived ﬂows over a link of 10 Mb/s with 25ms latency.
He concludes that the minimum amount of buﬀering needed
is a small multiple of the number of ﬂows, and points out
that for a bandwidth-delay product of 217 packets, each ﬂow
has only a fraction of a packet in transit at any time. Many
ﬂows are in timeout, which adversely eﬀects utilization and
fairness. We repeated the experiment in ns2 and obtained
similar results. However for a typical router used by a car-
rier or ISP, this has limited implications. Users with fast
access links will need several packets outstanding to achieve
adequate performance. Users with very slow access links
(e.g. 32kb/s modem users or 9.6kb/s GSM mobile access)
need additional buﬀers in the network so they have suﬃcient
packets outstanding. However this additional buﬀer should
be at the ends of the access link, e.g. the modem bank at the
local ISP, or GSM gateway of a cellular carrier. We believe
that overbuﬀering the core router that serves diﬀerent users
would be the wrong approach, as overbuﬀering increases la-
tency for everyone and is also diﬃcult to implement at high
line-rates. Instead the access devices that serve slow, last-
mile access links of under 1Mb/s should continue to include
a few packets worth of buﬀering for each link.
Avrachenkov et al [32] present a ﬁxed point model for
utilization (for long ﬂows) and ﬂow completion times (for
short ﬂows). They model short ﬂows using an M/M/1/K
model that only accounts for ﬂows but not for bursts. In
their long ﬂow model they use an analytical model of TCP
that is aﬀected by the buﬀer through the RTT. As the model
requires ﬁxed point iteration to calculate values for speciﬁc
settings and only one simulation result is given, we can not
directly compare their results with ours.
Garetto and Towsley [33] describe a model for queue
lengths in routers with a load below one that is similar to
our model in section 4. The key diﬀerence is that the au-
thors model bursts as batch arrivals in an M [k]/M/1 model
(as opposed to our model that models bursts by varying
the job length in a M/G/1 model). It accommodates both
slow-start and congestion avoidance mode, however it lacks
a closed form solution. In the end the authors obtain queue
distributions that are very similar to ours.
7. CONCLUSION
We believe that the buﬀers in backbone routers are much
larger than they need to be — possibly by two orders of
magnitude. If our results are right, they have consequences
for the design of backbone routers. While we have evidence
that buﬀers can be made smaller, we haven’t tested the hy-
pothesis in a real operational network. It is a little diﬃcult
to persuade the operator of a functioning, proﬁtable network
to take the risk and remove 99% of their buﬀers. But that
has to be the next step, and we see the results presented in
this paper as a ﬁrst step towards persuading an operator to
try it.
In the short-term, this is diﬃcult too.
If an operator veriﬁes our results, or at least demonstrates
that much smaller buﬀers work ﬁne, it still remains to per-
suade the manufacturers of routers to build routers with
fewer buﬀers.
In
a competitive market-place, it is not obvious that a router
vendor would feel comfortable building a router with 1% of
the buﬀers of its competitors. For historical reasons, the net-
work operator is likely to buy the router with larger buﬀers,
even if they are unnecessary.
Eventually, if routers continue to be built using the current
rule-of-thumb, it will become very diﬃcult to build linecards
from commercial memory chips. And so in the end, necessity
may force buﬀers to be smaller. At least, if our results are
true, we know the routers will continue to work just ﬁne,
and the network utilization is unlikely to be aﬀected.
8. ACKNOWLEDGMENTS
The authors would like to thank Joel Sommers and Profes-
sor Paul Barford from the University of Wisconsin-Madison
for setting up and running the measurements on a physical
router in their WAIL testbed; and Sally Floyd and Frank
Kelly for useful disucssions. Matthew Holliman’s feedback
on long ﬂows led to the central limit theorem argument.
9. REFERENCES
[1] C. Villamizar and C. Song. High performance tcp in
ansnet. ACM Computer Communications Review,
24(5):45–60, 1994 1994.
[2] Cisco line cards.
http://www.cisco.com/en/US/products/hw/modules/
ps2710/products data sheets list.html.
[3] R. Bush and D. Meyer. RFC 3439: Some internet
architectural guidelines and philosophy, December
2003.
[4] C. J. Fraleigh. Provisioning Internet Backbone
Networks to Support Latency Sensitive Applications.
PhD thesis, Stanford University, Department of
Electrical Engineering, June 2002.
[5] D. Ferguson. [e2e] Queue size of routers. Posting to
the end-to-end mailing list, January 21, 2003.
[6] S. H. Low, F. Paganini, J. Wang, S. Adlakha, and
J. C. Doyle. Dynamics of tcp/red and a scalable
control. In Proceedings of IEEE INFOCOM 2002, New
York, USA, June 2002.
[7] G. Appenzeller, I. Keslassy, and N. McKeown. Sizing
router buﬀers. Technical Report
TR04-HPNG-06-08-00, Stanford University, June
2004. Extended version of the paper published at
SIGCOMM 2004.
[8] S. Iyer, R. R. Kompella, and N. McKeown. Analysis of
a memory architecture for fast packet buﬀers. In
Proceedings of IEEE High Performance Switching and
Routing, Dallas, Texas, May 2001.
[9] C. Dovrolis. [e2e] Queue size of routers. Posting to the
end-to-end mailing list, January 17, 2003.
[10] S. Shenker, L. Zhang, and D. Clark. Some
observations on the dynamics of a congestion control
algorithm. ACM Computer Communications Review,
pages 30–39, Oct 1990.
[11] The network simulator - ns-2.
http://www.isi.edu/nsnam/ns/.
[12] S. Floyd and V. Jacobson. Random early detection
gateways for congestion avoidance. IEEE/ACM
Transactions on Networking, 1(4):397–413, 1993.
[13] L. Zhang and D. D. Clark. Oscillating behaviour of
network traﬃc: A case study simulation.
Internetworking: Research and Experience, 1:101–112,
1990.
[14] L. Qiu, Y. Zhang, and S. Keshav. Understanding the
performance of many tcp ﬂows. Comput. Networks,
37(3-4):277–306, 2001.
[15] G. Iannaccone, M. May, and C. Diot. Aggregate traﬃc
performance with active queue management and drop
from tail. SIGCOMM Comput. Commun. Rev.,
31(3):4–13, 2001.
[16] V. Paxson and S. Floyd. Wide area traﬃc: the failure
of Poisson modeling. IEEE/ACM Transactions on
Networking, 3(3):226–244, 1995.
[17] A. Feldmann, A. C. Gilbert, and W. Willinger. Data
networks as cascades: Investigating the multifractal
nature of internet WAN traﬃc. In SIGCOMM, pages
42–55, 1998.
[18] R. W. Wolﬀ. Stochastic Modelling and the Theory of
Queues, chapter 8. Prentice Hall, October 1989.
[19] F. P. Kelly. Notes on Eﬀective Bandwidth, pages
141–168. Oxford University Press, 1996.
[20] J. Cao, W. Cleveland, D. Lin, and D. Sun. Internet
traﬃc tends to poisson and independent as the load
increases. Technical report, Bell Labs, 2001.
[21] S. Floyd and V. Paxson. Diﬃculties in simulating the
internet. IEEE/ACM Transactions on Networking,
February 2001.
[22] R. Morris. Scalable tcp congestion control. In
Proceedings of IEEE INFOCOM 2000, Tel Aviv, USA,
March 2000.
[23] S. B. Fredj, T. Bonald, A. Prouti`ere, G. R´egni´e, and
J. Roberts. Statistical bandwidth sharing: a study of
congestion at ﬂow level. In Proceedings of SIGCOMM
2001, San Diego, USA, August 2001.
[24] Personal communication with stanford networking on
characteristics of residential traﬃc.
[25] Cisco 12000 series routers.
http://www.cisco.com/en/US/products/hw/
routers/ps167/.
[26] J. Sommers, H. Kim, and P. Barford. Harpoon: A
ﬂow-level traﬃc generator for router and network test.
In Proceedings of ACM SIGMETRICS, 2004. (to
appear).
[27] I. Cisco Systems. Netﬂow services solution guido, July
2001. http://www.cisco.com/.
[28] L. Zhang, S. Shenker, and D. D. Clark. Observations
on the dynamics of a congestion control algorithm:
The eﬀects of two-way traﬃc. In Proceedings of ACM
SIGCOMM, pages 133–147, September 1991.
[29] Microsoft. Tcp/ip and nbt conﬁguration parameters
for windows xp. Microsoft Knowledge Base Article -
314053, November 4, 2003.
[30] K. McCloghrie and M. T. Rose. RFC 1213:
Management information base for network
management of TCP/IP-based internets:MIB-II,
March 1991. Status: STANDARD.
[31] R. Morris. Tcp behavior with many ﬂows. In
Proceedings of the IEEE International Conference on
Network Protocols, Atlanta, Georgia, October 1997.
[32] K. Avrachenkov, U. Ayesta, E. Altman, P. Nain, and
C. Barakat. The eﬀect of router buﬀer size on the tcp
performance. In Proceedings of the LONIIS Workshop
on Telecommunication Networks and Teletraﬃc
Theory, pages 116–121, St.Petersburg, Russia, Januar
2002.
[33] M. Garetto and D. Towsley. Modeling, simulation and
measurements of queueing delay under long-tail
internet traﬃc. In Proceedings of SIGMETRICS 2003,
San Diego, USA, June 2003.