understand what makes a communication tool secure. P8 said
that companies do not provide a clear deﬁnition of security
because “things are always changing”, and what is secure
today will not be secure tomorrow. Legal liability is seen as
another reason: P26 believes companies want to be able to
change the deﬁnition of security in privacy policies in response
to developments.
Security is expensive. P3, P19, P22 and P26 believe none
of the tools are secure because security is expensive, and the
companies who own these tools put proﬁt ﬁrst. They said that
PII and conversations are not protected because most tools
are free. Without data collection, advertisements cannot be
generated and, hence, there will be no proﬁts.
Past experiences. P19 and P22 believe that all messengers
are secure because they have never experienced a breach.
P24 and P46, in contrast, experienced a security breach with
Yahoo! Messenger: “But, talking about this Yahoo! thing, my
Yahoo! email account is probably one of the least secure
because actually, you know, it has got hacked again recently”
(P46). Hence, they believe all tools are insecure.
Security is not possible. P8 believes that “completely
secure” tools exist only in theory. Due to bugs, software can be
attacked and communications traced. P2 and P12 were the only
participants to mention that one can evaluate the security of a
tool based on how well the program is written, and that source
code should be audited. P12, however, believes that audits need
to be conﬁdential because the designs of secure tools should
not be published (see Section IV-D on threat models).
H. EFF Secure Messaging Scorecard
We provided our participants with the ﬁrst-generation EFF
Secure Messaging Scorecard [2] (printed on a sheet of pa-
per), and invited them to compare their rankings with those
149
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:09 UTC from IEEE Xplore.  Restrictions apply. 
of the scorecard. Not a single participant gave a ranking
that reﬂected the scorecard. The scorecard contains seven
security criteria. Four criteria are completely misunderstood:
participants do not appreciate the difference between point-
to-point and E2E encryption, and do not comprehend forward
secrecy and ﬁngerprint veriﬁcation. The other three criteria
reﬂecting open design (documentation, open-source code and
security audits) are considered to be negative, with participants
believing security requires obscurity. We describe below how
participants perceive the importance of the scorecard’s criteria.
Encrypted in transit vs. encrypted so the provider
can’t read it. 57 participants (except for P2, P4 and P5) do
not differentiate between point-to-point encryption and E2E
encryption. Recent literature [41] suggests that users develop
more trust in an encrypted communication system that makes
the cipher-texts visible. However, whether the cipher-text is
visible or not, our participants do not know what security
properties each tool offers, and they (incorrectly) believe that
encryption can be broken anyway (see Section IV-D).
Can you verify contact’s identity? Recent studies [50],
[51] have assessed the usability and security of various repre-
sentations of veriﬁcation ﬁngerprints. However, no participant
(except for P2) appreciates why some communication tools
can verify a contact’s identity (i.e., the role of ﬁngerprints).
Are past communications secure if your keys are stolen?
All participants (except for P2 and P5) do not recognize the
importance of forward secrecy.
Open design. The EFF Scorecard has three explicit criteria
to ensure the design and code have undergone independent
reviews. Our participants, in contrast, said proprietary tools
are more secure. This belief in “security by obscurity”, an
anathema to security researchers, stems from the fact that users
perceive security properties to be akin to trade secrets: if a
skilled attacker learns how a tool works, they can compromise
it. This fundamental misconception feeds the perception of
futility. Only P2, P5 and P28 appreciate open design.
V. DISCUSSION
Most user studies of secure communication tools, in particu-
lar encrypted email, have been lab studies conducted following
the same pattern (see Section II): assessing the usability of
speciﬁc tools in an artiﬁcial setting, where participants are
given a series of security tasks associated with those tools
(e.g., managing keys, sharing keys, encrypting a message)
with ﬁctional communication partners (study coordinators) to
accomplish a particular security goal (e.g., conﬁdentiality)
without errors, and then measuring success, or failure, based
on the goals and tasks imposed on participants, rather than
being their own.
Indeed, users will not adopt a communication tool if they
cannot use it effectively and efﬁciently. Our study identiﬁed
some usability problems (e.g., participants who used Telegram
were not able to recognize the Secret Chat mode). However,
our results also show that to be adopted, secure tools have
to offer their intended users utility; i.e., the ability to reach
their communication partners. Security may be part of users’
150
primary communication goals, but given a choice between a
usable and secure tool that does not offer utility and a usable
but insecure tool that does, users choose the latter. Our results
suggest it is unrealistic to expect that users will switch to
secure tools and only communicate with those who do the
same. Also, they will not expend the effort associated with
maintaining two communication tools (one secure and one
insecure) depending on whom they are talking to. For example,
our participants with iOS devices used WhatsApp and Skype,
instead of iMessage and FaceTime, even when communicating
with other Apple users. Although they perceived the Apple
services as more secure (see Section IV-G), they did not live
in an Apple-only universe; using different tools was perceived
as an overhead they were not willing to carry for security.
When a new tool is usable and attractive enough, users
may accept the initial switching cost and adopt it. However,
creating a new tool that will be adopted by a critical mass of
users requires resources and a set of skills (e.g., user research,
user experience design, communication, affective interaction,
marketing) the creators of secure communication tools do
not have at their disposal. If we want users to adopt secure
communications in the near future, security engineers should
consider putting their skills to securing tools that have a large
use base. WhatsApp’s implementation of E2E encryption for
text, voice calls and video communications is an example of
this more pragmatic approach [18].
In [61], De Luca et al. found that security and privacy are
not a primary factor that drives users to adopt a particular
messenger. We argue that this is not because users do not
care about security at all. Users are aware of some threats and
willing to make some effort to manage them (e.g., by chopping
up credentials into segments and sending these via different
tools). Our participants preferred these quite cumbersome
processes, instead of using a secure tool, because they did not
believe the tools available are actually secure. This impression
was fed by several misconceptions (e.g., they believed service
providers can read E2E-encrypted messages). Besides the lack
of usability and utility, such misconceptions undermined the
case for adoption in their eyes.
There are some users who want
to be secure and are
“shopping” for tools that offer speciﬁc security properties.
The EFF Secure Messaging Scorecard [2] aims to tell users
about what security properties various communication tools
actually offer. Our ﬁndings show that the scorecard is not
supporting typical users effectively because our participants
did not understand these ﬁne-grained security properties. In-
deed, participants believed these properties are either impos-
sible to achieve or detrimental to security (like open design).
These misunderstandings cannot be ﬁxed by just changing the
wording on the scorecard, as our results show that participants
had very inaccurate understanding of fundamental security
properties, such as conﬁdentiality (see Section IV-E).
The key takeaway from mental models research is that
non-experts do not understand abstract security properties.
They can only understand why a property matters in the
context of a speciﬁc threat model that matters to them. For
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:09 UTC from IEEE Xplore.  Restrictions apply. 
example, if users do not want their service providers to be
able to read their messages, we need to explain how E2E
encryption protects against this threat. Based on our results,
our participants’ existing models were the “toxic root” of their
belief that ultimately using any form of a secure tool is futile
because they believed even the best encryption scheme can
be broken by the resources and skills of governments and
service providers. We need to make users understand that it is
in their power to protect themselves because several security
mechanisms have been developed based on the best available
knowledge from security research, and are open to audits by
security researchers and practitioners.
Based in part on our feedback, the EFF is redesigning
the scorecard to group tools into general tiers from “most
secure” to “insecure”. Instead of check marks for speciﬁc
properties,
textual descriptions will be provided for what
security properties each tool provides. The goal is to help
casual readers correctly understand which tools are considered
secure (e.g., E2E-encrypted) without needing to understand
security mechanisms speciﬁcally, while also providing text to
help readers acquire accurate mental models of conﬁdentiality,
integrity and authentication. The scorecard will also attempt to
provide more non-security information that users desire: Does
the tool have a large user base? What devices/platforms is it
available on? Can it be used over 3G and Wi-Fi? Does it offer
audio or video chats? Is the tool free? While not necessarily
related to security and privacy, these items drive adoption and
would be recommended to include them in the scorecard.
A ﬁnal interesting high-level observation is that while efforts
to secure email systems with PGP that were interoperable
across email providers failed on the usability front, current
approaches (e.g., iMessage) succeed on the usability front
at the expense of interoperability with different devices. We
believe examining whether some of the lessons learnt from
securing these communication tools can be transferred to
interoperable secure tools without sacriﬁcing usability is an
interesting open research question for the security community.
VI. CONCLUDING REMARKS
Our research, based on 10 unstructured and 50 semi-
structured interviews, provides the broadest study of user
perceptions of secure communications to date. Although our
participants have experienced usability issues with different
communication tools,
the primary obstacles
to adopting secure tools. Low motivation to adopt secure
communications is due to several factors (e.g., small user
bases,
incorrect mental models of
how secure communications work). Based on our ﬁndings,
we conclude with three concrete recommendations:
lack of interoperability,
these are not
Secure tools with proved utility. We encourage the security
community to prioritize securing the communication tools
that have already been adopted by mainstream users over
improving the usability of different secure tools. Users’ goal to
communicate with others overrides everything else, including
security. Growing a user base for a new tool is difﬁcult and
unpredictable. Therefore, we encourage security researchers to
work with today’s existing popular tools.
Understand the target population. In the long run, if
security developers want to develop new paradigms and secure
communication tools using a user-centered design process,
they need to understand users’ goals and preferences. The
technical security community must develop a deeper under-
standing of what is important (and not important) to users.
Security properties and threats should be framed in terms that
users can understand.
Improve QoS. Secure communication tools must feel pro-
fessional. Security itself is difﬁcult for users to evaluate
directly;
they often use proxy signals. This suggests that
engineering effort spent on improving the performance of
cryptographic tools still matters to the extent that it can reduce
latency and dropped packets.
VII. ACKNOWLEDGMENTS
We thank the reviewers for their helpful comments and
suggestions. This work is supported by a gift from Google.
Joseph Bonneau is supported by a Secure Usability Fellowship
from the Open Technology Fund and Simply Secure.
REFERENCES
[1] N. Unger, S. Dechand, J. Bonneau, S. Fahl, H. Perl, I. Goldberg, and
M. Smith, “SoK: Secure Messaging,” in IEEE Symposium on Security
and Privacy, 2015, pp. 232–249.
[2] Electronic Frontier Foundation (EFF), “Secure Messaging Scorecard,”
https://www.eff.org/secure-messaging-scorecard, accessed on:
09.07.2016.
[3] D. Yadron, “Apple Transparency Report: Over 1,000 Government
Requests for User Data,” The Guardian, 2016.
[4] S. Gibbs, “Gmail Does Scan All Emails, New Google Terms Clarify,”
The Guardian, 2014.
[5] R. Anderson, “Why Cryptosystems Fail,” in ACM Conference on
Computer and Communications Security, 1993, pp. 215–227.
[6] S. Fahl, M. Harbach, H. Perl, M. Koetter, and M. Smith, “Rethinking
SSL Development in an Appiﬁed World,” in ACM Conference on
Computer and Communications Security, 2013, pp. 49–60.
[7] A. Whitten and J. D. Tygar, “Why Johnny Can’t Encrypt: A Usability
Evaluation of PGP 5.0,” in USENIX Security Symposium, 1999.
[8] S. L. Garﬁnkel and R. C. Miller, “Johnny 2: A User Test of Key
Continuity Management with S/MIME and Outlook Express,” in ACM
Symposium on Usable Privacy and Security, 2005, pp. 13–24.
[9] S. Clark, T. Goodspeed, P. Metzger, Z. Wasserman, K. Xu, and
M. Blaze, “Why (Special Agent) Johnny (Still) Can’t Encrypt: A
Security Analysis of the APCO Project 25 Two-Way Radio System,”
in USENIX Security Symposium, 2011, pp. 8–12.
[10] M. Lee, “Encryption Works: How to Protect Your Privacy in the Age
of NSA Surveillance,” Freedom of the Press Foundation, 2013.
[11] “Tips, Tools and How-tos for Safer Online Communications,”
https://ssd.eff.org/en, accessed on: 19.08.2016.
[12] McGregor, Susan E, “Digital Security and Source Protection for
Journalists,” http://towcenter.org/
digital-security-and-source-protection-for-journalists-research-by-
susan-mcgregor/, accessed on: 20.08.2016.
[13] “The OpenPGP Alliance Home Page,”
http://www.openpgp.org/resources/downloads.shtml, accessed on:
20.08.2016.
[14] “Tor,” https://www.torproject.org/projects/torbrowser.html.en, accessed
[17] “SecureDrop: The Open-source Whistleblower Submission System,”
https://securedrop.org/, accessed on: 20.08.2016.
[15] “Tails: The Amnesic Incognito Live System,” https://tails.boum.org/,
[16] “Off-the-Record Messaging,” https://otr.cypherpunks.ca/, accessed on:
on: 20.08.2016.
accessed on: 20.08.2016.
20.08.2016.
151
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:09 UTC from IEEE Xplore.  Restrictions apply. 
[18] Natasha Lomas, “WhatsApp Completes End-to-End Encryption
Rollout,” https://techcrunch.com/2016/04/05/
whatsapp-completes-end-to-end-encryption-rollout, accessed on:
09.09.2016.
[19] A. J. Onwuegbuzie and N. L. Leech, “Validity and Qualitative
Research: An Oxymoron?” Quality & Quantity, vol. 41, no. 2, pp.
233–249, 2007.
[20] A. Strauss and J. Corbin, “Grounded Theory Methodology,” Handbook
of Qualitative Research, pp. 273–285, 1994.
[21] B. Harry, K. M. Sturges, and J. K. Klingner, “Mapping the Process:
An Exemplar of Process and Challenge in Grounded Theory
Analysis,” Educational Researcher, vol. 34, no. 2, pp. 3–13, 2005.