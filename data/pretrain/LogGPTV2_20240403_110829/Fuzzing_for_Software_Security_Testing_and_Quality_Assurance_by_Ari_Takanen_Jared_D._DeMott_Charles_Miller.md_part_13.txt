Quality Assurance and Testing
The purpose of this chapter is to give you some relevant background information
if you would like to integrate any form of fuzzing into your standard software
testing processes. This topic may be familiar to you if you have experience in any
type of testing including fuzzing as part of the software development process. You
might disagree with some of the arguments presented. This is not exactly the same
information you would find in generic testing textbooks, rather, it is based on our
practical real-life experience. And your experience might differ from ours.
Our purpose is not to describe testing processes and experiences, but we urge
you to look for a book on testing techniques, if you are interested in learning more
on this topic. Indeed, many of the topics discussed in this chapter have been touched
on in the amazing book called Software Testing Techniques, 2nd edition, written by
Boris Beizer in 1990. Back then, fuzzing was called syntax testing or fault injection,
as you will learn here. We highly recommend that you read that book if you work
in a testing profession. In this chapter, we will look at fuzzing from the eyes of a
quality assurance professional, identifying the challenges of integrating fuzzing in
your QA methods. We will leverage the similar nature of fuzzing when compared
to more traditional testing techniques in functional testing.
For readers with a security background, this chapter gives an overview of the
quality assurance techniques typically used in the software development life cycle
(SDLC), with the purpose of introducing common terminology and definitions.
The focus is on testing approaches that are relevant to fuzzing techniques,
although we briefly mention other techniques. To those who are new to both the
security assessment and testing scenes, we provide all the information you will need
to get started. We also recommend further reading that will give you more detailed
information on any of the presented testing approaches.
3.1 Quality Assurance and Security
How is quality assurance relevant to the topic of fuzzing? In short, software qual-
ity issues, such as design flaws or programming flaws, are the main reason behind
most, if not all, known software vulnerabilities. Quality assurance practices such as
validation and verification, and especially software testing, are proactive measures
used to prevent the introduction of such flaws, and to catch those quality flaws that
are left in a product or service before its initial release. Fuzzing is one of the tools
that will help in that process.
73
6760 Book.indb 73 12/22/17 10:50 AM
74 Quality Assurance and Testing
On the other hand, traditional vulnerability assurance practices have typically
taken place in a very late phase of the software development life cycle. Most security
assessments are reactive: They react to security-related discoveries (bugs) in software.
They focus on protecting you from known attacks and in identifying known vul-
nerabilities in already deployed systems. Although traditional security assessment,
consisting of running security scanners and other vulnerability detection tools,
does not attempt to find anything new and unique, it is still well suited for postde-
ployment processes. But for really efficient QA purposes, we need something else.
The main reason why we will discuss quality assurance in this book is to show
how current quality assurance processes can be improved if fuzzing is integrated
within them. Fuzzing is very different from vulnerability scanners, as its purpose is
to find new, previously undetected flaws. The discovery of those flaws after deploy-
ment of the software is costly. Fuzzing tools are very much like any traditional test-
ing tools used in quality assurance practices. Still, unfortunately, fuzzing is often
not part of the product development process. Security assessment using fuzzing is
almost always performed on a completed or even deployed product. Only vulner-
ability assessment professionals usually conduct fuzzing. Hopefully, this will begin
to change as testers realize the benefits that fuzzing can bring to the process.
Quality assurance is also an interesting topic to vulnerability assessment people
due to the possibility of learning from those practices. Although security experts
often focus on looking for known vulnerabilities in released products, sometimes
the processes and tools used by security assessment experts can be very similar to
those used by quality assurance professionals who take a more proactive approach.
Vulnerability assessment professionals already use many of those same processes
and tools, as you will see.
3.1.1 Security in Software Development
Security testing, as part of a quality assurance process, is a tough domain to explain.
This is partly because of the vagueness of the definition. As far as we know, there is
no clear definition for security testing. Far too many product managers view security
as a feature to be added during software development. Also, for some end users,
security is a necessary but very difficult-to-define property that needs to be added
to communications products and services. Both of these definitions are partly cor-
rect, as many security requirements are fulfilled with various security mechanisms.
Think of encryption or authentication. These are typical security features that
are implemented to protect against various mistakes related to confidentiality and
integrity. A security requirement will define a security mechanism, and testing for
that requirement can sometimes be very difficult. Some R&D managers have a
misconception that when all security requirements have been tested, the security
test is complete. For example, a team of developers at a company we worked with
felt they had excellent security and had designed their applications with security in
mind at every step of development. They implemented complex authentication and
authorization code and utilized strong encryption at all times. However, they had
never heard of buffer overflows and command injection flaws, or didn’t think they
were relevant. Consequently, their applications were vulnerable to many of these
implementation-level vulnerabilities.
6760 Book.indb 74 12/22/17 10:50 AM
3.2 Measuring Quality 75
3.1.2 Security Defects
One of the main reasons behind compromises of security are implementation mis-
takes—simple programming errors that enable the existence of security vulner-
abilities—and the existence of attacks such as viruses and worms that exploit those
vulnerabilities. End users neither care to nor have the skills necessary to assess the
security of applications. They rely on quality assurance professionals and, unwit-
tingly, on security researchers.
Certainly, some security features may be of interest to end users, such as the
presence and strength of encryption. Nevertheless, flaws such as buffer overflows
or cross-site scripting issues comprise a majority of security incidents, and malicious
hackers abuse them on a daily basis. It is uncommon that anyone actually exploits
a flaw in the design of a security mechanism, partly because those techniques are
today based on industry-proven reusable libraries. For example, very few people will
implement their own encryption algorithm. In general, it is a very bad idea to imple-
ment your own security library, as you are almost doomed to fail in your attempt.
This is another example in which it doesn’t make sense to reinvent the wheel.
In software development, quality assurance practices are responsible for the
discovery and correction of these types of flaws created during the implementation
and design of the software.
3.2 Measuring Quality
What is good enough quality? How do we define quality? And how can we measure
against that quality definition? These are important questions, especially because
it is impossible with current technologies to make complex code perfect. In all
quality assurance-related efforts, we need to be able to say when the product is
ready. Like the software developer who defines code as being ready by stating that
“it compiles,” at some point testers need to be able to say “it works and is mostly
free of bugs.” But, as everyone knows, software is far from ready when it compiles
for the first time. In similar fashion, it is very difficult to say when software really
works correctly.
Similarly, product security is also a challenging metric. When can we say that
a product is secure enough, and what are the security measures needed for that?
3.2.1 Quality Is About Validation of Features
The simplest measurement used in testing is checking against the features or use
cases defined in the requirement or test specifications. These requirements are then
directly mapped to individual test cases. If a test cycle consists of a thousand tests,
then each test has to have a test verdict that defines whether it passed or failed.
A requirement for systematic testing is that you know the test purpose before-
hand. This is the opposite of kiddie testing, in which any bug found in the test is
a good result, and before the test is started there is very little forecast as to what
might be found. Note that we do not want to downplay this approach—on the
contrary! Any exploratory testing approaches are very good at testing outside the
specifications, and a good exploratory tester will always find unexpected flaws in
6760 Book.indb 75 12/22/17 10:50 AM
76 Quality Assurance and Testing
software. The out-of-the-box perspective of exploratory testing can reveal bugs
that might be missed by testers blinded by the specifications. But there is always a
risk involved when the quality of the tests is based on chance and on the skills of
the individual tester.
Common technique for defining test success in functional, feature-oriented
black-box testing is by using an input/output oracle, which defines the right coun-
terpart for each request (or the right request to each response if testing client soft-
ware). Similarly, if you are able to monitor the internals of the software, an oracle
can define the right internal actions that have to be performed in a test.
A 100% success rate based on feature testing means everything that was speci-
fied in the test specification was tested, and the software passed the specified tests.
This metric is very feature-oriented, as it can be very challenging to proactively
assign verdicts to some tests during the test specification phase.
Fuzzing is an excellent example in which a test can consist of millions of test
cases, and whether each test case passes or fails is very difficult to assess. A strict
test plan that requires a pass/fail criterion for every single test case in the specifica-
tion phase will restrict the introduction of new testing techniques such as fuzzing.
Let’s look at an example from the fuzzing perspective.
In the protocol standardization side, IETF has defined a set of tests for test-
ing different anomalous communication inputs for the SIP1 protocol. IETF calls
these test specifications torture tests. Many commercial test tools implement these
tests, but when you think about them from a fuzzing perspective, the test coverage
in these specifications is very limited. An example test description from SIP RFC
4475 is shown below:
3.1.2.4. Request Scalar Fields with Overlarge Values
This request contains several scalar header field values outside
their legal range.
o The CSeq sequence number is >2**32-1.
o The Max-Forwards value is >255.
o The Expires value is >2**32-1.
o The Contact expires parameter value is >2**32-1.
An element receiving this request should respond with a 400 Bad
Request due to the CSeq error. If only the Max-Forwards field
were in error, the element could choose to process the request as
if the field were absent. If only the expiry values were in
error, the element could treat them as if they contained the
default values for expiration (3600 in this case).
Other scalar request fields that may contain aberrant values
include, but are not limited to, the Contact q value, the
Timestamp value, and the Via ttl parameter.
Most negative tests actually come in predefined test suites. The first such test
suites were released by the PROTOS research from the University of Oulu. PRO-
TOS researchers have provided free robustness testing suites for numerous proto-
cols since 1999, including tests for SIP released in 2002. One PROTOS test case
1 IETF RFC 4475 “Session Initiation Protocol (SIP) Torture Test Messages.”
6760 Book.indb 76 12/22/17 10:50 AM
3.2 Measuring Quality 77
description in which the SIP method has been replaced with an increasing string of
a characters is shown below:
aaaaaaaaaaaaaaaaa sip: SIP/2.0
Via: SIP/2.0/UDP :;branch=z9hG4bK00003
From: 3 >;tag=3
To: Receiver >
Call-ID: @
CSeq:  INVITE
Contact: 3 >
Expires: 1200
Max-Forwards: 70
Content-Type: application/sdp
Content-Length: 
v=0
o=3 3 3 IN IP4 
s=Session SDP
c=IN IP4 
t=0 0
m=audio 9876 RTP/AVP 0
a=rtpmap:0 PCMU/8000
PROTOS uses a BNF-style grammar to model the entire communication pro-
tocol, and that can be seen in the generated test case descriptions as  elements
that represent changing values in the test cases. The test execution engine, or test
driver, will replace these fields with the dynamic values required during the execu-
tion of the test.
As you can see, the IETF approach is rather different from the PROTOS approach.
Instead of a limited coverage of tests for each test requirement, the PROTOS SIP
test suite contains more than 4,500 individual test cases that systematically add
anomalies to different header elements of the protocol. Instead of one test case per
negative requirement, the test suite will execute a range of tests to try out different
unexpected values and exercise unusual corner cases. Test cases can be configured
with command-line options, and some dynamic functionality has been implemented
for protocol elements such as Content-Length, as shown above. PROTOS tests were
generated using a proprietary Mini-Simulation technology, which basically can be
thought of as a general-purpose fuzzing framework.2 In the IETF torture test suite,
the correct responses to error situations are defined, whereas PROTOS ignores the
responses and does not try to define the correct behavior under corrupted or hostile
situations. The approach of defining the responses to attacks limits the possible test
coverage of torture tests and any other testing approach based on test requirements
and use cases. Most fuzzers behave the same way as PROTOS suites did—that is,
the responses are rarely checked against any test oracle.3 For those testers who
2 The PROTOS Mini-Simulation framework was later acquired by Codenomicon.
3 A test oracle is the automated decision-making process that compares the received responses against
expected responses (input/output oracle) and makes the verdict if behavior was correct.
6760 Book.indb 77 12/22/17 10:50 AM
78 Quality Assurance and Testing
have trouble thinking about using test cases in which the response is unknown, it
is important to note that the purpose of fuzzing is not about verifying features, it
is about finding crash-level defects.
3.2.2 Quality Is About Finding Defects
Quality assurance aims to reduce defects in software through two means. The first
way is by making it more difficult for people to introduce the defects in the first
place. The second, and more relevant means of defect reduction from the fuzzing
perspective, is using various methods of finding bugs. When integrating fuzzers into
quality assurance, you need to remember both these requirements.
Quality assurance should not only be about validating correctness. Sometimes
finding just one flaw is enough proof of the need for improvement, and whether
it takes fifty or five million tests to find it is irrelevant. If you find one flaw, you
can be sure that there are others. Bugs often appear in groups, and this is typical
because the same person (or team) tends to make similar mistakes in other places in
their code. A common process failure created by the traditional patch-and-penetrate
race is that when in a hurry, a person tends to focus all efforts on finding and fix-
ing that one specific flaw, when the same flaw could be apparent just 10 lines later.
Even a good programmer can make mistakes when in a hurry, or when having a
bad day. If a programmer does not pay attention to the entire module when fixing
security problems, he or she will most probably never have a chance to review that
piece of code again.
Quality assurance is hunting for bugs in software, by whatever means. This
should be the mental mode for testers: Testers are bug hunters. It is quite common
that in real-life software development, there might be no real bug hunters involved
in the testing process at all. The results of this type of destructive testing can be
annoying to some organizations that are more used to the positive thinking of
validating and verifying (V&V) functionality. Still, the ultimate purpose is not to
blame the designers and the programmers for the found flaws, but rather find and
remove as many problems as possible.
3.2.3 Quality Is a Feedback Loop to Development
Quality assurance is also used to validate the correctness of the development process.
For quality assurance people, the driving motivation is to be able to assist devel-
opers in building better systems and potentially to improve the software develop-
ment process at the same time. A category of flaws that consistently appears and is
caught in the late phases of software development calls for a change in earlier steps
in the process. Security flaws are a good example of such a flaw category. If buffer
overflow vulnerabilities are consistently found in products ready to deploy, the best
solution is to radically improve the development practices.
Note that many security flaws go by different names in different phases of the
software development process. During unit testing, a tester might presume that a
boundary value flaw is not critical and will not label the bug as such. But the same
boundary value flaw may be labeled as critical if detected during integration testing.
Understanding these links is critical, so that people use the same terminology and
have the same understanding of severity of bugs when discussing flaws.
6760 Book.indb 78 12/22/17 10:50 AM
3.3 Testing for Quality 79
3.2.4 Quality Brings Visibility to the Development process
Quality assurance is a metric of the software development process. With good
quality assurance processes, we are able to get visibility into the software develop-
ment process and the current status of the software. Integration of system units and
software modules is one measurement of the software process.
When a module is ready and tested, it can be labeled as completed. The soft-
ware industry is full of experiences in which the software has been 90% ready for
half of the development time. Security testing should also be an integral part of the
software development life cycle and not a delay at the end that adds to this miscon-
ception of almost ready. Knowing the place and time for security testing enables
product managers to understand the requirements of security testing from a time
(and money) perspective.
3.2.5 End Users’ perspective
Quality assurance is a broad topic and we need to narrow it down to be able to
explain the selected parts in enough detail. Defining quality is a challenging task,
and different definitions apply to different categories of quality. For example, tests
that validate security properties can be very complex, and trust in their verdicts is
sometimes limited. The definition of quality depends on who is measuring it.
For many testers, the challenge is how to measure and explain the efficiency of