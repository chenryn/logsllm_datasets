the neighborhood of the node. LQSR piggy-backs Link Info
messages on all Route Requests when possible.
The link metric support also aﬀects Route Maintenance.
When Route Maintenance notices that a link is not func-
tional (because a requested Ack has not been received), it
penalizes the link’s metric and sends a Route Error. The
Route Error carries the link’s updated metric back to the
source of the packet.
Our LQSR implementation includes the usual DSR con-
ceptual data structures. These include a Send Buﬀer, for
buﬀering packets while performing Route Discovery; a Main-
tenance Buﬀer, for buﬀering packets while performing Route
Maintenance; and a Route Request table, for suppressing
duplicate Route Requests. The LQSR implementation of
Route Discovery omits some optimizations that are not worth-
while in our environment. In practice, Route Discovery is
almost never required in our testbed so we have not opti-
mized it. In particular, LQSR nodes do not reply to Route
Requests from their link cache. Only the target of a Route
Request sends a Route Reply. Furthermore, nodes do not
send Route Requests with a hop limit to restrict their prop-
agation. Route Requests always ﬂood throughout the ad
hoc network. Nodes do cache information from overheard
Route Requests.
The Windows 802.11 drivers do not support promiscuous
mode and they do not indicate whether a packet was suc-
cessfully transmitted. Hence our implementation of Route
Maintenance uses explicit acknowledgments instead of pas-
sive acknowledgments or link-layer acknowledgments. Every
source-routed packet carries an Ack Request option. A node
expects an Ack from the next hop within 500ms. The Ack
options are delayed brieﬂy (up to 80ms) so that they may be
piggy-backed on other packets ﬂowing in the reverse direc-
tion. Also later Acks squash (replace) earlier Acks that are
waiting for transmission. As a result of these techniques, the
acknowledgment mechanism does not add signiﬁcant byte or
packet overhead.
The LQSR implementation of Route Maintenance also
omits some optimizations. We do not implement “Auto-
matic Route Shortening,” because of the lack of promiscu-
ous mode. We also do not implement “Increased Spreading
of Route Error Messages”. This is not important because
LQSR will not reply to a Route Request from (possibly stale)
cached data. When LQSR Route Maintenance detects a bro-
ken link, it does not remove from the transmit queue other
packets waiting to be sent over the broken link, since Win-
dows drivers do not provide access to the transmit queue.
However, LQSR nodes do learn from Route Error messages
that they forward.
LQSR supports a form of DSR’s “Packet Salvaging” or
retransmission. Salvaging allows a node to try a diﬀerent
route when it is forwarding a source-routed packet and dis-
covers that the next hop is not reachable. The acknowledg-
ment mechanism does not allow every packet to be salvaged
because it is primarily designed to detect when links fail.
When sending a packet over a link, if the link has recently
(within 250ms) been conﬁrmed to be functional, we request
an Ack as usual but we do not buﬀer the packet for possible
salvaging. This design allows for salvaging the ﬁrst packets
in a new connection and salvaging infrequent connection-less
communication, but relies on transport-layer retransmission
for active connections. In our experience, packets traversing
“cold” routes are more vulnerable to loss from stale routes
and beneﬁt from the retransmission provided by salvaging.
We have not yet implemented the DSR “Flow State” opti-
mization, which uses soft-state to replace a full source route
with a small ﬂow identiﬁer. We intend to implement it in
the future. Our Link Cache implementation does not use
the Link-MaxLife algorithm [15] to timeout links. We found
that Link-MaxLife produced inordinate churn in the link
cache.
Instead, we use an inﬁnite metric value to denote
broken links in the cache. We garbage-collect dead links in
the cache after a day.
4. TESTBED
The experimental data reported in this paper are the re-
sults of measurements we have taken on a 23-node wireless
testbed. Our testbed is located on one ﬂoor of a fairly typical
oﬃce building, with the nodes placed in oﬃces, conference
rooms and labs. Unlike wireless-friendly cubicle environ-
ments, our building has rooms with ﬂoor-to-ceiling walls and
solid wood doors. With the exception of one additional lap-
top used in the mobility experiments, the nodes are located
in ﬁxed locations and did not move during testing. The node
density was deliberately kept high enough to enable a wide
variety of multi-hop path choices. See Figure 2.
The nodes are primarily laptop PCs with Intel Pentium
II processors with clock rates from 233 to 300 MHz, but
also included a couple slightly faster laptops as well as two
desktop machines. All of the nodes run Microsoft Windows
XP. The TCP stack included with XP supports the SACK
option by default, and we left it enabled for all of our TCP
experiments. All of our experiments were conducted over
IPv4 using statically assigned addresses. Each node has an
802.11a PCCARD radio. We used the default conﬁguration
23
00
Proxim ORiNOCO
24
04
20
22
25
65
10
64
67
13
00
Proxim Harmony
68
 m
2
. 3
x
ro
p
p
A
00
NetGear WAB 501
66
16
00
NetGear WAG 511
05
07
59
100
56
54
55
128
01
Mobile node walking path
Approx. 61 m
Figure 2: Our testbed consists of 23 nodes placed in ﬁxed locations inside an oﬃce building. Four diﬀerent
models of 802.11a wireless cards were used. The six shaded nodes were used as endpoints for a subset of
the experiments (see section 5.3). The mobile node walking path shows the route taken during the mobility
experiments (see section 6).
for the radios, except for conﬁguring ad hoc mode and chan-
nel 36 (5.18 GHz).
In particular, the cards all performed
auto-speed selection. There are no other 802.11a users in
our building.
We use four diﬀerent types of cards in our testbed: 11
Proxim ORiNOCO ComboCard Gold, 7 NetGear WAG 511,
4 NetGear WAB 501, and 1 Proxim Harmony. While we per-
formed no formal testing of radio ranges, we observed that
some cards exhibited noticeably better range than others.
The Proxim ORiNOCOs had the worst range of the cards
we used in the testbed. The NetGear WAG 511s and WAB
501s exhibited range comparable to each other, and some-
where between the two Proxim cards. The Proxim Harmony
had the best range of the cards we tried.
5. RESULTS
In this section, we describe the results of our experiments.
The section is organized as follows. First, we present mea-
surements that characterize our testbed. These include a
study of the overhead imposed by our routing software, and
a characterization of the variability of wireless links in our
testbed. Then, we present experiments that compare the
four routing metrics under various type of traﬃc.
5.1 LQSR Overhead
Like any routing protocol, LQSR incurs a certain amount
of overhead. First, it adds additional traﬃc in the form of
routing updates, probes, etc. Second, it has the overhead
of carrying the source route and other ﬁelds in each packet.
Third, all nodes along the path of a data ﬂow sign each
packet using HMAC-SHA1 and regenerate the hash to re-
ﬂect the changes in the LQSR headers when forwarding the
packet. Also, the end nodes encrypt or decrypt the payload
data using AES-128, since our sysadmin insisted upon bet-
ter protection than WEP. The cryptographic overhead can
be signiﬁcant given the slow CPU speeds of the nodes.
We conducted the following experiment measures the over-
head of LQSR. We equipped four laptops (named A, B, C
and D), with a Proxim ORiNOCO card each. The laptops
were placed in close proximity of each other. All the links
between the machines were operating at the maximum data
rate (nominally 54Mbps).
To establish a baseline for our measurements, we set up
static IP routes between these machines to form a chain
topology.
In other words, a packet from A to D was for-
warded via B and C, and a packet from D to A was for-
warded via C and B. We measured the throughput of long-
lived TCP ﬂows from A to B (a 1-hop path), from A to C (a
2-hop path) and from A to D (a 3-hop path). At any time,
only one TCP ﬂow was active. These throughputs form our
baseline.
Next, we deleted the static IP routes and started LQSR on
each machine. LQSR allows the user to set up static routes
that override the routes discovered by the routing protocol.
We set up static routes to form the same chain topology
described earlier. Note that LQSR continues to send its
normal control packets and headers, but routes discovered
through this process are ignored in favor of the static routes
supplied by the user. We once again measured the through-
put of long-lived TCP ﬂows on 1, 2 and 3 hop paths. Finally,
we turned oﬀ all cryptographic functionality and measured
the throughput over 1, 2 and 3 hop paths again.
The results of these experiments are shown in Figure 3.
Each bar represents the average of 5 TCP connections. The
variation between runs is negligible. The ﬁrst thing to note is
that, as one would expect, the throughput falls linearly with
the number of hops due to inter-hop interference. LQSR’s
overhead is most evident on 1-hop paths. The throughput
reduction due to LQSR, compared to static IP routing is
over 38%. However, when cryptographic functionality is
turned oﬀ, the throughput reduction is only 13%. Thus,
we can conclude that the LQSR overhead is largely due to
cryptography, which implies that the CPU is the bottleneck.
The amount of overhead imposed by LQSR decreases as the
path length increases. This is because channel contention
between successive hops is the dominant cause of through-
put reduction. At these reduced data rates, the CPU can
easily handle the cryptographic overhead. The remaining
overhead is due to the additional network traﬃc and head-
ers carried in each packet. Note that the amount of control
traﬃc varies depending on the number of neighbors that a
node has. This is especially true for the RTT and PktPair
metrics. We believe that this variation is not signiﬁcant.
)
s
p
b
K
(
t
u
p
h
g
u
o
r
h
T
P
C
T
e
g
a
r
e
v
A
Static IP
MCL No Crypto
MCL With Crypto
16000
14000
12000
10000
8000
6000
4000
2000
0
1
2
3
Number of Hops
Figure 3: MCL overhead is not signiﬁcant on multi-
hop paths.
5.2 Link Variability in the Testbed
In our testbed, we allow the radios to dynamically select
their own data rates. Thus, diﬀerent links in our testbed
have diﬀerent bandwidths. To characterize this variability in
link quality, we conducted the following experiment. Recall
the PktPair metric collects a sample of the amount of time
required to transmit a probe packet every 2 seconds on each
link. We modiﬁed the implementation of the PktPair metric
to keep track of the minimum sample out of every successive
50 samples (i.e minimum of samples 1-50, then minimum of
samples 51-100 etc.). We divide the size of the second packet
by this minimum. The resulting number is an indication of
the bandwidth of that link during that 100 second period.
In controlled experiments, we veriﬁed that this approach
approximates the raw link bandwidth. We gathered these
samples from all links for a period of 14 hours. Thus, for
each link there were a total of 14 × 60 × 60 ÷ 100 = 504
bandwidth samples. There was no traﬃc on the testbed
during this time. We discard any intervals in which the
calculated bandwidth is more than 36Mbps, which is the
highest data rate that we actually see in the testbed. This
resulted in removal of 3.83% of all bandwidth samples. Still,
the resulting number is not an exact measure of the available
bandwidth, since it is diﬃcult to correctly account for all
link-layer overhead. However, we believe that the number is
a good (but rough) indication of the link bandwidth during
the 100 second period.
Of the 23 × 22 = 506 total possible links, only 183 links
had non-zero average bandwidth, where the average was
computed across all samples gathered over 14 hours. We
found that bandwidths of certain links varied signiﬁcantly
over time, while other links it was relatively stable. Exam-
ples of two such links appear in Figures 4 and 5. In the ﬁrst
case, we see that the bandwidth is relatively stable over the
duration of the experient. In the second case, however, the
bandwidth is much more variable. Since the quality of links
in our testbed varies over time, we are careful to repeat our
experiments at diﬀerent times.
In Figure 6 we compare the bandwidth on the forward
and reverse direction of a link. To do this, we consider all
possible unordered node pairs. The number of such pairs is
23 × 22 ÷ 2 = 253. Each pair corresponds to two directional
links. Each of these two links has its own average band-
width. Thus, each pair has two bandwidths associated with
it. Out of the 253 possible node pairs, 90 node pairs had
non-zero average bandwidth in both forward and reverse di-
rections.
In Figure 6, we plot a point for each such pair.
The X-coordinate of the pair represents the link with the
larger bandwidth. The existence of several points below the
diagonal line implies that there are several pairs for which
the forward and the reverse bandwidths diﬀer signiﬁcantly.
In fact, in 47 node pairs, the forward and the reverse band-
widths diﬀer by more than 25%.
5.3 Impact on Long Lived TCP Flows
Having characterized the overhead of our routing software,
and the quality of links in our testbed, we can now discuss
how various routing metrics perform in our testbed. We
begin by discussing the impact of routing metrics on the
performance of long-lived TCP connections. In today’s In-
ternet, TCP carries most of the traﬃc, and most of the bytes
are carried as part of long-lived TCP ﬂows [13]. It is rea-
sonable to expect that similar types of traﬃc will be present
on community networks, such as [6, 27]. Therefore, it is
important to examine the impact of routing metrics on the
performance of long-lived TCP ﬂows.
We start the performance comparison with a simple exper-
iment. We carried out a TCP transfer between each unique
sender-destination pair. There are 23 nodes in our testbed,
so a total of 23 × 22 = 506 TCP transfers were carried out.
Each TCP transfer lasted for 3 minutes, and transferred as
much data as it could. On the best one-hop path in our
testbed a 3 minute connection will transfer over 125MB of
data. Such large TCP transfers ensure repeatability of re-
sults. We had previously determined empirically that TCP
connections of 1 minute duration were of suﬃcient length to
overcome startup eﬀects and give reproducible results. We
used 3 minute transfers in these experiments, to be conser-
vative. We chose to ﬁx the transfer duration, instead of the
amount of data transferred, to keep the running time of each
experiment predictable. Only one TCP transfer was active
at any time. The total time required for the experiment was
just over 25 hours. We repeated the experiment for each
metric.
In Figure 7 we show the median throughput of the 506
TCP transfers for each metric. We choose median to repre-
sent the data instead of the mean because the distribution
(which includes transfers of varying path lengths) is quite
skewed. The height error bars represent the semi-inter quar-
tile range (SIQR), which is deﬁned as half the diﬀerence be-
tween 25th and 75th percentile of the data. SIQR is the rec-
ommended measure of dispersion when the central tendency
of the data is represented by the median [14]. Since each
connection is run between a diﬀerent pair of nodes the rela-
tively large error bars indicate that we observe a wide range
of throughputs across all the pairs. The median through-
put using the HOP metric is 1100Kbps, while the median
throughput using the ETX metric is 1357Kbps. This repre-
sents an improvement of 23.1%.
In contrast, De Couto et al. [9] observed almost no im-
provement for ETX in their DSR experiments. There are
several possible explanations for this. First, they used UDP
instead of TCP. A bad path will have more impact on the
throughput of a TCP connection (due to window backoﬀs,
timeouts etc.) than on the throughput of a UDP connec-
tion. Hence, TCP ampliﬁes the negative impact of poor
route selection. Second, in their testbed the radios were set
to their lowest sending rate of 1Mbps, whereas we allow the
radios to set transmit rates automatically (auto-rate). We
believe links with lower loss rates also tend to have higher
data rates, further amplifying ETX’s improvement. Third,
our testbed has 6-7 hop diameter whereas their testbed has
From 65 to 100
From 68 to 66
)
s
p
b
K