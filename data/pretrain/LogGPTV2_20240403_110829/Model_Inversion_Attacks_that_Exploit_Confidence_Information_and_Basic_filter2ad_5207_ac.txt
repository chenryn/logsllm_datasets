for a modest decrease in recall (32% less).
In other
words, the white-box adversary is able to identify slightly
fewer “Yes” responses than the black-box adversary, but
with much better (i.e., perfect) precision.
Our results indicate that releasing a decision tree trained
over either of these datasets substantially increases the risk
of conﬁdential data leakage for the individuals who provided
responses. Details are given below.
Methodology. We ran the white-box and black-box inver-
sion attacks for each row of the datasets described above. To
evaluate the risk posed by existing, publicly-available mod-
els, we used trees obtained from the BigML service [4]. Be-
cause the sensitive feature in each dataset is binary-valued,
we use precision and recall to measure the eﬀectiveness of
the attack. In our setting, precision measures the fraction of
predicted “Yes” responses that are correct, and recall mea-
sures the fraction of “Yes” responses in the dataset that
are predicted by the adversary. We also measure the ac-
curacy which is deﬁned as the fraction of correct responses.
The trees published on BigML used the entire dataset for
training. To evaluate the eﬀect of training set inclusion,
we trained trees locally by constructing 100 trees using de-
fault parameters on randomly-sampled stratiﬁed training
sets comprised of 50% of the available data. We downloaded
the models and ran the experiments locally to avoid placing
a burden on BigML’s servers. We used a machine with 8
Xeon cores running at 2.5 Ghz, with 16G of memory.
To establish a baseline for comparison, we use three pre-
diction strategies corresponding to the capabilities of a ran-
dom adversary, a baseline adversary, and an ideal adversary.
• The random adversary has access to no information aside
from the domain of the sensitive attribute. This corre-
sponds to an attacker who cannot access a model, and
knows nothing about the prior. For both of our datasets,
the sensitive feature is binary, so the adversary’s best
strategy is to ﬂip a fair coin.
• The baseline adversary has access only to the marginal
probability distribution for the sensitive attribute, and
not the tree or any other information about the training
set. The baseline adversary’s best strategy is to always
guess the most likely value according to the marginal
distribution (i.e., its mode, “No” on our data).
• The ideal adversary has access to a decision tree trained
from the original dataset to predict the sensitive at-
tribute, and given the known features for an individual,
uses the tree to make a prediction.
This strategy for the ideal adversary is the appropriate one
in our setting as it inherits the limitations of the model
class: because the attack inverts a decision tree to make
predictions, we do not expect those predictions to be more
accurate than ones made by a decision tree trained to pre-
dict the sensitive feature in the forward direction, from the
algorithm acc. prec.
whitebox
100.0
blackbox
85.7
random
50.0
baseline
0.0
ideal
FiveThirtyEight
rec.
21.1
21.1
50.0
0.0
98.6
86.4
85.8
50.0
82.9
99.8
100.0
GSS
acc. prec.
100.0
80.3
80.0
38.8
50.0
50.0
0.0
82.0
80.3
61.5
rec.
0.7
1.0
50.0
0.0
2.3
Figure 4: MI results for for BigML models. All
numbers shown are percentages.
same data. This kind of same-model-class comparison was
also used in [13].
Performance. The amount of time needed to run both
the black-box and white-box attacks is negligible, with the
white-box attack being the most expensive. This attack took
about 6 milliseconds on average for both datasets, with the
most expensive component being path enumeration through
a fairly large (about 200 paths) tree. The number of calls
needed to run the black-box attack is small: 4 for the FiveThir-
tyEight dataset (there are 2 unknown binary features), and
2 for the GSS dataset. We conclude that for datasets similar
to these, having few unknown features from small domains,
the black-box attack is feasible to execute remotely.
Discussion. Figure 4 shows the full results for our ex-
periments. The largest diﬀerence between black-box and
white-box accuracy is precision: the white-box attack gave
no false positives, whereas black-box yielded about 15% in
FiveThirtyEight and about 60% on GSS. This shows that
the instance counts on tree paths gives useful information
about the joint prior, allowing the attacker to better iden-
tify negative instances of the sensitive feature. Measured
by both accuracy and precision, both attacks signiﬁcantly
outperform the random-guessing strategy, by at least 30%.
However, neither attack achieves higher recall. This is due
to the skewed prior distribution on the sensitive attribute
(about 80% “No” in both cases), which leads the attack to
favor answers which do not contribute to higher recall. The
upshot of this condition is much higher precision, which sup-
ports greater conﬁdence in positive predictions.
Figure 5a shows the advantage of the MI algorithms over
the baseline strategy, with advantage deﬁned as the increase
in accuracy, precision, or recall. Both algorithms compare
favorably in terms of precision on FiveThirtyEight (at least
80% improvement) and recall (at least 20% improvement),
but are only slightly better in terms of accuracy (3-5% im-
provement). However, on GSS accuracy is slightly lower,
whereas precision and recall are greater. Figure 5b shows
the results as a percentage of the ideal strategy’s perfor-
mance. Reaching 100% of the ideal strategy’s performance
means that the attack performs as well as one could rea-
sonably expect it to. The whitebox attack achieves this for
precision, and comes close (within 15%) for accuracy.
Figure 5c compares the performance of the attack on in-
stances from the training set versus instances outside it.
Both attacks dramatically increase the attacker’s chances
of predicting the sensitive attribute, by as much as 70% pre-
cision and 20% recall. This demonstrates that inclusion in
the training set poses a signiﬁcant risk to the conﬁdentiality
of these individuals’ responses.
1327e
n
i
l
e
s
a
b
r
e
v
o
.
v
d
A
100
80
60
40
20
0
l
a
e
d
I
f
o
%
100
80
60
40
20
0
acc.
prec.
rec.
acc.
FiveThirtyEight
rec.
prec.
GSS
Black-box
White-box
Ideal
acc.
prec.
rec.
acc.
FiveThirtyEight
Baseline
Black-box
rec.
prec.
GSS
White-box
l
a
e
d
I
f
o
%
100
80
60
40
20
0
acc.
prec.
rec.
acc.
FiveThirtyEight
prec.
GSS
rec.
BB Test
BB Train
WB Test
WB Train
(a) Results as advantage over baseline.
(b) Results as a percentage of ideal.
(c) Training vs. test attack performance.
Figure 5: BigML model inversion comparison to the baseline and ideal prediction strategies. Although the
white-box attack achieved greater than 100% improvement over the ideal strategy on the FiveThirtyEight
model, all percentages shown here are cropped at 100% to make the other results legible.
5. FACIAL RECOGNITION MODELS
Facial recognition models are functions that label an im-
age containing a face with an identiﬁer corresponding to the
individual depicted in the image. These models are used in-
creasingly often for a wide range of tasks such as authenti-
cation [8], subject identiﬁcation in security and law enforce-
ment settings [35], augmented reality [9], and photo organi-
zation (e.g., Facebook’s “DeepFace” [1]). A growing number
of web APIs support facial recognition, such as those oﬀered
by Lambda Labs [26], Kairos [21], and SkyBiometry [36]. A
number of local libraries also expose APIs for facial recog-
nition, such as OpenCV [5] and OpenBR [24]. Common to
all these APIs is the ability to train a model using a set of
images labeled with the names of individuals that appear in
them, and the ability to perform classiﬁcation given some
previously trained model. Notice that classiﬁcation may be
performed by a larger set of individuals, including ones who
do not have access to the labeled training set; use of the APIs
in this way has been suggested, for example, in augmented
reality applications [9].
In this section we discuss two MI attacks on the models
used by these APIs to violate the privacy of subjects in the
training set. Both attacks assume that the adversary has
access only to a trained model, but not to any of the original
training data.
1) In the ﬁrst attack we assume an adversary who knows
a label produced by the model, i.e. a person’s name or
unique identiﬁer, and wishes to produce an image of the
person associated with that label (i.e., the victim). The
adversary uses MI to “reconstruct” an image of the victim’s
face from the label. This attack violates the privacy of an
individual who is willing to provide images of themselves as
training data, as the adversary can potentially reconstruct
images of every individual in the training set. The adversary
“wins” an instance of this attack if, when shown a set of face
images including the victim, he can identify the victim. In
subsequent text, we refer to this as the reconstruction attack.
2) In the second attack we assume an adversary who has
an image containing a blurred-out face, and wishes to learn
the identity of the corresponding individual. The adversary
uses the blurred image as side information in a series of
MI attacks, the output of which is a deblurred image of the
subject. Assuming the original image was blurred to protect
anonymity, this attack violates the privacy of the person in
the image. Note that the image has been blurred to an
extent that the model no longer classiﬁes it correctly. The
adversary can only hope to succeed if the individual was in
the training set for the model. Here the adversary wins if he
identiﬁes the victim from a set of face images taken from the
training set, or if the subject of the blurred image was not
in the training set, and the adversary determines that the
image produced by the attack does not correspond to any
of the faces. We refer to this as the deblurring attack. This
attack builds directly on the reconstruction attack. Due
to space constraints we defer discussing it in detail, with
experimental results, to the companion technical report [14].
5.1 Background
There are many proposed algorithms and models for per-
forming facial recognition. Traditional approaches often re-
lied on complex, hand-coded feature extraction, alignment,
and adjustment algorithms, but more recently a number of
promising systems have achieved better performance using
neural networks [1,19,27]. These models are quickly becom-
ing the standard by which facial recognition systems are
evaluated, so we consider three types of neural network mod-
els: softmax regression, a multilayer perceptron network,
and a stacked denoising autoencoder network. These mod-
els vary in complexity, with softmax regression being the
simplest and the stacked denoising autoencoder network be-
ing the most complex.
Softmax regression. This classiﬁer is a generalization of
logistic regression that allows the class variable to take more
than two values—in our case, there are 40 individuals in the
dataset, so the classiﬁer needs to distinguish between 40 la-
bels. Softmax regression is often used as the ﬁnal layer in