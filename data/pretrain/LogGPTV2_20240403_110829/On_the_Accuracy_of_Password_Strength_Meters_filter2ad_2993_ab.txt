is given in Table 1:
• RockYou: This is a well-established leak used extensively in
previous work. 32 million plaintext passwords leaked from the
RockYou web service in December 2009, via an SQL injection attack,
which means that no bias was introduced. We include RockYou in
our evaluation because of its popularity in the community. However,
its passwords should be considered relatively weak ( ˜G0.25 = 16 bits).
• LinkedIn: The social networking website LinkedIn was hacked
in June 2012. The full leak became public in late 2016. The leak
contains a SQL database dump that includes approx. 163 million
unsalted SHA-1 hashes. In the following, we use a 98.68 % recov-
ered plaintext version resulting in approx. 161 million plaintext
passwords. We expect the bias introduced by ignoring 1.32 % of (pre-
sumably strong) passwords to be low, as we are mostly interested
in passwords whose probability can reasonably be approximated
by their count. We include LinkedIn in our evaluation because we
consider those passwords to be a reasonable candidate for medium-
strong passwords ( ˜G0.25 = 19 bits).
• 000Webhost: Leaked from a free web space provider for PHP
and MySQL applications. The data breach became public in October
2015. The leak contains 15 million plaintext passwords. Based on
the official statement, a hacker breached the server, by exploiting a
bug in an outdated PHP version, which again means that no bias
was introduced. We include 000Webhost in our evaluation because
of its enforcement of a lowercase and digits password composition
policy, which results in a different password distribution containing
relatively strong passwords ( ˜G0.25 = 21 bits).
To avoid processing errors in later steps (querying online meters),
we cleaned the leaks, by removing all passwords that were longer
than 256 characters or non-ASCII. This cleaning step removed
0.06 %, 0.09 %, and 0.19 % of the passwords from RockYou, LinkedIn,
and 000Webhost.
4.3 Reference
To reason about various candidate metrics that might be suitable
to measure the accuracy of a strength meter, we created a fourth
dataset. The dataset only contains the frequent passwords of the
LinkedIn leak. We have chosen LinkedIn because it was the largest
available leak at our disposal. As has been shown by Bonneau [8]
and Wang et al. [65], approximating strength for unlikely passwords
is error-prone. To avoid such approximation errors, we limited the
LinkedIn file only to include ASCII passwords that occur 10 or more
times (count ≥ 10), which resulted in the reference password file
containing approx. 1 million unique passwords.
We use the dataset as a) ideal reference and as b) strength meter
output. For this, we divided the set into two disjoint sets REF-A and
REF-B of about equal size by random sampling. In the following
experiments, REF-A will be used as the reference, whereas REF-B
will be used as a basis for the test cases, thus, simulates the meter
output. The experiments as described in Section 5.2 operate on
the count values; if the password abc123 occurs 36, 482 times in
LinkedIn, then REF-A and REF-B include a count value of ∼18, 240
for this password. In Section 5.3 we report on the reliability of this
reference by performing additional tests that include uncommon
passwords, as well as, the other leaks (RockYou and 000Webhost).
5 SIMILARITY MEASURES
In this section, we describe the process of selecting a suitable simi-
larity metrics.
5.1 Test Cases
An overview of test cases described below is given in Table 2.
5.1.1 Monotonic Transformations. We prepared several cases to
test a measures’ tolerance to monotonic transformations: DOUBLE:
For this test case we double the count values in REF-B. This repre-
sents the case that two strength meters use a different scale (e. g.,
one sets the cutoff for the Strong class at a different threshold than
the other). This would naturally occur when two strength meters
use the expected time to crack a password (such as zxcvbn [71]) but
assume different speeds of the cracking hardware. HALF: For this
test case we half the values in REF-B before applying the measure
to calculate the similarity with the ideal strength meter. LOG: For
this test case we take the logarithm to base 2 of the count values
in REF-B. This occurs naturally when one strength meter reports
strength in “expected number of guesses,” and one in “bits of en-
tropy.” SQR/SQRT: Further, we added test cases by applying the
square operation and the square root to REF-B, respectively.
5.1.2 Quantization. A substantial fraction of online meters uses
binned output. Thus, such test cases are highly relevant in prac-
tice. Q4-equi/Q10-equi: For this test case we use quantization into
four/ten bins, about the same number of passwords per bin (count-
ing with multiplicities). Q4-alt/Q10-alt: Similar to the test case above,
we use quantization into four/ten bins, but in this case, splitting into
bins of equal size based on unique passwords (without counting
multiplicities).
5.1.3 Disturbances. We have a number of test cases testing the
tolerance and sensitivity to disturbances in the data. RAND: We
use random values drawn from a uniform distribution between
1 and the maximum count value. This test case can be seen as a
calibration of low similarities as any matching only happens by
chance. ADD-RAND: We add small random disturbances to REF-B
drawn according to a uniform distribution between 1 and the re-
spective count of a password. INV-WEAK-5: We modify the weakest
5 % of passwords (with multiplicities), by setting their usually very
large count to 0 (i. e., we invert their scoring to very strong). INV-
STRONG-5: We modify the strongest 5 % of passwords (with multi-
plicities), by setting their usually very small count to the maximum
count value (i. e., we invert their scoring to very weak).
5.2 Testing Different Metrics
Next, we describe a number of similarity measures and evaluate
them on the test cases defined above to understand their properties
and usefulness. The results are shown in Table 2, we will discuss
these results in-depth in the remainder of this section.
5.2.1 Correlation. A straightforward way to measure similarity,
which has been used in most prior work, is the correlation between
the reference and the observed values.
Pearson Correlation Coefficient: The probably best known cor-
relation measure. It is defined as the covariance divided by both
standard deviations. Pearson correlation has several problems as
a similarity measure for PSMs: First, it is sensitive to monotonic
transformations (e. g., correlation of REF-A and LOG is 0.13), which
is undesirable. Even worse, it is highly sensitive to quantization
(which we typically encounter for most web-based meters), the
correlation between REF-A and quantized versions Q4-equi/Q10-
equi/Q4-alt/Q10-alt is close to zero (between approximately 0.1
and 0.05). Another issue is that it does not capture well the case
INV-STRONG-5, where 5 % of strong passwords are given a weak
score (arguably not a big problem at all), yet the similarity drops
to around zero (−0.02). Two properties of Pearson correlation un-
derlay this undesirable behavior. First, it is a parametric measure
and computed from the given values (instead of, e. g., ranks such
as Spearman correlation), which makes it sensitive to non-linear
transformations of the data. Second, it gives each data point equal
weight, even though the weak passwords have a much higher count
(by definition), thus Pearson correlation weights deviations for
strong passwords stronger, relatively speaking.
Spearman Rank Correlation Coefficient: It is defined as Pear-
son correlation over the ranks of the data. Thus it is based on ranks
of the (sorted) data only. Spearman correlation has been used by
previous work on password strength [13, 65]. Spearman is robust
against monotonic transformations and quite tolerant to quantiza-
tion, which is an improvement over Pearson. Still, it gives too much
weight to strong passwords, similarly to Pearson correlation. One
additional problem is visible for Spearman: the correlation between
REF-A and REF-B should be (close to) 1, as we expect perfect corre-
lation, however, Table 2 shows a correlation of 0.73. The underlying
reason is again the missing weights, which leads to the situation
that the strong passwords dominate the similarity score (around
50 % of passwords have a count of less than 20 in REF-A), and the
(small) errors from sampling on those strong passwords pull the
score from 1 (what would be expected) to around 0.7.
Kendall Rank Correlation Coefficient: Kendall’s tau coefficient
is quite similar to Spearman correlation, but conceptually simpler
(it only takes into account if ranks are wrong and the direction,
but not how big the difference is). Previous work, in fact, showed
very similar results for Spearman and Kendall [65]. However, it
has the disadvantage that naïve implementations (as in standard R)
are computational expensive for larger samples requiring O(N 2)
operations. While Kendall is expected to be robust to monotonic
transformations, a problem similar to Spearman reduces correlation
to 0.56. Furthermore, adding randomness (ADD-RAND) introduces
enough variation to reduce the similarity to 0.54, and the impact of
quantization is stronger than for Spearman.
5.2.2 Weighted Correlation. One common problem with the above
correlation measures is that they treat frequent and infrequent
passwords as equally weighted data points, i. e., an error in a sin-
gle infrequent password is rated equally as an error in a frequent
password which influences much more accounts. Weighted correla-
tion measures give specific weights to the data points, which we
take to be the frequency in the reference dataset. (To the best of
our knowledge, neither weighted Pearson nor weighted Spearman
correlation has been used to compare PSMs before.)
Table 2: Comparing REF-A with modified REF-B using various similarity measures.
(Weighted) Mean Error Metrics
(Weighted) One-Sided/Pairwise Error Metrics
Test Cases
REF-B
DOUBLE
HALF
LOG
SQR
SQRT
Q4-alt
Q10-alt
Q4-equi
Q10-equi
RAND
ADD-RAND
INV-WEAK-5
INV-STRO-5
Sim.
H
H
H
H
H
H
H
H
H
H
L
H
M
M
.
a
r
T
c
i
n
o
t
o
n
o
M
n
o
i
t
a
z
i
t
n
a
u
Q
s
e
c
n
a
b
r
u
t
s
i
D
Pear.
1.00
1.00
1.00
0.13
0.93
0.45
0.05
0.06
0.12
0.11
0.00
0.99
0.25
-0.02
(Weighted) Correlation Metrics
Spear. Kend. wPear. wSpear. MAE MSE
54
0.73
0.73
300402
75703
0.73
301526
0.73
7.E+16
0.73
300014
0.73
301711
0.89
0.91
301465
301766
0.72
301607
0.90
9.E+10
0.00
29799
0.70
283087
0.73
-0.13
7.E+10
4
28
14
24
3.E+05
23
25
22
26
25
3.E+05
15
6
1.E+05
1.00
1.00
1.00
0.49
0.99
0.96
0.08
0.09
0.21
0.25
0.12
1.00
-0.04
0.50
0.99
0.99
0.99
0.99
0.99
0.99
0.73
0.86
0.97
0.99
0.04
0.99
0.70
0.72
0.56
0.56
0.56
0.56
0.56
0.56
0.78
0.80
0.61
0.80
0.00
0.54
0.56
0.01
rMAE rMSE wrMAE wrMSE wrLAE wrLSE
7
0.16
0.16
7
7
0.16
7
0.16
7
0.16
7
0.16
18799
0.10
0.09
3003
25
0.16
3
0.09
1.E+05
0.33
9
0.17
1.E+06
0.16
0.37
28727
13
13
13
13
13
13
18803
3006
29
6
1.E+05
16
1.E+06
28742
1.39
1.39
1.39
1.39
1.39
1.39
4.34
1.86
2.58
0.96
20.37
1.62
4.13
12.89
2.77
2.77
2.77
2.77
2.77
2.77