in strict mode. These supervisor domains serve as the
MAC VMs and perform the necessary policy translations
from SELinux labels on an IPsec tunnel [19, 20] to local
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:30:16 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006domU User VM
Prototype Shamon
domU User VM
BOINC(cid:3)
client
Standard(cid:3)
Linux
dom0 MAC VM
SELinux
dom0 MAC VM
SELinux
SELinux-Labeled(cid:3)
IPsec Tunnel
BOINC(cid:3)
server(cid:3)
Apache,(cid:3)
MySql, PHP
Standard(cid:3)
Linux
Xen Hypervisor w/ sHype MAC
Xen Hypervisor w/ sHype MAC
Physical Machine 1
Physical Machine 2
Figure 4. Bridging the shared reference monitor
(Shamon) in our distributed computing prototype.
sHype labels, and vice versa. Our implementation is based
on a Simple Type Enforcement (STE) policy, where Xen
VMs can share resources and data only if they have been
assigned a common STE-type. Figure 4 shows the struc-
ture of our prototype.
sHype MAC in Xen. The foundation of a Shamon is
sHype, a hypervisor security architecture for different
virtual machine monitors [30].
sHype provides simple,
system-independent and robust security policies and en-
forcement guarantees within the boundaries of a single
VMM. sHype deploys mandatory access control policies
enforced independently of the controlled virtual machines.
It offers two policy components: a Simple Type Enforce-
ment policy (STE) that controls the sharing of resources
(e.g., network, block devices) between different VMs, and
a Chinese Wall [12] policy (CHWALL) that controls which
VMs can run simultaneously on the same system. We did
not use CHWALL policy in our experiments, but our Sha-
mon architecture supports its use.
The STE policy component controls sharing between
virtual machines by controlling access of virtual machines
to VM-to-VM communication, and to any virtual re-
sources through which VMs can share information indi-
rectly. Conceptually, the STE policy creates coalitions of
VMs and assigns VM and resource memberships to coali-
tions. Treating both VMs and virtualized hardware re-
sources equally as generic resources, access control deci-
sions using STE are based on common coalition member-
ship.
Device driver and MAC VMs on Xen. We built and
maintain our Shamon prototype on the current unsta-
ble development version of Xen 3.0: xen-unstable.
While one of the design goals for Xen 3.0 is the abil-
ity to assign various physical resources to device driver
VMs, such functionality is not currently implemented by
xen-unstable. When xen-unstable boots, it starts
a special privileged VM with ID 0 called domain 0, or
dom0. dom0 has access to all devices on the system, thus,
in our prototype, we have only one device driver VM –
dom0.
Our conﬁguration of xen-unstable has sHype en-
abled and enforces a Simple Type Enforcement (STE) pol-
icy. dom0 runs SELinux and serves as the MAC VM that
does policy translation between the labeled IPsec tunnel
and local sHype types. The SELinux policy needed on
dom0 is signiﬁcantly smaller than an SELinux policy for
a typical Linux distribution, as it deals primarily with net-
working controls.
User VMs on Xen. The domU on shype2 runs Fedora
Core 4 and consists of installations of Apache, MySQL,
PHP, and the BOINC server software. The BOINC server
issues compute jobs to clients, collects and tabulates re-
sults, and makes status information available via the web-
site it hosts.
The domU on shype1 runs Fedora Core 4 and the
BOINC client software. The BOINC client accepts com-
pute jobs from the BOINC server, runs them, and returns
the results.
4.2. Labeled IPsec Tunnels
The labeled IPsec tunnel(s) between machines in a dis-
tributed coalition provide authenticated, encrypted com-
munication while conveying MAC type information. We
use labeled IPsec [17] operating in tunnel-mode [20] as
the secure communication mechanism between the dom0s
(MAC VMs) on shype1 and shype2. We describe the
processing of packets arriving at a dom0 from a remote
system and destined for a local domU; processing is sym-
metric in the opposite direction, when packets arrive at a
dom0 from a local domU and destined for a remote system.
Packets arrive in dom0 having come in over the la-
beled IPsec tunnel from another machine in the distributed
coalition. The ﬁrst check is that these packets are des-
tined for some domU on the local hypervisor system (pack-
ets with any other destination are silently dropped using
iptables rules in dom0).
The packets in a ﬂow destined for a domU on the local
hypervisor system must pass through a reference monitor
before being delivered. It is the responsibility of the MAC
code in dom0 to perform the translation between SELinux
subject labels on the IPsec tunnel and the sHype labels on
each domU. As illustrated in Figure 4, reference monitor
functionality exists in both the endpoint of the IPsec tunnel
in dom0 and in the hypervisor with sHype.
The type check in dom0 occurs automatically as part
of the normal operating behavior of our IPsec conﬁgura-
tion. The IPsec tunnels that we employ use tunnel-mode
extensions to labeled IPsec [17]. These researchers added
support for SELinux subject labels to be included in the ne-
gotiation process when IPsec connections are established.
IPsec policies are authorized for subjects, which are user
VMs in our system. User VMs are labeled based on the
STE labels assigned to the VMs by sHype. Note that we
depend on six dom0 kernel mediation points for correct
IPsec policy enforcement: four authorize allocation and
deallocation of IPsec policies and security associations,
and two ﬁlter incoming and outgoing packets.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:30:16 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006The functionality of labeled IPsec [17] provides the
necessary guarantee that all IPsec packets will have subject
labels that are known to both endpoints. That is, an IPsec
connection cannot be established without both endpoints
having an entry for the tunnel label in their respective IPsec
and SELinux policies. Thus, packets with unknown labels
will never arrive via an established IPsec tunnel.
In our current implementation, the IPsec policy for each
dom0 (acting as a MAC VM) in the Shamon of a dis-
tributed coalition must be preconﬁgured with all possible
SELinux subject types that may be needed in a negotiation
to establish an IPsec tunnel. However, recent work by Yin
and Wang shows that it is possible to add new IPsec policy
on the ﬂy [40].
4.3. Type Mapping and Enforcement
The IPsec tunnel and MAC VM are tools that help
to ensure that machines in a distributed coalition enforce
semantically equivalent sHype policies. To achieve this
goal, we must translate between SELinux subject types
and sHype types.
In our prototype, the mapping from
sHype types to SELinux subject types is conﬁgured stati-
cally. SELinux subject types have the form user:role:type,
while sHype types can be arbitrary strings. Since cur-
rently we have no type transitions (in the SELinux sense)
for the types of domUs, we use the user domu u and the
role domu r. We adopted the convention that we inter-
pret the sHype type label as an SELinux type. For exam-
ple, an sHype type green t will map to SELinux type
domu u:domu r:green t.
We modiﬁed the authorization hook in the labeled IPsec
extensions to call our own authorization function for IPsec
packets destined for some domU. SELinux subject labels
for making authorization decisions are inferred from the
sHype label of the domU to which ﬂows are destined,
or from which they originate. On xen-unstable, the
OS running in each domU has a virtual network inter-
face driver known as a frontend. The backend drivers for
all these virtual network interfaces reside in dom0, mani-
fested in the form of additional network interfaces. sHype
mediates communication between frontends and their cor-
responding backends inside the hypervisor. Device drivers
for physical network interfaces reside entirely in dom0, so
that packets to and from physical networks always leave
and enter the platform via dom0.
Our authorization function, numbering approximately
850 lines of commented C code, returns an SELinux se-
curity identiﬁer (SID) when given a flowi and direction.
A flowi is a kernel struct which maintains state for a
generic Internet ﬂow, including the input interface (IIF),
output interface (OIF), and source and destination IP ad-
dresses.
sHype types. The ﬁrst list maintains metadata for each
domU: its domain ID, Internet-visible IP address, and
backend interface name. The second maintains a mapping
between sHype textual labels and their binary equivalents
in compiled sHype policy. Both of these lists are manipu-
lated by reading and writing to entries in /proc/dynsa
(for dynamic security association). Maintenance of the
ﬁrst list (domU metadata) is performed automatically by
extensions we made to the Xen scripts which start and
stop domUs. The second list (sHype mapping) is populated
whenever the sHype policy is loaded or changed (typically
once per boot, although it is possible to change the policy
while a system is running).
4.4. Integrity Measurement
We establish trust into the individual systems that form
a distributed MAC system by determining that each sys-
tem is running software that forms an acceptable refer-
ence monitor enforcing the required security properties,
that each system has been conﬁgured with a MAC policy
whereby the common MAC policy protects the coalition,
and that the software and policy have not been tampered
with. To this end we use remote attestation based on the
Trusted Platform Module (TPM).
The most important requirement is to establish trust into
the parts of each system that make up the Shamon. Re-
call from Figure 4 that the Shamon comprises the Xen hy-
pervisor and MAC VM (i.e., dom0) on all systems that
join a coalition. We attest to the integrity of these compo-
nents by inspecting measurements of the system BIOS and
boot loader, the Xen hypervisor image and its MAC policy,
as well as dom0’s SELinux kernel image, its initial RAM
disk and its MAC policy.
We also use the Integrity Measurement Architecture
(IMA) [31] to establish trust into the user VMs running on
top of the Shamon (i.e., domUs). We attest to the integrity
of a domU by inspecting measurements of its Linux kernel
image and its initial RAM disk, as well as application bina-
ries loaded in that virtual machine. For the BOINC client
and server, this involves measurement of their binaries.
We use a virtual TPM (vTPM) facility [9], which is al-
ready a part of xen-unstable, to report measurements
of software loaded into domUs. This facility is necessary to
make TPM functionality available to all virtual machines
running on a platform. It creates multiple vTPM instances
that each emulate the full functionality of a hardware TPM,
and multiplexes requests as needed to the single physical
TPM on the platform. Each domU is associated with a
vTPM instance that is automatically created and connected
to the domU when that virtual machine is created.
5. Experiments
We added two data structures (linked lists of structs)
to the dom0 kernel to maintain the additional informa-
tion necessary for policy translation between SELinux and
We ran a number of experiments to verify the work-
load isolation and software integrity properties of our dis-
tributed MAC system. In all these experiments we used
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:30:16 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006the prototype system shown in Figure 4 and described in
Section 4.
5.1. Isolation
To verify isolation, we ﬁrst constructed appropriate
sHype, SELinux and IPSec policies on shype1 and
shype2. To the sHype and SELinux policies we added
types named for colors, e.g., red t, green t, and
blue t.
In the sHype policy, we gave dom0s access
to all sHype types since each dom0 plays the role of a
MAC virtual machine in our system. Recall that MAC
virtual machines assist the hypervisor in enforcing MAC
policy and form part of the trusted computing base. Also
in the sHype policy, we assigned the same sHype type to
the client and server domUs, e.g., green t, since they
form part of the same distributed coalition of virtual ma-
chines. To our policy translation tables we added map-
pings between corresponding sHype and SELinux types,
e.g., green t in sHype mapped to green t in SELinux.
As a ﬁnal step in the policy conﬁguration, we created la-
beled IPsec policies based on the IP addresses of shype1
and shype2, and on the IP addresses of the client and
server domUs. The domUs, being full-featured virtual ma-
chines, have their own IP addresses separate from the IP
addresses of shype1 and shype2. So, for example, we
added an entry to the IPsec Security Policy Database on
both shype1 and shype2 that instructs the system to al-
low communication between the BOINC client domU and
the BOINC server domU via a dynamically established
IPSec tunnel between shype1 and shype2 that is la-
beled green t. The SELinux policy has authorization
rules that allow green t subjects to send and receive us-
ing green t security associations.
Next, we conﬁrmed that shype1 and shype2 could
not communicate unless the proper IPsec, SELinux and
sHype policies were in place at both endpoints. We veriﬁed
that the dom0s on shype1 and shype2 would not estab-
lish an IPsec tunnel between them until the necessary en-
tries had been added to the IPsec Security Policy Database
and the SELinux policy at each endpoint. We also ver-
iﬁed that neither system would forward packets between
the IPsec tunnel endpoint in dom0 and the local domU un-
til the necessary entries had been added to the the sHype
policy in the Xen hypervisor, the SELinux policy in dom0,
and the type mapping tables in dom0.
In summary, only when all the appropriate policies are
in place can packets ﬂow between the two domUs. In that
case, the BOINC server successfully sends compute jobs
to the BOINC client, who runs the jobs and successfully
sends the results back to the server.
5.2. Integrity
To verify the trustworthiness of the hypervisor envi-
ronments, including the dom0 integrity, we ﬁrst built a
database of software components. For each component,
the database contains its measurement (i.e., hash), and
whether it’s trusted or untrusted. We added database en-
tries for the key trusted components mentioned in the dis-
cussion of attestation in the previous section. For example,
for the Xen hypervisor we measured its loadable image