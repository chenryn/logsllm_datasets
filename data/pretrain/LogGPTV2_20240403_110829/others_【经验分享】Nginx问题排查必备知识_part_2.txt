    keepalive_timeout等各种超时限制。这就会造成系统会有一个最小下载速度的限制。
        像上面描述的各类超时时间，其实是会随着各类网络事件触发设置及更新。再Linux 环境下，套接字可写就是其中一个事件，如果
    套接字长时间不可写，超过Nginx配置的send_timeout，那么就会触发超时，引发Nginx主动断开连接，甚至reset 连接。Linux
    TCP 套接字在该该套接字上的剩余Buffer空间大于总Buffer 1/3 才会被epoll 等“反应堆”返回可写，也就是说，如果Buffer 被
    填满后，在timeout时间内，Buffer 中的数据1/3 还没被发出去的话，就会触发定时器超时，导致请求异常中断。假设Buffer 配置
    的是512k，send_timeout配置的是30s。那么必须在30s 内发送出去170k才行，也就是最低要达到5.69KB/s 的速度才行。
### 6.2 如何获取系统最低下载速度
    正常情况下，我们可以通过分析系统中各个Buffer 的大小及超时时间计算出一个理论的最低下载速度，但是一个复杂的系统，很难理
    清楚或者找到各个位置的Buffer 大小及超时时间。因此我们可以利用wget 的 --limit-rate 功能进行二分测试，直到找到最低下
    载速度的零界点，注意下载的时候文件不要选择太小，选择太小了会测试不出来，当然也不要太大，太大了会造成测试时间过长，设置为
    系统最大buffer 的2倍左右即可。
    二分测试过程：
    low_rate = 0k, up_rate = 100k
    deviation = 5k
    while up_rate - low_rate < deviation
       mid_rate = (low_rate + up_rate)/2
        wget url --limit-rate mid_rate
        if succ then 
            up_rate = mid_rate 
        else 
           low_rate = mid_rate
    print low_rate, up_rate 
    如下是测试OSS 最低下载速度：
    单连接持续 5k       以内速度必然出问题(一般持续30s+出问题）
    单连接持续 5 ~10k   以内速度随机出问题，看系统状况（比较具有偶然性）
    单连接持续 10k+     基本不出问题
    根据上述6.1 中的理论和6.2 中的测试方法，我们甚至可以测试出来服务器端设置的sndbuf 有多大。
### 6.3 如何解决
        在正常情况下，这个最低下载速度并不会造成什么问题，因为大家都想方设法让速度更快，但是有些计算密集型的场景，可能会遇
    到这个问题。比如说之前遇到一个OSS 客户，从OSS 一个文件中读10k 数据，处理30s，然后再读10k 数据，再处理30s，处理一段
    时间后发现服务器端数据没发完就莫名其妙关闭连接了。其实就是遇到“最低下载速度问题”了。
        针对上述情况，客户端不要在一个请求上一条连接反反复复缓慢读数据，如果文件不大，可以考虑一次性全读出来，放内存或者本
    地再慢慢处理。如果文件太大，可以使用RangeGet，需要多少数据就从服务器端RangeGet 读多少。     
注：针对上传类请求，通常来说没有速度下限要求。
### 6.4 为什么复现不出来
       有同学使用wget/curl 的limit_rate 功能把连接速度限制到很低，但是复现不出来最低下载速度的问题，这是因为测
    试的文件太小了，测试的文件大小需要比客户度的rcvbuf + 服务端的sndbuf 还要大才能测试出来，否则数据堆在两端的
    协议栈里，是触发不到应用的超时时间限制的。
## 7 access_log 中的400 408 及499 
### 7.1 产生原因
    400 是很普通的错误码，但是在Nginx里也有不是普通“400” 的时候，在这里我们只介绍非普通400 的情况。
    408 及499在Nginx中是不会作为错误码返回给用户的（除非upstream 返回了），只是Nginx利用了这两个状态码标识请求的一
        种完成状态。这两种错误码都是和时间相关，但是是不同场景下产生，都是在服务端才能看到的状态，客户端是感知不到的。
    400，如果用户请求数据还未发完之前，客户端主动断开或者连接异常断开（如被reset 掉），在Nginx的access_log 中计为400。
    499，客户端关闭请求，在proxy 场景下确切的说是客户端先于proxy upstream 返回前断开，Nginx在做proxy 的情况下
       （fastcgi_pass/proxy_pass 等），同一请求链路上，客户端与Nginx的连接先于Nginx后端返回前断开，此时在Nginx 
         access_log中计为499 的日志。
    408，客户端请求超时，确切说客户端发送数据超时，客户端向服务器发送请求数据时中间因某种原因中断了一会，引起服务器端读数
         据超时，此时在Nginx access_log 中会记为408。注意，发送header和发送body可能会有不同的超时时间。
### 7.2 如何复现
    400 请求数据发完之前提前断开连接, nc 建立连接后输入完成Host 头部后Ctrl + c 断掉, 或者发送PUT 请求在body 没有发送
        完成之前Ctrl + c 掉
    408 客户端发送超时, nc 建立连接后输入完成Host 头部后等待连接超时, 或者在Body 发送完成之前等待连接超时
    499 客户端在服务器返回之前提前关闭连接 直接Curl，在服务器返回之前Ctrl + c 掉, Nginx在等待upstream返回，此时客户
        端连接已经断开. 可能你的手速没服务端处理的快，可以找一些服务器处理相对耗时的请求来复现，比如OSS的大图片处理。
     注：用public-read-write权限的 bucket 进行测试
### 7.3 是否异常
        一般正常情况下，400、408、499 这三个状态码出现的会比较少，日志中偶尔零星出现一些也不是什么大问题，如果大量出现，那
     就可能出问题了。
        如果日志中大量出现400，如果请求的request_time_msec 很小，优先排查是否是客户端问题，如果这个时间很大，请检查服务器
     压力是否过大，是否有hang住情况。如果服务器端hang 住，请求在发送的时候数据堆在Nginx里，服务器端长时间不读，造成客户端
     超时断开连接，此时Nginx会产生大量因客户端发送超时而提前断连造成的400.
        如果日志中大量出现499，如果请求的request_time_msec 很小（ms 级别），需要排查是否是客户端问题，如果这个时间很大，
     需要从两个方向排查：
        1 检查用户请求，是否后端处理确实需要很长时间，而客户端设置的超时时间又很短，此时需要客户端调整超时时间，否则客户端
          的重试可能会导致雪崩（如果没有限流的话）
        2 检查服务器是否压力过大，是否有hang 住的情况，如果后端持续不返回客户端提前断开的话就会造成大量499.
     这三个状态码出现，多多少少都是有些异常的，通常情况下，我们需要快速判断是服务器端的异常还是客户端的异常，从而快速定位问题。
     当然上述描述的情况也不是绝对的，有时候需要特殊场景特殊分析。
​     
## 8 总结
        学会分析access_log 在日常调查问题中会方便很多，理解access_log 中一些特殊状态码的含义及出现的场景，会让调查问题事
    半功倍。同时对C/S 系统上Buffer 的理解也可以加快调查问题的速度，同时指导设置Buffer 的大小，解决系统在大压力下出现的一些
    性能及其他一些奇怪问题。
**上述信息由笔者翻阅源码及问题排查经验所得，如有错漏，敬请指出**