in alternative dataset A.
In Figure 7, we show the difference between semantic
similarity of a synthetic trace and its seed with the semantic
similarity of the same synthetic trace and any location trace in
our alternative dataset. The histogram shows that the majority
of fake traces have very low distinguishability to alternative
traces,
in the semantic domain. This is due to the high
semantic similarity between real traces (Figure 5b). Therefore,
we conclude that it is not difﬁcult to ﬁnd potential alternative
traces for a synthetic trace. Recall that we set δd to 0.1 to
obtain a high level of plausible deniability.
C. Utility
Because we release a set of synthetic traces to be used
instead of a real location dataset, to evaluate utility we must
take into account how the released traces are to be used.
Speciﬁcally, we must determine to what extent the key features
and statistics, which are relevant for the considered applica-
tions, are preserved. Clearly, we cannot expect all statistics
(of real
traces) to be preserved (in the synthetic dataset)
since some may conﬂict our privacy goal. Speciﬁcally, certain
geographic features are expected not to be preserved, e.g., if
a location is primarily visited by a single user in a seed, it is
unlikely that that location is visited with similar frequency in
a synthetic trace. This is because if such a synthetic trace were
generated from that seed, the privacy test would reject it. An
example of property that we do not preserve is the relationship
in the mobility of input traces, e.g., individuals commuting to
work together. Indeed, if two individuals carpool to work, the
corresponding synthetic traces will not exhibit analogous co-
traveling behavior (because each synthetic trace is generated
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
s
r
i
a
p
f
o
n
o
i
t
r
o
p
o
r
P
0.1
0.05
0
0
0.1
0.08
0.06
0.04
0.02
s
r
i
a
p
f
o
n
o
i
t
r
o
p
o
r
P
1
0
0
0.2
)
s
e
k
a
F
−
s
e
k
a
F
(
s
e
l
i
t
n
a
u
Q
1
0.8
0.6
0.4
0.2
0
0
0.8
1
0.2
0.8
Differential Semantic Similarity
0.4
0.6
Fig. 7: Histogram of
the differential
semantic similarity between fake and
traces.
real
It presents the distribution
of the absolute difference |simS(s, f ) −
, f )|, for all pairs of f (plus its seed
(cid:2)
simS(s
(cid:2)
s) and s
.
0.4
0.6
Semantic Similarity
Fig. 8: Normalized histogram of the se-
mantic similarity of all distinct pairs of:
each of the 30 real traces, along with their
associated fake traces.
0.2
0.4
Quantiles (Seeds − Seeds)
0.6
0.8
1
Fig. 9: Q-Q plot for comparing two dis-
tributions: semantic similarity among all
real seed traces, and semantic similarity
among all fake traces. The plot shows a
very strong correlation between two dis-
tributions.
independently). However, their common semantic features are
preserved.
That said, from the literature, we identiﬁed the following
prominent geo-data analysis tasks to evaluate utility of syn-
thetic traces.
(1) Points of Interests (PoIs) extraction. The goal is to dis-
cover locations that are frequently visited and are prominently
of interest to the public. PoIs can be used to provide travel
recommendations. In particular, [59] proposes techniques to
mine the top n interesting locations in a given region. A key
feature to preserve is the distribution of visits among locations,
speciﬁcally the most visited (i.e., popular) locations.
(2) Semantic annotation / labeling of locations. The goal is
to automatically assign labels to locations according to their
semantics (e.g., restaurant, bar, shopping mall). For exam-
ple, [55] proposes an SVM classiﬁer to assign multiple labels
to location-based social network check-ins. In contrast, [13]
proposes to do automatic labeling of locations into 10 semantic
categories using smartphone recorded GPS, WiFi, and cell-
tower data. In all cases, the distribution of visitors (and unique
visitors) per location are key features of the input data. In
addition, [13], [55] use users’ temporal behavioral data, such
as the amount of time a user spends in a location.
(3) Map inference.
[29] evaluates the two main approaches
to infer road maps from a large scale GPS traces: using
the sample coordinates themselves, or using the transitions
between samples. A related task is the discovery of semantic
regions in a city [57]. In both cases, key features of the
input data include the distribution of visited locations, and
transitions, particularly the popular ones.
(4) Modeling human mobility. [28] proposes to learn a multi-
layer spatial density model from social network check-ins.
In this case, temporal features of location data are largely
overlooked. Rather the focus is on features such as the spatial
location distribution in aggregate and at individual level.
(5) Determining optimal locations for retail stores. The goal
is to ﬁnd ideal geographic placement for a retail store, or a new
business. In particular, [22] proposes to mine online location-
based services to evaluate the retail quality of a geographic
area. Speciﬁcally, the focus is on a combination of mobility
features such as popularity of an area, and semantic features
such as visits to semantically similar venues (e.g., coffee shops
of the same franchise) or transitions between venues.
Based on the input features that those geo-data analysis
tasks require, we identiﬁed six statistics that need to be
preserved to guarantee that such tasks can reasonably be
performed on a set of synthetic traces instead of a real dataset.
In order to experimentally evaluate to what extent
these
statistics are preserved, we must use appropriate baselines.
We use the value of the statistic on the testing (day 2) dataset,
which consists of location traces of the same users as the seed
(day 1) dataset, as the baseline. When appropriate, we also
use uniformly random location traces as a baseline.
The corresponding useful features are the following.
(a) Distribution of the number of visits. Tasks such as (2)
and (3) exploit the fact that some locations are more frequently
visited than others. In fact, [13] explicitly mention “how often
places are visited” as a major feature.
In order to evaluate this, we do the following. For each
i.e., for each
dataset, we compute the spatial allocation,
location (from least to most popular, for that dataset), we
calculate the number of visits spent in that location across all
traces in the dataset. We then normalize this quantity to obtain
a probability distribution over locations (sorted by popularity),
i.e., for each location we have the probability of a random visit
to that location. From these distributions, we compute the KL-
divergence of the real (seed, i.e., day 1) dataset to each of
our synthetic datasets, and to a variety of baselines.5 The KL-
divergence is a natural way to compare distributions: it returns
a non-negative real number, where a larger value means a
greater distance between the two distributions. Note that some
related work such as [9] has used the relative error of counting
5We set zero probabilities to 0.1, before normalizing, for the sake of computing KL-
divergence (that requires nonzero probabilities). This is required because there may be
locations which are visited in the synthetics but not in the real traces, or vice-versa.
559559
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
KL-divergence
Relative error [9]
Real
0.037
0.144
Synthetic
0.384 ± 0.043
0.370 ± 0.010
Uniform Single
1.191
4.666
0.542
1.621
TABLE II: KL-divergence and relative error of the location visiting
probabilities of the real (seed, day 1) datasets against the 10 fake
datasets, and various baselines. “Real” is the testing (day 2) portion
of the real dataset (see Section VI-C); “Uniform” is the uniform
distribution over all locations; and “Single” is the distribution where
all users always visit the same location.
|n1−n2|
queries as a metric instead of the KL-divergence. Therefore,
we additionally calculate the relative error by interpreting the
number of visits to each location as the answer to a counting
query. That is, if the number of visits to location x is n1 for
dataset 1 and n2 for dataset 2, then we calculate the relative
max (n1,0.001·N ), where N is the total number of visits
error as
to any location (the same for all the datasets) [9]. We report
the average relative error over all locations. The results of both
metrics are shown in Table II. The results suggest that a lot of
information is preserved in this case: while the error for the
fakes is greater than that for the real (testing, day 2) dataset,
the error is signiﬁcantly lower than the other baselines.
(b) Distribution of number of visits for top 50 locations.
For most tasks, features of the most popular locations (i.e.,
the most frequently visited locations) are the most important
ones to preserve. In particular, this is consistent with the results
provided in [13] for automatic labeling.
To evaluate this, we use the same procedure as for fea-
ture (a), except we only consider the top 50 locations, and
plot the distribution instead of calculating the KL-divergence.
Figure 10 shows the results for this case, which plots a
histogram where the distributions for different datasets are
overlayed (with some transparency). The error (of the synthetic
dataset) for this case (i.e., top 50) is signiﬁcantly lower than
that for the entire distribution. This strongly indicates that
the information about the popular locations (i.e., the most
important ones) is largely preserved.
(c) Top n coverage of locations. For tasks such as (1), (3),
and (5), it may not be sufﬁcient to ensure that the distribution
of visits is preserved. Indeed, it may be required to ensure that
if a location is in the top n most frequently visited locations in
the real dataset, it is also in the top n most frequently visited
locations in the released (synthetic) dataset.
Therefore, we measure across two datasets (e.g., one real
and one synthetic), how many locations in their respective top-
n they have in common, for various values of n.
Speciﬁcally, we take the n most frequently visited locations
of the real (seed, day 1) dataset. For each of the other datasets,
we then compute how many top n locations (from the seed
dataset) are also in the top n most frequently visited locations
of that dataset. For the synthetic datasets and the uniform
baseline, we report the relative coverage as the ratio of the
coverage of that dataset and of the testing (day 2) dataset.
That is, if the coverage of the real (testing, day 2) dataset
is y (of the top n locs of the seed dataset), and the average
coverage of the fake datasets is x, then we report the relative
Fig. 10: Distribution of visiting probability for top-50 locations in
the real (seeds) datasets against the synthetic datasets. We overlay
(with some transparency) the histograms of the three datasets (i.e.,
seeds day 1, in black; real day 2, in red; synthetics, in yellow). The
difference in distribution between two datasets is the area where the
two corresponding histograms’ bars do not overlap. For example,
the lightest yellow region is where the synthetics’ histogram is non-
overlapping with the other two histograms; the darker yellows are
areas where the synthetics’ histogram overlaps with one (or both) of
the real datasets’ histograms. The majority of the colored area is a
region where all three datasets overlap, indicating that the synthetics
highly preserve the distribution of the top-50 most popular locations.
Synthetics
Uniform
100.0%
e
g
a
r
e
v
o
c
e
v
i
t
l
a
e
R
 80.0%
 60.0%
 40.0%
 20.0%
  0.0%
20
35
40
25
30
Top n locations
Fig. 11: Relative coverage of top n (most frequently visited) loca-
tions. The coverage is reported relative to the real (testing, day 2)
dataset. Uniform visiting of all locations (400 in total) is used as
comparative baseline.
coverage as min ( x
y , 1.0). The results are shown in Figure 11.
The relative coverage of the synthetic traces is typically in the
61% to100% range, whereas for the uniform baseline it is in
the 11% to 24% range, indicating a high-level of preservation.
(d) Users’ time allocation. For semantic labeling (2) and
other tasks, the users’ temporal behavior cannot be ignored.
Indeed both [13], [55] use the amount of the time spent per
location for each user as a major feature.
In order to evaluate this, we do the following: for each
dataset and each user, we calculate the time spent at each
location, among the locations visited. That is, we calculate, for
the three most popular locations of that user, what proportion
of the time is spent in each. We perform this calculation
across all 30 users and normalize the result. We compare this
560560
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
Real (day 2)
0.0189
0.0026
0.0114
Synthetics
0.0125 ± 0.0022
0.0092 ± 0.0031
0.0089 ± 0.0036
Uniform Random
0.1652
0.6794
0.5360
0.0778
0.0779
0.5092
1st
2nd
3rd
TABLE III: KL-divergence of the users’ time allocation distribution
among the three most popular locations (of each user) of the real
(seed, day 1) datasets against synthetic datasets, and baselines.
Fig. 12: Distribution of the proportion of time spent in the most
popular location (of each user) of the real datasets against synthetic