}
}
从这个程序可以看到新旧API的几个区别：
在新的API中，Mapper与Reducer已经不是接口而是抽象类。而且Map函数与Reduce函数也已经不再实现Mapper和Reducer接口，而是继承Mapper和Reducer抽象类。这样做更容易扩展，因为添加方法到抽象类中更容易。
新的API中更广泛地使用了context对象，并使用MapContext进行MapReduce间的通信，MapContext同时充当OutputCollector和Reporter的角色。
Job的配置统一由Configurartion来完成，而不必额外地使用JobConf对守护进程进行配置。
由Job类来负责Job的控制，而不是JobClient, JobClient在新的API中已经被删除。这些区别，都可以在以上的程序中看出。
3.2.3 MapReduce的数据流和控制流
前面已经提到了MapReduce的数据流和控制流的关系，本节将结合WordCount实例具体解释它们的含义。图3-2是上例中WordCount程序的执行流程。
图 3-2 MapReduce工作的简易图
由前面的内容知道，负责控制及调度MapReduce的Job的是JobTracker，负责运行MapReduce的Job的是TaskTracker。当然，MapReduce在运行时是分成Map Task和Reduce Task来处理的，而不是完整的Job。简单的控制流大概是这样的：JobTracker调度任务给TaskTracker, TaskTracker执行任务时，会返回进度报告。JobTracker则会记录进度的进行状况，如果某个TaskTracker上的任务执行失败，那么JobTracker会把这个任务分配给另一台TaskTracker，直到任务执行完成。
这里更详细地解释一下数据流。上例中有两个Map任务及一个Reduce任务。数据首先按照TextInputFormat形式被处理成两个InputSplit，然后输入到两个Map中，Map程序会读取InputSplit指定位置的数据，然后按照设定的方式处理该数据，最后写入到本地磁盘中。注意，这里并不是写到HDFS上，这应该很好理解，因为Map的输出在Job完成后即可删除了，因此不需要存储到HDFS上，虽然存储到HDFS上会更安全，但是因为网络传输会降低MapReduce任务的执行效率，因此Map的输出文件是写在本地磁盘上的。如果Map程序在没来得及将数据传送给Reduce时就崩溃了（程序出错或机器崩溃），那么JobTracker只需要另选一台机器重新执行这个Task就可以了。
Reduce会读取Map的输出数据，合并value，然后将它们输出到HDFS上。Reduce的输出会占用很多的网络带宽，不过这与上传数据一样是不可避免的。如果大家还是不能很好地理解数据流的话，下面有一个更具体的图（WordCount执行时的数据流），如图3-3所示。
图 3-3 WordCount数据流程图
相信看到图3-3，大家就会对MapReduce的执行过程有更深刻的了解了。
除此之外，还有两种情况需要注意：
1）MapReduce在执行过程中往往不止一个Reduce Task, Reduce Task的数量是可以程序指定的。当存在多个Reduce Task时，每个Reduce会搜集一个或多个key值。需要注意的是，当出现多个Reduce Task时，每个Reduce Task都会生成一个输出文件。
2）另外，没有Reduce任务的时候，系统会直接将Map的输出结果作为最终结果，同时Map Task的数量可以看做是Reduce Task的数量，即有多少个Map Task就有多少个输出文件。
3.3 MapReduce任务的优化
相信每个程序员在编程时都会问自己两个问题“我如何完成这个任务”，以及“怎么能让程序运行得更快”。同样，MapReduce计算模型的多次优化也是为了更好地解答这两个问题。
MapReduce计算模型的优化涉及了方方面面的内容，但是主要集中在两个方面：一是计算性能方面的优化；二是I/O操作方面的优化。这其中，又包含六个方面的内容。
1.任务调度
任务调度是Hadoop中非常重要的一环，这个优化又涉及两个方面的内容。计算方面：Hadoop总会优先将任务分配给空闲的机器，使所有的任务能公平地分享系统资源。I/O方面：Hadoop会尽量将Map任务分配给InputSplit所在的机器，以减少网络I/O的消耗。
2.数据预处理与InputSplit的大小
MapReduce任务擅长处理少量的大数据，而在处理大量的小数据时，MapReduce的性能就会逊色很多。因此在提交MapReduce任务前可以先对数据进行一次预处理，将数据合并以提高MapReduce任务的执行效率，这个办法往往很有效。如果这还不行，可以参考Map任务的运行时间，当一个Map任务只需要运行几秒就可以结束时，就需要考虑是否应该给它分配更多的数据。通常而言，一个Map任务的运行时间在一分钟左右比较合适，可以通过设置Map的输入数据大小来调节Map的运行时间。在FileInputFormat中（除了CombineFileInputFormat），Hadoop会在处理每个Block后将其作为一个InputSplit，因此合理地设置block块大小是很重要的调节方式。除此之外，也可以通过合理地设置Map任务的数量来调节Map任务的数据输入。
3.Map和Reduce任务的数量
合理地设置Map任务与Reduce任务的数量对提高MapReduce任务的效率是非常重要的。默认的设置往往不能很好地体现出MapReduce任务的需求，不过，设置它们的数量也要有一定的实践经验。
首先要定义两个概念—Map/Reduce任务槽。Map/Reduce任务槽就是这个集群能够同时运行的Map/Reduce任务的最大数量。比如，在一个具有1200台机器的集群中，设置每台机器最多可以同时运行10个Map任务，5个Reduce任务。那么这个集群的Map任务槽就是12000，Reduce任务槽是6000。任务槽可以帮助对任务调度进行设置。
设置MapReduce任务的Map数量主要参考的是Map的运行时间，设置Reduce任务的数量就只需要参考任务槽的设置即可。一般来说，Reduce任务的数量应该是Reduce任务槽的0.95倍或是1.75倍，这是基于不同的考虑来决定的。当Reduce任务的数量是任务槽的0.95倍时，如果一个Reduce任务失败，Hadoop可以很快地找到一台空闲的机器重新执行这个任务。当Reduce任务的数量是任务槽的1.75倍时，执行速度快的机器可以获得更多的Reduce任务，因此可以使负载更加均衡，以提高任务的处理速度。
4.Combine函数
Combine函数是用于本地合并数据的函数。在有些情况下，Map函数产生的中间数据会有很多是重复的，比如在一个简单的WordCount程序中，因为词频是接近与一个zipf分布的，每个Map任务可能会产生成千上万个＜the，1＞记录，若将这些记录一一传送给Reduce任务是很耗时的。所以，MapReduce框架运行用户写的combine函数用于本地合并，这会大大减少网络I/O操作的消耗。此时就可以利用combine函数先计算出在这个Block中单词the的个数。合理地设计combine函数会有效地减少网络传输的数据量，提高MapReduce的效率。
在MapReduce程序中使用combine很简单，只需在程序中添加如下内容：
job.setCombinerClass（combine.class）；
在WordCount程序中，可以指定Reduce类为combine函数，具体如下：
job.setCombinerClass（Reduce.class）；
5.压缩
编写MapReduce程序时，可以选择对Map的输出和最终的输出结果进行压缩（同时可以选择压缩方式）。在一些情况下，Map的中间输出可能会很大，对其进行压缩可以有效地减少网络上的数据传输量。对最终结果的压缩虽然会减少数据写HDFS的时间，但是也会对读取产生一定的影响，因此要根据实际情况来选择（第7章中提供了一个小实验来验证压缩的效果）。
6.自定义comparator
在Hadoop中，可以自定义数据类型以实现更复杂的目的，比如，当读者想实现k-means算法（一个基础的聚类算法）时可以定义k个整数的集合。自定义Hadoop数据类型时，推荐自定义comparator来实现数据的二进制比较，这样可以省去数据序列化和反序列化的时间，提高程序的运行效率（具体会在第7章中讲解）。
3.4 Hadoop流
Hadoop流提供了一个API，允许用户使用任何脚本语言写Map函数或Reduce函数。Hadoop流的关键是，它使用UNIX标准流作为程序与Hadoop之间的接口。因此，任何程序只要可以从标准输入流中读取数据并且可以写入数据到标准输出流，那么就可以通过Hadoop流使用其他语言编写MapReduce程序的Map函数或Reduce函数。
举个最简单的例子（本例的运行环境：Ubuntu, Hadoop-0.20.2）：
bin/hadoop jar contrib/streaming/hadoop-0.20.2-streaming.jar-input input-output
output-mapper/bin/cat-reducer usr/bin/wc
从这个例子中可以看到，Hadoop流引入的包是hadoop-0.20.2-streaming.jar，并且具有如下命令：
-input指明输入文件路径
-output指明输出文件路径
-mapper指定map函数
-reducer指定reduce函数
Hadoop流的操作还有其他参数，后面会一一列出。
 3.4.1 Hadoop流的工作原理
先来看Hadoop流的工作原理。在上例中，Map和Reduce都是Linux内的可执行文件，更重要的是，它们接受的都是标准输入（stdin），输出的都是标准输出（stdout）。如果大家熟悉Linux，那么对它们一定不会陌生。执行上一节中的示例程序的过程如下所示。
程序的输入与WordCount程序是一样的，具体如下：
file01：
hello world bye world
file02
hello hadoop bye hadoop
输入命令：
bin/hadoop jar contrib/streaming/hadoop-0.20.2-streaming.jar-input input-output
output-mapper/bin/cat-reducer/usr/bin/wc
显示：
packageJobJar：[/root/tmp/hadoop-unjar7103575849190765740/][]/tmp/
streamjob2314757737747407133.jar tmpDir=null
11/01/23 02：07：36 INFO mapred.FileInputFormat：Total input paths to process：2
11/01/23 02：07：37 INFO streaming.StreamJob：getLocalDirs（）：[/root/tmp/mapred/local]
11/01/23 02：07：37 INFO streaming.StreamJob：Running job：job_201101111819_0020
11/01/23 02：07：37 INFO streaming.StreamJob：To kill this job, run：
11/01/23 02：07：37 INFO streaming.StreamJob：/root/hadoop/bin/hadoop job-Dmapred.
job.tracker=localhost：9001-kill job_201101111819_0020
11/01/23 02：07：37 INFO streaming.StreamJob：Tracking URL：http：//localhost：50030/
jobdetails.jsp?jobid=job_201101111819_0020
11/01/23 02：07：38 INFO streaming.StreamJob：map 0%reduce 0%
11/01/23 02：07：47 INFO streaming.StreamJob：map 100%reduce 0%
11/01/23 02：07：59 INFO streaming.StreamJob：map 100%reduce 100%
11/01/23 02：08：02 INFO streaming.StreamJob：Job complete：job_201101111819_0020
11/01/23 02：08：02 INFO streaming.StreamJob：Output：output
程序的输出是：
2 8 46
wc命令用来统计文件中的行数、单词数与字节数，可以看到，这个结果是正确的。
Hadoop流的工作原理并不复杂，其中Map的工作原理如图3-4所示（Reduce与其相同）。
图 3-4 Hadoop流的Map流程图
当一个可执行文件作为Mapper时，每一个Map任务会以一个独立的进程启动这个可执行文件，然后在Map任务运行时，会把输入切分成行提供给可执行文件，并作为它的标准输入（stdin）内容。当可执行文件运行出结果时，Map从标准输出（stdout）中收集数据，并将其转化为＜key, value＞对，作为Map的输出。
Reduce与Map相同，如果可执行文件做Reducer时，Reduce任务会启动这个可执行文件，并且将＜key, value＞对转化为行作为这个可执行文件的标准输入（stdin）。然后Reduce会收集这个可执行文件的标准输出（stdout）的内容。并把每一行转化为＜key, value＞对，作为Reduce的输出。
Map与Reduce将输出转化为＜key, value＞对的默认方法是：将每行的第一个tab符号（制表符）之前的内容作为key，之后的内容作为value。如果没有tab符号，那么这一行的所有内容会作为key，而value值为null。当然这是可以更改的。
值得一提的是，可以使用Java类作为Map，而用一个可执行程序作为Reduce；或使用Java类作为Reduce，而用可执行程序作为Map。例如：
/bin/hadoop jar contrib/streaming/hadoop-0.20.2-streaming.jar
-input myInputDirs-output myOutputDir-mapper
org.apache.hadoop.mapred.lib.IdentityMapper-reducer/bin/wc
3.4.2 Hadoop流的命令
Hadoop流提供自己的流命令选项及一个通用的命令选项，用于设置Hadoop流任务。首先介绍一下流命令。
1.Hadoop流命令选项
Hadoop流命令具体内容如表3-1所示。
表3-1所示的Hadoop流命令中，必选的4个很好理解，分别用于指定输入/输出文件的位置及Map/Reduce函数。在其他的可选命令中，这里我们只解释常用的几个。
-file
-file指令用于将文件加入到Hadoop的Job中。上面的例子中，cat和wc都是Linux系统中的命令，而在Hadoop流的使用中，往往需要使用自己写的文件（作为Map函数或Reduce函数）。一般而言，这些文件是Hadoop集群中的机器上没有的，这时就需要使用Hadoop流中的-file命令将这个可执行文件加入到Hadoop的Job中。
-combiner
这个命令用来加入combiner程序。
-inputformat和-outputformat
这两个命令用来设置输入输出文件的处理方法，这两个命令后面的参数必须是Java类。
2.Hadoop流通用的命令选项
Hadoop流的通用命令用来配置Hadoop流的Job。需要注意的是，如果使用这部分配置，就必须将其置于流命令配置之前，否则命令会失败。这里简要列出命令列表（如表3-2所示），供大家参考。
3.4.3 两个例子
从上面的内容可以知道，Hadoop流的API是一个扩展性非常强的框架，它与程序相连的部分只有数据，因此可以接受任何适用于UNIX标准输入/输出的脚本语言，比如Bash、PHP、Ruby、Python等。
下面举两个非常简单的例子来进一步说明它的特性。
1.Bash
MapReduce框架是一个非常适合在大规模的非结构化数据中查找数据的编程模型，grep就是这种类型的一个例子。
在Linux中，grep命令用来在一个或多个文件中查找某个字符模式（这个字符模式可以代表字符串，多用正则表达式表示）。
下面尝试在如下的数据中查找带有Hadoop字符串的行，如下所示。
输入文件为：
file01：
hello world bye world
file02：
hello hadoop bye hadoop
reduce文件为：
reduce.sh：
grep hadoop
输入命令为：
bin/hadoop jar contrib/streaming/hadoop-0.20.2-streaming.jar-input input-output
output-mapper/bin/cat-reducer～/Desktop/test/reducer.sh-file～/Desktop/test/
reducer.sh
结果为：
hello hadoop bye hadoop
显然，这个结果是正确的。
2.Python
对于Python来说，情况有些特殊。因为Python是可以编译为JAR包的，如果将程序编译为JAR包，那么就可以采用运行JAR包的方式来运行了。
不过，同样也可以用流的方式运行Python程序。请看如下代码：
Reduce.py
#！/usr/bin/python
import sys；
def generateLongCountToken（id）：
return"LongValueSum："+id+"\t"+"1"
def main（argv）：
line=sys.stdin.readline（）；
try：
while line：
line=line[：-1]；
fields=line.split（"\t"）；
print generateLongCountToken（fields[0]）；
line=sys.stdin.readline（）；
except"end of file"：
return None
if__name__=="__main__"：
main（sys.argv）
使用如下命令来运行：