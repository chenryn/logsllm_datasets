21% 68% 66%
7% 18%
72% 81% 84% 85% 85%
66% 77% 79% 81% 79%
77% 82% 57% 87% 86% 43% 47%
0%
79%
60%
81%
76%
3%
0%
-
-
-
-
-
-
-
-
-
-
-
-
2.78%
52.11%
30.56%
68.11%
67.22%
57.11%
94.84%
94.55%
93.11%
89.29%
91.18%
69.70%
68.00%
69.40%
65.40%
62.10%
65.40%
No results are shown for DeepFool on MNIST because of the adversarial examples it generates appear unrecognizable to humans; no results
are shown for JSMA on ImageNet because it requires more memory than available to run.
When its cost is not prohibitive, though, adversarial training
is still beneﬁcial since it can be combined with feature squeez-
ing. Simply inserting a binary ﬁlter before the adversarially-
trained model
increases the robustness against an FGSM
adversary. Figure 5 shows that the accuracy on adversarial
inputs with  = 0.3 is 96.37% for the combined model, which
signiﬁcantly outperforms both standalone approaches: 92.05%
for adversarial training and 94.44% for the bit depth reduction.
V. Detecting Adversarial Inputs
From Section IV we see that feature squeezing is capable
of obtaining accurate model predictions for many adversarial
examples with little reduction in accuracy for legitimate ex-
amples. This enables detection of adversarial inputs using the
framework introduced in Figure 1. The basic idea is to compare
the model’s prediction on the original sample with the same
model’s prediction on the sample after squeezing. The model’s
predictions for a legitimate example and its squeezed version
should be similar. On the contrary, if the original and squeezed
examples result in dramatically diﬀerent predictions, the input
is likely to be adversarial. Table IV and Figure 6 summarize
the results of our experiments that conﬁrm this intuition for all
three datasets. The following subsections provide more details
on our detection method, experimental setup, and discuss the
results. Section V-D considers how adversaries may adapt to
our defense.
A. Detection Method
A prediction vector generated by a DNN classiﬁer nor-
mally represents the probability distribution how likely an
input sample is to belong to each possible class. Hence,
comparing the model’s original prediction with the prediction
on the squeezed sample involves comparing two probability
distribution vectors. There exist several ways to compare the
probability distributions, such as the L1 norm, the L2 norm and
K-L divergence [2]. For this work, we select the L1 norm4
as a natural measure of the diﬀerence between the original
4This turned out to work well, but it is certainly worth exploring in future
work if other metrics can work better.
Fig. 5: Composing adversarial training with feature squeezing.
The horizontal axis is , so the adversarial strength increases to the right.
By itself, bit depth reduction on the original model outperforms adversarial
training. The adversarial-trained model
training
examples as well as those generated by FGSM ( = 0.3) during the 100-
epoch training phase. Composing the 1-bit ﬁlter with the adversarial-trained
model performs even better.
is fed with the original
for  values ranging from 0.1 to 0.4 2. This is the best case
for adversarial training since the adversarially-trained model
is learning from the same exact adversarial method (retraining
is done with FGSM examples generated at  = 0.3) as the
one used to produce the adversarial examples in the test.
Nevertheless, feature squeezing still outperforms it, even at the
same  = 0.3 value: 94.44% accuracy on adversarial examples
compared to 92.05%.
Feature squeezing is far less expensive than adversarial
training. It is almost cost-free, as we simply insert a binary
ﬁlter before the pre-trained MNIST model. On the other
hand, adversarial training is very expensive as it requires both
generating adversarial examples and retraining the classiﬁer
for many epochs.3
2The choice of  is arbitrary, but examples where  > 0.3 are typically not
considered valid adversarial examples [10] since such high  values produce
images that are obviously diﬀerent from the original images.
3We would like to test retraining with the stronger adversaries, and on
the CIFAR-10 and ImageNet datasets also, but have not been able to do
this experiment as the time to do adversarial training on larger models is
prohibitively expensive.
10
.9794.9540.9205.8855.9444.9887.9784.9637.9444.88.90.92.94.96.980.00.10.20.30.4AdversarialTrainingComposedBinaryFilter(a) MNIST examples.
(b) CIFAR-10 examples.
(c) ImageNet examples.
Fig. 6: Diﬀerences in L1 distance between original and squeezed sample, for legitimate and adversarial examples across three
datasets. The L1 score has a range from 0.0 to 2.0 . Each curve is ﬁtted over 200 histogram bins each representing the L1 distance
range of 0.01. Each sample is counted in the bin for the maximum L1 distance between the original prediction and the output of
the best joint-detection squeezing conﬁguration shown in Table IV. The curves for adversarial examples are for all adversarial
examples, including unsuccessful ones (so the separation for successful ones is even larger than shown here).
prediction vector and the squeezed prediction:
B. Experimental Setup
score(x,xsqueezed) = (cid:107)g(x) − g(xsqueezed)(cid:107)1
(9)
Here g(x) is the output vector of a DNN model produced by
the softmax layer whose ith entry describes the probability how
likely input x is in the i-th class.
The L1 norm function is deﬁned as (cid:107)z(cid:107)1 = (cid:80)
i
|zi|. The L1
score has a range from 0 to 2 for the prediction vectors. A
higher score means there is a greater diﬀerence between the
original prediction and the squeezed prediction for an input
x. The maximum value of 2 is reached when each prediction
vector consists of a 1 and all zeros, but with diﬀerent classes
as the 1. Based on the accuracy results in Section IV, we
expect the score to be small for legitimate inputs and large for
adversarial examples. The eﬀectiveness of detection depends
on selecting a threshold value that accurately distinguishes
between legitimate and adversarial inputs.
Even though we can select an eﬀective feature squeezer
for a speciﬁc type of adversarial method, an operator typically
does not know the exact attack method that would be used
in practice. Hence, we combine multiple feature squeezers for
detection by outputting the maximum distance:
scorejoint = max
score(x,xsq1), score(x,xsq2), . . .
(cid:16)
(cid:17)
(10)
We choose the max operator based on an assumption that
there will be a squeezer being eﬀective on one adversarial ex-
ample which generates the highest L1 score. On the other hand,
it may increase the false positive rate because the max operator
also selects the most destructive squeezer on legitimate inputs.
We observed that we could usually make a reasonable trade-
oﬀ in the empirical results. We may investigate a better way
to combine the squeezers in the future work.
Figure 6 shows the histogram of scorejoint for both legiti-
mate (blue) and adversarial examples (red) on three datasets.
The peak for legitimate examples is always near 0, and the
peak for adversarial examples is always near 2. Picking a
threshold value between the two peaks is a balance between
high detection rates and acceptable false positive rates. For
our experiments, we require a false positive rate below 5%
(see Section V-B).
11
We report on experiments using all attacks from Section IV
with the three types of squeezers in diﬀerent conﬁgurations.
Datasets. To get a balanced dataset for detection, we select
the same number of legitimate examples from the test (or
validation) set of each dataset. For each of the attacks in
Section IV, we use the 100 adversarial examples generated for
each attack in the robustness experiments. This results in 2,000
total examples for MNIST (of which 1,000 are legitimate ex-
amples, and 1,000 are adversarial), 2,200 examples for CIFAR-
10 and 1,800 examples for ImageNet. We randomly split each
detection dataset into two groups: one-half for training the
detector and the remainder for validation. Note that some of
the adversarial examples are failed adversarial examples, that
do not confuse the original model, so the number of successful
adversarial examples varies slightly across the attacks.
Squeezers. We ﬁrst consider the artiﬁcial situation where
the defender knows the attack method, and evaluate how
well each squeezing conﬁguration does against adversarial
examples generated by each attack method. Then, we consider
the realistic scenario where the defender does not know that
attack method used by the adversary and needs to select a
conﬁguration that works well against a distribution of possible
attacks.
Training. The training phase of our detector is simply selecting
an optimal threshold of scorejoint. One typical practice is to
ﬁnd the one that maximizes the training accuracy. However, a
detector with high accuracy could be useless in many security-
sensitive tasks if it had a high false positive rate since the actual
distribution of samples is not balanced and mostly benign.
Therefore, we instead select a threshold that ensures the false
positive rate below 5%, choosing a threshold that is exceeded
by just below 5% of legitimate samples. Note that the training
threshold is set using only the legitimate examples, so does
not depend on the adversarial examples.
Validation. Next, we use the chosen threshold value to
measure the detection rate on three groups: the successful
adversarial examples (SAEs), the failed adversarial examples
(FAEs), and the legitimate examples (for false positive rate).
02004006008000.00.40.81.21.62.0Number of ExamplesLegitimateAdversarial02004000.00.40.81.21.62.0LegitimateAdversarial0204060801001201400.00.40.81.21.62.0LegitimateAdversarialExcept when noted explicitly, “detection rate” means the
detection rate on successful adversarial examples. We think it
is important to distinguish failed adversarial examples from
legitimate examples here since detecting failed adversarial
examples is useful for detecting attacks early, whereas an alarm
on a legitimate example is always undesirable and is counted
as a false positive.
positive rate, which means it is slightly more destructive than
the binary ﬁlter on the legitimate examples. The more aggres-
sive median smoothing with 3 × 3 window results in an even
higher threshold 0.0390, because the legitimate examples could
get over-squeezed to the target classiﬁer. A lower threshold is
always preferred for detection, which makes it more sensitive
to adversarial examples.
C. Results
First, we discuss the eﬀectiveness of diﬀerent squeezers
against diﬀerent attacks. Then, we consider how multiple
squeezers would work in a joint detection framework.
Detection with Single Squeezer. Table IV summarizes the
validation results for detection on each dataset, showing the
detection rates for successful adversarial examples for each
attack method with a variety of conﬁgurations. For each
dataset, we ﬁrst list the detection rate of several detectors
built upon single squeezers. For each squeezing method, we
tried several parameters and compare the performance for
each dataset. The “Best Attack-Speciﬁc Single Squeezer” row
gives the detection rate for the best single squeezer against a
particular attack. This represents the (unrealistically optimistic)
case where the model operator knows the attack type and
selects a single squeezer for detection that may be diﬀerent
for each attack. Below this, we show the best result of joint
detection (to be discussed later) with multiple squeezers where
the same conﬁguration is used for all attacks.
The best bit depth reduction for MNIST is squeezing the
color bits to one, which achieves 100% detection rate for 6
of the attacks and at least 98.9% detection for all the L∞ and
L2 attacks. It is not as eﬀective on the advanced CW0 attacks,
however, since these attacks are making large changes to a
small number of pixels. On the contrary, the 3 × 3 median
smoothing is the most eﬀective on detecting the L0 attacks with
detection rates above 92.00%. This matches the observation
from Table III that they have diﬀerent strengths for improving
the model accuracy. For MNIST, there is at least one squeezer
that provides good (above 92% detection) results for all of the
attacks.
For CIFAR-10, we ﬁnd that 2 × 2 median smoothing is
the best single squeezer for detecting every attack except
DeepFool, which is best detected by non-local smoothing.
This is consistent with the robustness results in Table III. For
the ImageNet dataset, we ﬁnd several diﬀerent squeezers are
similarly eﬀective on each attack type. For example, the CW2-
LL attack can be 100% detected with several bit depth ﬁlters,
the 2 × 2 median smoothing or some non-local mean ﬁlters.
The third column in the table gives the distance threshold
setting needed to satisfy the maximum false positive rate of
5%. These threshold values provide some insight into how well
a particular squeezer distinguishes adversarial from legitimate
examples. For the binary ﬁlter on MNIST, a tiny threshold
value of 0.0005 was suﬃcient to produce a false positive rate
below 5%, which means the squeezing has negligible impact
on the legitimate examples: 95% of the legitimate examples
have the L1-based distance score below 0.0005. On the other
hand, the best median smoothing ﬁlter (2 × 2) on MNIST
needs a larger threshold value 0.0029 to achieve a similar false
For some of the attacks, none of the feature squeezing
methods work well enough for the color datasets. The worst
cases, surprisingly, are for FGSM and BIM, two of the earlier
adversarial methods. The best single-squeezer-detection only
recognizes 25.88% of the successful FGSM examples and
52.17% of BIM on the CIFAR-10 dataset, while the detection
rates are 44.41% and 59.00% on ImageNet. We suspect the
reason the tested squeezers are less eﬀective against these
attacks is because they make larger perturbations than the
more advanced attacks (especially the CW attacks), and the
feature squeezers we use are well suited to mitigating small
perturbations. Understanding why these detection rates are so
much lower than the others, and developing feature squeezing
methods that work well against these attacks is an important
avenue for future research.
Joint-Detection with Multiple Squeezers. Table V summa-
rizes the overall detection rates for the best joint detectors
against a distribution of all adversarial methods. By comparing
the last two rows of each dataset in Table IV, we see that
joint-detection often outperforms the best detector with a single
squeezer. For example, the best single-squeezer-detection de-
tects 99.00% of the CW2-LL examples while the joint detection
makes it 100%.
The main reason to use multiple squeezers, however, is
because this is necessary to detect unknown attacks. Since the
model operator is unlikely to know what attack adversaries
may use, it is important to be able to set up the detection
system to work well against any attack. For each data set, we
try several combinations of the three squeezers with diﬀerent
parameters and ﬁnd out the conﬁguration that has the best
detection results across all the adversarial methods (shown as
the “Best Joint Detection” in Table IV). For MNIST, the best
combination was the 1-bit depth squeezer with 2 × 2 median
smoothing, combining the best parameters for each type of
squeezer. For the color image datasets, diﬀerent combinations
were found to outperform combining the best squeezers of
each type. For example, the best joint detection conﬁguration
for ImageNet includes the 5-bit depth squeezer, even though
the 3-bit depth squeezer was better as a single squeezer.
Comparing the “Best Attack-Speciﬁc Single Squeezer” and
“Best Joint Detection” rows in Table IV reveals that the joint
detection is usually competitive with the best single squeezers
over all the attacks. Since the joint detector needs to maintain
the 5% false positive rate requirement, it has a higher threshold
than the individual squeezers. This means in some cases its
detection rate for a particular attack will be worse than the
best single squeezer achieves. For MNIST, the biggest drop
is for detection rate for CW0 (LL) attacks drops from 98%
to 92%; for CIFAR-10, the joint squeezer always outperforms
the best single squeezer; for ImageNet, the detection rate drops
for BIM (59% to 52%), CW2 (Next) (93% to 90%), and CW0
(Next) (99% to 98%). For simplicity, we use a single threshold