performance. We compare NoiseScope with MesoNet in Section 5.2.
Also note that the above approaches have a fundamental weakness—
they can be evaded by the attacker, by re-training the GAN using
the defender’s DNN model as the discriminator.
Blind detection. Li et al.’s work [45] proposes a blind detection
scheme. The idea is that GANs fail to learn correlations among color
components in the RGB space, which results in inconsistencies
when examined in other color spaces, namely HSV, and YCbCr.
They train a one-class SVM classifier on features based on color
statistics of HSV and YCbCr color spaces of real images to detect
fake images. The intuition is that fake images will be flagged as
anomalies in the color (feature) space. We compare our approach
against Li et al.’s approach in Section 5.2.
Zhang et al. [83] uses real data to train “AutoGAN”, a compo-
nent that aims to simulate a GAN generator. The idea is to first
generate fake images using AutoGAN, and then train a supervised
classifier on the newly synthesized fake images and real images to
detect other fake images. Unfortunately, its performance largely
depends on the architecture of AutoGAN’s generator. Results show
significant drop in performance when tested on fake images from
a GAN that uses a different architecture compared to AutoGAN’s
generator.
915ACSAC 2020, December 7–11, 2020, Austin, USA
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath
3 DETECTING DEEPFAKES VIA NOISESCOPE
3.1 Attack and Defense Model
Attacker model. Attacker aims to generate high quality convinc-
ing deepfake images using deep generative models (GANs). Our
focus is on fake images that are entirely produced by a generative
model (GAN). Fake images created by image forgery techniques
such as replacing or adding content in real images (e.g., face swap-
ping [74]) are not considered. In Section 5, we consider an attacker
who is unaware of our defense scheme. Later, in Section 6, we con-
sider an attacker who is aware of our defense scheme pipeline and
employs a variety of countermeasures against NoiseScope.
Defender model. Defender has no a priori access to fake images,
and no knowledge of the generative scheme used by the attacker.
Defender is provided a test set of images, out of which an unknown
number of images are fake or real, and the goal is to flag fake images.
Defender also makes use of a reference set of real images, which is
only used to calibrate certain detection parameters of NoiseScope.
For example, if Facebook wants to detect deepfake profile pictures,
they can prepare a test set containing profile pictures (say faces), and
the reference set would include a set of known real profile pictures.
Our method is designed to be content agnostic, and therefore the
test set can be based on images from different content categories.
3.2 Method Basics
We do not rely on content-specific features that capture semantic
or statistical inconsistencies, e.g., finding abnormalities in human
face images. Such defenses will not survive for long, given the rate
at which GANs are advancing and producing photorealistic images.
Instead, we aim to identify patterns that are not tied to the semantic
aspects of image contents but allow us to differentiate between real
and fake images.
We borrow ideas from the rich literature of camera fingerprinting
schemes [11, 12, 32, 49]. Each imaging device (e.g., camera) leaves
a unique and stable pattern in each image due to imperfections
in various stages of the image acquisition process. Such patterns
known as photo-response non-uniformity (PRNU) patterns have
been used to fingerprint cameras or image acquisition devices [49].
Naturally, this first raises the question whether GAN-based image
generators would leave a unique and stable “artificial” pattern in the
generated images. In fact, preliminary work by Marra et al. shows
that such stable patterns do exist in GAN generated images [53].
These patterns are present, regardless of the content in the image,
be it images of human faces, objects, animals or landscapes. Sec-
ondly, we would expect those patterns to look different because
GAN models share no similarity with camera-based image acqui-
sition pipelines. We leverage these ideas and propose a complete
blind detection scheme that can accurately flag fake images with
any type of content. Next, we explain techniques from the camera
fingerprinting literature that we leverage to fingerprint generative
models.
Leveraging model fingerprints for detection. Consider a set
of images, Ii, where i ∈ {1, . . . , Np} generated by a GAN. Our goal
is to estimate a stable pattern left by the GAN, that is unrelated to
the semantics of the image content. The first step is to separate the
high-level content from the image, and estimate the noise residual
Figure 2: Camera fingerprints from Canon, iPhone, Nikon
cameras (top) and GAN fingerprints from StyleGAN, Cycle-
GAN, PGGAN, BigGAN (bottom).
Figure 3: Histogram of PCE correlation between model fin-
gerprint and images (fake and real).
Ri. The high-level content is estimated by applying an appropriate
denoising filter f (Ii). The noise residual is then computed as, Ri =
Ii − f (Ii). Now the assumption is that the noise residual Ri contains
the stable pattern or the fingerprint F, and some random noise Ni,
i.e., Ri = F + Ni. Therefore, one can estimate the fingerprint by
averaging the residuals:
Np
i =1
¯F = (
Ri)/Np
(1)
In practice, the larger the Np, the additive noise component tends
to cancel out, and we obtain a more accurate fingerprint. According
to prior work, it is possible to estimate a reliable fingerprint using
at least 50 images, i.e., Np > 50 [6].
Figure 2 shows camera and GAN fingerprints computed using
the above method for Np = 100 images. Note that model finger-
prints look very different from device fingerprints. In the case of
CycleGAN and PGGAN, there is a noticeable checkerboard pattern.
This observation is further discussed later in Section 5.1.
If model and device fingerprints are so dissimilar, can we use
the model fingerprint to distinguish between fake and real images?
To answer this, we take a set of face images composed of 200 real
(taken by Canon EOS 70D) and 200 fake images from StyleGAN. The
fingerprint for StyleGAN, say FGAN , is computed using a separate
set of 100 face images. Next, to attribute images in this set to the
device or the GAN, we compute the correlation between the model
fingerprint and residual of each image (Ri) in the test set, i.e.,
ρFGAN ,i = corr( ¯FGAN , Ri).
(2)
StyleGANCycleGANPGGANBigGANCanon	EOS	6DiPhone	7	PlusNikon	D90Nikon	D4 0 20 40 60 80 10001001,00010,000100,000Distribution of ImagesPCE Correlation b/w GAN Fingerprint and ImagesReal ImagesGAN Images916NoiseScope: Detecting Deepfake Images in a Blind Setting
ACSAC 2020, December 7–11, 2020, Austin, USA
For a given image, if this correlation is higher than a certain thresh-
old Tc, it is classified as a fake image, or real, otherwise. A corre-
lation measure called Peak to Correlation Energy (PCE) (described
next) is used. Figure 3 shows the histogram of correlation values
for all images in the set (both fake and real). The fake images can
be easily separated from the real images based on the PCE values.
PCE metric [30]. PCE is a similarity metric to compare two discrete
signals. It is computed as the ratio between squared normalized
correlation and sample variance of circular cross-relations. The
PCE implementation3 that we use carries the sign of normalized
correlation peak (can be negative). A high positive value of PCE
denotes a high correlation. Other than PCE, there are other cor-
relation measures, such as Pearson correlation [59], and quotient
correlation [84]. Compared to other metrics, PCE is a more stable
metric that can be used with images from devices with different
resolutions and sensor types [30]. We find PCE to be suitable for
GAN images as well.
To summarize, if an accurate model fingerprint is available, it is
straight-forward to detect fake images. However, in a blind setting
we have no knowledge of fake images or the associated GAN(s) to
compute the model fingerprint.
Key challenges in designing NoiseScope. 1○ The first challenge
is estimating a model fingerprint. It is hard to estimate a model fin-
gerprint from a single image in a blind setting (Equation 1 requires
averaging over multiple images). While prior work, NoisePrint [18]
provides a supervised (CNN-based) learning scheme to extract cam-
era fingerprint from a single image, such methods are not applicable
in a blind setting. Instead, our idea is to extract fingerprints from
the test set itself in an unsupervised manner. We propose an image
clustering scheme that identifies subsets of images belonging to
the same source (device or model), and estimate fingerprints based
on those subsets. Our method should work as long as a certain
minimum number of fake images (enough to reliably estimate a
fingerprint) are present in the test set. 2○ Once a fingerprint is
extracted from the test set, how do you tell whether it is a model
fingerprint or a device fingerprint? To achieve this, we propose a
fingerprint classification module based on anomaly detection to
identify model fingerprints. 3○ Method should be agnostic to the
specific GAN used, and should also work when test set contains fake
images from different GANs. To address this, our clustering scheme
is designed to be agnostic to the GAN(s) used, and is able to ex-
tract available fingerprints, even from multiple models. 4○ Method
should work for images with any type of high-level content (images
of faces, animals, objects, etc.) To address this challenge, we use
residual image extraction schemes that can effectively suppress
high-level content.
3.3 Detection Pipeline
NoiseScope includes 4 main components: (1) Noise residual extrac-
tor, (2) Fingerprint extractor, (3) Fingerprint classifier, and (4) Fake
image detector. Figure 4 provides an overview of NoiseScope’s de-
tection pipeline. The first component prepares the noise residuals,
the second component finds all available fingerprints in the test
set. The third component identifies model fingerprints among the
Figure 4: An illustration of NoiseScope detection pipeline: (a)
Noise Residual Extractor, (b) Fingerprint Extractor via Clus-
tering, (c) Fingerprint Classifier, (d) Fake Image Detector.
identified fingerprints, and the fourth component uses the model
fingerprints to flag fake images.
Noise Residual Extractor. This first step suppresses high level
image content and extracts the noise residual (which contains the
fingerprint). We use the Wavelet Denoising filter [54] to extract the
noise residual for each image in the test set. Prior work recommends
this as one of the best filters to suppress high-level content [13,
17]. However, there is no perfect filter, and we do notice Wavelet
denoising also leaking image contents into the noise residual in
some cases. If there is heavy content leakage, then fingerprint
extraction (next step) becomes harder. But in general, Wavelet
denoising tends to perform well. In Section 5.2, we analyze the
impact of different denoising filters on detection performance.
Fingerprint Extractor and Fingerprint Classifier. The second
step extracts model fingerprints from the test set. The fingerprint
extractor finds all available fingerprints (model or device) from the
test set, and the fingerprint classifier identifies those that are model
fingerprints. To extract fingerprints, we resort to unsupervised
clustering by starting with the individual noise residuals computed
from step 1. Our goal is to group images belonging to the same
source (model or device), and then use each group of images to
build a fingerprint (using Equation 1).
But there is a challenge—it is hard to cluster images in the resid-
ual space. Residual images contain random noise along with the
fingerprint pattern. So even images from the same source (model or
device) will not always show high correlation [38]. All our efforts
to cluster images in the residual space resulted in impure clusters,
i.e., clusters with mix of fake and real images. An impure cluster
would give us an inaccurate fingerprint which is not useful.
To address this challenge, we use a different strategy: Instead
of completely clustering images in the residual space, we use an
incremental clustering strategy, similar to bottom-up hierarchical
clustering. The idea is to mostly compute correlations between fin-
gerprints (which has less random noise), and less between residuals.
Initially, each residual image forms its own cluster. Next, any pair
of residuals with PCE correlation higher than a threshold Tmerдe
is merged into a new cluster. Each time a cluster is updated, we
compute a fingerprint (using cluster members), and two clusters are
merged if the PCE correlation between their fingerprints is greater
than Tmerдe.4 This is done iteratively at each step to grow clusters.
By computing correlations using fingerprints, we reduce the risk
of random noise impacting our correlation estimates. The larger a
3http://dde.binghamton.edu/download/camera_fingerprint/
4We update clusters such that each image is only present in one cluster.
917ACSAC 2020, December 7–11, 2020, Austin, USA
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath
cluster becomes, the more the random noise will vanish when we
estimate the fingerprint. The PCE threshold for merging, Tmerдe
is chosen such that clusters mostly end up being pure, i.e., contain
all fake images or all real images. If Tmerдe is too low, clusters end
up being impure, and we obtain inaccurate fingerprints which may
not be useful for detecting fake images in the next step. If Tmerдe is
too high, we run the risk of not finding sufficiently large clusters or
even no clusters to estimate an (accurate) fingerprint. In Section 4.2,
we discuss how we estimate Tmerдe.
The clustering process stops when no more clusters can be
merged using the threshold. However, to reduce the computational
complexity, we propose to stop clustering early when we find clus-
ter(s) with size > Tsize. Recall that we only require a small number
of images (> 50) to estimate a fingerprint. Once we stop clustering,
we pass any fingerprint computed using clusters greater than size
50, to the Fingerprint Classification component to decide whether it
is a model or device fingerprint. If no model fingerprints are found,
we continue the clustering process again (in case it was stopped
early), until no more merging is possible. Fingerprints found at
the end are again passed to the fingerprint classifier. Pseudo-code
for the fingerprint extraction and classification step is shown in
Algorithm 1.
Fingerprint Classifier. The fingerprint classifier is used to identify
model fingerprints. Key challenge here is that we have no a pri-
ori knowledge of model fingerprints. Our intuition is that GAN
fingerprints stand out as anomalies when compared to device fin-
gerprints in some feature space. Recall the checkerboard pattern in
GAN fingerprints shown earlier in Figure 2. We observe that model
fingerprints tend to have different texture patterns when compared to
device fingerprints. To capture texture features from a fingerprint,
the well-known Haralick texture features [35] are used. Haralick
texture features capture 14 statistical features from the Gray Level
Co-Occurrence Matrix (GLCM), which in turn captures the num-
ber of repeated pairs of adjacent pixels. For the anomaly detection
scheme, we use the Local Outlier Factor (LOF) scheme [8]. Input
to LOF are Haralick features extracted from fingerprints computed
over (real) images in the reference set. Once trained, the fingerprint
classifier can take any fingerprint as input (after extracting Har-
alick features), and check whether it is an anomalous sample. A
fingerprint is considered to be a model fingerprint if this component
marks it as an anomalous fingerprint.
In the last step, we take all the model
Fake Image Detector.
fingerprints detected in step 2 and compute the PCE correlation
between each fingerprint and all residual images in the test set
(using Equation 2). If correlation is higher than a threshold, the
image is flagged as a fake. An image is considered to be fake, if it is
flagged by at least one model fingerprint. The reference set is used
to calibrate the correlation threshold. The threshold is chosen such
that a model fingerprint when correlated with real images in the
reference set, should not flag any of them. A high threshold will
improve precision, while underestimating the threshold will bring
down precision, and improve recall.
Method Scalability. The clustering part is the most computa-
tionally heavy step of the system. In the worst case, the clustering
could run for log(n) iterations, where n is the number of images in
the dataset. Each iteration requires sorting of the pair-wise PCE
Algorithm 1: NoiseScope Fingerprint Extractor & Classi-
fier:
Data: Set of image residues: I, PCE merging threshold:
Tmerдe, cluster size threshold: Tsize.
Result: Set of model fingerprints: F P.
FakeFingerprintExtractor (I ,Tmerдe ,Tsize ):
Cluster set C = {I1, ..., INp } contains Np residuals.
Stopping flag merдeable = True
while merдeable do
Merged cluster set Cpairs = {}
for pair (ci , cj) in C with highest PCE do
if PCE(ci , cj) > Tmerдe then
Add merged pair (ci , cj) to Cpairs
Remove clusters ci and cj from C
if Cpairs is empty then
merдeable = False
else
if not merдeable or size(c : C) > Tsize then
Add Cpairs to C.
Fingerprint set F P = {}
for c in C where size(c) > 50 do
Compute fingerprint f pc = f inдerprint(c)
if f pc is flagged as outlier then
Add fingerprint f pc to set F P
if F P is not empty or not merдeable then
Return F P
correlation, with an O(n2 ·log(n2)) complexity. This gives the entire