### Class 3 (Section 2.4)

In this section, we focus on scenarios where all processes are correct, but the failure detectors are not accurate. We estimate the Quality of Service (QoS) parameters of a failure detector based on its history during the experiment, specifically from the state transitions (trust-to-suspect and suspect-to-trust) and the times at which these transitions occur. These transitions are recorded over the entire duration of the experiment, which includes multiple executions of the consensus algorithm, with a new execution every 10 milliseconds.

Although each execution of the consensus algorithm is isolated, the failure detectors are not reset to an initial state at the beginning of each consensus. This is because resetting would not be meaningful given the short latency of the consensus algorithm. As a result, we measure the QoS parameters of the failure detector over the full duration of the experiment (multiple consensuses) rather than just one single consensus.

Let \( T_{\text{exp}} \) be the duration of the experiment (multiple consensuses), and consider the pair of processes \( (p, q) \). Let \( T_{S}^{pq} \) be the time the failure detector of process \( p \) spent suspecting process \( q \), \( n_{T \rightarrow S}^{pq} \) be the number of trust-to-suspect transitions of \( p \) with respect to \( q \), and \( n_{S \rightarrow T}^{pq} \) be the number of suspect-to-trust transitions. The QoS metrics described in Section 3.4, specifically the average mistake duration \( T_M \) and the average mistake recurrence time \( T_{MR} \), are computed for the pair of processes \( (p, q) \) using the following equations:

\[
T_M^{pq} = \frac{T_S^{pq}}{T_{\text{exp}}}
\]

\[
T_{MR}^{pq} = \frac{T_{\text{exp}}}{n_{T \rightarrow S}^{pq} + n_{S \rightarrow T}^{pq}}
\]

In some experiments with extremely poor failure detection, we observed latencies exceeding 10 milliseconds (see Section 5.4). Therefore, we had to increase the separation between consensus executions.

The overall QoS metrics \( T_M \) and \( T_{MR} \) for the failure detector are obtained by averaging the values \( T_M^{pq} \) and \( T_{MR}^{pq} \) over all pairs \( (p, q) \).

### 5. Results

We present the results of our measurements and simulations. We chose simulation solvers over analytical methods to account for non-exponential distributions, which better capture the actual behavior of the system. We conducted similar experiments on both the Stochastic Activity Network (SAN) simulation model and the cluster whenever possible. We provide results using both approaches for 3 and 5 processes, and results obtained on the cluster for 7, 9, and 11 processes.

#### 5.1 Setting the Parameters of the SAN Model

As described in Section 3.3, we model the transmission of messages by reproducing the contention for processors and the network. The network model has three parameters: \( t_{\text{send}} \), \( t_{\text{receive}} \), and \( t_{\text{network}} \), which determine the end-to-end delay of a message. Messages sent to all \( n \) processes are treated specially to reduce the size of the SAN model. In the implementation, they are \( n-1 \) unicast messages, but in the model, they appear as a single broadcast message with a higher \( t_{\text{network}} \) than unicast messages.

We tuned the parameters based on measurements of the end-to-end delay of unicast and broadcast messages (for \( n = 3 \) and \( n = 5 \)). Figure 6 shows the cumulative distribution for each case. These distributions were approximated using uniform distributions in a bimodal fashion, resulting in, for unicast messages: \( U[0.1, 0.13] \) (with a probability of 0.8) and \( U[0.145, 0.35] \) (with a probability of 0.2), where \( U[x, y] \) denotes a uniform distribution between \( x \) and \( y \). The values are in milliseconds. From the measurements in Figure 6, we determined the parameters as follows:
1. \( t_{\text{send}} \) and \( t_{\text{receive}} \) are assumed constant and equal.
2. Experiments reported in Section 5.2 allowed us to obtain \( t_{\text{send}} = t_{\text{receive}} \).
3. \( t_{\text{network}} \) is computed as the end-to-end delay minus \( 2 \cdot t_{\text{send}} \).

In the model, this implies the use of an instantaneous activity for \( t_{\text{network}} \) with two outputs, whose case probabilities are the probabilities of the bimodal-like distribution, followed by two uniform timed activities.

#### 5.2 No Failures, No Suspicions

Our first results show the latency when no failures occur and no failure detector ever suspects another process. Figure 7(a) shows the cumulative distribution of all observed latency values for various values of \( n \) (the number of processes) obtained from measurements. The measurements come from 5000 consensus executions on the cluster for each value of \( n \). Figure 7(b) reports the cumulative distribution of latency values for 5 processes obtained by simulation. Simulations were performed with the same end-to-end delay for message transmission but varying \( t_{\text{send}} = t_{\text{receive}} \), and thus \( t_{\text{network}} \). The figure shows that the simulation and measurement results match well when \( t_{\text{send}} = 0.025 \) ms, suggesting a proper division between the different contributions of \( t_{\text{send}} \), \( t_{\text{receive}} \), and \( t_{\text{network}} \) to the end-to-end delay. Based on these results, we choose \( t_{\text{send}} = t_{\text{receive}} = 0.025 \) ms, and this value was used throughout all the simulations.

The mean values for the latency are as follows:
- For \( n = 3 \): 1.06 ms (measurements) and 1.030 ms (simulation)
- For \( n = 5 \): 1.43 ms (measurements) and 1.442 ms (simulation)
- For \( n = 7 \): 2 ms (measurements)
- For \( n = 9 \): 2.62 ms (measurements)
- For \( n = 11 \): 3.27 ms (measurements)

The 90% confidence intervals for the measured means have a half-width smaller than 0.02 ms.

#### 5.3 Failures, No Incorrect Suspicions

The next results were obtained for the case of one process crash. We assume the crash occurs before the start of the consensus algorithm; the crashed process is suspected forever from the beginning, while the other (correct) processes are never suspected. We distinguish two cases:
1. The first coordinator (process 1) has crashed, and thus the algorithm finishes in two rounds.
2. A participant of the first round (process 2) has crashed, and thus the algorithm finishes in one round.

Our results are summarized in Table 1. We observe that the crash of the coordinator always increases the latency compared to the crash-free case. This is because the consensus algorithm executes two rounds instead of one. On the other hand, the crash of a participant decreases the latency for consensus executions, except for the executions with 3 processes. The reason is that the crashed process does not generate messages, reducing contention on the network and the coordinator. The case \( n = 3 \) is special: the measurements show an increased latency. In this case, the number of messages exchanged is so small that the reduced contention plays a secondary role. The increased latency can be explained by the fact that the algorithm starts with the coordinator sending a message \( m \) to both participants: \( m \) is sent first to one participant \( p \), and then to the other participant \( q \). If \( p \) is crashed, \( q \) will reply, but the message \( m \) sent to \( p \) delays the sending of \( m \) to \( q \).

In the simulation, the sending of message \( m \) is modeled as one single broadcast message, which explains why the special case \( n = 3 \) is not observed.

#### 5.4 No Failures, Wrong Suspicions

The next scenario considered had no process crashes, but failure detectors sometimes wrongly suspected processes. We measured the QoS metrics of the failure detectors (see Section 3.4) for various values of the parameters \( T_h \) (heartbeat period) and \( T \) (timeout) (see Section 2.2). The QoS values served as input parameters for the SAN model. For both the QoS metrics and the latency measurements, we executed 20 runs for each setting of the parameters \( T \) and \( T_h \), where each run consisted of 1000 consensus executions. We computed the mean values and their 90% confidence intervals from the mean values measured in each of the runs.

We present the QoS metrics first, followed by the latency, along with the SAN simulation results for latency.

**Quality of Service Parameters:**

We found that modifying the heartbeat period \( T_h \) had little influence on the measured quantities. Therefore, we treated only \( T \) as an independent parameter and fixed \( T_h \) at \( 0.7 \cdot T \) for all experiments. The QoS metrics \( T_{MR} \) and \( T_M \) are plotted in Figure 8 as a function of \( T \).

The mistake recurrence time curve (Figure 8(a)) shows an increasing tendency: suspicions occur more rarely as \( T \) increases. The curve only shows values up to \( T = 30 \) ms. At \( T > 30 \) ms, \( T_{MR} \) rises very fast: \( T_{MR} > 190 \) ms at \( T = 40 \) ms, and \( T_{MR} > 5000 \) ms at \( T = 100 \) ms, for each value of \( n \) (90% confidence). The mistake duration curve (Figure 8(b)) is less regular but remains bounded (<12 ms) for all values of \( T \).