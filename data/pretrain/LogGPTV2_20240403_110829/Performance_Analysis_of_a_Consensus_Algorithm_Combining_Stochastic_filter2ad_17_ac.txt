class 3 (Section 2.4), i.e., in runs were all processes are cor-
rect and the failure detectors are not accurate. We estimate
the QoS parameters of a failure detector from its history dur-
ing the experiment, i.e., from the state transitions trust-to-
suspect and suspect-to-trust, and the time when these tran-
sitions occur. Note that these transitions were recorded dur-
ing the full duration of an experiment, which encompasses
multiple executions of the consensus algorithm: one new
execution every 10 ms. While the different executions of
the consensus are isolated one from another, the failure de-
tectors are not reset to some initial state at the beginning
of each consensus — this would not make any sense, con-
sidering the short latency of the consensus algorithm. The
consequence is that we had to measure the QoS parameters
of the failure detector for the full duration of the experiment
(multiple consensus), rather than the duration of one single
consensus.
Let Texp be the duration of the experiment (multiple con-
sensus), and let us consider the pair of processes (p, q). Let
T pq
S the time the failure detector of process p spent suspect-
ing process q, npq
T S the number of trust-to-suspect transi-
tions of p w.r.t. q, and npq
ST the number of suspect-to-trust
transitions. The QoS metrics described in Section 3.4, i.e.,
the average mistake duration TM and the average mistake
recurrence time TMR, are computed for the pair of pro-
cesses (p, q) from the two equations:
T pq
M
T pq
MR
= T pq
S
Texp
and Texp = npq
T S
+ npq
ST
2
· T pq
MR
2In the few experiments with extremely bad failure detection we ob-
served latencies above 10 ms (see Section 5.4) and thus we had to increase
the separation.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
We obtain the QoS metrics TM and TMR for the failure
MR for all
detector by averaging over the values T pq
pairs (p, q).
M and T pq
5 Results
We now present the results of the measurements and the
simulations (we chose simulation solvers instead of analyt-
ical ones because we had to account for non-exponential
distributions which capture much better the actual behavior
of the system). We executed the same kind of experiments
on both the SAN simulation model and the cluster whenever
that was possible. We present results using both approaches
for 3 and 5 processes, and results obtained on the cluster for
7, 9 and 11 processes.3
5.1 Setting the parameters of the SAN model
As we described in Section 3.3, we model the transmis-
sion of messages by reproducing the contention for the pro-
cessors and for the network. The network model has three
parameters tsend , treceive and tnetwork that determine the
end-to-end delay of a message. Messages sent to all n pro-
cesses are treated specially, in order to reduce the size of
the SAN model. Whereas in the implementation they are
n−1 unicast messages, in the model they appear as a single
broadcast message, with a higher parameter tnetwork than
unicast messages.
We tuned the parameters on the basis of measurements
that give end-to-end delay of unicast and broadcast mes-
sages (for n = 3 and n = 5). Figure 6 shows the cumula-
tive distribution in each of these three cases. These distribu-
tions were approximated by using uniform distributions in
a bi-modal fashion, thus giving, in the case of unicast mes-
sage: U[0.1, 0.13] (with a probability of 0.8) and U[0.145,
0.35] (with a probability of 0.2), where U[x,y] stands for
a uniform distribution between x and y. The values are
to be considered as milliseconds. From the measurements
in Figure 6 we determined the parameters as follows: (1)
tsend and treceive are assumed constant and equal (as sug-
gested in previous works [24, 5]), (2) experiments reported
in Section 5.2 allowed us to obtain tsend = treceive, and (3)
tnetwork is computed as the end-to-end delay minus 2·tsend .
In the model, this implies the use of an instantaneous activ-
ity for tnetwork with two outputs — whose case probabili-
ties are the probabilities of the bi-modal like distribution —
followed by two uniform timed activities.
5.2 No failures, no suspicions
Our ﬁrst results show the latency when no failures occur
and no failure detector ever suspects another process. Fig-
ure 7(a) shows the cumulative distribution of all observed
3Running the algorithm with an even number of hosts is not worth-
while: the consensus algorithms tolerates k crashes both with 2k + 1 and
2k + 2 processes.
y
t
i
l
i
b
a
b
o
r
p
1
0.8
0.6
0.4
0.2
0
0
unicast
broadcast to 3
broadcast to 5
0.1
0.2
0.3
0.4
0.5
0.6
transmission time [ms]
Figure 6. The cumulative distribution of the
end-to-end delay of unicast and broadcast
messages, averaged over the destinations.
latency values, for a variety of values for n (the number
of processes) obtained from measurements. The measure-
ments come from 5000 consensus executions on the clus-
ter for each value of n. Figure 7(b) reports the cumulative
distribution of latency values for 5 processes obtained by
simulation. Simulations were performed with the same end-
to-end delay for message transmission but varying tsend =
treceive, and thus tnetwork . The ﬁgure shows that the sim-
ulation and measurement results match rather well when
tsend = 0.025 ms, which suggests for that value a proper di-
vision between the different contributions of tsend , treceive
and tnetwork to the end-to-end delay. On the basis of these
results we choose tsend = treceive = 0.025 ms, and this
value was used throughout all the simulations.
The mean values for the latency are the following: for
n = 3, 1.06 ms (measurements) and 1.030 ms (simulation);
for n = 5, 1.43 ms (measurements) and 1.442 ms (simula-
tion); for n = 7, 2 ms (measurements); for n = 9, 2.62 ms
(measurements); and for n = 11: 3.27 ms (measurements).
The 90% conﬁdence intervals for the measured means have
a half-width smaller than 0.02 ms.
5.3 Failures, no incorrect suspicions
The next results were obtained for the case of one pro-
cess crash. We assume that the crash occurs before the start
of the consensus algorithm; the crashed process is suspected
forever from the beginning, while the other (correct) pro-
cesses are never suspected. We distinguish two cases: (1)
the ﬁrst coordinator (process 1) has crashed and thus the al-
gorithm ﬁnishes in two rounds, and (2) a participant of the
ﬁrst round (process 2) crashed and thus the algorithm ﬁn-
ishes in one round.
Our results are summarized in Table 1. We can see that
the crash of the coordinator always increases the latency
w.r.t.
the crash-free case. The reason is that the consen-
sus algorithm executes two rounds rather than one in that
case. On the other hand, the crash of a participant has a
more interesting inﬂuence: it decreases the latency for the
consensus executions, except for the executions with 3 pro-
cesses. The reason is that the crashed process does not gen-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
y
t
i
l
i
b
a
b
o
r
p
1
0.8
0.6
0.4
0.2
0
0
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
0
0.5
3 processes (meas.)
5 processes (meas.)
7 processes (meas.)
9 processes (meas.)
11 processes (meas.)
1
2
3
4
5
6
latency [ms]
(a) measurements for all n
tsend = 0.005 ms
tsend = 0.01 ms
tsend = 0.015 ms
tsend = 0.02 ms
tsend = 0.025 ms
tsend = 0.035 ms
measured
1
1.5
2
2.5
3
3.5
latency [ms]
(b) simulations with various settings for Tsend
and n = 5
Figure 7. The cumulative distribution of the
latency.
erate messages, and thus there is less contention on the net-
work and on the coordinator. The case n = 3 is special: the
measurements show an increased latency. In this case, the
number of messages exchanged is so small that the decreas-
ing contention plays a secondary role. We can explain the
latency increase as follows. The algorithm starts with the
coordinator sending a message m to both participants: m is
sent ﬁrst to one participant p, and then to the other partici-
pant q. The reply of one participant is enough to come to a
decision. Now, if p is crashed, q will reply, but the message
m sent to p delays the sending of m to q.
In the simulation, the sending of message m is modeled
as one single broadcast message. This explains that the spe-
cial case n = 3 is not observed.
5.4 No failures, wrong suspicions
The next scenario we considered had no process crashes,
but failure detectors sometimes wrongly suspected processes.
We measured the quality of service metrics of the failure de-
tectors (see Sect. 3.4) for a variety of values of the param-
eters Th (heartbeat period) and T (timeout) (see Sect. 2.2).
The QoS values served as input parameters for the SAN
model. For both the quality of service metrics and the la-
tency measurements, we executed 20 runs for each setting
of the parameters T and Th, where each run consisted of
1000 consensus executions. We computed the mean values
and their 90% conﬁdence intervals from the mean values
measured in each of the runs.
We present the quality of service metrics ﬁrst, and the
latency second, along with the SAN simulation results for
latency.
Quality of service parameters. We found that modifying
the heartbeat period Th hardly inﬂuenced any of the quan-
tities measured. For this reason, we treated only T as an
independent parameter and we ﬁxed Th at 0.7· T for all ex-
periments. The quality of service metrics TMR and TM are
plotted in Fig. 8 as a function of T .
The mistake recurrence time curve (Fig. 8(a)) has an in-
creasing tendency: suspicions occur more and more rarely.
The curve only shows values up to T = 30 ms. At T > 30
ms, TMR starts rising very fast: TMR > 190 ms at T =
40 ms, and TMR > 5 000 ms at T = 100 ms, for each
value of n (90% conﬁdence).4 The mistake duration curve
(Fig. 8(b)) is less regular. It remains bounded (<12 ms) for
all values of T .
]
s
m
[
e
m
i
t
e
c
n
e
r
r
u
c
e
r
e
k
a
i
t
s
m
50
45
40
35
30
25
20
15