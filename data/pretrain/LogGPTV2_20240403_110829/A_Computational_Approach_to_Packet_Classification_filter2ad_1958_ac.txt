Our evaluation (§5.3.1) shows that most of the evaluated rule-
sets can be covered with high coverage above 90% with only 2-3
iSets. This is enough to accelerate the external classiﬁer, as is evi-
dent from the performance results. On the other hand, the choice
of the number of iSets depends on the external classiﬁer properties,
in particular, its sensitivity to memory footprint. We analyze this
tradeoﬀ in §5.3.
Worst-case inputs. Some rule-sets cannot achieve good coverage
with only a few iSets. For example, a rule-set with a single ﬁeld
whose ranges overlap requires too many iSets to be covered.
To obtain a better intuition about the origins of worst-case in-
puts, we consider the notion of rule-set diversity for rule-sets with
exact matches. Rule-set diversity in a ﬁeld is the number of unique
values in it across the rule-set, divided by the total number of rules.
The rule-set diversity is an upper bound on the fraction of rules in the
largest iSet of that ﬁeld. In other words, low diversity implies that
using the ﬁeld for iSet partitioning would result in poor coverage.
We can also identify challenging rule-sets with ranges. We de-
ﬁne rule-set centrality as the maximal number of rules that each
pair of them overlap (they all share a point in a multi-dimensional
space). The rule-set centrality is a lower bound on the number of iSets
required for full coverage.
The diversity and centrality metrics can indicate the potential
of NuevoMatch to accelerate the classiﬁcation of a rule-set. On the
positive side, our iSet partitioning algorithm is eﬀective at segre-
gating the rules that cannot be covered well from the rules that can,
thereby accelerating the remainder classiﬁer as much as possible
for a given rule-set. We analyze this property in §5.3.3.
3.8 Putting it all together
We brieﬂy summarize all the steps of NuevoMatch.
Training
(1) Partition the input into iSets and a remainder set
(2) Train one RQ-RMI on each iSet
(3) Construct an external classiﬁer for the remainder set
Lookup
(1) Query all the RQ-RMIs
(2) Query the external classiﬁer
(3) Collect all the outputs, return the highest-priority rule
Throughput
Fast training
Long training
τ
2τ
3τ
Time
4τ
Figure 7: Updates impact on Throughput over time. An up-
per bound (in green) is for zero training time.
3.9 Rule Updates
We explain how NuevoMatch can support updates with a limited
performance degradation.
Firstly, an external classiﬁer used for the remainder must sup-
port updates. Among the evaluated external classiﬁers only Tuple-
Merge is designed for fast updates.
Secondly, we distinguish four types of updates: (i) a change in
the rule action; (ii) rule deletion (iii) rule matching set change; (iv)
rule addition.
The ﬁrst two types of updates are supported without perfor-
mance degradation, and require a lookup followed by an update
in the value array. However, if an update modiﬁes a rule’s match-
ing set or adds a new rule, it might require modiﬁcations to the
RQ-RMI model. We currently do not know an algorithmic way to
update RQ-RMI without retraining; therefore, an updated rule is
always added to the remainder set.
Unfortunately, this design leads to gradual performance degra-
dation, as updates are likely to increase the remainder set. Accord-
ingly, the model is retrained on the updated rule-set, either peri-
odically or when a large performance degradation is detected. Up-
dates occurring while retraining are accommodated in the follow-
ing batch of updates.
Estimating sustained update rate. Let r and u be the total num-
ber of rules and the number of updates that move a rule to the re-
mainder, respectively; u can be smaller than the real rate of rule up-
dates. We assume that the updates are independent and uniformly
distributed among the r rules. For each rule update, a rule is mod-
iﬁed w.p. (with probability) 1
r . Thus a rule is not modiﬁed in any
of the updates w.p. (1 − 1
r )u ≈ e −u/r . The expected number of un-
modiﬁed rules is r · (1 − 1
r )u ≈ r · e −u/r . Throughput behaves as a
weighted average between that of NuevoMatch and the remainder
implementation, based on the number of rules in each.
Figure 7 illustrates the throughput over time for diﬀerent re-
training rates given a certain update rate. If retraining is invoked
every τ time units, the slower the training process, the worse the
performance degradation.
With these update estimates, using the measured speedup as a
function of the fraction of the remainder (§5.3.3), NuevoMatch can
sustain up to 4k updates per second for 500K rule-sets, yielding
about half the speedup of the update-free case, assuming a minute-
long training. These results indicate the need for speeding up train-
ing, but we conjecture there might be a more eﬃcient way to per-
form updates directly in RQ-RMI without complete re-training of
all submodels. Accelerating updates is left for future work.
4 IMPLEMENTATION DETAILS
RQ-RMI structure. The number of stages and the width of each
stage depend on the number of rules to index. We increase the
width of the last stage from 16 for 10K rules to as much as 512 for
500K. See Table 4 in the Appendix.
Submodel structure. Each submodel is a fully connected 3-layer
neural net with 1 input, 1 output, and 8 neurons in the hidden layer
with ReLU activation. This structure aﬀords an eﬃcient vectorized
implementation (see below).
Training. We use TensorFlow [1] to train each submodel on a CPU.
Training a submodel requires a few seconds, but the whole RQ-RMI
may take up to a few minutes (see §5.3.4). We believe, however, that
a much faster training time could be achieved with more optimiza-
tions, i.e., replacing TensorFlow (known for its poor performance
on small models). We leave it for future work.
iSet partitioning. We implement the iSet partitioning algorithm
using Python. The partitioning takes at most a few seconds and is
negligible compared to RQ-RMI training time.
Inference and secondary search. We implement RQ-RMI infer-
ence in C++. For each iSet we sort the rules by the value of the
respective ﬁeld to optimize the secondary search. To reduce the
number of memory accesses, we pack multiple ﬁeld values from
diﬀerent rules in the same cache line.
Handling long ﬁelds. Both iSet partitioning algorithms and RQ-
RMI models map the inputs into single-precision ﬂoating-point
numbers. This allows the packing of more scalars in vector opera-
tions, resulting in faster inference. While enough for 32-bit ﬁelds,
doing so might cause poor performance for ﬁelds of 64-bits and
128-bits.
We compared two solutions: (1) splitting the ﬁelds into 32-bit
parts and treating each as a distinct ﬁeld, and (2) using a single-
precision ﬂoating-point to express long ﬁelds. The two showed
similar results for iSet partitioning with MAC addresses, while
with IPv6, splitting into multiple ﬁelds worked better. Note that
both the secondary search and the validation phases are not af-
fected because the rules are stored with the original ﬁelds.
Vectorization. We accelerate the inference by using wide CPU
vector instructions. Speciﬁcally, with 8 neurons in the hidden layer
of each submodel, computing the prediction involves a handful of
vector instructions. Validation is also vectorized.
Table 1 shows the eﬀectiveness of vectorization. The use of
wider units speeds up inference, highlighting the potential for scal-
ing NuevoMatch in future CPUs.
Parallelization. NuevoMatch lends itself to parallel execution
where iSets and the remainder classiﬁer run in parallel on diﬀerent
CPU cores. The system receives the packets and enqueues each for
Table 1: Submodel acceleration via vectorization. Methods
are annotated with the number of ﬂoats per single instruc-
tion.
Instruction set (width)
Serial(1)
SSE(4) AVX(8)
Inference Time (ns)
126
62
49
execution into the worker threads. The threads are statically allo-
cated to run RQ-RMI or the external classiﬁer with a balanced load
between the cores.
Note that since RQ-RMI are small and ﬁt in L1, running them
on a separate core enables L1-cache-resident executions even if
the remainder classiﬁer is large. Such an eﬃcient cache utilization
could not have been achieved with other classiﬁers running on two
cores.
Early termination. One drawback of the parallel implementation
is that the slowest thread determines the execution time. Our ex-
periments show that the remainder classiﬁer is the slowest one.
It holds only a small fraction of the rules, so it returns an empty
set for most of the queries, which in turn leads to the worst-case
lookup time. In TupleMerge, for example, a query which does not
ﬁnd any matching rules results in a search over all tables, whereas
in the average case some tables are skipped.
Instead, we query the remainder after obtaining the results from
the iSets, and terminate the search when we determine that the
target rule is not in the remainder.
To achieve that, we make minor changes to existing classiﬁca-
tion techniques. Speciﬁcally, in decision-tree algorithms, we store
in each node the maximum priority of all the sub-tree rules. When-
ever we encounter a maximum priority that is lower than that
found in the iSets, we terminate the tree-walk. The changes to the
hash-based algorithms are similar.
We call this optimization early termination. With this optimiza-
tion, both the iSets and the remainder are queried on the same
core. While a parallel implementation is possible, it incurs higher
synchronization overheads among threads.
5 EVALUATION
In the evaluation, we pursued the following goals.
(1) Comparison of NuevoMatch with the state-of-the-art algo-
rithms TupleMerge [3], CutSplit [21], and NeuroCuts [22];
(2) Systematic analysis of the performance characteristics, in-
cluding coverage in challenging data sets, the eﬀect of RQ-
RMI error bound, and training time.
5.1 Methodology
We ran the experiments on Intel Xeon Silver 4116 @ 2.1 GHz
with 12 cores, 32KB L1, 1024KB L2, and 16MB L3 caches, running
Ubuntu 16.04 (Linux kernel 4.4.0). We disable power management
for stable measurements.
Evaluated conﬁgurations. CutSplit (cs) is set with binth = 8, as
suggested in [21].
For NeuroCuts (nc), we performed a hyperparameter sweep and
selected the best classiﬁer per rule-set. As recommended in [22],
we focused on both top-mode partitioning and reward scaling. We
ran the search on three 12-core Intel machines, allocating six hours
per conﬁguration to converge. In total, we ran nc training for up
to 36 hours per rule-set. In addition, we developed a C++ imple-
mentation of nc for faster evaluation of the generated classiﬁers,
much faster than the authors’ Python-based prototype.
TupleMerge (tm) is used with the version that supports updates
with collision-limit = 40, as suggested in [3].
NuevoMatch (nm) was trained with a maximum error thresh-
old of 64. We present the analysis of the sensitivity to the chosen
parameters and training times in §5.3.2.
Multi-core implementation. We run a parallel implementation
on two cores. NuevoMatch allocates one core for the remainder
computations and the second for the RQ-RMIs. For cs, nc, and tm,
we ran two instances of the algorithm in parallel on two cores
using two threads (i.e., no duplication of the rules), splitting the
input equally between the cores. We discarded iSets with cover-
age below 25% for comparisons against cs and nc, and below 5%
for comparisons against tm. We used batches of 128 packets to
amortize the synchronization overheads. Thus, these algorithms
achieve almost linear scaling and the highest possible throughput
with perfect load-balancing between the cores.
Single-core implementation. We used a single core to measure
the performance of NuevoMatch with the early termination opti-
mization. For nm, we discarded iSets with coverage below 25%.
5.1.1 Packet traces and rule-sets. For evaluating each classiﬁer,
we generated traces with 700K packets. We processed each trace 6
times, using the ﬁrst ﬁve as warmup and measuring the last. We
report the average of 15 measurements.
Uniform traﬃc. We generate traces that access all matching rules
uniformly to evaluate the worst-case memory access pattern.
Skewed traﬃc. For each rule-set we generate traces that follow
Zipf distribution with four diﬀerent skew parameters, according to
the amount of traﬃc that accounts for the 3% most frequent ﬂows
(e.g., 80% of the traﬃc accounts for the 3% most frequent ﬂows).
This is representative of real traﬃc, as has been shown in previous
works [13, 33].
Additionally, we use a real CAIDA trace from the Equinix data-
center in Chicago [2]. As CAIDA does not publish the rules used
to process the packets, we modify the packet headers in the trace
to match each evaluated rule-set as follows. For each rule, we gen-
erate one matching ﬁve-tuple. Then, for each packet in CAIDA,
we replace the original ﬁve-tuple with a random ﬁve-tuple gen-
erated from the rule-set, while maintaining a consistent mapping
between the original and the generated one. Note that the rule-set
access locality of the generated trace is the same or as high as the
original trace.
ClassBench rules. ClassBench [39] is a standard benchmark
broadly used for evaluating packet classiﬁcation algorithms [3, 16,
21, 22, 28, 41, 44]. It creates rule-sets that correspond to the rule dis-
tribution of three diﬀerent applications: Access Control List (ACL),
Firewall (FW), and IP Chain (IPC). We created rule-sets of sizes
500K, 100K, 10K, and 1K, each with 12 distinct applications, all with
5-ﬁeld rules: source and destination IP, source and destination port,
and protocol.
t
u
p
h
g
u
o
r
h
T
p
u
d
e
e
p
S
y
c
n
e
t
a
L
p
u
d
e
e
p
S
4
3
2
1
0
8
6
4
2
0
100K Classiﬁers
500K Classiﬁers
1
2
3
4
5
6
7
8
9
10
11
12 GM 1
2
3
4
5
6
7
8
9
10
11
12 GM
100K Classiﬁers
500K Classiﬁers
1
2
3
4
5
6
7
8
9
10
11
12 GM 1
2
3
4
5
6
7
8
9
10
11
12 GM
NuevoMatch w/ CutSplit
NuevoMatch w/ NeuroCuts
NuevoMatch w/ TupleMerge
Figure 8: ClassBench: NuevoMatch vs. CutSplit, NeuroCuts, and TupleMerge, using two CPU cores. (See rule-set in the Appen-
dix.)
Real-world rules. We used the Stanford Backbone dataset which
contains a large enterprise network conﬁguration [46]. There are