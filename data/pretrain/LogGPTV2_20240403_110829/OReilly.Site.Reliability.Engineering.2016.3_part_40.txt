infrastructure.
As shown in Figure 24-1, the distributed cron uses a single leader job, which is the
only replica that can modify the shared state, as well as the only replica that can
launch cron jobs. We take advantage of the fact that the variant of Paxos we use, Fast
Paxos [Lam06], uses a leader replica internally as an optimization—the Fast Paxos
leader replica also acts as the cron service leader.
Figure 24-1. The interactions between distributed cron replicas
If the leader replica dies, the health-checking mechanism of the Paxos group discov‐
ers this event quickly (within seconds). As another cron process is already started up
and available, we can elect a new leader. As soon as the new leader is elected, we fol‐
low a leader election protocol specific to the cron service, which is responsible for
taking over all the work left unfinished by the previous leader. The leader specific to
the cron service is the same as the Paxos leader, but the cron service needs to take
additional action upon promotion. The fast reaction time for the leader re-election
allows us to stay well within a generally tolerable one-minute failover time.
The most important state we keep in Paxos is information regarding which cron jobs
are launched. We synchronously inform a quorum of replicas of the beginning and
end of each scheduled launch for each cron job.
The Roles of the Leader and the Follower
As just described, our use of Paxos and its deployment in the cron service has two
assigned roles: the leader and the follower. The following sections describe each role.
320 | Chapter 24: Distributed Periodic Scheduling with Cron
The leader
The leader replica is the only replica that actively launches cron jobs. The leader has
an internal scheduler that, much like the simple crond described at the beginning of
this chapter, maintains the list of cron jobs ordered by their scheduled launch time.
The leader replica waits until the scheduled launch time of the first job.
Upon reaching the scheduled launch time, the leader replica announces that it is
about to start this particular cron job’s launch, and calculates the new scheduled
launch time, just like a regular crond implementation would. Of course, as with regu‐
lar crond, a cron job launch specification may have changed since the last execution,
and this launch specification must be kept in sync with the followers as well. Simply
identifying the cron job is not enough: we should also uniquely identify the particular
launch using the start time; otherwise, ambiguity in cron job launch tracking may
occur. (Such ambiguity is especially likely in the case of high-frequency cron jobs,
such as those running every minute.) As seen in Figure 24-2, this communication is
performed over Paxos.
It is important that Paxos communication remain synchronous, and that the actual
cron job launch does not proceed until it receives confirmation that the Paxos quo‐
rum has received the launch notification. The cron service needs to understand
whether each cron job has launched in order to decide the next course of action in
case of leader failover. Not performing this task synchronously could mean that the
entire cron job launch happens on the leader without informing the follower replicas.
In case of failover, the follower replicas might attempt to perform the very same
launch again because they aren’t aware that the launch already occurred.
Figure 24-2. Illustration of progress of a cron job launch, from the leader’s perspective
The completion of the cron job launch is announced via Paxos to the other replicas
synchronously. Note that it does not matter whether the launch succeeded or failed
for external reasons (for example, if the datacenter scheduler was unavailable). Here,
we are simply keeping track of the fact that the cron service attempted the launch at
Building Cron at Google | 321
the given scheduled time. We also need to be able to resolve failures of the cron sys‐
tem in the middle of this operation, as discussed in the following section.
Another extremely important feature of the leader is that as soon as it loses its leader‐
ship for any reason, it must immediately stop interacting with the datacenter schedu‐
ler. Holding the leadership should guarantee mutual exclusion of access to the
datacenter scheduler. In the absence of this condition of mutual exclusion, the old
and new leaders might perform conflicting actions on the datacenter scheduler.
The follower
The follower replicas keep track of the state of the world, as provided by the leader, in
order to take over at a moment’s notice if needed. All the state changes tracked by
follower replicas are communicated via Paxos, from the leader replica. Much like the
leader, followers also maintain a list of all cron jobs in the system, and this list must
be kept consistent among the replicas (through the use of Paxos).
Upon receiving notification about a commenced launch, the follower replica updates
its local next scheduled launch time for the given cron job. This very important state
change (which is performed synchronously) ensures that all cron job schedules
within the system are consistent. We keep track of all open launches (launches that
have begun but not completed).
If a leader replica dies or otherwise malfunctions (e.g., is partitioned away from the
other replicas on the network), a follower should be elected as a new leader. The elec‐
tion must converge faster than one minute, in order to avoid the risk of missing or
unreasonably delaying a cron job launch. Once a leader is elected, all open launches
(i.e., partial failures) must be concluded. This process can be quite complicated,
imposing additional requirements on both the cron system and the datacenter infra‐
structure. The following section discusses how to resolve partial failures of this type.
Resolving partial failures
As mentioned, the interaction between the leader replica and the datacenter schedu‐
ler can fail in between sending multiple RPCs that describe a single logical cron job
launch. Our systems should be able to handle this condition.
Recall that every cron job launch has two synchronization points:
• When we are about to perform the launch
• When we have finished the launch
These two points allow us to delimit the launch. Even if the launch consists of a single
RPC, how do we know if the RPC was actually sent? Consider the case in which we
know that the scheduled launch started, but we were not notified of its completion
before the leader replica died.
322 | Chapter 24: Distributed Periodic Scheduling with Cron
In order to determine if the RPC was actually sent, one of the following conditions
must be met:
• All operations on external systems, which we may need to continue upon re-
election, must be idempotent (i.e., we can safely perform the operations again)
• We must be able to look up the state of all operations on external systems in
order to unambiguously determine whether they completed or not
Each of these conditions imposes significant constraints, and may be difficult to
implement, but being able to meet at least one of these conditions is fundamental to
the accurate operation of a cron service in a distributed environment that could suffer
a single or several partial failures. Not handling this appropriately can lead to missed
launches or double launch of the same cron job.
Most infrastructure that launches logical jobs in datacenters (Mesos, for example)
provides naming for those datacenter jobs, making it possible to look up the state of
jobs, stop the jobs, or perform other maintenance. A reasonable solution to the idem‐
potency problem is to construct job names ahead of time (thereby avoiding causing
any mutating operations on the datacenter scheduler), and then distribute the names
to all replicas of your cron service. If the cron service leader dies during launch, the
new leader simply looks up the state of all the precomputed names and launches the
missing names.
Note that, similar to our method of identifying individual cron job launches by their
name and launch time, it is important that the constructed job names on the datacen‐
ter scheduler include the particular scheduled launch time (or have this information
otherwise retrievable). In regular operation, the cron service should fail over quickly
in case of leader failure, but a quick failover doesn’t always happen.
Recall that we track the scheduled launch time when keeping the internal state
between the replicas. Similarly, we need to disambiguate our interaction with the
datacenter scheduler, also by using the scheduled launch time. For example, consider
a short-lived but frequently run cron job. The cron job launches, but before the
launch is communicated to all replicas, the leader crashes and an unusually long fail‐
over—long enough that the cron job finishes successfully—takes place. The new
leader looks up the state of the cron job, observes its completion, and attempts to
launch the job again. Had the launch time been included, the new leader would know
that the job on the datacenter scheduler is the result of this particular cron job launch,
and this double launch would not have happened.
The actual implementation has a more complicated system for state lookup, driven by
the implementation details of the underlying infrastructure. However, the preceding
description covers the implementation-independent requirements of any such sys‐
Building Cron at Google | 323
tem. Depending on the available infrastructure, you may also need to consider the
trade-off between risking a double launch and risking skipping a launch.
Storing the State
Using Paxos to achieve consensus is only one part of the problem of how to handle
the state. Paxos is essentially a continuous log of state changes, appended to synchro‐
nously as state changes occur. This characteristic of Paxos has two implications:
• The log needs to be compacted, to prevent it from growing infinitely
• The log itself must be stored somewhere
In order to prevent the infinite growth of the Paxos log, we can simply take a snap‐
shot of the current state, which means that we can reconstruct the state without need‐
ing to replay all state change log entries leading to the current state. To provide an
example: if our state changes stored in logs are “Increment a counter by 1,” then after
a thousand iterations, we have a thousand log entries that can be easily changed to a
snapshot of “Set counter to 1,000.”
In case of lost logs, we only lose the state since the last snapshot. Snapshots are in fact
our most critical state—if we lose our snapshots, we essentially have to start from zero
again because we’ve lost our internal state. Losing logs, on the other hand, just causes
a bounded loss of state and sends the cron system back in time to the point when the
latest snapshot was taken.
We have two main options for storing our data:
• Externally in a generally available distributed storage
• In a system that stores the small volume of state as part of the cron service itself
When designing the system, we combined elements of both options.
We store Paxos logs on local disk of the machine where cron service replicas are
scheduled. Having three replicas in default operation implies that we have three
copies of the logs. We store the snapshots on local disk as well. However, because they
are critical, we also back them up onto a distributed filesystem, thus protecting
against failures affecting all three machines.
We do not store logs on our distributed filesystem. We consciously decided that los‐
ing logs, which represent a small amount of the most recent state changes, is an
acceptable risk. Storing logs on a distributed filesystem can entail a substantial perfor‐
mance penalty caused by frequent small writes. The simultaneous loss of all three
machines is unlikely, and if simultaneous loss does occur, we automatically restore
from the snapshot. We thereby lose only a small amount of logs: those taken since the
last snapshot, which we perform on configurable intervals. Of course, these trade-offs
324 | Chapter 24: Distributed Periodic Scheduling with Cron
may be different depending on the details of the infrastructure, as well as the require‐
ments placed on the cron system.
In addition to the logs and snapshots stored on the local disk and snapshot backups
on the distributed filesystem, a freshly started replica can fetch the state snapshot and
all logs from an already running replica over the network. This ability makes replica
startup independent of any state on the local machine. Therefore, rescheduling a rep‐
lica to a different machine upon restart (or machine death) is essentially a nonissue
for the reliability of the service.
Running Large Cron
There are other smaller but equally interesting implications of running a large cron
deployment. A traditional cron is small: at most, it probably contains on the order of
tens of cron jobs. However, if you run a cron service for thousands of machines in a
datacenter, your usage will grow, and you may run into problems.
Beware the large and well-known problem of distributed systems: the thundering
herd. Based on user configuration, the cron service can cause substantial spikes in
datacenter usage. When people think of a “daily cron job,” they commonly configure
this job to run at midnight. This setup works just fine if the cron job launches on the
same machine, but what if your cron job can spawn a MapReduce with thousands of
workers? And what if 30 different teams decide to run a daily cron job like this, in the
same datacenter? To solve this problem, we introduced an extension to the crontab
format.
In the ordinary crontab, users specify the minute, hour, day of the month (or week),
and month when the cron job should launch, or asterisk to specify any value. Run‐
ning at midnight, daily, would then have crontab specification of "0 0 * * *" (i.e.,
zero-th minute, zero-th hour, every day of the week, every month, and every day of
the week). We also introduced the use of the question mark, which means that any
value is acceptable, and the cron system is given the freedom to choose the value.
Users choose this value by hashing the cron job configuration over the given time
range (e.g., 0..23 for hour), therefore distributing those launches more evenly.
Despite this change, the load caused by the cron jobs is still very spiky. The graph in
Figure 24-3 illustrates the aggregate global number of launches of cron jobs at Goo‐
gle. This graph highlights the frequent spikes in cron job launches, which is often
caused by cron jobs that need to be launched at a specific time—for example, due to
temporal dependency on external events.
Building Cron at Google | 325
Figure 24-3. The number of cron jobs launched globally
Summary
A cron service has been a fundamental feature in UNIX systems for many decades.
The industry move toward large distributed systems, in which a datacenter may be
the smallest effective unit of hardware, requires changes in large portions of the stack.
Cron is no exception to this trend. A careful look at the required properties of a cron
service and the requirements of cron jobs drives Google’s new design.
We have discussed the new constraints demanded by a distributed-system environ‐
ment, and a possible design of the cron service based on Google’s solution. This solu‐
tion requires strong consistency guarantees in the distributed environment. The core
of the distributed cron implementation is therefore Paxos, a commonplace algorithm
to reach consensus in an unreliable environment. The use of Paxos and correct analy‐
sis of new failure modes of cron jobs in a large-scale, distributed environment
allowed us to build a robust cron service that is heavily used in Google.
326 | Chapter 24: Distributed Periodic Scheduling with Cron
CHAPTER 25
Data Processing Pipelines
Written by Dan Dennison
Edited by Tim Harvey
This chapter focuses on the real-life challenges of managing data processing pipelines
of depth and complexity. It considers the frequency continuum between periodic
pipelines that run very infrequently through to continuous pipelines that never stop
running, and discusses the discontinuities that can produce significant operational
problems. A fresh take on the leader-follower model is presented as a more reliable
and better-scaling alternative to the periodic pipeline for processing Big Data.
Origin of the Pipeline Design Pattern
The classic approach to data processing is to write a program that reads in data,
transforms it in some desired way, and outputs new data. Typically, the program is
scheduled to run under the control of a periodic scheduling program such as cron.
This design pattern is called a data pipeline. Data pipelines go as far back as co-
routines [Con63], the DTSS communication files [Bul80], the UNIX pipe [McI86],
and later, ETL pipelines,1 but such pipelines have gained increased attention with the
rise of “Big Data,” or “datasets that are so large and so complex that traditional data
processing applications are inadequate.”2
1 Wikipedia: Extract, transform, load, http://en.wikipedia.org/wiki/Extract,_transform,_load
2 Wikipedia: Big data, http://en.wikipedia.org/wiki/Big_data
327
Initial Effect of Big Data on the Simple Pipeline Pattern
Programs that perform periodic or continuous transformations on Big Data are usu‐
ally referred to as “simple, one-phase pipelines.”
Given the scale and processing complexity inherent to Big Data, programs are typi‐
cally organized into a chained series, with the output of one program becoming the
input to the next. There may be varied rationales for this arrangement, but it is typi‐
cally designed for ease of reasoning about the system and not usually geared toward
operational efficiency. Programs organized this way are called multiphase pipelines,
because each program in the chain acts as a discrete data processing phase.
The number of programs chained together in series is a measurement known as the
depth of a pipeline. Thus, a shallow pipeline may only have one program with a corre‐
sponding pipeline depth measurement of one, whereas a deep pipeline may have a
pipeline depth in the tens or hundreds of programs.
Challenges with the Periodic Pipeline Pattern
Periodic pipelines are generally stable when there are sufficient workers for the vol‐
ume of data and execution demand is within computational capacity. In addition,
instabilities such as processing bottlenecks are avoided when the number of chained
jobs and the relative throughput between jobs remain uniform.
Periodic pipelines are useful and practical, and we run them on a regular basis at
Google. They are written with frameworks like MapReduce [Dea04] and Flume
[Cha10], among others.
However, the collective SRE experience has been that the periodic pipeline model is
fragile. We discovered that when a periodic pipeline is first installed with worker siz‐
ing, periodicity, chunking technique, and other parameters carefully tuned, perfor‐
mance is initially reliable. However, organic growth and change inevitably begin to
stress the system, and problems arise. Examples of such problems include jobs that
exceed their run deadline, resource exhaustion, and hanging processing chunks that
entail corresponding operational load.
Trouble Caused By Uneven Work Distribution
The key breakthrough of Big Data is the widespread application of “embarrassingly
parallel” [Mol86] algorithms to cut a large workload into chunks small enough to fit
onto individual machines. Sometimes chunks require an uneven amount of resources
relative to one another, and it is seldom initially obvious why particular chunks
require different amounts of resources. For example, in a workload that is partitioned
by customer, data chunks for some customers may be much larger than others.
328 | Chapter 25: Data Processing Pipelines
Because the customer is the point of indivisibility, end-to-end runtime is thus capped
to the runtime of the largest customer.