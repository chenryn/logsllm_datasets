Now equipped with all the necessary tools, we proceed to
the bounded t-means and union-split clustering algorithms.
3.3 Bounded t-Means Clustering Algorithm
In practice, however,
The task of clustering data has been studied in great
depth.
it has been observed that
classic clustering algorithms frequently produce small or
empty clusters, especially when clustering high-dimensional
datasets. In order to realize k-anonymity, we require each
cluster to have size at least k. The lack of a simple and
eﬃcient algorithm for minimum-size clustering in the litera-
ture has led us to develop two of our own. Our ﬁrst solution
builds on the conventional t-means algorithm, which has no
2In general, the task of mapping vertices’ local subgraphs
into a real coordinate system that preserves isomorphism is
reducible from the graph isomorphism problem, which is not
known to have a polynomial-time solution. [20]
minimum-size constraint. We refer readers to machine learn-
ing literature for details about conventional t-means. [11]
Our bounded t-means clustering method is described as
follows.
in c′ that are farthest from each other, and then
applying our bounded t-means algorithm from
Section 3.2 with t = 2 to ensure that the size
constraint is satisﬁed.
1. Let t = ⌊n/k⌋. Arbitrarily choose t vertices to be
cluster centers and denote them as vc1 , . . . , vct . Denote
the t clusters by c1, . . . , ct.
Initially, all clusters are
empty.
2. For each vertex v ∈ V :
(a) Cluster assignment Add vertex v to the nearest
cluster ci according to a chosen distance metric,
e.g. the one given in Deﬁnition 5.
(b) Bumping If |ci| = k + 1, i.e. ci was full and al-
ready had k members, then perform the following
procedure:
For each vertex u ∈ ci, compute the marginal
cost g(u, ci, cj ), where cj is the surrogate cluster
of u (deﬁnitions given in Section 3.1). Bump the
vertex u∗ with the lowest marginal cost to its sur-
rogate cluster cj .
(c) Extra vertices If |V | is not a multiple of k, re-
mainder vertices may be safely placed in their
nearest cluster, without fear of violating the
minimum-size constraint.
3. Cluster update For each cluster ci, a new cluster cen-
ter v∗
ci is computed. For our work, we use the methods
described in Section 3.2. If the new cluster centers are
the same as for the previous iteration, then an equilib-
rium has been reached and the algorithm terminates.
Otherwise, repeat from Step 2. 3
3.4 Union-Split Clustering Algorithm
Neither the conventional
t-means algorithm nor our
bounded version guarantee to produce a globally optimal
clustering solution. One property of t-means algorithms is
that the initial set of cluster centers are chosen arbitrarily,
which may aﬀect the clustering outcome. To avoid this vari-
ability in clustering results, we design a new deterministic
clustering algorithm, the union-split algorithm, which is de-
scribed below.
1. Initialize each vertex to be in its own cluster.
2. Compute all pair-wise distances between cluster cen-
ters (see Deﬁnition 5). For each cluster, maintain the
next nearest cluster to it using a min-heap data struc-
ture.
3. While there exists an undersized cluster (< k mem-
bers):
(a) Choose an undersized cluster c whose distance to
its nearest cluster (full or undersized) is the short-
est. Union cluster c with its nearest cluster.
(b) If the combined cluster c′ is overfull (size ≥ 2k),
split it into two clusters each of size ≥ k. Splitting
may be accomplished by ﬁnding the two vertices
3In our experiments, we impose an upper limit on the num-
ber of iterations, but equilibrium is usually achieved before
the limit is reached.
(c) Update all relevant cluster distances.
4. When all clusters are full, stop.
Theorem 3.1. The union-split clustering algorithm con-
verges in a ﬁnite number of iterations. In particular, at each
iteration the number of clusters whose sizes are under k is
strictly reduced.
Theorem 3.2. The complexity of the union-split cluster-
ing algorithm is O(n2 log n), where n is the number of ver-
tices in the original graph.
Proofs of the theorems are given in the appendix.
4.
INTER-CLUSTER MATCHING FOR
ANONYMIZATION
Once we have clustered the vertices in a graph, the prob-
lem still remains of anonymizing the vertices within each
cluster. That is, given a set of vertices in a graph, modify
the graph so that those vertices are indistinguishable to a
particular adversary.
One approach is called graph generalization [6]. The idea
is simple: once the nodes are grouped based on their simi-
larities, a general description of the graph consisting of the
number of nodes in each cluster and the numbers of edges
between each pair of clusters is revealed, and nothing else.
For example, in a generalized graph, we may know that clus-
ter 1 has 5 nodes, 10 internal edges, and 6 external edges
to cluster 2, etc. To use the published generalized graph
for research purposes, one must randomly generate a sam-
ple graph in accordance with the generalized description.
Although the generalization procedure provides strong pri-
vacy and is simple to carry out, we ﬁnd that it may have
negative impacts on the utility of the anonymized graphs,
as we show in our experiments in Section 5.
Figure 1 illustrates a simple example where the gen-
eralized graph approach to anonymization might perform
poorly. Due to the way edges are randomly inserted when
generating a sample anonymized graph, samples of a sin-
gle generalized graph may produce large variations in graph
properties.
In Figure 1, the solid black vertices represent
individuals with social roles of great inﬂuence over their lo-
cal networks. While the sample anonymized graph preserves
some local structure, much of the high-level graph structure
is lost. In our work, we strive to preserve social connectivity
by minimizing the changes introduced to the original graph.
In the following sections, we present a novel approach
to graph anonymization that we call inter-cluster matching.
Our method takes a clustered graph and strategically adds
and removes edges to anonymize the graph. We present al-
gorithms for both the degree-based and 1-hop degree-based
privacy models.
4.1 Basic Inter-Cluster Matching Method
The Basic Inter-Cluster Matching algorithm is for the 0-
hop degree-based privacy model, where the adversary only
has prior knowledge about the degree of the target node, not
about its neighbors. The anonymization task is to adjust the
Figure 1: A drawback of the generalized graph approach to anonymization.
degrees of nodes so that nodes within a cluster have the same
degree. Our procedure is described as follows:
1. Cluster using any clustering algorithm and compute
the (rounded) average degree of nodes within each clus-
ter.
2. For each node, determine how many edges it must add
or remove in order to match the degree of its cluster
center.
3. Match up vertices that are adjacent but both have too
many edges, and remove the edge between them.
4. If there are still vertices with too many edges, remove
the necessary number of edges arbitrarily.
5. Finally, match up vertices that have too few edges and
join them with an edge.
6. Add a fake vertex if needed. It can easily be shown
that at most one fake vertex needs to be added, and
it will likely be easy to anonymize because low-degree
vertices are common in most social networks.
4.2 Extended Inter-Cluster Matching Method
We build on the above simple method to design an Ex-
tended Inter-Cluster Matching algorithm so that the result-
ing anonymized graph is robust against 1-hop degree-based
attacks. The extended method is more complicated, as it
needs to match up not only nodes themselves but also their
neighbors. Suppose a vertex v has neighborhood degrees
{5, 4, 3, 2}, and the cluster center is {4, 4, 3}. Then v needs
to remove the edges to the vertices of degrees 5 and 2, and
add an edge to a vertex of degree 4. The removal process is
simple - just remove those edges from the graph. To add an
edge, however, is more complicated. To accomplish this, we
use a more sophisticated version of inter-cluster matching.
In this case, the other endpoint needs to be a vertex of
degree 4 that also needs to gain a neighbor of degree 3. To
state it generally, a vertex of degree X that needs to gain
a neighbor of degree Y must be matched with a vertex of
degree Y that needs to gain a neighbor of degree X. Note
that for all these purposes we use the anonymized degrees of
the vertices, not the actual degrees. Our algorithm proceeds
as follows:
1. Cluster using any clustering algorithm and compute
the cluster centers using the mode-based method from
Section 3.2.
2. Match up vertices that are adjacent but both desire to
lose their common edge, and remove the edge.
3. If there are still vertices who desire to lose edges, re-
move those edges accordingly.
4. Precompute a neighborhood matching table as follows.
The rows represent the anonymized degree of a vertex,
or the I am of that vertex. The columns represent the
neighbor degrees that need to be gained, or the I need
of a vertex. The cell at (X, Y ) contains a multi-set
of all vertices with anonymized degree X that need to
gain a neighbor of degree Y .
5. Match up remaining vertices in a way that brings them
mutual beneﬁt: for each vertex at cell (X, Y ) in the ta-
ble, pair it up with a vertex at cell (Y, X), adding an
edge between the two and removing them from the ta-
ble. Anytime multiple options are available, heuristics
may be used to choose the edge that best preserves the
social roles of the vertices involved.
6. If there are vertices left in the table for which the com-
plementary table entry is vacant, then create fake ver-
tices with the required degrees to pair up with these
left-over vertices.
The above inter-cluster matching approach can be further
generalized to handle i-hop degree-based attack models for
i ≥ 2. Due to space limitations, we omit the details here.
5. EXPERIMENTAL RESULTS
We run our experiments on Intel(R) Core(TM)2 CPU
2.40GHz machines with 2G memory, and running Fedora
Figure 2: Running time of several clustering algorithms on graphs of varying size. The graph to the left is under the 0-hop
degree model, and the graph to the right is under the 1-hop degree model.
Figure 3: The average distances and average squared distances of vertices to their cluster centers in an R-MAT(4096, 12)
graph using four diﬀerent clustering methods. The graphs to the left are under the 0-hop degree model, and the graphs to the
right are under the 1-hop degree model.
6 Linux. We implement all of our algorithms in Java. We
write a program to generate graphs using the R-MAT algo-
rithm [4]. R-MAT generated graphs have a power-law vertex
degree distribution and small-world characteristic, key prop-
erties exhibited by social networks [10]. All the experiments
described in the following are tested on R-MAT generated
graphs. R-MAT(n, d) represents a graph with n vertices and
average degree of d. The greedy method in our experiments
is brieﬂy described as follows: (1) pick an arbitrary node
v ∈ V , (2) ﬁnd the k − 1 remaining nodes most similar to
v, (3) consider those k nodes an anonymization group and
remove them from the set. (4) Repeat steps 1-3 until all
nodes are grouped. All versions of t-means algorithms are
allowed to run for at most 10 iterations.
The running time experiments are on R-MAT-generated
graphs with |V | = {128, 256, 512, 1024, 2048}, and average
degree of log |V |. The results are shown in Figure 2. The
performance of various clustering algorithms in terms of the
average distance and average squared distance is shown in
Figure 3. 4 For the 0-hop degree-based model, our union-
split algorithm runs in roughly half the time of the classic
or bounded t-means algorithms, all of which are signiﬁcantly
faster than the greedy algorithm. This is expected, since the
greedy algorithm has running time complexity of O(n2) and
the t-means algorithms run in O(nt) time, whereas union-
split only involves O(n) union or split operations. Although
the asymptotic running time complexities are the same for
the 1-hop degree-based model, union-split suﬀers from the
high overhead cost of computing the center of a cluster (us-
ing our mode-based method), which it must perform at each
iteration.
4We also measure distances for graph sizes 512, 1024, 2048.
Due to space limitations, we only report results on graph
size 4096.
Figure 4: Utilities of the anonymized graphs in comparison to the original graph R-MAT(512, 9) under four diﬀerent metrics
for k = 20. In each graph, the red line represents our anonymization method; the thick black line represents the original graph;
the grey lines represent several samples of generalized graphs; and the blue line is the random graph. Y-axis is frequency in
the lower two sub-graphs, the size of largest connected component in resiliency, and the number infected in infectiousness.
Figure 5: Comparison of the degree distribution as another utility measure under diﬀerent k = 5, 10, 20, 50 for R-MAT(512,9).
Y-axis is frequency.
Overall, we ﬁnd that our union-split algorithm performs
the best in terms of reasonable run-time and low average dis-
tances under both privacy models in most cases. However,
in a few cases, union-split produces higher average distances
and average squared distances than greedy or bounded t-
means. The reason for this observation is that union-split
may produce large cluster size between [k, 2k), whereas oth-
ers produce clusters of size exactly k. As the cluster size in-
creases, the average distance and squared distance from the
cluster center increase. On the other hand, larger clusters
bring better privacy as the crowd of similar nodes gets larger.
Thus, these sets of experiments show a trade-oﬀ between
clustering performance and privacy. In general, union-split
consistently performs well for diﬀerent graph sizes, cluster
sizes, and adversary models. Therefore, in the following tests
of the anonymization methods and their utility evaluations,
we use union-split as the clustering algorithm.
We have ﬁve measures of utility to evaluate how close
the anonymized graphs are to the original one. The utility
metrics are (1) degree distribution, (2) shortest path dis-
tribution (degrees of separation), (3) transitivity spectrum,
(4) resiliency, and (5) infectiousness. Our utility results are
shown in Figures 4 and 5. The transitivity (or clustering co-
eﬃcient) of a vertex is the fraction of pairs of its neighbors
that share an edge. We measure the distribution over the
transitivity values of all vertices in the graph. Resiliency
measures the size of the largest component after deleting a
fraction of the nodes in the graph, starting with those of
largest degree. Infectiousness computes the number of in-
fected nodes corresponding to a transmission rate.
Consider the degree distribution in Figure 5. When the
graph size n is relatively small and k is relatively large (e.g.,
n = 512 and k = 50), our anonymized graph has larger de-
viations from the original graph due to the k-anonymity re-
quirement on the degree. In comparison, generalized graphs
do not demonstrate this type of behavior as they are under
a diﬀerent privacy model. In most cases, however, graphs
anonymized by our method (red lines) produce quite ac-
curate utility values compared to the original graph (black
lines). In comparison, samples of the generalized graphs fol-
lowing the approach in [6] (grey lines) produce a large degree
of variation in some utilities. Certain utility measures (e.g.,
degree distribution) are better at distinguishing the diﬀer-
ent anonymization methods than others (e.g., shortest path
distribution).
Interestingly, we ﬁnd that the generalized graph samples
give similar utility values to those of an independently, ran-
domly generated R-MAT graph (blue line). This demon-
strates that while they might preserve properties common to
many social networks, generalized graphs may not accurately
represent important diﬀerences between social networks.
The advantages of our anonymization approach can be
summarized as follows:
1. As demonstrated by Figures 4 and 5, the graphs
anonymized via inter-cluster matching very closely
represent the original graphs for all ﬁve utility mea-
sures, except for the discrepancy in degree distribution
explained above.
2. The generalized graph method introduces a high
degree of variability in anonymized graph samples.
Therefore, while there is a chance that their method
will produce a graph that very closely resembles the
original, there is also a chance that a generalized graph
sample will be a very poor representative of the origi-
nal social network.
3. By attempting to retain both local and global elements
of graph structure, our methods may perform well in
preserving other properties pertaining to social con-
nections within a network that are not measured here.
6. RELATED WORK
Data anonymization has traditionally been studied by the
database community, as the focus is on relational data. How-