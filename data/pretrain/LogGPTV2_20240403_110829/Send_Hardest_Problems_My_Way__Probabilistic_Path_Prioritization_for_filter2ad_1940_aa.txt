title:Send Hardest Problems My Way: Probabilistic Path Prioritization for
Hybrid Fuzzing
author:Lei Zhao and
Yue Duan and
Heng Yin and
Jifeng Xuan
Send Hardest Problems My Way:
Probabilistic Path Prioritization for Hybrid Fuzzing
Lei Zhao∗§ Yue Duan† Heng Yin†
Jifeng Xuan‡
∗School of Cyber Science and Engineering, Wuhan University, China
§Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, China
†University of California, Riverside
‡School of Computer Science, Wuhan University, China
{yduan005, heng}@cs.ucr.edu
PI:EMAIL
PI:EMAIL
Abstract—Hybrid fuzzing which combines fuzzing and con-
colic execution has become an advanced technique for software
vulnerability detection. Based on the observation that fuzzing and
concolic execution are complementary in nature, the state-of-the-
art hybrid fuzzing systems deploy “demand launch” and “optimal
switch” strategies. Although these ideas sound intriguing, we
point out several fundamental limitations in them, due to over-
simpliﬁed assumptions. We then propose a novel “discriminative
dispatch” strategy to better utilize the capability of concolic
execution. We design a Monte Carlo based probabilistic path
prioritization model to quantify each path’s difﬁculty and prior-
itize them for concolic execution. This model treats fuzzing as a
random sampling process. It calculates each path’s probability
based on the sampling information. Finally, our model prioritizes
and assigns the most difﬁcult paths to concolic execution. We
implement a prototype system DigFuzz and evaluate our system
with two representative datasets. Results show that the concolic
execution in DigFuzz outperforms than those in state-of-the-art
hybrid fuzzing systems in every major aspect. In particular, the
concolic execution in DigFuzz contributes to discovering more
vulnerabilities (12 vs. 5) and producing more code coverage
(18.9% vs. 3.8%) on the CQE dataset than the concolic execution
in Driller.
I.
INTRODUCTION
Software vulnerability is considered one of the most serious
threats to the cyberspace. As a result, it is crucial to discover
vulnerabilities in a piece of software [12], [16], [25], [27],
[32]. Recently, hybrid fuzzing, a combination of fuzzing and
concolic execution, has become increasingly popular in vulner-
ability discovery [5], [29], [31], [39], [42], [46]. Since fuzzing
and concolic execution are complementary in nature, combin-
ing them can potentially leverage their unique strengths as well
as mitigate weaknesses. More speciﬁcally, fuzzing is proﬁcient
in exploring paths containing general branches (branches that
have large satisfying value spaces), but is by design incapable
of exploring paths containing speciﬁc branches (branches that
have very narrow satisfying value spaces) [27]. In contrast,
The main work was conducted when Lei Zhao worked at University of
California Riverside as a Visiting Scholar under Prof. Heng Yin’s supervision.
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23504
www.ndss-symposium.org
concolic execution is able to generate concrete inputs that
ensure the program to execute along a speciﬁc execution path,
but it suffers from the path explosion problem [9]. In a hybrid
scheme, fuzzing normally undertakes the majority tasks of path
exploration due to the high throughput, and concolic execution
assists fuzzing in exploring paths with low probabilities and
generating inputs that satisfy speciﬁc branches. In this way,
the path explosion problem in concolic execution is alleviated,
as concolic execution is only responsible for exploring paths
with low probabilities that may block fuzzing.
The key research question is how to combine fuzzing and
concolic execution to achieve the best overall performance.
Driller [39] and hybrid concolic testing [29] take a “demand
launch” strategy: fuzzing starts ﬁrst and concolic execution is
launched only when the fuzzing cannot make any progress
for a certain period of time, a.k.a., stuck. A recent work [42]
proposes an “optimal switch” strategy: it quantiﬁes the costs
for exploring each path by fuzzing and concolic execution
respectively and chooses the more economic method for ex-
ploring that path.
We have evaluated both “demand launch” and “optimal
switch” strategies using the DARPA CQE dataset [13] and
LAVA dataset [15], and ﬁnd that although these strategies
sound intriguing, none of them work adequately, due to unre-
alistic or oversimpliﬁed assumptions.
For the “demand launch” strategy, ﬁrst of all, the stuck
state of a fuzzer is not a good indicator for launching concolic
execution. Fuzzing is making progress does not necessarily
mean concolic execution is not needed. A fuzzer can still
explore new code, even though it has already been blocked by
many speciﬁc branches while the concolic executor is forced to
be idle simply because the fuzzer has not been in stuck state.
Second, this strategy does not recognize speciﬁc paths that
block fuzzing. Once the fuzzer gets stuck, the demand launch
strategy feeds all seeds retained by the fuzzer to concolic
execution for exploring all missed paths. Concolic execution
is then overwhelmed by this massive number of missed paths,
and might generate a helping input for a speciﬁc path after a
long time. By then, the fuzzer might have already generated a
good input to traverse that speciﬁc path, rendering the whole
concolic execution useless.
Likewise, although the “optimal switch” strategy aims to
make optimal decisions based on a solid mathematical model
(i.e., Markov Decision Processes with Costs, MDPC for short),
it is nontrivial to quantify the costs of fuzzing and concolic
execution for each path. For instance, to quantify the cost of
concolic execution for a certain path, MDPC requires to collect
the path constraint, which is already expensive. As a result, the
overall throughput of MDPC is greatly reduced. Furthermore,
when quantifying the cost of fuzzing, MDPC assumes a
uniform distribution on all
test cases. This assumption is
oversimpliﬁed, as many state-of-the-art fuzzing techniques [4],
[12], [16] are adaptive and evolutionary. Finally, even if the
costs of fuzzing and concolic execution can be accurately
estimated, it is challenging to normalize them for a uniﬁed
comparison, because these two costs are estimated by tech-
niques with different metrics.
Based on these observations, we argue for the following
design principles when building a hybrid fuzzing system:
1) since concolic execution is several orders of magnitude
slower than fuzzing, we should only let it solve the “hardest
problems”, and let fuzzing take the majority task of path
exploration; and 2) since high throughput is crucial for fuzzing,
any extra analysis must be lightweight to avoid adverse impact
on the performance of fuzzing.
In this paper, we propose a “discriminative dispatch”
strategy to better combine fuzzing and concolic execution. That
is, we prioritize paths so that concolic execution only works
on selective paths that are most difﬁcult for fuzzing to break
through. Therefore, the capability of concolic execution is
better utilized. Then the key for this “discriminative dispatch”
strategy to work is a lightweight method to quantify the
difﬁculty level for each path. Prior work solves this problem
by performing expensive symbolic execution [18], and thus is
not suitable for our purpose.
In particular, we propose a novel Monte Carlo based
probabilistic path prioritization (M CP 3) model, to quantify
each path’s difﬁculty in an efﬁcient manner. To be more
speciﬁc, we quantify a path’s difﬁculty by its probability of
how likely a random input can traverse this path. To calculate
this probability, we use the Monte Carlo method [35]. The
core idea is to treat fuzzing as a random sampling process,
consider random executions as samples to the whole program
space, and then calculate each path’s probability based on the
sampling information.
We have implemented a prototype system called DigFuzz.
It leverages a popular fuzzer, American Fuzzy Lop (AFL) [47],
as the fuzzing component, and builds the concolic executor on
top of Angr, an open-source symbolic execution engine [38].
We evaluate the effectiveness of DigFuzz using the CQE bina-
ries from DARPA Cyber Grand Challenge [13] and the LAVA
dataset [15]. The evaluation results show that the concolic
execution in DigFuzz contributes signiﬁcantly more to the
increased code coverage and increased number of discovered
vulnerabilities than state-of-the-art hybrid systems. To be more
speciﬁc, the concolic execution in DigFuzz contributes to
discovering more vulnerabilities (12 vs. 5) and producing more
code coverage (18.9% vs. 3.8%) on the CQE dataset than the
concolic execution in Driller [39].
of-the-art hybrid fuzzing strategies (“demand launch”
and “optimal switch”), and discover several important
limitations that have not been reported before.
• We propose a novel “discriminative dispatch” strategy
as a better way to construct a hybrid fuzzing system.
It follows two design principles: 1) let fuzzing con-
duct the majority task of path exploration and only
assign the most difﬁcult paths to concolic execution;
and 2) the quantiﬁcation of path difﬁculties must
be lightweight. To achieve these two principles, we
design a Monte Carlo based probabilistic path priori-
tization model.
• We implement a prototype system DigFuzz, and eval-
uate its effectiveness using the DARPA CQE dataset
and LAVA dataset. Our experiments demonstrate that
DigFuzz outperforms the state-of-the-art hybrid sys-
tems Driller and MDPC with respect to more discov-
ered vulnerabilities and higher code coverage.
II. BACKGROUND AND MOTIVATION
Fuzzing [30] and concolic execution [9] are two representa-
tive techniques for software testing and vulnerability detection.
With the observation that fuzzing and concolic execution can
complement each other in nature, a series of techniques [5],
[29], [31], [39], [42] have been proposed to combine them
together and create hybrid fuzzing systems. In general, these
hybrid fuzzing systems fall
into two categories: “demand
launch” and “optimal switch”.
A. Demand Launch
The state-of-the-art hybrid schemes such as Driller [39]
and hybrid concolic testing [29] deploy a “demand launch”
strategy. In Driller [39], the concolic executor remains idle
until the fuzzer cannot make any progress for a certain period
of time. It then processes all the retained inputs from the fuzzer
sequentially to generate inputs that might help the fuzzer and
further lead to new code coverage. Similarly, hybrid concolic
testing [29] obtains both a deep and a wide exploration of
program state space via hybrid testing. It reaches program
states quickly by leveraging the ability of random testing
and then explores neighbor states exhaustively with concolic
execution.
In a nutshell, two assumptions must hold in order to make
the “demand launch” strategy work as expected:
(1) A fuzzer in the non-stuck state means the concolic execu-
tion is not needed. The hybrid system should start concolic
execution only when the fuzzer gets stuck.
(2) A stuck state suggests the fuzzer cannot make any progress
in discovering new code coverage in an acceptable time.
Moreover, the concolic execution is able to ﬁnd and solve
the hard-to-solve condition checks that block the fuzzer so
that the fuzzing could continue to discovery new coverage.
Contributions. The contributions of the paper are summarized
as follows:
• We conduct an independent evaluation of two state-
Observations. To assess the performance of the “demand
launch” strategy, we carefully examine how Driller works on
118 binaries from DARPA Cyber Grand Challenge (CGC) for
12 hours and ﬁnd ﬁve interesting yet surprising facts.
2
Fig. 1: The distribution of the stuck state duration
Fig. 2: The number of inputs retained by the fuzzer and the
number of inputs taken by concolic execution.
(1) Driller invoked concolic execution on only 49 out of 118
binaries, which means that the fuzzer only gets stuck on
these 49 binaries. This fact is on par with the numbers
(42) reported in the paper of Driller [40].
(2) For the 49 binaries from Fact 1, we statistically calculate
the stuck time periods, and the the distribution of stuck
time periods is shown in Figure 1. We can observe that
that more than 85% of the stuck time periods are under
100 seconds.
(3) On average, it takes 1654 seconds for the concolic executor
to ﬁnish the dynamic symbolic execution on one concrete
input.
(4) Only 7.1% (1709 out of 23915) of the inputs retained by
the fuzzer are processed by the concolic executor within
the 12 hours of testing. Figure 2 presents this huge gap
between the number of inputs taken by concolic execution
and that the number of inputs retained by fuzzing.
(5) The fuzzer in Driller can indeed get help from concolic
execution (import at least one input generated by concolic
execution) on only 13 binaries among the 49 binaries from
Fact 1, with a total of 51 inputs imported after 1709 runs
of concolic execution.
concolic execution can help the fuzzing on merely 13 binaries
despite that it is launched on 49 binaries. Moreover, only 51
inputs from the concolic execution are imported by the fuzzer
after 1709 runs of concolic execution, indicating a very low
quality of the inputs generated by concolic execution.
B. Optimal Switch
A recent study [42] proposes a theoretical framework for
optimal concolic testing. It deﬁnes an “optimal switch” strategy
based on the probability of program paths and the cost of
constraint solving. The “optimal switch” strategy aims to make
an optimal decision on which method to use to explore a given
execution path, based on a mathematical model (i.e., Markov
Decision Processes with Costs, MDPC for short). To achieve
optimal performance, MDPC always selects the method with
lower cost to explore each path. In order for this strategy to
work well, the following assumptions must hold:
(1) The costs for exploring a path by fuzzing and concolic
execution can be accurately estimated.
(2) The overhead of cost estimation is negligible.
(3) The algorithm for making optimal decisions is lightweight.
Limitations. The aforementioned results indicate two major
limitations of the “demand launch” strategy.
First, the stuck state of a fuzzer is not a good indicator to
decide whether the concolic execution is needed. According
to Fact 1, the fuzzer only gets stuck on 49 binaries, meaning
concolic execution is never launched for the other 77 binaries.
Manual investigation on the source code of these 77 binaries
shows that they all contain speciﬁc branches that can block
fuzzing. Further combining with Fact 2, we could see that the
fuzzer in a stuck state does not necessarily mean it actually
needs concolic execution since most of the stuck states are
really short (85% of the stuck states are under 100 seconds).
These facts break the Assumption 1 described above.
Second, the “demand launch” strategy can not recognize
the speciﬁc paths that block fuzzing, rendering very low
effectiveness for concolic execution. On one hand, concolic