n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
STREAM
stream alone
with rdarray
with 2 rdarrays with 3 rdarrays with 3 streams
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
RDARRAY
rdarray alone
with stream
with 2 streams with 3 streams with 3 rdarrays
Figure 5: Slowdown of (a) stream and (b) rdarray when run alone/together on a dual dual-core system
STREAM
RDARRAY
stream alone
with 7 streams + 8 rdarrays with 15 rdarrays
rdarray alone with 7 rdarrays + 8 streams  with 15 streams
Figure 6: Slowdown of (a) stream and (b) rdarray when run alone and together on a simulated 16-core system
that have high row-buffer locality and prioritize threads
that have poor row-buffer locality. Whereas the standard
FR-FCFS scheduling algorithm can starve threads with
poor row-buffer locality (Section 2.3), any algorithm
seeking egalitarian memory fairness would unfairly pun-
ish “well-behaving” threads with good row-buffer local-
ity. Neither of the two options therefore rules out unfair-
ness and the possibility of memory attacks.
Another challenge is that DRAM memory systems
have a notion of state (consisting of the currently
buffered rows in each bank). For this reason, well-
studied notions of fairness that deal with stateless sys-
tems cannot be applied in our setting. In network fair
queuing [24, 40, 3], for example, the idea is that if N pro-
cesses share a common channel with bandwidth B, every
process should achieve exactly the same performance as
if it had a single channel of bandwidth B/N. When map-
ping the same notion of fairness onto a DRAM memory
system (as done in [23]), however, the memory sched-
uler would need to schedule requests in such a way as
to guarantee the following: In a multi-core system with
N threads, no thread should run slower than the same
thread on a single-core system with a DRAM memory
system that runs at 1/N th of the speed. Unfortunately,
because memory banks have state and row conﬂicts incur
a higher latency than row hit accesses, this notion of fair-
ness is ill-deﬁned. Consider for instance two threads in
a dual-core system that constantly access the same bank
but different rows. While each of these threads by itself
has perfect row-buffer locality, running them together
will inevitably result in row-buffer conﬂicts. Hence, it
is impossible to schedule these threads in such a way
that each thread runs at the same speed as if it ran by
itself on a system at half the speed. On the other hand,
requests from two threads that consistently access differ-
ent banks could (almost) entirely be scheduled in parallel
and there is no reason why the memory scheduler should
be allowed to slow these threads down by a factor of 2.
In summary, in the context of memory systems, no-
tions of fairness–such as network fair queuing–that at-
tempt to equalize the latencies experienced by different
threads are unsuitable. In a DRAM memory system, it
is neither possible to achieve such a fairness nor would
achieving it signiﬁcantly reduce the risk of memory per-
formance attacks. In Section 4.1, we will present a novel
deﬁnition of DRAM fairness that takes into account the
inherent row-buffer locality of threads and attempts to
balance the “relative slowdowns”.
The Idleness Problem: In addition to the above ob-
servations, it is important to observe that any scheme
that tries to balance latencies between threads runs into
the risk of what we call the idleness problem. Threads
that are temporarily idle (not issuing many memory re-
quests, for instance due to a computation-intensive pro-
gram phase) will be slowed down when returning to a
more memory intensive access pattern. On the other
hand, in certain solutions based on network fair queu-
ing [23], a memory hog could intentionally issue no or
few memory requests for a period of time. During that
time, other threads could “move ahead” at a proportion-
ally lower latency, such that, when the malicious thread
returns to an intensive access pattern, it is temporarily
prioritized and normal threads are blocked. The idleness
problem therefore poses a severe security risk: By ex-
264
16th USENIX Security Symposium
USENIX Association
ploiting it, an attacking memory hog could temporarily
slow down or even block time-critical applications with
high performance stability requirements from memory.
4.1 Fair Memory Scheduling: A Model
As discussed, standard notions of fairness fail in pro-
viding fair execution and hence, security, when mapping
them onto shared memory systems. The crucial insight
that leads to a better notion of fairness is that we need
to dissect the memory latency experienced by a thread
into two parts: First, the latency that is inherent to the
thread itself (depending on its row-buffer locality) and
second, the latency that is caused by contention with
other threads in the shared DRAM memory system. A
fair memory system should—unlike the approaches so
far—schedule requests in such a way that the second la-
tency component is fairly distributed, while the ﬁrst com-
ponent remains untouched. With this, it is clear why our
novel notion of DRAM shared memory fairness is based
on the following intuition: In a multi-core system with
N threads, no thread should suffer more relative perfor-
mance slowdown—compared to the performance it gets
if it used the same memory system by itself—than any
other thread. Because each thread’s slowdown is thus
measured against its own baseline performance (single
execution on the same system), this notion of fairness
successfully dissects the two components of latency and
takes into account the inherent characteristics of each
thread.
In more technical terms, we consider a measure χi for
each currently executed thread i.8 This measure captures
the price (in terms of relative additional latency) a thread
i pays because the shared memory system is used by mul-
tiple threads in parallel in a multi-core architecture. In
order to provide fairness and contain the risk of denial of
memory service attacks, the memory controller should
schedule outstanding requests in the buffer in such a way
that the χi values are as balanced as possible. Such a
scheduling will ensure that each thread only suffers a fair
amount of additional latency that is caused by the parallel
usage of the shared memory system.
Formal Deﬁnition: Our deﬁnition of the measure χi
is based on the notion of cumulated bank-latency Li,b
that we deﬁne as follows.
Deﬁnition 4.1. For each thread i and bank b, the cumu-
lated bank-latency Li,b is the number of memory cycles
during which there exists an outstanding memory request
by thread i for bank b in the memory request buffer. The
cumulated latency of a thread Li = Pb Li,b is the sum
of all cumulated bank-latencies of thread i.
8The DRAM memory system only keeps track of threads that are
currently issuing requests.
The motivation for this formulation of Li,b is best seen
when considering latencies on the level of individual
memory requests. Consider a thread i and let Rk
i,b denote
the kth memory request of thread i that accesses bank b.
Each such request Rk
i,b is associated with three speciﬁc
times: Its arrival time ak
i,b when it is entered into the re-
quest buffer; its ﬁnish time f k
i,b, when it is completely
serviced by the bank and sent to processor i’s cache; and
ﬁnally, the request’s activation time
i,b := max{f k−1
, ak
sk
i,b}.
i,b
i,b
i,b−sk
This is the earliest time when request Rk
i,b could be
scheduled by the bank scheduler. It is the larger of its
arrival time and the ﬁnish time of the previous request
Rk−1
that was issued by the same thread to the same
bank. A request’s activation time marks the point in time
from which on Rk
i,b is responsible for the ensuing latency
of thread i; before sk
i,b, the request was either not sent
to the memory system or an earlier request to the same
bank by the same thread was generating the latency. With
these deﬁnitions, the amortized latency `k
i,b of request
Rk
i,b is the difference between its ﬁnish time and its acti-
i,b = f k
vation time, i.e., `k
i,b. By the deﬁnition of the
activation time sk
i,b, it is clear that at any point in time,
the amortized latency of exactly one outstanding request
is increasing (if there is at least one in the request buffer).
Hence, when describing time in terms of executed mem-
ory cycles, our deﬁnition of cumulated bank-latency Li,b
corresponds exactly to the sum over all amortized laten-
cies to this bank, i.e., Li,b = Pk `k
i,b.
In order to compute the experienced slowdown of each
thread, we compare the actual experienced cumulated la-
tency Li of each thread i to an imaginary, ideal single-
core cumulated latency eLi that serves as a baseline. This
latency eLi is the minimal cumulated latency that thread
i would have accrued if it had run as the only thread in
the system using the same DRAM memory; it captures
the latency component of Li that is inherent to the thread
itself and not caused by contention with other threads.
Hence, threads with good and bad row-buffer locality
have small and large eLi, respectively. The measure χi
that captures the relative slowdown of thread i caused by
multi-core parallelism can now be deﬁned as follows.
Deﬁnition 4.2. For a thread i, the DRAM memory slow-
down index χi is the ratio between its cumulated latency
Li and its ideal single-core cumulated latency eLi:9
9Notice that our deﬁnitions do not take into account the service and
waiting times of the shared DRAM bus and across-bank scheduling.
Both our deﬁnition of fairness as well as our algorithm presented in
Section 5 can be extended to take into account these and other more
subtle hardware issues. As the main goal of this paper point out and
investigate potential security risks caused by DRAM unfairness, our
model abstracts away numerous aspects of secondary importance be-
cause our deﬁnition provides a good approximation.
USENIX Association
16th USENIX Security Symposium
265
χi := Li/eLi.
Finally, we deﬁne the DRAM unfairness Ψ of a
DRAM memory system as the ratio between the maxi-
mum and minimum slowdown index over all currently
executed threads in the system:
Ψ :=
maxi χi
minj χj
The “ideal” DRAM unfairness index Ψ = 1 is achieved
if all threads experience exactly the same slowdown; the
higher Ψ, the more unbalanced is the experienced slow-
down of different threads. The goal of a fair memory ac-
cess scheduling algorithm is therefore to achieve a Ψ that
is as close to 1 as possible. This ensures that no thread
is over-proportionally slowed down due to the shared na-
ture of DRAM memory in multi-core systems.
Notice that by taking into account the different row-
buffer localities of different threads, our deﬁnition of
DRAM unfairness prevents punishing threads for hav-
ing either good or bad memory access behavior. Hence,
a scheduling algorithm that achieves low DRAM un-
fairness mitigates the risk that any thread in the sys-
tem, regardless of its bank and row access pattern, is
unduly bogged down by other threads. Notice further
that DRAM unfairness is virtually unaffected by the idle-
ness problem, because both cumulated latencies Li and
ideal single-core cumulated latencies eLi are only accrued
when there are requests in the memory request buffer.
Short-Term vs. Long-Term Fairness: So far, the as-
pect of time-scale has remained unspeciﬁed in our def-
inition of DRAM-unfairness. Both Li and eLi continue
to increase throughout the lifetime of a thread. Conse-
quently, a short-term unfair treatment of a thread would
have increasingly little impact on its slowdown index
χi. While still providing long-term fairness, threads that
have been running for a long time could become vulnera-
ble to short-term DoS attacks even if the scheduling algo-
rithm enforced an upper bound on DRAM unfairness Ψ.
In this way, delay-sensitive applications could be blocked
from DRAM memory for limited periods of time.
We therefore generalize all our deﬁnitions to include
an additional parameter T that denotes the time-scale for
In particular, Li(T ) and
which the deﬁnitions apply.
eLi(T ) are the maximum (ideal single-core) cumulated
latencies over all time-intervals of duration T during
which thread i is active. Similarly, χi(T ) and Ψ(T ) are
deﬁned as the maximum values over all time-intervals
of length T . The parameter T in these deﬁnitions deter-
mines how short- or long-term the considered fairness is.
In particular, a memory scheduling algorithm with good
long term fairness will have small Ψ(T ) for large T , but
possibly large Ψ(T 0) for smaller T 0. In view of the se-
curity issues raised in this paper, it is clear that a mem-
ory scheduling algorithm should aim at achieving small
Ψ(T ) for both small and large T .
5 Our Solution
In this section, we propose FairMem, a new fair memory
scheduling algorithm that achieves good fairness accord-
ing to the deﬁnition in Section 4 and hence, reduces the
risk of memory-related DoS attacks.
5.1 Basic Idea
The reason why MPHs can exist in multi-core systems
is the unfairness in current memory access schedulers.
Therefore, the idea of our new scheduling algorithm is
to enforce fairness by balancing the relative memory-
related slowdowns experienced by different threads. The
algorithm schedules requests in such a way that each
thread experiences a similar degree of memory-related
slowdown relative to its performance when run alone.
In order to achieve this goal, the algorithm maintains
a value (χi in our model of Section 4.1) that character-
izes the relative slowdown of each thread. As long as all
threads have roughly the same slowdown, the algorithm
schedules requests using the regular FR-FCFS mecha-
nism. When the slowdowns of different threads start di-
verging and the difference exceeds a certain threshold
(i.e., when Ψ becomes too large), however, the algo-
rithm switches to an alternative scheduling mechanism
and starts prioritizing requests issued by threads experi-
encing large slowdowns.
5.2 Fair Memory Scheduling Algorithm
(FairMem)
The memory scheduling algorithm we propose for use
in DRAM controllers for multi-core systems is deﬁned
by means of two input parameters, α and β. These pa-
rameters can be used to ﬁne-tune the involved trade-offs
between fairness and throughput on the one hand (α)
and short-term versus long-term fairness on the other
(β). More concretely, α is a parameter that expresses
to what extent the scheduler is allowed to optimize for
DRAM throughput at the cost of fairness, i.e., how much
DRAM unfairness is tolerable. The parameter β corre-
sponds to the time-interval T that denotes the time-scale
of the above fairness condition. In particular, the mem-
ory controller divides time into windows of duration β
and, for each thread maintains an accurate account of
its accumulated latencies Li(β) and eLi(β) in the current
time window.10
10Notice that in principle, there are various possibilities of interpret-
ing the term “current time window.” The simplest way is to completely
reset Li(β) and eLi(β) after each completion of a window. More so-
phisticated techniques could include maintaining multiple, say k, such
windows of size β in parallel, each shifted in time by β/k memory
cycles. In this case, all windows are constantly updated, but only the
oldest is used for the purpose of decision-making. This could help in
reducing volatility.
266
16th USENIX Security Symposium
USENIX Association
Instead of using the (FR-FCFS) algorithm described
in Section 2.2.3, our algorithm ﬁrst determines two can-
didate requests from each bank b, one according to each
of the following rules:
• Highest FR-FCFS priority: Let R
FR-FCFS be the re-
quest to bank b that has the highest priority according
to the FR-FCFS scheduling policy of Section 2.2.3.
That is, row hits have higher priority than row con-
ﬂicts, and—given this partial ordering—the oldest re-
quest is served ﬁrst.
• Highest fairness-index: Let i0 be the thread with
highest current DRAM memory slowdown index
χi0 (β) that has at least one outstanding request in the
memory request buffer to bank b. Among all requests
to b issued by i0, let R
Fair be the one with highest FR-
FCFS priority.
Between these two candidates, the algorithm chooses the
request to be scheduled based on the following rule:
• Fairness-oriented Selection: Let χ`(β) and χs(β)
denote largest and smallest DRAM memory slow-
down index of any request in the memory request
buffer for a current time window of duration β. If
it holds that
χ`(β)
χs(β)
≥ α
then R
R
FR-FCFS otherwise.
Fair
is selected by bank b’s scheduler and
Instead of using the oldest-across-banks-ﬁrst strategy as
used in current DRAM memory schedulers, selection
from requests chosen by the bank schedulers is handled
as follows:
Highest-DRAM-fairness-index-ﬁrst across banks:
The request with highest slowdown index χi(β) among
all selected bank-requests is sent on the shared DRAM
bus.
In principle, the algorithm is built to ensure that at no
time DRAM unfairness Ψ(β) exceeds the parameter α.
Whenever there is the risk of exceeding this threshold,
the memory controller will switch to a mode in which it
starts prioritizing threads with higher χi values, which
decreases χi. It also increases the χj values of threads
that have had little slowdown so far. Consequently, this
strategy balances large and small slowdowns, which de-
creases DRAM unfairness and—as shown in Section 6—
keeps potential memory-related DoS attacks in check.
Notice that this algorithm does not–in fact, cannot–
guarantee that the DRAM unfairness Ψ does stay below
the predetermined threshold α at all times. The impos-
sibility of this can be seen when considering the corner-
case α = 1.
In this case, a violation occurs after the
ﬁrst request regardless of which request is scheduled by
the algorithm. On the other hand, the algorithm always
attempts to keep the necessary violations to a minimum.
Another advantage of our scheme is that an approxi-
mate version of it lends itself to efﬁcient implementation
in hardware. Finally, notice that our algorithm is robust
with regard to the idleness problem mentioned in Sec-
tion 4. In particular, neither Li nor eLi is increased or de-
creased if a thread has no outstanding memory requests
in the request buffer. Hence, not issuing any requests for
some period of time (either intentionally or unintention-
ally) does not affect this or any other thread’s priority in
the buffer.
5.3 Hardware Implementations
The algorithm as described so far is abstract in the sense
that it assumes a memory controller that always has full
knowledge of every active (currently-executed) thread’s
Li and eLi.
In this section, we show how this exact
scheme could be implemented, and we also brieﬂy dis-
cuss a more efﬁcient practical hardware implementation.
Exact Implementation: Theoretically, it is possible
to ensure that the memory controller always keeps accu-
rate information of Li(β) and eLi(β). Keeping track of
Li(β) for each thread is simple. For each active thread,
a counter maintains the number of memory cycles dur-
ing which at least one request of this thread is buffered
for each bank. After completion of the window β (or
when a new thread is scheduled on a core), counters are