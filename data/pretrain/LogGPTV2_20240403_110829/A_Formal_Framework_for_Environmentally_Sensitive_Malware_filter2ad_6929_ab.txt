vector of the entire machine.
Deﬁnition 2. Learnable Function [29]
A function f, computed by a TM M, is learnable if there exists a PPT L such
that
) : X = M] > α(|M|)
P r[X ← LM (1
|M|
Informally, a function is learnable if an adversary can query that function at a
ﬁnite number of locations and with high probability, correctly guess the deﬁnition
of that function.
216
J. Blackthorne et al.
Deﬁnition 3 (Virtual Black-Box Obfuscation) [5]. A PPT O is a virtual black-
box obfuscator if the following conditions hold:
1. Functionality: O(M) and M are input–output equivalent.
2. Polynomial Slowdown: O(M) is at most polynomial longer in running time
and polynomially larger in description length than M. Speciﬁcally there exist
a polynomial p such that for every TM M, |O(M)| ≤ p(|M|) and if M halts
in t steps on input x, then O(M) halts in at most p(t) steps on x.
3. Virtual Black-Box Property: For all semantic predicates π:
|P r[A(O(M)) = π(M)] − P r[SM (1
|M|
) = π(M)]| ≤ α(|M|)
3 System-Interaction Model
The model we outline in this chapter is conceptually simple, but allows us to
explore an obfuscation that is regularly used in practice but has yet to be for-
malized. We begin by explaining the fundamental assumptions of our model.
To decide properties of a TM M, we must ﬁrst observe M. We know that in
real software, program states are mostly hidden. For every line of text or single
animation shown, a program may be executing thousands of instructions with
many variables. To gain information about these hidden program states, we have
two options. First, we can try to infer the internal state of M based on its visible
output. Second, we can directly observe the internal state of M by simulating
M in an analysis environment such as a debugger, virtual machine, or emulator.
We state this assumption formally as follows:
Assumption 1 (Observation). Given any two TMs M1 and M2, the only ways
for M1 to view M2’s current conﬁguration are by simulating M2 and viewing
M2’s conﬁguration directly, or by viewing M2’s output and inferring M2’s con-
ﬁguration.
Although directly viewing a program’s conﬁguration through simulation is
tempting, it comes at a cost. Simulation changes the program’s environment.
These changes can be detected by a program and reacted to.
The second method of observation in Assumption 1 is to infer the program’s
conﬁguration through the program input and output. To increase observation
through this method, an observer can modify the program itself, as opposed to
the program’s environment. An example of modifying the program to increase
observation is inserting print statements into the program. This will leak internal
conﬁguration data about the program, but as mentioned before, these changes
can be detected by the program.
This idea, that increased observation requires modiﬁcation, is central to our
model and accurately reﬂects real programs. We highlight this concept in our
assumption below:
Assumption 2 (Modiﬁcation). The only ways to change a TM’s output is by
modifying the UTM simulating it or by modifying the TM itself.
A Formal Framework for Environmentally Sensitive Malware
217
When a real program is running on a computer, we know that the program
can take its environment as input. An example of this is a program checking for
how many other programs are currently running in the same environment. We
use the term sensor to refer to any mechanism that allows a program to read in
its environment.
Assumption 3 (Sensation). A TM M1 has the ability to read information about
a TM M2 when M2 is simulating M1.
We combine these simple ideas into a single model called the System-
Interaction model.
3.1 Deﬁnitions
Deﬁnition 4 (System-Interaction Model). The hardware H is a unique physi-
cally implemented two-tape UTM with access to a unique random oracle R based
on physical phenomena. H is a black-box that takes input from the user which we
represent by a PPT A. The user gives input in the form of an operating system
U, program M, and any input x1 to M. U is an encoding of a UTM and M is
an encoding of a TM. H then simulates U(M(x1)) and produces two outputs:
(y1, y2). The ﬁrst output y1, will be returned to the user via being written to
the user-tape. The second output y2, is the output to the operating system U via
the system-tape. The user can only see the output that is returned to it via the
user-tape. In addition to user-input, H can give an input to M, labeled x2. H
would then simulate U(M(x1, x2)) (Fig. 1).
Deﬁnition 5 (Program). A program M is a two-tape TM that implements the
following function:
M : Σ∗ × Σ∗ → Γ ∗ × Γ ∗
x1, x2 (cid:4)→ y1, y2
where
1. x1 is the user input string to M,
2. x2 is the system input string to M,
3. y1 is output from M written to the user-tape,
4. and y2 is the output from M written to the system-tape.
3.2 Adversaries
To more accurately represent the obfuscation of real programs, we have expanded
the single TM model into the System-Interaction model. But in order to gain any
insights from our work, we must limit the scope. We have chosen to not consider
the hardware-based adversary that can perform circuit-level instrumentation.
218
J. Blackthorne et al.
Fig. 1. This illustrates basic interaction between user, program, OS, and hardware.
We have also chosen not to address the problem of information leakage through
permanent changes to the system. An example of this would be the adversary
that inspects the state of the system after the program runs. For this work, we
assume that after the program has completed execution, the system U resets
back to the state it held upon being loaded onto the hardware. A practical
example of this would be a program that removes all traces of itself after it has
completed computation. We do allow the adversary to make any changes to U
and then load it onto the hardware. This is akin to loading a custom operating
system or hypervisor onto the hardware.
Given the limitations above, there are three natural approaches to deter-
mining a property of a program M: statically extracting information from M
alone, emulating the hardware H, and changing U or M to leak information. We
consider each in turn.
Static Analysis. Static analysis is the technique of analyzing a program with-
out running it. This means that a system is not needed to simulate it. VBB
obfuscation prevents any information leaking other than through the inputs and
outputs of a program. This is half of the solution to the problem of M leaking
information; the other half involves protecting M’s inputs and outputs. Through-
out this paper, we will utilize VBB obfuscation to address the static adversary.
As of this writing, no eﬃcient implementation of a VBB obfuscator exists. There
has been an attempt at implementation, with source code released, by Apon et
al. [1]. They even provided a obfuscated challenge binary to the community
which was quickly broken by Berstein et al. [8]. It is clear there is a long way to
go on practical implementations.
Emulation. We consider the possibility of the hardware H being simulated
by another software or hardware UTM H(cid:4). This would be akin to placing an
operating system in a hypervisor or hardware emulator. Empirical results show
that any H(cid:4) could simulate H, but not with exact similarity to that of the
original hardware H [19].
A Formal Framework for Environmentally Sensitive Malware
219
Assumption 4. The hardware H can be simulated on any UTM H(cid:4) (cid:7)= H, but
it is infeasible to do so with perfect accuracy.
Modiﬁcation. The third approach is the modiﬁcation of the operating system
U and is the one on which we focus in this paper. When we discuss modifying the
operating system, we are referring to techniques such as attaching a debugger to
the program under analysis. Although a debugger is not part of the OS proper, as
thought of in the software community, it is how we choose to model changes made
to the environment of the program. A much more detailed model would account
for the diﬀerences between the OS proper, debuggers, programs, and hardware.
We have chosen to simplify the system into a simple delineation between the
program, OS, and hardware in order to better highlight the relationships between
observation of programs and their functionality. We think that the relationships
highlighted within our simpliﬁed model are representative of those seen in real
computer systems.
3.3 Semantic Obfuscation
This new model of obfuscation calls for an exploration of obfuscation ideals.
In the single TM model, virtual black-box obfuscation was the absolute ideal
because no more information could be hidden without also hindering function-
ality. In our model with multiple observers, namely the user and the system, a
program can hide all semantic information from the user while still remaining
useful through interaction with the system.
Deﬁnition 6 (Semantic Obfuscation). A TM O is a semantic obfuscator if the
following conditions hold for the obfuscated program Mo = O(M):
1. Functionality: Mo and M are input–output equivalent.
2. Polynomial Slowdown: Mo is at most polynomial longer in running time and
polynomially larger in description length than M. Speciﬁcally there exist a
polynomial p such that for every TM M, |Mo| ≤ p(|M|) and if M halts in t
steps on input x, then Mo halts in at most p(t) steps on x.
3. Semantic Security: Mo hides all semantic properties from an adversary
bounded polynomially in time and space.
The property of semantic security is so strong that a program cannot allow
for any input–output access to the adversary. In the traditional single TM model,
this would cause the obfuscated program to be non-functional.
The idea of semantic security existing simultaneously with functionality is
predicated on two concepts: the distinction between user tape and system tape
and the ability to distinguish between a normal system and an adversarial one.
The ﬁrst requirement is that a program model must have a user tape and sys-
tem tape. Any input–output relationship exposed to the adversary via either tape
would violate the property of semantic security. Within the System-Interaction
model, the adversary has access to the user tape by default, so this restricts
220
J. Blackthorne et al.
our discussion of semantically obfuscated programs to those with no user input–
output. This leaves us with programs which only have system input–output.
This corresponds neatly with our chosen practical reference: malware, which
often does not accept user input or produce user output.
In addition to eschewing user interaction, a program must distinguish
between a normal system and an adversary. It is easy to see that any program
that cannot distinguish a system H from an adversary A can be simulated by
A, allowing the adversary to watch all of the program’s system input and out-
put. Even just poorly approximating the system would allow the adversary to
see some system inputs and outputs. Semantic security is violated if any input–
output relationships are learned by the adversary. To prevent an adversary from
any input–output access to an obfuscated program, we can wrap the obfuscated
program in another program. This wrapper program will only run the obfuscated
program if the system has not been changed by the adversary. This requires the
wrapper program to be able to measure the system; this is achieved through a
type of function called a sensor, which we explore in the following sections.
4 Sensors
Programs can distinguish normal systems from adversaries by measuring prop-
erties of themselves and the system on which they run. We call these types of
measurement functions sensors. Using sensors to distinguish between the system
and the adversary is necessary for any program to achieve semantic obfuscation.
All of our constructions follow a similar pattern. We construct a program Mk
which distinguishes a normal system from an adversarial one. This program calls
another program Mo as a subroutine. We only make claims about the security
properties of Mo and not the distinguisher program Mk. The ability for Mk to
measure the system is what allows the subroutine Mo to fulﬁll the properties of
semantic obfuscation.
Recall that the system input is labeled x2 = H(). When that value is depen-
dent on the environment, we refer to the machine as a sensor. An example of this
is x2 = sensor(U, M). This would return diﬀerent values based on the values of
U and M. In the following sections, we explore sensors with varying properties
to show what is necessary and suﬃcient to achieve semantic obfuscation.
4.1 Learnable Sensor
It is trivial to show that semantic obfuscation cannot be achieved in the System-
Interaction model when the sensor is a learnable function. A polynomial number
of queries by the adversary is enough to forge the sensor, making the hardware
unnecessary to run the program correctly.
Theorem 1. A semantically obfuscated program cannot exist within the System-
Interaction model when the sensor is learnable.
A Formal Framework for Environmentally Sensitive Malware
221