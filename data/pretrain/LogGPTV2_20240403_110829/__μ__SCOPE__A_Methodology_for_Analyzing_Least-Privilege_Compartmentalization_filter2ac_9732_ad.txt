150
50
50 600
10
10
Kernel Context
Page Table + EPT
SFI (baseline)
SFI (optimized)
Capability Hardware
Direct Hardware
Tunmed(op)
Architecture r, w call, ret
free int
ext
0
0
25
50 25
5
5
5
0 600
0
10
0 10
this tradeoff curve connects the two extremes (all-Mediated and
all-Unmediated) but that moderate points are likely more attractive
concrete compartmentalizations that balance minimizing privilege
with performance cost. Privilege-performance tradeoff curves gen-
erated from mediation selection are presented in Sec. 8.
6.5 Weighting Parameters
For the privilege optimization objective used during clustering and
mediation, we take a simple linear sum across the individual PS
metrics for ops, |PS(op)|. Another decision to make is how best
to weight objects. At a uniform object weight of 1, PSR could be
interpreted as the ratio of permitted interactions in an access control
matrix compared to the monolithic case. However, larger objects
(such as composite structures containing multiple fields) likely
represent additional privilege. We weight objects by their size; a
size component in the weight also means that a refactoring to split
apart objects reduces privilege. This means that our PSRs can be
interpreted as an exposure reduction per byte compared to the
monolithic case. Weight tuning is discussed further in Sec. 8.8.
For global objects and code we take the size to simply be the static
size in bytes. For heap objects we take the size to be the average live
data size in bytes associated with the allocation site in our dynamic
runs. We model stack memory as a single monolithic object with
a size equal to the average number of live stack bytes. Important
future work will be decomposing stack memory for more fine-
grained separation. For calls and returns we use w(o, {call, ret}) = 1.
An advantage of the above weighting scheme is that it can be applied
automatically with no human intervention (Sec. 4.2). There is an
opportunity to further tune the compartmentalization algorithms by
scaling the various privilege operation components or by weighting
them according to a policy; e.g., confidentiality or integrity.
6.6 Performance Profiles
For demonstration, we use a set of performance profiles that illus-
trate a range of potential costs for different protection mechanisms
(Tab. 1). All entries are given in cycles; references and calibration
are detailed in App. C. The numbers are best interpreted as average
times for operations including typical caching effects; as such, the
simple model does not account for the specific time of each opera-
tion instance in context. Consequently, we pick conservative values
to use for these averages, and, most importantly, the profiles model
costs that span orders of magnitude to illustrate how curves shift
with a range of costs.
Figure 4: Linux kernel dynamic tracing privilege coverage. Twenty passes of
the LTP test suite are added to a single CAPMAP (blue), followed by twenty
passes of the Phoronix benchmarks (green). Each point shows the total num-
ber of new CAPMAP graph entries that are observed for the first time in that
pass of testing. Note the log-scale Y axis.
7 EXPERIMENTAL METHODS
7.1 CAPMAP Tracer
To collect CAPMAPs from the Linux kernel, we use Memorizer [61].
Memorizer is a tracing kernel that uses a combination of source code
annotations and compile-time tooks to capture every call, return,
allocation, free, and memory access. Captured traces are stored in
memory and written out after a tracing run for post-processing and
analysis. We disable KASLR so that addresses are consistent across
runs and use a single core configuration, but otherwise use the
default kernel 4.10.0 configuration from Ubuntu LTS 16.04. Read,
write and call logging are turned off during boot, but memory
allocations are still traced. Logging is enabled before running a
workload or LTP [3] test on the kernel. This means the CAPMAPs
produced do not include permissions needed only during boot.
7.2 Coverage Test Sets
To exercise the kernel and build an initial CAPMAP, we use the
Linux Test Project (LTP) test suite [3] (release 20180926). The LTP
contains suites of tests for stressing various kernel components
(e.g., scheduling, syscalls). We run all the tests applicable to our
configuration (App. B). To improve coverage, we run the test suite
twenty times. In Fig. 4, we show the number of CAPMAP entries
(vertices and edges) that are found (instruction, object, or privilege
used for the first time) by the LTP tests as they are added to a single
CAPMAP (blue). On the last pass of the test suite, 35 new entries
were added, for a cumulative total of 331,013 graph elements after
training. To collect coverage CAPMAPs, we run the LTP tests on
the tracing kernel using QEMU for a total of ~8 CPU-months.
7.3 Performance Benchmarks
While the LTP benchmarks are good for coverage testing, their
emphasis on coverage means they do not represent a typical Linux
workload that one would see in practice. To represent more typ-
ical performance, we run the Phoronix Test Suite [5] (v8.2.0) for
performance overhead assessment. We combine the kernel and
linux-system test suites and run all of the benchmarks that run on
our configuration (22, see App. B). When we add twenty passes of
the Phoronix benchmark CAPMAPs to the full coverage CAPMAP
produced from the LTP runs, 1,196 (0.36%) new CAPMAP entries
are discovered (green in Fig. 4).5 Ten of the full benchmark passes
5These runs for coverage assessment were also collected using QEMU.
llllllllllllllllllllllllllllllllllllllll01101001,00010,000100,000151015151015LTP Pass                Phoronix PassNew entries303µSCOPE: A Methodology for Analyzing Least-Privilege Compartmentalization in Large Software Artifacts
RAID ’21, October 6–8, 2021, San Sebastian, Spain
Sep. Hypothesis
PSR all-Unmediated 0.215
PSR half-Unmediated 0.0687
0.0476
PSR all-Mediated
struct key
6.10%
38.4%
struct cred
24.5%
struct buffer_head
struct file
71.6%
TopDir. α=5e-8 α=1e-7 Dir.
α=5e-7 α=1e-6 α=1e-5
File
α=1e-4 α=1e-3 Func.
0.00520 0.00427 0.0302 0.00257 0.00133 0.000771 0.00289 0.000618 0.000578 0.000567
0.00282 0.00255 0.00745 0.00147 0.000697 0.0003102 0.000833 0.000204 0.000167 0.000140
0.00047 0.00040 0.00272 0.00018 0.000085 0.0000552 0.000200 0.000047 0.000045 0.000040
0.815% 0.785% 5.50% 0.770% 0.760% 0.738%
1.10%
25.1% 23.0% 20.2% 16.6% 1.63%
0.846%
24.2% 22.3% 20.1% 16.0% 3.95%
24.8% 22.9% 29.2% 17.0% 10.9%
4.66%
0.730% 0.720% 0.720%
0.695% 0.670% 0.664%
0.614% 0.604% 0.604%
2.65%
1.16%
1.40%
1.52%
2.05%
3.25%
1.32%
Table 2: Aggregate PSR and object write accessibility. For each separation hypothesis (row 1) we show the range of the aggregate PSR metric based on edge
mediation (rows 2-4). Rows 5-8 show the percent of write instructions that have write privilege to the shown object in the half-Unmediated case. Some objects are
very separable (struct key, struct cred) whereas other objects are poorly encapsulated and are difficult for the algorithms to separate (struct file).
encountered one or zero new instruction-level privileges; note that
the privileges exercised in Phoronix but were not present in the
LTP suite indicate ways to improve the quality of LTP.
For performance modeling, we boot the tracing kernel on a bare
metal system with a 2.1 GHz Intel Xeon CPU E5-2620 and 128GB of
memory.6 We collect baseline kernel runtime Tunsep (Eq. 5) from
the same system with the exact same kernel configuration, except
with tracing disabled (vanilla Linux) using perf [4]. We also collect
baseline function counts in the same manner on an independent
run. Some functions are invoked proportional to runtime. As a
result, our tracing kernel runs see more function invocations (by
27% on average) than the baseline function counts. For overhead
estimates, we use the function counts from the baseline system and
scale operation counts proportionally.
8 LINUX SEPARABILITY RESULTS
8.1 Linux Performance Separability
One important characteristic for performance is the External Call
Ratio (ECR); that is, the fraction of dynamic calls that are external
to the subject for a given choice of SDs and hence pay separation
overhead costs. Fig. 5 (top) shows the ECR for domains generated
from source code structure, and Fig. 5 (bottom) shows how the
ECR trends with α for our algorithmically generated domains. At
an α parameter of 10−4 the clusterer achieves a smaller External
Call Ratio than the TopDir syntactic domain, which has compart-
ments that are 400× larger on average. This shows the advantage
of the clustering algorithms over the syntactic cuts: they have the
freedom to place functions with high call connectivity in the same
compartment to minimize the cost of domain crossings.
8.2 Linux Privilege Separability
Tab. 2 shows how much separation we can get under various sepa-
ration hypotheses. For each separation hypothesis (row 1) we show
the range of the aggregate privilege metric PSR from three edge
assignments: all-Mediated (row 2), half-Unmediated (row 3) and
all-Unmediated (row 4).
To show how the accessibility of several concrete objects trends
with PSR and our various separation hypotheses, we pick a set of
common Linux kernel objects (rows 5-8) and show the percent of
write instructions from live code that have write privilege in the
half-Unmediated case. Note that some objects are very separable
(struct cred) while others are less so (struct file).
6We use the same vmlinux image in the coverage and performance experiments.
ECR
TopDir. Dir.
0.35
0.21
File
0.66
Func.
1.00
(a) Syntactic Subject Domains ECR
(b) Clustered Subject Domains ECR
Figure 5: The External Call Ratio for the syntactic domains (top) and the al-
gorithmic clustered domains (bottom).
8.3 Privilege-Performance Continuum
Fig. 6 shows how we trade off total Privilege Set Ratio and per-
formance overhead for the PageTable+EPT Performance Profile
(Sec. 6.6, Tab. 1). Given a tolerance for a certain level of overhead,
the privilege-performance graph allows us to see what level of priv-
ilege reduction we can potentially obtain. This is a key advantage of
systematic analysis and making the continuum available to develop-
ers. The data shows there is a large potential for privilege reduction
without manual refactoring or paying a substantial performance
penalty. At a 15% overhead, we can achieve a privilege reduction of
500×. Note that we calculate overheads for kernel time, which is
typically a small fraction of total time for most applications.
Each curve in Fig. 6 represents the range of privilege-performance
points generated by edge mediation choices (Sec. 6.4), with the low-
privilege/high-overhead end being fully mediated and the high-
privilege/low-overhead end being all unmediated accesses. The fact
the curves typically have a knee where the overhead drops quickly
at the expense of a small change in privileges shows the value of al-
lowing a small amount of unmediated access. Note that the domains
produced from clustering (colored lines) provide substantially better
privilege-performance tradeoffs than the code-structured domains
(grayscale lines). Larger domains (produced from a smaller α value)
have more privilege since no mediation is applied to calls and re-
turns within a domain. Larger domains have lower costs since more
calls and returns are internal to the domain and incur no overhead.
8.4 Highly-Connected Objects and Refactoring
There are some object outliers in the kernel that are accessed by
many subjects; these objects pose the greatest challenges in object
0.000.250.500.751.00Min1e−11e−21e−31e−41e−51e−61e−71e−8aECR304RAID ’21, October 6–8, 2021, San Sebastian, Spain
Roessler and Dautenhahn, et al.
Figure 6: The privilege-performance continuum for each separation hypothesis using the EPT enforcement mechanism. The privilege lower bound (PSRmin) is
shown as a black vertical line. The squares show the privilege-performance point when each object is owned (Unmediated) by the single subject with the highest
access frequency.
separability. The most highly accessed objects, measured in number
of accessing functions, are task_struct (1,136), ext4_inode (610),
file (529), and dentry (406).1 These objects would induce high
overhead if they could only be placed in a compartment with a
single subject. The ability to mark edges as unmediated in our
compartmentalization model, and, particularly, to allow unmediated
access to an object from multiple subjects, can keep the overhead
down for these subjects (Sec. 5.2). In Fig. 6, the squares show what
would happen if we forced every object exclusively into the single