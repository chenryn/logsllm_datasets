neural networks. IEEE Transactions on Neural Networks 21, 6 (2010), 906–917.
[17] Wouter Duivesteijn and Ad Feelders. 2008. Nearest neighbour classication with
monotonicity constraints. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases. Springer, 301–316.
[18] Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari.
2018. Output Range Analysis for Deep Feedforward Neural Networks. In NASA
Formal Methods Symposium. Springer, 121–138.
[19] Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic,
Brendan O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. 2018. Training
veried learners with learned veriers. arXiv preprint arXiv:1805.10265 (2018).
[20] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and
Pushmeet Kohli. 2018. A dual approach to scalable verication of deep networks.
arXiv preprint arXiv:1803.06567 (2018).
[21] Ruediger Ehlers. 2017. Formal Verication of Piece-Wise Linear Feed-Forward
Neural Networks. 15th International Symposium on Automated Technology for
Verication and Analysis (2017).
[22] Farzan Farnia, Jesse Zhang, and David Tse. 2018. Generalizable Adversarial
Training via Spectral Normalization. In International Conference on Learning
Representations.
[23] Ad Feelders. 2010. Monotone relabeling in ordinal classication. In 2010 IEEE
International Conference on Data Mining. IEEE, 803–808.
[24] Chris Finlay and Adam M Oberman. 2021. Scaleable input gradient regularization
for adversarial robustness. Machine Learning with Applications 3 (2021), 100017.
[25] Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang,
and Martin Vechev. 2019. DL2: Training and Querying Neural Networks with
Logic. In International Conference on Machine Learning (ICML).
[26] Matteo Fischetti and Jason Jo. 2017. Deep Neural Networks as 0-1 Mixed Integer
Linear Programs: A Feasibility Study. arXiv preprint arXiv:1712.06174 (2017).
[27] Jerome H Friedman, Bogdan E Popescu, et al. 2008. Predictive learning via rule
ensembles. The Annals of Applied Statistics 2, 3 (2008), 916–954.
[28] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. Ai 2: Safety and robustness certication
of neural networks with abstract interpretation. In IEEE Symposium on Security
and Privacy (SP).
[29] Maxim Goncharov. [n.d.]. Trac direction systems as malware distribution tools.
http://www.trendmicro.es/media/misc/malware-distribution-tools-research-
paper-en.pdf.
[30] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. 2021.
Regularisation of neural networks by enforcing lipschitz continuity. Machine
Learning 110, 2 (2021), 393–416.
[31] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2016. Adversarial perturbations against deep neural networks
for malware classication. arXiv preprint arXiv:1606.04435 (2016).
[32] Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini,
Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van Esbroeck.
2016. Monotonic calibrated interpolated look-up tables. The Journal of Machine
Learning Research 17, 1 (2016), 3790–3836.
[33] Matthias Hein and Maksym Andriushchenko. 2017. Formal guarantees on the
robustness of a classier against adversarial manipulation. In Advances in Neural
Information Processing Systems.
[34] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
verication of deep neural networks. In International Conference on Computer
Aided Verication (CAV). Springer, 3–29.
[35] Inigo Incer, Michael Theodorides, Sadia Afroz, and David Wagner. 2018.
Adversarially Robust Malware Detection Using Monotonic Classication. In
Proceedings of the Fourth ACM International Workshop on Security and Privacy
Analytics. ACM, 54–63.
[36] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. 2019. Certied
robustness for top-k predictions against adversarial perturbations via randomized
smoothing. International Conference on Learning Representations (ICLR) (2019).
[37] Alex Kantchelian, JD Tygar, and Anthony Joseph. 2016. Evasion and hardening
of tree ensemble classiers. In International Conference on Machine Learning.
2387–2396.
[38] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
2017. Reluplex: An ecient SMT solver for verifying deep neural networks. In
International Conference on Computer Aided Verication (CAV). Springer, 97–117.
[39] Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus,
Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljić, et al.
2019. The marabou framework for verication and analysis of deep neural
networks. In International Conference on Computer Aided Verication (CAV).
[40] Herbert Kay and Lyle H Ungar. 2000. Estimating monotonic functions and their
bounds. AIChE Journal 46, 12 (2000), 2426–2434.
[41] Amin Kharraz, Zane Ma, Paul Murley, Charles Lever, Joshua Mason, Andrew
Miller, Nikita Borisov, Manos Antonakakis, and Michael Bailey. 2019. Outguard:
Detecting in-browser covert cryptocurrency mining in the wild. In The World
Wide Web Conference. 840–852.
[42] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial machine
learning at scale. In International Conference on Learning Representations (ICLR).
[43] Heeyoung Kwon, Mirza Basim Baig, and Leman Akoglu. 2017. A domain-agnostic
approach to spam-URL detection via redirects. In Pacic-Asia Conference on
Knowledge Discovery and Data Mining. Springer, 220–232.
[44] Pavel Laskov et al. 2014. Practical evasion of a learning-based classier: A case
study. In Security and Privacy (SP), 2014 IEEE Symposium on. IEEE, 197–211.
[45] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certied robustness to adversarial examples with dierential privacy.
In 2019 IEEE Symposium on Security and Privacy (SP). IEEE.
[46] Kyumin Lee, James Caverlee, and Steve Webb. 2010. Uncovering social spammers:
social honeypots+ machine learning. In Proceedings of the 33rd international ACM
SIGIR conference on Research and development in information retrieval. 435–442.
[47] Kyumin Lee, Brian Eo, and James Caverlee. 2011. Seven months with the
devils: A long-term study of content polluters on twitter. In Proceedings of the
International AAAI Conference on Web and Social Media, Vol. 5.
[48] Sangho Lee and Jong Kim. 2013. Warningbird: A near real-time detection system
for suspicious urls in twitter stream. IEEE transactions on dependable and secure
computing 10, 3 (2013), 183–195.
[49] Sungyoon Lee, Jaewook Lee, and Saerom Park. 2020. Lipschitz-Certiable
Training with a Tight Outer Bound. Advances in Neural Information Processing
Systems (NeurIPS) (2020).
[50] Klas Leino, Zifan Wang, and Matt Fredrikson. 2021. Globally-Robust Neural
Networks. arXiv preprint arXiv:2102.08452 (2021).
[51] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2018. Second-order
adversarial attack and certiable robustness. (2018).
[52] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2019. Certied
adversarial robustness with additive noise. Advances in Neural Information
Processing Systems (NeurIPS) (2019).
[53] Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. 2020. SoK: Certied Robustness for
Deep Neural Networks. arXiv preprint arXiv:2009.04131 (2020).
[54] Linyi Li, Zexuan Zhong, Bo Li, and Tao Xie. 2019. Robustra: Training Provable
Robust Neural Networks over Reference Adversarial Space. In IJCAI.
[55] Zhou Li, Sumayah Alrwais, Yinglian Xie, Fang Yu, and XiaoFeng Wang. 2013.
Finding the linchpins of the dark web: a study on topologically dedicated hosts
on malicious web infrastructures. In 2013 IEEE Symposium on Security and
Privacy. IEEE, 112–126.
[56] Xuankang Lin, He Zhu, Roopsha Samanta, and Suresh Jagannathan. 2020. ART:
abstraction renement-guided training for provably correct neural networks.
In 2020 Formal Methods in Computer Aided Design (FMCAD). IEEE, 148–157.
Session 2C: Defenses for ML Robustness  CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea490[57] Alessio Lomuscio and Lalit Maganti. 2017. An approach to reachability analysis
for feed-forward relu neural networks. arXiv preprint arXiv:1706.07351 (2017).
[58] Justin Ma, Lawrence K Saul, Stefan Savage, and Georey M Voelker. 2009.
Identifying suspicious URLs: an application of large-scale online learning. In Pro-
ceedings of the 26th annual international conference on machine learning. 681–688.
[59] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
and Adrian Vladu. 2018. Towards deep learning models resistant to adversarial
attacks. International Conference on Learning Representations (ICLR) (2018).
[60] Stefano Melacci, Gabriele Ciravegna, Angelo Sotgiu, Ambra Demontis, Battista
Biggio, Marco Gori, and Fabio Roli. 2020. Can Domain Knowledge Alleviate Adver-
sarial Attacks in Multi-Label Classiers? arXiv preprint arXiv:2006.03833 (2020).
[61] Brad Miller, Alex Kantchelian, Michael Carl Tschantz, Sadia Afroz, Rekha
Bachwani, Riyaz Faizullabhoy, Ling Huang, Vaishaal Shankar, Tony Wu, George
Yiu, et al. 2016. Reviewer integration and performance measurement for malware
detection. In International Conference on Detection of Intrusions and Malware,
and Vulnerability Assessment. Springer, 122–141.
[62] Matthew Mirman, Timon Gehr, and Martin Vechev. 2018. Dierentiable Abstract
Interpretation for Provably Robust Neural Networks. In International Conference
on Machine Learning (ICML). 3575–3583.
[63] Christoph Müller, François Serre, Gagandeep Singh, Markus Püschel, and
Martin Vechev. 2021. Scaling Polyhedral Neural Network Verication on GPUs.
Proceedings of Machine Learning and Systems 3 (2021).
[64] Mark Niklas Müller, Gleb Makarchuk, Gagandeep Singh, Markus Püschel, and
Martin Vechev. 2021. Precise Multi-Neuron Abstractions for Neural Network
Certication. arXiv preprint arXiv:2103.03638 (2021).
[65] Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgower.
2021. Training robust neural networks using Lipschitz bounds. IEEE Control
Systems Letters (2021).
[66] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and
Lorenzo Cavallaro. 2019. TESSERACT: Eliminating experimental bias in malware
classication across space and time. In 28th USENIX Security Symposium (USENIX
Security 19). 729–746.
[67] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro. 2020.
Intriguing
Properties of Adversarial ML Attacks in the Problem Space. In 2020 IEEE
Symposium on Security and Privacy (SP). IEEE Computer Society, 1308–1325.
https://doi.org/10.1109/SP40000.2020.00073
[68] Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy
Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli.
2019. Adversarial robustness through local linearization. Advances in Neural
Information Processing Systems (NIPS) (2019).
[69] Mukund Raghothaman, Jonathan Mendelson, David Zhao, Mayur Naik, and
Bernhard Scholz. 2019. Provenance-guided synthesis of Datalog programs.
Proceedings of the ACM on Programming Languages 4, POPL (2019), 1–27.
[70] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018.
Certied
International Conference on Learning
defenses against adversarial examples.
Representations (ICLR) (2018).
[71] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. 2018. Semidenite
relaxations for certifying robustness to adversarial examples. In Advances in
Neural Information Processing Systems. 10900–10910.
[72] Gabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu, and Suman Jana. 2020.
CLN2INV: Learning Loop Invariants with Continuous Logic Networks. In
International Conference on Learning Representations (ICLR).
[73] Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya
Razenshteyn, and Sebastien Bubeck. 2019. Provably robust deep learning via
adversarially trained smoothed classiers. Advances in Neural Information
Processing Systems (NeurIPS) (2019).
[74] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang.
2019. A convex relaxation barrier to tight robustness verication of neural
networks. Advances in Neural Information Processing Systems (NeurIPS) (2019).
[75] Xujie Si, Woosuk Lee, Richard Zhang, Aws Albarghouthi, Paraschos Koutris, and
Mayur Naik. 2018. Syntax-guided synthesis of datalog programs. In Proceedings
of the 2018 26th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering. 515–527.
[76] Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin Vechev. 2019.
Beyond the single neuron convex barrier for neural network certication.
Advances in Neural Information Processing Systems (NeurIPS) (2019).
[77] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. 2019. An
abstract domain for certifying neural networks. Proceedings of the ACM on
Programming Languages 3, POPL (2019), 1–30.
[78] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T Vechev. 2019.
Boosting Robustness Certication of Neural Networks.. In ICLR (Poster).
[79] Sahil Singla and Soheil Feizi. 2019. Bounding singular values of convolution
layers. arXiv preprint arXiv:1911.10258 (2019).
[80] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay
Saraswat. 2006. Combinatorial sketching for nite programs. In Proceedings
of the 12th international conference on Architectural support for programming
languages and operating systems. 404–415.
[81] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural
networks. International Conference on Learning Representations (ICLR) (2013).
[82] Kurt Thomas, Chris Grier, Justin Ma, Vern Paxson, and Dawn Song. 2011. Design
and evaluation of a real-time url spam ltering service. In 2011 IEEE symposium
on security and privacy. IEEE, 447–462.
[83] Vincent Tjeng, Kai Xiao, and Russ Tedrake. 2017. Evaluating Robustness of Neu-
ral Networks with Mixed Integer Programming. arXiv preprint arXiv:1711.07356
(2017).
[84] David Wagner and Paolo Soto. 2002. Mimicry attacks on host-based intrusion
detection systems. In Proceedings of the 9th ACM Conference on Computer and
Communications Security. ACM, 255–264.
[85] Shiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. 2018. MixTrain:
arXiv preprint
Scalable Training of Formally Robust Neural Networks.
arXiv:1811.02625 (2018).
[86] Shiqi Wang, Kexin Pei, Whitehouse Justin, Junfeng Yang, and Suman Jana.
2018. Ecient Formal Safety Analysis of Neural Networks. Advances in Neural
Information Processing Systems (NIPS) (2018).
[87] Shiqi Wang, Kexin Pei, Whitehouse Justin, Junfeng Yang, and Suman Jana. 2018.
Formal Security Analysis of Neural Networks using Symbolic Intervals. 27th
USENIX Security Symposium (2018).
[88] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and
J Zico Kolter. 2021. Beta-CROWN: Ecient Bound Propagation with Per-neuron
Split Constraints for Complete and Incomplete Neural Network Verication.
arXiv preprint arXiv:2103.06624 (2021).
[89] Antoine Wehenkel and Gilles Louppe. 2019. Unconstrained monotonic neural
networks. In Advances in Neural Information Processing Systems.
[90] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel,
Duane Boning, and Inderjit Dhillon. 2018. Towards fast computation of certied
robustness for relu networks. In International Conference on Machine Learning
(ICML).
[91] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao,
Cho-Jui Hsieh, and Luca Daniel. 2018. Evaluating the robustness of neural
networks: An extreme value theory approach. In International Conference on
Learning Representations (ICLR).
[92] Eric Wong and Zico Kolter. 2018. Provable defenses against adversarial examples
via the convex outer adversarial polytope. In International Conference on Machine
Learning. 5283–5292.
[93] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. 2018. Scaling
provable adversarial defenses. Advances in Neural Information Processing Systems
(NIPS) (2018).
[94] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and
Cho-Jui Hsieh. 2021. Fast and complete: Enabling complete neural network
verication with rapid and massively parallel incomplete veriers. International
Conference on Learning Representations (ICLR) (2021).
[95] Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and
Jerry Li. 2020. Randomized smoothing of all shapes and sizes. In International
Conference on Machine Learning (ICML). PMLR.
[96] Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, and Ronghui Gu. 2020.
Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks.
In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language
Design and Implementation.
[97] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo
Li, Duane Boning, and Cho-Jui Hsieh. 2020. Towards stable and ecient training
of veriably robust neural networks.
International Conference on Learning
Representations (ICLR) (2020).
[98] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
2018. Ecient neural network robustness certication with general activation
functions. arXiv preprint arXiv:1811.00866 (2018).
[99] Xiao Zhang and David Evans. 2019. Cost-Sensitive Robustness against Adversar-
ial Examples. International Conference on Learning Representations (ICLR) (2019).
A STABILITY
FOR TWITTER ACCOUNT CLASSIFIER
To classify Twitter accounts that broadcast spam URLs, we can use
the number of followers and the ratio of posted URLs over total
number of tweets as features [47]. It is hard for spammers to ob-
tain large amount of followers, and they are likely to post more
URLs than benign users. We specify the URLRatio feature to be
stable, such that arbitrarily changing the feature will not change
the classier’s output by more than 1.
Figure 4 shows one CEGIS iteration to train the stability prop-
erty. The starting classier is a decision tree. For example, “1.0 ⇤
URLRatio  1
Classiﬁer Output:
FR(x) = R3, FR(xʹ) = R1
⎢FR(x) - FR(xʹ)⎥ > 1
3
Constraint
⎢R3 - R1⎥ ≤ 1
CLN:
Gradient-guided
Optimization
0.99*URLRatio <  0.294 ⋀ 
0.99*followers < 1429.5 → -1.71
1.01*URLRatio < 0.272 ⋀
1.0*followers ≥ 1429.49 → -0.11
1.01*URLRatio ≥ 0.274 ⋀ 
0.98* followers < 104.52 → -0.59
1.01*URLRatio ≥ 0.272 ⋀
0.98*followers ≥ 104.51 → 0.89
Updated Classiﬁer
R0
R1
R2
R3
Figure 4: One CEGIS iteration to train stability property for the Twitter account classier. We specify the classier’s output
score to change at most by one when the URLRatio feature is arbitrarily perturbed. Multiple weights of the classiers are
updated by gradient-guided optimization, and the classier after training no longer forms a tree structure.
B PROOF
Lemma 1. If a classier satises Property 3a, then it also satises
Property 3.
P. 8x, x0 2 Rn .[8i <   .xi = x0i]^  (F(x))   , we have
F(x)    1( ). Since F satises Property 3a, then we also have
F(x)  F(x0)   1( ). Therefore, F(x0)  F(x)    1( )  0.
⇤