B. Methodology
Toward practically efﬁcient obfuscation of cache conﬂicts,
the key idea of localized randomization consists of two phases
of randomness. First, for each address, we randomly select a
predeﬁned number of candidate sets for it. The selection uses
the address and a random mapping function to compute cache
set indices. Second, we randomly select one candidate set for
mapping the address. As shown in Figure 3, PhantomCache
enforces localized randomization through modiﬁed placement
and search policies while it does not touch the replacement
policy.
5
Fig. 3. Cache access handling of PhantomCache.
Placement policy. PhantomCache places a block into a ran-
domly selected cache set among several candidate sets. The
indices of the candidate sets are computed using the block’s
address and random salts. Given r candidate cache sets to use,
we introduce r random salts for cache conﬁguration. These
random salts are initialized using an on-chip pseudo-random
number generation (PRNG) upon machine booting. We can
compute the candidate set indices for an address as:
C = {ci | ci = F (address, salti) for ∀i ∈ [0, r − 1]}. (2)
We require that the memory-to-cache mapping function F
should generate candidate sets randomly and independently
among different addresses. We, however, do not simply du-
plicate the block into all of the candidate cache sets. That
would lead to an impractical cache fatigue. We select only one
candidate set at random for placement as the following.
SelectedSetIndex = ci f or i = P RN G(r),
(3)
where P RN G(r) generates a random number ranging from 0
to r − 1.
Search policy. PhantomCache needs to search a block in all
its candidate sets. Upon a cache access, we ﬁrst compute the
indices of all candidate sets by Equation 2. We then compare
the address’s tag ﬁeld against those cached in each set. A
matching indicates a cache hit. Otherwise, a cache miss occurs;
the CPU needs to fetch the block from memory and place it
in cache. Since a small number of candidate sets are sufﬁcient
for security guarantee (Section III-A and Section V), it is
feasible to implement parallel search in hardware. Although
fully parallelism is difﬁcult to realize because that needs a
multi-port cache, we can use a multi-banked cache to improve
parallelism.
Replacement policy. PhantomCache imposes no modiﬁcation
on the replacement policy. When placing a block into a cache
set, if there is no available cache line, one cached block needs
to be replaced. We simply follow the replacement policy in
use such as the commonly used LRU policy.
IV. DESIGN
In this section, we detail the PhantomCache design. Phan-
tomCache logics only reside in the LLC management module,
serving as a transparent layer between the L2 cache and LLC.
The key challenge is how to optimize the extra access latency
while implementing randomized localization. We explore a
cache accesscandidate setindices computationparallel searchphysical addressread/write accesshit?read a block from cache to memory; or write a block to cache from memorymemory fetchrandom gen: p LRU replacementempty line available?set selection:pth candidate setplacementyesnoyesnoFig. 4. PhantomCache architecture.
series of design strategies toward only a single clock cycle
overhead per access and a (cid:100)log2 r(cid:101)-bit storage overhead per
cache line, where r is the number of candidate sets for randomly
mapping an address. PhantomCache can be efﬁciently integrated
into the multi-banked LLC architecture.
A. Architecture: Localized Randomization
As with existing randomized mapping solutions, we imple-
ment PhantomCache logics mainly through a random memory-
to-cache mapping function. As shown in Figure 4, it functions
transparently to the cache and memory. The cache still passively
accepts an access request and returns the requested data block
upon a read hit or continues with caching in the data block
upon a write hit. Upon a cache miss, however, the access
request is directed to memory1. The corresponding data block
is then fetched from memory and written to a cache set. The
replacement policy (i.e., LRU) decides which cache line to
use in the cache set. The placement policy decides which
cache set to use for caching a data block with a speciﬁc
physical address. This is the part that localized randomization
shines forth. As discussed in Section III-B, our newly proposed
localized randomization technique bounds randomized mapping
within only several candidate cache sets across the entire cache.
Since any candidate set is likely to be selected, searching a data
block needs to walk through all its candidate sets. Moreover,
since the memory-to-cache mapping function is modiﬁed, we
accordingly modify the address restoration process as well.
When a dirty data block is written back from cache to memory,
the cached metadata should be sufﬁcient for calculating the
original memory address.
B. Memory-to-Cache Mapping
Randomness is the ultimate design goal of the memory-to-
cache mapping function. Toward randomized mapping, the part
of an address used for calculating the cache set index should
guarantee uniqueness. In current memory hierarchy, addresses
within the same data block always map to the same cache line.
We therefore need only a block-wise address uniqueness using
the tag and index bits (Figure 2). Since these two ﬁelds alone
always generate the same cache set index, we can introduce
r random salts for r-degree PhantomCache. Speciﬁcally, we
calculate the indices of r candidate sets using the address’s
tag and index bits as well as one of the r salts after another.
Toward localized randomization of PhantomCache, a random
1Note that a unit called memory controller coordinates the transmission
of control messages and data blocks between the cache and memory. Since
PhantomCache does not modify the memory access principle, we omit the
memory controller in Figure 4 for simplicity.
6
Fig. 5. Memory-to-cache mapping of PhantomCache.
selector is used for selecting one of the r candidate sets for
placement.
PhantomCache leverages fast built-in hardware random
number generators (HRNGs) on modern CPUs to generate
random salts. For example, an Intel CPU [2] can use an entropy
source to generate a random stream of bits at a high rate
of 3 Gbps. A pair of 256-bit sequences from the entropy
source is used as seeds to generate up to 1,022 128-bit random
numbers, which are stored in a random number pool in hardware.
As with existing hardware-level randomization schemes [35],
[45], PhantomCache can directly request r random numbers
from the random number pool. This avoids the delay of
random number generation upon requests. Furthermore, we
need another step of randomness to select one of r candidate
sets for block placement. The scale of r is decisive for
security. Consider an extreme case when r = 1. In this case,
PhantomCache degenerates to deterministic mapping that is
vulnerable to conﬂict-based cache timing attacks. We thus
require a sufﬁciently large r to secure PhantomCache. However,
a larger r imposes a higher performance overhead due to
searching across all the r candidate sets for every data access.
The analytical results in Section V and experimental results
in Section VII show that PhantomCache can secure a 16 MB
16-way LLC with a small r = 8 and only 0.50% performance
degradation. To select one from 8 candidate sets for data
placement upon each LLC miss, PhantomCache needs only 3
random bits. This should not incur observable latency to the
memory-to-cache mapping process.
While preserving mapping randomness, we need to min-
imize the so-caused cache overhead. The address portion in
a cache line should support both data search and address
restoration. Traditionally, the tag of a memory address is cached.
Since we use an address’s tag and index bits and a salt to
calculate the cache set index, a straightforward solution needs
to at least store the tag and index bits in a cache line. This
necessitates a wider cache. We optimize cache overhead such
that index bits are not cached. The optimization leverages the
fact that addresses with the same index bits (or tag) must have
different tags (or index bits). The mapping randomness can
still hold if we feed the tag and index bits separately into the
mapping function. Besides, to guarantee mapping invisibility to
attackers, a salt is divided into saltlef t and saltright, which are
used respectively at the beginning and ending of the mapping
function. Because a physical address is split into the tag and
the index bits, we respectively XOR saltlef t and saltright with
them after they are input to the mapping function to minimize
the attacker’s control over the mapping result. This idea comes
from the key whitening technique that is commonly used in
block cipher algorithms such as AES [10].
As shown in Figure 5, we ﬁrst compute over the tag and
saltlef t. The result then goes through another computation
addressrestorationdata blockcacheparallelmemory-to-cachemappingrandomindexselectorphysical addressmemory(miss) physical address(miss) data blockwrite back addresswrite back data blockwrite backdatablock  set indexmisshitsaltlefttaghashindex bitssaltrightcachexorxorxorcache indexAlgorithm 2 LFSR based Toeplitz Hash [20]
Input: message
Output: result
1: result := 0;
2: state := LFSR’s initial state;
3: for each bit b of message from LSB to MSB do
4: // LSB: Least Signiﬁcant Bit; MSB: Most Signiﬁcant Bit;
5:
6:
7:
8:
9: end for
end if
state := LFSR’s next state;
if b == 1 then
result := result ⊕ state;
together with index bits and saltright. Note that the second
round of computation essentially involves XORing the index
bits with saltright. The new result is taken as the cache
set
index. Then we only need to cache the tag and the
random number for specifying the adopted salt. This way,
cache overhead is minimized to only a (cid:100)log2 r(cid:101)-bit random
number per cache line. When we use r = 8 to guarantee
a strong security for an 16 MB 16-way LLC (Section V),
PhantomCache introduces only 0.50% storage overhead per
cache line. Furthermore, the length of salts decides security. The
longer the salts are, the more robust they are against brute-force
attacks. Current PhantomCache makes salt length exactly equal
to the length of the tag together with index bits. It incurs only
insigniﬁcant modiﬁcation to increase salt length. For example,
we can add a third part to the salt and concatenate it with the
input of the hash function in Figure 5. Our security analysis in
Section V shows that that the current two-part salt is already
sufﬁcient to withstand brute-force attacks.
Any random hash function sufﬁces to guarantee randomness
of the mapping function. Since PhantomCache requires r
candidate sets, we need implement r such mapping functions
to support parallel mapping. We adapt
the LFSR based
Toeplitz hash [12], [20] toward a single–clock-cycle hash
function (Section IV-C) with affordable hardware complexity
(Section VII-I).
C. Single–Clock-Cycle Hash
The hash function for mapping should be hardware-efﬁcient
and guarantee strong randomness. We select the LFSR based
Toeplitz hash that satisﬁes both requirements. An LFSR is a shift
register that generates a new state using a linear function and
the current state [20]. The LFSR based Toeplitz hash iteratively
generates the hash result of an input message (Algorithm 2).
Starting from the LSB, each iteration XORs the current state
to the result if the message bit is one (lines 5-7). Then LFSR
derives the next state for use in the next iteration (line 8).
Realization of the LFSR based Toeplitz hash [12], [20], however,
uses sequential logic. The message needs to be processed bit
by bit, incurring a high latency when it is long.
We adapt the LFSR based Toeplitz hash toward a single–
clock-cycle hash function using combinational logic. The state
values are pre-computed and stored in registers at boot time.
These state values can be directly input to the hash circuit
without the delay of re-generation upon each hash computation.
As shown in Figure 6, the combinational logic circuit requires
7
Fig. 6. Single–clock-cycle hash function by implementing the LFSR based
Toeplitz hash [20] using combinational logic.
only AND gates and XOR gates. The hash input message is
the XOR result of a t-bit tag and a salt. For each AND gate, a
message bit with value one makes the corresponding LFSR state
go through the XOR gates. Since the message is as long as the
tag ﬁeld, which should be shorter than the up to 64-bit memory
address in modern architecture. The number of XOR gates in the
critical path is at most log2 64 = 6. The number of gate delays
supported in a clock cycle is determined by several factors
such as circuit wiring, clock frequency, lithography, and energy
restriction. Typically, modern processors can process 15∼20
gate operations in one clock cycle [34]. Our hash function in
Figure 6 with a critical path of 7 gates (i.e., 1 AND gate and
6 XOR gates) thus brings only a single clock cycle latency in
most cases. For some processors with an extremely high clock
frequency, only 4∼5 gate operations may ﬁt in a single clock
cycle. In this case, our hash function may bring a latency of
two clock cycles, on par with state-of-the-art cache protection
schemes [35], [45]. Even if the mapping latency is two clock
cycles, the performance degradation by PhantomCache is still
only 1.34% (Section VII).
Randomness. Given an m-bit message M with an n-bit hash
output, the randomness of the LFSR based Toeplitz hash is
quantiﬁed by the following probability-inequality [20]:
∀M (cid:54)= 0, c, P r(h(M ) = c) ≤ m
2n−1 ,
(4)
44
where c denotes a certain hash output. In our case, consider
an LLC with 214 sets and 64-byte cache lines. Given 64-bit
physical addresses, we have 14-bit index bits and tags of 64-14-
log2 64=44 bits. PhantomCache uses tags as the hash input and
index bits as the hash output. By Formula 4, the probability
of a randomly picked tag being hashed to a given set is below
214−1 = 0.5%, which shows no signiﬁcant mapping bias.
Security. Admittedly, using a hardware-efﬁcient hash function
and simple XOR operations may not be cryptographically
secure. However, the application scenario of PhantomCache is
different from a typical cryptographic scenario. The attacker is
assumed to know only the victim physical address and can only
observe cache conﬂicts. Because of the adoption of random salts,
the attacker can neither control the input of the hash function
nor know the output of the hash function. This increases the
difﬁculty to create hash collisions deliberately. Furthermore,
we explore the LFSR based Toeplitz hash as a low-overhead
choice. If this hash function is found to be insecure, we can
replace it with other more secure ones with a longer latency
than one clock cycle (e.g., the ones used in [34], [45]) as long
as they can make the memory-to-cache mapping invisible. This
does not affect the key idea of PhantomCache, that is, localized
localization. In other words, the essential goal of our mapping
state0state1state2statet-1statet-2m[0]m[1]m[2]  m[t-2]m[t-1]      hash resultFig. 7. Parallel search of PhantomCache.
function is to hide the explicit correlation between the physical
address and the set index. Such techniques have been well