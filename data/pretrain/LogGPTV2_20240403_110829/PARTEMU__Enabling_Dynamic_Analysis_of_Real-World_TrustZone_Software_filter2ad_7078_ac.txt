it expects the hash of the root certiﬁcate signing the TA to
be present in a speciﬁc memory location [39]. We discuss
this in detail in Section 8.2. In particular, we only needed
to implement the standard SHA-2 hash algorithm. All other
TZOSes used software cryptography. Finally, TEEGRIS re-
quired a standard real-time clock (RTC), which was again
already implemented in QEMU.
Interrupts. All TZOSes used the ARM-standard global
interrupt controller (either GICv2 or GICv3), both of which
are supported by QEMU. We did not have to add anything
beyond these devices to handle interrupts.
8 PARTEMU Implementation
We implemented our design on PARTEMU, a framework that
we built on QEMU [5] and PANDA [17]. We chose QEMU
because it already has support for TrustZone. PANDA gives
us an extensible and modular framework with already imple-
mented modules such as taint analysis.
PARTEMU adds to PANDA a run management API to unify
the process of dynamic analysis (Table 2). The API is meant to
be invoked by “driver” programs running in the emulator. One
or more backend modules can register to receive callbacks
when the driver calls into the API. This API is implemented
using semihosting calls that call directly into QEMU. We have
currently implemented two modules on this run management
API: fuzz testing with AFL, and an LLVM run module that
outputs an LLVM IR representation of a run of the target.
This output could be fed to symbolic analysis engines such as
KLEE, as in S2E [10, 13].
8.1 AFL PARTEMU Module
We integrate feedback-driven fuzz testing using AFL [61] as a
module to PARTEMU. We base our code on TriforceAFL [24],
which adds AFL support to QEMU system emulation and
support for CPU and memory state duplication (forking), with
one important difference. In [24], AFL runs QEMU as it does
any normal process under test. In contrast, we start QEMU
separately and interact with AFL through a proxy that behaves
to AFL like the process under test. Thus, we are able to keep
our modular structure, and allow AFL to be one among many
backend modules for PARTEMU’s run management API.
Our implementation addresses some additional challenges.
First, we need to identify the target being tested. For exam-
ple, we might want to collect coverage feedback information
from a particular TA. However, there are many components
executing in TrustZone - other TAs, the TZOS, and the secure
monitor. How do we identify our target TA so that we collect
only the target’s coverage information? Second, we need to
ensure stability, i.e., that the same input to a component in a
particular state results in the same output. This is non-trivial
in full-system emulation with randomness and interrupts.
Depending on the TZOS implementation, we determined
two different methods to identify the target under test. First,
we found that Kinibi and TEEGRIS switched the address-
space identiﬁer (ASID) in the TTBR0_EL1 register when they
context-switched between TAs. While Kinibi returned the
ASID to the normal-world CA as part of the TA descriptor,
TEEGRIS used monotonically-increasing ASIDs. Thus, in
both cases, we were able to determine the exact ASID to mon-
itor and it to identify the target. Second, in contrast, neither
QSEE nor the version of OP-TEE we ported changed the
ASID when switching between TAs. However, we found we
could identify the target using address ranges. Before loading
a TA, QSEE requires the normal world to donate a region of
physical address to load the TA. OP-TEE hardcodes such a
region in its binary. Thus, for QSEE and OP-TEE, we identify
the target if the program counter falls within this region. Once
we identify that a particular basic block belongs to the target
TA, we use the block’s virtual address to populate AFL’s cov-
erage map. Selectively populating the coverage map using
only the target’s basic blocks can be viewed as an instance of
domain-speciﬁc fuzzing [36].
Stability is another challenge. AFL deﬁnes stability as the
property that a target returns the same feedback coverage
when fed the same input [60]. We identiﬁed four sources of
instability: interrupts, statefulness, randomness, and QEMU
optimizations. First, interrupts cause different program paths
to be executed. We handle this by simply disabling interrupts
to the secure world during a run. Second, prior inputs to a
stateful target program may drive it to a state where it responds
differently to the same input. We solve this issue by forking
PARTEMU just before starting the test, which forks the entire
CPU and memory state.
Randomness is another source of instability. Kinibi, TEE-
GRIS, and OP-TEE call into the secure monitor to obtain
randomness, whereas QSEE accesses hardware PRNG using
MMIO registers. We simply return a constant in response to
these calls. Finally, the QEMU optimization of translation-
block chaining [5] affects stability. When two or more basic
blocks always occur only in the same sequence, QEMU chains
them together into effectively one translation block. There-
fore, if we track each translation block for coverage, we will
miss these chained blocks. A simple way to solve this issue to
796    29th USENIX Security Symposium
USENIX Association
API
partemu_run_init(id, buffer)
partemu_run_monitor_asid(id, asid)
partemu_run_monitor_addr_range(id, range)
partemu_fork()
partemu_run_read_input(id)
partemu_run_start(id)
partemu_run_stop(id, ret)
partemu_exit(ret)
partemu_run_debug(id, ret)
Description
Register a client with id and buffer for input
Identify target to monitor with asid
Identify target to monitor with address range
Fork a QEMU instance with the same CPU and memory state
Read input from partemu module (e.g., AFL) into registered buffer
Signal run start; partemu module starts monitoring target
Signal run stop with ret value (e.g., crash) ; partemu module stops monitoring target
Exit forked QEMU child with ret value
Pause QEMU and wait for debugger when the target runs next
Table 2: PARTEMU Run Management API.
is to disable chaining, but we found that this reduced perfor-
mance signiﬁcantly. Instead, just before blocks are chained,
we add an inline QEMU IR callback at the end of each block
to the function that records the block. Therefore, blocks can
still be chained but will call into our function inline.
8.2 TA Authentication
TAs have to pass two TZOS checks before they are loaded:
(i) a signature check and (ii) a version check to prevent roll-
back. We describe below how we handle these checks for our
TZOSes.
To pass QSEE’s TA authentication [39] checks, we required
additional hardware emulation. QSEE TAs contain a signature
and the corresponding certiﬁcate chain. QSEE checks that
the hash of the certiﬁcate matches what is stored in a speciﬁc
memory area. On the device, this memory area is backed by
one-time-programmable fuses that are programmed during
device manufacture by the vendor. We faced the challenge
of obtaining this value. This value can be either read directly
from a real device or parsed from the TA binary. Due to our
inability to modify QSEE, we could not extract this value
directly from a device; neither would such an approach scale
to devices from multiple vendors. Instead, we extracted this
value by parsing the root certiﬁcate from the TA binary it-
self. Kinibi, OP-TEE, and TEEGRIS TA authentication, on
the other hand, worked out-of-the-box. They had hardcoded
public keys in the TZOS binary that it used to authenticate
TA signatures.
Our next challenge was overcoming rollback prevention
checks. When TA vulnerabilities are patched, TA version is
increased. The minimum acceptable TA version is typically
stored in secure storage (RPMB). We experimented with two
different approaches to overcome this check. First, for QSEE
and Kinibi, we re-signed the TA binary with a version number
of zero using our own signing key. For Kinibi, we injected
this signing key into the binary. For QSEE, we set the OTP-
fuse memory area with the hash of this signing key. Second,
for TEEGRIS, we emulated the RPMB interface so that it
effectively returned zero as the minimum acceptable TA ver-
sion. Finally, the version of OP-TEE we had did not enforce
rollback checks.
In addition to passing rollback prevention checks, the abil-
ity to sign TA binaries gives other advantages. First, we can
write and sign own custom TAs for testing. Second, it allows
us to test TAs across multiple ﬁrmware images and vendors
using the same TZOS image. Third, it allows us to instrument
TA binaries for particular purposes, such as for performance
optimizations.
8.3 Performance Optimizations
TA request processing loops are a potential source of inefﬁ-
ciency for testing. TA request processing passes through a lot
of components - starting from the CA, to the Linux kernel,
the secure monitor, the TZOS, the TA, and back. A shorter
loop would enable TA fuzz testing to run much faster.
We found that the TA request processing loop for Kinibi
could be optimized across all TAs. TAs in Kinibi have an
inﬁnite loop where they wait for a message from the normal
world, process it, and return to the normal world [18]. Wait-
ing for and returning to the normal world passes through a
common library that we were able to instrument to call into
PARTEMU to start and stop a test run, respectively. Thus, we
were able to entirely cut out all non-TA components from the
request processing loop, speeding up AFL’s executions per
second by 5×.
The TA request processing loop for TEEGRIS, OP-TEE,
and QSEE TAs, however, was different, and could not be eas-
ily optimized without symbols in the TA binaries. In contrast
to Kinibi, these TAs expect the OS to callback into a particular
function to handle a request (e.g., the GlobalPlatform TEE
Internal API [21] uses TA_InvokeCommandEntryPoint).
While we could have instrumented the beginning and end
of this function to indicate the start and stop of a run, ﬁnding
the location of this function per-TA from the TA binaries we
had was non-trivial in the absence of symbols.
9 Evaluation
In this section, we: (1) quantify the hardware and software
emulation required to run the TZOSes, showing that it is prac-
USENIX Association
29th USENIX Security Symposium    797
Category
Difﬁculty K
Q
Emulated Boot Information Structure
Constants
Any value
Simple value
Complex values
Total
Low
Low
Low
High
-
13
1
2
2
18
8
3
1
1[note a]
13
Emulated Secure Monitor Calls[note b]
Return simple value
Return constant
Store/retrieve values
Control transfer
Total
Low
Low
Low
High
-
0
1
1
3
5
-
-
-
-
-
T
2
0
14
0
16
3
5
2
2
12
O
3
0
2
0
5
-
-
-
-
-
Table 3: Table categorizing the number and difﬁculty of data
ﬁelds in the emulated boot information structure, and of em-
ulated secure monitor calls, for Kinibi (K), QSEE (Q), TEE-
GRIS (T), and OP-TEE (O).
[note a] To construct this complex value, we were able to use
an open-source implementation [29].
[note b] Since we reused the secure monitor for QSEE and
OP-TEE, we did not need to emulate them.
tical and feasible, (2) demonstrate the utility of emulation
through the use-cases of ﬁnding real-world TrustZone vulner-
abilities using AFL, and (3) evaluate the reproducibility of
results found by emulation on a real device.
9.1 Extent of Emulation Required
In this section, we quantify the extent of software and hard-
ware emulation we required to boot up and run the TZOSes.
Our targets for emulation were QSEE v4.0, Kinibi v400A,
TEEGRIS v3.1, and 32-bit OP-TEE based on v3.1.0. We ob-
tained QSEE, Kinibi, and TEEGRIS binaries from Android
ﬁrmware images, and OP-TEE from a leading IoT manufac-
turer’s ﬁrmware image. Despite these TZOSes being full-
ﬂedged and real-world, by following our approach to select
components to emulate, we found that the software and hard-
ware emulation required was practically feasible. Across all
these TZOSes, to emulate the required software components,
we only needed to specify 52 data ﬁelds, many simple to deter-
mine, and implement 17 SMCs, many again following simple
patterns. Hardware components required emulation of only
235 MMIO registers in 8 patterns (Section 7.1), and more
precise emulation of 3 additional devices. In many cases, we
were even able to re-use open-source components.
9.1.1 Software Emulation
Table 3 quantiﬁes the amount and difﬁculty of software emula-
tion required for the bootloader and secure monitor. First, we
had to emulate the boot information structure passed in by the
bootloader sufﬁciently to boot up and run the TZOSes. Table 3
categorizes the ﬁelds of this structure based on how difﬁcult it
was to determine their value. In summary, we only needed to
Register Type
Constant Read
Increment
Random
Poll
Shadow
Target
Commit
Total
Total (QSEE)
478
1
1
2
54
54
27
617
Unique (QSEE) OP-TEE
219
1
1
1
4
4
2
232
3
0
0
0
0
0
0
3
Table 4: Table showing the total and unique number of types
of registers we had to emulate to boot up and run QSEE and
OP-TEE. We set the MMIO region to be write-read by default
and initialized it to zero values unless otherwise speciﬁed.
specify 52 data ﬁelds, 49 of which were straightforward to de-
termine. First, a majority of these values were constants that
we obtained directly from the corresponding bootloader bina-
ries. Second, some values did not matter - any value worked.
Third, some values were not hardcoded constants but were
straightforward to specify - the extent of RAM and the lo-
cation of the normal-world software to transfer control to.
Finally, the most challenging were complex data structures
that the bootloader needed to setup. For Kinibi, we needed to
setup page tables for a structure describing shared memory
between the TZOS and the secure monitor. For QSEE, we
needed to setup the SMEM data structure, which describes
hardware such as RAM [32]. This task was simpliﬁed by the
open-source version available in the Little Kernel project [29].
Second, for Kinibi and TEEGRIS, we had to emulate 17
calls to the secure monitor. Again, we found most of these
values to be simple values (addresses), constants that we ob-
tained from the binaries, or values from the TZOS that simply
needed to be stored on one call and returned on the other.