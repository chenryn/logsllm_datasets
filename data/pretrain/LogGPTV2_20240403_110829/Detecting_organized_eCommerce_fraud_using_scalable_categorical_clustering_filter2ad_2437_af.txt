1: Let c = v1, . . . , vm denote a set of |c| = m elements v,
2: dmax , the maximum distance for cluster fusion.
3:
4: function AggloClust(c, dmax )
D ← DistanceMatrix(c, c, ’Hamming’)
5:
LM ← LinkageMatrix(D, ’single’)
6:
Cr es ← Cluster(LM, dmax , ’distance’)
7:
return Cr es
8:
9: end function
selects n samples from c (where n is a factor ρs of(cid:112)|c|). Thus, we
compute the distance matrix between the sample and all elements
in c. We use the single linkage method to compute the linkage
matrix since we only know the distance to a single element in each
cluster (because of sampling). Finally, it generates a clustering Cr es
using the maximum number of clusters criterion (’maxclust’) where
cmax is computed as a factor ρmc of the set size |c|. This criterion
is selected with the goal of splitting a large set of elements into
smaller clusters but without providing any guarantee of goodness
for the resulting clustering.
n ← ρs ×(cid:112)|c|
Algorithm 3 Agglomerative clustering with sampling
1: Let c = v1, . . . , vm denote a set of |c| = m elements v,
2: ρs, the multiplying factor for the sample size,
3: ρmc, the dividing factor for maxclust number.
4:
5: function SampleClust(c, ρs , ρmc)
6:
cmax ← m ÷ ρmc
7:
s ← RandomSample(c, n)
8:
D ← DistanceMatrix(s, c, ’Hamming’)
9:
LM ← LinkageMatrix(D, ’single’)
10:
Cr es ← Cluster(LM, cmax , ’maxclust’)
11:
return Cr es
12:
13: end function
B DATASETS COMPOSITION
Small datasets with artificial distribution: We selected TrainF-
15K and TestF-15K from Zalando orders passed in France over
2017. In each dataset, we simulate an artificial distribution where
frauds are over-sampled compared to a real-world distribution (2
legitimate / 1 fraud). TrainF-15K consists of 10 disjoint subsets
TrainF-15K-i, each composed of 10,000 legitimate and 5,000 frauds.
The 5,000 frauds are randomly selected from a continuous period
of 1-1.5 month. The 10,000 legitimate orders are randomly selected
from a period of 1-2 days. For each subset i, the period from which
legitimate orders are selected is included into the period from which
frauds are selected. TestF-15K consists of 10 disjoint subsets TestF-
15K-i selected the same way as for TrainF-15K (10,000 legitimate
and 5,000 frauds) TestF-15K and TrainF-15K are disjoint.
Large datasets with artificial distribution. We select TrainG-
30K and TrainG-100K from orders passed in Germany over 2017.
Each subset of TrainG-30K is composed of 25,000 legitimate and
13
5,000 frauds. The 5,000 frauds are randomly selected from a contin-
uous period of 1 month. The 25,000 legitimate orders are randomly
selected from a period of 1 day contained in the month from which
frauds are selected. Each subset of TrainG-100K is composed of
95,000 legitimate and 5,000 frauds. The 5,000 frauds are randomly
selected from a continuous period of 1 month. The 95,000 legitimate
orders are randomly selected from a period of 1 week contained in
the month from which frauds are selected.
C HYPERPARAMETER SELECTION
C.1 Linkage method selection
We want to select a linkage method that minimizes the impurity
I and maximizes the clustered fraud rate (CFR). We cluster the 10
TrainF-15K-i datasets using single linkage, complete linkage and
Ward linkage. Figure 6 shows the evolution of impurity I according
to CFR while varying the maximum distance for cluster fusion
dmax . Values are averaged over 10 clustering results. We see that
for CFR  0.6 Ward outperforms other linkage methods, while
single becomes the worst method with high increase in impurity.
Figure 6: Impurity vs. CFR for single / complete / Ward link-
age methods. Each linkage method provides the same Impu-
rity/CFR tradeoff when the impurity is low: 0.01-0.03.
Our primary goal is to keep the impurity as low as possible. All
methods are comparable at providing a high CFR while keeping the
impurity low (0.01 - 0.03). Single and complete linkage are computed
using a single distance between two points from two clusters: the
closest and the furthest away ones respectively. Thus, they are faster
to compute than Ward. They are also better suited for clustering
with sampling, since the distance between any two points is not
available using sampling. We can see from Fig. 6 (zoom) that single
linkage provides higher CFR than complete linkage for the same
impurity value. Thus, we select single linkage as our base linkage
technique.
C.2 Weighting strategy selection
Figure 7 shows the evolution of impurity I according to CFR while
varying the maximum distance for cluster fusion dmax . Values are
averaged over 10 clustering results. We see that for CFR  0.55
our label driven weighting becomes better than other strategies
providing up to a 2 percentage points higher CFR than other strate-
gies while keeping impurity low (0.025). Cardinality driven features
provide the best trade-off between CFR and impurity (for I > 0.10).
This is an interesting result but it is not useful since our goal is to
minimize impurity. We see that label driven weighting gives the
best trade-off between CFR and impurity.
C.3 SampleClust hyperparameters selection
Table 4 presents the impurity (I), CFR and computation time (t)
for each combination of SampleClust hyperparameters ρmc and
ρs tested during the grid search. These results are limited to the
grid search we performed on TrainG-30K and they depict how our
performance metrics vary according to ρmc and ρs values. We also
computed a performance score to choose the optimal hyperparame-
ter combination. It is the sum of normalized performance metrics ˆI,
ˆCFR and ˆt, which are respectively defined in Eq. (8), (9) and (10)3.
A low performance score depicts a combination of ρmc and ρs that
maximizes our clustering objectives: low impurity, high CFR and
low computation time.
ˆI(ρs =x, ρmc =y) =
I(ρs =x, ρmc =y) − min(I)
max(I) − min(I)
(8)
(9)
ˆCFR(ρs =x, ρmc =y) =
max(CFR) − CFR(ρs =x, ρmc =y)
max(CFR) − min(CFR)
t(ρs =x, ρmc =y) − min(t)
ˆt(ρs =x, ρmc =y) =
max(t) − min(t)
(10)
We see that hyperparamater choice heavily impacts the compu-
tation time while impurity and CFR remain almost constant. Too
low ρmc value (e.g., 1.01) or a high ρs value significantly increase
the computation time. This is expected since ρs defines the sample
size used in clustering with sampling. A high ρs value means a
large sample.
3We discarded time results obtained using rhomc from the normalization process
in Eq. (10). These are too high and represent outliers. max(t) and min(t) only take
ρmc = {1.5, 2, 3, 4, 6, 10} into account.
Table 4: Results of grid search for SampleClust hyperpa-
rameters: ρs = {0.25, 0.5, 1, 2} and ρmc = {1.01, 1.5, 2, 3, 4, 6, 10}.
Average impurity, CFR and computation time computed
over 10 runs of RecAgglo on TrainG-30K (δa = 1, 000). A low
performance score means a combination of hyperparame-
ters that maximizes our clustering objectives: low impurity,
high CFR and low computation time.
ρmc
1.01
1.5
2
3
4
6
10
ρs
0.25
0.5
1
2
0.25
0.5
1
2
0.25
0.5
1
2
0.25
0.5
1
2
0.25
0.5
1
2
0.25
0.5
1
2
0.25
0.5
1
2
I (%) CFR (%)
28.59
3.61
29.54
3.65
30.13
3.75
30.53
3.68
3.43
27.72
28.36
3.45
28.49
3.38
28.74
3.49
3.38
27.45
28.29
3.39
28.48
3.43
28.70
3.53
27.42
3.43
3.42
28.31
28.82
3.44
28.75
3.56
27.67
3.46
3.51
28.65
28.71
3.40
28.76
3.48
27.56
3.49
28.48
3.44
3.43
28.76
28.78
3.48
27.89
3.41
28.43
3.44
3.47
28.81
28.89
3.53
time (s)
2,070
4,722
9,049
16,657
461
657
1,010
1,365
397
620
920
1,300
410
629
913
1,262
417
632
884
1,265
456
682
936
1,350
540
720
995
1,456
performance score
3.06
5.10
9.33
15.90
1.17
1.00
0.89
1.61
0.98
0.72
1.09
1.82
1.30
0.86
0.91
1.91
1.29
1.12
0.72
1.49
1.57
0.88
0.88
1.52
0.99
0.94
1.12
1.83
Figure 7: Impurity vs. CFR for default / cardinality / label
driven attribute weighting. All methods have comparable
performance. Label driven weighting provides marginally
higher CFR for the same impurity.
14