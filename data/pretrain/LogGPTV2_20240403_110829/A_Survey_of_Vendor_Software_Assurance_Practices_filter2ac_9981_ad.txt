inquiries  from  customers  about  software  assurance;  they 
assume that R knows how to ensure that their products are 
secure. 
IV.  QUESTIONS AND FUTURE WORK 
This survey in some respects opens as many questions as 
it answers.  How will vendors respond to knowledge of the 
“industry norms”?  Will those doing less than the average be 
encouraged to invest more and catch up?  Will those at the 
midpoint be encouraged to continue their investments?  Will 
those investing  more be likely to reduce their investments, 
since  there’s  little  motivation  to  be  “better”  than  average 
when  there’s  little  customer  demand?    Vendors  can  use 
“industry norms” as a defense against customer requests for 
better  assurance  –  will  this  study,  which  was  started  to 
justify increasing investment in software assurance thereby 
(paradoxically)  reduce  the  motivation  to  do  more  than  the 
average?  In short, will this survey motivate vendors to raise 
the bar, or simply to establish the status quo? 
As  was  noted  in  section  II.E,  the  study  deliberately 
focused  on 
the  segment  of  shrink-wrapped  software 
vendors.    Future  extensions  to  this  survey  could  include 
surveys of: 
of 
the 
•  Embedded  systems,  such  as  medical  instruments, 
automobile systems, etc.  As these systems become 
are heavily software-based, they are increasingly at 
risk.    For  example  [7]  demonstrated  a  potential 
vulnerability  in  heart  pacemakers.    Hence,  an 
examination 
assurance 
methodologies used by medical instruments vendors 
would be helpful to understand the level of analysis 
being performed. 
Financial institutions, which are increasingly under 
attack,  and  are  subject  to  regulations  such  as  PCI 
[13] and OCC [11]. 
•  Online  merchants  (e.g.,  Amazon),  which  are 
generally subject to PCI [13] requirements, and are 
regularly attacked. 
software 
• 
• 
• 
Software  as  a  Service  (SaaS)  vendors,  which  may 
be  subject  to  requirements  including  PCI  [13]  or 
HIPAA, depending on the type of information they 
manage. 
Systems  integrators,  who  have  the  unenviable  task 
of  tying  together  products  from  different  vendors 
into complete systems. 
V.  ANALYSIS & CONCLUSION 
Table  1  and  Table  2  summarize  our  key  findings.    In 
Table  2,  the  terms  “primary”  and  “secondary”  mean  that 
these were the driving forces, while “yes” means that it was 
a consideration to the vendor but not a driving force.   
Table 1.  Motivations for Investment. 
Vendor
M
W
F
H
B
S
K
R
Customer 
expectations
Primary
Primary
Primary
Yes
Secondary
Yes
Primary
Primary
Fear of 
publicity 
Yes 
Minor 
Yes 
Primary 
Minor 
Primary 
Second 
Minor 
Explicit 
requests
Minor
Govt  only
Occasional
Govt only
Primary
Minor
Minor
Govt only
From this limited survey, we conclude that: 
• 
• 
Software vendors are aware of the risks of insecure 
software, and are generally motivated by fear of bad 
publicity to minimize the security vulnerabilities in 
their products. 
Few  non-government  customers  explicitly  ask  for 
software assurance, but vendors believe that it’s an 
unspoken expectation. 
•  Most  organizations  have  centralized 
security 
organizations that hold the expertise, with outreach 
into  the  product  development  teams  to  provide 
software assurance.  The head of software assurance 
typically  reports  directly  to  the  head  of  product 
development,  and  has  a  reasonable  degree  of 
influence  that  allows  him/her  to  prevent  product 
release in case of serious security flaws. 
•  The techniques used to gain software assurance vary 
among vendors, but nearly all agree that developer 
training is one of the most valuable uses of limited 
resources.    While  everyone  agrees  that  penetration 
testing has its limitations, it is still helpful as a way 
to know how good or bad a product is. 
Source code analysis is still early in the acceptance 
phase, both because tools are expensive and difficult 
to  use  effectively.    Dynamic  testing,  including 
fuzzing, seems to be more cost-effective. 
•  Common  Criteria  was  mentioned  by  nearly  all 
vendors,  and  all  but  one  felt  it  was  a  paperwork 
exercise  that  had  almost  no  impact  on  the  security 
of their products. 
•  Most  organizations  started  focusing  on  software 
assurance several years ago (perhaps influenced by 
• 
535
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:49 UTC from IEEE Xplore.  Restrictions apply. 
unable  to  thank  them  by  name.    He  also  thanks  the 
anonymous reviewers, who provided useful suggestions. 
The  work  described  in  this  paper  was  performed  when 
Software  AG 
author  was 
employed 
by 
the 
(www.softwareag.com). 
REFERENCES 
[1]  19  Deadly  Sins  of  Software  Security,  Michael  Howard,  David 
LeBlanc, and John Viega, McGraw-Hill, 2005. 
[2]  Building Security In Maturity Model, http://www.bsi-mm.com/ 
[3]  Common  Criteria  for  Information  Technology  Security  Evaluation, 
[4] 
[5] 
ISO/IEC 15408. 
J.  Epstein,  S.  Matsumoto,  and  G.  McGraw,  “Software  Security  and 
SOA, Danger Will Robinson!”, IEEE Security & Privacy magazine, 
February 2006. 
J.  Epstein,  “What  Measures  do  Vendors  Use  for  Software 
Assurance?”,  in  Making  the  Business  Case  for  Software  Assurance 
Workshop,  Carnegie  Mellon  University  Software  Engineering 
Institute, September 2008. 
[6]  Trustworthy  Computing  (memo),  Bill  Gates,  Microsoft,  January  15 
http://www.microsoft.com/mscorp/execmail/2002/07-
2002, 
18twc.mspx  
[7]  D.  Halperin  et  al,  “Pacemakers  and 
Implantable  Cardiac 
Defibrillators: Software Radio Attacks and Zero-Power Defenses”, in 
Proceedings  of  the  IEEE  Symposium  on  Security  and  Privacy, 
Oakland CA, May 2008. 
[8]  The  Security  Development  Lifecycle,  Michael  Howard  and  Steve 
Lipner, Microsoft Press, 2006. 
[9]  G.  Jelen  and  J.  Williams,  “A  Practical  Approach  to  Measuring 
Assurance”,  in  Proceedings  of  the  14th  Annual  Computer  Security 
Appications Conference, Phoenix AZ, December 1998. 
[10]  Software  Security:  Building  Security  In,  Gary  McGraw,  Addison-
Wesley, 2006. 
[11]  Application 
  Security,  OCC  Bulletin  2008-16,  May  2008, 
http://www.occ.treas.gov/ftp/bulletin/2008-16.html  
[12]  Software Assurance Maturity Model, www.opensamm.org 
[13]  Payment  Card  Industry  Data  Security  Standards,  version  1.1, 
2006, 
September 
https://www.pcisecuritystandards.org/security_standards/pci_dss.sht
ml  
[14]  Software Assurance: An Overview of Current Industry Best Practices, 
February 2008, www.safecode.org  
[15]  Fundamental Practices for Secure Software Development: A Guide to 
the  Most  Effective  Secure  Development  Practices  in  Use  Today, 
October 2008, www.safecode.org  
[16]  The  Complete  Searchable  2007  Software  500  Database,  Software 
Magazine, 
http://www.softwaremag.com/S_FocusAreas.cfm?Doc=The500 
[17]  Building Secure Software: How to Avoid Security Problems the Right 
Way, John Viega and Gary McGraw, Addison-Wesley, 2001. 
the  famous  “Trustworthy  Computing”  memo  [6]), 
and took several years to see results. 
Security engineers frequently ask why vendors sell software 
that has significant security problems.  This survey is a step 
towards  answering  that  question  –  customers  rarely  ask 
about  software  assurance,  but  despite  that,  vendors  are 
making significant strides in improving the security of their 
software. 
VI.  RELATED WORK 
The  idea  of  surveying  companies  to  find  out  about 
practices  was  also  used  by  the  developers  of  the  Build 
Security  In  Maturity  Model  [2].    BSIMM  was  built  by 
synthesizing  nine  companies’  processes 
to  determine 
“typical  practices”  (sometimes  given  the  misnomer  “best 
practices”). 
  This  study  and  BSIMM  used  similar 
methodologies  of  having  a  set  of  common  questions  that 
were asked of all participants, but also allowing open-ended 
answers.    While  this  study  was  explicitly  limited  to  the 
software  development  industry,  BSIMM  also  included 
companies  from  the  financial  and  other  sectors.    Thus, 
BSIMM relies on a smaller set of company practices in each 
of  several  sectors,  while  this  paper  describes  a  slightly 
deeper analysis of the ISV market.  BSIMM has released the 
names  of  some  of  the  companies  that  participated  in  their 
study; this study included some of the same companies. 
The Open Security Maturity Model [12] was developed 
by assessing the operation of real organizations and building 
a  hierarchy  of  recommendations.    However,  OpenSAMM 
did  not  use 
rather  makes 
recommendations  based  on  the  author’s  experiences  as  a 
security consultant. 
interviews  but 
As for timelines, this study was performed prior to either 
the BSIMM or OpenSAMM  efforts.  An earlier version of 
this study was published as [5]. 
The  SAFECode  group,  composed  of  major  software 
companies, defines a set of recommendations based on their 
survey  of  member  companies’  software  development 
practices  [14,  15].    Their  recommendations  in  [14]  are 
similar  in  breadth  to  this  study;  [15]  is  more  focused  on 
detailed coding recommendations when using C and C++. 
formal 
ACKNOWLEDGMENT 
The  author  thanks  his  contacts  in  each  of  the  vendors.  
As  each  of  the  vendors  provided  information  about  their 
processes  on  a  non-attribution  basis,  he  regrets  that  he  is 
536
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:49 UTC from IEEE Xplore.  Restrictions apply. 
Vendor 
Training? 
Design reviews? 
Table 2. Assurance Methods Used. 
Pentesting? 
Source analysis? 
Informal 
Not a focus 
Internal & external 
Internal, external & customers 
Manual 
Proprietary tools 
M 
W 
F 
H 
B 
S 
K 
R 
Informal 
Formal & 
refresher 
Informal & 
seminars 
Formal 
Formal, 
extensive 
Seminars 
Formal, 
mandatory 
Minimal 
Performed by 
developers 
Informal 
Workshop with 
experts 
Workshop with 
experts 
Performed by 
security expert 
Minimal 
Dynamic 
 testing? 
Yes 
Yes 
Yes 
Yes 
Yes 
Extensive internal, some external  Manual & proprietary tools 
Internal, external & customers 
Internal but discouraged 
Company-wide automated 
Company-wide automated 
Field only 
Partially automated, simple tools 
Minimal 
Minimal but varies by product 
Varies by product; some automated 
Yes 
Minimal 
Company-wide manual and 
automated 
Minimal 
537
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:49 UTC from IEEE Xplore.  Restrictions apply.