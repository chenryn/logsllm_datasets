Gaussian mixture models can be ﬁtted to data using the
well known Expectation-Maximization (EM) algorithm, and
in particular the version proposed in [23] automatically de-
termines the proper number of clusters using an information-
theoretic criterion. Once the Gaussian mixture model is de-
termined, each hop count contrast vector will be associated
most signiﬁcantly with a given Gaussian component. This
then provides a clustering of the sources, where the number
clusters is equal to the number of Gaussian components in-
ferred by the EM algorithm [23]. Moreover, we will see later
(in Section 6.1.2) that the Gaussian mixture model and EM
Sixy# of measurement nodes = K # of measurement nodes = J Figure 3: 2-D histogram of hop count contrast vec-
tors with clusters highlighted in ellipses.
Figure 4: Comparison of Gaussian mixture clusters
to random clusters. Simulated topology, N = 1000,
M = 8.
algorithm provide a powerful tool for imputing missing hop
count data.
Subnet Cluster Analysis
4.2.1
To assess the topological relevance of the clusters deter-
mined by the Gaussian mixture model, we consider the prob-
lem of shared infrastructure estimation (to be discussed in
detail in Section 5.2). The topology relating the sources in a
given cluster to the measurement nodes can be estimated by
selecting one source from the cluster and performing tracer-
oute measurements from this source to each measurement
node.
If the all the sources in the cluster share the same
paths, then this estimate is perfect. We do not, however,
expect this to be the case, even for sources located in the
same subnet, for the reasons state above. Nonetheless, these
routes should provide good predictions for the routes, if the
clusters are topologically meaningful. The accuracy of the
predictions is measured by calculating the error in predicted
shared hops in the paths between pairs of sources and a
measurement node (we deﬁne Root Mean Squared Error in
Section 5). The error rates in the predictions of shared path
lengths are shown in Figures 4 and 5, comparing the per-
formance of the predictions based on the Gaussian mixture
clusters with that of predictions based on randomly clus-
tered sets of sources. The clusters determined by the Gaus-
sian mixture model result in signiﬁcantly better predictions,
indicative of the fact that they are indeed grouping sources
that have share similar paths to the the measurement nodes.
5. TOPOLOGY DISCOVERY
The source clusters identiﬁed by our algorithm are
topologically meaningful. However, they do not reveal the
topological relationship between the shared paths from the
clusters to the measurement nodes. In this section, we will
show that by coupling the passive hop count data with
a small number of active measurements, we can identify
the topological relationships between clusters. The active
measurements will take the form of traceroutes from the
measurement nodes to a small subset of target hosts which
eﬀectively act as representatives for the clusters. This is
in contrast to the e.g., the Skitter methodology [5], where
active measurements are taken from all measurement nodes
to a large set of target hosts.
Figure 5: Comparison of Gaussian mixture clusters
to random clusters. Skitter topology, N = 700, M =
8.
5.1 Cluster-Level Topology Discovery
Given a set of IP source clusters, discovering shared topol-
ogy between clusters becomes a straightforward task. For
every cluster, randomly choose an IP source in the cluster
and perform active traceroute measurements between that
IP source (consider as a representative for its cluster) and
the set of measurement nodes. If the clusters are topologi-
cally signiﬁcant, the topology will have been discovered.
There are at least two potential problems with this
straightforward approach to topology discovery. First, the
source clusters may not be completely correct from a topo-
logical perspective, due the possible existence of multiple
egress points and missing hop counts (the missing data issue
will be thoroughly addressed in Section 6). Second, from an
Internet-wide perspective, the number of clusters may still
be prohibitively large for exhaustive (cluster-wise) tracer-
oute probing.
5.2 Shared Infrastructure Estimation
Given the drawbacks to the deterministic cluster-to-
cluster topology discovery technique, we address the prob-
lem of estimating shared infrastructure between pairs of IP
sources.
5.2.1 The Canonical Subproblem
Consider a triple {Si, Sj, Mk}, where two sources have
a path to a single measurement node as seen in Figure 6.
ﬁrst develop some metric for the amount of sharedness be-
tween the two vectors. The similarity of the hop count con-
trast vectors indicates the likelihood that the two sources
are within the same subnet. The greater the similarity the
stronger the evidence for shared infrastructure in the paths
to the measurement nodes. To assess the potential for shared
infrastructure to a given measurement node we consider the
diﬀerence in hop count distances to that node and calculate
the number of other measurement nodes that result in the
same hop count diﬀerence. Formally, we deﬁne,
Ui,j,k = |Ti,j,k|
(1)
Where Ti,j,k = {k(cid:48) : |hi,k(cid:48) − hj,k(cid:48)| − |hi,k − hj,k|  0)
As the value of Ui,j,k becomes closer to the number of
measurement nodes, there is a higher likelihood of a longer
shared path to each measurement node.
Given the training set Ik, where each element is the index
of an IP source for which we have exact knowledge (from
active measurements) of the labeled path to measurement
node Mk, we can then construct sets consisting of pairs of
training nodes that share the same oﬀset value for the par-
ticular measurement node k.
k = {[x, y] : x, y ∈ Ik, Ux,y,k = c}
Ic
(2)
Considering two paths from Si to Mk and Sj to Mk, we
can state that the shortest shared path would be of length
zero as shown in Figure 8-(left), and the longest shared path
would be of length = min (hi,k, hj,k) as shown in Figure 8-
(right).
Figure 8: Potential topologies for two sources to
one measurement node (each black dot represents
a router hop) (Left) - Shortest possible shared path,
(Right) - Longest possible shared path
Figure 6: The canonical subproblem: two IP sources
connecting to a single measurement node
There are three possible potential topologies connecting this
triple (two sources to one measurement node), with a shared-
ness spectrum ranging from absolutely no sharedness with
two separate paths from each source to the measurement
node, to complete sharedness with both sources on a single
path to the measurement node, with the intermediate stage
of some length of shared path between the two sources. It
is easy to verify that if the number of shared hops is known
for all such canonical subproblems, then the logical topol-
ogy relating the sources to the measurement nodes can be
determined. This follows by observing that the set of paths
from the sources to a given measurement node form a tree.
Therefore, this section will focus on estimating P (i, j, k),
the length of the shared path between two IP sources i, j to
a single measurement node k using the passive data and a
limited number of traceroute measurements.
5.2.2 Cluster-Level Shared Path Length Estimation
Toward the goal of cluster-level topology discovery, one
can discover shared path lengths by using active measure-
ments from a single representative of each cluster as seen in
Figure 7. We assume that all sources in Ci will share the
same path of length x to measurement node Mk with all
sources in Cj. Therefore, for a single active measurement of
each cluster, we have an estimate of the shared path lengths
between IP sources contained in all other clusters in the
topology.
Given this range of shared path lengths, we can estimate
the shared path length for any two sources i, j to any mea-
surement node k by attenuating the longest possible shared
path length (= min (hi,k, hj,k)) by some value less than one,
represented by α.(cid:98)P (i, j, k) = α · min (hi,k, hj,k)
(3)
Figure 7: Example of cluster-level path estimation.
5.2.3 Predictive Shared Path Length Estimation
For two hop count distance vectors,
it is necessary to
The problem becomes estimating the value of α. Given
some collection of training data where active measurements
give observed values for the shared path lengths, we can esti-
mate α as a function of the passive measurements of Si and
Sk. We hypothesize that the more hop count distance values
that are a constant integer apart, the more sharedness that
will be observed along the path. This results in learning a
function whose domain is the number of hop count elements
where the two vector hi and hj are a constant integer apart.
SiSjMkCiCjMkxSiSjSiSjMkMkWe can then learn the attenuation function by taking the
average of the observed path lengths for each integer oﬀset
value. Therefore, for each measurement node k and unifor-
mity metric value c:
(cid:88)
(cid:88)
(cid:179)
P (Ic
α (c, k) =
1
|Ic
k|2
i∈Ic
k
j∈Ic
k
min
k (j) , k)
k (i) , Ic
hIc
k(i),k, hIc
k(j),k
(cid:180)
(4)
Finally, we combine the learning attenuation function to
create an estimator of the shared path length for each pair
of IP sources,
(cid:98)P (i, j, k) = α (Ui,j,k, k) min (hi,k, hj,k)
(5)
5.3 Shared Path Estimation Analysis
In Table 3, we show the results for three diﬀerent methods
for shared path length estimation. The methods include :
1. Unique Contrast cluster-level Estimation - Cluster-
level estimation is performed on clusters where each
represents a unique hop count contrast vector in the
passive dataset.
2. Cluster-level estimation using Gaussian mixture model
- Cluster-level estimation is performed on clusters
found using the Mixture Gaussian algorithm.
3. Predictive Function Estimation - Using Equation 5,
the estimated shared path lengths are found.
(cid:115)(cid:88)
The results are based on a 1000 node synthetic topology,
which was generated by Orbis [24]. We randomly select
800 leaf nodes (sources) and 8 measurement nodes in the
graph, and assume “complete data” i.e., that probes from
all sources are received at all measurement nodes. The error
metric that we use to assess the estimation accuracy the
Root Mean Squared Error (RMSE) is deﬁned as:
RM SE(ˆh) =
|hi,j − ˆhi,j|2
(6)
i,j
Where an RMSE of x indicates that the estimated shared
number of hops is on average x hops away from the true
number of hops extracted from the graph.
The results show that the estimation from the unique con-
trast clustering performed the best, but required a larger
number of active measurements. Using the information-
theoretic approach from [23], 7 clusters were found, (in com-
parison to 47 active measurement needed for each measure-
ment node if performing unique contrast clustering). Using
the Gaussian mixture clustering, the predictive method out-
performs the cluster-level method. Simulations with diﬀer-
ent synthetic topologies provided similar results.
Estimation Type
Cluster-level
Predictive Function
Unique Contrast
# Clusters RMSE
1.28
1.00
0.70
7
7
47
Table 3: Shared path estimation results for a 1000
node synthetic topology assuming that probes from
800 randomly selected source nodes were observed
in 8 randomly selected monitors.
In Figure 9, we assess how increasing the number of clus-
ters aﬀects the performance of the Gaussian mixture EM
cluster-level algorithm from a RMSE perspective. For this
simulated synthetic topology (with N = 800, M = 24, in
contrast to M = 8 results in Figure 4), the addition of
more clusters (and hence more active measurements needed)
causes a signiﬁcant decrease in the error rate of the path
length estimation.
Figure 9: The eﬀect of increasing the number of clus-
ters on the shared path estimation performance on
the simulated topology using the cluster-level shared
path estimation method.
In Table 4, we show how the same three methods for
shared path length estimation considered above perform on
the Skitter topology described in Section 3. A random set
of 700 leaf nodes were selected as sources and 8 leaf nodes
were randomly selected measurement nodes.
Similar to the simulated topology, the estimation from
the unique contrast clustering performed the best from an
RMSE perspective but also required the largest number of
active measurements. Using the information-theoretic ap-
proach, 9 clusters were found, requiring 18 active measure-
ments of the topology for each measurement node. Again,
the predictive function outperforms the cluster-level method
when considering the smaller number of clusters found by
the Gaussian mixture EM algorithm.
Estimation Type
Cluster-level
Predictive Function
Unique Contrast
# Clusters RMSE
1.25
1.23
0.66
9
9
434
Table 4: Shared path estimation results for the Skit-
ter topology assuming that probes from 700 ran-
domly selected source nodes were observed in 8 ran-
domly selected monitors.
In Figure 10, we assess how increasing the number of
clusters aﬀects the performance of the Gaussian mixture
EM cluster-level algorithm from a RMSE perspective. For
the Skitter topology (with N = 700, M = 24 (in contrast
to the results for M = 8 in Figure 5), the addition of more
clusters causes a decrease in the error rate of path length
estimation, but not as signiﬁcant a decrease as seen in the
simulated topology example.
, for i iterations and K Gaussian modes.
(cid:161)
iKN M 4(cid:162)
Imputation Performance Analysis
rithm was purposed to both learn the parameters (mean,
variance, prior probabilities, responsibilities) for a group of
Gaussian distributions given a set of incomplete data, and
then use those estimated Gaussian mixtures to impute the
missing data values. The only necessary parameter input to
this algorithm is the number of Gaussian mixtures to use.
From [23], an information-theoretic technique was purposed
to determine, given a set of complete data, how many Gaus-
sian mixtures to use to model the data. This method is
a hybrid two-step iterative approach, where the ﬁrst step
consists of estimating the number of Gaussians from the
imputed data using the method from [23], and the second
step then estimates the new imputed data values using the
method from [14]. For N sources and M measurement nodes
in the network, this method has computation complexity
O
6.1.3
Using the honeynet dataset described in Section 3, we
can synthetically generate missing data examples by consid-
ering the sources that are seen in M measurement nodes and
knocking out (eliminating) a random subset of the hop count
measurements for each hop count vector. Where X observed