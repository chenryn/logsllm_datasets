e
p
h
c
t
i
w
s
r
e
p
d
a
e
h
r
e
v
o
l
o
r
t
n
o
C
1000
100
10
1
0.1
0.01
Eth (Num. of flooded packets)
SEA_CA (# of control messages)
SEA_NOCA (# of control messages)
10K
20K
30K
Number of hosts
40K
50K
(c)
Table size (right axis)
Location resolution prob. (left)
Ctrl overhead (right)
60
Time-out values for ingress caching (sec)
180
120
240
(a)
Figure 4: (a) Effect of cache timeout in AP-large with 50K hosts (b) Table size increase in DC (b) Control overhead in AP-large. These ﬁgures contain
error bars showing conﬁdence intervals for each data point. A sufﬁcient number of simulation runs reduced these intervals.
topologies. Campus is the campus network of a large (roughly
40,000 students) university in the United States, containing 517
routers and switches. AP-small (AS 3967) is a small access
provider network consisting of 87 routers, and AP-large (AS 1239)
is a larger network with 315 routers [34]. Because SEATTLE
switches are intended to replace both IP routers and Ethernet
bridges, the routers in these topologies are considered as SEATTLE
switches in our evaluation. To investigate a wider range of environ-
ments, we also constructed a model topology called DC, which rep-
resents a typical data center network composed of four full-meshed
core routers each of which is connected to a mesh of twenty one ag-
gregation switches. This roughly characterizes a commonly-used
topology in data centers [1].
Our topology traces were anonymized, and hence lack infor-
mation about how many hosts are connected to each switch. To
deal with this, we leveraged CAIDA Skitter traces [35] to roughly
characterize this number for networks reachable from the Internet.
However, since the CAIDA skitter traces form a sample represen-
tative of the wide-area, it is not clear whether they apply to the
smaller-scale networks we model. Hence for DC and Campus, we
assume that hosts are evenly distributed across leaf-level switches.
Given a ﬁxed topology, the performance of SEATTLE and Eth-
ernet bridging can vary depending on trafﬁc patterns. To quantify
this variation we repeated each simulation run 25 times, and plot
the average of these runs with 99% conﬁdence intervals. For each
run we vary a random seed, causing the number of hosts per switch,
and the mapping between hosts and switches to change. Addition-
ally for the cases of Ethernet bridging, we varied spanning trees by
randomly selecting one of the core switches as a root bridge. Our
simulations assume that all switches are part of the same broadcast
domain. However, since our trafﬁc traces are captured in each of
the 22 different subnets (i.e., broadcast domains), the trafﬁc pat-
terns among the hosts preserve the broadcast domain boundaries.
Thus, our simulation network is equivalent to a VLAN-based net-
work where a VLAN corresponds to an IP subnet, and all non-leaf
Ethernet bridges are trunked with all VLANs to enhance mobility.
6.2 Control-plane Scalability
Sensitivity to cache eviction timeout: SEATTLE caches host in-
formation to route packets via shortest paths and to eliminate re-
dundant resolutions.
If a switch removes a host-information en-
try before a locally attached host does (from its ARP cache), the
switch will need to perform a location lookup to forward data pack-
ets sent by the host. To eliminate the need to queue data packets
at the ingress switch, those packets are forwarded through a loca-
tion resolver, leading to a longer path. To evaluate this effect, we
simulated a forwarding table management policy for switches that
evicts unused entries after a timeout. Figure 4a shows performance
of this strategy across different timeout values in the AP-large net-
work. First, the fraction of packets that require data-driven location
lookups (i.e., lookups not piggy-backed on ARPs) is very low and
decreases quickly with larger timeout. Even for a very small time-
out value of 60 seconds, over 99.98% of packets are forwarded
without a separate lookup. We also conﬁrmed that the number of
data packets forwarded via location resolvers drops to zero when
using timeout values larger than 600 seconds (i.e., roughly equal
to the ARP cache timeout at end hosts). Also control overhead
to maintain the directory decreases quickly, whereas the amount
of state at each switch increases moderately with larger timeout.
Hence, in a network with properly conﬁgured hosts and reasonably
small (e.g., less than 2% of the total number of hosts in this topol-
ogy) forwarding tables, SEATTLE always offers shortest paths.
Forwarding table size: Figure 4b shows the amount of state per
switch in the DC topology. To quantify the cost of ingress caching,
we show SEATTLE’s table size with and without caching (SEA_CA
and SEA_NOCA respectively). Ethernet requires more state than
SEATTLE without caching, because Ethernet stores active hosts’
information entries at almost every bridge.
In a network with s
switches and h hosts, each Ethernet bridge must be provisioned
to store an entry for each destination, resulting in O(sh) state re-
quirements across the network. SEATTLE requires only O(h) state
since only the access and resolver switches need to store location
information for each host. In this particular topology, SEATTLE
reduces forwarding-table size by roughly a factor of 22. Although
not shown here due to space constraints, we ﬁnd that these gains
increase to a factor of 64 in AP-large because there are a larger
number of switches in that topology. While the use of caching
drastically reduces the number of redundant location resolutions,
we can see that it increases SEATTLE’s forwarding-table size by
roughly a factor of 1.5. However, even with this penalty, SEAT-
TLE reduces table size compared with Ethernet by roughly a factor
of 16. This value increases to a factor of 41 in AP-large.
Control overhead: Figure 4c shows the amount of control over-
head generated by SEATTLE and Ethernet. We computed this
value by dividing the total number of control messages over all
links in the topology by the number of switches, then dividing by
the duration of the trace. SEATTLE signiﬁcantly reduces control
overhead as compared to Ethernet. This happens because Ethernet
generates network-wide ﬂoods for a signiﬁcant number of pack-
ets, while SEATTLE leverages unicast to disseminate host loca-
tion. Here we again observe that use of caching degrades perfor-
mance slightly. Speciﬁcally, the use of caching (SEA_CA) increases
control overhead roughly from 0.1 to 1 packet per second as com-
pared to SEA_NOCA in a network containing 30K hosts. However,
SEA_CA’s overhead still remains a factor of roughly 1000 less than
in Ethernet. In general, we found that the difference in control over-
head increased roughly with the number of links in the network.
h
c
t
e
r
t
s
y
c
n
e
t
a
L
14
12
10
8
6
4
2
0
0
ROFL
SEATTLE
1
10
100
1000
10000
Maximum cache size per switch (entries) (log)
120
100
80
60
40
20
0
s
e
g
n
a
h
c
h
t
a
p
f
o
.
m
u
N
100
ROFL (AP_large)
ROFL (DC)
SEATTLE (AP_large)
SEATTLE (DC)
200
500
1000
2000
5000
Num. of host join/leave events during a flow (log)
50%
40%
30%
20%
10%
e
t
a
r
s
s
o
l
t
e
k
c
a
P
10000
0
0.01
Eth (ctrl ovhd)
SEA_CA (ctrl ovhd)
SEA_NOCA (ctrl ovhd)
Eth (loss)
SEA_NOCA (loss)
SEA_CA (loss)
0.02
0.1
0.2
Switch failure rate (fails/min) (log)
)
g
o
l
(
c
e
s
r
e
p
h
c
t
i
w
s
r
e
p
d
a
e
h
r
e
v
o
l
o
r
t
n
o
C
10K
1K
100
10
1
0.1
1
(a)
(b)
(c)
Figure 5: (a) Stretch across different cache sizes in AP-large with 10K hosts (b) Path stability (c) Effect of switch failures in DC.
Comparison with id-based routing approaches: We implemented
the ROFL, UIP, and VRR protocols in our simulator. To ensure a
fair comparison, we used a link-state protocol to construct vset-
paths [15] along shortest paths in UIP and VRR, and created a
UIP/VRR node at a switch for each end host the switch is at-
tached to. Performance of UIP and VRR was quite similar to
performance of ROFL with an unbounded cache size. Figure 5a
shows the average relative latency penalty, or stretch, of SEATTLE
and ROFL [13] in the AP-large topology. We measured stretch
by dividing the time the packet was in transit by the delay along
the shortest path through the topology. Overall, SEATTLE incurs
smaller stretch than ROFL. With a cache size of 1000, SEATTLE
offers a stretch of roughly 1.07, as opposed to ROFL’s 4.9. This
happens because i) when a cache miss occurs, SEATTLE resolves
location via a single-hop rather than a multi-hop lookup, and ii)
SEATTLE’s caching is driven by trafﬁc patterns, and hosts in an
enterprise network typically communicate with only a small num-
ber of popular hosts. Note that SEATTLE’s stretch remains be-
low 5 even when a cache size is 0. Hence, even with worst-case
trafﬁc patterns (e.g., every host communicates with all other hosts,
switches maintain very small caches), SEATTLE still ensures rea-
sonably small stretch. Finally, we compare path stability with
ROFL in Figure 5b. We vary the rate at which hosts leave and
join the network, and measure path stability as the number of times
a ﬂow changes its path (the sequence of switches it traverses) in the
presence of host churn. We ﬁnd that ROFL has over three orders of
magnitude more path changes than SEATTLE.
6.3 Sensitivity to network dynamics
Effect of network changes: Figure 5c shows performance during
switch failures. Here, we cause switches to fail randomly, with
failure inter-arrival times drawn from a Pareto distribution with
α = 2.0 and varying mean values. Switch recovery times are
drawn from the same distribution, with a mean of 30 seconds. We
found SEATTLE is able to deliver a larger fraction of packets than
Ethernet. This happens because SEATTLE is able to use all links in
the topology to forward packets, while Ethernet can only forward
over a spanning tree. Additionally, after a switch failure, Ethernet
must recompute this tree, which causes outages until the process
completes. Although forwarding trafﬁc through a location resolver
in SEATTLE causes a ﬂow’s fate to be shared with a larger number
of switches, we found that availability remained higher than that of
Ethernet. Additionally, using caching improved availability further.
Effect of host mobility: To investigate the effect of physical or vir-
tual host mobility on SEATTLE performance, we randomly move
hosts between access switches. We drew mobility times from a
Pareto distribution with α = 2.0 and varying means. For high mo-
bility rates, SEATTLE’s loss rate is lower than Ethernet (Figure 6).
This happens because when a host moves in Ethernet, it takes some
)
g
o
l
(
e
t
a
r
s
s
o
l
t
e
k
c
a
P
10%
5%
1%
0.5%
0.1%
0.2
Eth
SEA_CA
SEA_NOCA
1
2
10
20
100 200
Mobility rate (num. of moving hosts per sec)
Figure 6: Effect of host mobility in Campus.
time for switches to evict stale location information, and learn the
host’s new location. Although some host operating systems broad-
cast a gratuitous ARP when a host moves, this increases broadcast
overhead. In contrast, SEATTLE provides both low loss and broad-
cast overhead by updating host state via unicasts.
7.