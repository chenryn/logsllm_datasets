## Page 121
第5章
数据摄入
"parser":
"type":"hadoopyString"
"parseSpec":{
"timestampSpec":
"columns":[
"dimensionsSpec":
"type": "longSum"
"name":"count",
"format"："csv"
"fieldName":"count"
"column":
"dimensionExclusions":[],
"format":
"spatialDimensions":[]
"dimensions":[
'count"
"category",
"commodity"
"city",
"age",
"user_id",
"event_name"
"timestamp",
"category"
"commodity",
"city",
"age",
"user_id",
"auto"
"timestamp"
---
## Page 122
据摄人，如图5-3所示。
2016-07-19T23:36:29，浏览商品，1,90+，Beijing,aaaaa,3c,1
2016-07-19T16:36:29,浏览商品，1,90+,Beijing,ZZzzz,3c,1
2016-07-19T12:36:29,浏览商品，1,90+,Beijing,yyy,3c,1
2016-07-19T08:36:29,浏览商品，1,90+,Beijing,xx,3c,1
Curl -X‘POST′-H'Content-Type:application/json'-d @hadoop-index-task.json http
8
pplication.14677286425640044
lication 1467728642564 0043
Druid会提交一个MapReduce任务到Hadoop系统，所以这种方式非常适合大批量的数
测试数据如下：
://10.24.199.8:8090/druid/indexer/v1/task
启动任务：
"type":“index _hadoop"
"tuningConfig":{
"ioConfig":{
"type":"hadoop"
"type":"hadoop"
"inputSpec":
"paths":"/tmp/dianshang_order.json"
"type":"static"
图5-3Hadoop MapReduce批量摄入
Narsa
MAPREDUCEdfaut
delauil
Druid实时大数据分析原理与实践
FINISHEDSUCCEEDEDHISOYNA
NA
---
## Page 123
低延迟、可扩展的特性。LA的可行性和必要性基于如下假设和原则。
据的架构理念。Lambda架构（LA）旨在满足一个稳定的大规模数据处理系统所需的容错性、
做法是把数据保存起来，
弃。我们如何将这部分丢弃的数据重新摄取进Druid系统中，以提高数据的准确性？通常的
5.4
第5章数据摄入
5.4.1
Lambda是实时处理框架Storm的作者NathanMarz提出的用于同时处理离线和实时数
我们都知道，Druid在摄取时需要设置一个时间窗口，在时间窗口之外的数据，将会丢
2016-07-1305
2016-07-1306
2016-07-1702
2016-07-1900
2016-07-1904
2016-07-1908
2016-07-1915
dailymonthly
48.4kB
LA 基本框架如图5-5所示。
·重新计算（Recomputation）：因为上面两个原则，运行函数重新计算结果是可能的。
·任何数据系统可定义为：query=functional(all data)。
从DruidCoordinatorConsole页面我们可以看到相关的Segment已经生成，如图5-4所示。
·数据不可变性（DataImmutability）：数据是只读的，不再变化。
·人为容错性（HumanFalult-Tolerance）：数据是易丢失的。
Lambda架构
流式与批量数据摄取的结合
200
430
，等待重新摄取。目前，比较流行的处理方法是Lambda架构。
6dimensions
4.38kB
38K8
图5-4生成Segment
2016082010
1metrics
99
---
## Page 124
现方式如图5-6所示。
性的系统来说，是不可接受的。那么就需要重新摄人这部分数据，参考Lambda的思想，实
5.4.2
如何在Druid系统之外采用Lambda架构的思维去解决时间窗口的问题呢？
节点和历史节点，任何查询都是聚合实时节点和历史节点的数据得到查询结果。那么，我们
100
Druid在摄取数据时，对于超出时间窗口的数据会直接丢弃，这对于某些要求数据准确
从以上论述我们可以知道，Druid本身就是一个典型的Lambda架构系统。Druid有实时
·任何查询都可以通过实时处理层和批处理层的查询结果合并得到。
·实时处理层仅处理实时数据，并为服务层提供查询服务。
·服务层计算出批处理视图中的数据做索引，以提供低延时，即席查询。
·批处理层有两个功能：管理主要的数据（该类数据的特点是只能增加，不能更新）；为
·所有新数据分别分发到批处理层和实时处理层。
该架构具有如下特点。
下一步计算出批处理视图做预计算。
解决时间窗口问题
batchlayer
speedlayer
N
master dataset
real-timeview
图5-5LA基本框架
real-timeview
servinglayer
batchview
batchview
Druid实时大数据分析原理与实践
##
query
---
## Page 125
shardSpec指定分片方式。目前支持两种分片方式，即Linear和Numbered。
置的方式有些许区别，下面分别进行论述。
数据分布到更多的Druid节点，以提高并行查询的效率。我们先来看数据分片部分。
些Segment过大的情况，进而加大加载时间，影响查询效率。Druid通过数据分片与复制，使
5.5.1数据分片
第5章数据摄入
5.5
流式拉取数据的方式需要启动实时节点，启动实时节点可以通过tuningConfg部分的
实时节点数据分片
Druid 数据分片都是通过Ingestion Spec 的 tuningConfig设置的。对于实时、批量摄取，设
·添加新的实时节点时，不用更改原实时节点的配置。
Linear分片具有如下优势。
Druid数据是以时间分片的，然而，当数据集中出现在某个时间段的时候，就会出现某
（4）定时或者发现有数据丢失时，通过Druid Hadoop IndexJob重新摄入数据。
（3）Kafka的数据通过Flume备份到Hadoop。
（2）数据通过实时节点或者索引l服务进人Druid中。
（1）源数据都进人Kafka。
（1）Linear分片
流程如下。
数据摄取的其他重要知识
Kafka
图5-6参考Lambda思想的Druid应用架构
Druid
Flume
Druid Hadoop Index
Hadoop
qor
101
---
## Page 126
目前，支持如下两种分区方式。
定分片个数。
-1），其中 targetPartitionSize是通过设置分片大小，计算出分片个数；numShards 则直接设
ingConfig设置该任务摄取数据的分片方式。设置方式如下：
2.Druid IndexJob数据分片
"shardSpec":
是必要的。与Linear分片不同的是，Numbered分片还必须指定分片总数。
"shardSpec":{
102
3.
"tuningConfig":{
Druid Hadoop Index Job数据分片
对于Hadoop Index Job数据分片，同样是通过tuningConfig部分的partitionsSpec设置的。
targetPartitionSize和 numShards是两种不同的分片方式，只能设置一个（不能都不等于
"type":"index"
批量摄取方式，都是通过启动Druid IndexJob来实现的。启动IndexJob，可以通过tun-
"targetPartitionSize":5000000
"partitionNum": 0,
"type":“numbered"
Numbered分片配置如下：
"numShards":-1
"partitions":2
Numbered分片方式，
"partitionNum":0
"type":"linear"
Linear分片配置如下：
·查询时，即使有些分片缺失，所有分片也都会被查询（例如，系统中存在分片0、分
（2）Numbered分片
片2，缺失了分片1，系统仍能进行相应的查询）。
，要求必须所有的分片都存在，才能提供查询。当然，很多时候这
Druid实时大数据分析原理与实践
---
## Page 127
会更快，数据在各个分区的分布也更加均匀。
第5章数据摄入
"partitionsSpec":{
"partitionsSpec":{
partitionDimensions
numShards
targetPartitionSize
type
配置项
"targetPartitionSize":5000000
"type": "dimension",
更多的配置项如下：
"targetPartitionSize":5000000
"type":"hashed"
哈希分区配置如下：
在通常情况下，哈希分区能满足大部分需求，相对于范围分区，哈希分区在摄取速度上
范围分区配置如下：
（1）哈希分区
·范围分区：基于维度值的取值范围分区。
（2）范围分区
·哈希分区：
基于维度值的哈希值分区。
PartitionSize，该项被忽略
numShards 使用，设置了 target-
基于分区的维度，
分区个数
为
分区目标行数，
分片类型
描述
500MB~1GB
尽量使分区大小
仅仅配合
设置一个
targetPartitionSize和numShards只能
设置一个
targetPartitionSize和numShards只能
否
是否必需
"hashed"
103
---
## Page 128
104
assumeGrouped
partitionDimension
maxPartitionSize
targetPartitionSize
type
配置项
数据分区样例
"spec":{
以如下的Spec启动任务。
我们以Hadoop Index Job分区为例，仍使用用户行为数据摄取案例。
更多的配置项如下：
"dataSchema":
"parser":{
"granularitySpec":
"dataSource":
"metricsSpec":
"parseSpec":{
"type”:"uniform"
"segmentGranularity”:“HOUR"
"intervals":
"queryGranularity":"MINUTE"
"columns":[
"2016-08-19/2016-08-20"
是否假设数据已经预先按时间和维度分组
基于分区的维度，没有设置，会自动选择一个维度
"type":"longSum"
"name":“count",
"fieldName":"count",
分区最大行数，
分区目标行数，尽量使分区大小为500MB~1GB
分片类型
描述
"dianshang_order",
人
，默认最大值为1.5倍targetPartitionSize
Druid实时大数据分析原理与实践
否
否
否
是
"dimension"
是否必需
---
## Page 129
第5章
数据摄入
"tuningConfig":
"ioConfig":
"inputSpec":
"type":
"type":"static"
"paths":
"type":"hadoopyString”
"hadoop'
人
"format":"csv"
dimensionsSpec":{
"column":"timestamp",
"format";
"spatialDimensions":[]
"dimensionExclusions":[],
"/tmp/dianshang_order.json",
"dimensions":[
"count"
"category"
"commodity"
"city"
"age",
"user_id",
"event_name"
"timestamp",
"category"
"commodity"
"city",
"age""
"user_id",
"event_name"
"auto"
人
105
---
## Page 130
5.5.2
器上。