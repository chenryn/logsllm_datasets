	Evaluation Metrics. To evaluate the effectiveness of the models, the experiments use the following metrics:
• Precision (P) measures the proportion of correctly 	identified anomaly samples (TP), out of all the anomalies 	detected by the model, and is calculated as P = TP / 	(TP+FP)• Recall (R) measures the proportion of correctly identified 	anomaly samples (TP) out of all real anomalies, and is 	calculated as R = TP / (TP+FN).
• F1 Score (F1) is the harmonic mean of the Precision and 	Recall, and is calculated as F1 = 2 * (P*R)/(P+R).
• Specificity (S) measures the proportion of correctly identified normal samples (TN) out of all real normal samples, and is calculated as S = TN/(TN+FP).True Positives (TP) refer to the number of anomaly samples that were correctly detected by the model. False Positives (FP) refer to the number of normal log samples that the model incorrectly detected as anomaly. False Negatives (FN) are the anomaly samples that were not detected by the model. Finally True Negatives (TN) are the normal samples that the model correctly detected.In real-world deployment scenarios, having a predictive model with high Specificity is more advantageous since it minimises the chances of producing false positives or false alarms. A model with high Specificity can correctly identify normal samples, which makes it more likely that any detected
7
| Method | P | R | F1 | S |
|---|---|---|---|---|
| DeepLog |100.0 |60.90 |75.70 |100.0 || DeepLog |100.0 |60.90 |75.70 |100.0 |