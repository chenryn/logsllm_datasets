配置yum源（包括rhel、ceph的源）
与Client主机同步时间
node1允许无密码远程node4
## 2）部署元数据服务器
登陆node4，安装ceph-mds软件包
\[root@node4 \~\]# yum -y install ceph-mds
登陆node1部署节点操作
\[root@node1 \~\]# cd /root/ceph-cluster
//该目录，是最早部署ceph集群时，创建的目录
\[root@node1 ceph-cluster\]# ceph-deploy mds create node4
//给nod4拷贝配置文件，启动mds服务
同步配置文件和key
\[root@node1 ceph-cluster\]# ceph-deploy admin node4
## 3）创建存储池
\[root@node4 \~\]# ceph osd pool create cephfs_data 128
//创建存储池，对应128个PG
\[root@node4 \~\]# ceph osd pool create cephfs_metadata 128
//创建存储池，对应128个PG
\[root@node4 \~\]# ceph osd lspools
//查看存储池
## 5）创建Ceph文件系统
\[root@node4 \~\]# ceph mds stat //查看mds状态
e2:, 1 up:standby
**\[root@node4 \~\]# ceph fs new myfs1 cephfs_metadata cephfs_data**
内容显示：new fs with metadata pool 2 and data pool 1
//注意，先写metadata池，再写data池
//默认，只能创建1个文件系统，多余的会报错
\[root@node4 \~\]# ceph fs ls
name: myfs1, metadata pool: cephfs_metadata, data pools: \[cephfs_data
\]
\[root@node4 \~\]# ceph mds stat
e4: 1/1/1 up {0=node4=up:creating}
## 6）客户端挂载
\[root@client \~\]# mount -t ceph 192.168.4.11:6789:/ /mnt/cephfs/ \\
-o name=admin,secret=AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg==
//注意:文件系统类型为ceph
//192.168.4.11为MON节点的IP（不是MDS节点）
//admin是用户名,secret是密钥
//密钥可以在/etc/ceph/ceph.client.admin.keyring中找到
# 3 案例3：创建对象存储服务器
3.1 问题
延续前面的实验，实现Ceph对象存储的功能。具体实现有以下功能：
安装部署Rados Gateway
启动RGW服务
设置RGW的前端服务与端口
客户端测试
3.2 步骤
## 步骤一：部署对象存储服务器
### 1）准备实验环境，要求如下：
IP地址:192.168.4.15
主机名:node5
配置yum源（包括rhel、ceph的源）
与Client主机同步时间
node1允许无密码远程node5
修改node1的/etc/hosts，并同步到所有node主机
### 2）部署RGW软件包
\[root@node1 \~\]# ceph-deploy install \--rgw node5
同步配置文件与密钥到node5
\[root@node1 \~\]# cd /root/ceph-cluster
\[root@node1 \~\]# ceph-deploy admin node5
### 3）新建网关实例
启动一个rgw服务
\[root@node1 \~\]# ceph-deploy rgw create node5
登陆node5验证服务是否启动
\[root@node5 \~\]# ps aux \|grep radosgw
ceph 4109 0.2 1.4 2289196 14972 ? Ssl 22:53 0:00 /usr/bin/radosgw -f
\--cluster ceph \--name client.rgw.node4 \--setuser ceph \--setgroup
ceph
\[root@node5 \~\]# systemctl status ceph-radosgw@\\\*
### 4）修改服务端口
登陆node5，RGW默认服务端口为7480，修改为8000或80更方便客户端记忆和使用
\[root@node5 \~\]# vim /etc/ceph/ceph.conf
\[client.rgw.node5\]
host = node5
rgw_frontends = \"civetweb port=8000\"
//node5为主机名
//civetweb是RGW内置的一个web服务
## 步骤二：客户端测试
### 1）curl测试
\[root@client \~\]# curl 192.168.4.15:8000
\\\\anonymous\\\\\\\
### 2）使用第三方软件访问
登陆node5（RGW）创建账户
\[root@node5 \~\]# radosgw-admin user create \\
\--uid=\"testuser\" \--display-name=\"First User\"
... ...
\"keys\": \[
{
\"user\": \"testuser\",
\"access_key\": \"5E42OEGB1M95Y49IBG7B\",
\"secret_key\": \"i8YtM8cs7QDCK3rTRopb0TTPBFJVXdEryRbeLGK6\"
}
\],
\... \...
\#
\[root@node5 \~\]# radosgw-admin user info \--uid=testuser
//testuser为用户，key是账户访问密钥
### 3）客户端安装软件
\[root@client \~\]# yum install s3cmd-2.0.1-1.el7.noarch.rpm
修改软件配置（注意，除了下面设置的内容，其他提示都默认回车）
\[root@client \~\]# s3cmd \--configure
Access Key: 5E42OEGB1M95Y49IBG7B\
Secret Key: i8YtM8cs7QDCK3rTRopb0TTPBFJVXdEryRbeLGK6
S3 Endpoint \[s3.amazonaws.com\]: 192.168.4.15:8000
\[%(bucket)s.s3.amazonaws.com\]: %(bucket)s.192.168.4.15:8000
Use HTTPS protocol \[Yes\]: No
Test access with supplied credentials? \[Y/n\] n
Save settings? \[y/N\] y
//注意，其他提示都默认回车
### 4）创建存储数据的bucket（类似于存储数据的目录）
\[root@client \~\]# s3cmd ls
\[root@client \~\]# s3cmd mb s3://my_bucket
Bucket \'s3://my_bucket/\' created
\[root@client \~\]# s3cmd ls
2018-05-09 08:14 s3://my_bucket
\[root@client \~\]# s3cmd put /var/log/messages s3://my_bucket/log/
\[root@client \~\]# s3cmd ls
2018-05-09 08:14 s3://my_bucket
\[root@client \~\]# s3cmd ls s3://my_bucket
DIR s3://my_bucket/log/
\[root@client \~\]# s3cmd ls s3://my_bucket/log/
2018-05-09 08:19 309034 s3://my_bucket/log/messages
测试下载功能
\[root@client \~\]# s3cmd get s3://my_bucket/log/messages /tmp/
测试删除功能
\[root@client \~\]# s3cmd del s3://my_bucket/log/messages
# 4 案例2Ceph文件系统实际实验
2.1 问题
延续前面的实验，实现Ceph文件系统的功能。具体实现有以下功能：
部署MDSs节点
创建Ceph文件系统
客户端挂载文件系统
2.2 方案
添加一台虚拟机，部署MDS节点。
主机的主机名及对应的IP地址如表-1所示。
表－1 主机名称及对应IP地址表
Node4 192.168.4.14
## 1）添加一台新的虚拟机，要求如下：
以下第四天实验已安装:(如果新环境需要重新做)
IP地址:192.168.4.13 #实际中用的是node3
主机名:node3
配置yum源（包括rhel、ceph的源）
与Client主机同步时间
node1允许无密码远程node3
## 2）部署元数据服务器
登陆node3，安装ceph-mds软件包
\[root@node3 \~\]# yum -y install ceph-mds
登陆node1部署节点操作
\[root@node1 \~\]# cd /root/ceph-cluster
//该目录，是最早部署ceph集群时，创建的目录
\[root@node1 ceph-cluster\]# ceph-deploy mds create node3
//给nod4拷贝配置文件，启动mds服务
同步配置文件和key
\[root@node1 ceph-cluster\]# ceph-deploy admin node3 #之前已经做了
## 3）创建存储池
不创建就默认有个0 rdb存储池
\[root@node333 \~\]# ceph osd lspools
0 rbd,
创建名为:cephfs_data的存储池
\[root@node3 \~\]# ceph osd pool create cephfs_data 128
//创建存储池，无固定的值,尽量是2的次方.推荐是32-128之间.对应128个PG
创建名为:cephfs_metadata的存储池
\[root@node3\~\]# ceph osd pool create cephfs_metadata 128
//创建存储池，对应128个PG
\[root@node333 \~\]# ceph osd lspools
0 rbd,1 cephfs_data,2 cephfs_metadata,
## 4）创建Ceph文件系统
\[root@node4 \~\]# ceph mds stat //查看mds状态
e2:, 1 up:standby
\[root@node4 \~\]# ceph fs new myfs1 cephfs_metadata cephfs_data
new fs with metadata pool 2 and data pool 1
#用两个池子创建了文件系统
//注意，先写medadata池，再写data池
//默认，只能创建1个文件系统，多余的会报错
\[root@node4 \~\]# ceph fs ls #查看
name: myfs1, metadata pool: cephfs_metadata, data pools: \[cephfs_data
\]
\[root@node4 \~\]# ceph mds stat
e4: 1/1/1 up {0=node4=up:creating}
## 客户端挂载
\[root@client \~\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm
\[root@client \~\]# df -h
文件系统 容量 已用 可用 已用% 挂载点
/dev/mapper/rhel-root 17G 3.5G 14G 21% /
devtmpfs 481M 0 481M 0% /dev
tmpfs 497M 0 497M 0% /dev/shm
tmpfs 497M 7.0M 490M 2% /run
tmpfs 497M 0 497M 0% /sys/fs/cgroup
/dev/vda1 1014M 161M 854M 16% /boot
tmpfs 100M 0 100M 0% /run/user/0
\[root@client \~\]# mount -t ceph 192.168.4.11:6789:/ /mnt/cephfs/ \\
-o name=admin,secret=AQBTsdRapUxBKRAANXtteNUyoEmQHveb75bISg==
-t 类型 -o 用户名密码
//注意:文件系统类型为ceph
//192.168.4.11为MON节点的IP（不是MDS节点）
//admin是用户名,secret是密钥
//密钥可以在node3的/etc/ceph/ceph.client.admin.keyring中找到
\[root@client \~\]# df -h
文件系统 容量 已用 可用 已用% 挂载点
/dev/mapper/rhel-root 17G 3.5G 14G 21% /
devtmpfs 481M 0 481M 0% /dev
tmpfs 497M 0 497M 0% /dev/shm
tmpfs 497M 7.0M 490M 2% /run
tmpfs 497M 0 497M 0% /sys/fs/cgroup
/dev/vda1 1014M 161M 854M 16% /boot
tmpfs 100M 0 100M 0% /run/user/0
192.168.4.13:6789:/ 120G 552M 120G 1% /mnt
查询
\[root@node1 \~\]# ceph -s