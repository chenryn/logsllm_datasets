### USENIX Association 22nd International Symposium on Research in Attacks, Intrusions and Defenses

#### Figure 5: Execution Time Analysis
- **Amortized Time for Shuffling**
- **Client Computation Time**
- **Server Computation Time**
- **Total Time for Users**

**Execution Time (in seconds)**
- 10
- 1
- 0.1
- 0.01
- 0.001

**Storage Size (in GB)**
- 0.1
- 1
- 100
- 1000

**Block Size (in KB)**
- 10
- 100
- 1000
- 10000

(a) **Execution time is constant for fixed B = 256KB.**
(b) **Execution increases with B for N.B = 1GB.**
(c) **Execution time increases with B where N = 4096.**

**Figure 5 Explanation:**
This figure shows the execution time for the client, server, shuffling, and total latency per access for a fixed block size (B), fixed total storage (N.B), and a fixed number of blocks (N). The results confirm our theoretical claims of constant latency per request, and that the latency for large data sizes (in GBs) is practical (under 1 second for 256KB blocks).

#### Figure 6: Throughput Analysis
- **PRO-ORAM Throughput**
- **PathORAM Throughput**
- **GORAM + SGX Throughput**
- **Network Bandwidth**

**Throughput (in Mbps)**
- 1000
- 100
- 10

**Total Storage Size (in GB)**
- 0.1
- 1
- 100

**Block Size (in KB)**
- 10
- 100
- 1000
- 10000

(a) **Throughput for varying N.B where B = 256KB.**
(b) **Throughput for varying B where N.B = 1GB.**
(c) **Throughput for varying B where N = 4096.**

**Figure 6 Explanation:**
This figure illustrates the throughput of PRO-ORAM in Mbps for fixed block size (B), fixed total storage (N.B), and fixed number of blocks (N). The results show that PRO-ORAM's throughput exceeds the reference throughput of 7 Mbps, confirming that network latency is likely to dominate over computational overheads.

### Evaluation and Results

#### Baseline and Experimental Setup
For comparisons of communication and network latencies, we use a bandwidth link of 7 Mbps as a representative, which is the global average based on a recent report from Akamai [7]. Our evaluation covers varying data sizes, ranging from 20 MB to 2 GB, with block sizes (B) varying from 4 KB to 10 MB. In our experiments for parallelized shuffling, we set temporary buffers as \( \sqrt{N} \) data blocks to ensure security guarantees. We utilize all 40 cores for multi-threading during each distribution phase and cleanup phase. All results are averaged over 10 runs and reported on log-scale plots.

#### Goals of the Evaluation
- Validate the theoretical claim of constant communication and computation latencies.
- Confirm that the execution time per access in PRO-ORAM is dependent only on the block size.
- Show that the final bottleneck is the network latency, rather than computational latency.
- Demonstrate the effective throughput of PRO-ORAM for different blocks of practical data sizes.

#### 6.1 Results: Latency
To measure performance, we calculate the execution time (latency) at the user, server (i.e., access enclave), and the amortized shuffling time of the shuffle enclave for each request. The client computational latency, the amortized shuffle time, and the network latency are the three factors that contribute to the overall latency.

- **Impact on Latency with Increasing Storage:**
  - We measure the execution time to access a block of fixed size B = 256KB while increasing the total storage size from 20 MB to 2 GB.
  - The dominant cost is from the server computation. The access and shuffle enclaves each incur a constant execution time of around 0.016 seconds per access, irrespective of the data sizes.
  - The client computation time is constant at 0.002 seconds as the user only decrypts a constant-size encrypted block.
  - These results confirm our theoretical claims of constant latency per request and that the latency for large data sizes (in GBs) is practical (under 1 second for 256KB blocks).

- **Computational vs. Network Bottleneck:**
  - An important finding is that the latency per access observed by the user is a constant at 0.3 seconds, within experimental error, irrespective of the total data size.
  - Even though the server computation cost is high, the final latency has a primary bottleneck as the network, not PRO-ORAM’s computation.
  - The latency of shuffling per requested block is less than the network latency of sending a block from the server to the client on a 7Mbps link.
  - This suggests that even for 256 KB block sizes, the network latency dominates the overall latency observed by the user and is likely to be the bottleneck in an end application (e.g., streaming media) rather than the cost of all the operations in PRO-ORAM, including shuffling.
  - This result indicates that PRO-ORAM is optimized enough to compete with network latency, making it practical for real applications.

- **Latency Increase with Block Size:**
  - We perform three sets of experiments keeping (a) block size constant (B), (b) total storage size constant (N.B), and (c) number of blocks constant (N), while varying the remaining two parameters respectively in each experiment.
  - The results show that the computational latencies of server and client-side costs in PRO-ORAM depend primarily on the block size parameter and are unaffected by the number of blocks or size of data.
  - This is mainly because the cost of encryption and decryption per block increases these latencies.

#### 6.2 Results: Throughput
We calculate throughput as the number of bits that PRO-ORAM can serve per second. PRO-ORAM can serve a maximum of \( \sqrt{N} \) blocks in the time the shuffle enclave completes permutation of N data blocks. Thus, to calculate throughput, we use the following formula: 
\[ \text{Throughput} = \frac{\text{total\_shuffling\_time}}{\text{total\_data\_size}} \]

- **Throughput Increase with Block Size:**
  - We find that the throughput of PRO-ORAM increases with block size, ranging from 83 Mbps (for 100KB block size) to 235 Mbps (for 10MB block size).
  - For data objects larger than a few hundred KB, the throughput is almost 10x larger than the global average network bandwidth (7Mbps).
  - Such data object sizes are common for media content (e.g., photos, videos, music) and cache web page content [11].
  - The throughput measurements for increasing block sizes, keeping the total data size and the number of blocks fixed to 1 GB and 4096 respectively, show that the throughput increases with the block size.
  - If we keep the block size fixed, the throughput is constant at almost 125 Mbps with the increase in the total data size.
  - Our evaluation shows that PRO-ORAM’s throughput exceeds the reference throughput of 7 Mbps, reconfirming that network latency is likely to dominate latencies over computational overheads of PRO-ORAM.

- **Comparison to Tree-based ORAM:**
  - We compare the throughput of PRO-ORAM with the access overhead of using the simplest and efficient PathORAM scheme with SGX [43].
  - The throughput for PathORAM+SGX decreases and almost reaches the network latency limit (7 Mbps) with an increase in the number of blocks for a fixed block size of 256 KB.
  - The server computation overhead of O(logN) per access of PathORAM protocol becomes a bottleneck for reasonably large data sizes (e.g., 2 GB as shown in Figure 6a).
  - Figure 6b shows that PathORAM’s throughput increases from 7 to 15 Mbps with a decrease in blocks.

#### 6.3 Performance Breakdown
To understand the breakdown of the source of latency for the shuffle step, we calculate the time to perform cryptographic operations and ECALLs/OCALLs to copy data in and out of memory. This breakdown allows us to better understand the time-consuming operations in our system. We fix the block size to B = 256 KB and vary the total data size.

- **Amortized Shuffling Time:**
  - The dominant cost comes from the cryptographic operations, which take 0.014 seconds out of the 0.016 seconds.
  - Enclaves by design cannot directly invoke system calls to access untrusted memory. Each call to the outside enclave is performed using OCALL, and a function within an enclave is invoked using an ECALL.
  - To fetch \( \sqrt{N} \) data blocks in parallel for each access, we use asynchronous ECALLs/OCALLs in PRO-ORAM, similar to that proposed in a recent work [14].
  - These operations require 0.002 seconds (average) for a block of size 256 KB.

### Related Work
- **Write-Only ORAMs:**
  - Recent works have shown that constant computation and communication latency can be achieved for applications with restricted patterns.
  - Blass et al. propose Write-Only ORAM in the context of hidden volumes [18], achieving constant latencies per write access to the data untrusted storage.
  - Roche et al. propose a stash-free version of this Write-Only ORAM [35].
  - Flat ORAM improves over this solution using secure processors for efficient memory management [24].
  - ObliviSync uses the write-only ORAM idea to support sharing of files on a Dropbox-like storage server that supports auto-sync mechanisms [15].
  - These works inspire our focus on read-only patterns in PRO-ORAM.

- **Improvements to Square-Root ORAM:**
  - Although square-root ORAM is known to have very high worst-case overhead, Goodrich et al. provide a construction that reduces the worst-case overhead to \( O(\sqrt{N} \log^2 N) \).
  - Their solution de-amortizes the computation over \( \sqrt{N} \) batches, each taking \( O(\sqrt{N} \log^2 N) \) time after every access step.
  - This technique is similar to the distribution of shuffle steps in PRO-ORAM. However, our observations for the read-only setting allow us to execute the access and shuffle steps in parallel, which is not possible in their approach.