USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 205s
d
n
o
c
e
s
n
i
e
m
T
i
 10
 1
 0.1
 0.01
 0.001
Amortized time for shuffling
Client computation time
Server computation time
Total time for users
Amortized time for shuffling
Client computation time
Server computation time
Total time for users
 100
 10
 1
 0.1
 0.01
s
d
n
o
c
e
s
n
i
e
m
T
i
 0.1
Storage size in GB
 1
 0.001
 100
 1000
Block size in KB
Amortized time for shuffling
Client computation time
Server computation time
Total time for users
s
d
n
o
c
e
s
n
i
e
m
T
i
 10
 1
 0.1
 0.01
 0.001
 10000
 100
 1000
Block size in KB
(a) Execution time is constant for ﬁxed B =
256KB.
(b) Execution increases with B for N.B =
1GB.
(c) Execution time increases with B where
N = 4096.
Figure 5: Execution time for client, server, shufﬂe and total latency per access for a ﬁxed block size (B), ﬁxed total storage (N.B)
and a ﬁxed no. of blocks (N)
 1000
 100
 10
s
p
b
M
n
i
t
u
p
h
g
u
o
r
h
T
PRO-ORAM Throughput
PathORAM
GORAM + SGX Throughput
Network bandwidth
 0.1
 1
Total storage size in GB
 1000
 100
s
p
b
M
n
i
t
u
p
h
g
u
o
r
h
T
 10
 100
PathORAM
PRO-ORAM Throughput
GORAM + SGX Throughput
Network bandwidth
PathORAM
PRO-ORAM Throughput
GORAM + SGX Throughput
Network bandwidth
 1000
 100
 10
s
p
b
M
n
i
t
u
p
h
g
u
o
r
h
T
 1000
Block size in KB
 10000
 100
 1000
Block size in KB
(a) Throughput for varying N.B where B =
256KB.
(b) Throughput for varying B where N.B =
1GB.
(c) Throughput for varying B where N =
4096.
Figure 6: Throughput of PRO-ORAM in Mbps for ﬁxed block size (B), ﬁxed total storage (N.B) and ﬁxed number of blocks (N)
As a baseline for comparisons of communication and net-
work latencies, we take the bandwidth link of 7 Mbps as a
representative, which is the global average based on a recent
report from Akamai [7]. We perform our evaluation on vary-
ing data sizes such that the total data ranges from 20 MB to 2
GB with block sizes (B) varying from 4 KB to 10 MB. In our
√
experiments for parallelized shufﬂe, as shown in Algorithm 1,
we set temporary buffers as 2
N data blocks to ensure secu-
rity guarantees. To make full use of computation power, we
utilize all 40 cores for performing multi-threading for each
distribution phase and cleanup phase. All results are averaged
over 10 runs, reported on log-scale plots. We perform our
evaluation with the following goals:
• To validate our theoretical claim of constant communi-
cation and computation latencies.
• To conﬁrm that execution time per access in PRO-ORAM
is dependent only on the block size.
• To show that the ﬁnal bottleneck is the network latency,
rather than computational latency.
• To show the effective throughput of PRO-ORAM for differ-
ent blocks of practical data sizes.
6.1 Results: Latency
To measure the performance, we calculate the execution time
(latency) at the user, server (i.e., access enclave) and the
amortized shufﬂing time of the shuffle enclave for each
request. We point out that the client computational latency,
the amortized shufﬂe time, and the network latency are the
three factors that add up to the overall latency.
Impact on Latency with Increasing Storage. We measure
the execution time to access a block of ﬁxed size B = 256KB,
while increasing the total storage size from 20 MB to 2GB.
The measurements are reported in Figure 5a. The dominant
cost, as expected, is from the server computation. The access
and shuffle enclave each incur a constant execution time of
around 0.016 seconds per access, irrespective of the data sizes.
The client computation time is constant at 0.002 seconds as
the user only decrypts a constant-size encrypted block. Over-
all, these results conﬁrm our theoretical claims of constant
latency per request, and that the latency for large data size (in
GBs) is practical (under 1 sec for 256KB blocks).
Computational vs. network bottleneck. An important ﬁnd-
ing from Figure 5a is that the latency per access observed
by the user is a constant at 0.3 seconds, within experimen-
tal error, irrespective of the total data size. Even though the
server computation cost is high, the ﬁnal latency has primary
206          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX AssociationAmortized time for shuffling
Enc/Dec time
Ecall/Ocall time
 0.1
 0.01
s
d
n
o
c
e
s
s
i
e
m
T
i
 0.001
 0.1
 1
Total storage size in GB
Figure 7: Overhead breakdown for shufﬂe step for ﬁxed block-
size B = 256
bottleneck as the network, not PRO-ORAM’s computation. In
Figure 5a, the latency of shufﬂe per requested block is lesser
than the network latency of sending a block from the server
to the client on a 7Mbps link. This ﬁnding suggests that even
for 256 KB block sizes, the network latency dominates the
overall latency observed by the user, and is likely to be the
bottleneck in an end application (e.g. streaming media) rather
than the cost of all the operations in PRO-ORAM, including
shufﬂing. This result suggests that PRO-ORAM is optimized
enough to compete with network latency, making it practical
to use in real applications.
Latency increase with block size. We perform three sets
of experiments keeping (a) block size constant (B), (b) total
storage size constant (N.B), and (c) number of blocks constant
(N), while varying the remaining two parameters respectively
in each experiment. The results in Figure 5b and 5c show
evidence that the computational latencies of server and client-
side cost in PRO-ORAM depend primarily on the block size
parameter, and is unaffected by the number of blocks or size
of data. This is mainly because the cost of encryption and
decryption per block increases these latencies.
6.2 Results: Throughput
√
We calculate throughput as the number of bits that PRO-ORAM
can serve per second. PRO-ORAM can serve maximum
N
blocks in the time the shuffle enclave completes permuta-
tion of N data blocks. Thus, to calculate throughput we use
the following formula, Throughput =
total_shuffling_time.
Throughput increase with block size. We ﬁnd that through-
put of PRO-ORAM increases with block size, ranging from 83
Mbps (for 100KB block size) to 235 Mbps (for 10MB block
size), as shown in Figure 6b. Our experiments show that
for data objects of the size larger than few hundred KB, the
throughput is almost 10x larger than the global average net-
work bandwidth (7Mbps). Such data object sizes are common
for media content (e.g photos, videos, music) and cache web
√
N.B
page content [11]. Figure 6b and Figure 6c show the through-
put measurements for increasing block sizes, keeping the total
data size and the number of blocks ﬁxed to 1 GB and 4096
respectively. We observe that the throughput increases with
the blocksize. If we keep the block size ﬁxed, the throughput
is constant at almost 125 Mbps with the increase in the total
data size, as seen in Figure 6a. Our evaluation shows that
PRO-ORAM’s throughput exceeds reference throughput of 7
Mbps, re-conﬁrming that network latency is likely to domi-
nate latencies than computational overheads of PRO-ORAM.
Comparison to Tree-based ORAM. We compare the
throughput of PRO-ORAM with the access overhead of using
the simplest and efﬁcient PathORAM scheme with SGX [43].
The client side operations in the original PathORAM scheme
are executed within SGX. The throughput for PathORAM+SGX
scheme decreases and almost reaches the network latency
limit (7 Mbps) with increase in the number of blocks for ﬁxed
blocksize of 256 KB. Thus, the server computation overhead
of O(logN) per access of PathORAM protocol becomes a bot-
tleneck for reasonably large data sizes (e.g., 2 GB as shown
in Figure 6a). Figure 6b shows that PathORAM’s throughput
increases from 7 to 15 Mbps with a decrease in blocks.
6.3 Performance Breakdown
To understand the breakdown of the source of latency for
the shufﬂe step, we calculate the time to perform the cryp-
tographic operations and ECALLs/OCALLs to copy data in
and out of memory. Such a breakdown allows us to better
understand the time consuming operations in our system. We
ﬁx the block size to B = 256 KB and vary the total data size.
Figure 7 shows the amortized shufﬂing time, time to perform
encryption and decryption operations and the time to invoke
ECALLs/OCALLs per access in PRO-ORAM. We observe that
the dominant cost comes from the cryptographic operations
0.014 seconds out of the 0.016 seconds. Enclaves by design
cannot directly invoke system calls to access untrusted mem-
ory. Each call to the outside enclave performed using OCALL.
Similarly, a function within an enclave is invoked using an
ECALL. Thus, invocation of ECALLs/OCALLs is necessary to
perform multi-threading and for copying data in and out of
memory. To fetch
N data blocks in parallel for each access,
we use asynchronous ECALLs/OCALLs in PRO-ORAM similar
to that proposed in a recent work [14]. These operations re-
quire 0.002 seconds (average) for a block of size 256 KB.
√
7 Related Work
First, we discuss ORAM constructions that guarantee constant
latency per access for write-only patterns. Next, we summa-
rize related work with similarities in our threat model.
Write-Only ORAMs. Recently, it is shown that constant
computation and communication latency can be achieved
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 207for applications with restricted patterns. Blass et. al show
that some applications require hiding only write-patterns and
hence proposed Write-Only ORAM in the context of hid-
den volumes [18]. Their work achieves constant latencies
per write access to the data untrusted storage. Roche et al.
propose a stash-free version of this Write-Only ORAM [35].
Further, Flat ORAM improves over this solution using secure
processors to perform efﬁcient memory management [24].
ObliviSync uses the write-only ORAM idea to support shar-
ing of ﬁles on a Dropbox-like storage server that support
auto-sync mechanisms [15]. These works that guarantee con-
stant overhead for hiding write-only access patterns inspire
our work. PRO-ORAM focuses on applications that exhibit read-
only patterns and achieves constant latencies for them.
√
Improvements to square-root ORAM. Although square-
root ORAM is known to have very high i.e., O(N log2 N)
√
worst-case overhead, Goodrich et. al provide a construc-
N log2 N).
tion that reduces the worst-case overhead to O(
Instead of shufﬂing the entire memory at once taking
√
O(N log2 N) computation time, their solution de-amortizes
N log2 N)
N batches each taking O(
the computation over
time after every access step. This technique is similar to the
distribution of shufﬂe steps in PRO-ORAM. However, our obser-
vations for the read-only setting allows us to execute the ac-
cess and shufﬂe steps in parallel which is not possible in their