(cid:4)
partition), the simulator works as before.
B. Database privacy
Following Carlini et al. [25], we divide our analysis of
privacy for the database into two parts: physical privacy, which
is the leakage of an algorithm beyond what is learned by the
query answer itself, and functional privacy, which is the leakage
from the query answer itself. To quote Carlini et al. [25]: “we
emphasize that these two notions of privacy are incomparable
and complementary.” For the purpose of this analysis we will
consider the query answer to consist of the vector that is
output by oblivious masking, [0, . . . , 0, ID, rz+1, . . . , rL]. The
DPF-based symmetric-PIR technique combined with oblivious
masking gives us physical privacy. For functional privacy we
have to compare the leakage of this query answer to the leakage
of the answer in the baseline Functionality 1.
Physical privacy. The client’s view of the database can be
efﬁciently simulated given the query answer, consisting of the
ID and index z. The simulator must create two vectors, one
for each server’s response. It does so by ﬁrst constructing a
vector C with the ﬁrst z− 1 entries all zero, ID in the zth entry,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:59 UTC from IEEE Xplore.  Restrictions apply. 
9919
and random numbers for all remaining entries. It then secret
shares this vector into two “response vectors” CA and CB to
construct a statistically indistinguishable view for the client.
Functional privacy. We now turn to formalizing the database
leakage incurred from the answer to a client’s query. We start
by quantifying the baseline leakage, given that the client must
obtain information as output: the ANN ID relative to the query
q. In our setting, the client learns strictly more than just the
ID, which we must quantify as additional leakage.
Theorem 1 (Quantifying Baseline Leakage). Fix quantities
Dmax, Dmin, and ∆ as deﬁned in Section III-A. The baseline
leakage for an instance of approximate nearest neighbor search
as instantiated in Functionality 1 is captured by O(d + log N )
bits of information per query, where d is the intrinsic dimen-
sionality of the vector space and N is the size of the database.
Proof. We start by considering the (cid:96)∞-norm and induced
distance metric ∆. The (cid:96)∞-norm is the absolute value of
the maximum coordinate: (cid:96)∞||x|| = maxxi |xi|. The induced
distance metric is ∆(x, y) = (cid:96)∞||x − y||. We ﬁrst prove the
baseline leakage for the (cid:96)∞-norm, as it is easier to intuit. We
then show how to extend the proof to any (cid:96)p-norm induced
metric, which includes Euclidean distance. Other common
distance metrics, such as angular distance and Hamming
distance, can be embedded into Euclidean space [10, 26].
Recall that for any query q that returns an ID corresponding
to vector v, we have that ∆(v, q) ≤ Rmax  95%) without
needing more than 20 tables and 50 multi-probes per table.
Parallelism. The server overhead of answering queries (which
involves a linear scan over all hash table keys; see Section VI)
is easily parallelizable across cores or even across different
machines composing each logical server. In our runtime
experiments (Figure 8), we provide results for both single
core and parallelized executions (where each hash table is
processed on a separate core). In our experiments, we observe
a close-to-linear speedup in the degree of parallelism.
Performance. We report the end-to-end latency (as measured
on the client machine) for each dataset in Figure 8. The end-
to-end latency includes the server-side processing time, client-
side processing, and network delay. The server processing
time, per hash table, ranges from 28 ms on the MNIST dataset
(60,000 items) to approximately 6.5 s on the DEEP1B dataset
(10,000,000 items). The processing time is dominated by the
DPF and, in practice, increases moderately with the number of
multiprobes. The other steps, including OBLIVIOUSMASKING,
take less than 2 ms. The client processing overhead across all
datasets is small: never exceeding 10 ms.
Communication. We provide the total communication required
to query one hash table in Table IV. The communication
Fig. 6: Recall (fraction of queries that found an approximate nearest
neighbor) for different numbers of table multi-probes using the data
structure of Figure 4 and c = 2. Average of 10 trial runs for different
numbers of probes. Probes = 1 corresponds to no multi-probing (only
retrieving the hash of the query from the table; Section IV-C). The
standard deviation is 0.002 and invisible in the plot.
this leakage assumes that all points collide at the smallest
radius Rmin, when in practice the points will collide across
buckets deﬁned by radii between Rmin and Rmax.
VIII. EMPIRICAL EVALUATION
We now turn to describing our implementation and empirical
evaluation of Protocol 1. The goal of this section is to answer
the following questions:
• What are the parameters needed to obtain high accuracy in
practice using the data structure of Figure 4?
• What is the concrete performance of Protocol 1 when used
for ANN search on real data?
• How does Protocol 1 compare to the state-of-the-art approach
for private similarity search?
A. Implementation and environment
We implement Protocol 1 in approximately 4,000 lines
of code. Our implementation is written in Go v1.16. with
performance-critical components written in C. The code is
open source and available online [1]. Our DPF implementation
follows Boyle et al. [21] and is partially based on open-source
libraries [35, 37]. Our implementation uses AES as a pseudo-
random generator which exploits the AES-NI instruction for
hardware-accelerated operations. We use the GMP library [39]
for fast modular arithmetic.
Environment. We deploy our implementation on Amazon Elastic
Cloud Compute (EC2) for our experiments. We geographically
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:59 UTC from IEEE Xplore.  Restrictions apply. 
11921
151020304050Number of hash tables0.20.40.60.81.0Accuracy (recall)DEEP1B datasetProbes100501051151020304050Number of hash tables0.20.40.60.81.0Accuracy (recall)GIST datasetProbes100501051151020304050Number of hash tables0.20.40.60.81.0Accuracy (recall)SIFT datasetProbes100501051151020304050Number of hash tables0.20.40.60.81.0Accuracy (recall)MNIST datasetProbes100501051best of our knowledge, SANNS, is the most efﬁcient privacy-
preserving protocol for ANN search, achieving performance on
the order of several seconds when evaluated on both DEEP1B
and SIFT (over high-bandwidth network connections), which is
comparable with our work. However, our threat model differs
in that we use two servers, who may not collude, while in
SANNS all parties (clients and server) are expected to be
honest-but-curious. An in-depth comparison is provided below.
Comparison to SANNS. We note that Chen et al. [26] only
evaluate their approach on the DEEP1B and SIFT datasets, and
also use the smaller 10M feature vector version of DEEP1B
in their evaluation. To match the evaluation of SANNS, we
compare with two network settings. We note that the network
conﬁguration used by SANNS has throughput that is faster
than what we were able to measure on localhost using
iperf3 [46], which capped at 3.6 GB/s. The ﬁrst setting has
network throughput between 40 MB/s to 2.2 GB/s, which
we call the “fast” network. The second setting has network
throughput between 500 MB/s to 7 GB/s, which we call
the “localhost” network (Chen et al. [26] refer to this as
the “fast” network in their evaluation). Given these network
conﬁgurations, SANNS is by no means deployable over realistic
network connections [79], which are over 30× slower. Because
SANNS does not provide an open-source implementation, we
use the query times reported in their evaluation and note that
our deployment environment resembles theirs (comparable
CPUs, network, and degree of parallelism applied). We report
the results of our comparison in Table V. Our improvements
Protocol 1
(this work)
SANNS
(localhost)
SANNS
(fast network)
59.7 s (2.1× ↑)
14.2 s (12.9× ↑)
8.06 s (3.5× ↓)
1.55 s (1.4× ↑)
1.77 GB (1180× ↑)
SIFT
28.2 s
Latency (1 CPU):
Latency (32 CPUs): 1.1 s
Communication:
1.5 MB
DEEP1B
170 s
Latency (1 CPU):
Latency (32 CPUs): 6.13 s
Communication:
1.7 MB
30.1 s (5.65× ↓)
4.58 s (1.3× ↓)
181 s (1.1× ↑)
37.2 s (6.1× ↑)
5.53 GB (3250× ↑)
TABLE V: End-to-end comparison between Protocol 1 and SANNS
over a 500 MB/s to 7 GB/s network (localhost) and a fast network (40
MB/s to 2.2 GB/s). We ﬁx L = 20 hash tables and 50 multi-probes
per table (for ≈ 95% accuracy). SANNS is network-dominated and
hence parallelizes less favorably than Protocol 1 (see [26, Table 2]).
Fig. 8: Query latency (including server processing time and network