states [10]. Applications can accurately estimate DTP coun-
ters via a get_DTP_counter API that interpolates the
DTP counter at any moment using TSC counters and the es-
timated DTP clock frequency. Similar techniques are used to
implement gettimeofday(). The details of how a DTP
daemon works and how the API is implemented is standard.
Note that DTP counters of each NIC are running at the same
rate on every server in a DTP-enabled network and, as a re-
sult, software clocks that DTP daemons implement are also
tightly synchronized.
5.2 External Synchronization
We discuss one simple approach that extends DTP to sup-
port external synchronization, although there could be many
other approaches. One server (either a timeserver or a com-
modity server that uses PTP or NTP) periodically (e.g., once
per second) broadcasts a pair, DTP counter and universal
time (UTC), to other servers. Upon receiving consecutive
broadcast messages, each DTP daemon estimates the fre-
quency ratio between the received DTP counters and UTC
values. Next, applications can read UTC by interpolating
the current DTP counter with the frequency ratio in a similar
fashion as the method discussed in Section 5.1. Again, DTP
counters of each NIC are running at the same rate, and as a
result, UTC at each server can also be tightly synchronized
with some loss of precision due to errors in reading system
clocks. It is also possible to combine DTP and PTP to im-
prove the precision of external synchronization further: A
timeserver timestamps sync messages with DTP counters,
and delays between the timeserver and clients are measured
using DTP counters.
5.3
Incremental Deployment
DTP requires the physical layer to be modiﬁed. As a re-
sult, in order to deploy DTP, network devices must be modi-
ﬁed. As there is usually a single switching chip inside a net-
work device [2], the best strategy to deploy DTP is to imple-
ment it inside the switching chip. Then network devices with
DTP-enabled switching chips can create a DTP-enabled net-
work. This would require updating the ﬁrmware, or possibly
replacing the switching chip. PTP uses a similar approach
in order to improve precision: PTP-enabled switches have a
dedicated logic inside the switching chip for processing PTP
packets and PTP-enabled NICs have hardware timestamp-
ing capabilities and PTP hardware clocks (PHC). Therefore,
the cost of achieving the best conﬁguration of PTP is essen-
tially the same as the cost of deploying DTP, as both require
replacing NICs and switches.
An alternative way to deploy DTP is to use FPGA-based
devices. FPGA-based NICs and switches [5, 43] have more
ﬂexibility of updating ﬁrmware. Further, customized PHYs
can be easily implemented and deployed with modern FP-
GAs that are equipped with high-speed transceivers.
One of the limitations of DTP is that it is not possible
to deploy DTP on routers or network devices with multi-
ple line cards without sacriﬁcing precision. Network ports
on separate line cards typically communicate via a bus inter-
S1
S5
S4
S0
S2
S3
Further, we measured the precision of accessing DTP
from software and compared DTP against PTP.
S6
S7
S8
S9
S10
S11
6.1 Evaluation Setup
IBM Switch
DTP-NIC
Timeserver
Mellanox
Figure 5: Evaluation Setup
face. As a result, it is not possible to maintain a single global
counter with high precision over a shared bus, although each
line card can have its own separate global counter. Fortu-
nately, as long as all switches and line cards form a con-
nected graph, synchronization can be maintained.
Replacing or updating switches and NICs in a datacen-
ter at once is not possible due to both cost and availability.
Importantly, DTP can be incrementally deployed: NICs and
a ToR switch within the same rack are updated at the same
time, and aggregate and core switches are updated incremen-
tally from the lower levels of a network topology. Each DTP-
enabled rack elects one server to work as a master for PTP /
NTP. Then, servers within the same rack will be tightly syn-
chronized, but servers from different racks are less tightly
synchronized depending on the performance of PTP / NTP.
When two independently DTP-enabled racks start commu-
nicating via a DTP-enabled switch, servers from two racks
will be tightly synchronized both internally and externally
after communicating BEACON_JOIN messages.
5.4 Following The Fastest Clock
DTP assumes that oscillators of DTP-enabled devices op-
erate within a range deﬁned by IEEE 802.3 standard (Sec-
tion 3.1). However, in practice, this assumption can be bro-
ken, and an oscillator in a network could run at a frequency
outside the range speciﬁed in the standard. This could lead
to many jumps from devices with slower oscillators. More
importantly, the maximum offset between two devices could
be larger than 4TD. One approach to address the problem is
to choose a network device with a reliable and stable oscil-
lator as a master node. Then, through DTP daemons, it is
possible to construct a DTP spanning tree using the master
node as a root. This is similar to PTP’s best master clock
algorithm. Next, at each level of the tree, a node uses the
remote counter of its parent node as the global counter. If an
oscillator of a child node runs faster than its parent node, the
local counter of a child should stall occasionally in order to
keep the local counter monotonically increasing. We leave
this design as a future work.
6. EVALUATION
In this section, we attempt to answer following questions:
• Precision: In Section 3.3, we showed that the precision
of DTP is bounded by 4T D where D is the longest
distance between any two nodes in terms of number
of hops. In this section, we demonstrate and measure
that precision is indeed within the 4T D bound via a
prototype and deployed system.
• Scalability: We demonstrate that DTP scales as the
number of hops of a network increases.
For the DTP prototype and deployment, we used pro-
grammable NICs plugged into commodity servers: We used
DE5-Net boards from Terasaic [3]. A DE5-Net board is an
FPGA development board with an Altera Stratix V [15] and
four Small Form-factor Pluggable (SFP+) modules. We im-
plemented the DTP sublayer and the 10 GbE PHY using the
Bluespec language [1] and Connectal framework [32]. We
deployed DE5-Net boards on a cluster of twelve Dell R720
servers. Each server was equipped with two Xeon E5-2690
processors and 96 GB of memory. All servers were in the
same rack in a datacenter. The temperature of the datacenter
was stable and cool.
We created a DTP network as shown in Figure 5: A tree
topology with the height of two, i.e.
the maximum num-
ber of hops between any two leaf servers was four. DE5-
Net boards of the root node, S0, and intermediate nodes,
S1 ∼ S3, were conﬁgured as DTP switches, and those of the
leaves (S4 ∼ S11) were conﬁgured as DTP NICs. We used
10-meter Cisco copper twinax cables to a DE5-Net board’s
SFP+ modules. The measured one-way delay (OWD) be-
tween any two DTP devices was 43 to 45 cycles (≈ 280 ns).
We also created a PTP network with the same servers as
shown in Figure 5 (PTP used Mellanox NICs). Each Mel-
lanox NIC was a Mellanox ConnectX-3 MCX312A 10G
NIC. The Mellanox NICs supported hardware timestamp-
ing for incoming and outgoing packets which was crucial
for achieving high precision in PTP. A VelaSync timeserver
from Spectracom was deployed as a PTP grandmaster clock.
An IBM G8264 cut-through switch was used to connect the
servers including the timeserver. As a result, the number of
hops between any two servers in the PTP network was al-
ways two. Cut-through switches are known to work well in
PTP networks [52]. We deployed a commercial PTP solu-
tion (Timekeeper [16]) in order to achieve the best precision
in 10 Gigabit Ethernet. Note that the IBM switch was con-
ﬁgured as a transparent clock.
The timeserver multicasted PTP timing information every
second, i.e. the synchronization rate was once per second,
which was the recommended sync rate by the provider. Note
that each sync message was followed by Follow_Up and
Announce messages. Further, we enabled PTP UNICAST
capability, which allowed the server to send unicast sync
messages to individual PTP clients once per second in ad-
dition to multicast sync messages. In our conﬁguration, a
client sent two Delay_Req messages per 1.5 seconds.
6.2 Methodology
Measuring offsets at nanosecond scale is a very challeng-
ing problem. One approach is to let hardware generate pulse
per second (PPS) signals and compare them using an oscil-
loscope. Another approach, which we use, is to measure
the precision directly in the PHY. Since we are mainly inter-
ested in the clock counters of network devices, we developed
a logging mechanism in the PHY.
(cid:41)
(cid:115)
(cid:110)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:32)(cid:50)(cid:53)(cid:46)(cid:54)
(cid:32)(cid:49)(cid:50)(cid:46)(cid:56)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:50)(cid:46)(cid:56)
(cid:45)(cid:50)(cid:53)(cid:46)(cid:54)
(cid:115)(cid:49)(cid:45)(cid:115)(cid:52)
(cid:115)(cid:49)(cid:45)(cid:115)(cid:53)
(cid:115)(cid:49)(cid:45)(cid:115)(cid:48)
(cid:48)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:49)(cid:48)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:49)(cid:49)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:48)
(cid:115)(cid:50)(cid:45)(cid:115)(cid:55)
(cid:115)(cid:50)(cid:45)(cid:115)(cid:56)
(cid:115)(cid:50)(cid:45)(cid:115)(cid:48)
(cid:51)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:32)(cid:52)
(cid:32)(cid:50)
(cid:32)(cid:48)
(cid:45)(cid:50)
(cid:45)(cid:52)
(cid:41)
(cid:115)
(cid:107)
(cid:99)
(cid:105)
(cid:116)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:41)
(cid:115)
(cid:110)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:32)(cid:50)(cid:53)(cid:46)(cid:54)
(cid:32)(cid:49)(cid:50)(cid:46)(cid:56)
(cid:32)(cid:48)
(cid:45)(cid:49)(cid:50)(cid:46)(cid:56)
(cid:45)(cid:50)(cid:53)(cid:46)(cid:54)
(cid:54)
(cid:48)
(cid:115)(cid:49)(cid:45)(cid:115)(cid:52)
(cid:115)(cid:49)(cid:45)(cid:115)(cid:53)
(cid:115)(cid:49)(cid:45)(cid:115)(cid:48)
(cid:115)(cid:50)(cid:45)(cid:115)(cid:55)
(cid:115)(cid:50)(cid:45)(cid:115)(cid:56)
(cid:115)(cid:50)(cid:45)(cid:115)(cid:48)
(cid:51)
(cid:84)(cid:105)(cid:109)(cid:101)(cid:32)(cid:40)(cid:109)(cid:105)(cid:110)(cid:115)(cid:41)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:49)(cid:48)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:49)(cid:49)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:48)
(cid:54)
(cid:32)(cid:52)
(cid:32)(cid:50)
(cid:32)(cid:48)
(cid:45)(cid:50)
(cid:45)(cid:52)
(cid:41)
(cid:115)
(cid:107)
(cid:99)
(cid:105)
(cid:116)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:70)
(cid:68)
(cid:80)
(cid:32)(cid:48)(cid:46)(cid:54)
(cid:32)(cid:48)(cid:46)(cid:53)
(cid:32)(cid:48)(cid:46)(cid:52)
(cid:32)(cid:48)(cid:46)(cid:51)
(cid:32)(cid:48)(cid:46)(cid:50)
(cid:32)(cid:48)(cid:46)(cid:49)
(cid:32)(cid:48)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:57)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:49)(cid:48)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:49)(cid:49)
(cid:115)(cid:51)(cid:45)(cid:115)(cid:48)
(cid:45)(cid:50)
(cid:45)(cid:49)
(cid:49)
(cid:48)
(cid:50)
(cid:79)(cid:87)(cid:68)(cid:32)(cid:45)(cid:32)(cid:100)(cid:101)(cid:108)(cid:97)(cid:121)(cid:32)(cid:40)(cid:116)(cid:105)(cid:99)(cid:107)(cid:115)(cid:41)
(cid:51)
(cid:52)
(a) DTP: BEACON interval = 200.
Heavily loaded with MTU packets.
(b) DTP: BEACON interval = 1200.
Heavily loaded with Jumbo packets.
(c) DTP: Offset distribution from S3.
(BEACON interval = 1200 cycles)
(cid:41)
(cid:100)
(cid:110)
(cid:111)
(cid:99)
(cid:101)
(cid:115)
(cid:111)
(cid:110)
(cid:97)
(cid:110)
(cid:40)
(cid:32)
(cid:116)
(cid:101)
(cid:115)
(cid:102)
(cid:102)
(cid:79)
(cid:54)(cid:52)(cid:48)
(cid:51)(cid:50)(cid:48)
(cid:48)
(cid:45)(cid:51)(cid:50)(cid:48)