[5.41]
19195
[·] denotes the standard deviation.
Table 7: CIFAR-10 Benchmarks for Cryptographic Frameworks
Framework Accuracy
Arch.
ResNet-32
VGG-16
Delphi
Delphi
GForce
Comp. (ms)
Comm. (MB)
Ofﬂine
65.77% 109873
67.81% 178227
777222...888333%
849565
[0.075%]
[3171]
Online Ofﬂine Online
74
373
555000...444777
1397
6296
19197
2600
14200
333555000...111000
[10.51]
Table 8: CIFAR-100 Benchmarks for Cryptographic Frameworks
SHE [15], has the best accuracy in plaintext inference, while
ResNet-32 is slightly better. VGG-16 [22], used by us, sec-
onds in the plaintext accuracy. An early work MiniONN [14]
proposed Architecture A [14, Figure 13], but without report-
ing its accuracy. We implement a slightly modiﬁed version
A-MT that replaces all meanpool layers by maxpool layers
and inserts truncation layers between linear layers. The accu-
racy of A-MT on CIFAR-10 is slightly shy to VGG-16.
Neural networks with a higher accuracy incur a longer
runtime in general. A and A-MT have the shortest runtime.
SHE [15] adopts a convolutional neural network (BatN-CNN
in Table 7) [5] with a similar composition as A-MT, except it
adopts batch normalization layers instead of truncation layers,
and some of its convolution layers have more output channels,
meaning that it is more computationally intensive.
Prior arts also modify architectures to better ﬁt with cryp-
tographic tools. XONN [21] binarizes [20] VGG and prunes
out unimportant weights in convolution layers [16] to reduce
computational cost. Table 7 reports the one with the highest
accuracy (BC5) while more are reported in Figure 7. Del-
phi [18] also tunes architectures by replacing some ReLU
layers by their quadratic approximation. The measured accu-
racy and performance of Delphi are reported in Tables 7 and 8.
Notably, ResNet-18/32 runs faster than VGG-16 in plain-
text5. Still, our VGG-16 outperforms Delphi’s ResNet-32 in
both the accuracy and online runtime for oblivious inference.
VGG-16. We implement VGG-16 [22] for CIFAR-10/100
and train it with SWALP. VGG-16 has 16 convolution lay-
Figure 5: GForce’s Online Runtime of Non-linear Layers
Figure 6: Non-linear Layers Online Runtime, with or without GPU
less than their input size. Section 3.6 explained why maxpool
layers require less than n comparison for n inputs.
Gain from GPU. To see how GPU contributes to the lower
online runtime, we run GPU-DGK, ReLU layers, and maxpool
layers with only CPU. Figure 6 shows that when GPU is not
used, the online runtime of our protocols (dash lines) still
remain in milliseconds level but are much higher than their
GPU-enabled counterparts (solid lines). The gap becomes
wider as the input size increases, which aligns with the goal
of GForce to efﬁciently process DNNs for complicated tasks.
4.2 Oblivious Inference
Datasets. CIFAR-10 contains 10 classes of 32×32 colorful
images. It has 50,000 training images and 10,000 testing
images, each labeled with a class. CIFAR-100 has the same
number of colorful images, but they belong to 100 classes,
which is harder for classiﬁcation since each class has less
training images, and classiﬁers need to learn more classes.
They are popular benchmarks for (plaintext) neural networks4.
Neural Networks. The neural network architecture is cen-
tral to the runtime and accuracy of inference. Among the lists
in Tables 7-8, ResNet-32/18 [10], used by Delphi [18] and
4More than 100 machine-learning papers compete for higher accuracy on
them (https://paperswithcode.com/sota/image-classiﬁcation-on-cifar-10 and
https://paperswithcode.com/sota/image-classiﬁcation-on-cifar-100). CIFAR
datasets are arguably harder than MNIST evaluated in prior works [14].
5This result is from https://github.com/jcjohnson/cnn-benchmarks. We
adopt VGG-16 since SWALP provides off-the-shelf training code [28] for
it. Also, VGG-16’s convolution layers are easier to implement since its
stride = 1. It does not mean GForce cannot realize ResNet.
USENIX Association
30th USENIX Security Symposium    2157
210211212213214215216217218NumberofInputElements101102Runtime(ms)GPU-DGKProtocolReLULayerusingGPU-DGKMaxpool(2×2)LayerusingGPU-DGKSRTLayer213214215216217218NumberofInputElements050100150200250300Runtime(ms)GPU-DGKProtocolOnline/OﬄineDGKProtocolw/oGPUReLULayerwithGPUReLULayerw/oGPUMaxpool(2×2)LayerwithGPUMaxpool(2×2)Layerw/oGPUNote: The closer to the upper left corner the better
Figure 7: Accuracy and Online Latency on CIFAR-10
Note: The closer to the upper left corner the better
Figure 8: Accuracy and Online Latency on CIFAR-100
ers and 3 fully-connected layers and has widespread use
in medical diagnosis. Combining with SWALP, our VGG-
16 attains 93.12% accuracy on CIFAR-10 (Table 7). Our
accuracy outperforms almost all other cryptographic solu-
tions [11, 13, 14, 18, 21], except SHE [15]’s ResNet-18 [10].
However, SHE’s ResNet-18 performs impractically slow (tak-
ing more than 3 hours, the slowest among all other solutions).
Figure 7 compares both the accuracy and latency.
Delphi [18] trained several neural networks that trade accu-
racy for performance. Our latency is lower than Delphi’s, and
our most accurate DNN can attain an accuracy higher by at
least 5pp than the best of Delphi [18]. Figure 8 plots all the
reported accuracy-runtime data. We have a higher accuracy
and shorter online runtime than all Delphi’s neural networks.
CIFAR-100. To further examine GForce on handling com-
plicated tasks, we test it on CIFAR-100 with 100 classes, 600
images each. Running on VGG-16, we achieve 72.83% ac-
curacy in 350ms. Compared to Delphi [18]’s ResNet-32, our
VGG-16 is at least 5pp more accurate. Compared to even the
fastest DNN of Delphi [18], GForce is still 6.23× faster.
Figure 9: Dependency Graph of Protocols (and their Security Proof)
shows that GForce is an order of magnitude faster than Delphi.
We reckon that we still have an edge even if its runtime would
be halved. Note that accuracy is also our goal.
Comparison based on Existing Architecture (A). Without
the learned parameters of architecture A, we cannot guarantee
GForce’s plaintext is large enough to provide the same accu-
racy. Instead, we produce a trained DNN via SWALP with a
similar architecture A-MT, which attains 90.82% accuracy on
CIFAR-10. Compared with MiniONN, Gazelle, and Falcon,
GForce attains the shortest online latency and reduces it by
489×, 24×, and 20×, respectively. Figure 7 further illustrates
GForce’s improvement on CIFAR-10 over other frameworks.
5 Security Analysis
5.1 Threat Model and Protection Scope
We consider probabilistic polynomial-time (PPT) honest-but-
curious adversary that controls the communication and either
the server or the client. GForce protects the most sensitive
information of the network except its architecture and hyper-
parameters, which are costly to hide. Speciﬁcally, GForce
hides the learnable parameters and the kernel size of convo-
lution layers from the client, the query’s inputs and outputs
from the server, and all the intermediate results of non-output
layers from both parties. However, it leaks about DNN’s ar-
chitecture Archi, such as the intermediate outputs’ size, the
type of each layer, and the window size of pooling layers.
All in all, we have the same privacy guarantee as previous
works [11, 13, 14], modulo our unique SRT layers. Each of
the nT SRT layers has a divisor parameter di, which is always
a power-of-2 and within [20,220]. Quantitatively, it means
log2(21) ≈ 4.4 bits of information, whereas the weights of
the nL linear layers (denoted by {Mi}i∈[1:nL]), which GForce
can protect from the client, carry at least kilobytes or even
megabytes of information. While there seem no inference at-
tacks exploiting such divisors, it may deserve closer scrutiny.
Comparison with Delphi [18]. We quoted Delphi [18]’s
runtime from its paper, which would be lower if it ran in
a LAN setting. (Their experiments run two VMs located in
different regions of AWS with >20ms network delay.) Table 8
5.2 Overview of Security
GForce composes of many cryptographic protocols, and each
can be derived from other sub-protocols. Figure 9 shows their
2158    30th USENIX Security Symposium
USENIX Association
102103104105106107OnlineLatency(ms)0.820.840.860.880.900.920.94AccuracyDelphiXONNSHEMiniONNGazelleFalconGForce:VGG-16GForce:A-MT103104OnlineLatency(ms)0.660.680.700.72AccuracyDelphiGForce:VGG-16dependency with arrows from the building blocks to the higher
protocols. Following this graph and relying on other proofs,
we have the following theorem on the security of GForce.
Theorem 3. GForce’s oblivious inference, as a composition
of protocols SOS, GPU-DGK, and GPU-Trun over different
neural-network layers (third row of Figure 1), is secure:
• A corrupted server’s view can be generated by a PPT
simulator SimS(Archi,{di}i∈[1:nT],{Mi}i∈[1:nL]).
• A corrupted client’s view can be generated by a PPT sim-
ulator SimC((x, skAHE), out, (Archi,{di}i∈[1:nT])), where
x is the query and out = DNN(x) is the query result.
For all protocols, the simulators of both kinds (for a cor-
rupted client or a corrupted server) also take the following
inputs implicitly, which include the description of the cryp-
tographic groups used (e.g., the security parameter), the di-
mensional information (e.g., Zk
q in AHE-to-SOS transformed
protocol or Zq in pure-AHE protocols), and public key pkAHE.
The above spells out the relevant parts of DNN(·) required
for SimS, the simulator for the corrupted server. Note that the
server is run by the model owner and is supposed to know
{Mi}. For brevity, we suppose it is the client who gets the ﬁnal
output out. For many sub-protocols, out will be secret-shared
across the server and the client, which can be simulated easily.
Our AHE-to-SOS transformation plays a central role in
GForce for deriving many of its sub-protocols. The security
proof of AHE-to-SOS transformation is in Appendix D.1.
We prove the security of SC-DGK and GPU-DGK (Proto-
cols 1-2) in Appendices D.2-D.4. We only state the security
guarantees of GPU-Wrap and GPU-Trun (Protocols 3-4) but
postpone their proof to our full version (which is straightfor-
ward given the security of additive SS and AHE).
6 Complementing the Other Frameworks
High-Throughput HE Implementations. We aim for online
performance, so we did not optimize for the HE-dominated
ofﬂine phase. One can employ a more efﬁcient HE imple-
mentation (e.g., those used by Falcon/Gazelle) with a more
compact encoding or has been optimized for GPU (e.g., as
used by HCNN [2]). We can also integrate GForce with HE
compilers that aim for high inference throughput (e.g., [6]).
Integration with Delphi [18]. Adopting our GPU-friendly
comparison protocols can improve Delphi’s performance for
its maxpool layers and the remaining ReLU layers.
Oblivious Decision-Tree Inference. For a decision tree,
inference proceeds to the left child if the query satisﬁes the
predicate of a node; right otherwise. Tai et al. [23] proposed
the ﬁrst approach solely based on AHE that does not need to
pad a sparse tree. Their path-cost trick has been utilized in a
few subsequent works. In essence, the server runs DGK proto-
col for each node to produce an AHE-encrypted comparison
result bit and adds up these bits for each possible path, which
can be readily replaced by our protocols instead.
7 More on Related Works
Gazelle [11] and Falcon [13] use GC for non-linear layers,
which heavily relies on AES-NI on CPU for a decent per-
formance [4], with no GPU-friendly counterpart. They also
propose a compact encoding to speed up operations of leveled-
homomorphic encryption [8]. Falcon [13] aims to improve
the linear computations of Gazelle by a Fourier transform-
based approach. The best result (by Falcon) takes >2.88s for
a CIFAR-10 recognition at <81.62% accuracy.
XONN [21] restricts inference to binarized neural networks
(BNN) with conﬁned ({−1,1}) weights in linear layers and
only binary activation functions. It thus manages to use only
GC (except for the ﬁrst layer), which reduces the communica-
tion rounds and the total (ofﬂine + online) runtime to 5.79s for
CIFAR-10 image classiﬁcation at 81% accuracy (cf., Falcon’s
7.22s for ∼81.5%). However, using BNN requires a wider
neural network to maintain the accuracy, leading to a longer
latency. In particular, for 88% accuracy, it takes ∼2 minutes.
HCNN [2] and Plaid-HE [6] adopt GPU-optimized AHE
implementation, but it can only handle non-linear layers with
approximation, sacriﬁcing accuracy. Also, the overhead due
to AHE is still large. Our AHE-to-SOS approach remains
beneﬁcial for moving the AHE-related operations ofﬂine and
supporting common non-linear layers without approximation.
Using GPU is also a relatively new idea for SGX-based
frameworks. Slalom [24] securely outsources linear operation
from SGX to untrusted GPU for inference. Goten [19] solves
the challenges in supporting private learning left by Slalom.
8 Conclusion
GForce is an efﬁcient oblivious inference protocol that works
over a low-precision integer domain while maintaining high
accuracy. For this, we adopt SWALP [28] and formulate
stochastic rounding and truncation layers that fuse multiple
operations SWALP needs for efﬁciency and accuracy. We also
propose cryptographic protocols for leveraging GPU paral-
lelism even for non-linear layers, which reduce the online la-
tency and communication cost by orders of magnitude. These
are validated by our evaluation comparing prior frameworks.
We hope that this work can inspire further research of
machine-learning experts to devise new algorithms compati-
ble with ﬁnite ﬁelds used in cryptography and stimulate cryp-
tographers to propose more GPU-friendly protocols.
With a secret-sharing-based design, it seems promising to
explore if GForce can be extended to secure outsourced infer-
ence [17] or training, say, by using 3 non-colluding servers.
USENIX Association
30th USENIX Security Symposium    2159
References
[1] Gilad Asharov, Abhishek Jain, Adriana López-Alt, Eran
Tromer, Vinod Vaikuntanathan, and Daniel Wichs. Mul-
tiparty computation with low communication, computa-
tion and interaction via threshold FHE. In EUROCRYPT,