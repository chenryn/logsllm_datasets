5.6 Addressing the Computational Scalability
Because CDNs are complex systems, the response time may de-
pend on a large number of variables, and the dataset might com-
prise hundreds of millions of requests spanning a large multi-
dimensional space. To efﬁciently evaluate the what-if scenarios,
WISE must address how to efﬁciently organize and utilize the
dataset. In this section, we discuss our approach to these problems.
1. Curse of Dimensionality: As the number of dimensions (vari-
ables in the dataset) grow, exponentially more data is needed for
similar accuracy of estimation. WISE uses the CBN to mitigate this
problem. In particular, because when conditioned on its parents,
a variable is independent of all variables except its descendants,
we can use only the parents of the target variable in the regression
function. Because the cardinality of the parent-set would typically
be less than the total number of variables in the dataset, the accu-
racy of the regression function is signiﬁcantly improved for a given
amount of data. Due to this, WISE can afford to use fewer training
data points with the regression function and still get good accuracy.
Also, because the time complexity for the KR method is O(kn3),
with k variables and n points in the training dataset, WISE’s tech-
nique results in signiﬁcant computational speedup.
2. Overﬁtting: The density of the dataset from a real deployment
can be highly irregular; usually there are many points for combi-
nations of variable values that represent the normal network oper-
ation, while the density of dataset is sparser for combinations that
represent the fringe cases. Unfortunately, because the underlying
principle of most regression techniques is to ﬁnd parameters that
minimize the errors on the training data, we can end up with param-
eters that minimize the error for high density regions of the dataset
but give poor results in the fringes—this problem is called overﬁt-
ting. The usual solution to this problem is introducing a smooth-
ness penalty in the objective function of the regression method, but
ﬁnding the right penalty function requires cross-validation, which
is usually at least quadratic in the size of the global dataset1. In the
case of CDNs, even one day of data may contain entries for mil-
lions of requests, which makes the quadratic complexity of these
algorithms inherently unscalable.
WISE uses piece-wise regression to address this problem. WISE
divides the dataset into small pieces, that we refer to as tiles and
performs regression independently for each tile. WISE further
prunes the global dataset to produce a training dataset so that the
density of training points is more or less even across all the tiles.
To decompose the dataset, WISE uses ﬁxed-size buckets for each
dimension in the dataset for most of the variable value space. If the
bucket sizes are sufﬁciently small, having more data points beyond
a certain threshold does not contribute appreciably to the response-
time prediction accuracy. With this in mind, WISE uses two thresh-
olds nmin and nmax and proceeds with tile boundaries as follows.
WISE decides on a bucket width bi along each dimension i, and
forms boundaries in the space of the dataset at integer multiples of
bi along each dimension; for categorical variables, WISE uses each
category as a separate bucket. For each tile, WISE obtains a uniform
random subset of nmax points that belong in the tile boundaries
from the global dataset and adds them to the training dataset. If the
dataset has fewer than nmin data points for the tile, the tile bound-
aries are folded to merge it with neighboring tile. This process
is repeated until the tile has nmin number of points. Ultimately,
most of the tiles have regular boundaries, but for some tiles, espe-
cially those on the fringes, the boundaries can be irregular. Once
the preparation of training data is complete, we use cross-validation
to derive regression parameters for each tile; the complexity is now
only O(n2
max) for each tile.
3. Retrieval of Data: With large datasets, even the mundane tasks,
such as retrieving training and test data during the input distribution
preparation and response-time estimation phases are challenging.
Quick data retrieval and processing is imperative here because both
of these stages are online, in the sense that they are evaluated when
the designer speciﬁes the scenario.
WISE expedites this process by intelligently indexing the training
data off-line and the test data as it is created. Tiles are used here as
well: Each tile is assigned a tile-id, which is simply a string formed
by concatenating the tile’s boundaries in each dimension. All the
data points that lie in the tile boundaries are assigned the tile-id as a
key that is used for indexing. For the data preparation stage, WISE
performs the tile-id assignment and indexing along the dimensions
comprising the parents of most commonly used variables, and for
the regression phase, the tiling and indexing is performed for the
dimensions comprising the parents of the target variable. Because
the tile-ids use ﬁxed length bins for most of the space, mapping of
a point to its tile can be performed in constant time for most of the
data-points using simple arithmetic operations.
4. Parallelization and Batching: We have carefully designed the
various stages in WISE to support parallelization and batching of
jobs that use similar or same data. In the training data preparation
stage, each entry in the dataset can be independently assigned its
tile-id based key because WISE uses regular sized tiles. Similarly,
the regression parameters for each tile can be learned independently
1Techniques such as in [10] can reduce the complexity for such N-
body problems but are still quite complex than the approximations
that WISE uses.
and in parallel. In the input data preparation stage, WISE batches
the test and training data that belong in a tile and fetch the data from
the training data for all of these together. Finally, because WISE
uses piece-wise regression to evaluate the effects of intervention,
it can batch the test and training data for each tile; further because
the piece-wise computation are independent, they can take place in
parallel.
6.
IMPLEMENTATION
We have implemented WISE with the Map-Reduce frame-
work [16] using the Sawzall logs processing language [22] and
Python Map-Reduce libraries. We chose this framework to best ex-
ploit the parallelization and batching opportunities offered by the
WISE design2. We have also implemented a fully-functional proto-
type for WISE using a Python front-end and a MySQL backend that
can be used for small scale datasets. We provide a brief overview
of the Map-Reduce based implementation here.
Most steps in WISE are implemented using a combination of one
or more of the four Map-Reduce patterns shown in Figure 5. WISE
uses ﬁlter pattern to obtain conditional subsets of dataset for var-
ious stages. WISE uses the Tile-id Assignment pattern for prepar-
ing the training data. We set the nmin and nmax thresholds to
20 and 50, respectively to achieve 2-5% conﬁdence intervals. In
the input data preparation phase, the use-statement is implemented
using the ﬁlter pattern. The update-statements use update pattern
for applying the new values to the variable in the statement. If the
update-statement uses the INTERVENE keyword then WISE uses
the Training & Test Data Collation pattern to bring together the
relevant test and training data and update the distribution of the test
data in a batched manner. Each update- statement is immediately
followed by the Tile-id Assignment pattern because the changes in
the value of the data may necessitate re-assignment of the tile-id.
Finally, WISE uses the Training & Test Data Collation pattern for
piece-wise regression. Our Map-Reduce based implementation can
evaluate typical scenarios in about 5 minutes on a cluster of 50 PCs
while using nearly 500 GB of training data.
7. EVALUATING WISE FOR A REAL CDN
In this section, we describe our experience applying WISE to a
large dataset obtained from Google’s global CDN for Web-search
service. We start by brieﬂy describing the CDN and the service
architecture. We also describe the dataset from this CDN and the
causal structure discovered using WCD. We also evaluate WISE’s
ability to predict response-time distribution for the what-if scenar-
ios.
7.1 Web-Search Service Architecture
Figure 6(a) shows Google’s Web-search service architecture.
The service comprises a system of globally distributed HTTP re-
verse proxies, referred to as Front End (FE) and a system of glob-
ally distributed clusters that house the Web servers and other core
services (the Back End, or BE). A DNS based request redirection
system redirects the user’s queries to one of the FEs in the CDN.
The FE process forwards the queries to the BE servers, which gen-
erate dynamic content based on the query. The FE caches static
portions of typical reply, and starts transmitting that part to the re-
questing user as it waits for reply from the BE. Once the BE replies,
the dynamic content is also transmitted to the user. The FE servers
may or may not be co-located in the same data center with the BE
Figure 5: Map-Reduce patterns used in WISE implementation.
servers. If they are co-located, they can be considered to be on the
same local area network and the round-trip latency between them is
only a few milliseconds. Otherwise, the connectivity between the
FE and the BE is typically on a well-provisioned connection on the
public Internet. In this case the latency between the FE and BE can
be several hundred milliseconds.
The server response time for a request is the time between the
instance when the user issues the HTTP request and the instance
when the last byte of the response is received by the users. We esti-
mate this value as the sum of the round-trip time estimate obtained
from the TCP three-way handshake, and the time between the in-
stance when the request is received at the FE and when the last byte
of the response is sent by the FE to user. The key contributors to
server response time are: (i) the transfer latency of the request from
the user to the FE (ii) the transfer latency of request to the BE and
the transfer latency of sending the response from the BE to the FE;
(iii) processing time at the BE, (iv) TCP transfer latency of the re-
sponse from the FE to the client; and (v) any latency induced by
loss and retransmission of TCP segments.
Figure 6(b) shows the process by which a user’s Web search
query is serviced. This message exchange has three features that
affect service response time in subtle ways, making it hard to make
accurate “back-of-the-envelop calculations” in the general case:
1. Asynchronous transfer of content to the user. Once the TCP
handshake is complete, user’s browser sends an HTTP request con-
taining the query to the FE. While the FE waits on a reply from
the BE, it sends some static content to the user; this content—
essentially a “head start” on the transfer—is typically brief and
constitutes only a couple of IP packets. Once the FE receives the
response from the BE, it sends the response to the client and com-
pletes the request. A client may use the same TCP connection for
subsequent HTTP requests.
2Hadoop [11] provides an open-source Map-Reduce library. Mod-
ern data-warehousing appliances, such the ones by Netezza [18],
can also exploit the parallelization in WISE design.
2. Spliced TCP connections. FE processes maintain several TCP
connections with the BE servers and reuse these connections for
forwarding user requests to the BE. FE also supports HTTP pipelin-
ts
region
be
tod
fe
sB
(a) Google’s Web-search Service Architecture
febe_rtt
rtt
sP
cP
be_time
crP
srP
rt
Figure 7: Inferred causal structure in the dataset. A → B
means A causes B.
and regions. Another unexpected relationship is between region,
cP and sP attributes; we found that this relationship exists due to
different MTU sizes in different parts of the world. Our dataset,
unfortunately, did not have load, utilization, or data center capacity
variables that could have allowed us to model the be_time variable.
All we observed was that the be_time distribution varied some-
what among the data centers. Overall, we ﬁnd that WCD algorithm
not only discovers relationships that are faithful to how networks
operate but also discovers relationships that might escape trained
network engineers.
Crucially, note that many variables are not direct children of the
region, ts, f e or be variables. This means that when conditioned
on the respective parents, these variables are independent of the re-
gion, time, choice of FE and BE, and we can use training data from
past, different regions, and different FE and BE data centers to esti-
mate the distributions for these features! Further, while most of the
variables in the dataset are correlated, the in-degree for each vari-
able is smaller than the total number of variables. This reduces the
number of dimensions that WISE must consider for estimating the
value of the variables during scenario evaluation, allowing WISE to
produce accurate estimates, more quickly and with less data.
7.4 Response-Time Estimation Accuracy
Our primary metric for evaluation is prediction accuracy. There
are two sources of error in response-time prediction: (i) error in
response-time estimation function (Section 5.5) and (ii) inaccurate
input, or error in estimating a valid input distribution that is repre-
sentative of the scenario (Section 5.4). To isolate these errors, we
ﬁrst evaluate the estimation accuracy alone and later consider the
overall accuracy for a complete scenario in Section 7.5.
To evaluate accuracy of the piece-wise regression method in
isolation we can try to evaluate a scenario: “What-if I make no
changes to the network?” This scenario is easy to specify with
WSL by not including any optional scenario update statements.
For example, a scenario speciﬁcation with the following line:
USE WHERE country==deu
would produce an input distribution for the response-time estima-
tion function that is representative of users in Germany without any
error and any inaccuracies that arise would be due to regression
method. To demonstrate the prediction accuracy, we present re-
sults for three such scenarios:
(a) USE WHERE country==deu
(b) USE WHERE country==zaf
(b) Message Exchange
Figure 6: Google’s Web-search service architecture and mes-
sage exchange for a request on a fresh TCP connection.
ing, allowing the user to have multiple pending HTTP requests on
the same TCP connection.
3. Spurious retransmissions and timeouts. Because most Web
requests are short TCP transfers, the duration of the connection is
not sufﬁcient to estimate a good value for the TCP retransmit timer
and many Web servers use default values for retransmits, or esti-
mate the timeout value from the initial TCP handshake round-trip
time. This causes spurious retransmits for users with slow access
links and high serialization delays for MTU sized packets.
7.2 Data
We use data from an existing network monitoring infrastructure
in Google’s network. Each FE cluster has network-level sniffers,
located between the FE and the load-balancers, that capture trafﬁc
and export streams in tcpdump format. A similar monitoring infras-
tructure captures trafﬁc in the BE. Although the FE and BE servers
use NTP for time synchronization, it is difﬁcult to collate the traces
from the two locations using only the timestamps. Instead, we use
the hash of each client’s IP, port and part of query along with the
timestamp to collate the request between the FE and the BE. WISE
then applies the relevance tests (ref. Sec. 5.1) on the features in the
dataset collected in this manner. Table 1 describes the variables that
WISE found to be relevant to the service response-time variable.
7.3 Causal Structure in the Dataset
To obtain the causal structure, we use a small sampled data sub-
set collected on June 19, 2007, from several data center locations.
This dataset has roughly 25 million requests, from clients in 12,877
unique ASes.
We seed the WCD algorithm with the region and ts variables as
the no-cause variables. Figure 7 shows the causal structure that
WCD produces. Most of the causal relationships in Figure 7 are
straightforward and make intuitive sense in the context of network-
ing, but a few relationships are quite surprising. WCD detects a
relationship between the region and sB attribute (the size of the
result page); we found that this relationship exists due to the differ-
ences in the sizes of search response pages in different languages
Feature
ts, tod
sB
sP
cP
srP
crP
region
fe, be
rtt
febe_rtt
be_time
rt
Description
A time-stamp of instance of arrival of the request at the FE. We also extract the hourly time-of-day (tod) from the timestamp.
Number of bytes sent to the user from the server; this does not include any data that might be retransmitted or TCP/IP header bytes.
Number of packets sent by the server to the client excluding retranmissions.
Number of packets sent by the client that are received at the server, excluding retransmissions.
Number of packets retransmitted by the server to the client, either due to loss, reordering, or timeouts at the server.
Number of packets retransmitted by the client that are received at the server.
We map the IP address of the client to a region identiﬁer at the country and state granularity. We also determine the /24(s24) and /16(s16)
network addresses and the originating AS number. We collectively refer to these attributes as region.
Alphanumeric identiﬁers for the FE data center at which the request was received and the BE data center that served the request.
Round-trip time between the user and FE estimated from the initial TCP three-way handshake.
The network level round-trip time between the front end and the backend clusters.