title:CONMan: a step towards network manageability
author:Hitesh Ballani and
Paul Francis
CONMan: A Step Towards Network Manageability
Hitesh Ballani
Cornell University
Ithaca, NY
PI:EMAIL
Paul Francis
Cornell University
Ithaca, NY
PI:EMAIL
ABSTRACT
Networks are hard to manage and in spite of all the so called
holistic management packages, things are getting worse. We
argue that the diﬃculty of network management can partly
be attributed to a fundamental ﬂaw in the existing architec-
ture: protocols expose all their internal details and hence,
the complexity of the ever-evolving data plane encumbers the
management plane. Guided by this observation, in this paper
we explore an alternative approach and propose Complexity
Oblivious Network Management (CONMan), a network ar-
chitecture in which the management interface of data-plane
protocols includes minimal protocol-speciﬁc information. This
restricts the operational complexity of protocols to their im-
plementation and allows the management plane to achieve
high level policies in a structured fashion. We built the CON-
Man interface of a few protocols and a management tool that
can achieve high-level conﬁguration goals based on this in-
terface. Our preliminary experience with applying this tool
to real world VPN conﬁguration indicates the architecture’s
potential to alleviate the diﬃculty of conﬁguration manage-
ment.
Categories and Subject Descriptors: C.2.3 [Network
Operations]: Network Management.
General Terms: Management.
Keywords: Management, Abstraction, Conﬁguration.
1.
INTRODUCTION
IP networks are hard to manage. Network management
(installation, conﬁguration, provisioning, monitoring, testing,
debugging) requires detailed knowledge of many diﬀerent net-
work components, each with its own management interface.
To cope, network managers rely on a host of tools ranging
from sophisticated centralized network management packages
to home-brewed scripts and elementary tools such as ping and
traceroute. For instance, Cornell’s IT group uses half a dozen
diﬀerent tools, commercial and public domain, and has over
100K lines of scripts for managing the switch and router in-
frastructure alone (not including email, servers, DNS, DHCP,
billing, etc.). In spite of their ever increasing sophistication,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’07, August 27–31, 2007, Kyoto, Japan.
Copyright 2007 ACM 978-1-59593-713-1/07/0008 ...$5.00.
management tools seem to be waging a losing battle which
is shown by rising management costs and network downtime.
A recent survey [18] showed that 80% of the IT budget in
enterprises is devoted to maintain just the status quo - in
spite of this, conﬁguration errors account for 62% of network
downtime.
We believe that the management troubles of the Internet
have been aggravated by the lack of research on fundamentals.
Instead, there is an increasing reliance on temporary “band-
aids”. While this has allowed a number of ﬂaws to creep into
the way we manage networks, in this paper we focus on one
speciﬁc shortcoming:
Today, protocols and devices expose their internal de-
tails leading to a deluge of complexity that burdens
the management plane.
For instance, it is not uncommon for a network device to
have thousands of manageable objects. A review of SNMP
MIB modules found more than 13,000 MIB objects in IETF
MIBs alone [34]; MIBDepot [49] lists 6200 MIBs from 142
vendors for a total of nearly a million MIB objects. A single
router conﬁguration ﬁle can consist of more than 10,000 com-
mand lines [39]. Encumbering the management plane with all
this complexity leads to these problems:
• Perception diﬀers from reality. Management applications
need to eﬀectively reverse engineer the capabilities and the
functionality of protocols and devices from their detailed
MIBs. The low-level and non-intuitive nature of these pa-
rameters makes this task diﬃcult, if not impossible [28].
• Error-prone conﬁguration. Network conﬁguration involves
mapping high-level policies and goals to the values of proto-
col parameters. Since management applications don’t have
an understanding of the underlying network in the ﬁrst
place, they often resort to a cycle of setting the parame-
ters and correlating events to see if the high level goal was
achieved or not. Apart from being haphazard, the noise
in measurements and correlations is often the root-cause
of misconﬁgurations and related errors. The inability to
understand the network’s operation also makes debugging
these errors very diﬃcult [21].
• Fragmentation of tools. Since devices and their exposed
details keep evolving at a frantic pace, management ap-
plications tend to lag behind the power curve [26]. Ad-
ditionally, the inability of standard management interfaces
(IETF MIBs) to keep pace with data plane development
has led to a plethora of vendor speciﬁc MIBs and even ven-
dor speciﬁc management applications and has put us in a
situation where no one management approach suﬃces. For
example, SNMPLink [31] lists more than 1000 management
applications, many of them being vendor speciﬁc command
line or HTML-based tools. Hence, the Internet manage-
ment plane doesn’t have anything analogous to the IP “thin
waist” around which the Internet data-plane is built.
• Lack of dependency maintenance. Management state is
highly inter-dependent. These dependencies are not re-
ﬂected in the existing set-up; thus, when a low-level value
changes, the appropriate dependent changes don’t always
happen [28].
Instances of improper ﬁltering because the
address assigned to some machine changed, or the applica-
tion was started on some other port are very common. Re-
cent work details the challenges involved in tracking such
dependencies in the existing set-up [29] and gives exam-
ples of how failure to track them leads to problems in large
networks [20].
These shortcomings indicate that an (extreme) alternative
worth exploring is to conﬁne the operational complexity of
protocols to their implementation. As a matter of fact, we
observe that almost all data-plane protocols share some very
basic characteristics that should, in theory, suﬃce for the
management of the network. Guided by this observation, we
adopt a more modest approach and argue that:
The management
interface of data-plane protocols
should contain as little protocol-speciﬁc information as
possible.
This allows all data-plane protocols to have a generic yet
simple management interface. While such an approach can
be applied to all aspects of management, this paper restricts
itself to Conﬁguration Management. Hence, in this paper we
present the design and implementation of a network archi-
tecture, Complexity Oblivious Network Management (CON-
Man), that incorporates this principle for network conﬁgu-
ration tasks. In CONMan, all protocols and devices express
their capability and their functionality using a generic ab-
straction. This allows the management plane to understand
the potential of the underlying network and to conﬁgure it in
line with the desired high-level policies without being encum-
bered by the details of the protocol/device implementation.
Having a ﬁxed interface between the management plane and
the data plane also allows for independent evolution of the
two. To this eﬀect, this paper makes the following contribu-
tions:
• We present the detailed design of a network architecture
that minimizes the protocol-speciﬁc information in the man-
agement interface of data-plane protocols. We also present
protocol-independent conﬁguration primitives that can be
used to interact with this interface and hence, conﬁgure the
network.
• We describe the implementation of the management inter-
face of a few protocols in compliance with the proposed
architecture.
• We detail the implementation of a management application
that, given the abstraction of the protocols and devices in
the network, can achieve high-level conﬁguration goals us-
ing the aforementioned primitives.
• The paper presents the use of CONMan in a real-world
conﬁguration scenario (VPN conﬁguration) to highlight its
advantages over the status quo. Further, we also use a naive
but hopefully informative metric to compare the protocol
agnosticity of CONMan conﬁgurations against today’s con-
ﬁgurations in three diﬀerent scenarios (GRE tunnels, MPLS
LSPs and VLANs).
Note that CONMan doesn’t reduce the total system com-
plexity; it only attempts to correct the skewed division of
functionality between management done inside the managed
device and that done outside the managed device. While
the fact that management applications don’t have to deal
with myriad protocol details reduces their burden, proto-
cols still need various low-level details in order to operate.
With CONMan, it is the protocol implementation that uses
the high-level primitives invoked by the management appli-
cations and out-of-band communication with other protocols
to determine these. This, in eﬀect, puts the responsibility for
detailed understanding of protocol operation on the protocol
implementor. Since the protocol implementer requires this
knowledge in any event, this seems to be a smarter place-
ment of functionality.
2. CONMan ARCHITECTURE
Our architecture consists of devices (routers, switches, hosts,
etc.) and one or more network managers (NMs). A NM is
a software entity that resides on one of the network devices
and manages some or all of them. Each device has a glob-
ally unique, topology independent identiﬁer (device-id) that
can carry cryptographic meaning (for example, by hashing a
public key). Each device also has an internal management
agent (MA) that is responsible for the device’s participation
in the management plane. While the rest of the paper talks
about a device performing management tasks, in actuality it
is the device’s MA that is responsible for these. All protocols
and applications in devices are modeled as protocol modules.
Each protocol module has a name as well as an identiﬁer that
is unique within the device. Examples of module names in-
clude “IPv4”, “RFC791”, or even a URI (which might be use-
ful for naming applications). Thus, modules can be uniquely
referred to using tuples of the form .
2.1 Management Channel
As mentioned in section 1, a number of ﬂaws aﬄict the way
we manage networks. One such ﬂaw is that the existing man-
agement plane depends on the data plane [6,14]. For example,
SNMP operates on top of the data plane and hence, manage-
ment protocols rely on the correct operation of the very thing
they are supposed to manage.
In recent work, Greenberg
et. al. [14] discuss the implications of this dependency loop
and propose a technique for achieving a self-bootstrapping,
operationally independent management plane. While such
management plane independence can be established using a
few other approaches (for instance, a more generalized and
self-bootstrapping version of the separate management net-
work that is used by some large ISPs), we agree with their
basic hypothesis and in this paper assume the presence of a
management channel. This management channel should be
independent of the data-plane, should not require any pre-
conﬁguration and should allow devices in the network to com-
municate with the NM. However, we do not dictate whether
the management channel operates or does not operate over
the same physical links as used by the data-plane.
2.2 Overview
Our approach derives from two key observations: First, the
main purpose of a network is to provide paths between certain
applications on certain hosts while preventing certain other
applications and hosts from using those paths.1 Second, we
observe that most data-plane protocols have some basic char-
acteristics whose knowledge should suﬃce for conﬁguring the
aforementioned paths. For instance, most protocols have the
ability to connect to certain other protocols, to switch pack-
ets, ﬁlter packets, queue packets and so on. We believe that it
is these basic characteristics that should serve as the narrow
waist for Internet’s management plane. Consequently, the
management plane only maps the high-level communication
goal into the path through the network (i.e. which proto-
cols should connected and how) and the protocols themselves
ﬁgure out the low-level parameters that they need to operate.
In our proposal, we try to capture these basic characteris-
tics using a generic abstraction called the Module Abstraction
– all protocol modules in CONMan self-describe themselves
using this abstraction. To this eﬀect, we model every proto-
col module as a node with connections to other nodes, certain
generic switching capabilities, certain generic ﬁltering capa-
bilities, certain performance and security characteristics, and
certain dependencies (ﬁgure 1). Thus, the abstraction de-
scribes what the protocol is capable of (potential) and what
it depends on (dependencies). Further, the module can be
conﬁgured to operate in a certain fashion (actual) by manip-
ulating its abstraction using the CONMan primitives. Such
modeling of protocols using a generic abstraction decouples
the data and the management plane so that they can evolve
independently of each other.
Each device in the network uses the management channel
to inform the NM of its physical connectivity, all modules
that it contains, and their respective module abstractions.
The module abstraction allows the NM to understand exactly
how packets may ﬂow (or not ﬂow) through a given module.
This provides the NM with the real picture of the network -
it does not need to reverse engineer numerous low-level and
non-intuitive parameters.
Given the network’s real picture and the high-level goals
and policies that need to be satisﬁed, the NM builds a graph
of modules in various devices that satisfy these. This graph
captures how each module should operate. The NM can then
use the management channel to invoke the appropriate CON-
Man primitives and conﬁgure the modules accordingly. Thus,
the NM can conﬁgure the entire network from the ground
up with (almost) no protocol-speciﬁc knowledge. We believe
that such as approach would ease network conﬁguration and
in general, ameliorate a lot of the problems aﬄicting network
management today.
2.3 Module Abstraction
There are two kinds of modules: data plane modules and
control plane modules. Examples of data plane modules (or
data modules for short) include TCP, IP, Ethernet, while
examples of control plane modules (or control modules for
short) include routing algorithms and negotiation algorithms
like IPSec’s IKE or PPP’s LCP and NCPs.
Data modules connect to each other to carry data pack-
ets. These connections are called pipes. Control modules
also connect to data modules using pipes for delivery ser-
vices. Data modules may require the use of a control module;
we refer to this as a dependency. For instance, in Figure 1,
the IPsec module has a (data plane) pipe to IP, and has a
1Of course, this is a simpliﬁcation since the paths must per-
form adequately, have certain security properties, etc. but
the basic argument still applies.
IKE      
Dependency
IP- Sec  
UDP
IP   
ETH 
Module 
Up-Pipe
Performance 
Filtering  
Switching
Security 
Down-Pipe
Figure 1: Modules, pipes, and dependencies form a
graph that describes the operation of a device (in
particular) and the network (in general). The ﬁgure
on the right denotes the major components of the
module abstraction.
dependency on IKE, which in turn has a pipe to UDP. Ulti-
mately, modules, pipes, and dependencies form a graph that
in some sense describes the operation of the network. The
data modules self-describe themselves using the abstraction
shown on the right in ﬁgure 1. Below we brieﬂy comment on
the components of this abstraction:
2.3.1 Pipes
Up and Down pipes connect modules to other modules
above and below themselves in the same device. Such pipes
are point-to-point only. Point-to-point pipes are modeled as
unidirectional (and usually come in pairs), though for simplic-
ity we present them as bidirectional. The actual network links
are modeled as Physical pipes and can be point-to-point or
broadcast. Hence, the path between two modules in two dif-
ferent devices is the sequence of up-down and physical pipes
through which packets travel between the modules. Of these,
the NM can create up-down pipes. It cannot create physical
pipes, but can discover and enable them. Also, pipes have
identiﬁers which the NM can use to refer to them.
Modules are associated with a list of connectable-modules.
For example, the connectable-modules for the down pipe of a
particular TCP module might be restricted to {IPv4, IPv6}
implying that the TCP implementation in question can only
operate on top of (have a down pipe to) IPv4 or IPv6.
While modules pass packets between up and down pipes,
the end goal is to be able to communicate with modules in
other devices. To capture this, each pipe is associated with
one or more peers modules. For example, the peer module
for a down-pipe of a TCP module would be the remote TCP
module to which the down-pipe ultimately leads to. Also,
each module is associated with a set of peerable-modules. For
example, the peerable-modules for a TCP module are {TCP}
while the peerable-modules for a HTTP-server module are
{HTTP-client}.
In eﬀect, the notion of pipes abstracts away the details that
protocols need for basic operation. Given a connectivity goal,
the NM simply builds the corresponding path by creating
pipes while the modules determine the low-level parameters.
For instance, creating a down pipe from an IP module to an
ETH module might be a part of establishing IP connectivity
between two hosts and may cause the IP module to communi-
cate with its peer IP module through the management chan-