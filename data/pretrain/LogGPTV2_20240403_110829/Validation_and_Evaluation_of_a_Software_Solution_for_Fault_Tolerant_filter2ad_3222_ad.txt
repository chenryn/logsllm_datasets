### Synchronization Model and Performance Analysis

#### Figures
- **Figure 7.** The model for checking synchronization.
- **Figure 8.** The modified DS model.

#### Set Construction and Intersection
The set constructed by PROD contains 15 markings and was automatically named `%4`. When we build the intersection between `%4` and the livelock set (named `%%0` by a previous PROD query) using:
```plaintext
0#build %%0 & %4
```
we obtain an empty set, indicating that none of the markings in the livelock have all table entries set to NR. This is not surprising because after the first synchronization, there is always a level for each task that is equal to LR, which memorizes the last level on which the task synchronized. This behavior has been approved by the DS designers, and therefore, we did not modify the initial marking.

#### Experiments and Results
We performed our experiments on the final closed model and report here an example of the size of the reachability graph and a performance index.

**Table 1.** Ordinary and Symbolic Reachability Graphs.

| Tasks | Levels | RG (Vanishing Markings) | SRG (Tangible Markings) |
|-------|--------|-------------------------|-------------------------|
| 4     | 5      | 22                      | 73                      |
| 6     | 1133   | 307                     | 15636                   |
| 7     | 15841  | 1451                    | 754                     |
| 4     | 4      | 13                      | 24                      |
| 4     | 127    | 43                      | 567                     |
| 4     | 527    | 74                      | 38                      |
| 4     | 98     | 339                     | 2370                    |
| 4     | 2390   | 202                     | 45510                   |
| 4     | 15580  | 898510                  | 15317                   |
| 4     | 388    | 686454                  | 99390                   |
| 4     | 38     | 54                      | 178                     |
| 4     | 630    | 534                     | 63                      |
| 4     | 4217   | 1608                    | 28354                   |
| 4     | 1254   | 72                      | 19849                   |
| 4     | 3435   | 1                       | 2                       |
| 4     | 1      | 2                       | 1                       |
| 4     | 3      | 2                       | 3                       |
| 4     | 3      | 1                       | 4                       |
| 4     | 2      | 4                       | 1                       |
| 4     | 1      | 1                       | 2                       |
| 4     | 2      | 2                       | 3                       |
| 4     | 1      | 3                       | 2                       |
| 4     | 3      | 4                       | 1                       |
| 4     | 2      | 4                       | 1                       |

The size of the Markov chain to be solved is given by the number of tangible symbolic markings. As shown in Table 1, the saving can be quite significant.

#### Performance Index
As an example of a performance index, we considered the mean waiting time of tasks in place `tk5`, where tasks wait for a message of reached synchronization. Figure 9 plots two curves of the mean waiting time in front of a synchronization barrier for a number of tasks (2 and 3) with 2 levels, with the rate of the timed transition "working" on the x-axis. As expected, the waiting times are larger for the three-task case, and the faster the task activity, the smaller the waiting time. The two curves were obtained analytically, while for a number of tasks larger than 4, simulation is the only possibility.

**Figure 9.** Performance measure for the DS: Mean waiting time to synchronize (fault case).

**Figure 10.** Performance measure for the DS: Mean waiting time to synchronize (absence of faults).

By comparing the two diagrams, it appears that the presence of faults decreases the waiting time, which may seem counter-intuitive. However, this is correct because the DS mechanisms do not block, waiting for failed tasks, while trying to reach a global synchronization.

### Conclusion
In this paper, we presented the validation and evaluation process for a software solution to distributed synchronization. Several key observations and lessons learned from this case study include:

1. **Early Design Analysis:** For the analysis to be effective, it should be fast and not perceived as a non-useful add-on to the project. Formal specifications are essential to ensure the model's correctness.
2. **Compositional Facilities and Experiment Planner:** To achieve timely results, compositional facilities and an experiment planner are necessary.
3. **Exploiting Symmetries:** Symmetries in the model should be exploited in the analysis to handle complex cases more efficiently.
4. **Model Checkers and Simulation:** Large models can be tricky, and while simulation is often used, it is crucial to validate the model's adherence to reality. Our experience with PROD showed limitations due to priorities over transitions, and we plan to investigate other model checkers.
5. **Timed Transitions:** Introducing timed transitions significantly increases the number of states, a phenomenon we are still mastering.
6. **Modeling Failures:** The model assumes tasks can fail only when they are in the working state. If tasks can fail in any state, the model and the number of states can change significantly.

### References
- [1] M. Ajmone Marsan, G. Balbo, G. Conte, S. Donatelli, and G. Franceschinis. *Modelling with Generalized Stochastic Petri Nets*. J. Wiley, 1995.
- [2] S. Balsamo and M. Simeoni. *On transforming UML models into performance models*. In ETAPS01 Satellite Event, Genova (ITALY) April, 2001.
- [3] J. Merseguer, J. Campos, and E. Mena. *Performance evaluation for the design of agent-based systems: A Petri net approach*. In Mauro Pezzé and Sol M. Shatz, editors, Proceedings of the Workshop on Software Engineering and Petri Nets, within the 21st International Conference on Application and Theory of Petri Nets, Aarhus, Denmark, June 2000.
- [4] A. Bondavalli, I. Maizik, and I. Mura. *Automated Dependability Analysis of UML Designs*. Proc. ISORC’99 - 2nd IEEE International Symposium on Object-oriented Real-time distributed Computing, Saint Malo, France, 1999, IEEE Computer Society Press.
- [5] C. G. Bruno, Bertoncello, Franceschinis, G. Lungo Vaschetti, and A. Pigozzi. *SWN models of a contact center: a case study*. In Proc. 11th International Workshop on Petri Nets and Performance Models, Aachen, Germany, Sept. 11-14 2001. IEEE Computer Society Press.
- [6] S. Bernardi, C. Bertoncello, S. Donatelli, G. Franceschinis, R. Gaeta, M. Gribaudo, and A. Horváth. *GreatSPN in the new Millennium*. In Tools of Aachen 2001, Int. Multiconference on Measurement, Modelling and Evaluation of Computer-Communication Systems, Dortmund, (Germany), Sept. 2001. Technical report no.760/2001 - Dortmund Universitaet.
- [7] S. Bernardi, S. Donatelli, and A. Horváth. *Compositionality in the GreatSPN tool and its application to the modelling of industrial applications*. Int. Journal of Software Tools for Technology Transfer (STTT), 3(4), August 2001. Springer and Verlag.
- [8] V. DeFlorio, S. Donatelli, and G. Dondossola. *Flexible Development of Dependability Services: An Experience Derived from Energy Automation Systems*. In proc. of the 9th annual IEEE Conference and Workshop on Engineering of Computer-Based Systems, Lund, Sweden, April 8-10(11), 2002.
- [9] S. Donatelli, and L. Ferro. *Validation of GSPN and SWN models through the PROD tool*. In proc. of the 12th International Conference on Modelling Tools and Techniques for Computer and Communication System Performance Evaluation, London, UK, April 2002, Springer Verlag LNCS 2324.
- [10] O. Botti, V. De Florio, G. Deconinck, R. Lauwreins, F. Cassinari, A. Bobbio, S. Donatelli, A. Lein, H. Kufner, E. Thurner, and E. Verhulst. *The TIRAN approach to reusing software implemented fault-tolerance*. In Proc. 8th Euromicro Workshop on Parallel and Distributed Processing (PDP2000) , Rhodos, Greece, Jan. 2000. IEEE Comp. Soc. Press.
- [11] G. Chiola, G. Franceschinis, R. Gaeta, and M. Ribaudo. *GreatSPN 1.7: GRaphical Editor and Analyzer for Timed and Stochastic Petri Nets*. Performance Evaluation, special issue on Performance Modelling Tools, (1), 24, 1996.

Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021 at 04:16:05 UTC from IEEE Xplore. Restrictions apply.