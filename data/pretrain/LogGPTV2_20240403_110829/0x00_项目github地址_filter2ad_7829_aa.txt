### 0x00 项目github地址
链接：
### 0x01 概述
    在日常的渗透测试中，总会遇到这样的那样的建站公司，科技公司搭建的站，而这些网站真的是这些公司的人员一行一行搭出来的吗？其实不然，很多情况下都是用了现成的框架或者一些主流或非主流的建站CMS，为了进一步合法的渗透测试，我们必须要获取网站框架，获取域名相应服务器的标题、服务器名、服务器语言、服务器版本以及服务器的建站程序或框架等基本信息，FuckCMS就是笔者对于web指纹识别自动化的一次尝试
### 0x02 设计过程
##### （1）HTTP简介
    HTTP是一种超文本传输协议，是基于TCP协议之上的一种请求-响应请求协议，浏览器访问某个网站时发送的HTTP请求-响应，当浏览器希望访问某个网站时，浏览器和网站服务器之间先建立TCP连接，且服务器总是使用80端口和加密端口443，然后浏览器向服务器发送一个HTTP请求，服务器收到之后，返回一个HTTP响应，响应中包含了HTML的网页内容，浏览器解析HTML后就给用户显示网页，详细流程图如下所示：
##### （2）网站搭建语言及常见的网站服务器以及建站框架
建站框架 | 搭建语言 | 网站服务器  
---|---|---  
javabean | JAVA | Servlet  
Struts 2 | JAVA | Weblogic、Servlet...  
Spring | JAVA | Servlet、Weblogic  
Thinkphp | PHP | IIS、Tomcat、nginx、Apache...  
discuz | PHP | IIS、Tomcat、nginx、Apache...  
... | ... | ...  
##### （3）web服务器识别方法：
    首先根据选取的web服务器指纹特征，设计出一套简单方便有效的web服务器指纹识别方法。该识别方法可分为两步：首先第一步发送get请求，利用头部域指纹准确识别web服务器类型；然后，根据web服务器类型构造特定的http请求，利用状态码定义指纹可准确识别web服务器版本。
#### 二、项目模块代码
    我们开始编写获取服务器的项目工程，我们可以在pycharm上新建一个项目工程WebScan，其下建立4个文件，
    fuckcms.py作为主文件来运行，GetCms.py作为功能文件，GetCmsFromTide.py，GetCmsFromCms.py作为从不同指纹库中识别建站程序或框架的功能文件。
​ **fuckcms.py**
``
    #!/usr/bin/env python3
    # -*- coding: utf-8 -*-    # author: jeremy
    # github: https://github.com/cyber-word
    # 公众号： 剑南道极客
    import argparse
    import GetCms
    import sys
    import threading
    from threading import Thread  # 引入多线程
    from queue import Queue  # 引入队列机制
    import time
    import banner
    # Check py version
    def CheckVersion():
        PythonVersion = sys.version.split()[0]
        if PythonVersion <= "3":
            exit('Need Python3.x')
    CheckVersion()
    print(banner.banner())
    # Get argparse
    parser = argparse.ArgumentParser()
    parser.description = 'please enter -u (required) -f (optional) -t (optional)'
    parser.add_argument("-u", "--url", help="this is a url to be scanned", dest="url", type=str, default="no input url")
    parser.add_argument("-f", "--file", help="this is a url list to be scanned", dest="path", type=str, default="no input "
                                                                                                                "file")
    parser.add_argument("-t", "--thread", help="this is the numeber of threads to be used", dest="ThreadNum", type=int,
                        default=10)
    parser.add_argument("-d", "--dbs", help="this is the finger dbs that to be used", dest="FingerDbs", type=str,
                        default="all")
    args = parser.parse_args()
    print("\033[35m""*"*50+"\033[0m")
    print("\033[33m""url: "+args.url+"\033[0m")
    print("\033[33m""file path: "+args.path+"\033[0m")
    print("\033[33m""scanThreadNum: "+str(args.ThreadNum)+"\033[0m")
    print("\033[33m""Finger dbs: "+args.FingerDbs+"\033[0m")
    print("\033[35m""*"*50+"\033[0m")
    if args.path == "no input file":
        if args.url != "no input url":
            if args.FingerDbs == "all":
                queue = Queue()
                queue.put(args.url)
                for i in range(1, args.ThreadNum):
                    thread = Thread(target=GetCms.GetWebInfo, args=(queue,args.FingerDbs))
                    thread.start()
                queue.join()
        else:
            print("请重新输入")
    else:
        queue = Queue()
        txt_path = args.path  # 要扫描的网站存放路径
        f = open(txt_path)
        data_lists = f.readlines()  # 读出的是str类型
        start_time = time.time()
        for data in data_lists:
            data1 = data.strip('\n')  # 去掉开头和结尾的换行符
            queue.put(data1)  # 将url读取到queue中
            # getinfo.GetWebInfo(data1)
        for i in range(1, args.ThreadNum):
            thread = Thread(target=GetCms.GetWebInfo, args=(queue,args.FingerDbs))
            thread.start()
        queue.join()
        print("\033[32m""扫描完毕共花费了:" + str(time.time() - start_time) + "秒""\033[0m")
​ 我们用到了requests，user_agent，argparse，GetInfo，sys共五个库，各个库的作用如下表所示：
包名 | 功能  
---|---  
queue | 提供多线程间的队列通信机制  
argparse | 实现用户交互，通过命令行中输入形如”-u url”的形式来传递参数  
GetCms | 自定义文件，用来实现对响应包中特定内容的读取  
time | 确定程序运行时间，提高用户可阅读性  
sys | 确定用户的python版本  
threading | 提供多线程机制  
​ **GetCms.py** （实现信息收集）
``
    #!/usr/bin/env python3
    # -*- coding: utf-8 -*-    # @Time    : 2021/12/4
    # @Author  : jeremy
    # @公众号  : 剑南道极客
    # @参考项目: webscan、Tidefinger
    # @python 3.x
    # github: https://github.com/cyber-word
    import requests
    import user_agent
    from bs4 import BeautifulSoup
    import lxml
    import urllib
    from urllib.parse import urlparse
    import re
    import threading
    import sqlite3
    import time
    from queue import Queue
    import GetCmsFromTide
    import GetCmsFromCms
    # 创建一个WebInfo类用来存储扫描到的信息
    class WebInfo:
        def __init__(self, domain, title, http_server, language, set_Cookie, X_Powered_By, cms, body, header):
            self.header = header
            self.body = body
            self.set_Cookie = set_Cookie
            self.X_Powered_By = X_Powered_By
            self.language = language
            self.http_server = http_server
            self.domain = domain
            self.title = title
            self.cms = cms
        def print(self):
            print("\033[32m""domain:" + self.domain + "\033[0m")
            print("\033[33m""title:" + self.title + "\033[0m")
            print("\033[34m""http_server:" + self.http_server + "\033[0m")
            print("\033[35m""language:" + self.language + "\033[0m")
            print("\033[36m""X-Powered-By:" + self.X_Powered_By + "\033[0m")
            # print(self.__dict__) 原本用作遍历输出所有属性
    def GetWebInfo(queue, Dbs):
        while queue.empty() is not True:
            url = queue.get()
            headers = {
                'User-Agent': user_agent.generate_user_agent()
            }
            try:
                r = requests.get(url=url, headers=headers, timeout=5)
                print("-" * 25+"扫描对象"+"-"*25)
                print(url)
                r.encoding = 'unicode'
                headers = str(r.headers)
                bodys = r.text
                contents = r.content
                try:
                    title = BeautifulSoup(bodys, 'lxml').title.text.strip()
                except Exception as error:
                    title = "暂未识别出title"
                try:
                    Cookie = r.headers['Coolie']
                except Exception as error:
                    Cookie = "暂未识别，可能是当前页面没有cookie"
                try:
                    Server = r.headers['Server']
                except Exception as error:
                    Server = "暂未识别出当前页面服务器"
                domain = urlparse(url).netloc
                domain = domain.replace('www.', '')
                ThisWebInfo = WebInfo(domain, title, Server, "暂未识别当前页面语言", Cookie,
                                      "未识别出x-powered-by", "未识别出cms,但可能x-powered-by中有", bodys, headers)
                # 识别语言
                if 'X-Powered-By' in r.headers:
                    ThisWebInfo.X_Powered_By = r.headers['X-Powered-By']
                if 'set-Cookie' in r.headers:
                    ThisWebInfo.set_Cookie = r.headers['Set-Cookie']
                if 'Cookie' in r.headers:
                    ThisWebInfo.set_Cookie = r.headers['Set-Cookie']
                if "PHPSSIONID" in ThisWebInfo.set_Cookie:
                    ThisWebInfo.language = "PHP"
                if "JSESSIONID" in ThisWebInfo.set_Cookie:
                    ThisWebInfo.language = "JAVA"
                if "ASP.NET" in ThisWebInfo.X_Powered_By or "ASPSESS" in ThisWebInfo.set_Cookie or "ASP.NET" in ThisWebInfo.set_Cookie:
                    ThisWebInfo.language = "ASP.NET"
                if "JBoss" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "JAVA"
                if "Servlet" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "JAVA"
                if "Next.js" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "NODEJS"
                if "Express" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "NODEJS"
                if "PHP" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "PHP"
                if "JSF" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "JAVA"
                if "WP" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "PHP"
                if "enduro" in ThisWebInfo.X_Powered_By:
                    ThisWebInfo.language = "NODEJS"
                try:
                    if Dbs == "tide":