Triggers and reverse-engineer it using MINE [5]. However, this
method makes a strong assumption about the shape of the trigger
in order to utilize a GAN to reverse-engineer it. NeuralCleanse [55]
pioneers a line of work that focuses on reverse-engineering the
trigger and formulates the problem as a non-convex optimization
problem. It assumes that if a backdoor exists, then the l1 norm of
the mask of the Trojan trigger is smaller than the l1 norm of the
mask of any other benign perturbation that causes the prediction
of images to change to the target class. This isn’t realistic as the
Trojan trigger can have a larger l1 norm than benign features. The
optimization problem is also non-convex and can terminate on a
false trigger (local minimum) that won’t flag the label as target
label. In [17, 43] the authors report and try to address the short-
comings of NeuralCleanse such as identifying triggers that are
false alarms. In particular, the K-Arm defense [43] is an extension
of NeuralCleanse and still a patch-based optimization problem. It
only succeeds in detecting Instagram filters by tailoring the opti-
mization problem to the way the Instagram filters are applied, and
hence, making a strong assumption about knowing the trigger type
beforehand. TABOR [17] assumes that triggers are localized and
uses the NeuralCleanse optimization problem to add regulariza-
tion terms that penalize overly large triggers or scattered triggers.
Authors in [19] show an improvement by also enhancing the Neu-
ralCleanse optimization problem with additional regularization
terms. Even though these methods improve certain aspects of Neu-
ralCleanse, they include limited evaluation on complex localized
triggers and novel triggers that are not applied in the conventional
patch-based approach. For example, Instagram filters and smooth
triggers are not patch-based and are applied in a different way. Note
0.30.40.50.60.70.80.9Parameter 020406080100Final TPRFinal FPRImage with triggerDeepSHAP Attribution mapMin:-0.6765 Max:1.6955Reverse-engineered TriggerImage with triggerGrad-CAM Attribution mapMin:0.0 Max:1.0Reverse-engineered Trigger10020010101002001002001.00.50.00.51.00100200579MISA: Online Defense of Trojaned Models using Misattributions
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
that the Clean-Label Attack [51] and Hidden Backdoor Attack [39]
are still patch-based trigger attacks. Hence, these defenses fail to
detect triggers from different categories. Moreover, they can in-
cur a high computational cost that makes using the defense very
laborious [43].
Offline defenses that focus on erasing backdoors can lead to
degradation of the standard accuracy of the model and the removal
of the backdoor. The current state-of-the-art in this line of work [28]
removes the backdoor with up to 3% degradation on the standard
accuracy. In [18, 50] the authors perform statistical analysis over the
training data to identify Trojaned images and possibly remove them
during training. ActivationClustering [8] showed that the poisoned
training images can create a separate cluster from the clean images.
Assuming that the defender has access to the training data is not
realistic since the model can be trained by a third party. MNA [57]
trains a meta classifier to detect Trojaned models. Recently, in [61]
the authors introduced smooth triggers that we evaluate on and
have low frequencies, and show that MNA [57] has a low AUC
when evaluated on smooth triggers.
Online Defenses. Online methods aim at detecting whether the
model is Trojaned during inference. These methods seem more
promising for defending against different types of Trojan triggers
since they have the advantage of encountering actual Trojaned
images. In SentiNet [11], the authors propose a defense for local-
ized triggers by examining the saliency maps in the input space.
CleaNN [21] leverages the observation that certain Trojaned im-
ages exhibit high frequencies in the frequency domain. However,
the authors in [61] introduced the low-frequency trigger (men-
tioned as smooth trigger in our experiments) designed to break
this assumption. In STRIP [13], the authors introduced a simple
approach for distinguishing between Trojaned and clean images by
superimposing the inference-time image with random clean images
and checking the entropy of the resulting predictions. However,
their approach relies on the fact that the two entropy distributions
computed by Trojaned and clean images’ labels can be separated
by a threshold and its effectiveness is known to be very sensitive
to this threshold. In our experiments, we find that the two entropy
distributions cannot be cleanly separated and can have a significant
overlap. We hypothesize that this poor performance is caused by
their perturbation step which superimposes (adds) the input image
with a random clean image. This simple addition often results in
significantly out-of-distribution images and in some cases even
cancels out the trigger. Februus [12], uses attributions in the in-
put space to identify Trojan triggers. As we show in this paper,
relying on attributions at the input layer alone can’t detect cer-
tain triggers. NEO [53] is an input filtering method that makes a
strong assumption on the trigger type, assuming that triggers are
patch-based and localized in order to identify their position and
block them. Lastly, NNoculation [54] produces a second network
from the potentially Trojaned network, by re-training to be robust
to random perturbations. They make an interesting observation
that this simple approach erases most of the backdoor’s behavior
(ASR drops to 2-8%). Then, they identify disagreements between the
two deployed models and eventually reverse-engineer the trigger
using a Cycle-GAN. However, it is not clear if this approach can
reverse-engineer non-localized triggers. Additionally, this approach
requires 5% of the inference images to be Trojaned to be effective.
6 DISCUSSION
In this section, we discuss the possibility of an adaptive attack
where the attacker would choose a trigger that, when added to input
images, causes the attributions of the resulting Trojaned image to
be similar to clean images’ attributions, thereby avoiding detection
by MISA. Given that the function of computing attributions itself
(e.g. Integrated Gradients is approximated by a discrete sum) is not
differentiable, one possible approach is to use an additional loss
term in the training objective to penalize high attributions on the
Trojan pixels or features. In this case, the attacker must have control
over the whole training process, as opposed to simply poisoning
a small percentage of the training data as described in our threat
model. Even if we assume an attacker is able to implement such an
attack, this approach would require computing attributions for any
intermediate model at each iteration during training, which will
drastically slow down the training process. Finally, as shown in this
paper, misattribution is an inherent property of Trojaned images
– the trigger must be effective in a targeted attack regardless of
what the clean part of the image is. Thus, such an attack is unlikely
to produce a high attack success rate, and for the cases that it
succeeds, would still be caught by the extract-and-evaluate stage
of the MISA pipeline for a layer of the neural network. We leave
a more comprehensive investigation of adaptive attacks to future
work.
7 CONCLUSION
Our results demonstrate that we can successfully detect Trojaned
instances at inference time without prior knowledge of the attack
specifics by attributing the neural network’s decision to input or
intermediate-layer features. We observe that our method effectively
detects different types of triggers, including recent ones that are not
applied in the traditional patch-based approach, as explained in Sec-
tion 2. Our approach builds on the following two observations. First,
attributions to a layer’s features (input layer (pixels) or intermediate-
layer features) of a Trojaned input are out-of-distribution from the
clean features’ attributions. Second, the target label persists when
the high attributed Trojaned features are injected into the corre-
sponding features from clean images.
ACKNOWLEDGMENTS
This effort was supported by the Intelligence Advanced Research
Projects Agency (IARPA) under the contract W911NF20C0038. The
content of this paper does not necessarily reflect the position or
the policy of the Government, and no official endorsement should
be inferred.
REFERENCES
[1] William Aiken, Hyoungshick Kim, Simon Woo, and Jungwoo Ryoo. 2021. Neural
network laundering: Removing black-box backdoor watermarks from deep neural
networks. Computers & Security 106 (2021), 102277.
[2] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen,
Klaus-Robert Müller, and Wojciech Samek. 2015. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PloS one 10,
7 (2015), e0130140.
580ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha
[3] Eugene Bagdasaryan and Vitaly Shmatikov. 2020. Blind backdoors in deep
learning models. arXiv preprint arXiv:2005.03823 (2020).
[4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017.
Network dissection: Quantifying interpretability of deep visual representations.
In Conference on computer vision and pattern recognition. CVPR, 6541–6549.
[5] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua
Bengio, Aaron Courville, and Devon Hjelm. 2018. Mutual information neural
estimation. In International Conference on Machine Learning. PMLR, 531–540.
[6] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016).
[7] Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping,
Micah Goldblum, Tom Goldstein, and Arjun Gupta. 2021. Strong data augmenta-
tion sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In
ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, 3855–3859.
[8] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. 2018. Detecting back-
door attacks on deep neural networks by activation clustering. arXiv preprint
arXiv:1811.03728 (2018).
[9] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. 2019. DeepIn-
spect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural
Networks.. In International joint conferences on artificial intelligence. 4658–4664.
[10] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted
backdoor attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526 (2017).
[11] Edward Chou, Florian Tramèr, and Giancarlo Pellegrino. 2020. Sentinet: Detecting
localized universal attacks against deep learning systems. In 2020 IEEE Security
and Privacy Workshops (SPW). IEEE, 48–54.
[12] B Gia Doan, Ehsan Abbasnejad, and Damith C Ranasinghe. 2019. Februus: Input
purification defense against trojan attacks on deep neural network systems. In
arXiv: 1908.03369. arXiv.
[13] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe,
and Surya Nepal. 2019. Strip: A defence against trojan attacks on deep neural
networks. In Computer security applications conference. 113–125.
[14] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and An-
drew G Wilson. 2018. Loss surfaces, mode connectivity, and fast ensembling of
dnns. In Neural information processing systems. 8789–8798.
[15] Ian J et al. Goodfellow. 2014. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 (2014).
[16] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2019. Badnets:
Evaluating backdooring attacks on deep neural networks. IEEE Access 7 (2019),
47230–47244.
[17] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. 2020. TABOR: A
Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI
Systems. ICDM (2020). arXiv:1908.01763
[18] Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. 2021. SPEC-
TRE: Defending Against Backdoor Attacks Using Robust Statistics. arXiv preprint
arXiv:2104.11315 (2021).
[19] Xijie Huang, Moustafa Alzantot, and Mani Srivastava. 2019. Neuroninspect:
Detecting backdoors in neural networks via output explanations. arXiv preprint
arXiv:1911.07399 (2019).
[20] Todd Huster and Emmanuel Ekwedike. 2021. TOP: Backdoor Detection in Neural
Networks via Transferability of Perturbation. arXiv preprint arXiv:2103.10274
(2021).
[21] Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi, and Fari-
naz Koushanfar. 2020. Cleann: Accelerated trojan shield for embedded neural
networks. In 2020 IEEE/ACM International Conference On Computer Aided Design
(ICCAD). IEEE, 1–9.
[22] Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Viégas, and Michael Terry. 2019.
Xrai: Better attributions through regions. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision. 4948–4957.
[23] Kiran Karra, Chace Ashcraft, and Neil Fendley. 2020. The TrojAI Software
Framework: An OpenSource tool for Embedding Trojans into Deep Learning
Models. arXiv preprint arXiv:2003.07233 (2020).
[24] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. 2020. TrojDRL:
evaluation of backdoor attacks on deep reinforcement learning. In 2020 57th
ACM/IEEE Design Automation Conference (DAC). IEEE, 1–6.
[25] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. 2020.
Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs. In Conference
on computer vision and pattern recognition. 301–310.
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[27] Shaofeng Li, Benjamin Zi Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, and
Haojin Zhu. 2019. Invisible backdoor attacks against deep neural networks. arXiv
preprint arXiv:1909.02742 (2019).
[28] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma.
2021. Neural attention distillation: Erasing backdoor triggers from deep neural
networks. arXiv preprint arXiv:2101.05930 (2021).
[29] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
symposium on research in attacks, intrusions, and defenses. Springer, 273–294.
[30] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and
Xiangyu Zhang. 2019. ABS: Scanning neural networks for back-doors by artificial
brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security. 1265–1282.
[31] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In 25nd
Annual Network and Distributed System Security Symposium, NDSS 2018, San
Diego, California, USA, February 18-221, 2018. The Internet Society.
[32] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. 2020. Reflection backdoor: A
natural backdoor attack on deep neural networks. arXiv preprint arXiv:2007.02343
(2020).
[33] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model
Predictions. In Advances in Neural Information Processing Systems 30, I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(Eds.). Curran Associates, Inc., 4765–4774.
[34] Shervin Minaee, Amirali Abdolrashidi, Hang Su, Mohammed Bennamoun, and
David Zhang. 2019. Biometric recognition using deep learning: A survey. arXiv
preprint arXiv:1912.00271 (2019).
[35] Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, and Seong-Whan Lee. 2019.
Relative Attributing Propagation: Interpreting the Comparative Contributions
of Individual Units in Deep Neural Networks. arXiv preprint arXiv:1904.00605
(2019).
[36] Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng,
and Ting Wang. 2020. TROJANZOO: Everything you ever wanted to know about
neural backdoors (but were afraid to ask). arXiv preprint arXiv:2012.09302 (2020).
[37] Ximing Qiao, Yukun Yang, and Hai Li. 2019. Defending Neural Backdoors via
Generative Distribution Modeling. In Advances in Neural Information Processing
Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (Eds.), Vol. 32. Curran Associates, Inc., 14004–14013.
[38] Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani
Thuraisingham. 2021. Deepsweep: An evaluation framework for mitigating dnn
backdoor attacks using data augmentation. In Proceedings of the 2021 ACM Asia
Conference on Computer and Communications Security. 363–377.
[39] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. 2020. Hidden
trigger backdoor attacks. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 34. 11957–11965.
[40] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. 2020.
Dynamic backdoor attacks against machine learning models. arXiv preprint
arXiv:2003.03675 (2020).
[41] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A
unified embedding for face recognition and clustering. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 815–823.
[42] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from
deep networks via gradient-based localization. In Proceedings of the IEEE interna-
tional conference on computer vision. 618–626.
[43] Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan
Cheng, Shiqing Ma, and Xiangyu Zhang. 2021. Backdoor Scanning for Deep
Neural Networks through K-Arm Optimization. arXiv preprint arXiv:2102.05123
(2021).
[44] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning im-
portant features through propagating activation differences. In International
Conference on Machine Learning. PMLR, 3145–3153.
[45] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural
networks and tree search. nature 529, 7587 (2016), 484.