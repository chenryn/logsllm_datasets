一、asyncio  
下面通过举例来对比同步代码和异步代码编写方面的差异，其次看下两者性能上的差距，我们使用sleep(1)模拟耗时1秒的io操作。
同步代码：
    import time def hello():
        time.sleep(1) def run(): for i in range(5):
            hello() print('Hello World:%s' % time.time()) # 任何伟大的代码都是从Hello World 开始的！ if __name__ == '__main__':
        run()
输出：（间隔差不多是1s）
Hello World:1527595175.4728756 Hello World:1527595176.473001 Hello
World:1527595177.473494 Hello World:1527595178.4739306 Hello
World:1527595179.474482
异步代码：
    import time import asyncio # 定义异步函数 async def hello():
        asyncio.sleep(1) print('Hello World:%s' % time.time()) def run(): for i in range(5):
            loop.run_until_complete(hello())
    loop = asyncio.get_event_loop() if __name__ =='__main__':
        run()
输出：
Hello World:1527595104.8338501 Hello World:1527595104.8338501 Hello
World:1527595104.8338501 Hello World:1527595104.8338501 Hello
World:1527595104.8338501
async def
用来定义异步函数，其内部有异步操作。每个线程有一个事件循环，主线程调用asyncio.get_event_loop()时会创建事件循环，你需要把异步的任务丢给这个循环的run_until_complete()方法，事件循环会安排协同程序的执行。
二、aiohttp  
如果需要并发http请求怎么办呢，通常是用requests，但requests是同步的库，如果想异步的话需要引入aiohttp。这里引入一个类，from
aiohttp import
ClientSession，首先要建立一个session对象，然后用session对象去打开网页。session可以进行多项操作，比如post, get,
put, head等。
基本用法：
    async with ClientSession() as session:
        async with session.get(url) as response:
aiohttp异步实现的例子：
    import asyncio from aiohttp import ClientSession
    tasks = []
    url = "https://www.baidu.com/{}" async def hello(url):  async with ClientSession() as session:
            async with session.get(url) as response:
                response = await response.read() print(response) if __name__ == '__main__':
        loop = asyncio.get_event_loop()
        loop.run_until_complete(hello(url))
首先async def 关键字定义了这是个异步函数，await
关键字加在需要等待的操作前面，response.read()等待request响应，是个耗IO操作。然后使用ClientSession类发起http请求。
多链接异步访问
如果我们需要请求多个URL该怎么办呢，同步的做法访问多个URL只需要加个for循环就可以了。但异步的实现方式并没那么容易，在之前的基础上需要将hello()包装在asyncio的Future对象中，然后将Future对象列表作为任务传递给事件循环。
    import time import asyncio from aiohttp import ClientSession
    tasks = []
    url = "https://www.baidu.com/{}" async def hello(url):  async with ClientSession() as session:
            async with session.get(url) as response:
                response = await response.read() # print(response) print('Hello World:%s' % time.time()) def run(): for i in range(5):
            task = asyncio.ensure_future(hello(url.format(i)))
            tasks.append(task) if __name__ == '__main__':
        loop = asyncio.get_event_loop()
        run()
        loop.run_until_complete(asyncio.wait(tasks))
输出：
Hello World:1527754874.8915546 Hello World:1527754874.899039 Hello
World:1527754874.90004 Hello World:1527754874.9095392 Hello
World:1527754874.9190395
收集http响应
好了，上面介绍了访问不同链接的异步实现方式，但是我们只是发出了请求，如果要把响应一一收集到一个列表中，最后保存到本地或者打印出来要怎么实现呢，可通过asyncio.gather(*tasks)将响应全部收集起来，具体通过下面实例来演示。
    import time import asyncio from aiohttp import ClientSession
    tasks = []
    url = "https://www.baidu.com/{}" async def hello(url):
        async with ClientSession() as session:
            async with session.get(url) as response: # print(response) print('Hello World:%s' % time.time()) return await response.read() def run(): for i in range(5):
            task = asyncio.ensure_future(hello(url.format(i)))
            tasks.append(task)
        result = loop.run_until_complete(asyncio.gather(*tasks)) print(result) if __name__ == '__main__':
        loop = asyncio.get_event_loop()
        run()
输出：
Hello World:1527765369.0785167 Hello World:1527765369.0845182 Hello
World:1527765369.0910277 Hello World:1527765369.0920424 Hello
World:1527765369.097017 [b'\r\n\r\n\r\n\r\n......
异常解决
假如你的并发达到1000个，程序会报错：ValueError: too many file descriptors in
select()。这个报错的原因是因为 Python 调取的 select
对打开的文件字符有最大长度限制。这里我们有两种方法解决这个问题：1.我们可以需要限制并发数量。一次不要塞那么多任务，或者限制最大并发数量。2.我们可以使用回调的方式。这里个人推荐限制并发数的方法，设置并发数为500或者600，处理速度更快。