title:Where Has My Time Gone?
author:Noa Zilberman and
Matthew P. Grosvenor and
Diana Andreea Popescu and
Neelakandan Manihatty Bojan and
Gianni Antichi and
Marcin W&apos;ojcik and
Andrew W. Moore
Where Has My Time Gone?
Noa Zilberman(B), Matthew Grosvenor, Diana Andreea Popescu,
Neelakandan Manihatty-Bojan, Gianni Antichi,
Marcin W´ojcik, and Andrew W. Moore
Computer Laboratory, University of Cambridge, Cambridge, UK
{noa.zilberman,matthew.grosvenor,diana.popescu,
neelakandan.manihatty-bojan,gianni.antichi,
marcin.wojcik,andrew.moore}@cl.cam.ac.uk
Abstract. Time matters. In a networked world, we would like mobile
devices to provide a crisp user experience and applications to instan-
taneously return results. Unfortunately, application performance does
not depend solely on processing time, but also on a number of diﬀerent
components that are commonly counted in the overall system latency.
Latency is more than just a nuisance to the user, poorly accounted-for, it
degrades application performance. In ﬁelds such as high frequency trad-
ing, as well as in many data centers, latency translates easily to ﬁnancial
losses. Research to date has focused on speciﬁc contributions to latency:
from improving latency within the network to latency control on the
application level. This paper takes an holistic approach to latency, and
aims to provide a break-down of end-to-end latency from the applica-
tion level to the wire. Using a set of crafted experiments, we explore the
many contributors to latency. We assert that more attention should be
paid to the latency within the host, and show that there is no silver bul-
let to solve the end-to-end latency challenge in data centers. We believe
that a better understanding of the key elements inﬂuencing data center
latency can trigger a more focused research, improving the user’s quality
of experience.
1 Introduction
Time plays a major role in computing, as it translates directly to ﬁnancial
losses [6,13]. User demands for a highly interactive experience (e.g., online shop-
ping, web search, online gaming etc.) has put stringent demands on applications
to consistently meet tight deadlines. Nowadays, the question Can the application
(job) meet a deadline? is replaced by Will the application get the consistent, low
latency, guarantees needed to meet user demands?
In the past, large propagation delays and unoptimized hardware have eclipsed
ineﬃciencies in end-system hardware and software: operating systems and appli-
cations. Yet decades ago, latency was identiﬁed as a fundamental challenge [2,11].
The emergence of data centers increased the importance of the long tail of latency
problem: due to the scaling eﬀect within a data center, every small latency issue
is having an increasing eﬀect on the performance [1]. Only 5 years ago a switch
c(cid:2) Springer International Publishing AG 2017
M.A. Kaafar et al. (Eds.): PAM 2017, LNCS 10176, pp. 201–214, 2017.
DOI: 10.1007/978-3-319-54328-4 15
202
N. Zilberman et al.
latency of 10 µs and an OS stack latency of 15 µs were considered the norm [12],
however, since then, a signiﬁcant improvement has been achieved [3,5]. To fully
understand this latency improvement, this paper takes an end-to-end approach,
focusing upon the latency between the time a request is issued by an applica-
tion to the time a reply has returned to that application. This approach has the
advantage of maximizing the throughput of a system, which is the main goal
of a user, rather than optimizing discrete parts of the system. We consider the
best-possible conﬁgurations, which may not be identical to the most realistic
conﬁguration, and further focus on the Ethernet-based systems common in data
centers.
In this paper we use bespoke experiments (described in Sect. 2) to derive a
breakdown to the end-to-end latency of modules in commodity end-host sys-
tems (discussed in Sect. 3). We identify the latency components that require
the most focus for improvement and propose trajectories for such work. Finally,
we contribute a taxonomy of latency contributors: low-latency/low-variability:
the “Good”, high-latency/high-variability: the “Bad”, and heavy-tailed or oth-
erwise peculiar latency: the “Ugly”, while also noting the challenge of proﬁling
application network performance.
1.1 Motivation
The contribution of latency aﬀects a user-experience in a signiﬁcant, sometimes
subtle, manner. More than a simple, additive, increase in run-time, application
performance can be dramatically decreased with an increase in latency. Figure 1
illustrates the impact of latency upon performance for several common data
center applications.
1.2
1.0
0.8
0.6
0.4
0.2
0.0
e
c
n
a
m
r
o
f
r
e
P
d
e
z
i
l
a
m
r
o
N
Apache
Memcached
TPCCMySQL
.
0
0
.
1
0
.
2
0
5
0
.
0
1
.
.
0
2
.
0
5
.
0
0
1
.
0
0
2
.
0
0
5
.
0
0
0
1
Added Delay [ µs]
Fig. 1. Delay eﬀect on application performance.
Where Has My Time Gone?
203
Using an experimental conﬁguration described in Sect. 2, Fig. 1 illustrates
experimental results for three application-benchmarks. Each benchmark reports
results for an application-speciﬁc performance metric. These application-speciﬁc
benchmarks are normalized to allow comparisons to be made among the
applications.
The three benchmarks we use are Apache benchmark1 reporting mean
requests per second, Memcached benchmark2 reporting throughput, and TPC-
C MySQL benchmark3 reporting New-Order transactions per minute, (where
New-Order is one of the database’s tables).
Between the two hosts of the experimental conﬁguration described in Sect. 2,
we insert a bespoke hardware device to inject controlled latency. We imple-
mented a latency-injection appliance4 that allows us to add arbitrary latency
into the system. Past latency injection has been done with approaches such as
NetEm [4], yet this proved inappropriate for our work. Alongside limited gran-
ularity, such approaches may not reliably introduce latency of less than several
tens of microseconds [8]. In contrast, our latency gadget adds 700 ns of base
latency and permits further additional latency, at 5 ns granularity, up to a max-
imum5 determined by the rate of operation.
Each test begins by measuring a baseline, which is the performance of each
benchmark under the default setup conditions, taking into account the base
latency introduced by the latency-injection appliance. Latency is then artiﬁcially
inserted by the appliance, and the application-speciﬁc performance is measured.
We can derive the impact on experiments of the artiﬁcially inserted latency
by removing the baseline measurement. For the three benchmarks, Fig. 1 shows
the eﬀect of added latency. Each benchmark was run 100 times for the base-
line and for each added latency value. The graph plots the average values, and
standard errors are omitted for clarity, as they are below 0.005. In one run, the
Apache benchmark sends 100000 requests and the Memcached benchmark sends
10 million requests. The TPC-C benchmark runs continuously for 1000 s, with
an additional time of 6 minutes of warm-up, resulting in 100 measurements over
10 s periods. The application most sensitive to latency is Memcached: the addi-
tion of 20 µs latency leads to a performance drop of 25%, while adding 100 µs
will reduce its throughput to 25% of the baseline. The TPC-C benchmark is
the least sensitive to latency, although still exhibits some performance loss: 3%
reduction in performance with an additional 100 µs. Finally, the Apache bench-
mark observes a drop in performance that starts when 20 µs are added, while
adding 100 µs leads to a 46% performance loss.
1 https://httpd.apache.org/docs/2.4/programs/ab.html.
2 http://docs.libmemcached.org/bin/memaslap.html.
3 https://github.com/Percona-Lab/tpcc-mysql.
4 Our latency-injection appliance is an open-source contributed project as part of
NetFPGA SUME since release 1.4.0.
5 The maximum latency introduced is a function of the conﬁgured line-rate. The appli-
ance can add up to 700 µs of latency at full 10 Gb/s rate, and up to 7 s at 100 Mbps.
204
N. Zilberman et al.
While the results above are obtained under optimal setup conditions, within
an operational data center worse-still results would be expected as latency is
further increased under congestion conditions and as services compete for com-
mon resources. The results of Fig. 1 show clearly that even a small increase in
latency, of the scale shown in this paper, can signiﬁcantly aﬀect an application’s
performance.
2 Experiments
This section presents experiments we used to provide a decomposition of the
latency between the application and the physical-wire of the host. Full results
of these experiments are given in Sect. 3 with the outcome of successive tests
presented in Table 1. Each experiment in this section is annotated with the
corresponding entry number in Table 1.
2.1 Tests Setup
For our tests setup we use two identical hosts running Ubuntu server 14.04 LTS,
kernel version 4.4.0-42-generic. The host hardware is a single 3.5 GHz Intel Xeon
E5-2637 v4 on a SuperMicro X10-DRG-Q motherboard. All CPU power-saving,
hyper-threading and frequency scaling are disabled throughout our tests. Host
adapter evaluation uses commodity network interface cards (NICs), Solarﬂare
SFN8522, and Exablaze X10, using either standard driver or a kernel bypass
mode (test dependent). For minimum latency, interrupt hold-oﬀ time is set to
zero. Each host uses identical NICs for that particular NIC experiment and
we only consider Ethernet-based communication. As illustrated in Fig. 3, an
Endace 9.2SX2 DAG card (7.5 ns time-stamping resolution) and a NetOptics
passive-optical tap are used to intercept client-server traﬃc and permit inde-
pendent measurement of client & server latency. The experiments are repro-
ducible using the procedures documented at http://www.cl.cam.ac.uk/research/
srg/netos/projects/latency/pam2017/.
2.2
In-Host Latency
Figure 2 illustrates the various elements contributing to the experienced latency
within the host.
Timestamp Counter Latency (1). To accurately measure latency, we set an
accuracy baseline for our methods. Our latency measurements are based on the
CPU’s Time Stamp Counter (TSC). TSC is a 64-bit register, present on the
processor, it counts the number of cycles since reset and thus provides a res-
olution of approximately 288 ps-per-cycle although realistically there is tens of
cycles resolution due to CPU pipeline eﬀects. Access to TSC is done using rdtsc
x86 assembly instruction. In order to understand hidden latency eﬀects, and
following the Intel recommendations for TSC access [10], we conduct two read
Where Has My Time Gone?
205
Fig. 2. End host tests setup.
Fig. 3. Client-server tests setup.
operations consecutively. We repeat this simple TSC read operation a large num-
ber of times (order of 1010 events), and study the time gaps measured between