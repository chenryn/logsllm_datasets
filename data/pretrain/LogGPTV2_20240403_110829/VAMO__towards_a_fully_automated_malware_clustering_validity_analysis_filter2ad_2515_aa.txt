# VAMO: Towards a Fully Automated Malware Clustering Validity Analysis

## Authors
- Roberto Perdisci
- ManChon U

### Affiliations
- University of Georgia, Athens, GA 30602
- Department of Computer Science
- PI: EMAIL

## Abstract
Malware clustering is a common technique used by analysts to manage the rapidly increasing number of distinct malware variants collected daily from the internet. While these systems are valuable for various applications, assessing the quality of their results is inherently challenging due to the lack of complete ground truth. Previous studies have proposed evaluating malware clustering results using labels assigned by multiple anti-virus (AV) scanners. However, these methods require semi-manual adjustments and mappings between labels from different AVs, and they often select a subset of samples for which a majority agreement on labels can be reached. This approach may bias the reference set towards "easy to cluster" samples, leading to an overoptimistic estimate of the clustering accuracy.

In this paper, we introduce VAMO, a system that provides a fully automated quantitative analysis of the validity of malware clustering results. Unlike previous work, VAMO does not rely on majority voting to reach a consensus across different AV labels and does not discard samples for which such a consensus cannot be reached. Instead, VAMO explicitly handles inconsistencies in AV labels to build a more representative reference set. Additionally, VAMO eliminates the need for semi-manual mapping between AV labels from different scanners. Through extensive evaluations in controlled and real-world settings, we demonstrate that VAMO outperforms majority voting-based approaches and offers a better method for malware analysts to automatically assess the quality of their clustering results.

## 1. Introduction
The widespread use of packing and other code obfuscation techniques has led to a significant increase in the number of new malware samples collected by anti-virus (AV) vendors. For example, Symantec reported collecting 286 million distinct malware variants in 2010 [19]. To handle this growing volume and enhance the scalability and effectiveness of malware analysis, several malware clustering and categorization systems have been proposed [1, 2, 3, 8, 11, 15, 18].

The primary goal of malware clustering systems is to group similar malware samples into families, where each family represents a set of related variants. These clustering results can be useful in various ways, such as categorizing new malware samples, deriving more generic detection signatures, identifying new malware families, and performing malware triage.

To fully leverage these benefits, malware clustering systems must be accurate. However, quantitatively assessing the accuracy of clustering results is challenging due to the lack of reliable ground truth. A common approach is to compare clustering results with a reference clustering derived from family labels assigned by multiple AV scanners [2, 11]. To address inconsistencies in AV labels, previous work [2, 18, 11] uses a majority voting approach to select samples for which there is an agreement on the family label. While this approach seems natural, it may result in an overoptimistic estimate of clustering accuracy by focusing on "easy to cluster" samples. For instance, in [2], only 2,658 out of 14,212 samples had a majority voting-based consensus, excluding over 80% of the samples from the validity analysis.

In this paper, we propose VAMO, a system that enables a fully automated quantitative analysis of the validity of malware clustering results. VAMO leverages multiple AV labels but avoids the limitations of majority voting and semi-manual mappings. Instead, it explicitly addresses inconsistencies in AV labels to build a more representative reference clustering. We evaluate VAMO through extensive experiments in both controlled and real-world settings, demonstrating its superiority over existing approaches.

## 2. Related Work
### Cluster Validity Analysis
Several studies have proposed cluster validity indexes and metrics. Halkidi et al. [7] provide a comprehensive survey of clustering validity indexes. Rendon et al. [17] compare internal and external clustering validity indexes, while MeilÄƒ [14] and Pfitzner et al. [16] introduce new metrics for comparing different clusterings. Fowlkes and Mallows [5] present a measure of similarity between hierarchical clusterings. Our approach is inspired by [5] but focuses on leveraging hierarchical clustering to generate a reference dendrogram and compare third-party clustering results to this dendrogram.

### Malware Clustering
Bailey et al. [1] conducted one of the first studies on behavior-based malware clustering and analyzed the inconsistency in AV labels. Bayer et al. [2] introduced a scalable behavior-based malware clustering system and validated their results using a majority voting-based approach. Hu et al. [8] used static analysis and function-call graphs for malware clustering, while [11] introduced BitShred for scalable malware clustering. Li et al. [12] highlighted challenges in evaluating malware clustering results and suggested that majority voting may lead to biased results. Our work addresses these challenges by providing a more robust and automated validation method.

## 3. Background
### 3.1 Measuring Inconsistency in AV Labels
We aim to quantify the inconsistency in AV labels, which has been qualitatively discussed in previous work [2, 15, 18] and analyzed in [1, 13]. Creating a manual mapping between malware family labels and correcting inconsistent labels, as required in previous work, is a difficult task. We show that in many cases, no majority voting-based consensus can be reached. We performed measurements on a dataset of 1,108,289 distinct malware samples, labeled by four major AV vendors (Symantec, McAfee, Avira, and Trend Micro). The detection rates, number of distinct labels, and distinct family labels vary significantly across the AVs. For example, AV3 assigned the same label to 317 different samples on average, while AV4 assigned the same label to approximately six samples. Only 420,920 (38%) of the samples were labeled by more than two different AVs, suggesting that a majority voting approach would fail to reach a consensus for at least 38% of the samples.

### Table 1: AV Labels for a Dataset of 1,108,289 Distinct Malware Samples
| AV | Detected Samples | Detection Rate (%) | Distinct AV Labels | Distinct Family Labels | Distinct First Variants |
|----|------------------|--------------------|--------------------|------------------------|-------------------------|
| AV1 | 590,341          | 53.3%              | 20,217             | 3,330                  | 20,217                  |
| AV2 | 825,766          | 74.5%              | 15,138             | 4,729                  | 13,851                  |
| AV3 | 702,124          | 63.4%              | 2,208              | 1,710                  | 2,199                   |
| AV4 | 1,030,354        | 93.0%              | 175,333            | 3,520                  | 51,732                  |

This table summarizes the dataset of AV labels, highlighting the significant variations in detection rates and label assignments across different AVs.