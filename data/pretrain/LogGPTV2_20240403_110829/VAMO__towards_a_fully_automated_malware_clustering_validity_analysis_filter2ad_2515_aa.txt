title:VAMO: towards a fully automated malware clustering validity analysis
author:Roberto Perdisci and
ManChon U
VAMO: Towards a Fully Automated
Malware Clustering Validity Analysis
ManChon U
Roberto Perdisci
University of Georgia
Athens, GA 30602
University of Georgia
Athens, GA 30602
Dept. of Computer Science
Dept. of Computer Science
PI:EMAIL
PI:EMAIL
ABSTRACT
Malware clustering is commonly applied by malware analysts to
cope with the increasingly growing number of distinct malware
variants collected every day from the Internet. While malware clus-
tering systems can be useful for a variety of applications, assess-
ing the quality of their results is intrinsically hard. In fact, clus-
tering can be viewed as an unsupervised learning process over a
dataset for which the complete ground truth is usually not available.
Previous studies propose to evaluate malware clustering results by
leveraging the labels assigned to the malware samples by multiple
anti-virus scanners (AVs). However, the methods proposed thus far
require a (semi-)manual adjustment and mapping between labels
generated by different AVs, and are limited to selecting a reference
sub-set of samples for which an agreement regarding their labels
can be reached across a majority of AVs. This approach may bias
the reference set towards “easy to cluster” malware samples, thus
potentially resulting in an overoptimistic estimate of the accuracy
of the malware clustering results.
In this paper we propose VAMO, a system that provides a fully
automated quantitative analysis of the validity of malware cluster-
ing results. Unlike previous work, VAMO does not seek a majority
voting-based consensus across different AV labels, and does not
discard the malware samples for which such a consensus cannot
be reached. Rather, VAMO explicitly deals with the inconsisten-
cies typical of multiple AV labels to build a more representative
reference set, compared to majority voting-based approaches. Fur-
thermore, VAMO avoids the need of a (semi-)manual mapping be-
tween AV labels from different scanners that was required in previ-
ous work. Through an extensive evaluation in a controlled setting
and a real-world application, we show that VAMO outperforms ma-
jority voting-based approaches, and provides a better way for mal-
ware analysts to automatically assess the quality of their malware
clustering results.
1.
INTRODUCTION
Due to the extensive use of packing and other code obfuscation
techniques [6], the number of new malware samples collected by
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’12 Dec. 3-7, 2012, Orlando, Florida USA
Copyright 2012 ACM 978-1-4503-1312-4/12/12 ...$15.00.
anti-virus1 (AV) vendors has grown enormously in recent years,
reaching tens or even hundreds of thousand of new malware sam-
ples collected per day (e.g., in 2010 Symantec collected 286 mil-
lion distinct malware variants [19]). To cope with this increasingly
growing number of malware samples and boost the scalability and
effectiveness of current malware analysis infrastructures, a number
of malware clustering and automatic malware categorization sys-
tems have been recently proposed [1, 2, 3, 8, 11, 15, 18].
The main objective of malware clustering systems is to group
malware samples into families, whereby samples that are similar to
each other can be considered as variants of the same malware fam-
ily. Intuitively, malware clustering results can be useful in several
ways. For example, new malware samples that are clustered with
known malware variants of a given family f may be also catego-
rized as belonging to f. In turn, these newly discovered variants
may be used to derive more generic malware detection signatures
that have a better chance to match future variants of the same fam-
ily [15]. In addition, malware clustering results may make it easier
to identify new, previously unknown malware families [2], or may
be used to perform malware triage [11], thus allowing malware ex-
perts to select only a small number of variants of a given malware
family for manual analysis.
To take full advantage of the above mentioned beneﬁts, malware
clustering systems clearly need to be accurate. Unfortunately, it is
very challenging to quantitatively assess the accuracy of malware
clustering results, because of the lack of reliable ground truth. A
common approach to validating the quality of malware clustering
results is to compare them to a reference clustering obtained by
leveraging family labels assigned to the samples by multiple AV
scanners [2, 11]. To compensate for inconsistencies in the AV la-
bels, both [2, 18] and [11] use a majority voting approach to select
the samples for which an agreement regarding their AV family la-
bel can be reached. Therefore, a cluster in the reference clustering
will include all samples belonging to the same AV family. How-
ever, while this approach may appear as a natural choice in absence
of complete ground truth, Li et al. [12] have suggested that it may
result in an overoptimistic estimate of the malware clustering ac-
curacy. In particular, limiting the reference clustering to samples
for which a majority voting-based consensus on the family label
can be reached, and discarding the remaining ones, may reduce the
reference clusters to only include “easy to cluster” malware sam-
ples (i.e., clear-cut cases of malware samples that are very similar
to each other) [12], thus potentially causing the accuracy of the
malware clustering results to be largely overestimated. In fact, the
1While “anti-malware” is probably a more appropriate term, we
use “anti-virus” because that is the way in which many vendors of
malware scanners and defense solutions still advertise their prod-
ucts.
experiments reported in [2] state that among 14,212 malware sam-
ples, a majority voting-based consensus could be reached only for
2,658 cases. That is, more than 80% of the samples in the clustering
results had to be excluded from the cluster validity analysis.
In this paper we propose VAMO2, a system that enables an au-
tomatic quantitative analysis of the validity of malware clustering
results. Like previous work, VAMO leverages the labels assigned
to malware samples by multiple AV scanners to construct a refer-
ence clustering. However, unlike previous work, VAMO does not
seek a majority voting-based consensus, and does not discard the
samples for which such a consensus cannot be reached. Rather,
VAMO explicitly deals with (and aims to mitigate the effect of) the
inconsistencies typical of the AV labels to build a more representa-
tive reference clustering. Furthermore, VAMO avoids the need of
a (semi) manual mapping between AV labels from different scan-
ners that was required in previous work (notice that while some
efforts exist to standardize the “language” used to assign the AV
labels (e.g., http://maec.mitre.org), so far they have not
been successful). Also, we would like to emphasize that while AV
labels suffer from some limitations, as we discuss more in detail
in Section 7, they are used as a reference by many researchers be-
cause it is hard to obtain a more accurate ground truth for datasets
containing tens of thousands of malware samples.
VAMO leverages historic malware archives and the related mul-
tiple AV labels to learn an AV Label Graph (see Figure 1). An AV
Label Graph (see Section 5.1) is deﬁned as an undirected weighted
graph, which aims to: (1) automatically learn the mapping between
malware family names assigned by different AVs, thus avoiding the
need to manually build or adjust such mappings; (2) identify cases
in which one (or more) AV scanners tend to inconsistently use sev-
eral family names to label samples that belong to the same family
according to other competitors’ scanners; (3) learn the level of sim-
ilarity between AV labels assigned by different AV scanners, by
looking at the number of times that certain malware family labels
are jointly assigned to the same samples. While the concept of AV
Label Graph was ﬁrst introduced in [15], here we reﬁne its deﬁni-
tion and use it in the context of our novel VAMO system. Also, it
is worth noting that the AV Label Graph is only one component of
the entire VAMO system.
Learning the AV Label Graph enables us to measure the similar-
ity between malware samples in a dataset based purely on their AV
labels (see Section 5.1 for details). As shown in Figure 1, given
a malware dataset M and the related multiple AV labels assigned
to its malware samples, we can (a) apply a third-party malware
clustering algorithm (e.g., [2, 11, 15, 18]) on M to partition it in
a number of malware clusters, (b) use VAMO to build a reference
clustering for M using similarities among its samples measured ac-
cording to their AV labels, and (c) compute the level of agreement
between VAMO’s reference clustering and the third-party malware
clustering results, thus quantitatively assessing their quality.
In summary, this paper makes the following contributions:
• We propose a novel system, called VAMO, that enables a
fully automated malware clustering validity analysis.
• We perform an extensive evaluation of how different types
of AV label inconsistencies may negatively impact a valid-
ity analysis performed via majority voting-based approaches,
and show the advantages that VAMO brings over previous
work.
• We perform experiments with real-world malware archives,
and demonstrate how VAMO can be applied in practice to
2Validity Analysis of Malware-clustering Outputs.
assess the quality of malware clustering results over large
malware datasets.
2. RELATED WORK
Cluster Validity Analysis Besides the clustering validity indexes
reported in Halkidi et al.’s survey [7], which we summarize in Sec-
tion 3.2, a number of alternative validity indexes have been pro-
posed. In [17], Rendon et al. present a comparison of internal and
external clustering validity indexes, while Meil˘a [14] and Pﬁtzner
et al. [16] introduce a number of new metrics to compare two differ-
ent clusterings. In [5], Fowlkes and Mallows introduce a measure
of similarity between two hierarchical clusterings obtained by cut-
ting the two dendrograms at heights h1 and h2, respectively, which
yield the same number of clusters k. Than, for each value of k, the
number of matching entries from the two different clusterings are
counted to obtain a measure of comparison. Our approach to clus-
ter validity analysis (Section 5) is inspired by [5]. However, our
method does not focus on comparing different hierarchical cluster-
ings. Rather, VAMO leverages hierarchical clustering to generate
a reference clustering dendrogram, and compares third-party clus-
tering results to this dendrogram by ﬁnding the cut height h that
yields the maximum agreement between the third-party results and
VAMO’s reference clustering.
Malware Clustering
Bailey et al. [1] presented one of the ﬁrst
studies on behavior-based malware clustering. Furthermore, in [1]
the authors presented a quantitative analysis of the inconsistency in
the labels assigned by different AVs. Bayer et al. [2] introduced a
much more scalable way to perform behavior-based malware clus-
tering. In addition, they proposed to validate their clustering re-
sults by comparing them against a clustering obtained using a ma-
jority voting-based approach over multiple malware family labels
assigned to the samples by six different AVs [2]. A similar valida-
tion approach was used in [18]. In [8], Hu et al. perform malware
clustering using static analysis, instead of behavior-based features,
by leveraging function-call graphs, while [11] introduces a system
called BitShred that aims to improve scalability in malware cluster-
ing systems.
In [12], Li et al. discuss a number of challenges related to the
evaluation of results generated by malware clustering systems. In
particular, by using plagiarism detection algorithms to measure the
similarity between malware samples, they show that a factor con-
tributing to the strong results reported in [2] might be that the 2,658
validation instances selected via majority voting on multiple AVs
are simply easy to classify. However, no complete solution is of-
fered on how to perform a better malware clustering validity anal-
ysis. Our work is a step forward towards such a solution.
While most malware clustering systems are based on system-
level behavior or static-analysis-based features, [15] proposed a
malware clustering system that focuses on the network behavior
of malware and introduced the concept of AV Label Graph, which
we reﬁne and use in this paper in the context of VAMO. It is worth
noting that the use of AV Label Graphs in [15] is signiﬁcantly dif-
ferent from this paper. Previous work did not present a comprehen-
sive malware clustering validity analysis system, and the cohesion
and separation validity indexes used in [15] were mainly internal
validity indexes that required a signiﬁcant amount of interpretation
through manual analysis. On the other hand, VAMO introduces a
comprehensive, fully automated malware clustering validity analy-
sis process that can more readily be used to select the parameters of
a malware clustering system, or to compare results obtained using
different clustering algorithms.
Detected samples
Detection rate (%)
Distinct AV labels
Distinct family labels
Distinct ﬁrst variants
AV1
590,341
53.3%
20,217
3,330
20,217
AV2
825,766
74.5%
15,138
4,729
13,851
AV3
702,124
63.4%
2,208
1,710
2,199
AV4
1,030,354
93.0%
175,333
3,520
51,732
Table 1: AV labels for a dataset of 1,108,289 distinct malware
samples.
3. BACKGROUND
In this Section, we ﬁrst provide quantitative information regard-
ing the inconsistency typical of multiple AV labels. Then, we dis-
cuss the background concepts that we will use to perform auto-
mated clustering validity analysis.
3.1 Measuring Inconsistency in AV Labels
In this Section, we aim to quantify the “inconsistency” typical of
multiple AV labels that has been qualitatively discussed in previ-
ous work [2, 15, 18], and analyzed more in details in [1, 13]. Our
main goal is to suggest that (semi-) manually creating a mapping
between malware family labels and correct the inconsistent (or er-
roneous) labels, which was required in previous work to perform
malware cluster validity analysis (e.g., in [2]), is in fact a fairly
difﬁcult task. In addition, we show that in a large number of cases
no majority voting-based consensus can be reached. Our results
conﬁrm previous ﬁndings [1] by using a more recent and much
larger malware dataset.
To this end, we performed a number of measurements over a
large dataset of AV labels assigned by four different major AV ven-
dors (namely, Symantec, McAffee, Avira, and Trend Micro) to a
set of 1,108,289 distinct malware samples3. These malware sam-
ples were collected from different sources over the course of one
entire year, from 2011-01-01 to 2011-12-31 (it is worth noting that
we only consider malware samples that were detected as such by at
least one out of the four AV scanners). The AVs used to scan the
samples were updated daily, and each malware sample was scanned
with each AV once a day for 30 days4, starting from the day in
which the sample was collected. In the following, we will refer to
the four AV scanners, in no particular order, as AV1, AV2, AV3, and
AV4. We intentionally mask the speciﬁc AV vendor names, when
reporting the results, to avoid controversy (the results we report
may be seen as damaging to one or more vendors, due to their low
detection rate). After all, we do not intend to establish what vendor
performs the best over our malware dataset. Rather, we focus on
the inconsistencies in the malware labels, both within a given AV
vendor as well as across vendors.
3.1.1 Overview
Table 1 summarizes our AV label dataset. As we can see, the de-
tection rate, number of distinct (complete) labels, and the number
of distinct malware family labels varies greatly across the differ-
ent AV scanners. For example, AV3 assigned a label to 702,124
(63.4%) malware samples, but the number of distinct labels was
only 2,208. This means that, in average, the same label was as-
signed to 317 different samples. This behavior is very different
3This dataset was kindly provided by a well-known security com-
pany.
4If an AV scanner AVi detected a sample m and assigned it a label
on day d  30.
from the other AVs, and in particular from AV4 for which in av-
erage the same label was assigned to (approximately) six samples.
In addition, among the 1,108,289 distinct malware samples, only
420,920 (38%) were labeled (i.e., detected) by more than two dif-
ferent AVs. This suggests that because a majority voting approach
would require three out of four AVs to agree on the labels (two
out of four would only represent a tie), in our example scenario
no majority voting-based consensus can be reached on the correct
malware family label for at least 38% of the samples. This prob-