0
7
0
accuracy and reliability. However, we found that the industry image
sensors that are not designed for IR light also can detect the IR light.
Specifically, current industry image sensors are either a Comple-
mentary Metal-Oxide Semiconductor (CMOS) or Charge-coupled
Device (CCD), which uses arrays of silicon to convert the inci-
dent light (photons) into electronic charge (electrons). As shown in
Figure 3 (a), since silicon has high sensitivity in both visible and
invisible IR spectrum, the image sensor can be triggered to detect
the invisible IR light [8]. However, human eyes are only able to
detect lights with wavelengths from 380𝑛𝑚 to 700𝑛𝑚 [10], which
is shown in Figure 3 (b). To prove the concept, we use 850𝑛𝑚 and
940𝑛𝑚 IR light LEDs (3W) to generate invisible light and test those
lights on the most recent SONY IMX598 and IMX689 [22] image
sensors. As we can observe in Figure 4, although different image
sensors have different IR light sensitivities, both 850𝑛𝑚 and 940𝑛𝑚
IR lights still successfully trigger the image sensors. In addition,
since the wavelengths of IR lights are close to the wavelength of
red light, the invisible lights are detected as magenta color lights in
the camera.
Ideally, the attacker can use any IR lights that close to 700𝑛𝑚 to
attack the AV without human driver’s notice. As the wavelength
Radar SensorCameraUltrasonic SensorLiDAR SensorIMUSensor HubEnvironment PerceptionMultisensor Data FusionSensor FusionObstacle TrajectoryPedestrian  TrajectoryVehicle TrajectoryTrajectory PredictionDecision MakingRoute PlanningWheel SpeedRTK GPS…Motion PlanningBehavior PlanningLocalization and MappingMulti-Camera  FusionSLAMPre-processingRadar LayerSonar LayerLiDAR LayerVision Layer…30080090010001100700600500400Wavelength (nm)Temperature 25 ℃Photosensitivity (A/W)00.10.20.30.40.50.60.7Human visible light spectrum:  380nm to  700nm(a) Spectral sensitivity of silicon photo detector(b) Spectral sensitivity of a human eyeWAVE LENGTH IN NANOMETER700600580550475450400(a) SONY IMX598 with 850nm IR light (c) SONY IMX598 with 940nm IR light (b) SONY IMX689 with 850nm IR light (d) SONY IMX689 with 940nm IR light Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1932relies on the invisible light to attack the autonomous vehicle. By
carefully setting up the attacks in proper driving scenarios, the
human driver will not notice the IR light attacks.
II) The enterprise-level autonomous vehicle has to trust the data
gathered from cameras. Normally, to increase the environment per-
ception accuracy, the autonomous vehicle utilizes the data gathered
from multiple sensors to perform environment perception. Then,
during the sensor fusion process, an Extended Kalman Filter [3]
is implemented to dynamically assign different gains (i.e., Kalman
gains) to the sensors according to the current driving scenarios. For
example, the Radar sensor suffers high wireless interference in the
indoor parking lot, which will result in a relatively low Kalman gain.
On the other hand, LiDAR will be assigned with a high Kalman gain
to improve the environment perception accuracy. However, camera
data is critical for the AV (e.g., perform object detection and recog-
nition, localization and mapping, etc). It is important for an AV to
understand the shape, color and texture of an object, which cannot
be done by other sensors. As a result, the autonomous vehicle has
to trust the camera to perform autonomous driving.
III) The Invisible IR light is detected as the visible magenta color
light in the camera, which is difficult for an AV to verify if the
light is reflected by a real object or generated by an IR light source.
Moreover, since most people cannot see the IR light, the heavily
relied on ’human eyes defense strategy’ does not work properly. In
other words, the driver is unaware of the ICSL Attack until the AV
has already made unexpected harmful driving behaviors. To make
the situation even worse, although multiple approaches have been
proposed to defend against attacks on AV’s cameras [44, 46, 52, 70],
they are not designed to distinguish the IR lights from ambient
lights. In addition, these approaches require complex deep learning
algorithms and high computation resources, which is not suitable
for an autonomous vehicle with strict cost restrictions.
Figure 5: (a) The attacker creates fake invisible traffic signal;
(b) The attacker uses IR light to alter AV’s environment per-
ception results; (c) The attacker can blind the AV’s camera to
create frequent system alters; (d) The attacker controls the
IR sources on the road to alter AV’s SLAM results.
of selected IR light close to 700𝑛𝑚, the detected light intensity by
the camera will be high, which can increase the attack distance.
However, we found that some people can perceive lights with wave-
lengths that slightly larger than 700𝑛𝑚. This is because IR light
activates the human photoreceptors through a nonlinear optical
process, which enables human to visualize the near IR light [54].
To successfully attack the AV without human notice, we conduct a
survey of 100 men and 100 women with ages vary from 18 to 50
to see how many people can perceive IR lights. As shown in Table
1, only 3% of men and 4% of women can perceive IR lights with
wavelengths larger than 780𝑛𝑚 while no one can see the IR light
with wavelengths larger than 900𝑛𝑚. Since the lower wavelengths
IR lights can be detected by most of cameras with higher inten-
sity and the color is close to red, in this paper, we mainly utilize
780𝑛𝑚 − 850𝑛𝑚 IR lights to implement the attack.
3 ATTACK OVERVIEW AND THREAT MODEL
In this section, we first present the overview of I-Can-See-the-Light
Attack (ICSL Attack). Then, we introduce the threat model.
3.1 ICSL Attack Overview
Attack Goal. Our attack goal is to alter the environment percep-
tion and SLAM results of an autonomous vehicle embedded with
different sensors (i.e., Camera, Radar, USS, LiDAR and GPS etc). To
do this, we mainly attack the cameras on the AV by using invisi-
ble IR lights. As a result, the target vehicle will make unexpected
harmful driving behaviors. In the worst case, the autonomous vehi-
cle will wrongfully change its driving behavior, such as terminate
the autopilot mode, reducing its driving speed or even make un-
expected stop, etc. In order to make the human driver unaware of
the ICSL attack and the presence of attackers, we mainly utilize IR
light with wavelengths larger than 780𝑛𝑚 to attack AV’s cameras.
The Vulnerability. The AV is vulnerable to ICSL Attack for the
following reasons:
I) Human cannot see IR lights. Previous attacks on AV’s camera
are mainly based on the visible light, such as creating visible objects
[52] and toxic traffic signs [50, 70]. However, modern AVs require
the human driver to always be aware of the driving conditions (i.e.,
Defense Driving Strategy.) Therefore, although these attacks can
effectively attack the autonomous vehicle, they also can be detected
by the human driver. Different from these attacks, ICSL mainly
IV) During the SLAM process, it is possible for the invisible
IR source to be selected as the key points, which may vary the
localization and mapping results. Specifically, in order to guaran-
tee real-time processing, AVs mainly utilize ORB-SLAM-related
architectures to extract key points from each frame to perform lo-
calization and mapping [33, 51]. The key points extraction process
depends on the intensity weighted centroid of the patch in each
frame. However, since IR lights are bright and significantly different
from the background lights, the corresponding IR light pixels in
the camera may be selected as the key points. Therefore, when the
attacker moves the key points or turns off the IR lights, the SLAM
results in an AV may suffer high variance.
3.2 Threat Model
We assume the attacker knows the positions of the cameras on the
target autonomous vehicle. Since autonomous vehicle companies
normally publicly announce their autonomous driving solutions
and their hardware vendors for advertisement purposes, the at-
tacker can easily get the camera specifications by browsing the
internet. We also assume the attacker has the basic knowledge of IR
lights. As shown in Figure 5, to perform the ICSL Attack, multiple
IR light sources can either be deployed on attackers’ vehicles, flying
drones or on the road. In addition, we assume the attacker can set
up the attack in the night or find proper attack scenarios to reduce
the possibility of being detected by others. In addition, the attacker
(a)(b)(c)(d)Autonomous VehicleAV’s VisionAttackerInvisible  IR lightMultiple IR  Light SourcesInvisible  IR lightUnpredicted StopFake Traﬃc  SignalAutonomous VehicleAV’s VisionAutonomous VehicleAV’s VisionInvisible  IR lightAttackerAutonomous VehicleAV’s VisionSession 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1933IR light and considers it as the red traffic signal. Moreover, when
the IR light intensity is 30𝑊 , Tesla also shows the line to make the
stop. On the contrary, the human driver cannot see the red traffic
signal generated by IR light in this scenario.
Analysis. Since Tesla does not validate the size of the traffic light,
even a smaller traffic light model is considered as a legitimate traffic
light. Moreover, the color of the IR light in the camera is close to the
red. As a result, Tesla detects the fake invisible IR light and considers
it as a legitimate red traffic signal. According to this experiment,
the attacker can either modify the existing legitimate traffic light
on the road to create the invisible red traffic signal or even build a
smaller invisible traffic light to change the driving behavior of an
AV without human driver notice. Since the human cannot see the
IR light, the AV will make unpredicted driving behavior before the
driver has a chance to take control.
Figure 6: (a) Experiment setup: we implement the IR light LEDs in the traffic light to create fake invisible red signal. (b) Tesla
detects the fake red light signal; (c) The height of the traffic light will affect the attack result; (d) The fake traffic signal detected
distance vs. IR light intensity.
has an unlimited budget to purchase proper attack devices (i.e.,
drones and small IR LEDs). Specifically, the attacker can the create
following harmful results by performing the following attack:
• Alter Environment Perception Results. As shown in Figure
5 (a), the attacker can deploy the IR light source on the traffic light
to create fake traffic signals. Since the IR light is close to the red
color in the AV’s camera, the AV may make an unpredicted stop,
which will result in a traffic jam or potential accident. In Figure 5
(b), an attacker can also drive the vehicle close to the target AV and
uses IR light to alter AV’s environment perception results.
• Ruin the In-car User Experience. In Figure 5 (c), the attacker
can fly a drone with equipped IR light source to blind AV’s tar-
get cameras. By doing this, the AV will frequently show system
alert messages. Since human cannot see the IR light, the alert mes-
sages may be considered as system bugs and ruin the in-car user
experience.
• Introduce SLAM Errors. In Figure 5 (d), by deploying multiple
IR light sources on the road, the AV will select several detected IR
light sources in each frame as the key points for SLAM propose.
Then, by dynamically changing IR light sources’ positions and light
intensities, the AV will surfer relatively high SLAM errors.
4 ICSL ATTACKS ON TESLA
In this section, we mainly use ICSL Attack to 1) alter environment
perception results and 2) ruin the in-car user experience. We use
iPhone 12 pro to take photos in this paper. During the entire exper-
iment, human eyes cannot see IR lights.
4.1 Alter Environment Perception Results
In this experiment, we show how ICSL Attack can alter environment
perception results on Tesla Model 3. Then, we analyze the security
insight and discuss the related parameters that the attacker should
determine to improve the attack success rate.
4.1.1 Create Fake Invisible Traffic Light. We first create the fake
invisible traffic signal to alter the environment perception results
of Tesla, which is similar to the attack scenario in Figure 5 (a).
Experiment Setup. As shown in Figure 6 (a), since we cannot
make modifications to a real traffic light on the road, we embed IR
light LEDs on a smaller traffic light model (31.19×25.60×13.69𝑐𝑚) to
create fake invisible red traffic signals. We believe that it is sufficient
to prove the effectiveness of ICSL Attack. In practice, a real attacker
can implement IR LEDs with color close to red on a real traffic light
to conduct the attack.
Experiment Result. Figure 6 (b) shows the attack results when
the IR light intensities are 20𝑊 and 30𝑊 . Tesla detects the invisible
In Figure 6 (c) and (d), we analyze the parameters that will af-
fect ICSL Attack, including the height of the traffic light, the light
intensity, and the distance to the AV. In Figure 6 (c), we show that
the height of the traffic light will affect the attack results. Since the
height of the traffic light is normally fixed, the fake traffic light with
a smaller height will be misclassified as an obstacle. According to
our experiment, to avoid attack failure, the attacker should make
sure that the height of the fake traffic light is at least larger than
2.45𝑚. As shown in Figure 6 (d), we show the maximum attack
distances under different IR light intensities. When the IR light
intensity is smaller than 15𝑊 , Tesla cannot detect the fake traffic
light regardless of the distance. As the IR light intensity increases,
the distances for Tesla to detect the fake traffic signal also increase.
When the light intensity reaches 30𝑊 , the maximum attack dis-
tance is around 10𝑚. Therefore, the attacker should at least use the
15𝑊 IR light and make sure the height of the traffic light is higher
than 2.45𝑚 in order to successfully attack Tesla.
4.1.2 Create Fake Objects. In this experiment, we assume the at-
tacker is driving in front of the AV and uses a drone with equipped
850𝑛𝑚 IR light sources to create fake objects, which is similar to
the attack scenario in Figure 5 (b).
Experiment Setup. In this section, we show how to utilizes ICSL
Attack to create fake objects. Since Tesla utilizes a triple forward
camera and fuses the perception results to increase the detection
accuracy, simply attacking a single camera will not alter the envi-
ronment perception results. Therefore, we first utilized a DJI Robot
Master S1 [13] equipped with a 850𝑛𝑚 IR light source to blind the
right main forward camera and the narrow forward camera, which
is shown in Figure 7 (a). Then, we utilized a drone equipped with six
Infrared Light LEDs (30W)Infrared Light LEDs (20W)Infrared Light LEDs (30W)Infrared Light LEDs1015202530IR Light Intensity (W)0246810Maximum Detected Distance (m)(a)(b)(c)(d)Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1934Figure 7: (a) Experiment setup: we use the IR light to blind the right main forward camera and the narrow forward camera of
Tesla. (b) The environment perception results of Tesla is accurate (only one vehicle is in front of Tesla); (c) By implementing
ICSL Attack on a drone and attacking the right main forward camera, Tesla detects multiple surrounding objects.
Figure 8: (a) Experiment setup: Blind the triple forward cameras of Tesla. (b) System Alert is triggered. (c) Experiment setup:
Blind the left side camera. (d) System Alert is triggered.
850𝑛𝑚 IR light LEDs (3W) to attack the left main forward camera,
which is shown in Figure 7 (c).
Experiment Result. Figure 7 (b) shows the ground truth when
Tesla is not under ICSL Attack. In this scenario, Tesla successfully
detects the front vehicle even if the vehicle is making a left turn
(only the left main forward camera and the narrow forward camera
can ’see’ the vehicle). However, as shown in Figure 7 (c), when
we utilize drones equipped with IR light LEDs to attack the left
main forward camera, the environment perception results of Tesla
shows that there are two vehicles (one sedan and one truck) and
a pedestrian is in front of the Tesla and the sedan is making a left
turn.
Analysis. In this experiment, since the narrow forward camera
and the right main forward camera are blinded by the IR light, only
the left main camera is used to detect the front objects. Therefore,
when this camera is under ICSL Attack, Tesla still has to believe
the information provided by the left camera. Moreover, during the
entire experiment, Tesla’s system alert is not triggered. This is
because we only blind two cameras while the left main camera can
still detect the front objects. In addition, Tesla tends to consider the
IR light sources equipped on the drone as the rear position lamps
from a legitimate vehicle. As a result, multiple fake objects are
detected by Tesla. During the experiment, since the human cannot