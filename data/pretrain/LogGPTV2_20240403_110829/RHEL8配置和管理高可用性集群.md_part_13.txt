5.  创建 crypt 资源，作为 `shared_vg1`{.literal} 卷组的一部分。
    ``` literallayout
    [root@z1 ~]# pcs resource create crypt --group shared_vg1 ocf:heartbeat:crypt crypt_dev="luks_lv1" crypt_type=luks2 key_file=/etc/crypt_keyfile encrypted_dev="/dev/shared_vg1/shared_lv1"
    ```
:::
::: title
**验证步骤**
:::
确保加密资源已创建了加密设备，本例中为
`/dev/mapper/luks_lv1`{.literal}。
``` literallayout
[root@z1 ~]# ls -l /dev/mapper/
...
lrwxrwxrwx 1 root root 7 Mar 4 09:52 luks_lv1 -> ../dm-3
...
```
:::
::: section
::: titlepage
## []{#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters.html#_format_the_encrypted_logical_volume_with_a_gfs2_file_system_and_create_a_file_system_resource_for_the_cluster}使用 GFS2 文件系统格式化加密的逻辑卷，并为群集创建文件系统资源。 {.title}
:::
::: itemizedlist
**先决条件**
-   您已加密了逻辑卷并创建了 crypt 资源。
:::
::: orderedlist
**流程**
1.  在集群的一个节点中，使用 GFS2
    文件系统格式化卷。每个挂载文件系统的节点都需要一个日志。确保为集群中的每个节点创建足够日志。
    ``` literallayout
    [root@z1 ~]# mkfs.gfs2 -j3 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/mapper/luks_lv1
    /dev/mapper/luks_lv1 is a symbolic link to /dev/dm-3
    This will destroy any data on /dev/dm-3
    Are you sure you want to proceed? [y/n] y
    Discarding device contents (may take a while on large devices): Done
    Adding journals: Done
    Building resource groups: Done
    Creating quota file: Done
    Writing superblock and syncing: Done
    Device:                    /dev/mapper/luks_lv1
    Block size:                4096
    Device size:               4.98 GB (1306624 blocks)
    Filesystem size:           4.98 GB (1306622 blocks)
    Journals:                  3
    Journal size:              16MB
    Resource groups:           23
    Locking protocol:          "lock_dlm"
    Lock table:                "my_cluster:gfs2-demo1"
    UUID:                      de263f7b-0f12-4d02-bbb2-56642fade293
    ```
2.  创建文件系统资源，以便在所有节点上自动挂载 GFS2 文件系统。
    不要将文件系统添加到 `/etc/fstab`{.literal} 文件中，因为它将作为
    Pacemaker 群集资源进行管理。挂载选项可作为资源配置的一部分通过
    `options=`{.literal} 选项指定。运行
    `pcs resource describe Filesystem`{.literal}
    命令以查看完整配置选项。
    以下命令创建文件系统资源。这个命令在包含该文件系统逻辑卷资源的资源组中添加资源。
    ``` literallayout
    [root@z1 ~]# pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/mapper/luks_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence
    ```
:::
::: orderedlist
**验证步骤**
1.  验证 GFS2 文件系统是否已挂载到群集的两个节点上。
    ``` literallayout
    [root@z1 ~]# mount | grep gfs2
    /dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
    [root@z2 ~]# mount | grep gfs2
    /dev/mapper/luks_lv1 on /mnt/gfs1 type gfs2 (rw,noatime,seclabel)
    ```
2.  检查集群的状态。
    ``` literallayout
    [root@z1 ~]# pcs status --full
    Cluster name: my_cluster
    [...]
    Full list of resources:
      smoke-apc      (stonith:fence_apc):    Started z1.example.com
      Clone Set: locking-clone [locking]
          Resource Group: locking:0
              dlm    (ocf::pacemaker:controld):      Started z2.example.com
              lvmlockd       (ocf::heartbeat:lvmlockd):      Started z2.example.com
          Resource Group: locking:1
              dlm    (ocf::pacemaker:controld):      Started z1.example.com
              lvmlockd       (ocf::heartbeat:lvmlockd):      Started z1.example.com
         Started: [ z1.example.com z2.example.com ]
      Clone Set: shared_vg1-clone [shared_vg1]
         Resource Group: shared_vg1:0
                 sharedlv1      (ocf::heartbeat:LVM-activate):  Started z2.example.com
                 crypt       (ocf::heartbeat:crypt) Started z2.example.com
                 sharedfs1      (ocf::heartbeat:Filesystem):    Started z2.example.com
        Resource Group: shared_vg1:1
                 sharedlv1      (ocf::heartbeat:LVM-activate):  Started z1.example.com
                 crypt      (ocf::heartbeat:crypt)  Started z1.example.com
                 sharedfs1      (ocf::heartbeat:Filesystem):    Started z1.example.com
              Started:  [z1.example.com z2.example.com ]
    ...
    ```
:::
:::
:::
::: section
::: titlepage
# []{#assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_migrate-gfs2-rhel7-rhel8-configuring-gfs2-cluster}将 GFS2 文件系统从 RHEL7 迁移到 RHEL8 {.title}
:::
这个过程允许您在配置包含 GFS2 文件系统的 RHEL 8 集群时使用现有 Red Hat
Enterprise 7 逻辑卷。
在 Red Hat Enterprise Linux 8 中，LVM 使用 LVM 锁定守护进程
`lvmlockd`{.literal} `而不是 clvmd`{.literal}
来管理主动/主动集群中的共享存储设备。这要求您配置作为共享逻辑卷使用的主动/主动集群所需的逻辑卷。另外，这需要您使用
`LVM 激活的资源`{.literal} 来管理 LVM 卷，并使用 `lvmlockd`{.literal}
资源代理来管理 `lvmlockd`{.literal} 守护进程。有关 [配置使用共享逻辑卷的
GFS2 文件系统的 Pacemaker
集群，请参阅](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-gfs2-in-a-cluster.adoc-configuring-gfs2-cluster){.link}
在集群中配置 GFS2 文件系统。
要在配置包含 GFS2 文件系统的 RHEL8 集群时使用现有的 Red Hat Enterprise
Linux 7 逻辑卷，请从 RHEL8 集群中执行以下步骤。在本例中，群集式 RHEL 7
逻辑卷是卷组 `upgrade_gfs_vg`{.literal} 的一部分。
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
RHEL8 集群的名称必须与 RHEL7 集群的名称相同，其中包括 GFS2
文件系统才能使现有文件系统有效。
:::
::: orderedlist
**流程**
1.  确定包含 GFS2
    文件系统的逻辑卷当前不活跃。只有所有节点都停止使用卷组时，这个步骤才安全。
2.  从集群中的一个节点中，强制将卷组更改为本地。
    ``` literallayout
    [root@rhel8-01 ~]# vgchange --lock-type none --lock-opt force upgrade_gfs_vg
    Forcibly change VG lock type to none? [y/n]: y
      Volume group "upgrade_gfs_vg" successfully changed
    ```
3.  从集群中的一个节点，将本地卷组改为共享卷组
    ``` literallayout
    [root@rhel8-01 ~]# vgchange --lock-type dlm upgrade_gfs_vg
       Volume group "upgrade_gfs_vg" successfully changed
    ```
4.  在集群的每个节点中，开始锁定卷组。
    ``` literallayout
    [root@rhel8-01 ~]# vgchange --lock-start upgrade_gfs_vg
      VG upgrade_gfs_vg starting dlm lockspace
      Starting locking.  Waiting until locks are ready...
    [root@rhel8-02 ~]# vgchange --lock-start upgrade_gfs_vg
      VG upgrade_gfs_vg starting dlm lockspace
      Starting locking.  Waiting until locks are ready...
    ```
:::
执行此步骤后，您可以为每个逻辑卷创建 `LVM 激活的资源`{.literal}。
:::
:::
[]{#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters.html}
::: chapter
::: titlepage
# []{#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters.html#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters}第 8 章 pcsd Web UI 入门 {.title}
:::
`pcsd`{.literal} Web UI 是一个图形用户界面，用于创建和配置
Pacemaker/Corosync 群集。
::: section
::: titlepage
# []{#assembly_getting-started-with-the-pcsd-web-ui-configuring-and-managing-high-availability-clusters.html#proc_installing-cluster-software-getting-started-with-the-pcsd-web-ui}安装集群软件 {.title}
:::
此流程安装集群软件并为创建集群配置您的系统。
::: orderedlist
**流程**
1.  在集群的每个节点中，启用与您的系统架构对应的高可用性存储库。例如，要为
    x86_64 系统启用高可用性存储库，您可以输入以下
    `subscription-manager`{.literal} 命令：
    ``` literallayout
    # subscription-manager repos --enable=rhel-8-for-x86_64-highavailability-rpms
    ```
2.  在集群的每个节点中，安装 Red Hat High Availability Add-On
    软件包，以及 High Availability 性频道中的所有可用的隔离代理。
    ``` literallayout
    # yum install pcs pacemaker fence-agents-all
    ```
    另外，您可以使用以下命令安装 Red Hat High Availability Add-On
    软件包，并只安装您需要的隔离代理。
    ``` literallayout
    # yum install pcs pacemaker fence-agents-model
    ```
    以下命令显示可用隔离代理列表。
    ``` literallayout
    # rpm -q -a | grep fence
    fence-agents-rhevm-4.0.2-3.el7.x86_64
    fence-agents-ilo-mp-4.0.2-3.el7.x86_64
    fence-agents-ipmilan-4.0.2-3.el7.x86_64
    ...
    ```
    ::: {.warning style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 警告 {.title}
    在安装 the Red Hat High Availability Add-On
    软件包后，需要确定设置了软件更新首选项，以便不会自动安装任何软件。在正在运行的集群上安装可能会导致意外行为。如需更多信息，请参阅[将软件更新应用到
    RHEL High Availability
    或弹性存储集群的建议实践](https://access.redhat.com/articles/2059253/){.link}。
    :::
3.  如果您正在运行 `firewalld`{.literal}
    守护进程，请执行以下命令启用红帽高可用性附加组件所需的端口。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    您可以使用 `rpm -q firewalld`{.literal} 命令确定您的系统中是否安装了
    firewalld``{=html} 守护进程。如果安装了它，您可以使用
    `firewall-cmd --state`{.literal} 命令来确定它是否在运行。
    :::
    ``` literallayout
    # firewall-cmd --permanent --add-service=high-availability
    # firewall-cmd --add-service=high-availability
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    集群组件的理想防火墙配置取决于本地环境，您可能需要考虑节点是否有多个网络接口或主机外防火墙是否存在。在此示例中打开
    Pacemaker
    集群通常所需的端口，您需要根据具体情况进行修改。[为高可用性附加组件](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_creating-high-availability-cluster-configuring-and-managing-high-availability-clusters#proc_enabling-ports-for-high-availability-creating-high-availability-cluster){.link}
    启用端口会显示为红帽高可用性附加组件启用的端口，并解释每个端口的用途。
    :::
4.  要使用 `pcs`{.literal}
    配置集群并在节点之间进行通信，您必须在每个节点上设置用户 ID
    `hacluster`{.literal} 的密码，这是 `pcs`{.literal}
    管理帐户。建议每个节点上的用户 `hacluster`{.literal} 的密码都相同。
    ``` literallayout
    # passwd hacluster
    Changing password for user hacluster.
    New password:
    Retype new password:
    passwd: all authentication tokens updated successfully.
    ```
5.  在配置群集之前，必须启动并启用 `pcsd`{.literal}
    守护进程以便在每个节点上引导时启动。此守护进程与 `pcs`{.literal}
    命令配合使用，以管理群集中节点的配置。
    在集群的每个节点上，执行以下命令启动 `pcsd`{.literal}
    服务并在系统启动时启用 `pcsd`{.literal}。
    ``` literallayout
    # systemctl start pcsd.service
    # systemctl enable pcsd.service
    ```
:::
:::
::: section
::: titlepage