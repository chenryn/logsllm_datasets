            goto oops;
        }
        // 搜索异常表, 试图找到一个对应该异常的例程来进行修正
        if (fixup_exception(regs, X86_TRAP_PF, error_code, address)) {
            // 任何中断错误都会在fixup_exception中获得处理, 下面的错误处理流程只用于任务上下文中的错误
            if (in_interrupt())
                return;    
            if (current->thread.sig_on_uaccess_err && signal) {
                set_signal_archinfo(address, error_code);
                force_sig_fault(signal, si_code, (void __user *)address);
            }
            return;
        }
        /*
         * 32-bit:
         *
         *   Valid to do another page fault here, because if this fault
         *   had been triggered by is_prefetch fixup_exception would have
         *   handled it.
         *
         * 64-bit:
         *
         *   Hall of CPU/BIOS bugs.
         */
        if (is_prefetch(regs, error_code, address))
            return;
        if (is_errata93(regs, address))
            return;
        // 固件访问错误恢复
        if (IS_ENABLED(CONFIG_EFI))
            efi_recover_from_page_fault(address);
    oops:
        // 确定时内核缺陷, 使用oops打印错误...
        flags = oops_begin();
        show_fault_oops(regs, error_code, address);
        if (task_stack_end_corrupted(tsk))
            printk(KERN_EMERG "Thread overran stack, or stack corrupted\n");
        sig = SIGKILL;
        if (__die("Oops", regs, error_code))
            sig = 0;
        /* Executive summary in case the body of the oops scrolled away */
        printk(KERN_DEFAULT "CR2: %016lx\n", address);
        oops_end(flags, regs, sig);
    }
###  do_user_addr_fault
    static inline
    void do_user_addr_fault(struct pt_regs *regs,
                unsigned long hw_error_code,
                unsigned long address)
    {
        struct vm_area_struct *vma;
        struct task_struct *tsk;
        struct mm_struct *mm;
        vm_fault_t fault, major = 0;
        unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
        tsk = current;
        mm = tsk->mm;
        // 判断kprobe是否hook了缺页错误
        if (unlikely(kprobe_page_fault(regs, X86_TRAP_PF)))
            return;
        // Reserved bits不会被设置在用户的页表项, 如果存在Reserved bits则发生页表错误
        if (unlikely(hw_error_code & X86_PF_RSVD))
            pgtable_bad(regs, hw_error_code, address);
        // 如果开启smap且kernel(supervisor)访问用户态地址(X86_PF_USER=0)则进入bad_area_nosemaphore
        if (unlikely(cpu_feature_enabled(X86_FEATURE_SMAP) &&
                 !(hw_error_code & X86_PF_USER) &&
                 !(regs->flags & X86_EFLAGS_AC)))
        {
            bad_area_nosemaphore(regs, hw_error_code, address);
            return;
        }
        /*
         * If we're in an interrupt, have no user context or are running
         * in a region with pagefaults disabled then we must not take the fault
         */
        if (unlikely(faulthandler_disabled() || !mm)) {
            bad_area_nosemaphore(regs, hw_error_code, address);
            return;
        }
        // 因为到达这一步时cr2中的虚拟地址已经被另存且vmalloc_fault被处理所以开启中断是安全可行的
        if (user_mode(regs)) {
            local_irq_enable();
            flags |= FAULT_FLAG_USER;
        } else {
            if (regs->flags & X86_EFLAGS_IF)
                local_irq_enable();
        }
        // 记录事件
        perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
        // 更新flags标志位(后面引用)
        if (hw_error_code & X86_PF_WRITE)
            flags |= FAULT_FLAG_WRITE;
        if (hw_error_code & X86_PF_INSTR)
            flags |= FAULT_FLAG_INSTRUCTION;
    #ifdef CONFIG_X86_64
        // vsyscall没有vma, 所以在find_vma之前对vsyscall做模拟
        if (is_vsyscall_vaddr(address)) {
            if (emulate_vsyscall(hw_error_code, regs, address))
                return;
        }
    #endif
        // 1. 内核只能访问用户地址空间的在exception_tables上定义的指令, 如果直接进行这样的尝试(但指令却在exception_tables之外)则会因为持有了mmap_sem锁, 而让系统死锁
        // 2. 所以, 只有当获得mmap_sem锁失败后, 才能尝试使用1.
        if (unlikely(!down_read_trylock(&mm->mmap_sem))) {
            if (!user_mode(regs) && !search_exception_tables(regs->ip)) {
                bad_area_nosemaphore(regs, hw_error_code, address);
                return;
            }
    retry:
            down_read(&mm->mmap_sem);
        } else {
            // 如果获得mmap_sem锁成功则会错过down_read内的might_sleep, 这里补一个might_sleep
            might_sleep();
        }
        // 判断vma是否合法
        vma = find_vma(mm, address);
        if (unlikely(!vma)) {
            bad_area(regs, hw_error_code, address);
            return;
        }
        // 做简单的安全检测
        if (likely(vma->vm_start vm_flags & VM_GROWSDOWN))) {
            bad_area(regs, hw_error_code, address);
            return;
        }
        // 如果address在栈空间, 则根据address和vma->start的关系决定是否扩充栈
        // 如果address start, 则另vma->start=address向下扩充stack
        if (unlikely(expand_stack(vma, address))) {
            bad_area(regs, hw_error_code, address);
            return;
        }
    good_area:
        // 判断是否因为页操作与vma权限不符(保留X86_PF_PK错误, 因为这是留给写时复制的, 并非权限错误)
        if (unlikely(access_error(hw_error_code, vma))) {
            bad_area_access_error(regs, hw_error_code, address, vma);
            return;
        }
        // 缺页处理
        fault = handle_mm_fault(vma, address, flags);
        ...
####  handle_mm_fault
    static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
            unsigned long address, unsigned int flags)
    {
        struct vm_fault vmf = {
            .vma = vma,
            .address = address & PAGE_MASK,
            .flags = flags,
            .pgoff = linear_page_index(vma, address),
            .gfp_mask = __get_fault_gfp_mask(vma),
        };
        unsigned int dirty = flags & FAULT_FLAG_WRITE;
        // 以vma->vm_mm为根结点遍历页表, 定位到pmd
        struct mm_struct *mm = vma->vm_mm;
        pgd_t *pgd;
        p4d_t *p4d;
        vm_fault_t ret;
        // 定位pgd
        pgd = pgd_offset(mm, address);
        // 如果没开5级页表直接返回pgd
        p4d = p4d_alloc(mm, pgd, address);
        if (!p4d)
            return VM_FAULT_OOM;
        // 定位pud
        vmf.pud = pud_alloc(mm, p4d, address);
        if (!vmf.pud)
            return VM_FAULT_OOM;
        // 中间表项为空, 且开启huge_page, 设置
        // 如果pud为空, 且vma可以创建透明的huge_page, 则create_huge_pud触发huge_page错误(匿名页不支持)
        if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
            ret = create_huge_pud(&vmf);
            if (!(ret & VM_FAULT_FALLBACK))
                return ret;
        } else {
            pud_t orig_pud = *vmf.pud;
            barrier();
            // pud具有_PAGE_PSE标志位, 且pud为devmap
            if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {
                // pud将要被更新为脏页
                if (dirty && !pud_write(orig_pud)) {
                    // 触发huge_page错误(匿名页不支持)
                    ret = wp_huge_pud(&vmf, orig_pud);
                    if (!(ret & VM_FAULT_FALLBACK))
                        return ret;
                } else {
                    huge_pud_set_accessed(&vmf, orig_pud);
                    return 0;
                }
            }
        }
        vmf.pmd = pmd_alloc(mm, vmf.pud, address);
        if (!vmf.pmd)
            return VM_FAULT_OOM;
        // 如果pmd为空, 且vma可以创建透明的huge_page, 则create_huge_pmd创建大页
        if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {
            ret = create_huge_pmd(&vmf);
            if (!(ret & VM_FAULT_FALLBACK))
                return ret;
        } else {
            pmd_t orig_pmd = *vmf.pmd;
            barrier();
            // 判断pmd是否在swap分区(不在内存中)
            if (unlikely(is_swap_pmd(orig_pmd))) {
                // 如果支持迁移但并非迁移pmd入口, 则上报bug
                VM_BUG_ON(thp_migration_supported() &&
                          !is_pmd_migration_entry(orig_pmd));
                if (is_pmd_migration_entry(orig_pmd))
                    pmd_migration_entry_wait(mm, vmf.pmd);
                return 0;
            }
            // pud具有_PAGE_PSE标志位, 且pud为devmap
            if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
                if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
                    return do_huge_pmd_numa_page(&vmf, orig_pmd);
                if (dirty && !pmd_write(orig_pmd)) {
                    ret = wp_huge_pmd(&vmf, orig_pmd);
                    if (!(ret & VM_FAULT_FALLBACK))
                        return ret;
                } else {
                    huge_pmd_set_accessed(&vmf, orig_pmd);
                    return 0;
                }
            }
        }
        // vmf被填充, 下一步根据vmf分配物理页
        return handle_pte_fault(&vmf);
    }
#####  handle_pte_fault
    static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
    {
        pte_t entry;
        // 若pmd不存在, 则pte不存在
        if (unlikely(pmd_none(*vmf->pmd))) {
            vmf->pte = NULL;
        } else {
            // pmd_devmap_trans_unstable{return pmd_devmap(*pmd) || pmd_trans_unstable(pmd);}
            // pmd_devmap检测pmd是否为_PAGE_DEVMAP, 如果是则直接返回1
            //
            // pmd_trans_unstable->pmd_none_or_trans_huge_or_clear_bad{...}
            // 检测pmd是否为空, 或者是否可以转换为huge_page, 否则进入pmd_clear_bad
            // 提前检测_PAGE_DEVMAP, 可以避免后面devmap页进入pmd_none_or_trans_huge_or_clear_bad后陷入pmd_clear_bad, 而滥用dmesg打印错误
            // pmd_devmap_trans_unstable=pmd_devmap+pmd_trans_unstable 可这个命名太抽象 =.=
            if (pmd_devmap_trans_unstable(vmf->pmd))
                return 0;
            // 此时pmd存在且不可能变成huge_pmd, 使用pte_offset_map是安全的
            vmf->pte = pte_offset_map(vmf->pmd, vmf->address);
            vmf->orig_pte = *vmf->pte;
            barrier();
            if (pte_none(vmf->orig_pte)) {
                pte_unmap(vmf->pte);
                vmf->pte = NULL;
            }
        }
        // pte为空, 分配页表
        if (!vmf->pte) {
            if (vma_is_anonymous(vmf->vma))
                // 处理匿名页
                return do_anonymous_page(vmf);
            else
                // 处理文件映射页
                return do_fault(vmf);
        }
        // -----------  物理页存在  -----------    
        // 页表已建立, 但不存在与内存, 做页交换
        if (!pte_present(vmf->orig_pte))
            return do_swap_page(vmf);
        if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
            // 维持node平衡, 进行页迁移
            return do_numa_page(vmf);
        vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
        spin_lock(vmf->ptl);
        entry = vmf->orig_pte;
        // 锁定资源区后, 检测pte是否发生变化, 如果发生, 直接解锁资源区 return
        if (unlikely(!pte_same(*vmf->pte, entry)))
            goto unlock;
        // 因写入而触发中断
        if (vmf->flags & FAULT_FLAG_WRITE) {
            if (!pte_write(entry))
                // 写时复制缺页中断
                return do_wp_page(vmf);
            // 标记脏页
            entry = pte_mkdirty(entry);
        }
        entry = pte_mkyoung(entry);
        // 如果pte内容没有变化进入else
        if (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,
                    vmf->flags & FAULT_FLAG_WRITE)) {
            // pte内容更改, 刷新mmu
            update_mmu_cache(vmf->vma, vmf->address, vmf->pte);
        } else {
            // 如果pte没有发生变化, 且是写中断错误, 这里可能对应写时复制, 所以更新tlb
            if (vmf->flags & FAULT_FLAG_WRITE)
                flush_tlb_fix_spurious_fault(vmf->vma, vmf->address);
        }
    unlock:
        pte_unmap_unlock(vmf->pte, vmf->ptl);
        return 0;
    }
## ptmalloc
> ptmalloc是linux 用户态堆管理机制, 也是本文源码分析的最后一部分, 链接如下:
[ptmalloc源码解析](https://www.povcfe.site/posts/ptmalloc/)
## 杂记
  * 山高路远不畏险
  * 还有很多需要写的东西, 留给下一篇文章