a higher-level machine model, so long as a reﬁnement can be
proven between the layer’s implementation over the underlay
interface and the overlay interface.
A key aspect of our proofs is to abstract away the low-level
details of the machine model, layer by layer, by proving
reﬁnement between the software implementation using a
lower-level machine model and its speciﬁcation based on
a higher-level machine model. Speciﬁcally, by proving
reﬁnement relations between adjacent layers, we successively
verify that KCore’s implementation over AbsMachine
reﬁnes the abstract
top-level speciﬁcation deﬁned by
TrapHandler, as shown in Figure 2. For example, we verify
that the TLB behavior exposed by AbsMachine is wholly
encapsulated by our implementation, and is thus abstracted
from TrapHandler’s speciﬁcation.
3958    30th USENIX Security Symposium
USENIX Association
CPU0VA/gPAPAPAMain MemoryCoherent Data CachesVA/gPAPADEV1DEV0IOVAIOVAPAIOVAIOVAPAVA/gPATLBPTsSMMUPTsCPU1VA/gPAPATLBSMMU TLBPTsPACPU0PAMain MemoryCPU1Shared Data CacheVA/gPAPADEV1DEV0IOVAPAIOVAPAVA/gPAPTsPTsSMMUPTs4.2 Page Table Management
As shown in Figure 3a, AbsMachine models Arm hardware’s
multi-level page tables. A page table can include up to four
levels, referred to using Linux terminology as pgd, pud, pmd,
and pte. AbsMachine models both regular and huge page
table mappings, as used by KVM and also employed by
KCore. KCore maintains stage 2 page tables — one per VM
and one for KServ — as well as its own EL2 stage 1 page
table. The functions for KCore to manipulate page tables
are implemented and veriﬁed at the four layers of the MMU PT
module, shown in Figure 2. The PTAlloc layer dynamically
allocates page table levels, e.g.,pud, pmd, and pte. The
PTWalk layer provides helper functions for walking an
individual level of the page table, e.g., walk_pgd, walk_pud,
etc. The NPTWalk layer uses PTWalk’s primitives to perform
a full page table walk. The NPTOps layer grabs and releases
page table locks to perform page table operations, such as the
map_page function that maps a VM’s guest physical frame
number (gfn) to a physical frame number (pfn) by calling
the set_s2pt function in the NPTWalk layer to create a new
mapping in the VM’s stage 2 page table:
void map_page(u32 vmid, u64 gfn, u64 pfn, u64 attr) {
acq_lock_s2pt();
if (!get_s2pt(vmid, gfn)) {
set_s2pt(vmid, gfn, pfn, 4K, attr);
}
rel_lock_s2pt();
}
void set_s2pt(u32 vmid, u64 gfn, u64 pfn, u32 size,
u64 attr) {
u64 pgd, pud, pmd, pte;
pgd = walk_pgd(vmid, gfn);
pud = walk_pud(vmid, pgd, gfn);
pmd = walk_pmd(vmid, pud, gfn);
if (size == 2M) {
/* make sure pmd is not mapped to a pte */
if (pmd_table(pmd) != PMD_TYPE_TABLE)
set_pmd(vmid, pmd, gfn, pfn, attr);
} else if (size == 4K) {
if (pmd_table(pmd) == PMD_TYPE_TABLE) {
pte = walk_pte(vmid, pud, gfn);
set_pte(vmid, pte, gfn, pfn, attr);
}
}
}
tree data structure—every page table at lower levels (pud,
pmd, and pte) is referenced by only one page table entry at
higher levels. We verify that KCore enforces the following
two properties: (1) a lower-level page table can only be
allocated and inserted during the page table walk when the
target page table level does not exist, and (2) the allocated
page table is a free and empty page. The allocated page is
free such that no page table entry references it before the
insertion. The allocated page is empty such that it does not
contain any existing page tables. In this way, if the page table
initially forms a tree, inserting this allocated page still results
in a tree. The ﬁrst property ensures that each edge of the tree
before insertion remains unchanged after the insertion.
We then verify that the tree structure can be reﬁned to a ﬂat
map by showing that updating the mapping for a gfn does
not affect the mapping for any other gfn’ (cid:54)= gfn. Suppose
both gfn and gfn’ are regular or huge pages. If the page
walks for gfn and gfn’ diverge at some level, they will fall
into different leaf nodes due to the tree structure. If gfn and
gfn’ have the same page walk path, their pte indices will
be different if they are regular pages, and their pmd indices
will be different if they are huge pages, since gfn’ (cid:54)= gfn.
The proof becomes more complicated when one page is
a regular page and the other is a huge page. We have to prove
that, once a pmd is allocated to store huge page mappings, it
cannot be used to store lower-level pte pointers for regular
pages, and vice versa. This is ensured by checking the size
argument and the type of pmd during the page walk, as shown
in the above example.
To unify the representation for the ﬂat map at higher
layers, we logically treat a 2MB huge page as 512 4KB pages.
Changing one mapping for a 2MB huge page will cause
updates to the mappings for all of its 512 4KB pages.
After the reﬁnement proof at the layer NPTWalk, all the
modules and their properties at higher layers can be reasoned
about using this ﬂat map without the need to deal with the
implementation details of the multi-level page tables. For
example, the memory isolation proof can be simpliﬁed
signiﬁcantly using the ﬂat page map.
We need to prove that KCore correctly manages its own
stage 1 page table and all stage 2 page tables to enforce
memory isolation among VMs, regardless of how KServ and
VMs manage their own stage 1 page tables. To simplify this
proof and the reasoning related to page tables at higher layers,
we ﬁrst abstract away the underlying implementation details
and reﬁne the multi-level page table into a ﬂat map, as shown
in Figure 3b, at the layer NPTWalk. For example, we reﬁne
the stage 2 page table into a ﬂat map from gfn to a physical
frame tuple (pfn, size, attr), where size is the size of
the page, 4KB or 2MB, and attr encompasses attributes of
the page, such as its memory access permissions.
This reﬁnement is proven by ﬁrst showing that the
multi-level page table managed by KCore always forms a
4.3 TLB Management
As shown in Figure 3a, AbsMachine models Arm’s tagged
TLB for each CPU, which caches page translations to regular
and huge pages. In AbsMachine, each CPU is associated with
an abstract TLB mapping, which maps VMIDs as tags to a set
of TLB entries.
Arm TLBs cache three types of entries: (1) a stage 1
translation from a VM’s virtual address to a gPA, (2) a stage
2 translation from a gPA to a PA, and (3) a translation from a
VM’s virtual address to a PA that combines stage 1 and stage
2 translations. AbsMachine models all three types of TLB
entries, respectively, as: (1) a mapping from a virtual page
number vpn to a tuple (gfn, size, attr), and (2) a mapping
USENIX Association
30th USENIX Security Symposium    3959
from a gfn to a physical frame tuple (pfn, size, attr), and
(3) a mapping from a vpn to a gfn to a physical frame tuple
(pfn, size, attr), where size and attr are used the same
way as in AbsMachine’s page tables, described in Section 4.2.
Mappings are aligned to size (4KB or 2MB) of the mapped
page. AbsMachine provides the following four basic TLB
operations reﬂecting Arm’s hardware behavior:
• TLB lookup. For a given memory load or store made
by a VM VMID to access an address addr (gfn or vpn),
AbsMachine searches the running CPU’s TLB tagged with
VMID, and checks if any entry translates addr. AbsMachine
ﬁrst checks if addr maps to an exact 4KB pfn, If no
such mapping exists, it then checks if addr maps to a
2MB pfn by aligning addr to its 2MB base, pfn_2m, and
searching the TLB using pfn_2m. If a matching entry is
found, a TLB hits, the TLB returns the respective physical
frame number if the VM memory operation is permitted,
otherwise generates a permission fault. If no matching
entry is found, the TLB returns None to indicate a TLB
miss, and AbsMachine will then perform the address
translation using page tables directly.
• TLB reﬁll. If a TLB miss occurs on a memory access, Abs-
Machine reﬁlls the TLB with information from the ensuing
page table walk, either a 4KB or 2MB translation to the
CPU’s tagged TLB. As previously mentioned, the reﬁlled
pfn must be aligned to the corresponding mapping size.
• TLB eviction. In AbsMachine, a memory load or store
operation randomly invalidates a TLB entry before the
actual memory access to account for all possible TLB
eviction policies.
• TLB ﬂush. Like Arm, AbsMachine exposes two primitives,
mmu_tlb_flush1 and mmu_tlb_flush2, to ﬂush TLB
entries. mmu_tlb_flush2 takes a gfn and a VMID as
arguments and invalidates the second type of TLB entry
that maps the gfn. mmu_tlb_flush1 takes a VMID as an
argument and invalidates all TLB entries associated with
VMID that are either the ﬁrst or third type of TLB entry.
Hypervisors like KVM must use mmu_tlb_flush1 to
conservatively ﬂush all of a VM’s TLB entries related to
stage 1 translations when they update stage 2 page tables
because they do not track how VMs manage their own stage
1 page tables. Like KVM, KCore uses both primitives to
ﬂush TLB entries as needed when updating a VM’s stage 2
page tables. For simplicity, we use mmu_tlb_flush to refer
to a call to both mmu_tlb_flush1 and mmu_tlb_flush2.
Note that the ﬁrst three operations, TLB lookup, reﬁll, and
eviction, model Arm’s TLB hardware behavior during the
memory access, while the last operation, TLB ﬂush, provides
a set of primitives for the KCore software to perform TLB
maintenance, implemented and veriﬁed at the NPTOps layer
of the MMU PT module shown in Figure 2.
At the layer NPTOps, we verify that TLB entries are
correctly maintained by KCore and that no principal, a VM
or KServ, can use the TLB to access a physical page that
does not belong to it, regardless of the behavior of KServ
or any VM. In this way, we can hide TLB and TLB-related
operations from all the layers above NPTOps, as shown in
Figure 3b, to simplify the reasoning at higher layers.
This veriﬁcation step introduces a concept of page ob-
servers to represent the set of all possible principals that can
observe a pfn through TLBs or page tables. We write {pfn:
n kserv}@TLB to denote that VM n and KServ are page
observers to pfn through TLBs. As an example, consider the
unmap_pfn_kserv primitive in NPTOps. When a page pfn is
allocated by KServ to a VM n, KCore ﬁrst calls unmap_pfn_-
kserv to remove the pfn from KServ’s stage 2 page table,
then inserts pfn in n’s stage 2 page table. The page observers
before and after each step can be computed as follows:
// {pfn: kserv}@TLB
unmap_pfn_kserv (pfn);
// {pfn: kserv}@TLB
mmu_tlb_flush (pfn, kserv);
// {pfn: _}@TLB
{pfn: _}@PT
map_page (n, gfn, pfn, attr);
// {pfn: n}@TLB
{pfn: n}@PT
{pfn: kserv}@PT
{pfn: _}@PT
A TLB can be reﬁlled using page tables’ contents at
any point due to a memory access on another CPU, so the
(possible) page observers through TLBs must be a superset of
the ones through page tables. That is why VM n can observe
pfn through TLBs right after inserting pfn to n’s page table.
Intuitively, the superset relationship is because a TLB can
contain the earlier and current cached page table translations
while page tables contains only the current translations. The
TLB ﬂush collapses all possible (cached) observers to pfn
to the observers deﬁned by the page table.
The above example generates the following sequence of
page observers through TLB:
{pfn: kserv}, {pfn: kserv}, {pfn: _}, {pfn: n}
If we merge consecutive identical page observers into a page
observer group, we get the following page observer groups:
{pfn: kserv}, {pfn: _}, {pfn: n}
(1)
To prove that TLBs are maintained correctly and can be
hidden at higher layers, we just need to show that TLBs and
page tables generate the same sequence of page observer
groups, even if page tables’ observers are a subset of TLBs’
observers. In the above example, the page observers through
page tables are:
{pfn: kserv}, {pfn: _}, {pfn: _}, {pfn: n}
which can be merged to the same sequence of page observer
groups shown in Eq. (1).
This property can be generally proven as follows. Starting
with the same observer group through TLBs and page tables,
the resulting observer groups produced by operations such as
memory accesses, creating new page mappings in page tables,
and TLB ﬂushes are still the same. The only non-trivial case
3960    30th USENIX Security Symposium
USENIX Association
is unmapping pages, which introduces a new observer group
through page tables, while a TLB would still show the old
observer group. To avoid missing this new observer group, the
TLBs must be invalidated by KCore calling mmu_tlb_flush.
Using this approach, incorrect maintenance of TLBs can
be detected by a mismatch of page observer groups. Consider
the following insecure implementation that invalidates the
TLB before unmapping pfn.
// {pfn: kserv}@TLB
mmu_tlb_flush (gfn, kserv);
// {pfn: kserv}@TLB
unmap_pfn_kserv (pfn);
// {pfn: kserv}@TLB
map_page (n, gfn, pfn, attr);
// {pfn: kserv n}@TLB
{pfn: kserv}@PT
{pfn: n}@PT
{pfn: kserv}@PT
{pfn: _}@PT
Since TLBs can be reﬁlled by page tables’ contents, the
page observers through TLBs remain the same after the TLB
ﬂush. The subsequent page unmapping does not invalidate
TLBs such that the sequence of page observer groups through
TLB for this insecure implementation is as follows:
{pfn: kserv}, {pfn: kserv n}
which is different from the one in Eq. (1), meaning that more
information can be released through TLBs than page tables.