2
2
𝑖
(19)
2𝑑2
,
where the equal sign of the second inequality holds when 𝑑1 =
𝑑2 = · · · = 𝑑𝜅 = 𝑑 = 𝑁
𝜅 . In general situations, we can assume that
this condition holds. Thus, if we fist search within 𝑆𝑛𝑜𝑑𝑒 and then
search within 𝑆𝑙𝑖𝑛𝑘, 𝛽 can be approximated as follows:
𝑆𝑔𝑟𝑎𝑝ℎ
𝛽 =
𝑆𝑛𝑜𝑑𝑒 + 𝑆𝑙𝑖𝑛𝑘
≈ 2 𝑁 (𝑁−1)
2
÷ [𝜅 · 2 𝑑(𝑑−1)
2
+ 𝜅(𝜅 − 1)
2
· 2𝑑2]
(20)
Now suppose 𝑑 = 𝑡𝜅 (thus 𝑁 = 𝜅𝑑 = 𝑡𝜅2), where 𝑡 >> 1 often
in practice. Then, we have:
+ 𝜅(𝜅 − 1)
2
· 2𝑑2]
𝛽 ≈ 2 𝑁 (𝑁−1)
2
2
÷ [𝜅 · 2 𝑑(𝑑−1)
2 𝑡2𝜅4−𝑡𝜅2+𝑡2𝜅2
2 + 𝜅2−𝜅
2
2 𝑡2𝜅4−𝑡𝜅2+𝑡2𝜅2
2
2
𝜅 · 2𝑡2𝜅2− 𝑡𝜅
· 2 3𝑡2𝜅2
2
+ 𝜅2 · 2 3𝑡2𝜅2
2
2
𝜅2 · 2 3𝜅2𝜅2
2 𝑡2𝜅4
2
2
2𝜅2 · 2 3𝑡2𝜅2
1
2𝜅2 · 2 𝑡2(𝜅4−3𝜅2)
2
=
>
>
=
Finally, 𝛽 in general situations satisfies:
𝛽 ≈ 𝑂(2𝜅4)
C PROOF OF THEOREM 4.2
We first restate theorem 4.2:
(21)
(22)
Theorem 4.2. Given a normalized direction Θ𝑜𝑙𝑑 with 𝑔𝑜𝑙𝑑 and
𝑝𝑜𝑙𝑑 , there is one and only one 𝑔∗ at the direction of Θ𝑛𝑒𝑤 that satisfies
𝑝∗ = ∥𝑐𝑙𝑖𝑝(𝑔∗Θ𝑛𝑒𝑤 − 0.5)∥1 = 𝑝𝑜𝑙𝑑 .
We proof theorem 4.2 by showing that 𝑝(Θ) is a monotone in-
creasing function of 𝑔(Θ). Without lose of generality, we assume
two constants with 0  0 𝑓 𝑜𝑟 𝑖 ∈ 𝐼+. As the 𝑐𝑙𝑖𝑝(·) function limits
the inputs into [0, 1] which will set all negative values as 0, we can
rewrite 𝑝1 and 𝑝2 as follows
𝑝1 =
𝑝2 =
(𝑐𝑙𝑖𝑝(𝑔1Θ − 0.5))𝑖
(𝑐𝑙𝑖𝑝(𝑔2Θ − 0.5))𝑖
(24)


𝑖∈𝐼+
𝑖∈𝐼+
𝑖∈𝐼+
Furthermore, the components of 𝑔1Θ − 0.5 and 𝑔2Θ − 0.5 may also
be negative because of the −0.5 term. We thus further denote 𝐼 (1)
+
and 𝐼 (2)
where 𝑔1Θ𝑖 − 0.5 > 0 ∀ 𝑖 ∈ 𝐼 (1)
+ where 𝑔2Θ𝑗 − 0.5 >
+
+ . It is obvious that 𝐼 (1)
0 ∀ 𝑗 ∈ 𝐼 (2)
+ ⊆ 𝐼 (2)
as 0  0∀𝑘 ∈ 𝐼 (1)
+ . Then we have
(𝑐𝑙𝑖𝑝(𝑔2Θ − 0.5))𝑖 − 

𝑝2 − 𝑝1 =

+ 
≥ 
(𝑔2 − 𝑔1)Θ𝑖 + 
(𝑐𝑙𝑖𝑝(𝑔2Θ − 0.5) − 𝑐𝑙𝑖𝑝(𝑔1Θ − 0.5))𝑖
(𝑐𝑙𝑖𝑝(𝑔2Θ − 0.5)) 𝑗
(𝑐𝑙𝑖𝑝(𝑔2Θ − 0.5)) 𝑗
(𝑐𝑙𝑖𝑝(𝑔1Θ − 0.5))𝑖
(𝑐𝑙𝑖𝑝(𝑔1Θ − 0.5))𝑖
+ \𝐼 (1)
+
𝑖∈𝐼 (1)
+
𝑖∈𝐼 (2)
+
𝑖∈𝐼 (1)
+
𝑗 ∈𝐼 (2)
𝑖∈𝐼+
(25)
=
=
𝑖∈𝐼 (1)
+
≥ 0
𝑗 ∈𝐼 (2)
+ \𝐼 (1)
+
+ = 𝐼 (2)
The equal sign holds when one of the following two conditions
satisfied:
(i) 𝐼 (1)
+ = 𝐼+, which means that 𝑔1 and 𝑔2 are both large
enough such that all positive components of 𝑔1Θ−0.5 and 𝑔2Θ−0.5
exceed 1.0. Under this condition, we will perturb all edges corre-
spond to the positive components of 𝑔1Θ − 0.5.
+ = ∅, which means that 𝑔1 and 𝑔2 are both small
enough such that all positive components of 𝑔1Θ−0.5 and 𝑔2Θ−0.5
lower than 0. Under this condition, we do not perturb any edge.
+ = 𝐼 (2)
(ii) 𝐼 (1)
Note that, the two conditions above will never be satisfied during
our signSGD because we always start our gradient descent at an
initial direction Θ0 with a moderate 𝑔 value. At each time 𝑡, when
we step to a new direction, i.e., Θ𝑡+1, we may go into an extreme
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea123+
+
= 𝐼+ or 𝐼 (𝑡+1)
condition where 𝐼 (𝑡+1)
= ∅. However, both condi-
tions will be rejected as the former leads to large perturbations and
the later add no perturbations thus will never change the label of
target graph. Therefore, during our signSGD, we will always have
(26)
Then 𝑝 is a monotonically increasing function of 𝑔 thus 𝑝 and 𝑔 can
be mutually uniquely determined.
𝑝1  0 is the smoothing parameter and 𝑑 is the
dimension of Θ.
We first define some notations as follows:
𝑝(Θ𝑡 + 𝜇𝑢𝑞) − 𝑝(Θ𝑡)
(cid:164)▽𝑝(Θ𝑡; 𝑢𝑞) =
ˆ▽𝑝(Θ𝑡; 𝑢𝑞) = 𝑠𝑖𝑔𝑛( 𝑝(Θ𝑡 + 𝜇𝑢𝑞) − 𝑝(Θ𝑡)
𝜇
𝑢𝑞
𝑢𝑞)
𝜇
(cid:113)
𝑝𝜇(Θ) = E𝑢[𝑝(Θ + 𝜇𝑢)]
E[( (cid:164)▽𝑝(Θ𝑡; 𝑢𝑞) − ▽𝑝𝜇(Θ𝑡))2
𝑙 ]
(27)
(28)
(29)
𝑄
𝑄
𝑞=1
𝑄
𝑄
(cid:164)𝑝𝑡 ≈ 1
𝑄
𝛿𝑙 =
(30)
where 𝑝𝜇(Θ) is the randomized smoothing function of 𝑝(Θ). We
can observe that ˆ▽𝑝(Θ𝑡; 𝑢𝑞) = 𝑠𝑖𝑔𝑛( (cid:164)▽𝑝(Θ𝑡; 𝑢𝑞)). Moreover, the
corresponding estimated gradients are defined as:
𝑝(Θ𝑡 + 𝜇𝑢𝑞) − 𝑝(Θ𝑡)
𝑞=1
𝜇
𝑢𝑞 =
1
𝑄
(cid:164)▽𝑝(Θ𝑡 ; 𝑢𝑞)
(31)
𝜇
𝑄
𝑞=1
𝑞=1
1
𝑄
(32)
𝑢𝑞) =
ˆ▽𝑝(Θ𝑡 ; 𝑢𝑞)
(cid:12)(cid:12) 𝑃𝑟[𝑠𝑖𝑔𝑛(( ˆ𝑝𝑡)𝑙) ≠ 𝑠𝑖𝑔𝑛((▽𝑝𝜇(Θ𝑡))𝑙)] ≤
(cid:12)(cid:12)(▽𝑝𝜇(Θ𝑡))𝑙
𝑠𝑖𝑔𝑛( 𝑝(Θ𝑡 + 𝜇𝑢𝑞) − 𝑝(Θ𝑡)
ˆ𝑝𝑡 ≈ 1
Next, we introduce some lemmas.
Lemma D.1.
𝛿𝑙√