by running the experiments again with VMs connecting to
the Internet. We compare the results to those when VMs are
disconnected to the Internet on the same day. In total, 23 out
of 36 engines’ results remain consistent, with or without the
Internet. 13 out of 36 engines have different results, indicat-
ing that the remote servers play a role in desktop engines’
decision. Among the 13 engines, seven engines have a lower
precision after connecting to the Internet; nine engines have a
higher recall. Overall, the results of desktop engines are get-
ting closer to those of VirusTotal engines, but the conclusion
above is still valid: desktop engines are still more conservative
with a higher precision and a lower recall. The gap is smaller
with the Internet connection.
Observation 11: Inconsistency exists between the desktop
version and the online version (at VirusTotal) for all engines.
Surprisingly, for most of the vendors, their VirusTotal engines
are able to detect more malware than their desktop versions.
6.4 Comparison with the Main Dataset
As a sanity check, we have validated the key observations
we had on the main dataset using the ground-truth datasets
too. Due to space limit, we keep our discussions brief. First,
ground-truth datasets have more hazard ﬂips (6,708) than
non-hazard ﬂips (5,855). Second, ﬂips also widely exist
across ﬁles, dates, and engines. The majority of the ﬂips are
still highly correlated with engines’ model update (73.7%).
Third, engines that are highly correlated in the main dataset
are still highly correlated in the ground-truth datasets. The
strong inﬂuencer-inﬂuenced relationships observed in the
main dataset are also observed in the ground-truth datasets
(primarily in Malware-I and Malware-II).
USENIX Association
29th USENIX Security Symposium    2373
7 Discussion
Our measurement results have several important implications
regarding the methods of using VirusTotal for ﬁle labeling.
Data Preprocessing. Our results show that hazard ﬂips
count for the majority of all the label ﬂips and they affect
label stabilization of individual engines. The good news is that
hazard ﬂips, by deﬁnition, are short-lived, and it only incurs a
small cost to get rid of them. We recommend VirusTotal users
to submit the same ﬁles to VirusTotal in three consecutive
days to identify and remove potential hazards.
Label ﬂips happen widely across engines, ﬁles and time.
They do not necessarily disappear if researchers wait for a
longer time. The beneﬁt of querying the labels over a long
period of time (e.g., months) is quite limited.
Label Aggregation. We show that threshold-based aggre-
gation is surprisingly effective in stabilizing the aggregated
labels against the label ﬂips of individual engines, but the
threshold t needs to be set properly. For example, the aggre-
gated labels are still easily inﬂuenced by the ﬂips when the
threshold is too small (t = 1) or too big (t = 40 or t = 50). If
the threshold t is chosen within a reasonable range (2–39),
the aggregated labels are more likely to stay stable.
A stable aggregated label does not necessarily mean the la-
bel is correct. Our ground-truth analysis shows that choosing
a small threshold (e.g., t < 15) helps strike a good balance be-
tween precision and recall for the aggregated labels. However,
it becomes very difﬁcult to ﬁnd a good threshold when the
benign ﬁles contain obfuscated code. Our recommendation
is that researchers should not use a small threshold if their
ﬁles are obfuscated (especially the potentially benign ones).
A better idea could be only considering engines that perform
well on obfuscated ﬁles (see Figure 13).
Engine Independence. Most existing papers treat all en-
gines equally and do not consider possible correlations of
their labeling decisions. Our experiments conﬁrm the exis-
tence of both correlation and causality relationships between
engines. In particular, we identify groups of engines whose
label sequences are highly similar to each other (Section 5.1).
A practical suggestion is to consider them as “redundant votes”
and reduce their weights during label aggregation. We also
identify several engines whose labeling exhibits causal rela-
tionships (Section 5.2). This does not necessarily mean one
engine directly copies results from other engines – it is also
possible these engines change labels due to the impact of third
parties (blacklists), but some engines react slower than others.
High-reputation Engines.
Several existing papers hand-
picked high-reputation engines for label aggregation. Our
analysis shows that most of these engines perform well (e.g.,
having more stabilized labels, being an inﬂuencer instead
of being inﬂuenced). However, we ﬁnd one high-reputation
engine (F-Secure) constantly acting as an outlier. It is easily
inﬂuenced by other engines, and its label accuracy is subpar.
We notice that high-reputation engines are not always more
accurate. Four of them are not good at handling obfuscated
benign ﬁles, producing many false positives (e.g., Symantec).
Limitations & APK Experiments. As discussed in Sec-
tion 3, a key limitation is that our datasets are not diverse
enough (e.g., the main dataset only has PE ﬁles, the ground-
truth datasets are focused on ransomware). We defer in-depth
analysis on other ﬁle types to future work. Here, we brieﬂy run
a quick measurement on Android APK ﬁles (another popular
ﬁle type for malware) to cross-examine the results.
The methodology is similar to our main experiment on
PE ﬁles. We sampled 2,071 fresh APK samples (with no
prior history at VirusTotal), and collected their daily labels
from December 26, 2019 to February 09, 2020 (46 days).
About half of the ﬁles were labeled as “benign” (1,144) by
all engines on day-1, and the other half (927) were labeled as
“malicious” by at least one engine. 59 engines have returned
their labels. We collected 5,303,106 data points in total.
We ﬁnd the major observations on PE ﬁles still hold for
APK ﬁles, with some differences. First, there are 16,453 ﬂips,
including 9,984 hazard ﬂips. Among the APK ﬁles that have
no ﬂip (1,264 ﬁles, 60% of all ﬁles), the vast majority of them
have been labeled as benign by all engines for the entire mea-
surement period. This is similar to what we observed on PE
ﬁles. Second, the top three engines with most ﬂips are Mi-
crosoft (41%), Fortinet (15%), and Tencent (10%). The engine
ranking is different from that of PE ﬁles, possibly due to the
different specialties of engines. Third, in terms of engine la-
bel correlations, we also identify tightly clustered engines for
APK ﬁles. For example, GData, BitDefender, Ad-Aware, Em-
sisoft, and MicroWorld-eScan are still clustered together. The
cluster is largely similar to that under PE ﬁles, and the only
difference is ESET-NOD32 is no longer in the cluster. Finally,
interestingly, engines that were highly-inﬂuenced under PE
ﬁles (e.g., F-Secure, Comodo, AegisLab, Arcabit) now become
“inﬂuencers” under APK ﬁles. Overall, the takeaway is that
engines face common problems such as label instability, and
they have their own specialties for different malware types.
Malware Coverage Issues.
VirusTotal is arguably the
biggest public malware database that researchers can ac-
cess for free. Even so, its malware coverage still has limi-
tations [38, 48]. Beyond ﬁle label stability and accuracy (the
focus of our paper), another challenge is to further improve
the malware coverage of VirusTotal’s database. In some way,
VirusTotal is already trying to improve its coverage by pro-
viding free malware scanning services to the public to gather
new malware samples from users, companies, and other secu-
rity vendors. A recent report shows that VirusTotal receives
over one million ﬁle submissions every day [65]. Future work
could look into new incentive mechanisms to encourage the
broader sharing of malware intelligence.
Data Sharing. To beneﬁt future researchers and practition-
ers, we have released the raw data collected in this paper
2374    29th USENIX Security Symposium
USENIX Association
(timestamped ﬁle labels) and a number of ranked lists. The
engines can be ranked based on different criteria, such as
the number of (hazard) ﬂips, inﬂuence scores under differ-
ent inﬂuence models, and label accuracy. We have attached
the ranking method and the data with each ranked list. Our
released data is available at https://sfzhu93.github.io/
projects/vt/index.html.
8 Conclusions
In this paper, we surveyed 115 research publications that use
VirusTotal for data annotation. Then we took a data-driven
approach to reason and validate their data labeling method-
ologies. We collected a dataset of more than 14,000 ﬁles’
timestamped labels over a year from 65 anti-malware engines
at VirusTotal. We validated the beneﬁts of threshold-based
labeling methods in tolerating temporary label ﬂips. We also
pointed out the questionable approaches such as hand-picking
trusted engines, and ignoring the strong correlations among
engines. Future work will focus on extending the experiments
to other ﬁle types, using more diverse ground-truth datasets,
and developing new label aggregation methods.
Acknowledgement
We would like to thank our shepherd Gianluca Stringhini
and the anonymous reviewers for their helpful feedback; Xiao
Zhang at Palo Alto Networks, Inc. for sharing malware hashes;
Yiying Zhang for her valuable comments on the initial version
of this paper. Limin Yang and Gang Wang were supported by
NSF grants CNS-1750101 and CNS-1717028.
References
[1] Binutils - GNU Project - Free Software Foundation. https:
//www.gnu.org/software/binutils/.
[2] C# Websocket Implementation.
https://github.com/
statianzo/Fleck.
[3] Contributors
- VirusTotal.
https://support.
virustotal.com/hc/en-us/articles/115002146809-
Contributors.
[4] Coreutils - GNU core utilities. https://www.gnu.org/
software/coreutils/.
[5] Cygwin. https://cygwin.com/.
[6] Mono. https://www.mono-project.com/.
[7] Notepad++ ofﬁcial repository.
https://github.com/
notepad-plus-plus/notepad-plus-plus.
[8] Oreans technology : Software security deﬁned. https://
oreans.com/codevirtualizer.php.
[9] Oreans technology : Software security deﬁned. https://
oreans.com/themida.php.
[10] Statistics - VirusTotal. https://www.virustotal.com/en/
statistics/.
[11] VirusTotal. https://www.virustotal.com/.
[12] VirusTotal, Chronicle
and Google Cloud.
//blog.virustotal.com/2019/06/virustotal-
chronicle-and-google-cloud.html.
https:
[13] Randy Abrams.
VirusTotal Tips, Tricks and Myths.
https://www.virusbulletin.com/uploads/pdf/
magazine/2017/VB2017-Abrams.pdf.
[14] Kevin Allix, Quentin Jérôme, Tegawendé F. Bissyandé, Jacques
Klein, Radu State, and Yves Le Traon. A forensic analysis of
android malware – how is malware written and how it could
be detected? In COMPSAC, 2014.
[15] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gas-
con, Konrad Rieck, and CERT Siemens. Drebin: Effective and
explainable detection of android malware in your pocket. In
NDSS, 2014.
[16] Ulrich Bayer, Paolo Milani Comparetti, Clemens Hlauschek,
Christopher Kruegel, and Engin Kirda. Scalable, behavior-
based malware clustering. In NDSS, 2009.
[17] Stephen D. Brown and Zvonko G. Vranesic. Fundamentals of
digital logic with VHDL design. 2009.
[18] Zhenquan Cai and Roland H.C. Yap. Inferring the detection
logic and evaluating the effectiveness of android anti-virus
apps. In CODASPY, 2016.
[19] Margaux Canet, Amrit Kumar, Cédric Lauradoux, Mary-
Andréa Rakotomanga, and Reihaneh Safavi-Naini. Decom-
pression quines and anti-viruses. In CODASPY, 2017.
[20] Curtis Carmony, Xunchao Hu, Heng Yin, Abhishek Vasisht
Bhaskar, and Mu Zhang. Extract me if you can: Abusing pdf
parsers in malware detectors. In NDSS, 2016.
[21] Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and
Krishna P Gummadi. Measuring user inﬂuence in twitter: The
million follower fallacy. In AAAI, 2010.
[22] Mahinthan Chandramohan, Hee Beng Kuan Tan, and
Lwin Khin Shar. Scalable malware clustering through coarse-
grained behavior modeling. In FSE, 2012.
[23] Kai Chen, Peng Wang, Yeonjoon Lee, XiaoFeng Wang, Nan
Zhang, Heqing Huang, Wei Zou, and Peng Liu. Finding un-
known malice in 10 seconds: Mass vetting for new threats at
the google-play scale. In USENIX Security, 2015.
[24] Binlin Cheng, Jiang Ming, Jianmin Fu, Guojun Peng, Ting
Chen, Xiaosong Zhang, and Jean-Yves Marion. Towards
paving the way for large-scale windows malware analysis:
generic binary unpacking with orders-of-magnitude perfor-
mance boost. In CCS, 2018.
[25] Melissa Chua and Vivek Balachandran. Effectiveness of an-
In CODASPY,
droid obfuscation on evading anti-malware.
2018.
[26] Christian Collberg, GR Myles, and Andrew Huntwork.
IEEE S
Sandmark-a tool for software protection research.
& P, 2003.
USENIX Association
29th USENIX Security Symposium    2375
[27] Fady Copty, Matan Danos, Orit Edelstein, Cindy Eisner, Dov
Murik, and Benjamin Zeltser. Accurate malware detection by
extreme abstraction. In ACSAC, 2018.
[28] Wayne W Daniel. Applied nonparametric statistics. PWS-
Kent, 2nd edition, 1990.