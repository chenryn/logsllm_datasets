# 12 \| 数据集成：这些大号一共20亿粉丝？我们采集的数据经常会有冗余重复的情况。举个简单的例子，假设你是一个网络综艺节目的制片人，一共有12 期节目，你一共打算邀请 30位明星作为节目的嘉宾。你知道这些明星影响力都很大，具体在微博上的粉丝数都有标记。于是你想统计下，这些明星一共能直接影响到微博上的多少粉丝，能产生多大的影响力。然后你突然发现，这些明星的粉丝数总和超过了 20亿。那么他们一共会影响到中国 20亿人口么？显然不是的，我们都知道中国人口一共是 14 亿，这 30位明星的影响力总和不会覆盖中国所有人口。那么如何统计这 30位明星真实的影响力总和呢？这里就需要用到数据集成的概念了。数据集成就是将多个数据源合并存放在一个数据存储中（如数据仓库），从而方便后续的数据挖掘工作。据统计，大数据项目中 80%的工作都和数据集成有关，这里的数据集成有更广泛的意义，包括了数据清洗、数据抽取、数据集成和数据变换等操作。这是因为数据挖掘前，我们需要的数据往往分布在不同的数据源中，需要考虑字段表达是否一样，以及属性是否冗余。
## 数据集成的两种架构：ELT 和 ETL数据集成是数据工程师要做的工作之一。一般来说，数据工程师的工作包括了数据的ETL和数据挖掘算法的实现。算法实现可以理解，就是通过数据挖掘算法，从数据仓库中找到"金子"。``{=html}什么是 ETL 呢？ETL 是英文 Extract、Transform 和 Load的缩写，顾名思义它包括了数据抽取、转换、加载三个过程。ETL可以说是进行数据挖掘这项工作前的"备菜"过程。我来解释一下数据抽取、转换、加载这三个过程。抽取是将数据从已有的数据源中提取出来。转换是对原始数据进行处理，例如将表输入 1 和 表输入 2进行连接形成一张新的表。![](Images/f629f584188cb2de20068a8578e41406.png){savepage-src="https://static001.geekbang.org/resource/image/31/68/31a88246298e317c90412bc9c03eee68.png"}如果是三张表连接的话，可以怎么操作呢？先将表输入 1 和表输入 2进行连接形成表输入 1-2，然后将表输入 1-2 和表输入 3进行连接形成新的表。然后再将生成的新表写入目的地。![](Images/81a8d51423436c6a1352c5e0ff140c86.png){savepage-src="https://static001.geekbang.org/resource/image/bd/f7/bd39e4f480f92a1794ccc43f51acf9f7.png"}根据转换发生的顺序和位置，数据集成可以分为 ETL 和 ELT 两种架构。ETL 的过程为提取 (Extract)------转换 (Transform)------加载(Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。ELT 的过程则是提取 (Extract)------加载 (Load)------变换(Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如Spark 来完成转换的步骤。![](Images/00faddb37828850443364651bb4459ed.png){savepage-src="https://static001.geekbang.org/resource/image/90/c2/90df6593871fa5974e744907bb145bc2.jpg"}目前数据集成的主流架构是 ETL，但未来使用 ELT作为数据集成架构的将越来越多。这样做会带来多种好处：-   ELT 和 ETL    相比，最大的区别是"重抽取和加载，轻转换"，从而可以用更轻量的方案搭建起一个数据集成平台。使用    ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面    ELT 允许 BI    分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。-   在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL    中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。**ETL 工具有哪些？**介绍完了这两种架构，你肯定想要知道 ETL 工具都有哪些？典型的 ETL 工具有:-   商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle    Data Integrator、Microsoft SQL Server Integration Services 等-   开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等相对于传统的商业软件，Kettle是一个易于使用的，低成本的解决方案。国内很多公司都在使用 Kettle用来做数据集成。所以我重点给你讲解下 Kettle 工具的使用。
## Kettle 工具的使用Kettle 是一款国外开源的 ETL 工具，纯 Java 编写，可以在 Window 和 Linux上运行，不需要安装就可以使用。Kettle中文名称叫水壶，该项目的目标是将各种数据放到一个壶里，然后以一种指定的格式流出。Kettle 在 2006 年并入了开源的商业智能公司 Pentaho, 正式命名为 PentahoData Integeration，简称"PDI"。因此 Kettle 现在是 Pentaho的一个组件，下载地址：在使用 Kettle 之前还需要安装数据库软件和 Java 运行环境（JRE）。Kettle采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation转换和 Job 作业。-   Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。-   Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。接下来，我分别讲下这两个脚本的创建过程。**如何创建 Transformation（转换）**Transformation 可以分成三个步骤，它包括了输入、中间转换以及输出。![](Images/8ece72b9ab310f18b4fd2ee730ddacd3.png){savepage-src="https://static001.geekbang.org/resource/image/de/ea/de0b5c9d489a591fa1f478cfd09585ea.png"}在 Transformation 中包括两个主要概念：Step 和 Hop。Step的意思就是步骤，Hop 就是跳跃线的意思。-   Step（步骤）：Step 是转换的最小单元，每一个 Step    完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这    4 个步骤；-   Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。![](Images/a4c0ac4c14ab7885aa36464ec793159e.png){savepage-src="https://static001.geekbang.org/resource/image/ba/f7/bab04c4d05f3a5dbd02d8abd147630f7.jpg"}**如何创建 Job（作业）：**完整的任务，实际上是将创建好的转换和作业串联起来。在这里 Job包括两个概念：Job Entry、Hop。如何理解这两个概念呢？-   Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job    Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。-   Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。在 Kettle 中，你可以使用 Spoon，它是一种一种图形化的方式，来让你设计 Job和Transformation，并且可以保存为文件或者保存在数据库中。下面我来带你做一个简单的例子。**案例 1：如何将文本文件的内容转化到 MySQL 数据库中**这里我给你准备了文本文件，这个文件我上传到了 GitHub上，你可以自行下载：，数据描述如下：![](Images/d179f3fe0e20b5680400f74b25e5d3d2.png){savepage-src="https://static001.geekbang.org/resource/image/3b/97/3b6ad903051b066bfba1a4cf3a0d3197.png"}下面我来教你，如何将文本文件的内容转化到 MySQL 数据库中。Step1：创建转换，右键"转换→新建"；Step2：在左侧"核心对象"栏目中选择"文本文件输入"控件，拖拽到右侧的工作区中；Step3：从左侧选择"表输出"控件，拖拽到右侧工作区；Step4：鼠标在"文本文件输入"控件上停留，在弹窗中选择图标，鼠标拖拽到"表输出"控件，将一条连线连接到两个控件上；这时我们已经将转换的流程设计好了，现在是要对输入和输出两个控件进行设置。Step5：双击"文本文件输入"控件，导入已经准备好的文本文件；Step6：双击"表输出"控件，这里你需要配置下 MySQL数据库的连接，同时数据库中需要有一个数据表，字段的设置与文本文件的字段设置一致（这里我设置了一个wucai 数据库，以及 score 数据表。字段包括了name、create_time、Chinese、English、Math，与文本文件的字段一致）。具体操作可以看下面的演示：![](Images/1e7e33e8fc9c4d64c21fdc5f1b344b68.png){savepage-src="https://static001.geekbang.org/resource/image/6f/c9/6fb632fe0f3a2cce4169b9633a86c0c9.gif"}Step7：创建数据库字段的对应关系，这个需要双击"表输出"，找到数据库字段，进行字段映射的编辑；![](Images/7a0c6965faeb54022c940180686232f8.png){savepage-src="https://static001.geekbang.org/resource/image/8f/04/8fee5a90cab86200102a7006efe88104.png"}\Step8：点击左上角的执行图标，如下图：![](Images/eba599687ce9f3e105d9ae17b50b69db.png){savepage-src="https://static001.geekbang.org/resource/image/f9/6d/f9678df2fa2d1684a03d43f90cce716d.png"}这样我们就完成了从文本文件到 MySQL 数据库的转换。Kettle的控件比较多，内容无法在一节课内容中完整呈现，我只给你做个入门了解。另外给你推荐一个 Kettle 的开源社区： 。在社区里，你可以和大家进行交流。因为 Kettle相比其他工具上手简单，而且是开源工具，有问题可以在社群里咨询。因此我推荐你使用Kettle 作为你的第一个 ETL 工具。当然除了 Kettle 工具，实际工作中，你可能也会接触到其他的 ETL工具，这里我给你简单介绍下阿里巴巴的开源工具 DataX 和 Apache 的 Sqoop。**阿里开源软件：DataX**在以往的数据库中，数据库都是两两之间进行的转换，没有统一的标准，转换形式是这样的：![](Images/59dc0de94c455e81d0303e73b43e4a2d.png){savepage-src="https://static001.geekbang.org/resource/image/cf/50/cf2a017d2055afb1f8236bd6518e1c50.jpg"}但 DataX可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接了不同的数据源，以完成它们之间的转换。![](Images/33f1b56afb5cdb33a8f681e83c6cac4f.png){savepage-src="https://static001.geekbang.org/resource/image/ad/ac/ad35fb37962b548f6bf7a00b291ec9ac.jpg"}DataX 的模式是基于框架 + 插件完成的，DataX 的框架如下图：![](Images/718d73750fbbf5e16981624549e68119.png){savepage-src="https://static001.geekbang.org/resource/image/8c/67/8cac4b4298187dcd4621c9a1f9551a67.jpg"}在这个框架里，Job 作业被 Splitter 分割器分成了许多小作业 Sub-Job。在DataX 里，通过两个线程缓冲池来完成读和写的操作，读和写都是通过 Storage完成数据的交换。比如在"读"模块，切分后的小作业，将数据从源头装载到DataXStorage，然后在"写"模块，数据从 DataXStorage 导入到目的地。这样的好处就是，在整体的框架下，我们可以对 Reader 和 Writer进行插件扩充，比如我想从 MySQL 导入到 Oracle，就可以使用 MySQLReader 和OracleWriter 插件，装在框架上使用即可。**Apache 开源软件:Sqoop**Sqoop 是一款开源的工具，是由 Apache基金会所开发的分布式系统基础架构。Sqoop 在 Hadoop生态系统中是占据一席之地的，它主要用来在 Hadoop和关系型数据库中传递数据。通过Sqoop，我们可以方便地将数据从关系型数据库导入到 HDFS 中，或者将数据从HDFS 导出到关系型数据库中。Hadoop 实现了一个分布式文件系统，即 HDFS。Hadoop 的框架最核心的设计就是HDFS 和 MapReduce。HDFS 为海量的数据提供了存储，而 MapReduce则为海量的数据提供了计算。
## 总结今天我介绍了数据集成的两种架构方式，以及 Kettle工具的基本操作。不要小看了ETL，虽然它不直接交付数据挖掘的结果，但是却是数据挖掘前重要的工作，它包括了抽取各种数据、完成转化和加载这三个步骤。因此除了数据科学家外，还有个工作职位叫 ETL工程师，这份工作正是我们今天介绍的从事 ETL这种架构工作的人。如果你以后有机会从事这份工作，你不仅要对今天介绍的数据集成概念有所了解，还要掌握至少一种ETL 开发工具，如 Kettle、DataX、 Sqoop等；此外还需要熟悉主流数据库技术，比如 SQL Server、PostgeSQL、Oracle等。![](Images/d5a4a74175c7fe10f8959d69ca639e7d.png){savepage-src="https://static001.geekbang.org/resource/image/07/a0/0767b2fa01ea526d19857e9b95bc1ba0.jpg"}这是我操作 kettle 的流程视频，你可以看一下。```{=html}``````{=html}```今天我给你讲了数据集成的两种架构，以及帮助我们实现 ETL 的工具Kettle。纸上得来终觉浅，绝知此事要躬行。你不妨尝试下如何使用 Kettle 将MySQL 数据库内容转化到文本文件？另我想让你来谈谈，你对数据集成的理解，如果你之前做过 ETL的工具，也请你来谈一谈你对 ETL 的工具选择和使用经历。欢迎在评论区与我分享你的想法。如果你觉得这篇文章对你有帮助，欢迎点击"[请朋友读]{.orange}"，分享给你的朋友和同事。![](Images/8b75105190797b2e4f7be2536b6543db.png){savepage-src="https://static001.geekbang.org/resource/image/48/96/48cb89aa8c4858bbc18df3b3ac414496.jpg"}
# 13 \| 数据变换：考试成绩要求正态分布合理么？上一讲中我给你讲了数据集成，今天我来讲下数据变换。如果一个人在百分制的考试中得了 95分，你肯定会认为他学习成绩很好，如果得了 65分，就会觉得他成绩不好。如果得了 80分呢？你会觉得他成绩中等，因为在班级里这属于大部分人的情况。为什么会有这样的认知呢？这是因为我们从小到大的考试成绩基本上都会满足正态分布的情况。什么是正态分布呢？正态分布也叫作常态分布，就是正常的状态下，呈现的分布情况。比如你可能会问班里的考试成绩是怎样的？这里其实指的是大部分同学的成绩如何。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。正态分布的另一个好处就是，如果你知道了自己的成绩，和整体的正态分布情况，就可以知道自己的成绩在全班中的位置。![](Images/6cee212784a6174a303bd8a318cd7654.png){savepage-src="https://static001.geekbang.org/resource/image/e7/f5/e77a79d3c483c93e74933becd92b5af5.jpg"}另一个典型的例子就是，美国 SAT考试成绩也符合正态分布。而且美国本科的申请，需要中国高中生的 GPA 在 80分以上（百分制的成绩），背后的理由也是默认考试成绩属于正态分布的情况。为了让成绩符合正态分布，出题老师是怎么做的呢？他们通常可以把考题分成三类：第一类：基础题，占总分 70%，基本上属于送分题；第二类：灵活题，基础范围内 + 一定的灵活性，占 20%；第三类：难题，涉及知识面较广的难题，占 10%；``{=html}那么，你想下，如果一个出题老师没有按照上面的标准来出题，而是将第三类难题比重占到了70%，也就是我们说的"超纲"，结果会是怎样呢？你会发现，大部分人成绩都"不及格"，最后在大家激烈的讨论声中，老师会将考试成绩做规范化处理，从而让成绩满足正态分布的情况。因为只有这样，成绩才更具有比较性。所以正态分布的成绩，不仅可以让你了解全班整体的情况，还能了解每个人的成绩在全班中的位置。
## 数据变换在数据分析中的角色我们再来举个例子，假设 A 考了 80 分，B 也考了 80分，但前者是百分制，后者 500分是满分，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的分数代表的含义完全不同。所以说，有时候数据变换比算法选择更重要，数据错了，算法再正确也是错的。你现在可以理解为什么80% 的工作时间会花在前期的数据准备上了吧。那么如何让不同渠道的数据统一到一个目标数据库里呢？这样就用到了数据变换。在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。![](Images/58e2a03786f3804189c8139f3a622824.png){savepage-src="https://static001.geekbang.org/resource/image/90/e9/9081a928916973723e66d70c771162e9.jpg"}\所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它**通过数据平滑、数据聚集、数据概化和规范化等方式**将数据转换成适用于数据挖掘的形式。我来介绍下这些常见的变换方法：1.  **数据平滑**：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；2.  **数据聚集**：对数据进行汇总，在 SQL    中有一些聚集函数可以供我们操作，比如 Max()    反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；3.  **数据概化**：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。4.  **数据规范化**：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小---最大规范化、Z---score    规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；5.  **属性构造**：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个"总和"这个属性，来作为新属性。这样"总和"这个属性就可以用到后续的数据挖掘计算中。在这些变换方法中，最简单易用的就是对数据进行规范化处理。下面我来给你讲下如何对数据进行规范化处理。
## 数据规范化的几种方法**1. Min-max 规范化**Min-max 规范化方法是将原始数据变换到 \[0,1\] 的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。**2. Z-Score 规范化**假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80分与 B 的 80 分代表完全不同的含义。那么如何用相同的标准来比较 A 与 B 的成绩呢？Z-Score就是用来可以解决这一问题的。我们定义：新数值 =（原数值 - 均值）/ 标准差。假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值=(80-400)/100=-3.2。那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。我们能看到 Z-Score的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。**3. 小数定标规范化**小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A的取值范围就被规范化为 -0.999 到 0.088。上面这三种是数值规范化中常用的几种方式。
## Python 的 SciKit-Learn 库使用SciKit-Learn 是 Python的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。我现在来讲下如何使用 SciKit-Learn 进行数据规范化。**1. Min-max 规范化**我们可以让原始数据投射到指定的空间 \[min, max\]，在 SciKit-Learn里有个函数 MinMaxScaler是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到\[min, max\] 中。默认情况下 \[min,max\] 是\[0,1\]，也就是把原始数据投放到 \[0,1\] 范围内。我们来看下下面这个例子：    
# coding:utf-8from sklearn import preprocessingimport numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征x = np.array([[ 0., -3.,  1.],              [ 3.,  1.,  2.],              [ 0.,  1., -1.]])