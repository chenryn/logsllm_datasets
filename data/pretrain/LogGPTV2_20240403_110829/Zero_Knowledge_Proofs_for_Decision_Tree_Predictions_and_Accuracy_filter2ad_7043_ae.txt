create are more reliable.
Clustering-based method
To address the problem of ﬁnding the categories to transform the re-
gression problem in a classiﬁcation problem, we can perform a cluster-
ing algorithm on the output values of the training set. We chose to use
a gaussian mixture model [27] clustering because it takes into account
the variance of the data whereas the k-means does not. We thought
that modeling the variance was a real improvement regarding the re-
gression problem.
This clustering could be adaptive: at each new node in the con-
struction of the decision tree, a clustering step would be performed.
For our purpose, we only computed an initial global clustering on the
whole dataset and we did not try to update the clustering along the
building of the tree because it seemed to be too time consuming.
This clustering-based method is more complex than the previous
quantile methods. Since the main property that we want to have for
the output model is global explainability, we can use more advanced
methods in the preprocessing if it improves the perfomance of the
model even at the cost of longer training times.
To conclude, our main contribution is to perform a clustering on
the output value to transform the regression problem into a classiﬁ-
cation problem. This clustering can be adaptive if it is done at each
34
CHAPTER 3. METHODS AND EVALUATION
new node in the construction of the decision tree. This is made to use
the covariance split which seems to produce relevant bags of categor-
ical values in the returned decision tree. The covariance split can then
be used on the categories learnt by this clustering. This way, decision
trees can be adapted for regression.
3.3.2 Data augmentation for improving decision tree
predictive power
If decision trees have a good explainability, their predictive power
is often worse than other classic state-of-the-art models such as XG-
Boost[2]. We tried several methods to improve decision trees predic-
tions.
As seen in Chapter 2, data-augmentation is a very classic technique
to improve the results of a model, especially in image classiﬁcation.
We tried to use the same idea in our experiments and to reduce the
gap between the random forest algorithm, the XGboost algorithm and
a simple decision tree. For that, we had two approaches. Firstly, data
augmentation can be done by adding random samples that follow the
bounds and the distribution determined by the original dataset. To la-
bel them, we tried to use a more robust model like xgboost trained on
the original dataset hoping that we could gain information from it. We
decided to add weights to the generated datapoints in order to pre-
serve the importance of the original datasets. The main drawback of
this method is that we could possibly add some datapoints that are not
relevant at all for the problem, and create new outliers. On the other
side, our hope was that we could possibly regenerate a very accurate
tree only storing the strong model.
Secondly, we tried to increase the number of samples by adding some
noisy samples with the same label as the original sample. We were not
sure that this method was really relevant because decision trees must
be robust to noisy data. In fact, only the proportion of the samples
present on each side of the splitting value is taken into account and
not their distance from the boundary. We had to be sure that we were
not destroying the structure of our dataset. The solution was to add a
ﬁxed number of points around each datapoint and to associate them
its original label.
We were expecting that we could increase the predictive power of de-
cision tree by combining complex model and new data.
Chapter 4
Results
After presenting the results obtained from the new categorical split
algorithm, we explain the new proposed structure to increase decision
tree accuracy.
4.1 Categorical split
We mainly perform the experiments on the categorical split on classi-
ﬁcation problems to see if we can obtain relevant groups of categories
using these methods. We thought that it was relevant to benchmark
these methods in order to have more descriptive results about them.
Then we tried our new methods of splitting on the regression problem.
4.1.1 Categorical split on classiﬁcation
The datasets used for the experiments were introduced in Chapter 3.
All the results given in Table 4.1, 4.2 and 4.3 are obtained for the opti-
mised hyperparameters based on the cross-validated accuracy for each
splitting method.
35
36
CHAPTER 4. RESULTS
Dataset
Good client
Mushroom
Accuracy
Training Time (milliseconds) Nodes
7.2 ± 0.02
50 ± 7
0.83 ± 0.03
204 ± 24
Bank Marketing 0.94 ± 0.01
25 ± 5
Table 4.1: Main performance indicators using classic native C4.5 split
method
Classic split
29 ± 4
251 ± 12
183 ± 31
Covariance split
Dataset
Good client
Mushroom
Training Time (milliseconds) Nodes
Accuracy
23 ± 4
0.71 ± 0.03
108 ± 13
0.83 ± 0.02
8 ± 2
Bank Marketing 0.94 ± 0.01
Table 4.2: Main performance indicators using the covariance split
method for categorical features
37 ± 9
352 ± 15
227 ± 43
One versus all split
Dataset
Good client
Mushroom
Accuracy
0.717 ± 0.018
0.832 ± 0.029
Bank Marketing 0.947 ± 0.015
Table 4.3: Results using the one versus all method for categorical fea-
tures
Training Time (milliseconds) Nodes
29 ± 5
104 ± 11
16 ± 3
37 ± 5
324 ± 21
209 ± 36
In our experiments, all the methods give almost the same perfor-
mances in terms of accuracy. In terms of training time, more complex
methods lead to training times that are signiﬁcantly longer. This effect
is due to two main factors:
• Trees are deeper because we learn "less" information by separat-
ing all categorical values into two categories.
• Computation for the split evaluation is more complicated for the
covariance split and the one-versus-all split. In both cases, we
compute n different splits and, for the covariance split, we also
have to compute a covariance matrix and inverse it.
CHAPTER 4. RESULTS
37
Even if there is an increase in the training time, it seems to be very
limited. The biggest increase that we have seen in our experiments is
about 25 %, which is not very large compared to the speed of training
of a single decision tree model and to the gain that they provide in term
of compactness. In fact, it was on this point that our experiments were
really interesting: the covariance split method allowed us to divide by
2 (and up to 3) the number of nodes in the learned model. We also
tried to understand why the training time of trees that have less nodes
increases a lot. We noticed that the execution time for the learning
algorithm depends more on the depth of the tree than the number of
nodes because it simply goes through the whole dataset at each new
layer, and this does not depend on the number of nodes present at any
given level.
We performed an in-depth comparison between the covariance split
and the regular split. We represented the evolution of the the accuracy,
the number of nodes and the training time as a function of the depth
of the tree for the Good client dataset (Figure 4.1) for the mushroom
dataset (Figure 4.2) and for the bank campaign dataset (Figure 4.3).
On the dataset Good client, similar accuracy (Figure 4.1a) can be
achieved with both methods but not when using the same depth for
the two decision trees. For the simple method one should use a deci-
sion tree of depth 2, whereas the new proposed method on categorical
split select requires a depth of 4. A small increase in the computation
time is noticeable. Looking at the different computation steps, this
increase is mainly due to the inversion of covariance matrices which
takes a lot of time to be computed (Figure 4.1c). The number of nodes
decreases signiﬁcantly even if it is necessary to build deeper trees. the
results still have less nodes allowing to create more compact models
with more complex decision rules (Figure 4.1d).
It is also interesting to check directly if the subgroup of categorical
values created by the algorithm seems to be relevant. For example,
in this case, the algorithm makes two groups among the unemployed
and the retired people versus the ones who are in the active life and
currently working.
Inspecting the results on the second dataset on the success of a bank
campaign, the trend is almost the same. A deeper tree has to be built
to achieve the same accuracy (Figure 4.3a) but, even if it is deeper, it
still has less nodes (Figure 4.3d). This also lead to a real improvement
38
CHAPTER 4. RESULTS
(a) Accuracy in function of depth
on the testing set
(b) Accuracy in function of depth
on the training set
(c) Time in seconds to build a deci-
sion tree
(d) Number of nodes in produced
decision trees
Figure 4.1: Performances indicators on the Good client dataset
based on a 10-fold cross-validation
CHAPTER 4. RESULTS
39
(a) Accuracy in function of depth
on the testing set
(b) Accuracy in function of depth
on the training set
(c) Time in seconds to build a deci-
sion tree
(d) Number of nodes in produced
decision trees
Figure 4.2: Performances indicators on the Mushroom dataset based
on a 10-fold cross-validation
40
CHAPTER 4. RESULTS
in term of explainability, if it is measured by the number of nodes of
a model. In this case, the computation time also increases due to the
inversion of covariance matrices (Figure 4.3).
Performance on the Mushroom dataset seems to follow the same
pattern. We can achieve the same accuracy (Figure 4.2a) using both
splitting methods. The new method using covariance split reduces a
lot the number of nodes in the produced decision tree (Figure 4.2c) but
it is also at the cost of execution time (Figure 4.2d).
Good client
Mushroom
Bank Marketing
Reject equality of training time
(cid:88)
(cid:88)
x
p-value
1.02 × 10−2
1.028 × 10−10
0.12
Cohen d-test
1.14
7.43
1.17
Table 4.4: Summary of statistical tests on training time on classiﬁcation
tasks. Comparison between new splitting methods and classic one.
((cid:88): Equality rejected x: Cannot reject the equality)
Reject equality of accuracy p-value Cohen d-test
Good client
Mushroom
Bank Marketing
x
x
x
0.35
0.67
0.87
0.39
0
0
Table 4.5: Summary of statistical tests on accuracy on classiﬁcation
tasks. Comparison between new splitting method and classic one. ((cid:88):
Equality rejected x: Cannot reject the equality)
Good client
Mushroom
Bank Marketing
Reject equality of number of nodes
p-value
4.02 × 10−9
3.16 × 10−10
1.87 × 10−7
Table 4.6: Summary of statistical tests on number of nodes on classi-
ﬁcation tasks. Comparison between new splitting method and classic
one. ((cid:88): Equality rejected x: Cannot reject the equality)
(cid:88)
(cid:88)
(cid:88)
Cohen d-test
4.73
4.97
3.13
CHAPTER 4. RESULTS
41
(a) Accuracy in function of depth
on the testing set
(b) Accuracy in function of depth
on the training set
(c) Time in seconds to build a deci-
sion tree
(d) Number of nodes in produced
decision trees
Figure 4.3: Performances indicators on the Mushroom dataset based
on a 10-fold cross-validation
42
CHAPTER 4. RESULTS
We ﬁnally make statistical tests on the different evaluated metrics:
execution time, accuracy and number of nodes. We assume that the
classic splitting method and the new provided method using covari-
ance split provide the same performances. These experiments prove
that using the covariance split method enhances signiﬁcantly the ex-
plainability of a decision tree by making models more compact (Table
4.6) at the cost of the computation time (Table 4.4). Moreover, both
computation time and number of node measurement have a strong
effect size. The accuracy seems to be not affected by the change of
method for splitting on categorical values, the effect size is close to
zero.
(Table 4.5). The one versus all method has not been selected
for in-depth comparison because it does not create complex groups of
categorical values which was the interesting point of this study.
4.1.2 Categorical split on regression task
The regression task was very difﬁcult to achieve because it is hard to
ﬁnd relevant datasets with dozens of categorical features with several
of values for each feature. Only two datasets were used for the exper-
iments. The ﬁrst one is the prediction of the number of comments on
a Facebook page, given some indicators, and the second one was the
prediction of solar ﬂares according to several indicators.
First by looking at ﬁgure 4.4 obtained on the Facebook dataset, we
can see that the computation time does not increase that much if an-
other technique of split is used. However, the number of nodes in the
tree at equivalent depth is far lower when using a smart technique for
handling categorical values. The results are not so impressive as the
Facebook dataset does not have many categorical features.
Tables 4.7, 4.8 and 4.9 reference all the results obtained with the
different methods. Table 4.10 is a summary of all the results. The best
models were selected on the basis of the number of nodes which was
the relevant metric in this problem.
In these two examples, the number of nodes has been considerably
reduced by using new smart methods of categorical splits. Moreover,
the computation time does not increase that much on the Facebook
dataset. This can be explained by the relatively low number of cate-
CHAPTER 4. RESULTS
43
(a) Mean Squared Error in function
of depth on the testing set
(b) Mean Squared Error in function
of depth on the training set
(c) Time of execution to build a de-
cision tree
(d) Number of nodes in produced
decision trees
Figure 4.4: Performances indicators on the facebook dataset based on
a 10-fold cross-validation
44
CHAPTER 4. RESULTS
Best # of clusters
Quantile based
3
10
5
Mean Squared Error 25.03 26.02 24.75
Number of nodes
Training time (ms)
11.33 11.65 11.98
35
37
36
Adaptive
3
5
10
Clustering based
3
10
5
26.08 27.05 26.52
26.43 27.37 26.34
41
39
40
42
41
45
11.78 12.25 12.75
11.45 11.89 12.01
Table 4.7: Results on facebook dataset given different sets of hyperpa-
rameters (10 fold cross-validation, ms stands for milliseconds)
#cluster/quantile
Mean Squared Error 0.80 0.78
103
Number of nodes
Training time (ms)
161
98
121
5
Quantile based
10
3
0.79
99
212
Adaptive
3
0.82
102
173
5
10
0.84 0.86
104
105
250
312
5
3
Clustering based
10
0.78
108
270
0.79
101
146
0.81
106
198
Table 4.8: Results on solar ﬂare target 1 given different sets of hyper-
parameters (10 fold cross-validation, ms stands for milliseconds)
#cluster/quantile
Mean Squared Error 0.36 0.39
53
132
Number of node
Training time (ms)
52
101
5
Quantile based
10
3