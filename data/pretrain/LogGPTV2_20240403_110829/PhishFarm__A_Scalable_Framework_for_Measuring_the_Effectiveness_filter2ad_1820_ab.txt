mobile web browsers with native anti-phishing blacklists, as
listed in Table I. Although we also identiﬁed a handful of other
web browsers with blacklist protection, such as CM Browser,
we did not test them due to their low market share [4].
2) Filter Types: We chose a set of request ﬁltering tech-
niques based on high-level cloaking strategies found in a recent
study of .htaccess ﬁles from phishing kits [8]. Exhaustively
measuring every possible combination of request ﬁlters was
not feasible with a large sample size; we therefore chose a
manageable set of ﬁltering strategies which we felt would be
effective in limiting trafﬁc to broad yet representative groups
of potential victims while remaining simple (for criminals) to
implement and drawing inspiration from techniques found in
the wild [7]. Table III summarizes our ﬁlter selections.
It would not be responsible of us to disclose the exact condi-
tions required of each ﬁlter, but we can discuss them at a high
level. Filter A served as our control group; our expectation was
for every site in this group to be blacklisted at least as quickly
as other sites. Filter B sought to study how well mobile-only
TABLE II: Entities targeted by our experiments.
Entity (Report Location)
Full + Preliminary Tests
Report Type
URLs
APWG (PI:EMAIL)
Google Safe Browsing ([42])
Microsoft SmartScreen ([43])
PayPal (PI:EMAIL)
PhishTank (phish-{username}@phishtank.com)
E-mail
Web
Web
E-mail
E-mail
Preliminary Tests Only
ESET ([44])
Netcraft ([45])
McAfee ([46])
US CERT (PI:EMAIL)
WebSense (PI:EMAIL)
10 Entities Total
Web
Web
Web
E-mail
E-mail
40 (prelim.)
396 (full)
per entity
40
per entity
2,380 URLs
TABLE III: Cloaking techniques used by our phishing sites.
Cloaking Filter Name
A
B Mobile Devices
Control
C
D
E
F
US Desktop
GSB Browsers
Non-US Desktop
GSB Browsers
Block Known
Security Entities
“Real” Web
Browsers
HTTP Request Criteria
Allow all
Allow if user agent indicates:
Android or iOS
Allow if IP country (is/is not) US and user agent indicates:
Chrome, Firefox, or Safari; and
Windows, Macintosh, or Linux; and
not Opera, IE, or Edge; and
not iOS or Android
Allow if user agent, referrer, hostname, and IP:
not known to belong to a security entity or bot
Allow all; content retrieved asynchronously
during JavaScript onload event
phishing sites are blacklisted, coinciding with the recent uptick
in mobile users and phishing victims [28], [2]. Filters C
and D focus speciﬁcally on desktop browsers protected by
GSB, which span the majority of desktop users today. We
also included geolocation, which while not as frequent in the
wild as other cloaking types [8], is highly effective due to low
detectability and characteristics of spearphishing. A secondary
motivation was to see how well entities other than GSB protect
this group. Filter E is the most elaborate, but it also directly
emulates typical real-world phishing kits. It is based on top
.htaccess ﬁlters from a large dataset of recent phishing kits [8].
This ﬁlter seeks to block anti-phishing entities by hundreds of
IP addresses, hostnames, referrers, and user agents. Finally,
Filter F only displays phishing content if the client browser
can execute JavaScript, which may defeat simple script-based
web crawlers. This ﬁlter is common in modern search engine
cloaking [7] and further motivated by an ongoing study we
are conducting of JavaScript use in real-world phishing sites.
Although today’s phishing kits tend to favor straightforward
ﬁltering techniques (namely approaches such as Filter E or
geolocation), growing sophistication and adoption of cloaking
(and other types of evasion) is technically feasible and to be
expected as a risk to the anti-phishing ecosystem.
3) Tested Entities: In addition to the blacklist operators
themselves, major clearinghouses, and PayPal’s internal anti-
phishing system, we wanted to test as many of the other types
of anti-phishing entities discussed in Section II-C as possible.
We started with a recently-published list of entities commonly
targeted for evasion by phishers [8]. We then made selections
from this list, giving priority to the more common (and thus
potentially more impactful) entities in today’s ecosystem. We
had to exclude some entities of interest, as not all accept direct
(cid:18)(cid:20)(cid:21)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:45 UTC from IEEE Xplore.  Restrictions apply. 
external phishing reports and thus do not ﬁt our experimental
design. Also, we could not be exhaustive as we had to consider
the domain registration costs associated with conducting each
experiment. Table II summarizes our entity selections.
4) Reporting: When the time came for us to report our
phishing sites to each entity being tested, we would submit
the site URLs either via e-mail or the entity’s web interface
(if available and preferred by the entity). In all cases, we used
publicly-available submission channels; we had no special
agreements with the entities, nor did they know they were
being tested. In the case of the web submission channel, we
simply used a browser to submit each phishing site’s exact
URL to the entity. E-mail reports were slightly more involved,
as the industry prefers to receive attachments with entire
phishing e-mails in lieu of a bare URL. We thus used PayPal-
branded HTML e-mail templates to create lookalike phishing
messages. Each message contained a random customer name
and e-mail address (i.e. of a hypothetical victim) within one of
many body templates. We sent e-mails from unique accounts
across ﬁve security-branded domains under our control.
Reports of real-world phishing sites might be submitted in
the exact same manner by agents on behalf of victim brands.
Our reporting approach is thus realistic, though many larger
victim organizations contract the services of enterprise security
ﬁrms, which in turn have private communication channels to
streamline the reporting process.
B. Preliminary Tests
Our preliminary testing took place in mid-2017 and carried
out the full experimental approach from Section III-A on a
small scale. We will now detail the execution of each test.
One of our key goals was to minimize confounding effects
on our experimental results. In other words, we did not want
any factors other than our report submission and cloaking
strategy to inﬂuence the blacklisting of our phishing sites. As
part of this, we wanted to secure a large set of IP addresses,
such that no anti-phishing entity would see two of our sites
hosted on the same IP address. With the resources available
to us, we were able to provision 40 web servers powered
by Digital Ocean, a large cloud hosting provider [47] (we
informed Digital Ocean about our research to ensure the
servers would not be taken down for abuse [48]). Each server
had a unique IP address hosted in one of the host’s data centers
in Los Angeles, New York, Toronto, Frankfurt, Amsterdam,
or Singapore. Our batch size was thus 40 phishing sites per
preliminary test, for a total of 400 sites across the 10 entities
being tested. Within each batch, six sites used ﬁlter types A
and F, while seven used ﬁlter types B through E.
We also wanted to mimic actual phishing attacks as closely
as possible. We studied the classiﬁcation of phishing URL
types submitted to the Anti-phishing Working Group’s eCrime
Exchange in early 2017 and crafted our URLs for each of the
40 phishing sites while following this distribution as closely
as possible [8], [49]. We registered only .com domains for
our URLs, as the .com TLD accounts for the majority of real-
world phishing attacks. In addition, we chose GoDaddy as our
TABLE IV: URL and ﬁlter distribution for each experiment.
Phishing URL Content
Sample URL
(Type [8], [49])
Full Tests
Non-deceptive (random)
http://www.ﬂorence-central.com/logician/retch/
(V)
Preliminary Tests
(V)
(IVa)
Non-deceptive (random)
http://receptorpeachtreesharp.com/cultivable/
Brand in Domain
http://www.https-ofﬁcial-verifpaypal.com/signin
Deceptive Domain
(IVb)
http://services-signin.com/login/services/account/
Brand in Subdomain
(IIIa)
http://paypal1.com.835anastasiatriable.com/signin
Deceptive Subdomain
(IIIb)
http://services.account.secure.lopezben.com/signin
Deceptive Path
(II)
http://simpsonclassman.com/paypa1.com/signin
Qty.
Filters Used
A (66), B (66), C (66),
D (66), E (66), F (66)
A (1), B (2), C (2),
D (2), E (2), F (1)
A (1), B (1), C (1),
D (1), E (1), F (1)
396
10
6
6
6
6
6
registrar, which is among the most-abused registrars by real
phishers [1]. Furthermore, to prevent crawlers from landing
on our phishing sites by chance (i.e. by requesting the bare
hostname), paths were non-empty across all of our URLs.
Using URLs that appear deceptive is a double-edged sword:
while it allows us to gain insight into how various URL types
are treated by entities, it is also a factor which may skew
blacklisting speed. However, we decided to proceed with this
in mind as the purpose of the preliminary tests was to observe
more than measure, given the use of a relatively small sample
size per batch. Table IV shows the distribution of URL types
and ﬁlters per batch of sites.
We registered the required domain names and ﬁnalized
conﬁguration of our infrastructure in May 2017. In July,
we started our experiments by systematically deploying and
reporting our phishing sites. For each day over the course
of a 10-day period, we picked one untested entity at random
(from those in Table II), fully reported a single batch of URLs
(over the course of several minutes), and started monitoring
blacklist status across each targeted web browser. Monitoring
of each URL continued every 10 minutes for a total of 72
hours after deployment. Over this period and for several days
afterward, we also logged web trafﬁc information to each
of our phishing sites in an effort to study crawler activity
related to each entity. Prior empirical tests found blacklists to
show their weakness in early hours of a phishing attack [6].
Our 72-hour observation window allows us to study blacklist
effectiveness during this critical period while also observing
slower entities and potentially uncovering new trends.
C. Responsible Disclosure
Our analysis of the preliminary tests yielded several security
recommendations (discussed in Section VI). We immediately
proceeded to disclose our ﬁndings to the directly impacted
entities (i.e. browser vendors,
the brand itself, and major
blacklist operators) which we also intended to re-test. We held
our ﬁrst disclosure meeting with PayPal in August 2017. Fol-
lowing PayPal’s legal approval, we also disclosed to Google,
Microsoft, Apple, Mozilla, and the APWG in February 2018.
Each meeting consisted of a detailed review of the entity’s
performance in our study, positive ﬁndings, speciﬁc actionable
ﬁndings, and high-level comparisons to other entities. Our
(cid:18)(cid:20)(cid:21)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:45 UTC from IEEE Xplore.  Restrictions apply. 
disclosures were generally positively received and resulted
in close follow-up collaboration with Google, Mozilla, and
the APWG; this ultimately resulted in the implementation of
effective blacklisting within mobile GSB browsers and general
mitigations against certain types of cloaking. We clearly stated
that we would repeat our experiments in 4-6 months and
thereafter publish our ﬁndings.
We did not immediately disclose to the blacklists powering
Opera as we had not originally expected to have the resources
to re-test a ﬁfth entity. Additionally, given lesser short-term
urgency with respect to the remaining entities (and in the
absence of close relationships with them), at the time we felt
it would be more impactful to disclose to them the prelimi-
nary ﬁndings alongside the full test ﬁndings. This approach
ultimately allowed us to better guide our recommendations
by sharing deeper insight into the vulnerabilities within the
broader ecosystem. After the completion of the full tests,
we reached out the all remaining entities via e-mail; all but
PhishTank and Opera responded and acknowledged receipt of
a textual report containing our ﬁndings. US CERT and ESET
additionally followed up for clariﬁcations once thereafter.
D. Full-scale Tests
We believed that key ecosystem changes resulting from our
disclosures would still be captured by our original experi-
mental design. Thus, we did not alter our retesting approach
beyond increasing the scale to enable statistically signiﬁcant
observations. In addition, rather than using the URL distribu-
tion from the preliminary experiments, we solely used non-
deceptive paths and hostnames (i.e. with randomly-chosen
English words) in order to remove URL classiﬁcation as a
possible confounding factor in our cloaking evaluation.
We registered the required domains in May 2018 and
initiated sequential deployment of our full-scale tests in early
July. We re-tested all entities to which we disclosed. As our
budget ultimately allowed for an additional entity, we decided
to also include PhishTank for its promising performance in
the preliminary test. Each of the resulting ﬁve experiment
batches consisted of 396 phishing sites, evenly split into six
groups for each cloaking technique. Our reporting method
did not change, though we throttled e-mailing such that the
reports were spread over a one-hour period. Reporting through
Google and Microsoft’s web interfaces spanned a slightly
longer period of up to two hours due to the unavoidable manual
work involved in solving required CAPTCHA challenges.
E. Sample Size Selection
For our full tests, we chose a sample size of 384 phishing
sites for each entity. Our goal was to obtain a power of
0.95 at the signiﬁcance level of 0.05 in a one-way inde-
pendent ANOVA test, which, for each entity, could identify
the presence of a statistically signiﬁcant difference in mean
blacklisting speed between the six cloaking ﬁlters. Based on
Cohen’s recommendation [50] and our preliminary test results,
we assumed a medium effect size (f ) of 0.25. We added 12
sites (2 per ﬁlter) to each experiment to serve as backups in
Fig. 1: PhishFarm framework architecture.
case of unforeseen technical difﬁculties. All sites ultimately
delivered 100% uptime during deployment, thus we ended
up with an effective sample size of 396 per experiment.
While the assumption of f=0.25 introduced some risk into
our experimental design, we accepted it given the high cost of
registering a new .com domain for each site.
IV. PHISHFARM TESTBED FRAMEWORK
In order to execute our experimental design at scale, we de-
signed PhishFarm: a comprehensive framework for deploying
phishing sites, reporting them to anti-abuse entities, and mea-
suring blacklist response. The framework operates as a web
service and satisﬁes three important requirements: automation,
scalability, and reliability. We eliminated as many manual
actions as technically feasible to ensure that the difference
between launching hundreds and thousands of sites was only
a matter of minutes. Actual site deployment can thus happen
instantly and on-demand. Apart from up-front bulk domain
registration (IV-A) and reporting phishing through a web form,
all framework components feature end-to-end automation.
The framework consists of ﬁve interconnected components
as shown in Figure 1: bulk domain registration and DNS
setup, stateless cloud web servers to display the phishing
sites, an API and database that manages conﬁguration, a client
that
issues commands through the API, and a monitoring
system that regularly checks in which browsers each phishing
is blacklisted. In total, we wrote over 11,000 lines of PHP,
Java, and Python code for these backend components. We
extensively tested each component— in particular, the hosting
and monitoring infrastructure— to verify correct operation.
A. Domain and DNS Conﬁguration
A core component of any phishing site is its URL, which
consists of a hostname and path [49]. Our framework includes
a script to automatically generate hostnames and paths per the
experimental design. We then manually register the required
domain names in bulk and point them to our cloud hosting
provider’s nameservers. Finally, we automatically create DNS
(cid:18)(cid:20)(cid:21)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:45 UTC from IEEE Xplore.  Restrictions apply. 
records such that the domains for each experiment are evenly
spread across the IP addresses under our control. We set up
wildcard CNAME records [51] such that we could program-
matically conﬁgure the subdomain of each phishing site on
the server side. In total, registration of the 400 preliminary
domains took about 15 minutes, while registration of the 1,980
domains for the full tests took 30 minutes; apart from the
domain registration itself, no manual intervention is needed.
B. Research Client
We implemented a cross-platform client application to
control
the server-side components of the framework and
execute our research. This client enables bulk conﬁguration
of phishing sites and cloaking techniques, bulk deployment of
experiments, automated e-mail reporting, semi-automated web
reporting, monitoring of status, and data analysis.
C. Server-Side components
The server-side components in our framework display the
actual phishing sites based on dynamic conﬁguration. They
are also responsible for logging request data for later analysis
and monitoring blacklist status.
1) Central Database and API: At the heart of our frame-
work is a central API that serves as an interface to a database
with our phishing site and system state. For each site, the
database also maintains attributes such as date activated,
reported, and disabled; blacklisting status; site and e-mail
HTML templates; server-side and JavaScript request ﬁltering
code; and access logs. We interact with this API via the client
whenever we deﬁne new sites or deploy sites as part of an
experiment. All trafﬁc to and from the API is encrypted.
2) Hosting Infrastructure: Our hosting infrastructure con-
sists of Ubuntu cloud servers which run custom intermediate
software on top of Apache. The intermediate software captures
all requests and enables dynamic cloaking conﬁguration and
enhanced trafﬁc logging. Each server obtains all of its conﬁg-
uration from the API, which means that the number of servers
can ﬂexibly be chosen based on testing requirements (40 in our
case). When an HTTP request is received by any server, the
server checks the request URL against the list of live phishing
sites from the API, processes any cloaking rules, responds with
the intended content, and logs the request. In order to quickly
handle incoming requests, system state is cached locally on
each server to minimize API queries.
We served all our phishing sites over HTTP rather than
HTTPS, as this was typical among real-world phishing sites
at the time we designed the preliminary tests [1]. While our
framework supports HTTPS, we did not wish to alter the
experimental design between tests. Both approaches generate
potential artifacts that might beneﬁt the anti-phishing ecosys-
tem: unencrypted sites allow network-level packet snifﬁng,
while encrypted sites leave an evidence trail at the time a
certiﬁcate is issued. We mitigated the former risk to a large
extent through the design of our monitoring system.
Our framework supports arbitrary, brand-agnostic web page
content to be displayed for each phishing site. For our exper-
iments, we hosted all resources (e.g. images and CSS) locally
on each server to avoid confounding detection through external
web requests to these ﬁles. In the wild, we have observed
that sophisticated phishing kits follow a similar strategy to
reduce detectability, though today’s typical kits merely embed
resources from the legitimate website.
3) Monitoring Infrastructure: The purpose of the moni-
toring system is to identify, at a ﬁne granularity, how much
time elapses before a reported phishing site is blacklisted
(if at all). To obtain a clear picture of the early hours of
blacklist response, we conﬁgured the monitoring system to
access each live phishing URL at least once every 10 minutes
in each target desktop browser (the shortest interval feasible
given our resources). We check the blacklist status of each
URL by analyzing a screenshot of the respective browser
window; this is similar to the approach taken by Sheng et
al [6]. We chose this approach for its universal applicability
and lack of dependencies on browser automation libraries,
which commonly force-disable phishing protection. For our
purposes, it sufﬁced to check if the dominant color in each
image [52] was similar to red (used in the warning messages
of the browsers we considered). Although this image-based
approach proved reliable and can be fully automated, it does
not scale well because each browser requires exclusive use of
a single system’s screen.
To satisfy the scalability requirement, our monitoring sys-
tem follows a distributed architecture with multiple au-
tonomous nodes that communicate with the API; each node
is a virtual machine (VM) that runs on a smaller set of
host systems. Software on each VM points a browser to the
desired URL via command line and sends virtual keystrokes,
as needed,
to close stale tabs and allow for quick page