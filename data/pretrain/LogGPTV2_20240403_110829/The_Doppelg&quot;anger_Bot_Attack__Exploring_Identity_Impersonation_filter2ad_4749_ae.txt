To protect users we showed that humans are much better
at detecting impersonating accounts when they can also see
the victim account. Thus a system that protect users from
friend requests coming from cloned proﬁles could simply just
show the user all the accounts that portray the same person
with the account that is requesting the friendship.
Account matching.
There are a number of works that propose methods to match
the accounts a user has on multiple social networks that
are related to our techniques to detect doppelg¨anger pairs.
Note, however, the subtle diﬀerence, our goal is to ﬁnd ac-
counts that people think they portray the same person which
is slightly diﬀerent than detecting accounts that belong to
the same user. To detect doppelg¨anger pairs we ﬁrstly have
to only rely on visual features of accounts and then under-
stand when humans get confused.
While there are several studies that exploited visual fea-
tures similar to the features we use in this paper to match
accounts (refer to [9] for an overview), none of these stud-
ies applied their methods to detect impersonation attacks.
Most of the studies build classiﬁers that are able to detect
whether two accounts belong or not to the same person. We
drew inspiration from these works, however, we could not di-
rectly apply these techniques to gather doppelg¨anger pairs
because we could not estimate their accuracy as there is
no existing ground truth of accounts that portray the same
person in the same social network.
Sybil account detection.
One of the most widely used approach today to detect fake
accounts is to build behavioral proﬁles for trusted and un-
trusted users [3, 40, 29]. The behavioral proﬁle can include,
for example, the act of sending messages to other identities,
following identities, or rating a particular piece of content.
We showed that behavioral proﬁles are not optimal for de-
tecting impersonating accounts and that we have to exploit
features that characterize pairs of identities to identify im-
personating accounts.
To assess the trustworthiness of identities, another type
of information typically available on social networks is trust
relationship between identities (e.g., friendship relationship
between identities). Researchers have proposed a variety
of schemes such as SybilGuard [39] and SybilRank [6] that
analyze trust networks between identities to assess the trust-
worthiness of identities and thus identify Sybil attackers [39,
36, 35]. The key assumption is that an attacker cannot es-
tablish an arbitrary number of trust edges with honest or
good users in the network. This assumption might break
when we have to deal with impersonating accounts as for
them it is much easier to link to good users, but it would
be interesting to see whether these techniques are able to
detect doppelg¨anger bots.
A third approach to identify suspicious identities is to
crowdsource this task to experts who are familiar with iden-
tifying suspicious proﬁles or actions. Social networking ser-
vices typically have a tiered approach where suspicious pro-
ﬁles reported by end users are further veriﬁed by a group
of experts before taking a decision to suspend the account
or show Captchas to those suspicious users [6]. In fact, re-
searchers recently explored the possibility of using online
crowdsourcing services such as Amazon Mechanical Turk
(AMT) to crowdsource the task of detecting sybil identities
in a social network [37]. Our AMT experiments showed,
however, that such techniques are not optimal for detecting
impersonating accounts because AMT workers get tricked
easily to believe that impersonating accounts are legitimate.
1516. CONCLUSION
We conducted the ﬁrst study to characterize and detect iden-
tity impersonation attacks in Twitter. The key enabler of
this study is our method to gather data of impersonation at-
tacks. Our method is general and can be used to gather data
in other social networks such as Facebook and LinkedIn.
Besides celebrity impersonators and social engineering at-
tacks, we discovered a new type of impersonation attacks
where attackers copy the proﬁles of legitimate users to create
real-looking fake accounts that are used to illegally promote
content on Twitter. Our analysis revealed that attackers
target a wide range of users and anyone that has a Twitter
account can be victim of such attacks.
Finally, we proposed an automated technique to detect
impersonation attacks, that is able to detect 1,863 more im-
personation attacks in our dataset (up from 166).
Our ﬁndings reveal a new type of privacy threat against
the online image of users. Many surveys [12, 38] state that
U.S. ﬁrms do background checks for job applicants that in-
volve mining data from their online proﬁles. In this scenario,
the doppelg¨anger bot attacks can potentially have a signif-
icant negative impact on the online image of users if the
employer stumbles by mistake across the impersonating ac-
count.
7. REFERENCES
[1] Bing Maps API. http:
//www.microsoft.com/maps/developers/web.aspx.
[2] Get better results with less eﬀort with Mechanical
Turk Masters – The Mechanical Turk blog.
http://bit.ly/112GmQI.
[3] F. Benevenuto, G. Magno, T. Rodrigues, and
V. Almeida. Detecting spammers on Twitter. In
CEAS’10.
[4] P. Bhattacharya, M. B. Zafar, N. Ganguly, S. Ghosh,
and K. P. Gummadi. Inferring user interests in the
twitter social network. In RecSys ’14.
[5] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda. All
your contacts are belong to us: Automated identity
theft attacks on social networks. In WWW’09.
[6] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro.
Aiding the detection of fake accounts in large scale
social online services. In NSDI’12.
[7] W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A
comparison of string distance metrics for
name-matching tasks. In IJCAI’03.
[8] S. Corpus, 2015. http://anoncvs.postgresql.org/
cvsweb.cgi/pgsql/src/backend/snowball/stopwords/.
[9] O. Goga. Matching User Accounts Across Online
Social Networks: Methods and Applications. PhD
thesis, Universit´e Pierre et Marie Curie, 2014.
[10] O. Goga, P. Loiseau, R. Sommer, R. Teixeira, and
K. Gummadi. On the reliability of proﬁle matching
across large online social networks. In KDD, 2015.
[11] B.-Z. He, C.-M. Chen, Y.-P. Su, and H.-M. Sun. A
defence scheme against identity theft attack based on
multiple social networks. Expert Syst. Appl., 2014.
[12] Internetnews. Microsoft survey: Online ’reputation’
counts, 2010. http://www.internetnews.com/
webcontent/article.php/3861241/Microsoft+Survey+
Online+Reputation+Counts.htm.
[13] L. Jin, H. Takabi, and J. B. Joshi. Towards active
detection of identity clone attacks on online social
networks. In CODASPY ’11.
[14] A. M. Kakhki, C. Kliman-Silver, and A. Mislove.
Iolaus: Securing online content rating systems. In
WWW’13.
[15] M. Y. Kharaji, F. S. Rizi, and M. Khayyambashi. A
new approach for ﬁnding cloned proﬁles in online
social networks. International Journal of Network
Security, 2014.
[16] Klout. Klout, 2014. http://klout.com/.
[17] G. Kontaxis, I. Polakis, S. Ioannidis, and E. Markatos.
Detecting social network proﬁle cloning. In
PERCOM’11.
[18] D. G. Lowe. Distinctive image features from
scale-invariant keypoints. Int. J. Comput. Vision,
2004.
[19] Mediabistro. Was twitter right to suspend ’christopher
walken’ ?, 2009.
https://www.mediabistro.com/alltwitter/
was-twitter-right-to-suspend-christopher-walken
b5021.
[20] A. Mislove, A. Post, K. P. Gummadi, and P. Druschel.
Ostra: Leveraging trust to thwart unwanted
communication. In NSDI’08.
[21] M. Mondal, B. Viswanath, A. Clement, P. Druschel,
K. P. Gummadi, A. Mislove, and A. Post. Defending
against large-scale crawls in online social networks. In
CoNEXT’12.
[22] Nairobiwire. Sonko’s facebook impersonator arrested,
2014. http://nairobiwire.com/2014/07/
mike-sonko-arrested-swindling-public.html?utm
source=rss&utm medium=rss&utm campaign=
mike-sonko-arrested-swindling-public.
[23] D. Perito, C. Castelluccia, M. Ali Kˆaafar, and
P. Manils. How unique and traceable are usernames?
In Proceedings of the 11th Privacy Enhancing
Technologies Symposium (PETS), 2011.
[24] Phash. http://www.phash.org.
[25] A. Post, V. Shah, and A. Mislove. Bazaar:
Strengthening user reputations in online marketplaces.
In NSDI’11.
[26] Seattlepi. Racism and twitter impersonation prompt
lawsuit for kirkland teen, 2010.
http://www.seattlepi.com/local/sound/article/Racism-
and-Twitter-impersonation-prompt-lawsuit-
893555.php.
[27] Social Intelligence Corp. http://www.socialintel.com/.
[28] Spokeo. http://www.spokeo.com/.
[29] T. Stein, E. Chen, and K. Mangla. Facebook immune
system. In SNS’11.
[30] Turnto23. Impersonator continuously creating fake
facebook proﬁles of a well known bakersﬁeld pastor.
http://www.turnto23.com/news/local-
news/impersonator-continuously-creating-fake-
facebook-proﬁles-of-a-bakersﬁeld-pastor.
[31] Twitter. Explaining twitter’s eﬀorts to shut down
spam. https:
//blog.twitter.com/2012/shutting-down-spammers,
2012.
152[32] Twitter. Twitter reporting impersonation accounts,
2014. https://support.twitter.com/articles/20170142-
reporting-impersonation-accounts.
[33] B. Viswanath, M. A. Bashir, M. Crovella, S. Guha,
K. Gummadi, B. Krishnamurthy, and A. Mislove.
Towards detecting anomalous user behavior in online
social networks. In USENIX Security’14.
[34] B. Viswanath, M. A. Bashir, M. B. Zafar, L. Espin,
K. P. Gummadi, and A. Mislove. Trulyfollowing:
Discover twitter accounts with suspicious followers.
http://trulyfollowing.app-ns.mpi-sws.org/, April 2012.
Last accessed Sept 6, 2015.
[35] B. Viswanath, M. Mondal, A. Clement, P. Druschel,
K. Gummadi, A. Mislove, and A. Post. Exploring the
design space of social network-based sybil defenses. In
COMSNETS’12.
[36] B. Viswanath, A. Post, K. P. Gummadi, and
A. Mislove. An analysis of social network-based sybil
defenses. In SIGCOMM ’10.
[37] G. Wang, M. Mohanlal, C. Wilson, X. Wang, M. J.
Metzger, H. Zheng, and B. Y. Zhao. Social turing
tests: Crowdsourcing sybil detection. In NDSS’13.
[38] Wikibin. Employers using social networks for
screening applicants, 2008.
http://wikibin.org/articles/employers-using-social-
networks-for-screening-applicants.html.
[39] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman.
Sybilguard: Defending against sybil attacks via social
networks. In SIGCOMM ’06.
[40] C. M. Zhang and V. Paxson. Detecting and analyzing
automated activity on twitter. In PAM’11.
APPENDIX
Here we ﬁrst explain how we computed similarity between
various attribute values (e.g., names and photos) of accounts
and then describe the procedure we used to determine when
two attribute values (e.g., two names or two photos) are
“similar enough” to be deemed to represent the same entity.
A. SIMILARITY METRICS
Name similarity Previous work in the record linkage com-
munity showed that the Jaro string distance is the most
suitable metric to compare similarity between names both
in the oﬄine and online worlds [7, 23]. So we use the Jaro
distance to measure the similarity between user-names and
screen-names.
Photo similarity Estimating photo similarity is tricky as
the same photo can come in diﬀerent formats. To mea-
sure the similarity of two photos while accounting for image
transformations, we use two matching techniques: (i) per-
ceptual hashing, a technique originally invented for identify-
ing illegal copies of copyrighted content that works by reduc-
ing the image to a transformation-resilient “ﬁngerprint” con-
taining its salient characteristics [24] and (ii) SIFT, a size
invariant algorithm that detects local features in an image
and checks if two images are similar by counting the number
of local features that match between two images [18]. We
use two diﬀerent algorithms for robustness. The perceptual
hashing technique does not cope well with some images that
are resized, while the SIFT algorithm does not cope well
with computer generated images.
Location similarity For all proﬁles, we have the textual
representations of the location, like the name of a city. Since
social networks use diﬀerent formats for this information,
a simple textual comparison will be inaccurate.
Instead,
we convert the location to latitude/longitude coordinates
by submitting them to the Bing API [1]. We then compute
the similarity between two locations as the actual geodesic
distance between the corresponding coordinates.
Bio similarity The similarity metric is simply the num-
ber of common words between the bios of two proﬁles after
removing certain frequently used stop words (as is typically
done in text retrieval applications). As the set of stop words,
we use a popular corpus available for several languages [8].
B. SIMILARITY THRESHOLDS
Clearly the more similar two values of an attribute, the
greater the chance that they refer to the same entity, be it a
user-name or photo or location. To determine the threshold
similarity beyond which two attribute values should be con-
sidered as representing the same entity, we rely on human
annotators. Speciﬁcally, we attempt to determine when two
attribute values are similar enough for humans to believe
they represent the same entity.
We gathered human input by asking Amazon Mechanical
Turk (AMT) users to evaluate whether pairs of attribute
values represent the same entity or not. We randomly select
200 pairs of proﬁles and asked AMT users to annotate which
attribute values represent the same entity and which do not.
We followed the standard guidelines for gathering data from
AMT workers [2].
For each attribute, we leverage the AMT experiments to
select the similarity thresholds to declare two values as rep-
resenting the same entity. Speciﬁcally, we select similarity
thresholds, such that more than 90% of values that repre-
sent the same entity (as identiﬁed by AMT workers) and less
than 10% of the values that represent diﬀerent entities (as
identiﬁed by AMT workers) have higher similarities. Conse-
quently, we determine that two user-names or screen-names
represent the same name if they have a similarity higher
than 0.79, and 0.82 respectively. Two locations represent
the same place if they are less than 70km apart. Two photos
represent the same image if their SIFT similarity is higher
than 0.11 and two bios describe the same user if they have
more than 3 words in common.
153