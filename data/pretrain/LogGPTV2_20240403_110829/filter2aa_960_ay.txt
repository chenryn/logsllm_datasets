| store        | transactions | date        |
| store        | transactions | amount      |
| store        | transactions | cvv         |
| store        | transactions | exp         |
--snip--
Although using that query to retrieve schema information is pretty 
straightforward, the complexity in your code comes from logically trying 
to differentiate and categorize each row while defining your GetSchema() 
function. For example, consecutive rows of output may or may not belong 
to the same database or table, so associating the rows to the correct dbminer 
.Database and dbminer.Table instances becomes a somewhat tricky endeavor. 
Listing 7-10 defines the implementation.
type MySQLMiner struct {
    Host string
    Db   sql.DB
}
func New(host string) (*MySQLMiner, error) {
    m := MySQLMiner{Host: host}
    err := m.connect()
    if err != nil {
        return nil, err
    }
    return &m, nil
}
func (m *MySQLMiner) connect() error {
    db, err := sql.Open(
        "mysql",
        u fmt.Sprintf("root:password@tcp(%s:3306)/information_schema", m.Host))
    if err != nil {
        log.Panicln(err)
    }
    m.Db = *db
    return nil
}
func (m *MySQLMiner) GetSchema() (*dbminer.Schema, error) {
    var s = new(dbminer.Schema)
前沿信安资讯阵地  公众号：i nf osrc
168   Chapter 7
    v sql := `SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME FROM columns
    WHERE TABLE_SCHEMA NOT IN 
    ('mysql', 'information_schema', 'performance_schema', 'sys')
    ORDER BY TABLE_SCHEMA, TABLE_NAME`
    schemarows, err := m.Db.Query(sql)
    if err != nil {
        return nil, err
    }
    defer schemarows.Close()
    var prevschema, prevtable string
    var db dbminer.Database
    var table dbminer.Table
    w for schemarows.Next() {
        var currschema, currtable, currcol string
        if err := schemarows.Scan(&currschema, &currtable, &currcol); err != nil {
            return nil, err
        }
        x if currschema != prevschema {
            if prevschema != "" {
                db.Tables = append(db.Tables, table)
                s.Databases = append(s.Databases, db)
            }
            db = dbminer.Database{Name: currschema, Tables: []dbminer.Table{}}
            prevschema = currschema
            prevtable = ""
        }
        y if currtable != prevtable {
            if prevtable != "" {
                db.Tables = append(db.Tables, table)
            }
            table = dbminer.Table{Name: currtable, Columns: []string{}}
            prevtable = currtable
        }
        z table.Columns = append(table.Columns, currcol)
    }
    db.Tables = append(db.Tables, table)
    s.Databases = append(s.Databases, db)
    if err := schemarows.Err(); err != nil {
        return nil, err
    }
    return s, nil
}
func main() {
    mm, err := New(os.Args[1])
    if err != nil {
        panic(err)
    }
    defer mm.Db.Close()
前沿信安资讯阵地  公众号：i nf osrc
Abusing Databases and Filesystems   169
    if err := dbminer.Search(mm); err != nil {
        panic(err)
    }
}
Listing 7-10: Creating a MySQL database miner (/ch-7/db/mysql/main.go/)
A quick glance at the code and you’ll probably realize that much of it is 
very, very similar to the MongoDB example in the preceding section. As a 
matter of fact, the main() function is identical. 
The bootstrapping functions are also similar—you just change the 
logic to interact with MySQL rather than MongoDB. Notice that this logic 
connects to your information_schema database u, so that you can inspect the 
database schema.
Much of the code’s complexity resides within the GetSchema() implemen-
tation. Although you are able to retrieve the schema information by using a 
single database query v, you then have to loop over the results w, inspect-
ing each row so you can determine what databases exist, what tables exist 
in each database, and what columns exist for each table. Unlike in your 
MongoDB implementation, you don’t have the luxury of JSON/BSON with 
attribute tags to marshal and unmarshal data into complex structures; you 
maintain variables to track the information in your current row and com-
pare it with the data from the previous row, in order to determine whether 
you’ve encountered a new database or table. Not the most elegant solution, 
but it gets the job done.
Next, you check whether the database name for your current row 
differs from your previous row x. If so, you create a new miner.Database 
instance. If it isn’t your first iteration of the loop, add the table and data-
base to your miner.Schema instance. You use similar logic to track and add 
miner.Table instances to your current miner.Database y. Lastly, add each of 
the columns to our miner.Table z.
Now, run the program against your Docker MySQL instance to confirm 
that it works properly, as shown here:
$ go run main.go 127.0.0.1
[DB] = store
    [TABLE] = transactions
       [COL] = ccnum
       [COL] = date
       [COL] = amount
       [COL] = cvv
       [COL] = exp
[+] HIT: ccnum
The output should be almost indiscernible from your MongoDB output. 
This is because your dbminer.Schema isn’t producing any output—the dbminer 
.Search() function is. This is the power of using interfaces. You can have 
前沿信安资讯阵地  公众号：i nf osrc
170   Chapter 7
specific implementations of key features, yet still utilize a single, standard 
function to process your data in a predictable, usable manner.
In the next section, you’ll step away from databases and instead focus 
on pillaging filesystems.
Pillaging a Filesystem
In this section, you’ll build a utility that walks a user-supplied filesystem 
path recursively, matching against a list of interesting filenames that you 
would deem useful as part of a post-exploitation exercise. These files may 
contain, among other things, personally identifiable information, user-
names, passwords, system logins, and password database files. 
The utility looks specifically at filenames rather than file contents, and 
the script is made much simpler by the fact that Go contains standard func-
tionality in its path/filepath package that you can use to easily walk a direc-
tory structure. You can see the utility in Listing 7-11.
package main
import (
    "fmt"
    "log"
    "os"
    "path/filepath"
    "regexp"
)
u var regexes = []*regexp.Regexp{
    regexp.MustCompile(`(?i)user`),
    regexp.MustCompile(`(?i)password`),
    regexp.MustCompile(`(?i)kdb`),
    regexp.MustCompile(`(?i)login`),
}
v func walkFn(path string, f os.FileInfo, err error) error {
    for _, r := range regexes {
        w if r.MatchString(path) {
            fmt.Printf("[+] HIT: %s\n", path)
        }   
    }   
    return nil 
}
func main() {
    root := os.Args[1]
    x if err := filepath.Walk(root, walkFn); err != nil {
        log.Panicln(err)
    }   
}
Listing 7-11: Walking and searching a filesystem (/ch-7/filesystem/main.go)
前沿信安资讯阵地  公众号：i nf osrc
Abusing Databases and Filesystems   171
In contrast to your database-mining implementations, the filesystem 
pillaging setup and logic might seem a little too simple. Similar to the way 
you created your database implementations, you define a regex list for iden-
tifying interesting filenames u. To keep the code minimal, we limited the 
list to just a handful of items, but you can expand the list to accommodate 
more practical usage. 
Next, you define a function, named walkFn(), that accepts a file path 
and some additional parameters v. The function loops over your regular 
expression list and checks for matches w, displaying them to stdout. The 
walkFn() function x is used in the main() function, and passed as a param-
eter to filepath.Walk(). The Walk() function expects two parameters—a 
root path and a function (in this case, walkFn())—and recursively walks the 
directory structure starting at the value supplied as the root path, calling 
walkFn() for every directory and file it encounters.
With your utility complete, navigate to your desktop and create the 
following directory structure:
$ tree targetpath/
targetpath/
--- anotherpath
-   --- nothing.txt
-   --- users.csv
--- file1.txt
--- yetanotherpath
    --- nada.txt
    --- passwords.xlsx
2 directories, 5 files
Running your utility against this same targetpath directory produces the 
following output, confirming that your code works splendidly:
$ go run main.go ./somepath
[+] HIT: somepath/anotherpath/users.csv
[+] HIT: somepath/yetanotherpath/passwords.xlsx
That’s just about all there is to it. You can improve the sample code 
through the inclusion of additional or more-specific regular expressions. 
Further, we encourage you to improve the code by applying the regular 
expression check only to filenames, not directories. Another enhancement 
we encourage you to make is to locate and flag specific files with a recent 
modified or access time. This metadata can lead you to more important 
content, including files used as part of critical business processes.
前沿信安资讯阵地  公众号：i nf osrc
172   Chapter 7
Summary
In this chapter, we dove into database interactions and filesystem walking, 
using both Go’s native packages and third-party libraries to inspect data-
base metadata and filenames. For an attacker, these resources often contain 
valuable information, and we created various utilities that allow us to search 
for this juicy information.
In the next chapter, you’ll take a look at practical packet processing. 
Specifically, you’ll learn how to sniff and manipulate network packets.
前沿信安资讯阵地  公众号：i nf osrc
8
R AW  PACK E T PROCE S SING
In this chapter, you’ll learn how to capture 
and process network packets. You can  
use packet processing for many purposes, 
including to capture cleartext authentication 
credentials, alter the application functionality of the 
packets, or spoof and poison traffic. You can also use 
it for SYN scanning and for port scanning through 
SYN-flood protections, among other things.
We’ll introduce you to the excellent gopacket package from Google, 
which will enable you to both decode packets and reassemble the stream 
of traffic. This package allows you to filter traffic by using the Berkeley 
Packet Filter (BPF), also called tcpdump syntax; read and write .pcap files; 
inspect various layers and data; and manipulate packets. 
We’ll walk through several examples to show you how to identify devices,  
filter results, and create a port scanner that can bypass SYN-flood protections.
前沿信安资讯阵地  公众号：i nf osrc
174   Chapter 8
Setting Up Your Environment
Before working through the code in this chapter, you need to set up your 
environment. First, install gopacket by entering the following:
$ go get github.com/google/gopacket
Now, gopacket relies on external libraries and drivers to bypass the oper-
ating system’s protocol stack. If you intend to compile the examples in this 
chapter for use on Linux or macOS, you’ll need to install libpcap-dev. You 
can do this with most package management utilities such as apt, yum, or brew. 
Here’s how you install it by using apt (the installation process looks similar 
for the other two options):
$ sudo apt-get install libpcap-dev
If you intend to compile and run the examples in this chapter on 
Windows, you have a couple of options, based on whether you’re going to 
cross-compile or not. Setting up a development environment is simpler if 
you don’t cross-compile, but in that case, you’ll have to create a Go devel-
opment environment on a Windows machine, which can be unattractive 
if you don’t want to clutter another environment. For the time being, 
we’ll assume you have a working environment that you can use to compile 
Windows binaries. Within this environment, you’ll need to install WinPcap. 
You can download an installer for free from https://www.winpcap.org/.
Identifying Devices by Using the pcap Subpackage
Before you can capture network traffic, you must identify available devices 
on which you can listen. You can do this easily using the gopacket/pcap sub-
package, which retrieves them with the following helper function: pcap.Find 
AllDevs() (ifs []Interface, err error). Listing 8-1 shows how you can use it 
to list all available interfaces. (All the code listings at the root location of / 
exist under the provided github repo https://github.com/blackhat-go/bhg/.)
package main
import (
    "fmt"
    "log"
    "github.com/google/gopacket/pcap"
)
func main() {
    u devices, err := pcap.FindAllDevs()
    if err != nil {
        log.Panicln(err)
    }   
前沿信安资讯阵地  公众号：i nf osrc
Raw Packet Processing   175
    v for _, device := range devices {
        fmt.Println(device.Namew)
        x for _, address := range device.Addresses {
            y fmt.Printf("    IP:      %s\n", address.IP)
            fmt.Printf("    Netmask: %s\n", address.Netmask)
        }   
    }   
}
Listing 8-1: Listing the available network devices (/ch-8 /identify/main.go)
You enumerate your devices by calling pcap.FindAllDevs() u. Then you 
loop through the devices found v. For each device, you access various 
properties, including the device.Name w. You also access their IP addresses 
through the Addresses property, which is a slice of type pcap.InterfaceAddress. 
You loop through these addresses x, displaying the IP address and netmask 
to the screen y. 
Executing your utility produces output similar to Listing 8-2.
$ go run main.go 
enp0s5
    IP:      10.0.1.20
    Netmask: ffffff00
    IP:      fe80::553a:14e7:92d2:114b
    Netmask: ffffffffffffffff0000000000000000
any
lo
    IP:      127.0.0.1
    Netmask: ff000000
    IP:      ::1
    Netmask: ffffffffffffffffffffffffffffffff
Listing 8-2: Output showing the available network interfaces
The output lists the available network interfaces—enp0s5, any, and lo—
as well as their IPv4 and IPv6 addresses and netmasks. The output on your 
system will likely differ from these network details, but it should be similar 
enough that you can make sense of the information.
Live Capturing and Filtering Results
Now that you know how to query available devices, you can use gopacket’s 
features to capture live packets off the wire. In doing so, you’ll also filter the 
set of packets by using BPF syntax. BPF allows you to limit the contents of 
what you capture and display so that you see only relevant traffic. It’s com-
monly used to filter traffic by protocol and port. For example, you could 
create a filter to see all TCP traffic destined for port 80. You can also filter 