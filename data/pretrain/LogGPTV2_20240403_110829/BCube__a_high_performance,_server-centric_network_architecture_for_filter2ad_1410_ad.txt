Figure 8: The aggregate bottleneck throughput of
BCube, fat-tree, and DCell under switch and server
failures.
to replace several mini-switches and use the 10G link to con-
nect to the external network. The servers that connect to
the aggregator become gateways. When an internal server
sends a packet to an external IP address, it will choose one of
the gateways. The external IP to gateway mapping can be
manually conﬁgured or dynamically adjusted based on net-
work condition. The packet is then routed to the gateway
using BSR. After the gateway receives the packet, it strips
the BCube protocol header (which will be explained in Sec-
tion 7.1) and forwards the packet to the external network
via the 10G uplink. The paths from external computers to
internal servers can be constructed similarly.
We can add more aggregators to handle large traﬃc vol-
ume and introduce load-balancing mechanisms to evenly dis-
tribute the traﬃc among the gateways. These are topics for
our future work.
6. GRACEFUL DEGRADATION
In this section, we use simulations to compare the aggre-
gate bottleneck throughput (ABT) of BCube, fat-tree [1],
and DCell [9], under random server and switch failures.
Recall that ABT is the throughput of the bottleneck ﬂow
times the number of total ﬂows in the all-to-all traﬃc model
(Section 3.6). ABT reﬂects the all-to-all network capacity.
We show that only BCube provides high ABT and graceful
degradation among the three structures. Graceful degrada-
tion states that when server or switch failure increases, ABT
0510152005001000150020002500Server failure ratio (%)Aggregate bottleneck throughput (Gb/s)BCubeFat−treeDCell0510152005001000150020002500Switch failure ratio (%)Aggregate bottleneck throughput (Gb/s)BCubeFat−treeDCell70the TCP/IP protocol driver and the Ethernet NDIS (Net-
work Driver Interface Speciﬁcation) driver. The BCube
driver is located at 2.5 layer: to the TCP/IP driver, it is
a NDIS driver; to the real Ethernet driver, it is a protocol
driver. TCP/IP applications therefore are compatible with
BCube since they see only TCP/IP.
The BCube stack has the following key components: the
BSR protocol for routing, the neighbor maintenance proto-
col which maintains a neighbor status table, the packet send-
ing/receiving part which interacts with the TCP/IP stack,
and the packet forwarding engine which relays packets for
other servers.
The BCube packet header is similar to that of DCell [9].
Each packet includes a BCube header between the Ethernet
header and IP header. The header contains typical ﬁelds
including source and destination BCube addresses, packet
id, protocol type, payload length, and header checksum. We
use a 32-bit integer for server address. Similar to DCell, we
also maintain a ﬁxed, one-to-one mapping between an IP
address and a BCube address.
Diﬀerent from DCell, BCube stores the complete path and
a next hop index (NHI) in the header of every BCube packet.
If we directly use the 32-bit addresses, we need many bytes
to store the complete path. For example, we need 32 bytes
when the maximum path length is 8.
In this paper, we
leverage the fact that neighboring servers in BCube diﬀer
in only one digit in their address arrays to reduce the space
needed for an intermediate server, from four bytes to only
one byte. We call this byte NHA. NHA is divided into two
parts: DP and DV. DP indicates which digit of the next
hop is diﬀerent from the current relay server, and DV is the
value of that digit. In our current implementation, DP has
2 bits and DV has 6 bits, the path (NHA array) has 8 bytes,
and the BCube header length is 36 bytes.
7.2 Packet Forwarding Engine
We have designed an eﬃcient packet forwarding engine
which decides the next hop of a packet by only one ta-
ble lookup. The forwarding engine has two components:
a neighbor status table and a packet forwarding procedure.
The neighbor status table is maintained by the neighbor
maintenance protocol. Every entry in the table corresponds
to a neighbor and has three ﬁelds: NeighborMAC, OutPort,
and StatusFlag. NeighborMAC is the MAC address of that
neighbor, which is learnt from neighbor maintenance proto-
col; OutPort indicates the port that connects to the neigh-
bor; and StatusFlag indicates if the neighbor is available.
Every entry of the neighbor status table is indexed by
the neighbor’s NHA value. The number of entries is 256
since NHA is one byte. The number of valid entries is (k +
1)(n − 1), which is the number of neighbors a server has.
One entry needs 8 bytes and the entire table only needs
2KB memory. The table is almost static. For each entry,
OutPort never changes, NeighborMAC changes only when
the neighboring NIC is replaced, and StatusFlag changes
only when the neighbor’s status changes.
When an intermediate server receives a packet, it gets the
next hop NHA from the packet header. It then extracts the
status and the MAC address of the next hop, using the NHA
value as the index. If the next hop is alive, it updates the
MAC addresses, NHI and header checksum of the BCube
header, and forwards the packet to the identiﬁed output
port. The forwarding procedure only needs one table lookup.
We have implemented the forwarding engine in both hard-
ware and software. Our hardware implementation is based
on NetFPGA [21] and part of the implementation details is
given in [10]. Due to the simplicity of the forwarding engine
design, our NetFPGA implementation can forward packets
at the line speed and reduce the CPU forwarding overhead to
zero. We believe that hardware forwarding is more desirable
since it isolates the server system from packet forwarding.
But the the PCI interface of NetFPGA limits the sending
rate to only 160Mb/s. Hence we mainly use the software im-
plementation, which uses server CPU for packet forwarding,
in the rest experiments.
There are other BCube driver components (such as avail-
able bandwidth estimation and BCube broadcast), the BCube
conﬁguration program, and the NetFPGA miniport driver.
The BCube driver contains 16k lines of C code, the NetF-
PGA miniport driver has 9k lines of C code, and the BCube
NetFPGA implementation contains 7k lines of Verilog (with
3.5k lines of Verilog for the forwarding engine).
7.3 Testbed
We have built a BCube testbed using 16 Dell Precision 490
servers and 8 8-port DLink DGS-1008D Gigabit Ethernet
mini-switches. Each server has one Intel 2.0GHz dualcore
CPU, 4GB DRAM, and 160GB disk, and installs one Intel
Pro/1000 PT quad-port Ethernet NIC. The OS we use is
Windows Server 2003.
In our experiments, there is no disk access. This is to
decouple the network performance from that of disk I/O.
We turn oﬀ the xon/xoﬀ Ethernet ﬂow control, since it has
unwanted interaction with TCP [9]. Next, we study the
CPU overhead when using CPU for packet forwarding. After
that, we show BCube’s support for various traﬃc patterns.
7.4 CPU Overhead for Packet Forwarding
In this experiment, we form part of a BCube3 with ﬁve
servers. The servers are 0000, 0001, 0010, 0100, 1000. We
set up four TCP connections 0001 → 0100, 0100 → 0001,
0010→ 1000, 1000→ 0010. All the connections send data as
fast as they can. Server 0000 needs to forward packets for all
the other four servers. We vary the MTU size of the NICs
and measure the forwarding throughput and CPU overhead
at server 0000.
Fig. 9 illustrates the result. The result shows that when
MTU is larger than 1KB, we can achieve 4Gb/s packet for-
warding, and when we increase the MTU size, the CPU us-
age for packet forwarding decreases. When MTU is 1.5KB,
the CPU overhead is 36%. When MTU is 9KB, the CPU
overhead drops to only 7.6%. Our result clearly shows that
per-packet processing dominates the CPU cost.
The experiment demonstrates the eﬃciency of our soft-
ware forwarding. In the rest of our experiments, each server
forwards at most 2Gb/s and MTU is set to be 9KB. Hence
packet forwarding will not be the bottleneck. Again, the
software implementation is to demonstrate that BCube ac-
celerates representative bandwidth-intensive applications. Ide-
ally, packet forwarding needs to be oﬄoaded to hardware, to
reserve server resources (e.g., CPU, memory I/O, PCI/PCI-
E bus I/O) for other computing tasks.
7.5 Bandwidth-intensive Application Support
We use 16 servers to form a BCube1. This BCube1 has 4
BCube0s. Each BCube0 in turn has 4 servers. Our testbed
71Figure 9: The packet forwarding throughput and
CPU overhead with diﬀerent MTU sizes.
uses two ports of each NIC and four ports of each mini-
switch. Fig. 1(b) illustrates the topology. We perform ex-
periments to demonstrate BCube’s support for one-to-one,
one-to-several, one-to-all, and all-to-all traﬃc patterns. In
all our experiments, we set MTU to 9KB. In the one-to-x
experiments, the source server sends 10GB data to the rest
servers. In the all-to-all experiment, each server sends a to-
tal 10GB data to all the other servers. We compare BCube
with a two-level tree. In the tree structure, 16 servers are di-
vided into 4 groups and servers in each group are connected
to a ﬁrst-level mini-switch. The ﬁrst-level switches are con-
nected to a second-level mini-switch. Fig. 10 summarizes the
per-server throughput of the experiments. Compared with
the tree structure, BCube achieves near 2 times speedup for
the one-to-x traﬃc and 3 times higher throughput for the
all-to-all traﬃc.
One-to-one. We set up two TCP connections C1 and
C2 between servers 00 and 13. The two connections use
two parallel paths, P 1 {00, 10, 13} for C1 and P 2 {00, 03,
13} for C2, respectively. The total inter-server throughput
is 1.93Gb/s. The total throughput is 0.99Gb/s in the tree
structure, due to the fact that the two connections need to
share the single network port.
In this experiment, we also study how BSR reacts to fail-
ures. For example, when we shut down server 03, server 00
discovers that server 03 is not available using its neighbor
maintenance protocol (in three seconds). Then server 00
switches C2 to P 1, which is the available path cached for
C2 during the previous probing. At next BSR probing, C2
ﬁnds a new path P 3 {00, 02, 22, 23, 13} and switches to it.
The total throughput becomes 1.9Gb/s again.
One-to-several. In this experiment, we show that the
complete graph can speedup data replication. Server A (00)
replicates 10GB data to two servers B (01) and C (10). In
our complete graph approach, A splits the data into two
parts and sends them to both B and C, respectively. B
and C then exchange their data with each other. We com-
pare our approach with the pipeline approach using the tree
structure. In the pipeline approach, A sends the data to B,
and B sends the data to C. We need 89 seconds using the
pipeline approach and only 47.9 seconds using our complete
graph. This is 1.9 times speedup.
One-to-all. In this experiment, the server 00 distributes
10GB data to all the other 15 servers. We compare two
methods. The ﬁrst is the pipeline approach using the tree
structure in which server i relays data to i + 1 (i ∈ [0, 14]).
The second is our edge-disjoint spanning tree-based approach
Figure 10: Per-server throughput of the bandwidth-
intensive application support experiments under dif-
ferent traﬃc patterns.
which we depict in Fig. 6. We measure the average through-
put. In the pipeline approach, we get approximate 880Mb/s
throughput, whereas we can achieve 1.6Gb/s throughput us-
ing our spanning tree approach.
All-to-all. In the experiment, each server establishes a
TCP connection to all other 15 servers. Each server sends
10GB data and therefore each TCP connection sends 683MB.
This is to emulate the reduce-phase operations in MapRe-
duce. In the reduce phase, each Reduce worker fetches data
from all other workers, resulting in an all-to-all traﬃc pat-
tern. This experiment is similar to that presented in Section
7.3 of [9]. Fig. 11 plots the total throughput of BCube and
the tree structure.
The data transfer completes at times 114 and 332 sec-
onds for BCube and the tree, respectively. The per-server
throughput values of BCube and the tree are 750Mb/s and
260Mb/s, respectively. BCube is about 3 times faster than
the tree. The initial high throughput of the tree is due to
the TCP connections among the servers connecting to the
same mini-switches. After these connections terminate at
around 39 seconds, all the remaining connections have to
go through the root switch. The total throughput decreases
signiﬁcantly. There are no such bottleneck links in BCube.
Compared with the result in DCell, the per-server through-
put of a DCell1 with 20 servers is 360Mb/s (Section 7.3 of
[9]), which is less than 50% of BCube.
We further observe that the traﬃc be evenly distributed
in all the 32 links in BCube. The average throughput per-
link is 626Mb/s, with a standard deviation of 23Mb/s. (The
sending/receving throughput per-server is 750Mb/s and the
forwarding throughput per-server is 500Mb/s, hence the av-
erage throughput per-link is 626Mb/s by spitting the send-
ing/receving and forwarding throughput into two links.) This
shows that BSR does a ﬁne job in balancing traﬃc. We have
counted the number of path probings and path switchings.
On average, every TCP connection probed the network 9.5
times and switched its path 2.1 times, and every probe com-
pleted in 13.3ms. The result shows that BSR is both robust
and eﬃcient.
8. RELATED WORK
Though network interconnections have been studied for
decades [11, 18], to the best of our knowledge, none of the
previous structures meets the MDC requirements and the
physical constraint that servers can only have a small num-
ber of network ports. Switch-oriented structures (where
servers connect to a switching fabric), such as tree, Clos
network, Butterﬂy, and fat-tree [1, 19], cannot support one-
to-x traﬃc well and cannot directly use existing Ethernet
switches. Existing server-centric structures (where servers
directly connect to other servers) either cannot provide high
network capacity (e.g., 2-D and 3-D meshes, Torus, Ring)
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 0 10 20 30 40 50 60 70 80 90 100Forwarding Throughput (Gb/s)CPU Usage (%)MTU (byte)Forwarding ThroughputCPU Usage 0 0.5 1 1.5 2 2.5One-to-oneOne-to-severalOne-to-allAll-to-allThroughput (Gb/s)BCubeTree72Cost(k$)
Power(kw)
wires
switch NIC total
4161
Tree
55
Fat-tree 92
4198
4147
10
DCell
BCube
51
4188
10
10
41
41
switch NIC total No.
4.4
10
1.2
5.8
424
430
431
435
2091
10240
3468
8192
10
10
20
20
Table 2: Cost, power, and wiring comparison of dif-
ferent structures for a container with 2048 servers.