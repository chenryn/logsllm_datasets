公钥认证，具体参考9.2.5节关于配置Linux主机SSH无密码访问的介绍，本节将
不再陈述。
（1）安装
SSH登录Master主机，这里使用root账号进行相关演示。安装JDK环境：
# mkdir –p /usr/java/ && cd /usr/java
# wget http：//uni-smr.ac.ru/archive/dev/java/SDKs/sun/j2se/6/jdk-6u45-
linux-x64.bin
# chmod +x jdk-6u45-linux-x64.bin
# ./jdk-6u45-linux-x64.bin
# vi /etc/profile （配置Java环境变量，追加以下内容）
export JAVA_HOME=/usr/java/jdk1.6.0_45
export PATH=$PATH：$JAVA_HOME/bin
export CLASSPATH=.：$JAVA_HOME/jre/lib：$JAVA_HOME/lib：
$JAVA_HOME/lib/tools.jar
# cd /etc （使环境变量生效）
# . profile
安装Hadoop，版本为1.2.1，安装路径为/usr/local。
# cd /usr/local
# wget http：//mirrors.cnnic.cn/apache/hadoop/common/hadoop-1.2.1/hadoop-
1.2.1.tar.gz
# tar –zxvf hadoop-1.2.1.tar.gz
# cd /usr/local/hadoop-1.2.1/conf
修改目录（/usr/local/hadoop-1.2.1/conf）中的四个Hadoop核心配置文件
hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml，具体内容如下：
·hadoop-env.sh，Hadoop环境变量配置文件，指定JAVA_HOME。
export JAVA_HOME=/usr/java/jdk1.6.0_45
·core-site.xml，Hadoop core的配置项，主要针对Common组件的属性配置。
由于默认的hadoop.tmp.dir的路径为/tmp/hadoop-${user.name}，笔者的Linux系统
的/tmp文件系统的类型是Hadoop不支持的，会报“File/tmp//input/conf/slaves could
only be replicated to 0 nodes，instead of 1”异常，因此手工修改hadoop.tmp.dir指
向/data/tmp/hadoop-${user.name}，作为Hadoop用户的临时存储目录，配置如
下：
hadoop.tmp.dir
/data/tmp/hadoop-${user.name}
fs.default.name
hdfs：//192.168.1.20：9000 //master主机IP：9000端口
·hdfs-site.xml，Hadoop的HDFS组件的配置项，包括Namenode、
Secondarynamenode和Datanode等，配置如下：
dfs.name.dir
/data/hdfs/name //Namenode持久存储名字空间、事务日志路径
dfs.data.dir
/data/hdfs/data //Datanode数据存储路径
dfs.datanode.max.xcievers
4096 //Datanode所允许同时执行的发送和接受任务数量，默认为256
dfs.replication
2 //数据备份的个数，默认为3
·mapred-site.xml，配置map-reduce组件的属性，包括jobtracker和tasktracker，
配置如下：
mapred.job.tracker
192.168.1.20：9001
·masters，配置Secondarynamenode项，环境使用主设备192.168.1.20同时承担
Secondarynamenode的角色，生产环境要求使用独立服务器，起到HDFS文件系统
元数据（metadata）信息的备份作用，当NameNode发生故障后可以快速还原数
据，配置内容如下：
192.168.1.20
·slaves，配置所有Slave主机信息，填写IP地址即可。本示例中Slave的信息如
下：
192.168.1.21
192.168.1.22
接下来，从主节点（Master）复制jdk及Hadoop环境到所有Slave，目标路径
要与Master保持一致，切记！执行以下命令进行复制：
# ssh PI:EMAIL '[ -d /usr/java ] || mkdir -p /usr/java ]'
# ssh PI:EMAIL '[ -d /usr/java ] || mkdir -p /usr/java ]'
# scp -r /usr/java/jdk1.6.0_45 PI:EMAIL：/usr/java
# scp -r /usr/java/jdk1.6.0_45 PI:EMAIL：/usr/java
# scp -r /usr/local/hadoop-1.2.1 PI:EMAIL：/usr/local
# scp -r /usr/local/hadoop-1.2.1 PI:EMAIL：/usr/local
Hadoop部分功能是通过主机名来寻址的，因此需要配置主机名hosts信息
（生产环境建议直接搭建内网DNS服务），保证Hadooop环境所有主机
的/etc/hosts文件配置如下：
192.168.1.20 SN2013-08-020
192.168.1.21 SN2013-08-021
192.168.1.22 SN2013-08-022
管理员通过浏览器查看datanode信息，需要配置本地hosts，如Windows 7系
统hosts文件路径为C:\Windows\System32\drivers\etc，添加所有datanode主机信
息，如下：
192.168.1.21 SN2013-08-021
192.168.1.22 SN2013-08-022
如设备启用了iptables防火墙，需要对主节点（Master）及Slave主机添加以下
规则：
Master：
iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 50030 -j ACCEPT
iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 50070 -j ACCEPT
iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 9000 -j ACCEPT
iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 9001 -j ACCEPT
Slaves：
iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 50075 -j ACCEPT
iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 50060 -j ACCEPT
iptables -I INPUT -s 192.168.1.20 -p tcp --dport 50010 -j ACCEPT
配置完成后在主节点（Master）上格式化文件系统的namenode，执行：
# cd /usr/local/hadoop-1.2.1
# bin/hadoop namenode -format
最后，在主节点（Master）上执行启动命令，如下：
# bin/start-all.sh
（2）检验安装结果
Hadoop官方提供的一个测试MapReduce的示例，执行：
# bin/hadoop jar hadoop-examples-1.2.1.jar pi 10 100
如果返回如图12-1所示结果，则说明配置成功。
图12-1 计算pi的测试结果（部分截图）
访问Hadoop提供的管理页面，Map/Reduce管理地址：http://192.168.1.20：
50030/，如图12-2所示。
图12-2 Map/Reduce管理界面（部分截图）
HDFS存储管理地址：http://192.168.1.20：50070/，如图12-3所示。
图12-3 HDFS管理界面（部分截图）
12.3 使用Python编写MapReduce
Map与Reduce为两个独立函数，为了加快各节点的处理速度，使用并行的计
算方式，map运算的结果再由reduce继续进行合并。例如，要统计图书馆有多少
本书籍，首先一人一排进行统计（map），其次将每个人的统计结果进行汇总
（reduce），最终得出总数。Hadoop除了提供原生态的Java来编写MapReduce任
务，还提供了其他语言操作的API——Hadoop Streaming，它通过使用标准的输
入与输出来实现map与reduce之前传递数据，映射到Python中便是sys.stdin输入数
据、sys.stdout输出数据。其他业务逻辑也直接在Python中编写。
下面实现一个统计文本文件（/home/test/hadoop/input.txt）中所有单词出现的
词频功能，分别使用原生Python与框架方式来编写mapreduce。文本文件内容如
下：
【/home/test/hadoop/input.txt】
foo foo quux labs foo bar quux abc bar see you by test welcome test
abc labs foo me python hadoop ab ac bc bec python
12.3.1 用原生Python编写MapReduce详解
（1）编写Map代码
见下面的mapper.py代码，它会从标准输入（stdin）读取数据，默认以空格
分割单词，然后按行输出单词及其词频到标准输出（stdout），不过整个Map处
理过程并不会统计每个单词出现的总次数，而是直接输出“word 1”，以便作为
Reduce的输入进行统计，要求mapper.py具备可执行权限，执行
chmod+x/home/test/hadoop/mapper.py。
【/home/test/hadoop/mapper.py】
#！/usr/bin/env python
import sys
#输入为标准输入stdin；
for line in sys.stdin：
#删除开头和结尾的空格；
line = line.strip（）
#以默认空格分隔行单词到words列表；
words = line.split（）
for word in words：
#输出所有单词，格式为“单词，1”以便作为Reduce的输入；
print '%s\t%s' % （word， 1）
（2）编写Reduce代码
见下面的reducer.py代码，它会从标准输入（stdin）读取mapper.py的结果，
然后统计每个单词出现的总次数并输出到标准输出（stdout），要求reducer.py同
样具备可执行权限，执行chmod+x/home/test/hadoop/reducer.py。
【/home/test/hadoop/reducer.py】
#！/usr/bin/env python
from operator import itemgetter
import sys
current_word = None
current_count = 0
word = None
# 获取标准输入，即mapper.py的输出；
for line in sys.stdin：
#删除开头和结尾的空格；
line = line.strip（）
# 解析mapper.py输出作为程序的输入，以tab作为分隔符；
word， count = line.split（'\t'， 1）
# 转换count从字符型成整型；
try：
count = int（count）
except ValueError：
# count非数字时，忽略此行；
continue
# 要求mapper.py的输出做排序（sort）操作，以便对连续的word做判断；
if current_word == word：
current_count += count
else：
if current_word：
# 输出当前word统计结果到标准输出
print '%s\t%s' % （current_word， current_count）
current_count = count
current_word = word
# 输出最后一个word统计
if current_word == word：
print '%s\t%s' % （current_word， current_count）
（3）测试代码
我们可以在Hadoop平台运行之前在本地进行测试，校验mapper.py与
reducer.py运行的结果是否正确，测试结果如图12-4所示。
测试reducer.py时需要对mapper.py的输出做排序（sort）操作，当
然，Hadoop环境会自动实现排序，如图12-5所示。
（4）在Hadoop平台运行代码
首先在HDFS上创建文本文件存储目录，本示例中为/user/root/word，运行命
令：
# /usr/local/hadoop-1.2.1/bin/hadoop dfs -mkdir /user/root/word
上传文件至HDFS，本示例中为/home/test/hadoop/input.txt，如果有多个文
件，可采用以下方法进行操作，因为Hadoop分析目标默认针对目录，目录下的
文件都在运算范围中。
# /usr/local/hadoop-1.2.1/bin/hadoop fs –
put /home/test/hadoop/input.txt /user/root/word/
# /usr/local/hadoop-1.2.1/bin/hadoop dfs -ls /user/root/word/
Found 1 items
-rw-r--r-- 2 root supergroup 118 2014-02-10 09：
49 /user/root/word/input.txt
图12-4 mapper执行结果（部分截图）
图12-5 reducer执行结果
下一步便是关键的执行MapReduce任务了，输出结果文件指定/output/word，
执行以下命令：
# /usr/local/hadoop-1.2.1/bin/hadoop jar /usr/local/hadoop-
1.2.1/contrib/streaming/hadoop-streaming-1.2.1.jar -file ./mapper.py -
mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py -
input /user/root/word -output /output/word
图12-6为返回的执行结果，可以看到map及reduce计算的百分比进度。
图12-6 执行MapReduce任务结果
访问http://192.168.1.20：50030/jobtracker.jsp，点击生成的Jobid，查看
mapreduce job信息，如图12-7所示。
图12-7 Web查看mapreduce job信息（部分截图）
查看生成的分析结果文件清单，其中/output/word/part-00000为分析结果文
件，如图12-8所示。
图12-8 任务输出文件清单
最后查看结果数据，图12-9显示了单词个数统计的结果，整个分析过程结
束。
图12-9 查看结果文件part-00000内容
提示 HDFS常用操作命令有：
1）创建目录，示例：bin/hadoop dfs-mkdir/data/root/test。
2）列出目录清单，示例：bin/hadoop dfs-ls/data/root。
3）删除文件或目录，示例：bin/hadoop fs-rmr/data/root/test。
4）上传文件，示例：bin/hadoop fs-put/home/test/hadoop/*.txt/data/root/test。
5）查看文件内容，示例：bin/hadoop dfs-cat/output/word/part-00000。
12.3.2 用Mrjob框架编写MapReduce详解
Mrjob（http://pythonhosted.org/mrjob/index.html）是一个编写MapReduce任务
的开源Python框架，它实际上对Hadoop Streaming的命令行进行了封装，因此接
触不到Hadoop的数据流命令行，使我们可以更轻松、快速编写MapReduce任
务。Mrjob具有如下特点。
1）代码简洁，map及reduce函数通过一个Python文件就可以搞定；
2）支持多步骤的MapReduce任务工作流；
3）支持多种运行方式，包括内嵌方式、本地环境、Hadoop、远程亚马逊；
4）支持亚马逊网络数据分析服务Elastic MapReduce（EMR）；
5）调试方便，无须任何环境支持。
安装Mrjob要求环境为Python 2.5及以上版本，源码下载地址：