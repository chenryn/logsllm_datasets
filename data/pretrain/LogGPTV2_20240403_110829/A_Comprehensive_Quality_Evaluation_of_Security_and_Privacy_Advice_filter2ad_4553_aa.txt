title:A Comprehensive Quality Evaluation of Security and Privacy Advice
on the Web
author:Elissa M. Redmiles and
Noel Warford and
Amritha Jayanti and
Aravind Koneru and
Sean Kross and
Miraida Morales and
Rock Stevens and
Michelle L. Mazurek
A Comprehensive Quality Evaluation of 
Security and Privacy Advice on the Web
Elissa M. Redmiles, Noel Warford, Amritha Jayanti, and Aravind Koneru, 
University of Maryland; Sean Kross, University of California, San Diego; 
Miraida Morales, Rutgers University; Rock Stevens and Michelle L. Mazurek, 
University of Maryland
https://www.usenix.org/conference/usenixsecurity20/presentation/redmiles
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.A Comprehensive Quality Evaluation
of Security and Privacy Advice on the Web
Elissa M. Redmiles1, Noel Warford1, Amritha Jayanti1, Aravind Koneru1,
Sean Kross2, Miraida Morales3, Rock Stevens1, Michelle L. Mazurek1
1University of Maryland
2University of California San Diego
3Rutgers University
Abstract
End users learn defensive security behaviors from a variety of
channels, including a plethora of security advice given in on-
line articles. A great deal of eﬀort is devoted to getting users to
follow this advice. Surprisingly then, little is known about the
quality of this advice: Is it comprehensible? Is it actionable?
Is it eﬀective? To answer these questions, we ﬁrst conduct a
large-scale, user-driven measurement study to identify 374
unique recommended behaviors contained within 1,264 doc-
uments of online security and privacy advice. Second, we
develop and validate measurement approaches for evaluating
the quality – comprehensibility, perceived actionability, and
perceived eﬃcacy – of security advice. Third, we deploy these
measurement approaches to evaluate the 374 unique pieces
of security advice in a user-study with 1,586 users and 41
professional security experts. Our results suggest a crisis of
advice prioritization. The majority of advice is perceived by
the most users to be at least somewhat actionable, and some-
what comprehensible. Yet, both users and experts struggle
to prioritize this advice. For example, experts perceive 89%
of the hundreds of studied behaviors as being eﬀective, and
identify 118 of them as being among the “top 5” things users
should do, leaving end-users on their own to prioritize and
take action to protect themselves.
1 Introduction
It is often considered ideal to remove end users from the
security loop, reducing both their burden and the chance of
potentially harmful errors [12]. However, removing the user
entirely has proven diﬃcult, if not impossible. Users are still
responsible for protecting themselves in a variety of situa-
tions, from choosing and protecting passwords, to recogniz-
ing phishing emails, to applying software updates, and many
more.
Researchers and practitioners have spent signiﬁcant time
and eﬀort encouraging users to adopt protective behaviors.
Examples include redesigning warnings to make them harder
to ignore [7,8,15,61,62], testing scores of alternative authenti-
cation methods intended to reduce user burden [5], “nudging”
users toward better behavior [1, 17], and even using unicorns
to promote secure authentication in encrypted messaging [64].
Despite all this encouragement, user adoption of protective
behaviors remains inconsistent at best [43, 54, 67].
If we wish to improve overall outcomes, it is insuﬃcient
to consider protective behaviors independently from each
other; we must instead consider the cumulative ecosystem of
security-behavior messaging and its eﬀect on users. For ex-
ample, there are limits to how much time and eﬀort users can
spend on protective behaviors [3], and some protective behav-
iors may require more eﬀort than they are worth [23, 47].
Further, recommended behaviors are sometimes conﬂict-
ing [10, 26, 50], change over time (e.g., from changing pass-
words frequently to limiting password changes except in cases
of breach [9, 20], and (as with any topic on which people
provide advice to others) there is likely to be signiﬁcant mis-
information available.
It is critical, therefore, to understand where users get their
security information, and what they are learning. Previously,
researchers have identiﬁed several key sources of security in-
formation and advice: friends and family, ﬁctional media, de-
vice prompts, and of course, the web [18,41,43,46]. However,
the content of this advice has remained largely unexamined.
We make three primary contributions:
1. We create the ﬁrst comprehensive taxonomy of end-
user-focused security and privacy advice. To do so, we
scraped 1,264 documents of security advice from the
web, identiﬁed based on user-generated search queries
from 50 users and via recommendations from vetted ex-
pert. We then manually annotated 2,780 speciﬁc pieces
of advice contained in these 1,264 documents, ulti-
mately identifying 374 unique advice imperatives, 204
of which were documented for the ﬁrst time in this
work [4, 10, 11, 26, 35, 50].
2. We develop measurement approaches for and validate a
novel set of advice quality metrics: perceived actionabil-
USENIX Association
29th USENIX Security Symposium    89
ity, perceived eﬃcacy, and comprehensibility. We show
that these metrics correlate with the ultimate goal of
security advice: end-user adoption of secure behaviors.
3. We conduct a study with 1,586 users and 41 professional
security experts to evaluate the quality of the current
body of security advice: we evaluate all 374 advice im-
peratives along these quality axes, examining the relative
quality of diﬀerent topics (e.g., passwords vs. privacy)
and advice-givers (e.g., the government vs. popular me-
dia), identifying areas needing improvement.
Our results suggest the key challenge is not in the quality
of security advice, but in the volume and prioritization of
that advice. While users ﬁnd the majority of the 374 advice
imperatives they evaluate fairly actionable and somewhat
comprehensible, they struggle to identify which advice is
most important, listing 146 pieces of advice as being among
the top 3 things they should attempt to do. Yet, we know
that they do not adopt anywhere near this many protective
behaviors [43, 56, 67, 68], nor would doing so be practical [3].
We ﬁnd little evidence that experts are any better oﬀ than
end-users on the subject of security advice: experts identify
118 pieces of security advice as being among the top 5 things
they would recommend to a user, consider 89% of the 374
pieces of advice to be useful, and struggle with internal con-
sistency and alignment with the latest guidelines (for example,
claiming that failing to change passwords is harmful, despite
the latest NIST advice to the contrary). Thus, users – whose
priority ratings of advice have little to no correlation with
expert priority ratings – are left to fend for themselves, nav-
igating through a sea of reasonably well-crafted but poorly
organized advice. These ﬁndings suggest that the path forward
for security advice is one of data-driven measurement, mini-
mality and practicality: experts should rigorously measure the
impact of suggested behaviors on users’ risk and ruthlessly
identify only the minimal set of highest impact, most practical
advice to recommend.
2 Related Work
In this section, we review related work on security education
and advice, as well as measurement of text quality.
Security education and advice. Users receive security
advice from a variety of diﬀerent sources, including from
websites, TV, and peers, depending on their level of expertise,
access to resources, and demographics [18, 36, 43, 45, 46].
People also learn from negative experiences — their own and
others’ — through stories about security incidents [41]. The
negative experiences that inform these stories are eﬀective
but carry undesirable emotional and practical costs. Some
researchers have thus explored comic strips and interactive
approaches as eﬀective means of teaching security lessons [13,
29, 34, 53, 57, 71]; others have used visual media to teach
security [2, 19].
Rader and Wash [40] found that the types of security infor-
mation users encounter depends strongly on the source, with
websites seeking to impart information from organizations,
news articles focusing on large breaches or attacks, and inter-
personal stories addressing who is hacking whom and why.
While there are many sources of security information, prior
work has shown that websites are one of the most common
sources of advice speciﬁcally [43]. We therefore aim to char-
acterize advice that is available on the Internet. Rather than
use topic modeling, as in prior work [40], we manually coded
each document we collected in order to deeply understand the
online security advice ecosystem.
In addition to studying where and how people get security
advice, researchers have studied what is in that advice. Ion
et al. [26, 50] found that experts and non-experts consider
diﬀerent practices to be most important; Busse et al. replicated
this work in 2019 and found this was still true [10]. Reeder et
al. [50] additionally report on advice imperatives provided by
security experts. We leverage this work as a starting point for
our taxonomy, while also examining what users might ﬁnd by
directly seeking security advice.
Prioritizing advice is important, because people and orga-
nizations have a limited “compliance budget” with which to
implement security practices [3]. It has been shown that users
make time-beneﬁt tradeoﬀs when choosing a security behav-
ior [47], and may ﬁnd it irrational to follow all, or even most,
security advice [23]. Further, advice can be diﬃcult to retract
once disseminated, creating a continuously increasing burden
for users and organizations [24, 25].
There are many ways to deﬁne and mea-
Text evaluation.
sure text quality. Louis and Nenkova [32], for example, in-
vestigate the quality of science journalism articles using both
general measures, like grammar or spelling correctness, and
domain-speciﬁc measures, like the presence of narrative. Tan
et al. deﬁne quality using linguistic features — like Jaccard
similarity, number of words, and number of ﬁrst person pro-
nouns — of successfully persuasive arguments on Reddit [63].
Perhaps the most common measure of text quality is com-
prehensibility: how easy or diﬃcult it is for people to com-
prehend a document. Prior work has considered the compre-
hensibility of three types of security- and privacy-relevant
text: privacy policies [33, 60], warning messages [21], and
data breaches [72]. These investigations have shown that se-
curity and privacy content is often diﬃcult to read, and that
problems of readability may also be compounded by other
factors such as the display constraints of mobile devices [60].
In this work, we consider a broader class of security-relevant
documents — security advice from the web — and we apply
multiple measures of quality along three axes: comprehen-
sibility, actionability, and accuracy. There are a number of
diﬀerent mechanisms for measuring the comprehensibility of
90    29th USENIX Security Symposium
USENIX Association
adult texts. Redmiles et al. [49] evaluate the validity of these
diﬀerent mechanisms. We leverage their proposed decision
strategy and tools for our measurements (see Section 4.4 for
more detail).
3 Identifying Security Advice
We used two approaches to collect text-based security ad-
vice aimed at end users: (1) We collected search queries for
security advice from 50 crowdworkers and scraped the top
20 articles surfaced by Google for each query, and (2) we
collected a list of authoritative security-advice sources from
computer security experts and librarians and scraped articles
accordingly.
User search query generation. We recruited 50 partici-
pants from Amazon Mechanical Turk (AMT) to write search
queries for security advice. To obtain a broad range of queries,
we used two diﬀerent surveys. The ﬁrst survey asked partici-
pants to list three digital security topics they would be inter-
ested in learning more about, then write ﬁve search queries
for each topic. Participants in the second survey were shown
the title and top two paragraphs of a security-related news
article (See Appendix A), then asked if they were interested
in learning more about digital security topics related to the
article. If the participant answered yes, they were prompted
to provide three associated search queries. Participants who
answered no were asked to read additional articles until they
reported interest; if no interest was reported after six articles,
the survey ended without creating queries. Twenty-ﬁve people
participated in each survey and were compensated $0.25 (ﬁrst
survey, 2.5 min completion time) or $0.50 (second survey,
4 min completion time). Our protocol was approved by the
University of Maryland IRB.
From these surveys, we collected 140 security-advice
search queries. After manual cleaning to remove duplicates
and oﬀ-topic queries, 110 queries remained. Examples of
these queries include, “how safe is my information online?,”
“how to block all windows traﬃc manually?,” and “common
malware.”
We then used the Diﬀbot API1 to scrape and parse the top
twenty Google search results for these queries2. Our collec-
tion was conducted in September 2017.
To identify the types
Expert advice recommendations.
of articles users might be referred to if they asked an au-
thority ﬁgure for advice, we asked 10 people for a list of
websites from which they personally get security advice or
which they would recommend to others. These included ﬁve
people holding or pursuing a Ph.D. in computer security,
two employees of our university’s IT department who have
security-related job responsibilities, and three librarians from
1https://www.diﬀbot.com/
2Diﬀbot uses a variety of approaches to maximize stability of search
results and minimize personalization impact.
our university and local libraries. Two researchers visited
each recommended website and collected URLs for the ref-
erenced advice articles. Manual collection was required, as
many of these expert sites required hovering, clicking images,
and traversing multiple levels of sub-pages to surface relevant
advice. (An initial attempt to use an automated crawl of all
URLs one link deep from each page missed more than 90%
of the provided advice.) As with the search corpus, we then
used the Diﬀbot API to parse and sanitize body elements.
The resulting corpus contained
Initial corpus & cleaning.
1,896 documents. Examples include Apple and Facebook help
pages, news articles from Guardian and the New York Times,
advice or sales material from McAfee, Avast, or Norton, U.S.
CERT pages, FBI articles, and articles from Bruce Schneier’s
blog. To ensure that all of the documents in our corpus ac-
tually pertained to online security and privacy, we recruited
CrowdFlower crowdworkers3 to review all of the documents
and answer the following Yes/No question: “Is this article
primarily about online security, privacy, or safety?” We re-
tained all documents in our corpus for which three of three
workers answered ‘Yes.’ When two of the three initial workers
answered ‘Yes,’ we recruited an additional two workers to
review the document, retaining documents for which four of
the ﬁve workers answered ‘Yes.’ After this cleaning, 1,264 of
the initial 1,896 documents were retained in our corpus.
Extracting & evaluating advice imperatives. Next, we
decomposed these documents into speciﬁc advice imperatives
(e.g., “Use a password manager”). Two members of the re-
search team manually annotated each of the 1,264 documents
in our corpus to extract the advice imperatives contained
within them.
We constructed an initial taxonomy of advice imperatives
based on prior work that had identiﬁed user security behav-
iors [4, 11, 26, 35]. We manually reviewed each of these ar-
ticles, made a list of all described behaviors, and reached
out to the article authors to ask for any additional behaviors
not reported in the papers. The authors of [26] shared their
codebook with us. After merging duplicates, our initial list
contained 196 individual advice imperatives. We used this
taxonomy as a starting point for annotating our security ad-
vice corpus. To ensure validity and consistency of annotation,
two researchers double-annotated 165 (13.1%) of the advice
documents, adding to the taxonomy as needed.We reached a
Krippendorﬀ’s alpha agreement of 0.69 (96.36% agreement)
across the 12 high-level code categories, which is classiﬁed as
substantial agreement [30]. Given this substantial agreement,
and the large time burden of double annotating all 1,264 doc-
uments, the researchers proceeded to independently code the
remaining documents. To evaluate the consistency of our in-
3We use CrowdFlower instead of AMT because the CrowdFlower plat-
form is designed to allow for the validation of work quality for Natural
Language Processing text cleaning processes like this one, and the workers
are more used to such tasks.
USENIX Association
29th USENIX Security Symposium    91
dependent annotations, we compute the intraclass correlation
(ICC), a commonly used statistical metric [59] for assessing
the consistency of measurements such as test results or ratings.
We ﬁnd that both annotators had an ICC above 0.75 (0.823
for annotator 1 and 0.850 for annotator 2), indicating “good”
consistency in their annotations [27].
At the end of the annotation process, the researchers re-
viewed each other’s taxonomies to eliminate redundancies.
Ultimately, our analysis identiﬁed 400 unique advice imper-
atives targeting end users: 204 newly identiﬁed in our work,
170 identiﬁed in prior literature and also found in our corpus,
and 26 from the research literature that did not appear in any
of our documents. The full document corpus, set of advice
imperatives, together with linked evaluation metrics, can be
found here: https://securityadvice.cs.umd.edu.
As part of this process, we also identiﬁed two categories of
irrelevant documents present in our corpus: 229 documents
that were advertisements for security or privacy products and
421 documents (news reports, help pages for speciﬁc software,
etc.) containing no actionable advice. To maintain our focus
on end-user advice, we also discarded imperatives targeting,
e.g., system administrators or corporate IT departments. This
resulted in a ﬁnal corpus of 614 documents containing security
advice.
It is important to note that we use manual annotation to ana-
lyze this data because (a) we cannot use supervised automated
classiﬁcation, as there exists at present no labeled training
data from which to build a classiﬁer (this work establishes
such labeled data) and (b) unsupervised modeling of advice
“topics" and automated tagging of non-standardized open text
with those topics, with a very large number of possible classes
as in our case, remains an open, unsolved problem [31].
Twelve topics of security advice from 476 unique web
domains. Our annotation process identiﬁed 374 security
advice imperatives relevant to end-users. These pieces of ad-
vice occurred 2780 times overall, with an average of 4.53
imperatives per document. We categorized these pieces of
advice into 12 high-level topics, which are summarized in
Table 1. Figure 1 (left) shows the distribution of topics across
the documents in our corpus. We identiﬁed 476 unique web
domains in our corpus; we manually grouped these domains
into broader categories, while retaining certain speciﬁc, high-
frequency domain owners of interest, such as Google and
the Electronic Frontier Foundation (EFF). Hereafter, we use
“domain” to refer to these groupings. Figure 1 (right) shows
the distribution of domains in our corpus.
4 Evaluating Security Advice
After identifying and categorizing the broad set of security
advice being oﬀered to users, we next evaluated its quality.
Speciﬁcally, we measure the perceived actionability and per-
ceived eﬃcacy of the imperatives, as well as the comprehensi-
bility of the documents. Below we describe our measurement
approach, including the novel metrics we developed, the user
study (1,586 users and 41 professional security experts) we
conducted to instantiate these metrics, and our assessment of
the metrics’ validity.
4.1 Measurement Approach
Perceived actionability. We assess perceived actionability
by asking users from the general population to report how
hard they think it would be to put a given imperative into prac-
tice. In particular, our actionability questionnaire incorporates