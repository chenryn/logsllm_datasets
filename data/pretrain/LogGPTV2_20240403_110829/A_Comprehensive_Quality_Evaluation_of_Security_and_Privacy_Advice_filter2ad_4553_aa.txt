# A Comprehensive Quality Evaluation of Security and Privacy Advice on the Web

## Authors
Elissa M. Redmiles, Noel Warford, Amritha Jayanti, Aravind Koneru, University of Maryland; Sean Kross, University of California, San Diego; Miraida Morales, Rutgers University; Rock Stevens, Michelle L. Mazurek, University of Maryland

## Publication
This paper is included in the Proceedings of the 29th USENIX Security Symposium, August 12–14, 2020. ISBN: 978-1-939133-17-5. Open access to the Proceedings is sponsored by USENIX.

## Abstract
End users often learn defensive security behaviors from various sources, including online articles. Despite significant efforts to encourage users to follow this advice, little is known about its quality. This study first conducts a large-scale, user-driven measurement to identify 374 unique recommended behaviors within 1,264 documents of online security and privacy advice. Second, it develops and validates metrics for evaluating the comprehensibility, perceived actionability, and perceived efficacy of security advice. Third, it deploys these metrics in a user study involving 1,586 users and 41 professional security experts. The results suggest a crisis in advice prioritization, with both users and experts struggling to prioritize the numerous pieces of advice.

## 1. Introduction
Ideally, end users would be removed from the security loop to reduce their burden and the risk of errors. However, users still bear responsibility for protecting themselves in various situations, such as choosing and protecting passwords, recognizing phishing emails, and applying software updates. Researchers and practitioners have made significant efforts to encourage users to adopt protective behaviors, but user adoption remains inconsistent.

To improve outcomes, it is insufficient to consider protective behaviors independently; we must examine the cumulative ecosystem of security-behavior messaging and its effect on users. Users have limited time and effort to spend on protective behaviors, and some behaviors may require more effort than they are worth. Additionally, recommended behaviors can be conflicting and change over time, and misinformation is prevalent.

Understanding where users get their security information and what they are learning is critical. Previous research has identified several key sources of security information, including friends and family, fictional media, device prompts, and the web. However, the content of this advice has remained largely unexamined.

We make three primary contributions:
1. We create the first comprehensive taxonomy of end-user-focused security and privacy advice by scraping 1,264 documents of security advice from the web, identifying 374 unique advice imperatives.
2. We develop and validate novel metrics for measuring the quality of security advice: perceived actionability, perceived efficacy, and comprehensibility.
3. We conduct a study with 1,586 users and 41 professional security experts to evaluate the quality of the current body of security advice, examining the relative quality of different topics and advice-givers.

Our results suggest that the key challenge is not the quality of security advice but the volume and prioritization of that advice. While users find most advice actionable and somewhat comprehensible, they struggle to prioritize it. Experts also struggle, identifying many pieces of advice as top priorities and finding little internal consistency. Thus, users are left to navigate through reasonably well-crafted but poorly organized advice.

## 2. Related Work
### Security Education and Advice
Users receive security advice from various sources, including websites, TV, and peers. Negative experiences, both personal and others', are effective but carry undesirable emotional and practical costs. Some researchers have explored comic strips and interactive approaches to teach security lessons, while others have used visual media.

Rader and Wash found that the types of security information users encounter depend strongly on the source. Prior work has shown that websites are one of the most common sources of advice. We aim to characterize the advice available on the Internet by manually coding each document collected.

### Text Evaluation
There are many ways to define and measure text quality. Louis and Nenkova, for example, investigate the quality of science journalism articles using general and domain-specific measures. Tan et al. define quality using linguistic features of persuasive arguments on Reddit. Comprehensibility, or how easy it is for people to understand a document, is a common measure of text quality. Prior work has shown that security and privacy content is often difficult to read, especially on mobile devices.

In this work, we consider a broader class of security-relevant documents—security advice from the web—and apply multiple measures of quality along three axes: comprehensibility, actionability, and accuracy.

## 3. Identifying Security Advice
We used two approaches to collect text-based security advice aimed at end users:
1. **User Search Query Generation**: We recruited 50 participants from Amazon Mechanical Turk (AMT) to write search queries for security advice. We collected 140 security-advice search queries, which were then cleaned and reduced to 110.
2. **Expert Advice Recommendations**: We asked 10 people, including computer security experts and librarians, for a list of websites they personally use or recommend for security advice. We manually collected URLs for the referenced advice articles.

The resulting corpus contained 1,896 documents. To ensure relevance, CrowdFlower crowdworkers reviewed all documents, and 1,264 were retained. We then decomposed these documents into specific advice imperatives, constructing an initial taxonomy based on prior work. Two researchers manually annotated the documents, reaching substantial agreement.

## 4. Evaluating Security Advice
After identifying and categorizing the broad set of security advice, we evaluated its quality by measuring perceived actionability, perceived efficacy, and comprehensibility. Below, we describe our measurement approach, including the novel metrics developed, the user study conducted, and the assessment of the metrics' validity.

### 4.1 Measurement Approach
**Perceived Actionability**: We assess perceived actionability by asking users from the general population to report how hard they think it would be to put a given imperative into practice.