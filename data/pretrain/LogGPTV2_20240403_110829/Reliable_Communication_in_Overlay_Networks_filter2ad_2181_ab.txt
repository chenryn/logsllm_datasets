


TABLE I
AVERAGE LATENCY FOR UNDER LOSS
Protocol
Avg. delay (ms)
Tahoe
407.49
Reno NewReno
217.52
155.76
SACK Fack Vegas Redhat 7.1
144.70
84.66
74.07
90.06
5
Spines
117.55
)
s
m
(
y
a
l
e
d
e
g
a
r
e
v
A
450
400
350
300
250
200
150
100
50
0
0
 End-to-End
 Hop-by-Hop
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Loss rate (%)
)
s
m
(
r
e
t
t
i
J
600
500
400
300
200
100
0
0
 End-to-End
 Hop-by-Hop
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Loss rate (%)
Fig. 3. Average delay for a 500 Kbps stream (simulation)
Fig. 5. Average jitter for a 500 Kbps stream (simulation)
)
s
m
(
y
a
l
e
d
e
g
a
r
e
v
A
600
500
400
300
200
100
0
0
 End-to-End
 Hop-by-Hop
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Loss rate (%)
)
%
(
s
t
e
k
c
a
p
f
o
e
g
a
t
n
e
c
r
e
P
30
25
20
15
10
5
0
60
80
100
120
 End-to-End
 Hop-by-Hop
220
240
260
280
160
140
200
Delay more than (ms)
180
Fig. 4. Average delay for a 1000 Kbps stream (simulation)
Fig. 6. Packet delay distribution for a 500 Kbps stream (simulation)
use a modiﬁed version of TCP-Fack: the initial sender
(at node A) adds its original sequence number in an
additional packet header, intermediate receivers deliver
packets out of order, and the destination delivers packets
FIFO according to the original sequence number avail-
able in the new header. We did not change the congestion
control or the send and acknowledge mechanisms in any
way. We veriﬁed that our modiﬁed TCP and the original
TCP-Fack in ns2 behave identically with respect to each
packet on a point-to-point connection under different loss
rates. All the simulations in this section were run for
5000 seconds, sending 1000 byte messages.
Figure 3 shows that
the average delay for a 500
Kbps data stream increases faster with an end-to-end
connection while a hop-by-hop ﬂow maintains a low av-
erage delay even when it experiences a considerable loss
rate. This phenomenon is magniﬁed as the throughput
required by the ﬂow increases, as depicted by Figure 4
for a 1000 Kbps data stream.
Jitter is an important aspect of network protocols
behavior both due to its impact on other ﬂows at the
network level and due to its impact on the application
served by the ﬂow. Figure 5 shows that the jitter of
an end-to-end connection is considerably higher and
increases faster than the jitter of a hop-by-hop connection
for a 500 Kbps stream. We computed the jitter as the
standard deviation of the packet delay.
It is interesting to see the distribution of the packet
delay for a certain loss rate. In Figure 6, we see that for
a 500 Kbps data stream under 1% loss rate, over 27%
of the packets are delayed more than 60 milliseconds
(including the 50 milliseconds network delay) for an end-
)
%
(
s
t
e
k
c
a
p
d
e
y
a
l
e
D
70
60
50
40
30
20
10
0
1
 2% loss
 1% loss
2
3
4
5
6
7
8
9
10
Number of hops
Fig. 7.
Increasing the number of hops (simulation)
to-end connection, while for a hop-by-hop connection
only about 3% of the packets are delayed more than 60
milliseconds. Similarly, about 18% of the packets are
delayed more than 100 milliseconds by the end-to-end
connection, while for a hop-by-hop connection only 1%
of the packets are delayed as much.
We studied how the performance is affected by the
number of intermediate reliable hops in an overlay net-
work. We consider the same network of 50 milliseconds
delay, and we measure the percentage of packets that
are delayed as we increase the number of intermediate
hops from 1 to 10, while keeping the total path latency
constant. First, we use two hops of 25 milliseconds each,
then three hops of 16.66 milliseconds each, and so forth.
Figure 7 shows the percentage of packets delayed more
than 60 milliseconds (10 milliseconds more than the
path latency) for a 500 Kbps data stream with 1% and
2% packet loss as the number of hops increases. It is
interesting to note that two to four hops appear to be
sufﬁcient to capture almost all of the beneﬁt associated
with hop-by-hop reliability. This is encouraging as small
overlay networks are relatively easy to deploy.
The important factor in obtaining better performance
with hop-by-hop reliability is the latency of the lossy
link rather than the number of hops in the end-to-end
connection. The reason for the phenomenon depicted in
Figure 7 is that increasing the number of hops from one
to two reduces the latency of the lossy link by approx-
imately 50 percent (25 milliseconds in our case), while
increasing the number of hops from nine to ten reduces
the latency of the lossy link only by approximately 1
percent (0.55 milliseconds).
It is important how well we can isolate a potentially
lossy or congested Internet link in an overlay link that is
as short as possible. This can be achieved in practice by
placing a few overlay nodes such that we create close to
equal latency overlay links, as we do not usually know
6
in advance which Internet connections will be congested.
We believe that the simulation results are promising.
The reminder of the paper will investigate whether the
same behavior is not limited to our simulation environ-
ment but is in fact achieved in practice.
IV. THE SPINES OVERLAY NETWORK
In this section we introduce Spines, a software that
allows the deployment of an overlay network in the
Internet. We use Spines to evaluate the hop-by-hop
reliability properties in practice.
Spines instantiates overlay nodes on participating
computers and creates virtual links between these nodes.
Once a message is sent on a Spines overlay network it
will be forwarded on the overlay links until it reaches
the destination. Many Spines overlays can coexist in
the Internet, and even overlap on some of the nodes or
links. Both the source and the destination of a connection
should be part of the same Spines overlay network.
Spines runs a software daemon on each of the overlay
nodes. The daemon acts both as a router, forwarding
packets toward other nodes, and as a server, providing
network services to client applications.
Clients use a library to connect to a daemon through
an API very similar to the Unix Socket interface. A
spines socket() call will return a socket, which is actually
a TCP/IP connection to the daemon. The application
can use that socket to bind, listen, connect, send and
receive, using Spines library calls (e.g. a spines bind()
call is the equivalent to the regular bind(), etc.). The
interface is almost transparent, and virtually any socket-
based application can be easily adapted to work with
Spines. In addition to the TCP-like interface, the Spines
API also provides UDP-like functions for unreliable, best
effort communication.
The Spines daemon communicates with clients
through a Session layer as seen in Figure 8. There
is one session for each client connection, and if the
client requests a reliable connection, the daemon will
instantiate an end-to-end Reliable Session module that
will take care of end-to-end reliability, FIFO ordering,
and end-to-end congestion control.
An overlay link consists of three logical components.
 An Unreliable Data Link sends and receives data
packets with no regard to ordering and reliability.
It is used for unreliable, best effort, fast commu-
nication as it has no buffering other than the ones
provided by the operating system.
 A Reliable Data Link provides link reliability
through a selective repeat protocol and congestion
control, but does not provide FIFO ordering. Pack-
ets are buffered before being sent on a Reliable
7
Daemon-Client Interface
Reliable
Session
Session
API
Library
Routing
Overlay
Node
Data
Forwarder
Link state
Protocol
Hello
Protocol
Reliable
Datagram
Unreliable
Data Link
Reliable
Data Link
Control Link
Datalink (UDP/IP unicast)
Fig. 8. Spines daemon architecture
k
n
i
L
y
a
l
r
e
v
O
Data Link only in case the congestion control or
available link capacity limit the outgoing bandwidth
to a lower value than the incoming throughput.
The explicit congestion notiﬁcation mentioned in
Section II is based on the size of these buffers.
The link congestion control allows the deployment
of Spines in the Internet, providing fairness with
external TCP trafﬁc. Figure 9 shows the throughput
obtained by an end-to-end TCP stream and by the
Spines link protocol for a 10 and a 50 millisecond
delay link of 10 Mbps capacity under different lev-
els of losses, and compares it to the analytical TCP
model from [6]. The throughput achieved by Spines
is very close to that of a TCP connection under
similar conditions. Note that for a 10 millisecond
link, as the throughput of both TCP and Spines
approaches the maximum capacity of 10Mbps, they
start developing their own additional losses in order
to probe the available bandwidth. This is why they
appear to achieve less than the analytical model
that takes into account only the original losses we
enforced on the link.
 A Control Link is used for sending and receiving
control information between two neighbor daemons.
It provides both reliable and unreliable communi-
cation. In case of buffering for the reliable data,
the unreliable packets will bypass the buffer and go
directly on the network.
The overlay node is responsible for maintaining con-
nections to its neighbors and forwarding data packets
either on the overlay links or to its own clients. A Data
Forwarder parses the header of each message and passes
to on the next link or daemon-client interface. The Data
Forwarder allows any combination of reliable and unre-
liable session and reliable and unreliable link in order to
experiment with different forwarding mechanisms. The
type of Session and Data Link requested are stamped in
the header of each message. For example, one can create
a reliable end-to-end session using either unreliable links
or reliable links.
Neighboring overlay nodes ping each other period-
ically using unreliable hello packets. The Spines Hello
Protocol is responsible for creating, destroying and mon-
itoring overlay links between neighbor daemons. Each
Spines daemon sends information about its links reliably
to all the other daemons in the overlay network through
a link state protocol, only when the state of its links
change, or periodically at large intervals for garbage
collection. The link state protocol provides a complete
information about the existing overlay links, out of which
a Routing module chooses the neighbor providing the
shortest path to each destination.
In addition to the IP and UDP headers, Spines adds its
own headers for routing and reliability. Also, for reliable
connections Spines sends acknowledgments for every
packet at the level of each link for hop reliability and
)
s
p
b
K
(
t
u
p
h
g
u
o
r
h
T
e
g
a
r
e
v
A
10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
0
0
 TCP 10ms
 Spines 10ms
 Analytic 10ms
 TCP 50ms
 Spines 50ms
 Analytic 50ms
0.2
0.4