the non-linear color transformation of digital cameras [15]. Hence,
they will depend on the actual color composition of the displayed
video frame. Additionally, the intensity of light in the room inﬂu-
ences the amount of incoming light and could be treated as another
signal, but for simplicity, we consider it an additive constant as long
as the lights are not being repeatedly turned on and off. As such,
we omit its embedded signal in Equation 1, but instead simply con-
sider it as a source of “impulse noise” [3], similar to the lights of a
passing vehicle.
To complicate matters even further, there can be noise from a
myriad of other sources that impact the measured brightness value
in the adversary’s recording of the emanations coming from the
display. These include quantization noise of the camera during the
A/D conversion to obtain pixel values [16], thermal noise from the
sensor itself [8] and impulse noise. Again, for simplicity, we ac-
cumulate the above noise factors into a single noise variable Inoise.
The SNR can then be computed as:
σ (Icap)2
σ (Inoise)2
where σ 2(·) is the variance of the signal.
SNR =
(2)
Intuitively, lower screen brightness levels, smaller and darker re-
ﬂecting objects, and longer distances limit the amount screen light
captured by the adversary. Fortunately for the adversary, a high
quality camera can capture a good percentage of the incoming light
and reduce quantization and electronic noise. Finally, note that
while the intensity of a constant room light does not inﬂuence the
SNR directly—since it does not inﬂuence the noise variance—it in-
directly effects the quantization noise given that it affects the sen-
sitivity of camera’s sensor (i.e., higher room light intensity makes
the camera less able to capture subtle illumination changes).
For our experimental evaluations we can directly acquire Icap +
Inoise from the captured video. An estimate of the noise variance
σ (Inoise)2 can be measured by having the adversary capture the re-
ﬂection from a static image displayed on screen beforehand (e.g., at
her house). Similarly, room brightness can be approximated. With
these measurements at hand, Icap can be estimated with linear re-
gression using Ire f , and the SNR can be directly computed.
Takeaway. The factors that inﬂuence the signal we are interested
in can be approximated by Equation 1. Moreover, by using Equa-
tion 2, we can infer the SNR directly from the captured data, which
ranged from 5 to 107 in our empirical evaluations.
Point of Capture
Obviously, the point at which the recording of the light diffusions
is taken can inﬂuence how well the attacker can conﬁrm her hy-
potheses. Intuitively, the more she is able to record sudden inten-
sity changes, the higher the chances are that the correct content will
be inferred. A key challenge for the adversary is that the average
intensity of one frame is highly dependent on that of the previous
frame. For instance, in our empirical evaluations, nearly 95% of
consecutive frames have the same average intensity (up to round-
ing error precision).
To improve our ability to carry out the attack, we do not use
the raw data directly, but instead, use its gradient to reduce the
correlation. To see why that helps, assume that xt = Ire f (t + 1)−
Ire f (t),yt = Icap(t + 1)− Icap(t),t = 1,2,3, .... Then, given that the
vast majority (i.e., 95%) of the average intensities are similar, this
420means that 95% of the xs would be 0. Assuming the gradients are
independent of one another, the information with a particular frame
sequence {xt ,t = 0,1,2,3, ...} can be measured as:
In f ore f (x) = −ΣN
t=0log( f (xt )∆x)
(3)
where f (xt ) is the probability density function (PDF) of xt in a
In f ore f (x) can be viewed as the logarithm of the
single frame.
inverse probability of the reference sequence. The higher its value,
the less likely another reference sequence will "accidentally" be
the same as it, which means that the sequence has less ambiguity
and contains more information. Consequently, the more intensity
changes the adversary observes, the more likely it is that the correct
content will be inferred.
To gauge how well the attack should work, we can compute the
mutual information between x and y using Equation 4. In f omutual(x,y)
estimates the information captured by the adversary on average.
(cid:90)
In f omutual(x,y) =
p(x)p(y|x)log(
p(y|x)
p(y)
)
(4)
In practice, p(x) can be observed directly from a reference video.
Likewise, p(y) can be computed by ignoring impact noises (which
are rare) and assuming that the noise follows a Gaussian distribu-
tion. In fact, since Ire f and Icap are linearly related, we can also
assume y = x + noise, where Var{noise} = Var{x}/SNR. In doing
so, we can now compute the mutual information with the SNR we
acquired. For context, we note that in our evaluations that follow at
an SNR of 5 every frame conveyed roughly 1.5 bits of information.
Under much better conditions with SNR of 107 (observed when the
diffusions were captured while the victim watched an action scene
on a 50-inch TV) every frame conveyed 3.2 bits of information.
Takeaway. The above analysis tells us what one would expect:
the more intensity changes observed, the less the resulting ambigu-
ity. Therefore, if the adversary is lucky enough to observe several
sharp changes in intensity, she will have an easier time to identify
the content being watched by the victim. Not surprisingly, Equation
4 also tells us that bigger and brighter screens provide more than
twice as much information (compared to the smaller and darker
ones used in our experiments).
Length of Recording
Given the previous discussions, longer recordings are obviously
better for the adversary. To see that, assume that the arrival of in-
tensity changes are Markov, meaning that the distribution of arrival
time and magnitude of the next intensity change only depends on
the current state of the video being watched. If that is the case, then
the information learned by the adversary is linearly related to the
mutual information per frame. Ideally, the attacker’s best hope is
for a high SNR environment, a good starting point, and a suitable
recording length capturing multiple changes in intensity.
Size of the Reference Library
The last factor that affects the speed and accuracy of the attack
is the size of the reference collection the adversary must test her
hypotheses against. In the worst case, the amount of information
we need to uniquely identify a video is logarithmic with its total
length, which in turn, is linearly related to the size of the attacker’s
library. Therefore, linearly increasing the size of the library will
only have marginal inﬂuence on her ability to successfully conﬁrm
which content the victim is watching.
5. AUTOMATED VIDEO RETRIEVAL
Our approach (as shown in Figure 1) consists of two main parts
that comprise a feature extraction step from the captured recording
and a video retrieval step using a precomputed library of features
from reference content (i.e., the set of videos for which the adver-
sary wishes to conﬁrm her hypotheses against). The feature ex-
traction stage converts the captured video into a representative en-
coding that encodes the changes in brightness. This feature is then
compared during the video retrieval to the features in the database
to identify the content on the victim’s display.
5.1
Feature Extraction
Intuitively, in our feature representation we want to preserve the
brightness changes of the displayed image. Hence, for each frame t
of the video, with M frames, we calculate the average intensity s(t)
with t = 0, . . . ,M, by averaging all the brightness values of all pixels
in the image. The sequence s of these average brightness values
s(t) with t = 0, . . . ,M, provides us with a coarse characterization of
the captured video’s brightness. An example brightness sequence
for a video sc captured through the window and the corresponding
original video sd is provided in Figure 2. Notice that while the
variation of the two signals is comparable, the magnitude of the
brightness signals s(t) is signiﬁcantly different.
Figure 2: The intensity signal (top) and respective features (middle and bot-
tom). For illustrative purposes, the sequences are manually aligned. Noises
occur as peaks or masked out peaks in the feature sequence
To achieve comparability of the two signals sc and sd we charac-
terize them by their frame-wise intensity gradient over time ds(t).
Given the average intensity signals sc and sr respectively, the tem-
poral gradient ds(t) can be calculated as ds(t) = s(t + 1)− s(t).
Based on our conjecture that the brightness changes uniquely
characterize a video, we convert the temporal gradient ds into a
feature f by only preserving its signiﬁcant (|ds(t)| > 1) local max-
ima and minima
421ds(t),
0,else
f (t) =
i f |ds(t)| > 1∧|ds(t)| > |ds(t − 1)|
∧|ds(t)| > |ds(t + 1)|
(5)
If the video sequence’s brightness does not have scene changes,
ﬂashes or other sudden changes, the average intensity is nearly con-
stant, leading to zero values in f (t). By contrast, if there is a sud-
den intensity change (e.g., a drastic scene change or ﬂashes of gun
shots) f (t) will capture a "peak" which is either positive or nega-
tive, representing a sudden increase or decrease in average inten-
sity. Accordingly, f (t) can be viewed as a composition of peaks.
For a captured video, some of the peaks might correspond to noise
or noise might mask some peaks in f . Additionally, the magnitude
of the peaks might be still scaled by an unknown factor. Example
features fc and fd for a captured video and the retrieved database
video are shown in Figure 2 (middle, bottom).
5.2 Creating the Reference Library
Our video retrieval requires a database of reference videos to
retrieve the corresponding video being watched. This database is
typically obtained ahead of time by obtaining all content of interest.
If only the content for live TV is of interest to the adversary, she can
just record all the currently running TV channels. If the adversary is
interested in online videos, a database of popular videos (e.g., from
YouTube, Netﬂix, or her home collection) would be helpful. Once
all videos of interest are obtained, they are converted to feature
vectors using the same feature extraction technique used for the
captured sequences (see Section 5.1).
5.3 Locating the Best Matching Sequences
To identify the best match we use a nearest neighbor search
across subsequences because the captured sequence typically only
covers a small part of the overall content being watched on the dis-
play. For ease of exposition, we ﬁrst introduce our similarity metric
for the case that both the captured length length( fc) and the refer-
ence video length length( f ) are the same and start at the same time.
Later, we generalize the metric to account for different lengths and
starting points of the captured and the reference videos.
Intuitively, to measure the similarity of the feature vectors for the
captured video fc and a reference video fi ∈ { fre f}, we can exam-
ine how many extrema match between the features. The amount of
disturbance caused by erroneous noise peaks is represented by
Enoise( fi, fc) =
ΣL
t=1 fc(t)21( fi(t) == 0)
ΣL
t=1 fc(t)2
(6)
where L is the length of the videos and 1(x) is the indicator func-
tion, which is one if x is true and zero otherwise. Similarly
Emiss( fi, fc) =
ΣL
t=1 fi(t)21( fc(t) == 0)
ΣL
t=1 fi(t)2
(7)
measures the energy of missing peaks in the reference sequence.
Note that while Enoise and Emiss characterize the magnitude of dif-
ference in the number of peaks, we must also measure the amount
of difference in energy of the peaks by characterizing how similar
the extrema themselves are. This can be measured as the correla-
tion Corr( fi, fc)
ΣL
t=1 fc(t) fi(t)
t=1 fc(t)2)(ΣL
(ΣL
t=1 fi(t)2)
(8)
Corr( fi, fc) =
(cid:113)
between the two sequences, which has a value between -1 and 1. In
this paper, we use a similarity metric d that combines Enoise, Emiss
and Corr( fi, fc) into a single metric:
d( fc, fi) =α (Enoise( fi, fc) + Emiss( fi, fc))
+ (1−Corr( fi, fc))
(9)
with α representing the weighting between the energy of the miss-
ing or noise peaks and the correlation between the correct extrema;
the latter is necessary when distinguishing features in the case of
perfectly agreeing peaks. Given that the magnitudes of the peaks
may be different between the captured and reference signals, we
rely on the temporal information which is more accurate. As such,
we empirically chose α = 50 for all our experiments so that the
temporal agreement of peaks dominates the metric. It is only when
the temporal position of peaks matches perfectly that Corr( fi, fc)
is used to evaluate their similarity based on magnitude.
Returning to §2, it is important to remind the reader that our met-
ric is based on the gradient of average intensity. Therefore, it cap-
tures sharp intensity changes and ignores smooth terms such as am-
bient light condition, the auto exposure of camera and other gradual
changes. Even impulse noise (e.g. turning on/off the room light)
only result in a single extra peak in the feature vector and thus has
minor impact on the overall result. Other alternatives such as using
the correlation directly (e.g., as proposed by Greveler et al. [7]) fail
in our scenario since these approaches are signiﬁcantly impacted
by signal magnitudes which are often heavily distorted. Likewise,
the FFT transformation used in sequence matching schemes [5, 12]
also fails because the peaks are too sparse for frequency analysis
and the localized changes are too subtle to be useful.
In our evaluations that follow, the reference video that best matches
under our similarity metric d is reported as the likely content be-
ing watched. We note that in practice the temporal position of the
extrema may vary by one frame due to encoding and sampling of
the original video sequence. Therefore, when determining whether
fi(t) or fc(t) is non-zero, we consider the adjacent two frames
(t − 1,t + 1) in addition to the frame at time t by using the mod-
iﬁed indicator function ˜1(x) in Equations (6)-(9), which is one if
none of x or its temporal neighbors is true.
Notice that thus far, the retrieval using the similarity metric d
from Equation (9) assumes equal length and starting point of the
videos. To relax this assumption, for a recording of length lc =
length( fc) we search all subsequences of length lc among all database
sequences of length greater than or equal to lc. This has the added
beneﬁt that we not only identify what content was watched, but
also what part of the video was watched at the time the recording
was taken. The problem, however, is this type of exhaustive search
comes with a signiﬁcant computational burden. In what follows,