title:Flowroute: inferring forwarding table updates using passive flow-level
measurements
author:Amogh Dhamdhere and
Lee Breslau and
Nick G. Duffield and
Cheng Ee and
Alexandre Gerber and
Carsten Lund and
Subhabrata Sen
FlowRoute: Inferring Forwarding Table Updates Using
Passive Flow-level Measurements
Amogh Dhamdhere
CAIDA
University of California, San Diego
La Jolla, CA
PI:EMAIL
ABSTRACT
The reconvergence of routing protocols in response to changes
in network topology can impact application performance.
While improvements in protocol speciﬁcation and imple-
mentation have signiﬁcantly reduced reconvergence times,
increasingly performance-sensitive applications continue to
raise the bar for these protocols. As such, monitoring the
performance of routing protocols remains a critical activity
for network operators. We design FlowRoute, a tool based
on passive data plane measurements that we use in conjunc-
tion with control plane monitors for oﬄine debugging and
analysis of forwarding table dynamics. We discuss practical
constraints that aﬀect FlowRoute, and show how they can
be addressed in real deployment scenarios. As an applica-
tion of FlowRoute, we study forwarding table updates by
backbone routers at a tier-1 ISP. We detect interesting be-
havior such as delayed forwarding table updates and routing
loops due to buggy routers – conﬁrmed by network opera-
tors – that are not detectable using traditional control plane
monitors.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations, Network Management
General Terms
Design, Measurement, Performance
Keywords
Measurement, Netﬂow, Routing Update
1.
INTRODUCTION
The behavior of networks during and after routing changes
– when packets may be subject to looping, loss, and delay
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
Lee Breslau, Nick Dufﬁeld, Cheng Ee,
Alexandre Gerber, Carsten Lund,
Subhabrata Sen
AT&T Labs-Research
Florham Park, NJ
{breslau, dufﬁeld, ctee, gerber, lund,
sen}@research.att.com
variation – is an important determinant of the performance
that end users perceive. Historically, inter-domain routing
convergence could take minutes [12], while intra-domain pro-
tocols (e.g., OSPF) could take tens of seconds. Subsequent
improvements in protocol speciﬁcation and implementation
reduced the reconvergence times of these protocols. How-
ever, the recent rise of applications with stringent perfor-
mance requirements means that routing reconvergence is
still a subject of intense interest among service providers.
To study routing behavior, network operators often use
specialized control plane monitors [19–21] that peer with and
receive routing updates from one or more routers. Control
plane monitors can indicate when a router received a routing
update, i.e., when the router should have updated its forward-
ing table, and what it should have updated to. They cannot
detect, however, when the router actually implemented the
change in its forwarding table, and what change it made.
The actual change may be delayed, or may diﬀer from what
is expected due to hardware or protocol implementation
bugs in routers. Further, monitoring the control plane at
one or a few routers may not indicate how close in time dif-
ferent routers update their forwarding tables. If these times
diverge, we could see transient forwarding loops and poor
end-to-end performance. One could study forwarding table
updates using logs collected directly from routers. In today’s
networks, however, we cannot instrument routers to dump
their forwarding tables after every update. Even if this were
feasible, it would require transporting and processing giga-
bytes of data after every update. SNMP polling would also
have to deal with the same data volume, and under current
operational practices, would give us a 5-minute resolution
for inferring forwarding table changes – too coarse for our
needs. One could use active probing (e.g., traceroutes) to
monitor forwarding table updates. Active probing faces two
drawbacks, however; provisioning monitors to achieve high
spatial coverage can be costly, and the temporal resolution
is limited by probing rates, which we cannot increase indef-
initely without causing excessive overhead.
We describe the design and implementation of FlowRoute,
a tool that works in conjunction with existing control plane
monitors to analyze forwarding table dynamics. FlowRoute
can measure forwarding table update times across routers
and help operators identify slow forwarding table updates,
transient forwarding loops, and large traﬃc shifts. FlowRoute
works oﬄine, using passive ﬂow-level measurements (e.g.,
Cisco’s Netﬂow [4]) that operators routinely collect in to-
315day’s networks; as such, it does not impose additional over-
head on routers. FlowRoute is agnostic to the speciﬁc rout-
ing protocols in use and can be applied to both intra-domain
and inter-domain routing.
As an application of FlowRoute, we study forwarding ta-
ble updates at backbone routers in a large tier-1 ISP. We ﬁnd
unexpected cases wherein forwarding table updates at a sin-
gle router are delayed following a routing event. Delayed
updates, in turn, cause forwarding loops lasting tens of sec-
onds in some cases. Using FlowRoute, we attributed these
eﬀects to speciﬁc routers that were often late in updating
their forwarding tables. Network operators conﬁrmed that
those routers did indeed have protocol implementation bugs
causing performance issues.1 Such unexpected behavior is
not detectable using traditional control plane monitors.
We believe that operators can apply FlowRoute to a broad
set of other debugging/analysis problems. For example, by
measuring forwarding behavior across routers, FlowRoute
can help network operators examine the extent to which
forwarding table updates are synchronized across routers.
FlowRoute can also quantify the network-wide eﬀects of
routing changes, such as traﬃc shifts and changes in link
utilization after a routing change.
2. RELATED WORK
The OSPFmon [21] deployment at AT&T, the IP moni-
toring project at Sprint [10] and commercial products such
as Route Explorer [16] provide route monitoring services to
ISPs. However, none of these systems can study forward-
ing table performance at the timescales that FlowRoute is
capable of. Feldmann et al. [9] describe an approach to peri-
odically dump router conﬁguration ﬁles in order to identify
conﬁguration errors. Their approach provides a static view
of the routing state at each router. Such an approach would
face signiﬁcant scalability problems if extended to studying
routing table dynamics.
In theory, one could use traceroute-like active measure-
ment tools [3] to infer forwarding table changes. Active
measurement provides a view of the routing state at routers
on a path at the time the probes are sent. To achieve a
suﬃciently high temporal resolution for studying forward-
ing table updates, we would need a high probing rate and a
large number of vantage points. Previous studies have used
a combination of active probing, route monitoring and pas-
sive measurements to quantify the eﬀects of routing events
on end-to-end loss rates [23] and delays [18]. FlowRoute al-
lows us to study forwarding table dynamics, which we can
use to investigate the eﬀects of routing events on end-to-end
performance.
Operators use ﬂow-level measurements from ISP networks
for a variety of applications such as estimating intradomain
traﬃc matrices [13] and ﬂow size distributions [8]. Teix-
eira et al. [22] use NetFlow and routing data from a large
Tier-1 ISP to quantify the eﬀects of routing changes on the
intradomain traﬃc matrix. Agarwal et al. use passive data
to measure the eﬀect of BGP route changes on the ingress-
egress traﬃc matrix [1], and to study how traﬃc to neigh-
boring ASes shifts due to changes in a local AS’ IGP link
metrics [2]. To the best of our knowledge, no previous work
used ﬂow-level data to study forwarding table dynamics.
1These routers were subsequently retired from the network.
Figure 1: Basic model for routing change detection
3. FLOWROUTE DESIGN
We begin by describing, at a high level, the approach and
algorithms embodied in FlowRoute. Figure 1 shows a single
packet ﬂow f1 towards destination D. Suppose that router
R forwards this ﬂow towards next hop router N1 at time t1.
The ﬂow record at N1 indicates that the previous hop was
R, or, equivalently, that N1 was R’s next hop towards D
at time t1. If the next hop changes to N2 at time t2 > t1,
then a ﬂow f2 traversing R and destined to D will gener-
ate a corresponding ﬂow record at N2. By using these two
ﬂow records, we can infer when R started routing ﬂows to-
wards N2. We next describe how we process the raw ﬂow
records from routers and infer routing changes. Note that
FlowRoute processes ﬂow records oﬄine, on infrastructure
that operators already use for network management, and
does not impose additional overhead on routers.
3.1 Flow Records
We use ﬂow records generated by Netﬂow [4], which is
supported by multiple router vendors. Netﬂow records sum-
marize ﬂows, i.e., sets of packets which share common header
ﬁelds (e.g., source and destination IP addresses). To infer
routing changes, FlowRoute uses the following information
that is present by default in Netﬂow records: the router R
that observed the ﬂow; the incoming and outgoing interfaces
i and o at R; the times tf and tl of the ﬁrst and last pack-
ets of the ﬂow; and the destination D. We denote this ﬂow
record by the tuple (R, i, o, tf , tl, D).
While ﬂow records report both incoming and outgoing in-
terfaces, there is an important semantic diﬀerence between
the two. The incoming interface is part of the ﬂow key which
deﬁnes a ﬂow. A change in the incoming interface leads to
the creation of a new ﬂow record. Consequently, we know
that each packet of a ﬂow arrived on the input interface in-
dicated by the ﬂow tuple. In contrast, the outgoing interface
o is not part of the ﬂow key. Therefore, a change in the out-
going interface (next hop) while the ﬂow is active will not
generate a new ﬂow record. Rather, the timestamp tf in
the ﬂow record indicates the time when the ﬂow record was
created. Thus, we can only infer reliably that the reported
outgoing interface was used for the ﬁrst packet of the ﬂow.
3.2 Inferring Routing Changes
We collect Netﬂow records and process them oﬄine to ob-
tain a stream of Routing Flow Records (RFRs). Each
RFR is of the form (R1, t1, t2, D, R2), which states that
during the interval [t1, t2], router R1 is forwarding pack-
ets to destination D via next hop R2. In Figure 2, we de-
scribe how we construct two RFRs from each ﬂow record
316Netﬂow: (R,i,o,tf,tl,D)
Rp
RF R1
i
o
R
RF R2
(Rp,tf − δ,tl − δ,D,R)
(R,tf ,tf,D,Rn)
Rn
Figure 2: Construction of two Routing Flow Records
from a ﬂow record
(R, i, o, tf , tl, D) at router R. First, using the incoming in-
terface i, we generate RF R1=(Rp, tf − δ, tl − δ, D, R), which
indicates that during the interval [tf − δ, tl − δ], router Rp
used R as the next hop to reach destination D.2 To ob-
tain timestamps at Rp from the packet observation times
tf and tl at R, we subtract the propagation delay δ of the
link between Rp and R. We ignore queuing delays, as they
cannot be estimated using topology and conﬁguration infor-
mation alone. Moreover, queuing delays are typically neg-
ligible at backbone routers [17]. We also ignore clock skew
across routers, as routers use NTP [14] for clock synchro-
nization. Next, using the outgoing interface o, we generate
RF R2=(R, tf , tf , D, Rn). Due to the semantic diﬀerence be-
tween incoming and outgoing interfaces, the two timestamps
in RF R2 are the same.
Consider again the routers R, N1 and N2 in Figure 1.
Suppose we have RFRs (R,t1,t2,D,N1) and (R,t3,t4,D,N2)
with t1 < t4. The ﬁrst RFR indicates that R used N1 as
the next hop to D during the interval [t1, t2], and the second
RFR indicates that R used next hop N2 during [t3, t4]. Two
scenarios are possible: if t3 < t2, this may mean that Equal-
Cost Multi-Path (ECMP) routing [15] is sending similarly-
destined ﬂows over two diﬀerent downstream links (We ad-
dress the issue of ECMP in the following section). If, on the
other hand, t2 < t3, then R changed its forwarding path to
D at some point in the range [t2, t3]. Our fundamental unit
of route change measurement is thus a Range (R, [t, t′], D),
indicating that R changed its next hop towards destination
D at some point during the interval [t, t′].
4. PRACTICAL CONSIDERATIONS
In this section, we discuss some issues that must be ad-
dressed before FlowRoute can be deployed in operational
networks.
4.1 Destination Granularity
FlowRoute needs to observe consecutive ﬂows to the same
destination, D, to be able to infer forwarding changes to-
wards D. While the destination IP address of a ﬂow is an
obvious choice, a router may not observe suﬃcient ﬂow vol-
ume towards the same destination to obtain a suﬃcient tem-
poral resolution for the time window of the routing change.
Aggregating at the level of destination preﬁxes yields more
ﬂows to each destination but requires some additional infor-
mation to map a ﬂow to its associated preﬁx. A third option
is to use the iBGP next hop ﬁeld [6] in Netﬂow, in which
case routers need to have complete iBGP routing tables.
2We can map the incoming and outgoing interfaces in Net-
ﬂow records to previous and next hop routers, and obtain
link propagation delays using slowly changing information
available from SNMP and conﬁguration data.
Our motivation in developing FlowRoute was to under-
stand the intra-AS routing dynamics of a large ISP. Such
networks are often designed to have a route free core, mean-
ing that routes consist of label-switched MPLS paths be-
tween ingress and egress routers. For routing within such
a network, the address of the MPLS tunnel endpoint is the
“destination” of a ﬂow.