title:Lucky 13 Strikes Back
author:Gorka Irazoqui Apecechea and
Mehmet Sinan Inci and
Thomas Eisenbarth and
Berk Sunar
Lucky 13 Strikes Back
Gorka Irazoqui
Worcester Polytechnic Institute
PI:EMAIL
Worcester Polytechnic Institute
Mehmet Sinan ˙Inci
PI:EMAIL
Thomas Eisenbarth
Worcester Polytechnic Institute
PI:EMAIL
Berk Sunar
Worcester Polytechnic Institute
PI:EMAIL
ABSTRACT
In this work we show how the Lucky 13 attack can be res-
urrected in the cloud by gaining access to a virtual machine
co-located with the target. Our version of the attack exploits
distinguishable cache access times enabled by VM dedupli-
cation to detect dummy function calls that only happen in
case of an incorrectly CBC-padded TLS packet. Thereby, we
gain back a new covert channel not considered in the original
paper that enables the Lucky 13 attack. In fact, the new side
channel is signiﬁcantly more accurate, thus yielding a much
more eﬀective attack. We brieﬂy survey prominent crypto-
graphic libraries for this vulnerability. The attack currently
succeeds to compromise PolarSSL, GnuTLS and CyaSSL on
deduplication enabled platforms while the Lucky 13 patches
in OpenSSL, Mozilla NSS and MatrixSSL are immune to
this vulnerability. We conclude that, any program that fol-
lows secret data dependent execution ﬂow is exploitable by
side-channel attacks as shown in (but not limited to) our
version of the Lucky 13 attack.
Keywords
Lucky 13 attack, Cross-VM attacks, virtualization, dedupli-
cation
1. MOTIVATION
The Transport Layer Security (TLS) family of protocols
ensures the security of the entire communications infras-
tructure by providing conﬁdentiality and integrity services
across untrusted networks. Numerous web applications rely
on TLS to secure client-server data traﬃc. Similarly dis-
tributed applications use TLS to establish a secure chan-
nel for transporting application-layer data with centralized
cloud servers. At the higher level TLS uses X.509 certiﬁ-
cates along with public key cryptography to authenticate
the exchanged symmetric encryption keys and to authenti-
cate the server. This session key is then used to ensure the
integrity and conﬁdentiality of the data exchanged over a
secure session between the TLS client and server.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS’15, April 14–17, 2015, Singapore..
Copyright c(cid:13) 2015 ACM 978-1-4503-3245-3/15/04 ...$15.00.
http://dx.doi.org/10.1145/2714576.2714625.
Starting as Secure Sockets Layer (SSL), after adoption
by the IETF TLS has undergone many changes (SSL 1.0,
2.0, 3.0, TLS 1.0, 1.1, 1.2). Many releases were motivated
by attacks targeting both the protocols and the underlying
cryptographic schemes [38, 8, 7, 17, 29, 5, 16, 41]. In this
work we focus on attacks targeting the padding procedure
in TLS’s MAC-Encode-Encrypt (MEE) primitive.
Implementation Attacks on TLS. Handling CBC IVs,
and paddings in cryptographic algorithms has a long history
of attacks and countermeasures, and is notoriously hard to
get right in implementations. As early as in 1998 Bleichen-
bacher pointed to vulnerabilities in SSL 3.0 stemming from
leaked error messages due to incorrectly padded plaintexts.
Later Vaudenay [38] presented an attack in the symmet-
ric key setting on SSL/TLS induced by CBC mode padding.
The BEAST chosen plaintext attack (Browser Exploit Against
SSL/TLS) [15] exploited a long-known cipher block chaining
(CBC) mode IV vulnerability in TLS 1.0 [25] to achieve full
plaintext recovery. The exploit is based on the earlier work
in [32, 8, 7]. The padding oracle attack is most commonly
applied to CBC encryption mode, where the server leaks
whether the padding of an encrypted message was correctly
formed or not. Depending on the speciﬁcs of the encryption
scheme and the encapsulating protocol, this side-channel
leakage may be escalated to a full message recovery attack.
These are collectively referred to as padding oracle attacks.
A more recent striking application of the aforementioned
padding attacks was given by Bardou et al. [9] where many
cryptographic hardware tokens were determined to be vul-
nerable. Speciﬁcally, Bardou et al. apply Vaudenay’s CBC
attack and improve Bleichenbacher’s attack to signiﬁcantly
reduce the number of decryption oracle accesses, thereby
making attacks feasible on slow tokens.
Even though in the last years the padding oracle attacks
were considered a ﬁxed vulnerability in the community, in
2013 a new kind of padding oracle attack was presented by
AlFardan et al. [16]. The Lucky 13 attack was proposed
to recover TLS/DTLS encrypted messages by exploiting a
vulnerability in the implementation of HMAC. The attack
works by carefully modifying network packets during trans-
mission, and using network timing information to recover
the plaintext byte-by-byte from TLS encrypted packets. The
attack received signiﬁcant attention from the media and in-
dustry. A great deal of work went into ﬁxing the TLS vul-
nerability. A widely applied and immediate ﬁx—using RC4
encryption instead of a block cipher in CBC mode—turned
out to be ill-advised: The attack described in [5] exploits
statistical biases in the RC4 key stream to recover parts of
the plaintext using a large number of TLS encryptions. To
ﬁx the popular MEE mode that uses a block cipher in CBC
mode, cryptographic library providers applied various tech-
niques aimed to equalize packet processing times, e.g. by
calling a dummy HMAC function. Since then modiﬁcations
are being studied to solve attacks against MEE schemes and
the Lucky 13 issue has been considered closed by the security
community and the industry.
In this work, we revive the Lucky 13 attack on a number of
prominent cryptographic libraries which have been patched
to eliminate the network timing side-channel. We instead
run our attacks in the cross-VM setting using cache access
information to realize the Lucky 13 attack.
Cross-VM Attacks. Cross-VM attacks assume a co-located
process running on the same physical hardware as a target
process (e.g. same machine on diﬀerent cores) can extract
information from the target in spite of the VM sandboxing.
Many side-channel attacks have been proposed that manage
to recover sensitive data when a spy process is executed in
the same OS as the victim. For instance, the early proposal
by Bernstein [11] (and later in [12, 36, 18]) targets the time
variation due to memory accesses to recover a AES encryp-
tion key. These techniques are now being moved to cloud
servers to break sandboxing across virtual machines.
Cross-VM attacks assume the attacker to be able to co-
locate with the victim. Co-location was considered a major
obstacle until 2009 when Ristenpart et al. [31] demonstrated
that it is possible to co-locate with a potential victim and
extract sensitive data across VMs.This initial result fueled
many other research targeting a co-located victim in a cloud
system.
In 2011, Suzaki et al. [33, 34] exploited a memory saving
OS-optimization called Kernel Samepage Merging (KSM) to
recover data from another user and to identify a co-located
user running in KVM hypervisors. Shortly later, Zhang et
al. [42] used an access driven cache timing attack, namely
Prime and Probe to recover an El Gamal decryption key
from a victim process running in Xen VMs.
In order to
cope with multiple sources of microarchitectural noise, the
authors used a hidden Markov model.
In contrast to the
work of Ristenpart et al. [31], the authors of [42] were able
to extract ﬁne grain information from a cryptographic im-
plementation across VMs.
Recently the powerful Flush+Reload attack was used by
Yarom et.al in cloud-like environments such as VMware ESXI
and KVM to extract RSA [41, 10] and ECDSA keys, while
Irazoqui et al. used the same detection method to recover
AES keys from co-located VMware VMs [21].
1.1 Our Contribution
In this work we demonstrate that by mounting cache at-
tacks it is possible to revive a modiﬁed Lucky 13 attack on
many of the patched TLS libraries. Speciﬁcally, we show
that it is possible to recover plaintexts from TLS encrypted
sessions across VM boundaries by applying a ﬂush+reload
cache attack in VMware ESXi VMs. The vulnerability per-
sists even if the VMs are running on diﬀerent cores in the
same machine. The attack works because some TLS libraries
prevent the Lucky 13 attack by using dummy functions to
ensure constant time executions. By monitoring the instruc-
tion cache, we detect accesses to these dummy functions and
hence distinguish valid CBC-paddings, as done in the Lucky
13 attack. While requiring co-location, the cache side chan-
nel is less noisy than the network timing side channel orig-
inally exploited in [16], resulting in a more eﬃcient attack.
The eﬀectiveness of the new attack is demonstrated on a
number prominent cryptographic libraries: PolarSSL [30],
GnuTLS [24], and CyaSSL [1]. Fortunately, our results also
indicate that some libraries such as OpenSSL [35], Mozilla’s
NSS [26], and MatrixSSL [3] have been patched well and the
new attack does not apply to them. These libraries feature
carefully crafted constant run time execution while OpenSSL
and Mozilla’s NSS also ensure branch-free handling of MAC
checking.
2. BACKGROUND
In this work we substitute the network timing channel
with the cache timing channel as experienced in a Cross-VM
setting. There is a very rich literature of cache attacks and
here we only very brieﬂy review cache timing attacks and
focus on a more recent and eﬀective cache attack variant,
e.g the Flush+Reload cache attack.
Cache Architecture. The cache architecture is a set of
components that reside between the CPU and the RAM.
The principal function of the cache is to reduce the aver-
age access time to the main memory by exploiting spatial
and temporal locality principles. When the CPU requests a
memory line, the cache will be searched ﬁrst to see if it is
located there. If so, it is said that a cache hit has occurred
and therefore, the access delay is much smaller. However
when the data is not found in the cache, the CPU will try
to ﬁnd the memory line in the subsequent levels of cache or
in the memory, which translates to greater delays. In this
case it is said that a cache miss has occurred. When a cache
miss occurs, the data is retrieved from the memory and a
copy is stored in all levels of the cache hierarchy following
both the spatial and temporal locality principles: recently
accessed data and data in nearby locations are likely to be
accessed soon.
Cache Side channel attacks. Cache based side channel
attacks have been widely studied over the last two decades.
It was in 1992 when the cache was ﬁrst considered as a valid
covert channel to extract sensitive information [20], and this
approach was further studied theoretically later in [23, 28,
37]. In the last decade many implementations of cache based
side channel attacks have been investigated. Bernstein in
2005 [11] recovered an AES keys due to microarchitectural
time diﬀerences between diﬀerent memory lines, whereas
Osvik et al. studied the performance of diﬀerent spy pro-
cesses monitoring the data cache like Prime and Probe and
Evict+Time on AES [27]. Only one year later, Bonneau
et al. implemented a cache attack based on table look up
collisions on AES [13].
Shortly later Acıi¸cmez showed that the instruction cache
also leaks information by mounting an attack targeting RSA
encryptions [4]. In a follow up work, Chen et al. improved
the attack proposed in [4] and applied it in a more realistic
scenario [14]. One year later, cache attacks were moved to
the cloud by Zhang et al. where they managed to recover
an El Gamal encryption key across XEN VMs [42].
Recently Gullasch et al. [19] demonstrated that deduplica-
tion features implemented in modern OSs can open a covert
channel to recover sensitive information like AES keys with
the Flush+Reload attack, but assuming to have control over
the CFS. This approach was later followed by Yarom et al.
and Irazoqui et al. to recover RSA and AES keys respec-
tively, even in cloud environments [41, 21]. Finally Benger
et al. also showed that the security of ECDSA encryptions
is compromised when the adversary is able to monitor cache
accesses [10].
2.1 The Flush+Reload Technique
The Flush+Reload attack is a powerful cache-based side
channel attack technique that checks if speciﬁc cache lines
have been accessed or not by the code under attack. Gul-
lasch et al. [18] ﬁrst used this spy process on AES, although
the authors did not brand their attack as Flush+Reload at
the time. Later Yarom et al. [41, 10] used it to target spe-
ciﬁc functions instead of data. In their studies, they used
the Flush+Reload technique to recover keys from RSA and
ECDSA decryption processes. Here we brieﬂy explain how
Flush+Reload attack works. The attack is carried out in 3
stages:
• Flush step: In this stage, the attacker uses the clflush
instruction to ﬂush the desired memory lines from the
cache and make sure that they go to the main memory.
We have to remark here that the clflush command
does not only ﬂush the memory line from the cache
hierarchy of the corresponding working core, but it
ﬂushes from all the caches of all the cores in the CPU.
This is an important point: if it only ﬂushed from the
corresponding core’s cache hierarchy, the attack would
only work if the attacker and victim’s processes were
running on the same CPU core. This would have re-
quired a much stronger assumption than just being on
the same physical machine.
• Victim accessing step: In this stage the attacker
waits until the victim runs a fragment of the targeted
code, which uses the memory lines that have been
ﬂushed in the ﬁrst stage.
• Reload step: In this stage the attacker reloads the
previously ﬂushed memory lines and measures the time
it takes to reload them. Depending on the reloading
time, the attacker decides whether the victim accessed
the memory line (in which case the memory line would
be present in the cache) or if the victim did not ac-
cess the corresponding memory line (in which case the
memory line will not be present in the cache.) The
timing diﬀerence between a cache hit and a cache miss
makes this diﬀerence detectable by the attacker.
The fact that the attacker and the victim processes do not
run on the same core is not a problem here. Even though
there may be isolation at various levels of the cache, in most
systems there is some level of cache that is shared between
all the cores. Therefore, through this shared level of cache
(typically the L3 cache), one can still distinguish between
accesses to the main memory and accesses to the cache.
2.2 Memory Deduplication
Memory deduplication is an optimization technique that
was originally introduced in Linux as KSM to improve the
memory utilization by merging duplicate memory pages.
KSM ﬁrst appeared in Linux kernel version 2.6.32 [22, 2].
In this implementation, KSM kernel daemon ksmd, scans
the user memory for potential pages to be shared among
Figure 1: Memory Deduplication Scheme
users [6], creating signatures for these pages. The signa-
tures are kept in the deduplication table for matching and
merging. When two or more pages with the same signature
are found, they are cross-checked completely to determine
if they are identical in which case they are merged with the
copy-on-write tag set.
Deduplication later became a standard technique for im-