        ALL, ALL_SWITCHES=full;
Default=0x7fff, ipoib, rate=3, mtu=4, scope=2:
        mgid=ff12:401b::ffff:ffff       # IPv4 Broadcast address
        mgid=ff12:401b::1               # IPv4 All Hosts group
        mgid=ff12:401b::2               # IPv4 All Routers group
        mgid=ff12:401b::16              # IPv4 IGMP group
        mgid=ff12:401b::fb              # IPv4 mDNS group
        mgid=ff12:401b::fc              # IPv4 Multicast Link Local Name Resolution group
        mgid=ff12:401b::101             # IPv4 NTP group
        mgid=ff12:401b::202             # IPv4 Sun RPC
        mgid=ff12:601b::1               # IPv6 All Hosts group
        mgid=ff12:601b::2               # IPv6 All Routers group
        mgid=ff12:601b::16              # IPv6 MLDv2-capable Routers group
        mgid=ff12:601b::fb              # IPv6 mDNS group
        mgid=ff12:601b::101             # IPv6 NTP group
        mgid=ff12:601b::202             # IPv6 Sun RPC group
        mgid=ff12:601b::1:3             # IPv6 Multicast Link Local Name Resolution group
        ALL=full, ALL_SWITCHES=full;
ib0_2=0x0002, rate=7, mtu=4, scope=2, defmember=full:
        ALL, ALL_SWITCHES=full;
ib0_2=0x0002, ipoib, rate=7, mtu=4, scope=2:
        mgid=ff12:401b::ffff:ffff       # IPv4 Broadcast address
        mgid=ff12:401b::1               # IPv4 All Hosts group
        mgid=ff12:401b::2               # IPv4 All Routers group
        mgid=ff12:401b::16              # IPv4 IGMP group
        mgid=ff12:401b::fb              # IPv4 mDNS group
        mgid=ff12:401b::fc              # IPv4 Multicast Link Local Name Resolution group
        mgid=ff12:401b::101             # IPv4 NTP group
        mgid=ff12:401b::202             # IPv4 Sun RPC
        mgid=ff12:601b::1               # IPv6 All Hosts group
        mgid=ff12:601b::2               # IPv6 All Routers group
        mgid=ff12:601b::16              # IPv6 MLDv2-capable Routers group
        mgid=ff12:601b::fb              # IPv6 mDNS group
        mgid=ff12:601b::101             # IPv6 NTP group
        mgid=ff12:601b::202             # IPv6 Sun RPC group
        mgid=ff12:601b::1:3             # IPv6 Multicast Link Local Name Resolution group
        ALL=full, ALL_SWITCHES=full;
```
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Enabling_opensm}9.4.5. 启用 opensm {.title}
:::
::: para
[**opensm**]{.application}
服务安装时默认不启动，用户需要自行启动该服务。请作为 `root`{.systemitem}
运行以下命令：
``` screen
~]# systemctl enable opensm
```
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Testing_Early_InfiniBand_RDMA_operation}9.5. 测试早期 InfiniBand RDMA 操作 {.title}
:::
::: {.note xmlns:d="http://docbook.org/ns/docbook"}
::: admonition_header
**注意**
:::
::: admonition
::: para
本小节只适用于 InfiniBand 设备。因为 iWARP 和 RoCE/IBoE 设备是基于
`IP`{.systemitem} 的设备，用户应在配置 IPoIB 且设备有 `IP`{.systemitem}
地址后，按照执行 RDMA 测试操作一节的内容进行测试。
:::
:::
:::
::: para
启用 `rdma`{.systemitem} 服务和 [**opensm**]{.application}
服务（若需要），并为具体硬件安装适当的用户空间库后，就可以进行用户空间
`rdma`{.systemitem} 操作。[libibverbs-utils]{.package}
软件包中的简单测试程序可帮助您确定 RDMA
操作是否正常。[**ibv_devices**]{.application}
程序显示该系统中目前所有设备，而 `ibv_devinfo`{.command}
命令会给出每个设备的具体信息。例如：
``` screen
~]$ ibv_devices
    device                 node GUID
    ------              ----------------
    mlx4_0              0002c903003178f0
    mlx4_1              f4521403007bcba0
~]$ ibv_devinfo -d mlx4_1
hca_id: mlx4_1
        transport:                      InfiniBand (0)
        fw_ver:                         2.30.8000
        node_guid:                      f452:1403:007b:cba0
        sys_image_guid:                 f452:1403:007b:cba3
        vendor_id:                      0x02c9
        vendor_part_id:                 4099
        hw_ver:                         0x0
        board_id:                       MT_1090120019
        phys_port_cnt:                  2
                port:   1
                        state:                  PORT_ACTIVE (4)
                        max_mtu:                4096 (5)
                        active_mtu:             2048 (4)
                        sm_lid:                 2
                        port_lid:               2
                        port_lmc:               0x01
                        link_layer:             InfiniBand
                port:   2
                        state:                  PORT_ACTIVE (4)
                        max_mtu:                4096 (5)
                        active_mtu:             4096 (5)
                        sm_lid:                 0
                        port_lid:               0
                        port_lmc:               0x00
                        link_layer:             Ethernet
~]$ ibstat mlx4_1
CA 'mlx4_1'
        CA type: MT4099
        Number of ports: 2
        Firmware version: 2.30.8000
        Hardware version: 0
        Node GUID: 0xf4521403007bcba0
        System image GUID: 0xf4521403007bcba3
        Port 1:
                State: Active
                Physical state: LinkUp
                Rate: 56
                Base lid: 2
                LMC: 1
                SM lid: 2
                Capability mask: 0x0251486a
                Port GUID: 0xf4521403007bcba1
                Link layer: InfiniBand
        Port 2:
                State: Active
                Physical state: LinkUp
                Rate: 40
                Base lid: 0
                LMC: 0
                SM lid: 0
                Capability mask: 0x04010000
                Port GUID: 0xf65214fffe7bcba2
                Link layer: Ethernet
```
:::
::: para
`ibv_devinfo`{.command} 和 `ibstat`{.command}
命令输出信息稍有不同（比如端口 MTU 信息是在 `ibv_devinfo`{.command}
而不是 `ibstat`{.command} 输出中显示，而端口 PUID 信息是在
`ibstat`{.command} 而不是 `ibv_devinfo`{.command}
输出中显示。同时有些信息的命名方式也不同，例如：`ibstat`{.command}
输出中的基础*本地标识符*（``{=html}LID``{=html}）与
`ibv_devinfo`{.command} 输出中的 `port_lid`{.literal} 是相同的信息。
:::
::: para
可使用简单的 ping 程序，比如 [infiniband-diags]{.package} 软件包中的
[**ibping**]{.application} 测试 RDMA 连接性。[**ibping**]{.application}
程序采用客户端/服务器模式。必须首先在一台机器中启动
[**ibping**]{.application} 服务器，然后再另一台机器中将
[**ibping**]{.application} 作为客户端运行，并让它与
[**ibping**]{.application} 服务器相连。因为我们是要测试基础 RDMA
功能，因此需要用于 RDMA 的地址解析方法，而不是使用 `IP`{.systemitem}
地址指定服务器。
:::
::: para
在服务器机器中，用户可使用 `ibv_devinfo`{.command} 和 `ibstat`{.command}
命令输出 `port_lid`{.literal}（或基础 lid）以及所要测试端口的端口
GUID（假设是上述接口的端口 1，则
`port_lid`{.literal}/`基础 LID`{.literal} 是 `2`{.literal}，而端口 GUID
是 `0xf4521403007bcba1`{.literal}）。然后使用所需选项启动
[**ibping**]{.application} 捆绑至要测试的网卡和端口，同时还要指定
[**ibping**]{.application} 以服务器模式运行。使用 `-?`{.option} 或者
`--help`{.option} 选项即可查看 [**ibping**]{.application}
的所有可用选项，但在这个实例中可使用 `-S`{.option} 或
`--Server`{.option} 选项，同时在绑定到具体网卡和端口时可使用
`-C`{.option} 或者 `--Ca`{.option} 以及 `-P`{.option} 或者
`--Port`{.option}
选项。注：这个实例中的端口不会指示端口号，但会在使用多端口网卡时指示物理端口号。要测试所使用
RDMA 结构的连接性，比如多端口网卡的第二端口，则需要让
[**ibping**]{.application} 捆绑至网卡的端口
`2`{.literal}。使用单一端口网卡时不需要这个选项。例如：
``` screen
~]$ ibping -S -C mlx4_1 -P 1
```
:::
::: para
然后切换至客户端机器并运行 [**ibping**]{.application}。记录
[**ibping**]{.application} 程序所绑定端口的端口 GUID 或者
[**ibping**]{.application} 程序所绑定服务器端口的
*本地标识符*（``{=html}LID``{=html}）。另外，还需要记录客户端机器中与服务器为所捆绑网卡和端口连接网络相同的网卡和端口。例如：如果服务器中第一网卡的第二端口所捆绑的网络是辅
RDMA
结构，那么就需要在客户端中指定一个也连接到第二结构的网卡和端口。完成配置后，请作为客户端运行
[**ibping**]{.application} 程序，使用在服务器中找到的端口 LID 或者 GUID
作为地址连接到服务器。例如：
``` screen
~]$ ibping -c 10000 -f -C mlx4_0 -P 1 -L 2
--- rdma-host.example.com.(none) (Lid 2) ibping statistics ---
10000 packets transmitted, 10000 received, 0% packet loss, time 816 ms
rtt min/avg/max = 0.032/0.081/0.446 ms
```
或
``` screen
~]$ ibping -c 10000 -f -C mlx4_0 -P 1 -G 0xf4521403007bcba1 \
--- rdma-host.example.com.(none) (Lid 2) ibping statistics ---
10000 packets transmitted, 10000 received, 0% packet loss, time 769 ms
rtt min/avg/max = 0.027/0.076/0.278 ms
```
这个结果会验证端到端 RDMA 通讯是否在用户空间应用程序中正常工作。
:::
::: para
可能会看到以下出错信息：
``` screen
~]$ ibv_devinfo
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
No IB devices found
```
这个出错信息表示未安装所需用户空间库。管理员需要安装一个在 [第 9.2 节
"与 InfiniBand 及 RDMA
相关的软件包"](#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-InfiniBand_and_RDMA_related_software_packages){.xref}
小节中列出的用户空间库（根据其硬件要求）。这种情况极少发生，可能是因为用户为驱动程序或
[**libibverbs**]{.application} 安装了错误的架构类型。例如：如果
[**libibverbs**]{.application} 使用架构 `x86_64`{.literal}，且安装了
[**libmlx4**]{.application} 而不是 `i686`{.literal}
类型，则会得到出错信息。
:::
::: {.note xmlns:d="http://docbook.org/ns/docbook"}
::: admonition_header
**注意**
:::
::: admonition
::: para
很多简单应用程序喜欢使用主机名或地址而不是 LID
打开服务器与客户端之间的通讯。在那些应用程序中需要在尝试测试端到端 RDMA
通讯前设置 IPoIB。[**ibping**]{.application}
应用程序通常不属于此列，它可以接受简单 LID
作为寻址方式，同时也使其成为解决 IPoIB
寻址测试可能出现问题的最简单测试方法，因此让我们能够从更独立的角度确定简单
RDMA 通讯是否可行。
:::
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Configuring_IPoIB}9.6. 配置 IPoIB {.title}
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Understanding_the_role_of_IPoIB}9.6.1. 了解 IPoIB 角色 {.title}
:::
::: para
如 [第 1.2 节 "IP 网络 vs 非 IP
网络"](#ch-Introduction_to_RHEL_Networking.html#sec-IP_Networks_versus_non-IP_Networks){.xref}
所述，大多数网络都是 `IP`{.systemitem} 网络。IPoIB 的角色是在 InfiniBand
RDMA 网络顶层提供 `IP`{.systemitem}
网络模拟层。这样就可以让现有应用程序无需修改即可在 InfiniBand
网络中运行。但那些应用程序的性能相比原本使用 RDMA
通讯编写的应用程序要低。因此大多数 InfiniBand
网络中有一些应用程序集合必须使用其全部网络性能，而对另一些应用程序来说，如果不需要修改为使用
RDMA 通讯，则性能降级是可以接受的。IPoIB
可让那些不那么主要的应用程序在网络中以其原有形式运行。
:::
::: para
因为 iWARP 和 RoCE/IBoE 网络实际上是在其 `IP`{.systemitem}
链接层顶层附带 RDMA 层的 `IP`{.systemitem} 网络，他们不需要
IPoIB。因此，内核会拒绝在 iWARP 或 RoCE/IBoE 设备中创建任何 IPoIB 设备。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Understanding_IPoIB_communication_modes}9.6.2. 了解 IPoIB 通讯方式 {.title}
:::
::: para
可将 IPoIB 设备配置为以数据报或连接模式运行。不同之处在于 IPoIB
层尝试在通讯另一端机器中打开的队列对类型。在数据报模式中会打开不可靠、断开连接的队列对；而在连接模式中会打开可靠且连接的队列对。
:::
::: para
使用数据报模式时，不可靠、断开的队列对不允许大于 InfiniBand 链接层 MTU
的数据包通过。IPoIB 层会在要传输的 `IP`{.systemitem} 数据包顶部添加一个
4 字节的 IPoIB 标头。因此，IPoIB MTU 必须比 InfiniBand 链接层小 4
字节。因为在 InfiniBand 链接层 MTU 一般为 2048 字节，则数据报模式中的
IPoIB 设备则通常为 2044 字节。
:::
::: para
使用连接模式时，可靠且连接的队列对类型允许大于 InfiniBand 链接层 MTU
的信息通过，同时在两端均采用主机适配器处理数据包片段并将其组合。这样使用连接模式时不会对通过
InfiniBand 适配器发送的 IPoIB 信息有大小限制，但 `IP`{.systemitem}
数据包只有 16 个字节字段，因此最大字节数只能为
`65535`{.literal}。最大允许的 MTU 实际较小，因为我们必须确定不同 TCP/IP
标头也适合这个大小限制。因此，连接模式中获得 IPoIB MTU 最大为
`65520`{.literal}，这样可保证为所有需要的 `TCP`{.systemitem}
标头提供足够的空间。
:::
::: para
连接模式选项通常有较高性能，但也会消耗更多的内核内存。因为大多数系统考虑的是性能而不是用内存消耗，因此连接模式是最常用的用户模式。
:::
::: para
但如果将系统配置为连接模式，它必须可以使用数据报模式发送多播流量（InfiniBand
交换机及结构无法使用连接模式传递多播流量），且可以在与未配置连接模式的主机通讯时返回数据报模式。管理员应意识到如果他们要运行发送多播数据的程序，且那些程序要以接口中的最大
MTU
发送多播数据，则需要将该接口配置为数据报操作，或者使用将多播程序配置为将其发送数据包大小限制在数据报允许的数据包范围内的其他方法。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Understanding_IPoIB_hardware_addresses}9.6.3. 了解 IPoIB 硬件地址 {.title}
:::
::: para
IPoIB 设备有 20 字节的硬件地址。已弃用程序 [**ifconfig**]{.application}
无法读取全部 20 字节，因此不应再用来尝试并查找 IPoIB
设备的正确硬件地址。[iproute]{.package} 软件包中的
[**ip**]{.application} 程序可正常工作。
:::
::: para
IPoIB
硬件地址的前四个字节是标签及队列对号码。接下来的八个字节是子网前缀。首次创建
IPoIB 后，会使用默认的子网前缀
`0xfe:80:00:00:00:00:00:00`{.literal}。该设备在建立与子网管理器之间的联系前都会使用这个默认子网前缀（0xfe80000000000000）。建立联系后，它会重新设置子网前缀使其与子网管理器为其配置的前缀匹配。最后八位字节是
IPoIB 设备所连接 IPoIB 端口的 GUID
地址。因为前四个字节及随后的八位字节会随时更改，因此在为 IPoIB
接口指定硬件地址时不会使用这些字节进行匹配。[第 9.3.3 节
"70-persistent-ipoib.rules
用法"](#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Usage_of_70-persistent-ipoib.rules){.xref}
一节中论述了如何导出该地址，即在 [**udev**]{.application} 规则文件的