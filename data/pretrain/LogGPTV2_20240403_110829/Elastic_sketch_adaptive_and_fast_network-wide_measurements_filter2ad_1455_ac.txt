To compress a sketch, our key idea is first to group the
counters, and then merge counters in the same group into
one counter.
Grouping: As shown in Figure 2, given a sketch A of size
zw′×d (width w = zw′, depth d, z is an integer representing
the compression rate). Our grouping method proceeds as
follows: 1) We split A into z equal divisions. The size of each
division is w′ × d. 2) We build a sketch B of size w′ × d. 3)
i [j]}k =1,...,Z )
Counters with the same index in its division ({Ak
i [j]}
are in the same group, so we can set Bi[j] = OPz
k =1{Ak
(1 ⩽ i ⩽ d, 1 ⩽ j ⩽ z), where OP is the merging opera-
tor (e.g., Max or Sum). To query sketch B, we only need to
change the hash function hi(.)%w to hi(.)%w%w′, owing to
the following lemma.
Lemma 3.2. Given an arbitrary integer i, two integers w
and w′, if w is divisible by w′, then (i%w) %w′ = i%w′ .
29 January 201812BackgroundRationaleStructureOperationEvaluationConclusion…Local: Equal Division Compression (EDC)87348812237706510………119100991…90159831125114A1A2AdA11[1]A12[1]A1Z[1]A12[3]A1Z[3]8861512877129914…B1[1]B1[3]B1[2]maxBd[1]AdZ[1]Ad1[1]Ad2[1]For example, (10%6)%3=10%3. This lemma will be repeated
z
leveraged in this paper.
Merging: we propose two merging methods. The first
method is to sum up the counters in each group, i.e., Bi[j] =
k =1{Ak
i [j]}. We name this method Sum Compression
(SC). As mentioned in Section 2, to adapt to available band-
width, one can build two CM sketches S1 and S2 with mem-
ory size of M and M/2. A better solution is to compress S1 to
a half. Using SC, the compressed S1 has the same accuracy
as S2, while SC does not take advantage of the information
recorded by S1. The second method is Maximum Com-
pression (MC). Instead of “sum”, we can use “maximum”,
i [j]}. Compared with SC,
i[j], A
i.e., Bi[j] = max{A
1
the sum operation in MC uses more information in S1, and
thus has better accuracy.
Error bound of maximum compression (MC): Given a
CM sketch with size d × zw, we compress it into size of d ×w
using MC. Given an arbitrarily small positive number ϵ and
an arbitrary flow fj, the absolute error of the sketch after
maximum compression is bounded by
i[j], ..., Az
2
Pr { ˆnj ⩾ nj + ϵ N } ⩽
(cid:40)
(cid:18)
1 −
1 − 1
ϵzw
(cid:19)(cid:20)
1 −
N
zw(nj + ϵ N)
(cid:21)z−1(cid:41)d
(3)
where nj is the real size of fj, ˆnj is the estimated size of fj,
and N is the total number of packets.
About SC and MC, we have the following conclusions:
1) We prove that after Sum Compression, the error bound
of the CM sketch does not change, while after maximum
compression, the error bound is tighter. 2) We prove that
using MC, the compressed CM sketch has over-estimation
error but no under-estimation error. 3) Our Compression is
fast, and our experimental results show that the compressing
speed is accelerated by 5 ∼ 8 times after using SIMD (Single
Instruction and Multiple Data). 4) There is no need for de-
compression. 5) Compression does not require any additional
data structure. We refer the interested reader to the detailed
proof and experiments in Section A.2, A.3, A.4, A.5, and B.7
of our technical report [47].
3.2.2 Merging of Sketches.
Figure 3: Network-wide measurements. Servers can be
used to merge sketches when the network is large.
As shown in Figure 3, one can use servers to save band-
width. Each server receives many sketches from measure-
ment nodes, merges them, and then sends them to the collec-
tor. For the sake of merging, we need to use the same hash
functions for all sketches. If they have common flow IDs, we
propose to use a naive method – Sum merging. Otherwise,
we propose a novel method, namely Maximum merging.
Sum Merging: Given two CM sketches of the same size
d × w, the Sum merging algorithm just adds the two CM
sketches, by adding every two corresponding counters. This
algorithm is simple and fast, but not accurate.
Maximum Merging for same-size sketches: Our algo-
rithm is named Maximum Merging (MM). As shown in
Figure 4, given two sketches A and B of size w × d, we
build a new sketch C also of size w × d. We simply set
Ci[j] = max{Ai[j], Bi[j]} (1 ⩽ i ⩽ d, 1 ⩽ j ⩽ w). For exam-
ple in Figure 4, C1[2] = max{A1[2], B1[2]} = max{3, 4} = 4.
This merging method can be easily extended to multiple
sketches. Obviously, after MM merging, the sketch still has
no under-estimation error. We can also merge two sketches
of different widths, and the details are shown in Section A.8
of our technical report [47].
Figure 4: Maximum merging algorithm.
3.3 Adaptivity to Packet Rate
In measurement nodes, there is often an input queue to buffer
incoming packets. The packet rate (i.e., the number of incom-
ing packets per second) is variable: in most cases, it is low,
but in the worst case, it is extremely high [31–33, 58]. When
packet rate is high, the input queue will be filled quickly,
and it is difficult to record the information of all packets. To
handle this, the state-of-the-art solution SketchVisor [12],
leverages a dedicated component, namely fast path, to absorb
excessive traffic at high packet rate. However, it needs to
travel the entire data structure in the worst case, albeit with
an amortized O(1) update complexity. This incurs substantial
memory accesses and hinders performance. In contrast, our
proposed method always needs exactly one memory access.
We propose a new strategy to enhance the insertion speed
when needed. When the number of packets in the input
queue is larger than a predefined threshold, we let the in-
coming packets only access the heavy part, so as to record
the information of elephant flows only and discard mouse
flows. The insertion process of the heavy part is almost un-
changed except in the following case: if a flow f in a bucket
is replaced by another flow f ′, the flow size of f ′ is set to
MeasurementnodeMeasurementnodeMeasurementnodeCollectorMeasurementTasksQueriesAnswersperiodicreportElasticSketchindataplaneMeasurementnodeServerServerSketch	mergingSketch	mergingPeking University, China4 January 201814ABCBackgroundRationaleStructureOperationEvaluationConclusionNetwork wide aggregation: Our method1. Corresponding maximum 2. sup-compression13412…32015………2417…7599410………+=24412…7599410………maxA1[2]B1[2]C1[2]the flow size of f . Therefore, each insertion needs one probe
of a bucket in the heavy part. When packet rate goes down,
we use our previous algorithms.
Fortunately, this strategy achieves much higher speed
at the cost of slightly degrading the accuracy. When this
strategy is activated, we do not discard the light part, but
only do not update it during insertion, as light part will still
be used during queries. This means that only information
recorded by the light part when high packet rate occurs is
lost. This strategy does not affect much the query accuracy
in most cases, since the packet rate is usually low.
3.4 Adaptivity to Flow Size Distribution
Figure 5: Duplication of the heavy part of Elastic. The
original number of buckets in the heavy part is 4, and
becomes 8 after duplication.
A key metric of the flow size distribution is the number of
elephant flows. As it can vary a lot, it is hard to determine
the size of the heavy part. To address this issue, we need
to make the heavy part adaptive to changes in the traffic
distribution. We propose a technique to dynamically double
the heavy part. It works as follows. Initially, we assign a small
memory size to the heavy part. As more and more elephant
flows are inserted, the heavy part will become full. We
define a threshold T1. If an incoming packet is mapped into
a bucket in which all flows are larger than T2, we regard
the bucket is full. If the number of full buckets exceeds a
threshold, we regard the heavy part is full. When the heavy
part becomes full, we propose the following copy operation:
just copy the heavy part and combine the heavy part with the
copy into one. The hash function is changed from h(.)%w
to h(.)%(2w). Again, this copy operation works thanks to
Lemma 3.2. After the copy operation, half of the flows in the
buckets should be removed. The remove operation can be
performed incrementally. For each insertion, we can check
all flows in the mapped bucket, and on average half of the
flows are not mapped to that bucket and can be removed.
Even though some buckets may end up not being cleaned,
this does not negatively impact the algorithm.
Example: As shown in Figure 5, we show how to insert the
incoming packet with flow f2 after duplication. We compute
h(f2)%8 and get the mapped bucket, in which flow f3 is. We
compute h(f3)%8 = 6 and find that it should be mapped to
the bucket in the copy part. Therefore, we replace f2 by f3.
Overhead: As the heavy part is often very small (e.g., 150KB),
the time overhead of copying an array of 150KB is often small
enough to be negligible.
In addition to enlarging the heavy part, we can also ac-
tively compress heavy part. The compression method is sim-
ilar to Maximum Compression (MC, Section 3.2.1). Different
from MC, for the heavy part, we merge buckets (key, vote +,
flag, vote−) rather than counters. Take merging two buckets
as an example. Given two buckets, for the two keys in the
buckets, we query their frequencies in the Elastic sketch, and
keep the larger one, and evict the other one into the light
part. The compression operation endows the Elastic sketch
the ability to actively release memory when needed.
4 OPTIMIZATIONS
4.1 Optimizing Light Part
Using CM sketch with d=1: For the CM sketch, a key met-
ric is the depth d, i.e., the number of arrays. Indeed, we can
achieve higher accuracy if using d=3 or 4. However, we rec-
ommend setting d = 1, because of two reasons: 1) We care
more about the feasibility of implementation and speed than
accuracy; 2) Our sketch is already very accurate.
4.2 Hardware Version of Elastic Sketches
Figure 6: Hardware version of the Elastic sketch.
As mentioned above, the first classic solution for elephant
collisions is using several sub-tables in the heavy part. Each
sub-table is exactly the same as the heavy part of the basic
version, but is associated with different hash functions. The
elephant collision rate decreases exponentially as the num-
ber of sub-tables increases linearly. As each sub-table has
the same operations, this version is suitable for hardware
platforms.
Examples: The insertion and query operations are slightly
different from the basic version, and here we use examples
to show the differences in Figure 6. 1) To insert f8, in the
first sub-table, the vote− is incremented by 1, and f8 will be
inserted into the next sub-table. 2) To insert f9 in the first
sub-table, f4 with flow size 7 is evicted, and inserted into
the next stage. In the second sub-table, f4 is mapped to the
bucket with f4. In this case, we just increment the value from
2 to 9. 3) To query a flow, as it could appear in multiple heavy
parts, we need to add all the values.
(f1,…)(f3,…)(f6,…)(f1,…)(f3,…)(f6,…)h(f3)%4=2h(f3)%8=6f2h(f2)%8(f2,…)Copy partHeavy part 1(f3,12,F,11)f8(f4,7,T,55)f3,12,F,12f9f9,1,T,0. . .f4𝜆=8, 55+1≥7∗𝜆(f1,9,T,3). . .. . .. . .(f4,2,T,11)f8f4,2+7,T,11Heavy part 2To query f4: 9 + value in light part1702Light partAccording to our experimental results, using 4 subtables
is a good trade-off between accuracy and feasibility of some
hardware implementations, such as P4Switch.
4.3 Software Version of Elastic Sketches
Figure 7: Software version of the Elastic sketch.
As mentioned above, the second classic solution for ele-
phant collisions is: letting each bucket in the heavy part store
several flows. This allows several elephant flows be recorded
in one bucket, thus the elephant collision rate drops signif-
icantly. In this way, the bucket size could be larger than a
machine word, thus the accessing of the heavy packet could
be the bottleneck. Fortunately, this process can be acceler-
ated by using SIMD on CPU platforms, and thus this version
is suitable for software platforms. The differences from the
basic version are: 1) All the flows in each bucket share one
vote− field; 2) We always try to evict the smallest flow in the
mapped bucket.
Examples: We use two examples in Figure 7 to only show
the differences between this software version and the basic
version. 1) Given an incoming packet with flow f8, we first
hash it into a bucket. The bucket is full, and has no f8. We
increment the vote− from 10 to 11. The smallest flow is f6
with flow size 11. Because 11 ⩽ 11 ∗ λ = 11 ∗ 8, we do not
evict flow f6, but insert f8 into the light part. 2) Given an
incoming packet with flow f9, we first hash it into a bucket.
The bucket is full, and has no f9. We increment the vote−
from 55 to 56, because 56 ⩾ 7∗λ = 7∗8, we evict flow f4 into
the light part. After the eviction, we set the hashed bucket
to (f9, 1,T), and set vote− to 0.
5 APPLICATIONS
Flow Size Estimation: Our Elastic can be directly used to
estimate flow size in packets. Our sketch has a unique charac-
teristic: for those flows that have a flag of false, our estimation
has no error. According to our experimental results, we find
that more than 56.6% flows in the heavy part have no error
when using 600KB memory for 2.5M packets.
Heavy Hitter Detection: For this task, we query the size
of each flow in the heavy part. If one’s size is larger than the
predefined threshold, then we report this flow as a heavy