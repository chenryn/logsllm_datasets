	Filter 2: An item-set T is dropped if there exists a proper superset S, (S ⊃ T) such that supp(S) ∗Hsupp ≥ supp(T) and lif t(S) > lif t(T) ∗ Hlif t, where Hsupp ≥ 1 and Hlif t ≥ 1If a rule has a superset which describes a similar number of samples, i.e. similar support, and the superset’s lift is higher, the rule will be dropped as the superset is a more powerful rule that describes most or all of the samples. Similarly, Hsupp is applied to loosen the comparison criteria for support, and Hlif t is applied to ensure that the lift difference is sufficiently large based on the use case. For example, consider two rules:| { d a t a c e n t e r A} => | { d a t a c e n t e r A} => | { d a t a c e n t e r A} => | { d a t a c e n t e r A} => | f a i l u r e | Y with | support | support | 0 . 8 | and | l i f t | l i f t | 2 | 0 . 7 8 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| { d a t a c e n t e r A, |{ d a t a c e n t e r A, |{ d a t a c e n t e r A, |c l u s t e r B , |c l u s t e r B , |rack C} => |rack C} => |f a i l u r e |f a i l u r e |Y with |Y with |support |support |0 . 7 8 || and |l i f t |6 |6 |6 |rack C} => |rack C} => |f a i l u r e |f a i l u r e |Y with |Y with |support |support |0 . 7 8 |
In this scenario, almost all of the samples in datacenter A are also in cluster B and rack C. When trying to understand the root cause of the failures, the longer item-set with a higher lift and a similar support is more informative, so we keep it and remove the shorter item-set.In summary, we prefer to keep shorter item-sets when the lift values are similar. When a longer item-set’s lift value is significantly higher than that of a subset, and the support values are similar, we keep the longer item-set.
3.7 	The Proposed FrameworkOur final implementation incorporating the above-mentioned improvements is illustrated in Figure 1. Utilizing Scuba’s scalable, in-memory infrastructure [1], we query data that is aggregated by the group-by operation based on the distinct value combinations in a set of columns. The aggregation is done excluding columns that the users specify as not useful, and columns that our check finds to have too many distinct values to satisfy a practical minimum support threshold. One-hot encoding is then applied to the queried data for converting the column-value pairs to Boolean columns, or items. We apply frequent pattern mining techniques such as Apriori and FP-Growth on the dataset to identify frequent item-sets, which are then filtered by lift because in RCA we are only interested in item-sets that are useful in separating the target labels, e.g. specific failure states, from the rest of the label values, e.g. successful software task states. Finally the filtering criteria in Section 3.6 are applied to further condense the report for better interpretability.In our framework we choose to filter the association rules by lift and support after the rules are mined with FP-Growth, instead of pruning the association rules during the tree construction as the STUCCO algorithm does in a contrast set mining problem [4, 5, 11]. This helps us find more granular item-sets with high lift that would otherwise be missed. For example, if association rule {A, B} ⇒ Y has a high lift (or the χ2statistic as in STUCCO), but both {A} ⇒ Y and {B} ⇒ Ywe prune the tree based on both support and lift (or χ2statistic) as STUCCO does. In STUCCO, have lift (or χ2statistic) values below the pruning threshold, {A, B} ⇒ Y would not be found ifevery node needs to be significant based on chi-square tests and large based on support for it to have child nodes [5]. On the other hand, as FP-Growth mines frequent item-sets only based on support, as long as {A} ⇒ Y, {B} ⇒ Y, and {A, B} ⇒ Y have enough support, they would all be reported as frequent item-sets. Then {A} ⇒ Y and {B} ⇒ Y will be filtered out due to low lift while {A, B} ⇒ Y will be reported in the final result.Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
31:12 Lin, et al.
User-Specified Filters
Variability-Based Filters
Query and Dedup (Scuba)
One-Hot Encoding
Frequent Pattern Mining (Apriori / FP-Growth)
Initial Pattern Filtering Using Lift 
Further Filtering for Interpretability
Fig. 1. The proposed framework for the Fast Dimensional Analysis.4 	EXPERIMENTAL RESULTS
In this section we present the optimization result for runtime and interpretability, based on their relationships with the min-support and max-length parameters during item-set mining, as well as the min-lift during item-set filtering. We experimented on two production datasets:• Anomalous Server Events This is a dataset about anomalous behaviors (e.g. rebooted servers not coming back online) on some hardware and software configurations (more details can be found in Section 5.1). We compiled the dataset with millions of events, each described by 20 features. The features expand to about 1500 distinct items after one-hot encoding, with tens of thousands of distinct valid item-sets. Approximately 10% of the data are positive samples, i.e. anomalous behaviors. For simplicity we refer to this dataset as ASE in the rest of the paper.• Service Requests This is a dataset that logs the requests between services in a large-scale system. Each request is logged with information such as the source and destination, allocated resource, service specifications, and authentication (more details can be found in Section 5.2). For experimenta-tion, we compiled a dataset that contains millions of requests, each described by 50 features. The features expand to about 500 distinct items after one-hot encoding, with 7000 distinct valid item-sets. Approximately 0.5% of the data are positive samples. For simplicity we refer to this dataset as SR in the rest of the paper.As the datasets are different by an order of magnitude in terms of the proportion of positive samples, we expect our range of lift to vary considerably. Additionally, the ASE dataset contains almost 5X the number of distinct feature values as the SR dataset does, which would affect the count and length of item-sets mined with respect to min-support. We demonstrate the results based on these two datasets with the different characteristics below.4.1 	Performance Improvement
4.1.1 	Data Pre-Aggregation. As described in Section 3.5, many events in production logs are identical, except for the unique identifiers such as job IDs, timestamps, and hostnames. The pre-aggregation of the data could effectively reduce the size of the ASE dataset by 200X and the SR
Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.Fast Dimensional Analysis for Root Cause Investigation 	31:13
dataset by 500X, which significantly improves both the data preparation time and the execution time of the FP-Growth algorithm.Table 2 shows the runtime needed for querying the data and grouping the entries with and without the pre-aggregation using Scuba’s infrastructure. Without the pre-aggregation step in Scuba, the data is queried from Scuba and grouped in memory before our framework consumes it. Without pre-aggregation, the large amount of data that is transferred has a significant performance impact on the query time. Overall the data preparation time is improved by 10X and 18X for the ASE and SR datasets.Table 2. Data preparation time with and without pre-aggregation in Scuba (in seconds)
| ASE (without pre-agg.) ASE (with pre-agg.) 
SR (without pre-agg.) SR (with pre-agg.) | Scuba query time 
(including pre-agg. time) 3.00 
0.39 
3.35 
0.22 | Grouping time (in memory) 
0.94
-
0.71
- | Total time |
|---|---|---|---|0.71
- | Total time |
|---|---|---|---|
| ASE (without pre-agg.) ASE (with pre-agg.)  SR (without pre-agg.) SR (with pre-agg.) |Scuba query time  (including pre-agg. time) 3.00  0.39  3.35  0.22 |Grouping time (in memory)  0.94 - 0.71 - |3.94  0.39  4.06  0.22 |After the data is grouped, we execute our modified FP-Growth algorithm which takes the count of samples per unique item-set, as a weight for additional input. The details of the modification is discussed in Section 3.5. The additional weight value is used in calculating the support and lift of an item-set, and has negligible overhead on the algorithm’s runtime. Effectively, this means the algorithm now only need to handle 200X and 500X fewer samples from the ASE and SR datasets. Hence, the pre-aggregation of the data is critical for enabling automatic RCA at this scale in near real-time. This is one of the major features of this paper, as most of the prior works mentioned in Section 2.3 did not incorporate any optimization using a production data infrastructure, and could not handle large-scale datasets in near real-time.4.1.2 	Optimizing for Number of Item-Sets. Item-set generation is the biggest factor in runtime of the proposed framework. We first examine the relationship between the number of reported frequent item-sets and min-support and max-length in Figure 2. Note that the vertical axes are in log scale.In Figure 2a, based on the anomalous server event (ASE) dataset, we see an exponential decrease in the number of item-sets as we increase min-support. Theoretically, the number of candidate item-sets is bounded bymax−lenдth is much lower because many items are mutually exclusive, i.e. some item-sets would never exist in production. The number of item-sets based on the three max-length values converge to within annumber of items. In practice, however, the number of candidatesorder of magnitude when min-support is around 0.4, meaning a greater proportion of item-sets with support greater than 0.4 can be covered by item-sets at length 3.Checking the convergence point helps us decide the proper max-length, given a desired min-support. For example, in a use case where the goal is to immediately root cause a major issue in production, we would be interested in item-sets with higher supports. In the ASE example, if our desired min-support is greater than the convergence point in Figure 2a, say 0.95, we only need to run the analysis with max-length set to 3, to avoid unnecessary computations for optimized performance.Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
| 31:14 | Number of item-sets | (log-scale) | 10000 | Max-length 3 | Max-length 4 | Max-length 5 | Max-length 5 | Max-length 5 | Lin, et al. |
|---|---|---|---|---|---|---|---|---|---|
| 31:14 |Number of item-sets |(log-scale) |10000 | | | | | |Lin, et al. |
| 31:14 |Number of item-sets |(log-scale) |1000 | | | | | |Lin, et al. || 31:14 |Number of item-sets |(log-scale) |100 | | | | | |Lin, et al. |
| 31:14 |Number of item-sets |(log-scale) |10 | | | | | |Lin, et al. |
| 31:14 |Number of item-sets |(log-scale) |10 |0.2 |0.4 |0.6 |0.8 |1 |Lin, et al. |
Min-support
(a) ASE dataset
| Number of item-sets | (log-scale) | 1000000 | Max-length 3 | Max-length 4 | Max-length 5 | Max-length 5 | Max-length 5 |
|---|---|---|---|---|---|---|---||---|---|---|---|---|---|---|---|
| Number of item-sets |(log-scale) |1000000 | | | | | |
| Number of item-sets |(log-scale) |100000 | | | | | |
| Number of item-sets |(log-scale) |10000 | | | | | |
| Number of item-sets |(log-scale) |1000 | | | | | |
| Number of item-sets |(log-scale) |1000 |0.2 |0.4 |0.6 |0.8 |1 |
Min-support
(b) SR dataset
Fig. 2. Relationship between number of mined item-sets and min-support and max-length.On the other hand, if the goal is to thoroughly explore the root causes for smaller groups, with less concerns about the runtime, we could set the min-support to a smaller value such as 0.4. In this case, max-length should be set sufficiently high so that second filter discussed in Section 3.6 can be effective to improve interpretability. For example, if a rule of interest is described by {A, B,C, D, E} ⇒ Y, but max-length is smaller than 5, up togenerated to represent this rule at the same support, whereas if max-length is set to 5, the exact rule of interest would be created and the rest item-sets with smaller lengths would be dropped by 	max−lenдth item-sets could be 	5the second filter in Section 3.6, as long as the rule of interest has a higher lift.The same trends are observed in Figure 2b for the service request (SR) dataset when increasing min-support or max-length, but there does not exist a clear convergence point. Additionally, the number of item-sets are non-zero when min-support is as high as 1, implying there are multiple item-sets with support being 1 at the different max-lengths. In practice, these item-sets with support being 1 often could be represented by more specific item-sets, i.e. supersets, and therefore could be filtered out by the filters in Section 3.6 if the supersets have higher lift. Figure 2a and Figure 2b demonstrate that the relationship between the number of mined item-sets and min-support is dataset-dependent, and the convergence point of different max-lengths determined by the complexity of the datasets.Proc. ACM Meas. Anal. Comput. Syst., Vol. 4, No. 2, Article 31. Publication date: June 2020.
Fast Dimensional Analysis for Root Cause Investigation 	31:154.1.3 	Runtime Improvement. An advantage of Apriori is that it is easily parallelizable, by splitting up the candidate generation at each length. The optimal parallelism level depends on the number of candidates, since each thread induces additional overhead. Figure 3 illustrates the runtime of Apriori at different levels of parallelism, based on the ASE dataset. The runtime is reported based on a machine with approximately 50 GB memory and 25 processors. As shown in the figure, a 9-thread parallelism resulted in the shortest runtime due to a better balance between the runtime reduction from parallel processing and the runtime increase from parallelization overheads. Every parallelism level up to 24 threads outperforms the single-threaded execution.1 	5 	9 	24
200
100
50