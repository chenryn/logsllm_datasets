The ideal  result  would  have  been  a  step  pulse  of a  1  s 
interval  from 1 to 2 s. This problem occurred because the 
JIFI  and  application  programs  were  running  at  the  same 
priority 
the  default  LynxOS  round-robin 
scheduling policy had the quantum size of 0.25 s. Figure 
2  also  shows  that  the  fault  injection  started  after  2  s, 
much later than the specified starting time of 1 s. 
level  and 
This  quantudpriority  problem  has  been  fixed  in  the 
recent enhanced version of JIFI.  As described earlier in 
503 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:00:35 UTC from IEEE Xplore.  Restrictions apply. 
30 
25 
20 
U) 5 15 
10 
5 
0 
0 
1 
2 
3 
4 
5 
6 
7 
Execution Time (sec) 
a 
0 5  
1 
I 
1 5  
Execution Ttme ( s e c )  
2 
2 5  
3 
Figure 2.  Time distribution for 200  runs of  a  register 
fault  injection  per  run  with  initDelay  =  1  s  and 
regMTBF = 1 s, using an earlier version of JlFl before 
enhancement. Fault injections occur at 0.25 s discrete 
intervals  due  to  the  0.25  s  quantum  of  the  default 
round-robin scheduling policy. 
Figure  4.  Fault  time  distribution  of  1000  runs  of 
memory fault  injections (1 fault per  run), showing a 
fairly  uniform  step  pulse  distribution from  1 to  2  s 
with  initDelay =  1 s  and  memMTBF =  1 s.  Very few 
memory  fault  injections  failed  in  the  first  attempt, 
compared to the above register fault injections. 
I ,   the  JIFI  parent  process  now  changes  the 
Figure 
scheduling  policy  from  the  default  LynxOS  round-robin 
with  a 0.25 s quantum to the default POSIX round-robin 
with a 0 1 s quantum  Further, the JIFI process is now set 
to the maximum priority so that when the SIGALARM is 
up  at  the calculated fault injection time, the JIFI process 
can  pre-empt  the  application  process  immediately  to 
inject the  fault without waiting for the application to run 
for  the  entiie  quantum.  One  thousand  runs  of  the 
enhanced JIFI  version using the same “iterative  floating- 
point addition” program with the same JIFI configuration 
parameters  for  register  fault  injections  show  a  fairly 
uniform  distribution  over  the  1  s  interval  from  1  to  2  s 
(Figure 3). When JIFI was not  able to inject a fault in the 
first try, JIFI tried in the next  1  s interval. 
_ _ _  
I 
15 
e 
10 
5 
0 
0 
1 
I 
IT  1 
I I   I 
Ir 
RI 
0 5  
1 
1 5  
2 
2 5  
3 
E x e c u l l o n  T i m e  ( s e c )  
Figure  3.  Fault  time  distribution  for  1000  runs  of 
register fault  injections (1  fault  per  run),  showing  a 
fairly uniform step pulse distribution from 1 to 2 s with 
initDelay = 1 s and regMTBF = 1 s.  Some register fault 
injections failed in the first attempt, and new attempts 
were made in the next 2 to 3 s interval. 
The test  result  of the  enhanced  JIFI  version  from the 
same  1000  runs  for  memory  fault 
injections  also 
detronstrates a fairly uniform  distribution (Figure 4). As 
can be  seen in the  figure, JIFI  was able to  inject almost 
all rnemory faults on the first attempt. 
3.2 Uniform fault distribution over location 
As described earlier, JIFI  is designed to yield uniform 
random  fault  distributions  over  both  time  and  location. 
The register  fault  distribution  for the  same  1000 register 
fault injection runs corresponding to Figure 3 is shown in 
Figure  5, demonstrating a fairly uniform register- location 
distribution.  Registers  32,  33,  34,  and  39  are  not 
injectable.  The  memory  fault  distribution  for  the  same 
1000 memory fault injection runs corresponding to Figure 
4  is  shown  in  Figure  6,  again  demonstrating  a  fairly 
unifclrm memory  location  distribution.  For memory fault 
injections,  faults  are  injected  uniformly  over  all  four 
regions  of  text,  data,  heap,  and  stack.  In  this  specific 
example  of  the  “iterative  floating-point  add”  MPI 
application  program,  the  sizes  of 
text,  data,  heap,  and 
stack regions were 296 KB, 659 KB, 185 KB, and 5  KB, 
respectively.  Although  the  heap  and  stack  sizes  are 
generally  changing  dynamically, 
they  are  basically 
conslant  within  the  floating-point  add  loop.  Since  the 
stack  size  is  small  in  this  example,  only  one  stack  hit 
occurred  out  of  1000  runs.  The  gdb  debugger  symbol 
table  output  indicated  that  the  starting  addresses  of  the 
text,  data,  and  stack  regions  are  fixed  at  0x10001000, 
0x20000000,  and  0x80000000,  respectively,  while  the 
heap region starts at Ox200A0024 right after the end of the 
data  region.  These  address  values agree  with the  virtual 
memory map described in LynxOS manuals. 
504 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:00:35 UTC from IEEE Xplore.  Restrictions apply. 
time,  as well  as detailed  data  for each fault such  as fault 
injection  time,  fault  location,  values  before  and  after 
injection,  program  counter  at  the  time  of  injection,  and 
others.  It  generates  a  comma-delimited  format  so  that 
relevant data can be easily extracted. 
linker  map,  which  provides 
A debugger such as gdb can be used to get the symbol 
table  or 
the  address 
information  of  all  functions  and  global  data  in  the 
application  code  and  data  regions.  It  enables the  user to 
relate each fault location to the software component such 
as various application modules, MPI, C libraries, or JIFI. 
A  verifier  compares  the  current  output  file  with  the 
gold-run  file  and  categorizes  the  outcome  into  three 
groups:  1) CorrectOutputFile, 2) IncorrectOutputFile, and 
3) NoOutputFile. The correct output file produced with no 
faults injected is called the gold-run  file.  In  the  scientific 
parallel  programs  which  are  REE’s  applications,  two 
issues  arise  with  the  typical  “gold  run”  approach:  1) 
slightly  different  execution  environments  such  as  the 
number of nodes performing the application, may result in 
correspondingly different output files, both of which may 
be  “correct”  and  2)  due  to  the  noise  environment  of the 
application such as may occur in the instrument or simply 
noise  in  the  measured  variable,  there  is  an  acceptable 
range of “correct” output values, i.e., there is not a single, 
unique  “correct”  answer.  It  was  therefore  necessary  to 
work  with  application developers (research  scientists) to 
devise “verifiers” which  grade the application output and 
provide indication of acceptable variance from the “gold” 
output. Each application, and beyond this, potentially each 
phase of each mission where the application may be used, 
requires a unique verifier to ascertain the sensitivities and 
reliability  of 
the  application  under  simulated  fault 
conditions. 
A  classifier  uses  the  information  obtained  from  the 
above tools to classify the outcome of each fault injection 
run into different groups. In  the application-run level, the 
outcome can be categorized into five groups:  1) Correct -- 
-  application  output  file  was  created  and  correct,  2) 
Incorrect  ---  application  output  file  was  created  but 
incorrect,  3)  Crash  ---  application  output  file  was  not 
created and JIFl  output reported  application  crash status, 
4) Hang --- the application was timed out by JIFI after the 
maximum  time  limit,  and  5 )   Invalid  ---  no  faults  were 
injected. 
Figure 5.  Register fault distribution for  1000 runs of 
register fault  injections (1  fault  per  run), showing  a 
fairly uniform distribution across the  entire range of 
all 75  registers defined by JIFI.  Register 32,  33,  34, 
and 39 are not injectable. 
virtual Memory Address (HEX) 
Figure 6.  Memory fault distribution for  1000 runs of 
memory fault  injections (1 fault  per  run), showing a 
fairly uniform distribution over all four regions of user 
code  (296  KB), data  (659  KB), heap  (185  KB), and 
stack (5 KB). Each bin is 32 KB except for the end bin 
of  each  region.  Only  one  stack  hit  occurred in  this 
example due to a small stack size. 
4. Fault Injection Data Analysis Tools 
5. Statistical Error Tests 
Fault  injection  experiments  require  data  logging  and 
analysis tools  to  analyze  fault behavior  and  evaluate  the 
system.  Data  logging and  analysis tools include  1)  JIFI 
output, 2) debugger, 3) verifier, 4) classifier,  and  5 )  data 
plot tool. 
JIFl generates the JIFI output file for each run. It prints 
out the application  end  status, total  number of faults,  run 
A 
texture  segmentation  program  for  Mars  rover 
autonomy  applications  was  initially  used  for statistical 
error  tests.  For  each  run,  a  fault  was  injected  into  the 
entire  code  region  (application  code,  MPI  code,  and  C 
libraries  code)  during  the  FFT  module  execution.  Each 
run was classified as CORRECT, INCORRECT, CRASH, 
505 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:00:35 UTC from IEEE Xplore.  Restrictions apply. 
- 
90% 
0 
5 0 0  
1 0 0 0  
1 5 0 0  
2 0 n o  
CORRECT 
z
” .. 
’ %  
“ U  
t
0 
5 0 0  
i n o o  
1 5 0 0  
2000 
INCORRECT 
a % l  \ 
-------- 
-~ 
---- 
I
I
___---- 
n
.
5 0 0  
1 0 0 0  
1 5 0 0  
2 0 0 0  
CRASH 
Figure 7. Statistical error tests with  f2o curves. 
HANG 
HANG, or INVALID based on JIFI and verifier outputs as 
described  above.  Since the  outcome yields either  TRUE 
or FALSE for each class, each class outcome is a binomial 
distribution with 
p I  = C, / N, 
- 
Q I = .I CL,  * (1-P  I ) / N, 
( 2 )  
where p , is the mean, Q , is the standard deviation, c, is the 
number of runs that belong to the class i, and N is the total 
number of runs. Figure 7  shows the “moving” average vs. 
(1) 
the number of runs for each class, obtained from 2 sets of 
1000 fault injection runs. Thk p , k20,  curves are overlaid 
to show the 95% confidence level. If the data were far off 
from  between  these  curves, the  system would  have  been 
time-varying and the  time-varying factors would  have  to 
be  isolated  as  controlled  variables.  A  different  input 
image, for  example, could  produce  a  somewhat different 
outcome. The plots and  equation  (2) indicate that  500 to 
1000 runs  are  sufficient to  achieve  acceptable first-order 
statistically meaningful results for controlled experiments. 
In the case of 500 runs, 20=0.9%  for p=l% or 99%, and 
20=1.9%  for  p=5%  or  95%.  However,  if  one  wants  to 
achieve 20=0.1 YO for p= 1 %, 40,000 runs are required! 
6. Conclusion 
A  fault injection tool  set for use in experimentation on 
a  fault  tolerant  parallel  processing  cluster  has  been 
developed. Initial experience indicates that the tool will be 
useFul  in  both  developing  and  validating  the  computer 
design  and  its  fault  coverage  as  well  as  characterizing 
system  behavior  under  fault conditions.  In  the  course of 
the  development,  several  problems  including  achieving 
uniform  random  distribution  of  fault  injections  were 
encountered and their solutions were described. 
7. References 
[I]  R.  R.  Some and  D.  C. Ngo,  “REE:  A COTS-Based  Fault 
Tolerant  Parallel  Processing  Supercomputer  for  Spacecraft 
Onboard Scientific Data Analysis,” Proc. of the Digital Avionics 
Systemconference, vol. 2, pp. B3-1-7 - 83-1-12, 1999. 
[2]  J. J.  Beahan, L.  Edmonds,  R.  D.  Ferraro,  A. Johnston,  D. 
Katz,  R.  R.  Some, “Detailed  Radiation  Fault  Modeling  of  the 
Remote  Exploration  and  Experimentation 
(REE)  First 
Generation testbed Architecture,” Aerospace  Conf. Proc., vol. 5, 
pp  279-29 I,  2000. 
[3] D. T. Stott, B. Floering, D.  Burke, Z.  Kalbarcyzk  and R. K. 
Iyer,  “NFTAPE:  a  Framework  for  Assessing  Dependability  in 
Distributed  Systems  with 
fault 
injectors”  , 
Prixeeding  Computer  Performance 
and  Dependability 
Symposium, pp. 9 1 - 100, 2000. 
[4./ J.  Carreira,  H.  Madeira  and  J.  G.  Silva,  “Xception:  A 
technique  for the Experimental Evaluation  of Dependability in 
Modem  Computers,” 
IEEE  Transactions  On  Software 
Engineering, Vol 24, pp.  125-135, Feb. 1998. 
[5]  J.  J.  Beahan,  “SWIFI:  A  Software-Implemented  Fault 
Injection Tool,” JPL Intemal Document, June 2000. 
lightweight 
Acknowledgment 
This  work  was  performed  at  the  Jet  Propulsion  Laboratory, 
California  Institute  of  Technology  under  a  contract  with  the 
National Aeronautics and Space Administration.  This project is 
part  of  NASA’s  High  Performance  Computing 
and 
Communications  Program,  and  is  funded  through the  NASA 
Office of Space Sciences. 
506 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:00:35 UTC from IEEE Xplore.  Restrictions apply.