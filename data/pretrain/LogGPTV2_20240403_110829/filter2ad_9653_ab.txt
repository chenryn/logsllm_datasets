tf.load_library等。看这几个API名字大概就知其义，最终我们选择使用前2个读写文件的API来完成PoC，其他API的想象空间读者可自行发掘。在验证过程中，笔者发现这里其实有个限制，只能寻找Tensorflow内置的API操作，也叫做kernel
ops，如果是外部python库实现的API函数，是不会插入到最终的图模型中，也就无法用于这个攻击场景。
满足第一个条件，并没有想象的那么简单，笔者当时也颇费了一翻周折。
我们以一个简单的线性回归模型y=x+1为例，x为输入变量，y为输出结果，用Tensorflow的python API实现如下：
读写文件类的操作显然与线性回归计算无关，不能直接作为模型的输入或输出依赖来执行；如果直接执行这个操作呢？
从tf.write_file
API文档可以看到，返回值是一个operation，可以被Tensorflow直接执行，但问题是这个执行如何被触发呢？在Tensorflow中模型的执行以run一个session开始，这里当用户正常使用线性回归模型时，session.run(y)即可得到y的结果，如果要执行写文件的动作，那就要用户去执行类似session.run(tf.write_file)这样的操作，显然不正常。
在几乎翻遍了Tensorflow的API文档后，笔者找到了这样一个特性：
简单来说，要执行control_dependencies这个context中的操作，必须要先计算control_inputs里面的操作，慢着，这种依赖性不正是我们想要的么？来看看这段python代码：
这个success_write函数返回了一个常量1，但在control_dependencies的影响下，返回1之前必须先执行tf.write_file操作！这个常量1正好作为模型y=x+1的输入，漏洞利用的第一个条件也满足了。
最后还有一个小问题，完成临门一脚，能够读写本地文件了，能干什么“坏事”呢？在Linux下可以在crontab中写入后门自动执行，不过可能权限不够，笔者这里用了另外一种思路，在Linux下读取当前用户home目录，然后在bashrc文件中写入反连后门，等用户下次启动shell时自动执行后门，当然还有其他利用思路，就留给读者来思考了。值得注意的是，利用代码中这些操作都需要用Tensorflow内置的API来完成，不然不会插入到图模型中。
把上面的动作串起来，关键的PoC代码如下：
当用户使用这个训练好的线性回归模型时，一般使用以下代码：
运行效果如下：
模型使用者得到了线性回归预期的结果4（x=3, y=4），一切正常，但其实嵌入在模型中的反连后门已悄然执行，被攻击者成功控制了电脑。
图9 Tensorflow模型中反连后门被执行
在完成这个PoC后，我们仔细思考下利用场景，在Tensorflow中共享训练好的机器学习模型给他人使用是非常常见的方式，Tensorflow官方也在GitHub上提供了大量的模型供研究人员使用[9]，我们设想了这样一个大规模攻击场景，在GitHub上公布一些常用的机器学习模型，在模型中插入后门代码，然后静待结果。
回顾一下，这个安全问题产生的根本原因在于Tensorflow环境中模型是一个具有可执行属性的载体，而Tensorflow对其中的敏感操作又没有做任何限制；同时在一般用户甚至AI研究人员的认知中，模型文件是被视作不具有执行属性的数据文件，更加强了这种攻击的隐蔽性。
我们把这个问题报告给Google后，经过多轮沟通，Google
Tensorflow团队最终不认为该问题是安全漏洞，但认为是个高危安全风险，并专门发布了一篇关于Tensorflow安全的文章[10]，理由大致是Tensorflow模型应该被视作可执行程序，用户有责任知道执行不明模型的风险，并给出了相应的安全建议。
### **0x6 更进一步——发现多个传统安全漏洞**
在对Tensorflow其他攻击面的分析中，我们尝试了人工审计代码和Fuzzing的方法，又发现了多个安全漏洞，大部分属于传统的内存破坏型漏洞，涉及Tensorflow的图片解析处理、模型文件解析、XLA
compiler等功能，并且漏洞代码都属于Tensorflow框架本身，也从侧面反映了Tensorflow在代码安全上并没有做更多的工作。
下面是Tensorflow发布的安全公告及致谢[11]，目前为止共7个安全漏洞，均为Tencent Blade Team发现，其中5个为笔者发现。
在研究过程中，我们也注意到业界的一些类似研究，如360安全团队对多款机器学习框架用到的第三方库进行了安全审计，发现存在大量安全问题[12]，其中多为传统二进制漏洞类型。
### **0x7 一波三折——推动Tensorflow建立漏洞处理流程**
回顾整个漏洞报告和处理流程，可谓一波三折。最初上报漏洞时，我们发现除了GitHub上的issue，Tensorflow似乎没有其他的漏洞上报渠道，出于风险考虑，我们觉得发现的安全问题在修复之前不适合在GitHub上直接公开，最后在Google
Groups发帖询问，有一个自称是Tensorflow开发负责人的老外回复，可以把安全问题单发给他，开始笔者还怀疑老外是不是骗子，事后证明这个人确实是Tensorflow团队开发负责人。
经过持续近5个月、几十封邮件的沟通，除了漏洞修复之外，最终我们也推动Google Tensorflow团队建立了基本的漏洞响应和处理流程。
1）Tensorflow在GitHub上就安全问题作了特别说明Using Tensorflow
Securely[10]，包括安全漏洞认定范围，上报方法(邮件报告给PI:EMAIL)，漏洞处理流程等；
图10 Tensorflow安全漏洞处理流程
2）发布安全公告，包括漏洞详情和致谢信息[11]；
3）在Tensoflow官网(tensorflow.org)增加一项内容Security[13]，并链接至GitHub安全公告，引导用户对安全问题的重视。
### **0x8 修复方案和建议**
针对我们发现的模型机制安全风险，Google在Using Tensorflow Securely这篇安全公告中做了专门说明[10]，给出了相应的安全措施：
1）提高用户安全意识，把Tensorflow模型视作可执行程序，这里其实是一个用户观念的转变；
2）建议用户在沙箱环境中执行外部不可信的模型文件，如nsjail沙箱；
3）在我们的建议下，Tensorflow在一个模型命令行工具中增加了扫描功能（tensorflow/python/tools/saved_model_cli.py），可以列出模型中的可疑操作，供用户判断。
可以看出，Tensorflow团队认为这个安全风险的解决主要在用户，而不是Tensorflow框架本身。我们也在Blade
Team的官方网站上对这个风险进行了安全预警，并命名为“Columbus” [14]。
上文提到的其他内存破坏型漏洞，Tensorflow已在后续版本中修复，可参考安全公告[11]。
### **0x9 后记**
AI安全将走向何方？我们相信AI算法安全的对抗将会持续升级，同时作为背后生产力主角的基础设施软件安全理应受到应有的关注，笔者希望这个小小的研究能抛砖引玉（实际上我们的研究结果也引起了一些专家和媒体的关注），期待更多安全研究者投身于此，一起为更安全的未来努力。
### **0x10 参考**
[0] 
[1] 
[2] 
[3] 
[4] 
[5] 
[6] 
[7]
[8]
[9] 
[10] 
[11]
[12] 
[13] 
[14] 
* * *