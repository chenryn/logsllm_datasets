N
N
=
=
=
·
·
·
2N 2 + k · o
N 2
N 2 + o
N 2
i =0
as desired.
□
The next lemma estimates, for any x ∈ [N], the expected number
of preimages of the point h[1,k](x) and its variance. We use IA to
denote the indicator variable of probability event A.
and let Lj =N
Lemma A.3. Let {hi}k
i =1 ∈ ℱN be independent random functions,
i =1 Ih[1,k](i)=j be a random variable of the number of
different preimages under h[1,k] of j ∈ [N]. For every x ∈ [N],
(cid:104)
(cid:104)
(cid:105)
(cid:105)
E
Lh[1,k](x)
= k + 1 − o(1)
h1, ...,hk
Varh1, ...,hk
Lh[1,k](x)
= 1
2
(k + 1)2 .
Proof. From the linearity of expectation and the previous lemma,
we find that
(cid:104)
E
h1, ...,hk
Lh[1,k](x)
(cid:105)
=
=
x′=1
Ih[1,k](x)=h[1,k](x′)
(cid:34) N
(cid:35)
(cid:104)
(cid:105)
(cid:2)h[1,k](x) = h[1,k](x
(cid:18) k
Ih[1,k](x)=h[1,k](x′)
(cid:19)
− o( 1
N )
N
h1, ...,hk
E
N
N
x′=1
E
h1, ...,hk
=
Pr
h1, ...,hk
x′=1
= 1 + (N − 1) ·
′)(cid:3)
= k + 1 − o(1).
(cid:104)
Additionally,
L2
h[1,k](x)
h1, . . .,hk
E
=
E
h1, . . .,hk
(cid:105)
(cid:34) N
N
 N
x′=1
x′′=1
=
E
x′,x′′=1
h1, . . .,hk
= (N − 1)(N − 2) ·
Ih[1,k](x)=h[1,k](x′) · Ih[1,k](x)=h[1,k](x′′)
Ih[1,k](x)=h[1,k](x′)=h[1,k](x′′)
(cid:104)
(cid:35)
(cid:105)
Ih[1,k](x)=h[1,k](x′)=h[1,k](x′′)
+ 3(N − 1) ·
E
(cid:105)
(cid:19)
+ 1
h1, . . .,hk
Ih[1,k](x)=h[1,k](x′)
x,x′,x′′ different
E
h1, . . .,hk
x (cid:44)x′
(cid:104)
(cid:18) k(3k − 1 + oN (1))
2 + oN (1)(cid:1) k + 1
(cid:104)
(cid:105)
(cid:104)
(cid:105) −
2 + oN(1)(cid:17)
(cid:16) 1
h1, ...,hk
2k2 +
L2
h[1,k](x)
Lh[1,k](x)
2N 2
= 1
E
=
+ 3(N − 1) ·
2 k2 +(cid:0) 5
= (N − 1)(N − 2) ·
= 3
and thus
Varh1, ...,hk
(cid:1)(cid:19)
+ 1
(cid:18) k
N
N
− o(cid:0) 1
(cid:104)
(cid:105)2
Lh[1,k](x)
E
h1, ...,hk
2(k + 1)2.
k ≤ 1
and let Lj =N
□
i =1 ∈ ℱN be independent random functions,
i =1 Ih[1,k](i)=j be a random variable of the number of
Lemma A.4. Let {hi}k
different preimages under h[1,k] of j ∈ [N]. For every x ∈ [N],
Pr
h1, ...,hk
(cid:20)
Lh[1,k](x) ≥ 2k√
(cid:21)
ϵ
Proof. Applying Chebyshev’s inequality, we obtain
Pr
h1, . . .,hk
≤
Pr
(cid:34)
Lh[1,k](x) ≥ 2k√
Lh[1,k](x) ≥ (k + 1) +
ϵ
(cid:114) 2
ϵ
(cid:35)
(cid:114) 1
2
·
(k + 1)
≤ ϵ
2 .
□
h1, . . .,hk
Lemma A.5. Let {hi}k
dom functions, and let Lj =N
(cid:20)
i =1 ∈ ℱM, N be independent identically ran-
i =1 Ih[1,k](i)=j be a random variable of
the number of different preimages under h[1,k] of j ∈ [M] × [N]. For
every (s, x) ∈ [M] × [N],
(cid:21)
Pr
h1, ...,hk
Lh[1,k](s,x) ≥ 2k√
ϵ
≤ ϵ
2 .
Proof. Fix s ∈ [M], and define hi,s(x) to be the last n bits of
i =1 yields the result.
□
hi(s, x). Applying the previous lemma to {hi,s}k
(cid:21)
≤ ϵ
2 .
(cid:20)
B PROOF OF THEOREM 8.2
Our proof reduces the problem of inverting a random function to
the problem of inverting a random hash chain.
Let 𝒟 be a distribution over functions from 𝒳 to 𝒳 such that
for every x ∈ 𝒳
(cid:20)
(cid:21)
Lh[1,k](x) ≥ 2k√
ϵ
≤ ϵ
2 .
(7)
Pr
h1, ...,hk ∈𝒟
14
since Theorem 8.1, and therefore also our reduction, can be arbitrar-
ily non-uniform in the input size. Another way of thinking about
this is that since our model charges the algorithm only for oracle
queries, an algorithm in this model can deterministically determine
the best i∗ and the remaining functions by simulating A’s behavior
on all possible inputs (without making any oracle queries).
Consider the following algorithm A′ for inverting a random
function h ∈ 𝒟. Algorithm A′ gets the same S bits of advice as
A and is given oracle access to h. On input z ∈ 𝒳 , A′ computes
y = h[i∗+1,k](z) and then simulates A on y as follows: A′ uses its
own oracle to answer oracle queries to hi∗ and uses the chosen func-
tions h1, . . . , hi∗−1, hi∗+1, . . . , hk to answer all other oracle queries.
Furthermore, A′ bounds the number of queries to h by 2T
k . Thus,
if during the simulation A tries to make more than this number of
queries to h, algorithm A′ aborts. Otherwise, A′ obtains A(y) and
then computes and outputs h[1,i∗−1](A(y)).
To analyze the success probability of A′, the key observation is
h ), then A′ inverts h(w) successfully. To see
that if w ∈ h[1,i∗−1](Gi∗
h , then A′ simulates A
this, note that if w = h[1,i∗−1](x) for x ∈ Gi∗
on
y = h[i∗+1,k](h(w)) = h[1,k](x) ∈ Gi∗
h ,
(cid:2)A
thus A(y) = x, and A′(h(w)) = h[1,i∗−1](A(y)) = w as desired.
Therefore
Pr
h∈𝒟
w ∈𝒳
−1(h(w))(cid:3) ≥ Pr
′(h(w)) ∈ h
(cid:2)w ∈ h[1,i∗−1](G
′)(cid:3)
(cid:104)
(cid:105) ≥ ϵ3/2
h∈𝒟
w ∈𝒳
,
x ∈ Gi∗
h
8k
where the penultimate equality holds because h[1,k] and therefore
also h[1,i∗−1] have no collisions on Gi∗
h , and the last inequality
follows from Equation 8.
To complete the proof of the theorem, we apply the lower bound
given by Theorem 8.1 to algorithm A′ and the distribution SM, N ,
which gives
(cid:32)
(cid:33)
,
1 + S
M
≥ ˜Ω
ϵ3/2N
8k
(cid:19)
(cid:19)
(cid:18)
(cid:18)
1 + S
M
≥ ˜Ω(ϵ3/2N)
□
2T
k
T
and therefore
as required.
(cid:19)(cid:27)
(cid:18)
(cid:17) ∧
(cid:26)
(cid:16)
Let A be an oracle algorithm, which, given oracle access to h1, . . . , hk ∈
𝒟 and S bits of advice, makes at most T oracle queries to all of its or-
acles combined and successfully inverts with probability ϵ ∈ (0, 1).
For any choice of functions {hi}k
i =1 ∈ 𝒟, let
Gh1, ...,hk
=
x ∈ 𝒳 :
A(h[1,k](x)) = x
Lh[1,k](x) ≤ 2k√
ϵ
be the set of good points, where we say that a point is good if
A outputs x when executed on h[1,k](x), and the point does not
have many collisions under h[1,k]. Note that the first condition is
stronger than the condition that A merely inverts h[1,k](x). Denote
by h[1,k](Gh1, ...,hk ) the corresponding set of good images. Observe
that the second condition above guarantees that each point in the
image h[1,k](Gh1, ...,hk ) has at most 2k√
ϵ preimages under h[1,k].
Using this observation and a union bound, we conclude that
(cid:2)h[1,k](x) ∈ h[1,k](Gh1, . . .,hk )(cid:3)
h1, . . .,hk
Pr
x∈𝒳
√
ϵ
2k
≥
·
√
ϵ
2k
(cid:2)x ∈ Gh1, . . .,hk
(cid:104)
·(cid:169)(cid:173)(cid:173)(cid:171)
(cid:3) ≥
A(h[1,k](x)) ∈ h−1[1,k](h[1,k](x))(cid:105)
(cid:20)
Pr