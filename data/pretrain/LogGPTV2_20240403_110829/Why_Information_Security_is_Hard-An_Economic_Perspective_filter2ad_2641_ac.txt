ing them aren’t the people who suﬀer when they fail.
Much has been written on the ways in which corpo-
rate performance can be adversely aﬀected when exec-
utives have incentives at odds with the welfare of their
employer. For example, managers often buy products
and services which they know to be suboptimal or even
defective, but which are from big name suppliers. This
is known to minimize the likelihood of getting ﬁred
when things go wrong. Corporate lawyers don’t con-
demn this as fraud, but praise it as ‘due diligence’.
Over the last decade of the twentieth century, many
businesses have sought to ﬁx this problem by extend-
ing stock options to ever more employees. However,
these incentives don’t appear to be enough to ensure
prudent practice by security managers. (This might
be an interesting topic for a PhD; does it come down
to the fact that security managers also have less in-
formation about threats, and so cannot make rational
decisions about protection versus insurance, or is it
simply due to adverse selection among security man-
agers?)
This problem has long been perceived, even if not
in precisely these terms, and the usual solution to be
proposed is an evaluation system. This can be a pri-
vate arrangement, such as the equipment tests carried
out by insurance industry laboratories for their mem-
ber companies, or it can be public sector, as with the
Orange Book and the Common Criteria.
For all its faults, the Orange Book had the virtue
that evaluations were carried out by the party who re-
lied on them – the government. The European equiva-
lent, ITSEC, introduced a pernicious innovation – that
the evaluation was not paid for by the government but
by the vendor seeking an evaluation on its product.
This got carried over into the Common Criteria.
This change in the rules provided the critical per-
verse incentive.
It motivated the vendor to shop
around for the evaluation contractor who would give
his product the easiest ride, whether by asking fewer
questions, charging less money, taking the least time,
or all of the above. To be fair, the potential for this
was realized, and schemes were set up whereby con-
tractors could obtain approval as a CLEF (commercial
licensed evaluation facility). The threat that a CLEF
might have its license withdrawn was supposed to oﬀ-
set the commercial pressures to cut corners.
But in none of the half-dozen or so disputed cases
I’ve been involved in has the Common Criteria ap-
proach proved satisfactory. Some examples are doc-
umented in my book, Security Engineering [3]. The
failure modes appear to involve fairly straightforward
pandering to customers’ wishes, even (indeed espe-
cially) where these were in conﬂict with the interests of
the users for whom the evaluation was supposedly be-
ing prepared. The lack of sanctions for misbehaviour
– such as a process whereby evaluation teams can lose
their accreditation when they lose their sparkle, or get
caught in gross incompetence or dishonesty, is proba-
bly a contributory factor.
But there is at least one more signiﬁcant perverse
incentive. From the user’s point of view, an evaluation
may actually subtract from the value of a product. For
example, if you use an unevaluated product to gener-
ate digital signatures, and a forged signature turns up
which someone tries to use against you, you might rea-
sonably expect to challenge the evidence by persuad-
ing a court to order the release of full documentation
to your expert witnesses. A Common Criteria certiﬁ-
cate might make a court much less ready to order dis-
closure, and thus could severely prejudice your rights.
A cynic might suggest that this is precisely why it’s
the vendors of products which are designed to transfer
liability (such as digital signature smartcards), to sat-
isfy due diligence requirements (such as ﬁrewalls) or to
impress naive users (such as PC access control prod-
ucts), who are most enthusiastic about the Common
Criteria.
So an economist is unlikely to place blind faith in
a Common Criteria evaluation. Fortunately, the per-
verse incentives discussed above should limit the up-
take of the Criteria to sectors where an oﬃcial certi-
ﬁcation, however irrelevant, erroneous or misleading,
oﬀers competitive advantage.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:04:10 UTC from IEEE Xplore.  Restrictions apply. 
6 Conclusions
Much has been written on the failure of informa-
tion security mechanisms to protect end users from
privacy violations and fraud. This misses the point.
The real driving forces behind security system design
usually have nothing to do with such altruistic goals.
They are much more likely to be the desire to grab a
monopoly, to charge diﬀerent prices to diﬀerent users
for essentially the same service, and to dump risk. Of-
ten this is perfectly rational.
In an ideal world, the removal of perverse eco-
nomic incentives to create insecure systems would de-
politicize most issues. Security engineering would then
be a matter of rational risk management rather than
risk dumping. But as information security is about
power and money – about raising barriers to trade,
segmenting markets and diﬀerentiating products – the
evaluator should not restrict herself to technical tools
like cryptanalysis and information ﬂow, but also apply
economic tools such as the analysis of asymmetric in-
formation and moral hazard. As fast as one perverse
incentive can be removed by regulators, businesses
(and governments) are likely to create two more.
In other words, the management of information se-
curity is a much deeper and more political problem
than is usually realized; solutions are likely to be sub-
tle and partial, while many simplistic technical ap-
proaches are bound to fail. The time has come for en-
gineers, economists, lawyers and policymakers to try
to forge common approaches.
Acknowledgements
I got useful comments on early drafts of some of
this material from Avi Rubin, Hal Finney, Jack Lang,
Andrew Odlyzko and Hal Varian.
Postscript
The dreadful events of September 11th happened
just before this manuscript was ﬁnalised. They will
take some time to digest, and rather than rewriting
the paper it seemed better to add this short postscript.
I believe that the kind of economic arguments ad-
vanced here will be found to apply to protecting
‘bricks’ as much as ‘clicks’. It may take years for the
courts to argue about liability; there will remain a
strong public interest in ensuring that the operational
responsibility for protection does not become divorced
from the liability for the failure of that protection.
The arguments in section 4 are also brought into
sharper relief. In a world in which the ‘black hats’ can
attack anywhere but the ‘white hats’ have to defend
everywhere, the black hats have a huge economic ad-
vantage. This suggests that local defensive protection
is not enough; there is an essential role for global de-
fence, of which deterrence and retribution may be an
unavoidable part.
The suppression of piracy, mentioned in that sec-
tion, may be a useful example.
It might also be a
sobering one. Although, from the late seventeenth
century, major governments started to agree that the
use of pirates as instruments of state policy was unac-
ceptable, there was no single solution. It took many
treaties, many naval actions, and the overthrow of a
number of rogue governments, over a period of more
than a century, to pacify the world’s oceans. The
project became entwined, in complex ways, with other
campaigns, including the abolition of slavery and the
spread of colonialism. Liberals faced tough moral
dilemmas: was it acceptable to conquer and colonise
a particular territory, in order to suppress piracy and
slavery there? In the end, economic factors appear to
have been politically decisive; piracy simply cost busi-
ness too much money. History may not repeat itself,
but it might not be wise to ignore it.
References
[1] GA Akerlof, “The Market for ’Lemons’: Qual-
ity Uncertainty and Market Mechanism,”
Quarterly Journal of Economics v 84 (August
1970) pp 488–500
[2] J Anderson, ‘Computer Security Technology
Planning Study’, ESD-TR-73-51, US Air
Force Electronic Systems Division (1973)
http://csrc.nist.gov/publications/
history/index.html
[3] RJ Anderson, ‘Security Engineering – A Guide
to Building Dependable Distributed Systems’,
Wiley (2001) ISBN 0-471-38922-6
[4] RJ Anderson, “Why Cryptosystems Fail” in
Communications of the ACM vol 37 no 11
(November 1994) pp 32–40
[5] JA Bloom, IJ Cox, T Kalker, JPMG Linnartz,
ML Miller, CBS Traw, “Copy Protection for
DVD Video”, in Proceedings of the IEEE v 87
no 7 (July 1999) pp 1267–1276
[6] RM Brady, RJ Anderson, RC Ball, ‘Murphy’s
law, the ﬁtness of evolving species, and the lim-
its of software reliability’, Cambridge Univer-
sity Computer Laboratory Technical Report
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:04:10 UTC from IEEE Xplore.  Restrictions apply. 
[18] J
Spolsky,
Make Microsoft
http://joel.editthispage.com/stories/
storyReader$139
a Country?”
“Does
Issuing
Passports
at
[19] H Varian, ‘Intermediate Microeconomics – A
Modern Approach’, Fifth edition, WW Norton
and Company, New York, 1999; ISBN 0-393-
97930-0
[20] H Varian,
“Managing Online
Security
Risks”, Economic Science Column, The
New York Times, June 1, 2000, http:
//www.nytimes.com/library/financial/
columns/060100econ-scene.html
no. 476 (1999); at http;//www.cl.cam.ac.
uk/~rja14
[7] CERT, Results of the Distributed-Systems
Intruder Tools Workshop, Software Engi-
neering Institute, Carnegie Mellon Univer-
sity, http://www.cert.org/reports/dsit_
workshop-final.html, December 7, 1999
[8] W Curtis, H Krasner, N Iscoe, “A Field Study
of the Software Design Process for Large Sys-
tems”, in Communications of the ACM v 31
no 11 (Nov 88) pp 1268–1287
[9] D Davis, “Compliance Defects in Public-Key
Cryptography”, in Sixth Usenix Security Sym-
posium Proceedings (July 1996) pp 171–178
[10] “De l’inﬂuence des p´eages sur l’utilit´e des voies
de communication”, 1849, Annales des Ponts
et Chausses.
[11] European Union, ‘Network and Information
Security: Proposal for a European Policy Ap-
proach’, COM(2001)298 ﬁnal, 6/6/2001
[12] A Kerckhoﬀs, “La Cryptographie Militaire”, in
Journal des Sciences Militaires, 9 Jan 1883, pp
5–38; http://www.fabien-petitcolas.net/
kerckhoffs/
[13] DP Kormann, AD Rubin, “Risks of the Pass-
in Computer
port Single Signon Protocol”,
Networks (July 2000); at http://avirubin.
com/vita.html
[14] SJ Liebowitz, SE Margolis, “Network Exter-
nalities (Eﬀects)”, in The New Palgrave’s Dic-
tionary of Economics and the Law, MacMil-
lan, 1998; see http://wwwpub.utdallas.edu/
~liebowit/netpage.html
[15] WF Lloyd, ‘Two Lectures on the Checks to
Population’, Oxford University Press (1833)
[16] AM Odlyzko,
“Smart
and stupid net-
works: Why the
like Mi-
crosoft”, ACM netWorker, Dec 1998, pp
38–46, at http://www.acm.org/networker/
issue/9805/ssnet.html
Internet
is
[17] C Shapiro, H Varian,
‘Information Rules’,
Harvard Business School Press (1998), ISBN
0-87584-863-X
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:04:10 UTC from IEEE Xplore.  Restrictions apply.