0.001
0.064
0.000
0.000
0.013
0.056
0.026
0.001
0.000
0.025
0.000
χ2
2.563
4.709
6.247
11.999
6.864
7.469
14.326
4.194
6.260
7.357
4.998
2.930
0.022
1.689
p-value
0.278
0.030
0.012
0.001
0.009
0.006
0.000
0.041
0.012
0.007
0.025
0.087
0.881
0.194
t-value
0.75
2.29
1.71
3.372
1.942
5.466
4.012
2.601
1.965
2.292
3.601
6.425
2.697
5.799
Type of websites visited.
We further wanted to analyse if particular types of web-
sites were more prone to causing malware exposure. To this
end, we classiﬁed each website visited using the Site Safety
Center of Trend Micro [26]. We then performed a logistic
regression using the number of websites visited for each cat-
egory to determine if some categories were riskier. As shown
in Table 4, our results allowed us to identify eight risky cate-
gories: streaming media/MP3, peer-to-peer, Internet infras-
tructure, software downloads, sports, social networking, and
computers/Internet and gambling.
The general regression analysis showed quite similar re-
sults (Table 4) with nine categories showing that more fre-
quent visits led to higher number of detections. Statistically
signiﬁcant results were found for the following categories:
streaming media/MP3, infrastructure sites, software down-
load, sports, computers/Internet, gambling, pornography, il-
legal/questionable and translator/cached. Our results indi-
cate that type of website visited is a risk factor for infection.
Summary of user behaviour.
We have been able to identify three diﬀerent risk factors
related to user behaviour. We found that users who install
many applications are more prone to install infected applica-
tions, thus increasing his risk of being infected. Also, visiting
many websites could be considered as a risk factor as some
pages have malicious code that is automatically executed.
Finally, we have also conﬁrmed that certain categories of
websites place users at a greater risk of getting infected.
5. DISCUSSION
The results we have obtained and discussed here are sub-
ject to certain limitations. For one, the AV performance
evaluation is limited to only 113 detected threats, a very
small number compared to the numerous threats in the wild,
especially considering that some of these may be false posi-
tives. In addition, the false negative number might also be
underestimated due to the fact that we cannot guarantee
that our protocol caught all malware missed by the AV. In
other words, we do not have absolute ground truth.
One obvious limitations of the study concerns potential
biases in selection of the population. First of all, subjects
were located in the Greater Montr´eal area. Second, their
Figure 9: Box plot of applications installed by group
Figure 10: Box plot of websites visited by group
However, our general regression analysis did not conﬁrm
that the number of websites visited is a signiﬁcant risk fac-
tor. The associated p-value (0.09) is under 0.1, which could
suggest that the eﬀect may be a potential risk factor that
would need to be validated in a larger-scale study.
106age distribution is diﬀerent from that of Canadian Internet
users. However, unlike some of the studies discussed, ours is
not restricted to undergraduate students in the 18-24 group;
in our study, they represent only 38% of our subjects.
Another potential source of bias might have been the fact
the users knew that they were part of a computer security
experiment. This knowledge might have caused them to al-
ter their usage of their laptop. We asked that question in
the exit survey and 43 users claimed that they did not mod-
ify their behaviour. Of the other 7 users, 2 admitted having
modiﬁed their behaviour to fulﬁl experiment constraints (no
OS reinstallation, creation of partitions, etc.), 2 others ad-
mitted voluntarily not performing potentially embarrassing
activities on the computer, 1 mentioned refraining from vis-
iting secure Internet banking sites, 1 admitted forcing him-
self to use the computer more frequently, and the last one
explained that he controlled access to his computer in order
to ensure being its only user. All in all, and considering
the usage statistics we gathered show normal to high levels
of computer and web activity, and given the fact that the
laptops were sold to and were to be kept by the subjects,
we are conﬁdent this potential experimental bias did not
signiﬁcantly aﬀect our results.
Finally, even though we were able to determine several fac-
tors correlated with risk of infection, these factors in them-
selves are not suﬃcient to explain the causal link leading to
infection. To this eﬀect, a more detailed analysis of the col-
lected data is required in order to determine the sources and
means of infection for each of the 113 detected threats. Only
then will we be able to determine which of these of these fac-
tors are causes of infection, and which are consequences of
other causal common factors that were not considered in this
study, e.g. risk averseness or risk propensity of users, etc.
In terms of applications, we believe the single product
ﬁeld study methodology described in this paper is of poten-
tial utility in at least two contexts. One is that it should
be suitable for AV vendors seeking to understand how their
products perform in real-world usage and might help iden-
tify which aspects of the product (user interface, detection,
remediation, etc.) could be further improved. The other is
to help understand what characteristics of user behaviour
lead to higher risks of infection. These characteristics could
be used to improve the content and targeting of user edu-
cation and training; they could also be used by insurance
companies to assess relative risk in IT insurance policies.
Today most AV tests are lab-based and designed to iden-
tify which AV products perform better than others, whether
the purpose is to allow users to make a more educated choice
of product or to help AV vendors determine R&D and mar-
keting strategies. Given the advantages in term or ecological
validity and availability of user data of ﬁeld tests such as the
one described here, a natural question is whether it is feasi-
ble to conduct a comparative ﬁeld study of AV products to
complement the results of lab-based comparative testing.
One of the major issues in lab-based comparative AV test-
ing is ensuring that all AV products are evaluated under ex-
actly the same conditions. Not only should they be tested
in the same environment, but they should also be exposed
to the same threats at the same time. While it is relatively
easy to guarantee consistent conditions when tests are per-
formed within a controlled environment, such consistency
cannot be guaranteed in ﬁeld studies. The exposition of the
product to threats cannot be controlled as it is user driven.
On the other hand, ﬁeld studies are inherently unbiased in
that threats applied to each AV are independently “chosen”
by users who have no vested interest in the test results.
Furthermore, the law of large numbers guarantees that for
suﬃciently large populations, each product will be exposed
to a large enough sampling of threats to make the results
statistically signiﬁcant.
In order words, and to eliminate
bias in user-driven threat selection, comparative ﬁeld stud-
ies should be conducted with a large enough population and
over a large enough period of time to guarantee a statistically
signiﬁcant sample of user and malware behaviour. Based on
our experiences documented here, we postulate that such a
study should include at least 200 participants per AV tested
and last at least four months to accomplish that aim.
Thus, if we are to compare multiple AV products, we can
easily be looking at a study with a thousand or more sub-
jects.
Increasing to this scale will likely require increased
use of automation to gather user feedback. Such automa-
tion, however, is potentially beneﬁcial as well because it al-
lows for feedback to be obtained in context, i.e. just after
a user has interacted with the AV software. Care must be
taken, though, to make sure the automated questions do not
themselves overly inﬂuence the results.
It would also be nice to gather more detailed information
about user behaviour and computer state. Any such eﬀorts
will require very careful ethical review. We note that medi-
cal clinical trials involve people revealing intimate life details
and subjecting themselves sometimes to interventions that
can potentially kill them. As such, it should be possible to
create protocols that provide suﬃcient care for end users and
their computers while providing greater insight into malware
attacks and defences as they happen in the ﬁeld.
6. CONCLUSION
We have presented the results from the ﬁrst ﬁeld study of
anti-virus software performed with real users in non-laborato-
ry conditions. While the population studied was small com-
pared to medical clinical trials, it is comparable to that of
other usability studies and was suﬃcient to obtain some in-
teresting results with respect to infection risk factors.
In terms of AV performance, our results indicate that 38%
of users got exposed to a threat caught by the AV, indicating
that at least 38% of the population would have got infected
had they had no AV installed. In addition, 20% of our users
were found to have been infected by some form of undesir-
able software that was not detected by the AV. These ﬁgures
are indeed alarming, but they are comparable with ﬁgures
reported by other studies conducted by other methods (user
self-reporting and on-line scanning of machines).
In terms of risk factors, our results show that user be-
haviour is indeed signiﬁcant, thus conﬁrming previous work
in the area. However, our results show that user character-
istics such as age or gender are not signiﬁcant risk factors,
contradicting related research. We observed some surpris-
ing patterns in web browsing risk, with seemingly innocuous
categories of sites such as sports and Internet infrastruc-
ture being associated with a higher rate of infection while
more “shady” sites such those containing pornography and
illegal/questionable content were less so. And somewhat
non-intuitively, we found that computer expertise is a weak
factor increasing the risk of infection.
Beyond the contribution of these results, this work demon-
strates that ﬁeld studies are a viable alternative to lab-based
107AV evaluation. The methodological issues are comparable
to that of other computer science user studies, particularly
those in usable security. While studies comparing multiple
AV or other security products will require larger popula-
tions to get statistically signiﬁcant results, increasing use of
automation should allow such studies to be performed at
relatively modest cost. We thus believe the work presented
here illustrates the merits of future larger scale clinical trials
of AV and other security software.
7. ACKNOWLEDGMENTS
[13] F. Lalonde-Levesque. ´Evaluation d’un produit de
s´ecurit´e par essai clinique. Master’s thesis, ´Ecole
Polytechnique de Montr´eal, August 2013.
[14] F. Lalonde-Levesque, C. Davis, J. Fernandez,
S. Chiasson, and A. Somayaji. Methodology for a ﬁeld
study of anti-malware software. In Workshop on
Usable Security (USEC), pages 80–85. LNCS, 2012.
[15] F. Lalonde-Levesque, C. Davis, J. Fernandez, and
A. Somayaji. Evaluating antivirus products with ﬁeld
studies. In 22th Virus Bulletin International
Conference, pages 87–94, September 2012.
This project was funded by Trend Micro and Canada’s
[16] G. R. Milne, L. I. Labrecque, and C. Cromer. Toward
Natural Sciences and Engineering Research Council (NSERC),
through the Inter-networked Systems Security Network (ISS-
Net) Strategic Research Network, the Discovery Grant pro-
gramme, and a Canada Research Chair (fourth author).
8. REFERENCES
[1] F. Asgharpour, D. Liu, and L. J. Camp. Mental
models of computer security risks. In Workshop on the
Economics of Information Security (WEIS), 2007.
[2] AV Comparatives. File detection test of malicious
software. Technical report, AV Comparatives, 2013.
[3] D. Botta, R. Werlinger, A. Gagne, K. Beznosov,
L. Iverson, S. Fels, and B. Fisher. Towards
understanding IT security professionals and their
tools. In ACM Symposium on Usable Privacy and
Security (SOUPS), pages 100–111, 2007.
[4] J. Canto, M. Dacier, E. Kirda, and C. Leita. Large
scale malware collection: lessons learned. In IEEE
SRDS Workshop on Sharing Field Data and
Experiment Measurements on Resilience of Distributed
Computing Systems, 2008.
[5] A. De Luca, M. Langheinrich, and H. Hussmann.
Towards understanding ATM security: a ﬁeld study of
real world ATM use. In ACM Symposium on Usable
Privacy and Security (SOUPS), 2010.
[6] Eurostat. Nearly one third of internet users in the
EU27 caught a computer virus.
http://epp.eurostat.ec.europa.eu/cache/ITY_
PUBLIC/4-07022011-AP/EN/4-07022011-AP-EN.PDF,
February 2011.
[7] I. Gashi, V. Stankovic, C. Leita, and O. Thonnard. An
experimental study of diversity with oﬀ-the-shelf
antivirus engines. In IEEE International Symposium
on Network Computing and Applications (NCA), 2009.
[8] S. Gordon and R. Ford. Real world anti-virus product
reviews and evaluations: the current state of aﬀairs. In
National Information Systems Security Conference,
1996.
[9] D. Harley and A. Lee. Who will test the testers. In
18th Virus Bulletin International Conference, pages
199–207, 2008.
[10] J. Kephart and S. White. Directed-graph
epidemiological models of computer viruses. In IEEE
Symposium on Security and Privacy, 1991.
[11] S. Kondakci. Epidemic state analysis of computers
under malware attacks. Modelling Practice and
Theory, 16:571–584, 2008.
[12] P. Kosinar, J. Malcho, R. Marko, , and D. Harley. AV
testing exposed. In 20th Virus Bulletin International
Conference, 2010.
an understanding of the online consumer’s risky
behavior and protection practices. Journal of
Consumer Aﬀairs, 43:449–473, 2009.
[17] F. T. Ngo and R. Paternoster. Cybercrime
victimization: An examination of individual and
situational level factors. International Journal of
Cyber Criminology, 5(1):773–793, 2011.
[18] Panda Security Labs. Panda Labs quarterly report
January - March 2012. http://press.
pandasecurity.com/wp-content/uploads/2012/05/
Quarterly-Report-PandaLabs-January-March-2012.
pdf, 2012.
[19] PC Security Labs. Security solution review on
Windows 8 platform. Technical report, PC Security
Labs, 2013.
[20] J. A. Rode. Digital parenting: designing children’s
safety. In British Human Computer Interaction
Conference (British HCI), pages 244–251, 2009.
[21] S. Sheng, M. Holbrook, P. Kumaraguru, L. F. Cranor,
and J. Downs. Who falls for phish? A demographic
analysis of phishing susceptibility and eﬀectiveness of
interventions. In ACM Conference on Human Factors
in Computing Systems (CHI), pages 373–382, 2010.
[22] R. Shyamasundar, H. Shah, and N. Kumar. Malware:
from modelling to practical detection. Distributed
Computing and Internet Technology, pages 21–39,
2010.
[23] K. Solic and V. Ilakovac. Security perception of a
portable PC user (the diﬀerence between medical
doctors and engineers): A pilot study. In Medicinski
Glasnik, volume 6, pages 261–264, 2009.
[24] A. Somayaji, Y. Li, H. Inoue, J. Fernandez, and
R. Ford. Evaluating security products with clinical
trials. In USENIX Workshop on Cyber Security
Experimentation and Test (CSET), 2009.
[25] SurfRight. 32% of computers still infected, despite
presence of antivirus program.
http://www.surfright.nl/en/home/press/
32-percent-infected-despite-antivirus, 2009.
[26] Trend Micro. Website classiﬁcation. http:
//solutionfile.trendmicro.com/solutionfile/
Consumer/new-web-classification.html, 2012.
[27] J. Vrabec and D. Harley. Real performance? In
EICAR Annual Conference, 2010.
[28] R. Wash. Folk models of home computer security. In
ACM Symposium on Usable Privacy and Security
(SOUPS), page 11, 2010.
108