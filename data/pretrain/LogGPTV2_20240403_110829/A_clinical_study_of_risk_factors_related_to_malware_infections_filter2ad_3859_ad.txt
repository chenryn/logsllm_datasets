### Statistical Analysis Results

The following table presents the p-values and t-values for the statistical analysis conducted:

| p-value | 0.001 | 0.064 | 0.000 | 0.000 | 0.013 | 0.056 | 0.026 | 0.001 | 0.000 | 0.025 | 0.000 | 0.278 | 0.194 |
|---------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| χ²      | 2.563 | 4.709 | 6.247 | 11.999| 6.864 | 7.469 | 14.326| 4.194 | 6.260 | 7.357 | 4.998 | 2.930 | 0.022 |
| t-value | 0.75  | 2.29  | 1.71  | 3.372 | 1.942 | 5.466 | 4.012 | 2.601 | 1.965 | 2.292 | 3.601 | 6.425 | 2.697 | 5.799 |

### Website Category Analysis

To determine if specific types of websites are more likely to expose users to malware, we classified each visited website using Trend Micro's Site Safety Center [26]. We then performed a logistic regression to identify risky categories based on the number of visits. The results, presented in Table 4, indicate that the following eight categories are associated with higher malware exposure: streaming media/MP3, peer-to-peer, Internet infrastructure, software downloads, sports, social networking, computers/Internet, and gambling.

A general regression analysis (Table 4) showed similar results, with nine categories indicating that frequent visits correlate with a higher number of detections. Statistically significant results were found for the following categories: streaming media/MP3, infrastructure sites, software download, sports, computers/Internet, gambling, pornography, illegal/questionable, and translator/cached. These findings suggest that the type of website visited is a risk factor for infection.

### Summary of User Behavior

We identified three key risk factors related to user behavior:
1. **Application Installation**: Users who install many applications are more likely to install infected ones, thereby increasing their risk of infection.
2. **Website Visitation**: Frequent web browsing, especially to certain categories of sites, can be a risk factor due to the presence of malicious code.
3. **Website Categories**: Certain categories of websites, as identified above, place users at a greater risk of infection.

### Discussion

#### Limitations
- **AV Performance Evaluation**: Our study is limited by the small number of detected threats (113), which may include false positives. Additionally, the number of false negatives might be underestimated, as we cannot guarantee that our protocol caught all undetected malware.
- **Population Selection Bias**: The study participants were primarily from the Greater Montréal area, and their age distribution differs from that of the general Canadian internet user population. However, our sample includes a broader age range compared to studies focused solely on undergraduate students.
- **Experimental Bias**: Some participants knew they were part of a security experiment, which may have influenced their behavior. An exit survey revealed that most participants (43 out of 50) did not modify their behavior, while the remaining seven made minor changes. Despite this, the usage statistics indicate normal to high levels of computer and web activity, suggesting minimal impact on the results.

#### Potential for Future Research
- **Detailed Data Analysis**: Further analysis is needed to determine the causal links between the identified risk factors and actual infections. This will help distinguish between causes and consequences of infection.
- **Field Study Methodology**: The single product field study methodology can be useful for AV vendors to understand real-world performance and for identifying areas for improvement. It can also help in understanding user behaviors that lead to higher infection risks, which can inform education and training programs and IT insurance policies.

#### Comparative Field Studies
- **Feasibility**: Conducting comparative field studies of multiple AV products is feasible but requires a large enough population and a sufficient time period to ensure statistically significant results. Based on our experience, such a study should include at least 200 participants per AV and last at least four months.
- **Automation and Ethics**: Increased automation can help gather user feedback in context, but care must be taken to avoid influencing the results. Ethical considerations are crucial, especially when collecting detailed information about user behavior and computer state.

### Conclusion

Our field study, the first of its kind, provides valuable insights into AV performance and risk factors in real-world conditions. Key findings include:
- **Infection Risk**: 38% of users were exposed to threats, and 20% were infected despite having an AV installed.
- **Risk Factors**: User behavior, particularly the types of websites visited and the number of applications installed, is a significant risk factor. Surprisingly, seemingly innocuous categories like sports and Internet infrastructure were associated with higher infection rates, while more "shady" sites were less so.

This work demonstrates the viability of field studies for AV evaluation and highlights the need for larger-scale, more detailed studies to better understand and mitigate malware risks.

### Acknowledgments

This project was funded by Trend Micro and Canada’s Natural Sciences and Engineering Research Council (NSERC) through the Inter-networked Systems Security Network (ISS-Net) Strategic Research Network, the Discovery Grant program, and a Canada Research Chair.

### References

[1] F. Asgharpour, D. Liu, and L. J. Camp. Mental models of computer security risks. In Workshop on the Economics of Information Security (WEIS), 2007.
[2] AV Comparatives. File detection test of malicious software. Technical report, AV Comparatives, 2013.
[3] D. Botta, R. Werlinger, A. Gagne, K. Beznosov, L. Iverson, S. Fels, and B. Fisher. Towards understanding IT security professionals and their tools. In ACM Symposium on Usable Privacy and Security (SOUPS), pages 100–111, 2007.
[4] J. Canto, M. Dacier, E. Kirda, and C. Leita. Large scale malware collection: lessons learned. In IEEE SRDS Workshop on Sharing Field Data and Experiment Measurements on Resilience of Distributed Computing Systems, 2008.
[5] A. De Luca, M. Langheinrich, and H. Hussmann. Towards understanding ATM security: a field study of real world ATM use. In ACM Symposium on Usable Privacy and Security (SOUPS), 2010.
[6] Eurostat. Nearly one third of internet users in the EU27 caught a computer virus. http://epp.eurostat.ec.europa.eu/cache/ITY_PUBLIC/4-07022011-AP/EN/4-07022011-AP-EN.PDF, February 2011.
[7] I. Gashi, V. Stankovic, C. Leita, and O. Thonnard. An experimental study of diversity with off-the-shelf antivirus engines. In IEEE International Symposium on Network Computing and Applications (NCA), 2009.
[8] S. Gordon and R. Ford. Real world anti-virus product reviews and evaluations: the current state of affairs. In National Information Systems Security Conference, 1996.
[9] D. Harley and A. Lee. Who will test the testers. In 18th Virus Bulletin International Conference, pages 199–207, 2008.
[10] J. Kephart and S. White. Directed-graph epidemiological models of computer viruses. In IEEE Symposium on Security and Privacy, 1991.
[11] S. Kondakci. Epidemic state analysis of computers under malware attacks. Modelling Practice and Theory, 16:571–584, 2008.
[12] P. Kosinar, J. Malcho, R. Marko, , and D. Harley. AV testing exposed. In 20th Virus Bulletin International Conference, 2010.
[13] F. Lalonde-Levesque. Évaluation d’un produit de sécurité par essai clinique. Master’s thesis, École Polytechnique de Montréal, August 2013.
[14] F. Lalonde-Levesque, C. Davis, J. Fernandez, S. Chiasson, and A. Somayaji. Methodology for a field study of anti-malware software. In Workshop on Usable Security (USEC), pages 80–85. LNCS, 2012.
[15] F. Lalonde-Levesque, C. Davis, J. Fernandez, and A. Somayaji. Evaluating antivirus products with field studies. In 22th Virus Bulletin International Conference, pages 87–94, September 2012.
[16] G. R. Milne, L. I. Labrecque, and C. Cromer. Toward an understanding of the online consumer’s risky behavior and protection practices. Journal of Consumer Affairs, 43:449–473, 2009.
[17] F. T. Ngo and R. Paternoster. Cybercrime victimization: An examination of individual and situational level factors. International Journal of Cyber Criminology, 5(1):773–793, 2011.
[18] Panda Security Labs. Panda Labs quarterly report January - March 2012. http://press.pandasecurity.com/wp-content/uploads/2012/05/Quarterly-Report-PandaLabs-January-March-2012.pdf, 2012.
[19] PC Security Labs. Security solution review on Windows 8 platform. Technical report, PC Security Labs, 2013.
[20] J. A. Rode. Digital parenting: designing children’s safety. In British Human Computer Interaction Conference (British HCI), pages 244–251, 2009.
[21] S. Sheng, M. Holbrook, P. Kumaraguru, L. F. Cranor, and J. Downs. Who falls for phish? A demographic analysis of phishing susceptibility and effectiveness of interventions. In ACM Conference on Human Factors in Computing Systems (CHI), pages 373–382, 2010.
[22] R. Shyamasundar, H. Shah, and N. Kumar. Malware: from modelling to practical detection. Distributed Computing and Internet Technology, pages 21–39, 2010.
[23] K. Solic and V. Ilakovac. Security perception of a portable PC user (the difference between medical doctors and engineers): A pilot study. In Medicinski Glasnik, volume 6, pages 261–264, 2009.
[24] A. Somayaji, Y. Li, H. Inoue, J. Fernandez, and R. Ford. Evaluating security products with clinical trials. In USENIX Workshop on Cyber Security Experimentation and Test (CSET), 2009.
[25] SurfRight. 32% of computers still infected, despite presence of antivirus program. http://www.surfright.nl/en/home/press/32-percent-infected-despite-antivirus, 2009.
[26] Trend Micro. Website classification. http://solutionfile.trendmicro.com/solutionfile/Consumer/new-web-classification.html, 2012.
[27] J. Vrabec and D. Harley. Real performance? In EICAR Annual Conference, 2010.
[28] R. Wash. Folk models of home computer security. In ACM Symposium on Usable Privacy and Security (SOUPS), page 11, 2010.