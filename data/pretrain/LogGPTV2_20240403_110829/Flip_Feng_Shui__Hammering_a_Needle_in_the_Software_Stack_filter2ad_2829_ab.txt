In preliminary experiments, we also attempted to craft
an exploit to bit-ﬂip SSH’s moduli ﬁle containing Difﬁe-
Hellman group parameters and eavesdrop on the victim
USENIX Association  
25th USENIX Security Symposium  3
3
VM’s SSH trafﬁc. The maximum group size on current
distributions of OpenSSH is 1536. When we realized that
an exploit targeting such 1536-bit parameters would re-
quire a nontrivial computational effort (see Appendix A
for a formal analysis), we turned our attention to the two
more practical and powerful exploits above.
In Section 3, we present a cryptanalysis of RSA mod-
uli with a bit ﬂip as a result of our attacks. In Section 4,
we elaborate on the internals of the exploits, and ﬁnally,
in Section 5, we evaluate their success rate and time re-
quirements in a typical cloud setting.
3 Cryptanalysis of RSA with Bit Flips
RSA [49] is a public-key cryptosystem: the sender en-
crypts the message with the public key of the recipient
(consisting of an exponent e and a modulus n) and the re-
cipient decrypts the ciphertext with her private key (con-
sisting of an exponent d and a modulus n). This way
RSA can solve the key distribution problem that is inher-
ent to symmetric encryption. RSA can also be used to
digitally sign messages for data or user authentication:
the signing operation is performed using the private key,
while the veriﬁcation operation employs the public key.
Public-key cryptography relies on the assumption that
it is computationally infeasible to derive the private key
from the public key. For RSA, computing the private ex-
ponent d from the public exponent e is believed to require
the factorization of the modulus n. If n is the product of
two large primes of approximately the same size, factor-
izing n is not feasible. Common sizes for n today are
1024 to 2048 bits.
In this paper we implement a fault attack on the modu-
lus n of the victim: we corrupt a single bit of n, resulting
in n(cid:31). We show that with high probability n(cid:31) will be easy
to factorize. We can then compute from e the correspond-
ing value of d(cid:31), the private key, that allows us to forge
signatures or to decrypt. We provide a detailed analysis
of the expected computational complexity of factorizing
n(cid:31) in the following1.
RSA perform computations modulo n, where t is the
bitlength of n (t = 1 +(cid:30)log2 n(cid:29)). Typical values of t lie
between 512 (export control) and 8192, with 1024 and
2048 the most common values. We denote the ith bit of
n with n[i] (0 ≤ i  p2 > ··· ps.
In the RSA cryptosystem, the modulus n is the prod-
uct of two odd primes p1, p2 of approximate equal size,
hence s = 2, and γ1 = γ2 = 1. The encryption operation
is computed as c = me mod n, with e the public expo-
nent, and m,c ∈ [0,n− 1]) the plaintext respectively the
ciphertext. The private exponent d can be computed as
d = e−1 mod λ (n), with λ (n) the Carmichael function,
given by lcm(p1, p2). The best known algorithm to re-
cover the private key is to factorize n using the General
Number Field Sieve (GNFS) (see e.g. [42]), which has
complexity O(Ln[1/3,1.92]), with
Ln[a,b] = exp(cid:31)(b + o(1))(lnn)a(lnlnn)1−a(cid:30) .
For a 512-bit modulus n, Adrian et al. estimate that the
cost is about 1 core-year [3]. The current record is 768
bits [35], but it is clear that 1024 bits is within reach of
intelligence agencies [3].
If we ﬂip the LSB of n, we obtain n(cid:31) = n− 1, which is
even hence n(cid:31) = 2· n(cid:31)(cid:31) with n(cid:31)(cid:31) a t − 1-bit integer. If we
ﬂip the most signiﬁcant bit of n, we obtain the odd t − 1-
bit integer n(cid:31). In all the other cases we obtain an odd t-bit
integer n(cid:31). We conjecture that the integer n(cid:31)(cid:31) (for the LSB
case) and the integers n(cid:31) (for the other cases) have the
same distribution of prime factors as a random odd inte-
ger. To simplify the notation, we omit in the following
the LSB case, but the equations apply with n(cid:31) replaced
by n(cid:31)(cid:31).
Assume that an attacker can introduce a bit ﬂip to
j=1 p(cid:31) ˜γi
change n into n(cid:31) with as factorization n = ∏s(cid:31)
.
i
Then c(cid:31) = m(cid:31)e mod n(cid:31). The Carmichael function can be
computed as
λ (n(cid:31)) =lcm(cid:29)(cid:28)p(cid:31) ˜γi−1
i
· (p(cid:31)i − 1)(cid:27)(cid:26) .
If gcd(e,λ (n(cid:31))) = 1, the private exponent d(cid:31) can be found
as d(cid:31) = e−1 mod λ (n(cid:31)). For prime exponents e, the prob-
ability that gcd(e,λ (n(cid:31))) > 1 equals 1/e. For e = 3, this
means that 1 in 3 attacks fails, but for the widely used
value e = 216 + 1, this is not a concern. With the private
exponent d(cid:31) we can decrypt or sign any message. Hence
the question remains how to factorize n(cid:31). As it is very
likely that n(cid:31) is not the product of two primes of almost
equal size, we can expect that factorizing n(cid:31) is much eas-
ier than factorizing n.
Our conjecture implies that with probability 2/lnn(cid:31), n(cid:31)
is prime and in that case the factorization is trivial. If n(cid:31)
is composite, the best approach is to ﬁnd small factors
(say up to 16 bits) using a greatest common divisor oper-
ation with the product of the ﬁrst primes. The next step
is to use Pollard’s ρ algorithm (or Brent’s variant) [42]:
this algorithm can easily ﬁnd factors up to 40. . . 60 bits.
A third step consist of Lenstra’s Elliptic Curve factor-
ization Method (ECM) [38]: ECM can quickly ﬁnd fac-
tors up to 60. . . 128 bits (the record is a factor of about
4  25th USENIX Security Symposium 
USENIX Association
270 bits2). Its complexity to ﬁnd the smallest prime fac-
tor p(cid:31)s is equal to O(Lp(cid:31)s[1/2,√2]). While ECM is asymp-
totically less efﬁcient than GNFS (because of the param-
eter 1/2 rather than 1/3), the complexity of ECM depends
on the size of the smallest prime factor p(cid:31)s rather than on
the size of the integer n(cid:31) to factorize. Once a prime fac-
tor p(cid:31)i is found, n(cid:31) is divided by it, the result is tested for
primality and if the result is composite, ECM is restarted
with as argument n(cid:31)/p(cid:31)i.
The complexity analysis of ECM depends on the num-
ber of prime factors and the distribution of the size of
the second largest prime factor p(cid:31)2: it is known that its
expected valued is 0.210· t [36]. The Erdös–Kac theo-
rem [22] states that the number ω(n(cid:31)) of distinct prime
factors of n(cid:31) is normally distributed with mean and vari-
ance lnlnn(cid:31): for t = 1024 the mean is about 6.56, with
standard deviation 2.56. Hence it is unlikely that we have
exactly two prime factors (probability 3.5%), and even
less likely that they are of approximate equal size. The
probability that n(cid:31) is prime is equal to 0.28%. The ex-
pected size of the second largest prime factor p(cid:31)2 is 215
bits and the probability that it has less than 128 bits is
0.26 [36]. In this case ECM should be very efﬁcient. For
t = 2048, the probability that n(cid:31) is prime equals 0.14%.
The expected size of the second largest prime factor p(cid:31)2
is 430 bits; the probability that p(cid:31)2 has less than 228 bits
is 0.22 and the probability that it has less than 128 bits
is about 0.12. Similarly, for t = 4096, the expected size
of the second largest prime factor p(cid:31)2 is 860 bits. The
probability that p(cid:31)2 has less than 455 bits is 0.22.
The main conclusion is that if n has 1024-2048 bits,
we can expect to factorize n(cid:31) efﬁciently with a probability
of 12− 22% for an arbitrary bit ﬂip, but larger moduli
should also be feasible. As we show in Section 5, given a
few dozen templates, we can easily factorize any 1024 bit
to 4096 bit modulus with one (or more) of the available
templates.
4
Implementation
To implement dFFS reliably on Linux, we need to un-
derstand the internals of two kernel subsystems, ker-
nel same-page merging [6] (KSM) and transparent huge
pages [5], and the way they interact with each other.
After discussing them and our implementation of the
Rowhammer exploit (Sections 4.1, 4.2, and 4.3), we
show how we factorized corrupted RSA moduli in Sec-
tion 4.4 before summarizing our end-to-end attacks in
Section 4.5.
2https://en.wikipedia.org/wiki/Lenstra_elliptic_
curve_factorization
4.1 Kernel Same-page Merging
KSM, the Linux implementation of memory deduplica-
tion, uses a kernel thread that periodically scans memory
to ﬁnd memory pages with the same contents that are
candidates for merging. It then keeps a single physical
copy of a set of candidate pages, marks it read-only, and
updates the page-table entries of all the other copies to
point to it before releasing their physical pages to the
system.
KSM keeps two red-black trees, termed “stable” and
“unstable”, to keep track of the merged and candidate
pages. The merged pages reside in the stable tree while
the candidate contents that are not yet merged are in the
unstable tree. KSM keeps a list of memory areas that are
registered for deduplication and goes through the pages
in these areas in the order in which they were registered.
For each page that it scans, it checks if the stable tree
already contains a page with the same contents. If so,
it updates the page-table entry for that page to have it
point to the physical page in the stable tree and releases
the backing physical page to the system. Otherwise, it
searches the unstable tree for a match and if it ﬁnds one,
promotes the page to the stable tree and updates the page-
table entry of the match to make it point to this page. If
no match is found in either one of the trees, the page is
added to the unstable tree. After going through all mem-
ory areas, KSM dumps the unstable tree before starting
again. Further details on the internals of KSM can be
found in [6].
In the current
implementation of KSM, during a
merge, the physical page in either the stable tree or the
unstable tree is always preferred. This means that during
a merge with a page in the stable tree, the physical loca-
tion of the page in the stable tree is chosen. Similarly, the
physical memory of the page in the unstable tree is cho-
sen to back both pages. KSM scans the memory of the
VMs in order that they have been registered (i.e., their
starting time). This means that to control the location of
the target data on physical memory using the unstable
tree the attacker VM should have been started before the
victim VM. Hence, the longer the attacker VM waits, the
larger the chance of physical memory massaging through
the unstable tree.
The better physical memory massaging possibility is
through the stable tree. An attacker VM can upgrade a
desired physical memory location to the stable tree by
creating two copies of the target data and placing one
copy in the desired physical memory location and an-
other copy in a different memory location. By ensuring
that the other copy comes after the desired physical mem-
ory location in the physical address-space, KSM merges
the two pages and creates a stable tree node using the de-
sired physical memory location. At this point, any other
USENIX Association  
25th USENIX Security Symposium  5
5
Figure 2: A SO-DIMM with its memory chips.
page with the same contents will assume the same phys-
ical memory location desired by the attacker VM. For
this to work, however, the attacker needs to control when
the memory page with the target contents is created in
the victim VM. In the case of our OpenSSH attack, for
example, the attacker can control when the target page is
created in the victim VM by starting an SSH connection
using an invalid key with the target username.
For simplicity, the current version of dFFS implements
the memory massaging using the unstable tree by assum-
ing that the attacker VM has started ﬁrst, but it is trivial
to add support for memory massaging with stable tree.
Using either the stable or unstable KSM trees for mem-
ory massaging, all dFFS needs to do is crafting a page
with the same contents as the victim page and place it at
the desired physical memory page; KSM will then per-
form the necessary page-table updates on dFFS’s behalf!
In other words, KSM inadvertently provides us with ex-
actly the kind of memory massaging we need for suc-
cessful Flip Feng Shui.
4.2 Rowhammer inside KVM
Internally, DRAM is organized in rows. Each row pro-
vides a number of physical cells that store memory bits.
For example, in an x86 machine with a single DIMM,
each row contains 1,048,576 cells that can store 128 kB
of data. Each row is internally mapped to a number of
chips on the DIMM as shown in Figure 2.
Figure 3 shows a simple organization of a DRAM
chip. When the processor reads a physical memory lo-
cation, the address is translated to an offset on row i of
the DRAM. Depending on the offset, the DRAM selects
the proper chip. The selected chip then copies the con-
tents of its row i to the row buffer. The contents at the
correct offset within the row buffer is then sent on the
bus to the processor. The row buffer acts as a cache: if
the selected row is already in the row buffer, there is no
need to read from the row again.
Each DRAM cell is built using a transistor and a ca-
pacitor. The transistor controls whether the contents of
the cell is accessible, while the capacitor can hold a
charge which signiﬁes whether the stored content is a
Row i - 1
Row i
Row i + 1
Row Buffer
Figure 3: DRAM’s internal organization.
high or low bit. Since capacitors leak charge over time,
the processor sends refresh commands to DIMM rows
in order to recharge their contents. On top of the refresh
commands, every time a row is read by the processor, the
chip also recharges its cells.
As DRAM components have become smaller, they
keep a smaller charge to signify stored contents. With a
smaller charge, the error margin for identifying whether
the capacitor is charged (i.e., the stored value) is also
smaller. Kim et al. [34] showed that the smaller er-
ror margin, in combination with unexpected charge ex-
change between cells of different rows, can result in the
cell to “lose” its content. To trigger this DRAM relia-
bility issue, an attacker needs fast activations of DRAM
rows which causes a cell in adjacent rows to lose enough
charge so that its content is cleared. Note that due to the
row buffer, at least two rows need to activate one after
the other in a tight loop for Rowhammer to trigger. If
only one row is read from, the reads can be satisﬁed con-
tinually from the row buffer, without affecting the row
charges in the DRAM cells.
Double-sided Rowhammer. Previous work [51] re-
ported that if these two “aggressor” rows are selected in
a way that they are one row apart (e.g., row i − 1 and
i + 1 in Figure 3), the chances of charge interaction be-
tween these rows and the row in the middle (i.e., row i)
increases, resulting in potential bit ﬂips in that row. This
variant of Rowhammer is named double-sided Rowham-