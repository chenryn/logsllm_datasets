test cases that cannot be validated due to lower accuracy in the
grammar of PHP and Solidity. For example, we can use a
combination of the keywords {"pure", "view", "payable"}
to describe a function in Solidity as long as the same
keyword does not appear twice. However,
is
described as (pure|view|payable)* and then "pure pure" is
legal according to the grammar. Such errors will be treated as
semantic errors and cannot be fixed by POLYGLOT currently.
Code Coverage.
POLYGLOT identifies 51%, 39%, 23%,
31% more edge paths than POLYGLOT-syntax in Clang,
ChakraCore, php, and solc respectively. We show the trend
of edge coverage in 24h in Fig. 10. The increase is higher in
Clang and ChakraCore than the rest two because Clang and
ChakraCore perform heavy optimization (e.g., ChakraCore has
JIT compilation). With more semantically correct test cases,
POLYGLOT can reach deeper logic to explore the optimization
and compilation code. As shown in Table VI in Appendix,
POLYGLOT-syntax executes about 2Ã— as fast as POLYGLOT,
but it still achieves lower coverage. This result shows the
semantically correct test cases generated by POLYGLOT are
more effective in exploring the deep program states.
in BNF it
Overall, POLYGLOT outperforms POLYGLOT-syntax in the
number of unique bugs, language validity, and code coverage.
As they use the same mutation strategies, they generate test
cases with similar code structures. Under this condition, higher
language validity further improves the fuzzing performance
in various dimensions, showing the importance of semantic
correctness in testing deep logic.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:12:53 UTC from IEEE Xplore.  Restrictions apply. 
652
04812162006121824New edges (K)POLYGLOTPOLYGLOT-syntaxAFLQSYMNAUTILUSCsmithDIE04812162001224364860New edges (K)04812162006121824New edges (K)04812162001428425670New edges (K)04812162004812New edges (K)new edges than the three compared general-purpose fuzzers:
up to 442% more in Clang, 542% more in php, 1,543% more
in ChakraCore, and 3,064% more in solc. If we look at
the execution speed of AFL, QSYM, and Nautilus (Table VI
in Appendix), we can see that they all execute faster than
POLYGLOT. There are several reasons that might lead to
the performance gap. First, POLYGLOT puts more effort in
analyzing mutated test cases and fixing semantic errors so its
test case generation takes more time. Second, test cases of
higher semantic correctness lead to longer processing time,
because language errors cause the execution to bail out early.
As we see earlier, the test cases POLYGLOT generates have
a higher rate of language correctness and thus lead to slower
execution. However, POLYGLOT still achieves much higher
coverage, showing that POLYGLOT can generate high-quality
test cases to effectively explore program states with a reasonable
loss in efficiency of test case generation.
Code Coverage against Language-specific Fuzzers. As
shown in Fig. 10, POLYGLOT finds 863% more edges than
CSmith in Clang and 46% more than DIE in ChakraCore. We
should notice that CSmith and DIE actually have higher rate
of language validity (Fig. 9). We analyze the results and find
the following reasons. First, both CSmith and DIE execute
more slowly than POLYGLOT (Table VI in Appendix). This
is because CSmith and DIE adopt heavier and more complex
analyses than POLYGLOT (e.g., CSmith has 80k lines of code).
Also, as discussed before, higher language validity may lead to
slower execution. Second, CSmith generates test cases randomly
without utilizing guidance, so it might generate similar test
cases to keep exploring the same program logic. To confirm our
speculation, we perform extra evaluations by comparing CSmith
and POLYGLOT in the same conditions: we disable the feed-
back guidance of POLYGLOT, which is denoted by POLYGLOT-
nof eedback, as CSmith has no guidance; we randomly collect
5,000 test cases generated by POLYGLOT-nof eedback and
CSmith to eliminate the effect of different execution speeds.
We measure the language validity and code coverage in Clang
and repeat the process five times. POLYGLOT-nof eedback
gets 63.8% of language validity, while CSmith keeps its 100%
correctness. 5,000 test cases of POLYGLOT-nof eedback and
CSmith identify 672 and 1809 edges respectively. The results
show our assumption that higher language validity helps explore
more program states is still valid, but there are other aspects
that also affect the code coverage of fuzzing (e.g., feedback
guidance, execution speed, code structures of test cases).
Overall, POLYGLOT outperforms the three compared general-
purpose fuzzers in the evaluated metrics and also outperforms
the language-specific testing tools in the number of bugs and
edge coverage. The fuzzing effectiveness of POLYGLOT comes
from both its constrained mutation and semantic validation.
The mutation introduces various new code structures, while
the validation further improves the quality of the test cases.
Considering its generic applicability, we believe POLYGLOT
can save huge development efforts from developers and boost
the testing of language processors.
IX. DISCUSSION
We present several limitations of the current implementation
of POLYGLOT and discuss their possible solutions.
Limitation of Scope/Type System. POLYGLOT relies on
static analysis to collect type and scope information. Therefore,
the scope system of POLYGLOT only handles lexical scopes but
not dymanic scopes [69], and the information collected by type
system on dynamically-typed programming languages might
be imprecise as those types can only determined at run time.
To overcome this problem, we can utilize dymanic execution to
collect runtime information of the seed inputs before fuzzing
to assist the analysis of POLYGLOT. Also, as POLYGLOT tries
to be general, its scope and type system currently focus on
common features shared by popular languages. To support some
language-specific features, such as the ownership in Rust [68],
we need to specialize POLYGLOT case by case. This is not
our goal, but we believe that users can easily encode the extra
semantic information in the IR with semantic annotations and
extend the type and scope system according to their needs.
Inconsistent Grammar. POLYGLOT accepts a BNF grammar
as input and generates test cases that follow the grammar.
However, it still generates syntactically incorrect test cases as
shown in Fig.9, because a BNF grammar is usually a superset of
the real grammar that language processors accept. For example,
in C we can use "(int|long|void)+ identifier" to describe
the grammar of variable definitions, where "+" means one
or more. The "+" is intended for types like long int and
short int, but invalid types like long void and void int are
also valid according to the BNF grammar, which introduces
incorrect test cases. To address this problem, we plan to adopt
techniques that infer accurate grammar in runtime [40, 41].
Alternatively, we can try machine learning techniques to infer
the accurate input grammar from test cases [37, 50, 72].
Supporting More Semantics. POLYGLOT improves language
validity by ensuring that we use definitions of correct scopes
and types. We plan to support more common semantics to
further improve semantic correctness. For example, we use
a variable only after it is initialized, which can reduce the
frequency of undefined behaviors in programming languages
such as C and Pascal. Also, we can extend our symbol tables
to allow variable shadowing, which allows variables of the
same name to exist in different scopes. Furthermore, instead
of only using the type from variable definitions, we can track
when their types change with newly assigned values.
More Relaxed Mutation. POLYGLOT restricts its mutation to
preserve language correctness. However, this restriction limits
the possible definitions and code structures because we hardly
mutate IRs with definitions. We plan to relax the constraints
in the following ways. First, we can generate and insert new
definitions into the test cases. This can enrich the possible
definitions of the test cases and bring more code structures.
Second, we can perform test case minimization to remove
definitions that are not used in the program. This also increases
the possibility of mutation. For example, an IR might not be
mutable because it contains a definition. If the definition is
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:12:53 UTC from IEEE Xplore.  Restrictions apply. 
653
not used and removed during minimization, the IR becomes
mutable.
X. RELATED WORK
Generation-based Approaches. Generation-based fuzzing
can effectively test software that require structural inputs, such
as compilers and document viewers [1, 46, 51, 54, 60, 74].
They usually leverage a model or grammar, which describes the
required format of the inputs, so they can efficiently generate
test cases that pass the syntax check of the parsers. MoWF [54]
shows how to use file format information as the model to
fuzz the code beyond the parser. SQLsmith [1] generates SQL
queries utilizing SQL grammar and database schemas.
However, it can be nontrivial to get the model or grammar.
For example, the tested application is closed-source and has no
public documents. Recent works propose methods to infer the
structures of the inputs by static analysis or machine learning
on an initial seed corpus [24, 39, 45, 61]. Viide et al. [61]
proposes a model inference approach to assist fuzzing. Osbert
et al. [24] utilizes a set of test cases and the black-box access
to the tested binary to construct a context-free grammar of the
language. AUTOGRAM [45] uses dynamic taint analysis to
produce readable and accurate input grammars.
Considering the infinite input space, blindly generating test
cases is still inefficient for exploring program states. Therefore,
researchers also propose utilizing feedback from execution to
guide test case generation. Apollo [47] measures the difference
in execution time to favor generated SQL queries. Nautilus [22]
adopts code coverage as feedback to decide whether to keep
its generated test cases for mutation.
Programming language testing further requires the semantic
correctness of the inputs. Improving the semantic correctness
of the generated test case greatly helps fuzzers detect deeper
bugs in language processors [33, 74]. CodeAlchemist [43]
proposes semantics-aware assembly to synthesize semantics-
correct JavaScript test cases. CSmith [74] specializes its
analysis for C semantics and produces completely correct test
cases. Dewey et al. uses constraint logic programming to
specify syntactic features and semantic behaviors in test case
generation [33, 34], which relies on symbolic executions and
complex constraint programming.
POLYGLOT differs from the aforementioned works in the
following aspects: POLYGLOT adopts grammar for mutation
instead of pure generation so it can fully utilize coverage
guidance; POLYGLOT is generic and easy to apply on different
language processors.
Mutation-based Approaches. Mutation-based fuzzing is
effective in exploring deep logic of tested programs [38, 76, 77].
Unlike generation-based ones, mutation-based fuzzers usually
require an initial corpus to run. They perform random mutation
on existing test cases to generate new ones. If a test case
triggers a new execution path, it will be considered as useful
and saved for further mutation. In this way, fuzzers quickly
reach the deep logic and explore more program states. AFL [77]
adopts coverage feedback as guidance and performs random
bitflip mutation. As naive bitflip mutation can hardly pass
complicated checks such as magic numbers, existing fuzzers
[23, 25, 29, 30, 36, 56, 58, 76] adopt symbolic execution or
taint analysis to overcome the problem. Driller [58] performs
selective concolic execution while QSYM [76] integrates the
symbolic emulation with the native execution.
To fully utilize computation power, researchers try to find
a better feedback guidance other than naive code cover-
age [27, 28, 49, 65]. AFLGo [27] introduces directed greybox
fuzzing with the objective of reaching a given set of target
program locations efficiently. TaintScope [64] uses checksum
as feedback guidance to help fuzz file segments. SAVIOR [31]
prioritizes its concolic execution towards the locations with
potential vulnerabilities. Ijon [21] annotates the data that
represent the internal program states to guide the fuzzer.
However, the aforementioned mutation-based fuzzers are
unaware of the input structures. Their effectiveness greatly
reduces when highly structural inputs are required. Recent
fuzzers try to learn the structures [26, 32, 62, 75]. DIFUZE [32]
leverages static analysis to identify the input structures of kernel
drivers. GRIMORE [26] performs large-scale mutation using
grammar-like combinations to synthesize structured inputs
without usersâ€™ help. SLF [75] infers the relation between input
validity checks and input fields to generate valid seed inputs.
Alternatively, researchers propose advanced mutation on a
higher level than bits and bytes [42, 44, 52, 55, 63, 71, 73, 78].
LangFuzz [44] and Superion [63] accept a grammar to translate
test cases to AST and then mutate the AST. Fuzzilli [42] and
Squirrel [78] design their own IRs for mutation and semantic
analysis in JavaScript and SQL respectively.
Compared with these fuzzers, POLYGLOT adopts light-
weight analyses to efficiently generate valid inputs that pass
the syntactic and semantic checks of language processors.
Meanwhile, it keeps its generic applicability by basing its
analysis on a uniform IR.
XI. CONCLUSION
We present POLYGLOT, a generic fuzzing framework that
generates high-quality inputs for testing processors of different
programming languages. We applied POLYGLOT on 21 pro-
cessors of 9 languages and successfully identified 173 new
bugs. Our evaluation shows POLYGLOT is more effective in
testing language processors than existing fuzzers with up to
30Ã— improvement in code coverage.
ACKNOWLEDGMENT
We thank the anonymous reviewers for their helpful feed-
back. The work was supported in part by the Defense
Advanced Research Projects Agency (DARPA) under contracts
HR00112090031 and HR00112090034, the Office of Naval
Research (ONR) under grants N00014-17-1-2895, N00014-
15-1-2162, N00014-18-1-2662, N00014-16-1-2912, N00014-
16-1-2265 and N00014-17-1-2894, and the National Science
Foundation (NSF) under grant CNS-1652790. Any opinions,
findings, conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of DARPAR, NSF or ONR.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:12:53 UTC from IEEE Xplore.  Restrictions apply. 
654
REFERENCES
[1] SQLSmith. https://github.com/anse1/sqlsmith, 2016.
[2] Domato, A DOM fuzzer. https://github.com/googleprojectzero/domato,
2017.
[3] List of programming languages. https://en.wikipedia.org/wiki/List_of_
programming_languages, 2017.
[4] Dismissed PHP flaw shown
execution
https://portswigger.net/daily-swig/dismissed-php-flaw-shown-to-
pose-code-execution-risk, 2019.
code
pose
to
risk.
[5] PHP 7.0-7.4 disable_functions bypass. https://github.com/mm0r1/exploits/
tree/master/php7-backtrace-bypass, 2019.
[6] THE
OF
STORY
JIT
VULNERABILITIES
https:
//www.thezdi.com/blog/2019/4/18/the-story-of-two-winning-pwn2own-
jit-vulnerabilities-in-mozilla-firefox, 2019.
IN MOZILLA FIREFOX.
TWO WINNING
PWN2OWN
[7] Bugs in PHP interpreter found by OSS-FUZZ. https://bugs.chromium.
org/p/oss-fuzz/issues/list?q=-status%3AWontFix%2CDuplicate%20-
component%3AInfra%20PHP&can=1, 2020.
[8] Data type. https://en.wikipedia.org/wiki/Data_type, 2020.
[9] Flex, the fast lexical analyzer generator. https://github.com/westes/flex/,
2020.
[10] From Web to Pwn - FFI Arbitrary read/write without FFI::cdef or
FFI::load. http://blog.huntergregal.com/2020/07/from-web-to-pwn-ffi-
arbitrary-readwrite.html, 2020.
[11] Gnu bison. https://www.gnu.org/software/bison/, 2020.
[12] Grammars written for ANTLR v4. https://github.com/antlr/grammars-v4,
2020.
[13] JavaScript: Standard built-in objects. https://developer.mozilla.org/en-
US/docs/Web/JavaScript/Reference/Global_Objects, 2020.
[14] Memory unsafety problem in safe Rust. https://github.com/rust-lang/rust/
issues/69225, 2020.
[15] Miscompilation: for range loop reading past slice end. https://github.
com/golang/go/issues/40367, 2020.
[16] Online PHP Sandbox. https://wtools.io/php-sandbox/, 2020.
[17] OSS-Fuzz: Continuous Fuzzing for Open Source Software. https://github.
com/google/oss-fuzz, 2020.
[18] PHP: Internal (built-in) functions.
functions.internal.php, 2020.
https://www.php.net/manual/en/
[19] PHP Sandbox, Test your PHP code with this code tester. https://sandbox.
onlinephpfunctions.com/, 2020.
[20] Tiobe index for august 2020. https://www.tiobe.com/tiobe-index/, 2020.
[21] C. Aschermann, S. Schumilo, A. Abbasi, and T. Holz. Ijon: Exploring
deep state spaces via fuzzing. In 2020 IEEE Symposium on Security and
Privacy (SP), pages 1597â€“1612, 2020.
[22] Cornelius Aschermann, Tommaso Frassetto, Thorsten Holz, Patrick
Jauernig, Ahmad-Reza Sadeghi, and Daniel Teuchert. Nautilus: Fishing
for deep bugs with grammars. In NDSS, 2019.
[23] Cornelius Aschermann, Sergej Schumilo, Tim Blazytko, Robert Gawlik,
and Thorsten Holz. REDQUEEN: fuzzing with input-to-state corre-
spondence. In 26th Annual Network and Distributed System Security
Symposium, NDSS 2019, San Diego, California, USA, February 24-27,
2019. The Internet Society, 2019.
[24] Osbert Bastani, Rahul Sharma, Alex Aiken, and Percy Liang. Synthesiz-
ing program input grammars. In Proceedings of the 38th ACM SIGPLAN
Conference on Programming Language Design and Implementation,
PLDI 2017, pages 95â€“110, New York, NY, USA, 2017. Association for
Computing Machinery.
[25] Sofia Bekrar, Chaouki Bekrar, Roland Groz, and Laurent Mounier. A
taint based approach for smart fuzzing. In 2012 IEEE Fifth International
Conference on Software Testing, Verification and Validation, pages 818â€“
825. IEEE, 2012.
[26] Tim Blazytko, Cornelius Aschermann, Moritz SchlÃ¶gel, Ali Abbasi,
Sergej Schumilo, Simon WÃ¶rner, and Thorsten Holz. Grimoire: Syn-
thesizing structure while fuzzing. In Proceedings of the 28th USENIX
Conference on Security Symposium, SECâ€™19, pages 1985â€“2002, USA,
2019. USENIX Association.
[27] Marcel BÃ¶hme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik
Roychoudhury. Directed greybox fuzzing. In Proceedings of the 2017
ACM SIGSAC Conference on Computer and Communications Security,
pages 2329â€“2344, 2017.
[28] Marcel BÃ¶hme, Van-Thuan Pham, and Abhik Roychoudhury. Coverage-
based greybox fuzzing as markov chain. IEEE Transactions on Software
Engineering, 45(5):489â€“506, 2017.
[29] Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. Klee: unassisted
and automatic generation of high-coverage tests for complex systems
programs. In OSDI, volume 8, pages 209â€“224, 2008.
[30] P. Chen and H. Chen. Angora: Efficient fuzzing by principled search.
In 2018 IEEE Symposium on Security and Privacy (SP), pages 711â€“725,
2018.
[31] Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong
Zhang, Long Lu, et al. Savior: Towards bug-driven hybrid testing.
In Proceedings of the 41st IEEE Symposium on Security and Privacy