 **Is this a request for help?** (If yes, you should use our troubleshooting
guide and community support channels, see
http://kubernetes.io/docs/troubleshooting/.):
NO, it is not a request of help, this is a BUG REPORT
**What keywords did you search in Kubernetes issues before filing this one?**
(If you have found any duplicates, you should instead reply there.):
HPA not down-scaling deployment  
HPA not able to get load info
* * *
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
BUG REPORT
**Kubernetes version** (use `kubectl version`):
Tested to repeat on 1.3.5 and on 1.3.6
**Environment** :
  * **Cloud provider or hardware configuration** : GCE/GKE managed
  * **OS** (e.g. from /etc/os-release): PRETTY_NAME="Debian GNU/Linux 7 (wheezy)"
  * **Kernel** (e.g. `uname -a`): Linux gke-k8s-test1-default-pool-72fe9503-8tvm 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
  * **Install tools** : gcloud
  * **Others** : N/A
**What happened** :
We have development cluster with one of node pools of --min-nodes=2 and --max-
nodes=3.  
Normally cluster operates on 2 nodes in this pool.  
Deployment under question specifically has a nodeSelector set to allocate
nodes only to this pool.  
Deployment under question has an HPA configured with maximum pod count of 20.
We observed a spike in the load, where HPA decided to allocate all 20 pods.
Due to CPU resource constraint, it triggered an additional node allocation:
everything was as-expected at this time.
After additional node allocation, it was STILL NOT ENOUGH CPU to start all
pods, but now limit on number of nodes was reached and pods were stuck in
"Pending" state: as-expected.
What went wrong: after load was removed, HPA did not want to scale back to 1-2
pods we see normally running.
We found that HPA can not get CPU consumption info for some pods:
      4m            4m              2       {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        failed to get CPU consumption and request: metrics obtained for 20/22 of pods
      4m            4m              2       {horizontal-pod-autoscaler }                    Warning         FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: metrics obtained for 20/22 of pods
It was suspicious it reports more pods than we see with "kubectl get pod -l
...", so we found the missing ones to be "stuck" and waiting to be reaped:
    kubectl get pod -a -l run=cm-worker
    NAME                         READY     STATUS     RESTARTS   AGE
    cm-worker-2850697223-1wtmo   1/1       Running    0          11m
    cm-worker-2850697223-9evqk   1/1       Running    0          11m
    cm-worker-2850697223-dfhiu   0/1       OutOfCPU   0          5h
    cm-worker-2850697223-f6h9o   1/1       Running    0          11m
    [...]
    cm-worker-2850697223-wogui   1/1       Running    0          11m
    cm-worker-2850697223-zdxjg   0/1       OutOfCPU   0          4h
At this point, we upgraded master from 1.3.5 to 1.3.6, but it did not resolve
the situation.
Upon manual kill on these pods, cluster and HPA started to operate as
appropriate: HPA scaled down and cluster scaled down node it did not need.
**What you expected to happen** :
  1. It was expected that Kubernetes would properly discover and reap failed pods.
  2. It was expected that HPA would not consider dead pods for query of current load.
**How to reproduce it** (as minimally and precisely as possible):
We are not sure we can reproduce, so we investigated to the root cause right
in place.
**Anything else do we need to know** :
N/A