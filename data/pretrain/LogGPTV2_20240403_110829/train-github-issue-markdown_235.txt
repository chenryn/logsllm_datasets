### Bug Report

**Is this a request for help?**
No, this is not a request for help. This is a bug report. For troubleshooting and community support, please refer to the [Kubernetes Troubleshooting Guide](http://kubernetes.io/docs/troubleshooting/).

**Keywords searched in Kubernetes issues:**
- HPA not down-scaling deployment
- HPA not able to get load info

**Type of Issue:**
- **BUG REPORT**

**Kubernetes Version:**
- Tested on versions 1.3.5 and 1.3.6

**Environment:**
- **Cloud Provider or Hardware Configuration:** GCE/GKE managed
- **OS:** Debian GNU/Linux 7 (wheezy)
- **Kernel:** Linux gke-k8s-test1-default-pool-72fe9503-8tvm 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
- **Install Tools:** gcloud
- **Others:** N/A

**Description:**
We have a development cluster with a node pool configured with `--min-nodes=2` and `--max-nodes=3`. The cluster typically operates with 2 nodes in this pool. A specific deployment has a `nodeSelector` set to allocate pods only to this pool and is configured with an HPA that allows a maximum of 20 pods.

During a load spike, the HPA decided to allocate all 20 pods. Due to CPU resource constraints, an additional node was allocated, which was expected. However, even with the additional node, there was still insufficient CPU to start all pods, and the pods were stuck in a "Pending" state, as expected.

The issue occurred after the load was removed. The HPA did not scale back to the usual 1-2 pods. We observed that the HPA could not get CPU consumption information for some pods:

```
4m            4m              2       {horizontal-pod-autoscaler }                    Warning         FailedGetMetrics        failed to get CPU consumption and request: metrics obtained for 20/22 of pods
4m            4m              2       {horizontal-pod-autoscaler }                    Warning         FailedComputeReplicas   failed to get CPU utilization: failed to get CPU consumption and request: metrics obtained for 20/22 of pods
```

Upon further investigation, we found that the missing pods were "stuck" and waiting to be reaped:

```sh
kubectl get pod -a -l run=cm-worker
NAME                         READY     STATUS     RESTARTS   AGE
cm-worker-2850697223-1wtmo   1/1       Running    0          11m
cm-worker-2850697223-9evqk   1/1       Running    0          11m
cm-worker-2850697223-dfhiu   0/1       OutOfCPU   0          5h
cm-worker-2850697223-f6h9o   1/1       Running    0          11m
...
cm-worker-2850697223-wogui   1/1       Running    0          11m
cm-worker-2850697223-zdxjg   0/1       OutOfCPU   0          4h
```

Upgrading the master from version 1.3.5 to 1.3.6 did not resolve the issue. Manually killing these pods allowed the HPA and the cluster to operate correctly, scaling down as expected.

**Expected Behavior:**
1. Kubernetes should properly discover and reap failed pods.
2. The HPA should not consider dead pods when querying the current load.

**Steps to Reproduce:**
We are unsure if we can reproduce this issue, so we investigated the root cause directly in our environment.

**Additional Information:**
N/A

---

This format ensures the report is clear, concise, and provides all necessary details for the Kubernetes team to understand and address the issue.