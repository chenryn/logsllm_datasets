                   switch AS2_lan1
                   address 128.11.1.4/24
                   gateway 128.11.1.250
           }
}
switch AS3_lan1 { 
        unix_sock sock/as3_lan1
        host  planetlab8.lcs.mit.edu
}
router R2 {
      superclass router
      network eth0 {
            switch AS2_lan1
            address 128.11.1.250/24
      }
      network eth1 {
            switch AS1_AS2
            address 128.8.1.2/24
      }
      network eth2 {
            switch AS2_AS3
            address 128.9.1.2/24
      }
}
node   AS3_H1 {
           superclass slapper
           network eth0 { 
                   switch AS3_lan1
                   address 128.12.1.5/24
                   gateway 128.12.1.250
           }
}
node   AS3_H2 {
           superclass slapper
           network eth0 { 
                   switch AS3_lan1
                   address 128.12.1.6/24
                   gateway 128.12.1.250
           }
}
router R3 { 
      superclass router
      network eth0 {
            switch AS3_lan1
            address 128.12.1.250/24
      }
      network eth1 {
            switch AS2_AS3
            address 128.9.1.1/24
      }
}
Fig. 2. A sample vGround speciﬁcation
As an example, Figure 2 shows the speciﬁcation for the vGround in Figure 1. The
keyword template indicates the system template used to generate other images ﬁles.
For example, the image slapper.ext2 is used to generate the images of the following
end-hosts: AS1 H1, AS1 H2, AS2 H1, AS2 H2, AS3 H1, and AS3 H2; while
the image router.ext2 is used to generate the images of routers R1, R2, and R3.
The keyword switch indicates the creation of a network connecting various virtual
nodes. The internal keywords unix sock and udp sock indicate different network
virtualization techniques based on UNIX and INET-4 sockets, respectively. Note that
the keyword cluster is not used in this example. However, for a large-scale vGround,
it is more convenient to use cluster to specify a subnet, which has a large number of
end-hosts of similar conﬁguration.
After a vGround is created, the vGround platform also provides a collection of
toolkits to unleash the worm, collect worm infection traces, monitor worm propagation
status, and re-install or tear-down the vGround. More details will be described in
Sections 3 and 4.
Virtual Playgrounds for Worm Behavior Investigation
7
3 Design Details
3.1 Full-System Virtualization
The vGround platform leverages UML, an open-source VM implementation where the
guest OS runs directly in the unmodiﬁed user space of the host OS. Processes within a
UML-based VM are executed in the VM in exactly the same way as they are executed in
a native Linux machine. Leveraging the capability of ptrace, a special process is created
to intercept the system calls made by any process in the UML VM, and redirect them
to the guest OS kernel. Through system call interception, UML is able to virtualize
various resources such as memory, networks, and other “physical” peripheral devices.
An in-depth analysis of UML is beyond the scope of this paper and interested readers
are referred to [30].
For worm experiments, it is interesting to note that in earlier implementation of
UML termed as the “tt mode”[30], the UML guest-OS kernel needs to be present
at the last 0.5G of ptraced process address space and is writable by default. Such
placement prevents certain worms from exploiting stack-based overﬂows and therefore
limits applicability of vGrounds. In addition, the “write” permission incurs security
risk. The recent version of UML implements the “skas mode” [30], by which the tracing
process acts as a kernel-level thread, and does not impose such restriction or risk. In fact,
this explains why certain worms like Lion cannot successfully propagate in vGrounds
on top of PlanetLab, as the OS kernels of PlanetLab hosts do not usually support the
“skas” mode.
3.2 Link-Layer Network Virtualization
Figure 3 illustrates the link-layer network virtualization technique (marked within
a dotted rectangle) developed for the vGround purpose. It involves three different
entities: virtual NIC, virtual switch, and virtual cable, reﬂecting the corresponding
physical counterparts. The virtual switch, implemented as a regular server daemon, will
receive the connection requests from other virtual NICs. Each successful connection
essentially acts as a virtual cable. The virtual NIC is largely based on the original UML
implementation with certain extensions to communicate with remote virtual switch
daemons. We would like to point out that these entities are link-layer “devices”, which
Virtual End Host 0
Virtual Router 0
Virtual End Host 1
User Space
User Space
Netscape
Traceroute
iptables
route
BIND
Apache
t
e
k
c
o
S
TCP
t
e
k
c
o
S
UDP
IP
Ether
t
e
k
c
o
S
RAW
 ... 
 ... 
 ... 
t
e
k
c
o
S
TCP
t
e
k
c
o
S
UDP
IP
UNIX−socket
Ether
t
e
k
c
o
S
RAW
 ... 
 ... 
 ... 
128.10.10.2
Virtual NIC 0
Virtual Switch 0
128.10.10.1
Virtual NIC 0
Virtual NIC 1
128.10.11.1
t
e
k
c
o
S
TCP
t
e
k
c
o
S
UDP
IP
Ether
t
e
k
c
o
S
RAW
 ... 
 ... 
 ... 
Virtual NIC 0
128.10.11.2
UDP−tunnelling
Virtual Switch 1
Public IP: planetlab1.cs.purdue.edu
Public IP: planetlab2.cs.purdue.edu
Generic Linux Kernel Space / Host−OS
Link−Layer Network Virtualization
Generic Linux Kernel Space / Host−OS
Fig. 3. Illustration of link-layer network virtualization in vGround
8
X. Jiang et al.
[root@AS1_H1 /root]#traceroute  -n AS3_H2    
traceroute to AS3_H2 (128.12.1.6), 30 hops max, 40 byte packets
 1  128.10.1.250  2.342 ms  3.694 ms  2.054 ms
 2  128.8.1.2  69.29 ms  68.943 ms  68.57 ms
 3  128.9.1.1  104.556 ms  107.078 ms  109.224 ms
 4  128.12.1.6  116.237 ms  172.488 ms  108.982 ms
[root@AS1_H1 /root]#
Fig. 4. Running traceroute inside a vGround
are un-tamperable from inside a VM. This new design differentiates our technique from
other virtual networking techniques [45, 43] and is critical to the strict conﬁnement
feature of vGrounds. Also, the user-level implementation of our network virtualization
methods brings signiﬁcant deployability and topology ﬂexibility to vGrounds.
To demonstrate its effect, we again use the PlanetLab example shown in Figure 1.
In particular, we run the command traceroute in the VM AS1 H1 to ﬁnd the route to
AS3 H2. The result is shown in Figure 4. As we can see, the route is totally orthogonal
to the real Internet. More details can be found in [32].
3.3 Virtual Node Optimization and Customization
A virtual node in vGround can be one of the following: (1) an end-host exposing certain
software vulnerabilities that can be exploited by worms; (2) a router forwarding packets
according to routing and topology speciﬁcation; (3) a ﬁrewall monitoring and ﬁltering
packets based on ﬁrewall rules; or (4) a network/host-based intrusion detection system
(IDS) snifﬁng and analyzing network trafﬁc. We have applied and developed techniques
to customize VMs into different types of virtual nodes and to optimize VM space
requirement for better scalability.
The system template is a useful facility to share the common part of virtual node
images. As shown in Section 2.1, the images of the same type of virtual nodes have a
lot in common though they might have different network conﬁguration. Every image
ﬁle in vGround is composed of two parts: one is a shared system template and the
other part is node-speciﬁc. In the example in Figure 2, the Apache service started
by the script /etc/rc.d/init.d/httpd start is common among all end-host images, while
the OSPF service started by the script /etc/rc.d/init.d/ospfd start is common among
all router images. On the other hand, every virtual node has its unique networking
conﬁguration (e.g., IP address and routing table). which is speciﬁed in the node-
speciﬁc portion. To execute such speciﬁcation, we leverage the Copy-On-Write (COW)
support in UML. The COW support also helps to achieve high image generation
efﬁciency.
Another optimization is to strip down system templates. When a vGround contains
hundreds or thousands of virtual nodes, the templates need to tailored to remove
unneeded services. In worm experiments, this seems feasible because most worms
infect and spread via one or only a few vulnerabilities. For example, for the lion worm
experiment, a tailored system image of only 7M B (with BIND-8.2.1 service) can be
built. Since the system templates are just regular ext2/ext3 ﬁle systems, it is possible
to build customized system templates from scratch. However, available packaging tools
such as rpm greatly simplify this process.
Virtual Playgrounds for Worm Behavior Investigation
9
3.4 Worm Experiment Services
To provide users with worm experiment convenience, the vGround platform provides a
number of efﬁcient worm experiment services.
VM Image Generation (by VM). Every virtual node is created from its corresponding
image ﬁle containing a regular ﬁle system. However, image generation using direct ﬁle
manipulation operations such as mount and umount usually requires the root privilege
of the underlying physical host. To efﬁciently generate image ﬁles without the root
privilege, an interesting “VM generating VMs” approach is developed: the vGround
platform ﬁrst boots a specially crafted UML-based VM in each physical host, which
takes less than 10 seconds. With the support of hostfs [30], this special VM is able
to access ﬁles in the physical host’s ﬁle system with regular user privilege. Inside
the special VM, image generation will then be performed using the VM’s own root
privilege. It only takes tens of seconds for the special VM to generate hundreds of
system images. We note that the special VM will not be part of the vGround being
created. Therefore, there is no possibility of worm accessing ﬁles in the physical host.
vGround Bootstrapping and Tear-Down. The vGround platform also creates scripts
for automatic boot-up and tear-down of virtual nodes, to be triggered remotely by
the worm researcher. In particular, the sequence of virtual node boot-up/tear-down is
carefully arranged. For example, a virtual switch should be ready before the virtual
nodes it connects. In the current implementation, each virtual node is associated with a
boot-order/tear-order number to reﬂect such a sequence.
Generation and Collection of Worm Traces. Each virtual node in vGround has an
embedded logging module (included in its VM image). The logger generates worm
traces, which will be collected for analyzing different aspects of worms. The vGround
platform supports different types of logging modules. In fact, a Linux-based monitoring
or intrusion detection system, such as tcpdump [9], snort [8], and bro [1], can be readily
packaged into vGround. In addition, we have designed and implemented a kernelized
version of snort called kernort [33] that operates in the guest OS kernel of virtual nodes.
Kernort generates logs and pushes them down from the VM domain to the physical host
domain at runtime.
To collect traces generated by the hundreds and thousands of virtual nodes, manual
operation is certainly impractical, especially when the traces need to be collected
“live” at runtime. vGround automates the collection process via a toolkit that collects
traces generated by different loggers (e.g., tcpdump, kernort). Furthermore, after an
experiment, the worm’s “crime scene” in the vGround can also be inspected and
“evidence” be collected, in a way similar to VM image generation: a special VM is
quickly instantiated to mount the image ﬁle to be inspected (an ext2/ext3 ﬁle), and
“evidence” collection will be performed via the special VM.
4 Worm Experiments in vGrounds
To demonstrate the capability of vGrounds, we present in this section a number of worm
experiments we have conducted in vGround using the following real-world worms: the
10
X. Jiang et al.
Lion worm [16], Slapper worm [18], and Ramen worm [3]. The experiments span from
individual stages for worm infections (e.g., target network space selection (Section 4.1),
propagation pattern and strategy (Section 4.2), exploitation steps (Section 4.3), and ma-
licious payloads (Section 4.4)) to more advanced schemes such as intelligent payloads
(Section 4.4), multi-vector infections (Section 4.5), and polymorphic appearances (Sec-
tion 4.5). Throughout this section, we will highlight the new beneﬁts vGrounds bring
to a worm researcher, as well as interesting worm analysis results obtained during our
experiments. In fact, the worm bulletin misstatement mentioned in Section 1 was iden-
tiﬁed during these experiments. We discuss the limitations and extensions in Section 5.
The infrastructure in our experiments is a Linux cluster, which belongs to the
Computing Center of Purdue University (ITaP). Neither do we have root privilege nor
do we obtain special assistance from the cluster administrator, indicating vGround’s