fore, for very popular ﬁles, the cache hit ratio will be close to one.
For less popular updates or those with many deltas, the hit ratio
may be signiﬁcantly lower.
To evaluate the cache performance in a realistic setting with a
mixture of ﬁles, we have used the data from Set-II. Observe that
Set-II contains observations for a signiﬁcantly smaller population
than Set-I (due to observing a single server), but, on the other hand,
allows us to identify the ﬁles and deltas requested by each user.
The diversity in the set of ﬁles and deltas requested by the users
may reduce the beneﬁt of caching.
Using Set-II and assuming requests for ﬁles (and not deltas), we
have estimated S = 4.18, which translates into α = 76%. We
have also studied the impact of delta distribution on caching per-
formance (as opposed to publishing only the latest update), and
obtained values of S = 3.01, and α = 67%. The cache efﬁciency
would have been greater if we were able to monitor more servers
or collect data for larger periods of time. However, even for user
Figure 13: Aggregate server load for the distribution of one patch
with (i) client server; (ii) caching; (iii) p2p with upload time equal
to download time; (iv) p2p with upload time twice as download
time; (v) p2p with uploaded data as much as downloaded (full).
populations of tens of thousands (as opposed to hundreds of mil-
lions) that request many different sets of ﬁles and deltas, the cache
efﬁciency is still quite high.
5.2 Peer-to-Peer
We now consider P2P as a delivery mechanism for software up-
dates. In P2P , end-systems collaborate in the patch distribution,
e.g. a patch can be downloaded from any randomly chosen peer
that is online and has the ﬁle. P2P systems are already being used
to download software and they are becoming popular among con-
tent providers [2]. A P2P scheme is self-scalable, since the system
capacity increases with the number of users, copes well with ﬂash-
crowds, and, hence, it is attractive for patch dissemination.
P2P systems are very effective when a large number of users de-
mand a large ﬁle. However, P2P patch distribution systems face a
number of challenges: a) average patch size is small, which limits
the periods that a peer stays connected, b) potentially large set of
patches, which increases the diversity of peer requests, c) multiple
versions per patch, which reduces even further the opportunities for
sharing. Only if a large number of peers target the same version of
the same patch at the same time, P2P provides signiﬁcant savings
for the content provider and the end-users. Similarly, a P2P sys-
tem that favors intra-subnet connections is effective in reducing the
backbone trafﬁc (i.e.
inter-subnet trafﬁc) only if there is a large
concentration of peers in the same subnet. For instance, for a given
arrival pattern it is not clear how long peers need to stay online to
satisfy a target fraction of requests within a subnet. This will be
addressed later in the paper (see Corollary 1 in Sec. 5.3.2).
There are many challenging problems in designing a patching
distribution system based on P2P, such as guaranteeing secure and
timely patch delivery, and protecting user privacy, which are out-
side of the scope of this paper. Instead, we focus on the potential
beneﬁts for the content provider and the end users. We also do not
consider the impact of NATs, which has already been well stud-
ied [7, 9], but assume that efﬁcient NAT traversal mechanism will
be in place to permit full peer connectivity.
In Fig. 13 we show the number of requests received by the server
after the release of a patch (on 5-Jan-06) over a period of 1.5 days.
In the same ﬁgure, we show the server load when using a P2P and
a caching system. The peer arrival pattern was derived from Set-I.
We have used data from Set-II to assign download rates to the peers.
We assume that peers have asymmetric bandwidth and can serve
at a fraction, 1/4 in our experiments, of their download capacity.
01:1207:1213:1219:1201:1207:12024681012x 105Time# server requests per 5min  ServerCachingP2PP2P Stay LongerP2P FullFor the case of caches, we assume a scenario where caches have
inﬁnite resources and are placed at each IP address from which we
have received more than 25 requests (see Table 3).
From Fig. 13, we can see that current caches provide a load re-
duction in the server roughly equal to 20%. The beneﬁt of using
P2P depends on how long peers stay in the system. If peers dis-
connect from the P2P network immediately after downloading the
ﬁle, then they consume more download capacity into the system
that the upload capacity that they offer. This is due to the asym-
metric links that are common in today’s Internet; the server needs
to add extra capacity to deal with the asymmetry. However, even in
that case, P2P provides similar beneﬁts to caching. If peers stay in
the system to serve as many bytes as they have received, then, the
beneﬁt of the P2P system increases dramatically and the load at the
server becomes almost negligible (e.g. less than 10% of the load
with a client-server architecture). This is an interesting result since
it shows that for a given patch, there is enough overlap across peers
to ensure that a P2P system performs well, even if peers only stay
in the system for a small period of time.
Note that the assumption of inﬁnite cache capacity may be un-
realistic, and that our predictions of the cache performance may be
optimistic. In fact many caches may not be able to handle the load
generated by large ﬂash crowds that follow the release of a patch.
On the other hand, a P2P system is able to easily cope with this load
as presented in Fig. 13 as long as peers stay on-line a little longer
after they ﬁnish the download. Admittedly, the precise beneﬁts of
P2P will depend on the exact details of the P2P system, which we
have not modeled in our analysis; however, our results suggest that
P2P has great potential for patch dissemination.
5.3 Peer-to-Peer impact on the network
5.3.1 Peer-to-Peer with Random Matching
In P2P systems the burden of patch delivery shifts from the cen-
tral server to the individual peers and, subsequently, to their subnet
(e.g. AS, ISP). If the matching is random, then the vast majority
of the peer exchanges will be between peers of different subnets.
Assuming that each peer uploads as much as it downloads, then,
the trafﬁc that enters a subnet (total download) will equal the trafﬁc
that exits (total upload). As a result, the trafﬁc at the inter-subnet
links will double with random P2P matchings compared to the case
of downloading from the server.
We use the user arrival pattern of Set-I to quantify the amount of
inter-subnet trafﬁc. We assume that a new update is made available
at the beginning of the trace that is of interest to all users. We map
IP addresses to autonomous systems5 and use the ASes to assign
users to subnets. We assume that users stay online for a short period
of time, equal to 1min in our experiments. The 1min interval is
roughly equal to the mean online time observed in our data; later
we will consider with different online times. In Fig. 14 we plot
the normalized number of downloads from and uploads to remote
subnets as a function of the size of the subnet, i.e.
the number
of hosts in that subnet. From Fig. 14 we observe that, for large
enough subnets, the upload and download trafﬁc is linear to the size
of the subnet. Moreover, the incoming trafﬁc to a subnet equals the
outgoing, and both of them increase with the size of the subnet.
5.3.2 Peer-to-Peer with Locality
To alleviate the adverse impact of inter-subnet trafﬁc, we should
augment the peer matching algorithm to give preference to “local”
connections; in other words, peers should give priority to connec-
tions with other peers in the same subnet, instead of choosing peer
5Using data provided by the MS operations group.
Figure 14: Peer-to-peer: (Left) per subnet downloads from remote
subnets versus the number of distinct IPs per subnet; (Right) same
but for per subnet uploads from remote subnets. One day worth of
requests. Each host remains online for a ﬁxed time of 1 min.
uniformly at random [11]. We assume that peers have an efﬁcient
mechanism for discovering peers in the same subnet; the discus-
sion of such mechanisms is outside the scope of this paper. Next
we analyze and quantify the impact of P2P with locality.
We wish to derive analytical estimates for (i) the amount of data
downloaded from remote subnets, and (ii) the amount of data up-
loaded to other subnets. We perform trace-driven simulations to
validate our analytical predictions and to estimate the workload re-
duction.
In order to simplify the exposition of the analysis, and with no
loss of generality, we consider the dissemination of a single update.
In the remainder of this section, we omit the details of the analysis
details and present only the main results. The interested reader can
ﬁnd proofs in [6].
In summary, our ﬁndings are:
a) Locality decreases the amount of data uploaded per subnet by a
factor that (approximately) decreases exponentially with the mean
number of active users per subnet.
b) With locality the ratio of uploads to downloads per subnet in-
creases as a function of the size of the subnet (recall that without
locality the ratio was constant for large enough subnets).
c) Simple formulas that approximate the uploads and downloads
per subnet.
Download trafﬁc: We ﬁrst analyze the impact of P2P with locality
on the number of downloads from remote subnets. Recall that these
downloads occur when a local host cannot ﬁnd another (online)
host in the same subnet to download from.
We assume that there are Nj hosts from subnet j that are inter-
ested in the update.6 Each user query appears at a random time
from the patch release, independent of other users, with cumulative
distribution function (CDF) A(·) and density a(·).7 After query-
ing for an update, each user stays online for a random time drawn
from a CDF B(·) with the complementary CDF denoted by B(·).
The number of downloads from remote subnets depends on the two
distributions A(·) and B(·), and on the size of the subnet Nj as
follows:
THEOREM 1. The expected number of downloads, Dj(t), from
a remote subnet j in a time interval [0, t] is
Z t
`1 − B ? a(s)´Nj−1
E(Dj(t)) = Nj
a(s)ds
(1)
0
where B ? a(t) is the convolution of B(·) and a(·).8
6Since we focus on a single update, we drop the index i in Ni,j.
7For our polling users, the CDF A(·) is the residual-time distribu-
tion of the user inter-query time CDF (observed from a user query
instant).
8That is, B ? a(t) =R t
0 B(t − s)a(s)ds.
PROOF. See appendix of [6].
The ratio E(Dj(+∞))/Nj can be interpreted as the expected
fraction of queries requested from remote hosts. Observe that for
given A(·) and B(·) the fraction of remote downloads decreases
exponentially with the size of the local subnet.
We next present an estimate from the previous formula that re-
quires to know only the mean host online time and a quantity that
relates to the query arrival rate.
COROLLARY 1. Suppose a(t) is non-increasing9 with t and that
the following limit exists a∗ = supt>0 A(t)/t. Denote with b the
mean host online time and let ρ = ba∗. Then, if ρ ≤ 1, we have:
E(Dj(+∞)) ≥ Nj(1 − ρ)Nj−1.
PROOF. See appendix of [6].
If the queries arrive from always on machines, or if the inter-
poll times can be approximated by exponential distribution with
mean 1/a∗, then the parameter a∗ is precisely the per-host query
rate. Under the same assumptions, ρ can be interpreted as the mean
number of host queries that fall in a time interval of length equal to
the mean host online time. Note that for ρ (cid:28) 1, (1 − ρ)Nj−1 ≈
e−ρNj and thus for large subnets the estimate of downloads that
cannot be served locally can be approximated by Nje−ρNj .10
We assign users to subnets using the data in Set-I and simulate
peer-to-peer with locality with a) query times randomly generated
as if all users were AOM and b) with query times as observed in
our data. Due to space limitations we only present the results of
(a), but note that very similar results hold for (b). We ﬁrst calculate
the data downloaded from remote subnets and show the results in
Fig. 15 (a) and (b). Observe that the analytical results are very close
to the experimental. The number of requests that cannot be satisﬁed
locally is less than about 500 for any subnet and drops signiﬁcantly
for larger subnets.
Recall that the number of requests that cannot be satisﬁed lo-
cally for a subnet j can be estimated by Nje−ρNj . This func-
tion achieves maximum at Nj = 1/ρ and the maximum value is
(1/ρ)e−1. Given that the per-host query rate is 1 in 20 hours and
the mean host online time equals 1 min, this yields an estimate
for the maximum download from remote subnets of approximately
442, which agrees with the results of Fig. 15.
Upload trafﬁc: We now present similar analysis for the number of
remote uploads from a local subnet j with Nj hosts. This result
will provide us the estimate of the upload trafﬁc reduction for p2p
with locality. Due to the underlying sampling of the peering hosts,
the following distribution over “subnet sizes” plays a crucial role in
determining the beneﬁt of locality:
ν(i) =
number of subnets with i hosts · i
total number of hosts
.
(2)
9This assumption on the query arrival process does not hold always
in practice as we observed from our data of queries from distinct
IPs, but it does hold for queries from AOM users and, for instance,
it holds approximately for ﬂash-crowds.
10Observe that the result of Corollary 1 holds exactly for systems
where each poll counts as an update. Suppose each host polls ac-
cording to a Poisson process with a rate λ > 0. Then, the result of
= Nje−ρNj and similarly
Theorem 1 reads as limt→+∞
E(Uj (t))
∼ NjE(e−ρS),
for the result in Theorem 2, limt→+∞
where ρ is the ratio of the mean host online time to the mean host
inter-polling time.
E(Dj (t))
λt
λt
Figure 16: The CDF of ν (aggregated number of hosts in all subnets
with i hosts). (Data: Set-I)
Before stating our result, we present estimates for the distribution ν
of subnets that partition the users according to the autonomous sys-
tem they belong to, and according to the two most important bytes
of their IP address (/16 subnetting). Fig. 16 plots the empirical
CDF’s of ν using the data of Set-I.
The following result gives an estimate for the uploads from a
local subnet to remote subnets:
total number of hosts N =P
THEOREM 2. For subnet sizes Nj, bounded by an aribtrarily
ﬁxed constant, assume that the distribution ν is given by (2) for the
j Nj going to inﬁnity. The expected
uploads, Uj(t), from a local subnet j to remote subnets in a time
interval [0, t] satisﬁes
E(Uj(t))