7 LIMITATIONS
Recruitment Bias. We have not performed a complete exploration
of the ASO worker universe, and cannot claim that our participants
are a representative sample. Our recruitment process is biased, since
we selected only candidates who (1) we could reach out to, (2) re-
sponded, (3) were English speakers, (4) were willing to participate
after approving the consent form, and (5) claimed qualifying capa-
bilities (i.e., control at least 100 accounts, have at least 1 year of
ASO expertise and participated in at least 100 ASO jobs, § 4.1).
For instance, out of the 560 contacted workers, 72 replied to our
invitation, 25 quali(cid:27)ed, and 18 agreed to (cid:27)nally participate. Thus,
other workers will likely have both fewer and more capabilities than
the participants in our studies. However, from the answers and data
that we collected, we reveal previously unknown ASO strategies,
provide insights into previously proposed defenses that may be
e(cid:29)ective against them, and report Google defense vulnerabilities.
We leave for future work an investigation into the ability of
deception and more substantial (cid:27)nancial incentives, to increase
the recruitment success rate and identify novel ASO strategies. We
believe that our approach is a best e(cid:29)ort in recruiting workers,
without the use of deception.
Generalization of Results. We have used crowdsourcing sites
such as Upwork, Fiverr, Zeerk, and Peopleperhour for years, and
have found them to be reliable sources of ASO activities. In addition,
we have also found and used, after being pointed out by multiple
ASO worker contacts, large groups in Facebook, that specialize in
ASO. However, we do not claim that we were able to contact most
of the active ASO workers.
The participants in our studies also claimed expertise in fake re-
views and ratings in Google Maps, Apple Store, Amazon, Facebook
and Twitter, fake installs in Apple App Store, fake likes and follow-
ers in Facebook and Instagram, and in(cid:30)uential tweets in Twitter.
However, we did not ask participants, questions about their strate-
gies in other platforms. Thus, we do not claim that our (cid:27)ndings
apply to other sites or other types of ASO work.
Validation of Findings. Due to the sensitivity of the topic sur-
veyed and data collected, we did not perform the quantitative and
qualitative studies on the same participants. Our quantitative study
is also performed only on a subset of the accounts controlled by 39
participants. We have corroborated multiple survey answers with
quantitative measurements, and also manual veri(cid:27)cation by the
authors. In § 5.14 we describe the process we used to validate the
data collected in the quantitative study. However, several partic-
ipant claims are di(cid:28)cult to validate (e.g., team organization, size
and location, capabilities, interactions with employers, number of
devices controlled, etc). The particular nature of our participants,
makes any suspicion on these topics, legitimate.
8 CONCLUSIONS
In this paper we present results from the (cid:27)rst structured interview
study of 18 ASO workers we recruited from 5 sites, concerning
their fraud posting work in Google Play, and also a quantitative in-
vestigation with data that we collected from 39 other ASO workers
recruited from the same sites. We report Google Play vulnerabilities,
and new (cid:27)ndings about the capabilities, behaviors and detection
avoidance strategies claimed and exhibited by ASO workers.
Taken together, our study is limited by the di(cid:28)culty to recruit
participants and the sensitivity of the data. The presented (cid:27)ndings
are hence needed to be understood as situated information and not
as generalized facts. Since the nature of fraud detection research
involves elimination of risks and vulnerabilities, the presented (cid:27)nd-
ings, even with all their limitations, provide new suggestions for
future research. Further, given the observed ASO worker ability to
adapt, we believe that future research should focus on collecting
more such information from diverse sources, to extend and ensure
the continued relevance of our (cid:27)ndings.
9 ACKNOWLEDGMENTS
This research was supported in part by the NSF under grants
CNS-1840714 and CNS-1527153, NSERC grants RGPIN-2018-06185
and DGECR-2018-00103, and the Florida International University’s
Dissertation Year Fellowship.
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2451REFERENCES
[1] [n. d.]. App Reviews. http://www.app-reviews.org.
[2] [n. d.]. App Such. http://www.appsuch.com.
[3] [n. d.]. AppBrain. https://www.appbrain.com/info/about.
[4] [n. d.]. Apps Viral. http://www.appsviral.com/.
[5] [n. d.]. BlueStacks. https://www.bluestacks.com/.
[6] [n. d.]. Fake Name Generator. Your Randomly Generated Identity. https://www.
fakenamegenerator.com/.
[7] [n. d.]. Freelancer. http://www.freelancer.com.
[8] [n. d.]. Gadgets 360. https://gadgets.ndtv.com/.
[9] [n. d.]. Google Play. https://play.google.com/store?hl=en.
[10] [n. d.]. Google Play Help – Supported Devices. https://support.google.com/
[11] [n. d.]. Google Vulnerability Reward Program. https://www.google.com/about/
googleplay/answer/1727131?hl=en.
appsecurity/reward-program/.
[12] [n. d.]. GSMArena. https://www.gsmarena.com/.
[13] [n. d.]. microWorkers. https://microworkers.com/.
[14] [n. d.]. PeoplePerHour. https://www.peopleperhour.com.
[15] [n. d.]. Plan Ceibal. https://www.ceibal.edu.uy/en/institucional.
[16] [n. d.]. Rank Likes. http://www.ranklikes.com/.
[17] [n. d.]. RapidWorkers. https://rapidworkers.com/.
[18] [n. d.]. The Social Marketeers. http://www.thesocialmarketeers.org/.
[19] [n. d.]. Upwork Inc. https://www.upwork.com.
[20] [n. d.]. Write a review on Google Play. Google Play Help, https://tinyurl.com/
yc9stfy3.
[21] [n. d.]. Zeerk. https://zeerk.com/.
[22] 2018. How Arti(cid:27)cial Intelligence detects fake reviews. Scitech Europa, https:
//tinyurl.com/ycjwtmfw.
[23] Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast Algorithms for Mining
Association Rules in Large Databases. In Proceedings of the 20th International
Conference on Very Large Data Bases (VLDB ’94). Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, 487–499.
http://dl.acm.org/citation.cfm?id=
645920.672836
[24] Leman Akoglu, Rishi Chandy, and Christos Faloutsos. 2013. Opinion Fraud
Detection in Online Reviews by Network E(cid:29)ects. In Proceedings of the Seventh
International Conference on Weblogs and Social Media, ICWSM 2013, Cambridge,
Massachusetts, USA, July 8-11, 2013.
[25] Tasneem Akolawala. 2018. Google Play Store Removes Millions of Fake Reviews
and Bad Apps With New Anti-Spam System. Gadgets360, https://tinyurl.com/
ya6g2v9n.
[26] Prudhvi Ratna Badri Satya, Kyumin Lee, Dongwon Lee, Thanh Tran, and Ja-
son (Jiasheng) Zhang. 2016. Uncovering Fake Likers in Online Social Networks.
In Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management (CIKM ’16). ACM, New York, NY, USA, 2365–2370.
https://doi.org/10.1145/2983323.2983695
[27] Brian Barrett. 2018.
ble Right Out of
android-smartphones-vulnerable-out-of-the-box/.
the Box.
Millions of Android Devices are Vulnera-
Wired, https://www.wired.com/story/
[28] Alex Beutel, Wanhong Xu, Venkatesan Guruswami, Christopher Palow, and
Christos Faloutsos. 2013. CopyCatch: Stopping Group Attacks by Spotting
Lockstep Behavior in Social Networks. In Proceedings of the 22Nd International
Conference on World Wide Web (WWW ’13). ACM, New York, NY, USA, 119–130.
https://doi.org/10.1145/2488388.2488400
[29] Dearbhail Bracken-Roche, Emily Bell, Mary Ellen Macdonald, and Eric Racine.
2017. The concept of vulnerabilityin research ethics: an in-depth analysis of
policies and guidelines. Health research policy and systems 15, 1 (2017), 8.
[30] Kyle Bradshaw. 2018. Play Store’s machine learning based anti-spam system re-
moves millions of reviews per week. 9To5Google, https://tinyurl.com/ya3b6xjg.
[31] Elie Bursztein, Borbala Benko, Daniel Margolis, Tadek Pietraszek, Andy Archer,
Allan Aquino, Andreas Pitsillidis, and Stefan Savage. 2014. Handcrafted Fraud
and Extortion: Manual Account Hijacking in the Wild. In Proceedings of the 2014
Conference on Internet Measurement Conference (IMC ’14). ACM, New York, NY,
USA, 347–358. https://doi.org/10.1145/2663716.2663749
[32] Qiang Cao, Xiaowei Yang, Jieqi Yu, and Christopher Palow. 2014. Uncover-
ing Large Groups of Active Malicious Accounts in Online Social Networks.
In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Com-
munications Security (CCS ’14). ACM, New York, NY, USA, 477–488. https:
//doi.org/10.1145/2660267.2660269
[33] Kathy Charmaz and Linda Liska Belgrave. 2007. Grounded Theory. The Blackwell
Encyclopedia of Sociology (2007).
[34] Jason Cipriani. 2016. Google starts (cid:27)ltering fraudulent app reviews from Play
Store. ZDNet, https://tinyurl.com/hklb5tk.
[35] Nicholas Confessore, Gabriel Dance, Richard Harris, and Mark Hansen. 2018.
The Follower Factory. The New York Times (Jan 2018). https://www.nytimes.
com/interactive/2018/01/27/technology/social-media-bots.html
[36] Emiliano De Cristofaro, Arik Friedman, Guillaume Jourjon, Mohamed Ali Kaa-
far, and M. Zubair Sha(cid:27)q. 2014. Paying for Likes?: Understanding Facebook
Like Fraud Using Honeypots. In Proceedings of the 2014 Conference on Inter-
net Measurement Conference (IMC ’14). ACM, New York, NY, USA, 129–136.
https://doi.org/10.1145/2663716.2663729
[37] Amir Fayazi, Kyumin Lee, James Caverlee, and Anna Squicciarini. 2015. Un-
covering Crowdsourced Manipulation of Online Reviews. In Proceedings of the
38th International ACM SIGIR Conference on Research and Development in In-
formation Retrieval (SIGIR ’15). ACM, New York, NY, USA, 233–242. https:
//doi.org/10.1145/2766462.2767742
[38] G Fei, A Mukherjee, B Liu, M Hsu, M Castellanos, and R Ghosh. 2013. Exploiting
burstiness in reviews for review spammer detection. In Proceedings of the 7th
International Conference on Weblogs and Social Media, ICWSM 2013. 175–184.
[39] Fiverr. [n. d.]. https://www.(cid:27)verr.com/.
[40] Stephan Günnemann, Nikou Günnemann, and Christos Faloutsos. 2014. Detect-
ing Anomalies in Dynamic Rating Data: A Robust Probabilistic Model for Rating
Evolution. In Proceedings of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD ’14). ACM, New York, NY, USA,
841–850. https://doi.org/10.1145/2623330.2623721
[41] Nestor Hernandez, Mizanur Rahman, Ruben Recabarren, and Bogdan Carbunar.
2018. Fraud De-Anonymization for Fun and Pro(cid:27)t. In Proceedings of the 2018
ACM SIGSAC Conference on Computer and Communications Security (CCS ’18).
ACM, New York, NY, USA, 115–130. https://doi.org/10.1145/3243734.3243770
[42] Atefeh Heydari, Mohammadali Tavakoli, and Naomie Salim. 2016. Detection of
Fake Opinions Using Time Series. Expert Syst. Appl. 58, C (Oct. 2016), 83–92.
https://doi.org/10.1016/j.eswa.2016.03.020
[43] Bryan Hooi, Neil Shah, Alex Beutel, Stephan Günnemann, Leman Akoglu, Mo-
hit Kumar, Disha Makhija, and Christos Faloutsos. 2016. BIRDNEST: Bayesian
Inference for Ratings-Fraud Detection. In Proceedings of the 2016 SIAM Interna-
tional Conference on Data Mining, Miami, Florida, USA, May 5-7, 2016. 495–503.
https://doi.org/10.1137/1.9781611974348.56
[44] Bryan Hooi, Hyun Ah Song, Alex Beutel, Neil Shah, Kijung Shin, and Christos
Faloutsos. 2016. FRAUDAR: Bounding Graph Fraud in the Face of Camou(cid:30)age.
In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD ’16). ACM, New York, NY, USA, 895–904.
https://doi.org/10.1145/2939672.2939747
[45] Danny Yuxing Huang, Doug Grundman, Kurt Thomas, Abhishek Kumar, Elie
Bursztein, Kirill Levchenko, and Alex C. Snoeren. 2017. Pinning Down Abuse
on Google Maps. In Proceedings of the 26th International Conference on World
Wide Web (WWW ’17). International World Wide Web Conferences Steering
Committee, Republic and Canton of Geneva, Switzerland, 1471–1479. https:
//doi.org/10.1145/3038912.3052590
[46] Markus Jakobsson. 2018. Secure Remote Attestation. IACR Cryptology ePrint
Archive 2018 (2018), 31. http://eprint.iacr.org/2018/031
[47] Mark Jansen. 2018. Here’s how the Google Play Store detects fake ratings and
reviews. Digital Trends, https://tinyurl.com/yc5hvyq5.
[48] Meng Jiang, Peng Cui, Alex Beutel, Christos Faloutsos, and Shiqiang Yang. 2014.
Inferring Strange Behavior from Connectivity Pattern in Social Networks. In
Advances in Knowledge Discovery and Data Mining, Vincent S. Tseng, Tu Bao Ho,
Zhi-Hua Zhou, Arbee L. P. Chen, and Hung-Yu Kao (Eds.). Springer International
Publishing, Cham, 126–138.
[49] Nitin Jindal and Bing Liu. 2007. Review Spam Detection. In Proceedings of the
16th International Conference on World Wide Web (WWW ’07). ACM, New York,
NY, USA, 1189–1190. https://doi.org/10.1145/1242572.1242759
[50] Parisa Kaghazgaran, Majid Al(cid:27)(cid:27), and James Caverlee. 2019. TOmCAT: Target-
Oriented Crowd Review ATtacks and Countermeasures. In International AAAI
Conference on Web and Social Media, ICWSM.
[51] Parisa Kaghazgaran, James Caverlee, and Majid Al(cid:27)(cid:27). 2017. Behavioral Analysis
of Review Fraud: Linking Malicious Crowdsourcing to Amazon and Beyond. In
Proceedings of the Eleventh International Conference on Web and Social Media,
ICWSM 2017, Montréal, Québec, Canada, May 15-18, 2017. 560–563.
[52] Parisa Kaghazgaran, James Caverlee, and Anna Squicciarini. 2018. Combating
Crowdsourced Review Manipulators: A Neighborhood-Based Approach. In
Proceedings of the Eleventh ACM International Conference on Web Search and
Data Mining (WSDM ’18). ACM, New York, NY, USA, 306–314. https://doi.org/
10.1145/3159652.3159726
[53] Santosh KC and Arjun Mukherjee. 2016. On the Temporal Dynamics of Opin-
ion Spamming: Case Studies on Yelp. In Proceedings of the 25th International
Conference on World Wide Web (WWW ’16). International World Wide Web
Conferences Steering Committee, Republic and Canton of Geneva, Switzerland,
369–379. https://doi.org/10.1145/2872427.2883087
[54] Helen Knapman. 2019. Fake (cid:27)ve-star review farms are (cid:30)ooding Amazon with
positive comments, says Which? The Sun, https://tinyurl.com/yafthxdd.
[55] Srijan Kumar, Justin Cheng, Jure Leskovec, and V.S. Subrahmanian. 2017. An
Army of Me: Sockpuppets in Online Discussion Communities. In Proceedings of
the 26th International Conference on World Wide Web (WWW ’17). International
World Wide Web Conferences Steering Committee, Republic and Canton of
Geneva, Switzerland, 857–866. https://doi.org/10.1145/3038912.3052677
[56] Srijan Kumar and Neil Shah. 2018. False Information on Web and Social Media:
A Survey. CoRR abs/1804.08559 (2018). arXiv:1804.08559 http://arxiv.org/abs/
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom24521804.08559
[57] Jure Leskovec, Anand Rajaraman, and Je(cid:29)rey David Ullman. 2014. Mining of
Massive Datasets (2nd ed.). Cambridge University Press, New York, NY, USA.
[58] Huayi Li, Zhiyuan Chen, Arjun Mukherjee, Bing Liu, and Jidong Shao. 2015.
Analyzing and Detecting Opinion Spam on a Large-scale Dataset via Temporal
and Spatial Patterns. In Proceedings of the Ninth International Conference on Web
and Social Media, ICWSM 2015, University of Oxford, Oxford, UK, May 26-29, 2015.
634–637.
[59] Huayi Li, Geli Fei, Shuai Wang, Bing Liu, Weixiang Shao, Arjun Mukherjee,
and Jidong Shao. 2017. Bimodal Distribution and Co-Bursting in Review Spam
Detection. In Proceedings of the 26th International Conference on World Wide Web
(WWW ’17). International World Wide Web Conferences Steering Committee,
Republic and Canton of Geneva, Switzerland, 1063–1072. https://doi.org/10.
1145/3038912.3052582
[60] Shanshan Li, James Caverlee, Wei Niu, and Parisa Kaghazgaran. 2017. Crowd-
sourced App Review Manipulation. In Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
1137–1140.
[61] Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu, and Hady Wirawan
Lauw. 2010. Detecting Product Review Spammers Using Rating Behaviors.
In Proceedings of the 19th ACM International Conference on Information and