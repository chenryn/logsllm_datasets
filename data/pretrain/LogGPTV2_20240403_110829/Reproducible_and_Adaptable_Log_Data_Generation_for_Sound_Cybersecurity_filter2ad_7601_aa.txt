title:Reproducible and Adaptable Log Data Generation for Sound Cybersecurity
Experiments
author:Rafael Uetz and
Christian Hemminghaus and
Louis Hackl&quot;ander and
Philipp Schlipper and
Martin Henze
Reproducible and Adaptable
Log Data Generation for
Sound Cybersecurity Experiments
Christian Hemminghaus
Fraunhofer FKIE
Bonn, Germany
Louis Hackländer
Fraunhofer FKIE
Bonn, Germany
Rafael Uetz
Fraunhofer FKIE
Bonn, Germany
PI:EMAIL
PI:EMAIL
PI:EMAIL
Philipp Schlipper
Fraunhofer FKIE
Bonn, Germany
PI:EMAIL
Martin Henze
RWTH Aachen University
Aachen, Germany
Fraunhofer FKIE
Bonn, Germany
PI:EMAIL
ABSTRACT
Artifacts such as log data and network traffic are fundamental for
cybersecurity research, e.g., in the area of intrusion detection. Yet,
most research is based on artifacts that are not available to others
or cannot be adapted to own purposes, thus making it difficult to
reproduce and build on existing work. In this paper, we identify the
challenges of artifact generation with the goal of conducting sound
experiments that are valid, controlled, and reproducible. We argue
that testbeds for artifact generation have to be designed specifically
with reproducibility and adaptability in mind. To achieve this goal,
we present SOCBED, our proof-of-concept implementation and
the first testbed with a focus on generating realistic log data for
cybersecurity experiments in a reproducible and adaptable manner.
SOCBED enables researchers to reproduce testbed instances on
commodity computers, adapt them according to own requirements,
and verify their correct functionality. We evaluate SOCBED with an
exemplary, practical experiment on detecting a multi-step intrusion
of an enterprise network and show that the resulting experiment is
indeed valid, controlled, and reproducible. Both SOCBED and the
log dataset underlying our evaluation are freely available.
CCS CONCEPTS
• Security and privacy → Intrusion/anomaly detection and
malware mitigation; Network security; • Computing method-
ologies → Modeling and simulation.
KEYWORDS
log data, testbed, reproducibility, intrusion detection, cybersecurity
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8579-4/21/12...$15.00
https://doi.org/10.1145/3485832.3488020
ACM Reference Format:
Rafael Uetz, Christian Hemminghaus, Louis Hackländer, Philipp Schlipper,
and Martin Henze. 2021. Reproducible and Adaptable Log Data Generation
for Sound Cybersecurity Experiments. In Annual Computer Security Applica-
tions Conference (ACSAC ’21), December 6–10, 2021, Virtual Event, USA. ACM,
New York, NY, USA, 16 pages. https://doi.org/10.1145/3485832.3488020
1 INTRODUCTION
Successful cyberattacks against organizations’ computer networks
have ramped up in quantity and severity over the past years [71].
As a recent example, the 2020 SolarWinds hack alone affected thou-
sands of companies and United States government offices [20].
Timely detecting such breaches and thus stopping adversaries be-
fore they reach their final goals requires indicators of adversary
activity. Log data provide numerous valuable sources of such indica-
tors, ranging from operating system logs (e.g., Windows Event Logs
or syslogs) over service logs (e.g., Apache’s Common Logs) to dedi-
cated security system alerts (e.g., from firewalls, intrusion detection
systems (IDSs), or endpoint protection agents). Due to this large
number of different log data sources, thoughtful configuration and
analysis of these sources is vital for intrusion detection [4, 69, 74].
To aid in this task, companies employ security information and
event management (SIEM) systems, which try to tackle the task
of intrusion detection with several rule-based and anomaly-based
methods [6], but are far from being perfect [32]. Consequently,
current research is concerned with questions such as how events
or alerts can be enriched, prioritized, or correlated [43, 52, 68] as
well as how adversaries can be modeled to improve the discovery
of cyberattacks [59].
Any research dealing with these questions must be backed by
sound evaluations – which require meaningful log data to evaluate
against. Unfortunately, there is a significant lack of such data in the
scientific community [65] and freely available datasets usually do
not match researchers’ requirements for novel experiments [55].
Consequently, a recent survey by Landauer et al. [28], e.g., found
that almost 60 % of papers in the field of log clustering rely on
unpublished datasets for evaluation and the majority of those using
690ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Rafael Uetz, Christian Hemminghaus, Louis Hackländer, Philipp Schlipper, and Martin Henze
public datasets concentrates on only two of them. While unpub-
lished datasets prevent reproduction of findings, published yet
fixed datasets are limited in scope, not adaptable (e.g., w.r.t. changes
and up-to-dateness), and their creation process might lack trans-
parency [67]. As such, existing fixed datasets are often pointless for
novel experiments, as unchangeable underlying scenarios or system
configurations do not match the requirements of these experiments.
In this paper, we study the question of how to generate mean-
ingful, reproducible, and adaptable log datasets for sound scientific
cybersecurity experiments to address the lack of suitable and freely
available datasets that can be adapted to the requirements of novel
experiments. By formulating design goals for sound experiments in
log data research, we find a need for dedicated and publicly available
testbeds to efficiently generate suitable and realistic log datasets
as they would arise in a real enterprise network in an adaptable
and reproducible manner. To address this need, we present and
evaluate SOCBED, a proof-of-concept testbed allowing for a repro-
ducible and adaptable generation of log datasets. SOCBED enables
researchers to better build on existing work by reusing existing
scenarios and consequently save the effort of building own testbeds
from scratch while at the same time improving the comparability
of results.
We present the following contributions:
• We survey the field of log data generation for cybersecurity ex-
periments and find that data collection in productive networks or
proprietary testbeds leads to experiments that often lack validity,
controllability, and reproducibility.
• To remedy this situation, we derive design goals for sound ex-
periments in cybersecurity research, specifically focusing on the
generation of realistic, transparent, adaptable, replicable, and
available artifacts such as log datasets.
• To showcase and validate our approach, we present SOCBED, a
self-contained open-source cyberattack experimentation testbed
with a focus on generating reproducible and adaptable log data-
sets, e.g., for intrusion detection research. SOCBED simulates a
company network with clients, servers, and common services as
well as benign user activity and an adversary performing multi-
step attacks. The testbed can be built and run on a commodity
PC and is freely available [58].
• We use SOCBED to perform a practical attack detection experi-
ment and show that this experiment is reproducible on commod-
ity PCs, yields meaningful results, and allows for an adaptation
of log data generation in a controlled manner. The generated
dataset is also publicly available [66].
The remainder of this paper is structured as follows. In Section 2
we formulate challenges of acquiring log data for cybersecurity
research and motivate the need for reproducible and adaptable
log datasets. Subsequently, we derive design goals for sound cy-
bersecurity experiments in Section 3 and analyze to which extent
related work meets these goals in Section 4. To fill the gap of a
testbed particularly targeting the generation of reproducible and
adaptable log data, we present SOCBED in Section 5. We evaluate
the reproducibility and adaptability of SOCBED by performing an
exemplary experiment in Section 6, before discussing SOCBED’s
design decisions and resulting limitations in Section 7. Section 8
concludes this paper.
2 LOG DATA IN CYBERSECURITY RESEARCH
Log data are indispensable and extremely valuable sources for the
timely detection of network breaches [69]. Consequently, they pro-
vide the foundation for various streams of research, e.g., w.r.t. en-
richment, prioritization, and correlation of events [43, 52, 68] or
the realistic modeling of adversaries [59]. However, although being
required as foundation for sound evaluations, there is a significant
lack of meaningful log data in the scientific community [9, 55, 65].
In the following, we discuss why collecting sound log data from pro-
ductive networks is difficult and why fixed datasets generated from
proprietary testbeds have several drawbacks as well (Section 2.1).
Subsequently, we argue that the resulting limitations are an obsta-
cle for the reproducibility of log data research and for building on
existing work (Section 2.2) and argue how adaptable log datasets
can remedy this situation (Section 2.3).
2.1 Challenges of Acquiring Log Data
Log data as required for intrusion detection research are usually
generated by assets as they are typically found in company net-
works, i.e., operating systems, services, and dedicated security prod-
ucts such as firewalls, network-based intrusion detection systems
(NIDSs), and endpoint protection agents. Depending on the desired
experiment, logs of benign user activity and/or realistic cyberat-
tacks are required. To achieve this goal, log data acquisition can be
done in two fundamentally different ways: Collection in a produc-
tive network with real users or generation in a dedicated, controlled
lab environment. Both sources come with specific advantages and
disadvantages, which are discussed in the following.
While collecting real-world log data in a productive network has
the obvious advantage of providing realistic data, it also comes with
significant drawbacks: Most importantly, the variety of successful
cyberattacks may be too small for meaningful evaluations because
the productive network is either not vulnerable to the attacks, their
implementation is too costly, or attack execution is deemed too
dangerous and thus not permitted. Likewise, confidentiality or
privacy issues often forbid the publication of collected data or
necessitate extensive anonymization, severely reducing utility for
other researchers [65]. Furthermore, as there is only one instance
of each productive network and its state always changes, collected
data are neither replicable at a later point in time nor reproducible
by other researchers. This leads to a lack of controllability: It is not
possible to examine the effect of a changed parameter that affects
log data generation (e.g., configuration change) in an isolated way.
Finally, the adaptability of the productive network is usually limited.
It might not be possible to add, remove, or exchange certain assets as
required for an evaluation. In particular, other researchers without
access to the network cannot perform configuration changes that
might be required for subsequent experiments.
Consequently, researchers often rely on dedicated lab testbeds
for log data acquisition to avoid these issues [12]. Log datasets gen-
erated by such testbeds are usually not affected by confidentiality
or privacy concerns and can therefore be made available. However,
typically only datasets are published, but the testbeds with which
they were generated are not [9, 42, 55]. As all datasets are created
with a specific use case in mind (e.g., IDS evaluation), they often
do not fit the requirements of other researchers even though the
691Reproducible and Adaptable Log Data Generation for Sound Cybersecurity Experiments
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
underlying testbed could be adapted to generate the desired data
if it was publicly available. Thus, researchers who require slightly
different data often have to create an own testbed instead of using
an existing dataset.
2.2 Missing Reproducibility and Adaptability
Although being one of the most important properties of scientific
experiments [47], reproducibility is often limited in current log data
research [28]. Besides being an integral part of sound experiments,
reproducibility also facilitates adaptability and thus spurs further
research: With the data and information required to reproduce
others’ findings, experiments can be (slightly) adapted to study
novel research questions. Specifically focusing on the research
area of intrusion detection, numerous works [25] present and use
once generated datasets without the ability to reproduce or adapt
them, let alone the option to adjust them to other use cases. To
illustrate this issue, Sharafaldin et al. [55] provide an overview over
publicly available datasets ranging from network packet to system
call captures generated for IDS training. For all of these datasets,
the testbeds used to generate the dataset are not made available,
thus preventing reproduction or adaptation.
Even worse, for a large batch of work on log data and intrusion
detection, the underlying log data are not (publicly) available at
all, rendering the reproduction (and thus also extension) of their
experiments impossible. Examples include MalRank [43], Smoke
Detector [52], and Beehive [74]. We assume that the main reason for
not disclosing these datasets are confidentiality or privacy concerns.
2.3 The Case for Adaptable Log Datasets
Given the missing reproducibility and adaptability of existing log
datasets, using these as a basis for novel experiments is often mean-
ingless because the datasets’ underlying scenarios or configurations
differ from what is required for novel experiments. Common issues
include outdated scenario components (e.g., obsolete operating sys-
tems or attacks that are no longer prevalent in the real world), a
scenario not matching the new experiment’s context (e.g., Windows
vs. Linux clients/servers or different security measures in place),
or a logging configuration not producing the logs required for the
evaluation (e.g., logs produced by an up-to-date Sysmon version
are required as input for an IDS). We encountered such issues in
several experiments in the context of intrusion detection, leaving
us with no other choice than building testbeds from scratch to gen-
erate log data instead of using existing log datasets. Given these
problems with missing reproducibility and adaptability of log data
research, especially in the context of intrusion detection, we set out
to remedy this situation with our contributions in this paper.
To achieve this goal, we are convinced that long-ranging, usable
log datasets for sound cybersecurity research must be subject to fre-
quent updates and modifications by different groups of researchers.
This can only be achieved by an open-source testbed specifically
built for easy reproducibility and adaptability, thus allowing a large
number of researchers to reproduce log datasets and adapt, e.g., the
logging configuration, while retaining the same scenario. Vice versa,
the scenario (e.g., systems, services, or attacks) can be adapted or
extended while still producing the same types of log data.
Figure 1: Conducting sound experiments imposes require-
ments on the used artifacts (e.g., log data), which in turn im-
pose requirements on testbeds used for their generation.
3 DESIGN GOALS FOR SOUND
CYBERSECURITY EXPERIMENTS
Our analysis of the use of log data in cybersecurity research identi-
fies several pitfalls when performing experiments based on log data,
especially w.r.t. reproducibility and adaptability. In the following,
we summarize these issues and derive design goals for sound cyber-
security experiments. In this process, we abstract from log data to
artifacts in general, but still focus on log data in the examples given.
As summarized in Figure 1, we start by discussing three vital prop-
erties of scientific experiments in general: validity, controllability,
and reproducibility (Section 3.1). From these properties, we derive
design goals for artifacts used by such experiments (Section 3.2).
Finally, we derive design goals for testbeds for artifact generation
(Section 3.3). As a result, we argue that sound experiments that
require artifacts such as log data benefit strongly from testbeds
that allow for (1) realistic scenarios and (2) deterministic activity
while being easy to use for other researchers because they are (3)
open source, (4) can be run on commodity hardware, and (5) pro-
vide self-tests to verify correct functionality after installation or
adaptation.
3.1 Properties of Sound Experiments
Cybersecurity is both an engineering discipline and a science [47].
In both fields, sound experiments are fundamentally important to
test hypotheses and thus advance knowledge [21, 39]. While there
is no generally accepted definition of soundness [21], literature on
the design of experiments often states that a sound experiment must
allow for valid conclusions and be controlled and reproducible [8,
39, 47]. We now describe why these properties are important in
general and for log data-based experiments in particular.
Valid. Validity is the extent to which a concept, conclusion, or
measurement is well-founded and rules out alternative explana-
tions [49]. We can distinguish internal validity (i.e., the confidence
in the conclusions drawn in the strict context of an experiment)
from external validity (i.e., the degree to which conclusions can be
generalized and/or applied to the real world) [35]. In the field of
cybersecurity, there is an ongoing debate on the external validity
of research that is based on inappropriate datasets such as the old
and overused DARPA/KDD’99 datasets [42, 55, 60].
are available as open sourcevalidcontrolledreproducibleavailablerealisticemulate real-world scenariotransparentadaptablereplicablerun on commodity hardwareexecute deterministic activityprovide self-testssoundexperimentsartifacts forexperimentstestbeds forartifact generationsupports692ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Rafael Uetz, Christian Hemminghaus, Louis Hackländer, Philipp Schlipper, and Martin Henze
Controlled. Each experiment can be seen as a process that trans-
forms some input (e.g., attacks against a computer network) into
some output (e.g., intrusion alerts) [39]. There are usually multiple
variables that affect the outcome, some of which can be controlled
by the experimenter (e.g., timing of the attacks) while others can
not (e.g., activity of background processes). Controllability is the
extent to which variables can be controlled. Ideally, an experiment
can be repeated multiple times while changing exactly one variable
at a time to reliably examine cause-effect relationships [47].
Reproducible. An experiment is called reproducible if its results
can be reproduced by other researchers [3]. This serves multiple
purposes: (1) Results become more trustworthy when they are
independently verified by other researchers [39], (2) building on
existing research and advancing the state of the art often requires
reproduction of existing results in the first place [3], and (3) building
on original experiments also makes results directly comparable.
3.2 Research Artifacts for Sound Experiments
To avoid the previously discussed pitfalls, we now describe five
properties that artifacts used in experiments should exhibit to facili-
tate sound experiments (cf. Figure 1).
Realistic. For experiment results to be transferable to real-world
use cases and thus be externally valid, the used artifacts must re-
semble key properties of real-world data [60]. The notion of realism
depends strongly on the context of a concrete experiment. For exam-
ple, most IDSs only consider the contents of network packets, not
their timing, so a dataset with realistic contents but unrealistic tim-
ing would still be valid. On the contrary, some IDSs might consider
timing, making the same dataset invalid for their evaluation.
Transparent. All relevant details on the contents of artifacts
should be made transparent to other researchers. A failure to do
so can lead to incorrect assumptions about capabilities and limita-
tions of a dataset and ultimately to invalid conclusions [3]. Non-
transparent artifacts can also lead to uncontrolled experimental
behavior in case changed variables cause unexpected side effects.
Adaptable. Adaptability is the extent to which artifacts such as
log data can be recreated with changed parameters (e.g., updated
cyberattacks). Adaptability supports validity because it allows ex-
perimenters to adapt artifacts depending on the needs of a new
experimental context (e.g., re-running an experiment with an at-
tacked system updated from Windows 8 to Windows 10 to regain
external validity for real-world application).
Replicable. Replicability is the extent to which artifacts can be
recreated under the same conditions [3], e.g., running the same
testbed on the same host. It is a prerequisite for controlled experi-
ments as it allows for multiple iterations with changed parameters