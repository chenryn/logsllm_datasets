Random Attack
ResNet-20 Baseline
Pre-deﬁned Shufﬂe
Random Shufﬂe
90.77
90.77
90.77
90.77
87.9
10.94
11.0
53.3
180
28
26
180
Next, we discuss the case with shufﬂing the weight pack-
age for every transmission round as a very strong obfuscation.
The effect of such a strong obfuscation scheme can have three
possible implications. First, a randomly shufﬂed weight trans-
mission will fail to defend our attack in a white-box setting
as the attacker has full knowledge of the DNN and data trans-
mission scheme. Second, in a black-box setting, as shown in
Tab. 8, this defense will greatly limit the efﬁcacy of our at-
tack, requiring a larger amount of attack iterations (e.g., 180)
to degrade the accuracy to 53.3 %. But the attack remains
more successful than a random AWD attack with no search-
ing algorithm. It aligns with the recent work of adversarial
input attack [23], where the authors argue that obfuscation
based on an under-lying random function as defense may not
completely defend a progressive adversarial attack. Given a
large amount of model query, the progressive evolutionary
algorithm-based attack (i.e. our case) could estimate the ef-
fect and distribution of the randomness to improve the attack
efﬁcacy in comparison to a random attack. Moreover, ran-
domly shufﬂing data transmission every time would require
additional header information to synchronize the sequence
of weights at the receiver end. A recent work in [101] has
demonstrated random shufﬂing may cost up to 9 × energy
in-efﬁciency and 3.7 × lesser amount of throughput. Thus, an
effective defense scheme will always come at the expense of
additional (i.e., memory, speed & power) overhead.
Power-based side-channel analysis to detect Deep-Dup.
Here we discuss the feasibility of using power-based side-
channel analysis to detect Deep-Dup. The success of such
detection should rely on the ability to distinguish between
these two cases: 1) Normal case: two benign users execute
their applications simultaneously, and 2) Attack case: two
users share the FPGA resources, where one of them apply
Deep-Dup to attack the other one. Since it is impractical to
measure the real-time power trace in a cloud-FPGA with an
oscilloscope, an on-chip power sensor (e.g., TDC sensor) will
be the only option. As shown in Fig.4, similar as AWD attack,
our measured power trace of a benign user (e.g., YOLOv2)
also incurs large power glitches. More importantly, we did
not observe any AWD attack power glitch has a larger magni-
tude than that of benign user-YOLOV2. Instead, it is smaller
for most of the time. Therefore, the glitches caused by AWD
will be easily obfuscated. Further, it is difﬁcult to distinguish
AWD power glitches in the following practical scenarios: i)
Most cloud-FPGA users prefer to run compute-intensive ap-
plications, which generates many power glitches; ii) When
triggered, each fault injection by AWD only lasts for a short
time period (e.g., 50ns) and is disabled for most of the time;
iii) Faults are only injected at attacker’s will, i.e., without
a ﬁxed pattern to check. In other words, it is of different
challenges to use such power-based side-channel analysis for
defense and attack, i.e., the defender should acquire ultra-
high-resolution side-channel information to identify the ma-
licious power glitches from the noisy power background by
the compute-intensive application, e.g., the DNN execution;
while the attacker only needs to identify the temporal range
for the DNN weight transmission. More severely, an attacker
may even choose to inject faults in a more stealthy manner,
i.e., while the victim DNN model itself is generating lots of
power glitches, to exacerbate the overall voltage drop [102].
Therefore, we argue that it is extremely difﬁcult, if not impos-
USENIX Association
30th USENIX Security Symposium    1931
sible, to detect the proposed Deep-Dup attacks with power
anomaly in a multi-tenant FPGA.
research groups. IEEE Signal Processing Magazine,
29(6):82–97, 2012.
9 Conclusion
In this work, we study the security of DNN acceleration in
multi-tenant FPGA. For the ﬁrst time, we exploit this novel
attack surface where the victim and the attacker share the
same FPGA hardware sources. Our proposed Deep-Dup at-
tack framework is validated with a multi-tenant FPGA proto-
type, as well as some popular DNN architectures and datasets.
The experimental results demonstrate that the proposed attack
framework can completely deplete DNN inference perfor-
mance to as low as random guess or attack a speciﬁc target
class of inputs. It is worth mentioning that our attack suc-
ceeds even assuming the attacker has no knowledge about the
DNN inference running in FPGA, i.e. black-box attack. A
malicious tenant with such limited knowledge can implement
both targeted and un-targeted malicious objectives to cause
havoc for a victim user. Finally, we envision that the proposed
attack and defense methodologies will bring more awareness
to the security of deep learning applications in the modern
cloud-FPGA platforms.
Acknowledgement: The authors thank the designated shep-
herd (Dr. Nele Mentens) for her guidance, and the anony-
mous reviewers for their valuable feedback. This work is
supported in part by the National Science Foundation under
Grant No.2019548 and No.2043183.
References
[1] Yann LeCun and Yoshua Bengio. Convolutional
networks for images, speech, and time series. The
handbook of brain theory and neural networks,
3361(10):1995, 1995.
[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 248–255. IEEE,
2009.
[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information
processing systems, pages 1097–1105, 2012.
[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 770–778, 2016.
[5] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Se-
nior, Vincent Vanhoucke, Patrick Nguyen, and Tara N
Sainath. Deep neural networks for acoustic model-
ing in speech recognition: The shared views of four
[6] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
Deep learning. nature, 521(7553):436, 2015.
[7] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank
Seide, Mike Seltzer, Andreas Stolcke, Dong Yu,
and Geoffrey Zweig. Achieving human parity in
conversational speech recognition.
arXiv preprint
arXiv:1610.05256, 2016.
[8] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rashidi.
Deep ehr: A survey of recent advances in deep learning
techniques for electronic health record (ehr) analysis.
IEEE Journal of Biomedical and Health Informatics,
22(5):1589–1604, Sep. 2018.
[9] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and
Yibo Xue. Droid-sec: Deep learning in android mal-
ware detection. In Proceedings of the 2014 ACM Con-
ference on SIGCOMM, SIGCOMM ’14, pages 371–
372. ACM, 2014.
[10] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianx-
iong Xiao. Deepdriving: Learning affordance for direct
perception in autonomous driving. In Computer Vision
(ICCV), 2015 IEEE International Conference on, pages
2722–2730. IEEE, 2015.
[11] M. Teichmann, M. Weber, M. Zöllner, R. Cipolla, and
R. Urtasun. Multinet: Real-time joint semantic reason-
ing for autonomous driving. In 2018 IEEE Intelligent
Vehicles Symposium (IV), pages 1013–1020, June 2018.
[12] Altera and ibm unveil fpga-accelerated power systems.
https://www.hpcwire.com/off-the-wire/al
tera-ibm-unveil-fpga-accelerated-power-s
ystems/.
[13] Here’s what an intel broadwell xeon with a built-in
fpga looks like, 2016. https://www.theregister.
co.uk/2016/03/14/intel_xeon_fpga/.
[14] Inside the microsoft fpga-based conﬁgurable cloud,
https://azure.microsoft.com/en-us/
2017.
resources/videos/build-2017-inside-the-m
icrosoft-fpga-based-configurable-cloud/.
[15] Enable faster fpga accelerator development and deploy-
https://aws.amazon.c
ment in the cloud, 2020.
om/ec2/instance-types/f1/.
[16] George Provelengios, Daniel Holcomb, and Russell
Tessier. Power wasting circuits for cloud fpga at-
tacks. In 30th International Conference on Field Pro-
grammable Logic and Applications (FPL), 2020.
1932    30th USENIX Security Symposium
USENIX Association
[17] Yue Zha and Jing Li. Virtualizing fpgas in the cloud. In
Proceedings of the Twenty-Fifth International Confer-
ence on Architectural Support for Programming Lan-
guages and Operating Systems, pages 845–858, 2020.
[28] Fan Yao, Adnan Rakin, and Deliang Fan. Deepham-
mer: Depleting the intelligence of deep neural network-
sthrough targeted chain of bit ﬂips. In 29th {USENIX}
Security Symposium ({USENIX} Security 20), 2020.
[18] Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial ex-
amples. arXiv preprint arXiv:1412.6572, 2014.
[19] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083, 2017.
[20] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199, 2013.
[21] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In
International Conference on Learning Representations,
2018.
[22] Nicholas Carlini and David Wagner. Towards evaluat-
ing the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 39–57.
IEEE, 2017.
[29] Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu.
Fault injection attack on deep neural network. In 2017
IEEE/ACM International Conference on Computer-
Aided Design (ICCAD), pages 131–138. IEEE, 2017.
[30] Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao,
Chaitali Chakrabarti, and Deliang Fan. T-bfa: Tar-
geted bit-ﬂip adversarial weight attack. arXiv preprint
arXiv:2007.12336, 2020.
[31] Jason Cong, Zhenman Fang, Muhuan Huang, Peng
Wei, Di Wu, and Cody Hao Yu. Customizable comput-
ing—from single chip to datacenters. Proceedings of
the IEEE, 107(1):185–203, 2018.
[32] Xilinx: Socs, mpsocs
and rfsocs, 2020.
https://www.xilinx.com/products/silico
n-devices/soc.html.
[33] Intel: Soc fpgas, 2020. https://www.intel.com/
content/www/us/en/products/programmable/so
c.html.
[23] Anish Athalye, Nicholas Carlini, and David Wagner.
Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. arXiv
preprint arXiv:1802.00420, 2018.
[34] Mark Zhao and G Edward Suh. Fpga-based remote
power side-channel attacks. In 2018 IEEE Symposium
on Security and Privacy (SP), pages 229–244. IEEE,
2018.
[24] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan
Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang.
Trojaning attack on neural networks. In 25nd Annual
Network and Distributed System Security Symposium,
NDSS 2018, San Diego, California, USA, February 18-
221, 2018. The Internet Society, 2018.
[25] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth
Garg. Badnets: Identifying vulnerabilities in the ma-
chine learning model supply chain. arXiv preprint
arXiv:1708.06733, 2017.
[26] Sanghyun Hong, Pietro Frigo, Yi˘gitcan Kaya, Cristiano
Giuffrida, and Tudor Dumitras, . Terminal brain damage:
Exposing the graceless degradation in deep neural net-
works under hardware fault attacks. In 28th {USENIX}
Security Symposium ({USENIX} Security 19), pages
497–514, 2019.
[35] Jonas Krautter, Dennis RE Gnad, and Mehdi B Tahoori.
Fpgahammer: remote voltage fault attacks on shared
fpgas, suitable for dfa on aes.
IACR Transactions
on Cryptographic Hardware and Embedded Systems,
pages 44–68, 2018.
[36] Ilias Giechaskiel, Kasper B Rasmussen, and Ken Eguro.
Leaky wires: Information leakage and covert commu-
nication between fpga long wires. In Proceedings of
the 2018 on Asia Conference on Computer and Com-
munications Security, pages 15–27. ACM, 2018.
[37] Yukui Luo and Xiaolin Xu. Hill: A hardware isolation
framework against information leakage on multi-tenant
fpga long-wires. In 2019 International Conference on
Field-Programmable Technology (ICFPT), pages 331–
334. IEEE, 2019.
[27] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Bit-
ﬂip attack: Crushing neural network with progressive
bit search. In The IEEE International Conference on
Computer Vision (ICCV), October 2019.
[38] Dina Mahmoud and Mirjana Stojilovi´c. Timing vio-
lation induced faults in multi-tenant fpgas. In 2019
Design, Automation & Test in Europe Conference &
Exhibition (DATE), pages 1745–1750. IEEE, 2019.
USENIX Association
30th USENIX Security Symposium    1933
[39] George Provelengios, Chethan Ramesh, Shivukumar B
Patil, Ken Eguro, Russell Tessier, and Daniel Hol-
comb. Characterization of long wire data leakage
in deep submicron fpgas.
In Proceedings of the
2019 ACM/SIGDA International Symposium on Field-
Programmable Gate Arrays, pages 292–297. ACM,
2019.
[40] Machine learning on aws, 2020.
https://aws.am