### Random Attack
- **ResNet-20 Baseline**: 90.77%
- **Pre-defined Shuffle**: 90.77%
- **Random Shuffle**: 90.77%

### Analysis of Weight Shuffling for Every Transmission Round

Next, we discuss the case where the weight package is shuffled for every transmission round, which serves as a strong obfuscation technique. This approach can have three possible implications:

1. **White-Box Setting**: In a white-box setting, where the attacker has full knowledge of the DNN and data transmission scheme, a randomly shuffled weight transmission will fail to defend against the attack.
2. **Black-Box Setting**: In a black-box setting, as shown in Table 8, this defense significantly limits the efficacy of our attack, requiring a larger number of attack iterations (e.g., 180) to degrade the accuracy to 53.3%. However, the attack remains more successful than a random AWD attack without a searching algorithm.
3. **Progressive Adversarial Attack**: The recent work on adversarial input attacks [23] suggests that obfuscation based on an underlying random function may not completely defend against a progressive adversarial attack. Given a large number of model queries, a progressive evolutionary algorithm-based attack (as in our case) can estimate the effect and distribution of the randomness, improving the attack's efficacy compared to a random attack.

Additionally, shuffling the data transmission every time requires additional header information to synchronize the sequence of weights at the receiver end. Recent research [101] has shown that random shuffling can lead to up to 9× energy inefficiency and 3.7× lower throughput. Therefore, effective defense schemes often come with the cost of additional overhead (memory, speed, and power).

### Power-Based Side-Channel Analysis for Detecting Deep-Dup

We now discuss the feasibility of using power-based side-channel analysis to detect Deep-Dup. The success of such detection relies on the ability to distinguish between two cases:
1. **Normal Case**: Two benign users execute their applications simultaneously.
2. **Attack Case**: Two users share FPGA resources, with one applying Deep-Dup to attack the other.

Since it is impractical to measure real-time power traces in a cloud-FPGA with an oscilloscope, an on-chip power sensor (e.g., TDC sensor) is the only viable option. As shown in Figure 4, similar to AWD attacks, the measured power trace of a benign user (e.g., YOLOv2) also exhibits large power glitches. Importantly, we observed that AWD attack power glitches are generally smaller in magnitude than those of the benign user-YOLOv2. Thus, AWD-induced glitches can be easily obfuscated.

Furthermore, distinguishing AWD power glitches in practical scenarios is challenging due to the following reasons:
- Most cloud-FPGA users run compute-intensive applications, generating many power glitches.
- Each fault injection by AWD lasts for a short duration (e.g., 50ns) and is disabled most of the time.
- Faults are injected at the attacker's discretion, without a fixed pattern.

In other words, while the defender needs ultra-high-resolution side-channel information to identify malicious power glitches from the noisy background, the attacker only needs to identify the temporal range for DNN weight transmission. Additionally, an attacker can inject faults in a stealthier manner, exacerbating the overall voltage drop when the victim DNN model itself is generating many power glitches [102]. Therefore, detecting Deep-Dup attacks with power anomalies in a multi-tenant FPGA is extremely difficult, if not impossible.

### Conclusion

In this work, we study the security of DNN acceleration in multi-tenant FPGAs. We exploit a novel attack surface where the victim and the attacker share the same FPGA hardware resources. Our proposed Deep-Dup attack framework is validated with a multi-tenant FPGA prototype and popular DNN architectures and datasets. Experimental results show that the proposed attack can degrade DNN inference performance to as low as random guessing or target specific classes of inputs. Notably, our attack succeeds even in a black-box setting, where the attacker has no knowledge of the DNN inference running in the FPGA. A malicious tenant with limited knowledge can implement both targeted and untargeted malicious objectives, causing significant disruption for the victim user. Finally, we envision that the proposed attack and defense methodologies will raise awareness about the security of deep learning applications in modern cloud-FPGA platforms.

### Acknowledgements

The authors thank the designated shepherd (Dr. Nele Mentens) for her guidance and the anonymous reviewers for their valuable feedback. This work is supported in part by the National Science Foundation under Grant No. 2019548 and No. 2043183.

### References

[1] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995, 1995.

[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255. IEEE, 2009.

[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.

[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

[5] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, and Tara N Sainath. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.

[6] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.

[7] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv preprint arXiv:1610.05256, 2016.

[8] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rashidi. Deep EHR: A survey of recent advances in deep learning techniques for electronic health record (EHR) analysis. IEEE Journal of Biomedical and Health Informatics, 22(5):1589–1604, Sep. 2018.

[9] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. Droid-sec: Deep learning in Android malware detection. In Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM '14, pages 371–372. ACM, 2014.

[10] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. DeepDriving: Learning affordance for direct perception in autonomous driving. In Computer Vision (ICCV), 2015 IEEE International Conference on, pages 2722–2730. IEEE, 2015.

[11] M. Teichmann, M. Weber, M. Zöllner, R. Cipolla, and R. Urtasun. MultiNet: Real-time joint semantic reasoning for autonomous driving. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1013–1020, June 2018.

[12] Altera and IBM unveil FPGA-accelerated power systems. https://www.hpcwire.com/off-the-wire/altera-ibm-unveil-fpga-accelerated-power-systems/.

[13] Here’s what an Intel Broadwell Xeon with a built-in FPGA looks like, 2016. https://www.theregister.co.uk/2016/03/14/intel_xeon_fpga/.

[14] Inside the Microsoft FPGA-based configurable cloud. https://azure.microsoft.com/en-us/resources/videos/build-2017-inside-the-microsoft-fpga-based-configurable-cloud/.

[15] Enable faster FPGA accelerator development and deployment in the cloud, 2020. https://aws.amazon.com/ec2/instance-types/f1/.

[16] George Provelengios, Daniel Holcomb, and Russell Tessier. Power wasting circuits for cloud FPGA attacks. In 30th International Conference on Field Programmable Logic and Applications (FPL), 2020.

[17] Yue Zha and Jing Li. Virtualizing FPGAs in the cloud. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 845–858, 2020.

[28] Fan Yao, Adnan Rakin, and Deliang Fan. DeepHammer: Depleting the intelligence of deep neural networks through targeted chain of bit flips. In 29th USENIX Security Symposium (USENIX Security 20), 2020.

[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.

[20] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

[21] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.

[22] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39–57. IEEE, 2017.

[29] Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. Fault injection attack on deep neural network. In 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 131–138. IEEE, 2017.

[30] Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali Chakrabarti, and Deliang Fan. T-BFA: Targeted bit-flip adversarial weight attack. arXiv preprint arXiv:2007.12336, 2020.

[31] Jason Cong, Zhenman Fang, Muhuan Huang, Peng Wei, Di Wu, and Cody Hao Yu. Customizable computing—from single chip to datacenters. Proceedings of the IEEE, 107(1):185–203, 2018.

[32] Xilinx: SoCs, MPSoCs, and RFSoCs, 2020. https://www.xilinx.com/products/silicon-devices/soc.html.

[33] Intel: SoC FPGAs, 2020. https://www.intel.com/content/www/us/en/products/programmable/soc.html.

[23] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.

[34] Mark Zhao and G Edward Suh. FPGA-based remote power side-channel attacks. In 2018 IEEE Symposium on Security and Privacy (SP), pages 229–244. IEEE, 2018.

[24] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In 25nd Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-22, 2018. The Internet Society, 2018.

[25] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.

[26] Sanghyun Hong, Pietro Frigo, Yiğitcan Kaya, Cristiano Giuffrida, and Tudor Dumitras. Terminal Brain Damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks. In 28th USENIX Security Symposium (USENIX Security 19), pages 497–514, 2019.

[35] Jonas Krautter, Dennis RE Gnad, and Mehdi B Tahoori. FPGAHammer: Remote voltage fault attacks on shared FPGAs, suitable for DFA on AES. IACR Transactions on Cryptographic Hardware and Embedded Systems, pages 44–68, 2018.

[36] Ilias Giechaskiel, Kasper B Rasmussen, and Ken Eguro. Leaky Wires: Information leakage and covert communication between FPGA long wires. In Proceedings of the 2018 on Asia Conference on Computer and Communications Security, pages 15–27. ACM, 2018.

[37] Yukui Luo and Xiaolin Xu. HILL: A hardware isolation framework against information leakage on multi-tenant FPGA long-wires. In 2019 International Conference on Field-Programmable Technology (ICFPT), pages 331–334. IEEE, 2019.

[27] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Bit-Flip Attack: Crushing neural network with progressive bit search. In The IEEE International Conference on Computer Vision (ICCV), October 2019.

[38] Dina Mahmoud and Mirjana Stojilović. Timing violation induced faults in multi-tenant FPGAs. In 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE), pages 1745–1750. IEEE, 2019.

[39] George Provelengios, Chethan Ramesh, Shivukumar B Patil, Ken Eguro, Russell Tessier, and Daniel Holcomb. Characterization of long wire data leakage in deep submicron FPGAs. In Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pages 292–297. ACM, 2019.

[40] Machine learning on AWS, 2020. https://aws.amazon.com/machine-learning/.