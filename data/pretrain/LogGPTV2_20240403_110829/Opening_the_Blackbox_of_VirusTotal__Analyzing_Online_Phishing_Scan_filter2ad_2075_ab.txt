the URLs to VirusTotal scan API for once at the beginning of the first
week. Then we monitor incoming traffic to the phishing servers,
and query the VirusTotal labels in the next 4 weeks.
In total, 38 websites are set up for our experiments
Summary.
(36 for main, 2 for baseline). There are 19 PayPal sites and 19 IRS
sites. All the PayPal sites have identical web page content (hosted
under different domain names). All the IRS sites share the same
content (with different domain names).
4 MEASUREMENT RESULTS
Our measurement returns a number of important results.
Table 2 shows the statistics the
(a) Incoming Network Traffic.
incoming network requests that fetch the phishing URLs. Clearly,
PayPal sites have received significantly more network traffic than
IRS sites. On average, each PayPal site has received more than
12,000 requests while an IRS site has only received 335 requests.
12345Time (week)Phishing PageGets OnlineSubmit URL to3rd-party VendorSubmit URL to3rd-party VendorChange PhishingPage to BenignVirusTotal ScanExternal EventIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
P. Peng, L. Yang, L. Song, and G. Wang
Figure 4: The number of incoming net-
work requests per day per phishing site
(main experiment).
Figure 5: The number of incoming net-
work requests per day per phishing site
(baseline experiment).
Figure 6: The average, maximum, and
minimum number of malicious labels
per site (main experiment).
As shown in Figure 4, IRS sites barely have any traffic in the first
week, and only start to receive more traffic in the second week.
Interestingly, the traffic volume is correlated with the “labels”
received by the sites. Figure 6 shows the number of VirusTotal
vendors that flagged a phishing site as malicious (i.e., number of
malicious labels per site). PayPal sites get flagged by some vendors
right away in the first week, while IRS sites are only detected at a
much later time (after vendor API scan). The hypothesis is that after
a phishing site is flagged by some vendors, then it will be shared with
other vendors to perform more in-depth scanning. Figure 5 further
confirms this intuition. For the IRS site (baseline experiment), we
only submit its URL to VirusTotal scan for once which failed to
detect it. Then there is almost no more traffic in the following weeks.
The PayPal site, since it got flagged after the scan, continues to
receive incoming traffic.
After looking into the traffic log, we notice that not all the re-
quests are pointed towards the submitted phishing URLs. Some scan-
ners also attempted to retrieve the resources under the root direc-
tory (“/”) or non-existing pages such as “payload.php” or “shell.php”.
For example, in the baseline experiment, the PayPal site has received
6,291 requests for the phishing URL (see Table 2), and 19,222 re-
quests for other URLs or resources. This indicates that the scanners
are looking for signs of malware hosting or website compromise.
A closer examination of Figure 6
(b) Delay of Label Updating.
shows that VirusTotal has a delay of updating the labels to its data-
base. More specifically, the x-axis in Figure 6 is the label querying
time (label crawling is done every hour). We observe that only after
the second VirusTotal scan will the first scan result get updated to
VirusTotal database.
For example, in the first week, we submit the PayPal URLs to
VirusTotal on day-1. The querying API returns “benign” labels since
these URLs were never scanned before by any vendor. Then after we
submit the URLs again on day-4, the querying API starts to return
“malicious” labels from some vendors. Based on the “scanning time”
on the returned labels, we see that theses “malicious” labels are
actually originated from the scan of day-1. This means, although
some vendors have already detected the phishing page on day-1,
the results would not be updated to VirusTotal database until the
next scan request on day-4.
The result shows VirusTotal uses “pull” (instead of “push”) to
get scanning results from vendors. The pull action is only triggered
by VirusTotal’s scan API but not the querying API. Our baseline
Vendor Name
Forcepoint
Sucuri Site Check
Quttera
URLQuery
ZeroCERT
Fortinet
Google Safe Brows.
Netcraft
PayPal
PayPal
PayPal
PayPal
PayPal
IRS
PayPal
IRS
IRS
Brand VTotal
Before
Vendor
(week-2)
VTotal
After
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
Table 3: Inconsistent labels between VirusTotal scan and
Vendor scan. “1” means malicious and “0” means benign.
experiment further confirms that vendors do not proactively push
new results to VirusTotal database. In the baseline setting, we only
submit the URL to VirusTotal on day-1 without any further ac-
tions. By querying the labels for the next 4 weeks, we confirm
that the scanning results are never updated back to VirusTotal. If
a researcher only scans a URL once and queries the database af-
terward, she cannot get the updated labels. Instead, the researcher
needs to perform two scans: one for URL scanning, and the other
for triggering the database update.
The VirusTotal scan during week-1 failed
(c) PayPal vs. IRS.
to detect any IRS website (Figure 6). After we directly submitted
the IRS URLs to individual vendors, some of the IRS pages started
to be flagged. On the contrary, all PayPal sites were flagged by
at least 10 vendors during week-1. One hypotehsis is that PayPal
phishing pages are more common, and thus the vendors’ models
(e.g., classifiers) are better trained to detect them. To validate this
hypothesis, more rigorous tests are needed by testing a wide range
of phishing pages (content), which is part of our future work.
The vendors’ own scan APIs and
(d) VirusTotal vs. Vendors.
VirusTotal’s scan APIs do not always return consistent results. Note
that when we use the vendor’s API, the API returns the scanning
result too. Unlike VirusTotal API, vendors’ own APIs ask the user
to wait until the scanning is finished so that the user gets the real
scanning result. In Table 3, we show the vendor API results (on
Monday of week 2), and the VirusTotal labels right before and after
that (results for the Thursday of week 1 and the Thursday of week
2 respectively). We have considered the delay of label updating of
VirusTotal and manually aligned the scan time accordingly.
 0 400 800 1200 1600 1 2 3 4 5# Requests / DayWeeksPayPalIRS 0 100 200 300 400 500 1 2 3 4 5# Requests / DayWeeksPayPalIRS 0 4 8 12 16 1 2 3 4 5# Malicious Labels / SiteWeeksPayPalIRSAnalyzing Online Phishing Scan Engines
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Rank Vendor
Netcraft
1
Emsisoft
2
3
Fortinet
Sophos
4
5
CRDF
Malwarebytes hpHosts
6
7
BitDefender
ESET
8
G-Data
9
Kaspersky
10
Phishtank
11
12
CyRadar
Avira
13
CLEAN MX
14
15
Trustwave
Total
26
26
26
23
17
15
15
15
14
13
10
8
6
6
3
PayPal
14
14
14
14
14
14
14
14
14
1
10
5
0
4
3
IRS
12
12
12
9
3
1
1
1
0
12
0
3
6
2
0
Table 4: A list of all the vendors that successfully detected
the phishing pages (during the first 2 weeks).
As shown in Table 3, there are in total 8 vendors that show incon-
sistent results. Most vendors have a “0-1-0” pattern for PayPal sites
including Forcepoint, Sucuri, Quttera, URLQuery, ZeroCERT, and
Google Safe Browsing. This means through VirusTotal scan, these
vendors return the label “benign”, even though their own scan APIs
can detect that the page as “malicious”. A possible explanation is
that these vendors did not give VirusTotal the permission to trigger
their scanners. Instead, VirusTotal runs stripped-down versions of
the scanners [9, 27], which cannot detect the phishing page.
For IRS pages, we show that Fortinet, Google Safe Browsing,
and Netcraft have detected these IRS pages via their own scan
APIs. However, only Netcraft has shared this result to VirusTotal
after the scan. It should be noted that we have tried to analyze
which scanners indeed visited the phishing sites. This attempt
failed because scanners were actively hiding their identity by using
proxies and cloud services (see §5). Overall, the result shows the
VirusTotal does not always reflect the best detection capability of a
vendor. If possible, researchers should cross-check the results with
individual vendors’ APIs.
In Table 4, we list all 15
(e) Detection Accuracy of Vendors.
vendors that detected at least one phishing site during the first two
weeks (we took down the phishing pages after week-2). We show
that even the best vendors cannot detect all phishing sites. The most
effective vendors such as Netcraft flagged 14 (out of 18) PayPal
pages and 12 (out of 18) IRS pages. It is not clear why some sites are
not detected given that all 18 PayPal (IRS) sites have the identical
content (except for using a different random string as the domain
name). In addition, we observe that some of the vendors always flag
the same subset of phishing sites. For example, Netcraft, Emsisoft,
and Fortinet flagged the same 26 sites. Similarly, Malwarebytes,
BitDefender and ESET flagged the same 15 sites. This indicates
the possibility that certain vendors would copy (synchronize with)
each other’s blacklist. To validate this hypothesis, more rigorous
experiment is needed in future work.
(f) Reaction to Phishing Take-down. We observe that ven-
dors do not quickly take a URL off the blacklist after the phishing
site is taken down. On the Monday of week-3, we took down all
the phishing pages and replaced them with benign pages. However,
Figure 7: Four vendors have a sign of reaction to the phish-
ing take-down (PayPal sites).
Figure 6 shows the number of malicious labels does not drop even
after multiple re-scans.
After examining the results for each vendor, we find 4 vendors
that flip some “malicious” labels to “benign” after the third week
(for PayPal sites only). Figure 7 shows these 4 vendors and the
number of phishing sites they flagged over time. CyRadar and CLEAN
MX already started to flip their malicious labels in week-2 (before
phishing take-down), which is not necessarily a reaction to the take-
down. Fortinet flipped the label on one site in week-4. Avira is
likely to be reacting to the take-down since it changed all “malicious”
labels to “benign” right after the event. Interestingly, the labels were
quickly reversed to “malicious” in the next scan.
5 OTHER CONTROLLED EXPERIMENTS
Our experiments lead to new questions: which vendors have in-
deed visited the phishing sites? What would happen if a phishing
site applies simple obfuscation techniques or sets “robots.txt” to
prevent crawling? How well can VirusTotal detect benign pages?
To answer these questions, we conduct additional controlled exper-
iments by setting up 27 new sites.
Vendor identification based on the net-
Vendor Identification.
work traffic is very difficult. On average each phishing site was
visited by more than 2000 unique IPs (PayPal, Table 2). Leverag-
ing the whois records, User-Agents, and the known IP ranges of
security vendors, we only successfully confirmed the identity of 5
vendors, including Dr. Web, Forcepoint, Google Safe Browsing,
Quttera, and ZeroCERT. We also tried more controlled experiments
by submitting URLs to each of the 18 vendors (one URL per vendor).
Even so, we cannot build a reliable identifier for all 18 vendors. The
reason is that most vendors route their traffic via proxies or cloud
services. The IP set of each vendor dynamically changes too. 32.9%
of the traffic comes from known cloud services such as Amazon,