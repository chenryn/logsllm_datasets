LSP split up) into two smaller ones so that only the increased trafﬁc
traverses longer path.
Minimum/maximum bandwidth Minimum and maximum band-
width specify the bounds of the LSP bandwidth. A low minimum
bandwidth value, renders an LSP ﬁckle, triggering large number of
path changes (minimal latency difference) since a small increase in
bandwidth (a few 100KBs is sufﬁcient) is now sufﬁcient to trigger
the bandwidth threshold. LSPs in this case change their path even
though current path has sufﬁcient available bandwidth because the
autobandwidth tie-breaking algorithms (random, least ﬁlled, most
ﬁlled) forces them to migrate to another equal cost path. A high
value of minimum bandwidth wastes reservable bandwidth in the
network.
A lower maximum bandwidth bounds the LSP bandwidth reser-
vation forcing the fate of additional trafﬁc on LSP to be uncertain
and requires larger number of LSPs to be setup between the DC
pair to accommodate the entire trafﬁc. Each LSP incurs additional
overhead in terms of computation and storage on ingress router and
on the network. A large value for maximum bandwidth parameter
makes LSP immobile, since during load, it gets harder for LSP to
ﬁnd path with free large reservable bandwidth. The ’all-or-nothing’
policy causes further complications
Bandwidth threshold and subscription factor Bandwidth thresh-
old dictates when should the autobandwidth algorithm be triggered.
A small value renders and LSP to be ﬁckle while a large value
makes the LSP less responsive and requires larger headroom on
link to absorb additional bandwidth. Similarly subscription factor
which determines what fraction of link capacity must be reserved,
play an important role. A small value wastes network capacity,
while a large value diminishes the headroom for LSPs to grow so
that autobandwidth gets triggered.
As a part of future work we will study how to adjust the au-
tobandwidth parameters automatically. Also, so far, we have as-
sumed all the LSPs and trafﬁc are equally important. However, in
an OSP network, different application trafﬁc usually have distinct
latency requirement. For instance, the trafﬁc of most web appli-
cations is latency-sensitive, but the trafﬁc of backup and replicate
applications is not. A “smart” TE strategy would optimize latency
only for the former while provisioning sufﬁcient bandwidth for the
latter. Such strategy could be implemented by classifying appli-
cation trafﬁc into delay-sensitive and delay-tolerant LSPs and as-
signing higher priority to the former type of LSPs. We will study
effectiveness of such classiﬁcation as our future work.
6. CONCLUSION
In this paper, we presented the ﬁrst study of the effectiveness of
MPLS-TE in a multi-continent production network connecting tens
of data centers. Using detailed LSP traces collected over a 2-month
period, we showed that a substantial number of LSPs encounter
severe latency inﬂation. We further showed that 80% of latency
inﬂation occur due to LSP path changes concentrated on 9% of
the links, 30% of the routers, and 3% of the active DC-pairs. Our
analysis conﬁrms trafﬁc load changes exceeding the capacity of a
subset of links along the shortest paths of LSPs as the primary root
cause of latency inﬂation but also uncovers poor conﬁguration of
MPLS-TE’s autobandwidth algorithms in the studied network as a
source of inefﬁcacy. As future work, we are developing guidelines
and automatic schemes to adjust autobandwidth conﬁgurations to
changing trafﬁc loads.
Acknowledgments
We thank the program committee and reviewers for their helpful
comments, and especially our shepherd, K. Papagiannaki, whose
detailed feedback signiﬁcantly improved the paper and its presen-
tation
7. REFERENCES
[1] D. Applegate and E. Cohen. Making intra-domain routing
robust to changing and uncertain trafﬁc demands:
Understanding fundamental tradeoffs. In Proc. of
SIGCOMM. ACM, 2003.
[2] D. Awduche, L. Berger, D. Gan, T. Li, V. Srinivasan, and
G. Swallow. RFC 3209: RSVPâ ˘AˇTTE: exâ ˘AˇTtensions to
RSVP for LSP tunnels. 2001.
[3] D. Bertsekas and R. Gallager. Data networks. Prentice-hall
New York, 1992.
[4] P. Gill, N. Jain, and N. Nagappan. Understanding network
failures in data centers: measurement, analysis, and
implications. In Proc. of SIGCOMM. ACM, 2011.
[5] S. Kandula, D. Katabi, B. Davie, and A. Charny. Walking the
tightrope: Responsive yet stable trafﬁc engineering. In Proc.
of SIGCOMM. ACM, 2005.
[6] K. Kompella and G. Swallow. RFC 4379: Detecting
multi-protocol label switched (MPLS) data plane failures.
2006.
[7] H. Wang, H. Xie, L. Qiu, Y. Yang, Y. Zhang, and
A. Greenberg. COPE: trafﬁc engineering in dynamic
networks. In Proc. of SIGCOMM. ACM, 2006.
[8] Y. Wang, H. Wang, A. Mahimkar, R. Alimi, Y. Zhang,
L. Qiu, and Y. Yang. R3: Resilient routing reconﬁguration.
In Proc. of SIGCOMM. ACM, 2010.
[9] LSP Ping: MPLS LSP ping/traceroute for LDP/TE, and LSP
ping for VCCV. http://www.cisco.com/en/US/
docs/ios/12_4t/12_4t11/ht_lspng.html.
[10] MPLS for dummies, 2010. http://www.nanog.org/
meetings/nanog49/presentations/Sunday/
mpls-nanog49.pdf.
468Summary Review Documentation for 
“Latency Inflation with MPLS-based Traffic Engineering” 
Authors: A. Pathak, M. Zhang, Y. Hu, R. Mahajan, D. Maltz 
(problems!)  of  some  of 
Reviewer #1 
Strengths:  MPLS  traffic  engineering  mechanisms  are  likely  in 
wide use. Understanding how some of these mechanisms work in 
practice, and some of their pitfalls is likely to be of interest and 
value to the measurement community. Measurements are from a 
well-known online service provider (MSN). 
Weaknesses:  A  key  implication  of  the  work  is  that  MPLS  TE 
deployments that use autobandwidth also experience the kinds of 
problems identified in the paper, though it is not clear that that is 
the case.  
Comments  to  Authors:  MPLS  is  used  widely  in  the  Internet 
these  days,  and  it’s  nice  to  see  a  empirical  study  of  the 
effectiveness 
traffic  engineering 
mechanisms.  Since  many  ISPs  don’t  reveal  much  of  anything 
about their MPLS networks, this study is likely to be useful and to 
spur additional work in this area. I appreciate the fact that MSN 
was willing to air a bit of dirty laundry, because some of the inter-
DC latency numbers are pretty ugly (i.e., Fig 1).  
There’s an apparent mismatch in the caption of Figure 3 and the 
text describing it. In the caption, it refers to spike magnitudes (and 
spikes haven’t yet been defined). In the text, it refers to the CDF 
of the latency difference for all LSPs. Which is it? It seemed to 
me that the description of latency spikes might be best described 
first, before any of the results in that section.  
In  the  all-or-nothing  discussion  in  \S  5,  you  might  mention  that 
some ISPs are already doing some manual splitting of LSPs. See, 
e.g.,http://www.nanog.org/meetings/nanog49/presentations/Sunda
y/mpls-nanog49.pdf  
There are a number writing and presentation issues that should be 
addressed.  The  writing  and  presentation  were,  for  me,  the  very 
weakest  aspects  of  the  paper.  While  these  are  all  basically 
“minor” issues, addressing them would help greatly in making the 
paper  more  readable  and  useful.  There  are  precious  few 
references to related work. In particular, there are *zero* citations 
of any IETF RFCs that specify the behavior of MPLS, RSVP-TE, 
or  any  of  the  other  protocols  or  mechanisms  that  you  describe. 
RFC 3209 should at least be cited, and it should be mentioned that 
the autobandwidth algorithms are (as far as I understand) vendor-
specific  (i.e.,  not  standardized),  though  Cisco  and  Juniper 
implement  them  similarly.  Also,  ref  [6]  should  really  be  a 
reference  to  RFC  4379  rather  than  a  ref  to  a  proprietary  Cisco 
document: MPLS Ping is a standard mechanism. It might also be 
helpful  to  have  references  to  both  Cisco  and  Juniper  (perhaps 
others)  documents  that  specify  how  to  configure  autobandwidth 
parameters.  
There are a number of instances of the phrase “LSP Path”, which 
is of course redundant. There are a number of cases in which it is 
stated that “the LSP reserves the required bandwidth” (\S 2.2), or 
an “[LSP] continually monitors the traffic rate” (\S 2.1), or some 
other, similar statement that assigns action to the LSP. An LSP is 
simply  the  label  switched  path.  It  isn’t  a  router  or  any  other 
“active” agent. In a number of places, you could simply say that 
the  label  edge  router,  or  LSR,  or  simply  router,  performs  a 
specific  action.  In  Section  5,  LSPs  are  referred  to  as  “fickle” 
(twice), which also is a bit of anthropomorphising.  
Section  5  is  really  a  general  discussion  section  rather  than 
specifically  a  discussion  on  autobandwidth  parameters  (that  is, 
you discuss issues beyond those parameters). Might it be better to 
simply name this section “Discussion”? 
Reviewer #2 
Strengths: Latency between data centers is a critically important 
topic  as  is  MPLS  TE.  The  results  are  not  surprising  but  are 
critically important for operators. 
Weaknesses:  The  experimental  methodology  uses  geographic 
information  to  approximate  latency  instead  of  active  probing  to 
infer the real latency. 
Comments  to  Authors:  Good  short  paper.  This  paper  confirms 
what many including myself have expected that MPLS-TE is not 
optimal, quantifies the added delay, and attributes the root cause 
of  delay  to  the  hard-coded  MPLS  autobandwidth  parameters.  I 
believe  this  is  an  important  result  (read:  we  now  have  data  that 
there  really  is  a  problem  and  it’s  degree)  and  I  encourage  the 
authors to follow through on their future plans to come up with a 
good solution.  
My  one  big  quibble  with  the  paper  was  the  experimental 
methodology. If I understand S3 correctly (it was a bit terse for a 
measurement paper), you did not do any active probing but rather 
summed  the  geographic  distances  between  points  in  the  MPLS 
tunnel  --  is  that  correct?  This  methodology  fails  to  account  for 
delay  inherent  to  store  and  forward  devices  or  to  the  fact  that 
fibers layouts have addition constraints (mountains, roads, private 
property,  etc.)  and  are  not  always  able  to  take  the  shortest  path 
between two points. None of this contradicts your results, but in 
your followup work, I would recommend that you present some 
active  probing  data  to  better  validate  your  claim  that  this  is  a 
reasonable  approximation  of  latency  (I  found  the  one  sentence 
disclaimer  “We  verified  with  a  few  LSPs  that...”  a  bit  of  a  red 
flag).  Ok,  so  Microsoft  doesn’t  use  LP  ping...  just  use  standard 
ping  from  inside  the  data  centers  -  it  shouldn’t  be  too  hard  to 
setup a vantage point in each data center.  
Fig 9c: it seems like a very small number of DC-pairs are causing 
a lot of the flapping -- did you talk to the operators about this? If 
yes,  what  was  the  response?  Is  this  simply  that  these  nodes  are 
more heavily loaded, under-provisioned, or because they are sub-
optimally configured? Adding feedback from the operators could 
improve this paper.  
469Another point to look (likely in follow up work) at is the 5 minute 
timer  that  autobandwidth  uses  to  recalculate  routes: if that were 
decreased and the algorithms made more responsive, is there any 
additional benefit? 
Reviewer #3 
Strengths: The first look into MPLS-TE in practice. Nice dataset. 
Paper shows some problems with current TE implementations that 
should lead to further research. 
Weaknesses:  The  measurements  of  latency  are  not  direct  and 
some of the analysis could be a little deeper. 
Comments to Authors: This paper presents an interesting study 
of MPLS-TE. To the best of my knowledge this is the first study 
of a large-scale production network running MPLS-TE. I feel that 
I’ve learned more about MPLS-TE and the issues that can arise in 
practice. I just have a few comments. 
From your introduction, it is clear that OSP operations have strict 
requirements  for  latency  between  data  centers,  could  you  give 
some numbers of the ranges of acceptable latencies? It would help 
people  who  want  to  find  solutions  to  the  problems  you  are 
pointing out. 
When you present the network in Sec. 3, it would be nice to give 
an  idea  of  the  geographical  spread.  You  only  say  that  it  is 
intercontinental in the conclusion. 
At the last paragraph of Sec. 3, it would be nice to add a small 
explanation  of  how  you  verify  that  your  latency  estimation 
actually matches latency. It is a little disappointing that you can’t 
get direct measurements of latency (why not just do pings from 
the  data  centers?),  at  least  elaborating  on  the  validation  would 
help. 
Why  do  label  Figures  4,  7,  and  8  with  OWD,  if  earlier  you’ve 
defined latency and that is what you use in the text? 
People usually use the term stretch to refer to latency inflation.  
Sec. 4.2 talks about correlation, but it uses no metric of statistical 
correlation.  One  can  kind  of  see  the  correlation  in  the  example 
week in Fig. 11, but the numbers would be more general. 
Reviewer #4 
Strengths:  This  is  the  first  paper  I  know  that  demonstrates  the 
impact  of  MPLS  TE  on  latency  across  a  network.  The  authors 
show a good understanding of how MPLS TE works and discuss 
possible reasons behind such a phenomenon. 
Weaknesses: Unfortunately, the paper does not really prove that 
the reason behind the latency increase is the way autobandwidth 
works. The correlation with utilization is weak at best and there is 
no significant analysis performed to convince that this is the case 
-  for  that  reason  my  assessment  would  be  between  a  2  and  a  3 
with a technical correctness at similar levels. 
Comments to Authors: I find that this paper makes an attempt to 
look into a very interesting problem - that of the performance of 
traffic  engineering  mechanisms  like  MPLS  TE.  Providers  have 
deployed  such  a  solution  in  the  hope  that  they  will  be  able  to 
better  handle  performance  in  their  network,  but  as  the  authors 
mention very little is known as to how successful the employed 
mechanisms  actually  are.  Moreover,  as  the  authors  demonstrate 
these  mechanisms  are  typically  accompanied  by  a  number  of 
thresholds that network operators rarely change - and reasonably 
so,  since  nobody  could  really  estimate  the  impact  of  those 
changes.  
Interestingly, this paper shows that employing MPLS-TE can lead 
to significant increases in latency as compared to shortest paths. A 
large number of paths inside Microsoft’s Online Service Network 
are showing spikes in latency, thus affecting the performance of 
the provided services.  
The authors have studied the magnitude of latency increases, the 
number of data centers that they affect, as well as routers. They 
have further attempted to correlate it with link utilization with the 
goal to demonstrate that such latency increases may be due to the 
reaction  of  autobandwidth  mechanism  in  high  load  conditions. 
Unfortunately,  that  point  is  never  really  proven.  I  would  really 
have liked to see the authors formulating a hypothesis and testing 
it according to their data. Showing the result on Figure 11 is by no 
means a proof of that point. For each time bin they have identified 
the  70%  and  99%  percentile  load  across  the  network  and  then 
counted  the  number  of  path  changes.  They  claim  that  the 
correlation is high but the presented figure is far from conclusive. 
I  would  like  to  have  seen  something  more  precise,  where  the 
authors  track  utilization  across  the  network,  the  LSP  messages 
that will be triggered for sure if autobandwidth kicks in, and then 
the  precise  correlation  of  that  LSP  change  with  the  utilization 
increase in one of the links of the LSP. Otherwise, you have not 
really shown what you claim.  
For  that  reason,  i  would  say  that  this  paper  should  only  be 
accepted conditionally and published only if the authors manage 
to properly demonstrate such a correlation of remove that claim 
from the paper. You need to define the several causes that could 
lead to a path change (such as failure, etc) and then clearly test the 
hypothesis of link utilization being the culprit. Given the novelty 
of the data set and the rest of the analysis I would not say that this 
paper is a straight reject.  
Finally, the authors compare MPLS TE latency with that obtained 
through an optimization problem that is never precisely defined. I 
think  it  is  worth  spending  some  space  trying  to  make  the  paper 
self-consistent. What is byte weighted latency. 
Reviewer #5 
Strengths:  This  is  the  very  first  paper  that  I  have  read  on  the 
measured  performance  of  MPLS  networks.  These  results  would 
be interesting to network operators. 
Weaknesses:  The  paper  simply  reported  the  results  in  a 
straightforward way. Rather than just scratching the surface of the 
problem (hey look the paths are not optimal), I wish the paper had 
gone deeper to analyze the causes of why these sub-optimal paths. 
Comments to Authors: This is an interesting paper, perhaps the 
first one, to report MPLS path quality in a real, large operational 
network. The specific measurement results would be interesting to 
people in operations and perhaps also in research (to understand 
why sub-optimal paths occurring from time to time).  
However as someone who knows a bit about MPLS, the paper did 
not seem that exciting. I think a main reason is perhaps the result 
is a bit shallow. MPLS is simply a means to balance traffic flows 
across  network,  this  measurement  study  shows  that  the  decision 
470process (on the path selection and dynamic adjustment of MPLS 
circuits)  does  not  pick  the  best  decision  all  the  time  --  this  is 
expected  (I  actually  thought  the  performance  was  not  bad:).  To 
me  the  more  interesting  bit  is  not  how  bad  the  delay  can  be 
inflated, but to explain why MPLS picked sub-optimal paths from 
time to time. 
Response from the Authors 
We mention the two important points raised by the reviewers and 
how we addressed them in our paper. 
1.  The  paper  does  not  prove  directly  that  autobandwidth  is  the 
root cause of latency spikes 
MSN  comprises  tens  of  DCs  interconnected  with  a  dedicated 
network.  All  the  traffic  in  MSN  is  carried  over LSPs which are 
managed by autobandwidth algorithm. 
-  Most  inter-dc  links  have  three  9’s  availability  (figure  9(c)  in 
sigcomm’11  paper  (Understanding  Network  Failures  in  Data 
Centers: Measurement, Analysis, and Implications)). 
-  Three  9’s  for  50  days  is  1.2  hours.  However,  about  82.8%  of 
LSPs which show severe spikes have cumulative spike durations 
more than 1.2 hrs. Also, about 98.9% of global cumulative LSP 
spike duration (sum of all severe LSP spikes across all LSPs) is 
originated from LSP spikes where individual spikes are more than 
1.2 hrs. 
This  suggests  that  LSPs  spikes  which  are  severe  are  caused  by 
autobw  instead  of  failures.  We  confirmed  similar  LSP  spikes  in 
MPLS simulations. 
2.  Use  of  geographical  distances  to  estimate  latency  instead  of 
directly measuring it. 
We  currently  cannot  obtain  the  data  for  direct  LSP  latency 
measurement. The MSN operators do not provide LSP ping. It is 
also difficult to associate end-to-end ping data with an individual 
LSP  due  to  load-balancing  across  multiple  LSPs  between  the 
same DC pair. The latency estimate based on great-circle distance 
and speed-of-light is the best approximate we can get. 
We have included the above details in the paper. 
471