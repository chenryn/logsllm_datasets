This experiment also allowed the evaluation of how much traﬃc could be pro-
cessed by an SPE. Two hosts were employed, a processing node and a client node.
The processing node was an Athlon XP 2600+ with 1 GB of RAM. We imple-
mented and used a tool (dummysender) for generating synthetic Netﬂow data.
The client node, which executed the BigGiantHead, dummysender, flowsender
and ureceiver applications, was a 900MHz Celeron CPU with 2GB of RAM.
Both computers were connected to a 100Mbps Ethernet LAN. The traﬃc rate
generated ranged from 1,000 to 4,000 Netﬂow records per second, using steps of
1,000 records. The SPE executed a simple but useful query which computes the
number of packets processed within an interval (window) of 10 seconds. Since we
knew the expected results in advance, it was straightforward to verify the results.
The experiment was repeated 10 times for each record rate, and the results were
accurate for all record rates up to 35,000 records per second, approximately. At
rates above this point the SPE entered an error state and crashed. The same
experiment was later executed using a faster processor, a 2GHz Core 2 Duo ma-
chine with 1GB of RAM. In this case we were able to achieve accurate results
for rates up to 60,000 records per second.
Experiment 2: Multiple Nodes, Synthetic Traﬃc. In order to simulate an
environment similar to the Brazilian RNP backbone, with several geographically
distributed Autonomous Systems, we have deployed multiple SPE nodes, each
of them capturing a distinct stream of Netﬂow records, as illustrated in Figure
3(a). Similar to the ﬁrst experiment, we considered a synthetic load generated
by dummysender, and employed 3 processing nodes and one client node. The
48
C. Lyra, C.S. Hara, and E.P. Duarte Jr.
BackStreamDB
File
s
d
r
o
c
e
r
#
1.80 M
1.60 M
1.40 M
1.20 M
1.00 M
800.00 k
600.00 k
400.00 k
200.00 k
0.00  
(a)
 0
 10
 20
 30
 40
 50
 60
time (m)
(b)
Fig. 3. (a)Experiments with synthetic traﬃc. (b)Results generated on real traﬃc.
processing nodes were based on Athlon XP processors (models 2600+, 2400+ and
2200+) each with 1GB of RAM. The client node (executing BigGiantHead and
ureceiver applications) was the same one employed in the previous experiment,
featuring a 900MHz Celeron processor with 2GB of RAM. All computers were
connected to a 100Mbps Ethernet LAN.
The same query employed in the ﬁrst experiment was issued in this case:
the number of packets processed within 10 seconds window. However, in this
experiment each node processes only the locally generated records. Partial results
were then sent by each processing node to a node elected to produce the ﬁnal
result. On each processing node we generated traﬃc at a rate of 30,000 records
per second, which was correctly summed up by the elected node, which produced
the output of 90,000 records at all repetitions of the experiment. Observe that
similar rates are expected for processing queries as the ones illustrated in Figures
2(a) and 2(b). For 2(a), the grouping and sum of octets could be locally executed
at each processing node, and only the union of all results at the client node. For
distributing the query on Figure 2(b), the ﬁltering and the ﬁrst grouping and
counting operations could be executed locally at each processing node, while the
second aggregation and ﬁltering operations at the client. This experiment results
show that BackStreamDB scales well, and is capable of processing heavier traﬃc
loads by adding new processing nodes to the system.
Experiment 3: Multiple Nodes, Real Traﬃc. The goal of the third ex-
periment was to evaluate the tool with a real traﬃc load. This experiment was
conducted at the Brazilian Research Network (RNP), using traﬃc ﬂow data col-
lected at the Parana state Point of Presence (PoP-PR). Traﬃc information was
collected with Nprobe, an application that collects traﬃc from a mirrored port
of a border switch, and outputs a Netﬂow stream. Nprobe was conﬁgured to
send a copy of the Netﬂow stream to our acquisition application, and also to
an application for storing the data. The processor that collects Netﬂow streams
produces output data in intervals of 5 minutes, the standard window size for
this kind of application. We have issued queries to generate results in the same
window size by BackStreamDB. This allows a straightforward veriﬁcation of the
results, with a direct comparison with the expected results generated from stored
BackStreamDB: A Distributed System
49
data. The real traﬃc load diﬀers from the synthetic load of Experiments 1 and 2
described above because of its high variance. While the synthetic load is a con-
tinuous stream, the real load oscillates from very low rates to occasional bursts
containing a large number of packets. Records were sent to BackStreamDB at
intervals of 30 seconds.
The traﬃc processed by Nprobe was around 1Gbps, which corresponds to
100,000 packets per second. When this traﬃc is transformed to Netﬂow, about
one million records are generated every 5 minutes. We used the same query of
the other experiments. The processing node was a 2GHz Core 2 Duo CPU with
1GB of RAM. Figure 3(b) shows the total number of records produced by Back-
StreamDB and also the values obtained by processing the Netﬂow ﬁles for an
interval of one hour. It can be observed that the results are not identical through-
out the experiment. There are two reasons for this diﬀerence. First, there is a
small diﬀerence on the total number of records stored on the ﬁle, and processed
by BackStreamDB. While BackStreamDB processed 14,160,930 records during
an hour, the ﬁle contained 14,799,980 records, resulting in a diﬀerence of 0.1%
records, possibly due to the loss of a few UDP packets, which is the transport
protocol employed to carry Netﬂow data. The second reason is that the appli-
cation that collects Netﬂow records uses the machine local time to determine
its window, while BackStreamDB uses the timestamp from the record itself. So
it is possible that the set of records processed in each system on a given time
window is not the same. This can be evidenced on the graphic, which shows a
smaller number of records in a time frame, followed by a time frame with larger
number of records. However, during the entire experiment, this diﬀerence was at
most 5.2%, which happened around the 40th minute. This experiment showed
that BackStreamDB was able to monitor the traﬃc of a high-speed national
backbone, providing real time measurements that were consistent with the ones
generated from stored data.
4 Related Work
Several existing network management and traﬃc monitoring tools can be consid-
ered to be related to this work. An extensive list can be found in [6]. Among the
tools that process Netﬂow, one of the ﬁrst distributed as open source is cflowd7,
which later originated flowscan8. In [8] the authors argue that tools for packet
analysis do not scale well for high speed networks and propose a framework for
monitoring backbones in real time which is based on Netﬂow. The framework
has a centralized architecture, and requires users to write plugins for each mon-
itoring task. In [12], a tool which collects Netﬂow data and export them in pcap
format is described. Such data can then be used as input to Wireshark9 to obtain
analytical information. Ntop10 is a tool that can be used both for packet analysis
and for ﬂow management, using a Netﬂow/Sﬂow plugin. It is possible to generate
7
8
9
10
http://www.caida.org/tools/measurement/cflowd
http://www.caida.org/tools/utilities/flowscan/
http://www.wireshark.org
http://www.ntop.org
50
C. Lyra, C.S. Hara, and E.P. Duarte Jr.
various types of reports and graphics using Ntop, but they are limited to the
predeﬁned metrics provided by the tool, i.e. arbitrary queries are not supported.
Another system for traﬃc monitoring based on Netﬂow is presented in [4]. That
system captures Netﬂow records which are then stored in an Oracle Database.
Databases allow arbitrary queries to be issued on the stored data. However, this
approach still requires a considerable amount of storage, as opposed to on-the-ﬂy
stream processing. The authors of SMART [15] argue that traditional Netﬂow
tools that store collected data on disk for later processing are not eﬃcient for
large-scale network traﬃc monitoring. They propose an in memory storage in
order to process data eﬃciently. The reported results in terms of Netﬂow records
processed per second - thirty thousand records per second - is very close to what
we have achieved using an SPE, as detailed in Section 3.
Borealis is a second generation distributed SPE. Other prototypes have been
developed in the context of TelegraphCQ [5] and STREAM [3] projects. Gigas-
cope [7] is a system which uses an SPE tailored for high speed network mon-
itoring. Although reported results are promising, Gigascope is a proprietary
(AT&T’s) commercial product. Motivated by the possibility of developing an
open source SPE for traﬃc monitoring, the authors of [14] describe a case study
using TelegraphCQ SPE [5]. It involved a functionality analysis to determine
whether the SPE can be used to provide the same metrics of T-RAT, a tool
developed for analyzing TCP packet traces. The results obtained in terms of
traﬃc volume were modest, but it can be considered a seminal work, in which
a single centralized SPE is applied for network management. MaD-WiSe [2] is
a distributed monitoring system for managing data streams generated by wire-
less sensor networks. Our previous work, PaQueT [11], has been proposed to
monitor a single network segment, generating packet level metrics. Moreover,
PaQueT has not been designed as a distributed system in which modules with
speciﬁc functionalities can be spread over a wide area network. Some of the main
features of BackStreamDB include the following: it allows data gathering from
multiple data sources and features distributed processing at multiple nodes. It
is also based on an architecture with separate modules for data acquisition and
query result treatment, and is able to process data in Netﬂow format, considering
the whole backbone.
5 Conclusion
In this paper we proposed an architecture for real time backbone traﬃc moni-
toring that provides arbitrary measurements about individual segments or the
backbone as a whole. The strategy is based on both ﬂow protocols and stream
processing engines and does not require traﬃc logs to be stored. The distributed
nature of the proposed system is scalable as depending on the amount of traﬃc,
more nodes can be deployed for monitoring tasks. BackStreamDB is an imple-
mentation of the proposed strategy, based on Borealis SPE and Netﬂow. Back-
StreamDB was deployed and validated on the Brazilian national RNP backbone.
An experimental study involving both synthetic as well as real traﬃc shows that
the approach is feasible and dependable, generating consistent results in both
BackStreamDB: A Distributed System
51
settings. BackStreamDB was able to process a workload of one million Netﬂow
records of real traﬃc in intervals of ﬁve minutes. To highlight the system func-
tionality in order to show how it can help network administrators to fulﬁll their
tasks, case studies were presented for computing a traﬃc matrix, and detecting
traﬃc signatures.
We are currently developing a version of BackStreamDB to be a service
provider in the PerfSONAR framework [9], a service-oriented architecture for
multi-domain network monitoring.
Acknowledgments. We thank the Brazilian National Research Network, RNP,
for supporting the deployment of the proposed system through Working Group
GT-BackStreamDB. This work was partially supported by grant 304013/2009-9
from the Brazilian Research Agency (CNPq).
References
1. Abadi, D.J., Ahmad, Y., Balazinska, M., C¸ entintemel, U., Cherniack, M., Hwang,
J.H., Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul, N., Xing, Y.,
Zdonik, S.: The Design of the Borealis Stream Processing Engine. In: Proc. of the
Conf. on Innovative Data Systems Research, pp. 277–289 (2005)
2. Amato, G., Chessa, S., Vairo, C.: MaD-WiSe: A Distributed Stream Management
System for Wireless Sensor Networks. Software Practice and Experience 40(5),
431–451 (2010)
3. Arasu, A., Babcock, B., Babu, S., Cieslewicz, J., Datar, M., Ito, K., Motwani,
R., Srivastava, U., Widom, J.: STREAM: The Stanford Data Stream Management
System. IEEE Data Engineering Bulletin 26(1), 19–26 (2003)
4. Bin, L., Chuang, L., Jian, Q., Jianping, H., Ungsunan, P.: A NetFlow-based
Flow Analisys and Monitoring System in Enterprise Networks. Computer Net-
works 52(5), 1074–1092 (2008)
5. Chandrasekaran, S., Cooper, O., Deshpande, A., Franklin, M.J., Hellerstein, J.M.,
Hong, W., Krishnamurthy, S., Madden, S., Raman, V., Reiss, F., Shah, M.: Tele-
graphCQ: Continuous Dataﬂow Processing for an Uncertain World. In: Proc. of
the Conf. on Innovative Data Systems Research (2003)
6. Cottrell, L.: Network Monitoring Tools (2011),
http://www.slac.stanford.edu/xorg/nmtf/nmtf-tools.html
7. Cranor, C., Johnson, T., Spataschek, O.: Gigascope: A Stream Database for Net-
work Applications. In: Proc. of the ACM SIGMOD Int. Conf. on Management of
Data Conference, pp. 647–651 (2003)
8. Dubendorfer, T., Wagner, A., Plattner, B.: A Framework for Real-Time Worm
Attack Detection and Backbone Monitoring. In: Proc. of the IEEE Int. Workshop
on Critical Infrastructure Protection, pp. 3–12 (2005)
9. Hanemann, A., Boote, J.W., Boyd, E.L., Durand, J., Kudarimoti, L., (cid:3)Lapacz, R.,
Swany, D.M., Trocha, S., Zurawski, J.: PerfSONAR: A Service Oriented Architec-
ture for Multi-domain Network Monitoring. In: Benatallah, B., Casati, F., Traverso,
P. (eds.) ICSOC 2005. LNCS, vol. 3826, pp. 241–254. Springer, Heidelberg (2005)
10. Koudas, N., Srivastava, D.: Data Stream Query Processing: A Tutorial. In: Proc.
of the Int. Conf. on Very Large Data Bases, p. 1149 (2003)
11. Ligocki, N., Gomes, C.L., Hara, C.: A Flexible Network Monitoring Tool based on
a Data Stream Management System. In: Proc. of the IEEE Symp. on Computers
and Communications, pp. 800–805 (2008)
52
C. Lyra, C.S. Hara, and E.P. Duarte Jr.
12. Munz, G., Carle, G.: Distributed Network Analysis Using TOPAS and Wireshark.
In: Network Operations and Management Symp. Workshops, pp. 161–164 (2008)
13. Phaal, P., Panchen, S., McKee, N.: RFC 3176: InMon Corporation’s sFlow: A
Method for Monitoring Traﬃc in Switched and Routed Networks (2001)
14. Plagemann, T., Goebel, V., Bergamini, A., Tolu, G., Urvoy-Keller, G., Biersack,
E.W.: Using Data Stream Management Systems for Traﬃc Analysis – A Case
Study. In: Barakat, C., Pratt, I. (eds.) PAM 2004. LNCS, vol. 3015, pp. 215–226.
Springer, Heidelberg (2004)
15. Zhou, A., Yan, Y., Gong, X., Chang, J., Dai, D.: SMART: A System for Online
Monitoring Large Volumes of Network Traﬃc. In: Proc. of the IEEE Int. Conf. on
Data Engineering, pp. 1576–1579 (2008)