### Experiment 1: Single Node, Synthetic Traffic
This experiment aimed to evaluate the traffic processing capacity of a Stream Processing Engine (SPE). Two hosts were used: a processing node and a client node. The processing node was equipped with an Athlon XP 2600+ processor and 1 GB of RAM. We developed and utilized a tool called `dummysender` to generate synthetic NetFlow data. The client node, which ran applications such as BigGiantHead, dummysender, flowsender, and ureceiver, had a 900 MHz Celeron CPU and 2 GB of RAM. Both computers were connected via a 100 Mbps Ethernet LAN.

The generated traffic rate ranged from 1,000 to 4,000 NetFlow records per second, with increments of 1,000 records. The SPE executed a simple but useful query to compute the number of packets processed within a 10-second window. Since the expected results were known in advance, verifying the results was straightforward. The experiment was repeated 10 times for each record rate, and the results were accurate up to approximately 35,000 records per second. At higher rates, the SPE entered an error state and crashed. Subsequently, the experiment was conducted using a faster 2 GHz Core 2 Duo processor with 1 GB of RAM, achieving accurate results up to 60,000 records per second.

### Experiment 2: Multiple Nodes, Synthetic Traffic
To simulate an environment similar to the Brazilian RNP backbone, with multiple geographically distributed Autonomous Systems, we deployed multiple SPE nodes, each capturing a distinct stream of NetFlow records, as shown in Figure 3(a). Similar to the first experiment, we used `dummysender` to generate synthetic traffic and employed three processing nodes and one client node. The processing nodes were based on Athlon XP processors (models 2600+, 2400+, and 2200+) with 1 GB of RAM each. The client node, running BigGiantHead and ureceiver, was the same as in the previous experiment, featuring a 900 MHz Celeron processor with 2 GB of RAM. All computers were connected to a 100 Mbps Ethernet LAN.

The same query as in the first experiment was used: the number of packets processed within a 10-second window. Each node processed only the locally generated records, and partial results were sent to a designated node to produce the final result. On each processing node, traffic was generated at a rate of 30,000 records per second. The elected node correctly summed these results, producing an output of 90,000 records in all repetitions of the experiment. This experiment demonstrated that BackStreamDB scales well and can handle heavier traffic loads by adding more processing nodes.

### Experiment 3: Multiple Nodes, Real Traffic
The third experiment aimed to evaluate BackStreamDB with real traffic. It was conducted at the Brazilian Research Network (RNP) using traffic flow data collected at the Parana state Point of Presence (PoP-PR). Traffic information was collected using Nprobe, which captures traffic from a mirrored port of a border switch and outputs a NetFlow stream. Nprobe was configured to send a copy of the NetFlow stream to our acquisition application and to an application for storing the data. The processor that collects NetFlow streams produces output data in 5-minute intervals, the standard window size for this type of application. We issued queries to generate results in the same window size, allowing straightforward verification by comparing with the expected results from stored data.

Real traffic load differs from synthetic load due to its high variance, oscillating from very low rates to occasional bursts containing a large number of packets. Records were sent to BackStreamDB at 30-second intervals. The traffic processed by Nprobe was around 1 Gbps, corresponding to 100,000 packets per second. When transformed to NetFlow, about one million records are generated every 5 minutes. We used the same query as in the other experiments. The processing node was a 2 GHz Core 2 Duo CPU with 1 GB of RAM.

Figure 3(b) shows the total number of records produced by BackStreamDB and the values obtained by processing the NetFlow files for an interval of one hour. There were two reasons for the observed differences in results. First, there was a small difference in the total number of records stored in the file and processed by BackStreamDB. While BackStreamDB processed 14,160,930 records during an hour, the file contained 14,799,980 records, resulting in a 0.1% difference, possibly due to the loss of a few UDP packets. Second, the application that collects NetFlow records uses the machine's local time to determine its window, while BackStreamDB uses the timestamp from the record itself. This can lead to a different set of records being processed in each system within a given time window. During the entire experiment, the maximum difference was 5.2%, occurring around the 40th minute. This experiment showed that BackStreamDB could monitor the traffic of a high-speed national backbone, providing real-time measurements consistent with those generated from stored data.

### Related Work
Several existing network management and traffic monitoring tools are related to this work. An extensive list can be found in [6]. Among the tools that process NetFlow, one of the first distributed open-source tools is cflowd, which later evolved into flowscan. In [8], the authors argue that packet analysis tools do not scale well for high-speed networks and propose a framework for real-time backbone monitoring based on NetFlow. This framework has a centralized architecture and requires users to write plugins for each monitoring task. In [12], a tool that collects NetFlow data and exports it in pcap format is described. Such data can then be used as input to Wireshark for analytical information. Ntop is a tool for both packet analysis and flow management, using a NetFlow/SFlow plugin. It can generate various reports and graphics but is limited to predefined metrics, and arbitrary queries are not supported.

Another system for traffic monitoring based on NetFlow is presented in [4], which captures NetFlow records and stores them in an Oracle Database. Databases allow arbitrary queries on stored data but require significant storage. The authors of SMART [15] argue that traditional NetFlow tools that store collected data on disk for later processing are inefficient for large-scale network traffic monitoring. They propose in-memory storage for efficient data processing. The reported results in terms of NetFlow records processed per second—thirty thousand records per second—are close to what we achieved using an SPE, as detailed in Section 3.

Borealis is a second-generation distributed SPE. Other prototypes have been developed in the context of TelegraphCQ [5] and STREAM [3] projects. Gigascope [7] is a system that uses an SPE tailored for high-speed network monitoring. Although the reported results are promising, Gigascope is a proprietary commercial product. Motivated by the possibility of developing an open-source SPE for traffic monitoring, the authors of [14] describe a case study using TelegraphCQ SPE. It involved a functionality analysis to determine whether the SPE can provide the same metrics as T-RAT, a tool for analyzing TCP packet traces. The results in terms of traffic volume were modest but represent a seminal work where a single centralized SPE is applied for network management. MaD-WiSe [2] is a distributed monitoring system for managing data streams generated by wireless sensor networks. Our previous work, PaQueT [11], was designed to monitor a single network segment, generating packet-level metrics. However, PaQueT was not designed as a distributed system with specific functionalities spread over a wide area network.

Some of the main features of BackStreamDB include data gathering from multiple sources, distributed processing at multiple nodes, and an architecture with separate modules for data acquisition and query result treatment. It can process data in NetFlow format, considering the whole backbone.

### Conclusion
In this paper, we proposed an architecture for real-time backbone traffic monitoring that provides arbitrary measurements about individual segments or the backbone as a whole. The strategy is based on flow protocols and stream processing engines and does not require traffic logs to be stored. The distributed nature of the proposed system is scalable, allowing more nodes to be deployed depending on the amount of traffic. BackStreamDB is an implementation of the proposed strategy, based on Borealis SPE and NetFlow. It was deployed and validated on the Brazilian national RNP backbone. An experimental study involving both synthetic and real traffic showed that the approach is feasible and dependable, generating consistent results in both settings. BackStreamDB was able to process a workload of one million NetFlow records of real traffic in 5-minute intervals. Case studies were presented to demonstrate how the system can help network administrators fulfill their tasks, including computing a traffic matrix and detecting traffic signatures.

We are currently developing a version of BackStreamDB to be a service provider in the PerfSONAR framework [9], a service-oriented architecture for multi-domain network monitoring.

### Acknowledgments
We thank the Brazilian National Research Network, RNP, for supporting the deployment of the proposed system through Working Group GT-BackStreamDB. This work was partially supported by grant 304013/2009-9 from the Brazilian Research Agency (CNPq).

### References
1. Abadi, D.J., Ahmad, Y., Balazinska, M., C¸ entintemel, U., Cherniack, M., Hwang, J.H., Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul, N., Xing, Y., Zdonik, S.: The Design of the Borealis Stream Processing Engine. In: Proc. of the Conf. on Innovative Data Systems Research, pp. 277–289 (2005)
2. Amato, G., Chessa, S., Vairo, C.: MaD-WiSe: A Distributed Stream Management System for Wireless Sensor Networks. Software Practice and Experience 40(5), 431–451 (2010)
3. Arasu, A., Babcock, B., Babu, S., Cieslewicz, J., Datar, M., Ito, K., Motwani, R., Srivastava, U., Widom, J.: STREAM: The Stanford Data Stream Management System. IEEE Data Engineering Bulletin 26(1), 19–26 (2003)
4. Bin, L., Chuang, L., Jian, Q., Jianping, H., Ungsunan, P.: A NetFlow-based Flow Analysis and Monitoring System in Enterprise Networks. Computer Networks 52(5), 1074–1092 (2008)
5. Chandrasekaran, S., Cooper, O., Deshpande, A., Franklin, M.J., Hellerstein, J.M., Hong, W., Krishnamurthy, S., Madden, S., Raman, V., Reiss, F., Shah, M.: TelegraphCQ: Continuous Dataflow Processing for an Uncertain World. In: Proc. of the Conf. on Innovative Data Systems Research (2003)
6. Cottrell, L.: Network Monitoring Tools (2011), http://www.slac.stanford.edu/xorg/nmtf/nmtf-tools.html
7. Cranor, C., Johnson, T., Spataschek, O.: Gigascope: A Stream Database for Network Applications. In: Proc. of the ACM SIGMOD Int. Conf. on Management of Data Conference, pp. 647–651 (2003)
8. Dubendorfer, T., Wagner, A., Plattner, B.: A Framework for Real-Time Worm Attack Detection and Backbone Monitoring. In: Proc. of the IEEE Int. Workshop on Critical Infrastructure Protection, pp. 3–12 (2005)
9. Hanemann, A., Boote, J.W., Boyd, E.L., Durand, J., Kudarimoti, L., (cid:3)Lapacz, R., Swany, D.M., Trocha, S., Zurawski, J.: PerfSONAR: A Service Oriented Architecture for Multi-domain Network Monitoring. In: Benatallah, B., Casati, F., Traverso, P. (eds.) ICSOC 2005. LNCS, vol. 3826, pp. 241–254. Springer, Heidelberg (2005)
10. Koudas, N., Srivastava, D.: Data Stream Query Processing: A Tutorial. In: Proc. of the Int. Conf. on Very Large Data Bases, p. 1149 (2003)
11. Ligocki, N., Gomes, C.L., Hara, C.: A Flexible Network Monitoring Tool based on a Data Stream Management System. In: Proc. of the IEEE Symp. on Computers and Communications, pp. 800–805 (2008)
12. Munz, G., Carle, G.: Distributed Network Analysis Using TOPAS and Wireshark. In: Network Operations and Management Symp. Workshops, pp. 161–164 (2008)
13. Phaal, P., Panchen, S., McKee, N.: RFC 3176: InMon Corporation’s sFlow: A Method for Monitoring Traffic in Switched and Routed Networks (2001)
14. Plagemann, T., Goebel, V., Bergamini, A., Tolu, G., Urvoy-Keller, G., Biersack, E.W.: Using Data Stream Management Systems for Traffic Analysis – A Case Study. In: Barakat, C., Pratt, I. (eds.) PAM 2004. LNCS, vol. 3015, pp. 215–226. Springer, Heidelberg (2004)
15. Zhou, A., Yan, Y., Gong, X., Chang, J., Dai, D.: SMART: A System for Online Monitoring Large Volumes of Network Traffic. In: Proc. of the IEEE Int. Conf. on Data Engineering, pp. 1576–1579 (2008)