introduced by a correct authorized client to be executed by
at least f + 1 correct on-premises replicas (and thus have its
effects made visible) is upper bounded.
To guarantee bounded delay, we require that the conditions
of our threat model are met: at most f replicas are compro-
mised, at most one replica is undergoing proactive recovery,
and at most one site is downed or disconnected due to network
attack. In addition, the remaining replicas (i.e. all correct, non-
recovering replicas located outside the disconnected site) must
be able to communicate with one another, and the remaining
correct on-premises replicas must be able to communicate
with clients. Finally, communication among the remaining
correct replicas must meet the network stability requirements
of Prime [37], which is used as our underlying agreement
protocol and requires that the latency variance between each
pair of correct servers is bounded (see [4] for additional
discussion).
Our new contribution is to combine the above guarantees
with the following conﬁdentiality property.
Deﬁnition 3 (Complete Conﬁdentiality). System state and
state manipulation algorithms remain conﬁdential (known only
to on-premises replicas).
Our base system provides this guarantee as long as no on-
premises replica is compromised. An unlimited number of
data center replicas may be compromised without violating
conﬁdentiality. Note that a compromised client may always
leak its own state or updates; our model does not prevent this,
nor does any other conﬁdential BFT work we are aware of.
When we refer to system state in Deﬁnition 3, we refer to the
full state of the system maintained by the on-premises replicas.
Note that this guarantee is not comparable to those of the
conﬁdential BFT systems discussed in Section II-C: if any
on-premises server is compromised (over the entire lifetime
of the system), it can cause conﬁdentiality to be violated.
However, we argue that the novel combination of guarantees
we provide represents a signiﬁcant advance over the state
of the art. The Spire system provided a level of attack
resilience in terms of safety and performance guarantees
that was not possible before, but introduced a trade-off in
terms of conﬁdentiality. For a baseline system that provides
fault tolerance through standard primary-backup mechanisms
hosted fully on-premises, transitioning to the Spire architecture
offers much stronger safety and performance guarantees, but
at the cost of somewhat weaker conﬁdentiality guarantees. In
the baseline system, conﬁdentiality may be violated if an on-
premises server is compromised. However, if data center sites
are introduced, conﬁdentiality is violated if either a data center
server or an on-premises server is compromised, and even in
the case where no server is compromised, certain information
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
17
(cid:3)(cid:13)(cid:7)(cid:16)(cid:18)(cid:15)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:5)(cid:17)(cid:14)(cid:16)(cid:6)(cid:10)(cid:9)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:3)(cid:13)(cid:7)(cid:16)(cid:18)(cid:15)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:5)(cid:17)(cid:14)(cid:16)(cid:6)(cid:10)(cid:9)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:3)(cid:13)(cid:7)(cid:16)(cid:18)(cid:15)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:5)(cid:17)(cid:14)(cid:16)(cid:6)(cid:10)(cid:9)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:3)(cid:13)(cid:7)(cid:16)(cid:18)(cid:15)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:5)(cid:17)(cid:14)(cid:16)(cid:6)(cid:10)(cid:9)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:3)(cid:13)(cid:7)(cid:16)(cid:18)(cid:15)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:5)(cid:17)(cid:14)(cid:16)(cid:6)(cid:10)(cid:9)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:3)(cid:13)(cid:7)(cid:16)(cid:18)(cid:15)(cid:17)(cid:9)(cid:8)(cid:1)
(cid:5)(cid:17)(cid:14)(cid:16)(cid:6)(cid:10)(cid:9)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:3)(cid:7)(cid:15)(cid:7)(cid:1)(cid:2)(cid:8)(cid:12)(cid:15)(cid:8)(cid:13)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:4)(cid:12)(cid:16)(cid:5)(cid:13)(cid:8)(cid:11)(cid:9)(cid:14)(cid:8)(cid:14)(cid:1)(cid:6)(cid:9)(cid:15)(cid:8)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:3)(cid:7)(cid:15)(cid:7)(cid:1)(cid:2)(cid:8)(cid:12)(cid:15)(cid:8)(cid:13)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:4)(cid:12)(cid:16)(cid:5)(cid:13)(cid:8)(cid:11)(cid:9)(cid:14)(cid:8)(cid:14)(cid:1)(cid:6)(cid:9)(cid:15)(cid:8)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:1)(cid:9)(cid:4)(cid:5)(cid:3)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:15)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)(cid:17)(cid:11)(cid:14)(cid:13)(cid:1)
(cid:4)(cid:9)(cid:15)(cid:12)(cid:11)(cid:7)(cid:6)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:2)(cid:10)(cid:9)(cid:8)(cid:12)(cid:15)(cid:1)(cid:6)(cid:9)(cid:15)(cid:8)
(cid:1)(cid:9)(cid:7)(cid:11)(cid:12)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:1)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:2)(cid:8)(cid:4)(cid:6)(cid:3)(cid:10)
(cid:1)
(cid:2)(cid:10)(cid:9)(cid:8)(cid:12)(cid:15)(cid:1)(cid:6)(cid:9)(cid:15)(cid:8)
(cid:1)(cid:9)(cid:7)(cid:11)(cid:12)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
(cid:1)
(cid:1)(cid:4)(cid:3)(cid:2)(cid:5)(cid:6)
Fig. 1. System architecture overview, showing 2 on-premises sites (each containing 4 replicas) and 2 data centers (each containing 3 replicas).
is made accessible to the service provider managing the
data center servers. Our architecture eliminates this trade-off:
the strictly improved safety and performance guarantees are
provided while maintaining the same level of conﬁdentiality as
in the baseline system. This is likely to substantially increase
its acceptability to system operators. In Section V-D, we
discuss how the system can at least limit the amount of state
that can be disclosed if an on-premises server is compromised.
IV. PARTIALLY CLOUD-BASED BFT ARCHITECTURE
The key observation behind our system design, and the
reason that a straightforward separation of execution replicas
running in on-premises sites and agreement replicas running in
cloud data center sites cannot support our required guarantees,
is that network-attack resilience requires system state to be
stored in at least three distinct geographic sites. As discussed
in Section II-B, the work in [4] observed that because BFT
replication protocols require (more than) a majority of replicas
to be connected in order to safely make progress and order
updates, they cannot guarantee continuous availability in the
presence of network attacks unless at least 3 sites are used:
otherwise a network attack targeting a single site can isolate a
majority of the system, leaving the remainder unable to make
progress, and rendering the system unavailable.
In fact, exactly the same observation applies to the storage
of system state. To see why this is the case, consider a system
with exactly two on-premises sites. Under our threat model,
any one site may be disconnected at any time, so the system
must be able to make progress with only a single on-premises
site up and connected to the data center sites. Consider that on-
premises site A is up, connected to data center sites and client
sites, and receiving, submitting for ordering, and executing
incoming client updates, while on-premises site B is under
denial-of-service attack and isolated from the rest of the
network. Then, the attacker shifts focus to instead target on-
premises site A: site A is now isolated, while site B rejoins
the network and is now connected to the data center and client
sites. As before, the conditions of our threat model are met,
so the system should be able to process updates and make
progress. However, on-premises site B has missed all of the
client updates that were processed while it was disconnected. If
data center sites do not store any system state, it is impossible
for the replicas in site B to catch up and recover the state to
resume safely executing updates. In order to support our threat
model, a disconnected on-premises site must be able to rejoin
the network, catch up, and resume processing updates without
communicating with the other on-premises site.
Therefore, our approach is for data center replicas to
store encrypted updates and state checkpoints. By encrypting
updates and checkpoints with keys known only to the on-
premises replicas, we can allow data center replicas to store
them, without being able to decrypt and interpret them. This
allows a disconnected on-premises site to rejoin the network,
collect state, and resume processing updates based only on
information obtained from data center replicas, but without
requiring data center replicas to access unencrypted state or
perform any application-speciﬁc logic.
A. System Architecture
An overview of our architecture is shown in Figure 1. Our
high-level architecture is based on the Spire architecture [4].
System replicas are distributed across two on-premises sites
and a conﬁgurable number of data center sites. Sites are
connected through an instance of the Spines intrusion tolerant
network [33], [34] to provide resilience to a broad range of
network attacks. On-premises sites are additionally connected
to client sites through a separate Spines instance. Proxies
support clients that cannot be modiﬁed to use a BFT protocol.
Clients in a single physical location may be grouped behind a
single proxy, or each client can have its own proxy. A proxy
collects updates from its respective client(s), digitally signs
them so that server replicas can verify their authenticity, and
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
18
submits them to the system by sending them to on-premises
servers. Client proxies also validate responses received from
the server replicas: speciﬁcally, server replicas generate thresh-
old signatures on responses using an (f + 1, n)-threshold
scheme, so the proxy can verify a single signature to conﬁrm
that at least one correct replica agreed on the message.
The key difference from [4] is the separation of functionality
between on-premises and data center replicas. In our system,
all replicas host an instance of the Prime intrusion-tolerant
replication engine [38] and participate in the replication pro-
tocol. However, only on-premises servers host application
replica instances. Client updates received by on-premises
servers are encrypted before being submitted for ordering
and sent to data center servers. Post-ordering, updates are
decrypted and executed at (only) the on-premises application
replicas, while those same updates are stored in encrypted
form at the data center servers.
B. Replica Distribution
In conﬁguring the system, replicas must be distributed
across sites such that the system is able to safely process
updates and meet its bounded delay guarantee under the full
threat model we consider. Prime (when conﬁgured to support
proactive recovery, as in [4]) requires a total of 3f + 2k + 1
replicas to tolerate f compromised replicas and k unavailable
replicas, where a replica may be unavailable either because it
is going through proactive recovery or because it has been
disconnected from the network (or because it has simply
crashed). In order to guarantee progress, with bounded delay,
at least 2f + k + 1 of those replicas must be up, correct, and
connected (with sufﬁcient network stability).
(cid:3)
(cid:2)
S−2
3f +S+1
The work in [4] showed that in order to ensure 2f + k + 1
correct replicas are always available, it is necessary to ensure
that no single site contains more than k− 1 servers: otherwise
the disconnection of a single site, plus an ongoing proactive
recovery elsewhere in the system could cause more than k
replicas to become unavailable at the same time, preventing the
system from making progress. That work shows that providing
this guarantee requires setting k ≥
, where S is
the total number of sites (on-premises + data center), and
distributing replicas as evenly as possible across sites [4], [39].
However, an additional constraint under our threat model
comes from the separation of
functionality between on-
premises and data center replicas: only on-premises replicas
can execute updates and communicate with clients. To verify
that a received message is correct, a client must be able to
conﬁrm that f + 1 servers agreed to it (to ensure at least
one correct server was involved). This means that generating
veriﬁable responses requires that f +1 on-premises replicas are
available at any time: data center replicas cannot participate
in generating client responses, as this requires knowledge of
update contents, system state, and application logic.
In the worst case, under our threat model, one of the two
on-premises sites may be disconnected, and the other may
contain f compromised replicas and one replica undergoing
proactive recovery. Therefore, in order to ensure that f + 1
2 on-premises
+ 1 data center
6+6+6 (18)
9+9+9 (27)
12+12+12 (36)
2 on-premises
+ 2 data centers
4+4+3+3 (14)
6+6+5+4 (21)
8+8+6+6 (28)
2 on-premises
+ 3 data centers
4+4+2+2+2 (14)
6+6+3+3+3 (21)
8+8+4+4+4 (28)
f = 1