X + Z
V
V
H(V) + H(X)
H(V)+H(X)+H(Z)
H(X) + H(Z)
H(V)
idSum:
hashSum:
count:
Index:
2
0
V
X
X
1
1
V
X
Z
0
2
X
Z
X + Z
X + Z
idSum:
hashSum:
count:
Index:
H(X)
H(X) + H(Z)
H(X) + H(Z)
1
0
0
1
0
2
Pure:
DA-B:
DB-A:
{3, 4}
{V}
{}
Z
Z
H(Z)
-1
4
Pure:
DA-B:
DB-A:
{4}
{V}
{}
Z
Z
H(Z)
-1
4
1
3
V
0
0
0
3
Step 4: Update Pure List
Step 3: Remove
Figure 3: IBF Decode. We ﬁrst scan the IBF for pure cells and add these indices (3&4) to the Pure list (Step 1). In Step 2, we dequeue
the ﬁrst index from the Pure list, then add the value of the idSum to the appropriate output set (V → DA−B). We then remove V
from the IBF by using the k hash functions to ﬁnd where it was inserted during encoding and subtracting it from these cells. Finally,
if any of these cells have now become pure, we add them to the Pure list (Step 4). We repeat Steps 2 through 4 until no items remain
in the Pure list.
item in S is inserted into B, and each item in T is deleted from B.
Then with probability at most O(d−k) the IBFDecode operation
will fail to correctly recover S and T .
PROOF. The proof follows immediately from the previous anal-
ysis for invertible Bloom ﬁlters (e.g., see [13]).
PROOF. Let Yj be the size of the union of strata numbered j
or greater for j = 0, 1, . . . , i, and let µj be its expectation, that
is, µj = E(Yj) = m/2j. By standard Chernoff bounds (e.g.,
see [18]), for δ > 0,
We also have the following.
and
COROLLARY 1. Let S and T be two sets having at most d ele-
ments in their symmetric difference, and let BS and BT be invert-
ible Bloom ﬁlters, both with the parameters as stated in Theorem 1,
with BS representing the set S and BT representing the set T .
Then with probability at most O(d−k) we will fail to recover S
and T by applying the IBFSubtract operation to BS and BT and
then applying the IBFDecode operation to the resulting invertible
Bloom ﬁlter.
PROOF. Deﬁne S′ = S − T and T ′ = T − S. Then S′ and T ′
are disjoint sets of total size at most d, as needed by Theorem 1.
The corollary implies that in order to decode an IBF that uses 4
independent hash functions with high probability, then one needs
an overhead of k + 1 = 5. In other words, one has to use 5d cells,
where d is the set difference. Our experiments later, however, show
that an overhead that is somewhat less than 2 sufﬁces. This gap is
narrowed in [13] leveraging results on ﬁnding a 2-core (analogous
to a loop) in random hypergraphs.
Let us next consider the accuracy of our stratiﬁed size estimator.
Suppose that a set S has cardinality m, and let i be any natural num-
ber. Then the ith stratum of our strata estimator for S has expected
cardinality m/2i. The following lemma shows that our estimators
will be close to their expectations with high probability.
Pr (Yj > (1 + δ)µj)  1, with probability 1 − 2−s, and for
all j < i, the cardinality of the union of the strata numbered j or
greater is within a multiplicative factor of 1 ± O(ps2i/m) of its
expectation.
PROOF. By the same reasoning as in Corollary 1, each IBF at
level i in the estimator is a valid IBF for a sample of the symmetric
difference of S and T in which each element is sampled with prob-
ability 1/2i+1. By Theorem 1, having each IBF be of C = (k+1)g
cells, where k = ⌈log 1/ǫ⌉ and g ≥ 2, then we can decode a set of
g elements with probability at least 1 − ǫ/2.
We ﬁrst consider the case when d ≤ c2
0δ−2 log(1/ǫ), where c0 is
the constant in the big-oh of Lemma 1. That is, the size of the sym-
metric difference between S and T is at most a constant depending
only on ǫ and δ. In this case, if we take
g = max ˘2, ⌈c2
0δ−2 log(1/ǫ)⌉¯
and k = ⌈log 1/ǫ⌉, then the level-0 IBF, with C = (k + 1)d
cells, will decode its set with probability at least 1 − ǫ/2, and our
estimator will learn the exact set-theoretic difference between S
and T , without error, with high probability.
Otherwise, let i be such that d/2i ≈ c2
0δ2/ log(1/ǫ) and let
g = max ˘2, ⌈c2
0δ−2 log(1/ǫ)⌉¯ and k = ⌈log 1/ǫ⌉, as above.
So, with probability 1 − ǫ/2, using an IBF of C = (k + 1)d cells,
we correctly decode the elements in the ith stratum, as noted above.
By Lemma 1 (with s = ⌈log 1/ǫ⌉ + 1), with probability 1 − ǫ/2,
the cardinality of the number of items included in the ith and higher
strata are within a multiplicative factor of 1 ± δ of its expectation.
Thus, with high probability, our estimate for d is within a 1 ± δ
factor of d.
Comparison with Coding: Readers familiar with Tornado codes
will see a remarkable similarity between the decoding procedure
used for Tornado codes and IBF’s. This follows because set recon-
ciliation and coding are equivalent. Assume that we have a system-
atic code that takes a set of keys belonging to a set SA and codes
it using check words C1 through Cn. Assume further that the code
can correct for up to d erasures and/or insertions. Note that Tornado
codes are speciﬁed to deal only with erasures not insertions.
Then, to compute set difference A could send the check words
C1 through Cn without sending the "data words" in set SA. When
B receives the check words, B can treat its set SB as the data
words. Then, together with the check words, B can compute SA
and hence the set difference. This will work as long as SB has at
most d erasures or insertions with respect to set SA.
Thus any code that can deal with erasures and insertions can be
used for set difference, and vice versa. A formal statement of this
equivalence can be found in [15]. This equivalence explains why
the existing deterministic set difference scheme, CPI, is analogous
to Reed-Solomon coding. It also explains why IBF’s are analogous
to randomized Tornado codes. While more complicated Tornado-
code like constructions could probably be used for set difference,
the gain in space would be a small constant factor from say 2d to
d + ǫ, the increased complexity is not worthwhile because the real
gain in set reconciliation is going down from O(n) to O(d), where
n is the size of the original sets.
5. THE KEYDIFF SYSTEM
We now describe KeyDiff, a service that allows applications to
compute set differences using Difference Digests. As shown in
Figure 4, the KeyDiff service provides three operations, add, remove,
and diff. Applications can add and remove keys from an instance
of KeyDiff, then query the service to discover the set difference be-
tween any two instances of KeyDiff.
Suppose a developer wants to write a ﬁle synchronization appli-
cation. In this case, the application running at each host would map
ﬁles to unique keys and add these keys to a local instance of Key-
Diff. To synchronize ﬁles, the application would ﬁrst run KeyDiff’s
diff operation to discover the differences between the set stored
locally and the set stored at a remote host. The application can then
perform the reverse mapping to identify and transfer the ﬁles that
differ between the hosts.
Application
Application Data
Application
Add( key )
Remove( key )
Diff( loc1, loc2 )
KeyDiff
e
c
n
e
r
e
f
D i f
s
t
s
e
  D i g
KeyDiff
Application
KeyDiff
Figure 4: KeyDiff computes the set difference between any two
instances and returns this information to the application.
The KeyDiff service is implemented using a client-server model
and is accessed through an API written in C. When a client re-
quests the difference between its local set and the set on a remote
host, KeyDiff opens a TCP connection and sends a request contain-
ing an estimator. The remote KeyDiff instance runs the estimation
algorithm to determine the approximate size of the difference, then
replies with an IBF large enough for the client to decode with high-
probability. All requests and responses between KeyDiff instances
travel over a single TCP connection and the diff operation com-
pletes with only a single round of communication.
KeyDiff provides faster diff operations through the use of pre-
computation. Internally, KeyDiff maintains an estimator structure
that is statically sized and updated online as keys are added and re-
moved. However, computing the IBF requires the approximate size
of the difference, a value that is not know until after the estima-
tion phase. In scenarios where computation is a bottleneck, Key-
Diff can be conﬁgured to maintain several IBF’s of pre-determined
sizes online. After the estimation phase, KeyDiff returns the best
pre-computed IBF. Thus, the computational cost of building the
IBF can be amortized across all of the calls to add and remove.
This is reasonable because the cost of incrementally updating
the Strata Estimator and a few IBF’s on key update is small (a few
microseconds) and should be much smaller than the time for the
application to create or store the object corresponding to the key.
For example, if the application is a P2P application and is synchro-
nizing ﬁle blocks, the cost to store a new block on disk will be at
least a few milliseconds. We will show in the evaluation that if the
IBF’s are precomputed, then the latency of diff operations can be
100’s of microseconds for small set differences.
6. EVALUATION
Our evaluation seeks to provide guidance for conﬁguring and
predicting the performance of Difference Digests and the KeyDiff
system. We address four questions. First, what are the optimal pa-
rameters for an IBF? (Section 6.1). Second, how should one tune
the Strata Estimator to balance accuracy and overhead? (Section 6.2).
Third, how do IBF’s compare with the existing techniques? (Section 6.3).
Finally, for what range of differences are Difference Digests most
effective compared to the brute-force solution of sending all the
keys? (Section 6.4).
Our evaluation uses a U of all 32-bit values. Hence, we allo-
cate 12 bytes for each IBF cell, with 4 bytes given to each idSum,
hashSum and count ﬁeld. As input, we created pairs of sets
containing keys from U. The keys in the ﬁrst set, SA, were chosen
randomly without replacement. We then chose a random subset of
SA and copied it to the second set, SB. We exercised two degrees
of freedom in creating our input sets: the number of keys in SA,
and the size of the difference between SA and SB, which we refer
to as the experiment’s delta. For each experiment we created 100
d
e
d
o
c
e
D
y
l
l
u
f
s
s
e
c
c
u
S
t
n
e
c
r
e
P
 100
 80
 60
 40
 20
 0
100 keys
1K keys
10K keys
100K keys
1M keys
)
%
i
(
s
g
n
d
o
c
e
D
l
u
f
s
s
e
c
c
u
S
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Hash Cnt = 2
Hash Cnt = 3
Hash Cnt = 4
Hash Cnt = 5
Hash Cnt = 6
0
5
10
15
20
25
30
35
40
45
50
 0
 10
 20
 30
 40