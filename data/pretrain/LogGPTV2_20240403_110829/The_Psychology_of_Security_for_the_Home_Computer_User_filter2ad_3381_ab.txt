TPB [22] identiﬁes intention as the primary determinant
of speciﬁc behavior and focuses on three motivational factors
that directly predict intention to behave:
1) Attitude toward the speciﬁc behavior: person’s incli-
nation to perform some behavior; is it valued?
2) Subjective Norms: person’s perception of society’s
view on whether to perform the speciﬁc behavior.
3) Perceived Behavioral Control: how capable the person
thinks he/she is to perform the speciﬁc behavior.
Perceived Behavioral Control is considered quite important
because even should a person have the wherewithal to act,
his or her perception of his or her own inadequacy may
preclude acting. Additionally, Perceived Behavioral Control
can be used as a proxy for measures of actual control. Thus,
models can include both direct and indirect paths. The model
is often quantiﬁed as a regression model. Models based on
TPB have been used with considerable success (accounting
for a good portion of the variance) to predict adoption of a
wide variety of behaviors.
PMT [23] was developed to explain how appeals to fear
can change behavior. The theory has three components:
perceptions of the severity of the threat, probability of its
occurrence, and efﬁcacy of the protective response. Percep-
tions of threat can include both costs of the risky behavior
as well as costs of avoiding it, e.g., by not giving out
some information, you may not be able to access an online
service of interest. Self-efﬁcacy is a person’s belief in his
or her own ability to produce the intended effects through
behaviors [14]. Self-efﬁcacy can be divided into response
efﬁcacy (belief the action will be effective) and coping self-
efﬁcacy (belief in one’s own ability) [24]. The behaviors can
be adaptive (protective) or maladaptive (avoidance); a core
idea is that high self-efﬁcacy results in adaptive behaviors.
As with TPB, the three components can be viewed as factors
in factor analysis or regression modeling to predict
the
intention to behave in the desired manner.
IV. SECURITY RISK AND CONSEQUENCES
In [25], Ryan West summarizes principles of psychology
as related to computer security: people underestimate risk,
people have limited time and mental resources, security
consequences are hard for people to assess because they
are abstract and hypothetical and that losses are perceived
as higher magnitude than gains. These principles provide an
211
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:21 UTC from IEEE Xplore.  Restrictions apply. 
excellent general starting point to identifying factors that
inﬂuence home users’ understanding of security risk and
consequences. User studies have assessed users’ knowledge
of the range of security and privacy threats, and of what
might happen as a consequence of them. In some cases,
researchers have tried to capture the cognitive models that
users develop to understand the threats, with the goal of
improving education by explaining security in terms that
users are more likely to understand.
A. Understanding of Threats
trojan, keystroke logger,
People use metaphors or mental models to think about
complex processes. Camp et al. [26] collected conceptual
models from the security literature and compared how
different groups (experts and non-experts) associated these
models with key terms from security. The ﬁve models3
were: Physical Safety, Medical Infections, Criminal Be-
havior, Warfare, and Economic Failure. The security terms
were:
junk mail, virus, worm,
hacking, binder, exploit, zombie, authentication, click fraud,
password, userid, ﬁrewall, backdoor, blacklist, spooﬁng, ad-
dress book, honeypot, drive-by-download, dos attack, spam,
phishing, spyware, adware, cookies, and identity theft. They
used a card sorting procedure in which subjects were asked
to assign each term a color that corresponded to one of the
models; the subject’s ability to do this task was checked
by having him or her assign words chosen as synonyms
from a thesaurus. The study showed that subjects found
some words, e.g., ﬁrewall and userID, difﬁcult to categorize.
The data also showed considerable disagreement between
the experts and non-experts in categorization of words to
models, especially as the deﬁnition of expert was narrowed
to require ﬁve years of computer security experience. This
study suggested that security experts may have difﬁculty
communicating threats and risk to non-expert users and that
other ways of educating users about security are needed.
their understanding of privacy and security,
Mental models of attitudes towards threats can also be
assessed as associations between terms. Diesner et al. [13]
conducted interviews with a cross-section of 29 educated
adults in India to determine their attitudes about privacy
and security. Participants were asked 17 open questions
about
their
knowledge of risks and protections, and their concerns about
availability of personal data on computers. The goal was
to determine how much importance/concern was placed on
security and privacy. The data were analyzed by identifying
and extracting key terms as concepts and then organizing
the concepts in a network of relations (in a process called
“map analysis”). The resulting networks (mental models)
suggested that participants focused on personal information
and knowledge as the crux of security and privacy, that
security related terms are not central concepts in subjects’
minds, and that concerns about privacy and security do not
appear as prominent as concepts with positive meanings.
Another approach to understanding how home computer
users view threats is to construct “folk models” of their
knowledge and perceptions. Wash [16] conducted semi-
structured interviews with 23 participants in a ﬁrst round
and 10 in a second round in which he asked about their
perception of threats and defensive actions. Participants were
found via a snowball sample of home computer users in three
cities in the U.S.A. The ﬁrst round explored familiarity with
security problems and countermeasures; the second round
introduced hypothetical scenarios and asked participants for
their reaction. From the qualitative data, Wash identiﬁed four
folk models concerning viruses4:
• Bad reﬂected low understanding of how viruses were
created and a vague notion that viruses could have ’bad’
consequences.
• Buggy Software characterized viruses as regular soft-
ware that includes a lot of bad bugs. The software
must be intentionally placed on the computer and has
consequences similar to that of software with bugs (e.g.,
crashing, deleting data).
• Mischief allowed that viruses are created by malicious
hackers and have annoying consequences such as those
in the ’Buggy Software’ model. Viruses are caused by
actively clicking on attachments or by visiting ‘bad’
parts of the Internet.
• Crime viewed the purpose of viruses to be to collect
personal information to be used by criminals without
otherwise harming the computer. Viruses can be ac-
quired by clicking on attachments, downloaded from
websites or actively placed on the computer by hackers.
The group who viewed security as the ‘Bad’ model were
generally unconcerned with the threat. The ‘Buggy Soft-
ware’ group were also not concerned with the threat because
they avoided downloading software they did not trust. The
‘Mischief’ and the ‘Crime’ groups appeared more concerned
with the threat. Finally some participants believed multiple
of the models. These models suggest that home computer
users have little understanding of malware threats.
Three categories of concern (people, information, tech-
nology) were found in a study comparing conceptions of
Web risks and harms among different communities (rural,
suburban, high-tech) in 2002 [12]. People reﬂected concerns
about
issues, and was
further divided into: user’s experience, trust, online identity,
online interactions, children’s welfare and other. Information
captured the handling and dissemination of data, speciﬁcally:
quality, management, security, privacy, content, spam and
the online experience and social
3In [27], the models are described in some detail and another model,
Market, is added to the set.
4Because all participants used the term “virus” to encompass malicious
software, in these models, “virus” referred to a variety of malware including
viruses, adware, and spyware.
212
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:21 UTC from IEEE Xplore.  Restrictions apply. 
other. Technology referred to harms to the hardware and
software, and was divided further into: speciﬁc technologies,
threat to computer systems and other. The interview allowed
participants to describe their own concerns, including if they
had none.
Several other trends were noted in the results [12]. First,
each community differed in their concerns: 21% of the rural
participants had no concerns; all of the participants from
other groups had concerns. There were no observed differ-
ences in the level of concern about information security and
information privacy (21% of rural for both, 46% of suburban
for both and 63% of high-tech for both); this suggests that
the participants may not have been distinguishing the two.
Second, the participants did agree about the importance of
a few categories. For each of the groups, the issue with
the highest percentage of participants concerned was: “threat
to computer systems” (at 38%) for rural and “security” or
“privacy” for suburban and high tech (at 46% and 63%,
respectively). All three were generally more concerned about
“information” than the other two categories. These results
can be interpreted to mean that security solutions need to be
adjusted to suit the groups or alternatively that the suburban
and rural groups may develop more toward those of the high-
tech group with increased experience. Given the age of this
study, a followup should be conducted to see how the trends
have changed.
Semi-structured interviews of 33 people from 15 house-
holds had the goal of understanding how home users viewed
access to personal ﬁles and data and what would constitute
an ideal access control policy [28]. The participants were
solicited through ﬂyers and distribution lists. The study
found that most participants (18 our of 33) were highly
concerned about unauthorized access to personal data.
The U.K. survey [5] showed a high self-reported level
of understanding of key security threat terms: > 90% for
“virus”, “hacker”, “ﬁrewall” and “identity theft”, > 80% for
“spyware”, “worm” and “trojan horse”, 68% for “phishing”.
The authors pointed out that although they cast a wide net for
recruitment most of the participants reported a higher than
average level of education. Paradoxically, the knowledge
does not appear to translate into action;
the percentage
of users who had installed software to address the threats
was smaller than their knowledge of the terminology was:
22% lower for spyware, 8% lower for anti-spam installation
versus phishing, and 9% lower for ﬁrewall.
Unfortunately, self-reported knowledge and understanding
of threats may not translate into safe behavior. In a sim-
ulation study of phishing attacks [29], even sophisticated
users could be fooled by faked websites. The study asked
participants to judge a set of 20 websites for whether
each was legitimate or a fake. Nine of the websites were
selected from actual phishing attacks identiﬁed in summer
2005;
three were constructed using advanced techniques
identiﬁed by security organizations that monitor phishing
attacks; one website required the user to accept a self-signed
SSL certiﬁcate, and the remaining seven were legitimate.
Participants were given access to a fully functioning website
for each using a Firefox browser under Mac OS X; they
were told they could interact with the browser as normal
and allowed them to open other browser windows if they
wished. The 22 participants included students and staff at
a U.S.A. university who were given $15 for their time. All
participants were familiar with the use of computers and the
Web; 86% were in non-technical ﬁelds.
In judging the 20 websites, correct identiﬁcation scores
varied from 6 to 18, with a mean of 11.6. Participants who
judged only from the content of the website (e.g., logos,
designs,
information displayed) had the lowest accuracy
(mean=7.6). The most successful participants also looked at
domain name, preﬁx of HTTPS and presence of a padlock
icon – even when they reported that they did not know what
an IP address or SSL was. 15 participants clicked “OK”
on the self signed security certiﬁcate without reading the
warning. Generally, the authors concluded that participants
lacked critical computer and security knowledge (e.g., seven
had never heard of phishing before), but that more experi-
enced participants could still be fooled by visual deceptions.
B. Perceptions of Risky Behavior
In studies and surveys, risk has been examined as a
general characteristic of activities online or with respect
to speciﬁc behaviors. When asked about their conﬁdence
in the overall security of their computers, the U.K. survey
found that 70% were satisﬁed/conﬁdent, and that level of
conﬁdence was correlated with level of experience (the more
experienced, the more conﬁdent)[8]. Similarly, in the Online
Safety Study in 2010 [7], 85% of the 3,500 respondents
felt that their home computers were secure; their primary
security/privacy concern was identity theft.
When asked about speciﬁc behaviors in a 2004 study of
undergraduates [6], fewer than 12% felt there were no nega-
tive consequences from the following risky behaviors: open-
ing email attachments, sharing passwords, and not backing
up their disks. They were most aware of the problem with
email attachments, with all but 4% recognizing the possible
danger. They did not, however, think they personally would
suffer as 40% estimated that
the negative consequences
would happen to them “never” or “rarely”.
Security measures to discourage unsafe behavior and
heighten a user’s perception of risk are often designed
around cues speciﬁc to situations. For example, secure
transactions on the Web should be done via an https URL,
something a user should look for when banking or shopping.
Two studies of students at the University of South Australia
examined the effect of graphics in emails to help identify
phishing threats [30]. The ﬁrst study showed 75 students a
survey form that contained a risk message and a semantic
differential grid (a grid that solicits opinions about charac-
213
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:21 UTC from IEEE Xplore.  Restrictions apply. 
teristics of the message); one group received the form with a
graphic embedded behind the message and the other without.
The semantic grid collected reactions on three dimensions
(evaluation, potency and activity) captured as characteristics
such as active versus passive and strong versus weak. They
found no signiﬁcant difference with or without the graphic.
A second study tried moving the graphic and also showed no
signiﬁcant difference. Thus, it is hard to inﬂuence a user’s
perception, and graphics may not be the way to emphasize
risk in a computer message.
Different online activities incur different levels of risk and
exposure to threats. Milne et al. [14] hypothesized that self-
efﬁcacy was a strong determinant in a shopper’s tendency to
engage in risky behaviors online. They developed a protec-
tion motivation model based on PMT that relates perceived
online threats, perceived likelihood of online threat and
self-efﬁcacy to adaptive (actions taken with some business