title:Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
author:Lifan Mei and
Runchen Hu and
Houwei Cao and
Yong Liu and
Zifa Han and
Feng Li and
Jin Li
Realtime Mobile Bandwidth Prediction
Using LSTM Neural Network
Lifan Mei1(B), Runchen Hu1, Houwei Cao2, Yong Liu1, Zifa Han3,
Feng Li3, and Jin Li3
1 ECE, New York University, New York City, NY 11201, USA
{lifan,rh2619,yongliu}@nyu.edu
2 CS, New York Institute of Technology, New York City, NY 10023, USA
PI:EMAIL
3 Huawei Technologies, Nanjing, China
{hanzifa,frank.lifeng,mark.lijin}@huawei.com
Abstract. With the popularity of mobile access Internet and the higher
bandwidth demand of mobile applications, user Quality of Experience
(QoE) is particularly important. For bandwidth and delay sensitive appli-
cations, such as Video on Demand (VoD), Realtime Video Call, Games,
etc., if the future bandwidth can be estimated in advance, it will greatly
improve the user QoE. In this paper, we study realtime mobile band-
width prediction in various mobile networking scenarios, such as subway
and bus rides along diﬀerent routes. The main method used is Long
Short Term Memory (LSTM) recurrent neural network. In speciﬁc sce-
narios, LSTM achieves signiﬁcant accuracy improvements over the state-
of-the-art prediction algorithms, such as Recursive Least Squares (RLS).
We further analyze the bandwidth patterns in diﬀerent mobility scenar-
ios using Multi-Scale Entropy (MSE) and discuss its connections to the
achieved accuracy.
Keywords: Bandwidth prediction · Long Short Term Memory ·
Multi-Scale Entropy · Bandwidth measurement
1 Introduction
We have witnessed the tremendous growth of mobile traﬃc in the recent years.
Users are increasingly spending more time on mobile apps and consuming more
content on their mobile devices. The growth trend is expected to accelerate in the
foreseeable future with the introduction of 5G wireless access and new media-rich
applications, such as Virtual Reality and Augmented Reality. However, one main
challenge for mobile app developers and content providers is the high volatility
of mobile wireless connections. The physical channel quality of a mobile user is
constantly aﬀected by interference generated by other users, his/her own mobil-
ity, and signal blockages from static and dynamic blockers [8,9]. The bandwidth
available for a mobile session is ultimately determined by the adaptations cross
c(cid:2) Springer Nature Switzerland AG 2019
D. Choﬀnes and M. Barcellos (Eds.): PAM 2019, LNCS 11419, pp. 34–47, 2019.
https://doi.org/10.1007/978-3-030-15986-3_3
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
35
the protocol stack, ranging from adaptive coding and modulation at PHY layer,
cellular scheduling at data link layer, hand-overs between base stations, to TCP
congestion control, etc. For many mobile apps involving user interactivity and/or
multimedia content, e.g., gaming, conferencing and video streaming, it is criti-
cal to accurately estimate the available bandwidth in realtime to deliver a high
quality of user Quality-of-Experience (QoE). In the example of video streaming,
many recent algorithms on Dynamic Adaptive Streaming over Http (DASH)
optimize the video rate selection for incoming video chunks based on the pre-
dicted TCP throughput in a future time window of several seconds [5,11,15].
To cope with the unavoidable TCP throughput prediction errors, one has to be
conservative in video rate selection and resort to long video buﬀering to absorb
the mismatch between the predicted and actual TCP throughput. Both degrade
user video streaming QoE. Interactive video conferencing has much tighter delay
constraint than streaming. To avoid self-congestion, the available bandwidth on
cellular link has to be accurately estimated in realtime, which is used to guide
the realtime video coding and transmission strategies [10,16]. Bandwidth overes-
timate will lead to long end-to-end video delay or freezing, bandwidth underes-
timate will lead to unnecessarily poor perceptual video quality. Again, accurate
realtime bandwidth prediction is crucial for delivering good conferencing expe-
rience, especially in mobile networking scenarios.
In this paper, we study realtime mobile bandwidth prediction using Long
Short Term Memory (LSTM) [1] recurrent neural network. Recent advances in
Deep Learning have demonstrated that Recurrent Neural Networks (RNN) are
powerful tools for sequence modeling and can learn temporal patterns in sequen-
tial data. RNNs have been widely used in Natural Language Processing (NLP),
speech recognition and time series processing [17,18]. There are rich structures
in realtime mobile network bandwidth evolution, due to user mobility patterns,
wireless signal propagation laws, physical blockage models, and the well-deﬁned
behaviors of network protocols. This presents abundant opportunities for devel-
oping LSTM-based realtime mobile bandwidth estimation. The main idea is to
oﬄine train LSTM RNN models that capture the temporal patterns in various
mobile networking scenarios. The trained LSTM RNN models will be used online
to predict in realtime the network bandwidth within a short future time window.
Speciﬁcally, we investigate the following research questions:
1. How much prediction accuracy improvement can LSTM deep learning models
bring over the conventional statistical prediction models?
2. How predictable is realtime bandwidth at diﬀerent prediction intervals under
diﬀerent mobility scenarios? Is the LSTM prediction accuracy dependent on
speciﬁc mobility scenarios?
3. Should one train a separate LSTM model for each mobility scenario, or train
a universal LSTM model that can be used in diﬀerent scenarios?
36
L. Mei et al.
Towards answering these questions, we made the following contributions:
– We conducted a mobile bandwidth measurement campaign to collect consec-
utive bandwidth traces in New York City. Our traces cover diﬀerent trans-
portation methods along diﬀerent routes at diﬀerent time of day1.
– We developed LSTM models for realtime one-second ahead and multi-second
ahead bandwidth predictions. Through extensive experiments on our own
dataset and the HSDPA dataset [7], we demonstrated that LSTM signiﬁcantly
outperforms the existing realtime bandwidth prediction algorithms.
– We systematically evaluated the sensitivity of LSTM models to diﬀerent
mobility scenarios by comparing the performance of per-scenario, cross-
scenario and universal predictions. Using Multi-Scale Entropy (MSE) anal-
ysis, we studied the connection between prediction accuracy and bandwidth
regularity at diﬀerent time scales. MSE also provides us with guidelines to
explore cross-scenario bandwidth prediction.
The rest of the paper is organized as the following. The related work on real-
time bandwidth prediction is reviewed in Sect. 2. We formally deﬁne the realtime
bandwidth prediction problem and introduce our LSTM based prediction mod-
els in Sect. 3. The performance of LSTM models is evaluated by public dataset
and our own dataset in Sect. 4. We conduct Multi-Scale Entropy analysis on our
collected bandwidth traces and analyze the prediction accuracy in Sect. 5. The
paper is concluded with future work in Sect. 6.
2 Related Work
Realtime bandwidth prediction has been a challenging problem for the network-
ing community. Simple history-based TCP throughput estimation algorithm was
proposed in [12]. Authors of [13] proposed to train a Support Vector Regress
(SVR) model [14] to predict TCP throughput based on the measured packet
loss rate, packet delay and the size of ﬁle to be transmitted. In the context
of DASH video streaming, in [11], we adopted prediction algorithm in [12] to
guide realtime chunk rate selection, and used a customized SVR model similar
to [13] for DASH server selection. Authors of [20] and [15] used the Harmonic
Mean of TCP throughput for downloading the previous ﬁve chunks as the TCP
throughput prediction for downloading the next chunk. In [5], authors devel-
oped Hidden Markov Model (HMM) for bandwidth prediction. HMM model is
parameterized by history bandwidth, and HMM state transition is used to infer
future bandwidth. In the context of video conferencing, in [16], a cellular link is
modeled as a single-server queue driven by a doubly-stochastic service process.
Bandwidth available for a user is measured by the packet arrival dispersion at
the receiver end, and future bandwidth prediction is generated by probabilistic
inference based on the single-server queue model. In [10], we used an adaptive
1 The collected NYU Metropolitan Mobile Bandwidth Trace Dataset (NYU-METS),
is publicly available at https://github.com/NYU-METS/Main.
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
37
ﬁlter, Recursive Least Squared (RLS), to make realtime bandwidth prediction.
We showed that RLS achieves good prediction accuracy on volatile cellular links.
Based on the accurate bandwidth prediction, they proposed a new video con-
ferencing system that can deliver higher video rate and lower video delay than
Facetime in side-by-side comparisons.
All the previous predictors are based on the conventional statistical or
machine learning models and generate predictions based on short bandwidth
history. Diﬀerent from the conventional models, LSTM deep learning models are
more ﬂexible and can be trained by large datasets to better capture the long-
term and short-term temporal structures in bandwidth time series. A recent
work on Deep Reinforcement Learning (DRL) based DASH [19] takes historical
bandwidth samples as part of the input state vector for DRL to directly gener-
ate video chunk rate selection. DRL based DASH achieves better performance
and robustness than the traditional DASH. While DRL-DASH implicitly mines
the temporal structure in bandwidth, there is no direct/explicit training and
validation optimized for bandwidth prediction.
3 LSTM Based Realtime Bandwidth Prediction
3.1 Realtime Bandwidth Prediction Problem
Let x(t) be the bandwidth available for a user at time t. Given some band-
width measurement frequency, one can obtain a discrete-time series of {x(t), t =
1, 2,··· .}. The realtime bandwidth prediction problem at time t is to estimate
the bandwidth available for a user at some future time instant x(t + τ) given all
the observed bandwidth measurements so far, i.e.,
ˆx(t + τ) = f ({x(k), k = 1, 2,··· , t}) .
(1)
There are many ways to build the estimation function f(·), ranging from sim-
ple history-repeat, i.e., ˆx(t + τ) = x(t), Exponential Weighted Moving Aver-
age (EWMA), ˆx(t + 1) = (1 − α)ˆx(t) + αx(t), Harmonic Mean, ˆx(t + τ) =
(cid:2)h−1
k=0 1/x(t − k), etc., to more sophisticated signal processing approaches,
h/
such as Kalman ﬁlter [6] and Recursive Least Squares (RLS) [3]. In [10],
we used RLS for realtime bandwidth prediction. By assuming ˆx(t + 1) =
(cid:2)h−1
ω(k)x(t − k), RLS recursively ﬁnds the coeﬃcients ω that minimizes a
k=0
weighted linear least squares cost function.
In the bandwidth prediction part of [10], it was shown that RLS achieves
better accuracy than other averaging and signal processing algorithms, such as
Least Mean Square and EWMA etc.
3.2 LSTM-Based Prediction Model
While all those methods use history measurements to generate bandwidth pre-
diction, they did not fully explore the temporal patterns in realtime bandwidth
evolution for more accurate prediction. Meanwhile, LSTM network has recently
38
L. Mei et al.
(a) LSTM Network Architecture
(b) Internal Structure of LSTM Unit
Fig. 1. LSTM network for realtime bandwidth prediction
emerged as a powerful tool for exploring temporal structures in sequential data.
As illustrated in Fig. 1a, a LSTM network consists of layers of LSTM units.
As illustrated in Fig. 1b, a common LSTM unit is composed of a cell, an input
gate, an output gate and a forget gate. The cell is responsible for “memorizing”
values over arbitrary time intervals; hence the word “memory” in LSTM. Each
of the three gates can be thought of as a “conventional” artiﬁcial neuron, as in a
multi-layer (or feed-forward) neural network: they compute an activation (using
an activation function) of a weighted sum. Intuitively, they can be considered
as regulators of the ﬂow of values going through the connections between the
LSTM units; hence the denotation “gate”. There are connections between these
gates and the cell. Detailed LSTM reviews can be found in [1,2].
The input to our LSTM bandwidth prediction network is the recent band-
width measurements, i.e, x = [x(t), x(t − 1),··· , x(t − n + 1)] ∈ Rn, the output
is the predicted bandwidth in a future time window y = [ˆx(t + 1), x(t + 2),··· ,
x(t + m)] ∈ Rm. Note that since LSTM network adaptively keeps “memory”,
the bandwidth prediction for time window (t, t + m] is not only directly deter-
mined by the recent bandwidth history in (t − n, t], but also indirectly aﬀected
by bandwidth history before t − n through the memory cells. This gives LSTM
more ﬂexibility in capturing long-term bandwidth evolution trends than the tra-
ditional signal processing and averaging approaches working on a moving history
window. Following the architecture in Fig. 1a, we build a LSTM network with
one input Layer, one output layer and two hidden layers, each with 256 and
128 LSTM units respectively2. Given the LSTM architecture, the mapping from
input x to output y is parameterized by all the parameters in the LSTM network,
denoted as θ, which are obtained by minimizing the loss function in training.
Since we study realtime bandwidth prediction for a range of mobile network-
ing scenarios, one option is to train a separate LSTM network for each scenario,
that is using bandwidth data collected from scenario i to train a LSTM network
with parameters θ(i), and then use it to predict bandwidth for scenario i, i.e.,
2 We also tried a LSTM network with 256 and 256 nodes, and a LSTM network
with 128 and 128 nodes. The performance diﬀerence is not signiﬁcant. The results
presented in this paper is based on the 256 + 128 LSTM network.
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
39
per-scenario:
ˆy(i) = LSTM
(cid:3)
x(i), θ(i)
(cid:4)
,