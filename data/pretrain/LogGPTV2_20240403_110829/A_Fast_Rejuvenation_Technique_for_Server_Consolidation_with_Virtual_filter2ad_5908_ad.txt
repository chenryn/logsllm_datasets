200
150
100
50
0
warm-VM reboot
cold-VM reboot
total
throughput
m*p
(m-1)p
warm-VM reboot
cold-VM reboot
live migration
1st
2nd
before reboot
1st
2nd
after reboot
(b) web access
Figure 9. The total throughput in a cluster en-
vironment. m is the number of hosts and p is
the throughput of each host.
time
Figure 8. The throughput of ﬁle reads and
web accesses before and after the reboot.
cold-VM reboot, the throughput just after the reboot was de-
graded by 91 %, compared with that just before the reboot.
On the other hand, when we used the warm-VM reboot, the
throughput just after the reboot was not degraded. This im-
provement was achieved by no miss in the ﬁle cache even
when a ﬁle was accessed at the ﬁrst time after the reboot.
Next, we measured the throughput of a web server before
and after the reboot of a VMM. The Apache web server
served 10,000 ﬁles of 512 KB, all of which were cached
on memory. In this experiment, 10 httperf processes in a
client host sent requests to the server in parallel. All ﬁles
were requested only once. Figure 8 (b) shows the results.
When we used the warm-VM reboot, the performance just
after the reboot was not degraded, compared with that just
before the reboot. When we used the cold-VM reboot, the
throughput just after the reboot was degraded by 69 %.
5.6. Applying to Our Model
From our experimental results when we ran 11 VMs, we
can get the functions used in our model in Section 3.2:
rebootvmm(n) = −0.55n + 43
resume(n) = 0.43n − 0.07
rebootos(n) = 3.8n + 13
boot(n) = 3.4n + 2.8
resethw = 47
Using these functions, we can get the function of the down-
time reduced by using the warm-VM reboot:
r(n) = 3.9n + 60 − 17α
Since r(n) is always positive under α ≤ 1, the warm-VM
reboot can always reduce the downtime in our conﬁgura-
tion.
6. Cluster Environment
Software rejuvenation is naturally ﬁt with a cluster envi-
ronment as described in the literature [7, 25]. In a cluster
environment, multiple hosts provide the same service and
a load balancer dispatches requests to one of these hosts.
Even if some of the hosts are rebooted for the rejuvena-
tion of the VMM, the service downtime is zero. However,
the total throughput of the service is degraded while some
hosts are rebooted. The warm-VM reboot can mitigate the
performance degradation by reducing the downtime of re-
booted hosts.
Migration of VMs can be also used in a cluster environ-
ment to reduce the total cost. Unlike the warm-VM reboot,
live migration [8] in Xen and VMotion in VMware [26]
achieve negligible service downtime by using two hosts
when a VMM is rejuvenated. Before the VMM is rebooted,
it transfers the memory images of all VMs running on it to
a destination host without stopping the VMs. After that, the
VMM repeats transferring the changes in the memory im-
ages from the previous transmission until the changes be-
come small. Finally, the VMM stops the VMs and transfers
the changes and the execution state of the VMs. If we use
live migration in a cluster environment, that destination host
for migration can be shared among the remaining hosts.
Let us consider a cluster environment that consists of m
hosts to estimate the total throughput of the cluster. When
we let p be the throughput of each host, the total through-
put is m · p when all hosts are running. Figure 9 illustrates
the changes of the total throughput with time, based on our
experimental results. During the rejuvenation of a VMM
in one host, the total throughput is decreased to (m − 1)p
because the rejuvenated host cannot provide any services.
When we use the warm-VM reboot, the degradation of the
total throughput lasts only for a short period. The period
is the same as the downtime in the rejuvenated host and it
was 42 seconds in our experimental environment. The total
throughput is restored to m · p soon after the rejuvenation.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007However, when we use the cold-VM reboot, which is a nor-
mal reboot of a VMM, the degradation of the total through-
put lasts for a longer period. In our experimental environ-
ment, the period was 241 seconds when we created 11 VMs
and ran JBoss. In addition, the total throughput is degraded
to (m − δ)p (0 ≤ δ ≤ 1) for a while after the rejuvenation
due to cache misses. In our experiment of Section 5.5, δ
was 0.69.
On the other hand, when we use live migration, the total
throughput is (m − 1)p even when no hosts are being mi-
grated because one host is reserved as a destination host for
migration. This is m−1
m of the total throughput in a cluster
environment where migration is not used. This is critical if
m is not large enough. While one host performs live migra-
tion, the total throughput is (m − 1.12)p, which is led from
the report that the degradation of the Apache web server
was 12 % during live migration [8]. This degradation of the
total throughput is estimated to last for 17 minutes when we
run 11 VMs, each of which has 1 GB of memory. This is
calculated from the report that the time needed for migra-
tion was 72 seconds when only one VM with 800 MB of
memory was run [8]. This period of performance degrada-
tion is much longer than those in the warm-VM reboot and
the cold-VM reboot. Although these reported values are not
measured in our experimental environment, the trend would
not be changed.
According to these analyses, the warm-VM reboot is
more useful in a cluster environment than live migration. It
can reduce performance degradation by reducing the down-
time of rejuvenated hosts. On the other hand, for services
that cannot be replicated to multiple hosts, live migration
is still useful. It can reduce downtime by using alternative
host as a spare.
7. Related Work
Microreboot [6] enables rebooting ﬁne-grained applica-
tion components to recover from software failure. If reboot-
ing a ﬁne-grained component cannot solve problems, mi-
croreboot recursively attempts to reboot a coarser-grained
component including that ﬁne-grained component.
If re-
booting a ﬁner-grained component can solve problems, the
downtime of the application including that component can
be reduced. Microreboot is a reactive technique, but proac-
tively using it allows micro-rejuvenation. Likewise, micro-
kernel operating systems [1] allow rebooting only its sub-
systems implemented as user processes. Nooks [24] en-
ables restarting only device drivers in the operating system.
Thus, microreboot and other previous proposals are fast re-
boot techniques for subcomponents. On the other hand, the
warm-VM reboot is a fast reboot technique for a parent
component while the state of subcomponents is preserved
during the reboot.
In this paper, we have developed mechanisms to rejuve-
nate only a parent component when the parent component
is a VMM and the subcomponents are VMs. Checkpoint-
ing and restart [23] of processes can be used to rejuvenate
only an operating system. In this case, the parent compo-
nent is an operating system and the subcomponents are its
processes. This mechanism saves the state of processes to a
disk before the reboot of the operating system and restores
the state from the disk after the reboot. This is similar to
suspend and resume of VMs, but suspending and resuming
VMs are more challenging because they have to deal with a
large amount of memory. As we showed in our experiments,
simply saving and restoring the memory images of VMs to
and from a disk are not realistic. The warm-VM reboot is a
novel technique that hardly depends on the memory size by
preserving the memory images.
To speed up suspend and resume using slow disks, sev-
eral techniques are used. On suspend, VMware [26] incre-
mentally saves only the modiﬁcation of the memory image
of a VM to a disk. This can reduce accesses to a slow disk
although disk accesses on resume are not reduced. Win-
dows XP saves compressed memory image to a disk on hi-
bernation (Suspend To Disk). This can reduce disk accesses
not only on hibernation but also on resume. These tech-
niques are similar to incremental checkpointing [10] and
fast compression of checkpoints [22]. On the other hand,
the warm-VM reboot does not need any disk accesses.
Instead of using slow hard disks for suspend and resume,
it is possible to use faster non-volatile RAM disks such as
i-RAM [14]. Since most of the time for suspend and resume
is spent to access slow disks, RAM disks can speed up the
access. However, such non-volatile RAM disks are much
more expensive than hard disks. Moreover, it takes time
to copy the memory images from main memory to RAM
disks on suspend and copy them from RAM disks to main
memory on resume. The warm-VM reboot needs neither
such a special device nor extra memory copy.
Recovery Box [5] preserves only the state of an operat-
ing system and applications on non-volatile memory and re-
stores them quickly after the operating system is rebooted.
Recovery Box restores the partial state of a machine lost
by a reboot while the warm-VM reboot restores the whole
state of VMs lost by a reboot. In addition, Recovery Box
speeds up a reboot by reusing the kernel text segment left
on memory. This is different from our quick reload mecha-
nism in that Recovery Box needs hardware support to pre-
serve memory contents during a reboot.
To mitigate software aging of domain 0, Xen provides
driver domains, which are domain Us that enable running
device drivers. Device drivers are one of the most error-
prone components. In a normal conﬁguration of Xen, de-
vice drivers are run in domain 0 and the rejuvenation of de-
vice drivers needs to reboot domain 0 and the VMM. Driver
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007domains enable localizing the errors of device drivers in do-
main Us and rebooting the domains without rebooting the
VMM. Thus, using driver domains reduces the frequency of
the rejuvenation of the VMM. However, when the VMM is
rebooted, driver domains as well as domain 0 are rebooted
because driver domains cannot be suspended. Therefore,
the existence of driver domains increases the downtime.
8 Conclusion
In this paper, we proposed a new technique for fast re-
juvenation of VMMs called the warm-VM reboot. This
technique enables only a VMM to be rebooted by using
the on-memory suspend/resume mechanism and the quick
reload mechanism. The on-memory suspend/resume mech-
anism performs suspend and resume of VMs without ac-
cessing the memory images. The quick reload mechanism
preserves the memory images during the reboot of a VMM.
The warm-VM reboot can reduce the downtime and prevent
the performance degradation just after the reboot. We have
implemented this technique based on Xen and performed
several experiments to show the effectiveness. The warm-
VM reboot reduced the downtime by 83 % at maximum and
kept the same throughput after the reboot.
One of our future directions is to empirically evaluate the
reduction of performance degradation by using the warm-
VM reboot in a cluster environment. Another direction is to
enable privileged VMs to be rebooted without the reboot of
the VMM and to be suspended.
References
[1] M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid,
A. Tevanian, and M. Young. Mach: A New Kernel Founda-
tion for UNIX Development. In Proceedings of the USENIX
1986 Summer Conference, pages 93–112, 1986.
[2] Advanced Conﬁguration and Power Interface Speciﬁcation.
http://www.acpi.info/.
[3] AMD. AMD64 Virtualization Codenamed ”Paciﬁca” Tech-
nology: Secure Virtual Machine Architecture Reference
Manual, 2005.
[4] Apache Software Foundation. Apache HTTP Server Project.
http://httpd.apache.org/.
[5] M. Baker and M. Sullivan. The Recovery Box: Using Fast
Recovery to Provide High Availability in the UNIX Environ-
ment. In Proceedings of the Summer USENIX Conference,
pages 31–44, 1992.
[6] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and
A. Fox. Microreboot – A Technique for Cheap Recovery.
In Proceedings of the 6th Symposium on Operating Systems
Design and Implementation, pages 31–44, 2004.
[7] V. Castelli, R. Harper, P. Heidelberger, S. Hunter, K. Trivedi,
K. Vaidyanathan, and W. Zeggert. Proactive Management of
Software Aging. IBM Journal of Research & Development,
45(2):311–332, 2001.
[8] C. Clark, K. Fraser, S. Hand, J. Hansen, E. Jul, C. Limpach,
I. Pratt, and A. Warﬁeld. Live Migration of Virtual Ma-
chines. In Proceedings of the 2nd Symposium on Networked
Systems Design and Implementation, pages 1–11, 2005.
[9] B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, I. Pratt,
A. Warﬁeld, P. Barham, and R. Neugebauer. Xen and the
Art of Virtualization. In Proceedings of the Symposium on
Operating Systems Principles, pages 164–177, 2003.
[10] S. Feldman and C. Brown. IGOR: A System for Program
Debugging via Reversible Execution.
In Proceedings of
the Workshop on Parallel and Distributed Debugging, pages
112–123, 1989.
[11] K. Fraser. Xen changeset 11752. Xen Mercurial reposito-
ries.
[12] S. Garg, Y. Huang, C. Kintala, and K. Trivedi. Time and
Load Based Software Rejuvenation: Policy, Evaluation and
Optimality. In Proceedings of the 1st Fault Tolerance Sym-
posium, pages 22–25, 1995.
[13] S. Garg, A. Moorsel, K. Vaidyanathan, and K. Trivedi. A
Methodology for Detection and Estimation of Software Ag-
ing. In Proceedings of the 9th International Symposium on
Software Reliability Engineering, pages 283–292, 1998.
http://www.
[14] GIGABYTE Technology.
i-RAM.
gigabyte.com.tw/.
[15] V. Hanquez. Xen changeset 8640. Xen Mercurial reposito-
ries.
[16] Y. Huang, C. Kintala, N. Kolettis, and N. Fulton. Soft-
ware Rejuvenation: Analysis, Module and Applications. In
Proceedings of the 25th International Symposium on Fault-
Tolerant Computing, pages 381–391, 1995.
[17] Intel Corporation. Intel Virtualization Technology Speciﬁca-
tion for the IA-32 Intel Architecture, 2005.
[18] JBoss Group. JBoss Application Server. http://www.
jboss.com/.
[19] M. Kanno. Xen changeset 9392. Xen Mercurial repositories.
[20] D. Mosberger and T. Jin. httperf: A Tool for Measuring
Web Server Performance. Performance Evaluation Review,
26(3):31–37, 1998.
[21] A. Pﬁffer. Reducing System Reboot Time with kexec.
http://www.osdl.org/.
[22] J. Plank, J. Xu, and R. Netzer. Compressed Differences: An
Algorithm for Fast Incremental Checkpointing. Technical
Report CS–95–302, University of Tennessee, 1995.
[23] B. Randell. System Structure for Software Fault Tolerance.
IEEE Transactions on Software Engineering, SE-1(2):220–
232, 1975.
[24] M. Swift, B. Bershad, and H. Levy. Improving the Relia-
bility of Commodity Operating Systems. In Proceedings of
the 19th Symposium on Operating Systems Principles, pages
207–222, 2003.
[25] K. Vaidyanathan, R. Harper, S. Hunter, and K. Trivedi.
Analysis and Implementation of Software Rejuvenation in
Cluster Systems. In Proceedings of the 2001 ACM SIGMET-
RICS International Conference on Measurement and Mod-
eling of Computer Systems, pages 62–71, 2001.
[26] VMware Inc. VMware. http://www.vmware.com/.
[27] C. Waldspurger.
in
VMware ESX Server. In Proceedings of the 5th Symposium
on Operating Systems Design and Implementation, pages
181–194, 2002.
Memory Resource Management
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007