tions in organizations [35], yet TI is currently used mostly in
operational processes. That being said, metrics could help to
optimize the selection of TI sources for event detection and
to understand the potential for false negatives by looking at
coverage and overlap. Metrics for TI are useful in this, more
narrow, context.
Currently we lack a good understanding of the coverage of
PTI vendors due to secrecy around their methods. While that
is understandable in order to maintain operational security
in the face of advanced attackers, it does make it harder for
customers to evaluate what they are actually buying. This pa-
per seeks to address this by describing overlap and timeliness
through the comparison of indicators. We ﬁnd that even when
looking at the same actor groups, two of the leading PTI
providers have diverging information with very small overlap.
The secrecy around their methods to observe threat actors also
beneﬁts vendors economically: as long as their methods re-
main opaque, myths will live on about how TI providers may
offer some special degree of TI coverage, possibly through
an exclusive skillset, ‘hacking back’, or by means of access to
restricted information. As described by Shires [36], vendors
use “cyber noir” symbols that portray their work as deploy-
ing unconventional tactics in mythical battles between good
and evil, often aligned with national security. Such stories
and symbols give rise to an understanding of detection and
attribution capabilities of PTI vendors that currently cannot
be substantiated nor vetted.
As a consequence of the low transparency, the market for
paid TI shows signs of asymmetric information, in which
the vendors know what they are selling, but customers don’t
know what they are buying. Consumers in the market for
TI therefore ﬁnd it hard to compare services [20, 14]. As
Metcalf concluded already in 2015 for blacklists: “secrecy
does not beneﬁt the operational analyst who must must decide
which lists to apply” [25]. And indeed, ﬁve years later, our
respondents say it is still “mostly guesswork” to understand
the visibility and methods of paid TI providers, and with that
the value of the services they offer.
Under conditions of information asymmetry, buyers rely
on signals. One such signal is whether the ﬁrm is seen as a
market leader, which is partially signalled via a high price for
its services. In this sense, the phrase ‘nobody ever got ﬁred
for buying IBM’ also rings true for threat intelligence. Cus-
tomers are incentivized to purchase from leading providers –
the safe choice under uncertainty. This way, economic value
is linked to vendor reputation. In the longer run though, struc-
tural information asymmetry holds the risk for vendors that
customers may lose trust in the value of PTI services, which
would decrease the willingness to pay. This effect is known
as a ‘market for lemons’ [2]. Grigg [38] went one step further
and argued that even vendors might lack reliable information
on the quality of their products. Providers of PTI might know
what data they collect and how, but they do not know – and
can’t know, Grigg would argue – how effective their product
is in improving the security of their clients. Our analysis
suggests that, in light of lacking ground truth and low overlap
in indicators, vendors themselves may not even know how
well they are able to track speciﬁc threat actors. When both
seller and the buyer lack reliable information on the quality
of a product, this creates – in Grigg’s analysis – a market
for ‘silver bullets’, where herding behavior and arbitrary best
practices triumph over rational purchasing decisions.
Finally, we note that through their forensic work, TI ven-
dors have profound inﬂuence on how the general public and
the political leadership understands security incidents. Report-
ing on such incidents is not just neutral technical analysis but
also requires interpretation and ‘sense-making’, as Stevens
(2019) showed for the analysis of Stuxnet by Symantec [37].
Indeed, public understanding of such incidents is shaped by
the political and economic prisms of the experts who carry out
the analysis [13, 42]. Information asymmetry in the market
for paid threat intelligence is therefore not only of economic,
but also of political signiﬁcance.
9 RELATED WORK
There is a rich line of research that has studied the properties
of open threat intelligence, also known as abuse feeds and
blocklists – e.g., [39, 25, 19]. Problems in coverage, timeli-
ness and accuracy have consistently been observed in these
studies.
In recent years, proposals have been put forward to formal-
ize and measure the quality of TI [21, 33, 15, 29, 30, 27].
This includes metrics on features such as coverage, accuracy,
timeliness, relevance, overlap, latency, and volume. [34] has
investigated how to present TI quality to analysts. Applica-
tions of these approaches to measure quality of TI have been
limited to OTI, also in the recent studies by Li et al. [21] and
Grifﬁoen et al. [15].
Aside from the availability of high quality information,
it is essential how this information is used. [31] identiﬁes
that organizations have issues interpreting threat intelligence,
triaging large volumes of threat information or dealing with
large numbers of false positives. In this sense, TI has similar
operational issues as blacklists of IP addresses and domain
names, which have an established history in computer security.
While TI as contextualized, high-level information has the
potential to remediate these issues [6], a 2019 SANS survey
nonetheless found (low-level) indicators of compromise to be
valued higher by respondents than information about (high-
USENIX Association
29th USENIX Security Symposium    445
level) tactics, techniques and procedures (TTPs) [7]. The
authors attribute this to the fact that most of their respondents
were security operations analysts, who might view the value
proposition of TI primarily as enriching alerts with technical
details. Our study provides a detailed analysis of how threat
intelligence is actually being used within organizations, and
how the value is perceived by those directly affected by it.
We go beyond the related work in two key ways. First,
we present the ﬁrst empirical study of the PTI of market
leaders. The nearest study is [21], which was not focused
explicitly on PTI, but did include two edge cases of paid
services. These services were not providing original high-end
TI sources, but helped curate and aggregate otherwise free
or low-end indicator feeds, and were priced in the range of
USD 1-10k per year, as kindly conﬁrmed to us by one of the
paper’s co-authors. We followed the measurement approach
developed by [21] but provide the ﬁrst application to ‘real’
PTI: services of commercial threat intelligence providers
which operate their own detection network and perform foren-
sic analysis to generate original data about threats. With this
value proposition, vendors justify pricing between USD 100-
600k per year. A common sentiment in the TI industry is that
‘real’ high-quality threat intelligence may only be obtained
from these exclusive closed-source commercial providers,
and [31] ﬁnds PTI sources are used twice as often as OTI in
industry.
Second, we contextualize these quantitative approaches to
measuring quality by conducting a user study of PTI cus-
tomers and identify their perceptions of value. This has en-
abled us to ﬁnd that users use and evaluate TI differently than
the measurement approaches developed by researchers as-
sume. In reality, users hardly calculate the proposed metrics.
Their perception of value is determined by various use cases
in which this quantiﬁcation is not only missing, but some-
times points in conﬂicting directions – as around the issues
of accuracy and coverage.
10 LIMITATIONS
Our mixed-methods approach introduces several limitations.
First, we only analyzed the services of two PTI vendors. As
they are among the market leaders at the high end of the
market, we assume that our ﬁndings are representative for
that market, but future work is needed to corroborate this.
Second, our analysis was based on data of a single customer
of these two vendors. This customer acquired 3-5 subsets of
indicators of each vendor in the same topic areas, of a total
offering of 5-10 subsets that each vendor offers. Other subsets
might show somewhat different results for the target indus-
tries (Figure 4) or the overlap between vendors. Given that
the available selection of subsets form a signiﬁcant portion
of all subsets, we expect that they provide a valid basis for
comparison. The exact numbers, however, are likely to vary
across other subsets.
Third, our analysis of PTI has to contend with a lack of
ground truth. We followed the approach from prior work on
OTI [21] and conducted a comparative analysis among dif-
ferent feeds. For the analysis of indicators on different threat
actors, we relied on the well-known mapping developed by
Florian Roth across the different threat actor naming schemes
used by PTI vendors. We cannot ascertain how reliable this
mapping is, other than the fact that Roth is an expert in the
ﬁeld, his mapping is well known, and he is collaborating on it
with other industry insiders – so mistakes would presumably
be corrected.
Fourth, the comparison with OTI was limited to a single
month of four feeds. While these feeds were chosen because
they are actually re-used by many other feeds in the OTI
landscape [15], a broader set of feeds will provide a more
reliable result. That said, the lack of overlap with PTI was
quite stark and unlikely to change when analyzing other feeds.
In OTI research, the low overlap among any two feeds has
been a consistent ﬁnding for years.
Fifth, regarding our user study, our main limitations stem
from a small sample size (n=14). Our sample contains a
variety of organizations, but it may contain selection bias as
respondents are geographically located in the Netherlands
and Japan only, were working with TI (rather than choosing
not to), and willing to talk about this in an interview. All of
this makes that we do clearly do not claim that our ﬁndings
are generalizable for all organizations using TI. Given that
no prior work existed, neither on the PTI feeds nor on users
of PTI, we chose to do an in-depth exploration of the views
of such users using the grounded theory method. For more
generalizable results, a survey could be designed based on
our ﬁndings.
11 CONCLUSIONS
This study explored services in the market of commercial
threat intelligence. We analyzed the indicators of two paid
TI vendors and found 13.0% of vendor 1’s indicators appear
in vendor 2’s set and – vice versa – a mere 1.3% of vendor
2’s indicators in vendor 1’s set. If we drill down to the 22
threat actors for which both vendors have indicators, we ﬁnd
an average overlap of these indicators of no more than 2.5 to
4.0% per group, depending on the type of indicator. Further,
this overlap occurs primarily with a handful number of actors.
The fact that the indicators of two vendors are largely separate
sets, even when assessed for speciﬁc threat actors that they
both track, raises questions on the coverage that services of
these vendors actually provide.
Reports produced by paid TI providers describe the tactics
of threat actors, the results of malware reverse-engineering, or
give advisories for current events, amongst other things. The
reports concern primarily government, military, and ﬁnancial
institutions – important customers of TI vendors – but sur-
prisingly also pay a lot of attention to campaigns targeting
446    29th USENIX Security Symposium
USENIX Association
civil society, possibly due to their political signiﬁcance.
Besides indicators and reports, paid TI services also con-
sist of requests for information from analysts, portals with
historic information, data mining platforms, and custom alerts.
These services are expensive, with subscription costs of major
vendors often upwards of $100,000 per year.
Whereas paid sources offer ‘polished’ TI, open sources
contain ‘raw TI’ as one respondent described it. We ﬁnd that
this statement holds for the two types of paid intelligence
products that we have compared with open TI, namely in-
dicators and reports. In terms of substance, paid reports are
for example similar to open source blog posts and tweets of
security researchers that are freely available online, but the
paid reports are more comprehensive in their descriptions of
context and recommendations. Further, paid reports are pack-
aged with machine-readable indicators. We compare these to
open TI indicators feeds (which are much larger in volume),
and ﬁnd less than 1% overlap between them, suggesting that
PTI providers successfully differentiate themselves and are
capturing a different part of the threat landscape. In terms of
timeliness, we ﬁnd no evidence that PTI is faster than OTI,
surprisingly enough, although this is based on the small sam-
ple of overlapping indicators. There is a delay of around one
month before indicators from one set are found in another.
The main use case for TI is network detection, followed
by situational awareness – which we understand to mean: in-
forming your threat proﬁle – and prioritization of resources
in the SOC. We ﬁnd that one-third of respondents use threat
intelligence to improve organizational decision-making: to
inform security engineering, to reduce ﬁnancial fraud, but in
one instance also for risk management around international
mergers and acquisitions. Asked what makes TI valuable, re-
spondents name properties related to actionability, relevance,
and conﬁdence. All respondents describe valuing the ability
for TI to provide context, which suggests that they view TI
as a reference. Further, the ability to automate using TI is
important for respondents: almost all name valuing a low
false-positive ratio and interoperability with their detection
systems. Importantly, only half of our respondents discuss
coverage as something they value in threat intelligence – re-
ducing false-negatives or misses seems to be much less of
a concern. We conclude that TI consumers evaluate threat
intelligence mostly through the impact on their organization’s
detection processes.
Evaluation of TI sources is done mostly through informal
processes by our respondents. When a subscription renewal
comes up, TI professionals decide if to continue largely based
on implicit criteria and tacit understanding of the value of that
source. This is surprising, given that research has focused on
developing metrics and heuristics that could enable a quan-
titative understanding of value of TI. In practice, metrics or
intelligence requirements are used by less than half of the
professionals we interviewed.
There is this promise that leading paid TI vendors would
be able to overcome the persistent problem of sharing threat
information among defenders. They can aggregate and ana-
lyze threat data at scale from vantage points across different
clients and networks – as was also argued by Crowdstrike
Senior Director Vicenzo Iozzo (see Section 2). We do not
dispute that important advances are being made. That being
said, our study raises doubts as to the extent in which this
promise has been fulﬁlled today. Even when the vendors
claim to track the same threat actor, they each see only a tiny
fraction of the associated indicators. The fact that almost all
PTI indicators are unique to one vendor, is a pattern we know
all too well from OTI sources. So the pay-off of aggregating
data across clients and networks, as claimed by Iozzo, is not
very clear in terms of detection capability, to say the least.
Even when a client would be willing to pay the steep price
of simultaneously acquiring the feeds of all market leaders –
a proposition that would cost them millions each year – it is
likely that this strategy would reproduce the pattern that we
know from OTI: feeds contain mostly singletons and more
feeds still get nowhere close to comprehensive coverage.
The sharing of indicators across vendors would still be a
ﬁrst step to improve coverage and the detection of attackers.
The current state of affairs in paid TI resembles the market
for anti-phishing services about a decade ago. The lack of
data sharing meant that each anti-phishing company thought
it had strong coverage and could protect its client brands well,
while the truth was that they missed most of the attacks they
were hired to detect [26]. In that market, like in the market
of malware detection, sharing across vendors was eventually