title:Sedic: privacy-aware data intensive computing on hybrid clouds
author:Kehuan Zhang and
Xiao-yong Zhou and
Yangyi Chen and
XiaoFeng Wang and
Yaoping Ruan
Sedic: Privacy-Aware Data Intensive Computing
on Hybrid Clouds
Kehuan Zhang, Xiaoyong Zhou,
Yangyi Chen and XiaoFeng Wang
School of Informatics and Computing
Indiana University, Bloomington, IN, USA
{kehzhang, zhou, yangchen,
xw7}@indiana.edu
ABSTRACT
The emergence of cost-effective cloud services offers organizations great
opportunity to reduce their cost and increase productivity. This develop-
ment, however, is hampered by privacy concerns: a signiﬁcant amount of
organizational computing workload at least partially involves sensitive data
and therefore cannot be directly outsourced to the public cloud. The scale of
these computing tasks also renders existing secure outsourcing techniques
less applicable. A natural solution is to split a task, keeping the computation
on the private data within an organization’s private cloud while moving the
rest to the public commercial cloud. However, this hybrid cloud computing
is not supported by today’s data-intensive computing frameworks, MapRe-
duce in particular, which forces the users to manually split their computing
tasks. In this paper, we present a suite of new techniques that make such
privacy-aware data-intensive computing possible. Our system, called Sedic,
leverages the special features of MapReduce to automatically partition a
computing job according to the security levels of the data it works on, and
arrange the computation across a hybrid cloud. Speciﬁcally, we modiﬁed
MapReduce’s distributed ﬁle system to strategically replicate data, moving
sanitized data blocks to the public cloud. Over this data placement, map
tasks are carefully scheduled to outsource as much workload to the public
cloud as possible, given sensitive data always stay on the private cloud. To
minimize inter-cloud communication, our approach also automatically ana-
lyzes and transforms the reduction structure of a submitted job to aggregate
the map outcomes within the public cloud before sending the result back
to the private cloud for the ﬁnal reduction. This also allows the users to
interact with our system in the same way they work with MapReduce, and
directly run their legacy code in our framework. We implemented Sedic on
Hadoop and evaluated it using both real and synthesized computing jobs
on a large-scale cloud test-bed. The study shows that our techniques effec-
tively protect sensitive user data, ofﬂoad a large amount of computation to
the public cloud and also fully preserve the scalability of MapReduce.
Categories and Subject Descriptors
K.6.5 [Security and Protection]: Unauthorized access
General Terms
Security
Keywords
Cloud security, MapReduce, data privacy, computation split, auto-
matic program analysis
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.
Yaoping Ruan
IBM T.J. Watson Research Center
Hawthorne, NY, USA
PI:EMAIL
1.
INTRODUCTION
With the rapid growth of information within organizations, rang-
ing from hundreds of gigabytes of satellite images to terabytes
of commercial transaction data, the demands for processing such
data are on the rise. Meeting such demands requires an enormous
amount of low-cost computing resources, which can only be sup-
plied by today’s commercial cloud-computing systems: as an ex-
ample, Amazon Elastic Compute Cloud (EC2) can easily handle
terabytes of data at a price as low as 0.015 dollar per hour. This
newfound capability, however, cannot be fully exploited without
addressing the privacy risks it brings in: on one hand, organiza-
tional data contains sensitive information (e.g., ﬁnancial data, health
records, etc.) and therefore cannot be shared with the cloud provider
without proper protection; on the other hand, today’s commercial
clouds do not offer high security assurance, a concern that has been
signiﬁcantly aggravated by the recent incidents of Amazon out-
ages [13] and the Sony PlayStation network data breach [10], and
tend to avoid any liability [36]. As a result, attempts to outsource
the computations involving sensitive data are often discouraged.
A natural solution to this problem is cryptographic techniques
for secure computation outsourcing, which has been studied for a
decade [38, 17, 19, 16]. However, existing approaches are still
not up to the challenge posed by data-intensive computing. For
example, homomorphic encryption [30, 46, 49] was found to be
prohibitively expensive for a large-scale computation [25]. As an-
other example, the secret-sharing techniques underlying most out-
sourcing proposals can lead to intensive data exchanges between
the share holders on different clouds during a computation involv-
ing an enormous amount of data, and are therefore hard to scale.
Secure hybrid-cloud computing. Oftentimes, a data-intensive com-
putation involves both public and sensitive data. For example, a
simple grep across an organizational ﬁle system encounters adver-
tising slogans as well as lines of commercial secrets. Also, many
data analysis tasks, such as intrusion detection [8], targeted adver-
tising [14], etc., need to make use of the information from pub-
lic sources, sanitized network traces and social-network data [9]
for example, as well as information within an organization.
If
the computation on the public data can be separated from that on
the sensitive data, the former can be comfortably delegated to the
public commercial clouds and the latter, whose scale can be much
smaller than the original task, will become much easier to handle
within the organization. Such a split of computation is an effective
ﬁrst step to securely outsource computations and can be naturally
incorporated into today’s cloud infrastructure, in which a public
cloud typically receives the computation “overﬂow” from an orga-
nization’s internal system when it is running out of its computing
resources. This way of computing, involving both the private cloud
within an organization and the public commercial cloud, is called
515hybrid cloud computing [29]. The hybrid cloud has already been
adopted by most organizational cloud users and is still undergoing
a rapid development, with new techniques mushroomed to enable
a smoother inter-cloud coordination (e.g. [18]). It also presents a
new opportunity that makes practical, secure outsourcing of com-
putation tasks possible.
However, today’s cloud-based computing frameworks, such as
MapReduce [22], are not ready for secure hybrid-cloud comput-
ing: they are designed to work on a single cloud and not aware of
the presence of the data with different security levels, which forces
cloud users to manually split and re-arrange each computation job
across the public/private clouds. This lack of a framework-level
support also hampers the reuse of existing data-processing code,
and therefore signiﬁcantly increases the cloud users’ programming
burden. Given the fact that privacy concerns have already become
the major hurdle for a broader adoption of the cloud-computing
paradigm [37], it is in urgent need to develop practical techniques
to facilitate secure data-intensive computing over hybrid clouds.
Our work. To answer this urgent call, a new, generic secure com-
puting framework needs to be built to support automatic splitting of
a data-intensive computing job and scheduling of it across the pub-
lic and private clouds in such a way that data privacy is preserved
and computational and communication overheads are minimized.
Also desired here is accommodation of legacy data-processing code,
which is expected to run directly within the framework without the
user’s manual interventions. In this paper, we present a suite of
new techniques that make this happen. Our system, called Sedic,
includes a privacy-aware execution framework that automatically
partitions a computing job according to the security levels of the
data it involves, and distributes the computation between the pub-
lic and private clouds. Sedic is based on MapReduce, which in-
cludes a “map” step and a “reduce” step: the map step divides in-
put data into lists of key-value pairs and assigns them to a group
of concurrently-running mappers; the reduce step receives the out-
puts of these mappers, which are intermediate key-value pairs, and
runs a reducer to transform them into the ﬁnal outputs. This way
of computation is characterized by its simple structure, particularly
the map operations that are performed independently and concur-
rently on different data records. This feature is leveraged by our
execution framework to automatically decompose a computation
on a mixture of public and sensitive data, which is actually difﬁ-
cult in general. More speciﬁcally, Sedic transparently processes
individual data blocks, sanitizes those carrying sensitive informa-
tion along the line set by the smallest data unit (“record”) a map
operation works on, and replicates these sanitized copies to the
public cloud. Over those data blocks, map tasks are assigned to
work solely on the public or sensitive data within the blocks. These
tasks are carefully scheduled and executed to ensure the correctness
of the computing outcomes and the minimum impacts on perfor-
mance. In this way, the workload of map operations is distributed
to the public/private clouds according to their available computing
resources and the portion of sensitive data in the original dataset.
A signiﬁcant technical challenge here is that reduction usually
can not be done on private nodes and public nodes separately and
only private nodes are suitable for such a task in order to preserve
privacy. This implies that the intermediate outputs of computing
nodes on the public cloud need to be sent back to the private cloud
for reduction, which could bring in a signiﬁcant communication
overhead. To reduce such inter-cloud data transfer as well as move
part of the reduce computation to the public cloud, we developed
a new technique that automatically analyzes and transforms reduc-
ers to make them suitable for running on the hybrid cloud. Our
approach extracts a combiner from the original reducer for pre-
processing the intermediate key-value pairs produced by the public
cloud, so as to compress the volume of the data to be delivered to
the private cloud. This was achieved, again, by leveraging the spe-
cial features of MapReduce: its reducer needs to perform a fold-
ing operation on a list, which can be automatically identiﬁed and
extracted by a program analyzer embedded in Sedic. If the oper-
ation turns out to be associative or even commutative, as happens
in the vast majority of cases1 , the combiner can be built upon it
and deployed to the public cloud to process the map outcomes. In
our research, we implemented Sedic on Hadoop [28] and evalu-
ated it over FutureGrid [40], a large-scale, cross-the-country cloud
testbed. Our experimental results show that the techniques effec-
tively protected conﬁdential user data and minimized the workload
of the private cloud at a small overall cost.
Contributions. The contributions of the paper are summarized as
follows:
• A new and user-transparent secure data-intensive computing frame-
work. We have developed the ﬁrst hybrid-cloud based secure data-
intensive computing framework. Our framework ensures that sen-
sitive user data will not be exposed to the public cloud without the
user’s consent, while still letting the public cloud shoulder most
of the computing workload when possible. Also important is the
transparency our design offers, which enables cloud users to work
on the framework in exactly the same way they use the original
MapReduce. As a result, legacy MapReduce jobs can be directly
executed within the framework. An additional beneﬁt that comes
with this transparency is the ﬂexibility of our computing frame-
work: not only can it outsource all non-sensitive map tasks, but our
approach can also automatically move them back to the organiza-
tion perimeter when necessary (e.g., when the public cloud suffers
an outage). It is important to note that these properties are achieved
when the scalability of MapReduce is fully preserved. This is by
no means trivial given the complexity of this execution framework,
which involves carefully-designed algorithms for achieving high
performance, such as the replication strategies of its distributed ﬁle
system, task assignment and scheduling and others.
• Automatic reducer analysis and transformation. We have built a
new program analysis tool that automatically evaluates and trans-
forms the reduction structure of a computing job to optimize it for
hybrid-cloud computing. The tool breaks down a reducer into com-
ponents that can work on the public and private clouds respectively,
which not only moves part of the reduce computation away from
the private cloud but also helps control the amount of the inter-
mediate outcomes to be delivered back to the private cloud which
could cause signiﬁcant delay and bandwidth charges on today’s
cloud model. Since inter-cloud data transfers are known to be a
bottleneck in cloud computing, the new techniques offer a critical
support that makes secure hybrid-cloud computing practical.
• Implementation and evaluation. We have implemented our de-
sign and evaluated it over a large-scale cloud testbed, using both
real and synthesized MapReduce jobs. Our experimental study
demonstrates that the new techniques we propose are both effec-
tive and practical.
Sedic is designed to protect data privacy during map-reduce op-
erations, when the data involved contains both public and private
records. This protection is achieved by ensuring that the sensi-
tive information within the input data, intermediate outputs and
ﬁnal results will never be exposed to untrusted nodes during the
computation. Another important concern in data-intensive comput-
ing is integrity, i.e., whether the public cloud honestly performs a
computing task and deliveries the right results back to the private
cloud. We chose to address the conﬁdentiality issue ﬁrst, as it has
1As a prominent example, 10 out of all 11 examples coming with Hadoop
distribution contain commutative and associative folding loops.
516already impeded the extensive use of the computing resources of-
fered by the public cloud [4]. By comparison, many cloud users
today live with the risk that their computing jobs may not be done
correctly on the public cloud. As a prominent example, the Na-
tional Institutes of Health still prohibits outsourcing of the compu-
tation involving human DNA data to the commercial cloud, though
the same tasks on non-human genomes has already been delegated
to Amazon EC2 [35].
Roadmap. The rest of the paper is organized as follows. Sec-
tion 2 outlines the high-level design of Sedic and also explicates
our objectives and adversary model. Section 3 and 4 describes the
details of our secure execution framework and code transformation
tool, including our implementation of these techniques. Section 5
reports our experimental study that evaluates the performance of
Sedic. Section 6 surveys related prior research. Section 7 discusses
the limitation of our current design and possible future research,
and Section 8 concludes the paper.
2. OVERVIEW
In this section, we ﬁrst explain the properties expected from
Sedic and then present its high-level design and the adversary model
used in our research.
2.1 Background and Design Objectives
MapReduce. MapReduce is a software framework for supporting
data-intensive computing, such as web searching [22], document
format conversion [43], genome sequence analysis [42, 34] and
others. The computation within this framework ﬁrst divides the
input data into lists of key-value pairs and assigns them to a group
of concurrently-running problem solvers, i.e., the mappers. Each
mapper is iteratively invoked by the node that performs the compu-
tation (called tasktracker in Hadoop terminology) to convert an in-
put pair into one or more intermediate key-value pairs. These pairs
are fed into the reducers to transfer the pairs with the same key into
a list of output pairs. Although conceptually simple, this computing
framework includes a set of complicated mechanisms to ensure the
scalability and fault tolerance during a computation, through repli-