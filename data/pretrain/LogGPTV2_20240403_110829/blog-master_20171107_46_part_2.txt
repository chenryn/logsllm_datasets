         0.001  \set id random(1,1000000000)    
         0.078  select * from test3 where id=:id;    
```      
2、更新测试TPS    
```    
pgbench -M prepared -n -r -P 1 -f ./test.sql -c 48 -j 48 -T 120    
transaction type: ./test1.sql    
scaling factor: 1    
query mode: prepared    
number of clients: 48    
number of threads: 48    
duration: 120 s    
number of transactions actually processed: 27703709    
latency average = 0.208 ms    
latency stddev = 0.126 ms    
tps = 230828.616797 (including connections establishing)    
tps = 230853.344303 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.002  \set id random(1,100000000)    
         0.207  update test1 set crt_time=now() where id=:id;    
pgbench -M prepared -n -r -P 1 -f ./test1.sql -c 48 -j 48 -T 120    
transaction type: ./test1.sql    
scaling factor: 1    
query mode: prepared    
number of clients: 48    
number of threads: 48    
duration: 120 s    
number of transactions actually processed: 29387603    
latency average = 0.196 ms    
latency stddev = 0.110 ms    
tps = 244891.957430 (including connections establishing)    
tps = 244917.399306 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.001  \set id random(1,100000000)    
         0.195  update test1 set crt_time=now() where id=:id;    
pgbench -M prepared -n -r -P 1 -f ./test3.sql -c 48 -j 48 -T 120    
transaction type: ./test3.sql    
scaling factor: 1    
query mode: prepared    
number of clients: 48    
number of threads: 48    
duration: 120 s    
number of transactions actually processed: 28026501    
latency average = 0.205 ms    
latency stddev = 0.110 ms    
tps = 233533.801692 (including connections establishing)    
tps = 233554.689137 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.002  \set id random(1,100000000)    
         0.001  \set id1 random(1,1000000000)    
         0.203  with tmp as (select * from test3 where id=:id1)    
```    
索引深度的差别：   
```
postgres=# explain (analyze,verbose,timing,costs,buffers) select * from test where id=1;
                                                       QUERY PLAN                                                       
------------------------------------------------------------------------------------------------------------------------
 Index Scan using test_pkey on public.test  (cost=0.43..2.85 rows=1 width=44) (actual time=0.074..0.075 rows=1 loops=1)
   Output: id, info, crt_time
   Index Cond: (test.id = 1)
   Buffers: shared read=4
 Planning time: 0.215 ms
 Execution time: 0.102 ms
(6 rows)
postgres=# explain (analyze,verbose,timing,costs,buffers) select * from test1 where id=1;
                                                        QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
 Index Scan using test1_pkey on public.test1  (cost=0.57..2.98 rows=1 width=44) (actual time=0.094..0.094 rows=1 loops=1)
   Output: id, info, crt_time
   Index Cond: (test1.id = 1)
   Buffers: shared read=5
 Planning time: 0.217 ms
 Execution time: 0.119 ms
(6 rows)
postgres=# explain (analyze,verbose,timing,costs,buffers) select * from test3 where id=1;
                                                        QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
 Index Scan using test3_pkey on public.test3  (cost=0.57..2.99 rows=1 width=44) (actual time=0.054..0.055 rows=1 loops=1)
   Output: id, info, crt_time
   Index Cond: (test3.id = 1)
   Buffers: shared hit=5
 Planning time: 0.413 ms
 Execution time: 0.080 ms
(6 rows)
```
### 性能小结    
数据量 | 写入吞吐 | 查询tps | 更新tps    
---|---|---|---  
1000万 | 58万行/s | 67万 | 23.1万    
1亿 | 53.2万行/s | 63.4万 | 24.5万    
10亿 | 162.6万行/s | 60.6万 | 23.4万    
### 表分区建议  
单表多大需要分区？     
1、非常频繁更新的表（考虑到autovacuum, freeze的速度）  
2亿  
指表中频繁被更新的记录数在2亿以内，表本身的记录数可以更多。   
2、更新、删除不频繁或毫无的表（考虑到设计rewrite的DDL，建索引，逻辑备份等的速度）  
20亿(还需要考虑单行大小，直接影响DDL rewrite table的开销)    
## 参考      
[《PostgreSQL、Greenplum 应用案例宝典《如来神掌》 - 目录》](../201706/20170601_02.md)        
[《数据库选型之 - 大象十八摸 - 致 架构师、开发者》](../201702/20170209_01.md)        
[《PostgreSQL 使用 pgbench 测试 sysbench 相关case》](../201610/20161031_02.md)        
[《数据库界的华山论剑 tpc.org》](../201701/20170125_01.md)        
https://www.postgresql.org/docs/10/static/pgbench.html        
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")