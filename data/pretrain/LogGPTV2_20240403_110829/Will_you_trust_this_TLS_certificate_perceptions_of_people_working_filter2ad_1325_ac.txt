“That it is not signed by the other authority, but it’s signed by itself.”
[P15, original]
“It was signed by local server for which it was generated. It was not
signed by official authority.” [P20, original]
“Self-signed certificate? Anyone can create self-signed certificates.” [P78,
original]
“If I knew that the certificate should be self-signed, I could consider it
trustworthy.” [P09, original]
“[...] and it’s usually used either by internally or for testing purposes.
It shouldn’t be used publicly.” [P11, original]
“[...] because that can be any hacker, [they] can phish.” [P66, original]
“Microsoft certificate has expired, it’s out of date.” [P30, original]
“[...] it could be just forgotten and they’re about to do it, they’re about to
renew it or something.” [P10, original]
“Ah, right, so, expired certificates are pretty common, so from what I can
see [...]” [P01, redesigned]
“Yeah, the Microsoft one is expired. So it was valid in the past, and
I looked at the date [...]” [P18, redesigned]
“If it’s like a small businesses from my local neighborhood, I would
probably trust them.” [P62, original]
“[maybe] the attacker has stolen a certificate which was previously valid
and has been revoked [...]” [P37, original]
“I understood that there is some chain and a certain point in chain
is restricting the hostname to ...” [P39, original]
“I find out that one of the authorities was listed as false, but the other two
were fine.” [P10, original]
“I don’t really understand the whole thing.” [P62, original]
“[I’d] let Google know that they have a rogue admin...” [P26, redesigned]
“[...] CA has explicitly said ‘I am not allowed to sign this, you should not
trust this.’” [P26, redesigned]
“The certificate authority up the chain specifies that only domains with
‘api.google.com’ are valid.” [P18, redesigned]
“It seemed like it was just an innocent misconfiguration of the kind that
happens all the time.” [P19, original]
“For this one I really try to find some documentation, but there was no
documentation on this.” [P68, original]
“There wasn’t a problem, it was good, OK.” [P22, original]
“I think it was safe, but I looked into the cert and I couldn’t find anything
wrong, so I would trust it...” [P13, redesigned]
“[...] everything looked fine and I thought: ‘Well, if the testing tool
is good, I’ll trust that.’” [P77, redesigned]
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Ukrop and Kraus, et al.
Figure2:Meantrustinthecertificatesdependingonhowlong
it has been expired, compared across conditions. Only par-
ticipants with different ratings for durations are included.
Figure 1: Comparison of trust assessment among certificate
cases split by condition. Bars are normalized due to the
different number of participants in each condition (44 for
the original condition, 31 for the redesigned one).
a certificate (AnyoneCan, 21 ). Some participants mentioned that
they would find self-signed certificates acceptable in places where
they were told to expect them (IfExpected, 10 ). The self-signed cer-
tificates were multiple times said to be OK for testing or internal pur-
poses (Internal, 10 ). A few participants mentioned the possibility
of an attack when encountering self-signed certificates (Attack, 8 ).
The connection with a self-signed certificate was perceived as
‘looking suspicious’ (mean 1.92±1.62,median 2, see Fig. 1). The rating
similar to the expired case (see Section 3.3) is surprising, given that
the expired certificate had at least provided an identity guarantee
in the past, whereas the self-signed certificate provides literally no
identity guarantees and never did.
3.3 Expired Case
When participants described their perception of the problem, most of
themseemedtocomprehendthesourceoftheflaw–thecertificatebe-
ing no longer valid (NoLonger, 62 ) or having been valid in the past
but not now (OkBefore, 14 ). The prevalence of the first code is un-
surprising as it is literally the content of the displayed error message.
Several considerations seemed to play a role in opinion forming:
Many thought expired certificates can be grounded in a misconfigu-
ration, negligence or operator error on the server side, and thus the
flaw is non-malicious (Mistake, 27 ). A considerably lower number
of participants mentioned that the connection might be attacked or
an old certificate may be abused (Attack, 8 ).
Next, multiple participants expressed the opinion that expired
certificates are quite common (Common, 18 ). The last notable men-
tion comes for participants binding the trust in expired certificates
to the subjects using them (Reputation, 13 ) – seeing an expired
certificate as more acceptable for local business than for Microsoft
(the specific comparison was drawn because the expired certificate
used in the task was issued for login.microsoft.com).
Participants rated the trustworthiness of the connection with the
server certificate expired one week ago as rather low, close to a ‘looks
suspicious’ point on our scale (mean 2.43±1.66, median 3, see Fig. 1).
Given that the provided certificate does not guarantee the identity
information anymore, it would have been unsurprising to us if the
trustworthiness assessment went even lower.
From our own experience as programmers and IT security educa-
tors, we further suspected the trust might be influenced by the expiry
duration.Subsequently,wehadpreparedseveralfollow-upquestions
for the expired case in the exit interview. Fair enough, the majority of
participants (77%, 58 ) reported that the rating would be different for
shorter/longerperiodspasttheexpirationdate.However,only65%of
these (36 ) actually checked the expiry date in the given task (an ex-
ample for the not uncommon difference between self-reported inten-
tion and behavior). When subsequently asked to rate trust in the con-
nection with the server certificate expired a day/week/month/year
ago, the mean trust spanned from ‘looking OK’ (mean 3.64±1.30,
median 4) for a day-expired certificate to almost ‘outright untrust-
worthy’ (mean 0.47±0.82,median 0) for the certificate expired a year
ago. The detailed gradual decline of trust can be seen in Fig. 2. The
results indicate significant differences in trust among the differently
2(3) =141.58, p <0.001).
old expired certificates (Friedman ANOVA, χ
Almost all pairwise comparisons (except for month/year) were sig-
nificant (Dunn-Bonferroni post-hoc analysis, p <0.005). The effect
size was the largest for the day/year comparison (r =0.22), followed
by day/month (r = 0.16) and week/year (r = 0.15). This confirms
that expiry is not a binary feature and people’s trust significantly
depends on the time elapsed from the expiry date.
When asked about the relation of the expiry date and the certifi-
cate revocation, only 24% of the participants (18 ) knew that expired
certificates are no longer included in the certificate revocation lists
(CRL). Of the 57 who did not know this, 25 indicated that know-
ing it would lower their trust rating. That suggests that insufficient
knowledge of the ecosystem may cause over-trusting the certificates.
3.4 Name Constraints Case
The participants’ problem descriptions in this case suggest only
a limited comprehension of the flaw and its security implications.
Less than half of the participants mentioned that the domain name
in the endpoint certificate is constrained or that there is a black-
list/whitelist for these names (Constraint, 25 ). Even fewer people
said that the CA was not allowed to issue the endpoint certificate
(CAProblem, 10 ) or that it is the CA who is constraining the name
0%25%50%75%100%Trust level:Outright untrustworthy = 0      1      2      3      4      5      6 = I’m totally satisﬁed.OKoriginalredesigned5.51±1.14Expiredo.r.2.43 ±1.66Self-signedo.r.1.92 ±1.62Hostnamemismatcho.r.0.68 ±1.08Nameconstraintso.r.2.05 ±1.66mean± std. dev.Original condition (32   )Redesigned condition (26   )1 day3.64 ±1.29mean ± std.dev.0“Outrightuntrustworthy.”2“Lookssuspicious.”4“LooksOK.”5316“I’m totallysatisﬁed.”2.62 ±1.321.33 ±1.260.47 ±0.827 days30 days365 daysWill You Trust This TLS Certificate?
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
(CAConstr,9 ).Moreparticipantswereabletodescribetheproblem
in the redesigned case (see details in Section 4.1).
The name constraint was often surrounded by explicit statements
of not understanding the problem (NotKnow, 14 ) or difficulty to
obtain further information (NoInfo, 7 ). This was expected, as name
constraints are little used (and thus little known) and the error mes-
sage in the original condition only says ‘permitted subtree violation’
without further details. The notion of name constraints being the
most complicated case is further supported by the opinion of the
participants themselves – when asked in the post-task interview
what task they saw as the most difficult to understand, almost all of
them (90%, 61 ) indicated the name constraints case.
A high count of people provided wrong reasoning (Wrong, 19 )
for the cause of the problem. A few participants (5 ) considered
the case to be the same as the hostname mismatch case. The others
blamed other (often unrelated and flawless) parts of the certificates:
the basic constraints extension (‘the CA is set to false’), the key pur-
pose extension (‘the purpose is wrong’), the name format (‘there are
wildcards’, ‘the third-level domain is missing’) or other certificates
in the chain (‘the chain integrity is broken’, ‘the intermediate CA
certificate is missing’). Note that misconceptions and the lack of un-
derstanding were more often mentioned in the original condition
(see Section 4.1). Of the few opinions on the causes of the validation
error, more participants saw it as a potential attack (Attack, 10 )
than a mistake or a misconfiguration (Mistake, 7 ).
The average trust into the connection was ‘looking suspicious’
(mean 2.05 ± 1.66, median 2, see Fig. 1). We see it as surprising
that the name constraints error is rated similarly to the expired case,
given that its security implications are potentially more severe (the
intermediate authority being corrupted). The described misunder-
standings further suggest the error message does not sufficiently
pinpoint the relevant issue in the certificate structure.
3.5 OK Case (Flawless)
Concerning the control case, the majority of participants stated there
was nothing wrong with the certificate (NoIssue, 61 ). Some par-
ticipants did an extra check manually (ExtraCheck, 13 ), though
only a few of them (3 ) hinted this behavior was influenced by the
experimental environment. Many mentioned that their opinion and
trust is based on the fact that they were instructed to presume that
the used program is bug-free (BugFree, 12 ).
Regarding the perceived trust in connections where the server is
using such a certificate, participants were on average close to ‘being
totally satisfied’ with the certificate that validated without errors
(mean 5.51±1.14, median 6, see Fig. 1). We do not find this result
surprising, given that the certificate was flawless.
3.6 Case Comprehension and Trust
Let us now compare and contrast the individual cases. We reason
about case comprehension by analyzing frequently occurring inter-
view codes describing the problem, see Fig. 3. Note that this measure
is only approximative – (not) mentioning the particular things does
not necessarily mean (not) comprehending the issue. We report only
descriptive stats due to the qualitative nature of the data.
The name constraints seem to be less understood compared to
the other cases: Firstly, the codes describing the problem are less
Figure 3: An overview of codes indicating case comprehen-
sion for all cases, compared across conditions. For more
details on the codes, see Table 1.
frequent (only about half of the participants having at least one of
these codes compared to about 80% for other cases). Secondly, there
are frequent codes explicitly admitting not understanding the issue
(NotKnow, NoInfo) or misunderstanding (Wrong). The compre-
hension seems to have been better for the redesigned condition in
some cases (see Section 4.1 for details).
The trust is compared in Fig. 1. As expected, there are again signif-
2(4) =183.5,
icant differences among the cases (Friedman ANOVA, χ
p <0.001). Pairwise comparisons (Dunn-Bonferroni) show that the
certificate in the OK case was trusted significantly more (median 6)
and the one in the hostname mismatch case was trusted significantly
less (median 0) than other cases (p < 0.005 for all comparisons).
The expired, self-signed and name constraints cases did not signif-
icantly differ from each other (median 3 for expired, median 2 for
self-signed and name constraints). The effect size was the largest for
the comparison OK/hostname mismatch (r =0.26), followed by the
comparison of OK with other cases (0.15≤ r ≤ 0.18).
Neither the high trust in the OK case nor the low trust in the host-
name mismatch case is surprising. However, we see the self-signed
and name constraints cases as over-trusted (at least when compared
to the expired case): The expired certificate provided full authenticity
assurances before expiration, but the self-signed certificate never
did (indeed anyone could have created such a certificate). The name
constraints case suggests malicious activity at the authority level,
which is far more severe (the intermediate CA was prohibited from
issuing the certificate, yet it did). Obtained trust assessments signal
potential misunderstanding of security implications in these cases.
3.7 Task Times and Used Resources
We have measured the time spent on each certificate case from the
momentofseeingthevalidationmessageforthefirsttimetillthemes-
sage of the following case was invoked. The differences among cases
2(4) =48.81,p <0.001). Post-hoc
are significant (Friedman ANOVA, χ
Original condition (39   )Redesigned condition (28   )orig., redesig.0%25%75%50%100%31%, 32%77%, 71%41%, 43%BYITSELFNOCAANYONECANSelf-signed74%, 75%BADNAMEHostnamemismatch31%, 46%5%, 29%5%, 25%CONSTRAINTCAPROBLEMCACONSTRName constraints87%, 96%NOISSUEOK87%, 100%10%, 36%OKBEFORENOLONGERExpiredACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Ukrop and Kraus, et al.
differences between experimental conditions – 44 for the original
error messages and documentation versus 31 for the redesigned
condition (39 /28 for interview codes).
Figure 4: Overview of online browsing during task comple-
tion, split and normalized by condition, compared across
cases. One participant with connection problems is omitted.
analysis (Dunn-Bonferroni) shows a significantly higher mean time
for the name constraints case (mean 4.9min.,median 4.5min.) when
compared to other cases (p ≤ 0.001 for all comparisons). All other
cases had similar means of about 2 minutes (OK 2.1 min., expired
1.75 min., self-signed 2.3 min., hostname mismatch 2.6 min.). For
detailed time distribution, see Appendix A. The standard deviations
are comparable to means, indicating the timing data might be influ-
enced by other participant-specific factors that we did not measure
(experience did not seem to have an influence, see Section 3.8).
Looking at the resources used, about three-quarters of the par-
ticipants (73%, 55 ) browsed the Internet. One of the participants
experienced technical problems with the Internet connection – we
thus only report data for 74 participants. The basic overview of the
online resource use per case can be seen in Fig. 4. The differences
2(4) = 68.85,
among the cases are significant (Cochran’s Q test, χ
p < 0.001) with the most participants browsing for information
about the name constraints case (61% of all participants, 45 ), less
about self-signed and hostname mismatch cases (32%, 24 and 31%,
23 ) and the least about the expired and OK cases (14%, 10 and 12%,
9 ). The post-hoc analysis (Dunn-Bonferroni) shows significant pair-
wise comparisons for all combinations with the name constraints
case (p ≤ 0.001) and also among three other pairs (p ≤ 0.05).
In summary, the comparison of timings and resource use confirms
obstacles in the comprehension of the name constraints case, which
is in alignment with results from previous sections.
3.8 Influence of the Previous Knowledge
Taking the self-reported previous experience into account, we find
no significant systematic influence on trust assessment or task times
(testing for the number of years in IT, formal education in com-
puter science, knowledge of security/certificates or previous usage
of other tools using a diversity of parametric and non-parametric
tests). This result is in contrast to the 2017 study by Acar et al. [3] that
reports a significant difference in both functionality and security of
programming tasks, depending on self-reported years of experience.
4 INFLUENCE OF REDESIGNED ERRORS
In this section, we examine whether error comprehension by IT
professionals can be influenced by a content change in the error mes-
sages and documentation (without a major redesign). The section
reiterates over the presented results again, focusing on the observed
4.1 Case Comprehension and Trust
Let us look again at the most common interview codes. For the
comprehension codes (see Fig. 3): In the name constraints and ex-
pired cases, the comprehension codes occur notably more frequently
in the redesigned condition (Constraint 31% vs. 46% in the re-
designed condition, CAProblem 5%/29%, CAConstr 5%/25%, No-
Longer 87%/100%, OKBefore 10%/36%). The redesigned condition
further features a decrease in the codes that describe incompre-
hension (Wrong 31%/25%, NotKnow 28%/11%, NoInfo 13%/7%,
absolute numbers in Table 1).
Looking at other codes: In the OK case, more people did man-
ual certificate checks in the redesigned condition (ExtraCheck
15%/29%), probably due to the new documentation suggesting to the
user to perform extra checks. In the redesigned condition, there was
an increase in believing that the self-signed and name constraints
error is a consequence of an attack (Attack, 5%/21% for self-signed,
8%/25% for name constraints). Other than the few mentioned differ-
ences, the occurring codes are consistent between the conditions.
Let us now look back at the trust assessment overview in Fig. 1.
We compared the differences between conditions using a Mann-
Whitney-U test for each case. The results indicate that, in the re-
designed condition, the trust was significantly lower for the self-
signed case (U = 433.00; p = 0.006, z =−2.77, r =−0.32). The mean
trust decreased from 2.32± 1.54 (median 2) to 1.35±1.58 (median 1).
There was a similar trend for hostname mismatch and name con-
straints, but the differences are not statistically significant.
Allinall,theredesigneddocumentationseemstobenoworsethan
theoriginalone.Itseemstosignificantlyincreasethecomprehension
of the flaw in the name constraints case and better conveys the possi-
ble attack vectors in the OK, self-signed and name constraints cases.
As we considered the self-signed case to be over-trusted, we see the
influence of the redesigned documentation on the trust ratings as
helpful, shifting the perception of IT professionals in the desired
direction.
4.2 Task Times and Used Resources
The overview of the task times across cases was given in Section 3.7.
For all cases, there was a general trend towards lower tasks times for
participants in the redesigned condition (see Fig. 5 in Appendix A). In
addition, there were notably fewer people with very high times (over
8 minutes) in the name constraints case (20%, 9 for the original con-