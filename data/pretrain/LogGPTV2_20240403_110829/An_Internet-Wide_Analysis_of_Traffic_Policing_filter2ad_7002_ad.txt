Another way to assess playback quality is to explore the
impact of observing a high-goodput short burst at the be-
ginning of the ﬂow, before policing starts. This can hap-
pen when the policer’s token bucket starts out with a sizable
amount of tokens. As such, a ﬂow might temporarily sustain
a rate that is good enough for HD video delivery, while the
policing rate enforced later prevents this, i.e., the rate is be-
low the target of 2.5 Mbps. To quantify the impact of this
behavior on the application, we evaluate the wait time. This
is the delay between a segment request and the time when
its playback can commence without incurring additional re-
buffering events later. We can compute wait time from our
traces since we can observe the complete segment behavior.
In this pattern, the sender slowly increases the conges-
tion window while a small number of excess tokens accu-
mulate in the bucket (1). Towards the end of this phase, the
progress curve has a slightly steeper slope than the polic-
ing rate curve. Consequently, we exceed the policing rate at
some point (the black dashed line) resulting in packet loss
(2). The congestion window is reduced during the fast re-
covery, followed by another congestion avoidance phase (3).
Staircase Pattern. A particularly destructive interaction be-
tween TCP and policers is a “staircase” when ﬂow rates be-
475
0.990.90.50 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4CDFRebuffer Time to Watch Time RatioAll other playbacksPoliced playbacks0.990.90.50 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4CDFRebuffer Time to Watch Time RatioAll other playbacksPoliced playbacks0.990.90.50 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4CDFRebuffer Time to Watch Time RatioAll other playbacksPoliced playbacks123TimeDataProgressPolicingRate1TimeDataProgressPolicingRate234123TimeDataProgressPolicingRatefore the policer drops packets are multiple times the policed
rate (Figure 10b). This results in short periods of progress
followed by long periods of stagnation, with the sequence
graph resembling a staircase.
Initially the sender pushes data successfully at a high rate
(bubble 1 in the ﬁgure). Eventually, the policer runs out of
tokens and starts dropping. Since the token refresh rate is
much lower than the transmission rate, (almost) all packets
are lost (2). This results in a high probability of the last
packet in a burst being lost, so TCP needs to fall back to
timeout-based loss detection, since there are no subsequent
packets to trigger duplicate ACKs. Consequently, the sender
idles for a long time (3). This is problematic on low-RTT
connections, since the loss detection mechanism accounts
for possibly delayed ACKs, usually requiring a timeout of
200 ms or more [7], which may be much higher than the
RTT. Once packets are marked as lost and retransmitted, the
sender accelerates quickly (4), as the policer accumulated a
large number of tokens during the idle time. In §5.1.1 and
§5.2.2 we discuss how we can avoid this pattern by optimiz-
ing a policer’s conﬁguration and reducing bursty transmits.
Doubling Window Pattern. For clients near the server,
the very low RTTs can enable connections to sustain high
throughput rates even when the congestion window (cwnd)
enables the sender to have only one packet carrying MSS
bytes in ﬂight at a time, where MSS is the maximum seg-
ment size allowed by the network. The throughput rate equals
RT T excluding loss events. The policing rate lies between
cwnd
the throughputs achieved when using a congestion window
of 1 MSS and a window of 2 MSS (see Figure 10c). Note
that the window will grow linearly on a byte granularity,
thus observing values between 1 and 2 MSS. However, Na-
gle’s algorithm in TCP delays transmissions until the win-
dow allows the transmission of a full MSS-sized packet [46].
The pattern starts with the sender pushing data while using
a congestion window of 1 MSS. In congestion avoidance
mode, the window increases by 1 MSS every RTT. Thus,
even though the window is supposed to grow slowly, it dou-
bles in this extreme case (1). Next, the higher transmission
rate makes the policer drop packets (2). The sender backs
off, setting the congestion window back to 1 MSS. Timeout-
based recovery isn’t necessary since the low amount of in-
ﬂight data enables “early retransmit” upon the reception of a
single duplicate ACK (3).
Even though the connection makes continuous progress
without excessive loss periods, valuable bandwidth is wasted.
To avoid this pattern the sender would need to send pack-
ets that carry fewer bytes than the MSS allows to match the
policing rate. Since the protocol is not conﬁgured to do this,
using a window of 1 MSS is the only setting enabling per-
manent stability. This is not supported by TCP’s congestion
control mechanism, since “congestion avoidance” will in-
crease the window by 1 MSS every RTT.
ISP
A
B
C
D
E
ISP Region
Azerbaijan
USA
India
India
Algeria
Samples
64K
31K
137K
17K
112K
RTT
Medium
Medium
Very low
Low
Medium
Mobile
(cid:88)
Table 4: Overview of 5 highly policed ISPs. The RTT estimates
apply only when content is fetched from the local cache. With
cache misses content needs to be fetched from a data center
which is potentially located much farther away, resulting in
higher RTTs.
ﬁc. Table 4 gives an overview of ﬁve ISPs where policing
was prevalent, selected to illustrate interesting pathologies
arising from policing.
Figures 11 and 12 show the policing and loss rates seen
when delivering video to clients in each ISP. We can clearly
distinguish the small set of policing rates used within each
ISP. The most popular choices are 1 and 2 Mbps, both of
which are below the 2.5 Mbps needed for HD quality videos.
For all ISPs except ISP B, we found the advertised band-
width of their data plans on their websites, and, in each case,
the plan rates matched the observed policing rates.9 For ISP
C, we recently observed a drastic change in the rate distri-
bution. In our earlier analysis from 2014, most traces were
policed at 4 Mbps, at that point a plan offered by the ISP.
Now we see 10 Mbps as the most prominent rate, which is
consistent with a change of data plans advertised. We do
observe two smaller bumps at roughly 3 Mbps and 4 Mbps.
These rates do not correspond to a base bandwidth of any of
their plans, but instead reﬂect the bandwidth given to cus-
tomers once they exceed their monthly data cap.
Losses on long-distance connections. Trafﬁc policing causes
frequent loss, but losses can be particularly costly when the
packets propagate over long distances just to be dropped
close to the client. For example, for ISP A, a local cache
node in Azerbaijan serves half the video requests, whereas
the other half is served from more than 2,000 kilometers
away. We conﬁrmed that the policer operates regardless of
content source. So the high drop rates result in a signiﬁcant
fraction of bandwidth wasted along the paths carrying the
content. The same applies to many other ISPs (including C,
D, and E) where content is sometimes fetched from servers
located thousands of kilometers away from the client.
Policing in wireless environments. We observe policing
in many areas across the globe, even in developed regions.
ISP B provides mobile access across the United States while
heavily policing some of its users to enforce a data cap.
While we understand that it is necessary to regulate access
by heavy users, we ﬁnd that there are many cases where the
bandwidth used by throttled connections is actually higher
than the bandwidth used by unthrottled ones carrying HD
content, since the latter do not incur costly retransmissions.
Large token buckets. ISP C sees heavy loss, with 90% of
segments seeing 10% loss or more. Yet, ﬂows achieve good-
4.5 Policing Pathologies
We now focus on the analysis of traces from a small set
of ISPs to highlight different characteristics of policed traf-
9Since matching policing rates to data plans is a manual process, we only
did this for the selected ISPs. However, it is unlikely that every ISP uses
policing only to enforce data plans, and we leave a thorough root cause
analysis to future work.
476
Cap. (KB)
Rebuf. (s)
16
8
64
3.5 2.0 1.5 1.6
32
128 256 512 1K 2K
1.6
3.1 3.1
1.6
2.4
Table 5: Impact of token bucket capacity on rebuffering time
of the same 30-second video playback. Policing rate is set to
500 kbps.
5. MITIGATING POLICER IMPACT
We now explore several solutions to mitigate the impact of
policing. Unless otherwise speciﬁed, we use the same setup
as for the PD validation (see §3).
5.1 Solutions for ISPs
Figure 11: Policing rates in policed segments for selected ISPs.
5.1.1 Optimizing Policing Conﬁgurations
The selection of conﬁguration parameters for a policer can
determine its impact. The policed rate usually depends on
objectives such as matching the goodput to the bandwidth
advertised in a user’s data plan and therefore may be inﬂex-
ible. However, an ISP can play with other knobs to improve
compatibility between policers and the transport layer, while
maintaining the same policing rate.
For example, we showed earlier that the staircase pattern
can arise in the presence of large token buckets. To prevent
the associated long bursty loss periods, two options come
to mind. First, the enforcing ISP could conﬁgure policers
with smaller burst sizes. This would prevent TCP’s con-
gestion window from growing too far beyond the policing
rate. For this, we again measured the performance of video
playbacks when trafﬁc is passed through a policer. We lim-
ited the rate to 500 kbps and varied the burst size between
8 kB (the smallest conﬁgurable size) and 8 MB, using pow-
ers of two as increments. In this setting, a fairly small buffer
size of 32 kB results in the lowest rebuffering delays (Ta-
ble 5). Smaller buffers prevent the policer from absorbing
any bursty trafﬁc. Larger buffers allow connections to tem-
porarily achieve throughput rates that are much larger than
the policing rates, which can result in long rebuffering events
if a quality level can no longer be sustained (i.e., the player
has to adjust to a lower bandwidth once trafﬁc is policed) or
if loss recovery is delayed (i.e., we observe a staircase pat-
tern). A more thorough sensitivity analysis is left to future
work. Second, policing can be combined with shaping, as
discussed below.
5.1.2
In contrast to a policer dropping packets, a trafﬁc shaper
enforces a rate r by buffering packets:
if the shaper does
not have enough tokens available to forward a packet im-
mediately, it queues the packet until sufﬁcient additional to-
kens accumulate. The traces of segments that pass through
a shaper resemble those of segments limited by a bottle-
neck. Shaping can provide better performance than polic-
ing. It minimizes the loss of valuable bandwidth by buffer-
ing packets that exceed the throttling rate instead of drop-
ping them immediately. However, buffering packets requires
more memory. As with policers, shapers can be conﬁgured
in different ways. A shaper can even be combined with a
policer.
In that case, the shaper spreads packet bursts out
Shaping Instead of Policing
Figure 12: Loss rates in policed segments for selected ISPs.
puts that match the policing rates (10 Mbps or more in this
case). There are three reasons for this. First, median bottle-
neck capacity is 50 Mbps on affected connections. Second,
most connections see a very small RTT. Finally, the policer
is conﬁgured to accommodate fairly large bursts, i.e., buck-
ets can accumulate a large number of tokens. This allows
the connection to “catch up” after heavy loss periods, where
progress stalls, by brieﬂy sustaining a goodput rate exceed-
ing the policing rate by an order of magnitude. When plot-
ting the progress over time, this looks like a staircase pattern
which was discussed in more detail in §4.4.
While goodputs are not adversely affected, application per-
formance can still degrade. For example, a video player
needs to maintain a large buffer of data to bridge the time
period where progress is stalled, otherwise playback would
pause until the “catch up” phase.
Small token buckets. ISP D is at the other end of the spec-
trum, accommodating no bursts by using a very small token
bucket. The small bucket combined with the low RTT re-
sults in the doubling window pattern discussed earlier (§4.4).
The small capacity also prevents a connection from “catch-
ing up.” After spending considerable time recovering from
a loss, the policer immediately throttles transmission rates
again since there are no tokens available that could be used
to brieﬂy exceed the policing rate. As such, the overall good-
put rate is highly inﬂuenced by delays introduced when re-
covering from packet loss.
Repressing video streaming. Finally, we note that we ob-
served conﬁgurations where a video ﬂow is throttled to a rate
that is too small to sustain even the lowest quality. The small
number of requests coming from affected users suggests that
they stop watching videos altogether.
477
 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5CDFPolicing Rate (Mbps)ISP AISP BISP CISP DISP E 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5CDFRetransmitted packets / all packets (per segment)ISP AISP BISP CISP DISP E(a) No modiﬁcations.
(b) With sender-side rate limit.
(c) With TCP pacing.
Figure 13: TCP sequence graphs for three ﬂows passing through a policer with a token refresh rate of 1.5 Mbps and a bucket capacity
of 8KB. The rate limit in (b) is set to 95% of the policing rate (i.e., 1.425 Mbps).
evenly before they reach the policer, allowing tokens to gen-
erate and preventing bursty losses. One key conﬁguration for
a shaper is whether to make it burst-tolerant by enabling a
“burst” phase. When enabled, the shaper temporarily allows
a goodput exceeding the conﬁgured shaping rate, similar to
Comcast’s Powerboost feature [5, 6].
Burst-tolerant Shapers. We developed a detection algo-
rithm for burst-tolerant shaping which determines whether a
given segment has been subjected to this type of shaper, and