throughput is measured by MSps (sample-per-second).
naive write-back
62.2
78.15
lock-free
81.45
are known, the receiving server can simply distribute its window
equally among all of its upstream servers. Since the number of
packets in aggregate will not exceed the switch buffer capacity, the
incast problem is avoided. Further, our application-level ﬂow con-
trol helps to keep the Ethernet switch buffer occupancy at a very low
level, and therefore reduce the Ethernet communication latency.
Lock-free computing structure. After the receiving thread reads
data from the network, it will put them into local buffers. Then, the
computing threads need to read data from all input queues and com-
pute a result for each of the output queues. How should the comput-
ing threads interact with these data queues? Figure 7 illustrates two
possible ways to connect the servers and queues. In Figure 7(a),
the computing thread can read symbols from all input queues; once
it gets a data block from one queue, it computes partial results for
all output queues, e.g., an SD server calculates the products of an
entire column of H + and the input symbol block of Y . However,
this may create heavy contention for the output queues as multi-
ple computing threads may try to write to the same output queue at
the same time. While the output queues can be protected by locks,
these locks will be heavily contended for.
A carelessly implemented locking mechanism may signiﬁcantly
reduce the system performance. For example, in our initial imple-
mentation, the computing thread would lock a buffer in the output
queue, perform the computation, write back the results in the buffer,
and then release the lock. We call this scheme naive locking. Naive
locking signiﬁcantly reduces the system processing throughput as it
locks the buffer for an unnecessarily long period.
A better scheme is using write-back locks. In this scheme, the
computing thread calculates the result in a temporary buffer ﬁrst.
Only after the computation of an entire data block does the thread
acquire the lock on the output queue, write back the results, and re-
lease the lock. This approach requires a computing thread to main-
tain an additional buffer for each output queue, but will greatly re-
duce the locking time.
The best approach, however, is to avoid locks completely. As
shown in Figure 7(b), while each computing thread is still able to
read from all input queues, it is only responsible for updating a
small group of output queues. Since each output queue is assigned
to only one computing thread, contention is avoided.
Table 2 compares the processing throughput of an SD server with
different locking schemes. We can see the lock-free scheme has the
best performance, while naive locking can reduce the processing
throughput by 23%.
Thread and core allocation. A ﬁnal question to ask is how we as-
sign the computing threads to CPU cores. We have tried a few dif-
ferent conﬁgurations. We ﬁnd that mixing the computing and com-
munication threads on one CPU core, or on two hyper-threading
cores that share the same physical core, would signiﬁcantly reduce
the communication throughput. This is because both the computing
and the communication threads will compete ﬁercely for the CPU
resource. This result leads to our ﬁrst rule: Isolating the communi-
cation and computing threads on different physical cores. Second,
we ﬁnd that assigning only one computing thread to a core has the
best performance. This is reasonable as our computing thread has
been highly optimized to maximize CPU utilization. Therefore, as-
(a)
(b)
Figure 7: Software pipeline in the processing server.
signing more threads to the same core will only incur additional
overhead (e.g., context-switching). Also, we ﬁnd utilizing hyper-
threading for computing threads does not increase the overall pro-
cessing throughput. Nor does it decrease the performance. There-
fore, our second rule is: The number of computing threads should
be between the numbers of physical and hyper-threading cores that
are dedicated to computation.
6.3 Link layer operations
Our current BigStation prototype employs a very simple TDMA
MAC. Each TDMA time slot is 2 ms long and ﬁts one frame. The
slot can be dynamically allocated to uplink or downlink transmis-
sions by a simple packet scheduler. For each uplink frame, all trans-
mitters need to send out an orthogonal training symbol (pilot) for
BigStation to learn the channel state information (CSI) (Figure 2).
Each training symbol is 8 µs long and contains a repeated pattern
like the 802.11 long training symbol (LTS), which can be used to
estimate the carrier frequency offset between each transmitter and
BigStation.
BigStation relies on channel reciprocity to obtain downlink CSI
from the uplink channel measurements. We use an approach similar
to that in [18] to calibrate the coefﬁcients between the uplink and
downlink channels. Basically, an internal calibration is performed
ﬁrst among all antennas on BigStation, after which an equivalent
downlink channel matrix can be derived from the uplink channel
measurements. This internal calibration is only needed once when
BigStation boots up. We omit the algorithm here and refer the in-
terested readers to [18] for the details.
BigStation maintains a database to store all CSI. Once a new
channel measurement is taken (e.g., through an uplink transmis-
sion), the database is updated. The CSI is removed after the chan-
nel coherence time.
In this work, we manually set this time to
20 ms. We defer the dynamic estimation of the channel coherence
time to future work. When scheduling downlink MU-MIMO trans-
missions, all selected clients should have a fresh CSI record taken
within the coherence time.
7. EVALUATION
7.1 Micro-benchmarks
We ﬁrst evaluate the capability of our existing servers for sig-
nal processing in BigStation. By benchmarking the server perfor-
mance, we try to answer the following question: how many servers
do we need to build a BigStation with a given capacity? Specif-
ically, we consider three example conﬁgurations: Medium scale,
100 Mbps to 6 users; 2) Large scale, Gbps to 10 users; and 3) Ultra-
large scale, Gbps to 50 users. The parameters of these three con-
ﬁgurations are listed in Table 3. We perform all our experiments
on the Dell servers with Intel Xeon E5520 CPUs (§6). Addition-
ally, for large scale and ultra-large scale settings, we also consider
another high-end server conﬁguration with more CPU cores. For
example, the latest Dell server is equipped with 32 cores [3].
comm. thrdcomp. thrdqueue406Table 3: Example conﬁgurations of BigStation
Channel
width
20 MHz
80 MHz
160 MHz
W M
52
234
468
12
40
100
Rate per
spatial
stream
54 Mbps
293 Mbps
585 Mbps
Medium scale
Large scale
Ultra-large scale
Parallel algorithms. As discussed earlier, the complexity of chan-
nel inversion increases with O(N 2M ). Therefore, it is more likely
to become a bottleneck as M increases (and N increases accord-
ingly, i.e., N = M in the worst case). Figure 8(a) shows the pro-
cessing time of the channel inversion on a single CPU core with
different N values. Clearly, we can see that the processing time
increases quickly with N, although the absolute processing time is
actually affordable when N is modest (< 50). For example, when
N is 12, inverting a single channel matrix takes merely 10µs. Re-
call that only one channel inversion is computed for every frame.
So a single core is able to handle about 200 subcarriers, if the frame
length is 2 ms. When N is 40, the channel inversion time for a
single subcarrier increases to 236µs. Still using 2ms frames as an
example, a single core can handle 8 subcarriers. When N grows
to 100, the invertion time rises to 3.3 ms, and a single core is not
able to handle even one subcarrier in real time. Parallel processing
among multiple cores is then essential. Figure 8(b) shows the pro-
cessing time of inverting a single channel using multiple cores. We
can see that with more cores, the processing time is reduced propor-
tionally. For example, when there are four cores to invert a channel
matrix in parallel, the processing time for N = 100 can decrease
to 607µs. One 4-core PC server can handle 3 subcarriers.
Figure 9 shows the spatial demultiplexing throughput. With a
single core, the demultiplexing throughput for 10 spatial streams
from M = 10 antennas is around 4 Gbps, sufﬁcient to support 50
subcarriers. The throughput, however, reduces to 888 Mbps and
400 Mbps, when M is 40 and 100 respectively. We can similarly
improve the processing speed with multiple cores. With 4 cores,
our server can speed up processing by 4 times to 3.2 Gbps or 8
subcarriers (M = 40) worth, and 1.6 Gbps or 1 subcarrier (M =
100).
We further evaluate our parallelized Viterbi algorithm in Fig-
ure 10. Similarly, the decoding throughput increases linearly with
the number of cores. With 4 cores, our server can deliver a through-
put of 283 Mbps.
Summary. Based on the above micro-benchmarks, we can extrap-
olate the number of servers needed to construct BigStation at dif-
ferent scales. We note that in all three example conﬁgurations, the
computation is the bottleneck. However, as discussed in §6.2, we
still need to allocate one or two cores on each server to handle the
network trafﬁc. Table 4 summarizes the results.
Although our design can scale even with low-end, 4-core servers,
we have not considered the network cost. Indeed, we expect the cost
of network devices to be signiﬁcant, but this issue can be mitigated
by upgrading servers. Given the existing trend of server technolo-
gies, we expect more cores to become available even for low-cost
commodity servers. With more cores per server, the number of total
required servers decreases proportionally, thereby reducing the cost
of network devices. All in all, we conclude that our architecture
can scale to tens to hundreds of antennas with very wide channel
widths.
(a)
(b)
Figure 8: Processing time of matrix inversion. (a) Using a single
CPU core. (b) Using multiple cores.
Figure 9: Spatial demultiplexing throughput using multiple
cores, M = N.
Table 4: # of servers to construct BigStation
4-core servers
CI
1
15
156
SD CD CI
2
1
2
30
468
20
32-core servers
SD CD
1
1
10
4
59
25
4
80
300
Medium scale
Large scale
Ultra-large scale
0500100015002000250030003500020406080100Processing time (microsecond)Matrix Size N0500100015002000250030003500405060708090100Processing time (microsecond)Nsingle core2 cores3 cores4 cores02000400060008000100001200014000020406080100Throughput (Mbps)Msingle core2 cores3 cores4 cores407Figure 10: Decoding throughput of the parallel Viterbi algo-
rithm using multiple cores.
Figure 12: Sum of peak rate of BigStation. Error bar shows the
standard deviation.
Figure 11: Layout of our testing environment. The BigStation
prototype is mounted on a mobile rack in the middle of a cu-
bicle. Clients are not marked as they move around in nearby
cubicles.
Figure 13: Channel matrix condition number with different N,
M = β N.
7.2 System performance
Testbed. We have built a medium-scale BigStation with 12 anten-
nas on our 15-server platform (§6.1). We deploy one CI server and
two SD servers – each SD server handles 26 subcarriers. All SD
and CI servers are connected to the 10G ports on the Pronto Ether-
net switch. We deploy a decoding server on each of the remaining
12 PC servers.
We test our prototype in a typical ofﬁce environment with cubi-
cles. Figure 11 shows the layout of our testbed. We have also de-
ployed 9 single-antenna clients in nearby cubicles around BigSta-
tion. Since the clients are close to BigStation, the signal-to-noise
ratios (SNRs) between the client antennas and BigStation are high,
usually between 20 ∼ 30 dB.
Sum peak rate. The ﬁrst question we ask is does large-scale MU-
MIMO even make sense? Can we indeed linearly scale wireless ca-
pacity with more antennas on BigStation? We let increasing num-
bers of clients send data packets to BigStation. Then, BigStation
tries to decode each spatial stream and ﬁnds out its peak rate, i.e.,
the maximal modulation rate it can support on each spatial stream.
Since we use 802.11a modulation rates, the peak rate is capped at
54 Mbps for each stream. For each experiment, we collect 500
frames.
In the ﬁrst experiment, we always let M = N. This case is in-
teresting as it can fully utilize the antennas on BigStation. To do
so, we randomly pick N antennas from the 12 antennas on BigSta-
tion and use only these N sample streams to decode packets. To
our surprise, the sum peak rate of N spatial streams does not scale
as we expected (“dot” line in Figure 12). When N is small, i.e., 2
and 3, the capacity seems to increase linearly – with a small slope.
When N becomes larger, the sum peak rate remains unchanged
or even decreases! The reason behind this observation lies in the
random antenna selection for MU-MIMO operations in BigStation,
which induces wireless channel hardening [11]. In an M × N MU-
MIMO system, when N is large, the sum rate can be modeled as
follows [11]:
N(cid:88)
log(cid:0)1 + P/[N (H
∗
C =