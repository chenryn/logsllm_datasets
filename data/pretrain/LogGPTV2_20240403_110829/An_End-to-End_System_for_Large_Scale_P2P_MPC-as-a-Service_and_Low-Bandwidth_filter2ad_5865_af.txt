optimizations made possible by the fact that we compromise on security with abort. The
main diﬀerence between HyperMPC and [5] is that we can skip the generation of Beaver
triples. In particular, [5] generates random double-sharings in a preparation phase, and uses
those to generate Beaver triples. Then, in the evaluation phase, the Beaver triples are used
for multiplying shared values. In contrast, HyperMPC uses the random double-sharings
generated in the preparation phase directly to multiply shared values in the evaluation
31
phase. As a result, the dominating costs per multiplication gate in the protocols are as
follows:
Protocol [5]:
1. Preparation phase: 3 random double-sharings and 1 public reconstruction
2. Evaluation phase: 2 public reconstructions
HyperMPC:
1. Preparation phase: 1 random double-sharing
2. Evaluation phase: 1 public reconstruction
This already yields a factor of 4 in the preparation phase and 2 in the evaluation phase
(and so a factor 3 overall). However, since we do not require guaranteed output delivery,
In particular, [5] can carry out n − 2t public recon-
HyperMPC is even more eﬃcient.
structions at once, whereas HyperMPC achieves n− t public reconstructions at exactly the
same cost. This means that our reconstructions are actually doubly as eﬃcient per recon-
structed value (since for t = n/3, it holds that n − t is double n − 2t). Thus, in actuality,
the improvement is is also a factor of 4 in the evaluation phase, making HyperMPC four
times faster than [5] overall.
4
Implementation and Experimental Results
We ran experiments to verify the feasibility of running MPC on end-user devices via MP-
SaaS, and to test the eﬃciency of the HyperMPC protocol. Our experiments demonstrate
that MPC is feasible in these environments, albeit not providing real-time results. We
stress that most end-user secure computations that MPSaaS would be used for do not re-
quire execution times that are measure in milliseconds. However, very long computations
are also problematic, mainly due to the possibility of parties failing midway.
4.1 Endpoint HyperMPC Experiments
We ran experiments with mobile phones, MPC-in-the-browser, and Raspberry Pi3, running
the HyperMPC protocol. We ran two sets of experiments; in the ﬁrst the size of the circuit
was varied, and in the second the depth of the circuit was varied. In all of these experiments,
we ran with 3 parties (due to the technical diﬃculties of deploying many endpoints). These
experiments will be extended to a large number of endpoints in the full version of this paper.
In Figure 10 you can see the running times (in milliseconds) of four diﬀerent platforms for
a circuit of size 10,000 multiplication gates with depths 10, 20 and 30. The Raspberry Pi3
was connected via a LAN and so depth was less signiﬁcant than for the other platforms.
32
In our experiments we used 31-bit and 61-bit Mersenne primes to deﬁne the prime ﬁelds
(modular multiplication is extremely eﬃcient modulo Mersenne primes, and these suﬃce
for integer computations).
Figure 10: 3 parties running a circuit of size 10,000 mult. gates (over a 61-bit prime), with
varying depths and platforms (time in ms).
Figure 11 presents the results of the analogous experiment with varying circuit size
(i.e., varying number of multiplication gates), where the parties run three diﬀerent circuits
of sizes 10,000, 50,000 and 100,000. In each case, the depth of the circuit was 20.
Figure 11: 3 parties running a circuit of depth 20 (over a 61-bit prime), with varying sizes
and platforms (time in ms).
33
4.2 Server HyperMPC Experiments
In addition to the endpoint experiments, we ran larger scale experiments on HyperMPC
in order to verify its scalability for a large number of parties and large circuits. In this
section, we describe these experiments. In our experiments, all parties are in a single AWS
region, running on m5.12xlarge instances.
Statistics computation – varying parties. First, we ran an experiment aimed at
demonstrating a real-world computation of interest in the multiparty setting. We consider
the case of parties holding private data, who wish to compute statistics of their joint data.
Speciﬁcally, each party holds a series of two variables (e.g., patient weight and blood sugar),
and the parties wish to compute the mean and standard deviation of each variable, and to
compute a regression test between them. The circuit has 4,000,000 inputs and 16,000,000
gates, of which approximately 10,000,000 are addition and 6,000,000 are multiplication.
This speciﬁc circuit is only of depth 1, but it models a very real computation of interest
that one may wish to compute privately (experiments on circuits of diﬀerent depths appear
above). We ran this experiment with a diﬀering number of parties, from 10 parties up to
150 parties, with increments of 10. The number of inputs is 4,000,000 always, and thus
with 10 parties it models a case where each of the 10 parties has an input database of size
400,000, and with 150 parties each of the parties has an input database of size ≈26,000. We
stress that the speciﬁc circuit we used was not optimized since the speciﬁc computation
is not of relevance; our aim was to take a circuit of a large size that represents a real
computation. Our results unequivocally demonstrate that it is possible to carry out large
scale computations using HyperMPC. The results are presented in Table 2 and Figure 12,
and clearly show the viability of the computation, and the linear scaling of the protocol.
Parties Results Parties Results Parties Results
10
20
30
40
50
5.01
7.42
9.73
14.08
15.99
60
70
80
90
100
18.42
22.57
25.13
27.35
31.34
110
120
130
140
150
33.84
36.23
40.20
41.97
44.18
Table 2: Results of the statistics experiment with a 31-bit prime (sec)
Diﬀerent ﬁeld sizes and comparison to [10]. We also ran a circuit of 1,000,000
multiplication gates and depth 20 for diﬀerent ﬁeld sizes with HyperMPC; see Table 3
and Figure 13 for the results. As is expected, the running-time of the protocol is lower for
smaller ﬁelds since the communication is lower. This is unlike previous protocols like [21, 10]
that require repetition to reduce the statistical error when the ﬁeld is small. In addition,
we compared the running times of HyperMPC to [10] for the same large ﬁeld Mersenne61.
Observe that the running times are almost the same (this can be clearly seen in Figure 13),
34
Figure 12: Statistics experiment with 6,000,000 multiplication gates; 10–150 parties
which is as expected by the theoretical analysis since [10] sends 12 ﬁeld elements per gate
and HyperMPC sends 13 ﬁeld elements per gate. We stress, however, that HyperMPC
would be much faster than [10] for smaller ﬁelds since [10] would require multiple repetitions
of the protocol in order to obtain the required security. For example, [10] would require 5
repetitions of the protocol in the case of GF [28] (the small-ﬁeld version of [10] was never
implemented and so we cannot compare to it for actual running times). We stress that
HyperMPC requires t  2n.)
36
020004000600080001000012000102030405060708090100110120130140150[12]	(Mers61)HyperMPC	(Mers61)HyperMPC	(Mers31)HyperMPC	(GF[2^8])Deeper circuits. We ran the same experiment as described in Table 3 for HyperMPC
on a circuit with 1,000,000 multiplication gates and depth 100 (instead of depth 20). The
results appear in Table 4. Observe that the running times are not much slower than for the
depth-20 circuit. This is due to the fact that the majority of the work in HyperMPC is the
preparation of double-random sharings and this is computed in parallel for all gates at the
beginning of the execution. Then, each multiplication is very cheap, and so when running
on a fast network, the diﬀerence between depth-20 and depth-100 is only about 10-20%
for most cases (indeed, the penalty is higher for GF [28]; we do not know why there is a
diﬀerence between this ﬁeld and the others on this issue). This experiment demonstrates
that even quite deep circuits can be eﬀectively run on fast networks.
Parties HyperMPC HyperMPC HyperMPC
(Mers61)
(Mers31)
(GF [28])
10