readers to write-back metadata. Metadata write-backs (i)
help to prevent malicious readers from compromising the
storage, and (ii) feature low communication latency and
bandwidth usage even in worst-case conditions. By doing
so, PoW provide an eﬃcient alternative to digital signatures
in BFT storage protocols. In what follows, we describe two
eﬃcient instantiations of PoW using cryptographic hashes,
and polynomial evaluation; we also outline their relative per-
formance gains when compared to digital signatures.
3.2 PoW based on Cryptographic Hashes
We start by outlining a PoW implementation that is based
on the use of one-way collision-resistant functions seeded
with pseudo-random input.
In the ﬁrst write round, the writer generates a pseudo-
random nonce and writes the hash of the nonce together
with the data in a quorum of servers. In the second write
round, the writer discloses the nonce and stores it in a quo-
BobAliceBobAlice?rum. During the ﬁrst round of a read operation, the client
collects the nonce from a quorum and sends (writes-back)
the nonce to a quorum of servers in the second round. The
server veriﬁes the nonce by checking that the received nonce
matches the hash of the stored nonce. If so, the server con-
ﬁrms the validity of the nonce by exposing the corresponding
stored data to the client. The client obtains a PoW after re-
ceiving t + 1 conﬁrmations pertaining to a nonce.
Since the writer keeps the nonce secret until the start of
the second round, it is computationally infeasible for the
adversary to fabricate the nonce unless the ﬁrst round of
write has completed, and hence the data is written. Thus,
if the nonce received in the ﬁrst read round hashes to the
stored hash at t + 1 servers (one of which is necessarily cor-
rect), then this provides suﬃcient proof that the nonce has
been disclosed by the writer, which implies that the data
has been written.
3.3 PoW based on Polynomial Evaluation
In what follows, we propose an alternative construction
of PoW based on polynomial evaluation. Here, at the start
of every write operation, the writer constructs a polyno-
mial P (·) of degree t with coeﬃcients {αt, . . . , α0} chosen at
random from Zq, where q is a public parameter. That is,
P (x) =(cid:80)j=t
j=0 αjxj.
The writer then constructs the PoW as follows: for each
server si, the writer picks a random point xi on P (·), and
constructs the share (xi, Pi), where Pi = P (xi). As such,
the writer constructs S diﬀerent shares, one for each server,
and sends them to each server si over a conﬁdential channel.
Note that since there are at most t Byzantine servers, these
servers cannot reconstruct the polynomial P (·) from their
In the second write round,
shares, even if they collude.
the writer reveals the polynomial P (·) to all servers. This
enables a correct server si to establish that the ﬁrst write
round has been completed by checking that the share (xi, Pi)
is indeed a point of P (·).
The argument of PoW relies on the assumption that the
correct servers holding shares agree on the validity of the
polynomial. Therefore, it is crucial to ensure that even after
the disclosure P (·), the adversary cannot fabricate a poly-
ˆP (·) (cid:54)= P (·) that intersects with P (·) in the share
nomial
of a correct server. By relying on randomly chosen xi, and
the fact that correct servers never divulge their share, our
construction prevents an adversary from fabricating ˆP (·).
Note that there exist other variant implementations for
PoW that do not leak the polynomial to the servers [26].
We point that, unlike our prior solution based on cryp-
tographic hash functions, this PoW construction provides
information-theoretic guarantees [4, 30]. Table 1 illustrates
the PoW construction and veriﬁcation costs incurred in our
PoW constructs. Owing to its reduced costs, we focus in
this paper on hash-based PoWs (Sec. 3.2).
4. PoWerStore
In this section, we provide a detailed description of the
PoWerStore protocol and we analyze its correctness. In Ap-
pendix A, we show that PoWerStore exhibits optimal worst-
case latency.
4.1 Overview of PoWerStore
In PoWerStore, the WRITE operation performs in two
consecutive rounds, called store and complete. Likewise,
Algorithm 1 Algorithm of the writer in PoWerStore.
ts : structure num, initially ts0 (cid:44) 0
1: Deﬁnitions:
2:
3: operation write(V )
ts ← ts + 1
4:
5: N ← {0, 1}λ
6: N ← H(N )
7:
8:
9:
store(ts, N , V )
complete(ts, N )
return ok
10: procedure store(ts, V, N )
{f r1, . . . , f rS} ← encode(V, t + 1, S)
11:
cc ← [H(f r1), . . . , H(f rS )]
12:
for 1 ≤ i ≤ S do send store(cid:104)ts, f ri, cc, N(cid:105) to si
13:
14: wait for store ack(cid:104)ts(cid:105) from S − t servers
15: procedure complete(ts, N )
16:
17: wait for complete ack(cid:104)ts(cid:105) from S − t servers
send complete(cid:104)ts, N(cid:105) to all servers
the read performs in two rounds, called collect and fil-
ter. For the sake of convenience, each round rnd ∈ {store,
complete, collect, filter} is wrapped by a procedure
rnd. In each round rnd, the client sends a message of type
rnd to all servers. A round completes at the latest when
the client receives messages of type rnd ack from S − t
correct servers. The server maintains a variable lc to store
the metadata of the last completed write, consisting of a
timestamp-nonce pair, and a variable LC that stores a set of
such tuples written-back by clients. In addition, the server
keeps a variable Hist storing the history, i.e., a log consist-
ing of the data written by the writer3 in the store round,
indexed by timestamp.
4.2 Write Implementation
The write implementation is given in Algorithm 1. To
write a value V , the writer increases its timestamp ts, gen-
erates a nonce N and computes its hash N = H(N ), and
invokes store with ts, V and N . When the store proce-
dure returns, the writer invokes complete with ts and N .
After complete returns, the write completes.
In store, the writer encodes V into S fragments f ri
(1 ≤ i ≤ S), such that V can be recovered from any sub-
set of t + 1 fragments. Furthermore, the writer computes
a cross-checksum cc consisting of the hashes of each frag-
ment. For each server si (1 ≤ i ≤ S), the writer sends a
store(cid:104)ts, f ri, cc, N(cid:105) message to si. On reception of such a
message, the server writes (f ri, cc, N ) into the history entry
Hist[ts] and replies to the writer. After the writer receives
S − t replies from diﬀerent servers, the store procedure
returns, and the writer proceeds to complete.
In complete, the writer sends a complete(cid:104)ts, N(cid:105) mes-
sage to all servers. Upon reception of such a message, the
server changes the value of lc to (ts, N ) if ts > lc.ts and
replies to the writer. After the writer receives replies from
S − t diﬀerent servers, the complete procedure returns.
4.3 Read Implementation
The read implementation is given in Algorithm 3; it con-
sists of the collect procedure followed by the filter pro-
cedure. In collect, the client reads the tuples (ts, N ) in-
3Recall that PoWerStore is a single-writer storage protocol.
Algorithm 2 Algorithm of server si in PoWerStore.
lc : structure (ts, N ), initially c0 (cid:44) (ts0, null)
LC : set of structure (ts, N ), initially ∅
18: Deﬁnitions:
19:
20:
21: Hist[. . . ] : vector of (f r, cc, N ) indexed by ts, with all entries initialized to (null, null, null)
22: upon receiving store(cid:104)ts, f r, cc, N(cid:105) from the writer
23: Hist[ts] ← (f r, cc, N )
24:
25: upon receiving complete(cid:104)ts, N(cid:105) from the writer
26:
27:
if ts > lc.ts then lc ← (ts, N )
send complete ack(cid:104)ts(cid:105) to the writer
send store ack(cid:104)ts(cid:105) to the writer
28: procedure gc()
29:
30:
31:
chv ← max({c ∈ LC : valid(c)} ∪ {c0})
if chv.ts > lc.ts then lc ← chv
LC ← LC \ {c ∈ LC : c.ts ≤ lc.ts}
cluded in the set LC ∪ {lc} at the server, and accumulates
these tuples in a set C together with the tuples read from
other servers. We call such a tuple a candidate and C a
candidate set. Before responding to the client, the server re-
moves obsolete tuples from LC using the gc procedure. Af-
ter the client receives candidates from S−t diﬀerent servers,
collect returns.
In filter, the client submits C to each server. Upon
reception of C, the server performs a write-back of the can-
didates in C (metadata write-back ). In addition, the server
picks chv as the candidate in C with the highest timestamp
such that valid(chv) holds, or c0 if no such candidate ex-
ists. The predicate valid(c) holds if the server, based on
the history, is able to verify the integrity of c by check-
ing that H(c.N ) equals Hist[c.ts].N . The server then re-
sponds to the client with a message including the timestamp
chv.ts, the fragment Hist[chv.ts].f r and the cross-checksum
Hist[chv.ts].cc. The client waits until S − t diﬀerent servers
responded and either (i) safe(c) holds for the candidate with
the highest timestamp in C, or (ii) all candidates have been
excluded from C, after which collect returns. The pred-
icate safe(c) holds if at least t + 1 diﬀerent servers si have
responded with timestamp c.ts, fragment f ri and cross-
checksum cc such that H(f ri) = cc[i]. If C (cid:54)= ∅, the client
selects the candidate with the highest timestamp c ∈ C and
restores value V by decoding V from the t + 1 correct frag-
ments received for c. Otherwise, the client sets V to the
initial value ⊥. Finally, the read returns V .
4.4 Analysis
In what follows, we show that PoWerStore is robust, guar-
anteeing that read/write operations are linearizable and
wait-free. We start by proving a number of core lemmas, to
which we will refer to throughout the analysis.
Definition 4.1
(Valid candidate). A candidate c is
valid if valid(c) holds at some correct server.
Lemma 4.2
(Proofs of Writing). If c is a valid can-
didate, then there exists a set Q of t + 1 correct servers such
that for each server si ∈ Q, H(c.N ) = Histi[c.ts].N .
Proof:
If c is valid, then by Deﬁnition 4.1, H(c.N ) =
Histj[c.ts].N holds at some correct server sj. By the pre-
image resistance of H, no computationally bounded adver-
sary can acquire c.N from the sole knowledge of H(c.N ).
//last completed write
//set of written-back candidates
gc()
send collect ack(cid:104)tsr, LC ∪ {lc}(cid:105) to client r
32: upon receiving collect(cid:104)tsr(cid:105) from client r
33:
34:
35: upon receiving filter(cid:104)tsr, C(cid:105) from client r
36:
37:
38:
39:
LC ← LC ∪ C
chv ← max({c ∈ C : valid(c)} ∪ {c0})
(f r, cc) ← πf r,cc(Hist[chv.ts])
send filter ack(cid:104)tsr, chv.ts, f r, cc(cid:105) to client r
//write-back
40: Predicates:
41:
valid(c) (cid:44) (H(c.N ) = Hist[c.ts].N )
Hence, c.N stems from the writer in a write operation wr
with timestamp c.ts. By Algorithm 1, line 8, the value of
c.N is revealed only after the completion of the store round
in wr. Hence, by the time c.N is revealed, there is a set Q
of t + 1 correct servers such that each server si ∈ Q assigned
Histi[c.ts].N to H(c.N ).
Lemma 4.3
(No exclusion). Let c be a valid candi-
date and let rd be a read by a correct reader that includes
c in C during collect. Then c is never excluded from C.
Proof: As c is valid, by Lemma 4.2 there is a set Q of t + 1
correct servers such that each server si ∈ Q, H(c.N ) =
Histi[c.ts]. Hence, valid(c) is true at every server in Q.
Thus, no server in Q replies with a timestamp ts  ts0}
return C
64: procedure filter(tsr, C)
65:
66: wait until |R| ≥ S − t ∧
send filter(cid:104)tsr, C(cid:105) to all servers
((∃c ∈ C : highcand(c) ∧ safe(c)) ∨ C = ∅)
return C
67: