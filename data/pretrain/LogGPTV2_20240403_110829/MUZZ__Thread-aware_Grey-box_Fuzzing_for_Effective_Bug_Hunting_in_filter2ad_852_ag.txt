paths over time among MUZZ, MAFL, AFL, and MOPT.
This reason is simple: we resort to a separate procedure after
fuzzing to determine whether it covers thread-forking rou-
tines. We have to do so since AFL and MOPT do not pro-
vide a builtin solution to discovering seeds’ relevance with
multithreading. Consequently, we cannot plot multithreading-
relevant crashing states over time.
Third, despite that the statistical variance is important, it
is not easy to be calculated comprehensively. During eval-
uation, to reduce the variance among individuals, we apply
an ensemble strategy by sharing seeds among the six runs,
for each of the speciﬁc fuzzers [63]. However, for multi-
threaded target programs, another variance comes from the
thread scheduling for different threads (in our experiments,
four working threads were speciﬁed). MUZZ and MAFL have
the schedule-intervention instrumentation to help diversify
the effects, while it is absent in AFL and MOPT. In fact, from
the case studies in §6.5.2, we envision that the variance may
be huge for different machines under different workloads. Due
to this, providing fair statistical results w.r.t. the variance may
still be impractical. Therefore, we tend to exclude variance
metrics and only choose those that exhibit the “overall re-
sults”, i.e., Nmt, Nmt
B . Similarly, the case
Nall
studies or comparisons in §6.2, §6.3, §6.4 are all based on
“overall results”. During the evaluation, we indeed observed
that the results of MUZZ and MAFL are more stable than
those of AFL and MOPT.
e , and Nm
, Nm
c , Nm
v , Nm
7 Related Work
7.1 Grey-box Fuzzing Techniques
The most relevant is the fuzzing techniques on concurrency-
vulnerability detection. ConAFL [30] is a thread-aware GBF
that focuses on user-space multithreaded programs. Much dif-
ferent from MUZZ’s goal to reveal both Vm and Bm, ConAFL
only detects a subset of concurrency-bug induced vulnera-
bilities (Vcb) that cause buffer-overﬂow, double-free, or use-
after-free. ConAFL also utilizes heavy thread-aware static
and dynamic analyses, making it suffer from scalability is-
sues. The other difference is that MUZZ’s thread-aware anal-
yses aim to provide runtime feedback to distinguish more
execution states in multithreading contexts, to bring more
multithreading-relevant seeds; meanwhile, ConAFL relies on
the discovery of sensitive concurrency operations to capture
pairs that may introduce the aforementioned three kinds of
vulnerabilities. Further, since the static and dynamic analyses
aim to capture and intervene “sensitive concurrency operation
pairs”, ConAFL suffers from the scalability issue. In fact,
the biggest binary it evaluated was 196K (bzip2smp), while
MUZZ can handle programs scaling to 19.4M (im-cnvt). In the
evaluation, we did not evaluate ConAFL — the GitHub ver-
sion of ConAFL (https://github.com/Lawliar/ConAFL)
does not work since its static analysis is not publicly available
and it is not trivial to implement that technique ourselves; fur-
ther, we have not obtained the runnable tool after we requested
from the authors. RAZZER [24] utilizes a customized hyper-
visor to control thread-interleaving deterministically to trigger
data races in Linux kernel. It is a kernel fuzzer that cannot re-
veal multithreading-relevant bugs in user-space programs. As
a matter of fact, the proof-of-crashes are essentially sequences
of system calls that could trigger race conditions, and the ﬁx
of the detected vulnerabilities requires patches to the kernel
code. Consequently, the guidance of fuzzing is also different.
RAZZER spots the over-approximated racing segments and
tames non-deterministic behavior of the kernel such that it can
deterministically trigger a race. While MUZZ’s solution is to
distinguish more thread-interleaving states to trap the fuzzing
to reveal more multithreading-relevant paths. Practically, it is
not easy to effectively sequentialize the thread-interleavings
to fuzz the user-space programs [64].
Multithreading-relevant bugs are inherently deep. To re-
veal deep bugs in the target programs, some GBFs facilitate
other feedback [7,14,29,44,52,55,56,61]. Angora [7] distin-
guishes different calling context when calculating deputy in-
struction transitions to keep more valuable seeds. Driller [44],
QSYM [61], and Savior [8] integrate symbolic execution to
provide additional coverage information to exercise deeper
paths. MUZZ inspires from these techniques in that it pro-
vides more feedback for multithreading context with strati-
ﬁed coverage-oriented and thread-context instrumentations, as
well as schedule-intervention instrumentation. Other fuzzing
techniques utilize the domain knowledge of the target pro-
USENIX Association
29th USENIX Security Symposium    2339
gram to generate more effective seeds [39,53,54]. Skyﬁre [53]
and Superion [54] provide customized seed generation and
mutation strategies on the programs that feed grammar-based
inputs. SGF [39] relies on the speciﬁcations of the structured
input to improve seed quality. These techniques are orthog-
onal to MUZZ and can be integrated into seed mutation (c.f.
B in Figure 3).
7.2 Static Concurrency-bug Prediction
Static concurrency-bug (Bm) predictors aim to approximate
the runtime behaviors of a concurrent program without actual
execution. Several static approaches have been proposed for
analyzing Pthread and Java programs [40, 45, 50]. LOCK-
SMITH [40] uses existential types to correlate locks and data
in dynamic heap structures for race detection. Goblint [50]
relies on a thread-modular constant propagation and points-to
analysis for detecting concurrent bugs by considering condi-
tional locking schemes. [51] scales its detection to large code-
bases by sacriﬁcing soundness and suppressing false alarms
using heuristic ﬁlters. FSAM [45, 46] proposes a sparse ﬂow-
sensitive pointer analysis for C/C++ programs using context-
sensitive thread-interleaving analysis. Currently, MUZZ re-
lies on ﬂow- and context-insensitive results of FSAM for
thread-aware instrumentations. We are seeking solutions to
integrating other bug prediction techniques to further improve
MUZZ’s effectiveness.
7.3 Dynamic Analysis on Concurrency-bugs
There are a large number of dynamic analyses on concurrency-
bugs. They can be divided into two categories: modeling
concurrency-bugs and strategies to trigger these bugs.
The techniques in the ﬁrst category [12,41,42,59] typically
monitor the memory and synchronization events [19]. The
two fundamentals are happens-before model [12] and lockset
model [41]. Happens-before model reports a race condition
when two threads read/write a shared memory arena in a
causally unordered way, while at least one of the threads write
this arena. Lockset model conservatively considers a potential
race if two threads read/write a shared memory arena without
locking. Modern detectors such as TSan [42], Helgrind [49]
usually apply a hybrid strategy to combine these two mod-
els. MUZZ does not aim to improve existing concurrency
violation models; instead, it relies on these models to detect
concurrency-bugs with our fuzzer-generated seeds.
The second category of dynamic analyses focuses on how
to trigger concurrency violation conditions. This includes
random testings that mimic non-deterministic program execu-
tions [4, 25, 38], regression testings [47, 60] that target inter-
leavings from code changes, model checking [13, 57, 62] and
hybrid constraint solving [20–22] approaches that systemati-
cally check or execute possible thread schedules, heuristically
avoid fruitless executions [10, 17, 18, 66], or utilizing multi-
core to accelerate bug detection [37]. Our work differs from
all the above, as our focus is not to test schedules with a given
seed ﬁle, but to generate seed ﬁles that execute multithreading-
relevant paths. In particular, our goal of schedule-intervention
instrumentation is to diversify the actual schedules to help
provide feedback during fuzzing.
8 Conclusion
This paper presented MUZZ, a novel technique that empow-
ers thread-aware seed generation to GBFs for fuzzing multi-
threaded programs. Our approach performs three novel instru-
mentations that can distinguish execution states introduced by
thread-interleavings. Based on the feedback provided by these
instrumentations, MUZZ optimizes the dynamic strategies to
stress different kinds of multithreading context. Experiments
on 12 real-world programs demonstrate that MUZZ outper-
forms other grey-box fuzzers such as AFL and MOPT in gen-
erating valuable seeds, detecting concurrency-vulnerabilities,
as well as revealing concurrency-bugs.
Acknowledgement
This research was supported (in part) by the National
Research Foundation, Prime Ministers Ofﬁce, Singapore
under its National Cybersecurity R&D Program (Award
No. NRF2018NCR-NCR005-0001), National Satellite of
Excellence in Trustworthy Software System (Award No.
NRF2018NCR-NSOE003-0001), and NRF Investigatorship
(Award No. NRFI06-2020-0022) administered by the Na-
tional Cybersecurity R&D Directorate. The research of Dr
Xue is supported by CAS Pioneer Hundred Talents Program.
References
[1] L. O. Andersen. Program analysis and specialization for
the c programming language. Technical report, DIKU,
University of Copenhagen, 1994.
[2] S. Blackshear, N. Gorogiannis, P. W. O’earn, and
I. Sergey. Racerd: Compositional static race detection.
OOPSLA, 2:144:1–144:28, Oct. 2018.
[3] M. Böhme, V. T. Pham, and A. Roychoudhury.
Coverage-based greybox fuzzing as markov chain. In
CCS ’16, pages 1032–1043. ACM, 2016.
[4] Y. Cai and W. K. Chan. Magicfuzzer: Scalable dead-
lock detection for large-scale applications. In ICSE ’12,
pages 606–616. IEEE, 2012.
[5] Y. Cai, B. Zhu, R. Meng, H. Yun, L. He, P. Su, and
B. Liang. Detecting concurrency memory corruption
vulnerabilities. In ESEC/FSE ’19, pages 706–717, 2019.
2340    29th USENIX Security Symposium
USENIX Association
[6] H. Chen, Y. Xue, Y. Li, B. Chen, X. Xie, X. Wu, and
Y. Liu. Hawkeye: Towards a desired directed grey-box
fuzzer. In CCS ’18, pages 2095–2108. ACM, 2018.
[22] J. Huang, P. O. Meredith, and G. Rosu. Maximal sound
predictive race detection with control ﬂow abstraction.
In PLDI ’14, pages 337–348, 2014.
[7] P. Chen and H. Chen. Angora: Efﬁcient fuzzing by
principled search. In SP ’18, pages 711–725, 2018.
[8] Y. Chen, P. Li, J. Xu, S. Guo, R. Zhou, Y. Zhang, T. Wei,
and L. Lu. SAVIOR: towards bug-driven hybrid testing.
In SP ’20, 2020.
[9] I. Chowdhury and M. Zulkernine. Using complexity,
coupling, and cohesion metrics as early indicators of vul-
nerabilities. Journal of System Architecture, 57(3):294–
313, Mar. 2011.
[23] IEEE and The Open Group. POSIX.1-2017, 2001.
[24] D. R. Jeong, K. Kim, B. Shivakumar, B. Lee, and I. Shin.
RAZZER: Finding kernel race bugs through fuzzing. In
SP ’19, volume 00, pages 279–293, 2019.
[25] P. Joshi, C. Park, K. Sen, and M. Naik. A randomized
dynamic program analysis technique for detecting real
deadlocks. In PLDI ’09, pages 110–120, 2009.
[26] M. Kerrisk. The Linux Programming Interface. No
Starch, 2010.
[10] M. Christakis, A. Gotovos, and K. Sagonas. System-
atic testing for detecting concurrency errors in erlang
programs. In ICST 2013, pages 154–163, March 2013.
[27] G. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks.
Evaluating fuzz testing. In CCS ’18, pages 2123–2138.
ACM, 2018.
[11] P. Di and Y. Sui. Accelerating dynamic data race detec-
tion using static thread interference analysis. In PMAM
’16, pages 30–39. ACM, 2016.
[28] C. Lattner and V. Adve. LLVM: A compilation frame-
work for lifelong program analysis & transformation. In
CGO ’04, pages 75–. IEEE Computer Society, 2004.
[12] C. Flanagan and S. N. Freund. FastTrack: efﬁcient and
In PLDI ’09, pages
precise dynamic race detection.
121–133. ACM, 2009.
[29] Y. Li, B. Chen, M. Chandramohan, S. Lin, Y. Liu, and
A. Tiu. Steelix: Program-state based binary fuzzing. In
ESEC/FSE ’17, pages 627–637. ACM, 2017.
[13] C. Flanagan and P. Godefroid. Dynamic partial-order
reduction for model checking software. In POPL ’05,
pages 110–121. ACM, 2005.
[14] S. Gan, C. Zhang, X. Qin, X. Tu, K. Li, Z. Pei, and
In SP ’18,
Z. Chen. Collaﬂ: Path sensitive fuzzing.
pages 1–12. IEEE, 2018.
[15] Google Inc. OSS-Fuzz, 2018.
[16] Google Inc. Clusterfuzz, 2019.
[17] S. Guo, M. Kusano, and C. Wang. Conc-iSE: Incremen-
tal symbolic execution of concurrent software. In ASE
’16, pages 531–542. ACM, 2016.
[18] S. Guo, M. Kusano, C. Wang, Z. Yang, and A. Gupta.
Assertion guided symbolic execution of multithreaded
programs. In ESEC/FSE ’15, pages 854–865, 2015.
[19] S. Hong and M. Kim. A survey of race bug detec-
tion techniques for multithreaded programmes. STVR,
25(3):191–217, May 2015.
[20] J. Huang. Stateless model checking concurrent pro-
grams with maximal causality reduction. In PLDI ’15,
pages 165–174. ACM, 2015.
[21] J. Huang. UFO: Predictive concurrency use-after-free
detection. In ICSE ’18, pages 609–619. ACM, 2018.
[30] C. Liu, D. Zou, P. Luo, B. B. Zhu, and H. Jin. A heuristic
In
framework to detect concurrency vulnerabilities.
ACSAC ’18, pages 529–541. ACM, 2018.
[31] LLVM. libFuzzer, 2015.
[32] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from
mistakes: A comprehensive study on real world con-
In ASPLOS ’08, pages
currency bug characteristics.
329–339. ACM, 2008.
[33] C. Lyu, S. Ji, C. Zhang, Y. Li, W. Lee, Y. Song, and
R. Beyah. MOPT: Optimized mutation scheduling for
In USENIX Security ’19, pages 1949–1966.
fuzzers.
USENIX Association, 2019.
[34] V. J. M. Manès, H. Han, C. Han, S. K. Cha, M. Egele,
E. J. Schwartz, and M. Woo. Fuzzing: Art, science, and
engineering. CoRR, abs/1812.00140:1–29, 2018.
[35] T. J. McCabe. A complexity measure. IEEE Transac-
tions on Software Engineering, SE-2(4):308–320, 1976.
[36] B. P. Miller, L. Fredriksen, and B. So. An empirical
study of the reliability of unix utilities. Communications
of the ACM, 33(12):32–44, Dec. 1990.
[37] S. Nagarakatte, S. Burckhardt, M. M. K. Martin, and
M. Musuvathi. Multicore acceleration of priority-based
schedulers for concurrency bug detection. In PLDI ’12,
pages 543–554. ACM, 2012.
USENIX Association
29th USENIX Security Symposium    2341
[38] C. Park and K. Sen. Randomized active atomicity vio-
lation detection in concurrent programs. In ESEC/FSE
’08, pages 135–145, 2008.
[39] V. Pham, M. Böhme, A. E. Santosa, A. R. Caciulescu,
and A. Roychoudhury. Smart greybox fuzzing. CoRR,
abs/1811.09447:1–16, 2018.
[40] P. Pratikakis, J. S. Foster, and M. Hicks. LOCKSMITH:
context-sensitive correlation analysis for race detection.
ACM SIGPLAN Notices, 41(6):320–331, 2006.
[41] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and
T. Anderson. Eraser: A dynamic data race detector for
multithreaded programs. ACM Trans. Comput. Syst.,
15(4):391–411, Nov. 1997.
[42] K. Serebryany and T. Iskhodzhanov. Threadsanitizer:
In WBIA ’09, pages
Data race detection in practice.
62–71. ACM, 2009.
[43] M. O. Shudrak and V. Zolotarev.
Improving
fuzzing using software complexity metrics. CoRR,
abs/1807.01838:1–16, 2018.
[44] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang,
J. Corbetta, Y. Shoshitaishvili, C. Kruegel, and G. Vigna.
Driller: Augmenting fuzzing through selective symbolic
execution. In NDSS ’16. The Internet Society, 2016.
[45] Y. Sui, P. Di, and J. Xue. Sparse ﬂow-sensitive pointer
analysis for multithreaded programs. In CGO ’16, pages
160–170. ACM, 2016.
[46] Y. Sui and J. Xue. SVF: Interprocedural static value-
In CC ’16, pages 265–266.
ﬂow analysis in LLVM.
ACM, 2016.
[47] V. Terragni, S. Cheung, and C. Zhang. RECONTEST:
effective regression testing of concurrent programs. In
ICSE ’15, pages 246–256, 2015.
[48] The MITRE Corporation. Download CVE List, 1999.
[49] Valgrind. Helgrind: a thread error detector, 2000.
[50] V. Vojdani and V. Vene. Goblint: Path-sensitive data
race analysis. Annales Univ. Sci. Budapest., Sect. Comp.,
pages 1–12, 2009.
[51] J. W. Voung, R. Jhala, and S. Lerner. Relay: static race
detection on millions of lines of code. In ESEC/FSE
’07, pages 205–214. ACM, 2007.
[52] H. Wang, X. Xie, Y. Li, C. Wen, Y. Liu, S. Qin, H. Chen,
and Y. Sui. Typestate-guided fuzzer for discovering
use-after-free vulnerabilities. In ICSE ’20, 2020.
[53] J. Wang, B. Chen, L. Wei, and Y. Liu. Skyﬁre: Data-
driven seed generation for fuzzing. In SP ’17, pages
579–594, May 2017.
[54] J. Wang, B. Chen, L. Wei, and Y. Liu.
Superion:
grammar-aware greybox fuzzing. In ICSE ’19, pages
724–735. IEEE / ACM, 2019.
[55] Y. Wang, X. Jia, Y. Liu, K. Zeng, T. Bao, D. Wu, and
P. Su. Not all coverage measurements are equal: Fuzzing
by coverage accounting for input prioritization.
In
NDSS’ 20, 2020.
[56] C. Wen, H. Wang, Y. Li, S. Qin, Y. Liu, Z. Xu, H. Chen,
X. Xie, G. Pu, and T. Liu. Memlock: Memory usage
guided fuzzing. In ICSE ’20, 2020.
[57] Y. Yang, X. Chen, and G. Gopalakrishnan. Inspect: A
runtime model checker for multithreaded c programs.
Technical report, Technical Report UUCS-08-004, Uni-
versity of Utah, 2008.
[58] Y. Yang, X. Chen, G. Gopalakrishnan, and R. M. Kirby.
Distributed dynamic partial order reduction based veriﬁ-
cation of threaded software. In SPIN ’07, pages 58–75.
Springer-Verlag, 2007.
[59] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam.
Maple: a coverage-driven testing tool for multithreaded
programs. In OOPSLA ’12, pages 485–502, 2012.
[60] T. Yu, Z. Huang, and C. Wang. Contesa: Directed test
suite augmentation for concurrent software. IEEE Trans-
actions on Software Engineering, 2018.
[61] I. Yun, S. Lee, M. Xu, Y. Jang, and T. Kim. QSYM :
A practical concolic execution engine tailored for hy-
brid fuzzing. In USENIX Security ’18, pages 745–761.
USENIX Association, 2018.
[62] A. Zaks and R. Joshi. Verifying multi-threaded C pro-
grams with SPIN. In SPIN ’08, pages 325–342, 2008.
[63] M. Zalewski. Technical "whitepaper" for aﬂ-fuzz, 2014.
[64] M. Zalewski. What is the drawback of fuzzing a multi-
threaded binary?, 2015.
[65] M. Zalewski. "ﬁdgetyaﬂ" implemented in 2.31b, 2016.
[66] W. Zhang, J. Lim, R. Olichandran, J. Scherpelz, G. Jin,
S. Lu, and T. Reps. ConSeq: detecting concurrency
bugs through sequential errors. In ASPLOS ’11, pages
251–264. ACM, 2011.
2342    29th USENIX Security Symposium
USENIX Association