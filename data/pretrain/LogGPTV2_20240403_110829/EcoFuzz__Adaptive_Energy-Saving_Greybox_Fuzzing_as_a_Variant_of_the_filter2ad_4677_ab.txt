program A, several assumptions are stated below.
Assumption 3.1 The number of total paths and unique
crashes that can be executed of program A are ﬁnite, denoted
as np and nc, respectively.
This assumption helps to consider the mathematical model
in the ﬁnite state space, which could simplify the problem.
Assumption 3.2 The program A is stateless. That is, the path
of each execution depends only on the input generated by
fuzzer.
This assumption ensures that the reward probability is in-
dependent in VAMAB model, only determined by the seed.
The following are some important deﬁnitions.
Deﬁnition 3.1 The set of total paths of program A is signi-
ﬁed as S = {1,2, ...,np} and the corresponding seeds set is
denoted as T = {t1,t2, ...,tnp}.
Deﬁnition 3.2 We followed the deﬁnitions of transition prob-
ability pi j and the minimum energy E[Xi j] in [6]. pi j is the
probability of generating a test case exercising path j from the
seed ti. E[Xi j] is the expectation of minimum energy (i.e., the
number of test cases generated by ti) of this process, deduced
as 1/pi j in [6].
Deﬁnition 3.3 Based on Deﬁnition 3.2, we deﬁne the tran-
sition frequency fi j as the frequency of path transition from
path i to path j, as
fi j =
fi( j)
s(i)
(3)
fi( j) indicates the number of test cases exercising path j
generated by seed ti. Particularly, fii is deﬁned as the self-
transition frequency. s(i) is the number of trials conducted
to seed ti, satisfying
s(i) =
np
∑
j=1
fi( j)
(4)
Deﬁnition 3.4 We deﬁne the probability of mutating ti for
generating inputs executing other paths as pi∗, deduced as
pi∗ = 1− pii =
pi j − pii =
np
∑
j=1
np
∑
j=1, j(cid:54)=i
pi j
(5)
Providing the queue with n seeds is Tn, |Tn| = n, 1 ≤ n < np,
some of the seeds in Tn that have been fuzzed are denoted
n and the others are marked as T−
as T +
n . Additionally, the
number of trials being conducted thus far is m.
When fuzzing the program A, the aim might be maximizing
the number of discovered crashes and paths of A as well as
assuming them as the arms in the MAB model. However, Woo
et al. [36] pointed out that focusing on one seed may trigger
the same crashes, thus impacting the selection in exploitation.
Thus, our model regards the seeds as the arms and aims
to maximize path coverage in ﬁnite trials. Therefore, we
deﬁne the reward of each trial as generating an input that
triggers new path. Each trial to play an arm i denotes mutating
a corresponding seed ti and executing the generated test case.
Now we have conducted the trials for m times. ∀ti ∈ Tn, we
denote earn a reward in next trial as,
Ri(m + 1,Tn) = 1
(6)
The probability of the arm i to earn a reward (i.e., discovering
a new path) in this trial is deduced as
P(Ri(m + 1,Tn) = 1) =
pi j
np
∑
= 1− n
∑
j=n+1
j=1
(7)
pi j
We deﬁne this probability as the reward probability. Ac-
cording to Equation (7), we can deduce that: (1) the reward
probability P(Ri(m + 1,Tn) = 1) depends only on the seed ti
and the seeds set Tn of discovered paths, and is not related
to the number of trials being conducted (i.e., m). Thus, the
reward probability is simpliﬁed as PRi,n; (2) with a rise in
the number of discovered seeds n, there is a decrease in the
number of undiscovered paths (np − n) which leads to a re-
duction in the probability of arm i to ﬁnd new paths. These
are following the general results in most evaluation that as
more paths are found, the discovery of new paths decelerates
monotonically [6].
Therefore, it is evident that the distribution of the reward of
each arm is not invariant. Actually, the probability decreases
2310    29th USENIX Security Symposium
USENIX Association
once a reward is gained in some trials. This is called proba-
bility attenuation. As a result, the process of fuzzing is not
modeled as the classic MAB model, which is closer to the
AMAB model. Moreover, according to the mechanism of
CGF, once a reward is earned, it leads to a new and interesting
path. New seed will also be added into the queue of seeds,
with the seeds set Tn transferring into Tn+1 and the number of
arms increasing to n + 1, as shown in Fig. 2. Based on these
differences, this problem is deﬁned as a VAMAB.
As opposed to the traditional MAB model, the number of
arms of the VAMAB model will increase, and the reward
probability will decrease if rewards are earned until all paths
of program A are found. Therefore, before discovering all
paths, there is always a trade-off between exploration (fuzzing
seeds that have been not fuzzed) and exploitation (selecting
the fuzzed seeds to get more rewards).
Figure 2: The ﬁgure illustrating VAMAB model , in which the
grey color block symbolizes that this seed has been fuzzed.
3.2 Exploration vs Exploitation in VAMAB
Model
Providing we could calculate the reward probability of seeds
after conducting some trials on them, for the seeds set Tn,
we can determine the reward probability PRi,n of the seed
ti from T +
n , which is the set of fuzzed seeds. Then we can
calculate the minimum energy the seed requires to ﬁnd new
paths following Deﬁnition 3.2. For gaining more rewards in a
short period, it may be better to select the seeds from T +
n with
the highest reward probability, as “exploitation”. In contrast,
focusing on the unfuzzed seeds in T−
n and allocating them
enough energy can help to calculate their reward probability.
Seeds with higher reward probability may be found from T−
n
compared to those from T +
n , as “exploration”.
Thus, based on the level of testing on the seeds, as shown
in Fig. 3, the states of Tn were classiﬁed into three categories:
(1) Initial State. The initial state refers to the ﬁrst stage of
the fuzzing process, where all seeds are unfuzzed. After
beginning the fuzzing of the seeds, the initial state transi-
tions to the exploration or exploitation state, as indicated
by Curve 1 and Curve 2 in Fig. 3.
(2) Exploration State. In this state, some seeds in Tn are
fuzzed, while some are not. Therefore, energy should
be assigned to the seeds that have not been fuzzed to
earn rewards and estimate their reward probability. After
Figure 3: The three states of the seeds set and the transition
relationship between them, in which the grey color block
symbolizes that this seed has been fuzzed.
attaining a reward, Tn transits to Tn+1. Once all seeds in
Tm are fuzzed, the exploration state transitions into the
exploitation state, as shown by Curve 3 in Fig. 3.
(3) Exploitation State. In this state, all seeds have been
fuzzed. It is crucial to select those seeds with the highest
reward probability to test for discovering new paths. Once
a test case exercises an undiscovered path, the transition
from the exploitation to exploration occurs until all paths
have been found, as shown by Curve 4 in Fig. 3.
For these three states, it is necessary to implement different
strategies to maximize rewards. As previously discussed, it
is risky to focus only on exploitation and skip exploration.
Therefore, we considered the strategy of testing each seed in
the initial and exploration stage and selecting the high-quality
seeds with high reward probabilities in the exploitation stage.
3.3 Challenges in VAMAB Model
Although we have proposed how to improve the efﬁciency of
the scheduling algorithm, some challenges persisted.
The ﬁrst challenge is how to determine the reward prob-
ability of each seed to select the next seed in the exploita-
tion stage. Given ti ∈ Tn, its reward probability PRi,n is certain.
According to Equation (7), the reward probability depends
on transition probability. In [6], Böhme et al. calculated the
transition probability between seeds in an example. How-
ever, determining the transition probability pi j relies on the
path constraints of path i and j, which can only be inferred
through manual analysis with source code, not accessed by
CGF. Therefore, we could not accurately calculate the re-
ward probability of seeds despite conducting several trials
on the seeds. We can only estimate it. A common method is
to estimate the transition probability through transition fre-
quency. That is, for pi j, it is possible to approximate it as fi j
for 1 ≤ i, j ≤ n. However, based on Equation (3), (4) and (7),
we may estimate the reward probability PRi,n as
PRi,n ≈ 1− n
∑
j=1
fi j = 1− n
∑
j=1
fi( j)
s(i)
= 0
(8)
USENIX Association
29th USENIX Security Symposium    2311
Seed-1Seed-3Seed-2Seed-N……Seed-1Seed-3Seed-2Seed-N……R1R2R1R2R3???Fuzzing for some timesSeed-(N+1)?Getting a rewardTnSeedSeedSeedSeedSeedSeedSeedSeedSeedSeedSeedInitialExplorationExploitation1234This is useless for CGF to select seeds. Consequently, it is im-
portant to ﬁnd other criteria or parameters for approximating
the reward probability to select the seeds to fuzz.
The second challenge pertains to how to assign suitable
energy to each arm to balance the trade-off between ex-
ploration and exploitation. Especially in the exploration
stage, assigning too much energy to an unfuzzed seed in T−
n
is very risky. Researchers proposed some algorithms for re-
solving the problem of trade-off in the Adversarial MAB
problem (e.g., Exp3) [3]. However, this algorithm is based
on the assumption that the number of arms is constant. Our
model differs from the traditional AMAB problem on the
variability of the number of arms. Therefore, some current
algorithms are not suitable for our model.
Therefore, to maximize the path coverage, we need to es-
tablish efﬁcient mechanisms, which use existing information
to estimate the reward probability of each seed for searching
seeds in the exploitation stage and allocate appropriate energy
to seeds for reducing energy waste.
4 Implementation
In this section, we implemented a prototype tool called Eco-
Fuzz. We introduce the framework and algorithm of EcoFuzz
ﬁrstly. After that, we detail the search strategy and energy
schedule algorithm implemented in EcoFuzz.
4.1 Main Framework of EcoFuzz
EcoFuzz is based on AFL 2.52b, which follows the framework
and most of the mechanisms of AFL, including the feedback-
driven coverage and crash-ﬁlter mechanisms. Based on these,
we developed a scheduling algorithm called AAPS and a
search strategy called SPEM. The state determination mech-
anism was added. EcoFuzz is based on the VAMAB model
to determine which state the seeds queue stays at. Moreover,
EcoFuzz runs without the deterministic strategies, while our
algorithm eliminated the mechanism in AFL that doubling
energy when a new path is found. Fig. 4 presents an overview
of EcoFuzz. Further details are given in Algorithm 1. The
three states of EcoFuzz are introduced below:
Initial State. EcoFuzz only stays at this state before
fuzzing. In this state, EcoFuzz chooses the ﬁrst seed to fuzz.
Then, EcoFuzz turns to the exploration or exploitation state.
Exploration State. In this state, EcoFuzz selects the next
seed based on the index order of the seeds which are not
fuzzed, without skipping the seeds that are not preferred, and
assigns energy by AAPS. If all seeds in the queue have been
fuzzed, EcoFuzz transfers into the exploitation state.
Exploitation State. In this state, as all seeds have been
fuzzed, EcoFuzz implements SPEM for estimating the reward
probability of all seeds and prioritizes the seeds with high
reward probability for testing. Each seed is selected at most
once until all seeds have been selected or a new path is found.
Figure 4: The overview of EcoFuzz, where the SPEM and
AAPS denote the search strategy and energy schedule we
propose in Section 4.2 and Section 4.3, respectively.
If all seeds have been selected in this state, EcoFuzz will re-
select the seeds until ﬁnding paths. After a new path is found,
EcoFuzz transfers from exploitation to exploration.
Algorithm 1 The algorithm of EcoFuzz
Require: Initial Seeds Set S
total_ f uzz = 0
rate = 1
Q = S
repeat
queued_path = |Q|
average_cost = CalculateCost(total_ f uzz, queued_path)
state = StateDetermine(Q)
if state == Exploitation then
s = ChooseNextBySPEM(Q)
else
s = ChooseNext(Q)
end if
Energy = AssignEnergy(s, state, rate, average_cost)
for i from 1 to Energy do
t = Mutate(s, Indeterministic)
total_ f uzz += 1
res = Execute(t)
if res == CRASH or IsInteresting(res) then
regret = i / Energy
s.last_ f ound += 1
if IsInteresting(res) then
add t to Q
add t to Tc
else
end if
end if
end for
rate = UpdateRate(regret, rate)
s.last_energy = Energy
until timeout reached or abort-signal
Ensure: Tc
Additionally, according to [11], we add a static analysis
module for extracting some magic bytes to a dictionary for
certain programs. In detail, the static analysis module extracts
some hardcode and magic bytes in the target binary by search-
ing from its disassembly information, which is efﬁcient and
uncomplicated.
2312    29th USENIX Security Symposium
USENIX Association
Initial SeedsState DetermineInitialSeedsQueue TExploitationExplorationtAssignEnergy As AAPSAssignEnergy By AAPSChooseNextAssignEnergy By AAPSChooseNext By SPEMinputMutateIsInteresting?YesAdd t into T4.2 Self-transition-based Probability Estima-
tion Method
In Section 3, we introduced the reward probability of each
seed and proved that it is not possible to determine the reward
probability accurately. Fortunately, our model aims to select
the seeds with high reward probability in the exploitation state.
Therefore, there is a greater focus on the magnitude relation-
ship but not on the speciﬁc value of the reward probability.
From Equation (5) (7), we can deduce that
PRi,n = pi∗ − n
∑
j=1, j(cid:54)=i
pi j
(9)
Considering a typical fuzzing process, as shown in Fig. 5,
Curve s represents the relationship p(e) between the number
of paths p and the number of total executions e when the CGF
is fuzzing a target. Further, Fig. 5 shows that the derivative of
p(e) decreases with an increase in the number of executions
e, meaning that the CGF found new paths more efﬁciently
in an early stage than a later stage. Particularly, the point
(0, p0) denotes the initial state of fuzzing and the point (e1, p1)
shows that the CGF found (p1 − p0) unique paths with the e1
executions. The average-cost of ﬁnding a path is deﬁned as
C(p1,e1, p0) =
e1