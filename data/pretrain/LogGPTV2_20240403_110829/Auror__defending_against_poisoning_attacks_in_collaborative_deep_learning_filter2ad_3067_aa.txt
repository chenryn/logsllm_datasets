title:Auror: defending against poisoning attacks in collaborative deep learning
systems
author:Shiqi Shen and
Shruti Tople and
Prateek Saxena
AUROR: Defending Against Poisoning Attacks in
Collaborative Deep Learning Systems
Shiqi Shen
Shruti Tople
Prateek Saxena
National University of Singapore
{shiqi04, shruti90, prateeks}@comp.nus.edu.sg
ABSTRACT
Deep learning in a collaborative setting is emerging as a corner-
stone of many upcoming applications, wherein untrusted users col-
laborate to generate more accurate models. From the security per-
spective, this opens collaborative deep learning to poisoning at-
tacks, wherein adversarial users deliberately alter their inputs to
mis-train the model. These attacks are known for machine learning
systems in general, but their impact on new deep learning systems
is not well-established.
We investigate the setting of indirect collaborative deep learning
— a form of practical deep learning wherein users submit masked
features rather than direct data. Indirect collaborative deep learn-
ing is preferred over direct, because it distributes the cost of com-
putation and can be made privacy-preserving. In this paper, we
study the susceptibility of collaborative deep learning systems to
adversarial poisoning attacks. Speciﬁcally, we obtain the follow-
ing empirical results on 2 popular datasets for handwritten images
(MNIST) and trafﬁc signs (GTSRB) used in auto-driving cars. For
collaborative deep learning systems, we demonstrate that the at-
tacks have 99% success rate for misclassifying speciﬁc target data
while poisoning only 10% of the entire training dataset.
As a defense, we propose AUROR, a system that detects mali-
cious users and generates an accurate model. The accuracy un-
der the deployed defense on practical datasets is nearly unchanged
when operating in the absence of attacks. The accuracy of a model
trained using AUROR drops by only 3% even when 30% of all the
users are adversarial. AUROR provides a strong guarantee against
evasion; if the attacker tries to evade, its attack effectiveness is
bounded.
1.
INTRODUCTION
Deep learning techniques have brought a paradigm shift in data
driven applications, from speech recognition (e.g., Apple’s Siri [4],
Google Now [7], Microsoft’s Cortana [6] and recently Facebook
M [3]), to image identiﬁcation (e.g., Google’s Photos [5], Face-
book’s Moments [2]). Several security applications like spam ﬁl-
tering [9], malware detection [23, 25] and others use neural net-
work algorithms as their back-bone. However, to attain reasonable
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ACSAC ’16, December 05-09, 2016, Los Angeles, CA, USA
c(cid:2) 2016 ACM. ISBN 978-1-4503-4771-6/16/12. . . $15.00
DOI: http://dx.doi.org/10.1145/2991079.2991125
accuracy in these systems, deep learning algorithms require exten-
sive training datasets. This constraint restricts the widespread use
of learning techniques in many applications. To address this limita-
tion, collaborative deep learning is emerging as a popular technique
to gather data from different sources and combine them to generate
larger datasets. For example, asking users to provide movie rat-
ings, tagging photos, crowd sourcing, marking emails as spam are
widely adopted ways to generate large and varied datasets [8, 11].
Direct collaborative learning, i.e., training directly on user pro-
vided data has two major concerns. First, submitting personal in-
formation to external parties exposes users to privacy risks since
they cannot control how their data is used after sharing. Even pas-
sive inference attacks are a known threat; for example, anonymized
movie ratings can reveal enough information to de-anonymize users
when combined with public data sources [31]. The second concern
is the vast amount of computation time (over few weeks) neces-
sary to train the learning algorithms on large datasets. Indirect col-
laborative learning addresses both these concerns [35, 39, 43]. In
this technique, each user computes partially on its data to generate
masked features. Instead of the original data, all the users submit
the masked features to the server. The server performs the remain-
ing computation of the learning algorithm on the masked features to
generate a global model, thereby guaranteeing each user’s data pri-
vacy and substantially reducing the computation costs on the cen-
tralized server.
Indirect collaborative learning, unlike direct collaboration offers
a weaker adversarial setting. The adversary in this setting can only
poison its own local data without observing the training data of
other users. Moreover, the poisoned data only inﬂuences the global
model indirectly via the masked features. Although, the training
process becomes privacy-preserving and cost-efﬁcient due to dis-
tributed computation, as we highlight, it remains susceptible to poi-
soning attacks. Malicious / adversarial users can poison or tamper
the training data to inﬂuence the behavior of the global model [42].
However, the severity of poisoning attacks in indirect collabora-
tive deep learning is not yet well-understood. Thus, understanding
whether such restricted poisoning via masked features can inﬂu-
ence the global model substantially is important. As our ﬁrst con-
tribution in the paper, we study the efﬁcacy of poisoning attacks to
varying levels of poisoning in state-of-the-art indirect collaborative
deep learning systems.
To understand the effect of these attacks, we test a state-of-the-
art indirect collaborative deep learning system which incorporates
differential privacy techniques [39]. We test using two popular
datasets and demonstrate targeted poisoning attacks that inﬂuence
the global model, misclassifying speciﬁc target data (e.g., digit 1
as 3). We select the well-known MNIST images for handwritten
characters as the ﬁrst dataset for this system [24]. Using the at-
508tack strategy of mislabeling dataset, we show that when 10% of all
the participants are malicious, the attack success rate is upto 99%.
Moreover, for a total of 10 output classes, the average accuracy of
the global model drops by 24% as compared to the model trained
with benign dataset when 30% of users are malicious. As our sec-
ond dataset, we select German trafﬁc sign images (GTSRB) that
has 43 output classes [40]. Our attack exhibits 79% success rate
for speciﬁc target and the accuracy drops by about 9% when 30%
of users are malicious. Our experiments show that the attacker’s
advantage increases with fewer output classes in the system.
Existing defenses against poisoning attack focus speciﬁcally on
non-deep learning algorithms and assume access to complete train-
ing datasets. Nelson et. al [32] and Mozaffari-Kermani et. al [29]
assume the existence of a golden model generated from a trusted
dataset and classify the training data as malicious or benign based
on its effect on the accuracy of this model. However, in our setting,
we do not have access to such a pre-deﬁned trusted model. Muhlen-
bach et. al [30] propose to identify malicious training data based on
the label of their neighboring data values. This method, however,
requires access to the entire training dataset beforehand. Hence,
previous defenses are insufﬁcient and cannot be re-purposed in a
straightforward manner to prevent poisoning attacks in indirect col-
laborative deep learning systems. This problem raises the question
— is it possible to defend against poisoning attacks without having
access to the complete training data?
In this paper, we propose a defense against poisoning attacks that
does not require the availability of entire training data. To this end,
we design AUROR— an automated defense that ﬁlters out mali-
cious users based only on their masked features. The key insight in
designing AUROR is that poisoning of training data strongly inﬂu-
ences the distribution of the masked features learnt by the system.
Since each masked feature corresponds to a different information in
the training data, the main challenge lies in identifying which set of
masked features are affected due to poisoning of the dataset. Thus,
AUROR involves two key steps a) identifying relevant masked fea-
tures corresponding to the attack strategy and b) detecting mali-
cious users based on the anomalous distribution of the masked fea-
tures. Our solution provides a strong accuracy guarantee against
arbitrary poisoning of data by all the colluding malicious users.
We implement our defense and evaluate its effectiveness on real
datasets for deep learning systems. We employ AUROR to iden-
tify malicious users in our attack set up. The detection rate for
identifying malicious users in both MNIST and GTSRB datasets
is 100% for a fraction of malicious users between 10% to 30%.
We measure the accuracy drop in the trained model after removing
the malicious users detected using AUROR. The trained model ex-
hibits a small accuracy drop ranging from 0% to 3% as compared
to benign setting for both the datasets. Thus, AUROR trained model
provides accuracy guarantees comparable to benign datasets. Our
experiments demonstrate that even an optimal adversarial strategy
cannot do any better in degrading this guarantee.
Contributions. We make the following contributions in this paper:
• Measuring Sensitivity of Poisoning Attacks. We empiri-
cally demonstrate that poisoning attacks are a serious threat
to indirect collaborative deep learning systems regardless of
masking the essential features of the training data and re-
stricted poisoning capacity of the adversary.
• AUROR. We introduce a statistical mechanism called AUROR
as a countermeasure to poisoning attacks in indirect collabo-
rative deep learning systems. AUROR automates the process
of identifying masked features exhibiting abnormal distribu-
tion and detects malicious users in the system based on these
features.
(cid:4)(cid:8)(cid:13)(cid:17)(cid:8)(cid:13)
(cid:2)(cid:11)(cid:14)(cid:5)(cid:4)(cid:11)(cid:1)(cid:12)(cid:14)(cid:7)(cid:8)(cid:11)
(cid:11)(cid:6)(cid:14)(cid:10)(cid:8)(cid:7)(cid:1)(cid:9)(cid:8)(cid:6)(cid:15)(cid:16)(cid:13)(cid:8)(cid:14)(cid:1)
(cid:3)(cid:14)(cid:6)(cid:4)(cid:11)(cid:1)(cid:12)(cid:14)(cid:7)(cid:8)(cid:11)
(cid:3)(cid:14)(cid:6)(cid:4)(cid:11)(cid:1)(cid:12)(cid:14)(cid:7)(cid:8)(cid:11)
(cid:3)(cid:14)(cid:6)(cid:4)(cid:11)(cid:1)(cid:12)(cid:14)(cid:7)(cid:8)(cid:11)
(cid:3)(cid:14)(cid:6)(cid:4)(cid:11)(cid:1)(cid:16)(cid:15)(cid:4)(cid:10)(cid:13)(cid:10)(cid:13)(cid:9)(cid:1)(cid:7)(cid:4)(cid:16)(cid:4)
(cid:3)(cid:14)(cid:6)(cid:4)(cid:11)(cid:1)(cid:16)(cid:15)(cid:4)(cid:10)(cid:13)(cid:10)(cid:13)(cid:9)(cid:1)(cid:7)(cid:4)(cid:16)(cid:4) (cid:18)
(cid:3)(cid:14)(cid:6)(cid:4)(cid:11)(cid:1)(cid:16)(cid:15)(cid:4)(cid:10)(cid:13)(cid:10)(cid:13)(cid:9)(cid:1)(cid:7)(cid:4)(cid:16)(cid:4)
(cid:5)(cid:14)(cid:8)(cid:13)(cid:1)(cid:2)
(cid:5)(cid:14)(cid:8)(cid:13)(cid:1)(cid:3)
(cid:5)(cid:14)(cid:8)(cid:13)(cid:1)(cid:12)
Figure 1: The indirect collaborative learning setting. The
users compute a local model and submit masked features to
the server. The server computes a global model based on all the
abstract features.
• Evaluation. Our empirical evaluation validates that AUROR
can identify malicious users with almost 100% detection rate,
while limiting accuracy drop to under 3% for the ﬁnal model
even when 30% users are malicious. AUROR thus enables
accurate and robust indirect collaborative learning systems.
2. OVERVIEW
Our goal is to understand the impact of targeted poisoning on
deep learning systems and investigate a practical defense that is
resistant to evasion.
2.1 Problem Setting
Several enterprises employ collaborative learning to understand
their customer’s behavior towards their products [8, 26]. They col-
lect data from the users in the form of reviews, feedback or other
attributes. In this work, we refer to the entity that collects the data
and processes learning algorithms over them as server. The server
does not necessarily need to be a centralized entity. The tasks of the
server can be performed in a distributed manner among the partic-
ipants. The participants or customers that submit their data to the
server are referred as users. The users can be individuals, groups or
companies with their own dataset. A user does not know or learn
any direct information about the training data of other users.
In this paper, we speciﬁcally examine the indirect collaborative
learning setting (as shown in Figure 1). This setting differs from
the direct collaborative learning in the way in which users upload
their data. Instead of submitting the raw data directly to the server,
users mask some information about their data and send it to the
server. This saves both the bandwidth (data costs) and yields better
privacy which is essential for practical adoption of any new tech-
nique. We refer to the masked information as masked features. The
users compute a local model on their machines that generates the
masked features. The server collects these masked features from all
the users and performs operations (for e.g., summation) to gener-
ate a global trained model. This global model captures the features
from the entire dataset, thus has a very high accuracy. Lastly, we
use the global model to perform the classiﬁcation of the test dataset.
We select the privacy preserving deep learning (PPDL) system
by Shokri et. al [39] as the classiﬁcation system. The PPDL sys-
tem performs image recognitions by indirect collaborative learning
setting as shown in Figure 1. This system uses differential pri-
vacy techniques to mask the features before submitting them to
509the server. PPDL uses a distributed selective stochastic gradient
descent (SSGD) technique to distribute the computation between
the local and global model. Users of the system compute gradi-
ents from the input data and upload them as features of the data.
Section 3.2 gives a detailed explanation about their system and its
working.
2.2 Threat Model
In the above discussed indirect collaborative learning setting, we
consider that a constant fraction (f) of the users are malicious.
These malicious users are incentivized to modify or poison the
training dataset to affect the accuracy of the global model. For
example, spammers can mark genuine emails as spams, thereby re-
ducing the accuracy of the learnt spam ﬁlter. Such a spam ﬁlter
will then assign emails with genuine content as spam resulting in
reduced credibility of the ﬁlter among its users. In this model, a
majority of the users are honest while a small fraction f (such that
f < n/2) of the users are malicious. This is a rational setting, for
example, consider the case where a product company bribes users
to give higher ratings to their products on an e-commerce web-
site [1,10]. Although, some of the users are compromised, the total
number of participants is much higher, resulting in benign users
forming a majority. We consider that all the adversaries know the
learning algorithm used by a particular application and are able to
tamper with the actual data accordingly (e.g., either by adding fake
data or mislabeling the training data). Moreover, all the malicious
users can collude together to poison their training data. However,
the adversary cannot learn the global model beforehand as the ma-
licious users have no knowledge about the training data generated
from the benign users.
The server in our model is honest-but-curious i.e., it passively
observes the data gathered from the users to learn information about
it. Our model directly inherits privacy guarantees from the previous
work [39]. The server cannot infer information about the original
data as all the users submit masked features. Our attack does not
break the privacy guarantees of the previous system but only inﬂu-
ences the accuracy of the ﬁnal global model.
2.3 Problem Deﬁnition
Targeted Poisoning Attacks. In non-deep learning systems, sev-
eral known adversarial learning techniques reduce the accuracy of
the global model. For example, attackers can modify the test data to
avoid the detection of malicious samples on a well-trained model [13,
16, 21, 33, 34, 41, 46] or alter the training data to affect the model
accuracy during the learning phase [17, 36, 42]. In this paper, we
speciﬁcally focus on the latter problem known as causative or poi-
soning attacks caused by tampering with the training data [14, 22,
27] on deep learning systems. Depending on the attacker’s mo-
tive, poisoning attacks can be of two kinds a) random attacks and
b) targeted attacks [22]. Random attacks are perpetrated with the
intention to reduce the accuracy of the model, whereas targeted at-
tacks are performed with a more focused objective. Speciﬁcally, in
targeted attack, the attacker’s goal is, for a given source data, the
model M outputs target data of the attacker’s choice. For example,
an adversary can poison the training data to classify a source data
(e.g., a spam email) as a target data (e.g., a genuine email).
Deﬁnition 2.1. (Source Data) Source data is any input value to the
global model for which the attacker wants to inﬂuence the model’s
output.
Deﬁnition 2.2. (Target Data) Target data is a value of attacker’s
choice that the inﬂuenced model should output for a given source
input.
The attacker arbitrarily chooses the source and target data values
ﬁrst and then poisons its training data known as the poison set. The
poisoning strategy depends on the underlying learning algorithm
used by the system. The ﬁnal poisoned model MP represents the
features from the entire training data provided by malicious as well
as benign users. We deﬁne the success rate of attack as follows:
Deﬁnition 2.3. (Attack Success Rate) The attack succeeds if the
poisoned model outputs the desired target value T for a source
input I, MP (I) → T and sets SI = 1 otherwise the attack fails
with SI = 0 where SI is used to indicate whether the target attack
succeeds. The success rate (SR) of the attack for the model MP is
given as:
SRMP =
(ΣI∈DSI )
|D|
× 100
where D is the domain of all possible source inputs.