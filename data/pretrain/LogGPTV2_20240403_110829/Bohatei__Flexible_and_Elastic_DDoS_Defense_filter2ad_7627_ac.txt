put trafﬁc to the graph (i.e., to A1) is expected to traverse
edge A2 → R2.
Since modules may vary in terms of compute com-
plexity and the trafﬁc rate that can be handled per VM-
slot, we need to account for the parameter Pa,i that is
the trafﬁc processing capacity of a VM (e.g., in terms of
compute requirements) for the logical module va,i, where
va,i is node i of graph DAGannotated
Network footprint: We denote the network-level cost
of transferring the unit of trafﬁc from ingress e to data-
center d by Le,d; e.g., this can represent the path latency
per byte of trafﬁc. Similarly, within a datacenter, the
units of intra-rack and inter-rack trafﬁc costs are denoted
by IntraUnitCost and InterUnitCost, respectively (e.g.,
they may represent latency such that IntraUnitCost <
InterUnitCost).
4.2 Problem statement
Our resource management problem is to translate the an-
notated graph into a physical graph (see Figure 4); i.e.,
each node i of the annotated graph DAGannotated
will be
realized by one or more VMs each of which implement
the logical module va,i.
Fine-grained scaling:
To generate physical graphs
given annotated graphs in a resource-efﬁcient manner,
we adopt a ﬁne-grained scaling approach, where each
logical module is scaled independently. We illustrate this
idea in Figure 5. Figure 5a shows an annotated graph
with three logical modules A, B, and C, receiving differ-
ent amounts of trafﬁc and consuming different amounts
of compute resources. Once implemented as a physical
graph, suppose module C becomes the bottleneck due to
its processing capacity and input trafﬁc volume. Using
a monolithic approach (e.g., running A, B, and C within
a single VM), we will need to scale the entire graph as
shown in Figure 5b. Instead, we decouple the modules
to enable scaling out individual VMs; this yields higher
resource efﬁciency as shown in Figure 5c.
Goals: Our objective here is to (a) instantiate the VMs
across the compute servers throughout the ISP, and (b)
a
(cid:3)(cid:14)(cid:10)(cid:10)(cid:5)(cid:9)(cid:12)(cid:8)(cid:16)(cid:1)(cid:10)(cid:14)(cid:9)(cid:9)(cid:7)(cid:9)(cid:6)(cid:1)
(cid:9)(cid:5)(cid:15)(cid:8)(cid:16)(cid:1)(cid:7)(cid:9)(cid:11)(cid:12)(cid:2)(cid:9)(cid:13)(cid:2)(cid:12)(cid:5)(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:8)(cid:7)(cid:10)(cid:1)
(cid:2)(cid:1)
(cid:8)(cid:7)(cid:11)(cid:1)
(cid:4)(cid:1)
(cid:6)(cid:5)(cid:2)(cid:4)(cid:3)(cid:1)
(cid:9)(cid:1)
(cid:11)(cid:10)(cid:2)(cid:6)(cid:3)(cid:1)(cid:1)
(cid:14)(cid:9)(cid:4)(cid:5)(cid:10)(cid:14)(cid:12)(cid:8)(cid:7)(cid:15)(cid:5)(cid:4)(cid:1)
(cid:2)(cid:1)
(cid:2)(cid:1)
(cid:3)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:4)(cid:1)
(cid:6)(cid:5)(cid:2)(cid:4)(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:3)(cid:1)
(cid:4)(cid:1)
(cid:4)(cid:1)
(a) Annotated graph.
(c) Fine-grained.
Figure 5: An illustration of ﬁne-grained elastic scal-
ing when module C becomes the bottleneck.
(b) Monolithic.
distribute the processing load across these servers to min-
imize the expected latency for legitimate trafﬁc. Further,
we want to achieve (a) and (b) while minimizing the foot-
print of suspicious trafﬁc.6
To this end, we need to assign values to two key sets
of decision variables: (1) the fraction of trafﬁc Te,a to
send to each datacenter Dd (denoted by fe,a,d), and (2)
the number of VMs of type va,i to run on server s of dat-
acenter Dd. Naturally, these decisions must respect the
datacenters’ bandwidth and compute constraints.
Theoretically, we can formulate this resource manage-
ment problem as a constrained optimization via an In-
teger Linear Program (ILP). For completeness, we de-
scribe the full ILP in Appendix A. Solving the ILP for-
mulation gives an optimal solution to the resource man-
agement problem. However, if the ILP-based solution is
incorporated into Bohatei, an adversary can easily over-
whelm the system. This is because the ILP approach
takes several hours (see Table 2). By the time it computes
a solution, the adversary may have radically changed the
attack mix.
4.3 Hierarchical decomposition
To solve the resource management problem, we decom-
pose the optimization problem into two subproblems: (1)
the Bohatei global controller solves a Datacenter Selec-
tion Problem (DSP) to choose datacenters responsible for
processing suspicious trafﬁc, and (2) given the solution
to the DSP, each local controller solves a Server Selec-
tion Problem (SSP) to assign servers inside each selected
datacenter to run the required VMs. This decomposition
is naturally scalable as the individual SSP problems can
be solved independently by datacenter controllers. Next,
we describe practical greedy heuristics for the DSP and
SSP problems that yield close-to-optimal solutions (see
Table 2).
Datacenter selection problem (DSP): We design a
greedy algorithm to solve DSP with the goal of reduc-
ing ISP-wide suspicious trafﬁc footprint. To this end,
the algorithm ﬁrst sorts suspicious trafﬁc volumes (i.e.,
6While it
is possible to explicitly minimize network conges-
tion [33], minimizing suspicious trafﬁc footprint naturally helps reduce
network congestion as well.
822  24th USENIX Security Symposium 
USENIX Association
Te,a values) in a decreasing order. Then, for each sus-
picious trafﬁc volume Te,a from the sorted list, the algo-
rithm tries to assign the trafﬁc volume to the datacenter
with the least cost based on Le,d values. The algorithm
has two outputs: (1) fe,a,d values denoting what fraction
of suspicious trafﬁc from each ingress should be steered
to each datacenter (as we will see in §5, these values will
be used by network orchestration to steer trafﬁc corre-
spondingly), (2) the physical graph corresponding to at-
tack type a to be deployed by datacenter d. For complete-
ness, we show the pseudocode for the DSP algorithm in
Figure 16 in Appendix B.
Server selection problem (SSP):
Intuitively, the SSP
algorithm attempts to preserve trafﬁc locality by instan-
tiating nodes adjacent in the physical graph as close as
possible within the datacenter. Speciﬁcally, given the
physical graph, the SSP algorithm greedily tries to assign
nodes with higher capacities (based on Pa,i values) along
with its predecessors to the same server, or the same rack.
For completeness we show the pseudocode for the SSP
algorithm in Figure 17 in Appendix B.
5 Network Orchestration
Given the outputs of the resource manager module (i.e.,
assignment of datacenters to incoming suspicious traf-
ﬁc and assignment of servers to defense VMs), the role
of the network orchestration module is to conﬁgure the
network to implement these decisions. This includes set-
ting up forwarding rules in the ISP backbone and inside
the datacenters. The main requirement is scalability in
the presence of attack trafﬁc. In this section, we present
our tag-based and proactive forwarding approach to ad-
dress the limitations of the per-ﬂow and reactive SDN
approach.
5.1 High-level idea
As discussed earlier in §3.2, the canonical SDN view of
setting up switch forwarding rules in a per-ﬂow and re-
active manner is not suitable in the presence of DDoS
attacks. Furthermore, there are practical scalability and
deployability concerns with using SDN in ISP back-
bones [21,29]. There are two main ideas in our approach
to address these limitations:
• Following the hierarchical decomposition in re-
source management, we also decompose the net-
work orchestration problem into two-sub-problems:
(1) Wide-area routing to get trafﬁc to datacenters,
and (2) Intra-datacenter routing to get trafﬁc to the
right VM instances. This decomposition allows us
to use different network-layer techniques; e.g., SDN
is more suitable inside the datacenter while tradi-
tional MPLS-style routing is better suited for wide-
area routing.
• Instead of the controller reacting to each ﬂow arrival,
we proactively install forwarding rules before trafﬁc
arrives. Since we do not know the speciﬁc IP-level
suspicious ﬂows that will arrive in the future, we use
logical tag-based forwarding rules with per-VM tags
instead of per-ﬂow rules.
5.2 Wide-area orchestration
The Bohatei global controller sets up forwarding rules
on backbone routers so that trafﬁc detected as suspicious
is steered from edge PoPs to datacenters according to
the resource management decisions speciﬁed by the fe,a,d
values (see §4.3).7
To avoid a forklift upgrade of the ISP backbone and
enable an immediate adoption of Bohatei, we use tra-
ditional tunneling mechanisms in the backbone (e.g.,
MPLS or IP tunneling). We proactively set up static
tunnels from each edge PoP to each datacenter. Once
the global controller has solved the DSP problem, the
controller conﬁgures backbone routers to split the traf-
ﬁc according to the fe,a,d values. While our design is
not tied to any speciﬁc tunneling scheme, the widespread
use of MPLS and IP tunneling make them natural candi-
dates [34].
5.3
Inside each datacenter, the trafﬁc needs to be steered
through the intended sequence of VMs. There are two
main considerations here:
1. The next VM a packet needs to be sent to depends on
the context of the current VM. For example, the node
check UDP count of src in the graph shown in Fig-
ure 3 may send trafﬁc to either forward to customer
or log depending on its analysis outcome.
Intra-datacenter orchestration
2. With elastic scaling, we may instantiate several phys-
ical VMs for each logical node depending on the de-
mand. Conceptually, we need a “load balancer” at
every level of our annotated graph to distribute traf-
ﬁc across different VM instances of a given logical
node.
Note that we can trivially address both requirements
using a per-ﬂow and reactive solution. Speciﬁcally, a lo-
cal controller can track a packet as it traverses the phys-
ical graph, obtain the relevant context information from
each VM, and determine the next VM to route the traf-
ﬁc to. However, this approach is clearly not scalable and
can introduce avenues for new attacks. The challenge
here is to meet these requirements without incurring the
overhead of this per-ﬂow and reactive approach.
Encoding processing context:
Instead of having the
controller track the context, our high-level idea is to en-
7We assume the ISP uses legacy mechanisms for forwarding non-
attack trafﬁc and trafﬁc to non-Bohatei customers, so these are not the
focus of our work.
USENIX Association  
24th USENIX Security Symposium  823
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:27)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:25)(cid:25)(cid:25)(cid:1)
(cid:24)
(cid:1)
(cid:2)(cid:26)(cid:23)(cid:26)(cid:1)
(cid:18)(cid:17)(cid:19)(cid:20)(cid:1)(cid:26)(cid:1)
(cid:7)(cid:8)(cid:1)
(cid:2)(cid:27)(cid:23)(cid:26)(cid:1)
(cid:6)(cid:26)(cid:23)(cid:26)(cid:1)
(cid:5)(cid:8)(cid:1)(cid:3)(cid:18)(cid:19)(cid:23)(cid:9)(cid:19)(cid:11)(cid:15)(cid:17)(cid:13)(cid:1)(cid:6)(cid:9)(cid:10)(cid:16)(cid:12)(cid:1)
(cid:10)(cid:13)(cid:12)(cid:3)(cid:10)(cid:11)(cid:12)(cid:1)
(cid:2)(cid:8)(cid:10)(cid:14)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:27)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:27)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:27)(cid:1)
(cid:24)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:25)(cid:25)(cid:25)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:28)(cid:1)
(cid:7)(cid:4)(cid:1)(cid:2)(cid:25)(cid:24)(cid:25)(cid:1)(cid:6)(cid:9)(cid:13)(cid:1)(cid:6)(cid:9)(cid:10)(cid:16)(cid:12)(cid:1)
(cid:4)(cid:5)(cid:7)(cid:1)
(cid:1)(cid:10)(cid:9)(cid:12)(cid:6)(cid:15)(cid:12)(cid:1)
(cid:26)(cid:1)
(cid:3)(cid:11)(cid:16)(cid:13)(cid:12)(cid:16)(cid:1)
(cid:2)(cid:21)(cid:9)(cid:10)(cid:14)(cid:1)
(cid:27)(cid:1)
(cid:5)(cid:8)(cid:1)(cid:3)(cid:18)(cid:19)(cid:23)(cid:9)(cid:19)(cid:11)(cid:15)(cid:17)(cid:13)(cid:1)(cid:6)(cid:9)(cid:10)(cid:16)(cid:12)
(cid:4)(cid:5)(cid:7)(cid:1) (cid:10)(cid:13)(cid:12)(cid:3)(cid:10)(cid:11)(cid:12)(cid:1)
(cid:26)(cid:1)
(cid:27)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:27)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:28)(cid:1)
(cid:22)(cid:20)(cid:15)(cid:17)(cid:13)(cid:1)(cid:21)(cid:9)(cid:13)(cid:13)(cid:15)(cid:17)(cid:13)(cid:1)
(cid:23)(cid:15)(cid:21)(cid:14)(cid:18)(cid:22)(cid:21)(cid:1)(cid:21)(cid:9)(cid:13)(cid:13)(cid:15)(cid:17)(cid:13)(cid:1)
Figure 6: Context-dependent forwarding using tags.
code the necessary context as tags inside packet head-
ers [31]. Consider the example shown in Figure 6 com-
posed of VMs A1,1, A2,1, and R1,1. A1,1 encodes the pro-
cessing context of outgoing trafﬁc as tag values embed-
ded in its outgoing packets (i.e., tag values 1 and 2 denote
benign and attack trafﬁc, respectively). The switch then
uses this tag value to forward each packet to the correct
next VM.
Tag-based forwarding addresses the control channel
bottleneck and switch rule explosion. First, the tag gen-
eration and tag-based forwarding behavior of each VM
and switch is conﬁgured proactively once the local con-
troller has solved the SSP. We proactively assign a tag
for each VM and populate forwarding rules before ﬂows
arrive; e.g., in Figure 6, the tag table of A1,1 and the for-
warding table of the router have been already populated
as shown. Second, this reduces router forwarding rules
as illustrated in Figure 6. Without tagging, there will be
one rule for each of the 1000 ﬂows. Using tag-based for-
warding, we achieve the same forwarding behavior using
only two forwarding rules.
Scale-out load balancing: One could interconnect VMs
of the same physical graph as shown in Figure 7a us-
ing a dedicated load balancer (load balancer). However,
such a load balancer may itself become a bottleneck, as
it is on the path of every packet from any VM in the set
{A1,1,A1,2} to any VM in the set {R1,1,R1,2, ,R 1,3}. To
circumvent this problem, we implement the distribution
strategy inside each VM so that the load balancer capa-
bility scales proportional to the current number of VMs.
Consider the example shown in Figure 7b where due to
an increase in attack trafﬁc volume we have added one
more VM of type A1 (denoted by A1,2) and one more
VM of type R1 (denoted by R1,2). To load balance trafﬁc
between the two VMs of type R1, the load balancer of
A1 VMs (shown as LB1,1 and LB1,2 in the ﬁgure) pick a
tag value from a tag pool (shown by {2,3} in the ﬁgure)
based on the processing context of the outgoing packet
and the intended load balancing scheme (e.g., uniformly
at random to distribute load equally). Note that this tag
pool is pre-populated by the local controller (given the
defense library and the output of the resource manager
(cid:2)(cid:32)(cid:29)(cid:32)(cid:1)
(cid:5)(cid:3)(cid:32)(cid:29)(cid:32)(cid:1)