thus masking the reality of a near-meltdown in which
temperatures reached over 4000 degrees.
(cid:127) Medical care. The most often cited example of a
software failure in a medical device is the Therac 25
accelerator, which resulted in several deaths as a re-
sult of a race condition in a nonatomic transaction of
switching from the high-intensity research mode to
the low-intensity therapeutic mode [10]. A physical
interlock had been present in hardware in the Therac
20, and was mistakenly assumed to have been imple-
mented in software in the Therac 25. At the other end
of the technology spectrum was the heart-monitoring
device with a standard electrical-socket wall plug in-
stead of a jack, which had come loose; a cleaning
person instinctively plugged it into the wall socket
rather than into the monitor, thereby electrocuting
the patient. Recent reports show new cases of op-
erations on the wrong patient because of mistaken or
misinterpreted computer data, erroneous test results,
a mode-change fault in a glucose-monitoring device,
and so on.
For each of these application areas (and many more),
the safety risks of untrustworthy systems are considerable.
In addition, risks tend to arise in supposedly safe systems
with respect to security, privacy, reliability, system sur-
vivability, graceful degradation, and so on. Overall, much
greater care is needed in developing safe systems than is
devoted to run-of-the-mill software.
(For example, see
work by Leveson [7, 8, 9] for some serious approaches
to enhancing safety that could be an inspiration to R&D
in trustworthiness for secure applications.)
Safety-related accidents continue to occur, particularly
in air, rail, and medical applications — e.g., caused
by hardware/software malfunctions and errors by con-
trollers, pilots, and operators. In hindsight, some of those
should have been preventable with better human inter-
faces, cross-checking, adequate stafﬁng, preventive di-
agnostics and maintenance, training, pervasive oversight,
and so on.
6 Unsecure Systems
We next consider problems of unsecure systems, with the
hopes of gaining some insights from the previous sec-
tions. For ACSAC, documenting historical and recent se-
curity vulnerabilities and their exploitations might seem
to be preaching to the ACSACramental choir, and might
cause me to be ACSACked or ACSACriﬁced for rampant
repetitiousness. Of course, typical risks include penetra-
tions by outsiders and misuse by insiders, e-mailstorms
(e-maelstroms?) of spam and phishing attacks, and iso-
lated and coordinated distributed denials of service, to
name just a few. Vulnerable applications include a wide
array of ﬁnancial systems with potentials for fraud and
undetected errors, databases with rampant opportunities
for identity thefts and other privacy violations, all of the
critical national infrastructures, electronic voting systems
with serious needs for system integrity and voter privacy
as well as detection and prevention of manipulations and
errors, and many other types of systems.
Buffer overﬂows, bounds checks, type mismatches, and
many other program ﬂaws continue to appear, frequently
causing security failures. There have been numerous ef-
forts to provide a taxonomy for such problems (as for
example [1, 6, 18, 22]), as well as efforts such as static
analysis tools to detect the presence of the characteristic
ﬂaws. However, disciplined software development and
systematic use of analysis tools are required. There are
enormous needs for well-designed and well-implemented
trustworthy systems that can satisfy a broad set of security
requirements. Curiously, for many years, system integrity
tended to be subordinated to conﬁdentiality, whereas ac-
countability remained more or less in the dark; preventing
denials of service is often still widely ignored.
From a total-system perspective, it would be highly de-
sirable that systems designed for security also be able to
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006satisfy some of the other requirements for trustworthi-
ness that transcend security per se. For example, sys-
tems that are supposedly secure but unreliable may no
longer be secure when unreliable. Similarly, supposedly
secure systems that are not predictably survivable under
certain environmental disruptions may become unsecure
when transformed into fail-safe or other degraded oper-
ation. The concept of fail-secure systems presents some
signiﬁcant challenges.
7 Conclusions
Considering the broad scope of the problems considered
here, including huge diversities among causes and effects,
it should not be surprising that rather far-reaching mea-
sures are needed to prevent or even detect the emerg-
ing likelihood of risks relating to untrustworthiness. For
starters, prevention and remediation of the risks must en-
compass better understanding of the full range of require-
ments, as well as better system architectures explicitly ad-
dressing those trustworthiness requirements with appro-
priate assurance, usability, operation, and maintainabil-
ity. Greater attention needs to be devoted to the software
engineering disciplines for implementing trustworthy ap-
plications, either based on underlying trustworthy infras-
tructures such as secure operating systems and network-
ing, or alternatively able to architecturally surmount some
untrustworthiness in subsystems.
(Twenty-two paradig-
matic approaches to building trustworthy systems out of
less trustworthy components are examined in Section 3.5
of [14]. However, beware of compromises resulting from
coordinated attacks or accidental misbehavior originat-
ing inside the implementations or in underlying layers of
abstraction.) In addition, these concepts must be incul-
cated into the practice of developers and operational staff
through enlightened education and training. Also valuable
would be a stronger sense of corporate altruism and per-
haps some intelligent government action. (However, with
regard to legislation and regulation, be very careful what
you ask for; you might get it, or — perhaps more likely
— a badly distorted version of it).
Above all, trustworthiness demands a pervasive sense
of systems in their entirety, considering the long-term
risks in the global context of all relevant applications rel-
ative to the totality of all relevant requirements. The pre-
ceding four sections illustrate four types of systems in
which greater trustworthiness is urgently needed: surviv-
able backup systems, robust networking, safe systems,
and secure total systems (such as networked operating
systems together with all their networked applications).
In each of these and many other application areas, the
potentials for untrustworthiness must be considered with
respect to the environments in which those systems op-
erate. The desired trustworthiness properties are mostly
emergent properties of the entire system, rather than iso-
lated properties of subsystems. Nevertheless, a huge step
forward in avoiding or circumventing untrustworthiness
would result if the emergent properties of application sys-
tems as a whole could be systematically derived or other-
wise inferred from the composable properties of the sub-
systems, and so on iteratively into lower layers of ab-
straction. For example, see the 1977 Robinson–Levitt pa-
per [19] and its application to the Provably Secure System
design [3, 16, 17], and Neumann’s report on predictable
composability [14].
Life-critical systems and supposedly secure systems
should of course be held to higher standards than conven-
tional software, although criteria and evaluations tend to
be not very rigorous. As one rather sad example, today’s
electronic voting systems (e.g., [11, 21]) are held to much
weaker standards than gambling machines!
Myopia is very dangerous with respect to trustworthi-
ness. The commonalities among the different applica-
tion areas considered here are likely to transcend would-
be single-discipline solutions. Thus, a massive culture
shift is needed to proactively develop and composition-
ally evaluate systems in their entirety and to assure their
operational conﬁgurations and usability. This is of course
especially important for applications with critical require-
ments for trustworthiness. The pleas for such approaches
in many Classic Papers are old but nevertheless still
timely.
Acknowledgments
The author thanks Douglas Maughan, who sponsored
reference [14] when he was a Program Manager in the
Defense Advanced Research Projects Agency (DARPA).
Many of the concepts discussed there on how to develop
composable high-assurance trustworthy systems and net-
works are relevant to avoiding risks such as those dis-
cussed here. This paper was prepared in part under Na-
tional Science Foundation Grant Number 0524111.
References
[1] R.P. Abbott et al. Security analysis and enhance-
ments of computer operating systems. Technical re-
port, National Bureau of Standards, 1974. Order No.
S-413558-74.
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006[2] K. Borg.
Re:
LA power outages.
22 August
ACM
2006.
Forum,
Risks
http://catless.ncl.ac.uk/Risks/24.39.html#subj8.
24(39),
[3] R.J. Feiertag and P.G. Neumann.
The foun-
dations of a Provably Secure Operating System
(PSOS). In Proceedings of the National Computer
Conference, pages 329–334. AFIPS Press, 1979.
http://www.csl.sri.com/neumann/psos.pdf.
[4] D. Ford. Three Mile Island: Thirty Minutes to Melt-
down. Viking Press, 1982. Sensor-related quote re-
produced in ACM SIGSOFT Software Engineering
Notes, 11, 3, 9–10, July 1986.
[5] J. Garman. The bug heard ’round the world. ACM
SIGSOFT Software Engineering Notes, 6(5):3–10,
October 1981.
[6] C.E. Landwehr, A.R. Bull, J.P. McDermott, and
W.S. Choi. A taxonomy of computer program se-
curity ﬂaws, with examples. Technical report, Cen-
ter for Secure Information Technology, Information
Technology Division, Naval Research Laboratory,
Washington, D.C., November 1993.
[7] N.G. Leveson. Safeware: System Safety and Com-
puters. Addison-Wesley, Reading, Massachusetts,
1995.
[8] N.G. Leveson. A new accident model for engi-
neering safer systems. Safety Science (Elsevier),
42(4):237–270, April 2004.
[9] N.G. Leveson. A systems-theoretic approach to
safety in software-intensive systems.
IEEE Trans.
on Dependable and Secure Computing, 1(1), Jan-
uary 2005.
[10] N.G. Leveson and C. Turner. An investigation of the
Therac-25 accidents. Computer, pages 18–41, July
1993.
[11] R. Mercuri.
Electronic Vote Tabulation Checks
and Balances. PhD thesis, Department of Com-
puter Science, University of Pennsylvania, 2001.
http://www.notablesoftware.com/evote.html.
[12] P.G. Neumann.
index to RISKS cases.
Illustrative risks to the public
in the use of computer systems and related tech-
nology,
Technical re-
port, Computer Science Laboratory, SRI Interna-
tional, Menlo Park, California. Updated regularly
at http://www.csl.sri.com/neumann/illustrative.html;
also in .ps and .pdf form for printing in a denser for-
mat.
[13] P.G. Neumann. Computer-Related Risks. ACM
Press, New York, and Addison-Wesley, Reading,
Massachusetts, 1995.
[14] P.G. Neumann.
Principled assuredly trustwor-
thy composable architectures.
re-
port, Computer Science Laboratory, SRI Interna-
tional, Menlo Park, California, December 2004.
http://www.csl.sri.com/neumann/chats4.html,
.pdf,
and .ps.
Technical
[15] P.G. Neumann. System and network trustworthi-
ness in perspective. In Proceedings of the Thirteenth
ACM Conference on Computer and Communica-
tions Security (CCS), Alexandria, Virginia, Novem-
ber 2006.
[16] P.G. Neumann, R.S. Boyer, R.J. Feiertag, K.N.
Levitt, and L. Robinson. A Provably Secure Op-
erating System: The system, its applications, and
proofs. Technical report, Computer Science Labora-
tory, SRI International, Menlo Park, California, May
1980. 2nd edition, Report CSL-116.
[17] P.G. Neumann and R.J. Feiertag.
In Proceedings of
PSOS re-
visited.
the 19th Annual
Computer Security Applications Conference (AC-
SAC 2003), Classic Papers section, pages 208–
216, Las Vegas, Nevada, December 2003. IEEE
Computer Society.
http://www.acsac.org/ and
http://www.csl.sri.com/neumann/psos03.pdf.
[18] P.G. Neumann and D.B. Parker. A summary of
In Proceedings of
computer misuse techniques.
the Twelfth National Computer Security Conference,
pages 396–407, Baltimore, Maryland, 10–13 Octo-
ber 1989. NIST/NCSC.
[19] L. Robinson and K.N. Levitt.
Proof techniques
for hierarchically structured programs. Communi-
cations of the ACM, 20(4):271–283, April 1977.
[20] E. Rosen. Vulnerabilities of network control proto-
cols. ACM SIGSOFT Software Engineering Notes,
6(1):6–8, January 1981.
[21] A. Rubin. Brave New Ballot. Random House, 2006.
[22] K. Tsikpenyuk, B. Chess, and G. McGraw. Seven
pernicious kingdoms: A taxonomy of software se-
curity errors.
IEEE Security and Privacy, 3(6),
November-December 2005.
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006