• RQ 1. Are current obfuscation techniques efﬁcient
against automated bug-ﬁnding via fuzzing?
• RQ 2. Are the techniques we designed effective at dis-
rupting the targeted fuzzing assumptions?
• RQ 3. Are the techniques effective at preventing fuzzers
from ﬁnding bugs?
• RQ 4. Are the techniques effective at reducing the
amount of code that is being tested?
• RQ 5. Do our techniques introduce any signiﬁcant per-
formance overhead?
To answer the ﬁrst research question RQ 1., we demon-
strate that modifying a custom dummy application (which
is illustrated in Listing 2) using the state-of-the-art obfusca-
tion tool TIGRESS [15] does not yield a satisfying level of
protection against current fuzzers.
Following the answer to RQ 1., we test all our techniques
individually on multiple fuzzers to demonstrate that they are
effective if and only if the fuzzer employs the targeted as-
sumptions. From this experiment, we can answer RQ 2. and
conclude that our mitigations are working as intended. We use
the same dummy application used in RQ 1. to evaluate eight
fuzzers and bug-ﬁnding tools, namely: AFL 2.52b, VUZZER,
HONGGFUZZ 1.6, DRILLER commit 66a3428, ZZUF 0.15,
PEACH 3.1.124, and QSYM commit d4bf407. Besides the
aforementioned fuzzers, we consider one purely symbolic
execution based tool to complete the set of automatic bug
ﬁnding techniques: KLEE 1.4.0.0 [12].
To answer RQ 3., we test a subset of these fuzzers against
the LAVA-M dataset to demonstrate that ANTIFUZZ is able
to prevent bug ﬁnding in real-world applications.
USENIX Association
28th USENIX Security Symposium    1939
Figure 2: A simple program before (a) and after integration (b) with ANTIFUZZ.
To address RQ 4., we evaluate ANTIFUZZ on binary exe-
cutables from binutils to show the difference in test coverage
in a protected and unprotected application. This experiment
demonstrates that ANTIFUZZ does not simply hide bugs, but
also drastically reduces the attack surface. It is worth mention-
ing that in all experiments mentioned above, the bug ﬁnding
tools were able to ﬁnd the bugs in a matter of minutes prior
to enforcing ANTIFUZZ protection. After applying our tech-
niques, there were zero bugs found by the tested tools within
a period of 24 hours.
In the last step, we measure the overhead introduced by
ANTIFUZZ using the SPEC CPU2006 benchmarking suite to
answer RQ 5..
Note that, due to the conﬁgurable nature of ANTIFUZZ, we
use the following conﬁguration for all experiments:
• Attacking Coverage-guidance: Generates 10,000 fake
functions with constraints, and 10,000 basic blocks for
random edge generation.
• Delaying Execution: The signal handler introduces a
slowdown in case of a crash to timeout the application
(in addition to slowdowns due to malformed inputs). The
duration of the sleep is set to 750ms.
• Preventing Crash Detection: We enabled all tech-
niques mentioned in Section 5.2.
• Overloading Symbolic Execution Engines: Important
comparisons for equivalence were replaced with SHA-
512 hash comparisons and the input data was encrypted
and decrypted via AES-256 in ECB mode.
If the fuzzer supported both binary instrumentation and
compile-time instrumentation, we used the compile-time in-
strumentation. While in reality, a fuzzer would have to use
binary-only instrumentation mechanisms (given our attacker
model), we chose to use compile-time instrumentation as it
achieves better performance and is also more robust. There-
fore, we erred on the side of caution by assuming that an
attacker is more powerful than state-of-the-art tools.
6.1 ANTIFUZZ versus Software Obfuscation
One of the goals of software obfuscation is to prevent se-
curity researchers, who rely on traditional manual reverse
engineering techniques, from ﬁnding bugs. In this section,
we demonstrate that obfuscation on its own fails to thwart
automatic bug ﬁnding tools.
Intuitively, blind fuzzers without feedback mechanisms are
not hindered by obfuscation at all, because they neither have
nor need any knowledge about the code. Feedback-driven
fuzzers, however, do need access to edges and basic blocks to
obtain coverage information they can use to guide the fuzzing
process. Thus, obfuscating the control ﬂow via common tech-
niques such as control ﬂow ﬂattening or virtual machine based
obfuscation [21] might impact coverage-guided fuzzers.
Experiment To demonstrate that obfuscation techniques
alone do not protect an application from automatic bug ﬁnding
tools, we obfuscated a dummy application (see Listing 2) with
TIGRESS 2.2 [15] and let different fuzzers ﬁnd the correct
crashing input.
Listing 2: Dummy application that crashes if input is ’crsh’
i n t check ( char * i n p u t ,
i n t
s i z e ) {
i f ( s i z e == 4 && i n p u t [ 0 ] == ' c ' && i n p u t [ 1 ] == ' r ' &&
i n p u t [ 2 ] == ' s ' && i n p u t [ 3 ] == ' h ' ) {
c r a s h ( ) ;
}
}
1940    28th USENIX Security Symposium
USENIX Association
mainSIG handlercrashif(in==1337)Bugprint("bad")exit()if(h(in)==af32...)SIG handlerif(fake)enc(in)dec(in)install signal-handleranti-debugfake SEGVidle()exit()idle()exit()yesnoyesno(b)mainif(in==1337)Bugprint("bad")exit()yesno(a)CRASHCRASH......N Layers of Fake Edgesfake_funcs[hash(in+3)]();Random Fake Constraintsif(in[3]==in[4]) {... }Anti-SE Anti-Crash Delay Anti-SE For this experiment, we use AFL, HONGGFUZZ, KLEE and
ZZUF which are representative of all three fuzzer categories.
Note that VUZZER was excluded because (1) VUZZER is
based on the IDA Pro disassembler, which is thwarted by
obfuscation before the fuzzing process even begins, and (2)
Tigress had trouble compiling non-64bit executables while
VUZZER (at the time of the experiment) was not working on
64-bit binaries. Additionally, any fuzzer which is based on the
aforementioned tools was excluded from the experiment. For
example, QSYM and DRILLER use AFL with an additional
symbolic execution engine. Therefore, if AFL is able to ﬁnd
the bug, we conclude that other tools that use AFL under the
hood can also ﬁnd the bug.
We conﬁgured TIGRESS by enabling as many of the obfus-
cation features as we could. The exact conﬁguration is shown
in Table 1 of Appendix A.
Result This experiment revealed that all fuzzers could ﬁnd
the crashing input despite all obfuscation techniques being
enabled. This answers research question RQ 1., current ob-
fuscation techniques are not efﬁcient against automated bug
ﬁnding techniques. Even though changing the control-ﬂow
graph might have an impact on the feedback mechanism, the
changes are static or random. In contrast, in ANTIFUZZ the ad-
ditional information for the feedback mechanism is dependant
on the input, which is a major difference between common
obfuscation methods and our approach.
6.2 Finding Crashes in a Simple Dummy Ap-
plication
To answer research question RQ 2., we use the same dummy
application from the previous experiment.
For this evaluation, we enable our anti-fuzzing techniques
one at a time, rather than enabling all of them at once. This
allows us to observe which fuzzer is vulnerable to each tech-
nique we introduced. We use this rather simple target for two
reasons. (1) If a fuzzer is unable to ﬁnd this very shallow bug,
they will most likely also fail to ﬁnd more complex crashes,
and (2) the code is simple enough to be adjusted to different
systems and fuzzers (e.g., DRILLER needs CGC binaries).
Any input that is not the crashing input is deemed to be
malformed, i.e., ANTIFUZZ decides to slow down the applica-
tion in that case. If countermeasures against program analysis
techniques are activated, the data from the input ﬁle is ﬁrst
encrypted and then decrypted again. The comparisons against
the individual bytes of “crsh” are done via hash comparisons
(e.g., hash("c") == hash(input[0])). Signal tampering
and anti-coverage techniques are all applied before the input
ﬁle is opened. Since both PEACH and ZZUF are not able to
overcome the four-byte constraints on their own, we provided
ZZUF with the seed ﬁle where only the “c” character was
missing. Similarly, PEACH was evaluated on an ELF64 parser
Table 2: Evaluation against the dummy application.
means ANTIFUZZ
was successful in preventing bug ﬁnding (no crash was found) and means
that at least one crashing input was found. None means ANTIFUZZ was
disabled, All means that all techniques against fuzzers (Coverage, Crash,
Speed and Symbolic Execution) were turned on.
None
Coverage
Crash
Speed
Symbolic Exec.
All
AFL
Honggfuzz
Vuzzer
Driller
Klee
zzuff
Peach
QSYM








-
-









a
a Klee ran at least 24h and then crashed due to memory constraints.
(readelf). We modiﬁed the elf parser to include an additional
one-byte check of a ﬁeld in ELF64 that guards the crash.
Every possible combination of fuzzer and ANTIFUZZ con-
ﬁguration ran for a period of 24 hours. Moreover, in this ex-
periment, the conﬁguration with all fuzzing countermeasures
enabled (“All”) ran for a total of 100 hours.
Result The results of this experiment are shown in Table 2.
Without ANTIFUZZ, it only took a couple of seconds up to
a few minutes for all eight fuzzers to ﬁnd the crashing input.
However, when ANTIFUZZ was fully activated, no fuzzer
was able to do so even after 100 hours. Comparing this ta-
ble to Table 1 shows that our techniques clearly address the
fundamental assumptions that fuzzers use to ﬁnd bugs.
All coverage-guided fuzzers were impeded by our anti-
coverage feature. As expected, all fuzzers were unable to ﬁnd
crashes when we used our anti-crash detection technique. It
is worth mentioning that DRILLER was not tested with this
conﬁguration because the CGC environment does not allow
custom signal handlers. Surprisingly, KLEE was also unable
to ﬁnd the crash because of its incomplete handling of custom
signals. Since delaying execution technique (speed) also relies
on custom signals, the experiment with DRILLER was omitted
and KLEE failed to ﬁnd the bug. ZZUF was able to crash the
target because there were only 256 different inputs to try.
As expected, KLEE was not able to ﬁnd the correct input
once countermeasures against symbolic execution were acti-
vated. Surprisingly, VUZZER is confused by this technique
as well. A closer inspection suggests that this behavior was
due to the fact that this technique is also highly effective at
obfuscating taint information.
6.3 Finding Crashes in LAVA-M
The dummy application demonstrated our ability to thwart
fuzzers for simple examples. To make sure that our tech-
niques also hold up on more complex applications (and an-
swer RQ 3.), we evaluate ANTIFUZZ with the LAVA-M
dataset [18], which consists of four applications (base64,
who, uniq and md5sum) where several bugs were artiﬁcially
USENIX Association
28th USENIX Security Symposium    1941
Table 3: Statistical analysis of the code coverage on eight binaries from binutils. The effect size is given in percentage of the branches that could be covered after
enabling ANTIFUZZ as compared to the coverage achieved on an unprotected program. Experiments where the two-tailed Mann-Whitney U test resulted in
p < 0.05 are displayed in bold.
vuzzer
aﬂ
hongg
qsym
addr2line
12.12%, p: 0.04
9.49%, p: 0.04
0.00%, p: 0.03
7.12%, p: 0.04
ar
-
-
0.00%, p: 0.25
11.69%, p: 0.03
nm-new
1.81%, p: 0.04
1.92%, p: 0.04
0.00%, p: 0.03
5.30%, p: 0.04
objdump
2.65%, p: 0.03
4.98%, p: 0.04
0.00%, p: 0.03
5.47%, p: 0.04
readelf
1.10%, p: 0.04
0.70%, p: 0.04
0.00%, p: 0.03
1.75%, p: 0.04
size
13.41%, p: 0.33
6.30%, p: 0.04
0.00%, p: 0.03
9.79%, p: 0.04
strings
6.25%, p: 0.04
16.17%, p: 0.04
0.00%, p: 0.03
8.55%, p: 0.04
strip-new
0.84%, p: 0.19
4.52%, p: 0.04
0.00%, p: 0.03
4.89%, p: 0.04
Table 4: Evaluation against base64, uniq, who, md5sum from the LAVA-M
data set.
means ANTIFUZZ was successful in preventing bug ﬁnding (no
crash was found) and  means that at least one crashing input was found, the
# sign denotes the number of unique crashes found. None means ANTIFUZZ
was disabled, All means that all techniques against fuzzers (Coverage, Crash,
Speed and Symbolic Execution) are turned on.
base64
AFL
Honggfuzz
QSYM
Vuzzer
zzuf
uniq
AFL
Honggfuzz
QSYM
Vuzzer
zzuf
who
AFL
Honggfuzz
QSYM
Vuzzer
zzuf