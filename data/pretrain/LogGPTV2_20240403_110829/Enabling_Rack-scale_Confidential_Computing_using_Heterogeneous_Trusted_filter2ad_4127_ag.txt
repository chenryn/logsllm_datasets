### HETEE Box Cost Analysis

A typical configuration of the HETEE box includes one Security Controller, four proxy nodes, and 16 GPUs. We have chosen three widely used GPUs in AI computing: the Nvidia GTX TITAN X, Tesla P40, and Tesla V100. The cost breakdown for each GPU is as follows:

- **Intel Xeon-E3 1220V6 Chip:** $213 [57]
- **Xilinx Zynq Chip:** $60 [58]
- **16GB DDR4 2400MHz ECC Memory:** $548 [59]
- **Self-destructing Module:** $500 [34]
- **Nvidia GTX TITAN X GPU:** $1150 [60]
- **Nvidia Tesla P40 GPU:** $5999 [61]
- **Nvidia Tesla V100 GPU:** $8799 [62]
- **Broadcom PEX9797 Chip:** $590 [63]

The self-destructing module includes a set of pressure sensor modules, an MCU control board, and disks with security protection functions. The security cost of the HETEE box accounts for approximately 17.8% of the total cost when using the TITAN X GPU, and only 2.9% when using the Tesla V100 GPU. As more GPUs are integrated (e.g., 32), the proportion of the security expense will further decrease.

### Extending HETEE to Support SaaS Model

HETEE can be extended to support software-as-a-service (SaaS), where the enclave creator provides application software such as a genetic testing program for disease discovery [110], [111]. The user can verify the integrity of the software through remote attestation, and HETEE ensures that the user data (e.g., DNA data) is protected from unauthorized access (e.g., by the cloud provider who creates the enclave).

If the cloud provider's application software contains secrets (e.g., secret parameters of a machine learning model) that need to be protected from the enclave user, an application-specific component can run on the Security Controller (SC) for input check and sanitization [112]. This protects the software against memory attacks from malicious inputs uploaded by the enclave user, which could otherwise lead to the exposure of the software’s secrets.

In cases where the application software itself is unknown to the enclave user and cannot be audited through integrity checks, a sandbox (similar to Ryoan [113]) can be used to prevent unauthorized data leaks. However, the sandbox must be included in the Trusted Computing Base (TCB).

### Related Works

#### Isolated Execution

Mainstream processor vendors have implemented Trusted Execution Environments (TEEs) in some of their chip products, such as Intel Software Guard Extensions (SGX) [3], AMD Secure Encrypted Virtualization (SEV) [4], and ARM TrustZone [5]. Additionally, open-source security concepts like Keystone [64] and Sanctum [65]–[67] have been proposed for RISC-V processors. These TEEs generally isolate a secure world from the insecure one, allowing protected data to be processed securely. However, none of these TEEs fully support Confidential Data-Intensive (CDI) computing tasks, which often use heterogeneous architectures. For example, Intel SGX does not support trusted I/O paths to protect data transmissions between enclaves and I/O devices. Although ARM TrustZone can support trusted I/O paths for certain peripherals in the ARM ecosystem, it still lacks full support for heterogeneous computing units like GPUs.

#### Trusted Paths

Graviton [15] modifies existing GPU chips by enhancing the internal hardware command processor to support trusted execution environments on GPUs. HIX [16] extends an SGX-like design to enable secure access to GPUs from CPU enclaves, requiring modifications to the MMU and PCIe Root Complex on the CPU chip. In contrast, HETEE does not require any changes to existing commercial CPUs or accelerators. SGXIO [68] is a generic trusted path extension for Intel SGX, but its capacity limitations make it unsuitable for high-performance accelerators like off-chip GPUs. Other studies propose specific trusted paths for external devices, such as USB devices [69] and displays [70], but not for PCIe accelerators.

#### Privacy-Preserving Deep Learning

Nick Hynes et al. evaluated two types of secure AI computing [72]. One scenario involves running entire AI workloads inside SGX enclaves, which cannot utilize accelerators. Another scenario is the Slalom solution [73], which decomposes the AI model into two parts: the upper control flow part runs inside the SGX enclave, while non-privacy-sensitive basic computations are offloaded to untrusted GPUs for acceleration. However, this approach can result in accuracy loss. HETEE, on the other hand, securely encapsulates the entire AI network without altering its structure, thus maintaining accuracy. Researchers have also proposed protecting AI networks by introducing noise [85].

### Conclusion

Large-scale confidential computing is driven by significant real-world demands. However, current TEEs lack scalable and effective protection for high-throughput accelerators like GPUs. To address these issues, this paper presents HETEE, the first Heterogeneous TEE design that supports large-scale CDI computing without requiring chip-level changes. HETEE is a device for centralized management of all computing units in a server rack, designed to work with modern data centers and clouds. It leverages resource pooling technologies to dynamically compartmentalize computing tasks, enforce strong isolation, and reduce the TCB through hardware support. Specifically, HETEE uses the PCIe switch fabric to allocate accelerators to server nodes for non-sensitive CDI tasks and moves them back into a secure enclave for confidential computing. Our implementation and evaluations show that HETEE can support rack-scale confidential computing tasks with minimal performance overhead and good scalability. Future work includes formal verification and investigating hardware-software separation in the design.

### Acknowledgments

We thank the anonymous reviewers and the shepherd Prof. Emmett Witchel for their insightful comments and suggestions. We also thank Dr. Shijun Zhao for his help with the remote attestation protocol. This work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences under grant No. XDC02010200 and the National Natural Science Foundation of China (Grant No. 61802397).

### References

[1] Josh Constine. Facebook bug exposed up to 6.8M users’ unposted photos to apps, https://techcrunch.com/2018/12/14/facebook-photobug/, 2018.
[2] CBSNEWS. Facebook CEO warns data exposure could be more widespread, https://www.cbsnews.com/video/facebook-ceo-warns-data-exposure-could-be-more-widespread/, 2018.
[3] Intel. Intel Software Guard Extensions (Intel SGX), https://software.intel.com/en-us/sgx/details, 2019.
[4] AMD. AMD Secure Encrypted Virtualization (SEV), https://developer.amd.com/sev/, 2019.
[5] ARM. Architecting a more Secure world with isolation and virtualization, https://developer.arm.com/products/architecture/security-architectures?ga=2.65596206.1465614028.1550155414-1474563975.1550155414, 2019.
[6] Facebooks. Accelerating Facebook’s infrastructure with application-specific hardware. POSTED ON MAR 14, 2019 TO DATA CENTER ENGINEERING. By Kevin Lee, Vijay Rao, William Christie Arnold. https://code.fb.com/data-center-engineering/accelerating-infrastructure/
[7] Amazon. Machine Learning on AWS: Putting machine learning in the hands of every developer, https://aws.amazon.com/machinelearning/?hp=tile&tile=solutions, 2019.
[8] Adrian M Caulfield, Eric S Chung, Andrew Putnam, Hari Angepat, Jeremy Fowers, Michael Haselman, Stephen Heil, Matt Humphrey, Puneet Kaur, Jooyoung Kim, et al. A cloud-scale acceleration architecture. International Symposium on Microarchitecture, pages 1-13, 2016.
[9] Jeremy Fowers, Jooyoung Kim, Doug Burger, and Scott Hauck. A scalable high-bandwidth architecture for lossless compression on FPGAs. Pages 52-59, 2015.
[10] Google. Cloud Tensor Processing Units (TPUs), https://cloud.google.com/tpu/, 2019.
[11] Muhuan Huang, Di Wu, Cody Hao Yu, Zhenman Fang, Matteo Interlandi, Tyson Condie, and Jason Cong. Programming and runtime support to blaze FPGA accelerator deployment at datacenter scale. In Proceedings of the Seventh ACM Symposium on Cloud Computing, pages 456-469. ACM, 2016.
[12] Microsoft. Azure Reference Architectures, https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/, 2019.
[13] Andrew Putnam, Adrian M Caulfield, Eric S Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, Jeremy Fowers, Gopi Prashanth Gopal, Jan Gray, et al. A reconfigurable fabric for accelerating large-scale datacenter services. International Symposium on Computer Architecture, 42(3):13-24, 2014.
[14] Charles Roe. The Future of the Data Center: Heterogeneous Computing, https://www.dataversity.net/future-data-center-heterogeneous-computing/, 2016.
[15] Stavros Volos, Kapil Vaswani, and Rodrigo Bruno. Graviton: Trusted execution environments on GPUs. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 681-696, Carlsbad, CA, 2018. USENIX Association.
[16] Jang, Insu and Tang, Adrian and Kim, Taehoon and Sethumadhavan, Simha and Huh, Jaehyuk. Heterogeneous Isolated Execution for Commodity GPUs. Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS ’19, pages 455-468. ACM, 2019.
[17] NVIDIA. CUDA Toolkit Documentation v9.0.176. https://docs.nvidia.com/cuda/archive/9.0/
[18] Google. TensorFlow API documents, version 1.11. https://tensorflow.google.cn/versions/r1.11/api_docs/python/tf
[19] NVIDIA DGX-2. NVSwitch Accelerates NVIDIA DGX-2. By Robert Sohigian. August 21, 2018. https://devblogs.nvidia.com/nvswitch-accelerates-nvidia-dgx2/
[20] NVIDIA DGX-2 Details at Hot Chips 30. By Patrick Kennedy. August 21, 2018. https://www.servethehome.com/nvidia-dgx-2-details-at-hot-chips-30/
[21] NVIDIA HGX-2. WORLD’S MOST POWERFUL ACCELERATED SERVER PLATFORM FOR DEEP LEARNING, MACHINE LEARNING, AND HPC. https://www.nvidia.com/en-us/data-center/hgx/
[22] NVIDIA. HGX-2 Fuses HPC and AI Computing Architectures. By William. May 29, 2018. https://devblogs.nvidia.com/hgx-2-fuses-ai-computing/
[23] HGX-1. Siamak Tavallaei, CSI, Azure Cloud. Microsoft Project Olympus Hyperscale GPU Accelerator. May 26, 2017. https://azure.microsoft.com/mediahandler/files/resourcefiles/00c18868-eba9-43d5-b8c6-e59f9fa219ee/HGX-1%20Blog%205%2026%202017.pdf
[24] Siamak Tavallaei, Robert Ober. Microsoft Project Olympus Hyperscale GPU Accelerator (HGX-1). OCP U.S. SUMMIT 2017. Santa Clara, CA, March 8, 2017. http://schd.ws/hosted_files/ocpussummit2017/85/OCP17%20Microsoft%20Project%20Olympus%20Hyperscale%20GPU%20Accelerator%20HGX-1%20March%208%202017.pdf
[25] Broadcom. PEX9797. 97 lane, 25 port, PCI Express Gen3 ExpressFabric Platform. https://www.broadcom.com/products/pcie-switches-bridges/expressfabric/pex9797
[26] Avago’s PEX9700 turns the PLX PCIe3 switch into a fabric. May 12, 2015 by Charlie Demerjian. https://semiaccurate.com/2015/05/12/avagos-pex9700-turns-plx-pcie3-switch-fabric/
[27] Avago Announces PLX PEX9700 Series PCIe Switches: Focusing on Data Center and Racks. by Ian Cutress on May 12, 2015. https://www.anandtech.com/show/9245/avago-announces-plx-pex9700-series-pcie-switches
[28] Peter X. Gao and Akshay Narayan and Sagar Karandikar and Joao Carreira and Sangjin Han and Rachit Agarwal and Sylvia Ratnasamy and Scott Shenker. Network Requirements for Resource Disaggregation. 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 249-264, Savannah, GA, 2016. USENIX Association. https://www.usenix.org/system/files/conference/osdi16/osdi16-gao.pdf
[29] Huawei. High Throughput Computing Data Center Architecture, Thinking of Data Center 3.0. Technical White Paper. June, 2014. https://www.huawei.com/ilink/en/download/HW-349607
[30] EDP Europe. Infrasolution rack access control. https://www.edpeurope.com/product/rack-accesscontrol/.
[31] EDP Europe. iaccess IT cabinet security system. https://www.edpeurope.com/product/iaccesscomputer-cabinet-door-security-access-controlsystem/.
[32] EDP Europe. Biometric swipe access card. https://www.edpeurope.com/product/biometric-swipe-access-card.
[33] Southco. Rack-level security. http://lp.southco.com/rs/southco/images/Rack%20Level%20Security%20Brochure.pdf.
[34] Gary Smolker and Leon Chernick. Method and apparatus for combustibly destroying microelectronic circuit board interconnections, May 6, 1975. US Patent 3,882,324.
[35] Open Neural Network Exchange Format, The open ecosystem for interchangeable AI models. https://onnx.ai
[36] Open Neural Network Exchange (ONNX). Tutorials for creating and using ONNX models. https://github.com/onnx/tutorials
[37] Dean, Jeffrey and Barroso, Luiz Andre. The tail at scale. Communications of The ACM 2013. Pages 74-80.
[38] Cong, Jason and Ghodrat, Mohammad Ali and Gill, Michael and Grigorian, Beayna and Gururaj, Karthik and Reinman, Glenn. Accelerator-Rich Architectures: Opportunities and Progresses. Pages 1-6, 2014.
[39] Barroso, Luiz Andre and Clidaras, Jimmy and Holzle, Urs. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second edition. Synthesis Lectures on Computer Architecture, pages 1-154, 2013.
[40] Chen, Quan and Yang, Hailong and Mars, Jason and Tang, Lingjia. Baymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers. Operating Systems Review 2016, pages 681-696.
[41] Jain, Paras and Mo, Xiangxi and Jain, Ajay and Subbaraj, Harikaran and Durrani, Rehan Sohail and Tumanov, Alexey and Gonzalez, Joseph E and Stoica, Ion. Dynamic Space-Time Scheduling for GPU Inference. arXiv: Distributed, Parallel, and Cluster Computing, 2019. https://arxiv.org/pdf/1901.00041.pdf
[42] Zhichao Hua and Jinyu Gu and Yubin Xia and Haibo Chen and Binyu Zang and Haibing Guan. vTZ: Virtualizing ARM TrustZone. 26th USENIX Security Symposium (USENIX Security 17), pages 541-556, Vancouver, BC, 2017. USENIX Association. https://www.usenix.org/system/files/conference/usenixsecurity17/sec17-hua.pdf
[43] Caffe. Yangqing Jia. http://caffe.berkeleyvision.org
[44] Caffe: a fast open framework for deep learning. https://github.com/BVLC/caffe/
[45] PyTorch. FROM RESEARCH TO PRODUCTION. An open source deep learning platform that provides a seamless path from research prototyping to production deployment. https://pytorch.org
[46] PyTorch. Tensors and Dynamic neural networks in Python with strong GPU acceleration. https://github.com/pytorch/pytorch
[47] PCI-SIG SR-IOV Primer: An Introduction to SR-IOV Technology, Revision 2.5, January 2011. https://www.intel.com/content/dam/doc/application-note/pci-sig-sr-iov-primer-sr-iov-technology-paper.pdf
[48] SR-IOV Architecture. 04/20/2017, Duncan MacMichael. https://docs.microsoft.com/en-us/windows-hardware/drivers/network/sr-iov-architecture
[49] Michael Krause (HP, co-chair), Renato Recio (IBM, co-chair). PCIe I/O virtualization and sharing. PCI-SIG 2006. http://weblab.cs.uml.edu/bill/cs520/slides_15D_PCI_Express_IOV.pdf, http://weblab.cs.uml.edu/bill/cs520/slides_15E_PCI_Express_IOV.pdf
[50] Using PCI express as the primary system interconnect in multiroot compute storage communications and embedded systems (White Paper). 2008. https://www.idt.com/document/whp/idt-pcie-multi-root-white-paper
[51] SGD. Stochastic gradient descent. https://en.wikipedia.org/wiki/Stochastic_gradient_descent
[52] Diederik P. Kingma, Jimmy Ba. Adam: A Method for Stochastic Optimization. 3rd International Conference for Learning Representations (ICLR2015), San Diego, 2015. https://arxiv.org/abs/1412.6980
[53] ImageNet. Large Scale Visual Recognition Challenge, 2012 (ILSVRC2012). http://image-net.org/challenges/LSVRC/2012/
[54] VGG16. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Computer Science, 2014.
[55] GoogLeNet. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 2014.
[56] ResNet. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.