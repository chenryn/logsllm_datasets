TITAN X GPU 
(b) Integrating
Tesla P40 GPU
(c) Integrating
Tesla V100 GPU
Fig. 9. HETEE Box cost analysis. ( Intel Xeon-E3 1220V6 Chip: $213
[57], Xilinx Zynq Chip:$60 [58], 16GB DDR4 2400MHz ECC: $548
[59],Self-destructing module:$500 [34], Nvidia GTX TITAN X:
$1150 [60], Nvidia Tesla P40: $5999 [61], Nvidia Tesla V100:
$8799 [62], Broadcom PEX9797 Chip: $590 [63]; )
A typical conﬁguration of the HETEE box includes 1
Security Controller, 4 proxy nodes, and 16 GPUs. We
choose three widely used GPUs in AI computing, and the
cost breakdown is shown in Figure 9. In the case of the self-
destructing module, we consider one practical solution that
includes a set of pressure sensor modules, MCU control board,
and disks with security protection functions. It can be seen that
the security cost of the HETEE box takes about 17.8% of the
total HETEE box for TITAN X GPU, and only 2.9% for the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:57:53 UTC from IEEE Xplore.  Restrictions apply. 
1461
Tesla V100 GPU. When more GPUs are integrated (e.g. 32),
the portion of the security expense will go further down.
Extending HETEE to support the SaaS model. HETEE
can be extended to support software-as-a-service (SaaS, in
which the enclave creator also provides application software
such as a genetic testing program for disease discovery [110],
[111]), when the application itself does not include secret. The
user can verify the integrity of the software through remote
attestation, and HETEE protects the user data (e.g., DNA data)
uploaded from exposing to the unauthorized party (e.g., the
cloud provider who creates the enclave through HETEE).
In the case that the cloud provider’s application software
does contain secrets (e.g., secret parameters of a ML model)
and needs to protect from the enclave user, however, an
application-speciﬁc component can run on the SC for input
check and sanitization [112], so as to protect the software
against memory attacks from malicious inputs uploaded by the
enclave user (which can lead to the exposure of the software’s
secret). When the application software itself is unknown to the
enclave user and therefore cannot be audited through integrity
check, a sandbox (similar to Ryoan [113]) could be used
to prevent the unauthorized leak of user data. In this case,
however, we needs to include the sandbox in the TCB.
IX. RELATED WORKS
Isolated Execution. Mainstream processor vendors have im-
plemented TEEs in some of their chip products, such as
Intel Software Guard Extensions (SGX) [3], AMD Secure
Encrypted Virtualization (SEV) [4] and ARM TrustZone [5].
In addition, based on the concept of open-source security,
Keystone [64] and Sanctum [65]–[67] are proposed for RISC-
V processor. These TEEs generally isolate a secure world from
the insecure one, and the protected data can be processed
in such secure world. However, none of those TEEs can
truly support CDI computing tasks which widely adopt a
heterogeneous architecture. For example, Intel SGX does not
support trusted I/O paths to protect the data transmissions
between enclaves and I/O devices. Although ARM TrustZone
can support trusted I/O paths for certain peripherals in the
ARM ecosystem, it is noted that TrustZone still does not truly
support heterogeneous computing units like GPU.
Trusted paths. Graviton [15] modiﬁes the existing GPU chips
via enhancing the internal hardware command processor, to
support trusted execution environments on GPUs. HIX [16]
extends an SGX-like design to enable secure access to GPU
from the CPU enclave, which needs to modify the MMU and
PCIe Root Complex on the CPU chip. By comparison, HETEE
does not require any changes to existing commercial CPUs or
accelerators. SGXIO [68] is a generic trusted path extension
for Intel SGX. Trusted paths are established via a dedicated
trusted boot enclave. However, the capacity limitation of the
SGX enclave prevent SGXIO being widely used for high-
performance accelerators like off-chip GPUs. Besides, there
are several prior studies that propose speciﬁc trusted paths
for some extertnal devices but not PCIe accelerators. For
instance, Zhou et al. propose a trusted path to a single USB
device [69]. Yu et al. demonstrate how to build a trusted path
for display [70]. Filyanov et al. discuss a pure uni-directional
trusted path using the TPM and Intel TXT [71].
Privacy preserving deep learning. Nick Hynes et al evaluated
two types of secure AI computing [72]. One scenario is
to compute the entire AI workloads inside SGX enclaves,
which cannot utilize accelerators. Similar work can be found
in other studies [83], [84]. Another scenario is the Slalom
solution [73]. It needs to decompose the AI model network
into two parts, in which the upper control ﬂow part runs inside
the SGX enclave and is tightly protected, while some non-
privacy-sensitive basic computations is thrown to the untrusted
GPU for acceleration. However, splitting AI networks result in
possible accuracy decrease, while our HETEE programming
model securely encapsulate the whole AI network, without
any change of the AI structure, so the accuracy will not be
affected. Researchers also proposed to protect the privacy of
AI networks by introducing noise [85].
X. CONCLUSION
Large-scale conﬁdential computing is driven by huge de-
mands in the real world. However, it still cannot be supported
by today’s Trusted Execution Environment (TEE), due to the
lack of scalable and effective protection of high-throughput
accelerators like GPUs. To address these problems, this pa-
per presents the ﬁrst Heterogeneous TEE design that can
truly support large-scale compute and/or data intensive (CDI)
computing, without any chip-level change. Our approach,
called HETEE, is a device for centralized management of all
computing units of a server rack. It is uniquely designed to
work with today’s data centers and clouds, leveraging modern
resource pooling technologies to dynamically compartmental-
ize computing tasks, and enforce strong isolation and reduce
TCB through hardware support. More speciﬁcally, HETEE
utilizes the PCIe switch fabric to allocate its accelerators to
the server node on the same rack for a non-sensitive CDI task,
and move them back into a secure enclave in response to the
demand for conﬁdential computing. We implemented HETEE
on a real hardware system, and evaluated it with popular
neural network inference and training tasks. Our evaluations
show that HETEE can easily support such rack-scale conﬁ-
dential computing tasks with a small performance overhead
and exhibits good scalability when elastically using multiple
accelerators. Down the road, we plan to further evaluate our
design and implementation through formal veriﬁcation, and
investigate hardware-software separation of the design.
XI. ACKNOWLEDGMENTS
We thank the anonymous reviewers and the shepherd Prof.
Emmett Witchel for their insightful comments and sugges-
tions. We also thank Dr. Shijun Zhao for his help in remote
attestation protocol. This work was supported by the Strategic
Priority Research Program of Chinese Academy of Sciences
under grant No. XDC02010200 and National Natural Science
Foundation of China (Grant No.61802397).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:57:53 UTC from IEEE Xplore.  Restrictions apply. 
1462
REFERENCES
[1] Josh Constine. Facebook bug exposed up to 6.8M users’ unposted photos
to apps, https://techcrunch.com/2018/12/14/facebook-photobug/, 2018.
[2] CBSNEWS. Facebook CEO warns data exposure could be more
widespread, https://www.cbsnews.com/video/facebook-ceo-warns-data-
exposure-could-be-more-widespread/, 2018.
[3] Intel. Intel Software Guard Extensions (Intel SGX), https://software.
intel.com/en-us/sgx/details, 2019.
[4] AMD. AMD Secure Encrypted Virtualization (SEV), https://developer.
amd.com/sev/, 2019.
[5] ARM. Architecting a more Secure world with isolation and
virtualization, https://developer.arm.com/products/architecture/security-
architectures? ga=2.65596206.1465614028.1550155414-1474563975.1
550155414, 2019.
[6] Facebooks. Accelerating Facebook’s infrastructure with application-
speciﬁc hardware. POSTED ON MAR 14, 2019 TO DATA CENTER
ENGINEERING. By Kevin Lee, Vijay Rao, William Christie Arnold.
https://code.fb.com/data-center-engineering/accelerating-infrastructure/
[7] Amazon. Machine Learning on AWS: Putting machine learning in the
hands of every developer, https://aws.amazon.com/machinelearning/?
hp=tile&tile=solutions, 2019.
[8] Adrian M Caulﬁeld, Eric S Chung, Andrew Putnam, Hari Angepat,
Jeremy Fowers, Michael Haselman, Stephen Heil, Matt Humphrey,
Puneet Kaur, Jooyoung Kim, et al. A cloud-scale acceleration architec-
ture. international symposium on microarchitecture, pages 1-13, 2016.
[9] Jeremy Fowers, Jooyoung Kim, Doug Burger, and Scott Hauck. A
scalable high-bandwidth architecture for lossless compression on fpgas.
pages 52-59, 2015.
[10] Google. Cloud Tensor Processing Units (TPUs), https://cloud.google.
com/tpu/, 2019.
[11] Muhuan Huang, Di Wu, Cody Hao Yu, Zhenman Fang, Matteo Inter-
landi, Tyson Condie, and Jason Cong. Programming and runtime support
to blaze fpga accelerator deployment at datacenter scale. In Proceedings
of the Seventh ACM Symposium on Cloud Computing, pages 456-469.
ACM, 2016.
[12] Microsoft. Azure Reference Architectures, https://docs.microsoft.com/
enus/azure/architecture/reference-architectures/, 2019.
[13] Andrew Putnam, Adrian M Caulﬁeld, Eric S Chung, Derek Chiou,
Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, Jeremy Fow-
ers, Gopi Prashanth Gopal, Jan Gray, et al. A reconﬁgurable fabric for
accelerating large-scale datacenter services. international symposium on
computer architecture, 42(3):13-24, 2014.
[14] Charles Roe. The Future of the Data Center: Heterogeneous Computing,
https://www.dataversity.net/future-data-center-heterogeneous-comput-
ing/, 2016.
[15] Stavros Volos, Kapil Vaswani, and Rodrigo Bruno. Graviton: Trusted
execution environments on gpus. In 13th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 18), pages 681-
696, Carlsbad, CA, 2018. USENIX Association.
[16] Jang, Insu and Tang, Adrian and Kim, Taehoon and Sethumadha-
van, Simha and Huh, Jaehyuk. Heterogeneous Isolated Execution for
Commodity GPUs. Proceedings of the Twenty-Fourth International
Conference on Architectural Support for Programming Languages and
Operating Systems, ASPLOS ’19, pages 455-468. ACM, 2019
[17] NVIDIA. CUDA Toolkit Documentation v9.0.176. https://docs.nvidia.
com/cuda/archive/9.0/
[18] Google. TensorFlow API documents, reversion 1.11. https://tensorﬂow.
google.cn/versions/r1.11/api docs/python/tf
[19] NVIDIA DGX-2. NVSwitch Accelerates NVIDIA DGX-2. By Robert
Sohigian. August 21, 2018. https://devblogs.nvidia.com/nvswitch-
accelerates-nvidia-dgx2/
[20] NVIDIA DGX-2 Details at Hot Chips 30. By Patrick Kennedy. Au-
gust 21, 2018. https://www.servethehome.com/nvidia-dgx-2-details-at-
hot-chips-30/
[21] NVIDIA HGX-2. WORLD’S MOST POWERFUL ACCELERATED
SERVER PLATFORM FOR DEEP LEARNING, MACHINE LEARN-
ING, AND HPC. https://www.nvidia.com/en-us/data-center/hgx/
[22] NVIDIA. HGX-2 Fuses HPC and AI Computing Architectures. By
William. May 29, 2018. https://devblogs.nvidia.com/hgx-2-fuses-ai-
computing/
[23] HGX-1. Siamak Tavallaei, CSI, Azure Cloud. Microsoft Project
Olympus Hyperscale GPU Accelerator. May 26, 2017. https://azure.
microsoft.com/mediahandler/ﬁles/resourceﬁles/00c18868-eba9-43d5-
b8c6-e59f9fa219ee/HGX-1%20Blog 5 26 2017.pdf
[24] Siamak Tavallaei, Robert Ober.Microsoft Project Olympus Hyperscale
GPU Accelerator ( HGX-1 ). OCP U.S. SUMMIT 2017. Santa Clara,
CA, March 8, 2017. http://schd.ws/hosted ﬁles/ocpussummit2017/85/
OCP17%20Microsoft%20Project%20Olympus%20Hyperscale%20GPU
%20Accelerator HGX-1 March 8 2017.pdf
[25] Broadcom. PEX9797. 97 lane, 25 port, PCI Express Gen3 ExpressFabric
Platform. https://www.broadcom.com/products/pcie-switches-bridges/
expressfabric/pex9797
[26] Avago’s PEX9700 turns the PLX PCIe3 switch into a fabric. May 12,
2015 by Charlie Demerjian. https://semiaccurate.com/2015/05/12/avagos
-pex9700-turns-plx-pcie3-switch-fabric/
[27] Avago Announces PLX PEX9700 Series PCIe Switches: Focusing on
Data Center and Racks. by Ian Cutress on May 12, 2015. https://www.
anandtech.com/show/9245/avago-announces-plx-pex9700-series-pcie-
switches
[28] Peter X. Gao and Akshay Narayan and Sagar Karandikar and Joao
Carreira and Sangjin Han and Rachit Agarwal and Sylvia Ratnasamy and
Scott Shenker. Network Requirements for Resource Disaggregation. 12th
USENIX Symposium on Operating Systems Design and Implementation
(OSDI 16), pages 249-264, Savannah, GA, 2016. USENIX Association.
https://www.usenix.org/system/ﬁles/conference/osdi16/osdi16-gao.pdf
[29] Huawei. High Throughput Computing Data Center Architecture,
Thinking of Data Center 3.0. Technical White Paper. June, 2014.
https://www.huawei.com/ilink/en/download/HW 349607
[30] EDP Europe. Infrasolution rack access control. https://www.edpeurope.
com/product/rack-accesscontrol/.
[31] EDP Europe. iaccess it cabinet security system. https://www.edpeurope.
com/product/iaccesscomputer-cabinet-door-security-access-
controlsystem/.
[32] EDP Europe. Biometric swipe access card. https://www.edpeurope.
com/product/biometric-swipe-access-card.
[33] Southco. Rack-level security. http://lp.southco.com/rs/southco/images/Ra
ck%20Level%20Security%20Brochure.pdf.
[34] Gary Smolker and Leon Chernick. Method and apparatus for com-
bustibly destroying microelectronic circuit board interconnections, May
6 1975. US Patent 3,882,324.
[35] Open Neural Network Exchange Format, The open ecosystem for
interchangeable AI models. https://onnx.ai
[36] Open Neural Network Exchange (ONNX). Tutorials for creating and
using ONNX models. https://github.com/onnx/tutorials
[37] Dean, Jeffrey and Barroso, Luiz Andre. The tail at scale. Communica-
tions of The ACM 2013. pages 74-80.
[38] Cong, Jason and Ghodrat, Mohammad Ali and Gill, Michael and Grigo-
rian, Beayna and Gururaj, Karthik and Reinman, Glenn. Accelerator-
Rich Architectures: Opportunities and Progresses. pages 1-6, 2014.
[39] Barroso, Luiz Andre and Clidaras, Jimmy and Holzle, Urs. The Datacen-
ter as a Computer: An Introduction to the Design of Warehouse-Scale
Machines, Second edition. Synthesis Lectures on Computer Architec-
ture, pages 1-154, 2013.
[40] Chen, Quan and Yang, Hailong and Mars, Jason and Tang, Lingjia.
Baymax: QoS Awareness and Increased Utilization for Non-Preemptive
Accelerators in Warehouse Scale Computers. Operating Systems Review
2016, pages 681-696.
[41] Jain, Paras and Mo, Xiangxi and Jain, Ajay and Subbaraj, Harikaran
and Durrani, Rehan Sohail and Tumanov, Alexey and Gonzalez, Joseph
E and Stoica, Ion. Dynamic Space-Time Scheduling for GPU In-
ference. arXiv: Distributed, Parallel, and Cluster Computing, 2019.
https://arxiv.org/pdf/1901.00041.pdf
[42] Zhichao Hua and Jinyu Gu and Yubin Xia and Haibo Chen
and Binyu Zang and Haibing Guan. vTZ: Virtualizing ARM
TrustZone. 26th USENIX Security Symposium (USENIX Security
17), pages 541-556, Vancouver, BC, 2017. USENIX Association.
https://www.usenix.org/system/ﬁles/conference/usenixsecurity17/sec17-
hua.pdf
[43] Caffe. Yangqing Jia. http://caffe.berkeleyvision.org
[44] Caffe: a fast open framework for deep learning. https://github.com/
BVLC/caffe/
[45] PyTorch. FROM RESEARCH TO PRODUCTION. An open source
deep learning platform that provides a seamless path from research
prototyping to production deployment. https://pytorch.org
[46] PyTorch. Tensors and Dynamic neural networks in Python with strong
GPU acceleration. https://github.com/pytorch/pytorch
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:57:53 UTC from IEEE Xplore.  Restrictions apply. 
1463
[47] PCI-SIG SR-IOV Primer: An Introduction to SR-IOV Technology,
Revision 2.5, January 2011. https://www.intel.com/content/dam/doc/
application-note/pci-sig-sr-iov-primer-sr-iov-technology-paper.pdf
[48] SR-IOV Architecture. 04/20/2017, Duncan MacMichael. https://docs.
microsoft.com/en-us/windows-hardware/drivers/network/sr-iov-
architecture
[49] Michael Krause (HP, co-chair), Renato Recio (IBM, co-chair). PCIe I/O
virtualization and sharing. PCI-SIG 2006.
http://weblab.cs.uml.edu/ bill/cs520/slides 15D PCI Express IOV.pdf
http://weblab.cs.uml.edu/ bill/cs520/slides 15E PCI Express IOV.pdf
[50] Using PCI express as the primary system interconnect
in mul-
tiroot compute storage communications and embedded systems (
White Paper). 2008. https://www.idt.com/document/whp/idt-pcie-multi-
root-white-paper
[51] SGD. Stochastic gradient descent. https://en.wikipedia.org/wiki/Stochasti
c gradient descent
[52] Diederik P. Kingma, Jimmy Ba. Adam: A Method for Stochastic
Optimization. 3rd International Conference for Learning Representations
(ICLR2015), San Diego, 2015. https://arxiv.org/abs/1412.6980
[53] ImageNet. Large
Scale Visual Recognition Challenge,
2012
(ILSVRC2012). http://image-net.org/challenges/LSVRC/2012/
[54] VGG16. Karen Simonyan and Andrew Zisserman. Very deep convolu-
tional networks for large-scale image recognition. Computer Science,
2014.
[55] GoogLeNet. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,
and Andrew Rabinovich. Going deeper with convolutions. 2014.
[56] ResNet. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learningfor
image recognition. In 2016 IEEE Conference on Computer Vision and