title:Dirty Road Can Attack: Security of Deep Learning based Automated Lane
Centering under Physical-World Attack
author:Takami Sato and
Junjie Shen and
Ningfei Wang and
Yunhan Jia and
Xue Lin and
Qi Alfred Chen
Dirty Road Can Attack: Security of Deep Learning 
based Automated Lane Centering under 
Physical-World Attack
Takami Sato, Junjie Shen, and Ningfei Wang, University of California, Irvine; 
Yunhan Jia, ByteDance; Xue Lin, Northeastern University; Qi Alfred Chen, 
University of California, Irvine
https://www.usenix.org/conference/usenixsecurity21/presentation/sato
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Dirty Road Can Attack: Security of Deep Learning based Automated Lane
Centering under Physical-World Attack
Takami Sato∗
UC Irvine
PI:EMAIL
Junjie Shen∗
UC Irvine
PI:EMAIL
Xue Lin
Ningfei Wang
UC Irvine
PI:EMAIL
Yunhan Jia
ByteDance
PI:EMAIL
Northeastern University
PI:EMAIL
Qi Alfred Chen
UC Irvine
PI:EMAIL
Abstract
Automated Lane Centering (ALC) systems are convenient and
widely deployed today, but also highly security and safety crit-
ical. In this work, we are the ﬁrst to systematically study the
security of state-of-the-art deep learning based ALC systems
in their designed operational domains under physical-world
adversarial attacks. We formulate the problem with a safety-
critical attack goal, and a novel and domain-speciﬁc attack
vector: dirty road patches. To systematically generate the at-
tack, we adopt an optimization-based approach and overcome
domain-speciﬁc design challenges such as camera frame inter-
dependencies due to attack-inﬂuenced vehicle control, and the
lack of objective function design for lane detection models.
We evaluate our attack on a production ALC using 80 sce-
narios from real-world driving traces. The results show that
our attack is highly effective with over 97.5% success rates
and less than 0.903 sec average success time, which is sub-
stantially lower than the average driver reaction time. This
attack is also found (1) robust to various real-world factors
such as lighting conditions and view angles, (2) general to
different model designs, and (3) stealthy from the driver’s
view. To understand the safety impacts, we conduct experi-
ments using software-in-the-loop simulation and attack trace
injection in a real vehicle. The results show that our attack can
cause a 100% collision rate in different scenarios, including
when tested with common safety features such as automatic
emergency braking. We also evaluate and discuss defenses.
1 Introduction
Automated Lane Centering (ALC) is a Level-2 driving au-
tomation technology that automatically steers a vehicle to
keep it centered in the trafﬁc lane [1]. Due to its high con-
venience for human drivers, today it is widely available on
various vehicle models such as Tesla, GM Cadillac, Honda
Accord, Toyota RAV4, Volvo XC90, etc. While convenient,
such system is highly security and safety critical: When the
ALC system starts to make wrong steering decisions, the hu-
man driver may not have enough reaction time to prevent
∗Co-ﬁrst authors.
safety hazards such as driving off road or colliding into ve-
hicles in adjacent lanes. Thus, it is imperative and urgent to
understand the security property of ALC systems.
In an ALC system, the most critical step is lane detection,
which is generally performed using a front camera. So far,
Deep Neural Network (DNN) based lane detection achieves
the highest accuracy [2] and is adopted in the most performant
production ALC systems today such as Tesla Autopilot [3].
Recent works show that DNNs are vulnerable to physical-
world adversarial attacks such as malicious stickers on traf-
ﬁc signs [4, 5]. However, these methods cannot be directly
applied to attack ALC systems due to two main design chal-
lenges. First, in ALC systems, the physical-world attack gen-
eration needs to handle inter-dependencies among camera
frames due to attack-inﬂuenced vehicle actuation. For exam-
ple, if the attack deviates the detected lane to the right in a
frame, the ALC system will steer the vehicle to the right ac-
cordingly. This causes the following frames to capture road
areas more to the right, and thus directly affect their attack
generation. Second, the optimization objective function de-
signs in prior works are mainly for image classiﬁcation or
object detection models and thus aim at changing class or
bounding box probabilities [4, 5]. However, attacking lane
detection requires to change the shape of the detected trafﬁc
lane, making it difﬁcult to directly apply prior designs.
The only prior effort that studied adversarial attacks on a
production ALC is from Tencent [6], which fooled Tesla Au-
topilot to follow fake lane lines created by white stickers on
road regions without lane lines. However, it is neither attack-
ing the designed operational domain for ALC, i.e., roads with
lane lines, nor generating the perturbations systematically by
addressing the design challenges above.
To ﬁll this critical research gap, in this work we are the
ﬁrst to systematically study the security of DNN-based ALC
systems in their designed operational domains (i.e., roads with
lane lines) under physical-world adversarial attacks. Since
ALC systems assume a fully-attentive human driver prepared
to take over at any time [1, 7], we identify the attack goal
as not only causing the victim to drive out of the current
USENIX Association
30th USENIX Security Symposium    3309
lane boundaries, but also achieving it shorter than the average
driver reaction time to road hazard. This thus directly breaks
the design goal of ALC systems and can cause various types of
safety hazards such as driving off road and vehicle collisions.
Targeting this attack goal, we design a novel physical-world
adversarial attack method on ALC systems, called DRP (Dirty
Road Patch) attack, which is the ﬁrst to systematically ad-
dress the design challenges above. First, we identify dirty
road patches as a novel and domain-speciﬁc attack vector
for physical-world adversarial attacks on ALC systems. This
design has 2 unique advantages: (1) Road patches can appear
to be legitimately deployed on trafﬁc lanes in the physical
world, e.g., for ﬁxing road cracks; and (2) Since it is common
for real-world roads to have dirt or white stains, using similar
dirty patterns as the input permutations can allow the mali-
cious road patch to appear more normal and thus stealthier.
With this attack vector, we then design systematic mali-
cious road patch generation following an optimization-based
approach. To efﬁciently and effectively address the ﬁrst design
challenge without heavyweight road testing or simulations, we
design a novel method that combines vehicle motion model
and perspective transformation to dynamically synthesize
camera frame updates according to attack-inﬂuenced vehicle
control. Next, to address the second design challenge, one
direct solution is to design the objective function to directly
change the steering angle decisions. However, we ﬁnd that the
lateral control step in ALC that calculates steering angle deci-
sions are generally not differentiable, which makes it difﬁcult
to effectively optimize. To address this, we design a novel
lane-bending objective function as a differentiable surrogate
function. We also have domain-speciﬁc designs for attack
robustness, stealthiness, and physical-world realizability.
We evaluate our attack method on a production ALC sys-
tem in OpenPilot [8], which is reported to have close perfor-
mance to Tesla Autopilot and GM Super Cruise, and better
than many others [9]. We perform experiments on 80 attack
scenarios from real-world driving traces, and ﬁnd that our
attack is highly effective with over 97.5% success rates for
all scenarios, and less than 0.903 sec average success time,
which is substantially lower than 2.5 sec, the average driver
reaction time (§3.1). This means that even for a fully-attentive
driver who can take over as soon as the attack starts to take
effect, the average reaction time is still not enough to prevent
the damage. We further ﬁnd this attack is (1) robust to real-
world factors such as different lighting conditions, viewing
angles, printer color ﬁdelity, and camera sensing capability,
(2) general to different lane detection model designs, and (3)
stealthy from the driver’s view based on a user study.
To understand the potential safety impacts, we further con-
duct experiments using (1) software-in-the-loop simulation
in a production-grade simulator, and (2) attack trace injec-
tion in a real vehicle. The simulation results show that our
attack can successfully cause a victim running a produc-
tion ALC to hit the highway concrete barrier or a truck in
the opposite direction with 100% success rates. The real-
vehicle experiments show that it causes the vehicle to col-
lide with (dummy) road obstacles in all 10 trials even with
common safety features such as Automatic Emergency Brak-
ing (AEB) enabled. Demo videos are available at: https:
//sites.google.com/view/cav-sec/drp-attack/. We also ex-
plore and discuss possible defenses at DNN level and those
based on sensor/data fusion.
In summary, this work makes the following contributions:
• We are the ﬁrst to systematically study the security of
DNN-based ALC in the designed operational domains
under physical-world adversarial attacks. We formulate
the problem with a safety-critical attack goal, and a novel
and domain-speciﬁc attack vector, dirty road patches.
• To systematically generate attack patches, we adopt an
optimization-based approach with 2 major novel and do-
main speciﬁc designs: motion model based input genera-
tion, and lane-bending objective function. We also have
domain-speciﬁc designs for improving the attack robust-
ness, stealthiness, and physical-world realizability.
• We perform evaluation on a production ALC using 80
attack scenarios from real-world driving traces. The re-
sults show that our attack is highly effective with ≥97.5%
success rates and ≤0.903 sec average success time, which
is substantially lower than the average driver reaction time.
This attack is also found (1) robust to various real-world
factors, (2) general to different lane detection model de-
signs, and (3) stealthy from the driver’s view.
• To understand the safety impacts, we conduct experiments
using (1) software-in-the-loop simulation, and (2) attack
trace injection in a real vehicle. The results show that
our attack can cause a 100% collision rate in different
scenarios, including when tested with safety features such
as AEB. We also evaluate and discuss possible defenses.
Code and data release. Our code and data for the attack
and evaluations are available at our project website [10].
2 Background
2.1 Overview of DNN-based ALC Systems
Fig. 1 shows an overview of a typical ALC system design [8,
11, 12], which operates in 3 steps:
Lane Detection (LD). Lane detection (LD) is the most
critical step in an ALC system, since the driving decisions
later are mainly made based on its output. Today, produc-
tion ALC systems predominately use front cameras for this
step [3, 13]. On the camera frames, an LD model is used to
detect lane lines. Recently, DNN-based LD models achieve
the state-of-the-art accuracy [14–16] and thus are adopted
in the most performant production ALC systems today such
as Tesla Autopilot [3]. Since lane line shapes do not change
much across consecutive frames, recurrent DNN structure
(e.g., RNN) is widely adopted in LD models to achieve more
stable prediction [8, 17, 18]. LD models typically ﬁrst predict
the lane line points, and then post-process them to lane line
3310    30th USENIX Security Symposium
USENIX Association
Figure 1: Overview of the typical ALC system design.
curves using curve ﬁtting algorithms [14, 15, 19, 20].
Before the LD model is applied, a Region of Interest (ROI)
ﬁltering is usually performed to the raw camera frame to crop
the most important area out of it (i.e., the road surface with
lane lines) as the model input. Such ROI area is typically
around the center and much smaller than the original frame,
to improve the model performance and accuracy [21].
Lateral control. This step calculates steering angle deci-
sions to keep the vehicle driving at the center of the detected
lane. It ﬁrst computes a desired driving path, typically at the
center of the detected left and right lane lines [22]. Next, a con-
trol loop mechanism, e.g., Proportional-Integral-Derivative
(PID) [23] or Model Predictive Control (MPC) [24], is applied
to calculate the optimal steering angle decisions that can fol-
low the desired driving path as much as possible considering
the vehicle state and physical constraints.
Vehicle actuation. This step interprets the steering angle
decision into actuation commands in the form of steering an-
gle changes. Here, such actuated changes are limited by a max-
imum value due to the physical constraints of the mechanical
control units and also for driving stability and safety [22]. For
example, in our experiments with a production ALC with 100
Hz control frequency, such limit is 0.25◦ per control step (ev-
ery 10 ms) for vehicle models [25]. As detailed later in §3.3,
such a steering limit prevents ALC systems from being af-
fected too much from successful attack in one single LD
frame, which introduces a unique challenge to our design.
2.2 Physical-World Adversarial Attacks
Recent works ﬁnd that DNN models are generally vulnerable
to adversarial examples, or adversarial attacks [26, 27]. Some
works further explored such attacks in the physical world [4,5,
28–31]. While these prior works concentrate on DNN models
for image classiﬁcation and object detection tasks, we are
the ﬁrst to systematically study such attacks on production
DNN-based ALC systems, which requires to address several
new and unique design challenges as detailed later in §3.3.
3 Attack Formulation and Challenge
3.1 Attack Goal and Incentives
In this paper, we consider an attack goal that directly breaks
the design goal of ALC systems: causing the victim vehicle
a lateral deviation (i.e., deviating to the left or right) large
enough to drive out of the current lane boundaries. Mean-
while, since ALC systems assume a fully-attentive human
driver who is prepared to take over at any moment [1,7], such
deviation needs to be achieved fast enough so that the human
driver cannot react in time to take over and steer back. Table 1
Table 1: Required deviations and success time for successful
attacks on ALC systems on highway and local roads. Detailed
calculations and explanations are in Appendix A.
Road Type
Highway
Local road
<2.5 seconds (average driver
reaction time to road hazard)
Required Lateral Deviation
Required Success Time
0.735 meters
0.285 meters
shows concrete values of these two requirements for success-
ful attacks on highway and local roads respectively, which
will be used as evaluation metrics later in §5. In the table,
the required deviations are calculated based on representative