embedding or metric.
To better see the advantage of the triplet network approach,
note that DeepCorr learns a pairwise combination of Tor and
exit ﬂow features and thus must evaluate a costly DNN on
every pair of ﬂows (cost of N 2 DNN runs). In contrast, we
develop feature embedding networks to learn two types of
embeddings, namely Tor and exit ﬂow embeddings, extending
ideas from prior work in website ﬁngerprinting attacks [28].
This architecture learns a pair of functions to generate entry
ﬂow and exit ﬂow embeddings separately, and both functions
are applied to each ﬂow, which costs just two DNN evaluations
per ﬂow (cost of 2N DNN runs). The pairwise comparisons
can then be done much less expensively using simple distance
functions in the embedding space.
The key advantage of ampliﬁcation, an idea borrowed from
randomized algorithms [27], comes from the ability to conduct
multiple, partially independent tests of correlation on each pair
of ﬂows. To do this, we divide ﬂows into a sequence of k short
time segments, or windows. We then extract k embeddings per
ﬂow (modestly raising our computational cost by a factor of k),
and conduct step-by-step pairwise comparisons of Tor and exit
ﬂows. Correctly matched ﬂows should be correlated in nearly
all k windows, while mismatched ﬂows are likely to only be
correlated for at most some of the windows. This windowing
approach thus ampliﬁes the difference between true positives
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1916
State-of-the-Art Attacks. In more recent work focusing on
passive correlation attacks on Tor, Sun et al. [17] presented the
RAPTOR attack, which exploited asymmetric trafﬁc analysis
to monitor both ends of a higher fraction of Tor connections.
They measured the correlation between 50 pairs of ﬂows, each
consisting of 300 seconds of trafﬁc, using Spearman’s rank
correlation algorithm. They found that such previously-studied
statistical correlation metrics suffered from a scalability issue,
because a long sample of ﬂow data was necessary to yield
acceptable results.
Further
investigating this
issue, Nasr, Bahramali and
Houmansadr [25] later described a new ﬂow correlation metric,
DeepCorr, which was trained speciﬁcally for Tor ﬂows using
deep learning. To evaluate DeepCorr, they also collected the
largest and most comprehensive correlated ﬂow dataset appear-
ing in the literature, using live Tor network trafﬁc with varying
circuits and in different
time periods. Although DeepCorr
achieved much higher TPR and lower FPR compared to
RAPTOR using only 100 packets of ﬂow data, their models
yielded a low BDR. For example, based on our experiments
using 2,093 testing ﬂow pairs, DeepCorr achieved 81.7% TPR
and 0.16% FPR; however, the BDR was 19.8%.
The DeepCorr approach also has signiﬁcant
limitations
that might lead skeptics to discount the results of the paper.
First, the computational complexity of conducting a correlation
attack on a popular network like Tor using DeepCorr is high,
since the DeepCorr network operates on pairs of ﬂows to
estimate their correlation, so it must be evaluated on N × N
pairs of ﬂows. Additionally, because the metric learns which
features are best to predict correlated ﬂows under the current
network conditions, it must be re-trained every 3-4 weeks.
B. Embeddings and Triplet Networks
Schroff et al. [26] proposed a face recognition system, called
FaceNet, based on CNNs. Rather than directly comparing
high-dimensional images of faces, FaceNet maps an image
to a lower-dimensional vector (which we call a feature em-
bedding) so that two images of the same person have very
similar embeddings, while images of different people have
embeddings with lower similarity. More speciﬁcally, FaceNet
attempts to train a CNN with a “triplet” loss function that
should minimize the Euclidean distance between embeddings
of images of the same person and maximize the distance
between the embedding of images of different people. Thus,
we call this DNN training model a triplet network.
FaceNet is trained by having three copies of the embedding
network – called the Anchor (A), Positive (P), and Negative
(N) CNNs – with jointly trained and shared weights, namely a
uniﬁed embedding. Each input to this network consists of three
images: A, P, and N triplets. A and P are always selected from
the same person class (i.e., positive pair) while A and N are
chosen from different people classes (i.e., negative pair). To
select the optimal triplets for training, researchers used several
triplet mining methods: random, hard-negative, and semi-hard-
negative. Hard negative samples mean N triplets that are more
likely to be close to A triplets.
The objective function in their work aims to ensure a
minimum margin or gap α between the L2 distance between A
and P embeddings and the L2 distance between A and N em-
beddings. That is, (cid:107)f (A) − f (P )(cid:107)2
2+α  1 AND q < 1).
Time
Algorithm
Space
CTA
O(N L/R)
O(N L/R) O(N (1+q)L/R)
O(N 2L)
DeepCoFFEA O(N L/R)
CTA+LSH
DeepCorr
O(N 2L/R)
O(N 2L)
O(N 2L/R)
Thus, we implement a modiﬁed triplet generator, detailed in
Section IV-A. Eventually, using the triplet loss, DeepCoFFEA
trains these embedding networks towards maximizing the
similarity between a and p while minimizing the similarity
(i.e. maximizing the difference) between a and n.
Thus, even though DeepCoFFEA computes the pairwise
cosine similarity of n2 embeddings, it only needs to evaluate
2n FENs. This signiﬁcantly reduces the complexity of corre-
lation attacks, because the low-dimensional cosine similarity
computation between embedding pairs is much less expensive
than the deep CNN computation on full ﬂow pairs.
Comparison to CTA. The use of dimensionality reduction
to improve the efﬁciency of ﬂow correlation was also a
central idea in Compressive Trafﬁc Analysis (CTA) [29]. To
efﬁciently correlate ﬂows that had been perturbed by network
noise, rather than routing through Tor, Nasr, Houmansadr, and
Mazumdar applied a Gaussian random projection algorithm
to compress IPD feature vectors into ﬁxed-dimension cosine-
similar embeddings and then used Locality Sensitive Hashing
(LSH) to further reduce the pairwise correlation cost. For N
ﬂows with L packets, the sensing basis matrix, ΦF×L in which
F < L led to F -dimensional feature vectors, which reduced
F . Fur-
the both time and space complexity by a factor of L
thermore, they extended CTA with LSH to avoid comparing
some pairs of embeddings, decreasing the comparison cost
from O(N 2F ) to O(N (1+q)F ), where 1 + q < 2.
FENs also generate lower-dimensional feature embedding
vectors, which reduces the complexity by a factor of L
F . Note
that we empirically chose F in Section V-C. We note that it
might be possible to extend DeepCoFFEA to apply an adapted
version of LSH to avoid some comparisons between pairs of
embeddings, but we leave this for future work.
We summarize the complexity comparison of CTA, Deep-
Corr, and DeepCoFFEA in Table I. In Section VI-C, we
also evaluate the accuracy of CTA (without LSH, so we
maximize the TPR for a given FPR) using the implementation
by Nasr, Houmansadr, and Mazumdar [29] and show that when
applied to Tor/Exit ﬂow pairs, CTA does not produce usefully
correlated results.
Ampliﬁcation. To reduce the number of FPs, we adapt the
concept of ampliﬁcation from randomized algorithms [27], in
which a randomized decision procedure that has any signif-
icant gap between acceptance probabilities for positive and
negative cases can be repeated multiple times to create a
decision procedure with exponentially small false positive and
false negative rates. In the context of DeepCoFFEA, we apply
ampliﬁcation by window partitioning,
in which we divide
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1918
the ﬂow into several smaller partially-overlapping subﬂows
(windows). Then, we evaluate each window separately and
aggregate the results using voting in an ensemble fashion.
By dividing t and x into k discrete windows, and computing
the similarity between G and H on the subﬂows in each
window, we end up with k-dimensional vectors comprised of
1s if the subﬂows are correlated and 0s otherwise. These act as
votes from k windows, which we then aggregate to determine
the ﬁnal decision: if at least k − (cid:96) votes are positive, the
ﬂows are correlated, and otherwise they are uncorrelated. For
example, if k = 5 and (cid:96) = 1, and the votes for a given ﬂow
pair are [1,1,1,0,1], the pair is predicted as correlated.
By adjusting k and (cid:96) for a given anticipated ﬂow set size n,
we can push the false positive rate below the base rate of 1/n,
giving a BDR that does not trend asymptotically to 0.
Compared to DeepCorr [25], instead of learning a compari-
son metric d(t, x), the adversary computes G(t) for every Tor
trace and H(x) for every exit trace in k successive windows.
We expect only one exit trace, xj, to line up with the same Tor
trace, ti, with at least k− (cid:96) 1 votes. We show in Section VI-C
that ampliﬁcation can signiﬁcantly reduce the number of
FPs against a pairwise cosine similarity computation, and
consequently, DeepCoFFEA becomes more effective with a
much lower FPR and higher BDR than the state-of-the-art
correlation techniques.
Furthermore, our models do not
learn based on labels;
rather,
they learn statistical differences between correlated
and uncorrelated ﬂow pairs, leading to more effective feature
extraction as well as a more generalized model. Notably,
our empirical study in Section VI-B shows the robustness of
DeepCoFFEA against padding-based defenses.