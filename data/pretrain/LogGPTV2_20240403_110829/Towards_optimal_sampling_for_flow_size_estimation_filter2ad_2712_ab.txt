b12
b22
b1W
b2W
···
···
...
···
then noticing that the W × W matrix 1
0 has rank 1, we can
use Theorem 7.7 from the Appendix (see [9] for more details) to
show
b0bT
bW W
d0
...
...
...
0
0
J((cid:126)θ) =
1
d0
b0bT
0 + ˜J((cid:126)θ)
(8)
−1
where ˜J((cid:126)θ) = ˜BT ˜D((cid:126)θ) ˜B, and ˜D((cid:126)θ) = diag(d
W ).
Since ˜B and ˜D are both square, there is greater hope for an explicit
inverse from (8) than from (6). In fact, since each term in (8) is a
positive semideﬁnite matrix, we can prove
−1
1 , . . . , d
PROPOSITION 2.2. The inverse of J((cid:126)θ) is given by
−1((cid:126)θ) = ˜J
−1((cid:126)θ)b0bT
˜J
J
−1((cid:126)θ) −
0
1
˜J−1((cid:126)θ)b0
d0 + bT
0
−1((cid:126)θ).
˜J
PROOF. By (8), J((cid:126)θ) can be written as a sum of a single rank
matrix and a symmetric, positive deﬁnite, nonsingular matrix ˜J−1((cid:126)θ)
(Lemma 7.2). In this case, the single rank matrix is positive semidef-
inite since it is a product of two vectors. Using Lemma 7.6 J((cid:126)θ) ≥
˜J((cid:126)θ). Since ˜J((cid:126)θ) is symmetric, positive deﬁnite, this implies J((cid:126)θ) >
0W×W , which implies nonsingularity. This permits the use of The-
˜J−1((cid:126)θ)) (cid:54)= −d0. Since ˜J−1((cid:126)θ) is
orem 7.7 provided tr(b0bT
0
symmetric, positive deﬁnite,
tr(b0bT
0
−1((cid:126)θ)) = bT
˜J
0
−1((cid:126)θ)b0 > 0 > −d0.
˜J
The expression then follows from Theorem 7.7.
Again, this explicit inverse, valid for any general sampling matrix
B, is made possible by the very simple form of (4).
The matrix ˜J corresponds to the information carried by the out-
comes 1 ≤ j ≤ W only. We expect J to carry more information
through the knowledge of Nf which gives access to j = 0, and
therefore J−1 (corresponding to the CRLB) to have reduced vari-
ance (read, uncertainty). The following result conﬁrms this intu-
ition (proof in Appendix).
THEOREM 2.3. An upper bound for J−1((cid:126)θ) is
−1((cid:126)θ) ≤ ˜J
J
−1((cid:126)θ).
The reduction in uncertainty is given by the second term in the
expression for J−1((cid:126)θ) in Proposition 2.2.
The Constrained Fisher Information and CRLB
Intuitively, constraints on the parameters should increase the Fisher
information since they tell us something more about them, ‘for
free’.
In fact [10] shows that this is only true for equality con-
straints. Since are assuming that 0 < θk < 1, the only active
constraint is
(cid:80)W
(cid:80)W
j=1 θj − 1.
k=1 θk = 1. Its gradient is
G((cid:126)θ) = ∇(cid:126)θg((cid:126)θ)
where g((cid:126)θ) =
(9)
The inverse constrained Fisher information [10] is
−1
−1G
I + = J
−1 − J
−1G
GTJ
GTJ
(10)
where I + denotes the Moore-Penrose pseudo-inverse [11, Chap-
ter 20, pp. 493-514] of the constrained Fisher information matrix
I. The matrix I + is rank W − 1 due to the single equality con-
straint and is thus singular (see [10, Remark 2]). This somewhat
formidable expression can be simpliﬁed in our case, as we now
show.
(cid:180)−1
(cid:179)
LEMMA 2.4. J diag(θ1, . . . , θW )1W = 1W .
PROOF. The row sum of row i of J diag(θ1, . . . , θW ) is
W(cid:88)
(cid:80)W
k=1
bjkθk =
W(cid:88)
j=0
W(cid:88)
j=0
(cid:162)
since B is column stochastic. Since
lows.
k=1 θk = 1 the result fol-
(cid:161)
It is easy to see that G = 1W . Hence J−1G = J−11W =
diag(θ1, . . . , θW )1W from Lemma 2.4. It is then straightforward
to verify that
is simply the number 1, and further that
(10) can be reduced to
GTJ−1G
I + = J
−1 − (cid:126)θ(cid:126)θT.
(11)
The remarkable thing here is that the constraint term (cid:126)θ(cid:126)θT depends
on (cid:126)θ only, and so is constant for all sampling matrices B, a great
advantage when comparing different methods.
Since we are assuming ﬂows are sampled independently, the
Fisher information for N ﬂows is just N J, and the inverse becomes
I +/N. For any unbiased estimator ˆθ of (cid:126)θ, the CRLB then states
that
E[(ˆθ − (cid:126)θ)(ˆθ − (cid:126)θ)T] ≥ I +((cid:126)θ)
Because of independence we study N = 1. In practice all ﬂows
are sampled and so N = Nf .
3. THE SAMPLING METHODS
In this section we deﬁne the sampling methods we consider and
derive their main properties.
W(cid:88)
W(cid:88)
k=1
j=0
bjibjk
dj
θk =
bji
dj
bji
dj
dj = 1
B =
3.1 The Sampling Matrices
The methods described here include simple packet and ﬂow sam-
pling, as well as others exploiting protocol information, in particu-
lar those proposed in [1, 4]. Apart from their inherent interest, we
revisit these because in the conditional framework these methods
are now all different to before, sometimes surprisingly so. We also
derive inverses analytically which has not been possible before.
To better see the connection between the usual framework and
ours, recall that bj,k is always a conditional probability with respect
to the size k of the original ﬂow. Typically however, it is also made
conditional with respect to j, but we do not so here. Hence, if Bc is
the usual j-conditional matrix, then BcC = ˜B where C = IW −
diag(b01, . . . , b0W ), i.e. the matrix C−1 does the conditioning.
We use the decomposition of (7) to describe each sampling ma-
trix B. In each case we deﬁne B and ˜B, give the inverse ˜B−1 of
˜B, and give explicit expressions for the diagonal terms (J−1)jj,
or in some case for the entire inverse J−1. The importance of the
diagonal terms will become very clear in Section 4. Proofs will be
placed in the Appendix, or if omitted, can be found in [9].
Packet Sampling (PS)
By this we mean the simplest form of sampling, i.i.d. packet sam-
pling, where each packet is retained independently with probability
pp and otherwise dropped with qp = 1 − pp.
The chief beneﬁt of PS is its simplicity, and the fact that it can
be implemented at high speed because a sampling decision can be
made without even inspecting the packet. The chief disadvantage
is the fact that it has a strong length bias, small ﬂows are very likely
to evaporate.
(cid:195)
(cid:33)
It is easy to see that bjk =
pqk−j
pj
p
, or
k
j
qp
pp
0
...
0
q2
p
q3
p
q4
p
2ppqp
3ppq2
p
4ppq3
p
···
···
p2
p
...
0
3p2
pqp
...
0
6p2
pq2
p
...
···
···
...
0
(cid:195)
(cid:195)
W
1
W
2
(cid:33)
(cid:33)
qW
p
ppqW−1
p
p
pqW−2
p2
...
pW
p
(cid:195)
(cid:33)
k
j
Using a result on binomial matrices [12] we can show that the in-
verse has elements b(cid:48)
jk = (−1)k−j
p−k
p , or
qk−j
p
−1 =
˜B
p−1
p
0
...
0
−2p−2
p qp
p−2
...
p
0
···
···
...
···
···
···
...
···
(−1)W−1W p−W
p
...
...
p−W
p
qW−1
p
For PS J−1 is difﬁcult to write in a compact form and will be omit-
ted. It is however feasible to give
 .
(cid:195)
(cid:33)2
j
k
j(cid:88)
k=1
.
(12)
−1)jj =
(˜J
N
q2(j−k)
p
−2j
p
.
p
(13)
Packet Sampling with Sequence Numbers (PS+SEQ)
First PS with parameter pp is performed as above. Sequence num-
bers are then used as follows. Let sl be the lowest sequence number
among the sampled packets, and sh the highest. All packets inbe-
tween these can now reliably inferred, hence j = sh − sl + 1 is
the number of sampled packets returned. This is called “ALL-seq-
sﬂag” in [4].
The chief beneﬁt of PS+SEQ is the fact that a potentially large
number of packets can be ‘virtually’ observed without having to
physically sample them. The disadvantage is the additional pro-
cessing involved to perform the inference. Also, the technique does
not really work if ﬂows are too short (as we discuss later).
If j = 0, 1 then sequence numbers cannot help and bjk is as for
PS. Otherwise, note that the j packets must occur in a contiguous
block bordered by sl and sh. There are k− j + 1 possible positions
for such a block, each characterized by k − j unsampled packets
outside it and the borders sl and sh. It follows that bjk = (k − j +
1)p2
···
···
···
···
...
0
W ppqW−1
p
qW
p
(W − 1)p2
(W − 2)p2