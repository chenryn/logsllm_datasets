gaze to enhance typing privacy,” in IEEE INFOCOM, Atlanta, GA, May
2017.
[36] A. Al-Haiqi, M. Ismail, and R. Nordin, “The eye as a new side
channel threat on smartphones,” in IEEE SCORed, Putrajaya, Malaysia,
December 2013.
“Structure of human eye,” https://en.wikipedia.org/wiki/Human_eye.
[37]
[38] D. Hansen and Q. Ji, “In the eye of the beholder: A survey of models
for eyes and gaze,” IEEE Trans. PAMI, vol. 32, no. 3, pp. 478–500,
March 2010.
[39] Z. Zhu and Q. Ji, “Novel eye gaze tracking techniques under natural
head movement,” IEEE Trans. BME, vol. 54, no. 12, pp. 2246–2260,
December 2007.
[40] G. Ye, Z. Tang, D. Fang, X. Chen, K. Kim, B. Taylor, and Z. Wang,
“Cracking Android pattern lock in ﬁve attempts,” in ISOC NDSS, San
Diego, CA, February 2017.
[41] P. Viola and M. Jones, “Rapid object detection using a boosted cascade
of simple features,” in IEEE CVPR, Kauai, HI, December 2001.
[42] F. Timm and E. Barth, “Accurate eye centre localisation by means of
gradients.” in VISAPP, Algarve, Portugal, March 2011.
J. Daugman, “High conﬁdence visual recognition of persons by a test
of statistical independence,” IEEE Trans. PAMI, vol. 15, no. 11, pp.
1148–1161, November 1993.
J. Wang, E. Sung, and R. Venkateswarlu, “Eye gaze estimation from a
single image of one eye,” in IEEE ICCV, Nice, France, October 2003.
´Swirski, A. Bulling, and N. Dodgson, “Robust real-time pupil
tracking in highly off-axis images,” in ACM ETRA, Santa Barbara, CA,
March 2012.
[43]
[44]
[45] L.
[46] M. Kumar, J. Klingner, R. Puranik, T. Winograd, and A. Paepcke,
“Improving the accuracy of gaze input for interaction,” in ACM ETRA,
Savannah, GA, March 2008.
“corn-cob dictionary,” http://www.mieliestronk.com/wordlist.html.
“Trie data structure,” https://en.wikipedia.org/wiki/Trie.
“120
http://www.research.lancs.ac.uk/portal/ﬁles/138568011/Patterns.pdf.
[47]
[48]
[49]
keyboard,”
patterns
for
pattern
lock
[50] D. Ping, X. Sun, and B. Mao, “Textlogger: inferring longer inputs on
touch screen using motion sensors,” in ACM WiSec, New York, NY,
June 2015.
157
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
[51] K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar,
W. Matusik, and A. Torralba, “Eye tracking for everyone,” in IEEE
CVPR, Las Vegas, NV, June 2016.
[52] E. Wood, “Gaze tracking for commodity portable devices,” Ph.D.
dissertation, University of Cambridge, 2013.
APPENDIX A
POINT-OF-GAZE ESTIMATION
In this step, we estimate one gaze point from each frame to
obtain a complete gaze trace from the entire video. First, we
calculate the 3D center and optical axis of each eye from the
eye center and limbus obtained from limbus detection. Denote
the coordinate of the eye center on the 2D image plane by
(ex, ey) and the ﬁtted ellipse of limbus by E(x, y) = Ax2 +
Bxy + Cy2 + Dx + Ey + F . The 3D center of an eye, denoted
by c = [cx, cy, cz]T , can be calculated as
(ex − μ0)
(ey − υ0)
fx + fy
· r0
rmax
2
fy
fx
, cz =
cx = cz
, cy = cz
,
(5)
where fx and fy are the focal lengths in pixel along horizontal
and vertical axis, respectively, (μ0, υ0) is the coordinate of the
principal point on the 2D image plane, rmax is the semi-major
axis of the ﬁtted ellipse E(x, y) on the 2D image plane, and
r0 is the actual size of human limbus. By deﬁnition, the line
determined by the focal point and the principal point is perpen-
dicular to the 2D image plane, which allows us to calculate the
principal point from the focal point. In practice, fx, fy, μ0, and
υ0 can be obtained by one-time camera calibration. In addition,
parameters ex, ey, and rmax can be computed from E(x, y),
and r0 is set to 6 mm in our implementation.
The optical axis of an eye, denoted by k, can be written
as k = c + mn. Here n is the unit normal vector of the
supporting plane of the limbal circle, and m is a constant. In
the coordinate system of the eye, n is equal to [0, 0, 1]T . Next,
we obtain its corresponding form in the coordinate system of
the camera by the rotation matrix between the two coordinate
systems through the following equation [52],
n = [v1 v2 v3]
,
(6)
(cid:3)
(cid:4)
h
0
g
where v1, v2, and v3 are three eigenvectors of Qe deﬁned as
Qe =
B
2
⎡
⎢⎣ A
− D
(cid:11)
λ2 − λ3
λ1 − λ3
fx+fy
g =
B
2
C
− E
fx+fy
(cid:11)
, h =
− D
− E
fx+fy
fx+fy
4F
(fx+fy)2
λ1 − λ2
λ1 − λ3
,
⎤
⎥⎦ ,
(7)
(8)
and λ1, λ2, and λ3 are the eigenvalues corresponding to v1,
v2, and v3, respectively.
After obtaining the optical axis of each eye, we calculate
the PoG as
(cid:3)
(cid:4)
x
y
0
(cid:3)
(cid:4)
cx
cy
cz
(cid:3)
(cid:4)
nx
ny
nz
PoG =
=
+ m
.
(9)
158
It follows that
m = − cz
nz
and
(cid:12)
(cid:13)
x
y
(cid:12)
(cid:13)
=
cx + mnx
cy + mny
,
(10)
where [x, y]T is the estimated PoG of a video frame.
APPENDIX B
KEYBOARD SPECIFICATION
Table. XI shows the soft keyboard dimensions illustrated
in Fig. 7 and Fig. 8(a).
TABLE XI.
SOFT KEYBOARD DIMENSIONS IN PIXEL ILLUSTRATED IN
FIG. 7 AND FIG. 8(A).
Keyboard
PIN
Pattern lock
Alphabetical
Quasi-PIN
Radius Width Height Horizontal
Gap
65
20
N/A
N/A
N/A
N/A
60
216
N/A
N/A
80
80
50
340
12
12
Vertical
Gap
30
340
24
24
Fig. 15 shows all the possible segments on PIN keyboard,
similar to Fig.9.






	









	









Fig. 15. All possible segments of a PIN keyboard.
Table. XII shows the lengths and angles of the segments
in Fig. 9.
APPENDIX C
ADDITIONAL EVALUATION RESULTS
A. Word for Inference
Table XIII shows the 27 English words from the corn-cob
dictionary to evaluate the performance of EyeTell for word
inference.
B. Experiments with Task Randomization
In this session, we present more details and results on
the set of experiments with task randomization. For these
experiments, the number of participants is 10. As mentioned in
Section VI-A2, each participant was assigned 54 ordered tasks,
of which the order was indicated by her/his given vector. A
task can be inputting a single segment, a lock pattern, a 4-
digit PIN, or a 6-digit PIN. Each participant was asked to
repeat the same task for ﬁve times. To reduce the impact of
fatigue as much as possible, besides following the instruction
in Section VI-A2, the participants were told to stop their
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
TABLE XII.
ALL POSSIBLE SEGMENTS OF PATTERN-LOCK KEYBOARD.
Index
Length Angle
Index
Length Angle
Index
Length Angle
Index
1
2
3
4
5
6
1
√
2
√
5
√
2
√
2
2
5
0
0
0.464
π
4
π
4
1.11
7
8
9
10
11
12
1
√
2
√
5
√
2
√
2
2
5
π
2
π
2
2.03
3π
4
3π
4
2.68
13
14
15
16
17
18
1
√
2
√
5
√
2
√
2
2
5
π
π
4
-2.68
− 3π
− 3π
4
-2.03
19
20
21
22
23
24
2
Length Angle
− π
− π
2
-1.11
− π
− π
-0.464
1
√
2
√
5
√
2
√
2
2
5
4
4
TABLE XIII.
WORDS FOR INFERENCE.
Length Words
7
8
9
10
11
13
between, spanish, nuclear
identity, emirates, platinum, homeland, security
institute, extremely, sacrament, dangerous
difference, wristwatch, processing, unphysical
inquisition, pomegranate, feasibility, polytechnic, obfus-
cating
paediatrician, interceptions, abbreviations, impersonating,
soulsearching, hydromagnetic
2) Experiments on Inferring Lock Patterns: For these ex-
periments, each participant input four simple patterns, three
medium patterns, and three complex patterns from [49] on a
Nexus 6 under task randomization. The patterns were randomly
selected when preparing all the tasks for each participant.
As shown in Table XV,
top-10,
and top-50 accuracy of EyeTell inferring pattern locks under
task randomization are 55.8%, 70.1%, 75.1%, and 84.1%,
respectively.
the average top-1,
top-5,
experiments at any time they wished. Also, we collected
the data of the same participant on different days. For each
participant, the experimental time on the same day was less
than half an hour. The total experimental time for a participant
ranged from one and a half to three hours.
1) Inferring a Single Lock-Pattern Segment: For these
experiments, each participant input each segment in Table XII
on a Nexus 6 for ﬁve times under task randomization. As we
can see in Table XIV, EyeTell can infer the angle of a single
ﬁnger movement on the pattern-lock keyboard under task
randomization with top-1, top-2, and top-3 inference accuracy
up to 87.19%, 97.10%, and 99.62%, respectively.
TABLE XIV.
INFERENCE ACCURACY ON A SINGLE SEGMENT OF
PATTERN-LOCK KEYBOARD.
Index
segment
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Average
of
top-1
top-2
top-3
top-4
top-5
100% 100%
100%
82.5%
100% 100%
92.5%