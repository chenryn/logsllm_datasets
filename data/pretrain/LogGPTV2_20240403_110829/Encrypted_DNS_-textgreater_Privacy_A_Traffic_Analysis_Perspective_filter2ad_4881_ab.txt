Recommended EDNS0 padding
EDNS0 padding with ad-blocker
DoT with Stubby client
LOC1
LOC2
LOC3
RPI
GOOGLE
CLOUD
CL-FF
OW
WEB
TOR
EDNS0-128
EDNS0-468
EDNS0-128-adblock
DOT
Location
Lausanne
Leuven
Singapore
Lausanne
Leuven
Leuven
Leuven
Lausanne
Leuven
Lausanne
Lausanne
Lausanne
Lausanne
Lausanne
# webpages
# samples
1,500
1,500
1,500
700
700
700
700
5,000
700
700
700
700
700
700
200
60
60
60
60
60
60
3
60
60
60
60
60
60
We capture network trafﬁc between the DoH client and the
resolver using tcpdump, and we ﬁlter the trafﬁc by destination
port and IP to obtain the ﬁnal DoH trafﬁc trace.
We collect traces for the top, middle, and bottom 500 webpages
in Alexa’s top million websites list on 26 March 2018, 1,500
webpages in total. We note that even though 1,500 is still
a small world compared to the size of the Web, the largest
world considered in evaluations similar to ours for website
ﬁngerprinting on web trafﬁc over Tor is of 800 websites [51,
30, 52, 18, 53, 19].
We visit each webpage in a round-robin fashion, obtaining
up to 200 samples for every webpage. For our open world
analysis, we collect traces of an additional 5,000 webpages
from the Alexa top million list. We collected data during two
periods, from 26 August 2018 to 9 November 2018, and from
20 April 2019 to 14 May 2019. Data from these two periods
is never mixed in the analysis.
We consider different scenarios varying DoH client and re-
solver, end user location and platform, and the use of DNS
trafﬁc analysis defenses (padding, ad-blocker usage, DNS-
over-Tor). Table I provides an overview of the collected
datasets. In order to better understand the vulnerability of
DNS encryption to trafﬁc analysis, we designed heterogenous
experiments that look into different aspects of the problem,
resulting in multiple datasets of varied sizes and collected
under different conditions – in many cases, several datasets
for each experiment type. In each experiment, we vary one
characteristic (e.g., location or platform), and keep the default
conﬁguration for the rest.
We also collected a dataset (using the Stubby client and
Cloudﬂare’s resolver) to study the effectiveness of defenses
on DoT trafﬁc as compared to DoH. Since Cloudﬂare had
already implemented padding of responses for DoT trafﬁc at
the time of data collection, we were not able to collect a dataset
of DoT without padding. In the following sections, we use
the Identiﬁer provided in the second column to refer to each
of the datasets. Note that unless speciﬁed otherwise, we use
Cloudﬂare’s DoH client.
Data curation. We curate the datasets to ensure that our
results are not biased. First, we aim at removing the effect
of spurious errors in collection. We deﬁne those as changes
in the websites for reasons other than those variations due
4
the HTTP request/responses, not
to their organic evolution that do not represent the expected
behavior of the page. For instance, pages that go down during
the collection period.
Second, we try to eliminate website behavior that is bound to
generate classiﬁcation errors unrelated to the characteristics
of DNS trafﬁc. For instance, different domains generating
the same exact DNS traces, e.g., when webpages redirect to
other pages or to the same resource; or web servers returning
standard errors (e.g., 404 Not Found or 403 Forbidden).
For curation, we use the Chrome over Selenium crawler to
collect
the DNS queries
responses, of all the pages in our list in LOC1. We conduct
two checks. First, we look at the HTTP response status of the
FQDN (Fully Qualiﬁed Domain Name), that is being requested
by the client. We identify the webpages that do not have an
HTTP OK status. These could be caused by a number of fac-
tors, such as pages not found (404), anti-bot mechanisms, for-
bidden responses due to geoblocking [54] (403), internal server
errors (500), and so on. We mark these domains as invalid. Sec-
ond, we conﬁrm that the FQDN is present in the list of requests
and responses. This ensures that the page the client is request-
ing is not redirecting the browser to other URLs. This check
triggers some false alarms. For example, a webpage might redi-
rect to a country-speciﬁc version (indeed.com redirecting to
indeed.fr, results in indeed.com not being present in the
list of requests); or in domain redirections (amazonaws.com
redirecting to aws.amazon.com). We do not consider these
cases as anomalies. Other cases are full redirections. Examples
are malware that redirects browser requests to google.com,
webpages that redirect to GDPR country restriction notices, or
webpages that redirect to domains that specify that the site is
closed. We consider these cases as invalid webpages and add
them to our list of invalid domains.
We repeat these checks multiple times over our collection
period. We ﬁnd that 70 webpages that had invalid statuses
at some point during our crawl, and 16 that showed some
ﬂuctuation in their status (from invalid to valid or vice versa).
We study the effects of keeping and removing these webpages
in Section V-B.
V. DNS-BASED WEBSITE FINGERPRINTING
Website ﬁngerprinting attacks enable a local eavesdropper
to determine which web pages a user is accessing over an
encrypted or anonyimized channel. It exploits the fact that the
size, timing, and order of TLS packets are a reﬂection of a
website’s content. As resources are unique to each webpage,
the traces identify the web even if trafﬁc is encrypted. Website
ﬁngerprinting has been shown to be effective on HTTPS [27,
26, 55], OpenSSH tunnels [56, 57], encrypted web proxies [58,
59] and VPNs [60], and even on anonymous communications
systems such as Tor [61, 30, 51, 52, 16, 17, 18, 19].
The patterns exploited by website ﬁngerprinting are correlated
with patterns in DNS trafﬁc: which resources are loaded, and
their order determines the order of the corresponding DNS
queries. Thus, we expect that website ﬁngerprinting can also be
effective on DNS encrypted ﬂows such as DNS-over-HTTPS
(DoH) or DNS-over-TLS (DoT). In this paper, we call DNS
ﬁngerprinting the use of trafﬁc analysis to identify the web
page that generated an encrypted DNS trace, i.e., website ﬁn-
gerprinting on encrypted DNS trafﬁc. In the following, when-
ever we do not explicitly specify whether the target of website
ﬁngerprinting is DNS or HTTPS trafﬁc, we refer to traditional
website ﬁngerprinting on web trafﬁc over HTTPS.
A. DNS ﬁngerprinting
As in website ﬁngerprinting, we treat DNS ﬁngerprinting as
a supervised learning problem: the adversary ﬁrst collects a
training dataset of encrypted DNS traces for a set of pages.
The page (label) corresponding to a DNS trace is known. The
adversary extracts features from these traces (e.g., lengths of
network packets) and trains a classiﬁer that, given a network
trace, identiﬁes the visited page. Under deployment, the adver-
sary collects trafﬁc from a victim and feeds it to the classiﬁer
to determine which page the user is visiting.
Trafﬁc variability. Environmental conditions introduce vari-
ance in the DNS traces sampled for the same website. Thus,
the adversary must collect multiple DNS traces in order to
obtain a robust representation of the page.
Some of this variability has similar origin to that of web trafﬁc.
For instance, changes on the website that result in varying DNS
lookups associated with third-party embedded resources (e.g.,
domains associated to ad networks); the platform where the
client runs, the conﬁguration of the DoH client, or the software
using the client which may vary the DNS requests (e.g., mobile
versions of websites, or browsers’ use of pre-fetching); and
the effects of content
localization and personalization that
determine which resources are served to the user.
Additionally, there are some variability factors speciﬁc to DNS
trafﬁc. For instance, the effect of the local resolver, which
depending on the state of the cache may or may not launch
requests to the authoritative server; or the DNS-level load-
balancing and replica selection (e.g., CDNs) which may pro-
vide different IPs for a given domain or resource [62].
Feature engineering. Besides the extra trafﬁc variability com-
pared to web trafﬁc, DNS responses are generally smaller and
more chatty than web resources [62, 63]. In fact, even when
DNS lookups are wrapped in HTTP requests, DNS requests
and responses ﬁt in one single TLS record in most cases.
These particularities hint that traditional website ﬁngerprinting
features, typically based on aggregate metrics of trafﬁc traces
such as the total number of packets, total bytes, and their
statistics (e.g., average, standard deviation), are inadequate
to characterize DoH trafﬁc. We test this hypothesis in the
following section, where we show that state-of-the-art web
trafﬁc attacks’ performance drops in 20% when applied on
DoH traces (see Table III).
To address this problem, we introduce a novel set of features,
consisting of n-grams of TLS record lengths in a trace.
Following the usual convention in website ﬁngerprinting,
we represent a trafﬁc trace as a sequence of integers, where
the absolute value is the size of the TLS record and the
sign indicates direction: positive for packets from the client
to the resolver
the packets
(incoming). An example
from the resolver
of this represenation is the trace: (−64,88,33,−33). Then,
the uni-grams for this trace are (−64),(88),(33),(−33), and
the bi-grams are (−64,88),(88,33),(33,−33). To create the
features, we take tuples of n consecutive TLS record lengths
(outgoing), and negative for
to the client
the number of their
in the DoH trafﬁc traces and count
occurrences in each trace.
In some of our experiments, we used a proxy to man-in-
the-middle the DoH connection between the client and the
resolver4, and obtained the OpenSSL TLS session keys using
Lekensteyn’s scripts5. In all the decrypted TLS records we
observe only one single DoH message (either a request or a
response). However, as Houser et al. indicate, some clients and
resolvers’ implementations result on multiple DoT messages
in the same TLS record [64].
The intuition behind our choice of features is that n-grams
capture patterns in request-response size pairs, as well as the
local order of the packet size sequence. To the best of our
knowledge, n-grams have never been considered as features in
the website ﬁngerprinting literature.
We extend the n-gram representation to trafﬁc bursts. Bursts
are sequences of consecutive packets in the same direction (ei-
ther incoming or outgoing). Bursts correlate with the number
and order of resources embedded in the page. Additionally,
they are more robust to small changes in order than individual
sizes because they aggregate several records in the same
direction. We represent n-grams of bursts by adding lengths of
packets in a direction inside the tuple. In the previous example,
the burst-length sequence of the trace above is (−64,121,−33)
and the burst bi-grams are (−64,121),(121,−33).
We experimented with uni-, bi- and tri-grams for both features
types. We observed a marginal improvement in the classiﬁer
on using tri-grams at a substantial cost on the memory require-
ments of the classiﬁer. We also experimented with the timing
of packets. As in website ﬁngerprinting [30], we found that
it does not provide reliable prediction power. This is because
timing greatly varies depending on the state of the network and
thus is not a stable feature to ﬁngerprint web pages. In our ex-
periments, we use the concatenation of uni-grams and bi-grams
of both TLS record sizes and bursts as feature set.
Algorithm selection. After experimenting with different su-
pervised classiﬁcation algorithms, we selected Random Forests
(RF) which are known to be very effective for trafﬁc analysis
tasks [18, 65].
Random forests (RF) are ensembles of simpler classiﬁers
called decision trees. Decision trees use a tree data structure
to represent splits of the data: nodes represent a condition
on one of the data features and branches represent decisions
based on the evaluation of that condition. In decision trees,
feature importance in classiﬁcation is measured with respect
to how well they split samples with respect to the target classes.
The more skewed the distribution of samples into classes is,
the better the feature discriminates. Thus, a common metric
for importance is the Shannon’s entropy of this distribution.
Decision trees, however, do not generalize well and tend
to overﬁt the training data [66]. RFs mitigate this issue by
randomizing the data and features over a large amount of
trees, using different subsets of features and data in each tree.
The ﬁnal decision of the RF is an aggregate function on the
individual decisions of its trees. In our experiments, we use
100 trees and a majority vote to aggregate them.
4https://github.com/facebookexperimental/doh-proxy
5https://git.lekensteyn.nl/peter/wireshark-notes
5
Validation. We evaluate the effectiveness of DNS ﬁngerprint-
ing in two scenarios typically used in the website ﬁngerprinting
literature. A closed world, in which the adversary knows the
set of all possible pages users may visit; and an open-world,
in which the adversary only has access to a set of monitored
pages, and the user may visit pages outside of this set.
In the closed world, we evaluate the effectiveness of our
classiﬁer measuring the per-webpage Precision, Recall and
F1-Score. We consider positives as DoH traces generated by
that webpage and negatives as traces generated by any other
webpage. For each webpage, true positives are DoH traces
generated by visits to the webpage that the classiﬁer correctly
assigns to that webpage; false positives are traces generated
by vists to other pages that are incorrectly classiﬁed as the
webpage; false negatives are traces of the webpage that are
classiﬁed as other pages; and true negatives are traces of other
pages that are not classiﬁed as the webpage. Then, Precision
is the ratio of true positives to the total number of traces that
were classiﬁed as positive (true positives and false positives);
Recall is the ratio of true positives to the total number of
positives (true positives and false negatives); and the F1-score
is the harmonic mean of Precision and Recall. We aggregate
these metrics over all the webpages, providing their average
and standard deviation.
In the open world there are only two classes: monitored
(positive) and unmonitored (negative). Thus, a true positive
in the open world is a trace of a monitored webpage that is
classiﬁed as monitored, and a false positive is a trace of an
unmonitored page that is classiﬁed as monitored. Likewise, a
true negative is a trace of an unmonitored page classiﬁed as
unmonitored and a false negative a trace of a monitored page
classiﬁed as unmonitored.
We use 10-fold cross-validation, a standard methodology in
machine learning, to measure the generalization error of the
classiﬁer, also known as overﬁtting. In cross-validation, the
samples of each class are divided in ten disjoint partitions.
The classiﬁer is then trained on each set of nine partitions and
tested in the remaining one. Since there are(cid:0)10
sets of nine partitions, this provides us ten samples of the
classiﬁer performance on a set of samples on which it has not
been trained on. Taking the average and standard deviation of
these samples gives us an estimate of the performance of the
classiﬁer on unseen examples.
B. Evaluating n-grams features
We now evaluate the effectiveness of n-grams features to
launch DNS ﬁngerprinting attacks. We also compare these
features with traditional website ﬁngerprinting features in both
DoH trafﬁc and web trafﬁc over HTTPS.
(cid:1) = 10 possible
9
Evaluation in closed and open worlds. We ﬁrst evaluate the
attack in a closed world using the LOC1 dataset. We try three
settings: i. an adversary that attacks the full dataset of 1,500
websites, ii. an adversary that attacks the curated dataset of
1,414 websites after we eliminate spurious errors, and iii. an
adversary that attacks the full dataset but considers regional
versions of given pages to be equivalent. For example,
classifying google.es as google.co.uk, a common
error in our classiﬁer, is considered a true positive. We see
in Table II that testing on the clean dataset offers just a 1%
performance increase, and that considering regional versions
6