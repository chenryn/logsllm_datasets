by our program P(m), respects the invariant above. We
were able to quickly extract the security skeleton of the
security kernel from Flicker’s approximately 250 lines of
C code. To verify that the skeleton respects the exact
invariant from our security proof, we checked that instruc-
tions were present to evaluate the function f , that the EOL
marker was subsequently extended into m.d pcr.k, and that
each of the instructions would only be executed once on
all code paths. In several cases, we matched multiple C
instructions to a single action since the instructions are a
reﬁnement of the action. For example, the extension of
EOL consists of two instructions, a memset to create the
sequence of characters corresponding to an EOL and a
call to a wrapper for the extend instruction. The entire
manual process of extracting the security skeleton and
auditing the invariant
took less than one hour for an
individual with no previous experience with the Flicker
security kernel. Although we did not formally verify the
property, one interesting direction for future work is to use
these invariants to derive reﬁned invariants to check on the
implementation, possibly using software model checking
techniques.
5. Related Work
LS2 draws on certain conceptual ideas from PCL [3],
in particular, the local reasoning style by which security
properties of protocols are proved without explicitly rea-
soning about adversary actions. In PCL, global security
properties are derived by combining properties achieved
by individual protocol steps with invariants proved by
induction over the protocol programs executed by honest
parties. LS2 supports this form of reasoning for a much
richer language that includes not only network communi-
cation and cryptography as in PCL, but also shared mem-
ory, memory protection, machine resets, and dynamically
loaded unknown pieces of code. The insights on which the
new proof rules are based are described in Section 2.2.
The technical deﬁnition of LS2 also differs signiﬁcantly
from PCL: instead of associating pre-conditions and post-
conditions with all actions in a process (as PCL does),
we model time explicitly, and associate monotonically
increasing time points with events on a trace. The presence
of explicit
time allows us to express invariants about
memory; for instance, we may express in LS2 that a
memory location contains the same value throughout the
interval [t1,t2]. Explicit time is also used to reason about
the relative order of events. Whereas explicit use of time
may appear to be low-level and cumbersome for practical
use, the proof system for LS2 actually uses time in a very
limited way that is quite close to temporal logics such as
LTL [31]. Indeed, it seems plausible to rework the proof
system in this paper using operators of LTL in place of
explicit time. However, we refrain from doing so because
we believe that a model of real time may be needed to
analyze some systems of interest (e.g., [32]–[34]).
LS2 also shares some features with other logics of
programs [8], [10], [35]. Hoare logic and dynamic logic
focus on sequential
imperative programs, and do not
consider concurrency, network communication and adver-
saries. LS2’s abstract locks are similar to regions that are
used to reason about synchronized access to memory in
concurrent separation logic [8]. However, the two primi-
tives differ in application. Whereas we use locks to enforce
integrity of data stored in memory, regions are intended to
prevent race conditions. Another key difference between
concurrent separation logic and LS2 is that the former
does not consider network communication. Furthermore,
concurrent separation logic and other approaches for ver-
ifying concurrent systems [36] typically do not consider
an adversary model. An adversary could be encoded as
a regular program in these approaches, but then proving
invariants would involve an induction over the steps of the
honest parties programs and the attacker.
Prior proposals for reasoning about dynamically loaded
code use higher-order extensions of Hoare logic [24]–[28].
However, they are restricted to reasoning about sequential
programs only and require that invariants of code being
called be known in the program at the point of the call.
LS2’s method addresses the problem of reasoning about
dynamically loaded code in the more general context of
concurrent program execution where one thread is allowed
to modify code that is loaded by another. As illustrated
in Section 4.1, using the (Jump) rule, evidence that some
code executed can be combined with separate evidence
about the identity of the code to reason precisely about
the effects of the jump. Such reasoning is essential in
some applications including trusted computing, and is
impossible in all prior work known to us.
There have been several previous analyses of trusted
computing. Abadi and Wobber used an authorization logic
to describe the basic ideas of NGSCB, the predecessor to
the TCG [37]. Their formalization documents and clariﬁes
basic NGSCB concepts rather than proving speciﬁc prop-
erties of systems utilizing a TPM. Chen et al. developed a
formal logic tailored to the analysis of a remote attestation
protocol and suggested improvements [38]. Unlike LS2,
these logics are not tied to the execution semantics of
the protocols. Gurgens et al. used a model checker to
analyze the security of several TCG protocols [39]. Millen
et al. employed a model checker to understand the role
and trust relationships of a system performing a remote
attestation protocol [40]. Our analysis with LS2 is a
complementary approach: It proves security properties
even for an inﬁnite number of simultaneous invocations
of attestation protocols, but with a more abstract model
of the TPM’s primitives. LS2 is designed to be a more
general logic with TCG protocols providing one set of
applications. Lin [41] used a theorem prover and model
ﬁnder to analyze the security of the TPM against invalid
sequences of API calls.
6. Conclusion
In this paper, we presented LS2 and used it to carry
out a substantial case study of trusted computing attesta-
tion protocols. The design of LS2 was conceptually and
technically challenging. Speciﬁcally, it was difﬁcult to
deﬁne a realistic adversary model and formulate sound
reasoning principles for dynamically loaded unknown (and
untrusted) code. The proof system was designed to support
reasoning at a high level of abstraction. This was partic-
ularly useful in the case studies where the proofs yielded
many insights about the security of trusted computing
systems.
In future work, we will build upon this work to model
and analyze security properties of web browsers, security
hypervisors and virtual machine monitors. We also plan
to develop further principles for modeling and reasoning
about security at the level of system interfaces, in partic-
ular, to support richer access control models and system
composition and reﬁnement.
Acknowledgments. The authors would like to thank
Michael Hicks, Jonathan McCune, and the anonymous re-
viewers for their helpful comments and suggestions. This
work was partially supported by the U.S. Army Research
Ofﬁce contract on Perpetually Available and Secure Infor-
mation Systems (DAAD19-02-1-0389) to CMU’s CyLab,
the NSF Science and Technology Center TRUST, and
the NSF CyberTrust grant “Realizing Veriﬁable Security
Properties on Untrusted Computing Platforms”. Jason
Franklin is supported in part by an NSF Graduate Re-
search Fellowship.
References
[1] “Trusted
Computing
Group
//www.trustedcomputinggroup.org/, 2008.
(TCG),”
https:
[2] A. Datta, A. Derek, J. C. Mitchell, and D. Pavlovic, “A
derivation system and compositional
logic for security
protocols,” Journal of Computer Security, vol. 13, no. 3,
pp. 423–482, 2005.
[3] A. Datta, A. Derek, J. C. Mitchell, and A. Roy, “Protocol
Composition Logic (PCL).” Electr. Notes Theor. Comput.
Sci., vol. 172, pp. 311–358, 2007.
[4] N. Durgin, J. C. Mitchell, and D. Pavlovic, “A composi-
tional logic for proving security properties of protocols,”
Journal of Computer Security, vol. 11, pp. 677–721, 2003.
[5] A. Roy, A. Datta, A. Derek, J. C. Mitchell, and J.-P. Seifert,
“Secrecy analysis in protocol composition logic,” Formal
Logical Methods for System Security and Correctness,
2008.
[6] R. Milner, M. Tofte, and R. Harper, The Deﬁnition of
Standard ML. Cambridge, MA, USA: MIT Press, 1990.
[7] J. Saltzer and M. Schroeder, “The protection of information
in computer systems,” Proceedings of the IEEE, vol. 63,
no. 9, pp. 1278–1308, September 1975.
[8] S. Brookes, “A semantics for concurrent separation logic,”
in Proceedings of 15th International Conference on Con-
currency Theory, 2004.
[9] Z. Manna and A. Pnueli, Temporal Veriﬁcation of Reactive
Systems: Safety. Springer-Verlag, 1995.
[10] C. A. R. Hoare, “An axiomatic basis for computer pro-
gramming,” Communications of the ACM, vol. 12, no. 10,
pp. 576–580, 1969.
[11] P. W. O’Hearn, J. C. Reynolds, and H. Yang, “Local
reasoning about programs that alter data structures,” in Pro-
ceedings of the 15th International Workshop on Computer
Science Logic. London, UK: Springer-Verlag, 2001, pp.
1–19.
[12] J. C. Reynolds, “Separation logic: A logic for shared
mutable data structures,” in Proceeding of the 17th Annual
IEEE Symposium on Logic in Computer Science (LICS).
IEEE Computer Society, 2002, pp. 55–74.
[27] H. Cai, Z. Shao, and A. Vaynberg, “Certiﬁed self-
modifying code,” in ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation. New
York, NY, USA: ACM, 2007, pp. 66–77.
[13] Trusted
“TCG
Computing
Speciﬁcation
Architecture Overview, Speciﬁcation Revision
1.4,”
https://www.trustedcomputinggroup.org/groups/TCG 1 4
Architecture Overview.pdf, August 2007.
Group,
[14] TCG, “PC client speciﬁc TPM interface speciﬁcation
(TIS),” Version 1.2, Revision 1.00, Jul. 2005.
[15] Advanced Micro Devices, “AMD64 virtualization: Secure
virtual machine architecture reference manual,” AMD Pub-
lication no. 33047 rev. 3.01, May 2005.
[16] “Intel Trusted Execution Technology: Software Develop-
ment Guide,” Document Number: 315168-005, Intel Cor-
poration, June 2008.
[17] A. Datta, J. Franklin, D. Garg, and D. Kaynar, “A logic of
secure systems and its application to trusted computing,”
Carnegie Mellon University, Tech. Rep. CMU-CyLab-09-
001, 2009.
[18] E. M. Chan, J. C. Carlyle, F. M. David, R. Farivar, and
R. H. Campbell, “BootJacker: Compromising computers
using forced restarts,” in Proceedings of 15th ACM Con-
ference on Computer and Communications Security, 2008.
[19] S. Garriss, R. C´aceres, S. Berger, R. Sailer, L. van Doorn,
and X. Zhang, “Towards trustworthy kiosk computing,” in
Workshop on Mobile Computing Systems and Applications,
Feb. 2006.
[20] H. DeYoung, D. Garg, and F. Pfenning, “An authorization
logic with explicit time,” in Proceedings of the 21st IEEE
Computer Security Foundations Symposium (CSF-21), Jun.
2008.
[21] J. Reed, “Hybridizing a logical framework,” in Interna-
tional Workshop on Hybrid Logic 2006 (HyLo 2006), ser.
Electronic Notes in Computer Science, August 2006.
[22] T. Bra¨uner and V. de Paiva, “Towards constructive hybrid
logic,” in Electronic Proceedings of Methods for Modalities
3 (M4M3), 2003.
reference
[23] “Secure
manual.”
[Online].
Available: http://www.amd.com/us-en/assets/content type/
white papers and tech docs/33047.pdf
machine
Corp., May
architecture
2005.
virtual
AMD
[24] N. Krishnaswami, “Separation logic for a higher-order
typed language,” 2006, in Workshop on Semantics, Pro-
gram Analysis and Computing Environments for Memory
Management, SPACE06.
[25] H. Thielecke, “Frame rules from answer types for code
pointers,” in 33rd ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages. New York, NY,
USA: ACM, 2006, pp. 309–319.
[26] Z. Ni and Z. Shao, “Certiﬁed assembly programming with
embedded code pointers,” in 33rd ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages.
New York, NY, USA: ACM, 2006, pp. 320–333.
[28] A. Nanevski, G. Morrisett, and L. Birkedal, “Hoare type
theory, polymorphism and separation,” Journal of Func-
tional Programming, vol. 18, no. 5&6, pp. 865–911, 2008.
[29] B. Kauer, “OSLO: Improving the security of trusted com-
puting,” in Proceedings of the USENIX Security Sympo-
sium, Aug. 2007.
[30] J. M. McCune, B. Parno, A. Perrig, M. K. Reiter, and
H. Isozaki, “Flicker: An execution infrastructure for tcb
minimization,” in Proceedings of the ACM European Con-
ference in Computer Systems (EuroSys), Apr. 2008.
[31] A. Pnueli, “The temporal
logic of programs,” in Pro-
ceedings of 19th Annual Symposium on Foundations on
Computer Science, 1977.
[32] R. Kennell and L. H. Jamieson, “Establishing the genuinity
of remote computer systems,” in Proceedings of the 2003
USENIX Security Symposium, Aug. 2003.
[33] A. Seshadri, A. Perrig, L. van Doorn, and P. Khosla,
“SWATT: Software-based attestation for embedded de-
vices,” in Proceedings of the IEEE Symposium on Security
and Privacy, May 2004.
[34] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn,
and P. Khosla, “Pioneer: Verifying code integrity and
enforcing untampered code execution on legacy platforms,”
in Proceedings of ACM Symposium on Operating Systems
Principles (SOSP), Oct. 2005.
[35] D. Harel, D. Kozen, and J. Tiuryn, Dynamic Logic, ser.
Foundations of Computing. MIT Press, 2000.
[36] L. Lamport, “The temporal logic of actions,” ACM Trans-
actions on Programming Languages and Systems, vol. 16,
no. 3, May 1994.
[37] M. Abadi and T. Wobber, “A logical account of NGSCB,”
in Proceedings of Formal Techniques for Networked and
Distributed Systems, 2004.
[38] S. Chen, Y. Wen, and H. Zhao, “Formal analysis of
secure bootstrap in trusted computing,” in Proceedings of
4th International Conference on Autonomic and Trusted
Computing, 2007.
[39] S. Gurgens, C. Rudolph, D. Scheuermann, M. Atts, and
R. Plaga, “Security evaluation of scenarios based on the
TCG’s TPM speciﬁcation,” in Proceedings of 12th Euro-
pean Symposium On Research In Computer Security, 2007.
[40] J. Millen, J. Guttman, J. Ramsdell, J. Sheehy, and B. Snif-
fen, “Analysis of a measured launch,” The MITRE Corpo-
ration, Tech. Rep., 2007.
[41] A. H. Lin, “Automated analysis of security apis,” Master’s
thesis, Massachusetts Institute of Technology, 2005.