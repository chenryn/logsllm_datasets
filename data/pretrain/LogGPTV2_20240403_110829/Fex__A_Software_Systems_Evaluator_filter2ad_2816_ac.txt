gcc_asan.mk
src . . . . . . . . . . . . . . . . . . . . . makeﬁles and sources for benchmarks
applications
apache
Makeﬁle
phoenix
histogram
Makeﬁle
[...source ﬁles...]
experiments . . . . . . . . . . . . . . . . scripts to run-parse-plot results
phoenix
run.py
collect.py
plot.py
run.py
build . . . . . . . . . . . . . . . . . . automatically generated ﬁnal binaries
phoenix
histogram
gcc_native
[. . . ]
gcc_asan
[. . . ]
Fig. 5: Example directory tree of FEX.
experiment. The user can specify -v for verbose output
and -d to build debug versions of benchmarks. To
increase the number of runs of each benchmark, the user
adds -r 10. To run benchmarks with different numbers
of threads, the -m 1 2 4 ﬂag must be added. To run
only one benchmark from the benchmark suite, the user
speciﬁes -b histogram.
Note that the ﬁnal binaries of benchmarks are put un-
der build/ directory, see Figure 5. Sometimes it is useful
to run the binary directly from there, e.g., to debug spuri-
ous errors or to perform additional quick measurements.
After the experiment is ﬁnished, the user should fetch
the ﬁnal CSV results from the server and run the “plot”
command locally:
>> fex.py plot −n phoenix −t perf
This builds the “perf” (performance overhead barplot)
graph and saves it in a PDF ﬁle. The examples of such
graphs are shown in the next section.
C. Currently supported experiments
During the months of
expanded it
currently
internal use of FEX, we
in several directions. Table I lists the
and
supported benchmarks,
compilers
Phoenix, SPLASH, PARSEC, SPEC CPU2006*
– Benchmark suites
– Add. benchmarks Apache, Nginx, Memcached, RIPE, micro
– Compilers
– Types
– Experiments
GCC, Clang/LLVM
AddressSanitizer (as example)
Performance and memory overheads,
security evaluation
perf-stat (generic), perf-stat (memory), time
Lineplot, regular barplot, stacked barplot,
grouped barplot, stacked-grouped barplot
– Tools
– Plots
*Will not be open-sourced as part of FEX due to proprietary license.
TABLE I: Currently supported experiments in FEX.
compilation types, and experiments.
FEX supports four benchmark suites: Phoenix [3],
SPLASH [14], PARSEC [4], and SPEC CPU20062 [2].
Additionally, FEX comes with Apache, Memcached,
and Nginx programs that showcase real-world usage
scenarios. They are installed via scripts and not put
under src/. FEX also provides several statically linked
libraries like libevent and OpenSSL, required for at least
one of the above benchmarks. Lastly, we wrote a suite
of microbenchmarks—e.g., reading from an array—that
can be useful for debugging purposes.
FEX provides installation scripts and makeﬁles for
GCC version 6.1 and Clang/LLVM 3.8.0 [22]. It is easy
to update these scripts to install newer versions of these
compilers. As of type-speciﬁc makeﬁles, the current
version of the framework includes only AddressSanitizer
as an example.
The list of supported experiments includes (1)
performance- and memory-overhead experiments as
well as variable-inputs experiments of Phoenix, PARSEC,
and SPEC, and (2) throughput-latency and security
experiments of Apache, Nginx, and Memcached.
For plotting, FEX provides the following generic plots:
barplot (e.g., for performance and memory overheads),
lineplot (for multithreading overheads), stacked barplot,
grouped barplot, and stacked-and-grouped barplot (for
complicated statistics such as cache misses at different
levels).
IV. CASE STUDIES
In this section, we evaluate extensibility and ease
of use of FEX on a set of benchmarks. To showcase the
required end-user effort, we considered the following
scenario: a researcher wants to compare performance
of Clang compiler against GCC using SPLASH-3 [14]
benchmark suite and Nginx web server [15], as well as
the security guarantees provided by the two compilers
using the RIPE testbed [16].
The presented evaluation of effort was done by an ex-
perienced user—we wrote all the extensions on our own.
A. Multithreaded Benchmark Suite: SPLASH-3
SPLASH-3 benchmark suite is used to evaluate
parallel applications on large-scale NUMA architectures
2SPEC CPU cannot be made publicly available and will not be
open-sourced as part of FEX
547
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
)
e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
C
C
G
e
v
i
t
a
n
.
t
.
r
.
w
(
Native (Clang)
2
1
0
cholesky fft
barnes
m lu
fm
radiosity
ocean
radix
raytrace
volrend
water-n
water-s All
Fig. 6: Example of Clang-GCC comparison produced
by FEX and tested on SPLASH-3.
[14]. To include it in FEX, the ﬁrst step was to add the
source code of SPLASH-3 itself. This required:
• Changes in the build system of the suite: renaming
of the variables, restructuring of directories, and
removing unnecessary build targets—194 LoC in
total. Note that most of the changes can be done
by automatic renaming and do not take much time.
Also, the resulting build system became smaller and
more generic, since many variables and build targets
are now deﬁned globally (167 LoC deleted).
• An installation script to download input ﬁles (5 LoC).
• A Runner subclass to control the experiment (36
LoC) and collect.py script to process the ﬁnal
results (9 LoC).
Next, Clang must be added as a build type (GCC
is supplied with the framework). It required writing
an installation script for Clang and all its dependencies
(50 LoC if built from sources) and a compiler-speciﬁc
Makeﬁle (6 LoC).
Finally, the results had to be represented as a plot of
slowdown (speedup) of Clang versions over GCC ones.
The natural choice for this type of data is a barplot since
it can clearly depict overheads of several applications.
FEX already has the functionality for building such
plots, therefore the effort was minimal—only 26 LoC in
plot.py script. The total effort summed up to 326 LoC
or approximately 5 man-hours of work.
The experiment was run with the following command:
>> fex.py run −n splash −t gcc_native clang_native
As a result, it produced the plot shown in Figure 6.
From this plot the researcher might deduct, for example,
that the given version of Clang has slightly worse
performance than GCC and it is especially bad with
operations on matrices, as represented by FFT.
B. Real-world Application: Nginx Web-server
To evaluate the effort of adding a standalone
application, we integrated Nginx web server into FEX [15].
First, we did not put Nginx’s sources under the
src/ directory, but instead wrote an installation script
(9 LoC). Next, we created a performance experiment:
we wrote a specialized collect.py script to collect
0.7
0.6
0.5
0.4
0.3
0.2
)
s
m
(
y
c
n
e
t
a
L
0
Native (GCC)
Native (Clang)
20
10
(cid:55)(cid:75)(cid:85)(cid:82)(cid:88)(cid:74)(cid:75)(cid:83)(cid:88)(cid:87)(cid:3)(cid:11)(cid:13)(cid:100)(cid:40)(cid:110)(cid:3)(cid:80)(cid:86)(cid:74)(cid:18)(cid:86)(cid:12)
30
40
50
Fig. 7: Example throughput-latency plot of Nginx
produced by FEX. Remote clients fetch a 2K static
web-page over a 1Gb network.
throughput and latency statistics (14 LoC), a plot.py
script to adjust the appearance of a throughput-latency
plot (34 LoC), and a run.py script to pre-conﬁgure the
server side, start a client on a separate machine via SSH,
wait for the experiment to ﬁnish, and fetch the logs (89
LoC). Finally, we created a makeﬁle with conﬁguration
options to build Nginx (20 LoC). The whole effort was
166 LoC—mostly due to a complicated running scenario
with a remote client—or approximately two man-hours.
We ran the Nginx experiment like this:
>> fex.py run −n nginx −t gcc_native clang_native
The resulting throughput-latency measurements are
shown in Figure 7. In our hypothetical study this plot
would support the previous observations: the Clang
version has worse throughput than GCC.
C. Security Benchmark: RIPE
To highlight that FEX supports types of experiments
other than performance ones, we experimented with
RIPE security testbed [16]. At its core, RIPE is a C
program that tries to attack itself in a variety of ways
(with 850 possible attacks in total).
As a ﬁrst step, we put sources of RIPE—two source
and two header ﬁles—together with a simple Makeﬁle
under src/. We did not change the source code of RIPE,
and our resultant Makeﬁle was 14 LoC. Next, we created
an experiment. The run.py script (44 LoC) simply calls
a script to run security tests, shipped together with RIPE.
The collect.py script extracts RIPE-speciﬁc statistics
from the ﬁnal log (17 LoC). Note that for this security
experiment, we do not need any plot. In the end, the
effort took 75 LoC and less than one hour of work.
The experiment was run with the following command:
>> fex.py run −n ripe −t gcc_native clang_native
This produced the aggregated results shown in Table II. It
is interesting to note that even under our “insecure” con-
ﬁguration (Ubuntu 16.04 with disabled ASLR and build-
ing with disabled stack canaries and enabled executable
stack), only a handful of attacks were successful: through
the shellcode that creates a dummy ﬁle and through
return-into-libc. Another interesting result is that Clang
548
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 