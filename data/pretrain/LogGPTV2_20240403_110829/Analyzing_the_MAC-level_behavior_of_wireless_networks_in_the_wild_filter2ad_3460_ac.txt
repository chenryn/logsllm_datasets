our techniques.
4.2 Merging
To evaluate merging, we check its correctness and characterize
the quality of its time synchronization. Both are important to facil-
itate a wide range of MAC layer studies.
To check if halfWit merges correctly, we use it to merge the views
of the monitors in a simulation. For all three grid sizes, we ﬁnd
s
t
k
p
e
t
a
c
i
l
p
u
d
f
o
%
60
40
20
0
Ch. 1
Ch. 11
0
1
2
3
4
uncertainty (usecs)
s
t
k
p
e
t
a
c
i
l
p
u
d
f
o
%
60
40
20
0
2
3
1
0
4
uncertainty (usecs)
s
e
c
n
e
r
e
f
n
i
t
c
e
r
r
o
c
%
100
95
90
85
80
75
0
5
s
e
c
n
e
r
e
f
n
i
t
c
e
r
r
o
c
%
100
95
90
85
80
75
40
20
% pkts captured
60
80 100
20
0
80 100
% client pkts captured (est.)
60
40
Figure 5: The uncertainty of merging, shown as the histogram of
uncertainty values. Left: Real traces. Right: Simulator traces.
using the authoritative log that all duplicates and only duplicates
are removed. Because clock behaviors in the real and simulated
environments likely differ, this is not a litmus test. But, along with
manual veriﬁcation of merged real traces, it boosts our conﬁdence
in the tool’s implementation and its output with real traces.
Next, we evaluate synchronization quality, which tests the ro-
bustness of our timestamp mapping method to variabilities in real
clocks. We use real traces from a live network (described in detail
in the next section) for this experiment. It requires that quality be
measured without knowledge of ground truth. We accomplish this
by using the difference in the translated timestamps of the packets
that are identiﬁed as duplicates during the merge. This is a measure
of timestamp uncertainty. The minimum uncertainty value is zero,
for perfect synchronization. The maximum is 106 µsecs (half of the
minimum time to transmit an 802.11b packet), since duplicate iden-
tiﬁcation is limited to packets within that threshold. Each pair of
identiﬁed duplicates at each waterfall step produces one uncertainty
value; we study the distribution of the values. Obtaining 106 µsecs
for even a small fraction of values suggests an incorrect merge, as
there are probably unidentiﬁed duplicates beyond the threshold.
Figure 5 plots the histogram of uncertainty values. The left graph
is for real traces from Channels 1 and 11 which have four and ﬁve
monitors, respectively. For both, the merge is very precise. The
99.9 percentile uncertainty is 2 µsecs. The worst is 8 µsecs (not in
the graph). For comparison, the uncertainty of merging the simula-
tor traces is shown on the right. Due to possibly different times-
tamps on identical packets across monitors, rather than it being
zero, the 99.9 percentile uncertainty of even simulator merges is
2 µsecs. This suggests that potential inaccuracies of real clocks do
not signiﬁcantly increase the uncertainty of merging.
At 2 µsecs, the uncertainty in merged timestamps is smaller than
the slot time of 802.11b (20 µsecs). This enables a class of infer-
ences that are otherwise not possible. For instance, consider two
packets are in ﬂight simultaneously: we can distinguish a collision
in which the two sources start in the same slot from a failure of
carrier sense in which one source does not sense the other.
We now study the relationship between the quality of time syn-
chronization and the frequency of common references. To do so,
we compute uncertainty of merging two real traces for reference
periods of 1, 10, 100, 1000, and 10,000 seconds. A period of 10
seconds means that the successive references used for time trans-
lation are spread roughly 10 seconds apart; we ignore intermediate
references. The 99.9 percentile uncertainty is 2 µsecs for 100 sec-
onds or less, 18 µsecs for 1000 seconds, and 106 µsecs for 10,000
seconds. Given that APs send beacons roughly every 100 ms, this
implies that the uncertainty can be kept down to 2 µsecs as long as
the two traces have in common at least 0.1% of the beacons from
at least one AP.
Figure 6: Left: The accuracy of inferring packet reception. Each
point corresponds to a trace with a certain percentage of cap-
tured packets (x-axis); the y-axis shows the percentage of pack-
ets whose reception status was correctly inferred. Right: The
accuracy of inferences as a function of nitWit’s estimate of the
percentage of client’s packets captured. The y-axes start at 75%.
To compare our technique with that of Yeo et al. [27], we use
their method to merge two real traces of different lengths. We
ﬁnd that the uncertainty increases with length. The 99.9 percentile
value is 5 µsecs for 1-hour traces, 12 µsecs for 2-hour traces, and
106 µsecs for 4-hour traces. (These results are better than those
reported by Yeo et al., who obtain a 40-µsec uncertainty for two
12.5-minute traces.) The last merge is likely incorrect.
4.3
Inference
To evaluate nitWit, we run it over simulator traces and study its
ability to infer packet reception statuses and missing packets.
The left side of Figure 6 shows how accurately nitWit infers whe-
ther packets were received. Correct inferences are shown as a func-
tion of the percentage of the total packets captured in a trace. We
obtain traces with different capture percentages by using different
monitors and merge combinations. Correctness and capture per-
centages are computed using the authoritative simulator log.
We see that nitWit is quite accurate: its accuracy is 95% even
when the trace contains only half of the total packets and 90% even
when it contains only a third. In our data, a common scenario in
which nitWit is relatively less accurate is when it observes one or
more ACKs without corresponding DATA packets; the ACKs by
themselves yield little information about their reception.
Interestingly, we ﬁnd that nitWit can estimate when its inferences
will be relatively less accurate. This is because the accuracy of
the inferences for a client depends on the fraction of the client’s
packets that were captured. This fraction can in turn be estimated
from traces without knowledge of ground truth as a side-effect of
how many missing packets are inferred. The capture estimate we
compute is the ratio of the number of packets captured for the client
to the sum of the packets captured and inferred for the client.
The right side of Figure 6 shows how accurately nitWit infers re-
ceptions for a client’s packets as a function of this capture estimate.
Clients are binned by their capture estimate into 10%-wide bins,
and the average accuracy of the bin is plotted as the y-value. Over-
all, nitWit does well even for clients from whom a small fraction of
packets are captured. This is because the monitors often capture the
other end of the conversation. We see that accuracy decreases with
the estimate of packets captured. This enables a user of nitWit to
judge the accuracy of inferences for a client and, if need be, focus
on clients with accurate inferences.
Next, we study the ability of nitWit to complete a trace by in-
ferring missing packets. Figure 7 plots the percentage of packets
that are either inferred or captured versus the percentage captured.
)
d
e
r
r
e
f
n
i
+
d
e
r
u
t
p
a
c
(
%
100
80
60
40
20
0
0
40
20
% pkts captured
60
80 100
)
d
e
r
r
e
f
n
i
+
d
e
r
u
t
p
a
c
(
%
100
90
80
70
70
1.10
1.05
.
b
o
r
p
.
p
e
c
e
r
l
a
u
t
c
a
/
.
t
s
e
1.00
70
90%
70%
50%
30%
10%
95
10%
30%
50%
70%
90%
95
80
75
90
% pkts captured
85
80
75
90
% pkts captured
85
Figure 7: The ability of Wit to infer missing packets.
s
t
k
p
f
o
%
100
80
60
40
20
0
100x100
600x600
900x900
-20
-10
0
10
20
error
Figure 8: The CDF of error in estimating the number of con-
tenders, for all three grid sizes.
Traces are binned into 5%-wide bins based on their capture per-