There appears to be a race condition in the index CUDA kernels where multiple
threads are writing to the same address.
Reproducer code:
    import torch
    ref = torch.arange(0, 20).view(4, 5).float()
    indexer= [slice(None), [1 for i in range(5)]]
    src = torch.randperm(ref[indexer].numel()).view(ref[indexer].size()).float()
    print('ref', ref)
    print('indexer', indexer)
    print('ref[indxer]', ref[indexer])
    print('src', src)
    cpu_ans = ref.clone()
    cpu_ans[indexer]=src
    nump_ans = ref.double().numpy()
    idxs = tuple(indexer)
    nump_ans[idxs] = src.double().numpy()
    print('ref numpy\n', ref.numpy())
    print('idx numpy\n', idxs)
    print('src numpy\n', src.numpy())
    gpu_ans = ref.cuda()
    gpu_ans[indexer]=src.cuda()
    print('cpu result', cpu_ans)
    print('numpy result\n', nump_ans)
    print('cuda result', gpu_ans)
Output:
    cpu result 
      0   6   2   3   4
      5   4   7   8   9
     10  13  12  13  14
     15  19  17  18  19
    [torch.FloatTensor of size 4x5]
    numpy result
     [[  0.   6.   2.   3.   4.]
      [  5.   4.   7.   8.   9.]
      [ 10.  13.  12.  13.  14.]
      [ 15.  19.  17.  18.  19.]]
    cuda result 
      0     6     2   3   4
      5  **16**   7   8   9
     10    13    12  13  14
     15   **5**  17  18  19
    [torch.cuda.FloatTensor of size 4x5 (GPU 0)]
Looks like a preprocessing step will need to be added to identify which source
indices is allowed to write to destination.
I'm not sure if this will also be a problem in the other advanced indexing
kernels.