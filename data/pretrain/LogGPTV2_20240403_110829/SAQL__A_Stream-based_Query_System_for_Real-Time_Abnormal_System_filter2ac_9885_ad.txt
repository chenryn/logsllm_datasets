if depQ == masQ then
return execution results of masQ;
else if Both masQ and depQ deﬁne states then
if masQ and depQ have the same sliding window length
and masQ deﬁnes a super set of state ﬁelds of depQ
then
Fetch the state aggregation results of masQ,
enforce additional ﬁlters, and feed into the
execution pipeline of depQ;
else
Fetch the matched events of masQ, enforce additional
ﬁlters, and feed into the execution pipeline of depQ;
1. The scheme ﬁrst checks if M is empty (i.e., no con-
current running queries). If so, the scheme sets newQ
as a master query, stores it in M, and executes it.
2. If M is not empty, the scheme checks newQ against
every master query masQi for compatibility and tries
to construct a semantic cover covQ. If the construc-
tion is successful, the scheme then checks whether
covQ equals masQi.
3. If covQ is different from masQi, the scheme updates
the master query by replacing masQi with covQ and
updates all the dependent queries of masQi to covQ.
646    27th USENIX Security Symposium
USENIX Association
in
1
4. The scheme then adds newQ as a new dependent
query of covQ, and executes newQ based on covQ.
key
steps
Algorithm
and execDep().
5. Finally, if there are no master queries found to be
compatible with newQ, the scheme sets newQ as a
new master query, stores it in M, and executes it.
Two
are
constructSemanticCover()
The
construction of a semantic cover requires that (1) the
masQ and depQ both deﬁne a single event pattern
and (2) their event types, operation types, and sliding
window types must be the same1. The scheme then
explores the following four optimization dimensions:
event attributes, agent ID, sliding window, and state
aggregation. Speciﬁcally, the scheme ﬁrst constructs
an event pattern cover by taking the union of the two
queries’ event attributes and agent IDs, and taking the
greatest common divisor (GCD) of the window lengths.
It then constructs a state block cover by taking the union
of the two queries’ state ﬁelds (if applicable), and returns
the semantic cover by concatenating the event pattern
cover, the state block cover, and the rest parts of masQ.
The execution of depQ depends on the execution of
masQ. If two queries are the same, the engine directly
uses the execution results of masQ as the execution re-
sults of depQ. Otherwise, the engine fetches the interme-
diate results from the execution pipeline of masQ based
on the level of compatibility. The scheme currently en-
forces the results sharing in two execution phases: event
pattern matching and stateful computation: (1) if both
dep and masQ deﬁne states and their sliding window
lengths are the same, the engine fetches the state aggre-
gate results of masQ; (2) otherwise, the engine fetches
the matched events of masQ without its further state ag-
gregate results. The engine then enforces additional ﬁl-
ters and feed the ﬁltered results into the rest of the exe-
cution pipeline of depQ for further execution.
6 Deployment and Evaluation
We deployed the SAQL system in NEC Labs America
comprising 150 hosts (10 servers, 140 employee stations;
generating around 3750 events/s). To evaluate the ex-
pressiveness of SAQL and the SAQL’s overall effective-
ness and efﬁciency, we ﬁrst perform a series of attacks
based on known exploits in the deployed environment
and construct 17 SAQL queries to detect them. We fur-
ther conduct a pressure test to measure the maximum per-
formance that our system can achieve. Finally, we con-
duct a performance evaluation on a micro-benchmark (64
queries) to evaluate the effectiveness of our query engine
in handling concurrent queries. In total, our evaluations
use 1.1TB of real system monitoring data (containing 3.3
1We leave the support for multiple event patterns for future work
billion system events). All the attack queries are avail-
able in Appendix, and all the micro-benchmark queries
are available on our project website [19].
6.1 Evaluation Setup
The evaluations are conducted on a server with an In-
tel(R) Xeon(R) CPU E1650 (2.20GHz, 12 cores) and
128GB of RAM. The server continuously receives a
stream of system monitoring data collected from the
hosts deployed with the data collection agents. We de-
veloped a web-based client for query submission and de-
ployed the SAQL system on the server for query execu-
tion. To reproduce the attack scenarios for the perfor-
mance evaluation in Section 6.4, we stored the collected
data in databases and developed a stream replayer to re-
play the system monitoring data from the databases.
6.2 Attack Cases Study
We performed four major types of attack behaviors in
the deployed environment based on known exploits: (1)
APT attack [2, 1], (2) SQL injection attack [43, 78], (3)
Bash shellshock command injection attack [7], and (4)
suspicious system behaviors.
6.2.1 Attack Behaviors
APT Attack: We ask white hat hackers to perform an
APT attack in the deployed environment, as shown in
Figure 3. Below are the attack steps:
c1 Initial Compromise: The attacker sends a crafted
email to the victim. The email contains an Excel ﬁle
with a malicious macro embedded.
c2 Malware Infection: The victim opens the Excel ﬁle
through the Outlook client and runs the macro, which
downloads and executes a malicious script (CVE-
2008-0081 [6]) to open a backdoor for the attacker.
c3 Privilege Escalation: The attacker enters the victim’s
machine through the backdoor, scans the network
ports to discover the IP address of the database, and
runs the database cracking tool (gsecdump.exe) to
steal the credentials of the database.
c4 Penetration into Database Server: Using the creden-
tials, the attacker penetrates into the database server
and delivers a VBScript to drop another malicious
script, which creates another backdoor.
c5 Data Exﬁltration: With the access to the database
server, the attacker dumps the database content using
osql.exe and sends the data dump back to his host.
For each attack step, we construct a rule-based
anomaly query (i.e., Queries 7 to 11). Besides, we con-
struct 3 advanced anomaly queries:
USENIX Association
27th USENIX Security Symposium    647
Figure 3: Environmental setup for the APT attack
• We construct an invariant-based anomaly query
(Query 12) to detect the scenario where Excel executes
a malicious script that it has never executed before:
The invariant contains all unique processes started by
Excel in the ﬁrst 100 sliding windows. During the
detection phase, new processes that deviate from the
invariant will be reported as alerts. This query can
be used to detect the unseen suspicious Java process
started by Excel (i.e., step c2).
• We construct a time-series anomaly query (Query 13)
based on SMA to detect the scenario where abnor-
mally high volumes of data are exchanged via network
on the database server (i.e., step c5): For every process
on the database server, this query detects the processes
that transfer abnormally high volumes of data to the
network. This query can be used to detect the large
amount of data transferred from the database server.
• We also construct an outlier-based anomaly query
(Query 14) to detect processes that transfer high vol-
umes of data to the network (i.e., step c5): The query
detects such processes through peer comparison based
on DBSCAN. The detection logic here is different
from Query 13, which detects anomalies through com-
parison with historical states based on SMA.
Note that the construction of these 3 queries assumes
no knowledge of the detailed attack steps.
SQL Injection Attack: We conduct a SQL injection at-
tack [54] for a typical web application server conﬁgura-
tion. The setup has multiple web application servers that
accept incoming web trafﬁcs to load balance. Each of
these web servers connects to a single database server to
authenticate users and serves dynamic contents. How-
ever, these web applications provide limited input saniti-
zation and thus are susceptible to SQL injection attack.
We use SQLMap [22] to automate the attack against
one of the web application servers.
In the process of
detecting and exploiting SQL injection ﬂaws and taking
over the database server, the attack generates an exces-
sive amount of network trafﬁc between the web appli-
cation server and the database server. We construct an
outlier-based anomaly query (Query 15) to detect abnor-
mally large data transfers to external IP addresses.
Bash Shellshock Command Injection Attack: We con-
duct a command injection attack against a system that in-
stalls an outdated Bash package susceptible to the Shell-
shock vulnerability [7]. With a crafted payload, the at-
tacker initiates a HTTP request to the web server and
tivities of Dropbox processes.
opens a Shell session over the remote host. The behav-
ior of the web server in creating a long-running Shell
process is an outlier pattern. We construct an invariant-
based anomaly query (Query 16) to learn the invariant of
child processes of Apache, and use it to detect any un-
seen child process (i.e., /bin/bash in this attack).
Suspicious System Behaviors: Besides known threats,
security analysts often have their own deﬁnitions of sus-
picious system behaviors, such as accessing credential
ﬁles using unauthorized software and running forbidden
software. We construct 7 rule-based queries to detect a
representative set of suspicious behaviors:
• Forbidden Dropbox usage (Query 17): ﬁnding the ac-
• Command history probing (Query 18): ﬁnding the
processes that access multiple command history ﬁles
in a relatively short period.
• Unauthorized password ﬁles accesses (Query 19):
ﬁnding the unauthorized processes that access the pro-
tected password ﬁles.
• Unauthorized login logs accesses (Query 20): ﬁnding
the unauthorized processes that access the log ﬁles of
login activities.
• Unauthorized SSH key ﬁles accesses (Query 21): ﬁnd-
ing the unauthorized processes that access the SSH
key ﬁles.
• Forbidden USB drives usage (Query 22): ﬁnding the
• IP frequency analysis (Query 23): ﬁnding the pro-
processes that access the ﬁles in the USB drive.
cesses with high frequency network accesses.
6.2.2 Query Execution Statistics
To demonstrate the effectiveness of the SAQL system in
supporting timely anomaly detection, we measure the
following performance statistics of the query execution:
• Alert detection latency:
the difference between the
time that the anomaly event gets detected and the time
that the anomaly event enters the SAQL engine.
• Number of states: the number of sliding windows en-
countered from the time that the query gets launched
to the time that the anomaly event gets detected.
• Average state size: the average number of aggregation
results per state.
The results are shown in Table 3. We observe that:
(1) the alert detection latency is low (≤10ms for most
queries and <2s for all queries). For sql-injection, the
latency is a bit larger due to the additional complexity of
the speciﬁed DBSCAN clustering algorithm in the query;
(2) the system is able to efﬁciently support 150 enterprise
hosts, with < 10% CPU utilization and <2.7GB memory
utilization. Note that this is far from the full processing
power of our system on the deployed server, and our sys-
tem is able to support a lot more hosts (as experimented
648    27th USENIX Security Symposium
USENIX Association
 Windows ClientMail ServerDB ServerFirewallInternetWindows DCc1c2c3c4c5AttackerSAQL Query
apt-c1
apt-c2
apt-c3
apt-c4
apt-c5
apt-c2-invariant
apt-c5-timeseries
apt-c5-outlier
shellshock
sql-injection
dropbox
command-history
password
login-log
sshkey
usb
ipfreq
N/A
N/A
N/A
N/A
N/A
5
812
812
3
14
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
5
3321
3321
3
13841
N/A
N/A
N/A
N/A
N/A
N/A
N/A
Table 3: Execution statistics of 17 SAQL queries for four major types of attacks
Alert Detection Latency Num. of States Tot. State Size Avg. State Size CPU Memory
1.7GB
1.8GB
1.6GB
1.5GB
1.6GB
1.8GB