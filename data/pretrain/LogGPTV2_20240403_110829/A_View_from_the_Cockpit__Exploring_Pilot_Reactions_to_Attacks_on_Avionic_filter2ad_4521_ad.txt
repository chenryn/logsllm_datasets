not have. This allows more time for participants to respond, in
10
400500600700800900100011001200Height above ground (ft)0.00.20.40.60.81.01.21.41.61.82.02.22.4Distance to touchdown (miles)contrast to TCAS or GPWS attacks which trigger alarms and
need immediate attention.
Despite this, 19 (63.3%) participants felt that the attack
made the aircraft less safe. A number of participants noted
that this attack would be harder to deal with in other situations.
In worse weather conditions such as extremely low visibility
they would have fewer reference points against which they
could check the glideslope. This would make it hard to even
identify that an issue exists until very late in the approach.
Some participants also commented that if the glideslope was
short, rather than long, of the runway threshold (i.e. touchdown
was before the runway started), it would be signiﬁcantly more
dangerous. This is because the approach might look normal
until very late at which point the aircraft would be at risk of
landing off the runway.
Although there was little additional workload, 26 (86.7%)
participants performed at least one go-around as a result of
being unsure about the approach, instead seeking the safest
option possible. Here, this involved taking a second approach,
in many cases with a different landing system. Some pilots
noted that low fuel situations would limit the options and
possibly only allow one more approach, making the attack
more difﬁcult to manage.
As with TCAS and GPWS, the attack caused ‘some’ dis-
trust in aircraft systems, with 23 (76.7%) participants remark-
ing ‘some’ or ‘signiﬁcant’ impact. However, some participants
correctly identiﬁed that the ground systems were at fault and so
did not distrust the aircraft. In this situation, they were able to
diagnose the issue and ‘cut out’ ILS, thus mitigating the attack.
As such, attackers would have to consider other vectors if they
wished to guarantee disruption. This is supported by responses
to Q5, on trusting the system later. In Tab. I we can see that
of the 26 (86.7%) participants, who did perform a go-around,
all but one would not trust the GS on a second approach.
Evaluation: Generally, this attack was considered a
nuisance rather than a signiﬁcant safety issue but did manage
to disrupt. Our results indicate that:
•
• Whilst the attack consistently caused ﬁrst approach
disruption, its effect was limited beyond this as par-
ticipants used other approach methods,
The subtlety of the attack and a lack of alarms meant
that aircraft got close to the runway—within a couple
of minutes before landing—before they had to abort,
After an initial problem diagnosis, the attack was fairly
easily managed with little excess workload,
Variants such as poor weather may be much more
difﬁcult to handle and pose a greater safety risk.
•
•
Despite the limited effect, the attack can cause short term dis-
ruption through triggering go-arounds, in turn burning excess
fuel and increasing delay. However, it is likely that attacking
consecutive aircraft would see ATC instructing aircraft not to
use ILS. Whilst a more sophisticated attacker might tamper
with multiple systems, this signiﬁcantly increases cost and the
risk of detection.
11
TABLE IV: Summary of attack costs, equipment requirements,
difﬁculty and overall impact. H is high, M is medium and L
is low.
GS
Actor
M
Cost
M
Equipment
Difﬁculty
M
Disruption/Impact M
Safety Effect
M
Attack Name
TCAS
GPWS
H
H
H
H
H
L
M
L
L
L
TABLE V: Mapping of attacks against threat actors.
Actor
Activists
Terrorists
Nation State
GS



Attack Name
TCAS
GPWS

/




VII. DISCUSSION
We now discuss results across attacks and within wider
contexts such as cost and compared to system faults.
A. Attack Comparison
In Tab. IV we provide a high-level summary of the costs,
equipment requirements and difﬁculty based on Sec. IV, fol-
lowed by the potential for disruption/impact and safety effect
derived directly from participant impact (Q1), and safety (Q6)
assessment in Sec VI.3 Using this, we then map the attacks to
threat actors in Tab. V.
Whilst all aircraft were handled safely in our experiment,
there appears to be a meaningful effect on safety from the
TCAS attack, with variants on the GS attack also able to create
unsafe situations. Despite participants taking the safest option
in the circumstances, TCAS saw 93.3% participants feel that
the attack made the aircraft less safe with GS at 63.3%. For
TCAS, this could be the uncertainty of the situation, with pilots
not expecting false alarms; for GS, the safety concern comes
from how late the discrepancy is apparent and the situation
this leaves the aircraft in. GPWS safety was split with 46.7%
feeling that the aircraft was less safe. This is due to the terrain
avoidance maneuver being the de facto safe option, making the
automatic response the safest response. However, the GPWS
attack highlights the interplay of safety and security; even
though most pilots took the safe option, they still felt they
were compromised by factors out of their control.
All three attacks have the potential to cause some degree of
disruption, but the level and means vary. Clearly, TCAS has
the greatest potential with the attack causing participants to
respond for a longer portion of ﬂight than the others. This
is in contrast to GS and GPWS both of which caused an
initial disruption but were then quickly managed, with most
participants landing by their second approach. However, TCAS
3For example, impact an average impact response of ‘signiﬁcant impact’
is a score of ‘high’. With safety, we base the score on the proportion of
respondents judging there to be a safety impact.
is also the most complex attack to carry out, requiring high
skill and resource levels. A more simplistic version may be
achievable by less capable, highly determined attackers such as
terrorist groups. For less capable attackers, the less disruptive
attacks such as GS and GPWS are in scope, but as shown, the
effects are short-term and identiﬁed faster.
By comparing key ﬁndings across the three scenarios, we
can extract some general insights:
2)
1) Whilst alarms force action they are quickly turned
off or ignored if considered spurious. In the case
of TCAS and GPWS, the procedural need to respond
to alarms meant
that participants looked at ways
to ‘manage’ this which sometimes involved turning
the system to a lower sensitivity level or off. Since
these systems are all key to safety, having to switch
them off because of their susceptibility to attack is
suboptimal.
Attackers can force pilots away from systems. Best
demonstrated in the GS scenario, attacking systems
makes participants treat them as faulty and seek to
use others. This can lead to further disruption but
limits the long-term effect of attacks.
3) Gray areas can be managed using existing proce-
dure but variability is high. Whilst safety was com-
promised by some attacks, all participants handled
them without major incident. However, the eventual
response and steps to get there varied—signiﬁcant in
some scenarios—partly due to difﬁculty in diagnosing
the problem. This could be exploited by an attacker
to create confusion.
The lack of security for the systems in our scenarios not only
allows attackers to cause disruption directly but can also mean
that the systems become unusable and so are switched off.
Considering that pilots are taught to trust cockpit systems and
rely on them being accurate, this is a dangerous combination.
B. Comparison to System Faults
Many faults on an aircraft are identiﬁed and reported by
on board computers then presented to ﬂight crew through
screens, warning lights or alarms. Extensive development and
testing of the aircraft allows potential faults to be identiﬁed
and management methods to be provided to crew, usually
through checklists. This means that crew are prepared for
faulty behavior, usually with a predeﬁned series of actions to
take for the safest outcome.
Our scenarios take advantage of edge cases in procedure or
develop in ways which do not trigger alarms. Whilst they might
have similarities to faults—and are handled in this way by
most participants—this can be a confusion factor. For example,
in the GS attack, participants noted the slow development
of the attack with no other warnings. In the case of TCAS,
whilst alarms were going off, participants commented that
no checklist exists for spurious TCAS, which led them to
eventually turn the sensitivity down as the best decision in
the circumstances. Because of this slight difference, although
existing training helps pilots to ultimately handle the issue, it
might not help them diagnose the problem in the ﬁrst place.
12
C. Additional Impact Factors
As discussed in Sec. VI, participants highlighted a number
of other factors which would affect the impact of attacks.
Weather conditions were prominent; all scenarios would be
more difﬁcult to handle in poor visibility. Some participants
noted it would be hard to identify the GS attack under auto-
matic landing conditions (i.e. poor visibility), leaving much
less time for pilots to respond. Other contributing factors
include tiredness and terrain. In response to the GPWS attack,
one participant who chose not to go around commented that
their action in a real aircraft would depend on tiredness, as
well as weather and how busy the crew were. Again in the
GPWS attack, others identiﬁed that terrain surrounding the
airport affects their choice—they would be much more likely
to abort an approach in challenging terrain, and less if they
are familiar with the airport.
D. Cost of Disruption
We have demonstrated the ability for these attacks to cause
missed approaches and diversions. With this in mind, we can
estimate the resulting costs.
For go-arounds, as caused by GS or GPWS, we can
calculate the cost of a missed approach using a representative
Boeing aircraft.4 For a smaller 737-800 aircraft, the missed
approach uses 127 kg (41.79 gal) more fuel than a successful
one; for the larger 777-200, it is 399 kg (111.55 gal) more [38].
Coupled with a nominal jet fuel cost of 184.58 c/gal, this costs
approximately $77 for the 737 or $205 for the 777.5 Added to
the expense of further time in the air—more difﬁcult to predict
as it depends on factors such as the airﬁeld and trafﬁc—plus
a second approach, which costs approximately $139 (using
230 kg, or 75.68 gal) or for the 737, or $516 for the 777 (using
850 kg, or 279.69 gal), this becomes expensive for the airline.
All three attacks created the possibility of having to divert,
with four participants choosing to follow this through during
the TCAS scenario. Diversions add further expense on top
of excess fuel burn, as well as having knock-on effects for
scheduling or causing passenger inconvenience. The UK Civil
Aviation Authority estimates that these can cost an airline
between £10,000–£80,000, depending on the size of the air-
craft and location of diversion [9]. For example, passenger
disruption causing diversion aboard a Norwegian ﬂight cost
e100,000 in 2018 [13], [16]. Closed airports are similarly
costly, with drones closing London Gatwick for two days in
December 2018 and costing airline Easyjet £15 million [30].
E. Simulation for Training
To assess whether responses were realistic, we asked each
participant whether their response to each scenario would be
the same in a real aircraft. We found that for:
• GPWS, 27 (90.0%) would do the same, and the
remaining three would go around in the same scenario
again,
TCAS, 30 (100.0%) would do the same,
•
4This is chosen due to the public availability of fuel usage information
about Boeing aircraft.
5Calculated using IATA Jet Fuel Price Monitor for 18th January 2019 [26].
• Glideslope, 28 (93.3%) would do the same with the
to
remaining two opting to go around and revert
RNAV.
We asked each participant for their views the value of such
experiments or training in preparation for cyber attack. All
participants felt the scenarios were useful, and 28 (93.3%)
commented that training for cyber attacks using a simulator
would be valuable.
The results suggest that this method can be valuable both in
identifying crew response to attacks and providing cyber attack
readiness. Furthermore, the fact that the scenarios in this paper
lie in procedural gray areas and do not have a series of steps to
resolve them provides an ideal opportunity for training. One
point of caution is negative training, with some participants
noting that care must be taken to avoid training pilots to ignore
or distrust their systems.
Finding a balance between awareness and negative training
is important to fully prepare pilots for attack scenarios. Cur-
rently, pilots are trained to handle a wide range of aircraft faults
from diagnosis through to remedy or mitigation. The capability
to address these faults is reassessed regularly as part of pilot
license revalidation—in the case of commercial pilots, this is
usually once or twice a year, often in a ﬂight simulator.
One way to approach this balance would be to include
attack simulations in training and revalidation based on known-
possible effects, which could be derived from penetration
testing or reports of real incidents. Importantly, this would
need to be coupled with a comparison to existing faults, how an
attack differs and an honest discussion of the likelihood of such
an attack occurring. Since pilots are already used to the fact
that faults can occur at any time, this simply augments their
knowledge with fault diagnosis-style tools for attacks instead.
Ultimately, there should be little difference in how a fault
and an attack is handled on the ﬂight deck as both impinge on
the function of the aircraft. As our results show, existing fault
handling procedure often gets pilots part-way to managing
attacks, so additional training can extend these procedures to
cover cases where attack effects deviate from failures. We
would expect further research to establish such best-practice
procedures, with input from both the computer security and
the aviation communities.
F. Experimental Limitations
As addressed in Sec. V, there are some limitations to
our experimental approach such as not being a full crew
complement or taking place inside a full replica cockpit. We
acknowledge that this may have some effect on the results and
so surveyed participants about it, asking if they felt limited by
the simulation set up (Q23, App. B), with 8 feeling heavily
limited, 18 somewhat limited and 4 not limited. The average
response was ‘somewhat’, with the main limits being the lack
of a second crew member and the general (rather than Airbus
speciﬁc) controls. We note that these ﬁgures are subject to
some bias due to the experimenter interviewing the participant.
Prior Knowledge: Since we did not have existing
access to pilots to sample, we had to recruit externally. Our
recruitment material revealed that the experiment related to
aviation cyber security in general but no further details such
as the systems being attacked. We felt
this level of
prior information was important in recruiting participants as
attending an experimental session required a reasonably high
level of effort on their part, namely in arranging around a busy
ﬂying schedule and usually long-distance travel to our lab.
that
According to methodology research on human participant
studies, we consider our participants naive since they are aware
of the topic but not its methods or expectations [36]. Relevant
literature suggests that having fully non-naive participants
can affect results slightly—one study asked participants to
complete a series of tasks twice with some time gap, with up to
a 25% reduction in effect [8]. However, a meta-study identiﬁed
works which suggest that non-naive participants can also be
less likely to conform to experimenter expectations [34]. Since
our participants only had knowledge of the topic but no
speciﬁcs, we are conﬁdent that participants did not lose naivety.
Even if such an effect is signiﬁcant, we expect that participants
were more likely to anticipate malfunction and so be more
alert, providing a ‘best case’ reaction.
VIII. LESSONS LEARNED
Having considered the results of our study, we now look
at lessons arising from it, applicable to aviation and transport
or infrastructure security scenarios with humans in the loop.
a) Diagnosis is key: Our results show that it is un-
realistic to rely on humans to plug the gap between safety
and security. Pilots are extensively trained to deal with the
many faults which can emerge when ﬂying an aircraft, and
this was reﬂected in the results. However, the attacks generated
situations which shared some features with faults but largely
were different; they lacked indication of failure. This meant
that even though they knew something was wrong, a lot of time
had to be spent diagnosing the issue. One way to improve on
this would be to factor attack scenarios into existing simulator
training schedules, and to add failures caused by attack into
existing fault diagnosis and handling procedures. As well as
general preparedness, this might help to reduce startle should
an attack occur. Furthermore, being upfront with crew about
the effects and likelihood of attacks will help them handle said
attacks better if they happen.
b) Value of simulation: We uncovered a number of
factors which affect how an attack develops that would have
been out-of-scope if we had focused on individual components.
By taking a wider system view with a simulator, we could
allow scenarios to unfold, providing more information about
how pilots responded to the attacks. We could also gather
information about other factors affecting the response which
we might not have considered in our initial analysis, such
as typical system behavior or other air trafﬁc. These factors
are important in assessing the true impact an attack has—for
example, with GPWS, our paper-based analysis indicated that
it would be more problematic than it turned out to be. More
generally, this approach is especially valuable in systems where
humans play a key role and are used to simulators as part of
their training; examples include transport such as trains, or
nuclear power plant operators.
c) Real usage matters: One of the key motivators of
this work is an attempt to understand whether what operators
should do during an attack differs to what actually happens.
13
Aviation is one amongst many areas of infrastructure known
for strict safety rules and robust policies. In theory, systems in
these domains should be predictable under attack. However,
we found that quirks and oddities of such complex systems
can initially mask attacks; in our case, pilots were willing to
deviate from strict procedure in order to manage workload or
distraction. These were reasoned decisions with the intention
to maintain or improve safety, but often ran contrary to what
the rules or regulations say. For instance, TCAS is considered