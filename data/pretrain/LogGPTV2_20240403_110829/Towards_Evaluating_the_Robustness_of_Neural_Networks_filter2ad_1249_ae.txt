image.
Notice that
the all-black image requires no change to
become a digit 1 because it is initially classiﬁed as a 1, and
the all-white image requires no change to become a 8 because
the initial image is already an 8.
Runtime Analysis. We believe there are two reasons why one
may consider the runtime performance of adversarial example
generation algorithms important: ﬁrst, to understand if the
performance would be prohibitive for an adversary to actually
mount the attacks, and second, to be used as an inner loop in
adversarial re-training [11].
Comparing the exact runtime of attacks can be misleading.
For example, we have parallelized the implementation of
our L2 adversary allowing it
to run hundreds of attacks
simultaneously on a GPU, increasing performance from 10×
to 100×. However, we did not parallelize our L0 or L∞
attacks. Similarly, our implementation of fast gradient sign
is parallelized, but JSMA is not. We therefore refrain from
giving exact performance numbers because we believe an
unfair comparison is worse than no comparison.
All of our attacks, and all previous attacks, are plenty
efﬁcient to be used by an adversary. No attack takes longer
than a few minutes to run on any given instance.
When compared to L0, our attacks are 2 × −10× slower
than our optimized JSMA algorithm (and signiﬁcantly faster
than the un-optimized version). Our attacks are typically 10×
−100× slower than previous attacks for L2 and L∞, with
exception of iterative gradient sign which we are 10× slower.
VIII. EVALUATING DEFENSIVE DISTILLATION
Distillation was initially proposed as an approach to reduce
a large model (the teacher) down to a smaller distilled model
[19]. At a high level, distillation works by ﬁrst training the
teacher model on the training set in a standard manner. Then,
we use the teacher to label each instance in the training set with
50
soft labels (the output vector from the teacher network). For
example, while the hard label for an image of a hand-written
digit 7 will say it is classiﬁed as a seven, the soft labels might
say it has a 80% chance of being a seven and a 20% chance
of being a one. Then, we train the distilled model on the soft
labels from the teacher, rather than on the hard labels from
the training set. Distillation can potentially increase accuracy
on the test set as well as the rate at which the smaller model
learns to predict the hard labels [19], [30].
Defensive distillation uses distillation in order to increase
the robustness of a neural network, but with two signiﬁcant
changes. First, both the teacher model and the distilled model
are identical in size — defensive distillation does not result
in smaller models. Second, and more importantly, defensive
distillation uses a large distillation temperature (described
below) to force the distilled model to become more conﬁdent
in its predictions.
Recall that, the softmax function is the last layer of a neural
network. Defensive distillation modiﬁes the softmax function
to also include a temperature constant T :
softmax(x, T )i =
exi/T(cid:11)
j exj /T
It is easy to see that softmax(x, T ) = softmax(x/T, 1). Intu-
itively, increasing the temperature causes a “softer” maximum,
and decreasing it causes a “harder” maximum. As the limit
of the temperature goes to 0, softmax approaches max; as
the limit goes to inﬁnity, softmax(x) approaches a uniform
distribution.
Defensive distillation proceeds in four steps:
1) Train a network, the teacher network, by setting the
temperature of the softmax to T during the training
phase.
2) Compute soft labels by apply the teacher network to
each instance in the training set, again evaluating the
softmax at temperature T .
3) Train the distilled network (a network with the same
shape as the teacher network) on the soft labels, using
softmax at temperature T .
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
4) Finally, when running the distilled network at test time
(to classify new inputs), use temperature 1.
A. Fragility of existing attacks
We brieﬂy investigate the reason that existing attacks fail
on distilled networks, and ﬁnd that existing attacks are very
fragile and can easily fail to ﬁnd adversarial examples even
when they exist.
L-BFGS and Deepfool fail due to the fact that the gradient
of F (·) is zero almost always, which prohibits the use of the
standard objective function.
When we train a distilled network at temperature T and
then test it at temperature 1, we effectively cause the inputs to
the softmax to become larger by a factor of T . By minimizing
the cross entropy during training, the output of the softmax
is forced to be close to 1.0 for the correct class and 0.0 for
all others. Since Z(·) is divided by T , the distilled network
will learn to make the Z(·) values T times larger than they
otherwise would be. (Positive values are forced to become
about T times larger; negative values are multiplied by a
factor of about T and thus become even more negative.)
Experimentally, we veriﬁed this fact: the mean value of the
L1 norm of Z(·) (the logits) on the undistilled network is
5.8 with standard deviation 6.4; on the distilled network (with
T = 100), the mean is 482 with standard deviation 457.
Because the values of Z(·) are 100 times larger, when
we test at temperature 1, the output of F becomes  in all
components except for the output class which has conﬁdence
1−9 for some very small  (for tasks with 10 classes). In fact,
in most cases,  is so small that the 32-bit ﬂoating-point value
is rounded to 0. For similar reasons, the gradient is so small
that it becomes 0 when expressed as a 32-bit ﬂoating-point
value.
This causes the L-BFGS minimization procedure to fail to
make progress and terminate. If instead we run L-BFGS with
our stable objective function identiﬁed earlier, rather than the
objective function lossF,l(·) suggested by Szegedy et al. [46],
L-BFGS does not fail. An alternate approach to ﬁxing the
attack would be to set
(cid:2)
F
(x) = softmax(Z(x)/T )
where T is the distillation temperature chosen. Then mini-
mizing lossF (cid:2),l(·) will not fail, as now the gradients do not
vanish due to ﬂoating-point arithmetic rounding. This clearly
demonstrates the fragility of using the loss function as the
objective to minimize.
JSMA-F (whereby we mean the attack uses the output of
the ﬁnal layer F (·)) fails for the same reason that L-BFGS
fails: the output of the Z(·) layer is very large and so softmax
becomes essentially a hard maximum. This is the version of the
attack that Papernot et al. use to attack defensive distillation
in their paper [39].
JSMA-Z (the attack that uses the logits) fails for a com-
pletely different reason. Recall that in the Z(·) version of
51
the attack, we use the input to the softmax for computing
the gradient instead of the ﬁnal output of the network. This
removes any potential
issues with the gradient vanishing,
however this introduces new issues. This version of the attack
is introduced by Papernot et al. [38] but it is not used to attack
distillation; we provide here an analysis of why it fails.
Since this attack uses the Z values, it is important to realize
the differences in relative impact. If the smallest input to
the softmax layer is −100, then, after the softmax layer, the
corresponding output becomes practically zero. If this input
changes from −100 to −90, the output will still be practically
zero. However, if the largest input to the softmax layer is 10,
and it changes to 0, this will have a massive impact on the
softmax output.
Relating this to parameters used in their attack, α and β
represent the size of the change at the input to the softmax
layer. It is perhaps surprising that JSMA-Z works on un-
distilled networks, as it treats all changes as being of equal
importance, regardless of how much they change the softmax
output. If changing a single pixel would increase the target
class by 10, but also increase the least likely class by 15, the
attack will not increase that pixel.
Recall that distillation at temperature T causes the value of
the logits to be T times larger. In effect, this magniﬁes the sub-
optimality noted above as logits that are extremely unlikely but
have slight variation can cause the attack to refuse to make
any changes.
Fast Gradient Sign fails at ﬁrst for the same reason L-
BFGS fails: the gradients are almost always zero. However,
something interesting happens if we attempt the same division
trick and divide the logits by T before feeding them to the
softmax function: distillation still remains effective [36]. We
are unable to explain this phenomenon.
B. Applying Our Attacks
When we apply our attacks to defensively distilled net-
works, we ﬁnd distillation provides only marginal value. We
re-implement defensive distillation on MNIST and CIFAR-10
as described [39] using the same model we used for our eval-
uation above. We train our distilled model with temperature
T = 100, the value found to be most effective [39].
Table VI shows our attacks when applied to distillation. All
of the previous attacks fail to ﬁnd adversarial examples. In
contrast, our attack succeeds with 100% success probability
for each of the three distance metrics.
When compared to Table IV, distillation has added almost
no value: our L0 and L2 attacks perform slightly worse, and
our L∞ attack performs approximately equally. All of our
attacks succeed with 100% success.
C. Effect of Temperature
In the original work, increasing the temperature was found
to consistently reduce attack success rate. On MNIST, this
goes from a 91% success rate at T = 1 to a 24% success rate
for T = 5 and ﬁnally 0.5% success at T = 100.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Best Case
Average Case
Worst Case
MNIST
mean
10
prob
100%
CIFAR
mean
7.4
prob
100%
MNIST
mean
19
prob
100%
CIFAR
mean
15
prob
100%
MNIST
mean
36
prob
100%
CIFAR
mean
29
prob
100%
1.7
100%
0.36
100%
2.2
100%
0.60
100%
2.9
100%
0.92
100%
0.14
100%
0.002 100%
0.18
100%
0.023 100%
0.25
100%
0.038 100%
Our L0
Our L2
Our L∞
COMPARISON OF OUR ATTACKS WHEN APPLIED TO DEFENSIVELY DISTILLED NETWORKS. COMPARE TO TABLE IV FOR UNDISTILLED NETWORKS.
TABLE VI
e
n
i
l
e
c
n
a
t
s
D
i
l
a
i
r
a
s
r
e
v
d
A
n
a
e
M
.
0
3
5
2
.
0
.
2
5
.
1
0
.
1
5
.
0
0
.
0
●
● ●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
e
s
a
B
,
s
r
e
f
s
n
a
r
T
e
p
m
a
x
E
l
l
a
i
r
a
s
r
e
v
d
A
y
t
i
l
i
b
a
b
o
r
P
0
20
40
60
80
100
0
1
.
.
8
0
6
.
0
4
.
0
2
.
0
0
.
0
Untargetted
Targetted
0
10
20
30
40
Distillation Temperature
Value of k
Fig. 8. Mean distance to targeted (with random target) adversarial examples
for different distillation temperatures on MNIST. Temperature is uncorrelated
with mean adversarial example distance.
Fig. 9. Probability that adversarial examples transfer from one model to
another, for both targeted (the adversarial class remains the same) and
untargeted (the image is not the correct class).
We re-implement this experiment with our improved attacks
to understand how the choice of temperature impacts robust-
ness. We train models with the temperature varied from t = 1
to t = 100.
When we re-run our implementation of JSMA, we observe
the same effect: attack success rapidly decreases. However,
with our improved L2 attack, we see no effect of temperature
on the mean distance to adversarial examples: the correlation
coefﬁcient is ρ = −0.05. This clearly demonstrates the fact
that increasing the distillation temperature does not increase
the robustness of the neural network, it only causes existing
attacks to fail more often.
D. Transferability
Recent work has shown that an adversarial example for one
model will often transfer to be an adversarial on a different
model, even if they are trained on different sets of training data
[46], [11], and even if they use entirely different algorithms
(i.e., adversarial examples on neural networks transfer to
random forests [37]).
Therefore, any defense that
is able to provide robust-
ness against adversarial examples must somehow break this
transferability property; otherwise, we could run our attack
algorithm on an easy-to-attack model, and then transfer those
adversarial examples to the hard-to-attack model.
Even though defensive distillation is not robust
to our
stronger attacks, we demonstrate a second break of distillation
by transferring attacks from a standard model to a defensively
distilled model.
We accomplish this by ﬁnding high-conﬁdence adversar-
ial examples, which we deﬁne as adversarial examples that
are strongly misclassiﬁed by the original model. Instead of
looking for an adversarial example that just barely changes
the classiﬁcation from the source to the target, we want one
where the target is much more likely than any other label.
Recall the loss function deﬁned earlier for L2 attacks:
)t,−κ).
) = max(max{Z(x
)i : i (cid:5)= t} − Z(x
f (x
(cid:2)
(cid:2)
(cid:2)
The purpose of the parameter κ is to control the strength of
adversarial examples: the larger κ, the stronger the classiﬁ-
52
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
d
e
l