(and did not have the same branch outcome every time).
4) Finally, ABSynthe also instruments the top of the parent
loops of instrumented branches to let the spy differentiate
between a varying number of non-taken secret branches.
An overview of the analyzed branches for different target
programs can be seen in Table I. We see the total number
of branches in the binary, and the selection process that leads
to the ﬁnal number of secret-dependent, instrumented branches
to collect the ground truth. We veriﬁed that the found branches
are indeed exactly the desired secret-dependent branches.
This approach generalizes to secret values that are larger
than a single bit. In the general case, the ground-truth instru-
mentation records a unique pattern for each secret value. We
give the spy code all the information needed to reconstruct the
control ﬂow, and with that, reconstruct the secret values.
As an example of the instrumentation for one of the EdDSA
25519 target, consider again Figure 5. The instrumentation
in the top of the loop, CRYPTLOOP_START(), signals the
start of the processing of a key bit by writing a marker to
shared memory. Whenever a secret-dependent branch is taken,
we signal that with a CRYPTLOOP_VALUE(1) by simply
writing the corresponding value to shared memory. The spy
code will read these values and collect them together with the
side channel signal for training and evaluating.
Collecting the Ground Truth Given the instrumentation,
ABSynthe extracts additional ground truth, namely how dif-
ferently the contention-based signal
looks when the target
software is processing a particular secret bit value.
In the previous section, we explained how we instrument
is taking or
the target software to report when the target
not
in
turn, stores the contention-based measurements during the
processing of these secret bits.
taking secret-dependent branches. Our spy process,
Synchronizing when the target is processing a particular
secret value with the spy code is challenging since the synchro-
nization itself can introduce noise into the measurements. To
address this challenge, instead of using hard synchronization,
ABSynthe’s Ground Truth Engine relies on a soft synchroniza-
tion strategy between the software target and the spy code. In
particular, our design uses an efﬁcient shared memory channel
and signaling protocol between the target software and the spy
code. To implement the shared memory channel, we simply
allocate a single shared memory page. We design our signaling
protocol to be asynchronous and minimalistic.
The spy code constantly monitors the target shared memory
location and tags each latency measurement with the sampled
value. This simple mechanism provides us with a reliable way
to derive the ground truth of what a side channel signal looks
like for the processing of the different secret key bits.
Virtualization support Our prototype implementation ex-
pands support to virtual environments by running both the
target binary and the spy code in VMs. Instead of the native
shared memory, we use a guest-accessible shared memory
implementation, IVSHMEM [23], on KVM/Linux to facilitate
the communication between the instrumented target software
and the spy code. Our end-to-end strategy addresses C1.
B. C2: Side-channel Reﬁnement
We seek to synthesize, within parameters, the best perform-
ing side-channel attack by creating contention on potentially
7
Fig. 9: Secret-dependent branches divide the execution time between themselves. This ﬂame graph of execution of ed25519
shows this property which we found to be common across other cryptographic targets.
Fig. 10: Recipe for a combination of side channel primi-
tives. The four best performing single instructions are INSTR1
through INSTR4. The recipe states that 4 INSTR1 instructions
are used, 3 INSTR2 instructions, and so on. That they are
combined in an interleaving fashion. Further a memory fence
instruction should be emitted before the code starts, and the
whole snippet should be repeated 11 times.
multiple resources at the same time. To do this, we need to ﬁnd
the right sequence of instructions for a software target that we
instrumented in the previous step. To ﬁnd such a sequence, we
choose a guided search algorithm that uses classiﬁcation reli-
ability (i.e., detecting correct information) as its optimization
metric. The input to this algorithm are instructions that can
create contention on various microarchitectural components.
We use the best performing instructions as seeding set.
We wish to give the search algorithm freedom in choosing
the ﬁnal instruction sequence. The only requirement is that the
instructions remain valid. Meeting both of these criteria, we
design recipes that the system uses for synthesizing instruction
sequences. Our evolutionary search algorithm mutates these
recipes when looking for the best-performing solution. Before
providing more details on the search algorithm, we ﬁrst discuss
the format of the recipes shown in Figure 10. Each recipe
consists of a short string of integer parameters, each with a
speciﬁc range that the search algorithm must respect.
Repeat number This parameter deﬁnes the number of iter-
ations (between 1 and 20) the complete code needs to run.
The execution time of the entire loop provides the raw signal
for detecting the secret operation. We wish to strike a balance
between observing a meaningful stretch of time and having
a high-resolution measurement. The optimal value for this
parameter depends on the synthesized code and on the target
program.
they prevent the memory trafﬁc from creating noise on the
instructions that measure the execution time due to out-of-
order execution. Barriers, however, may come at a cost of
lowering the temporal resolution, due to the incurred latency of
draining the memory pipeline. Hence, they can have adverse
effects on the instructions that do not exercise the memory
subsystem, and we don’t want to force them, and leave their
inclusion as a parameter.
Instruction blocks These parameters are each aimed at cre-
ating contention. Currently, for each parameter, we use an
architecture-dependent sequence of instructions that yields a
covert channel, as discussed in Section IV. Each parameter
deﬁnes the number of instructions that
is desired from a
speciﬁc covert channel.
Combine mode This parameter speciﬁes the method for
combining the instruction blocks. We identify three possibil-
ities: concatenation, interleaving, and a random shufﬂe after
concatenation. With concatenation, we simply concatenate the
instruction blocks in the recipe. With interleaving, we inter-
leave instructions from different blocks. Finally, with random
shufﬂing, we ﬁrst concatenate different instruction blocks and
then do a random shufﬂe of these instructions.
Evolutionary search algorithms require a ﬁtness function
that evaluates the ﬁtness of the current population of solu-
tions. We use a Gaussian Naive Bayes (GNB) classiﬁer to
decide whether a given sequence of instructions gives a signal
measurement that is able to differentiate between the various
code paths the target is executing.
To do this, we train the classiﬁer on the signal values,
a vector of 220 latency measurements. We label them with
the ground truth obtained from the target instrumentation. The
ground truth is the code path being executed by the target when
the signal was measured. As an example we label these code
paths 0 and 1.
To increase accuracy, we ﬁrst apply a normalization step
on the raw measurements from a given instruction sequence.
We subtract the mean latency from all measurements, and then
divide each measure in order to give the signal unit variance.
Barriers These parameters deﬁne whether or not there is a
memory barrier 1) before executing the instruction sequence
or 2) after the instruction sequence is executed. Memory-
dependent side channels beneﬁt from these barriers, since
Finally, we then train the GNB using 75 target executions
(providing empirically accurate results). Typically each exe-
cution gives us hundreds of code path samples, giving us a
training set of e.g. 19200 ’0’ values and 9600 ’1’ values. We
8
REPEATNUMBERBARRIERBEFORESTARTBARRIERBEFOREENDINSTR1INSTR2INSTR3INSTR4COMBINEMODE11TrueFalse4365INTERLEAVEthen and observe how well the trained GNB classiﬁer performs
on a separate training set of 25 target executions. From this
test, we compute the f1 score. We use this f1 score as the
ﬁtness function for the genetic algorithm. As an example of
the raw signals, and of the effect of the signal preprocessing
step, see Figure 11.
We chose the GNB classiﬁer since it performs well without
tuning parameters, and its training and evaluation have linear-
time complexity which allows for our evolutionary algorithm
to quickly progress.
We select Differential Evolution [24] (DE) for our evolu-
tionary search algorithm. This choice is motivated by DE’s
ability of treating the optimized function as a black box, for
not relying on a well-deﬁned gradient to exist, and also for
its resistance to a noisy ﬁtness function, which is the case
for our GNB classiﬁer. The algorithm works by assembling
a population of candidate solutions, described by a set of
parameters for each candidate, and continuously trying to
improve the candidates’ ﬁtness functions by combining their
parameters into a new generation of the population. We deﬁne
the discussed recipes as candidates which the DE strives to
optimize using the GNB classiﬁer.
We will show the effectiveness of our search strategy and
a sample of reﬁned sequence of instructions in Section VII.
C. C3: Secret Recovery
The ground truth captured in the training phase includes
synchronization information. This means we can train and
test on signals with known equal starting points and length.
Under such assumptions, the classiﬁer can easily tells us the
corresponding secret value. However, a realistic attack requires
processing a captured signal with many consecutive such sub-
signals starting at unknown position. In order to obtain secret
recovery in practical settings, we need to recognize the sub-
signals of many samples that correspond to a particular secret
value and extract just the single value. A 2-label classiﬁer,
while fast
is not equipped for this
purpose, nor can it easily be adapted to reliably do so. The
primary reason is the fact that we express a time series of
samples, as a vector, to which the classiﬁer assigns the same
meaning in each position. However, the samples may easily
desynchronize over time when analyzing a long capture.
to train and evaluate,
Unsynchronized key recovery To detect signals correspond-
ing to secret values in absence of synchronization, we turn to
a sequence classiﬁcation algorithm that is intended for time
series data and is robust in the face of imperfect synchro-
nization: a Long Short-Term Memory [25] (LSTM) Recurrent
Neural Network (RNN). Not only does this network improve
the synchronized classiﬁcation signiﬁcantly, it is also robust in
face of small time shifts in the signal, allowing unsynchronized
secret recovery.
LSTM models For robustness reasons that we will discuss
later in this section, we design two different LSTM models,
each with different detection characteristics. Model 1 consists
of 3 stacked LSTM layers with decreasing number of cells
(400, 300, 200). In order to avoid over-ﬁtting, we have set
a dropout of 0.2 on each layer. The ﬁrst 2 LSTM layers
propagate their hidden state to the subsequent layer. As we
have a 3-label classiﬁcation, we use a softmax layer to cal-
culate the output probabilities. Model 2 is more complex in
comparison to model 1, totaling six layers. We engineered
the model so that after an LSTM layer, a fully connected
layer with RELU activation follows, which we ﬁnd to work
well in practice. We use three such layer packets, again with
decreasing numbers of cells (400, 256, 300, 128, 200). The
LSTM layers also propagate their hidden state to the RELU
layers. Therefore the RELU layers have input dimension 3.
Similar to model 1, we use dropout to encourage resistance
to over-ﬁtting and a softmax layer for classiﬁcation. The two
models are the result of engineering various models of different
shapes and parameters and selecting the ones that provided the
best test-score. In most cases, model 1 performs slightly better
than model 2. However, we use the two deep neural networks
in a stacked ensemble, which results in a better performing
classiﬁer. We implement our models using the Keras [26]
interface to Tensorﬂow [27].
We combine the results from two different models in a
stacked ensemble in order to be more resistant to miss-guessing
secret values, as not guessing any secret value is much less
damaging than guessing the wrong value. We fully detail
this design decision when discussing the brute force tradeoff,
below. We train the weights on the models as follows.
1) We assemble a labeled training set. The features of the
training samples are latency values, which have been
processed for normalization, in the same way as in the
Gaussian Naive-Bayes classiﬁer described above.
2) We include 2 types of samples in the training set. The ﬁrst
type is a signal of latencies, where the start time coincides
with the moment a code path is being processed by the
target according to the ground truth data. The second type
is a signal that starts somewhere between the start of one
path and the start of the next. We label this with a special,
extra label that is not used by the instrumentation.
3) As with the Gaussian classiﬁer, we train the models on 75
executions of the target, typically giving 384 code paths
each execution of the ﬁrst, synchronized type. Added to
these is a equal number of special training samples that
are not synchronized.
Brute force tradeoff A recovered secret is frequently not
exactly correct. A reasonable amount of brute-force search
performed by the analyst is acceptable in order to get to the
correct key from a recovered key bit stream. We assume we
can verify a candidate secret, which we can do if the secret is,
say, a cryptographic key that is used in a signing operation.
This leads to a crucial observation.
The secret recovery algorithm will return a sequence of
(time, secret) pairs. Whenever a secret is miss-detected as a
wrong value, brute forcing means we have to try hundreds of
positions to ﬁnd the wrong bit. For a 384-position guess and
N wrong guesses, brute forcing leads to a 384N work factor
for every bit. The approach quickly becomes infeasible.
However, we can reliably detect whenever a secret value is
missing (deletion), or whenever a spurious value was inserted
(insertion), because of the time series information. Deleted bits
leave a large gap, and inserted bits create very narrow gaps.
9
For deletions, it is clear where to insert a trial bit, or not, giving
3 possibilities: ignore, insert 0, insert 1. For insertions, assume
one of the three bits forming the two narrow gaps is to blame,
and try to delete each of them, also leading to 3 possibilities.
We can tolerate quite a few insertions and deletions before
brute force becomes infeasible—for a total of N insertions
plus deletions, we have to do 3N trials.
This means we should design our detection algorithm to be
conservative in secret value predictions. We can compensate
missing values with brute force to a large degree. This is the
key reason we train 2 different LSTM models and only accept
a secret value prediction when they both predict the same one.
This approach results in a more reliable classiﬁcation
system, so robust in classifying time series data that, given
a sufﬁciently high-quality underlying side channel, a practi-
cal secret recovery system can be built on it. This strategy
addresses C3.
VII. EVALUATION
In this section, we quantify the effectiveness of ABSynthe
in synthesizing practical side-channel attacks. We ﬁrst discuss
our evaluation environment and our example software targets.
We aim to answer six questions in our evaluation of ABSynthe.
1) Can aligned snippets of execution traces corresponding
to single secret bits be reliably classiﬁed into the correct
secret bit by ABSynthe’s spy process? We show that this
the case by presenting our results in Section VII-C.
2) Can a complete capture of a secret key operation, without
any alignment, reliably be mapped back to the original
secret bits? This is a more challenging task than the pre-
vious one. We show how ABSynthe can make predictions
without any alignment information in Section VII-D. We
show how these predictions can easily be turned into
a fully automated secret recovery using a case-speciﬁc
heuristic in one of our software targets.
3) How does black-box generation of
instruction se-
quences in ABSynthe compare against the state-of-the-
art contention-based attacks such as PortSmash [10] and
SMoTherSpectre [11]? We show that ABSynthe’s DE
algorithm manages to automatically ﬁnd better sequence
of instructions for leaking information in Section VII-E.
4) Is ABSynthe robust to signal capturing larger than the
execution region of interest? Can we detect where the re-
gion of interest is, namely where secret key bits (which we
have trained our classiﬁers to detect) are being processed?
We evaluate this using a crypto-as-a-service scenario in a
larger application (i.e., GnuPG) in Section VII-F1.
5) How robust are the synthesized attacks against noise?
We detail the impact of interference from concurrently
executing other processes on key recovery performance
in Section VII-F2.
6) Can we generalize our system to not only detect secret-
dependent code accesses, but also secret-dependent data
accesses? We show the results of extending ABSynthe to
synthesize cache attacks in Section VII-G.
A. Experimental Environments
We use four different testbeds. Two are based on Intel
processors, one with a desktop processor, a 4-core 2-way SMT
10
Intel Skylake i7-6700K running Ubuntu 18.04 and another with
a server processor, an 8-core 2-way SMT Intel Broadwell E5-
2620 running Debian Buster. We also use a system with a
24-core 2-way SMT AMD EPYC Zen 7401P running Ubuntu
18.04.1. Finally, we report partial results on ARM for a two
sockets machine, each with a 56-core 4-way ARM Cavium
Thunder X2 running Ubuntu 16.04.6 LTS. Note that the results
presented in this paper are the ﬁrst exploration of SMT-based
side channels on ARM platforms, but since we do not have a
machine-readable ISA for ARM641 for creating its leakage
map, we only apply ABSynthe on hand-written instruction
sequences and report its effectiveness.
To study the effectiveness of ABSynthe in virtualized
environments, we create VMs on the Broadwell and EPYC
testbeds. We run both the spy process and the victim process
in different VM instances locked to different threads on the
same physical core in order to assess cross-VM information
leakage with ABSynthe.
Unless otherwise mentioned, we report median ﬁgures. In
all our experiments, ASLR is turned on, which means that
contention-based attacks synthesized by ABSynthe are not