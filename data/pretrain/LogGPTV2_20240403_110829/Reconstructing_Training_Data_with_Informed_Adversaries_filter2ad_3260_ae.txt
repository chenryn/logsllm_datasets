Another way to interpret Theorem 2 is through the lens
of zero-concentrated DP (zCDP) [40]. A mechanism is ρ-
zCDP if it satisfies (α, αρ)-RDP for every α > 1. This
definition provides a natural and convenient way to express the
privacy afforded by the ubiquitous Gaussian mechanism [38].
Applying Theorem 2 to a ρ-zCDP mechanism and optimizing
α to minimize the upper bound yields the following.
Corollary 4. Fix π, ℓ and η > 0, and let κ = κπ,ℓ(η). If
a mechanism M satisfies ρ-zCDP with ρ  0. Suppose M is a
mechanism satisfying ϵ-DP with ϵ = o(d) or ρ-zCDP with
ρ = o(d). Then M is (η, γ)-ReRo with respect to N (w, σ2Id)
and ℓ2 with γ = e−Ω(d) as long as σ ≥ 2η√
.
d
it asks that
The idea that
large values of ϵ can protect against re-
construction when the adversary’s prior contains significant
uncertainty (i.e. it is diffused) was previously noticed in [41]
in the context of local DP (LDP) with priors close to uniform.
Inspired by FL applications where adversaries get access to
LDP gradients, the authors propose a notion of protection
against reconstruction breaches that is more stringent than
Definition 2:
the adversary cannot effectively
reconstruct a particular feature of interest about the target point
no matter what the output of the mechanism is – in contrast,
ReRo uses an average-case requirement over the outputs of
the mechanism. Technically, [41, Lemma 2.2] shows that the
bound in Corollary 3 also holds for this worst-case notion of
protection against reconstruction.4 Such worst-case guarantees,
however, are not attainable under relaxations of ϵ-DP like RDP
because the latter does not enforce an almost sure bound on
the privacy loss: instead, it just guarantees that the privacy
loss will be small with high probability. Thus, Theorem 2 and
Proposition 6 are natural generalizations of the results from
[41] to RDP, which is the default notion of privacy provided
by DP-SGD and other popular private ML algorithms [42, 43].
E. Is Reconstruction Robustness Useful in Practice?
To deploy the bounds from Theorem 2 two things are
necessary: the description of a criterion for reconstruction error
ℓ with an associated threshold η, and an understanding of the
success rate of η-approximate reconstruction by the adversary
prior to the release. Equipped with ℓ and η, one can then
engage in a conversation with stakeholders and domain experts
to determine what success rate of reconstruction is reasonable
to adjudicate to a potential adversary before the release is
made. An interesting feature of Theorem 2 is that it reduces
adversarial modelling to a question about determining a single
number κπ,ℓ(η). Furthermore, it is possible that one does not
need to be overly conservative in estimating this number. After
all, the theorem bounds the success probability of the wost-
case adversary which, in particular, knows all the fixed dataset.
Realistic adversaries will often have less knowledge of the
fixed dataset, so it might be possible to trade-off knowledge
of the fixed dataset with the amount of diffusion required from
the prior. We leave this question for future work.
F. Further Related Work
a) Threat modelling and privacy semantics: The use
of informed adversaries in formal privacy analyses can be
tracked back to the sub-linear queries (SuLQ) framework [44].
SuLQ was later subsumed by DP [6], where mentions to a
4Although the bound in [41] is stated in terms of ϵ-LDP, it is easy to see
that the same holds for central ϵ-DP in the presence of an informed adversary.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1149
concrete adversary were expressly avoided in the definition
that is widely used nowadays [45]. Nonetheless, [6, Appendix
A] provides a “semantically flavored” definition equivalent to
DP which involves the likelihood ratio between the prior and
posterior beliefs of an informed adversary about any property
of the target data point. The adversarial model put forward in
Section II uses the same notion of informed adversary.
In other frameworks where the adversary is not (necessarily)
informed (e.g. Pufferfish privacy [46] and inferential privacy
[47]), side knowledge about the whole dataset is encoded in a
probabilistic prior capturing information about the individual
entries in the dataset as well as their statistical dependencies.
These frameworks extend the semantic approach to DP by
replacing the prior-vs-posterior condition with an odds ratio
condition – such modification is motivated by the observa-
tion that prior-vs-posterior bounds cannot hold in general
for uninformed adversaries unless the prior distribution over
the dataset assumes the records are mutually independent.
Alternatively, [48] provides posterior-vs-posterior semantics
for DP in the presence of an uninformed adversary with
an arbitrary prior. In the definition of reconstruction robust-
ness, our use of an informed adversary with a prior over
the target data point circumvents the complications arising
from dependencies between points in the training data: the
prior captures the adversary’s residual uncertainty about the
target point after observing the fixed dataset. On the opposite
direction, several authors have proposed approaches where the
adversary’s uncertainty with respect to the input data of a
mechanism is leveraged to increase the privacy provided to
individuals [49, 50, 51, 52]. Implicitly, these works assume a
less powerful adversary than the one considered in this paper.
Most of the semantic definitions we discussed formalize
the privacy protection goal without assuming the adversary is
interested in a particular inference task; that is, protection ap-
plies simultaneously to all possible inferences about the target
point(s). In contrast, the use of an explicit reconstruction error
ℓ makes the definition of reconstruction robustness syntactic in
nature. Section II-B briefly discusses how the problem of de-
signing an appropriate error function for each application can
be approached. A similar dilemma arises in location privacy,
where distortion-based notions include an explicit measure
of reconstruction error [53, 54]. Nonetheless, as Theorem 5
shows, by considering a very stringent reconstruction goal
and a set of sufficiently informative priors one can recover
semantic privacy notions from reconstruction robustness.
The connection between DP and protection against mem-
bership inference is perhaps best understood via its hypothesis
testing interpretation [55, 56]. A comprehensive discussion of
the adversary implicit in the definition of DP from the hy-
pothesis testing standpoint can be found in [14]. Interestingly,
[57] shows that, unlike standard DP, RDP does not admit
a hypothesis testing interpretation. A semantic (Bayesian)
interpretation of RDP in terms of moment bounds on the odds
ratio is presented in [39]. Theorem 2 provides an alternative
characterization of the privacy protection afforded by RDP in
terms of resilience to reconstruction attacks.
b) DP and protection against reconstruction: How stan-
dard DP offers concrete protection against reconstruction
attacks has been studied in other contexts. Indeed, one of the
original motivations for the definition of DP was to defeat
database reconstruction attacks in the context of interactive
query mechanisms [58, 59, 60, 61, 62]. In such attacks, the
adversary receives (noisy) answers to a sequence of specially
crafted queries against a database and, if the noise is small
enough, uses the answers to (partially) reconstruct every record
in the database. The success of these attacks is contingent on
the adversary’s ability to control these queries; in contrast, in
ML applications like the ones we consider the computation
performed by the mechanism is completely under the model
developer’s control.
The quantitative information flow literature seeks to provide
information-theoretic bounds on data leakage in information
processing systems [63, 64]. When applied to differentially
private mechanisms, these ideas yield bounds on the protection
against exact reconstruction when Z is finite. In particular,
when specialized to informed adversaries and translated into
our terminology, [65, Theorem 3] shows that any ϵ-DP mech-
anism is (η, γ)-ReRo with η ∈ (0, 1) with respect to ℓ0/1 and
any prior π with γ ≤ |Z|κeϵ
|Z|+eϵ−1. Taking |Z| → ∞ recovers
the bound from Corollary 3 in the case of ℓ0/1. Our results
can thus be interpreted as a generalization of this line of work
where no assumptions about Z are necessary.
VIII. CONCLUSIONS
Our work provides compelling evidence that standard ML
models can memorize enough information about their training
data to enable high-fidelity reconstructions in a very stringent
threat model. By instantiating an informed adversary that
learns to map model parameters to training images, we suc-
cessfully attacked standard MNIST and CIFAR-10 classifiers
with up to 100K parameters, and showed the attack is signifi-
cantly robust to changes in the training hyper-parameters. Two
aspects of our attack we would like to improve in future work
are its data and computational efficiency, and its scalability
to larger, more performant released models. This would not
lead to real-world adversaries mounting practical attacks due
to the nature of our threat model, but it would enable model
developers to assess potential privacy leakage in models before
deployment. Extending our attacks to reconstruct N > 1
targets simultaneously would also be interesting, but we expect
this to be substantially harder. For example, in this setting
our attacks against convex models lead to a problem with
more unknowns than equations. On the defenses side, we
empirically showed that DP training with large values of ϵ can
effectively mitigate our reconstruction attacks. Our theoretical
discussion, stemming from a new definition of reconstruction
robustness and a study of its connection to (R)DP, shows this
is a general phenomenon: informed reconstruction attacks can
be prevented with large values of ϵ under some assumptions
on the adversary. Validating such assumptions in particular
applications would open the door to practical models which
are accurate and resilient against reconstruction attacks.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1150
ACKNOWLEDGMENT
The authors would like to thank: Leonard Berrada, Adri`a
Gasc´on and Shakir Mohamed for feedback on an earlier
version of this manuscript; Brendan McMahan for suggesting
the idea that random initialization in SGD might make privacy
attacks harder which inspired some of our experiments; and
Olivia Wiles for discussions on how to improve reconstructor
network training on CIFAR-10. This work was done while
G.C. was at the Alan Turing Institute.
REFERENCES
[1] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals,
“Understanding deep learning requires rethinking gen-
eralization,” in International Conference on Learning
Representations (ICLR), 2017.
[2] V. Feldman, “Does learning require memorization? a
short tale about a long tail,” in ACM Symposium on
Theory of Computing (STOC), 2020.
[3] V. Feldman and C. Zhang, “What neural networks mem-
orize and why: Discovering the long tail via influence
estimation,” in Conference on Neural Information Pro-
cessing Systems (NeurIPS), 2020.
[4] G. Brown, M. Bun, V. Feldman, A. D. Smith, and
K. Talwar, “When is memorization of irrelevant training
data necessary for high-accuracy learning?” in ACM
Symposium on Theory of Computing (STOC), 2021.
[5] R. Shokri, M. Stronati, C. Song, and V. Shmatikov,
“Membership inference attacks against machine learning
models,” in IEEE Symposium on Security and Privacy
(SP), 2017.
[6] C. Dwork, F. McSherry, K. Nissim, and A. D. Smith,
“Calibrating noise to sensitivity in private data analysis,”
in Theory of Cryptography Conference (TCC), 2006.
[7] N. Carlini, C. Liu, ´U. Erlingsson, J. Kos, and D. Song,
“The secret sharer: Evaluating and testing unintended
memorization in neural networks,” in USENIX Security
Symposium, 2019.
[8] N. Carlini, F. Tram`er, E. Wallace, M.
Jagielski,
A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown,
D. Song, ´U. Erlingsson, A. Oprea, and C. Raffel, “Ex-
tracting training data from large language models,” in
USENIX Security Symposium, 2021.
[9] B. McMahan, E. Moore, D. Ramage, S. Hampson, and
B. Ag¨uera y Arcas, “Communication-efficient learning
of deep networks from decentralized data,” in Interna-
tional Conference on Artificial Intelligence and Statistics
(AISTATS), 2017.
[10] L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradi-
ents,” in Conference on Neural Information Processing
Systems (NeurIPS), 2019.
[11] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and
T. Ristenpart, “Privacy in pharmacogenetics: An end-
to-end case study of personalized warfarin dosing,” in
USENIX Security Symposium, 2014.
[12] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and
N. Borisov, “Property inference attacks on fully con-
[13] A. Suri and D. Evans, “Formalizing and estimating
distribution inference risks,” arXiv:2109.06024, 2021.
nected neural networks using permutation invariant rep-
resentations,” in ACM Conference on Computer and
Communications Security (CCS), 2018.
[14] M. Nasr, S. Song, A. Thakurta, N. Papernot, and N. Car-
lini, “Adversary instantiation: Lower bounds for differ-
entially private machine learning,” in IEEE Symposium
on Security and Privacy (SP), 2021.
[15] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan,
I. Mironov, K. Talwar, and L. Zhang, “Deep learning with
differential privacy,” in ACM Conference on Computer
and Communications Security (CCS), 2016.
[16] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inver-
sion attacks that exploit confidence information and basic
countermeasures,” in ACM Conference on Computer and
Communications Security (CCS), 2015.
[17] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Pri-
vacy risk in machine learning: Analyzing the connection
to overfitting,” in IEEE Computer Security Foundations
Symposium (CSF), 2018.
[18] Y. Zhang, R. Jia, H. Pei, W. Wang, B. Li, and D. Song,
“The secret revealer: Generative model-inversion attacks