ones. Among the positive suspect models, the areas of RT-
AL and P-60% are noticeably smaller than the other two,
meaning they are harder to detect. This is because these
two attacks make the most parameter modiﬁcations to the
victim model. Comparing the metrics, activation-based metrics
(e.g., NAD) demonstrate better performance than output-based
metrics (e.g., NOD), while white-box metrics are stronger than
black-box metrics, especially against strong attacks like RT-
AL. In Appendix D, we also analyze the inﬂuencing factors
including adversarial test case generation and layer selection
(for computing the testing metrics) via several calibration
experiments. An analysis of how different levels of ﬁnetuning
or pruning affect DEEPJUDGE is presented in Appendix H.
Time Cost of DEEPJUDGE. The time cost of generating test
cases using 1k seeds is provided in appendix Table IX. For the
black-box setting, we report the cost of PGD-based generation,
while for the white-box setting, we report that of Algorithm 2.
It shows that the time cost of white-box generation is slightly
higher but is still very efﬁcient in practice. The maximum time
cost occurs on the SpeechCommands dataset for white-box
generation, which is ∼ 1.2 hours. This time cost is regarded
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
831
TABLE IV: Performance of DEEPJUDGE against model ﬁnetuning and pruning attacks in the black-box setting. PGD [28]
is used to generate the adversarial test cases. ACC is the validation accuracy. For each metric, the values below (indicating
‘copy’) or above (indicating ‘not copy’) the threshold τλ (the last row) are highlighted in red (copy alert) and green (no
alert), respectively. ‘Yes (2/2)’: two of the metrics vote for ‘copy’ (pcopy = 100%); ‘No (0/2)’: none of the metrics vote for
‘copy’ (pcopy = 0%).
Model Type
Victim Model
ACC
98.5%
MNIST
RobD
FT-LL 98.8±0.0% 0.019±0.003
FT-AL 98.7±0.1% 0.045±0.016
RT-AL 98.4±0.2% 0.298±0.039
P-20% 98.7±0.1% 0.058±0.014
P-60% 98.6±0.1% 0.172±0.024
98.4±0.3% 0.968±0.014
Neg-1
98.3±0.2% 0.949±0.029
Neg-2
CIFAR-10
RobD
JSD
–
Copy?
–
ACC
84.8%
0.016±0.002 Yes (2/2)
0.033±0.010 Yes (2/2)
0.151±0.017 Yes (2/2)
0.035±0.009 Yes (2/2)
0.097±0.010 Yes (2/2)
0.614±0.016
No (0/2)
0.600±0.020
No (0/2)
82.1±0.1% 0.000±0.000
79.9±1.4% 0.192±0.028
79.4±0.8% 0.237±0.055
81.7±0.2% 0.155±0.032
81.1±0.6% 0.318±0.036
84.2±0.6% 0.920±0.021
84.9±0.5% 0.926±0.030
JSD
–
Copy?
–
0.002±0.001 Yes (2/2)
0.162±0.014 Yes (2/2)
0.197±0.027 Yes (2/2)
0.128±0.018 Yes (2/2)
0.233±0.019 Yes (2/2)
0.603±0.016
No (0/2)
0.615±0.021
No (0/2)
τλ
–
0.852
0.538
–
–
0.816
0.537
–
–
–
–
–
Positive
Suspect
Models
Negative
Suspect
Models
Positive
Suspect
Models
Negative
Suspect
Models
Model Type
Victim model
ACC
74.4%
ImageNet
RobD
FT-LL 73.2±0.4% 0.034±0.007
FT-AL 70.8±0.9% 0.073±0.011
RT-AL 53.3±0.8% 0.192±0.008
P-20% 69.7±1.1% 0.106±0.010
P-60% 68.8±1.0% 0.161±0.017
74.2±0.3% 0.737±0.007
Neg-1
73.9±0.5% 0.760±0.010
Neg-2
SpeechCommands
RobD
JSD
–
Copy?
–
ACC
94.9%
0.009±0.003 Yes (2/2)
0.043±0.011 Yes (2/2)
0.251±0.015 Yes (2/2)
0.064±0.003 Yes (2/2)
0.091±0.004 Yes (2/2)
0.395±0.006
No (0/2)
0.429±0.004
No (0/2)
95.2±0.1% 0.104±0.007
95.8±0.3% 0.326±0.024
94.3±0.3% 0.445±0.019
95.4±0.2% 0.310±0.026
95.0±0.5% 0.437±0.030
94.9±0.7% 0.819±0.025
94.5±0.8% 0.832±0.024
JSD
–
Copy?
–
0.036±0.006 Yes (2/2)
0.155±0.014 Yes (2/2)
0.231±0.016 Yes (2/2)
0.152±0.013 Yes (2/2)
0.215±0.013 Yes (2/2)
0.456±0.014
No (0/2)
0.472±0.012
No (0/2)
τλ
–
0.659
0.356
–
–
0.727
0.405
–
Fig. 4: The detection ROC curves of metrics RobD and JSD
on CIFAR-10 suspect models, and AU C = 1 for both metrics.
as efﬁcient since test case generation is a one-time effort, and
the additional time cost of scanning a suspect model with the
test cases is almost negligible.
Remark 1: DEEPJUDGE is effective and efﬁcient in
identifying ﬁnetuning and pruning copies.
3) Comparison with existing techniques: We compare
DEEPJUDGE with three state-of-the-art copyright defense
methods against model ﬁnetuning and pruning attacks. More
details of these defense methods can be found in Appendix E.
Black-box: Comparison to Watermarking and Fingerprint-
ing. DNNWatermarking [47] is a black-box watermarking
method based on backdoors, and IPGuard [2] is a black-box
ﬁngerprinting method based on targeted adversarial attacks.
Here, we compare these two baselines with DEEPJUDGE in
the black-box setting. For DNNWatermarking, we train the
watermarked model (i.e., victim model) using additionally
patched samples from scratch to embed the watermarks, and
the TSA (Trigger Set Accuracy) of the suspect model
is
calculated for ownership veriﬁcation. IPGuard ﬁrst generates
targeted adversarial examples for the watermarked model then
calculates the MR (Matching Rate) (between the victim and the
suspect) for veriﬁcation. For DEEPJUDGE, we only apply the
RobD (robustness distance) metric here for a fair comparison.
The left subﬁgure of Fig. 6 visualizes the results. DEEP-
JUDGE demonstrates the best overall performance in this
black-box setting. DNNWatermarking and IPGuard fail
to
identify the positive suspect models duplicated by FT-AL, RT-
AL, P-20% and P-60%. Their scores (TSA and MR) drop
drastically against these four attacks. This basically means
that the embedded watermarks are completely removed, or
the ﬁngerprint can no longer be veriﬁed. While for the RobD
metric of DEEPJUDGE, the gap remains huge between the
negative and positive suspects, demonstrating much better
effectiveness to diverse ﬁnetuning and pruning attacks.
White-box: Comparison to Watermarking. Embedding-
Watermark [40] is a white-box watermarking method based
on signatures. It requires access to model parameters for
signature extraction. We train the victim model with the
embedding regularizer [40] from scratch to embed a 128-bits
signature. The BER (Bit Error Rate) is calculated and used
to measure the veriﬁcation performance. The right subﬁgure
of Fig. 6 visualizes the comparison results to two white-
box DEEPJUDGE metrics NOD and NAD. The three metrics
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
832
TABLE V: Performance of DEEPJUDGE against model ﬁnetuning and pruning attacks in the white-box setting. Algorithm 2
is used to generate the test cases. For each metric, the values below (indicating ‘copy’) or above (indicating ‘not copy’) the
threshold τλ (the last row) are highlighted in red (copy alert) and green (no alert) respectively. ‘Yes (4/4)’: all 4 metrics
vote for ‘copy’ (pcopy = 100%); ‘No (0/4)’: none of the metrics vote for ‘copy’ (pcopy = 0%).
Model Type
NOD
FT-LL 0.00±0.00
FT-AL 0.08±0.01
RT-AL 0.31±0.02
P-20% 0.10±0.01
P-60% 0.11±0.01
0.77±0.07
Neg-1
0.79±0.08
Neg-2
τλ
0.45
Positive
Suspect
Models
Negative
Suspect
Models
Model Type
NOD
FT-LL 0.00±0.00
FT-AL 0.02±0.01
RT-AL 0.03±0.00
P-20% 0.11±0.01
P-60% 0.77±0.01
6.55±0.78
Neg-1
6.25±0.39
Neg-2
τλ
3.48
Positive
Suspect
Models
Negative
Suspect
Models
NAD
0.00±0.00
0.23±0.21
0.37±0.20
0.16±0.12
0.82±0.26
11.46±1.14
12.28±1.50
6.74
MNIST
LOD
0.00±0.00
0.32±0.03
0.97±0.04
0.36±0.03
0.43±0.03
1.73±0.06
1.78±0.13
1.03
NAD
0.00±0.00
0.18±0.09
0.30±0.07
0.83±0.06
3.09±0.12
32.18±2.97
30.04±2.44
17.17
ImageNet
LOD
0.00±0.00
0.16±0.05
0.25±0.03
0.76±0.01
3.41±0.03
35.03±3.13
44.21±3.11
20.74
LAD
Copy?
NOD
0.00±0.00 Yes (4/4) 0.00±0.00
0.82±0.16 Yes (4/4) 0.15±0.02
1.27±0.29 Yes (4/4) 0.18±0.02
0.79±0.15 Yes (4/4) 0.28±0.03
1.16±0.08 Yes (4/4) 0.62±0.03
6.42±0.84 No (0/4)
3.09±0.30
3.21±0.18
6.37±0.47 No (0/4)
3.65
–
1.79
NAD
0.00±0.00
0.30±0.12
0.26±0.10
0.32±0.09
1.65±0.34
10.94±1.74
11.09±0.71
6.14
CIFAR-10
LOD
0.00±0.00
0.74±0.07
0.78±0.03
0.77±0.06
2.80±0.21
11.85±1.01
12.60±1.33
6.89
LAD
Copy?
0.00±0.00 Yes (4/4)
0.21±0.04 Yes (4/4)
0.22±0.02 Yes (4/4)
0.24±0.02 Yes (4/4)
0.93±0.10 Yes (4/4)
5.41±0.67 No (0/4)
5.37±0.72 No (0/4)
3.01
–
LAD
Copy?
NOD
0.00±0.00 Yes (4/4) 0.000±0.000
0.58±0.13 Yes (4/4) 0.037±0.003
0.78±0.05 Yes (4/4) 0.055±0.003
1.67±0.22 Yes (4/4) 0.038±0.002
6.63±0.23 Yes (4/4) 0.094±0.004
0.488±0.013
30.32±1.91 No (0/4)
29.58±0.86 No (0/4)
0.480±0.012
NAD
LOD
SpeechCommands
0.00±0.00
0.42±0.02
0.64±0.08
0.44±0.02
0.67±0.04
2.82±0.08