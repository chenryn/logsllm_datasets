### Table 4.9: Results on Solar Flare Target 2 with Different Sets of Hyper-parameters (10-Fold Cross-Validation, ms Stands for Milliseconds)

| Method | Accuracy | Nodes | Training Time (ms) |
|--------|----------|-------|--------------------|
| Adaptive | 0.38 | 51 | 180 |
| Clustering Based | 0.41 | 55 | 129 |
| Quantile Based | 0.39 - 0.42 | 49 - 56 | 210 - 176 |

### Categorical Features and Training Time

Experiments on other datasets indicate a significant increase in training time. Despite extensive testing, it is challenging to select the best method as it appears to be use-case dependent. However, the adaptive quantile method shows promise due to its ability to provide more relevant quantiles at each decision tree level. This method, however, is relatively time-consuming. Additionally, having too many quantiles can be counterproductive, especially when the algorithm reaches high depths where only a few data points are left in each node. Summarized statistics on the ranking of tested methods are provided in Table 4.11.

### Table 4.10: Best Results for Regression Datasets (10-Fold Cross-Validation, ms Stands for Milliseconds)

| Dataset | Nodes | Training Time (ms) | MSE |
|---------|-------|--------------------|-----|
| Facebook | 53 ± 7 | 10520 ± 215 | 24.33 ± 5.25 |
| Solar Flare Target 1 | 327 ± 44 | 97 ± 12 | 0.73 ± 0.08 |
| Solar Flare Target 2 | 172 ± 23 | 75 ± 11 | 0.32 ± 0.05 |
| Reworked Categorical Split | 35 ± 5 | 11230 ± 260 | 25.03 ± 4.34 |
| Solar Flare Target 1 | 98 ± 22 | 121 ± 14 | 0.80 ± 0.07 |
| Solar Flare Target 2 | 49 ± 4 | 115 ± 13 | 0.41 ± 0.04 |

### Table 4.11: Ranking (in Terms of Number of Nodes) of Tested Methods for Categorical Split

| Method | Facebook | Solar Flare Target 1 | Solar Flare Target 2 | Average Rank |
|--------|----------|----------------------|----------------------|--------------|
| Basic Method | 4 | 4 | 4 | 4.00 |
| Quantile Based | 1 | 1 | 3 | 1.66 |
| Adaptive | 2 | 3 | 1 | 2.00 |
| Clustering Based | 3 | 2 | 2 | 2.33 |

### Statistical Testing

We conducted statistical tests on three evaluation metrics, comparing the new splitting method to the classic one described in [7]. We assumed no differences in performance between the algorithms. The results show a significant reduction in the number of nodes in each decision tree (Table 4.14), but at the cost of increased computation time (Table 4.13). The accuracy does not differ significantly between the methods, with an effect size close to zero (Table 4.12).

### Table 4.12: Summary of Statistical Tests on Accuracy for Regression Tasks

| Dataset | p-value | Cohen d-test | Equality Rejected? |
|---------|---------|--------------|--------------------|
| Facebook | 1.8 × 10−4 | 0.86 | x |
| Solar Flare Target 1 | 0.16 | 0.93 | x |
| Solar Flare Target 2 | 0.14 | 1.98 | ✓ |

### Table 4.13: Summary of Statistical Tests on Training Time for Regression Tasks

| Dataset | p-value | Cohen d-test | Equality Rejected? |
|---------|---------|--------------|--------------------|
| Facebook | 5.23 × 10−7 | 2.97 | ✓ |
| Solar Flare Target 1 | 2.02 × 10−4 | 1.84 | ✓ |
| Solar Flare Target 2 | 6.44 × 10−8 | 3.32 | ✓ |

### Table 4.14: Summary of Statistical Tests on Number of Nodes for Regression Tasks

| Dataset | p-value | Cohen d-test | Equality Rejected? |
|---------|---------|--------------|--------------------|
| Facebook | 1.14 × 10−5 | 2.95 | ✓ |
| Solar Flare Target 1 | 5.72 × 10−11 | 6.64 | ✓ |
| Solar Flare Target 2 | 2.07 × 10−8 | 7.45 | ✓ |

### 4.2 Sampling Method and Data Augmentation

Several methods were tested to increase the available data and enhance the decision tree. Initial experiments on toy datasets provided insights into the augmentation phase and the relevance of the methods. The following figures and tables summarize the results.

#### Figure 4.5: Gaussian Augmentation on Half Circle Dataset

- **(a) Original Dataset**
- **(b) Augmented Dataset**
- **(c) Boundaries Before Data Augmentation**
- **(d) Boundaries After Data Augmentation**

#### Figure 4.6: Noise Augmentation on Circle Dataset

- **(a) Original Dataset**
- **(b) Augmented Dataset**
- **(c) Boundaries Before Data Augmentation**
- **(d) Boundaries After Data Augmentation**

The results from Gaussian augmentation (Figure 4.5) show that generated points near the boundaries of the first classifier are classified with lower confidence, indicating areas of uncertainty. These points are weighted less in the augmented dataset, which is reassuring.

Noise augmentation (Figure 4.6) also retrieves the correct boundaries, both with and without sampling, though with slight differences.

### Table 4.15: Accuracies Obtained with the Gaussian Sampling Method

| Dataset | Augmentation Factor 1 | Augmentation Factor 3 | Augmentation Factor 5 |
|---------|-----------------------|-----------------------|-----------------------|
| Avila | 97.29 | 98.27 | 98.87 |
| Hand Posture | 93.12 | 93.31 | 93.41 |
| Segmentation | 95.71 | 95.00 | 95.00 |
| Spam Base | 91.63 | 92.07 | 91.63 |
| Sensorless | 98.35 | 98.33 | 98.37 |
| Bank Credit | 73.00 | 73.08 | 73.68 |
| Wine Quality | 60.00 | 62.50 | 60.93 |
| HTRU | 97.09 | 97.43 | 97.09 |

### Table 4.16: Results for the Gaussian Method

| Dataset | Augmentation Factor 1 | Augmentation Factor 3 | Augmentation Factor 5 |
|---------|-----------------------|-----------------------|-----------------------|
| Avila | 97.65 | 97.79 | 98.03 |
| Hand Posture | 93.62 | 93.88 | 94.49 |
| Segmentation | 97.14 | 96.66 | 95.95 |
| Spam Base | 90.77 | 92.18 | 93.26 |
| Sensorless | 98.45 | 98.47 | 98.30 |
| Bank Credit | 72.65 | 73.81 | 73.75 |
| Wine Quality | 60.62 | 62.18 | 62.18 |
| HTRU | 97.54 | 97.23 | 97.26 |

### Table 4.17: Augmented Data Tree versus Other Classic Algorithms

| Dataset | Decision Tree | Random Forest | XGBoost | Augmented DT |
|---------|---------------|---------------|---------|---------------|
| Avila | 94.55 ± 1.03 | 99.84 ± 0.01 | 97.03 ± 0.45 | 98.87 ± 0.08 |
| Hand Posture | 99.23 ± 0.38 | 98.18 ± 0.01 | 96.63 ± 0.58 | 94.51 ± 0.37 |
| Segmentation | 96.77 ± 0.21 | 72.14 ± 0.68 | 97.79 ± 0.03 | 97.38 ± 0.13 |
| Spam Base | 94.56 ± 1.65 | 75.81 ± 0.43 | 94.34 ± 0.08 | 93.26 ± 0.04 |
| Sensorless | 97.44 ± 0.04 | 58.84 ± 1.39 | 99.03 ± 0.01 | 98.47 ± 0.02 |
| Bank Credit | 95.75 ± 0.07 | 66.04 ± 1.4 | 75.78 ± 0.34 | 75.11 ± 0.48 |
| Wine Quality | 90.63 ± 0.01 | 97.97 ± 0.01 | 62.78 ± 0.4 | 64.06 ± 1.24 |
| HTRU | 95.21 ± 0.01 | 96.73 ± 0.03 | 97.95 ± 0.02 | 97.56 ± 0.05 |

### Conclusion

Data augmentation techniques show promising results, with significant accuracy improvements in most cases. Out of eight tested datasets, seven showed significant accuracy improvements. For more than half of the datasets, the gap between the single decision tree and complex models (XGBoost, Random Forest) was reduced by 50%. Statistical tests confirm these findings.