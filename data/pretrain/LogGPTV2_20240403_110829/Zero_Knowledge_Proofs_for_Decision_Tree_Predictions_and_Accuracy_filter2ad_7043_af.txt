0.38
51
180
Adaptive
3
0.41
55
129
5
10
0.39 0.42
49
56
210
176
5
3
Clustering based
10
0.39
51
165
0.41
49
115
0.43
52
130
Table 4.9: Results on solar ﬂare target 2 given different sets of hyper-
parameters (10 fold cross-validation, ms stands for milliseconds)
gorical features. Nevertheless, experiments on the other dataset point
out a large increase in the training time. Despite all those experiments,
it is not possible to select a best method because it seems to depend on
each use case. However, during the experimentation we have noticed
that the adaptive quantile method could be interesting because it pro-
vides quantile that are more revelvant at each level of the decision tree.
Nevertheless this method is relatively time-consuming. We have also
noticed that sometimes it is not interesting to have a lot of quantiles
because when the algorithm reaches a high depth, there are only a few
datapoints in each node, so dividing the data in 10 different categories
is no longer relevant. Table 4.11 summarizes statistics about ranking
of tested methods, because there is no statistical difference in term of
accuracy Table 4.12 we choose the number of nodes to rank them (the
CHAPTER 4. RESULTS
45
Dataset
Facebook
Nodes
53 ± 7
Solar ﬂare target 1 327 ± 44
Solar ﬂare target 2 172 ± 23
Training time (milliseconds)
Basic method
10520 ± 215
97 ± 12
75 ± 11
Reworked categorical split
Dataset
Facebook
Solar ﬂare target 1
Solar ﬂare target 2
Nodes
35 ± 5
98 ± 22
49 ± 4
Training time (ms)
11230 ± 260
121 ± 14
115 ± 13
MSE
24.33 ± 5.25
0.73 ± 0.08
0.32 ± 0.05
MSE
25.03 ± 4.34
0.80 ± 0.07
0.41 ± 0.04
Table 4.10: Best results for regression datasets (10 fold cross-validation,
ms stands for milliseconds)
lesser the better).
Basic method Quantile based Adaptive Clustering based
Facebook
Solar ﬂare target 1
Solar ﬂare target 2
Average
4
4
4
4
1
1
3
1.66
2
3
1
2
3
2
2
2.33
Table 4.11: Ranking (in terms of number of nodes) of tested method
for categorical split
To conclude we perform statistical testing on the three metrics of
evaluation, comparing the new splitting method to the classic one de-
scribed in [7]. For that, we assume that there were no differences in
terms of performances on both algorithms. The number of nodes in
each decision tree has been signiﬁcantly reduced (Table 4.14) but at
the cost of computation time (Table 4.13), in both case the effect size
is important. The accuracy seems to not differ between both methods
with an effect size close to zero (Table 4.12).
46
CHAPTER 4. RESULTS
Reject equality of mean squared error
p-value
Cohen d-test
Facebook
Solar ﬂare target 1
Solar ﬂare target 2
1.8 × 10−4
Table 4.12: Summary of statistical tests on accuracy on regression
tasks, comparing new splitting methods and classic one ((cid:88): Equality
rejected x : Cannot reject the equality)
x
x
(cid:88)
0.86
0.16
0.14
0.93
1.98
Facebook
Solar ﬂare target 1
Solar ﬂare target 2
Reject equality of training time
(cid:88)
(cid:88)
x
p-value
5.23 × 10−7
2.02 × 10−4
6.44 × 10−8
Cohen d test
2.97
1.84
3.32
Table 4.13: Summary of statistical tests on training time on regression
tasks. Comparison between new splitting method and classic one. ((cid:88):
Equality rejected x : Cannot reject the equality)
Facebook
Reject equality of number of nodes
Solar ﬂare target 1
Solar ﬂare target 2
p-value
1, 14 × 10−5
5, 72 × 10−11
2, 07 × 10−8
Table 4.14: Summary of statistical tests on number of nodes on re-
gression tasks. Comparison between new splitting method and classic
one.((cid:88): Equality rejected x : Cannot reject the equality)
(cid:88)
(cid:88)
(cid:88)
Cohen-d test
2.95
6.64
7.45
4.2 Sampling method and data augmentation
Several methods has been tried to increase the number of available
data in order to make an enhanced decision tree on top of this en-
riched dataset. The ﬁrst experiment was to try on toy datasets de-
scribed above in order to have an idea of what was happening during
the augmentation phase, and to see if the method was relevant. Here
are the results on the two complex datasets which are non-linear.
CHAPTER 4. RESULTS
47
(a) Original dataset
(b) Augmented dataset
(c) Boundaries before data aug-
mentation
(d) Boundaries after data augmen-
tation
Figure 4.5: Experiments of Gaussian augmentation on the half circle
dataset
48
CHAPTER 4. RESULTS
(a) Original dataset
(b) Augmented dataset
(c) Boundaries before data augmen-
tation
(d) Boundaries after data augmenta-
tion
Figure 4.6: Experiments of noise augmentation on circle dataset
Figure 4.5 represents the results obtained by performing a Gaus-
sian augmentation on the initial half circle dataset. Results show that
generated points near the boundaries of the ﬁrst classiﬁer appear to
be classiﬁed with a smaller conﬁdence because this area is a zone of
uncertainty. We used this level of conﬁdence to weight new generated
dataset we provide to our learner, so this result are quite reassuring
because points that are far from boundaries and far from the dataset
will be less taken into account.
Figure 4.6 shows the principle of the noise dataset augmentation,
and allows to verify that the experiment is working. In this case the
right boundaries are also retrieved both with and without sampling,
even though they are slightly different.
The following tables show the results obtained by using these tech-
niques on several datasets that were presented in the previous sec-
tions. Table 4.15 and Table 4.16 present all the results we added. Table
4.17 presents the best results compared to the one obtained by tradi-
tional algorithms without data augmentation.
CHAPTER 4. RESULTS
49
1
Augmentation factor
3
5
2
1
Sampling variance
0.5
97.29 98.27 98.87
93.12 93.31 93.41
95.71 95.00 95.00
91.63 92.07 91.63
98.35 98.33 98.37
73.00 73.08 73.68
60.00 62.50 60.93
97.09 97.43 97.09
2
1
Sampling variance
0.5
97.65 97.79 98.03
93.62 93.88 94.49
97.14 96.66 95.95
90.77 92.18 93.26
98.45 98.47 98.30
72.65 73.81 73.75
60.62 62.18 62.18
97.54 97.23 97.26
1
2
Sampling variance
0.5
97.19 97.84 98.58
93.77 94.41 94.51
95.95 97.38 95.23
91.74 92.94 91.74
98.18 98.27 98.28
72.80 74.70 75.25
64.06 58.43 62.50
97.56 97.56 97.37
Dataset
Avila
Hand posture
Segmentation
Spam base
Sensorless
Bank credit
Wine quality
HTRU
Table 4.15: Accuracies obtained with the Gaussian sampling method.
Text in red indicate the best methods over all tested.
Dataset
Gaussian mixture
Shooting manner
Simple Gaussian
Uniform
Augmentation factor Augmentation factor Augmentation factor
1
Avila
98.75
Hand posture 93.09
Segmentation 95.23
92.61
98.21
73.33
61.50
97.21
Spam base
Sensorless
Bank credit
Wine quality
Htru
Average rank
3
98.41
93.41
95.00
92.39
98.43
73.78
61.93
97.37
2
5
97.96
93.21
93.33
91.96
98.37
73.81
61.00
97.12
1
98.46
93.02
96.42
92.94
97.92
74.05
62.56
97.02
5
96.83
94.18
95.47
92.94
97.70
74.07
61.50
97.16
3
98.44
94.31
96.19
91.85
97.98
74.11
62.18
97.23
1.62
1
3
98.13 97.91
94.02 92.93
95.47 94.76
91.53 92.07
98.03 98.05
73.56 73.85
62.32 62.56
96.99 97.11
2.37
5
98.10
93.14
95.95
92.18
97.80
73.71
63.50
97.05
Table 4.16: Results for the Gaussian method
50
CHAPTER 4. RESULTS
Dataset
Avila
Hand posture
Segmentation
Spam base
Sensorless
Bank credit
Wine quality
Htru
Decision tree Random forest
94.55 ± 1.03
99.23 ± 0.38
96.77 ± 0.21
94.56 ± 1.65
97.44 ± 0.04
95.75 ± 0.07
90.63 ± 0.01
95.21 ± 0.01
99.84 ± 0.01
98.18 ± 0.01
72.14 ± 0.68
75.81 ± 0.43
58.84 ± 1.39
66.04 ± 1.4
97.97 ± 0.01
96.73 ± 0.03
Average rank
3.87
1.12
XGBoost
97.03 ± 0.45
96.63 ± 0.58
97.79 ± 0.03
94.34 ± 0.08
99.03 ± 0.01
75.78 ± 0.34
62.78 ± 0.4
97.95 ± 0.02
2.12
Augmented DT
98.87 ± 0.08
94.51 ± 0.37
97.38 ± 0.13
93.26 ± 0.04
98.47 ± 0.02
75.11 ± 0.48
64.06 ± 1.24
97.56 ± 0.05
2.87
Table 4.17: Augmented data tree versus other classic algorithms, the
number after the ± symbol represents the standard deviation of the ac-
curacy based on a 10-fold cross validation. Average rank is computed
by averaging over all datasets the rank of the algorithm, DT stands for
decision tree
The results on these methods are quite promising because in most
cases, data augmentation techniques give a strong improvement in
term of accuracy. In fact, on eight tested datasets, seven have a signif-
icant accuracy improvement by using data augmentation. Moreover,
for more than half of datasets, we managed to reduce the gap by 50
% between the single decision tree accuracy and the complex models
accuracy (XGboost, Random forest). Statistical tests are summarized