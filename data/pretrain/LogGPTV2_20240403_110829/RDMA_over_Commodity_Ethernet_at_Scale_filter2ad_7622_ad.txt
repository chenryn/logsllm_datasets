podset 0 was paired with ToR i in podset 1. In each
ToR, we selected 8 servers, and let each server estab-
210
(b) The aggregate RDMA throughput.
Figure 7: The aggregate RDMA throughput in a three-
layer Clos network. The y-axis shows the number of
frames/second. A frame size is 1086 bytes.
lish 8 RDMA connections to the corresponding server
in the other ToR. All these RDMA connections needed
to traverse the Leaf-Spine links. All the RDMA con-
nections sent data as fast as possible. In total we had
3074 connections distributed among the 128 Leaf-Spine
links, which were the bottlenecks in this experiment.
Figure 7(b) shows the aggregate throughput mea-
sured from the servers. The unit of the y-axis is frames
per second. The RDMA frame size is 1086 bytes with
1024 bytes as payload. The aggregate throughput is
3.0Tb/s. This is 60% network utilization of the total
5.12Tb/s network capacity. During the whole experi-
ment, not a single packet was dropped. Every server
was sending and receiving at 8Gb/s with the CPU uti-
lization close to 0%.
Since we use ECMP for multi-path routing in our net-
work, 60% utilization is what we can achieve for this ex-
periment. This 60% limitation is caused by ECMP hash
collision, not PFC or HOL blocking. Both our simula-
tion and the results in [2], in which no PFC was used,
showed similar utilization numbers for ECMP routing
in three-tier Clos networks.
6. EXPERIENCES
6.1 RDMA Deployment
RoCEv2 was a new technology to us when we began
this work three years ago. We were unaware of any
large-scale RoCEv2 deployment at that time. Though
the beneﬁts (zero packet drops, low latency, and low
CPU overhead) were attractive, we were concerned about
the maturity of RoCEv2. We devised a step-by-step
procedure to onboard RDMA.
For the ﬁrst step, we built a small lab network with
tens of servers. This step helped us eliminate most of
the bugs at early stage.
In the second step, we used
test clusters to improve the maturity of RoCEv2. The
test clusters’ setup and management were the same as
their production counterparts.
In the third step, we
enabled RDMA in production networks at ToR level
only. In the fourth step, we enabled PFC at the Podset
level, i.e., we enabled PFC in the ToR and Leaf switches
within the Podsets. In the last step, we enabled PFC
up to the Spine switches. In every step when we carried
out deployment in production, we followed our safe de-
ployment procedure to enable RDMA through several
phases in our global data centers.
This step-by-step procedure turned out to be eﬀec-
tive in improving the maturity of RoCEv2. The RDMA
transport livelock and most of the bugs were detected
in lab tests. The PFC deadlock and slow-receiver symp-
tom were detected in the test clusters. Only the NIC
PFC pause frame storm and a few other bugs hit our
production networks.
Using the same management and monitoring for both
the test clusters and the production networks turned
out to be invaluable.
It made our life easier as the
test clusters were always well managed. At the same
time, it let us thoroughly test RoCEv2 as well as the
management and monitoring systems before RoCEv2
went into production.
6.2 Incidents
NIC PFC storm. The following is one of the few NIC
PFC storm incidents we have encountered. In this inci-
dent, one of our customers experienced a service avail-
ability issue. Many of their servers became unavailable
as shown in Figure 9(a). At the same time, we ob-
served that many of the servers were continuously re-
ceiving large number of PFC pause frames as shown by
our monitoring system in Figure 9(b). The y-axis shows
the number of PFC pause frames sent/received in every
ﬁve minutes.
We were able to trace down the origin of the PFC
pause frames to a single server. That server was unre-
sponsive and was in Failing (F) state as detected by
our data center management system. But from the
connected ToR switch, we could observe the number
of pause frames from the server was always increasing,
at more than two thousands pause frames per second.
Figure 8: The end-to-end RDMA latency jumped up
as the experiment started and network throughput in-
creased.
We unfortunately did not record the end-to-end RDMA
latency in the above throughput experiment. To fur-
ther investigate the relationship between network la-
tency and throughput, we conducted the following ex-
periment in our testbed with a two-tier network. We
had two ToR switches in this testbed. Each ToR switch
had 24 servers, and each ToR used 4 uplinks to con-
nect to four Leaf switches. All the links were 40GbE.
The oversubscription ratio was 6:1. We mimicked The
traﬃc pattern in Figure 7. We chose 20 servers in ev-
ery ToR and paired every server in one ToR with one
server in another ToR and let every server-pair estab-
lish 8 RDMA connections. Every server achieved 7Gb/s
sending/receiving throughput. We show the RDMA la-
tency measured in Pingmesh in Figure 8. Once the
experiment started, the end-to-end RDMA latencies in-
creased from 50us at the 99th percentile and 80us at the
99.9th percentile to 400us and 800us, respectively.
Note that the 99th percentile latency of TCP did not
change during the experiment in Figure 8. This is be-
cause we put RDMA and TCP packets into two diﬀerent
queues in the switches. Hence RDMA and TCP did not
interfere with each other. We note that the 99th per-
centile latency of TCP was 500us in Figure 8, whereas
it was 700us in Figure 6. The diﬀerence was caused by
the fact that the servers in Figure 6 were servicing real-
world workload whereas the servers in Figure 8 were al-
most idle (except running the RDMA traﬃc generator).
Figure 8 also demonstrated that the RDMA latency in-
crease was due to the network congestion created by the
RDMA traﬃc.
The above measurement results show that, compared
to TCP, RDMA achieves low latency and high through-
put by bypassing the OS kernel and by eliminating
packet drops. But RDMA is not a panacea for achiev-
ing both low latency and high throughput. The RDMA
latency can still increase as the network becomes con-
gested and queues build up.
211
(a) Server availability reduction. H (healthy), F (fail-
ing), and P (probation) are server states.
(b) The PFC pause frames received by the servers.
Figure 9: An incident caused by the NIC PFC storm
problem of a single server.
We also observed that the server was not sending or re-
ceiving any data packets. After we power-cycled that
server, the server came back up and the pause frames
were gone.
NIC PFC storms happened very infrequently. With
hundreds of thousands of servers in production, the
number of the NIC PFC storm events we have expe-
rienced is still single digit. Nonetheless, once NIC PFC
storm happens, the damage is huge due to the PFC
pause frame propagation. As we can see from this in-
cident, half of our customers servers were aﬀected and
put into non healthy state.
After we put the NIC PFC storm prevention watch-
dogs at both the servers and the ToR switches, we did
not experience NIC PFC storms anymore.
Switch buﬀer misconﬁguration. The ToR and Leaf
switches we use have a small and limited buﬀer size of
9MB or 12MB. To better utilize the scarce buﬀer space,
we need to enable dynamic buﬀer sharing. In dynamic
buﬀer sharing, the ports allocate memory from a shared
buﬀer pool. The shared buﬀer allocation per port per
traﬃc class is controlled by a parameter called α. As
long as α × U B > Bp,i, where U B is the unallocated
shared buﬀer size and Bp,i is the allocated buﬀer size for
traﬃc class i of ingress port p, we can allocate memory
from the shared buﬀer for traﬃc class i from ingress
port p. Hence a large α can help reduce the chance of
PFC pause frames from been generated. But a large α
may cause imbalanced and unfair buﬀer allocation.
We have found that the default α value (α = 1
16 )
for a type of ToR switch worked well in our production
network. When we onboarded a new type of switch
from the same switch provider, we took it for granted
that it would use the same default settings as before.
Then in the midnight of 07/12/2015, we ran into an
incident. As shown in Figure 10(a), the latencies of
many latency-sensitive services increased dramatically.
Also we have observed that many servers were receiving
212
(a) Services latency increase caused by the PFC pause
frame propagation. Every color here represents an im-
pacted service.
(b) The PFC pause frames received by the servers.
Figure 10: An incident caused by the buﬀer misconﬁg-
uration of a newly introduced switch type.
a large number of PFC pause frames, up to 60000 pause
frames in 5 minutes (Figure 10(b)).
Further analysis revealed the origins of the pause frames.
The pause frames were generated by two ToR switches,
then propagated into the rest of the network, and af-
fected thousands of servers.
Why there were so many pause frames been gener-
ated? There were two reasons. The ﬁrst was the incast
traﬃc pattern. These two ToR switches hosted many
chatty servers, which sent queries to more than one
thousand servers simultaneously. Once the responses
came back to the chatty servers, incast happened, which
created network congestion condition for PFC pause
frame generation.
The second reason was that we found the α value of
the new type of ToR switch was changed to 1
64 , though
these two types of switches were from the same provider.
A much smaller α made the dynamic buﬀer allocated
to the congested ingress ports much smaller. Hence the
PFC pause frames could be triggered much more easily.
We could not change the traﬃc pattern, so we tuned
the α value back to 1
16 for these switches.
The lesson we learned from this incident is that PFC
pause frames did propagate and cause collateral damage
in our production network. To reduce the damage, we
need to reduce PFC pause frames from being generated.
Our work on the NIC PFC storm and the slow-receiver
symptom prevent servers from been generating pauses.
Moreover, parameter tuning of the dynamic buﬀer shar-
ing and the per-ﬂow based DCQCN [42] congestion con-
trol reduce the pauses generated by the switches.
6.3 Lessons learned and discussion
During the three years period of designing, building,
and deploying RoCEv2, we have learned several lessons
which we share as follows.
Deadlock, livelock, and PFC pause frames prop-
agation did happen. The PFC deadlock we met was
a surprise to us, as we once believed that our Clos-based
network topology was free of cyclic buﬀer dependency
hence free of deadlock. We did not expect the slow-
server symptom, though we were fully aware that PFC
backpressure can cause PFC pause frame propagation
in the network. We did not foresee the RDMA transport
livelock either. The lesson we learned is that a design
works in theory is not enough, as there may be many
hidden details which invalidate the design. We have to
use well designed experiments, test clusters, and staged
production deployments, to verify the designs and to
unveil the unexpected facets methodologically.
NICs are the key to make RDMA/RoCEv2
work. Most of the RDMA/RoCEv2 bugs we ran into
were caused by the NICs instead of the switches. We
spent much more time on the NICs than on the switches.
In hindsight, this happened for two reasons. The ﬁrst
reason is because the NIC implements the most compli-
cated parts of the RDMA functionalities, including the
RDMA verbs and the RDMA transport protocol. As a
comparison, the switch side functionalities are relatively
simple (e.g., PFC implementation) or well tested (e.g.,
ECN implementation). The second reason is that the
NICs we use are resource constrained. The NIC lever-
ages the server’s DRAM to store its data structures and
uses its own local memory as the cache. Cache manage-
ment then becomes a big part of the NIC and introduces
bugs as well as performance bottlenecks, e.g., the slow-
receiver symptom.
Be prepared for the unexpected. Our experi-
ences of running one of the largest data center networks
in the world taught us that network incidents happen.
From day one when we began to work on RoCEv2,
we put RDMA/RoCEv2 management and monitoring
as an indispensable part of the project. We upgraded
our management and monitoring system for RDMA sta-
tus monitoring and incidents handling at the same time
when we worked on the DSCP-based PFC design and
the safety and performance bugs. As a result, when our
customers began to use RDMA, the RDMA manage-
ment and monitoring capabilities were already in pro-
duction. This RDMA management and monitoring sys-
tem is essential for RDMA health tracking and incident
troubleshooting. It helped us detect, localize, and root-
cause the RoCEv2 bugs and incidents as we have shown
in Sections 6.2 and 4.
Is lossless needed? RoCEv2 depends on a loss-
less network to function well.
In this work, we have
demonstrated that we indeed can build a lossless net-
work using PFC, and all the real-world scalability and
safety challenges can be addressed. Looking forward
into the future, the question we would like to ask is:
do we really need a lossless network to get the bene-
ﬁts of RoCEv2? Given the progress on programmable
hardware, e.g., FPGA and FPGA integrated CPU [8],
it may become feasible and economical to build much