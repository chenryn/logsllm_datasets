在代码的这一部分中，请注意，我们用`sklearn`库来导入树模型，
以及通过`graphviz`库来产生[#resulting_decision_treedot_blue_is_falsed](#resulting_decision_treedot_blue_is_falsed)中看到的图形。数据变量填充了几个正常运行的服务器的示例。状态名称为
y 轴，指标为 x 轴：
    mytree = tree.DecisionTreeClassifier()
    mytree = mytree.fit(metrics, states)
填充数据集后，我们使用`mytree`决策树分类器从 scikit-learn
实例化对象，并使用我们以前的数据训练模型。`mytree.fit`则来自之前的代码：
    # is 10% cpu, 80% RAM, 10% Storage healthy?
    print("10% CPU, 80% RAM, 10% Storage", (mytree.predict([[10, 80, 10]])))
    # is 80% cpu, 10% RAM, 90% Storage healthy?
    print("80% CPU, 10% RAM, 90% Storage (high)", mytree.predict([[80, 10, 90]]))
    # is 60% cpu, 90% RAM, 10% Storage healthy?
    print("60% CPU, 90% RAM (high), 10% Storage", mytree.predict([[60, 90, 10]]))
这将导致以下输出：
    10% CPU, 80% RAM, 10% Storage ['healthy']
    80% CPU, 10% RAM, 90% Storage (high) ['unhealthy']
    60% CPU, 90% RAM (high), 10% Storage ['unhealthy']
接下来，我们测试并打印预测结果，如[#resulting_decision_treedot_blue_is_falsed](#resulting_decision_treedot_blue_is_falsed)所示。请注意，决策树能够找出不正常的，超高
CPU 和 RAM 使用率，达到了我们培训的目的：
    # Visualize the decision tree
    dot_data = tree.export_graphviz(mytree,
    feature_names=['CPU','RAM','Storage'],
    class_names=['healthy','unhealthy'],
    filled=True, rounded=True,out_file=None)
    graphviz.Source(dot_data)
![结果决策树：右侧分支为 false，左侧分支更接近
true，直到最后一个分支](media/rId59.png){width="3.25083552055993in"
height="3.635450568678915in"}
结果决策树：右侧分支为 false，左侧分支更接近 true，直到最后一个分支
最后，让我们简要分析一下它是如何工作的。[#resulting_decision_treedot_blue_is_falsed](#resulting_decision_treedot_blue_is_falsed)显示了我们通过培训创建的决策树。待决策的数据会通过一系列布尔决策，这些决策基于已有的用于比较的值。如果任何比较返回
false，则返回 `'unhealthy'` ，但如果所有检查都返回
true，则我们会得到`'healthy'` 。
此演示表明，通过使用一组简单的数据和决策树，我们已经可以做很多事了。  
### 神经网络入门
  Next, let's explore some more examples, like some simple Python code
to train a three-layer neural network from scratch. First, just using
NumPy, we can construct a simple neuron using a  "Sigmoid." This is the
activation of a neuron, a function that will map any value to a value
between 0 and 1 so that it *creates probabilities* out of numbers:
    import numpy as np
    def nonlin(x,deriv=False):
      if(deriv==True):
        return (x*(1–x))
      return (1/(1+np.exp(-x)))
[#sigmoid_function](#sigmoid_function)显示上述代码在操作中。
![Sigmoid 函数。](media/rId61.png){width="4.8125in"
height="3.2083333333333335in"}
Sigmoid 函数
[我们在#biological_neuron_and_an_artificial_neuro](#biological_neuron_and_an_artificial_neuro)看到的人工神经元有不同的输入（x*~1~\...x^n^*）具有不同的权重（*w~1~\...w~n~*）然后，这些输入的加权总和通过
Sigmoid 或 Heaside 步长函数*f*传递，如下所示：
    # Initialize the dataset as a matrix with input Data:
    X = np.array([[0,0,1],
                 [0,1,1],
                 [1,0,1],
                 [1,1,1]])
    # Output Data with one output neuron each:
    Y = np.array([[1],
                  [0.7],
                  [1],
                  [0]])
    # Seed to make them deterministic
    np.random.seed(1)
    # Create synapse matrices.
    synapse0 = 2 * np.random.random((3, 4)) - 1
    synapse1 = 2 * np.random.random((4, 1)) - 1
接下来，我们将数据集初始化为包含输入数据（x）的矩阵。每行是一个不同的训练示例，每列表示不同的神经元。输出数据（y）每个输出神经元，我们将它们播种，使它们具有确定性；这将产生具有相同起点的随机数（可用于调试），因此，每次运行程序时，都可以获得相同的生成数序列。为了完成这个简单的神经网络，我们创建了两个*突触矩阵*并初始化神经网络的权重。我们刚刚创建了一个具有两层权重的神经网络：
    # Training code (loop)
    for j in xrange(100000):
        # Layers layer0,layer1,layer2
        layer0 = X
        # Prediction step
        layer1 = nonlin(np.dot(layer0, synapse0))
        layer2 = nonlin(np.dot(layer1, synapse1))
        # Get the error rate
        layer2_error = Y - layer2
        # Print the average error
        if(j % 10000) == 0:
            print "Error:" + str(np.mean(np.abs(layer2_error)))
        # Multiply the error rate
        layer2_delta = layer2_error * nonlin(layer2, deriv=True)
        # Backpropagation
        layer1_error = layer2_delta.dot(synapse1.T)
        # Get layer1's delta
        layer1_delta = layer1_error * nonlin(layer1, deriv=True)
        # Gradient Descent
        synapse1 += layer1.T.dot(layer2_delta)
        synapse0 += layer0.T.dot(layer1_delta)
上例中的训练代码涉及更多，我们为给定数据集优化网络。第一层（`layer0`
）只是我们的输入数据。预测步骤在每个图层及其突触之间执行矩阵乘法。然后，我们在矩阵上运行
Sigmoid
函数以创建下一个图层。通过`layer1`中的数据除以`layer2`产生预测数据，对于`layer2`我们可以使用减法将其与预期输出数据进行比较，以获得错误率。然后，我们继续按设定的时间间隔打印平均误差，以确保每次错误都会下降。
我们将误差率乘以 Sigmoid
的斜率，在`layer2`中的值和 做*反向传播*，这是在*神经网络*上执行*梯度下降*的主要算法。首先，每个节点的输出值在正向传递中计算（和缓存）。然后，在图形的向后传递中计算与每个参数误差的局部导数。它是\"错误向后传播\"的缩写，即导致
`layer1` 上的错误的原因， `layer2` 并将 `layer2` 增量乘以突触 1 的转置。
接下来，我们将 `layer1` 通过将误差乘以 Sigmoid
函数的结果，并通过*梯度下降*这是一种通过计算相对于模型参数的损耗梯度来最小化*损失*的技术，条件在训练数据上。非正式地，梯度下降迭代地调整参数，逐渐找到*权重*和偏置的最佳组合，以尽量减少损耗。来获得的增量，也就是一个一阶迭代优化算法，用于查找函数的最小值，最后更新权重。现在，我们每个层都有增量，我们可以使用它们来更新突触速率，以便每次迭代时都能更降低错误率。这将生成以下内容：
    Error:0.434545246367
    Error:0.00426490134801
    Error:0.00285547679431
    Error:0.00226684843815
    Error:0.00192718684831
    Error:0.00170049171577
    Error:0.00153593455208
    Error:0.00140973826096
    Error:0.00130913223749
    Error:0.00122657710472
这意味着随着神经网络的学习，误差越来越接近于零。如果我们打印每个
`layer2` 和我们的目标：
    print "Output after training"
    print layer2
    Output after training
    [[ 0.99998867]
     [ 0.69999105]
     [ 0.99832904]
     [ 0.00293799]]
    print "Initial Objective"
    print Y
    Initial Objective
    [[ 1. ]
     [ 0.7]
     [ 1. ]
     [ 0. ]]
我们已经成功地创建了一个神经网络，仅使用NumPy和一些数学，并训练它接近初始目标，通过使用反向传播和梯度下降。这在更大的场景中非常有用，我们教神经网络识别模式，如异常检测、声音、图像，甚至我们平台中的某些事件，稍后将会看到。   
### 使用 TensorFlow 和 TensorBoard
  正如我们现在所看到的，Google 的 TensorFlow 只不过是我们刚刚看到的
NumPy。主要区别在于 TensorFlow
首先构建了要完成的所有操作的图形，然后在调用"会话"时，它"运行"图形。它可以扩展，并通过
CUDA 来使用 GPU。另一个库 Keras 简化了 TensorFlow
的编码（参见[#tensordot9](#tensordot9)）。
![一个张量。](media/rId63.png){width="2.1199989063867015in"
height="1.5533333333333332in"}
在数学中，*张量*是描述几何矢量、标量和其他张量之间的线性关系的几何对象。此类关系的基本示例包括点积、交叉产品和线性贴图。（来源：[*https://en.wikipedia.org/wiki/Tensor*](https://en.wikipedia.org/wiki/Tensor)。）
请记住，TensorFlow
本身没有"神经元"，但它确实喜欢线性代数。神经元只能算是过去对生物智能的一种模拟，其实一切都只是*矩阵数学*，或者从抽象角度来说算是*张量数学*，这是
TensorFlow 名字的由来。这个模型使 TensorFlow
非常灵活，并使其能够实现之前难以企及的高效计算。
可以从 `pip` 包管理器安装 TensorFlow：
    pip install tensorflow
要测试安装，可以运行以下命令集：
    import tensorflow as tf
    a = tf.constant(1.0)
    b = tf.constant(2.0)
    c = a + b
    sess = tf.Session()
    print(sess.run(c))
生成的操作以[#resulting_operation](#resulting_operation)来展示。
![生成的操作。](media/rId65.png){width="1.2107020997375328in"
height="0.5284273840769904in"}
生成的操作
忽略有关 CPU 的任何警告。我们的 TensorFlow 运行的结果应该是 `3.0` 。
前面的测试虽然不是很令人印象深刻，但显示了 TensorFlow
在实际在会话中运行之前如何声明内容。
以下
Python[*代码，https://www.oreilly.com/learning/hello-tensorflow*](https://www.oreilly.com/learning/hello-tensorflow)；感谢
Aaron Schumacher 的分享。虽然不使用真实数据集，但使用 TensorFlow
进行一些数据神经元训练：
    1  import tensorflow as tf
    2 
    3  x = tf.constant(1.0, name='input')
    4  w = tf.Variable(0.8, name='weight')
    5  y = tf.multiply(w, x, name='output')