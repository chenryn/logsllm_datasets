### Table 5: Lines of Code (LOC) of Test Scripts
| Script Language | Average LOC |
|-----------------|-------------|
| VBA             | 3.8         |
| VBScript        | 2.75        |
| PowerShell      | 2           |

### Human Effort for Test Script Preparation

To address Research Question 6 (RQ6), we conducted an experiment to evaluate the human effort required to prepare test scripts. We assessed this from two perspectives: the lines of code (LOC) in the test scripts and the time required to create them.

#### Participants
We recruited ten participants, including eight graduate students, one technical staff member, and one visiting researcher, all from the computer science department.

#### Methodology
We provided the participants with an explanation of the concept and requirements of the test scripts as described in Section 3.2. They were then asked to write valid test scripts while their time was recorded. Many participants had no prior experience with VBA, VBScript, or PowerShell. Therefore, we allowed them some time to familiarize themselves with the language specifications, assuming that test script writers should have knowledge of the target language. All created test scripts were validated using STAGER.

#### Results
Table 5 shows the average LOC for the test scripts written in each language. The LOC ranges from 2 to 3.8, indicating that the test scripts used in our method are relatively simple.

Figure 8 illustrates the average time required to create test scripts for each language:
- **VBScript**: 36.6 seconds per script API
- **VBA**: 42.6 seconds per script API
- **PowerShell**: 42.6 seconds per script API

The overall average time for all languages was approximately 59.5 seconds. These results suggest that writing valid test scripts is a quick process for programmers who are knowledgeable about the target script language. Consequently, the human effort required for using STAGER is significantly less than that needed for manual reverse-engineering of script engines, which can take weeks or months.

### Discussion

#### Limitations
We identified three scenarios where our proposed method may not detect hook and tap points:

1. **No Arbitrary Argument Values**:
   - If the target script API does not have arguments that can be set to arbitrary values, tap point detection, which relies on argument matching, will fail.

2. **Small Amount of Program Code**:
   - For script APIs with minimal program code, differential execution analysis might not be effective because the differences are not well observed. However, such simple APIs are unlikely to be significant targets for malware analysts.

3. **Heavily Obfuscated Script Engines**:
   - If the script engine is heavily obfuscated, such as when the control flow graph is flattened into a single function, hook point detection may not be accurate. However, such obfuscation is rare in modern script engines.

#### Human-Assisted Analysis
While our method automates the detection of hook and tap points, human-assisted analysis can still be beneficial, especially in cases where tap point detection fails. Manual analysis can consider the semantics of values, which our automated method does not. This can help in discovering tap points even when value matching is not possible. Additionally, manual analysis can provide better type information for variables, improving the accuracy of tap point detection.

The burden of manual analysis with our method is much less than complete manual analysis. As shown in Table 3, hook point detection reduces the number of functions that need to be analyzed from thousands to just tens.

### Related Work

#### Script Analysis Tools
Several tools have been developed for script analysis, including jAEk, Revelo, box-js, jsunpack-n, and JSDetox. These tools use script-level monitoring but do not meet the requirements outlined in Section 2.1 due to their dependence on JavaScript language specifications.

Other tools, such as Sulo, JSAND, and FlashDetect, use script engine-level monitoring. ViperMonkey is an emulator for VBA that logs notable script APIs. System-level monitoring tools, like API Chaser, can hook system APIs and calls but do not fulfill the requirements introduced in Section 2.1.

#### Script Engine Enhancement
Chef is a symbolic execution engine for script languages that uses a real script engine. It is similar to STAGER in reusing the target script engine for instrumentation. However, Chef's approach is based on manual source code analysis, and its goal is to build symbolic execution engines, whereas STAGER aims to build script API tracers.

#### Mitigation of Semantic Gap
Methods like Virtuoso and Tappan Zee (North) Bridge (TZB) have been developed to mitigate the semantic gap between guest and host OSes in virtual machine monitors (VMMs). Virtuoso creates VM introspection tools by analyzing execution traces, while TZB discovers tap points for VM introspection by monitoring memory access.

#### Reverse Engineering of Virtual Machines
Sharif et al. proposed a method for automatically reverse engineering VMs used by malware. Their focus is on identifying bytecode syntax and semantics, which is different from our goal of detecting local functions and their arguments related to script APIs. Coogan et al. focused on identifying bytecode instructions for system call invocations, which is also different from our approach.

#### Differential Execution Analysis
Carmony et al. and Zhu et al. used differential execution analysis to identify tap points in Adobe Acrobat Reader and anti-adblockers, respectively. Although they used similar methods, their focus and differentiation algorithms differ from ours.

### Conclusion
In this paper, we addressed the limitations of current script dynamic analysis tools and proposed a method for automatically building script API tracers by analyzing the binaries of script engines. Our method detects appropriate hook and tap points through dynamic analysis using test scripts. Experiments with a prototype system confirmed that our method can effectively append script behavior analysis capabilities to script engines. Future work will focus on enhancing these capabilities further.

### Acknowledgments
We thank Tomoya Matsumoto, Yuki Kimura, and the members of Matsuura Laboratory for their support in the experiment. We also appreciate the insightful comments from anonymous reviewers. This work was partially supported by JSPS KAKENHI Grant Number JP17KT0081.

### References
[1] VirusTotal. [n. d.]. https://www.virustotal.com/. (accessed: 2017-03-09).
[2] Pieter Agten, Steven Van Acker, Yoran Brondsema, Phu H Phung, Lieven Desmet, and Frank Piessens. 2012. JSand: complete client-side sandboxing of third-party JavaScript without browser modifications. In Proceedings of the 28th Annual Computer Security Applications Conference (ACSAC ’12). ACM, 1–10.
[3] The Dependable Systems Lab at EPFL in Lausanne. [n. d.]. Chef. https://github.com/S2E/s2e-old/tree/chef. (accessed: 2018-01-01).
[4] Rohitab Batra. [n. d.]. API Monitor. http://www.rohitab.com/apimonitor. (accessed: 2019-02-15).
[5] Stefan Bucur, Johannes Kinder, and George Candea. 2014. Prototyping symbolic execution engines for interpreted languages. In ACM SIGPLAN Notices, Vol. 49. ACM, 239–254.
[6] CapacitorSet. [n. d.]. box.js. https://github.com/CapacitorSet/box-js. (accessed: 2019-02-15).
[7] Curtis Carmony, Xunchao Hu, Heng Yin, Abhishek Vasisht Bhaskar, and Mu Zhang. 2016. Extract Me If You Can: Abusing PDF Parsers in Malware Detectors.. In Proceedings of the 23rd Annual Network and Distributed System Security Symposium (NDSS ’16). Internet Society, 1–15.
[8] Kevin Coogan, Gen Lu, and Saumya Debray. 2011. Deobfuscation of Virtualization-Obfuscated Software: A Semantics-Based Approach. In Proceedings of the 18th ACM conference on Computer and Communications Security (CCS ’11). ACM, 275–284.
[9] Brendan Dolan-Gavitt, Tim Leek, Josh Hodosh, and Wenke Lee. 2013. Tappan Zee (North) Bridge: Mining Memory Accesses for Introspection. In Proceedings of the 2013 ACM SIGSAC Conference on Computer and Communications Security (CCS ’13). ACM, 839–850.
[10] Brendan Dolan-Gavitt, Tim Leek, Michael Zhivich, Jonathon Giffin, and Wenke Lee. 2011. Virtuoso: Narrowing the Semantic Gap in Virtual Machine Introspection. In Proceedings of the IEEE Symposium on Security and Privacy 2011 (SP ’11). IEEE, 297–312.
[11] Blake Hartstein. [n. d.]. jsunpack-n. https://github.com/urule99/jsunpack-n. (accessed: 2019-02-15).
[12] Jingxuan He, Pesho Ivanov, Petar Tsankov, Veselin Raychev, and Martin Vechev. 2018. Debin: Predicting Debug Information in Stripped Binaries. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS ’18). ACM, 1667–1680.
[13] Timo Hirvonen. [n. d.]. Sulo. https://github.com/F-Secure/Sulo. (accessed: 2019-02-15).
[14] Timo Hirvonen. 2014. Dynamic Flash instrumentation for fun and profit. Blackhat USA briefings 2014, https://www.blackhat.com/docs/us-14/materials/us-14-Hirvonen-Dynamic-Flash-Instrumentation-For-Fun-And-Profit.pdf. (accessed: 2019-02-15).
[15] Ralf Hund. 2016. The beast within - Evading dynamic malware analysis using Microsoft COM. Blackhat USA briefings 2016.
[16] KahuSecurity. [n. d.]. Revelo Javascript Deobfuscator. http://www.kahusecurity.com/posts/revelo_javascript_deobfuscator.html. (accessed: 2019-02-15).

This revised version aims to improve clarity, coherence, and professionalism, making the text more accessible and easier to understand.