the formulation captures the bandwidth overhead. By using a
higher replication factor, we can achieve lower delay and higher
success probability at the expense of increased bandwidth over-
head. For example, ﬂooding achieves the smallest possible delay
at the cost of high bandwidth overhead. In some cases, delay
and success probability can be in competition, as in the choice
between a highly reliable, long-latency path and a lossy but
low-latency path.
In our formulation, we have chosen to ﬁx the replication fac-
tor and the maximum delay as constraints, and then try to max-
imize the probability of delivery. In the presence of unreliable
paths, maximizing the chance of a message arriving “on time” is
preferable to minimizing the transit time if the message arrives
at all. Attempting to minimize delay and maximize delivery
probability simultaneously presents a challenging multidimen-
sional optimization problem. In particular, even computing the
average delay for a given allocation is hard because it depends
on the precise combination of which paths were successful.
Finally, note that by using feasible paths we are able to cap-
ture delay constraints. To ﬁnd feasible paths, we assume that
the routing algorithm selects only those paths that have delay
less than the message lifetime. The routing algorithm presented
in prior work [8] satisﬁes this requirement. If there are paths
that may meet the delay constraint only with a certain proba-
bility p (p  P(k), hence, it is beneﬁcial to split as much as
possible.
Case 2: Medium p (.5  ko, P(k + 1) > P(k), hence it is
beneﬁcial to split only if k is suﬃciently large.
Case 3: Small p (0 ≤ p ≤ .5): The lines are decreasing and
asymptotically approach 0. In this case, ∀k, P(k + 1)  .5, then the odds
are in favor of success, and more trials reduces the variance and
increases the overall probability. This behavior for large k can
also be deduced from the weak law of large numbers. Similarly,
if p  4
3 ,
and case 3 occurs when pr ≤ 1.1 Details and proofs are covered
in the technical report [17].
3.2 Si are different
−1).
We now turn our attention to the case when paths may have
diﬀerent and dependent probabilities. Our solution approach
uses a Mixed Integer Program (MIP) to capture the objective
function P rob(Y > r
We start with a few deﬁnitions. Given n paths, there are 2n
possible outcomes corresponding to the diﬀerent combinations
of successful paths. Let the possible outcomes be numbered
0 . . . (2n − 1). The binary representation of an n-bit integer
j can be used to encode the success of the jth outcome in the
1The value 4
3 can be understood relatively easily when r = 2.
Here, P(k + 1) > P(k) iﬀ (k + 1)(1 − p) > kp. Substituting
k = 1, we get that P(k + 1) > P(k) iﬀ pr > 4
3 .
)
s
s
e
c
c
u
s
f
o
y
t
i
l
i
b
a
b
o
r
P
(
)
k
(
P
 1
 0.8
 0.6
 0.4
 0.2
 0
pr > 4/3
pr > 1
pr ≤ 1
Dip
p = 0.75
p = 0.60
p = 0.50
p = 0.35
 0
 5
 10
 15
 20
 25
k (Number of utilized paths)
Figure 2: P(k) as a function of the number of utilized paths
k. r = 2, and diﬀerent lines correspond to diﬀerent values
of p. The nature of P(k) varies greatly depending upon
whether pr > 1 or not. As k approaches inﬁnity, if pr > 1,
the line converges to 1. If pr = 1, the line converges to .5. If
pr  r
−1) can be rewritten as:
2n−1
Xj=0
P rob(cid:16)Y > r
th
j
outcome(cid:17) P rob(cid:16)j
th
outcome(cid:17)
This is because all the 2n outcomes are mutually exclusive and
exhaustive. Therefore, the objective function has a term yj wj
for the jth outcome.
−1(cid:12)(cid:12)(cid:12)
yj = {0, 1},
n
Xi=1
yjwj
Xj=0
cjixi ≥ yj/r
0 ≤ xi ≤ ui
Xi=1
xi = 1
n
n − 1) (2)
for j ∈ 0 . . . (2
for i ∈ 1 . . . n
(3)
(4)
The formulation has an exponential number of constraints
(2n) and uses integer variables. This is not surprising given
−1) is NP-hard [17]. If paths
that even computing P rob(Y > r
are dependent, the description of the input (wj ) itself may be
exponential in n. Despite these diﬃculties, we were able to use
this formulation to solve problems with n as large as 16 using
the CPLEX optimization suite [5] in typically less than 15 min-
utes.2 The MIP formulation also allows us to gain insight into
simple examples such as those discussed in the introduction.
For many cases with larger n, we used the MIP approach on
the best 16 paths to get a lower bound on the solution. We
can also ignore the integrality constraints on yj and convert
the MIP to a linear program which can be solved much more
eﬃciently. This will give us an upper bound on the solution.
4. PARTIAL PATH FAILURES
We now consider the case when Y can be approximated by a
Gaussian distribution. If n is moderately large, we can use the
central limit theorem to argue that Y can be approximated by a
Gaussian distribution. This approximation, however, requires
that the mean and the variance of each individual term xiSi
be very small relative to the mean and the variance of Y . For
our application, this means that as long as the path probabili-
ties have comparable mean and variance, this approximation is
reasonable. Also, if Si can be captured by a Gaussian distribu-
tion, then Y is also Gaussian because a linear combination of
Gaussian random variables is also Gaussian.
We approach this case in two steps. First, we show that
−1) is equivalent to maximizing the
maximizing P rob(Y > r
Sharpe-Ratio . We then apply results from the economic theory
literature to maximize this ratio.
4.1 New objective function
If Y is Gaussian with mean µY and variance σ2
Y , then:
P rob(Y > r
−1
) =
erf (z) ≡
1
√
2(cid:18)1 + erf(cid:16) µY − 1/r
2√
π Z
−t2
σY
dt
e
0
z
2 (cid:17)(cid:19), where,
erf (z) is the Gaussian error function and is strictly increas-
−1) is equivalent to
ing. Therefore, maximizing P rob(Y > r
maximizing the argument of the erf function,
. This
is a signiﬁcant simpliﬁcation (compared to the Bernoulli case)
because the objective function has a closed form expression in
terms of the mean and the variance of Y . Recall that Y =
µY −1/r
σY
n
i=1 xiSi and hence, we can express µY and σY as:

n
n
n
µY =
xipi,
2
Y =
σ
Xi=1
xixjσij
Xi=1
Xj=1
µY −1/r
The ratio
Here, pi is the success probability for the ith path (E[Si]) and
σij is the covariance between Si (ith path) and Sj (jth path).
is called the Sharpe-Ratio [1], and plays an
important role in the theory of allocating assets in investment
portfolios. It measures the expected added return per unit of
added risk. We now investigate how to maximize it.
σY
2The CPLEX solver was conﬁgured to accept any solution
which was within 1% of the optimal solution. CPLEX was run
on a 2-processor Xeon (3.2GHz) machine with 2 GB of RAM.
)
Y
µ
(
n
r
u
e
R
t
 0.8
 0.7
 0.6
 0.5
Efficient Frontier
Tangent to Frontier
 0
 0.1
 0.2
B
A
C
    Min variance
Max sharpe-ratio
      Max return
A
B
C
 0.3
Risk (σ
 0.4
Y)
 0.5
 0.6
Figure 3: Eﬃcient frontier generated from an experiment
with six paths with probabilities .85, .7, .65, .65, .6 and .6
and r = 2. For any point (x, y) on the frontier, the slope of
a line from the point (0, 1/r) to (x, y) measures the Sharpe-
Ratio at (x, y). This ratio is maximized by the line (drawn
from the point (0, 1/r)) that is tangent to the frontier.
4.2 Maximizing the Sharpe-Ratio ( µY −1/r
)
σY
Maximizing the Sharpe-Ratio is a natural exercise in asset
management, and we adopt the related economic terminology
by ﬁrst formulating a standard portfolio optimization problem: