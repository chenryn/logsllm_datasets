clustering goodness by requiring a minimum similarity between
elements belonging to the same cluster, which is typical in clus-
tering [30]. This guarantee is provided by enforcing a maximum
distance between elements that compose the same cluster. Keeping
this distance below a threshold is our criterion for cluster goodness.
3 RECURSIVE AGGLOMERATIVE
CLUSTERING
We introduce Recursive Agglomerative Clustering (RecAgglo) a
novel approach for categorical clustering. It combines the benefits
of two existing techniques [30]: agglomerative clustering, which is
able to generate small clusters (R1) and sampling, which reduces the
time complexity of clustering methods (R4). These two techniques
are selectively applied to recursively divide a large set of samples
into small clusters which eventually meet our goodness criterion.
The code for the RecAgglo algorithm is publicly available [33].
3.1 Agglomerative clustering and sampling
Agglomerative clustering is a bottom up hierarchical clustering
approach. Each element is initially placed into a singleton cluster.
Pairs of clusters having the smallest distance to each other are then
sequentially merged into larger clusters until all elements are in a
single cluster. The distance between two clusters is defined using
a linkage method. For instance, single linkage uses the minimum
distance between any two points in each cluster. The algorithm
produces a dendrogram which represents consecutive merges. Us-
ing the dendrogram, a desired clustering (set of clusters) can be
chosen using different criteria, e.g., ’distance’: the maximum dis-
tance dmax between elements in a cluster, ’maxclust’: the maximum
number of clusters cmax to generate. In contrast to many clustering
techniques [5, 22], agglomerative clustering does not generate a
predefined number of clusters. It generates clusters by grouping the
3
most similar singletons first and it can create many small clusters
which meet our goodness criterion as defined by the ’distance’ dmax .
Elements that cannot be assigned to any cluster while meeting this
criterion remain isolated in singletons. Agglomerative clustering
requires computing pairwise distances between elements (O(n2)
complexity) and does not scale to large datasets. The AggloClust
algorithm is presented in App. A.1. It takes a set c of elements to
cluster and a distance dmax as inputs.
The sampling algorithm is applied on top of existing clustering
techniques. It selects a random sample of reference elements from
a set of n elements. These reference elements are clustered and the
remaining ones (not sampled) are assigned to the initially formed
clusters. This reduces the number of distance computations between
elements from n×n to the sample size ×n. If the sample size is in the
order of O(loд(n)), the complexity of the base clustering algorithm
is reduced by the same factor.
We use sampling to reduce the complexity of agglomerative
clustering to O(n × loд(n)) by using a sample size in the order of
O(loд(n)). Our algorithm for agglomerative clustering with sam-
pling SampleClust is detailed in App. A.2. It takes 3 inputs: a set c
of elements to cluster, ρs a multiplying factor to(cid:112)|c| defining the
sample size and ρmc the maximum number of clusters to generate
(using the ’maxclust’ criterion for cluster generation). ρs and ρmc
are parameters to be defined according to the desired computa-
tion time. Sampling deprives agglomerative clustering of its ability
to generate many small clusters since the number of clusters is
bounded by the sample size. Also, clusters generated using sam-
pling do not meet any goodness criterion defined by the maximum
’distance’ dmax .
3.2 Our RecAgglo algorithm
We introduce a scalable approach to generating clusters that meet
our goodness criterion, i.e., the distance between elements in the
same cluster is lower than dmax . Our solution 1 recursively divides
large clusters into smaller ones using SampleClust. When clusters
are small enough, 2 it runs AggloClust to generate a clustering in
which each cluster meets the ’distance’ criterion dmax . All resulting
clusters are recursively aggregated to form the final clustering Cr es
composed of clusters that meet our goodness criterion.
Our approach RecAgglo is defined in Algorithm 1. It takes
as inputs an initial clustering C and a set of parameters: δa (for
RecAgglo), dmax (for AggloClust), ρs and ρmc (for SampleClust).
δa is a parameter defined according to computation time restric-
tions. RecAgglo loops over clusters c ∈ C to split them into smaller
clusters.
If the size of c is larger than a threshold δa, c is split using Sam-
pleClust. We use the ’maxclust’ criteria to generate clusters small
enough to be eventually processed using AggloClust. The result-
ing clustering Cs does not meet our goodness criterion yet and it is
re-processed using RecAgglo. We observed that SampleClust may
not be able to split an input set c given a specific ’maxclust’ factor
ρmc. We address this in two ways. Firstly, we re-try SampleClust
with a lower value ρmc = 1.01 providing the ability to generate
more clusters. Secondly, we fall back to plain agglomerative cluster-
ing given that the cluster size is still reasonably low (4 × δa). Both
these measures were determined empirically. Alternatively, ρmc
if |c| > δa then
▷ Recursive clustering
▷ Loop to split existing clusters
▷ Cluster sampling
Cr es ← ∅
remain ← ∅
maxs ← 4 × δa
for c : c ∈ C do
Cs ← SampleClust(c, ρs , ρmc)
if |Cs| > 1 then
else if ρmc > 1.01 then ▷ Recursive clustering alt.
▷ Set higher maxclust
else if |c|  1 then
else
# Clustering non-clustered elements (remain)
if |remain| > δa then
Csample ← SampleClust(remain, ρs , ρmc)
if |Csample| > 1 then
else
else if |c| > 1 then
else
end if
Cloop ← AggloClust(c, dmax )
remain ← remain ∪ c
end if
Cr es ← Cr es ∪ Cloop
end for
▷ Cluster sampling
▷ Recursive clustering
▷ Agglomerative clustering
▷ Elements are singletons
▷ Elements are singletons
▷ Agglomerative clustering
▷ Elements to re-cluster
▷ Add new clusters to result
could be progressively decreased or the multiplying factor of δa can
be changed. If both measures fail to split c in clusters, elements in c
are added to the set remain for further processing. Alternatively, we
4
obtain a resulting clustering Cloop from RecAgglo or AggloClust
that meets our goodness criterion.
If the size of c is lower than δa but larger than 1, c is split using
AggloClust with parameter dmax . If c is a singleton, it is added
to the remain set for later processing. During each iteration, we
add the new clustering Cloop to the final clustering Cr es or we
complement the remain set of elements for reprocessing.
The remain set contains clustered elements resulting from Sam-
pleClust and singletons - obtained due to lack of sufficiently simi-
lar elements in the drawn sample. Thus, we try to re-cluster these
remaining elements, following the same steps as previously. We use
SampleClust (if |remain| > δa), AggloClust (if δa ≥ |remain| >
1) or keep a singleton (if |remain| = 1). If SampleClust is suc-
cessful at splitting remain, we recursively run RecAgglo on the
resulting clustering. However, in contrast to the previous process,
we do not apply alternative measures if SampleClust fails and just
keep all elements as singletons.
The resulting clustering Cend is added to Cr es which is our final
clustering where all clusters meet our goodness criterion. It is worth
noting that many of these clusters may be singletons.
3.3 RecAgglo properties
Achieving cluster goodness: RecAgglo uses agglomerative clus-
tering to generate the final clustering Cr es. Consequently, any clus-
ter in Cr es of two or more elements meets our goodness criterion
defined by the maximum distance dmax . These clusters are smaller
than δa and meet R1 for a sensible choice of δa.
Computational complexity: The complexity of RecAgglo de-
pends on its recursive nature and SampleClust complexity. Ag-
gloClust runs on sets of size with the static upper bound δa. Its
running time is bounded by a constant. The maximum complexity of
SampleClust during the initial run is O(n×loд(n)) and it decreases
during subsequent recursions. In the worst-case scenario, we re-
quire at most n recursions to obtain the final clustering. This makes
the worst-case complexity of O((n × loд(n))n) for RecAgglo. This
theoretical complexity is completely untractable and RecAgglo can-
not scale to large datasets in theory. However, we show in Sect. 7.3
that its actual complexity is sub-quadratic when clustering sets
containing up to 100,000s orders. In this setting, RecAgglo is faster
than most categorical clustering algorithms.
Non-optimal solution: RecAgglo is non-deterministic and it
does not produce a globally optimal clustering. This is due to the
stochastic nature of the sampling process used SampleClust. We
show in Sect. 7.3 that clusters that we obtain during different runs
are consistent. Also, their goodness is close to the one of clusters
generated using plain agglomerative clustering, while improving
on the basic sampling method for clustering.
Hybrid clustering (using numerical features): Numerical fea-
tures can be input to a clustering algorithm for continuous data (e.g.,
K-means, DBScan, etc.) in order to generate clusters in a standalone
manner. The resulting clustering (cluster indexes) can be used as
an additional categorical attribute that is input to RecAgglo in a
cluster aggregation fashion [16].
5
d
i =1
4 ATTRIBUTE WEIGHTING STRATEGIES
Hamming distance and Jaccard Index are the most widely used
metrics for computing the distance between two elements u and
v represented using categorical attributes [30]. We use Hamming
distance in our clustering algorithms since it is fast to compute.
It counts the number of different attribute values between two
elements:
Hamminд(u, v) = 1
d
wi × (ui (cid:44) vi)
(1)
By default, Hamming like many other metrics gives the same
weight to every attribute (wi = 1). However, different attributes
might not contribute equally to quantifying the similarity between
u and v, or to produce “good” clusters. For instance, if attributes
having high cardinality are matching, this may indicate a higher
similarity than if attributes having low cardinality are matching.
We propose two novel strategies for weighting attributes which
capture these aspects and help addressing C1. The first strategy
is based on feature cardinality while the second uses labels from
known frauds and legitimate orders.
4.1 Cardinality driven attribute weight
We define a function to compute the weight wi for an attribute ai
based on its cardinality. The cardinality cardi is the total number of
values attribute ai can take. The rationale for weighting attributes
based on cardinality is the following: the probability of two ele-
ments u and v having equal value for an attribute i is inversely
proportional to the attribute cardinality cardi for uniformly dis-
tributed attribute values. The goal of this weighting strategy is to
give larger weights to attributes having high cardinality.
cardi
= ni
We use a sigmoid function (
Cardinality of the attributes in our dataset is not bounded since
values may be added as new orders are made, e.g., new customers
signing up. Thus, we use the inverse normalized richness index [24]
as the basis for weight computation: R−1
. ni is the number
i
of instances in a given set of size N for which ai is not null . R−1
is
a positive decreasing function of the attribute cardinality.
to [0, 1]. We
normalize its value over this range using the median value of R−1
i
computed over all 37 attributes: median(R−1). Finally, we scale our
weight to an intended range that controls the maximum difference
between attribute weights. We chose the range [1, 3] - we do not
discard any attributes and a given attribute can have at most 3
times higher weight than any other. We compute cardinality driven
weights as follows:
x|1+x | ) to scale R−1
i
i
(cid:32)
R−1
i
median(R−1
i
(cid:33)
, w#