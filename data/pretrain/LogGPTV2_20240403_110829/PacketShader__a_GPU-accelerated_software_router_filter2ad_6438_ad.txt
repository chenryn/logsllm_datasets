(gather), processes them with the GPU at a time, and splits the re-
sults into the output queues of workers from which the chunks came
(scatter).
This optimization technique is based on the observations from
Section 2: (i) having more GPU threads per GPU kernel launch
amortizes per-packet kernel launch overhead, (ii) the use of many
GPU threads can effectively hide memory access latency without
extra scheduling overhead. Additionally, pipelined copies yield bet-
ter PCIe data transfer rate. Our gather/scatter mechanism gives the
GPU more parallelism and improves the overall performance of the
system.
Concurrent Copy and Execution: In the shading step, the data
transfers and the GPU kernel execution of a chunk have dependency
on each other and are serialized. In the meantime, NVIDIA GPUs
support copying data between host and device memory while exe-
cuting a GPU kernel function. This optimization is called “concur-
rent copy and execution” and is popular for many GPGPU applica-
tions [40]. This technique can improve the throughput of the GPU by
a factor of two, if data transfer and GPU kernel execution completely
overlap.
CUDA supports a stream to allow a CPU thread have multiple
device contexts. With multiple streams PacketShader can overlap
data transfers and kernel launches for consecutive chunks, as shown
in Figure 10(c). However, we ﬁnd that using multiple streams sig-
niﬁcantly degrades the performance of lightweight kernels, such as
IPv4 table lookup. Since having multiple streams adds non-trivial
overhead for each CUDA library function call, we selectively use
this technique for the IPsec application in Section 6.
5.5 GPU Programming Considerations
All GPU kernels used in Section 6 are the straightforward porting
of CPU code (one exception is IPsec; we have made an effort to
maximize the usage of in-die memory for optimal performance).
In general, turning a typical C program into a correct GPU kernel
requires only little modiﬁcation. However, efﬁcient implementation
of GPU programs requires the understanding of characteristics and
trade-offs of GPU architecture. We brieﬂy address the general con-
siderations of GPU programming in the context of packet processing
acceleration.
What to ofﬂoad: The ofﬂoaded portion to GPU should have non-
trivial costs to compensate the overheads of copy from/to GPU mem-
ory and kernel launch. Computation and memory-intensive algo-
rithms with high regularity suit well for GPU acceleration.
How to parallelize: Most applications of software routers operate
on packet headers. In this case, the most intuitive way to parallelize
packet processing is to map each packet into an independent GPU
thread; CPU code for per-packet processing can be easily ported to
GPU. If there exists exploitable parallelism within a packet (e.g.,
parallel pattern matching or block cipher operation), we can map
each packet into multiple threads for optimal performance with ﬁne-
grained parallelism (see our IPsec implementation in Section 6.2.4).
Data structure usage: Simple data structures such as arrays and
hash tables are recommended in GPU. Data structures highly scat-
tered in memory (e.g., balanced trees) would make update the data
difﬁcult and degrade the performance due to small caches in GPU
and uncoalesced memory access pattern [38].
Divergency in GPU code: For optimal performance, the SIMT ar-
chitecture of CUDA demands to have minimal code-path divergence
caused by data-dependent conditional branches within a warp (a group
of 32 threads; see Section 2.1). We believe that inherently diver-
gent operations in packet processing are rare or at least avoidable
with choices of appropriate algorithms and data structures; all GPU
kernels used in this work have no or little code-path divergence.
To avoid warp divergence for differentiated packet processing (e.g.,
packet encryption with different cipher suites), one may classify and
sort packets to be grouped into separate warps in GPU so that all
threads within a warp follows the same code path.
We expect that these requirements of GPU programming will be
202much relaxed considering the current evolution trends of GPU ar-
chitectures. For example, Intel has recently introduced Larrabee,
the many-core x86 GPU architecture with full cache coherency [45].
Its MIMD architecture (multiple-instruction, multiple-data) will ease
the current divergency issue of PacketShader. AMD Fusion will
integrate CPU and GPU into a single package [1], with reduced GPU
communication overheads and a uniﬁed memory space shared by
CPU and GPU.
6. EVALUATION
In order to demonstrate the performance and ﬂexibility of Pack-
etShader, we implement four applications on top of PacketShader:
IPv4 and IPv6 forwarding, OpenFlow switch, and IPsec tunneling.
We focus on the data-path performance in our evaluation and assume
IP lookup tables, ﬂow tables, and cipher keys are static.
6.1 Test Methodology
We have implemented a packet generator that can produce up to
80 Gbps trafﬁc with 64B packets. It is based on our optimized packet
I/O engine, and generates packets with random destination IP ad-
dresses and UDP port numbers (so that IP forwarding and OpenFlow
look up a different entry for every packet). The generator is directly
connected to the PacketShader server via eight 10 GbE links, and it
works as both a packet source and a sink.
We implement each application in two modes, the CPU-only mode
and the CPU+GPU mode, to evaluate the effectiveness of GPU ac-
celeration. The experiments are performed on the server with eight
CPU cores and two GPUs, as described in Section 3.1. The CPU-
only mode runs eight worker threads rather than six workers and two
masters in the CPU+GPU mode since there is no shading step in the
CPU-only mode.
6.2 Applications
6.2.1 IPv4 Forwarding
The performance of forwarding table lookup is typically limited
by the number of memory access because the table is too big to ﬁt
in the CPU cache. The required number of memory access depends
on the lookup algorithm and the table size. In our implementation
we use DIR-24-8-BASIC in [22]. It requires one memory access per
packet for most cases, by storing next-hop entries for every possible
24-bit preﬁx. If a matching preﬁx is longer than 24 bits, it requires
one more memory access. To measure the performance under a real-
istic condition, we populate the forwarding table with the BGP table
snapshot collected on September 1, 2009 from RouteViews [14].
The number of unique preﬁxes in the snapshot is 282,797, and only
3% percent of the preﬁxes are longer than 24 bits.
GPU-accelerated IPv4 table lookup runs in the following order.
In the pre-shading step, a worker thread fetches a chunk of packets.
It collects packets that require slow-path processing (e.g., destined
to local, malformed, TTL expired, or marked as wrong IP check-
sum by NICs) and passes them onto Linux TCP/IP stack. For the
remaining packets it updates TTL and checksum ﬁelds, gathers des-
tination IP addresses into a new buffer, and passes the pointer to the
master thread. In the shading step, the master thread transfers the
IP addresses into the GPU memory and launches the GPU kernel
to perform the table lookup. The GPU kernel returns a pointer of
the buffer holding the next-hop information for each packet. The
master thread copies the result from device memory to host memory,
and then passes it to the worker thread. In the post-shading step,
the worker thread distributes packets into NIC ports based on the
forwarding decision.
6.2.2
IPv6 Forwarding
IPv6 forwarding requires more memory access than IPv4 for-
warding due to the 128-bit width of IPv6 addresses. For IPv6 table
lookup we adopt the algorithm in [55], which performs binary search
on the preﬁx length to ﬁnd the longest matching preﬁx. It requires
seven memory accesses, thus memory bandwidth and access latency
limit the IPv6 forwarding performance.
IPv6 is not popular in practice yet and the number of routing pre-
ﬁxes is much smaller than that of IPv4. Although forwarding table
lookup requires a constant number of memory access, a small lookup
table would give the CPU-only approach unfair advantage because
the small memory footprint would ﬁt in the CPU cache.
Instead,
we randomly generate 200,000 preﬁxes for our experiments. IPv6
forwarding works similarly to IPv4, except that a wide IPv6 address
causes four times more data to be copied into the GPU memory.
6.2.3 OpenFlow Switch
OpenFlow is a framework that runs experimental protocols over
existing networks [13,33]. Packets are processed on a ﬂow basis and
do not interfere with other packets of existing protocols. OpenFlow
consists of two components, the OpenFlow controller and the Open-
Flow switch, running on separate machines in general. The Open-
Flow controller, connected via secure channels to switches, updates
the ﬂow tables and takes the responsibility of handling unmatched
packets from the switches. The OpenFlow switch is responsible for
packet forwarding driven by ﬂow tables.
We focus on the OpenFlow switch, based on the OpenFlow 0.8.9r2
speciﬁcation [10]. The OpenFlow switch maintains the exact-match
and the wildcard-match tables. Exact-match entries specify all ten
ﬁelds in a tuple, which is used as the ﬂow key. In contrast, wildcard
match entries specify only some ﬁelds (bitmask is also available for
IP addresses). An exact-match entry always takes precedence over a
wildcard entry. All wildcard entries are assigned a priority.
When a packet arrives, our OpenFlow switch extracts the ten-ﬁeld
ﬂow key from the packet header. For an exact table lookup, the
switch matches the ﬂow key against the exact-match entries in the
hash table. For a wildcard-table lookup, our switch performs linear
search through the table, as the reference implementation does [9],
while hardware implementation typically uses TCAM. Our CPU-
only OpenFlow switch implements all operations (ﬂow key extrac-
tion, hash value calculation and lookup for exact entries, linear search
for wildcard matching, and follow-up actions) in CPU. In the GPU-
accelerated implementation, we ofﬂoad hash value calculation and
the wildcard matching to GPU, while leaving others in CPU for load
distribution.
6.2.4
IPsec Gateway
IPsec is widely used to secure VPN tunnels or for secure com-
munication between two end hosts. Since cryptographic operations
used in IPsec are highly compute-intensive, IPsec routers often use
hardware accelerator modules. The computational requirement of
IPsec makes GPU attractive as it is well-suited for cryptographic
operation [24].
For IPsec evaluation, we choose AES-128-CTR for block cipher
and SHA1 for Hash-based Message Authentication Code (HMAC).
For the CPU+GPU mode, we ofﬂoad AES and SHA1 to GPU, while
leaving other IPsec operations in CPU. The CPU-only approach uses
highly optimized AES and SHA1 implementations using SSE in-
structions for fair comparison. Our implementation runs Encapsula-
tion Security Payload (ESP) IPsec tunneling mode. While this mode
increases the packet size with the extra IP header, the ESP header,
203(a) IPv4 forwarding
(b) IPv6 forwarding
(c) OpenFlow switch (with 64B packets)
(d) IPsec gateway
Figure 11: Performance measurement of PacketShader on CPU-only and CPU+GPU modes
and the padding, we take input throughput as a metric rather than
output throughput.
Our GPU implementation exploits two different levels of paral-
lelism. For AES we maximize parallelism for high performance at
the ﬁnest level; we chop packets into AES blocks (16B) and map
each block to one GPU thread. However, SHA1 cannot be par-
allelized at the SHA1 block level (64B) due to data dependency
between blocks; we parallelize SHA1 at the packet level.
6.3 Throughput
Figure 11 depicts the performance of PacketShader for four ap-
plications. We measure the throughput over different packet sizes,
except for the OpenFlow switch.
IPv4 and IPv6 Forwarding: Figures 11(a) and 11(b) show IP packet
forwarding performance. For all packet sizes, the CPU+GPU mode
reaches close to the maximum throughput of 40 Gbps, bounded by
packet I/O performance in Figure 6. PacketShader runs at 39 Gbps
for IPv4 and 38 Gbps for IPv6 with 64B packets, which are slightly
lower than 41 Gbps of minimal forwarding performance. This is
because IOH gets more overloaded due to copying IP addresses and
lookup results between host memory and GPU memory. We expect
to see better throughput, once the hardware problem (in Section 3.2)
is ﬁxed.
We ﬁnd that GPU-acceleration signiﬁcantly boosts the performance
of memory-intensive workloads. The improvement is especially no-
ticeable with IPv6 since it requires more memory access (seven for
each packet) than IPv4 (one or two). A large number of threads
running on GPU effectively hide memory access latency, and the
large GPU memory bandwidth works around the limited host mem-
ory bandwidth.
OpenFlow Switch: For OpenFlow switch, we measure the perfor-
mance variation with different table sizes (Figure 11(c)). CPU+GPU
mode outperforms CPU-only mode for all conﬁgurations. The per-
formance improvement comes from ofﬂoading the hash value com-
putation for small table sizes. Wildcard-match ofﬂoad becomes dom-
inant as the table size grows.
We compare our PacketShader with the OpenFlow implementa-
tion on a NetFPGA card [36]. The NetFPGA implementation is
capable of 32K + 32 table entries at maximum, showing 4 Gbps
line-rate performance. For the same conﬁguration, PacketShader
runs at 32 Gbps, which is comparable with the throughput of eight
NetFPGA cards.
IPsec Gateway: Figure 11(d) shows the results of the IPsec ex-
periment. The GPU acceleration improves the performance of the
CPU-only mode by a factor of 3.5, regardless of packet sizes. The
CPU+GPU throughput for 64B packets is around 10.2 Gbps, and
20.0 Gbps for 1514B packets. PacketShader outperforms Route-
Bricks by a factor of 5 for 64B packets. RouteBricks achieves IPsec
performance 1.9 Gbps for 64B trafﬁc and 6.1 Gbps for larger pack-
ets [19]. We note that the IPsec performance of PacketShader is
comparable or even better than that of commercial hardware-based
IPsec VPN appliances [3].
We suspect that our current performance bottleneck lies in the
dual-IOH problem (Section 3.2) again for the CPU+GPU case, as
CPUs have not been 100% utilized. In IPsec encryption, entire packet
payloads and other metadata (such as keys and IVs) are transmitted
from/to GPU, weighing on the burden of IOHs. Experiments done
without packet I/O, thus with less trafﬁc through IOHs, show that the
performance of two GPUs scales up to 33 Gbps, which implies GPU
is not a bottleneck as well.
OpenFlow and IPsec represent compute-intensive workloads of
software routers in our work. We have conﬁrmed that compute-
intensive applications can beneﬁt from GPU as well as memory-
intensive applications.
6.4 Latency
Some techniques used in our work, namely batched packet I/O in
Section 4.3 and parallel packet processing with GPU in Section 5,
may affect the latency of PacketShader. To quantify it, we measure
the average round-trip latency for IPv6 forwarding. The measure-
ment is done at the generator with timestamped 64B packets, over a
range of input trafﬁc levels.