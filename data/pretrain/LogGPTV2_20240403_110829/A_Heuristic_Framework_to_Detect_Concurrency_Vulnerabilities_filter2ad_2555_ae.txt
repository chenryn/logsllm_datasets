### 7.5 Abnormal Time Cost of Static Analysis

From Table 3, it is evident that the static analysis of `bzip2smp` incurs an exceptionally high time cost of 1500 seconds, while the time cost for other programs in the benchmark suite is 3.5 seconds or less. This significant discrepancy prompted us to investigate the root cause of this outlier.

Upon examining the code of `bzip2smp`, we identified a macro, `BZ_ITAH`, which was invoked 50 times. Listing 3 shows the relevant code snippet. The repeated calls to this macro result in the generation of at least 50 branches in both the data flow and control flow graphs, leading to a prolonged execution time for the static analysis. By replacing these 50 macro calls with a for-loop, `for (i=0; i<=49; i++)`, the semantics and functionality of the code remain unchanged, but the complexity of the data flow and control flow graphs is significantly reduced. Consequently, the time cost decreased from 1500 seconds to 13 seconds.

**Listing 3: Macro used in bzip2smp**
```c
# define BZ_ITAH ( nn ) \
mtfv_i = mtfv [ gs +( nn ) ] ; \
bsW( s , s_len_sel_selCtr[ mtfv_i ] , s_code_sel_selCtr[ mtfv_i ] )

BZ_ITAH ( 0 ) ;
BZ_ITAH ( 3 ) ;
...
BZ_ITAH ( 45 ) ; BZ_ITAH ( 46 ) ; BZ_ITAH ( 47 ) ;
BZ_ITAH ( 48 ) ; BZ_ITAH ( 49 ) ;
BZ_ITAH ( 1 ) ;
BZ_ITAH ( 4 ) ;
BZ_ITAH ( 2 ) ;
```

### 8. Limitations and Future Work

#### 8.1 Scalability of Static Analysis

The static analysis tool, LOCKSMITH [23], which we used to implement our static analysis, is precise but complex. This complexity limits its applicability to programs with more than tens of thousands of lines of code [28]. As a result, we were constrained to using small utility programs in our evaluation experiments. To address this, it would be beneficial to adopt a more scalable open-source static analysis tool, enabling the testing of larger and more commonly used concurrent programs.

Additionally, improving the semantic checking can help reduce false positives, thereby avoiding wasted time on testing non-issues in our fuzz testing.

#### 8.2 Capacity of AFL in Exploring Paths

We utilized AFL to implement our interleaving exploring fuzzer and vulnerability detection fuzzer. It is well-known that AFL explores sophisticated programs in a shallow manner, a problem that has been addressed in recent research [6, 14]. A more advanced fuzzer capable of deep path exploration or one that can guide testing towards executing sensitive operations reported by the static analysis, such as the one presented in [2], would be desirable.

#### 8.3 Restrictions of Manual Validation

The manual validation process described in Section 7.3 for confirming detected concurrency vulnerabilities is labor-intensive. Based on a crash report from our vulnerability detection fuzzer, we need to manually insert scheduling code into the source to ensure the same interleaving that caused the crash during fuzz testing is used in validation. We then examine the program code to determine the root cause of the reported concurrency vulnerability and decide on the condition to confirm the vulnerability. Next, we insert assertive code around the sensitive operations to check if the condition is met during validation, and run the program repeatedly with the crash input to hit the assertive code.

Since the crash report alone is insufficient to replay the crash, multiple runs are necessary. According to [22], eight factors must be recorded to deterministically replay a concurrency error, which is far more information than what AFL records.

Among these limitations, the insertion of scheduling code during validation can be automated, similar to ConMem-v in [36]. However, this is of lower priority currently due to the small number of cases requiring validation. Although time-consuming, running the program repeatedly in the validation phase has a high chance of reproducing the crash.

The most challenging task in our manual validation is comprehending the code to identify the root cause of the reported concurrency vulnerability. This allows us to determine a condition to place in the inserted assertive code, ensuring that triggering the condition confirms the reported vulnerability. This condition is different from the condition for a pair of sensitive operations found by the static analysis and used by the fuzz testing to trigger a suspected concurrency vulnerability. The latter is coarse and at the thread level, while the former is fine-grained and requires a thorough understanding of the code. Due to its complexity, there is a risk that the derived assertive condition may be incomplete, leading to a failure to confirm a true positive. Therefore, a false positive determined by our manual validation is probabilistic rather than deterministic, whereas a true positive is always deterministic.

#### 8.4 Additional Limitations

In addition to the above limitations, there are some additional constraints in our implementation of the heuristic framework. The framework currently works only with concurrent programs written in C using POSIX multi-thread functions and requires the source code to detect concurrency errors and vulnerabilities. Extending the framework to cover programs written in other languages and using other multi-thread functions, as well as binary programs without source code, would be beneficial. While the ideas presented in this paper are applicable to these extensions, their implementation would require substantial effort.

### 9. Conclusion

In this paper, we proposed a heuristic framework to detect concurrency errors and vulnerabilities in concurrent programs. The framework includes two separate fuzzers: the interleaving exploring fuzzer, which effectively explores interleavings to test as many as possible, and the vulnerability detection fuzzer, which applies static analysis to locate sensitive concurrent operations, categorize each finding as a potential concurrency vulnerability, and determine the execution order of the sensitive operations that would trigger the vulnerability. Fuzz testing is then directed to explore the specific execution order to trigger the potential vulnerability.

We evaluated the proposed framework using three types of common concurrency vulnerabilities—concurrency buffer overflow, double-free, and use-after-free—on a benchmark suite of six real-world programs. Our experimental results show that the interleaving exploring fuzzer reported three new crashes not detected by the existing fuzzer, AFL, and typically produced the first crash within 10 minutes, while the original AFL might not report any crash after several days. This indicates that our interleaving exploring fuzzer can effectively explore interleavings in detecting concurrency errors, unlike the original AFL. Additionally, the vulnerability detection fuzzer detected two concurrency vulnerabilities, both confirmed as true positives, demonstrating its power and effectiveness in detecting targeted concurrency vulnerabilities.