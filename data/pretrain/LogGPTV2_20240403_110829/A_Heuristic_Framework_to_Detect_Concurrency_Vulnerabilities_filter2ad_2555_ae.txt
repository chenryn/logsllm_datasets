such a pair that had failed to trigger the concurrency vulnerability.
Both would lead to false negatives.
7.5 Abnormal Time Cost of Static Analysis
From Table 3, we can see an extreme time cost, 1500 seconds, of the
static analysis on bzip2smp, while the time cost for other programs
539
in the benchmark suite are all 3.5 seconds or less. This observation
led us to investigate the root cause of the outlier.
By examining the code of bzip2smp, we found a macro that was
repeatedly called many times in bzip2smp. Listing 3 shows the piece
of code. It contains a macro BZ_ITAH, which is called literally 50
times. This would cause the static analysis to generate at least 50
branches in both the data-ow and the control-ow graph, resulting
in a long execution time for the static analysis. When we replaced
the 50 calls of the macro with a for-loop, for (i=0; i<=49; i++), the
semantics and functionality of the piece of code remain intact, but
the complexity of the data-ow and the control-ow graph in the
static analysis is signicantly reduced: the time cost reduced to 13
seconds from the original 1500 seconds.
Listing 3: Macro used in bzip2smp
# define BZ_ITAH ( nn )
mtfv_i = mtfv [ gs +( nn ) ] ;
bsW( s , s _ l e n _ s e l _ s e l C t r [ mtfv_i ] ,
s _ c o d e _ s e l _ s e l C t r [ mtfv_i ] )
BZ_ITAH ( 0 ) ;
BZ_ITAH ( 3 ) ;
. . .
BZ_ITAH ( 4 5 ) ; BZ_ITAH ( 4 6 ) ; BZ_ITAH ( 4 7 ) ;
BZ_ITAH ( 4 8 ) ; BZ_ITAH ( 4 9 ) ;
BZ_ITAH ( 1 ) ;
BZ_ITAH ( 4 ) ;
BZ_ITAH ( 2 ) ;
8 LIMITATIONS AND FUTURE WORK
As reported in Section 7.2, our interleaving exploring fuzzer found
three new crashes that the original AFL did not nd, and typically
produced the rst crash within 10 minutes of running while the
original AFL might not report any crash after running for several
days. This indicates that the original AFL is ineective in exploring
thread interleavings in testing a concurrent program, and the same
fuzzer, when combined with our interleaving exploring priority, can
explore thread interleavings very eectively. This is because our
interleaving exploring priority aims at exploring as many thread
interleavings as possible. Our interleaving exploring priority em-
powers a fuzzer to eectively detect concurrency errors, a great
enhancement to existing fuzzers.
In addition, our vulnerability detection fuzzer could detect two
concurrency vulnerabilities, and both vulnerabilities were con-
rmed to be true positives, as reported in Section 7.2. This demon-
strates the power and eectiveness of our vulnerability detection
fuzzer in detecting targeted concurrency vulnerabilities.
Nevertheless, there are several limitations for the current im-
plementation of the heuristic framework, mainly due to the tools
we based on to implement the framework. These limitations are
discussed in the following subsections. We are actively working on
improving the heuristic framework to address some of these issues.
8.1 Scalability of Static Analysis
LOCKSMITH [23], the static analysis tool we based on to implement
our static analysis, is precise but complex, which prevents it from
working on programs exceeding tens of thousands of lines of code
[28]. This was the main reason to choose small utility programs
instead of more interesting ones in our evaluation experiments. It
is desirable to choose a more scalable open-source static analysis
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Changming Liu, Deqing Zou, Peng Luo, Bin B. Zhu, and Hai Jin
tool to implement our static analysis so that larger and commonly
used concurrent programs can also be tested with the heuristic
framework.
In addition, the semantic checking can be improved to reduce
false positives to avoid wasting time on testing false positive in our
fuzz testing.
8.2 Capacity of AFL in Exploring Paths
We have adopted AFL to implement our interleaving exploring
fuzzer and our vulnerability detection fuzzer. It is well-known that
AFL explores sophisticated programs in a shallow manner, and this
problem has been addressed recently in [6, 14]. It is desirable to use
a more sophisticated fuzzer that can explore code paths deeply or
guide testing towards executing the sensitive operations reported
by the static analysis such as that presented in [2].
8.3 Restrictions of Manual Validation
The manual validation described in Section 7.3 to validate detected
concurrency vulnerabilities is a labor-intensive work. Based on
a crash report by our vulnerability detection fuzzer, we need to
manually insert scheduling code into the source to ensure that the
same interleaving that caused the crash in the fuzz testing would
be used in validation, then examine the program code to determine
the root cause of a reported concurrency vulnerability in order to
decide the condition to conrm the concurrency vulnerability. Next
we need to insert assertive code around the sensitive operations to
determine if the condition is really hit in validation, and then run
the program fed with the crash input repeatedly in order to hit the
assertive code.
We need to run the tested program repeatedly in the validation
process since the crash report is insucient to replay the crash.
According to [22], it requires to record the information of eight
factors to deterministically replay a concurrency error, which is far
more than the information recorded by AFL.
Among all these limitations, the insertion of scheduling code into
the source during validation can be automated in a way similar to
ConMem-v in [36]. Writing such automatic tool is of lower priority
since the number of cases to be validated is small by now. Although
taking some time, running a program to be tested repeatedly in the
validation phase has a high chance to repeat the crash.
The most challenging task in our manual validation is actually
the comprehension of the code in order to identify the root cause
of a reported concurrency vulnerability so that we can determine a
condition to place into inserted assertive code such that triggering
the assertive condition conrms the reported concurrency vulnera-
bility. This assertive condition diers from the condition for a pair
of sensitive operations that the static analysis nds and the fuzz
testing uses to trigger a suspected concurrency vulnerability. The
latter is the execution order of the two sensitive operations in a
pair that would trigger the its potential concurrency vulnerability.
It is coarse, at the thread level. The former, on the other hand, is
ne-grained and requires understanding the root cause of the con-
currency vulnerability. It needs to guarantee conrmation of the
suspected concurrency vulnerability once triggered. Obtaining this
assertive condition in our validation typically requires thorough
understanding of the relevant code written by others, which is
labor-intensive and time-consuming, especially for concurrency
buer overows. Due to its complexity, there is a chance that the
derived assertive condition is incomplete, which may lead to failure
to conrm a true positive. As a result, a false positive determined by
our manual validation is probabilistic instead of deterministic. On
the other hand, a true positive determined by our manual validation
is always deterministic.
8.4 Additional Limitations
In addition to the above limitations, there are some additional lim-
itations in our implementation of the heuristic framework. The
heuristic framework currently works only with concurrent pro-
grams written in C using POSIX multi-thread functions requires
the source code to detect concurrency errors and vulnerabilities in
a concurrent program. It is desirable to extend the heuristic frame-
work to cover programs written in other languages and using other
multi-thread functions, and to cover binary programs without using
source code. The ideas presented in this paper work for the these
extensions, but it requires a great eort to realize them.
9 CONCLUSION
In this paper, we proposed a heuristic framework to detect concur-
rency errors and vulnerabilities in concurrent programs. It includes
two separate fuzzers. One fuzzer, the interleaving exploring fuzzer,
explores interleavings eectively to test as many interleavings as
possible. It can detect concurrency errors eectively and eciently.
The other fuzzer, the vulnerability detection fuzzer, rst applies
static analysis to locate sensitive concurrent operations, categorize
each nding to a potential concurrency vulnerability, and determine
the execution order of the sensitive operations in each nding that
would trigger the potential concurrency vulnerability; and then
directs fuzz testing to explore the specic execution order of each
nding in order to trigger the potential concurrency vulnerability.
We used three types of common concurrency vulnerabilities,
i.e., concurrency buer overow, double-free, and use-after-free to
evaluate the proposed heuristic framework with a benchmark suite
of six real-world programs. In our experimental evaluation, the
interleaving exploring fuzzer reported three new crashes that were
not reported by the existing fuzzer, AFL, that our fuzzer was based
on. The interleaving exploring fuzzer typically produced the rst
crash within 10 minutes of running while the original AFL might not
report any crash after running for several days. These experimental
results indicate that our interleaving exploring fuzzer can eectively
explore interleavings in detecting concurrency errors while the
original AFL cannot. Additionally, the vulnerability detection fuzzer
detected two concurrency vulnerabilities, and both vulnerabilities
were conrmed to be true positives. This demonstrates the power
and eectiveness of the vulnerability detection fuzzer in detecting
targeted concurrency vulnerabilities.
REFERENCES
[1] D. A. Bader, V. Kanade, and K. Madduri. 2007. SWARM: A Parallel Programming
Framework for Multicore Processors. 1–8 pages. https://doi.org/10.1109/IPDPS.
2007.370681
[2] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoud-
hury. 2017. Directed Greybox Fuzzing. In Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’17). ACM, New York,
NY, USA, 2329–2344. https://doi.org/10.1145/3133956.3134020
540
A Heuristic Framework to Detect Concurrency Vulnerabilities
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
[3] Sebastian Burckhardt, Pravesh Kothari, Madanlal Musuvathi, and Santosh Na-
garakatte. 2010. A Randomized Scheduler with Probabilistic Guarantees of
Finding Bugs. In Proceedings of the Fifteenth Edition of ASPLOS on Architectural
Support for Programming Languages and Operating Systems (ASPLOS XV). ACM,
New York, NY, USA, 167–178. https://doi.org/10.1145/1736020.1736040
[4] Jacob Burnim, Koushik Sen, and Christos Stergiou. 2011. Testing Concurrent
Programs on Relaxed Memory Models. In Proceedings of the 2011 International
Symposium on Software Testing and Analysis (ISSTA ’11). ACM, New York, NY,
USA, 122–132. https://doi.org/10.1145/2001420.2001436
[5] Cristian Cadar and Koushik Sen. 2013. Symbolic Execution for Software Testing:
Three Decades Later. Commun. ACM 56, 2 (Feb. 2013), 82–90. https://doi.org/10.
1145/2408776.2408795
[6] Peng Chen and Hao Chen. 2018. Angora: Ecient Fuzzing by Principled Search.
CoRR abs/1803.01307 (2018). arXiv:1803.01307 http://arxiv.org/abs/1803.01307
[7] Ankit Choudhary, Shan Lu, and Michael Pradel. 2017. Ecient Detection of
Thread Safety Violations via Coverage-guided Generation of Concurrent Tests. In
Proceedings of the 39th International Conference on Software Engineering (ICSE ’17).
IEEE Press, Piscataway, NJ, USA, 266–277. https://doi.org/10.1109/ICSE.2017.32
[8] Tayfun Elmas, Jacob Burnim, George Necula, and Koushik Sen. 2013. CONCUR-
RIT: a domain specic language for reproducing concurrency bugs. Acm Sigplan
Notices 48, 6 (2013), 153–164.
[9] Azadeh Farzan, P. Madhusudan, Niloofar Razavi, and Francesco Sorrentino. 2012.
Predicting null-pointer dereferences in concurrent programs. In Proceedings of
ACM Sigsoft International Symposium on the Foundations of Software Engineering.
1–11.
[10] Patrice Godefroid, Michael Y. Levin, and David Molnar. 2008. Automated White-
box Fuzz Testing. In Proceedings of the 16th Network and Distributed System
Security Symposium, Vol. 8. 151–166.
[11] Pallavi Joshi, Mayur Naik, Chang Seo Park, and Koushik Sen. 2009. CalFuzzer: An
Extensible Active Testing Framework for Concurrent Programs. In Proceedings
of Computer Aided Verication. Berlin, Heidelberg, 675–681.
[12] Marek Kroemeke. 2014. Apache 2.4.7 mod_status - Scoreboard Handling Race
Condition. https://www.exploit-db.com/exploits/34133/.
[13] Lcamtuf. 2018. American Fuzzy Lop. http://lcamtuf.coredump.cx/a/.
[14] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu,
and Alwen Tiu. 2017. Steelix: Program-state Based Binary Fuzzing. In Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE
2017). ACM, New York, NY, USA, 627–637. https://doi.org/10.1145/3106237.
3106295
[15] Barton P. Miller, Louis Fredriksen, and Bryan So. 1990. An Empirical Study of
the Reliability of UNIX Utilities. Commun. ACM 32, 12 (1990), 32–44.
[16] NIST. 2018. National Vulnerability Database. https://nvd.nist.gov/.
[17] NVD. 2018.
CVE-2010-5298 Detail.
https://nvd.nist.gov/vuln/detail/
CVE-2010-5298.
[18] OpenBSD. 2014. OpenBSD 5.4 errata 8. https://ftp.openbsd.org/pub/OpenBSD/
patches/5.4/common/008_openssl.patch.
[19] Soyeon Park, Shan Lu, and Yuanyuan Zhou. 2009. CTrigger: Exposing Atomicity
Violation Bugs from Their Hiding Places. In Proceedings of the 14th International
Conference on Architectural Support for Programming Languages and Operating
Systems (ASPLOS XIV). ACM, New York, NY, USA, 25–36. https://doi.org/10.
1145/1508244.1508249
[20] Sangmin Park, Richard Vuduc, and Mary Jean Harrold. 2015. UNICORN: a
unied approach for localizing non-deadlock concurrency bugs. Software Testing,
Verication and Reliability 25, 3 (2015), 167–190. https://doi.org/10.1002/stvr.1523
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/stvr.1523
[21] Sangmin Park, Richard W. Vuduc, and Mary Jean Harrold. 2010. Falcon: Fault
Localization in Concurrent Programs. In Proceedings of the 32Nd ACM/IEEE
International Conference on Software Engineering - Volume 1 (ICSE ’10). ACM, New
York, NY, USA, 245–254. https://doi.org/10.1145/1806799.1806838
[22] Harish Patil, Cristiano Pereira, Mack Stallcup, Gregory Lueck, and James Cownie.
2010. PinPlay: A Framework for Deterministic Replay and Reproducible Analysis
of Parallel Programs. In Proceedings of the 8th Annual IEEE/ACM International
Symposium on Code Generation and Optimization (CGO ’10). ACM, New York,
NY, USA, 2–11. https://doi.org/10.1145/1772954.1772958
[23] Polyvios Pratikakis, Jerey S. Foster, and Michael Hicks. 2006. LOCKSMITH:
Context-sensitive correlation analysis for race detection. Acm Sigplan Notices 41,
6 (2006), 320–331.
[24] Nishant Sinha and Chao Wang. 2010. Staged Concurrent Program Analysis. In
Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foun-
dations of Software Engineering (FSE ’10). ACM, New York, NY, USA, 47–56.
https://doi.org/10.1145/1882291.1882301
[25] Sherri Sparks, Shawn Embleton, Ryan K Cunningham, and Cli Changchun Zou.
2007. Automated Vulnerability Analysis: Leveraging Control Flow for Evolu-
tionary Input Crafting. In Twenty-Third Annual Computer Security Applications
Conference (ACSAC 2007). 477–486.
[26] Sebastian Steenbuck and Gordon Fraser. 2013. Generating unit tests for concur-
rent classes. In IEEE Sixth International Conference on Software Testing, Verication
and Validation (ICST). IEEE, 144–153.
[27] Valerio Terragni and Shing-Chi Cheung. 2016. Coverage-driven Test Code Gen-
eration for Concurrent Classes. In Proceedings of the 38th International Confer-
ence on Software Engineering (ICSE ’16). ACM, New York, NY, USA, 1121–1132.
https://doi.org/10.1145/2884781.2884876
[28] Jan Wen Voung, Ranjit Jhala, and Sorin Lerner. 2007. RELAY: Static Race Detection
on Millions of Lines of Code. In Proceedings of the the 6th Joint Meeting of the
European Software Engineering Conference and the ACM SIGSOFT Symposium on
The Foundations of Software Engineering (ESEC-FSE ’07). ACM, New York, NY,
USA, 205–214. https://doi.org/10.1145/1287624.1287654
[29] Pengfei Wang, Jens Krinke, Kai Lu, Gen Li, and Steve Dodier-Lazaro. 2017. How
Double-fetch Situations Turn into Double-fetch Vulnerabilities: A Study of Double
Fetches in the Linux Kernel. In Proceedings of the 26th USENIX Conference on
Security Symposium (SEC’17). USENIX Association, Berkeley, CA, USA, 1–16.
http://dl.acm.org/citation.cfm?id=3241189.3241191
[30] Tielei Wang, Tao Wei, Guofei Gu, and Wei Zou. 2010. TaintScope: A Checksum-
Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection. In
Proceedings of the 2010 IEEE Symposium on Security and Privacy (SP ’10). IEEE
Computer Society, Washington, DC, USA, 497–512. https://doi.org/10.1109/SP.
2010.37
[31] Wikipedia. 2018. Dirty COW. https://en.wikipedia.org/wiki/Dirty_COW.
[32] Maverick Woo, Sang Kil Cha, Samantha Gottlieb, and David Brumley. 2013.
Scheduling Black-box Mutational Fuzzing. In Proceedings of the 2013 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’13). ACM, New York,
NY, USA, 511–522. https://doi.org/10.1145/2508859.2516736
[33] Junfeng Yang, Ang Cui, Sal Stolfo, and Simha Sethumadhavan. 2012. Concurrency
Attacks. In Proceedings of the 4th USENIX Conference on Hot Topics in Parallelism
(HotPar’12). USENIX Association, Berkeley, CA, USA, 15–15. http://dl.acm.org/
citation.cfm?id=2342788.2342803
[34] Jie Yu, Satish Narayanasamy, Cristiano Pereira, and Gilles Pokam. 2012. Maple:
A Coverage-driven Testing Tool for Multithreaded Programs. In Proceedings
of the ACM International Conference on Object Oriented Programming Systems
Languages and Applications (OOPSLA ’12). ACM, New York, NY, USA, 485–502.
https://doi.org/10.1145/2384616.2384651
[35] Wei Zhang, Junghee Lim, Ramya Olichandran, Joel Scherpelz, Guoliang Jin, Shan
Lu, and Thomas Reps. 2011. ConSeq: Detecting Concurrency Bugs Through
Sequential Errors. In Proceedings of the Sixteenth International Conference on
Architectural Support for Programming Languages and Operating Systems (ASPLOS
XVI). ACM, New York, NY, USA, 251–264.
https://doi.org/10.1145/1950365.
1950395
[36] Wei Zhang, Chong Sun, and Shan Lu. 2010. ConMem: Detecting Severe Con-
currency Bugs Through an Eect-oriented Approach. In Proceedings of the
Fifteenth Edition of ASPLOS on Architectural Support for Programming Lan-
guages and Operating Systems (ASPLOS XV). ACM, New York, NY, USA, 179–192.
https://doi.org/10.1145/1736020.1736041
[37] Shixiong Zhao, Rui Gu, Haoran Qiu, Tsz On Li, Yuexuan Wang, Heming Cui, and
Junfeng Yang. 2018. OWL: Understanding and Detecting Concurrency Attacks.
In Proceedings of IEEE/IFIP International Conference on Dependable Systems and
Networks. 219–230.
541