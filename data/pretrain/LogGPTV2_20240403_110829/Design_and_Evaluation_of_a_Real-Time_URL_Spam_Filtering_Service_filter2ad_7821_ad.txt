872
265
352
98,900
Non-spam
22,083
6,782
4,884
3,597
2,534
2,097
1,107
409
116
169
43,778
Spam
28,205
8,590
5,775
5,422
2,841
2,143
1,351
463
149
183
55,122
TABLE 4: Breakdown of features used for classiﬁcation before and
after regularization.
Feature Type
Source URLs
HTTP Headers
HTML Content
Initial URL
Final URL
IP (Geo/ASN)
Page Links
Redirects
DNS
Frame URLs
FN
FP
Accuracy
1.17% 19.38%
89.74%
1.23% 28.07%
85.37%
1.36% 28.04%
85.32%
1.14% 30.88%
84.01%
2.34% 30.53%
83.59%
81.52%
2.33% 34.66%
75.72% 15.46% 37.68%
71.93%
0.85% 55.37%
72.40% 25.77% 29.44%
60.17%
0.33% 79.45%
TABLE 5: Accuracy of classiﬁer when trained on a single type
of feature. Sources, headers, and HTML content provide the best
individual performance, while frame URLs and DNS data perform
the worst.
number of pages.
To understand the most inﬂuential features in our system,
we train a classiﬁer exclusively on each feature category. For
this experiment, we use the data set from the previous section,
applying 10-fold validation with training data at a 4:1 non-
spam to spam ratio and the testing set again at a 1:1 ratio.
Any feature category with an accuracy above 50% is consid-
ered better than a classiﬁer that naively guesses the majority
population. The results of per-feature category training are
shown in Table 5. Source URLs, which is an amalgamation of
every URL requested by the browser as a page is constructed,
provides the best overall performance. Had our classiﬁer relied
exclusively on initial URLs or ﬁnal
landing page URLs,
accuracy would be 7% lower and false negatives 10% higher.
Surprisingly, DNS and redirect features do not perform well on
their own, each achieving approximately 72% accuracy. The
combination of all of these features lowers the false positive
rate while maintaining high accuracy.
Accuracy Over Time. Because criminals introduce new mali-
cious websites on a continual basis, we want to determine how
often we need to retrain our classiﬁer and how long it takes for
the classiﬁer to become out of date. To answer these questions,
we evaluate the accuracy of our classiﬁer over a 20 day period
where we had continuous spam and non-spam samples. We
train using two different training regimens: (1) training the
classiﬁer once over four days’ worth of data, then keeping
the same classiﬁcation model for the rest of the experiment;
(2) retraining the classiﬁer every four days, then testing the
Training Set
Tweet spam
Tweet spam
Email spam
Email spam
Testing Set
Tweet spam
Email spam
Tweet spam
Email spam
Accuracy
94.01%
80.78%
79.78%
98.64%
FP
FN
1.92% 22.25%
1.92% 88.14%
0.55% 98.89%
0.58%
4.47%
TABLE 6: Effects of training and testing on matching and mismatch-
ing data sets. Email and tweet spam are largely independent in their
underlying features, resulting in low cross classiﬁcation accuracy.
Training Method
With Tweet Features
Without Tweet Features
Accuracy
94.15%
94.16%
FP
FN
1.81% 22.11%
1.95% 21.38%
TABLE 7: Effects of
information.
Omitting account and tweet properties from classiﬁcation has no
statistically signiﬁcant effect on accuracy (the error rates are within
one standard deviation of each another).
including contextual Twitter
tweets slip past our classiﬁer. This same training regimen
utterly fails on email, resulting in 88% of email spam going
uncaught. These results are mirrored on a mixed data set
of email spam and non-spam samples. We can achieve an
accuracy of 98.64% with 4.47% false negatives when we train
a classiﬁer to exclusively ﬁnd email spam. When we apply
this same classiﬁer to a testing set of Twitter spam, 98% of
spam samples go uncaught.
These results highlight a fundamental challenge of spam
ﬁltering. Within the spam ecosystem, there are a variety of
actors that each execute campaigns unique to individual web
services. While Monarch’s infrastructure generalizes to any
web service, training data is not guaranteed to do the same.
We require individual labeled data sets from each service in
order to provide the best performance. A second unexpected
result is the difﬁculty of identifying tweet spam compared to
email spam. On matched training and testing sets, email spam
classiﬁcation achieves half the false negatives of tweet spam
classiﬁcation and a ﬁfth of the false positives. We explore the
underlying reason for this discrepancy in Section 6.3.
Context vs. Context Free Training. Because spam URLs
can appear on different web services such as email, social
networks, blogs, and forums, the question arises whether using
context-aware features can improve classiﬁcation accuracy
at the cost of generalizability. To investigate this issue, we
compare the error rate of classifying Twitter spam URLs (we
exclude email spam) with and without account-based features.
These features include account creation time, a tokenized
version of tweet text, a tokenized version of an account’s
proﬁle description, the number of friends and followers an
account has, the number of posts made by an account, a
tokenized screen name, the account’s unique Twitter ID, the
application used to access Twitter (e.g., web, Twitter’s API, or
a third-party application), hashtags present in the tweet, and
“mentions” present in the tweet. Comprehensive historical data
such as the ratio of URLs to posts is unavailable.
We perform 5-fold cross validation over a data set con-
taining 400,000 non-spam samples and 100,000 tweet spam
samples. The results of the experiment are shown in Table 7.
Fig. 3: Performance of classiﬁer over time. Regular retraining is
required to guarantee the best accuracy, else error slowly increases.
model on the subsequent four days of data. The data for each
four-day window consists of 100,000 examples sampled at a
4:1 non-spam to spam ratio. We repeat this experiment four
times by resampling each window’s data, and take the average
result.
Figure 3 shows the results for our time-sensitive evaluations.
The error of the statically trained classiﬁer gradually increases
over time, whereas the classiﬁer retrained daily maintains
roughly constant accuracy. This indicates that in a deployment
of Monarch, we will need to retrain the classiﬁer on a continual
basis. We explore the temporal nature of features that cause
this behavior further in Section 6.3.
Training Across Input Sources. One of the primary chal-
lenges of training a classiﬁer is obtaining labeled spam sam-
ples. Consequently, if a single labeled data set generalized to
all web services, it would alleviate the problem of each web
service being required to obtain their own spam samples. For
instance, a great deal of time and effort could be saved if
spam caught by passive email spam traps were applicable to
Twitter where we currently are forced to crawl every link and
retroactively blacklist spam URLs. However, spam targeting
one web service is not guaranteed to be representative of spam
targeting all web services. To this end we ask: how well can
an email-trained classiﬁer perform on Twitter data? How well
can a Twitter-trained classiﬁer perform on email data?
Table 6 displays the results of an experiment where we
train our classiﬁer on matching and mismatched data sources.
We construct a 5-fold data set containing 400,000 non-spam
samples and 100,000 tweet spam samples. Then, we copy
the 5 folds but replace the 100,000 tweet spam samples
with 100,000 email spam examples. We perform 5-fold cross
validation to obtain classiﬁcation rates. For a given testing
fold, we test on both the tweet spam and email spam version
of the fold (the non-spam samples remain the same in both
version to ensure comparable results with respect to false
positives).
Using a mixture of Twitter spam and non-spam samples,
we are able to achieve 94% accuracy, but let 22% of spam
456
Even if Twitter account features are included, accuracy is
statistically identical to training without these features. This
contrasts with previous results that rely on account-based
features to identify (fraudulent) spam accounts [12]–[14], but
agrees with recent studies that have shown compromised
accounts are the major distributors of spam [5], [11] which
would render account-based features obsolete.
While this result is not guaranteed to generalize to all web
services, we have demonstrated that strong performance for
ﬁltering email and Twitter spam is achievable without any
requirement of revealing personally identiﬁable information.
Omitting contextual information also holds promise for iden-
tifying web spam campaigns that cross web service boundaries
without signiﬁcant loss of accuracy due to disparate contextual
information.
6.2. Run Time Performance
In addition to Monarch’s accuracy, its overall performance
and cost to execute are important metrics. In this section we
measure the latency, throughput, and the cost of Monarch,
ﬁnding a modest deployment of our system can classify URLs
with a median time of 5.54 seconds and a throughput of
638,000 URLs per day, at a monthly cost of $1,600 on cloud
machinery.
Latency. We measure latency as the time delta from when
we receive a tweet or email URL until Monarch returns a
ﬁnal decision. Table 8 shows a breakdown of processing time
for a sample of 5,000 URLs. URL aggregation takes 5 ms
to parse a URL from Twitter’s API format (email requires no
parsing) and to enqueue the URL. Feature collection represents
the largest overhead in Monarch, accounting for a median
run time 5.46 seconds. Within feature collection, crawling
a URL in Firefox consumes 3.13 seconds, while queries
for DNS, geolocation and routing require 2.33 seconds. The
majority of the processing time in both cases occurs due to
network delay, not execution overhead. The remaining 70ms
are spent extracting features and summing weight vectors for
a classiﬁcation decision.
Given that Firefox browsing incurs the largest delay, we
investigate whether our instrumentation of Firefox for feature
collection negatively impacts load times. We compare our
instrumented Firefox against an uninstrumented copy using
a sample of 5,000 URLs on a system running Fedora Core 13
machine with a four core 2.8GHz Xeon processor with 8GB
of memory. We ﬁnd instrumentation adds 1.02% overhead,
insigniﬁcant to the median time it takes Firefox to execute all
outgoing network requests which cannot be reduced. Instru-
mentation overhead results from interposing on browser events
and message passing between the browser and monitoring
service, accounting on average 110KB of log ﬁles.
Throughput. We measure the throughput of Monarch for a
small deployment consisting of 20 instances on Amazon’s EC2
infrastructure for crawling and feature collection. The crawling
Component
URL aggregation
Feature collection
Feature extraction
Classiﬁcation
Total
Median Run Time (seconds)
0.005
5.46
0.074
0.002
5.54
TABLE 8: Breakdown of the time spent processing a single URL.
Component
URL aggregation
Feature collection
Feature extraction
Classiﬁcation
Storage
Total
AWS Infrastructure
1 Extra Large
20 High-CPU Medium
—
50 Double Extra Large
700GB on EBS
Monthly Cost
$178
$882
$0
$527
$70
$1,587
TABLE 9: Breakdown for the cost spent for Monarch infrastructure.
Feature extraction runs on the same infrastructure as classiﬁcation.
and feature extraction execute on a high-CPU medium instance
that has 1.7GB of memory and two cores (5 EC2 compute
units), running a 32-bit version of Ubuntu Linux 10.04. Each
instance runs 6 copies of the crawling and feature collection
code. We determined that the high-CPU medium instances
have the lowest dollar per crawler cost, which make them the
most efﬁcient choice for crawling. The number of crawlers that
each instance can support depends on the memory and CPU
the machine. Using this small deployment, we can process
638,000 URLs per day.
Training Time. For the experiments in Section 6.1, we trained
over data sets of 400,000 examples (80 GB in JSON format).
The training time for 100 iterations of the distributed logistic