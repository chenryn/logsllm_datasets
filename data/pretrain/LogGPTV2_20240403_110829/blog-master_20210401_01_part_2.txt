here could cause the planner to choose slower plans than it did previous  
to having this feature.  Distinct estimations are also fairly hard to  
estimate accurately when several tables have been joined already or when a  
WHERE clause filters out a set of values that are correlated to the  
expressions we're estimating the number of distinct value for.  
For now, the costing we perform during query planning for result caches  
does put quite a bit of faith in the distinct estimations being accurate.  
When these are accurate then we should generally see faster execution  
times for plans containing a result cache.  However, in the real world, we  
may find that we need to either change the costings to put less trust in  
the distinct estimations being accurate or perhaps even disable this  
feature by default.  There's always an element of risk when we teach the  
query planner to do new tricks that it decides to use that new trick at  
the wrong time and causes a regression.  Users may opt to get the old  
behavior by turning the feature off using the enable_resultcache GUC.  
Currently, this is enabled by default.  It remains to be seen if we'll  
maintain that setting for the release.  
Additionally, the name "Result Cache" is the best name I could think of  
for this new node at the time I started writing the patch.  Nobody seems  
to strongly dislike the name. A few people did suggest other names but no  
other name seemed to dominate in the brief discussion that there was about  
names. Let's allow the beta period to see if the current name pleases  
enough people.  If there's some consensus on a better name, then we can  
change it before the release.  Please see the 2nd discussion link below  
for the discussion on the "Result Cache" name.  
Author: David Rowley  
Reviewed-by: Andy Fan, Justin Pryzby, Zhihong Yu  
Tested-By: Konstantin Knizhnik  
Discussion: https://postgr.es/m/CAApHDvrPcQyQdWERGYWx8J%2B2DLUNgXu%2BfOSbQ1UscxrunyXyrQ%40mail.gmail.com  
Discussion: https://postgr.es/m/CAApHDvq=yQXr5kqhRviT2RhNKwToaWr9JAN5t+PI:EMAIL  
```  
```  
   1 -- Perform tests on the Result Cache node.  
   2 -- The cache hits/misses/evictions from the Result Cache node can vary between  
   3 -- machines.  Let's just replace the number with an 'N'.  In order to allow us  
   4 -- to perform validation when the measure was zero, we replace a zero value  
   5 -- with "Zero".  All other numbers are replaced with 'N'.  
   6 create function explain_resultcache(query text, hide_hitmiss bool) returns setof text  
   7 language plpgsql as  
   8 $$  
   9 declare  
  10     ln text;  
  11 begin  
  12     for ln in  
  13         execute format('explain (analyze, costs off, summary off, timing off) %s',  
  14             query)  
  15     loop  
  16         if hide_hitmiss = true then  
  17                 ln := regexp_replace(ln, 'Hits: 0', 'Hits: Zero');  
  18                 ln := regexp_replace(ln, 'Hits: \d+', 'Hits: N');  
  19                 ln := regexp_replace(ln, 'Misses: 0', 'Misses: Zero');  
  20                 ln := regexp_replace(ln, 'Misses: \d+', 'Misses: N');  
  21         end if;  
  22         ln := regexp_replace(ln, 'Evictions: 0', 'Evictions: Zero');  
  23         ln := regexp_replace(ln, 'Evictions: \d+', 'Evictions: N');  
  24         ln := regexp_replace(ln, 'Memory Usage: \d+', 'Memory Usage: N');  
  25         return next ln;  
  26     end loop;  
  27 end;  
  28 $$;  
  29 -- Ensure we get a result cache on the inner side of the nested loop  
  30 SET enable_hashjoin TO off;  
  31 SELECT explain_resultcache('  
  32 SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1  
  33 INNER JOIN tenk1 t2 ON t1.unique1 = t2.twenty  
  34 WHERE t2.unique1   Nested Loop (actual rows=1000 loops=1)  
  39          ->  Bitmap Heap Scan on tenk1 t2 (actual rows=1000 loops=1)  
  40                Recheck Cond: (unique1   Bitmap Index Scan on tenk1_unique1 (actual rows=1000 loops=1)  
  43                      Index Cond: (unique1   Result Cache (actual rows=1 loops=1000)  
  45                Cache Key: t2.twenty  
  46                Hits: 980  Misses: 20  Evictions: Zero  Overflows: 0  Memory Usage: NkB  
  47                ->  Index Only Scan using tenk1_unique1 on tenk1 t1 (actual rows=1 loops=20)  
  48                      Index Cond: (unique1 = t2.twenty)  
  49                      Heap Fetches: 0  
  50 (13 rows)  
  51   
  52 -- And check we get the expected results.  
  53 SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1  
  54 INNER JOIN tenk1 t2 ON t1.unique1 = t2.twenty  
  55 WHERE t2.unique1   Nested Loop (actual rows=1000 loops=1)  
  70          ->  Bitmap Heap Scan on tenk1 t1 (actual rows=1000 loops=1)  
  71                Recheck Cond: (unique1   Bitmap Index Scan on tenk1_unique1 (actual rows=1000 loops=1)  
  74                      Index Cond: (unique1   Result Cache (actual rows=1 loops=1000)  
  76                Cache Key: t1.twenty  
  77                Hits: 980  Misses: 20  Evictions: Zero  Overflows: 0  Memory Usage: NkB  
  78                ->  Index Only Scan using tenk1_unique1 on tenk1 t2 (actual rows=1 loops=20)  
  79                      Index Cond: (unique1 = t1.twenty)  
  80                      Heap Fetches: 0  
  81 (13 rows)  
  82   
  83 -- And check we get the expected results.  
  84 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
  85 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
  86 WHERE t1.unique1   Nested Loop (actual rows=800 loops=1)  
 106          ->  Bitmap Heap Scan on tenk1 t2 (actual rows=800 loops=1)  
 107                Recheck Cond: (unique1   Bitmap Index Scan on tenk1_unique1 (actual rows=800 loops=1)  
 110                      Index Cond: (unique1   Result Cache (actual rows=1 loops=800)  
 112                Cache Key: t2.thousand  
 113                Hits: Zero  Misses: N  Evictions: N  Overflows: 0  Memory Usage: NkB  
 114                ->  Index Only Scan using tenk1_unique1 on tenk1 t1 (actual rows=1 loops=800)  
 115                      Index Cond: (unique1 = t2.thousand)  
 116                      Heap Fetches: 0  
 117 (13 rows)  
 118   
 119 RESET enable_mergejoin;  
 120 RESET work_mem;  
 121 RESET enable_hashjoin;  
 122 -- Test parallel plans with Result Cache.  
 123 SET min_parallel_table_scan_size TO 0;  
 124 SET parallel_setup_cost TO 0;  
 125 SET parallel_tuple_cost TO 0;  
 126 -- Ensure we get a parallel plan.  
 127 EXPLAIN (COSTS OFF)  
 128 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
 129 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
 130 WHERE t1.unique1   Gather  
 135          Workers Planned: 2  
 136          ->  Partial Aggregate  
 137                ->  Nested Loop  
 138                      ->  Parallel Bitmap Heap Scan on tenk1 t1  
 139                            Recheck Cond: (unique1   Bitmap Index Scan on tenk1_unique1  
 141                                  Index Cond: (unique1   Result Cache  
 143                            Cache Key: t1.twenty  
 144                            ->  Index Only Scan using tenk1_unique1 on tenk1 t2  
 145                                  Index Cond: (unique1 = t1.twenty)  
 146 (13 rows)  
 147   
 148 -- And ensure the parallel plan gives us the correct results.  
 149 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
 150 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
 151 WHERE t1.unique1 < 1000;  
 152  count |        avg           
 153 -------+--------------------  
 154   1000 | 9.5000000000000000  
 155 (1 row)  
 156   
 157 RESET parallel_tuple_cost;  
 158 RESET parallel_setup_cost;  
 159 RESET min_parallel_table_scan_size;  
```  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")