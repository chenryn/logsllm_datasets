tool awareness and use numbers can be found in Appendix B.
For those tools they had heard about, we asked them
where they had heard about them. Overall, participants were
recommended a tool by a colleague 33 times, heard of a tool
from its authors 20 times, read the paper of the tool 27 times,
read about the tool in a different paper or blog post 42 times
and heard of it some other way 24 times. 2 were involved
in a development of a tool. A general tool they are already
using can also be used for constant-time-analysis, which P18
learned through our survey:
“I already use MemSan primarily for memory fault detec-
tion. Was not aware of its use for side-channel detection
but will try it in future since it is already integrated with
my workflow to some extent.” (P18)
Again for the tools they were aware of, we asked which
(if any) they had (tried to) use in the context of verifying
or testing resistance to timing attacks. Table III displays the
results, with 19 having tried to use at least one tool and 25
having never tried any of the tools.
6) Tool experience and use cases (RQ3b): Here, we answer
the research question which experiences participants made
with tools (RQ3b). As we were anecdotally aware that tools
may be hard to obtain, unmaintained, and may be closer to
research artifacts than ready-to-use tooling, we were interested
in participants’ experiences, finding that experience varied by
tool, use cases and expectations. We therefore asked partici-
pants to describe the process of using the tools.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:33 UTC from IEEE Xplore.  Restrictions apply. 
640
12 reported that they managed to get the respective tool to
work at least once, but not necessarily repeatedly, while 3
reported that the tool they attempted to use failed to work even
once, for various reasons, including excessive use of resources,
such as effort, time, RAM, CPU cores, machines etc. One
participant said of the DATA tool: “it uses a ridiculous amount
of resources” (P17).
2 reported that they had integrated the tool into CI and
were using it automatically. 12 reported that they used it
manually, of which 6 said they use it during development, and
6 said they use it after development, on release. A participant
said: “Periodically, and manually, used when altering / writing
code to check constant-time property.” (P12)
For those who had heard of specific tools, but had not
attempted to use them, we were also interested in their
reasoning. The reasons were varied, many including a lack of
resources such as time (26 ) or RAM, CPU cores and machine
(1 ). Participants also reported on bad availability (4 ), and
maintenance (5 ), as well as insufficient language support (4
), and other usability issues, such as problems with setting up
the tool (3 ), or getting it to work properly post setup (1 ).
The difficulty or impossibility of fulfilling the required code
changes, such as markup for secret/public values, memory
regions/aliasing, and additional header files was also a problem
(reported by 1 ), as was the inability to ignore reported issues,
once flagged by the tool (8 ).
Some reported not needing the respective tool (22 ), using
other tools (18 ), gave reasoning that to our understanding
was based on misconceptions of the respective tool (2 ), or
reported having been unaware of the tool’s capabilities in the
context of resistance to timing attacks (1 ).
One participant also said that the tool was also used to verify
a security disclosure. “Tried to use to reproduce results, verify
disclosures. Tried to use it to discover new defects in existing
code.” (P14) — since the tool is later stated as in use by
another member of the same project, this confirms that the
tool not only verified the initial defect, but works as planned.
Fig. 3. Reported likeliness of tool use based on requirements and guarantees.
7) Potential Tool use (RQ4): In addition to understanding
participants’ current threat models and behaviors concerning
constant-time code, we were also interested in what
they
thought about potential future use of testing/verification tools,
and whether they would potentially be willing to fulfill certain
requirements in exchange for guarantees (RQ4, see Figure 3).
Generally, they were most willing to use dynamic instrumen-
tation tools, and also spoke about them the most positively,
Fig. 4. Participant reasoning behind their likelihood of tool use.
whereas they mostly mentioned drawbacks when asked about
formal analysis tools.
We presented the participants with the requirements and
guarantees offered by three categories of tools: dynamic in-
strumentation based tools, statistical runtime tests and formal
analysis tools.5 We then asked them to rate their likeliness of
using the presented group of tools on a 5-point Likert scale
from “1=very unlikely” to “5=very likely”. Figure 3 shows
a strong preference for dynamic tools, while formal analysis
tools are least likely to be used. We perform statistical tests on
these ratings to establish that these differences are statistically
significant. We find a significant difference in participants’
self-reported likeliness to use tools in the different categories
(Friedman Test-Statistic=18.477, p,
where all code was designed to be constant-time” (P11). This
sentiment comes with several problems: on the one hand,
humans make mistakes, so testing code is a best practice in
software engineering for precisely this reason. Additionally,
compiling code that does fulfill the constant-time property
may create problems, as the compiler may change the original
control-flow while adding some optimizations.
While talking about compilation units and control-flow, a
partial misconception can be found in verification scope: “a
lot of code will exist outside of the boundaries of the library. A
project using  would be more likely to be successful.”
(P20) While the library may not know which inputs are secret
, looking at an API should make it clear which inputs can be
secret, and the constant-time criterion could be tested for all
of them without knowing the actual usage patterns.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:33 UTC from IEEE Xplore.  Restrictions apply. 
642
Furthermore, the different answers about random delays and
statistical analysis tools show that there is no universal con-
sensus among the participants. A participant said: “Anything
involving secret data, and in particular private-key data, has
the timing dithered and with throttling of repeated attempts to
make attacks of this kind difficult.” (P24) We are skeptical
about this due to the results of Brumley and Tuveri [15].
If a side channel signal is measured as a timing difference
between executions, adding a random noise distribution to
these executions will reproduce a similar difference if enough
samples of the executions are obtained. This can be done
in parallel from different sources or over a long time, going
around the throttling defense. A more practical quote is from
P9:“We once tried to test actual execution timings, but it
wasn’t reliable. We no longer do that. Now we use Valgrind.”
Lastly, even if cryptography is rather heavy in mathematics,
some participants associate math/formal analysis as a barrier to
using tools from that research area. “[P]roving things like loop
bounds is often arcane. Also, it’s knowledge that would present
a barrier to new engineers joining the team.” (P12) This is
most likely a misconception, potentially caused by unclear
writing in formal analysis tools’ documentation, or scientific
publications that do not separate tool use from general formal
verification and theorem proving.
9) Developer Concerns and Wishlist (RQ5):
In addition
to misconceptions, participants also voiced understandable
concerns about constant-time development, as well as wishes
for verification tools that would allow them to use these tools
more effectively (RQ5). Major concerns were voiced about the
tools’ resource usage being too high (see Section IV-B6).
In addition to these issues, P14 listed concerns as: “the
execution time of static and dynamic analyzers tailored for
SCA, the need for human interaction, the rate of false positives,
etc. are usually preventing a systematic adoption”. The issue
with flagging false positives and not linking false positives
and negatives was addressed by another participant also: “We
noticed a couple false positive, where there *is* a path
from the contents of the buffer to timings, but we decided
that doesn’t
leak any meaningful secret.” (P9). They also
mentioned security concerns for tools based only on the source
code. These may miss vulnerabilities due to miscompilation,
as explained by P13: “Any "constant-time" code is an endless
arms race against the compiler”.
Interestingly, participants had many precise ideas for what
could be done to improve the status quo of testing/verification
tools. For example, for better usability, they ask for the ability
to ignore some issues and/or some part of the code, as noted
by P14: “Also, expect a lot of "noise" from BIGNUM behavior
that is not CT and requires a full redesign to be fixed.”
We saw many wishes for improvements concerning annota-
tions, asking for external annotations. Participants also asked
for easy maintenance of code annotations (see IV-B7a), and
requested that tools work on complex code, as P14 explained:
“even for expert users the chances of exposing something non-
consttime to remote attackers are high, especially given the
complex nature of  under the hood.” They also asked
for test cases to be fast to set up, to avoid a “non-trivial amount
of effort to set up comprehensive tests.” (P14)
to run, deploying it
To address the issue of scale, they want to be able to use
tools in CI. Otherwise, when the code changes, the guarantees
are lost. This means that error code outputs, easy CI setup and
runtime are important, as explained by P19: “Static analysis
tools tend to have a high engineering overhead: getting the
tool
to CI systems, maintaining the
installation over the years.” Similarly, participants demanded
that tools not require rewrites of their code: P2 ruled out an
“awesome tool”, because it “cannot verify existing code.” Par-
ticipants also required no restricted language or environment
for their code instead of “a pretty special-purpose language”
(P26). Similarly, they asked for no use of a specialized com-
piler; as P4 stated: “Requiring a dedicated compiler sounds
like a potential problem.” Generally, they asked for integration
into type system and APIs they are already using: “which
values are public and which private, we have flags on APIs
to allow the caller to specify this too” (P28), so the project
already has a form of security annotations for the users of their
API, which a tool should be able to integrate for its analysis.
They also requested long-term available source code and
longterm maintenance. As P25 stated, tools being unavailable
or unmaintained makes it impossible to use them.
V. DISCUSSION
Based on our findings, we make suggestions for four groups
of actors who can take action to make cryptographic code
resistant to timing attacks: tool developers, compiler writers,
cryptographic library developers, and standardization bodies.
A. Tool developers