title:Using RDMA efficiently for key-value services
author:Anuj Kalia and
Michael Kaminsky and
David G. Andersen
Using RDMA Efﬁciently for Key-Value Services
Anuj Kalia Michael Kaminsky† David G. Andersen
†Intel Labs
{akalia,dga}@cs.cmu.edu PI:EMAIL
Carnegie Mellon University
ABSTRACT
This paper describes the design and implementation of HERD, a key-
value system designed to make the best use of an RDMA network.
Unlike prior RDMA-based key-value systems, HERD focuses its
design on reducing network round trips while using efﬁcient RDMA
primitives; the result is substantially lower latency, and throughput
that saturates modern, commodity RDMA hardware.
HERD has two unconventional decisions: First, it does not use
RDMA reads, despite the allure of operations that bypass the remote
CPU entirely. Second, it uses a mix of RDMA and messaging verbs,
despite the conventional wisdom that the messaging primitives are
slow. A HERD client writes its request into the server’s memory;
the server computes the reply. This design uses a single round trip
for all requests and supports up to 26 million key-value operations
per second with 5 µs average latency. Notably, for small key-value
items, our full system throughput is similar to native RDMA read
throughput and is over 2X higher than recent RDMA-based key-
value systems. We believe that HERD further serves as an effective
template for the construction of RDMA-based datacenter services.
INTRODUCTION
Keywords
RDMA; InﬁniBand; RoCE; Key-Value Stores
1.
This paper explores a question that has important implications for
the design of modern clustered systems: What is the best method
for using RDMA features to support remote hash-table access? To
answer this question, we ﬁrst evaluate the performance that, with
sufﬁcient attention to engineering, can be achieved by each of the
RDMA communication primitives. Using this understanding, we
show how to use an unexpected combination of methods and system
architectures to achieve the maximum performance possible on a
high-performance RDMA network.
Our work is motivated by the seeming contrast between the funda-
mental time requirements for cross-node trafﬁc vs. CPU-to-memory
lookups, and the designs that have recently emerged that use multiple
RDMA (remote direct memory access) reads. On one hand, going
between nodes takes roughly 1-3 µs, compared to 60-120 ns for a
memory lookup, suggesting that a multiple-RTT design as found in
the recent Pilaf [21] and FaRM [8] systems should be fundamen-
tally slower than a single-RTT design. But on the other hand, an
RDMA read bypasses many potential sources of overhead, such as
servicing interrupts and initiating control transfers, which involve
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses. contact the Owner/Author.
Copyright is held by the owner/author(s).
SIGCOMM’14, Aug 17-22 2014, Chicago, IL, USA
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2626299
the host CPU. In this paper, we show that there is a better path to
taking advantage of RDMA to achieve high-throughput, low-latency
key-value storage.
A challenge for both our and prior work lies in the lack of richness
of RDMA operations. An RDMA operation can only read or write a
remote memory location. It is not possible to do more sophisticated
operations such as dereferencing and following a pointer in remote
memory. Recent work in building key-value stores [21, 8] has
focused exclusively on using RDMA reads to traverse remote data
structures, similar to what would have been done had the structure
been in local memory. This approach invariably requires multiple
round trips across the network.
Consider an ideal RDMA read-based key-value store (or cache)
where each GET request requires only 1 small RDMA read. Design-
ing such a store is as hard as designing a hash-table in which each
GET request requires only one random memory lookup. We instead
provide a solution to a simpler problem: we design a key-value
cache that provides performance similar to that of the ideal cache.
However, our design does not use RDMA reads at all.
In this paper, we present HERD, a key-value cache that leverages
RDMA features to deliver low latency and high throughput. As we
demonstrate later, RDMA reads cannot harness the full performance
of the RDMA hardware. In HERD, clients transmit their request to
the server’s memory using RDMA writes. The server’s CPU polls its
memory for incoming requests. On receiving a new request, it exe-
cutes the GET or PUT operation in its local data structures and sends
the response back to the client. As RDMA write performance does
not scale with the number of outbound connections, the response is
sent as a SEND message over a datagram connection.
Our work makes three main contributions:
• A thorough analysis of the performance of RDMA verbs and
expose the various design options for key-value systems.
• Evidence that “two-sided” verbs are better than RDMA reads
for key-value systems, refuting the previously held assump-
tion [21, 8].
• Describing the design and implementation of HERD, a key-
value cache that offers the maximum possible performance of
RDMA hardware.
The following section brieﬂy introduces key-value stores and
RDMA, and describes recent efforts in building key-value stores
using RDMA. Section 3 discusses the rationale behind our design
decisions and demonstrates that messaging verbs are a better choice
than RDMA reads for key-value systems. Section 4 discusses the
design and implementation of our key-value cache. In Section 5, we
evaluate our system on a cluster of 187 nodes and compare it against
FaRM [8] and Pilaf [21].
2952. BACKGROUND
This section provides background information on key-value stores
and caches, which are at the heart of HERD. We then provide an
overview of RDMA, as is relevant for the rest of the paper.
2.1 Key-Value stores
DRAM-based key-value stores and caches are widespread in large-
scale Internet services. They are used both as primary stores (e.g.,
Redis [4] and RAMCloud [23]), and as caches in front of backend
databases (e.g., Memcached [5]). At their most basic level, these
systems export a traditional GET/PUT/DELETE interface. Internally,
they use a variety of data structures to provide fast, memory-efﬁcient
access to their underlying data (e.g., hash table or tree-based in-
dexes).
In this paper, we focus on the communication architecture to
support both of these applications; we use a cache implementation
for end-to-end validation of our resulting design.
Although recent in-memory object stores have used both tree and
hash table-based designs, this paper focuses on hash tables as the
basic indexing data structure. Hash table design has a long and rich
history, and the particular ﬂavor one chooses depends largely on the
desired optimization goals. In recent years, several systems have
used advanced hash table designs such as Cuckoo hashing [24, 17, 9]
and Hopscotch hashing [12]. Cuckoo hash tables are an attractive
choice for building fast key-value systems [9, 31, 17] because, with
K hash functions (usually, K is 2 or 3), they require only K memory
lookups for GET operations, plus an additional pointer dereference if
the values are not stored in the table itself. In many workloads, GETs
constitute over 95% of the operations [6, 22]. This property makes
cuckoo hashing an attractive backend for an RDMA-based key-value
store [21]. Cuckoo and Hopscotch-based designs often emphasize
workloads that are read-intensive: PUT operations require moving
values within the tables. We evaluate both balanced (50% PUT/GET)
and read-intensive (95% GET) workloads in this paper.
To support both types of workloads without being limited by the
performance of currently available data structure options, HERD
internally uses a cache data structure that can evict items when
it is full. Our focus, however, is on the network communication
architecture—our results generalize across both caches and stores,
so long as the implementation is fast enough that a high-performance
communication architecture is needed. HERD’s cache design is
based on the recent MICA [18] system that provides both cache
and store semantics. MICA’s cache mode uses a lossy associative
index to map keys to pointers, and stores the values in a circular log
that is memory efﬁcient, avoids fragmentation, and does not require
expensive garbage collection. This design requires only 2 random
memory accesses for both GET and PUT operations.
2.2 RDMA
Remote Direct Memory Access (RDMA) allows one computer to
directly access the memory of a remote computer without involving
the operating system at any host. This enables zero-copy trans-
fers, reducing latency and CPU overhead. In this work, we focus
on two types of RDMA-providing interconnects: InﬁniBand and
RoCE (RDMA over Converged Ethernet). However, we believe that
our design is applicable to other RDMA providers such as iWARP,
Quadrics, and Myrinet.
InﬁniBand is a switched fabric network widely used in high-
performance computing systems. RoCE is a relatively new network
protocol that allows direct memory access over Ethernet. InﬁniBand
and RoCE NICs achieve low latency by implementing several layers
of the network stack (transport layer through physical layer) in
hardware, and by providing RDMA and kernel-bypass.
In this
section, we provide an overview of RDMA features and terminology
that are used in the rest of this paper.
2.2.1 Comparison with classical Ethernet
To distinguish from RoCE, we refer to non-RDMA providing Ether-
net networks as “classical Ethernet.” Unlike classical Ethernet NICs,
RDMA NICs (RNICs) provide reliable delivery to applications by
employing hardware-based retransmission of lost packets. Further,
RNICs provide kernel bypass for all communication. These two
factors reduce end-to-end latency as well as the CPU load on the
communicating hosts. The typical end-to-end ( 1
2 RTT) latency in In-
ﬁniBand/RoCE is 1 µs while that in modern classical Ethernet-based
solutions [2, 18] is 10 µs. A large portion of this gap arises because
of differing emphasis in the NIC design. RDMA is increasing its
presence in datacenters as the hardware becomes cheaper [21]. A
40 Gbps ConnectX-3 RNIC from Mellanox costs about $500, while
a 10 Gbps Ethernet adapter costs between $300 and $800. The
introduction of RoCE will further boost RDMA’s presence as it will
allow sockets applications to run with RDMA applications on the
same network.
2.2.2 Verbs and queue pairs
Userspace programs access RNICs directly using functions called
verbs. There are several types of verbs. Those most relevant to
this work are RDMA read (READ), RDMA write (WRITE), SEND,
and RECEIVE. Verbs are posted by applications to queues that are
maintained inside the RNIC. Queues always exist in pairs: a send
queue and a receive queue form a queue pair (QP). Each queue pair
has an associated completion queue (CQ), which the RNIC ﬁlls in
upon completion of verb execution.
The verbs form a semantic deﬁnition of the interface provided
by the RNIC. There are two types of verbs semantics: memory
semantics and channel semantics.
Memory semantics: The RDMA verbs (READ and WRITE)
have memory semantics: they specify the remote memory address
to operate upon. These verbs are one-sided: the responder’s CPU
is unaware of the operation. This lack of CPU overhead at the
responder makes one-sided verbs attractive. Furthermore, they have
the lowest latency and highest throughput among all verbs.
Channel semantics: SEND and RECEIVE (RECV) have channel
semantics, i.e., the SEND’s payload is written to a remote memory
address that is speciﬁed by the responder in a pre-posted RECV. An
analogy for this would be an unbuffered sockets implementation
that required read() to be called before the packet arrived. SEND
and RECV are two-sided as the CPU at the responder needs to post
a RECV in order for an incoming SEND to be processed. Unlike
the memory verbs, the responder’s CPU is involved. Two-sided
verbs also have slightly higher latency and lower throughput than
one sided verbs and have been regarded unfavorably for designing
key-value systems [21, 8].
Although SEND and RECV verbs are technically RDMA verbs,
we distinguish them from READ and WRITE. We refer to READ
and WRITE as RDMA verbs, and refer to SEND and RECV as
messaging verbs.
Verbs are usually posted to the send queue of a QP (except RECV,
which is posted to the receive queue). To post a verb to the RNIC,
an application calls into the userland RDMA driver. Then, the driver
prepares a Work Queue Element (WQE) in the host’s memory and
296Verb
SEND/RECV 
WRITE

READ

RC UC UD






Table 1: Operations supported by each connection type. UC does not
support READs, and UD does not support RDMA at all.
rings a doorbell on the RNIC via Programmed IO (PIO). For Con-
nectX and newer RNICs, the doorbell contains the entire WQE [27].
For WRITE and SEND verbs, the WQE is associated with a pay-
load that needs to be sent to the remote host. A payload up to the
maximum PIO size (256 in our setup) can be inlined in the WQE,
otherwise it can be fetched by the RNIC via a DMA read. An inlined