title:Cutting the electric bill for internet-scale systems
author:Asfandyar Qureshi and
Rick Weber and
Hari Balakrishnan and
John V. Guttag and
Bruce M. Maggs
Cutting the Electric Bill for Internet-Scale Systems
Asfandyar Qureshi
MIT CSAIL
PI:EMAIL
Rick Weber
Akamai Technologies
PI:EMAIL
Hari Balakrishnan
MIT CSAIL
PI:EMAIL
John Guttag
MIT CSAIL
PI:EMAIL
Bruce Maggs
Carnegie Mellon University
PI:EMAIL
ABSTRACT
Energy expenses are becoming an increasingly important
fraction of data center operating costs. At the same time,
the energy expense per unit of computation can vary sig-
niﬁcantly between two diﬀerent locations.
In this paper,
we characterize the variation due to ﬂuctuating electricity
prices and argue that existing distributed systems should be
able to exploit this variation for signiﬁcant economic gains.
Electricity prices exhibit both temporal and geographic vari-
ation, due to regional demand diﬀerences, transmission inef-
ﬁciencies, and generation diversity. Starting with historical
electricity prices, for twenty nine locations in the US, and
network traﬃc data collected on Akamai’s CDN, we use sim-
ulation to quantify the possible economic gains for a realistic
workload. Our results imply that existing systems may be
able to save millions of dollars a year in electricity costs, by
being cognizant of locational computation cost diﬀerences.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems
General Terms
Economics, Management, Performance
1.
INTRODUCTION
With the rise of “Internet-scale” systems and “cloud com-
puting” services, there is an increasing trend toward massive,
geographically distributed systems. The largest of these are
made up of hundreds of thousands of servers and several data
centers. A large data center may require many megawatts
of electricity [1], enough to power thousands of homes.
Millions of dollars must be spent annually on the electric-
ity needed to power one such system. Furthermore, these
already large systems are increasing in size at a rapid clip,
outpacing data center energy eﬃciency gains [2], and elec-
tricity prices are expected to rise.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$10.00.
Company
eBay
Akamai
Rackspace
Microsoft
Google
USA (2006)
MIT campus
Servers
Electricity
Cost
16K ∼0.6×105 MWh ∼$3.7M
40K ∼1.7×105 MWh ∼$10M
∼2×105 MWh ∼$12M
50K
>6×105 MWh >$36M
>200K
>500K >6.3×105 MWh >$38M
10.9M 610×105 MWh
$4.5B
2.7×105MWh
$62M
Figure 1: Estimated annual electricity costs for large
companies (servers and infrastructure) @ $60/MWh.
These are conservative estimates, meant to be lower
bounds. See §2.1 for derivation details. For scale, we
have included the actual 2007 consumption and utility
bill for the MIT campus, including dormitories and labs.
Organizations such as Google, Microsoft, Amazon, Ya-
hoo!, and many other operators of large networked systems
cannot ignore their energy costs. A back-of-the-envelope cal-
culation for Google suggests it consumes more than $38M
worth of electricity annually (ﬁgure 1). A modest 3% reduc-
tion would therefore exceed a million dollars every year. We
project that even a smaller system like Akamai’s1 consumes
an estimated $10M worth of electricity annually2.
The conventional approach to reducing energy costs has
been to reduce the amount of energy consumed [3, 4]. New
cooling technologies, architectural redesigns, DC power, multi-
core servers, virtualization, and energy-aware load balanc-
ing algorithms have all been proposed as ways to reduce the
power demands of data centers. That work is complemen-
tary to ours.
This paper develops and analyzes a new method to reduce
the energy costs of running large Internet-scale systems. It
relies on two key observations:
1. Electricity prices vary. In those parts of the U.S. with
wholesale electricity markets, prices vary on an hourly
basis and are often not well correlated at diﬀerent lo-
cations. Moreover, these variations are substantial, as
much as a factor of 10 from one hour to the next. If,
when computational demand is below peak, we can dy-
namically move demand (i.e., route service requests) to
places with lower prices, we can reduce energy costs.
2. Large distributed systems already incorporate request
routing and replication. We observe that most Internet-
scale systems today are geographically distributed, with
1This paper covers work done outside Akamai and does not rep-
resent the oﬃcial views of the company.
2Though Akamai seldom pays directly for electricity, it pays for
it indirectly as part of co-location expenses.
123machines at tens or even hundreds of sites around the
world. To provide clients good performance and to
tolerate faults, these systems implement some form of
dynamic request routing to map clients to servers, and
often have mechanisms to replicate the data necessary
to process requests at multiple sites.
We hypothesize that by exploiting these observations, large
systems can save a signiﬁcant amount of money, using mech-
anisms for request routing and replication that they already
implement. To explore this hypothesis, we develop a simple
cost-aware request routing policy that preferentially maps
requests to locations where energy is cheaper.
Our main contribution is to identify the relevance of elec-
tricity price diﬀerentials to large distributed systems and to
estimate the cost savings that could result in practice if the
scheme were deployed.
Problem Speciﬁcation. Given a large system composed
of server clusters spread out geographically, we wish to map
client requests to clusters such that the total electricity cost
(in dollars, not Joules) of the system is minimized. For sim-
plicity, we assume that the system is fully replicated. Addi-
tionally, we optimize for cost every hour, with no knowledge
of the future. This rate of change is slow enough to be com-
patible with existing routing mechanisms, but fast enough
to respond to electricity market ﬂuctuations. Finally, we in-
corporate bandwidth and performance goals as constraints.
Existing frameworks already exist to optimize for bandwidth
and performance; modeling them as constraints makes it
possible to add our process to the end of the existing opti-
mization pipeline.
Note that our analysis is concerned with reducing cost, not
energy. Our approach may route client requests to distant
locations to take advantage of cheap energy. These longer
paths may cause overall energy consumption to rise slightly.
Energy Elasticity. The maximum reduction in cost our
approach can achieve hinges on the energy elasticity of the
clusters. This is the degree to which the energy consumed by
a cluster depends on the load placed on it. Ideally, clusters
would draw no power in the absence of load. In the worst
case, there would be no diﬀerence between the peak power
and the idle power of a cluster. Present state-of-the-art sys-
tems [5, 6] fall somewhere in the middle, with idle power
being around 60% of peak. A system with inelastic clusters
is forced to always consume energy everywhere, even in re-
gions with high energy prices. Without adequate elasticity,
we cannot eﬀectively route the system’s power demand away
from high priced areas.
Zero-idle power could be achieved by aggressively consol-
idating, turning oﬀ under-utilized components, and always
activating only the minimum number of machines needed to
handle the oﬀered load. At present, achieving this without
impacting performance is still an open challenge. However,
there is an increasing interest in energy-proportional servers
[6] and dynamic server provisioning techniques are being ex-
plored by both academics and industry [7, 8, 9, 10, 11].
Results. To conduct our analysis, we use trace-driven
simulation with real-world hourly (and daily) energy prices
obtained from a number of data sources. We look at 39
months of hourly electricity prices from 29 US locations.
Our request traces come from the Akamai content distribu-
tion network (CDN): we obtained 24-days worth of request
traﬃc data (ﬁve-minute load) for each server cluster located
at a commercial data center in the U.S. We used these data
sets to estimate the performance of our simple cost-aware
routing scheme under diﬀerent constraints.
We show that:
• Existing systems can reduce energy costs by at least
2%, without any increase in bandwidth costs or sig-
niﬁcant reduction in client performance (assuming a
Google-like energy elasticity, an Akamai-like server dis-
tribution and 95/5 bandwidth constraints). For large
companies this can exceed a million dollars a year.
• Savings rapidly increase with energy elasticity:
in a
fully elastic system, with relaxed bandwidth constraints,
we can reduce energy cost by over 30% (around 13%
if we impose strict bandwidth constraints), without a
signiﬁcant increase in client-server distances.
• Allowing client-server distances to increase leads to in-
creased savings. If we remove the distance constraint,
a dynamic solution has the potential to beat a static
solution (i.e., place all servers in cheapest market) by a
substantial margin (45% maximum savings versus 35%
maximum savings).
Presently, energy cost-aware routing is relevant only to
very large companies. However, as we move forward and
the energy elasticity of systems increases, not only will this
routing technique become more relevant to the largest sys-
tems, but much smaller systems will also be able to achieve
meaningful savings.
Paper Organization. In the next section, we provide
some background on server electricity expenditure and sketch
the structure of US energy markets. In section 3 we present
data about the variation in regional electric prices. Section
4 describes the Akamai data set used in this paper. Section
5 outlines the energy consumption model used in the simu-
lations covered in section 6. Section 7 considers alternative
mechanisms for market participation. Section 8 presents
some ideas for future work, before we conclude.
2. BACKGROUND
This section ﬁrst presents evidence that electricity is be-
coming an increasingly important economic consideration,
and then describes the salient features of the wholesale elec-
tricity markets in the U.S.
2.1 The Scale of Electricity Expenditures
In absolute terms, servers consume a substantial amount
of electricity. In 2006, servers and data centers accounted for
an estimated 61 million MWh, 1.5% of US electricity con-
sumption, costing about 4.5 billion dollars [3]. At worst, by
2011, data center energy use could double. At best, by re-
placing everything with state-of-the-art equipment, we may
be able to reduce usage in 2011 to half the current level [3].
Most companies operating Internet-scale systems are se-
cretive about their server deployments and power consump-
tion. Figure 1 shows our estimates for several such com-
panies, based on back-of-the-envelope calculations3. The
3Energy in Wh ≈ n·(Pidle+(Ppeak −Pidle)·U +(P U E−1)·Ppeak )·
365 · 24, where: n is server count, Ppeak is server peak power in
Watts, Pidle is idle power, and U is average server utilization.
124RTO
ISONE
Region
New England Boston (MA-BOS), Maine (ME),
Some Regional Hubs
NYISO
New York
PJM
Eastern
MISO
Midwest
CAISO
ERCOT Texas
California
Connecticut (CT)
NYC, Albany (CAPITL), Buﬀalo
(WEST), PJM import (PJM)
Chicago (CHI), Virgina (DOM),
New Jersey (NJ)
Peoria (IL), Minnesota (MN),
Indiana (CINERGY)
Palo Alto (NP15), LA (SP15)
Dallas (N), Austin (S)
Figure 2: The diﬀerent regions studied in this paper.
The listed hubs provide a sense of RTO coverage and a
reference to map electricity market location identiﬁers
(hub NP15) to real locations (Palo Alto).
server numbers are from public disclosures for eBay [12] and
Rackspace (Q1 2009 earnings report). To calculate energy,
we have made the following assumptions: average data cen-
ter power usage eﬀectiveness (PUE)4 is 2.0 [3] and is cal-
culated based on peak power; average server utilization is
around 30% [6, 7]; average peak server power usage is 250
Watts (based on measurements of actual servers at Akamai);
and idle servers draw 60-75% of their peak power [5, 8]. Our
numbers for Microsoft are based on company statements [13]
and energy ﬁgures mentioned in a promotional video [14].
To estimate Google’s power consumption, we assumed
500K servers (based on an old, widely circulated number
[13]), operating at 140 Watts each [5], a PUE of 1.3 [4] and
average utilization around 30% [6]. Such a system would
consume more than 6.3 × 105 MWh, and would incur an an-
nual electricity bill of nearly $38 million (at $60 per MWh
wholesale rate). These numbers are consistent with an in-
dependent calculation we can make. comScore estimated
that Google performed about 1.2B searches/day in August
2007 [15], and Google oﬃcially stated recently that each
search takes 1 kJ of energy on average (presumably amor-
tized to include indexing and other costs). Thus, search
alone works out to 1 × 105 MWh in 2007. Google’s servers
handle GMail, YouTube, and many other applications, so
our earlier estimates seem reasonable. Google may well have
more than a million servers [1], so an annual electric bill ex-
ceeding $80M wouldn’t be surprising.
Akamai’s electricity costs represent indirect costs not seen
by the company itself. Like others who rely on co-location
facilities, Akamai seldom pays directly for electricity. Power
is mostly built into the billing model, with charges based on
provisioned capacity rather than consumption.
In section
7 we discuss why our ideas are relevant even to those not
directly charged per-unit of electricity they use.
2.2 Wholesale Electricity Markets
Although market details diﬀer regionally, this section pro-
vides a high-level view of deregulated electricity markets,
providing a context for the rest of the paper. The discus-
sion is based on markets in the United States.
Generation. Electricity is produced by government util-
ities and independent power producers from a variety of
sources. In the United States, coal dominates (nearly 50%),
followed by natural gas (∼20%), nuclear power (∼20%), and
hydroelectric generation (6%) [16].
4A measure of data center energy eﬃciency.
Diﬀerent regions may have very diﬀerent power genera-
tion proﬁles. For example, in 2007, hydroelectric sources
accounted for 74% of the power generated in Washington
state, while in Texas, 86% of the energy was generated us-
ing natural gas and coal.
Transmission. Producers and consumers are connected
to an electric grid, a complex network of transmission and
distribution lines. Electricity cannot be stored easily, so
supply and demand must continuously be balanced.
In addition to connecting nearby nodes, the grid can be
used to transfer electricity between distant locations. The
United States is divided into eight reliability regions, with
varying degrees of inter-connectivity. Congestion on the
grid, transmission line losses (est. 6% [17] in 2006), and
boundaries between regions introduce distribution ineﬃcien-
cies and limit how electricity can ﬂow.
Market Structure. In each region, a pseudo-government-
al body, a Regional Transmission Organization (RTO), man-
ages the grid (ﬁgure 2). An RTO provides a central author-
ity that sets up and directs the ﬂow of electricity between
generators and consumers over the grid. RTOs also provide
mechanisms to ensure the short-term reliability of the grid.
Additionally, RTOs administer wholesale electricty mar-
kets. While bilateral contracts account for the majority of
the electricity that ﬂows over the grid, wholesale electric-
ity trading has been growing rapidly, and presently covers
about 40% of total electricity.
Wholesale market participants can trade forward contracts
for the delivery of electricity at some speciﬁed hour. In or-
der to determine prices for these contracts, RTOs such as
PJM use an auctioning mechanism: power producers present
supply oﬀers (possibly price sensitive), consumers present
demand bids (possibly price sensitive); and a coordinating
body determines how electricity should ﬂow and sets prices.
The market clearing process sets hourly prices for the dif-
ferent locations in the market. The outcomes depend not
only on bids and oﬀers, but also account for a number of
constraints (grid-connectivity, reliability, etc.).
Each RTO operates multiple parallel wholesale markets.
There are two common market types:
Day-ahead markets (futures) provide hourly prices for
delivery during the following day. The outcome is
based on expected load5.
Real-time markets (spot) are balancing markets where
prices are calculated every ﬁve minutes or so, based on
actual conditions, rather than expectations. Typically,
this market accounts for a small fraction of total energy
transactions (less than 10% of total in NYISO).
Generally speaking, the most expensive active generation
resource determines the market clearing price for each hour.
The RTO attempts to meet expected demand by activating
the set of resources with the lowest operating costs. When
demand is low, the base-load power plants, such as coal and
nuclear can fulﬁll it. When demand rises, additional re-
sources, such as natural gas turbines, need to be activated.
Security constraints, line losses and congestion costs also
impact price. When transmission system restrictions, such
as line capacities, prevent the least expensive energy sup-
plier from serving demand, congestion is said to exist. More
5Hour-ahead markets, not discussed here, are analogous.
125 150
 100
 50
h
W
M
$