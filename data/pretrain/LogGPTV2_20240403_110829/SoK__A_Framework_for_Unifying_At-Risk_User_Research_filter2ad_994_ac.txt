(cid:32) (cid:32)
(cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:32)
(cid:32)
Population Category
Children
Teens
Foster teens
Older adults
Activists
Activists × Transgender people
People involved with political campaigns × US
Teachers
Journalists
Sex workers
ER staﬀ
NGO staﬀ
Crowd workers
LGBTQ+ people
LGBTQ+ people × With HIV
Marginalized racial group × US
People with an illness
Older adults × With cognitive impairments
People with visual impairments
People with other or multiple disabilities
Non-Western culture × Women
Developing regions × Older adults
Developing regions × Low SES
Developed regions × Low SES
Developed regions × Low SES × Marginalized racial group
Undocumented immigrants
Refugees
People involved with armed conﬂict
Survivors of sexual assault
Survivors of intimate partner abuse
Survivors of traﬃcking
TABLE I: Meta-analysis showing how the contextual risk factors from our at-risk framework apply to categories of populations in our
dataset. A black circle means one or more citations indicated the contextual risk factor was relevant. Note that the absence of a black circle
does not mean the population does not have that digital-safety risk. Rather, it means the set of papers cited did not discuss or explore that
risk factor as deﬁned in this framework.
Citations
[37, 39, 47, 52–54, 72, 74, 77, 124]
[37–39, 71, 118–120]
[12]
[36, 45, 73, 78]
[9, 25, 49, 59, 101]
[56]
[21]
[53]
[30, 67–70]
[13, 100]
[99]
[19, 55]
[122]
[15, 16, 20, 41, 56, 91]
[113, 114]
[104]
[81, 87, 94]
[14, 22, 36, 66, 76]
[5–7, 32, 43, 112]
[61, 82]
[8, 28, 88, 89, 109]
[46]
[2–4, 75, 86, 108]
[50, 83, 84, 97, 110, 115, 117]
[31]
[40]
[96]
[95]
[10, 79]
[18, 33–35, 42, 63]
[19]
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32) (cid:32) (cid:32) (cid:32)
(cid:32)
(cid:32)
(cid:32) (cid:32) (cid:32) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
Attackers may target at-risk populations with online hate
and harassment due to their beliefs, identity, or social status,
such as people who are LGBTQ+ [15, 16, 41, 56, 91, 113,
114], undocumented immigrants [40], marginalized racial and
ethnic groups [31, 104], or people with low socioeconomic
status (SES) [31, 50]. These threats were typiﬁed by a broad
set of potential attackers, and contending with hate and harass-
ment led to emotional distress [16, 104, 109]. For this factor,
past research has described the perception that anyone may be
a potential attacker [91], exacerbated by the fact that attackers
may hide behind anonymous identities [88, 104].
Harassment experienced by marginalized populations can be
subtle or even unintentional. For example, in To et al. [104],
people in marginalized racial and ethnic groups have reported
experiencing widespread microaggressions pertaining to their
race, via their interpersonal interactions with others on- and
oﬄine. Given the prevalence of this threat, some populations
may be reluctant to fully participate online due to the risk of
revealing a marginalized characteristic [10, 13, 35, 35, 79, 94,
113, 114].
Given that this factor can be associated with stigmatized
information, attackers may also coerce at-risk users by threat-
ening to leak information that could be harmful. This can lead
to reputation damage [109, 113] or sexual violence [40, 88].
For example, attackers may threaten to “out” transgender
individuals [56] or in conservative regions, may threaten to
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
42347
leak chat logs between a man and a woman to damage the
woman’s reputation [88, 109]. Algorithmic bias may also
create or exacerbate digital-safety risks in this space. For ex-
ample, automated gender recognition systems may misgender
transgender people [41].
Social norms. Social norms are informal rules that govern
behavior in society. Some norms can particularly restrict op-
tions for members of an at-risk population, leading to increased
digital-safety risks when there is a mismatch between the
expectations of technology creators and the lived experiences
of that population.
For example, technology creators may assume that devices
and accounts are personal and private. However, device-
sharing norms mean that this assumption may not hold for a
variety of at-risk populations, limiting the privacy protections
aﬀorded by private personal devices and accounts. For exam-
ple, Sambasivan et al. and Ahmed et al. found that women in
South Asia were expected to share devices or accounts with
family members [2, 89]. Similar device sharing norms were
also found among rural women in Greenland [117].
Technology creators may also assume that users understand
implicit norms around when it is appropriate (or dangerous) to
share personal information. However, some at-risk populations
face changing norms, which can lead to unexpected threats.
For example, alongside numerous other challenges, refugees
must adapt to the technology norms and associated digital-
safety risks of their new country of residence. For refugees
in the U.S. speciﬁcally, certain types of information, such as
Social Security numbers, were more sensitive than expected,
placing refugees at
increased risk of ﬁnancial harm until
they learned when it was appropriate to disclose this type of
information [96].
B. Relationships
The second set of risk factors is driven by the relationships
of at-risk users, including direct relationships with an attacker
and relationships with a third party. These risks tended to be
associated with focused targeting, in which attackers pursue
speciﬁc at-risk users (often with intense motivation).
Relationship with the attacker. A personal relationship
with an attacker can lead to heightened risk of what Levy
and Schneier [57] classify as “intimate threats.” It can be
particularly diﬃcult for individuals targeted by intimate threats
to prevent and detect harm, because the attackers are likely
to have physical access to the target’s devices and accounts,
implicit or explicit authority over the target, and potentially
detailed knowledge about the target they can leverage [57].
Intimate threats can involve a wide range of digital-safety
attacks including surveillance, device or account compromise,
destruction of data or devices, harassment, and more [57].
For example, survivors of intimate partner abuse [33, 35,
42, 63] were reported as often facing relentless attacks from
abusers aiming to limit the survivor’s autonomy. These abusers
may have physical or digital access to the survivors’ devices
and accounts,
including surveillance via coercive physical
access [63] or spyware [18, 35]. Similarly, survivors of traﬃck-
ing reported worrying about being recaptured because, prior to
escape, the traﬃcker had full access to the survivor’s digital
life [19]. Stalking attacks are also often characterized by a
relationship with the attacker, as “the majority of stalkers
are ...obsessionally focused on a speciﬁc person with whom
they have had some previous relationship,” often an intimate
one [26]. Workers on online crowdsourcing platforms also
experienced a severe power imbalance with requesters on those
platforms, who could decide whether or not to pay workers
based on their responses, and could gather detailed information
on them via completed tasks [122].
Reliance on a third party. Individuals may be at risk due
to their reliance on a third party for safety-focused care or
help with essential tasks. Unlike intimate threats associated
with a relationship with the attacker,
the third parties in
this context typically have helpful or safety-focused reasons
for “privacy invasions” [57]. However, even with generally
supportive intentions, these invasions can increase the indi-
vidual’s digital-safety risks due to reduced privacy (from the
third party), leading at-risk users to feel uncomfortable or that
their autonomy is limited [76]. These types of risks can also
increase the attack surface (e.g., through the third-party), or
leave at-risk users vulnerable to attackers that impersonate the
third party.
For example, children and teens often shared their devices
and personal data with parents or caregivers [52, 72], or had
applications or privacy settings enabled that allowed parental
monitoring [38, 39, 118]. People with visual impairments sent
photos to crowdsourced services for object identiﬁcation, but
also worried that there might be sensitive content in the photos
(e.g., credit card numbers) [7]. Older adults with cognitive
impairments depended on help from caregivers for digital
tasks [36, 76]. Refugees relied on case workers for processing
personal data, applying to jobs, or applying for support [96].
Access to other at-risk users. Having access to another at-
risk person or population (the “primary target”) can also put
someone at greater risk of focused, stepping-stone attacks
that ultimately aim to harm the primary target. For example,
children of survivors of intimate partner abuse may be targeted
by abusers in order to regain access to the survivor [63],
journalists may be targeted to try to gain access to their
sources [30, 67–70], and elementary school teachers may be
targeted to gain access to their students [53].
C. Personal circumstances
Our third set of contextual risk factors cover personal cir-
cumstances that can increase risk, such as public prominence
or socioeconomic constraints. All of these factors tended to
involve diﬀuse targeting of groups, except prominence, which
involved focused targeting of speciﬁc individuals.
Prominence. At-risk users who stand out in a population,
because they are well-known publicly or have noticeable at-
tributes (e.g., accomplishments, outspokenness, attractiveness,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
52348
etc.), may face heightened risk associated with their promi-
nence. Prominence can expose individuals to new attacks, like
focused targeting. Celebrities, for example, may experience
parasocial relationships with audiences, increasing the risk of
stalking or personal information leaking [121].
Prominence may also exacerbate risks associated with legal
or political factors or marginalization, a dynamic identiﬁed
for prominent journalists [30, 67–69], politicians [21], NGO
staﬀ [55], and activists [25, 59]. In these cases, the level
of prominence inﬂuenced the severity of risks. For example,
people involved in a national U.S. political campaign were
more at risk of focused attacks than campaigns receiving only
state or local attention [21].
Resource or time constrained. Populations can experience
elevated digital-safety risks and decreased ability to respond to
digital-safety threats if they have limited access to technology
or other resources (e.g., money, devices, connectivity), or
limited ability to set aside time to cope with risks. At-risk users
experiencing this risk factor face constraints that go beyond
what is typical. Individuals with low-SES, for example, may
not be able to aﬀord private personal devices, or may need
to rely on old devices that can no longer receive security
updates [31, 97, 108, 110].
At-risk users can also face extraordinary time constraints.
Political workers, for example, operated within extremely
limited election timelines, which made it challenging to set
up the security infrastructure needed to counter nation-state at-
tackers [21]. Hospital emergency departments similarly “have
strong availability demands ... and must provide services as
quickly as possible” [99], leading to password reuse and use
of unsecured personal devices in a sensitive work context.
Underserved accessibility needs. Some at-risk users have
accessibility needs that are underserved by current technology,
contributing to their digital-safety risks. In our dataset, this
included accessibility needs due to a disability, neurodiversity,
a language barrier, or developmental maturity. Inaccessible
technology can cause anxiety about, and susceptibility to,
potential attacks.
Members of populations experiencing this risk factor have
described general anxiety about falling prey to “hackers” or
vague bad actors, as well as worries about their ability to
eﬀectively protect themselves with existing technology. For
example, some older adults reported asking trusted sources
for digital-safety help because they did not feel conﬁdent
protecting themselves [36]. This can stem from negative past
experiences or from inaccessible language in online resources
about digital safety [78]. Assumptions built into systems were
unrealistic for older adults with mild cognitive impairments,
who were sometimes unable to remember passwords and other
crucial
they had trouble
remembering whether they made a particular purchase [66],
making it diﬃcult to diﬀerentiate an attack from a memory
lapse.
Similarly, people with disabilities may struggle with a lack
of accessible technology for some tasks [5–7, 32, 43, 61, 112].
information [76]. In some cases,
For example, people with visual impairments reported to Wang
et al. the conﬂict between the need to use screen readers in
public and concerns about eavesdropping and safety [112].
Additionally, refugees with developing English skills re-
ported struggling with language accessibility, ﬁnding it dif-
ﬁcult to distinguish legitimate callers from scammers when
the call was in English [96]. Zhao et al. found that children
do not completely understand certain online privacy risks due
to their age and development [124].
Access to a sensitive resource. Access to a sensitive resource
(e.g., sensitive data, credentials, money) can increase the risk
of attacks aimed at co-opting this access. In most cases, the
at-risk users’ professional activities provided them privileged
access to these resources.
For example, emergency department staﬀ may be targeted
for their access to patient medical data [99], journalists for
their access to original source material, such as legal docu-
ments or ﬁnancial records [30, 67, 68], and executive staﬀ for
their ability to approve wire transfers [27].
D. How do contextual risk factors interact?
Most at-risk populations in Table I experienced more than
one contextual risk factor. Each factor contributed to risk on
its own, but risk factors also combined to yield new digital-
safety risks or amplify existing risks. Thus, we argue that
technology creators and researchers should consider all of an
at-risk population’s risk factors together when possible. This is
related to prior work on intersectionality [24, 93], which con-
siders how multiple marginalized identities or circumstances
can combine to create unique modes of discrimination. We
provide an illustrative list of examples from our thematic
analysis below, chosen because they represented experiences