### References

1. DU, P., SUN, Z., CHEN, H., CHO, J.-H., AND XU, S. Statistical Estimation of Malware Detection Metrics in the Absence of Ground Truth. *arXiv e-prints* (Sept. 2018), arXiv:1810.07260.

2. FAMOYE, F. Restricted Generalized Poisson Regression Model. *Communications in Statistics-Theory and Methods* 22, 5 (1993), 1335–1354.

3. HUANG, W., AND STOKES, J. W. MTNet: A Multi-Task Neural Network for Dynamic Malware Classification. In *International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment* (2016), Springer, pp. 399–418.

4. HUANG, Y., WANG, W., AND WANG, L. Unconstrained Multimodal Multi-Label Learning. *IEEE Transactions on Multimedia* 17, 11 (2015), 1923–1935.

5. IOFFE, S., AND SZEGEDY, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. *arXiv preprint* arXiv:1502.03167 (2015).

6. JADERBERG, M., SIMONYAN, K., VEDALDI, A., AND ZISSERMAN, A. Deep Structured Output Learning for Unconstrained Text Recognition. *arXiv preprint* arXiv:1412.5903 (2014).

7. KINGMA, D. P., AND BA, J. Adam: A Method for Stochastic Optimization. *arXiv preprint* arXiv:1412.6980 (2014).

8. KINGMA, D. P., MOHAMED, S., REZENDE, D. J., AND WELLING, M. Semi-Supervised Learning with Deep Generative Models. In *Advances in Neural Information Processing Systems* (2014), pp. 3581–3589.

9. KUMAR, A., AND DAUME III, H. Learning Task Grouping and Overlap in Multi-Task Learning. *arXiv preprint* arXiv:1206.6417 (2012).

10. MCCULLAGH, P. *Generalized Linear Models*. Routledge, 2018.

11. RANJAN, R., PATEL, V. M., AND CHELLAPPA, R. HyperFace: A Deep Multi-Task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2017).

12. RASMUS, A., BERGLUND, M., HONKALA, M., VALPOLA, H., AND RAIKO, T. Semi-Supervised Learning with Ladder Networks. In *Advances in Neural Information Processing Systems* (2015), pp. 3546–3554.

13. RUDD, E., ROZSA, A., GUNTHER, M., AND BOULT, T. A Survey of Stealth Malware: Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions. *IEEE Communications Surveys & Tutorials* 19, 2 (2017), 1145–1172.

14. RUDD, E. M., GÜNTHER, M., AND BOULT, T. E. Moon: A Mixed Objective Optimization Network for the Recognition of Facial Attributes. In *European Conference on Computer Vision* (2016), Springer, pp. 19–35.

15. RUDD, E. M., HARANG, R., AND SAXE, J. Meade: Towards a Malicious Email Attachment Detection Engine. *arXiv preprint* arXiv:1804.08162 (2018).

16. RUDER, S., BINGEL, J., AUGENSTEIN, I., AND SØGAARD, A. Sluice Networks: Learning What to Share Between Loosely Related Tasks. *stat* 1050 (2017), 23.

17. SAXE, J., AND BERLIN, K. Deep Neural Network Based Malware Detection Using Two-Dimensional Binary Program Features. In *Malicious and Unwanted Software (MALWARE)*, 2015 10th International Conference on (2015), IEEE, pp. 11–20.

18. SAXE, J., HARANG, R., WILD, C., AND SANDERS, H. A Deep Learning Approach to Fast, Format-Agnostic Detection of Malicious Web Content. *arXiv preprint* arXiv:1804.05020 (2018).

19. SRIVASTAVA, N., HINTON, G., KRIZHEVSKY, A., SUTSKEVER, I., AND SALAKHUTDINOV, R. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. *The Journal of Machine Learning Research* 15, 1 (2014), 1929–1958.

20. VAPNIK, V., AND IZMAILOV, R. Learning Using Privileged Information: Similarity Control and Knowledge Transfer. *Journal of Machine Learning Research* 16, 2023-2049 (2015).

21. VAPNIK, V., AND VASHIST, A. A New Learning Paradigm: Learning Using Privileged Information. *Neural Networks* 22, 5-6 (2009), 544–557.

22. WEISS, G. M. Mining with Rarity: A Unifying Framework. *ACM Sigkdd Explorations Newsletter* 6, 1 (2004), 7–19.

23. WU, F., WANG, Z., ZHANG, Z., YANG, Y., LUO, J., ZHU, W., AND ZHUANG, Y. Weakly Semi-Supervised Deep Learning for Multi-Label Image Annotation. *IEEE Trans. Big Data* 1, 3 (2015), 109–122.

24. XU, C., TAO, D., AND XU, C. A Survey on Multi-View Learning. *arXiv preprint* arXiv:1304.5634 (2013).

### Appendix A: Dataset Statistics

#### A.1 Vendor Counts Distribution
To better characterize the distribution of the vendor counts auxiliary target for our Poisson loss experiment, we plot a histogram representing the distribution of the number of vendor convictions in Figure A.1. The x-axis depicts the number of vendors that identify a given sample as malicious, while the y-axis represents the number of samples for which that number of detections was observed in our training dataset (note the logarithmic scale). The statistics of the test and validation datasets are similar and not shown here due to space considerations.

We observe a peak at zero detections, accounting for the majority of the benign files. Most of the samples considered malicious by our labeling scheme have more than 20 individual detections out of 67 total vendors, with a peak around 57 detections.

**Figure A.1:** Histogram of vendor detections per file. Files with zero or one detections (green bars) are considered benign under our labeling scheme, samples with two, three, or four vendor detections (gray bars) are considered gray files, and files with more than four detections (red bars) are considered malicious.

#### A.2 Individual Vendor Responses
Table A.1 summarizes the number of samples identified as malware, benign, and the number of missing samples per vendor for the nine vendors used to compute the auxiliary per-vendor malware loss.

In Figure A.2, we plot the pairwise similarity of the predictions between vendors. The value in the j, k position of the matrix is the fraction of samples for which the predictions of vendor j are equal to the predictions for vendor k. Even though the predictions by each vendor are created in a quasi-independent manner, they tend to agree for most of the samples. The diagonal elements of the matrix indicate the fraction of samples for which we have a classification by the vendor (fraction of non-missing values).

| **Vendor** | **Malware** | **Benign** | **None** |
|------------|-------------|------------|----------|
| v1         | 13,752,004 (69%) | 6,110,180 (31%) | 20,979 (<1%) |
| v2         | 14,751,413 (74%) | 5,122,728 (26%) | 9,022 (<1%) |
| v3         | 14,084,689 (71%) | 5,713,116 (29%) | 85,358 (<1%) |
| v4         | 14,438,043 (73%) | 5,239,896 (26%) | 205,224 (1%) |
| v5         | 13,778,367 (69%) | 5,922,859 (30%) | 181,937 (1%) |
| v6         | 15,065,196 (76%) | 4,704,695 (24%) | 113,272 (1%) |
| v7         | 14,935,624 (75%) | 4,927,436 (25%) | 20,103 (<1%) |
| v8         | 12,704,512 (64%) | 7,009,855 (35%) | 168,796 (1%) |
| v9         | 14,234,545 (72%) | 5,613,604 (28%) | 35,014 (<1%) |

**Table A.1:** Individual vendor counts for files in the training set identified as malicious, benign, or missing value for the set of nine vendors used in the per-vendor malware loss.

**Figure A.2:** Vendor predictions similarity matrix. Each entry in the matrix represents the percentage of samples that are the same for any two vendors. The elements in the diagonal of the matrix represent the percentage of the samples for which predictions from the vendor are present (i.e., not missing). Note that diagonal values of less than 1.0 are due to missing labels (compare to the final column of Table A.1), which we treat as disagreeing with any label.

#### A.3 Semantic Tags Distribution
In this section, we analyze the distribution of the semantic tags over three sets of samples:
1. Samples in the training set.
2. Samples in the test set.
3. Those which the baseline model classifies incorrectly but our model trained with all targets classifies correctly either as malicious or benign samples.

The percentages in Table A.2 represent the number of samples in each set labeled with a given tag. The total number of samples in the test set for which the improved model makes correct conviction classification but the baseline model fails is 665,944. The binarization of the predictions for the baseline and the final model was done such that each would have an FPR of \(10^{-3}\) in the test set. As shown below, those samples with the adware tag are the ones that benefit the most from the addition of auxiliary losses during training, but we also see notable improvements on packed samples, spyware, and droppers.

| **Tag** | **Train Set** | **Test Set** | **Improvement over Baseline** |
|---------|---------------|--------------|------------------------------|
| Adware  | 21%           | 18%          | 41%                          |
| Crypto-Miner | 7%            | 2%           | 1%                           |
| Downloader | 25%           | 18%          | 11%                          |
| Dropper | 29%           | 22%          | 17%                          |
| File-Infector | 19%          | 12%          | 9%                           |
| Flooder | 1%            | 1%           | <1%                          |
| Installer | 7%            | 1%           | 5%                           |
| Packed | 34%           | 25%          | 19%                          |
| Ransomware | 5%            | 6%           | 1%                           |
| Spyware | 40%           | 25%          | 18%                          |

**Table A.2:** Tag statistics for three sets of interest: train set; test set; and the set of samples for which the full model classifies correctly but the baseline model fails.

### Appendix B: Gray Samples Evaluation
In Section 3.6, we observed that 2.5% of the samples in our training set and 3.7% of the samples in our test set are considered gray samples by our labeling function. While training, this is not necessarily an issue since we can assign a weight of zero for those samples in their malware/benign label, as noted in Section 3.5. For the evaluation of the detection algorithms, however, the performance on these samples becomes more relevant.

To evaluate how our proposed detection model performs on these samples, we re-scanned a random selection of 10,000 gray samples in the test set five months after the original collection. From these, 5,000 were predicted by the model as benign and 5,000 as malicious. We expect, after this time-lag, that, with updated detection rules from the AV community, samples originally labeled as "gray" that are effectively malicious will accrue additional detections, and samples originally labeled as "gray" that are effectively benign will accrue fewer detections as vendors have rewritten their rules to suppress false positives and recognize false negatives. Thus, the gray sample labels will tend to converge to either malicious or benign under our 1-/5+ criterion. Out of these 10,000 rescans, we were able to label 5,653 gray samples: 3,877 (68.6%) as malicious and 1,776 (31.4%) as benign.

**Figure B.1:** Mean and standard deviation ROC curve over re-scanned samples.

### Appendix C: Relative Improvements
In Table C.1, we present the relative percentage reduction both in true positive detection error and standard deviation with respect to the baseline model trained only using the malware/benign target for various values of false positive rates (FPRs).

| **FPR** | **Poisson** | **RG Poisson** | **Vendors** | **Tags** | **All Targets** |
|---------|-------------|----------------|-------------|----------|-----------------|
| \(10^{-5}\) | 38.05, 61.84 | 0.00, -52.63 | 47.12, 55.26 | 43.63, 64.47 | 53.75, 81.58 |
| \(10^{-4}\) | 30.19, 30.61 | 6.17, 16.33 | 32.47, 51.02 | 32.47, 81.63 | 37.01, 65.31 |
| \(10^{-3}\) | 28.68, 48.39 | 4.41, 48.39 | 18.38, 35.48 | 8.09, 29.03 | 42.65, 87.10 |
| \(10^{-2}\) | 14.29, 85.71 | 2.86, 57.14 | 14.29, 42.86 | 17.14, 42.86 | 20.00, 57.14 |
| \(10^{-1}\) | 5.56, 97.14 | 2.78, 95.71 | 0.00, 80.00 | 5.56, 88.57 | 8.33, 94.29 |

**Table C.1:** Relative percentage reductions in true positive detection error and standard deviation compared to the baseline model (displayed as detection error reduction, standard deviation reduction) at different false positive rates (FPRs) for the different experiments in Section 4. Results were evaluated over five different weight initializations and minibatch orderings. Best detection error reduction consistently occurred when using all auxiliary losses. Best results are shown in bold.