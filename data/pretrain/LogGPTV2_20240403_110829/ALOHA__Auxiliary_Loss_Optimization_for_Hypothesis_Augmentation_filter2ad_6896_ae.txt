[9] DU, P., SUN, Z., CHEN, H., CHO, J.-H., AND XU,
S. Statistical Estimation of Malware Detection Metrics
in the Absence of Ground Truth. arXiv e-prints (Sept.
2018), arXiv:1810.07260.
[10] FAMOYE, F. Restricted generalized poisson regres-
sion model. Communications in Statistics-Theory and
Methods 22, 5 (1993), 1335–1354.
[11] HUANG, W., AND STOKES, J. W. Mtnet: a multi-
task neural network for dynamic malware classiﬁca-
tion. In International Conference on Detection of In-
trusions and Malware, and Vulnerability Assessment
(2016), Springer, pp. 399–418.
[12] HUANG, Y., WANG, W., AND WANG, L. Uncon-
strained multimodal multi-label learning. IEEE Trans-
actions on Multimedia 17, 11 (2015), 1923–1935.
[13] IOFFE, S., AND SZEGEDY, C. Batch normalization:
Accelerating deep network training by reducing inter-
nal covariate shift. arXiv preprint arXiv:1502.03167
(2015).
[14] JADERBERG, M., SIMONYAN, K., VEDALDI, A.,
AND ZISSERMAN, A. Deep structured output learn-
ing for unconstrained text recognition. arXiv preprint
arXiv:1412.5903 (2014).
[15] KINGMA, D. P., AND BA,
J.
method for stochastic optimization.
arXiv:1412.6980 (2014).
Adam: A
arXiv preprint
[16] KINGMA, D. P., MOHAMED, S., REZENDE, D. J.,
AND WELLING, M. Semi-supervised learning with
deep generative models. In Advances in neural infor-
mation processing systems (2014), pp. 3581–3589.
[17] KUMAR, A., AND DAUME III, H. Learning task
arXiv
grouping and overlap in multi-task learning.
preprint arXiv:1206.6417 (2012).
[18] MCCULLAGH, P. Generalized linear models. Rout-
ledge, 2018.
[19] RANJAN, R., PATEL, V. M., AND CHELLAPPA, R.
Hyperface: A deep multi-task learning framework for
face detection, landmark localization, pose estimation,
and gender recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2017).
[20] RASMUS, A., BERGLUND, M., HONKALA, M.,
Semi-supervised
VALPOLA, H., AND RAIKO, T.
In Advances in Neu-
learning with ladder networks.
ral Information Processing Systems (2015), pp. 3546–
3554.
[21] RUDD, E., ROZSA, A., GUNTHER, M., AND BOULT,
T. A survey of stealth malware: Attacks, mitigation
measures, and steps toward autonomous open world so-
lutions. IEEE Communications Surveys & Tutorials 19,
2 (2017), 1145–1172.
[22] RUDD, E. M., G ¨UNTHER, M., AND BOULT, T. E.
Moon: A mixed objective optimization network for the
In European Confer-
recognition of facial attributes.
ence on Computer Vision (2016), Springer, pp. 19–35.
[23] RUDD, E. M., HARANG, R., AND SAXE, J. Meade:
Towards a malicious email attachment detection en-
gine. arXiv preprint arXiv:1804.08162 (2018).
316    28th USENIX Security Symposium
USENIX Association
[24] RUDER12, S., BINGEL, J., AUGENSTEIN, I., AND
SØGAARD, A. Sluice networks: Learning what to
share between loosely related tasks. stat 1050 (2017),
23.
[25] SAXE, J., AND BERLIN, K. Deep neural network
based malware detection using two dimensional binary
In Malicious and Unwanted Soft-
program features.
ware (MALWARE), 2015 10th International Confer-
ence on (2015), IEEE, pp. 11–20.
[26] SAXE, J., HARANG, R., WILD, C., AND SANDERS,
H. A deep learning approach to fast, format-agnostic
detection of malicious web content. arXiv preprint
arXiv:1804.05020 (2018).
[27] SRIVASTAVA, N., HINTON, G., KRIZHEVSKY, A.,
SUTSKEVER, I., AND SALAKHUTDINOV, R. Dropout:
a simple way to prevent neural networks from overﬁt-
ting. The Journal of Machine Learning Research 15, 1
(2014), 1929–1958.
[28] VAPNIK, V., AND IZMAILOV, R. Learning using priv-
ileged information: similarity control and knowledge
Journal of machine learning research 16,
transfer.
2023-2049 (2015), 2.
[29] VAPNIK, V., AND VASHIST, A. A new learning
paradigm: Learning using privileged information. Neu-
ral networks 22, 5-6 (2009), 544–557.
[30] WEISS, G. M. Mining with rarity: a unifying frame-
ACM Sigkdd Explorations Newsletter 6, 1
work.
(2004), 7–19.
[31] WU, F., WANG, Z., ZHANG, Z., YANG, Y., LUO, J.,
ZHU, W., AND ZHUANG, Y. Weakly semi-supervised
deep learning for multi-label image annotation. IEEE
Trans. Big Data 1, 3 (2015), 109–122.
[32] XU, C., TAO, D., AND XU, C. A survey on multi-view
learning. arXiv preprint arXiv:1304.5634 (2013).
A Dataset Statistics
A.1 Vendor Counts Distribution
To better characterize the distribution of the Vendor Counts
auxiliary target for our Poisson loss experiment, we plot a
histogram representing the distribution of the number of ven-
dor convictions in Figure A.1. The x-axis depicts the number
of vendors that identify a given sample as malicious, while
the y-axis represents the number of samples for which that
number of detections was observed in our training dataset
(note the logarithmic scale). The statistics of the test and val-
idation datasets are similar and not shown here due to space
considerations.
We note that there is a peak at zero detections, accounting
for the majority of the benign ﬁles. Most of the samples
considered malicious by our labeling scheme have more than
20 individual detections out of 67 total vendors considered,
with a peak around 57 detections.
Figure A.1: Histogram of vendor detections per ﬁle. Files
with zero or one detections (green bars) are considered be-
nign under our labeling scheme, samples with two, three or
four vendor detections (gray bars) are considered gray ﬁles,
and ﬁles with more than four detections (red bars) are con-
sidered malicious.
A.2
Individual Vendor Responses
Table A.1 summarizes the number of samples identiﬁed as
malware, benign and number of missing samples per ven-
dor for the nine vendors used to compute the auxiliary per-
vendor malware loss.
In Figure A.2 we plot the pairwise
similarity of the predictions between vendors. The value in
the j, k position of the matrix is the fraction of samples for
which the predictions of vendor j are equal to the predictions
for vendor k. Even though the predictions by each vendor are
created in a quasi independent manner, they tend to agree for
most of the samples. The diagonal elements of the matrix
USENIX Association
28th USENIX Security Symposium    317
indicate the fraction of samples for which we have a classiﬁ-
cation by the vendor (fraction of non-missing values).
Malware
Benign
v1
v2
v3
v4
v5
v6
v7
v8
v9
13,752,004 (69%)
14,751,413 (74%)
14,084,689 (71%)
14,438,043 (73%)
13,778,367 (69%)
15,065,196 (76%)
14,935,624 (75%)
12,704,512 (64%)
14,234,545 (72%)
6,110,180 (31%)
5,122,728 (26%)
5,713,116 (29%)
5,239,896 (26%)
5,922,859 (30%)
4,704,695 (24%)
4,927,436 (25%)
7,009,855 (35%)
5,613,604 (28%)
None
20,979 (<1%)
9,022 (<1%)
85,358 (<1%)
205,224 (1 %)
181,937 (1 %)
113,272 (1 %)
20,103 (<1%)
168,796 (1 %)
35,014 (<1%)
Table A.1: Individual vendor counts for ﬁles in the training
set identiﬁed as malicious, benign, or missing value for the
set of nine vendors used in the per-vendor malware loss 3.3.
A.3 Semantic Tags Distribution
In this section we analyze the distribution of the semantic
tags over three sets of samples:
i) samples in the training
set; ii) samples in the test set; and iii) those which the base-
line model classiﬁes incorrectly but our model trained with
all targets classiﬁes correctly either as malicious or benign
samples. The percentages in Table A.2 represent the number
of samples in each set labeled with a given tag. The total
number of samples in the test set for which the improved
model makes correct conviction classiﬁcation but the base-
line model fails is 665,944. The binarization of the predic-
tions for the baseline and the ﬁnal model was done such that
each would have a FPR of 10−3 in the test set. As shown be-
low, those samples with the adware tag are the ones that most
beneﬁt from the addition of auxiliary losses during training,
however we also see notable improvements on packed sam-
ples, spyware, and droppers.
B Gray Samples Evaluation
In Section 3.6 we observed that 2.5% of the samples in our
training set, and 3.7% of the samples in our test set are con-
sidered gray samples by our labeling function. While train-
ing this is not necessarily an issue since we can assign a
weight of zero for those samples in their malware/benign
label as noted in Section 3.5. For the evaluation of the
detection algorithms though, the performance on those be-
comes more relevant. To evaluate how our proposed detec-
tion model performs on those samples we re-scanned a ran-
dom selection of 10,000 gray samples in the test set 5 months
later than the original collection. From these, 5,000 were
predicted by the model as benign and 5,000 as malicious.
We expect, after this time-lag, that, with updated detection
Figure A.2: Vendor predictions similarity matrix. Each en-
try in the matrix represents the percentage of samples that are
the same for any two vendors. The elements in the diagonal
of the matrix represent the percentage of the samples for pre-
dictions from the vendor are present (i.e., not missing). Note
that diagonal values of less than 1.0 are due to missing labels
(compare to the ﬁnal column of table A.1), which we treat as
disagreeing with any label.
Train Set Test Set
adware
crypto-miner
downloader
dropper
ﬁle-infector
ﬂooder
installer
packed
ransomware
spyware
21 %
7 %
25 %
29 %
19 %
1 %
7 %
34 %
5 %
40 %
18%
2%
18%
22%
12%
1%
1 %
25%
6%
25%
Improvement
over
baseline
41 %
1 %
11 %
17 %
9 %
<1%
5 %
19 %
1 %
18 %
Table A.2: Tag statistics for three sets of interest: train set;
test set; and the set of samples for which the full model clas-
siﬁes correctly but the baseline model fails.
rules from the AV community, that samples originally la-
beled as “gray” that are effectively malicious will accrue ad-
ditional detections and samples originally labeled as “gray”
that are effectively benign will accrue fewer detections as
vendors have re-written their rules to suppress false positives
318    28th USENIX Security Symposium
USENIX Association
and recognize false negatives. Thus, the gray sample labels
will tend to converge to either malicious or benign under our
1-/5+ criterion. Out of these 10,000 rescans, we were able
to label 5,653 gray samples: 3,877 (68.6%) as malicious and
1,776 (31.4%) as benign.
In Figure B.1 we plot the ROC curve for the predictions
on the re-scanned samples for our ﬁnal model trained with
all targets, which achieves an AUC of 0.84. Even though
the AUC is much lower than the one obtained on our test set,
our model trained with knowledge from 5 months earlier (af-
ter the ﬁrst collection) still correctly predicts more than 77%
of the samples correctly. We measure this by binarizing the
predictions using a threshold that would achieve an FPR of
10−3 on the original test set. Furthermore we note that the
samples we are evaluating on in this case are more difﬁcult
or even ambiguous in nature, to the point that at the origi-
nal collection time there was not consensus across the AV
community.
Figure B.1: Mean and standard deviation ROC curve over
rescanned samples.
USENIX Association
28th USENIX Security Symposium    319
C Relative Improvements
In Table C.1 we present the relative percentage reduction both in true positive detection error and standard deviation with
respect to the baseline model trained only using the malware/benign target for various values of false positive rates.
10−5
38.05, 61.84
0.00, -52.63
47.12, 55.26
43.63, 64.47
53.75, 81.58
10−4
30.19, 30.61
6.17, 16.33
32.47, 51.02
32.47, 81.63
37.01, 65.31
FPR
10−3
28.68, 48.39
4.41, 48.39
18.38, 35.48
8.09, 29.03
42.65, 87.10
10−2
14.29, 85.71
2.86, 57.14
14.29, 42.86
17.14, 42.86
20.00, 57.14
10−1
5.56, 97.14
2.78, 95.71
0.00, 80.00
5.56, 88.57
8.33, 94.29
Poisson
RG Poisson
Vendors
Tags
All Targets
Table C.1: Relative percentage reductions in true positive detection error and standard deviation compared to the baseline
model (displayed as detection error reduction, standard deviation reduction) at different false positive rates (FPRs) for the
different experiments in Section 4. Results were evaluated over ﬁve different weight initializations and minibatch orderings.
Best detection error reduction consistently occurred when using all auxiliary losses. Best results are shown in bold.
320    28th USENIX Security Symposium
USENIX Association