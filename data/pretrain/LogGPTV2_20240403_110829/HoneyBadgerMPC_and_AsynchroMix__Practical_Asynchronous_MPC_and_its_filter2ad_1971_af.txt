correct computations. In ACM CCS. 30â€“41.
Atom: Horizontally Scaling Strong Anonymity. In SOSP. 406â€“422.
Enhancing Technologies (2016), 115â€“134.
Systems (TOCS) 16, 2 (1998), 133â€“169.
Private Messaging Immune to Passive Traffic Analysis. In OSDI. 711â€“725.
ACM Symposium on Symbolic and Algebraic Computation. 260â€“270.
Secure Two-Party Computation System.. In USENIX Security Symposium.
[65] Fabio Massacci, Chan Nam Ngo, Jing Nie, Daniele Venturi, and Julian Williams.
2018. FuturesMEX: secure, distributed futures market exchange. In IEEE Sympo-
sium on Security and Privacy (SP). 335â€“353.
[66] Ian Miers, Christina Garman, Matthew Green, and Aviel D Rubin. 2013. Zerocoin:
Anonymous distributed e-cash from bitcoin. In IEEE Symposium on Security and
Privacy. 397â€“411.
[67] Andrew Miller, Yu Xia, Kyle Croman, Elaine Shi, and Dawn Song. 2016. The
honey badger of BFT protocols. In ACM CCS. 31â€“42.
in public. In International Workshop on Public Key Cryptography. 431â€“448.
[69] Ania M Piotrowska, Jamie Hayes, Tariq Elahi, Sebastian Meiser, and George
Danezis. 2017. The loopix anonymity system. In USENIX Security Symposium.
16â€“18.
[70] Tim Ruffing, Pedro Moreno-Sanchez, and Aniket Kate. 2017. P2P Mixing and
[68] Udaya Parampalli, Kim Ramchen, and Vanessa Teague. 2012. Efficiently shuffling
Unlinkable Bitcoin Transactions. In NDSS.
[71] Adi Shamir. 1979. How to share a secret. Commun. ACM 22, 11 (1979), 612â€“613.
[72] Victor Shoup et al. 2005. NTL, a library for doing number theory.
[73] Nigel P Smart and Tim Wood. 2019. Error Detection in Monotone Span Programs
with Application to Communication-Efficient Multi-party Computation. In CT-
RSA. 210â€“229.
[74] Alexandre Soro and JÃ©rÃ´me Lacan. 2010. FNT-based Reed-Solomon erasure codes.
In 2010 7th IEEE Consumer Communications and Networking Conference. 1â€“5.
[75] Shi-Feng Sun, Man Ho Au, Joseph K Liu, and Tsz Hon Yuen. 2017. RingCT 2.0:
A compact accumulator-based (linkable ring signature) protocol for blockchain
cryptocurrency Monero. In European Symposium on Research in Computer Security.
456â€“474.
[76] Yixin Sun, Anne Edmundson, Laurent Vanbever, Oscar Li, Jennifer Rexford, Mung
Chiang, and Prateek Mittal. 2015. RAPTOR: Routing Attacks on Privacy in Tor.
In USENIX Security Symposium. 271â€“286.
[77] Nirvan Tyagi, Yossi Gilad, Derek Leung, Matei Zaharia, and Nickolai Zeldovich.
2017. Stadium: A Distributed Metadata-Private Messaging System. In 26th SOSP.
423â€“440.
[78] Xiao Wang, Samuel Ranellucci, and Jonathan Katz. 2017. Global-scale secure
[79] David Isaac Wolinsky, Ewa Syta, and Bryan Ford. 2013. Hang with your buddies
multiparty computation. In ACM CCS. 39â€“56.
to resist intersection attacks. In ACM CCS. 1153â€“1166.
compiler for private distributed computation. In ACM CCS. 813â€“826.
[80] Yihua Zhang, Aaron Steele, and Marina Blanton. 2013. PICCO: a general-purpose
A BATCH SECRET SHARING WITH
QUASILINEAR COMPUTATION
DamgÃ¤rd et al. [38, 39] first suggested the use of FFT-based opera-
tions for batch secret sharing, although to our knowledge this has
never been implemented previously. We would naturally expect
quasilinear operations to be necessary when scaling n to extreme
large networks. However, even at the smaller values of n up to 100
that we consider, we investigated whether FFT-based operations
could offer performance improvements.
A.1 Shamir Sharing in FFT-friendly fields
In Section 2 we give a description of Shamir sharing and batch
operations for arbitrary prime-order field Fp, and for arbitrary
evaluation points Î±i. To enable FFT-based operations, we choose
Fp such that 2Îº|p âˆ’ 1, and hence we can find a 2Îº-th root of unity,
Ï‰. Concretely, in our implementation we choose p as the order of
the BLS12-381 elliptic curve, such that 232|p âˆ’ 1, and p â‰ˆ 255 bits.
A.2 Batch secret share operations using FFT
Given a polynomial Ï•(Â·) in coefficient form, it is clear how to use
FFT to evaluate it at points Ï‰i for i < n. The offline phase makes use
of randomness extraction. As mentioned in Section 2, the standard
approach is to perform multiplications by a hyperinvertible matrix
multiplication, such as the Vandermonde matrix. By choosing the
Vandermonde matrix defined by Î±i = Ï‰i, this can be evaluated
efficiently using FFT.
As defined in Section 5, Robust-Interpolate depends on a subrou-
tine to interpolate a polynomial from an arbitrary subset of t + 1
shares. Soro and Lacan [74] give a transformation that relies on
several FFTs and is quasilinear overall. Soro and Lacanâ€™s approach
has a setup cost of ð’ª(n loÐ´
n) which depends on the points we
are interpolating from, and a cost of ð’ª(n loÐ´ n) per interpolation
after that. More specifically, the cost per interpolation consists of
a standard inverse FFT and a polynomial multiplication which is
done using an FFT/CRT based approach by NTL. In A.4 we give a
detailed explanation of this method.
If the first attempt at decoding 2t + 1 received shares fails, we
know there is at least one error, but we donâ€™t know where it is. With
2
Session 4C: Secure Computing IIICCS â€™19, November 11â€“15, 2019, London, United Kingdom900we can make use of the Taylor series expansion 1/(X âˆ’ xi) =
âˆ’
X j. We therefore have
j x
i
Rearranging, we have
j
âˆ’jâˆ’1
i
P(X)/A(X) = âˆ’ t
(cid:169)(cid:173)(cid:171) t
(cid:32) t
P(X)/A(X) = âˆ’ t
P(X)/A(X) = âˆ’ t
i
j
(yi/bi)x
(yi/bi)x
X j(cid:170)(cid:174)(cid:172) mod X t +1
(cid:33)
X j mod X t +1
âˆ’jâˆ’1
i
âˆ’jâˆ’1
i
and finally since xi = Ï‰zi , we can replace each coefficient with a
polynomial evaluation
(2)
where we define the polynomial
N(Ï‰
âˆ’jâˆ’1)X j mod X t +1
j
t
i
N(X) =
(yi/bi)X zi .
(6)
(7)
(8)
(9)
each additional value we wait for, we either identify the error, or
learn the number of errors is one more, in which case we wait for
an additional point. This is known as Online Error Correction [28].
We implement Gaoâ€™s algorithm for Reed Solomon decoding, which
is O(n log n) when using using FFT for polynomial multiplication.
A.3 Vandermonde interpolation
Given t + 1 points ((x0, y0),(x1, y1), . . . ,(xt , yt)) for distinct values
(x0, x1, . . . , xt), polynomial interpolation means finding the lowest
degree polynomial P(X) such that P(xi) = yi. In general, given t +1
points we can always find such a polynomial that is of degree at
most t. Lagrange interpolation is the standard algorithm used for
polynomial interpolation,
t
i
(cid:169)(cid:173)(cid:171)yi
t
j(cid:44)i
X âˆ’ xj
xi âˆ’ xj
(cid:170)(cid:174)(cid:172) .
P(X) =
j
i
P(X) =
2), and is
However, this has a quadratic computational cost of O(t
impractical for large t. An alternative approach to interpolation,
as in HyperMPC [8] for example, is to use matrix multiplication
with the inverse Vandermonde matrix, Mâˆ’1, where Mi, j = x
. To
summarize:
Step 1 (depends only on x0, . . . , xt ):
â€“ Compute the inverse of Mâˆ’1
Step 2 (depends also on y0, . . . , yt ):
â€“ Matrix multiply (a0, . . . , at)T = Mâˆ’1(y0, . . . , yt)T such that
To interpolate a batch of k polynomials at once, we multiply Mâˆ’1
by a matrix of size {t + 1} Ã— k.
A.4 FFT-based interpolation
Here we give a self-contained explanation of the FFT-based poly-
nomial interpolation algorithm from Soro and Lacan [74]. In this
setting we assume the additional constraint that each xi is a power
of Ï‰, a primitive nth root of unity,
i ai X i.
xi = Ï‰zi
zi âˆˆ {0, 1, . . . , n âˆ’ 1}
The goal is to get an expression for P(X) that can be com-
puted within O(n log n) steps depending on y0, . . . , yt , along with
a precomputation phase depending only on x0, . . . , xt . We start by
rewriting Equation (2) as
t
t
i
P(X)/A(X) =
yi/bi
X âˆ’ xi
.
A(X) =
(X âˆ’ xj),
t
j(cid:44)i
j
(xi âˆ’ xj) = A(xi)
xi âˆ’ xj
.
bi =
where we define
and
(3)
(4)
(5)
The degree-t polynomial A(X) as well as each bi depends only
on {xi} and so we compute them explicitly during an initialization
phase. The right hand side is intractable to compute directly, but
To summarize, we can compute P(X) through the following
steps:
Step 1 (depends only on x0, . . . , xt ):
â€“ Compute A(X), {bi}.
Step 2 (depends also on y0, . . . , yt ):
â€“ Compute N(X) from coefficients {yi/bi}.
â€“ Evaluate each N(Ï‰j) using FFT to obtain the coefficients of
P(X)/A(X) mod X t +1.
â€“ Multiply by A(X) to recover P(X).
For interpolation of a batch of k polynomials from shares received
from the same set of t +1 parties, Step 1 can be computed once based
on the party identifiers. Soro and Lacan [74] give an algorithm to
compute this step in O(n log2
n) overall time. Step 2 can clearly be
computed in O(n log n) time, and must be computed for each of
polynomial in the batch.
A.5 Microbenchmarks
We now perform microbenchmarks to evaluate when FFT-based
methods are more performant than Vandermonde matrix multipli-
cations. We consider the following tasks and algorithms:
â‰ˆ O(n logc n)
Task
â‰ˆ O(n
1+c)
Matrix Mul
Matrix Mul
Berlekamp-Welch
Soro-Lacan [74]
FFT
Gao
Encode Shares
Interpolate
RSDecode
We implemented all algorithms in C++ using the NTL library.
Additional details on costs for interpolation, evaluation, matrix
inversions, etc and on methodology are given below.
Timing evaluation algorithms: The core component of eval-
uation using Vandermonde matrices is multiplication of a nÃ—(t +1)
matrix and a (t + 1) Ã— k matrix, where k is the number of poly-
nomials to evaluate. We use NTL for matrix multiplication. We
set k = 8192 to be large enough to estimate the amortized cost
per evaluated polynomial. For FFT-based evaluation, the operation
Session 4C: Secure Computing IIICCS â€™19, November 11â€“15, 2019, London, United Kingdom901Figure 15: Interpolation (Step 2) and Evaluation Micro-
Benchmarks
Figure 16: Interpolation preparation (Step 1) time micro-
benchmarks
consists simply of an FFT applied to each of the k polynomials in
turn. Figure 15 shows the costs of these components.
Timing interpolation algorithms: The interpolation algorithms
both have a setup phase which only depends on the x-coordinates
of the points we are interpolating on. In the context of batch recon-
struction, these coordinates only depend on the first t + 1 parties
we received shares from. Therefore, the setup phase only needs to
be done once within a single round of batch reconstruction. The
primary component of the interpolation algorithms are also depen-
dent on the batch sizes. We time these two parts of all algorithms
separately which helps us accurately predict how our execution
time would vary with both n and the batch size.
Vandermonde-based interpolation and evaluation costs roughly
ð’ª(n
2), while their FFT-counterparts take ð’ª(n loÐ´ n) time. However,
FFT has a relatively large constant behind the big-O notation but is
only better than Vandermonde-based operations at relatively larger
values of n (n â‰¥ 8192). When the costs for matrix inversion, as
shown in Figure 16, are included in the total costs, in practice we
see a cross-over much earlier since matrix inversion.
Total cost for batch reconstruction: Our current implemen-
tation of batch reconstruction requires 3 evaluations and 2 inter-
polations. Additionally, we perform batch size/(t + 1) evaluations /
interpolations per batch. Therefore, the total cost of a single batch
reconstruction is given by
2 Ã— Cost per interpolationÃ—
batch size/(t + 1) + 3Ã—
Cost per evaluation Ã— batch size/(t + 1)
Regions
Virginia
Ohio
Oregon
Frankfurt
Tokyo
Mumbai
Canada
London
Paris
South America
n = 4
1
0
0
0
1
1
1
0
0
0
n = 10
1
1
1
1
1
1
1
1
1
1
n = 16
2
1
2
1
2
1
2