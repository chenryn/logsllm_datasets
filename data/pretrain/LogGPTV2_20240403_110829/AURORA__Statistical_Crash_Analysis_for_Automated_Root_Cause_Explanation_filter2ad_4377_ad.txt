48
50
tance between developer ﬁx and crashing location in terms of
executed assembly instructions. More speciﬁcally, for each
target, we determine the maximum distance, the average dis-
tance over all crashing inputs and—to put this number in
relation—the average of total instructions executed during a
program run. Each metric is given in the number of assembly
instructions executed and unique assembly instructions exe-
cuted, where each instruction is counted at most once. Note
that some bugs only crash in the presence of a sanitizer (as
indicated by ASAN or MSAN in Table 1) and that our tracing
binaries are never instrumented to avoid sanitizer artifacts
disturbing our analysis. As a consequence, our distance mea-
surement would run until normal program termination rather
than program crash for such targets. Since this would distort
the experiment, we exclude such bugs from the comparison.
Finally, to provide an intuition of how well our approach
performs, we analyze the top 50 predicates (if available) pro-
duced for each target, stating whether they are related to the
bug or unrelated false positives. We consider predicates as
related to the bug when they pinpoint a location on the path
from root cause to crashing location and separate crashing and
non-crashing inputs. For false positives, we evaluate various
heuristics that allow to identify them and thereby reduce the
amount of manual effort required.
6.3 Results
Following AURORA’s results, the developer ﬁx will be cov-
ered by the ﬁrst few explanations. Typically, an analyst would
have to inspect less than ten source code lines to identify
the root cause. Exceptions are larger targets, such as Python
(13 MiB) and PHP (31 MiB), or particularly challenging bugs
such as type confusions (#16 and #17). Still, the number of
source code lines to inspect is below 28 for all but the Python
type confusion (#17), which contains a large amount of false
positives. Despite the increased number of source code lines
to investigate, the information provided by AURORA is still
useful: for instance, for bug #16—where 15 lines are needed—
most of the lines are within the same function and only six
functions are identiﬁed as candidates for containing the root
244    29th USENIX Security Symposium
USENIX Association
Table 2: Maximum and average distance between developer ﬁx and crashing location in both all and unique executed assembly instructions. For reference, the
average amount of instructions executed between program start and crash is also provided.
Target
Perl
screen
Python
tcpdump
NASM
Bash
Bash
Python
mruby
Python
NASM
Sleuthkit
#3
#4
#9
#10
#11
#12
#13
#14
#16
#17
#23
#24
unique
7,321
3,441
9,330
1,567
8,256
3,549
1,094
13,028
840
428
4,842
156
all
435,873
127,459
743,216
2,263
1,940,592
11,965
178,873
58,990
2,154
498
184,036
197
all
Maximum #Instructions Average #Instructions Average Total #Instructions
unique
32,259
9,456
60,508
3,622
9,729
19,221
16,817
60,917
26,982
74,590
8,244
5,960
all
845,689
28,289,736
3,759,699
6,727
22,678,105
450,428
2,584,606
3,923,167
253,173
800
7,401,732
199
1,355,013
397,595
34,914,300
103,655
2,546,740
1,053,498
1,100,495
29,226,209
14,926,707
46,112,224
2,885,104
25,780
unique
5,697
1,932
5,445
546
4,383
116
612
835
533
407
2,919
155
cause. We explain the increased number of false positives
found for these targets at the end of this section in detail.
Another aspect of a bug’s complexity is the distance be-
tween the root cause and crashing location. As Table 2 in-
dicates, AURORA is capable of both identifying root causes
when the distance is small (a few hundred instructions, e. g.,
197 for Sleuthkit) and signiﬁcant (millions of instructions,
e. g., roughly 28 million for screen). Overall, we conclude
RQ 1 and RQ 2 by ﬁnding that AURORA is generally able
to provide automated root cause explanations close to the
root cause—less than 30 source code lines and less than 50
predicates—for diverse bugs of varying complexity.
The high quality of the explanations generated by AURORA
is also reﬂected by its high precision (i. e., the ratio of true
positives to all positives). Among the top 50 predicates, there
are signiﬁcantly more true positives than negatives. More
precisely, for 18 out of 25 bugs, we have a precision ≥ 0.84,
including 12 bugs with a precision of 1.0 (no false positives).
Only for two bugs, the precision is less than 0.5—0.14 for #17
and 0.24 for #6. Note that for #6, the predicate pinpointing
the developer ﬁx is at the top of the list, rendering all these
false positives irrelevant to triaging the root cause.
Despite the high precision, some false positives are gener-
ated. During our evaluation, we observed that they are mostly
related to (1) (de-)allocation operations as well as garbage
collectors, (2) control-ﬂow, i. e., predicates which indicate that
non-crashes executed the pinpointed code in diverse program
contexts (e. g., different function calls or more loop iterations),
(3) operations on (complex) data structures such as hash maps,
arrays or locks, (4) environment, e. g., parsing command-line
arguments or environment variables (5) error handling, e. g.,
signals or stack unwinding. Such superﬁcial features may
differentiate crashes and non-crashes but are generally not
related to the actual bug (excluding potential edge cases like
bugs in the garbage collector). Many of these false positives
occur due to insufﬁcient code coverage achieved during crash
exploration, causing the sets of crashing and non-crashing
inputs to be not diverse enough.
To detect such false positives during our evaluation, we
employed various heuristics: First, we use the predicate’s
annotations to identify functions related to one of the ﬁve
categories of false positives and discard them. Then, for each
predicate, we inspect concrete values that crashes and non-
crashes exhibit during execution. This allows us to compare
actual values to the predicate’s explanation and—together
with the source code line—recognize semantic features such
as loop counters or constants based on timers. Once a false
positive is identiﬁed, we discard any propagation of the predi-
cate’s explanation and thereby subsequent related predicates.
In our personal experience, these heuristics allow us to reli-
ably detect many false positives without considering data-ﬂow
dependencies or other program context. This is supported by
our results detailed in Table 3. Based on the ﬁve categories,
we evaluate how many false positives within the top 50 predi-
cates can be identiﬁed heuristically. Additionally, we denote
the number of propagations as well as the number of false
positives that must be analyzed in-depth. Note that an analyst
had to conduct such an analysis for only half of the targets
with false positives. We note that this may differ for other
bugs or other target applications, especially edge cases such
as bugs in the allocator or garbage collector.
Since we use a statistical model, false positives are a natural
side effect, yet, precisely this noisy model is indispensable.
For 15 of the analyzed bugs, we could ﬁnd a perfect predicate
(with a score of 1.0), i. e., predicates that perfectly distinguish
crashes and non-crashes. In the remaining ten cases, some
noise has been introduced by crash exploration. However, as
our results indicate, small amounts of noise do not impair our
analysis. Therefore, we answer RQ 3, concluding that nearly
USENIX Association
29th USENIX Security Symposium    245
Table 3: Analysis results of false positives within the top 50 predicates. For each target, we classify its false positives into the categories they are related to:
allocation or garbage collector (Alloc), control ﬂow (CF), data structure (DS), environment (Env) or error handling (Error). Additionally, we track the number of
predicates an analyst has to inspect in more detail (In-depth Analysis) as well as propagations of false positives that can be discarded easily.
Target
Perl
mruby
objdump
Python
Bash
Bash
Python
Python
PHP
mruby
NASM
Sleuthkit
#3
#6
#7
#9
#12
#13
#14
#17
#20
#22
#23
#24
-
-
2
1
1
1
-
-
-
1
-
2
7
38
-
-
-
-
-
2
-
-
-
-
False Positive Categories
Alloc CF DS Env Error
-
-
-
3
4
4
-
-
-
-
2
-
-
-
-
-
1
1
-
40
-
-
3
-
-
-
-
2
1
-
3
-
21
-
-
-
Propagations
In-depth Analysis
-
-
-
-
8
5
15
-
-
4
2
-
-
-
-
-
7
4
5
1
-
3
2
-
all predicates found by AURORA are strongly related to the
actual root cause.
Since the statistical model is only as good as the data it
operates on, we also investigate the crash exploration and
tracing phases. The results are presented in Table 4. Most
traces produced by crash exploration could be traced success-
fully. The only exception being Bash, which caused many
non-terminating runs that we excluded from subsequent anal-
ysis. Note that we were still able to identify the root cause.
We also investigate the time required for tracing, predicate
analysis and ranking. We present the results in Table 5. On av-
erage, AURORA takes about 50 minutes for tracing, while the
predicate analysis takes roughly 18 minutes and ranking four
minutes. While these numbers might seem high, remember
that the analysis is fully automated. In comparison, an ana-
lyst triaging these bugs might spend multiple days debugging
speciﬁc bugs and identifying why the program crashes.
6.4 Case Studies
In the following, we conduct in-depth case studies of various
software faults to illustrate different aspects of our approach.
6.4.1 Case Study: Type Confusion in mruby
First, we analyze the results of our automated analysis for the
example given in Section 2.1 (Bug #16). As described, the
NotImplementedError type is aliased to the String type,
leading to a type confusion that is hard to spot manually.
Consequently, it is particularly interesting to see whether our
automated analysis can spot this elusive bug. As exploring
the behavior of mruby was challenging for AFL, we ran the
initial crash exploration step for 12 hours in order to get more
than 100 diversiﬁed crashes and non-crashes. Running our
subsequent analysis on the best 50 predicates reported by
AURORA, we manually found that all of the 50 predicates are
related to the bug and provide insight into some aspects of
the root cause.
The line with the predicate describing the location of the
developers’ ﬁx is ranked 15th. This means that an analyst
has to inspect 14 lines of code that are related to the bug
but do not point to the developer ﬁx. In terms of predicates,
the 33rd predicate explains the root cause. This discrepancy
results from the fact that one source code line may translate
to multiple assembly instructions. Thus, multiple predicates
may refer to values used in the same source code line.
The root cause predicate itself conditions on the fact that the
minimal value in register rax is smaller than 17. Remember
that the root cause is the missing type check. Types in mruby
are implemented as enum, as visible in the following snippet
of mruby’s source code (mruby/value.h):
112
113
114
MRB_TT_STRING ,
MRB_TT_RANGE,
MRB_TT_EXCEPTION ,
/ *
/ *
/ *
16 * /
17 * /
18 * /
Our identiﬁed root cause pinpoints the location where the
developers insert their ﬁx and semantically states that the type
of the presumed exception object is smaller than 17. In other
words, the predicate distinguishes crashes and non-crashes
according to their type. As can be seen, the String type has
a value of 16; thus, it is identiﬁed as crashing input, while
the exception type is assigned 18. This explains the type
confusion’s underlying fault.
The other predicates allow tracing the path from the root
cause to the crashing location. For example, the predicates
rated best describe the freeing of an object within the garbage
collector. This is because the garbage collector spots that
NotImplemenetedError is changed to point to String in-
246    29th USENIX Security Symposium
USENIX Association