a
t
l
i
n
m
0
1
10
100
1000
mistake duration TM [ms]
n = 3, throughput = 300 1/s, TMR = 10000 ms
n = 7, throughput = 300 1/s, TMR = 100000 ms
50
45
40
35
30
25
20
15
10
5
0
FD
GM
1
10
100
mistake duration TM [ms]
1000
]
s
m
[
y
c
n
e
t
a
l
i
n
m
50
45
40
35
30
25
20
15
10
5
0
FD
GM
1
10
100
mistake duration TM [ms]
1000
Figure 7. Latency vs. TM in the suspicion-
steady scenario, with TMR ﬁxed.
n = 7 (right) and a variety of values for TD.
The results show that (1) both algorithms perform rather
well (the latency overhead of both algorithms is only a few
times higher than the latency in the normal-steady scenario;
see Fig. 4) and that (2) the FD algorithm outperforms the
GM algorithm in this scenario.
8 Discussion
We have investigated two uniform atomic broadcast al-
gorithms designed for the same system model: an asyn-
chronous system (with a minimal extension to allow us to
have live solutions to the atomic broadcast problem) and
f < n/2 process crashes (the highest f that our system
model allows). We have seen that in the absence of crashes
and suspicions, the two algorithms have the same perfor-
mance. However, a long time after any crashes, the group
membership (GM) based algorithm performs slightly better
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
]
s
m
[
D
T
-
y
c
n
e
t
a
l
i
n
m
]
s
m
[
D
T
-
y
c
n
e
t
a
l
i
n
m
80
70
60
50
40
30
20
10
0
after crash of p1, n = 3
TD = 0 ms
TD = 10 ms
TD = 100 ms
GM algorithm
FD algorithm
0
100
200
400
300
500
throughput [1/s]
600
700
800
140
120
100
80
60
40
20
0
0
after crash of p1, n = 7
GM algorithm
TD = 0 ms
TD = 10 ms
TD = 100 ms
FD algorithm
100
200
300
400
500
600
700
throughput [1/s]
Figure 8. Latency overhead vs. throughput in
the crash-transient scenario.
and has better resilience. In the scenario involving wrong
suspicions of correct processes and the one describing the
transient behavior after crashes, the failure detector (FD)
based algorithm outperformed the GM based algorithm. The
difference in performance is much greater when correct pro-
cesses are wrongly suspected.
Combined use of failure detectors and group member-
ship. Based on our results, we advocate a combined use of
the two approaches [27]. Failure detectors should be used
to make failure handling more responsive (in the case of
a crash) and more robust (tolerating wrong suspicions). A
different failure detector, making fewer mistakes (at the ex-
pense of slower crash detection) should be used in the group
membership service, to get the long term performance and
resiliency beneﬁts after a crash. A combined use is also
desirable because the failure detector approach is only con-
cerned with failure handling, whereas a group membership
service has a lot of essential features beside failure han-
dling: processes can be taken ofﬂine gracefully, new pro-
cesses can join the group, and crashed processes can re-
cover and join the group. Also, group membership can be
used to garbage collect messages in buffers when a crash
occurs [27].
Generality of our results. We have chosen atomic broad-
cast algorithms with a centralized communication scheme,
with one process coordinating the others. The algorithms
are practical: in the absence of crashes and suspicions, they
are optimized to have small latency under low load, and to
work under high load as well (messages needed to establish
delivery order are aggregated). In the future, we would like
to investigate algorithms with a decentralized communica-
tion scheme (e.g., [28]) as well.
Non-uniform atomic broadcast. Our study focuses on
uniform atomic broadcast. What speedup can we gain by
dropping the uniformity requirement in either of the ap-
proaches (of course, the application must work with the re-
laxed requirements)? The ﬁrst observation is that there is no
way to transform the FD based algorithm into a more efﬁ-
cient algorithm that is non-uniform: the effort the algorithm
must invest to reach agreement on Total Order automati-
cally ensures uniformity ([29] has a relevant proof about
consensus). In contrast, the GM based algorithm has an efﬁ-
cient non-uniform variant that uses only two multicast mes-
sages (see Fig. 1). Hence the GM based approach allows for
trading off guarantees related to failures and/or suspicions
for performance. Investigating this tradeoff in a quantita-
tive manner is a subject of future work. Also, we would
like to point out that, unlike in our study, a state transfer to
wrongly excluded processes cannot be avoided when using
the non-uniform version of the algorithm, and hence one
must include its cost into the model.
Methodology for performance studies.
In this paper, we
proposed a methodology for performance studies of fault-
tolerant distributed algorithms. Its main characteristics are
the following: (1) we deﬁne repeatable benchmarks, i.e.,
scenarios specifying the workload, the occurence of crashes
and suspicions, and the performance measures of interest;
(2) the benchmarks include various scenarios with crashes
and suspicions; (3) we describe failure detectors using qual-
ity of service (QoS) metrics.
The methodology allowed us to compare the two algo-
rithms easily, as only a small number of parameters are in-
volved. Currently, it is deﬁned only for atomic broadcast
algorithms, but we plan to extend it to analyze other fault
tolerant algorithms.
Acknowledgments. We would like to thank Danny Dolev
and Gregory Chockler for their insightful comments on uni-
formity and its interplay with group membership and the
limits of this study.
References
[1] M. Barborak, M. Malek, and A. Dahbura, “The consensus
problem in distributed computing,” ACM Computing Sur-
veys, vol. 25, pp. 171–220, June 1993.
[2] X. D´efago, A. Schiper, and P. Urb´an, “Totally ordered broad-
cast and multicast algorithms: A comprehensive survey,”
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply. 
Tech. Rep. DSC/2000/036, ´Ecole Polytechnique F´ed´erale de
Lausanne, Switzerland, Sept. 2000.
[3] G. Chockler, I. Keidar, and R. Vitenberg, “Group communi-
cation speciﬁcations: A comprehensive study,” ACM Com-
puting Surveys, vol. 33, pp. 427–469, May 2001.
[4] T. D. Chandra and S. Toueg, “Unreliable failure detectors for
reliable distributed systems,” Journal of ACM, vol. 43, no. 2,
pp. 225–267, 1996.
[5] P. Urb´an, X. D´efago, and A. Schiper, “Contention-aware
metrics for distributed algorithms: Comparison of atomic
broadcast algorithms,” in Proc. 9th IEEE Int’l Conf. on Com-
puter Communications and Networks (IC3N 2000), pp. 582–
589, Oct. 2000.
[6] W. Chen, S. Toueg, and M. K. Aguilera, “On the quality of
service of failure detectors,” IEEE Transactions on Comput-
ers, vol. 51, pp. 561–580, May 2002.
[7] F. Cristian, R. de Beijer, and S. Mishra, “A performance com-
parison of asynchronous atomic broadcast protocols,” Dis-
tributed Systems Engineering Journal, vol. 1, pp. 177–201,
June 1994.
[8] F. Cristian, S. Mishra, and G. Alvarez, “High-performance
asynchronous atomic broadcast,” Distributed System Engi-
neering Journal, vol. 4, pp. 109–128, June 1997.
[9] A. Coccoli, S. Schemmer, F. D. Giandomenico, M. Mock,
and A. Bondavalli, “Analysis of group communication pro-
tocols to assess quality of service properties,” in Proc. IEEE
High Assurance System Engineering Symp. (HASE’00), (Al-
buquerque, NM, USA), pp. 247–256, Nov. 2000.
[10] A. Coccoli, A. Bondavalli, and F. D. Giandomenico, “Analy-
sis and estimation of the quality of service of group commu-
nication protocols,” in Proc. 4th IEEE Int’l Symp. on Object-
oriented Real-time Distributed Computing (ISORC’01),
(Magdeburg, Germany), pp. 209–216, May 2001.
[11] H. Duggal, M. Cukier, and W. Sanders, “Probabilistic veri-
ﬁcation of a synchronous round-based consensus protocol,”
in Proc. 16th Symp. on Reliable Distributed Systems (SRDS
’97), (Washington - Brussels - Tokyo), pp. 165–174, IEEE,
Oct. 1997.
[12] H. Ramasamy, P. Pandey,
J. Lyons, M. Cukier, and
W. Sanders, “Quantifying the cost of providing intrusion tol-
erance in group communication systems,” in Proc. 2002 Int’l
Conf. on Dependable Systems and Networks (DSN-2002),
(Washington, DC, USA), pp. 229–238, June 2002.
[13] L. M. Malhis, W. H. Sanders, and R. D. Schlichting, “Numer-
ical evaluation of a group-oriented multicast protocol using
stochastic activity networks,” in Proc. 6th Int’l Workshop on
Petri Nets and Performance Models, (Durham, NC, USA),
pp. 63–72, Oct. 1995.
[14] N. Sergent, X. D´efago, and A. Schiper, “Impact of a fail-
ure detection mechanism on the performance of consensus,”
in Proc. IEEE Paciﬁc Rim Symp. on Dependable Computing
(PRDC), (Seoul, Korea), pp. 137–145, Dec. 2001.
[15] A. Coccoli, P. Urb´an, A. Bondavalli, and A. Schiper, “Perfor-
mance analysis of a consensus algorithm combining Stochas-
tic Activity Networks and measurements,” in Proc. Int’l
Performance and Dependability Symp., (Washington, DC,
USA), pp. 551–560, June 2002.
[16] T. D. Chandra, V. Hadzilacos, and S. Toueg, “The weakest
failure detector for solving consensus,” Journal of the ACM,
vol. 43, pp. 685–722, July 1996.
[17] P. Urb´an, X. D´efago, and A. Schiper, “Chasing the FLP im-
possibility result in a LAN or how robust can a fault toler-
ant server be?,” in Proc. 20th IEEE Symp. on Reliable Dis-
tributed Systems (SRDS), (New Orleans, LA, USA), pp. 190–
193, Oct. 2001.
[18] V. Hadzilacos and S. Toueg, “A modular approach to fault-
tolerant broadcasts and related problems,” TR 94-1425,
Dept. of Computer Science, Cornell University, Ithaca, NY,
USA, May 1994.
[19] P. Urb´an, I. Shnayderman, and A. Schiper, “Comparison
of failure detectors and group membership: Performance
study of two atomic broadcast algorithms (extended ver-
sion),” Tech. Rep. IC/2003/15, ´Ecole Polytechnique F´ed´erale
de Lausanne, Switzerland, Apr. 2003.
[20] S. Frolund and F. Pedone, “Revisiting reliable broadcast,”
Tech. Rep. HPL-2001-192, HP Laboratories, Palo Alto, CA,
USA, Aug. 2001.
[21] K. Birman, A. Schiper, and P. Stephenson, “Lightweight
causal and atomic group multicast,” ACM Transactions on
Computer Systems, vol. 9, pp. 272–314, Aug. 1991.
[22] C. P. Malloth and A. Schiper, “View synchronous communi-
cation in large scale distributed systems,” in Proc. 2nd Open
Workshop of ESPRIT project BROADCAST (6360), (Greno-
ble, France), July 1995.
[23] F. B. Schneider, “Implementing fault-tolerant services using
the state machine approach: A tutorial,” ACM Computing
Surveys, vol. 22, pp. 299–319, Dec. 1990.
[24] P. Urb´an, X. D´efago, and A. Schiper, “Neko: A single envi-
ronment to simulate and prototype distributed algorithms,”
Journal of Information Science and Engineering, vol. 18,
pp. 981–997, Nov. 2002.
[25] K. Tindell, A. Burns, and A. J. Wellings, “Analysis of
hard real-time communications,” Real-Time Systems, vol. 9,
pp. 147–171, Sept. 1995.
[26] J. Gray, “Why do computers stop and what can be done about
it ?,” in Proc. 5th Symp. on Reliablity in Distributed Software
and Database systems, Jan. 1986.
[27] B. Charron-Bost, X. D´efago, and A. Schiper, “Broadcasting
messages in fault-tolerant distributed systems: the beneﬁt of
handling input-triggered and output-triggered suspicions dif-
ferently,” in Proc. 20th IEEE Symp. on Reliable Distributed
Systems (SRDS), (Osaka, Japan), pp. 244–249, Oct. 2002.
[28] L. Lamport, “Time, clocks, and the ordering of events in a
distributed system,” Communications of the ACM, vol. 21,
pp. 558–565, July 1978.
[29] R. Guerraoui, “Revisiting the relationship between non-
blocking atomic commitment and consensus,” in Proc.
9th Int’l Workshop on Distributed Algorithms (WDAG-9),
LNCS 972,
(Le Mont-St-Michel, France), pp. 87–100,
Springer-Verlag, Sept. 1995.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:12:39 UTC from IEEE Xplore.  Restrictions apply.