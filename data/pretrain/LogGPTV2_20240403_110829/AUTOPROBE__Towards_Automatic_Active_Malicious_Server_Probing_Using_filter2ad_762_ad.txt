60%
60%
60%
60%
Found
6
4
5
4
2
9
2
1
0
0
32
12
3
3
17
5
9
11
0
4
3
0
8
1
11
7
4
9
172
Known
4
3
2
2
2
4
2
1
0
0
12
3
0
1
4
4
5
4
0
2
1
0
8
1
3
5
3
5
81
New
2
1
3
2
0
5
0
0
0
0
20
9
3
2
13
1
4
7
0
2
2
0
0
0
8
2
1
4
91
VT MD
2
1
0
0
0
0
0
0
0
0
0
1
1
0
1
1
0
0
0
0
0
1
1
1
0
0
0
1
0
2
0
0
0
0
1
1
0
0
0
0
1
1
0
1
1
1
0
0
0
3
2
1
1
3
2
3
24
11
UQ
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
1
0
3
Table 3: Localized Scanning Results of AUTOPROBE.
6.5
Internet-wide Scanning and Comparison
with CYBERPROBE
We next conduct Internet-wide scanning and compare the results
with CYBERPROBE. To minimize the impact to the whole Internet
because of our scanning while still clearly verifying the effective-
ness of AUTOPROBE, instead of scanning all ﬁngerprints, we focus
on three malware families (soft196, ironsource, optinstaller) also
scanned by CYBERPROBE [25].
Since these 3 malware families use HTTP C&C, we ﬁrst perform
an Internet-wide horizontal scan of hosts listening on the target
port 80.
For the horizontal scan, we collect the BGP table
from RouteViews and compute the total number of advertised IP
addresses. We conducted two horizontal scans on November 4,
2013 and February 19th, 2014. Both are summarized in Table 4.
We limit the scan rate to 60,000 packets per second (pps) for good
citizenship. The scan takes 2.9 hours and we ﬁnd over 71 million
live hosts listening on port 80.
After obtaining this 71 million live HTTP server list, we per-
formed 3 scans using the ﬁngerprints from AUTOPROBE and
CYBERPROBE for the three selected malware families. Table 5
summarizes the comparison. The top part of the table has the
results for the CYBERPROBE scans (CP-x) and the bottom part
the results for AUTOPROBE (AP-x). Each row corresponds to one
scan. Similar to the localized scanning, we also compare the results
with popular blacklist databases: VirusTotal (VT) [36], Malware
Domain List (MD) [23] and URLQuery (UQ) [35].
The results show that for every malware family the ﬁngerprints
produced by AUTOPROBE ﬁnd more servers than the one produced
by CYBERPROBE. Overall, AUTOPROBE has found 54 malware
servers, versus 40 malware servers found by CYBERPROBE, which
represents a 35% improvement. Finally, we also conduct ﬁve
additional Internet-wide scans for probes that cannot be generated
by CYBERPROBE, i.e., those from the NoResponse malware
server cases. The results are summarized in Table 6. They show
that AUTOPROBE can detect 83 malware servers and most of them
(80%) are new servers. Compared with CYBERPROBE, which
cannot generate any probe for NoResponse cases, AUTOPROBE has
a unique advantage and complements existing work well.
False positives and false negatives. Given the lack of perfect
ground truth, to measure false positives we check whether the
malware can establish successful communication with the detected
remote servers and whether a server’s response successfully trig-
gers the malware’s malicious logic. In particular, for each detected
server, we conduct another round of veriﬁcation by redirecting
the malware’s request to the detected servers and monitor the
malware’s execution afterwards. If the malware’s execution goes
into the behaviors we found in the analysis phase, we consider it a
true positive. In our test, we do not ﬁnd any false positive.
We cannot properly measure false negatives as the total number
of malicious servers is unknown.
Instead, we use the detection
result of CYBERPROBE as the ground truth to check that AU-
TOPROBE does not miss servers found by CYBERPROBE. The
result shows that AUTOPROBE can correctly detect all the servers
in CYBERPROBE using different signatures for the same families.
We further discuss potential false positives and false negatives in
Section 7.
HID Type
1
2
I
I
Start Date
2013-11-04
2014-02-19
Port
80
80
Targets
2,528,563,104
2,659,029,804
# Scanners Rate(pps)
60,000
50,000
4
4
Time
2.9h
3.5h
Live Hosts
71,068,585 (2.8%)
71,094,003(2.8%)
Table 4: Horizontal scanning results.
ID
CP-1
CP-2
CP-3
AP-1
AP-2
AP-3
Scan Date
2013-11-06
2013-11-06
2013-11-08
2013-11-08
2013-11-08
2013-11-08
Port
80
80
80
Fingerprint
soft196
ironsource
optinstaller
80
80
80
soft196
ironsource
optinstaller
SC
2
2
2
Time
24.6h
24.6h
24.6h
Resp.
91%
92%
90%
CYBERPROBE TOTALS:
25.3h
25.3h
25.3h
2
2
2
AUTOPROBE TOTALS:
90%
92%
90%
Found Known New
1
4
16
21
1
4
16
21
9
11
20
40
13
17
24
54
8
7
4
19
8
6
5
19
VT MD UQ
0
1
4
0
0
6
0
11
0
3
0
9
9
0
0
21
0
1
0
1
1
2
2
5
Table 5: Comparison of malware servers found using AUTOPROBE and CYBERPROBE for three malware families. Here CP-x denotes
CYBERPROBE and AP-x denotes AUTOPROBE.
7. DISCUSSION
PROBE.
We now discuss limitations and possible evasions of AUTO-
Possible false positives and false negatives. As discussed in
Section 6, we do not ﬁnd any false positives and false negatives in
our detection result. The lack of false positives may be due to our
strict criteria to determine that a server is malicious. For example,
we ensure the response can indeed trigger malware to download
malicious ﬁle or send some response. However, since our criteria of
detecting malicious server purely depends on malware’s behaviors,
lacking of full and precise understanding of malware logic could
introduce inacuraccies. For example, malware may download one