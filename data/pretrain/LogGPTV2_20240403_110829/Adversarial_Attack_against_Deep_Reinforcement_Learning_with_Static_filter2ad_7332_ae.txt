x
o
b
e
t
i
h
W
/
k
c
a
B
l
d
o
h
t
e
M
i
d
o
h
t
e
m
g
n
k
c
a
t
t
a
e
s
r
e
v
d
i
f
o
s
e
t
u
b
i
r
t
t
a
e
h
t
r
o
f
y
r
a
m
m
u
S
3
e
l
b
a
T
d
o
h
t
e
m
k
c
a
t
t
a
h
c
a
e
r
o
l
f
e
y
t
s
g
n
n
r
a
e
i
l
e
h
t
s
e
t
o
n
e
d
g
n
n
r
a
e
L
i
,
s
n
o
i
t
a
b
r
u
t
r
e
p
e
h
t
g
n
i
t
a
u
c
a
c
r
o
l
l
f
d
e
z
i
l
i
t
u
s
r
e
t
e
m
a
r
a
p
e
h
t
s
e
t
o
n
e
d
s
r
e
t
e
m
a
r
a
P
e
h
T
Chen et al. Cybersecurity            (2019) 2:11 
Page 15 of 22
Modifying input
Adversarial training and its variants
• Adversarial training
Adversarial training is the one of the most com-
mon strategies in the related literature to improve the
robustness of neural networks. By continuously inputting
new types of adversarial examples and conducting adver-
sarial training, the network’s robustness is continuously
improved. In 2015, Goodfellow et al. (2014b) developed
a method of generating adversarial examples (FGSM, see
(Goodfellow et al. 2014a)), and they also proposed to
conduct adversarial training to resist adversarial pertur-
bation exploiting the adversarial examples generated by
the attack method, the adversarial examples are constantly
updated during the training process so that the classifi-
cation model can resist the adversarial examples. How-
ever, Moosavi-Dezfooli et al. (2017) pointed out that no
matter how many adversarial examples are added, there
are new adversarial examples that can cheat the trained
networks in 2017. After that, by combining adversarial
examples with other methods, researchers have produced
better approaches defending adversarial examples in some
recent works.
• Ensemble adversarial training
It trains networks by utilizing the several pre-trained
vanilla networks to generate one-step adversarial exam-
ples. The model by adversarial training can defend weak
perturbations attack but can’t defend against strong ones.
Based on this, Florian Tramer et al. (2017) introduced the
ensemble adversarial training, which enhances training
data with perturbations transferred from other static pre-
trained models, this approach separates the generation of
adversarial examples from the model being trained, simul-
taneously drawing an explicit connection with robustness
to black-box adversaries. This model trained by ensem-
ble adversarial training has strong robustness to black-box
attacks on ImageNet.
• Cascade adversarial training
For unknown iterative attacks, Na et al. (2018) proposed
cascade adversarial training, they trained the network by
inputting adversarial images generated from the iterative
defended network and one-step adversarial images from
the network being trained. At the same time, the authors
regularized the training with a unified embedding so that
the convolution filters can gradually learn how to ignore
pixel-level perturbations. The cascade adversarial training
is shown in Fig. 9.
• Principled adversarial training
From the perspective of distributed robust optimiza-
tion, Aman Sinha et al. (2018) provided a principled
adversarial training, which guaranteed the performance of
neural networks under adversarial data perturbation. By
utilizing the Lagrange penalty form of perturbation under
the potential data distribution in the Wasserstein ball,
the authors provide a training process that uses worst-
case perturbations of training data to reinforce model
parameter updates.
• Gradient Band-based Adversarial Training
Chen et al. (2018b) proposed a generalized attack
immune model based on gradient band, which can be
shown in Fig. 10, mainly consists of Generation Module,
Validation Module, and Adversarial Training Module.
For the original clean map, Generation Module can
generate dominant adversarial examples based on the
Common Dominant Adversarial Examples Generation
Method (CDG) (see Section 3.2.4). Validation Module
can utilize the well trained A3C agent against the origi-
nal clean map, to calculate the Fattack for each example
based on the success criteria for attack proposed in this
paper. Adversarial Training Module utilize a single exam-
ple which can attack successfully for adversarial training,
and obtain a newly well trained A3C agentnew which can
finally realize “1:N” attack immunity.
Data randomization
In 2017, Xie et al. (2017) found that introducing random
resizing to the training images can reduce the strength of
the attack. After that, they further proposed (Xie et al.
2018) to use randomization at inference time to mitigate
the effects of adversarial attack. They add a random resize
layer and a random padding layer before the network of
classification, their experiments demonstrate that the pro-
posed randomization method is very effective at resisting
one-step and iterative attacks.
Input transformations
Guo et al. (2018) proposed strategies to defend against
adversarial examples through transforming the inputs
before feeding them to the image-classification sys-
tem. The input transformations include bit-depth reduc-
tion, JPEG compression, total variance minimization, and
image quilting before feeding the image. And the authors
showed that total variance minimization and image quilt-
ing are very effective defenses on ImageNet.
Input gradient regularization
Ross and Doshi-Velez (2017) first exploited input gradient
regularization (Drucker and Le Cun 1992) to improve the
adversarial robustness. In this defense technology trains
differentiable models that penalizes the degree to which
small changes in inputs can alter model predictions. And
the work shown that training with gradient regularization
strengthened the robustness to adversarial perturbations,
and it has a greater robustness combined the gradient
Chen et al. Cybersecurity            (2019) 2:11 
Page 16 of 22
Fig. 9 The structure of cascade adversarial training
regularization with adversarial training, but the computa-
tional complexity is too high.
Modifying the objective function
Adding stability term
Zheng et al. (2016) conducted stability training through
addingstability term to the objective function to encourage
DNN to generate similar output for images of various per-
turbed versions. The perturbed copy I(cid:7) of the input image
I is generated by a Gaussian noise , the final loss L is
consisted of the task objective Lo and the stability loss
Lstability.
Adding regularization term
Yan et al. (2018) append the regularization term based
on adversarial perturbations to the objective function,
they proposed a training recipe called “deep defense”.
Specifically, the authors optimize the objective function
jointed the original objective function term and a scaled
(cid:4)x(cid:4)p as a regularization term. Given a training set
{(xk, yk)} and the parameterized function f, and W collects
learnable parameters of f, the new objective function can
be optimized as bellow:
(cid:8)
(14)
min
W
(cid:9)
k
L(yk, f (xk; W )) + λ
(cid:9)
R
k
(cid:7)
−(cid:4)xk(cid:4)p
(cid:4)xk(cid:4)p
By combining an adversarial perturbation-based regular-
ization with the classification objective function, the train-
ing model can learn to defend against adversarial attacks
directly and accurately.
Dynamic quantized activation function
Rakin et al. (2018) first explored to use quantization
of activation functions and proposed to exploit adaptive
Fig. 10 Architecture for the gradient band-based generalized attack immune model
Chen et al. Cybersecurity            (2019) 2:11 
Page 17 of 22
quantization techniques for the activation functions so
that training the network to defend against adversarial
examples. They show the proposed Dynamic Quantized
Activation(DQA) method greatly heightened the robust-
ness of DNN under white-box attack, such as FGSM
(Goodfellow et al. 2014a), PGD (Madry et al. 2017), and
C&W (Carlini and Wagner 2017) attacks on MNIST and
CIFAR-10 datasets. In this approach, the authors integrate
the quantized activation functions into on adversarial
training method, in which training model to learn param-
eters γ to minimize the risk R(x,y)∼L[J(γ ,x,y)], γ consists of
parameters in DNN. Based on this, given the input image
x and the adversary example x + , this work aim to min-
imize the objective function to enhance the robustness
min R(x,y)∼L[maxJ([γ ,T],x+,y)]
(15)
where adding a new set of learnable parameters T :=
[ t1, t2, ..., tm−1]. For n-bit quantized activation function,
the quantization will have 2n − 1 threshold values T, let
m = 2n − 1, sgn represents the sign function, then m level
quantization function is as follows:
f (x) = 0.5 ×
ti(sgn(ti − x)
⎡
⎣sgn(x − tm−1) + m/2+1(cid:9)
i=m−1
ti−1(sgn(ti − x)
+ sgn(x − ti−1)) + 2(cid:9)
i=m/2
+sgn(x − ti−1)) − sgn(t1 − x)
⎤
⎦
(16)
Stochastic activation pruning
Inspired by game theory, S. Dhillon et al. (2018) proposed
a mixed strategy Stochastic Activation Pruning (SAP) for
adversarial defense. The method prunes a random acti-
vation subset (preferentially pruning those with smaller
magnitude) and expands survivors to compensate, using
SAP to pretrained networks without any additional train-
ing provides robustness against adversarial examples. And
the authors showed that combining SAP with adversar-
ial examples has a greater benefits. In particularly, their
experiments demonstrate that SAP can effectively defend
against adversarial examples in reinforcementlearning.
Modifying the network structure
Defensive distillation
Papernot et al. (2016a) proposed the defensive distilla-
tion mechanism for training network to resist adversarial
attacks. Defensive distillation, a strategy that trains mod-
els to output the probability of different classes rather than
the difficult decision of which class to output, the prob-
ability is provided by an early model that uses the labels
of hard classification to train on the same task. Papernot
et al. showed that defensive distillation can be used to
resist small-disturbed adversarial attacks through training
network to defend L-BFGS (Szegedy et al. 2013) attack and
FGSM (Goodfellow et al. 2014a) attack. Unfortunately,
defensive distillation is only applicable to DNN models
based on energy probability distributions. Nicholas Car-
lini and David Wagner proved that defensive distillation
is ineffective in (Carlini and Wagner 2016), and they
introduced a method of constructing adversarial examples
(Carlini and Wagner 2017), this method is not affected by
variousanti-attackmethods, including defensive distillation.
High-level representation guided denoiser
Liao et al. (2018) proposed high-level representation
guided denoiser (HGD) to defend adversarial exam-
ples for image classification. The main idea is to
train a denoiser based on neural network for remov-
ing the adversarial perturbation before sending them
to the target model. FLiao et al. use denoising U-net
(Ronneberger et al. 2015)
(DUNET) as a denois-
ing model. Compared to denoising autoencoder (DAE)
(Vincent et al. 2008), DUNET is directly connected
between encoder layers and decoder layers of the same
resolution, so the network only needs to learn how to
remove noise, instead of learning how to reconstruct the
whole image. And without using a pixel-level reconstruc-
tion loss function, the authors use the difference between
top-level outputs of the target model induced by orig-
inal and adversarial examples as the loss function to
guide the training of an image denoiser. The proposed
HGD has a good generalization and the target model
is more robust against both white-box and black-box
attacks.
Add detector subnetwork
Metzen et al. (2017) proposed to add a detector sub-
network for augmenting deep neural networks, the sub-
network is trained on a binary classification task that
distinguishes real data from data containing adversarial
perturbations. Considering that detector is also adver-
sarial, they proposed dynamic adversary training, which
introduces a novel adversary that aims at fooling both the
classifier and the detector, and trains the detector to coun-
teracting this novel adversary. The experiment results
show that dynamic detector has the robustness and its
detectability is more than 70% on the CIFAR10 dataset
(Krizhevsky and Hinton 2009).
Multi-model-based defense
Srisakaokul et al.
(2018) explored a novel defense
approach, MULDEF, based on the principle of diversity.
The MULDEF approach firstly constructs a family of
models by combining the seed model (the target model)
with additional models(constructed from the seed model),
the constructed family of models are complementary to
Chen et al. Cybersecurity            (2019) 2:11 
Page 18 of 22
each other to obtain robustness diversity, specifically, the
adversarial examples of a model usually doesn’t be the
adversarial examples of other models in model family.
Then the method randomly selects one model in these
models to be applied on a given input example. The ran-
domness of selection reduces can reduce the success rate
of the attack. The evaluation results demonstrate that
MULDEF augmented the adversarial accuracy of the tar-
get model by about 35-50% and 2-10% in the white-box
and black-box attack scenarios, respectively.