### Enhancing Neural Network Robustness: Adversarial Training and Beyond

#### Adversarial Training and Its Variants

**Adversarial Training**

Adversarial training is a widely used strategy in the literature to enhance the robustness of neural networks. By continuously inputting new types of adversarial examples and conducting adversarial training, the network's robustness is progressively improved. In 2015, Goodfellow et al. (2014b) introduced a method for generating adversarial examples (FGSM, see Goodfellow et al. 2014a) and proposed using these examples to conduct adversarial training, thereby improving the model's resistance to adversarial perturbations. The adversarial examples are continually updated during training to ensure that the classification model can resist such attacks. However, Moosavi-Dezfooli et al. (2017) noted that no matter how many adversarial examples are added, there will always be new ones capable of deceiving the trained network.

**Ensemble Adversarial Training**

This approach trains networks by utilizing several pre-trained vanilla networks to generate one-step adversarial examples. While it can defend against weak perturbations, it struggles with strong ones. Tramer et al. (2017) introduced ensemble adversarial training, which enhances training data with perturbations transferred from other static pre-trained models. This method separates the generation of adversarial examples from the model being trained, explicitly connecting robustness to black-box adversaries. Networks trained with this method exhibit strong robustness against black-box attacks on ImageNet.

**Cascade Adversarial Training**

For unknown iterative attacks, Na et al. (2018) proposed cascade adversarial training. This method involves training the network with adversarial images generated from an iteratively defended network and one-step adversarial images from the network being trained. The authors also regularized the training with a unified embedding, enabling convolution filters to gradually learn to ignore pixel-level perturbations.

**Principled Adversarial Training**

From the perspective of distributed robust optimization, Sinha et al. (2018) provided a principled adversarial training method that guarantees the performance of neural networks under adversarial data perturbation. By using the Lagrange penalty form of perturbation under the potential data distribution in the Wasserstein ball, the authors developed a training process that uses worst-case perturbations of training data to reinforce model parameter updates.

**Gradient Band-based Adversarial Training**

Chen et al. (2018b) proposed a generalized attack-immune model based on gradient bands, consisting of a Generation Module, Validation Module, and Adversarial Training Module. The Generation Module generates dominant adversarial examples, the Validation Module calculates the attack success rate, and the Adversarial Training Module uses successful adversarial examples for training, resulting in a well-trained A3C agent capable of "1:N" attack immunity.

#### Data Randomization

Xie et al. (2017) discovered that introducing random resizing to training images can reduce the strength of attacks. They further proposed (Xie et al. 2018) using randomization at inference time to mitigate the effects of adversarial attacks. By adding a random resize layer and a random padding layer before the classification network, their experiments demonstrated that the proposed randomization method is effective against one-step and iterative attacks.

#### Input Transformations

Guo et al. (2018) proposed strategies to defend against adversarial examples by transforming inputs before feeding them into the image-classification system. These transformations include bit-depth reduction, JPEG compression, total variance minimization, and image quilting. Their results showed that total variance minimization and image quilting are particularly effective defenses on ImageNet.

#### Input Gradient Regularization

Ross and Doshi-Velez (2017) first utilized input gradient regularization (Drucker and Le Cun 1992) to improve adversarial robustness. This defense technique trains differentiable models that penalize small changes in inputs that alter model predictions. Combining gradient regularization with adversarial training significantly enhances robustness, though it increases computational complexity.

#### Modifying the Objective Function

**Adding Stability Term**

Zheng et al. (2016) conducted stability training by adding a stability term to the objective function, encouraging DNNs to produce similar outputs for various perturbed versions of an image. The final loss includes the task objective and the stability loss.

**Adding Regularization Term**

Yan et al. (2018) appended a regularization term based on adversarial perturbations to the objective function, proposing a training recipe called "deep defense." The new objective function optimizes the original objective and a scaled regularization term, allowing the model to directly and accurately learn to defend against adversarial attacks.

**Dynamic Quantized Activation Function**

Rakin et al. (2018) explored the use of quantized activation functions and proposed adaptive quantization techniques to train networks to defend against adversarial examples. Their Dynamic Quantized Activation (DQA) method significantly enhanced the robustness of DNNs under white-box attacks, such as FGSM, PGD, and C&W attacks on MNIST and CIFAR-10 datasets.

**Stochastic Activation Pruning**

Inspired by game theory, Dhillon et al. (2018) proposed Stochastic Activation Pruning (SAP) for adversarial defense. SAP prunes a random subset of activations and expands survivors to compensate, providing robustness without additional training. Combining SAP with adversarial examples offers significant benefits, particularly in reinforcement learning.

#### Modifying the Network Structure

**Defensive Distillation**

Papernot et al. (2016a) introduced defensive distillation, a strategy that trains models to output class probabilities rather than hard classifications. This method was shown to resist small-disturbed adversarial attacks, but Carlini and Wagner (2016, 2017) demonstrated its ineffectiveness and introduced a method for constructing adversarial examples that bypasses defensive distillation.

**High-Level Representation Guided Denoiser**

Liao et al. (2018) proposed a high-level representation guided denoiser (HGD) for defending against adversarial examples in image classification. HGD uses a denoising U-net (DUNET) to remove adversarial perturbations before they reach the target model. The HGD has good generalization and enhances the robustness of the target model against both white-box and black-box attacks.

**Add Detector Subnetwork**

Metzen et al. (2017) proposed adding a detector subnetwork to deep neural networks to distinguish real data from data containing adversarial perturbations. They introduced dynamic adversary training, which trains the detector to counteract a novel adversary. The results showed that the dynamic detector has robustness and detectability exceeding 70% on the CIFAR10 dataset.

**Multi-Model-Based Defense**

Srisakaokul et al. (2018) explored MULDEF, a defense approach based on diversity. MULDEF constructs a family of complementary models and randomly selects one for a given input example, reducing the attack success rate. Evaluation results demonstrated that MULDEF augmented the adversarial accuracy of the target model by about 35-50% and 2-10% in white-box and black-box attack scenarios, respectively.