6
Fig. 9: Speedup of M3 over M1 for SVM-SHL as the training
dataset size gets larger.
As a result, the speedup is more signiﬁcant for larger training
datasets. This implies that a user can beneﬁt more by using
M3 when the user has a larger training dataset, which is often
the case in the era of big data.
M3 vs. M2: Figure 10 compares M3 with M2 with respect
to their relative performance over M1 as we sample more
training dataset for M2 (i.e., we increase p). For M3, we set
q% such that the relative MSE (or ACC) error of M3 over M1
is smaller than 0.1%. In particular, q% = 3% and q% = 1%
for RR on the UJIndoorLoc dataset and SVM-SHL on the
Bank dataset, respectively. We observe that when M3 and M2
achieve the same speedup over M1, the model learnt by M3
is more accurate than that learnt by M2. For instance, for RR
on the UJIndoorLoc dataset, M2 has the same speedup as M3
when sampling 10% of training dataset, but M2 has around
4% of relative MSE error while M3’s relative MSE error is
almost 0. For SVM-SHL on the Bank dataset, M2 has the same
speedup as M3 when sampling 4% to 5% of training dataset,
but M2’s relative ACC error is much larger than M3’s.
The reason is that M2 learns both the hyperparameter
and the model parameters using a subset of the training
dataset. According to Figure 1, the unrepresentativeness of
the subset is “doubled” because 1) it directly inﬂuences the
model parameters, and 2) it inﬂuences the hyperparameter,
through which it indirectly inﬂuences the model parameters.
In contrast, in M3, such unrepresentativeness only inﬂuences
the hyperparameter and the learning algorithms are relatively
robust to small variations of the hyperparameter.
3) Attacking Amazon Machine Learning: We also evaluate
the three methods using Amazon Machine Learning [1]. Ama-
zon Machine Learning is a black-box MLaaS platform, i.e.,
it does not disclose model parameters nor hyperparameters to
users. However, the ML algorithm is known to users, e.g., the
default algorithm is logistic regression. In our experiments, we
use Amazon Machine Learning to learn a logistic regression
model (with L2 regularization) for the Bank dataset. We
leverage the SigOpt API [46], a hyperparameter tuning service
for Amazon Machine Learning, to learn the hyperparameter.
We obtained a free API token from SigOpt.
We split the Bank dataset into two halves; one for training
and the other for testing. For M2 and M3, we sampled 15%
and 3% of the training dataset, respectively, i.e., p%=15%
and q%=3% (we selected these settings such that M2 and M3
have around the same overall training costs). Since Amazon
Machine Learning is black-box, we use the model parameter
stealing attack [54] to steal model parameters in our M3.
Speciﬁcally, in M3, we ﬁrst used 3% of the training dataset
to learn a logistic regression model. Amazon discloses the
prediction API of the learnt model. Second, we queried the
prediction API for 200 testing examples and used the equation-
solving-based attack [54] to steal the model parameters. Third,
we used our hyperparameter stealing attack to estimate the
hyperparameter. Fourth, we used the entire training dataset
and the stolen hyperparameter to re-train a logistic regression
model. We also evaluated the accuracy of the three models
learnt by the three methods on the testing data via their
prediction APIs.
The overall training costs for M1, M2, and M3 (including
the cost of querying the prediction API for stealing model
parameters) are $1.02, $0.15, and $0.16, respectively. The cost
per query of the prediction API is $0.0001. The relative ACC
error of M2 over M1 is 5.1%, while the relative ACC error of
M3 over M1 is 0.92%. Therefore, compared to M1, M3 saves
training costs signiﬁcantly with little accuracy loss. When M2
and M3 have around the same training costs, M3 is much more
accurate than M2.
4) Summary: Through empirical evaluations, we have the
following key observations. First, M3 (i.e., the Train-Steal-
Retrain strategy) can learn a model that is as accurate as that
45
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
learnt by M1 with much less computational costs. This implies
that, for the considered MLaaS platforms, a user can use our
attacks to learn an accurate model while saving a large amount
of economic costs. Second, M3 has bigger speedup over M1
when the training dataset is larger. Third, M3 is more accurate
than M2 when having the same speedup over M1.
VI. ROUNDING AS A DEFENSE
According to our Theorem 2,
the estimation error of
the hyperparameter is linear to the difference between the
learnt model parameters and the minimum of the objective
function that is closest to them. This theorem implies that
we could defend against our hyperparameter stealing attacks
via increasing such difference. Therefore, we propose that the
learner rounds the learnt model parameters before sharing them
with the end user. For instance, suppose a model parameter is
0.8675342, rounding the model parameter to one decimal and
two decimals results in 0.9 and 0.87, respectively. We note
that this rounding technique was also used by Fredrikson et
al. [14] and Tram`er et al. [54] to obfuscate conﬁdence scores
of model predictions to defend against model inversion attacks
and model stealing attacks, respectively.
Next, we perform experiments to empirically evaluate the
effectiveness of the rounding technique at defending against
our hyperparameter stealing attacks.
A. Evaluations
1) Setup: We use the datasets listed in Table II. Speciﬁ-
cally, for each dataset, we ﬁrst randomly split the dataset into
a training dataset and a testing dataset with an equal size.
Second, for each ML algorithm we considered, we learn a
hyperparameter using the training dataset via 5-fold cross-
validation, and learn the model parameters via the learnt
hyperparameter and the training dataset. Third, we round
each model parameter to a certain number of decimals (we
explored from 1 decimal to 5 decimals). Fourth, we estimate
the hyperparameter using the rounded model parameters.
Evaluation metrics: Similar to evaluating the effectiveness of
our attacks, the ﬁrst metric we adopt is the relative estimation
error of the hyperparameter value, which is formally deﬁned
in Eqn. 9. We say rounding is an effective defense for an
ML algorithm if rounding makes the relative estimation error
larger. Moreover, we say one ML algorithm can more effec-
tively defend against our attacks than another ML algorithm
using rounding, if the relative estimation error of the former
algorithm increases more than that of the latter one.
However, relative estimation error alone is insufﬁcient
because it only measures security, while ignoring the testing
performance of the rounded model parameters. Speciﬁcally,
severely rounding the model parameters could make the ML
algorithm secure against our hyperparameter stealing attacks,
but the testing performance of the rounded model parameters
might also be affected signiﬁcantly. Therefore, we also con-
sider a metric to measure the testing-performance loss that
is resulted from rounding model parameters. In particular,
suppose the unrounded model parameters have a testing MSE
(or ACC for classiﬁcation algorithms), and the rounded model
parameters have a testing MSEr (or ACCr) on the same testing
dataset. Then, we deﬁne the relative MSE error and relative
|MSE−MSEr|
|ACC−ACCr|
ACC
MSE
and
ACC error as
, respectively. Note
that the relative MSE error and the relative ACC error used in
this section are different from those used in Section V-C. A
larger relative estimation error and a smaller relative MSE (or
ACC) error indicate a better defense strategy.
2) Results: Figure 11, 12, 13, and 14 illustrate defense
results for regression, logistic regression, SVM, and three-layer
neural networks, respectively. Since we use log scale in the
−10
ﬁgures, we set the relative MSE (or ACC) errors to be 10
when they are 0.
Rounding is not effective enough for certain ML algo-
rithms: Rounding has small impact on the testing performance
of the models. For instance, when we keep one decimal, all
ML algorithms have relative MSE (or ACC) errors smaller
than 2%. Moreover, rounding model parameters increases the
relative estimation errors of our attacks for all ML algorithms.
However, for certain ML algorithms, the relative estimation
errors are still very small when signiﬁcantly rounding the
model parameters,
implying that our attacks are still very
effective. For instance, for LASSO, our attacks have relative
estimation errors that are consistently smaller than around
−3 across the datasets, even if we round the model parame-
10
ters to one decimal. These results highlight the needs for new
countermeasures for certain ML algorithms.
Comparing regularization terms: L2 regularization is more
effective than L1 regularization: Different ML algorithms
could use different regularization terms, so one natural ques-
tion is which regularization term can more effectively defend
against our attacks using rounding. All the SVM classiﬁca-
tion algorithms that we studied use L2 regularization term.
Therefore, we use results on regression algorithms and logistic
regression classiﬁcation algorithms (i.e., Figure 11 and Fig-
ure 12) to compare regularization terms. In particular, we use
three pairs: RR vs. LASSO, L2-LR vs. L1-LR, and L2-KLR
vs. L1-KLR. The two algorithms in each pair have the same
loss function, and use L2 and L1 regularizations, respectively.
We observe that L2 regularization can more effectively de-
fend against our attacks than L1 regularization using rounding.
Speciﬁcally, the relative estimation errors of RR (or L2-LR or
L2-KLR) increases faster than those of LASSO (or L1-LR or
L1-KLR), as we round the model parameters to less decimals.
For instance, when we round the model parameters to one
decimal, the relative estimation errors increase by 1011 and
102 for RR and LASSO on the Diabetes dataset, respectively,
compared to those without rounding.
These observations are consistent with our Theorem 2. In
particular, Appendix F shows our approximations to the gradi-
ent ∇ˆλ(w(cid:3)) in Theorem 2 for RR, LASSO, L2-LR, L2-KLR,
L1-LR, and L1-KLR. For an algorithm with L2 regularization,
the magnitude of the gradient at the exact model parameters is
inversely proportional to the L2 norm of the model parameters.
However, if the algorithm has L1 regularization, the magnitude
of the gradient is inversely proportional to the L2 norm of
the sign of the model parameters. For algorithms with L2
regularization, the learnt model parameters are often small
numbers, and thus the L2 norm of the model parameters is
smaller than that of the sign of the model parameters. As a
result, the magnitude of the gradient for an algorithm with
L2 regularization is larger than that for an algorithm with
46
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
RR
LASSO
KRR
0
−2
−4
−6
−8
−10
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
2
0
−2
−4
−6
−8
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
E
S
M
e
v
i
t
a
l
e
R
RR
LASSO
KRR
0
−2
−4
−6
−8
−10
2
5
1
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
2
0
−2
−4
−6
−8
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
E
S
M
e
v
i
t
a
l
e
R
RR
LASSO
KRR
0
−2
−4
−6
−8
−10
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
2
0
−2
−4
−6
−8
2
1
5
Number of decimals
3
4
(a) Diabetes
(b) GeoOrigin
(c) UJIIndoor
Fig. 11: Defense results of the rounding technique for regression algorithms.
0
−2
−4
−6
−8
−10
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
)
0
1
g
o
l
(
r
o
r
r
e
C
C
A
e
v
i
t
a
l
e
R
L2-LR
L1-LR
L2-KLR
L1-KLR
0
−2
−4
−6
−8
−10
2
5
1
Number of decimals
3
4
)
0