3.3 Rate Prediction
In this section we describe how we predict the rate ˆr of
an application, which we use to calculate the sum of the
rates of all applications running on components of a node,
ˆrt. The latter is used to predict the application execution
time ˆte using Equation 4. We base the prediction of the
rate of every application that is using a component hosted
on the node on both auto- and cross-correlation. We take
into account auto-correlation by building our prediction of
a component’s future input rate on its previous input rate.
This captures any self-similarity the application trafﬁc may
have, which has been known to be the case for various types
of trafﬁc in stream processing environments [24]. We take
into account cross-correlation, by also building our predic-
tion of the input rate of a component on the current input
rate of a previous component in the application component
graph. This captures the fact that preceding components
observe changes in the application input rate before the cur-
rent component. Since data ﬂow from one component to
the next, the observed trends are often seen in the current
component as well. In particular, we identify the preceding
component m in the application component graph, the rate
of which has the maximum correlation with the rate of the
current component so far. In summary, we estimate the k-
th input rate ˆrk of a component based on its previous input
rate rk−1, as well as the current and previous input rates of
component m, rk(m) and rk−1(m) respectively.
We transfer the current input rate values to the down-
stream components using the same path followed by the
data tuples, as shown in Figure 3. This way, for each of the
previous i components in the application component graph,
a series of (k − 1) pairs (r, r(i)) is built. This series asso-
ciates the (k−1) rate values r of the current component with
the (k − 1) rate values r(i) of each of the previous i com-
ponents. We use the Pearson Product Moment R, a popular
correlation coefﬁcient [12], to estimate how the rate of each
of the previous i components in the application component
graph is correlated to the rate of the current component.
We use the current (k-th) and previous ((k − 1)-th) rates
of the component m with the maximum correlation coef-
ﬁcient, argm maxR(k) and argm maxR(k−1) respectively,
as predictors for the rate of the current component. Hence,
assuming we have (k − 1) pairs of recorded input rates so
far, the estimated input rate for the current component is:
Figure 3. Propagation of rate values for cor-
related rate estimation.
ˆrk =
argm maxR(k)
argm maxR(k−1)
· rk−1 =
rk(m)
rk−1(m)
· rk−1
(6)
and the component m is decided as the one with the maxi-
mum among all correlation coefﬁcients Ri of each preced-
ing component i in the application component graph:
Ri =
Pj∈1...(k−1)
(rj(i) − ¯r(i))(rj − ¯r)
r Pj∈1...(k−1)
(rj(i) − ¯r(i))2 Pj∈1...(k−1)
(rj − ¯r)2
(7)
where the average rate values of the i-th preceding and the
current component, ¯r(i) and ¯r respectively, are:
¯r(i) = Pj∈1...(k−1)
k − 1
rj(i)
rj
¯r = Pj∈1...(k−1)
k − 1
(8)
4. Application Hot-Spot Alleviation
4.1. Identifying the Components to Migrate
After an application hot-spot has been predicted, the next
step is to determine which component execution(s) to mi-
grate in order to resolve the hot-spot. We perform QoS pro-
jection and choose the migrations in such a way, so that the
predicted execution times of the remaining applications in
the node are within their QoS requirements.
Speciﬁcally, our goal is to determine the minimum num-
ber of migrations that will result to all the remaining appli-
cations satisfying their QoS requirements. In other words,
we seek the minimum number of migrations that will re-
duce the sum of rates of all the applications in the node to
such a degree, that all projected execution times for the re-
maining applications will be within their QoS requirements.
More formally, and by building on the concepts introduced
in Section 3, we migrate the component execution(s) that
remove the minimum number of predicted rates ˆr (from
Equation 6), so that the predicted sum of application rates
on the node ˆrt results to predicted execution times ˆte (from
Equation 4) such that, for every application remaining in
the node, the slack time ts (from Equation 1) is positive.
This optimization problem lends itself to a dynamic pro-
gramming solution in pseudo-polynomial time. After ob-
serving that usually one migration sufﬁces to alleviate a
hot-spot, and to minimize the execution time overhead, as
migration decisions need to be taken online, we employ a
simple heuristic of selecting for migration the component
with the largest ˆr until all slack times become positive.
4.2. Identifying the Target Nodes
Once a component the execution of which is to be mi-
grated has been identiﬁed, the host to migrate to has to be
decided. The choice for migration targets is made among
the nodes that host the same component. Among them we
try to identify a node probable to satisfy the migrating appli-
cation’s QoS requirements, while not violating the QoS of
the applications currently running locally. Such nodes are
most probable to be found among the ones that are predicted
to be less loaded. Each node predicts its local load using
linear regression, based on predicted rate values, using a
methodology similar to the one described in Section 3.2.
We use a simple model, according to which a component’s
load is proportional to the number of input data tuples it is
receiving, which is an assumption also made by previous
works [23, 24]. We store load information in a decentral-
ized architecture [15] on top of the DHT, as was described
in Section 2. By utilizing the load monitoring architecture
a node determines the least loaded node offering the compo-
nent the migration requires. After the migration target has
been identiﬁed, the migration from the source to the target
takes place, to resolve the application hot-spot.
To avoid QoS violations we perform QoS projection that
predicts whether the QoS of the migrating and of the cur-
rently running applications will be able to be met after the
migration has occurred. Once it has received a migration
request, a node determines whether after accepting the mi-
gration it will be able to provide the migrating application
its required QoS. Additionally, it determines whether the
migration will not result to QoS violations for the locally
executing applications. To achieve these goals, a migra-
tion target performs QoS projection involving the migrating
and the currently running applications, that is similar to the
one described in Section 4.1. Speciﬁcally, it ensures that
by adding ˆr for the new application, the sum of application
rates on the node ˆrt will not result to a predicted ˆte (from
Equation 4) that results to a negative execution time slack
for any application (from Equation 1). If that is the case, the
migration is accepted and takes place using the migration
protocol presented in [15]. Our current migration mecha-
nism caters to stateless components and simple components
whose state is captured in small buffers. State transfer is a
separate issue by itself and worth future investigation.
5. Experimental Evaluation
To evaluate the performance of our hot-spot prediction
and alleviation mechanisms we have implemented them in
our Synergy distributed stream processing middleware and
performed experiments over the PlanetLab [5] wide-area
network testbed. We used 34 hosts, each one of them is-
suing a request for a distributed stream processing applica-
tion. Each node was hosting stream processing components
that were processing data tuples as they arrived. We set the
application end-to-end delay QoS requirement to 20s.
To evaluate the accuracy of our prediction mechanisms
we implemented a real stream processing application from
the network trafﬁc management domain, which we fed
with real TCP trafﬁc traces. We used a stream process-
ing application from the Stream Query Repository [20],
in which, assuming a packet capturing device installed in
a network, a system administrator wishes to monitor the
source-destination pairs in the top 5 percentile in terms
of total trafﬁc in the past 20 minutes over a backbone
link. We generated the streaming data to be processed
by replaying a TCP trafﬁc trace available from the Inter-
net Trafﬁc Archive [22]. Similar results where obtained
with the rest of the traces from [22]. The trace contained
two hours’ worth of all wide-area TCP trafﬁc between the
Lawrence Berkeley Laboratory and the rest of the world,
consisting of 1.8 million packets. Each packet contained
a timestamp, and ﬁelds deﬁning the source and destina-
tion (IPs and ports), as well as the size of the packets ex-
changed between them. Our implementation of the above
stream processing application to process the packet input
over 20-minute windows to generate the monitoring output
involved eight components, and screenshots are available
at http://synergy.cs.ucr.edu/screenshots.html. Each node in-
stantiated a different stream processing application that in-
cluded all eight components of the application component
graph, distributed randomly on different nodes of the sys-
tem. Each node predicted the rate and the execution time of
the components it was hosting using the statistical methods
described in section 3. We plot predicted and actual values
to show correlation and burstiness. The differences between
actual and predicted values were also plotted but are omitted
due to lack of space.
Rate Prediction Accuracy.
In our ﬁrst set of experi-
ments we investigated the accuracy of our rate prediction
algorithm described in Section 3.3. Figures 4, 5, 6, 7, and 8
compare the predicted rate for the individual components
of an application to their actual rate. Similar results were
obtained for all applications, as well as for the rest of the
components of the application component graph, but are
not included here due to lack of space. We observe that
the predicted rate closely follows the measured rate for the
different component types, namely sort, project, aggregate,
Predicted vs Measured Rate for Sort
Predicted vs Measured Rate for Project
Predicted vs Measured Rate for Aggregate
)
s
p
b
k
(
t
e
a
R
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Measured Rate
Predicted Rate
 0
 20
 40
 60
 80
 100
 120
 140
 160
)
s
p
b
k
(
t
e
a
R
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Measured Rate
Predicted Rate
 0
 20
 40
 60
 80
 100
 120
 140
 160
)
s
p
b
k
(
t
e
a
R
 8
 7
 6
 5
 4
 3
 2
 1
 0
Measured Rate
Predicted Rate
 0
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
Time (s)
Time (s)
Figure 4. Rate prediction ac-
curacy for “sort”.
Figure 5. Rate prediction ac-
curacy for “project”.
Figure 6. Rate prediction ac-
curacy for “aggregate”.
Predicted vs Measured Rate for Count
Predicted vs Measured Rate for Compare
Predicted vs Measured Execution Time for Sort
)
s
p
b
k
(
e
t
a
R
 7
 6
 5
 4
 3
 2
 1
 0
Measured Rate
Predicted Rate
 0
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
)
s
p
b
k
(
e
t
a
R
 2.5
 2
 1.5
 1
 0.5
 0
 0
Measured Rate
Predicted Rate
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
)
s
m
(
i
e
m
T
n
o
i
t
u
c
e
x
E
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Measured Execution Time
Predicted Execution Time
 0
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
Figure 7. Rate prediction ac-
curacy for “count”.
Figure 8. Rate prediction ac-
curacy for “compare”.