### Transfer Learning for Software Defect Prediction

The application of transfer learning to the problem of software defect prediction, especially when the source and target domains differ, represents a significant challenge. Nam et al. [103] address this issue by extending Transfer Component Analysis (TCA) to learn common latent factors between source and target projects. The extended version, TCA+, enhances cross-project prediction by selecting an appropriate normalization option for both the source and target data. The features derived from TCA+ are then used as input for a logistic regression classifier to predict faulty module files. This approach is validated on two datasets: Relink [147] and AEEEM [35]. In these datasets, documents are represented using various software metrics, and the results are compared with other prediction methods using the F-measure. For cross-project prediction, the F-measure is 0.61 for the Relink dataset and 0.41 for the AEEEM dataset.

### Critique of Traditional Code Metrics

Traditional code metrics are often criticized for being too handcrafted and simplistic. An alternative approach is to directly parse the source code using Abstract Syntax Trees (ASTs). Wang et al. [141] argue that code metrics may not effectively model semantics and thus may fail to discriminate between code regions with the same structure but different semantics. They propose using latent semantic representations instead. In their work, after parsing the code with ASTs, a Deep Belief Network (DBN) is trained in an unsupervised manner to learn semantic features. Subsequently, Naïve Bayes, Decision Trees, and Logistic Regression models are used to predict faults from the latent representations in both within- and cross-project settings. The results, obtained on Java projects from the PROMISE repository dataset, show a significant improvement in the F-score for cross-project prediction (0.568, +9.1% over [103]).

### Convolutional Neural Networks for SDP

Li et al. [76] explore the use of Convolutional Neural Networks (CNNs) for Software Defect Prediction (SDP). During the parsing step, a subset of AST nodes corresponding to different types of semantic operations is extracted. These tokens are mapped to numerical features using embeddings and fed into a 1D convolutional architecture. This architecture learns intermediate representations of the input code, which are later integrated with handcrafted features for the final prediction. Results obtained on a smaller subset of projects from the PROMISE dataset are compared with a traditional logistic regression model and the DBN approach of Reference [141], showing a substantial F-score improvement (0.608, +0.084 over traditional methods and +0.065 over Reference [141] for within-project SDP).

### Fault Injection

Fault injection is the deliberate introduction of faults into a working system to evaluate its fault tolerance. In traditional computer systems, this evaluation serves as a validation of the reactive capabilities of off-the-shelf systems. In the context of AIOps and failure management, fault injection also allows for the evaluation of externally deployed reactive mechanisms. Several works have applied fault injection techniques for this purpose [25, 87, 110, 119, 124, 127]. Key definitions in fault injection include the set of injected faults (F), the set of activations of faults in the system (A), the set of readouts from the system (R), and the set of measured values (M). F and A model the injection procedure from the input, while R and M model the output [5].

Fault injection can be applied at various levels, including hardware, software, and network. At the hardware level, it emulates the appearance of faults in physical components such as hard drives and CPUs. At the software level, it helps understand the effect of bugs on software behavior or models the causal dynamics of software components in shared-resource environments. Software Fault Injection (SFI) involves the automated and planned insertion of software faults. Faults can be inserted directly or simulated to model their consequences. For example, injecting erroneous data into a program introduces a failure, representing an undesired behavior but not a root-cause problem. Another common target for data corruption is function interfaces, such as APIs.

### Mutation Testing and SFI

Mutation Testing (MT) is closely related to SFI and involves introducing small modifications in the source code to evaluate the quality of code coverage achieved by test cases. MT more closely mimics faults compared to error injection because it introduces software faults directly rather than their consequences. However, random code changes can be inefficient if not guided, due to the high cost of running the test case suite for each mutated version. Reducing the number of mutations to the minimum essential while maintaining the discernment ability of mutation operations is crucial. Many tools now aim to identify code changes that can represent real software faults. While MT improves the quality and coverage of test cases, SFI identifies and emulates faults that cannot be discovered by test suites, as testing is performed in sandboxed scenarios.

### Distributed Computing and Fault Injection

In distributed computing environments, fault injection can be applied at the cluster or data center scale, such as randomly terminating operations at the instance level to stress-test the resiliency of the distributed service. Tools like ChaosMonkey and Kong by Netflix [13] and Facebook’s Project Storm [69] are examples of this. Fault injection techniques can also be applied at the network level based on principles similar to those at the hardware and software levels.

### AI in Fault Injection

Currently, most approaches to fault injection are outside the realm of AI, as the problem is primarily algorithmic and does not require intelligent agents. However, AI contributions in this field focus on the selection policies for the fault load, where the number of applied faults needs to be reduced for computational reasons.

### Resource Exhaustion and Rejuvenation

Software aging describes the accumulation of errors during program execution, leading to terminal failures such as hangs, crashes, or performance degradations. Causes of software aging include memory leaks, unreleased file locks, data fragmentation, and numerical error accumulation [21]. Machine Learning techniques have been applied to predict resource exhaustion preemptively. Garg et al. [49] estimate the time-to-exhaustion of system resources using instrumentation tools under the UNIX operating system. Trends are detected, and exhaustion time is quantified using regression techniques and seasonal testing.

Vaidyanathan et al. [136] investigate the effect of software aging due to the current system workload. A semi-Markov reward model is fitted from available workload and resource data under UNIX, where model states represent different workload scenarios. Time-to-exhaustion of memory and swap space is estimated using non-parametric regression techniques for each workload state. Alonso et al. [4] consider non-linear and piecewise linear resource consumption, adopting an ensemble of linear regression models selected using a decision tree on the same input features. Decision trees are chosen for their balance between accuracy and interpretability, and they are proposed for root-cause diagnosis as well.

Software rejuvenation is a corrective measure where the execution of a piece of software is temporarily suspended to clean its internal state. Common cleaning operations include garbage collection, flushing kernel tables, and re-initializing internal data structures. Castelli et al. [21] distinguish between periodic (synchronous) and prediction-based (asynchronous) rejuvenation policies. The latter requires a prediction model for future software failures, which can belong to the category of resource exhaustion predictors described above.

### AI in Rejuvenation Scheduling

AI has been used to suggest efficient rejuvenation scheduling policies. Castelli et al. [21] apply stochastic reward nets to model service downtimes and draw conclusions on the efficiency of rejuvenation policies. Their analysis considers both time-based and prediction-based policies. For the latter, they use resource exhaustion prediction algorithms to estimate future failure periods, ideal for refreshing the internal state and examining the expected downtime as a function of the prediction model's accuracy. Both periodic and prediction-based policies significantly reduce downtime, with prediction-based methods showing a larger overall improvement (−60% downtime at 90% coverage versus −25% with optimal time-based). High-frequency periodic policies can better tolerate simultaneous failures (−95% versus −85% of prediction-based methods).

Vaidyanathan et al. [137] build upon their workload-inclusive Markov prediction approach [136] to derive optimal periodic rejuvenation policies. They develop a comprehensive transition-based model between three states (working, failure, and rejuvenation) and optimize model parameters to reach different objectives, such as maximum steady-state availability or minimum downtime cost. This strategy improves the preventive strategies of distributed systems during the design stage.

### Checkpointing

Checkpointing is a concept related to software rejuvenation, involving the continuous and preemptive process of saving the system state before a failure occurs. Similar to software rejuvenation, checkpointing tolerates failures by occasionally interrupting the execution of a program to take precautionary actions. However, during checkpointing, the interruption period is used to save the internal state to persistent storage. If a fatal failure occurs, the created checkpoint file can be used to resume the program and reduce failure overhead. Checkpointing techniques are common in distributed and large-scale computing systems, although the higher error rates in large-scale systems make it less practical. Multi-level checkpointing addresses this by creating checkpoints at different component levels, with a different checkpointing strategy selected for each level based on component resiliency and permissible time overhead.

AI is used to model a faulty execution workload under different checkpointing strategies and select the most suitable strategy according to different objectives. Checkpointing can be static or dynamic, depending on the scheduling of checkpoints. All the papers investigated operate within the framework of Markov processes [62, 95, 114]. Okamura et al. [114] develop a dynamic checkpointing scheme for single-process applications based on these principles.