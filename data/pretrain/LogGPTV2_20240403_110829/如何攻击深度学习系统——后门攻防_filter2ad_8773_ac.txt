这里简单介绍一下模型窃取。模型窃取指攻击者利用模型窃取攻击来窃取供应商提供的模型，例如机器学习即服务（mlaas），通常依赖于查询受害模型并观察返回的响应。此攻击过程类似于密码学中的明文选择攻击。
传统的水印可以做，但是存在一个问题：负责主要任务和水印（后门）任务的模型参数是分开的。因此，当攻击者查询旨在窃取主要任务功能的模型时，作为不同子任务的水印可能不会传播到被窃副本，而这里防御的关键是模型提供者插入的后门将不可避免地传播到被盗模型，事实上Jia[19]等人已经实现了这方面的防御方案。
后门是一门技术，用于消极的一面还是积极的一面，用于什么场景完全取决于我们的想法，本文上述列举的工作仅是部分典型，如果有兴趣的读者可以进一步自行深入研究。
## 0x07
后门攻防领域的研究一直在不断探索中，在本文的最后根据笔者经验简单指出可以进一步研究的方向，限于笔者水平，可能一些研究方向没有研究价值或已经在近期发表了，希望读者可以批判看待：
1）运行机制
后门的生成机制、trigger激活后门的机制并不透明，这也涉及到深度学习系统的不可解释下问题，如果这些机制可以被深入研究清楚，那么未来后门领域的攻防将会更有效、更精彩。
2）防御措施
目前的防御措施都是针对特定的攻击手段进行防御的，并不存在一种通用的解决方案，究竟有没有这种方案，如果有的话应该怎么实现目前来看都是未知的。此外，一些方案要求有海量良性数据集，一些方案要求强大的计算资源，这些是否是必要的，是否可以进一步改进也是值得研究的。
3）攻击方案
深度学习应用的场景都很多，但是大部分后门攻击仅仅关注于图像识别、自动驾驶等领域，在语音识别、推荐系统等方面还缺乏深入研究。另外，对抗攻击具有可迁移性，那么后门攻击是否可以实现也是未知的，这也是可以进一步研究的方向。
4）trigger的设计
尽管我们前面看到的那篇文章将trigger设计的很自然，但是毕竟没有消除trigger，是否有可能在trigger的模式上进行优化，比如自动适应图像，将trigger叠加在肉眼不可见的地方，这方面的研究并不完善。目前的trigger设计都是启发式的，是否可以将其形式化为一个可优化的式子进行研究目前也是不清楚的。
## 0x07
参考：
[1]https://zh.wikipedia.org/wiki/%E8%BB%9F%E9%AB%94%E5%BE%8C%E9%96%80
[2]Gu T, Dolan-Gavitt B, Garg S. Badnets: Identifying vulnerabilities in the
machine learning model supply chain[J]. arXiv preprint arXiv:1708.06733, 2017.
[3]Liu Y, Ma S, Aafer Y, et al. Trojaning attack on neural networks[J]. 2017.
[4]https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db
[5]Yuan X, He P, Zhu Q, et al. Adversarial examples: Attacks and defenses for
deep learning[J]. IEEE transactions on neural networks and learning systems,
2019, 30(9): 2805-2824.
[6]Li J, Ji R, Liu H, et al. Universal perturbation attack against image
retrieval[C]//Proceedings of the IEEE/CVF International Conference on Computer
Vision. 2019: 4899-4908.
[7]Tang R, Du M, Liu N, et al. An embarrassingly simple approach for trojan
attack in deep neural networks[C]//Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2020: 218-228.
[8]Liu Y, Ma X, Bailey J, et al. Reflection backdoor: A natural backdoor
attack on deep neural networks[C]//European Conference on Computer Vision.
Springer, Cham, 2020: 182-199.
[9]Chen, X., Liu, C., Li, B., Lu, K., Song, D.: Targeted backdoor attacks on
deeplearning systems using data poisoning. arXiv preprint arXiv:1712.05526
(2017)
[10]Barni, M., Kallas, K., Tondi, B.: A new backdoor attack in cnns by
training setcorruption without label poisoning. In: IEEE International
Conference on ImageProcessing (ICIP). pp. 101–105. IEEE (2019)
[11]Tran, B., Li, J., Madry, A.: Spectral signatures in backdoor attacks. In:
NIPS(2018)
[12] Turner A, Tsipras D, Madry A. Clean-label backdoor attacks[J]. 2018.
[13]Te Lester Juin Tan and Reza Shokri. Bypassing Backdoor Detection
Algorithms in Deep Learning. In Proceedings of IEEE European Symposium on
Security and Privacy (Euro S&P), 2020.
[14]W. Xu, D. Evans, and Y. Qi. Feature Squeezing: Detecting Adversarial
Examples in Deep Neural Networks. In Proceedings of Network and Distributed
System Security Symposium (NDSS), 2018.
[15]Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Ranas- inghe, and
Surya Nepal. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks.
In Proceedings of Annual Computer Security Applications Conference (ACSAC),
2019.
[16]Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-Pruning:
Defending Against Backdooring Attacks on Deep Neural Networks. In Proceedings
of Symposium on Research in Attacks, Intrusions and Defenses (RAID), 2018.
[17]Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. DeepIn- spect:
A Black-box Trojan Detection and Mitigation Framework for Deep Neural
Networks. In Proceedings of International Joint Confer- ence on Artificial
Intelligence, 2019.
[18]Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your
weakness into a strength: Watermarking deep neural networks by backdooring,”
in USENIX Security Symposium, 2018.
[19]H. Jia, C. A. Choquette-Choo, and N. Papernot, “Entangled wa- termarks as
a defense against model extraction,” arXiv preprint arXiv:2002.12200, 2020.
[20]Li Y, Wu B, Jiang Y, et al. Backdoor learning: A survey[J]. arXiv preprint
arXiv:2007.08745, 2020.
[21]Gao Y, Doan B G, Zhang Z, et al. Backdoor attacks and countermeasures on
deep learning: a comprehensive review[J]. arXiv preprint arXiv:2007.10760,
2020.
[22]Pang R, Zhang Z, Gao X, et al. TROJANZOO: Everything you ever wanted to
know about neural backdoors (but were afraid to ask)[J]. arXiv preprint
arXiv:2012.09302, 2020.