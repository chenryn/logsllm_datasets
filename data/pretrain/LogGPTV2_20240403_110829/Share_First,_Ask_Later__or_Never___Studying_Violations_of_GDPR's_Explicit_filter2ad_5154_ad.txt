30
29
26
Domains
ﬂurry.com
my.com
amazon-adsystem.com GPS
unity3d.com
vungle.com
IMEI
GPS
Table 4: Top 5 of ad domains receiving AAID along with
other PD in our two app datasets.
through LibScout [9]. We chose not to apply LibScout to all
apps given the signiﬁcant runtime overhead this would have
caused (99.47 seconds per app for 100 randomly tested apps
on macOS/Core-i7/16GB RAM).
Out of the 353 apps which contacted Flurry, we were unable
to extract SDK version information for 202 apps from their
trafﬁc. For these 202 apps, LibScout successfully detected
SDK versions for 53 of the apps. In particular, it detected
SDK versions for 45 of the 190 apps which sent the IMEI,
all of which were pre-GDPR versions. We note that based
on the release notes of Flurry [28], the feature for the IMEI
collection was removed already in 2016. Since we are unable
to download Flurry SDKs before version 6.2.0 (released in
November 2015), it is highly likely that LibScout’s failure to
detect the version stems from pre-6.2.0 versions being used
by the apps in question. Hence, we believe that the IMEI
collection can be attributed to extremely old versions of the
SDK still in use in the apps we tested. For the versions which
sent the serial, LibScout detected two out of the four SDK
versions, both of which ran pre-GDPR versions. For the single
detected case of sending out the MAC address, LibScout
detected a version with GDPR support. Finally, for the class
of apps that sent out GPS, we could detect SDK versions
for 151/156 based on the trafﬁc, and LibScout successfully
detected the SDK version for the remaining ﬁve. Notably, all
these apps used current versions of the Flurry SDK. However,
based on the Flurry manual, it appears that if an app has
GPS permissions, Flurry defaults to sending this unless the
developer explicitly opts-out [27].
For the 42 cases in which my.com received (at least)
the MAC with the AAID, we found that 23 ran SDK ver-
sions which support GDPR. However, the documentation
is sparse [44] and it remains unclear if the default behav-
ior is to collect such data (or the developer has to set
setUserConsent to true ﬁrst). For the remaining 19 cases,
they all used outdated SDK versions without GDPR support.
Considering the data sent out to Amazon, we ﬁnd that 20/30
apps are running a current version of the Mobile Ads SDK.
For the 29 cases of apps which sent the AAID along with the
IMEI to Unity, these all used outdated SDKs (released before
2018 when GDPR came into effect). For Vungle, 16/26 apps
which sent out GPS with the AAID ran pre-GDPR versions
of the library (added in version 6.2.5 [66]). Yet, for the re-
maining ten, the version numbers indicated GDPR support;
i.e., in these cases developers opted into sending said data.
Further, out of 24,838 apps that sent PD to ad-related do-
mains, we found that 2,082 (8.4%) of these apps have a pre-
GDPR update date (before May 2018). We note from this
analysis that the most egregious violations can be attributed
both to extremely old versions of libraries (e.g., developers
often neglect SDK updates when adding functionality [18]),
but also to the complex conﬁguration required to make apps
GDPR (and Play Store)-compliant. This is particularly obvi-
ous for the collected GPS coordinates in Flurry’s SDK, which
seems to be enabled by default unless developers opt-out. In
the following, we aim to understand if the violations discussed
thus far are speciﬁc to either high-proﬁle or long-tail apps.
Comparing the Datasets A natural question that arises is
about potential differences between the datasets. As men-
tioned earlier, we ﬁltered the long-tail apps through Exodus-
Privacy, which introduces a selection bias. To account for that,
before comparing the datasets, we apply the same ﬁltering to
the high-proﬁle apps. After that, we ﬁnd that only 10,799 of
the 14,975 high-proﬁle apps we could successfully analyze
would have passed this ﬁltering step. Notably, we would have
missed 888 high-proﬁle apps which sent out data if we had
preﬁltered the high-proﬁle apps. Assuming similar distribu-
tions of undetected ad-related libraries in the long-tail set, this
is an obvious limitation of our sampling approach and should
be kept in mind for future work. After applying the ﬁltering to
the high-proﬁle set, we consider 3,527/10,799 (32.6%) high-
proﬁle apps which sent out data as well as 20,423/57,299
(35.6%) apps from the long-tail dataset in the following.
We ﬁrst compare the number of apps in each dataset which
send out PD to ad-related domains. Our null hypothesis H0 is
that there is no difference between high-proﬁle and long-tail
apps in terms of sending out PD. By applying χ2, we ﬁnd that
with p < 0.01, H0 is rejected. However, computing Cramer’s
V (v = 0.0228) we ﬁnd that the effect is negligible [20]. Next,
we investigate to what extent the apps in the datasets differ in
terms of domains to which they send PD. For this, we apply
the Kruskal-Wallis test, which rejects H0 of no difference
between the sets with p < 0.01; however, computing the effect
size ε2 = 0.0178, there is again only a small to negligible
effect [20]. Similarly, for the number of different types of PD
sent out, Kruskal-Wallis shows p = 0.022, but ε2 = 0.0002,
i.e., again signiﬁcant difference, yet negligible effect.
In addition to the overall trend, we also analyzed whether
we can observe differences in the parties which are contacted
by apps in each category. Figure 4 shows the most frequently
3676    30th USENIX Security Symposium
USENIX Association
conﬁrmation – stops collecting of data still meant the app
sent out data before asking for the user’s consent. Among
these 100 apps, we found only 25 apps present any type of
consent notices to users. Of these, only 11 apps provide an
option to reject the data collection, while the remaining 14
apps ask users to accept the data collection to use without
options to reject the data collection and sharing. Overall, this
indicates that the vast majority of apps do not even attempt to
achieve GDPR compliance, nor do they add meaningful ways
of rejecting consent after the fact.
5 Developer Notiﬁcation
In addition to the technical analyses described thus far, we
also notiﬁed affected developers. This had two main goals:
ﬁrst, to inform them about the potential breach of GDPR
regulations, which may lead to severe ﬁnes [16]. Second, we
wanted to gain insights into the underlying reasons that caused
the observed phenomena in the ﬁrst place. Since disclosing
the ﬁndings to authorities (e.g., regulators, Google) might
cause ﬁnancial harm to developers, we consciously decided
not to involve authorities but rather notify developers directly
to remedy compliance issues. We note that our institution’s
ethics guidelines do not mandate approval for such a study.
To notify the developers, we extracted the email addresses
used to upload the apps into the Play Store. To assess how
many developers actually received our reports, rather than
including the technical details about the issues in the email,
we sent developers a link to our web interface. On this, we
brieﬂy explained our methodology of testing and showed the
developers information about which hosts received which type
of data (see the previous section). In addition, in our email,
we asked recipients if they had been aware of the potential
violation of their apps, their general understanding of what
is personal data under GDPR, and their plans to address the
issues as well as proposals for tool support (see Appendix A
for the full email). We decided to have this rather than a
full-ﬂedged survey, as we wanted to keep the overhead for
respondents as low as possible to prompt more responses. We
note that the notiﬁcation was carefully worded not to make
legally conclusive statements, since this could amount to legal
consulting which is strictly regulated by German law.
5.1 Notiﬁcation and Accessed Reports
Out of the 24,838 apps for which we had discovered some
potential GDPR issue, 7,043 had been removed from the Play
Store by the time we conducted our notiﬁcation. For the re-
maining 17,795 apps, we sent out notiﬁcations in two batches,
each with a reminder. The ﬁrst batch of apps were notiﬁed
on December 15, 2020, with a reminder on January 5, 2021.
In this batch, we only included such apps that had not been
updated since our download from the Play Store until the
day of the notiﬁcation, totalling 8,006 apps. We took this
Figure 4: Top 5 ad domains receiving PD in each app set after
applying the Exodus-Privacy ﬁltering to the high-proﬁle set
(percentages relative to the PD-sending apps per dataset).
contacted domains across both datasets together with the per-
centage of apps that sent PD to them. In particular, we con-
sider the percentages relative to the apps after the Exodus
ﬁltering step. We note that facebook.com is the most preva-
lent in both sets, yet occurs more often in the high-proﬁle than
the long-tail apps. Contrary to that, we ﬁnd that unity3d.com
is more pronounced in the long-tail apps.
By analyzing the categories of the apps, we found that
Unity is frequently used in games, which are at the core of
Unity’s business model. Notably, AppBrain combines all sub-
categories of games into a single category, meaning our high-
proﬁle apps set contains only 500 such games. In contrast, in
the long-tail apps set, almost 20% of the apps are related to
games, explaining the signiﬁcant skew towards Unity in that
dataset. Generally, out of 72,274 successfully analyzed apps,
the top 5 categories that have more violating apps than others
in both app sets are game (73.29%), comics (64.97%), social
(41.39%), shopping (37.04%), and weather (36.59%).
Overall, the results of our comparative analysis lead us to
conclude that the phenomenon of sending out personal data
without prior explicit consent occurs as frequently and with as
many parties in both high-proﬁle and long-tail apps. While we
did observe statistically signiﬁcant differences, the associated
effect size was negligible. And while there certainly exists a
difference between the two datasets in terms of who receives
data, we cannot observe a difference that would warrant the
hypothesis that high-proﬁle apps violate GDPR less than long-
tail ones.
Manually Analyzing Consent Dialogues To investigate
whether developers may have merely misunderstood the con-
cept of consent (or the GDPR requirements thereof), we ran-
domly sampled 100 apps which sent out data in our exper-
iment and checked the screenshots (which we had taken as
part of our analysis to show on the Web interface). Speciﬁ-
cally, we checked for both implicit consent dialogues (such
as those indicating that by using the app, we consent to data
being sent out) or explicit opt-out dialogues. We note here
that even having an opt-out dialogue which – after negative
USENIX Association
30th USENIX Security Symposium    3677
0%10%20%30%40%50%60%70%Percentage of appsfacebook.comunity3d.comflurry.comsupersonicads.comchartboost.comDomain names57.71%63.25%42.27%22.23%13.59%12.73%8.52%6.49%8.34%2.81%App datasetHigh-Profile AppsLong-Tail Appsstep to ensure that we would not notify developers who had
removed the problematic code between our dataset down-
load and notiﬁcation date. For these 9,789 apps with recent
changes, we re-downloaded the latest version, conducted our
analysis again to conﬁrm our ﬁndings, and added those apps
to the second batch which still had some issues. The second
batch of notiﬁcations was sent on January 6, 2021, with re-
minders on January 20, 2021. Note that we decided to give an
additional week between notiﬁcation and reminder for round
1 of the notiﬁcations, given the overlap with the Christmas
vacation. In both cases, we grouped emails to developers (i.e.,
if a single developer had more than one app in the store, they
only received one email with multiple links). We followed
best practices established by prior work [19, 38, 59, 60] al-
lowing developers to opt-out and not send reminders for those
apps for which we had previously seen an access to the report.
In total, we notiﬁed 11,914 developers responsible for
17,795 apps. Of those developers, eight asked to be removed
from our experiment. Until February 1, 2021, we saw 2,199
accessed reports. Notably, some accesses were related to spam
checking (e.g., from Barracuda’s IP range or clients not down-
loading subresources like CSS ﬁles), which we ignore in our
analysis. This leaves us with accessed reports for 2,083 apps.
Notably, considering that a single owner may have multiple
apps affected by the same issue, we count the overall number
of apps for which their developer accessed some report, to-
talling 2,791 (15.7%) apps for which we reached their owner.
5.2 Developer Responses
In addition to the accessed reports and the updated apps, we
also analyzed the responses we received from developers. In
total, this amounted to 448 distinct senders that we classiﬁed
emails for. Based on an initial set of responses, three coders
developed an initial code book and then separately analyzed
the entire set of responses. For all cases in which their assess-
ment of an email/thread differed, they discussed the cases in
a group until they agreed on a classiﬁcation. Note that not all
respondents answered the stated questions from our email 1.
Of the 448 respondents, 114 acknowledged receipt of our
email and wanted to take it under advisement. 54 stated that
they required further investigation, either within their respec-
tive companies or their third-party SDK vendor. 48 further
inquired with us about potential solutions to the problem, such
as adding privacy policies to explain the data collection. We
faithfully answered these emails while stating that we cannot
provide conclusive individual legal assessments. Notably, 20
respondents argued that the EU was not their main market,
and that hence either GDPR would not apply to them or they
did not feel the need to implement consent, either being un-
aware of their app being downloadable from an EEA country
Play Store or that having users resident in the EU leads to
applicability of the GDPR.
1All developers we quote in this paper have given their explicit consent.
On the other side of the spectrum, 116 respondents dis-
agreed with our assessment. These ranged from comments
like “i am not aware that my app might not be GDPR com-
pliant”, “I show my privacy policy at the start of the app”,
“where seems to be the problem” to simple claims that their
apps do not transmit any user information, but also argued that
their advertisement libraries ﬁrst ask for user consent before
transmitting any data. This highlights a misconception that
having a privacy policy supersedes the need to have explicit
consent under the GDPR. Notably, as also highlighted by
our manual analysis, many ad-related libraries sent out data
before showing the consent dialogue. In the most extreme
cases, developers also argued that it was infeasible for them
to fully support GDPR, with one developer stating: “I haven’t
done anything wrong in my eyes. I show adverts in my Apps
from BIG F****** COMPANIES like Google and Facebook
and it’s up to them to tell developers what they collect so we
can then pass that on to our users. Like how the hell am I
supposed to know what a black box SDK from Google does
with data in the App I publish to people???”.
When asked about the data collection, 70/151 respondents
(46%) said they were aware of the types of data being col-
lected and 53/122 (43%) said they knew this data was pro-
tected by GDPR. Of the 139 respondents who answered our
question regarding reasons for lacking explicit consent, 66
(47%) argued they rely on a third-party app builder or SDK to
make their apps compliant and 40 (29%) believed their app to
be compliant already. Ten explained their app was outdated,
seven noted that they lacked resources for proper implementa-
tion, and seven said this was a bug. Finally, nine respondents
stated there was no particular reason.
When asked about their plans to change their apps (218
answers), 136 (62%) stated to update their app, with another
29 (13%) claiming to plan to remove the app from the Play
Store altogether or make it inaccessible from EEA countries.
Eleven said to conduct additional research into GDPR and
their responsibilities as the developers, and 17 said they did
not feel the necessity to take any steps. Our data is heavily
skewed towards those developers that plan to take action; we
attribute this to the fact that developers that disagreed with
our assessment rarely answered our follow-up questions.
Regarding our ﬁnal question about developer support, we
received 72 answers. Of those, 44 wanted to have an auto-
mated tool like ours to analyze their apps for compliance,
while 19 asked for better documentation around how to im-
plement GDPR compliance. Finally, nine respondents argued
that third-party tools should be compliant by default (e.g.,
“requiring ad providers to take responsibility for all the compli-
ance to this unnecessarily complicated law”). Naturally, the
skew towards automated tooling is not surprising, given that
we notiﬁed developers after applying our automated toolchain.
It is noteworthy, though, that fewer developers answered this
ﬁnal question, implying that it is not even clear to them how
they could be better supported in this particular issue space.
3678    30th USENIX Security Symposium
USENIX Association
5.3 Updates to Notiﬁed Apps
To assess our notiﬁcation’s impact on the affected apps, we
downloaded new versions of all apps that had looked at our
reports at least one by April 06, 2021. We re-ran our pipeline
for each app with an updated version to assess if the changes
were related to the reported GDPR infringement. For the 2,791
apps for which we reached a developer (i.e., they looked at at
least one report for their apps), 91 apps were removed from
Google Play, and 8 apps were no longer available to download
from Germany. We found 1,075 apps with updates since our
notiﬁcation for the remaining apps. By rerunning our pipeline
on these 1,075 apps, we observed that 250 (23%) apps no
longer sent PD to ad-related domains without prior consent.
Considering those 136 respondents that claimed to plan to up-
date their apps to incorporate proper GDPR consent, we found
that 92 apps (for which the respondents were responsible) had
been updated until the end of our experiment. Notably, though,
only 43 were updated in such a fashion that they did not send
out any data without interaction.
We note here that the overall number of apps which ad-
dressed the issue is low. Based on the responses we received,
we believe this to have two core reasons. First, many apps
are developed by small teams (if not individuals) who would
rather focus on functionality updates. Second, as shown in
our analysis of popular SDKs such as the one from Facebook,
they do not provide a consent dialogue, but rather put the
burden on the developer to integrate a new UI to ask for con-
sent, which is then passed to the SDK. Hence, we believe the
number of apps which address this issue will rise over time
and the seemingly small change in overall numbers can be
attributed to a lack of time to properly address the issue.
6 Calls to Action
Our results thus far have shown that the sharing of PD with
third-party data controllers is very pronounced in the datasets
we tested. More than one-third of all apps we tested sent out
PD before any users’ interaction. More notably, we could not
ﬁnd a signiﬁcant difference between high-proﬁle and long-tail
apps, i.e., the problem affects both high-proﬁle and long-tail
apps. Given these insights, we now discuss which involved
parties can take which steps to remedy the situation.
6.1 Third Parties Should Take Responsibility
Today, digital content is largely funded by advertising, which
means that companies monetize our behavior, attention, and
PD rather than us paying for services with money [29]. To
maximize revenue, advertising services heavily rely on con-
tinuous data collection and tracking PD from users [43]. Our
results show a signiﬁcant skew towards apps sending out PD
to advertisement companies without user’s explicit prior con-
sent (i.e., 86.6% of all apps that sent PD to the Internet) —
which is the most prominent business case of third parties
receiving and processing user data for their own business pur-
poses. However, we found that these third parties make it
cumbersome for developers to comply with GDPR or shift
the responsibility to app developers.
For example, Facebook required developers to obtain users’
consent before sending data via the SDK [23], whereas the
default behavior is automatically collecting user PD such
as AAID [24]. Our insights further show that such popular
companies play key roles in the widespread receiving of PD
without users’ explicit consent, i.e., more than half of apps that
sent data without consent sent it to (at least) Facebook. How-
ever, many developers believed that their apps are compliant
by default when using these popular companies’ services, as
noted by one respondent as “These third party SDKs are from