party server, denoted as the word server (WS). Users, the SP and 
the WS cooperate and learn the  personalized  tweet  recommenda-
tion  model.  More  specifically,  users  are  mainly  responsible  for 
collecting and preparing training samples that are used to learn the 
recommendation  model,  the  SP  is  responsible  for  computing  the 
weight parameters for explicit features, the latent factors of  users 
as recipients of tweets and the latent factors of users as publishers 
of  tweets  while  the  WS  is  responsible  for  computing  the  latent 
factors of words in the tweet vocabulary. By separating the latent 
factors  of  words  from  the  latent  factors  of  users,  neither  the  SP 
nor the WS can learn the contents of users’ tweets and users’ in-
terests without colluding with each other (more details in Section 
5).  
4.2  Threat Model 
In pTwitterRec, we assume both the SP and the WS to be honest 
but  curious;  that  is,  they  follow  our  protocol,  but  are  curious 
about  passively  learning  the  contents  of  users'  tweets  and  users' 
interests. More specifically, we assume that neither the SP nor the 
WS  creates  spurious  users  or  falsifies  user  requests  in  order  to 
learn secrets from each other. We consider  this  assumption  to  be 
reasonable  in  OSNs,  because  it  is  not  in  OSN  providers’  best 
interests  to  lose  users  as  a  result  of  any  suspicion  of  malicious 
behaviors  [12].  Furthermore,  we  assume  that  the  SP  and  the  WS 
do not collude. 
As mentioned in the system model, we assume that a user’s tweets 
are encrypted with a secret key shared only among the user’s ap-
proved  followers.  No  entities  can  decrypt  the  tweets  without  the 
appropriate decryption key. Both the SP and the WS are interested 
in learning the contents of a user's tweets. In addition, both the SP 
and the WS are interested in inferring users’ interests by learning 
users’ preference of specific words. For example, a  user  who  ex-
hibits a strong preference for words “Ford” and “Toyota” is most 
likely to be interested in cars.  
Furthermore,  we  assume  that  the  social  relations  between  users 
are not hidden from the SP in pTwitterRec. pTwitterRec works for 
Twitter-like systems where user’s social relations and the contents 
of  users’  tweets  are  both  hidden  from  the  provider.  However, 
pTwitterRec  may  lose  some  recommendation  accuracy  if  neither 
the SP nor users are able to compute some of the explicit features 
that depend on user’s social relations (more details in Section 5.3). 
5.  pTwitterRec 
In this section, we present the main components of pTwitterRec.  
5.1  Design Overview 
The  main  challenges  are:  a)  when  using  stochastic  gradient  des-
cent to learn the recommendation model, for any given tuple  in D, update the parameters of the model using equations 5-9 
without  revealing  the  words  contained  in  tweets  k  and  h  to  any 
unauthorized entities; b) when the model is learned, only user u is 
u wp q where pu is the latent factor of user u as 
allowed to compute 
the  recipient  of  tweets  and  qw  is  the  latent  factor  of  word  w,  be-
u wp q represents user u’s preference of word w 
cause the result of 
T
T
and thereby potentially reveals user u’s interest. 
We make the following observations in relation to equations 5-9: 
a) computing  ˆe  in equation 9 does not require  knowing  the  spe-
cific words contained in tweets k and h. Instead, we only need to 
∑ ; b) Similarly, updating 
know the result of
−∑
q
q
w
w
1
k
Z
∈
w T
k
∈
w T
h
1
h
Z
1
k
Z
pu  using  equation  5  and  updating  dp(k)  using  equation  7  only  re-
∑ ; c)  updating 
quire knowing the result of 
−∑
q
q
w
w
1
h
Z
∈
w T
k
∈
w T
h
bj  using  equation  8  only  requires  knowing  the  result  of 
 which  is  the  difference  between  the  explicit  features 
(
r−
r
)
,
,
u k
j
u h
j
computed from tweets k and h regarding user u, and the result of 
∑ ; d) finally, updating qw using equation 6 
−∑
q
q
w
w
1
k
Z
1
h
Z
∈
w T
k
∈
w T
h
does not require to know  the  identity  of  user  u.  Instead,  we  only 
need to learn the result of ˆ uep . 
Intuition.  Based  on  the  above  observations,  we  split  the  task  of 
learning the parameters of the recommendation model among user 
u, the SP and the WS. User u is responsible for selecting tweets k 
and h which constitute the tuple . User u and the SP coo-
perate to compute explicit features from tweets k and h and com-
pute 
without revealing the content of the tweets to the 
r−
r
(
)
,
,
u k
j
u h
j
SP.  The  SP  is  responsible  for  updating  bj,  Pu  and  dp(k)  while  the 
WS is responsible for updating qw. The SP updates bj, Pu and dp(k) 
∑ from the WS 
upon receiving the result of 
−∑
q
q
w
w
1
k
Z
1
h
Z
∈
w T
k
∈
w T
h
without learning the words contained in  tweets  k  and  h.  The  WS 
updates  qw  upon  receiving  the  result  of  ˆ uep from  the  SP  without 
learning the identity of user u. As a result, neither the SP nor the 
WS can calculate 
u wp q without a coalition between them. 
T
The main components of pTwitterRec include: a)  word  indexing, 
b) explicit feature computation and training sample submission, c) 
model  learning,  and  d)  tweet  publishing,  receiving  and  ranking. 
We will describe each  component  in  details  in  following  subsec-
tions. 
368Table 1. Explicit features (tweet k published by user p received by user u) 
Feature 
Category 
Description 
Computed By 
Co-follow Score 
Relation 
The similarity of followee sets of between user u and user p 
Mention Score 
Relation 
The number of times user u has mentioned user p in his 
previous tweets 
Friend 
Relation 
1 when user u and user p follow each other and 0 otherwise 
Relevance to Tweet 
History 
Relevance to Retweet 
History 
Relevance to Hash 
Tags 
Content-
relevance 
Content-
relevance 
Content-
relevance 
The relevance between tweet k and the posting history of 
user u 
The relevance between tweet k and the retweeted history of 
user u 
The count of words in tweet k that ever appeared as hash 
tags through user u’s posting history 
Length of Tweet 
Twitter Specific  The number of words contained in tweet k 
Hash Tag Count 
Twitter Specific  The number of hash tags contained in tweet k 
URL Count 
Twitter Specific  The number of URLs contained in tweet k 
Retweet Count 
Twitter Specific  The number of times tweet k has been retweeted 
Mention Count 
Followee Count 
Follower Count 
Tweet Count 
Publishers’ 
Authority 
Publishers’ 
Authority 
Publishers’ 
Authority 
Publishers’ 
Authority 
The times user p is mentioned in all tweets 
The number of users who user p follows 
The number of users who follow user p 
The number of tweets ever posted by user p 
SP 
SP 
SP 
User u 
User u 
User u 
User u 
User u 
User u 
SP 
SP 
SP 
SP 
SP 
5.2  Word Indexing 
In pTwitterRec, user u is responsible for selecting tweets k and h 
(where tweet k is some tweet that user u has previously retweeted 
and tweet h is some tweet that user u has not retweeted) and pre-
paring the tuple  in D before submitting it to the SP. User 
u  intends  to  keep  the  content  of  the  tweets,  i.e.,  the  words  con-
tained  in  tweets  k  and  h,  hidden  from  the  SP.  In  addition,  even 
though  the  WS  only  manages  the  latent  factor  of  words  and  will 
not learn the identity of user u at the model learning stage (more 
details  in  Section  5.3),  user  u  wants  to  keep  the  content  of  the 
tweets hidden from the WS as well, because the tweets may con-
tain some words that potentially reveal the identity of user u. On 
the  other  hand,  in  order  to  update  the  latent  factor  of  each  word 
contained  in  tweets  k  and  h,  the  WS  must  be  able  to  uniquely 
index  each  word.  Therefore,  each  word  contained  in  D  must  be 
uniquely  indexed  in  a  manner  that  the  WS  does  not  learn  the 
mapping between the word and the corresponding index.  
Since all users communicate with the SP, we require the SP to be 
the  central  server  responsible  for  indexing  words  using  some  se-
cret that is unknown to the WS. Note that when users request the 
indexes of words from the SP, users want to  keep  the  words  and 
the  corresponding  indexes  hidden  from  the  SP.  We  propose  that 
the  SP  adopts  a  deterministic  commutative  encryption  scheme 
such  as  Pohlig-Hellman  encryption  [24],  denoted  as  Ecomm,  to 
generate  the  unique  index  for  each  word.  Loosely  speaking,  an 
encryption scheme is commutative if a message encrypted by key 
k1 first and then by key k2 can be decrypted by the decryption key 
corresponding to k1 to reveal the message singly encrypted by k2. 
Assuming that user u is requesting the index of word w from the 
SP, the protocol works  as  follows:  during  system  setup,  both  the 
SP  and  user  u  generate  a  separate  secret  encryption  key  for  the 
commutative encryption scheme, denoted as ksp and ku respective-
ly. Then user u encrypts word w using Ecomm with the secret key ku 
and sends the encrypted result to the SP. Let the encrypted result 
be Ecomm(w, ku). Because the SP does not know the corresponding 
decryption key for ku, the SP cannot learn word w. Upon receiving 
Ecomm(w, ku), the SP further encrypts it using Ecomm with the SP’s 
secret  key  ksp  and  sends  Ecomm(Ecomm(w,  ku),  ksp)  back  to  user  u. 
Finally, user u decrypts Ecomm(Ecomm(w, ku) with the corresponding 
decryption key for ku. Because of the commutative property of the 
encryption  scheme  Ecomm,  user  u  obtains  Ecomm(w,  ksp)  and  com-
putes  indexw  =  H(Ecomm(w,  ksp))  where  H  is  the  SHA-224  hash 
function and indexw is the index of word w. Because of the deter-
ministic  property  of  the  encryption  scheme  Ecomm,  the  index  of 
word w is unique in the whole system. In order to reduce the on-
line computational and communication overhead on the user side, 
an alternative approach is to require that the SP pre-computes the 
indexes  of  popular  words  and  users  pre-download  the  indexes 
beforehand  without  revealing  their  interests  to  the  SP.  For  some 
obscure  words  that  are  not  pre-computed  by  the  SP,  users  adopt 
the  commutative  encryption  scheme  as  mentioned  above  to  re-
quest the indexes from the SP. 
5.3  Explicit Feature Computation and Train-
ing Sample Submission 
For each tweet in D, information such as the quality of the tweet 
(e.g., the number of URLs contained in the tweet) and the authori-
ty of the publisher (e.g., the number of followers) can be indicated 
as  features,  which  explicitly  reflect  the  possibility  of  a  user  ret-
weeting  the  tweet,  known  as  explicit  features.  Chen  et  al.  [10] 
propose four categories of explicit features: relation features, con-
tent-relevance  features,  twitter-specific  features  and  publishers’ 
authority features.  
369learns the index of each word at the model learning stage. The WS 
does learn the frequency of each word and might be able to corre-