∫ ∞
∫ ∞
−∞
+
 0.
Input
Output
:𝑘-sparse vector 𝑥 ∈ R𝑑+.
:(𝜀, 𝛿)-differentially private approximation of 𝑥.
(1) Apply random rounding to non-zero entries below 1 such
that:
(cid:40)RandRound (𝑥𝑖) ,
𝑦𝑖 =
𝑥𝑖,
if 0  0 with
𝛿 = 𝑂(1/𝑘). By using Algorithm 9 in Algorithm 7 the access time is
𝑂(log(1/𝛿)). The expected per-entry error is 𝑂(1/𝜀) and the expected
. The combined mechanism satisfies
maximum error is 𝑂(cid:16) log(1/𝛿)
(cid:17)
𝜀
(𝜀1 + 𝜀2, 𝛿)-differential privacy.
Proof. The proof is the same as the proofs of Lemmas 5.3 and 5.5.
□
Lemma 5.9. Let 𝛼 = Θ(1), 𝑠 = Θ(𝑘), and 𝜀1 = Θ(𝜀2). Then
the memory requirement of combining Algorithm 9 and the ALP
mechanism is 𝑂(𝑘(log(𝑑 + 𝑢) + log(1/𝛿))).
11
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1233Proof. The output of Algorithm 9 is always 𝑘-sparse and can
be represented using 𝑂(𝑘 log(𝑑 + 𝑢)) bits. We set 𝛽 =
+ 2
and therefore ℎ and ˜𝑧 are represented using 𝑂(𝑘 log(1/𝛿)) bits by
Lemma 4.13.
□
We are now ready to summarize our results for both pure and
ln(1/𝛿)
𝜀2
approximate differential privacy.
𝜀
(cid:17)
per-entry error, 𝑂(cid:16) log(𝑑)
Theorem 5.10. Let 𝛼 = Θ(1), 𝑠 = Θ(𝑘), and 𝜀 > 0. Then there
exists an 𝜀-differentially private algorithm with 𝑂(1/𝜀) expected
expected maximum error, access time of
𝑂(log(𝑑)), and space usage of 𝑂(𝑘 log(𝑑 + 𝑢)) with high probability.
Proof. It follows directly from Lemmas 5.3, 5.4 and 5.5.
□
Theorem 5.11. Let 𝛼 = Θ(1), 𝑠 = Θ(𝑘), and 𝜀, 𝛿 > 0. Then there
exist an (𝜀, 𝛿)-differentially private algorithm with 𝑂(1/𝜀) expected
expected maximum error, access time of
per-entry error, 𝑂(cid:16) log(1/𝛿)
𝑂(log(1/𝛿)), and space usage of 𝑂(𝑘(log(𝑑 + 𝑢) + log(1/𝛿))).
Proof. It follows directly from Lemmas 5.6, 5.8 and 5.9.
(cid:17)
□
𝜀
6 EXPERIMENTS
In this section, we discuss the per-entry error of ALP1-estimator
(Algorithm 3) in practice. Let 𝛾 = 𝛼+2
− 2. By Lemma 4.4 and 4.7
1+ 𝛼𝑘
the expected per-entry error of ALP1-estimator is upper bounded
by:
𝑠
(cid:18) 1
2 + 4𝛼 + 4
𝛼2
E[|𝑥𝑖 − ˜𝑥𝑖|] ≤
(cid:19)
+ 4𝛾 + 4
𝛾2
· 𝛼 .
Figure 3(a) shows the upper bound for varying values of 𝑘/𝑠 and
𝛼. Recall that 𝑘/𝑠 is a bound on the probability of a hash collision.
We see that the effect of hash collisions on the error increases for
large values of 𝛼, as each bit in the embedding is more significant.
We discuss how the upper bound compares to practice next.
Experimental Setup. We designed experiments to evaluate the
effect of the adjustable parameters 𝛼 and 𝑠 on the expected per-
entry error of ALP1-estimator. The experiments were performed on
artificial data. For our setup, we set parameter 𝛽 = 5000 and chose
a true value 𝑥𝑖 uniformly at random in the interval [0, . . . , 𝛽]. We
run only on artificial data, as uniform data does not benefit the al-
gorithm, and we can easily simulate worst-case conditions for hash
collisions. We simulate running the ALP1-projection algorithm by
computing 𝑦𝑖, simulating hash collisions, and applying randomized
response. The probability for hash collisions is fixed in each experi-
ment and the same probability is used for all bits. This simulates
worst-case input in which all other non-zero entries have a true
value of at least 𝛽. We increment 𝛼 by steps of 0.1 in the interval
[0.1, . . . , 10] and the probability of a hash collision by 0.05 in the
interval [0, . . . , 0.2]. The probability of 0 serves only as a baseline,
as it is not achievable in practice for 𝑘 > 1. The experiment was
repeated 105 times for every data point.
Figure 3(b) shows plots of the mean absolute error of the exper-
iments. As 𝛼 is increased, the error drops off at first and slowly
climbs. The estimates of 𝑦𝑖 are more accurate for large values of 𝛼.
However, any inaccuracy is more significant, as ˜𝑦𝑖 is scaled back
by a larger value. The error from the random rounding step also
increases with 𝛼. The plots of the upper bound and observed error
follow similar trajectories. However, the upper bound is approxi-
mately twice as large for most parameters.
Fixed Parameters. The experiments show how different values of
𝛼 and 𝑠 affect the expected per-entry error. However, the parameters
also determine constant factors for space usage and access time.
The space requirements scale linearly in 𝑠
𝛼 and the access time is
inversely proportional to 𝛼. As such, the optimal parameter choice
depends on the use case due to space, access time, and error trade-
offs.
To evaluate the error distribution of the ALP1-estimator algo-
rithm we fixed the parameters of an experiment. We set 𝛼 = 3 and
the hash collision probability to 0.1. We repeated the experiment
106 times.
The error distribution is shown in Figure 3(c). The mean absolute
error of the experiment is 6.4 and the standard deviation is 11.
Plugging in the parameters in Lemma 4.11, with probability at least