evaluating realistic attacks against machine learning sys-
tems. We also propose StingRay, a targeted poisoning at-
tack designed to bypass existing defenses. We show that
our attack is practical for 4 classiﬁcation tasks, which use
3 different classiﬁers. By exploring the FAIL dimen-
sions, we observe new transferability properties in ex-
isting targeted evasion attacks and highlight characteris-
tics that could provide resiliency against targeted poison-
ing. This exploration generalizes the prior work on attack
transferability and provides new results on the transfer-
ability of poison samples.
Acknowledgments We thank Ciprian Baetu, Jonathan
Katz, Daniel Marcu, Tom Goldstein, Michael Maynord,
Ali Shafahi, W. Ronny Huang, our shepherd, Patrick Mc-
Daniel and the anonymous reviewers for their feedback.
We also thank the Drebin authors for giving us access to
their data set and VirusTotal for access to their service.
This research was partially supported by the Department
of Defense and the Maryland Procurement Ofﬁce (con-
tract H98230-14-C-0127).
References
[1] ALEXEY MALANOV 12 POSTS MALWARE EXPERT, ANTI-
MALWARE TECHNOLOGIES DEVELOPMENT, K. L. The mul-
tilayered security model in kaspersky lab products, Mar 2017.
[2] ARP, D., SPREITZENBARTH, M., HUBNER, M., GASCON, H.,
AND RIECK, K. Drebin: Effective and explainable detection of
android malware in your pocket. In NDSS (2014).
[3] BARRENO, M., NELSON, B., JOSEPH, A. D., AND TYGAR,
J. D. The security of machine learning. Machine Learning 81
(2010), 121–148.
[4] BIGGIO, B., CORONA,
I., MAIORCA, D., NELSON, B.,
ˇSRNDI ´C, N., LASKOV, P., GIACINTO, G., AND ROLI, F. Eva-
sion attacks against machine learning at test time. In Joint Euro-
pean Conference on Machine Learning and Knowledge Discov-
ery in Databases (2013), Springer, pp. 387–402.
[5] BIGGIO, B., NELSON, B., AND LASKOV, P. Poisoning attacks
against support vector machines. arXiv preprint arXiv:1206.6389
(2012).
[6] BOJARSKI, M., YERES, P., CHOROMANSKA, A., CHOROMAN-
SKI, K., FIRNER, B., JACKEL, L., AND MULLER, U. Explain-
ing how a deep neural network trained with end-to-end learning
steers a car. arXiv preprint arXiv:1704.07911 (2017).
[7] BR ¨UCKNER, M., AND SCHEFFER, T. Stackelberg games for
adversarial prediction problems. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge discovery and
data mining (2011), ACM, pp. 547–555.
[8] CARLINI, N., AND WAGNER, D. Adversarial examples are not
easily detected: Bypassing ten detection methods. In Proceedings
of the 10th ACM Workshop on Artiﬁcial Intelligence and Security
(2017), ACM, pp. 3–14.
[9] CARLINI, N., AND WAGNER, D. Towards evaluating the robust-
ness of neural networks. In Security and Privacy (SP), 2017 IEEE
Symposium on (2017), IEEE, pp. 39–57.
[10] CHAU, D. H. P., NACHENBERG, C., WILHELM, J., WRIGHT,
A., AND FALOUTSOS, C. Polonium : Tera-scale graph min-
ing for malware detection. In SIAM International Conference on
Data Mining (SDM) (Mesa, AZ, April 2011).
[11] CHEN, X., LIU, C., LI, B., LU, K., AND SONG, D. Targeted
Backdoor Attacks on Deep Learning Systems Using Data Poison-
ing. ArXiv e-prints (Dec. 2017).
[12] COLVIN, R.
Stranger danger
application reputation.
archive/2010/10/13/stranger-danger-introducing-
smartscreen-application-reputation.aspx, Oct 2010.
-
introducing smartscreen
http://blogs.msdn.com/b/ie/
[13] CRETU, G. F., STAVROU, A., LOCASTO, M. E., STOLFO, S. J.,
AND KEROMYTIS, A. D. Casting out demons: Sanitizing train-
ing data for anomaly sensors. In Security and Privacy, 2008. SP
2008. IEEE Symposium on (2008), IEEE, pp. 81–95.
1314    27th USENIX Security Symposium
USENIX Association
[14] ERNST YOUNG LIMITED. The future of underwriting. http:
//www.ey.com/us/en/industries/financial-services/
insurance/ey-the-future-of-underwriting, 2015.
[15] FAIR ISAAC CORPORATION. FICO enterprise security score
gives long-term view of cyber risk exposure, November 2016.
ttp://www.fico.com/en/newsroom/fico-enterprise-
security-score-gives-long-term-view-of-cyber-
risk-exposure-10-27-2016.
[16] GILAD-BACHRACH, R., DOWLIN, N., LAINE, K., LAUTER,
K., NAEHRIG, M., AND WERNSING, J. Cryptonets: Applying
neural networks to encrypted data with high throughput and accu-
racy. In International Conference on Machine Learning (2016),
pp. 201–210.
[17] GOODFELLOW, I. J., SHLENS, J., AND SZEGEDY, C. Ex-
plaining and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572 (2014).
[18] GOOGLE RESEARCH BLOG.
Assisting
learning.
pathologists
https:
detecting
cancer with
in
//research.googleblog.com/2017/03/assisting-
pathologists-in-detecting.html, Mar 2017.
deep
[19] GROSSE, K., PAPERNOT, N., MANOHARAN, P., BACKES,
M., AND MCDANIEL, P. Adversarial perturbations against
deep neural networks for malware classiﬁcation. arXiv preprint
arXiv:1606.04435 (2016).
[20] GU, T., DOLAN-GAVITT, B., AND GARG, S. Badnets: Identi-
fying vulnerabilities in the machine learning model supply chain.
arXiv preprint arXiv:1708.06733 (2017).
[21] HEARN, M. Abuse at scale. In RIPE 64 (Ljublijana, Slovenia,
https://ripe64.ripe.net/archives/video/
Apr 2012).
25/.
[22] HUANG, L., JOSEPH, A. D., NELSON, B., RUBINSTEIN, B. I.,
AND TYGAR, J. Adversarial machine learning. In Proceedings
of the 4th ACM workshop on Security and artiﬁcial intelligence
(2011), ACM, pp. 43–58.
[23] KOH, P. W., AND LIANG, P. Understanding black-box predic-
tions via inﬂuence functions. arXiv preprint arXiv:1703.04730
(2017).
[24] KRIZHEVSKY, A., AND HINTON, G. Learning multiple layers
of features from tiny images. Citeseer (2009).
[25] LASKOV, P., ET AL. Practical evasion of a learning-based clas-
In Security and Privacy (SP), 2014 IEEE
siﬁer: A case study.
Symposium on (2014), IEEE, pp. 197–211.
[26] LECUN, Y.
The mnist database of handwritten digits.
http://yann.lecun.com/exdb/mnist/ (1998).
[27] LIU, W., AND CHAWLA, S. A game theoretical model for ad-
versarial learning. In Data Mining Workshops, 2009. ICDMW’09.
IEEE International Conference on (2009), IEEE, pp. 25–30.
[28] LIU, Y., CHEN, X., LIU, C., AND SONG, D. Delving into
transferable adversarial examples and black-box attacks. arXiv
preprint arXiv:1611.02770 (2016).
[29] LIU, Y., MA, S., AAFER, Y., LEE, W.-C., ZHAI, J., WANG,
W., AND ZHANG, X. Trojaning attack on neural networks. Tech.
Rep. 17-002, Purdue University, 2017.
[30] LIU, Y., SARABI, A., ZHANG, J., NAGHIZADEH, P., KARIR,
M., BAILEY, M., AND LIU, M. Cloudy with a chance of breach:
Forecasting cyber security incidents. In 24th USENIX Security
Symposium (USENIX Security 15) (2015), pp. 1009–1024.
[31] MIT TECHNOLOGY REVIEW. How to upgrade judges with
https://www.technologyreview.com/
machine learning.
s/603763/how-to-upgrade-judges-with-machine-
learning/, Mar 2017.
[32] MOZAFFARI-KERMANI, M., SUR-KOLAY, S., RAGHU-
NATHAN, A., AND JHA, N. K. Systematic poisoning attacks on
and defenses for machine learning in healthcare. IEEE journal of
biomedical and health informatics 19, 6 (2015), 1893–1905.
[33] MU ˜NOZ-GONZ ´ALEZ, L., BIGGIO, B., DEMONTIS, A., PAU-
DICE, A., WONGRASSAMEE, V., LUPU, E. C., AND ROLI,
F. Towards poisoning of deep learning algorithms with back-
gradient optimization. In Proceedings of the 10th ACM Workshop
on Artiﬁcial Intelligence and Security (2017), ACM, pp. 27–38.
[34] NELSON, B., BARRENO, M., CHI, F. J., JOSEPH, A. D., RU-
BINSTEIN, B. I. P., SAINI, U., SUTTON, C., TYGAR, J. D.,
AND XIA, K. Exploiting machine learning to subvert your spam
In Proceedings of the 1st Usenix Workshop on Large-
ﬁlter.
Scale Exploits and Emergent Threats (Berkeley, CA, USA, 2008),
LEET’08, USENIX Association, pp. 7:1–7:9.
[35] PAPERNOT, N., MCDANIEL, P., JHA, S., FREDRIKSON, M.,
CELIK, Z. B., AND SWAMI, A. The limitations of deep learn-
ing in adversarial settings. In 2016 IEEE European Symposium
on Security and Privacy (EuroS&amp;P) (2016), IEEE, pp. 372–
387.
[36] PAPERNOT, N., MCDANIEL, P. D., AND GOODFELLOW, I. J.
Transferability in machine learning: from phenomena to black-
box attacks using adversarial samples. CoRR abs/1605.07277
(2016).
[37] PAPERNOT, N., MCDANIEL, P. D., GOODFELLOW, I. J., JHA,
S., CELIK, Z. B., AND SWAMI, A. Practical black-box attacks
against deep learning systems using adversarial examples.
In
ACM Asia Conference on Computer and Communications Secu-
rity (Abu Dhabi, UAE, 2017).
[38] PAPERNOT, N., MCDANIEL, P. D., WU, X., JHA, S., AND
SWAMI, A. Distillation as a defense to adversarial perturbations
against deep neural networks. In IEEE Symposium on Security
and Privacy (2016), IEEE Computer Society, pp. 582–597.
[39] RAJAB, M. A., BALLARD, L., LUTZ, N., MAVROMMATIS, P.,
AND PROVOS, N. CAMP: Content-agnostic malware protection.
In Network and Distributed System Security (NDSS) Symposium
(San Diego, CA, Feb 2013).
[40] SABOTTKE, C., SUCIU, O., AND DUMITRA, T. Vulnerability
disclosure in the age of social media: exploiting twitter for pre-
dicting real-world exploits. In 24th USENIX Security Symposium
(USENIX Security 15) (2015), pp. 1041–1056.
[41] SAINI, U. Machine learning in the presence of an adversary:
Attacking and defending the spambayes spam ﬁlter. Tech. rep.,
DTIC Document, 2008.
[42] STEINHARDT, J., KOH, P. W. W., AND LIANG, P. S. Certi-
ﬁed defenses for data poisoning attacks. In Advances in Neural
Information Processing Systems (2017), pp. 3520–3532.
[43] SUCIU, O., M ˘ARGINEAN, R., KAYA, Y., DAUM ´E III, H., AND
DUMITRAS¸, T. When does machine learning fail? generalized
transferability for evasion and poisoning attacks. arXiv preprint
arXiv:1803.06975 (2018).
[44] SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J.,
ERHAN, D., GOODFELLOW, I., AND FERGUS, R.
Intriguing
properties of neural networks. arXiv preprint arXiv:1312.6199
(2013).
[45] TAMERSOY, A., ROUNDY, K., AND CHAU, D. H. Guilt by as-
sociation: large scale malware detection by mining ﬁle-relation
graphs. In KDD (2014).
[46] TRAM `ER, F., ZHANG, F., JUELS, A., REITER, M., AND RIS-
TENPART, T. Stealing machine learning models via prediction
In 25th USENIX Security Symposium (USENIX Security
APIs.
16) (Austin, TX, Aug. 2016), USENIX Association.
USENIX Association
27th USENIX Security Symposium    1315
rest of the training set, GETBASEINSTANCE randomly
selects a base instance from S′, labeled with the desired
target class yd, that lies within τD distance from the tar-
get. By choosing base instances that are as close to the
target as possible, the adversary reduces the risk that the
crafted samples will become outliers in the training set.
The adversary can further reduce this risk by trading tar-
get resemblance (modifying fewer features in the crafted
samples) for the need to craft more poison samples (in-
creasing Nmin). The adversary then checks the negative
impact of the crafted instance on the training set sam-
ple S′. The crafted instance xc is discarded if it changes
the prediction on t above the attacker set threshold τNI or
added to the attack set otherwise. To validate that these
techniques result in individually inconspicuous samples,
we consider whether our crafted samples would be de-
tected by three anti-poisoning defenses, discussed in de-
tail in Section 4.1.
Crafting collectively inconspicuous samples. After the
crafting stage, GETPDR checks the perceived PDR on
the available classiﬁer. The attack is considered success-
ful if both adversarial goals are achieved: changing the
prediction of the available classiﬁer and not decreasing
the PDR below a desired threshold τPDR.
Guessing the labels of the crafted samples. By modi-
fying only a few features in crafted sample, CRAFTIN-
STANCE aims to preserve the label yd of the base in-
stance. While the adversary is unable to dictate how the
poison samples will be labeled, they might guess this la-
bel by consulting an oracle. We discuss the effectiveness
of this technique in Section 4.3.
[47] VERIZON. Data breach investigations reports (dbir), February
2012. http://www.verizonenterprise.com/DBIR/.
[48] VIRUSTOTAL. http://www.virustotal.com.
[49] XU, W., EVANS, D., AND QI, Y. Feature squeezing: Detect-
ing adversarial examples in deep neural networks. arXiv preprint
arXiv:1704.01155 (2017).
[50] XU, W., QI, Y., AND EVANS, D. Automatically evading classi-
ﬁers. In Proceedings of the 2016 Network and Distributed Sys-
tems Symposium (2016).
[51] YANG, C., WU, Q., LI, H., AND CHEN, Y. Generative poi-
soning attack method against neural networks. arXiv preprint
arXiv:1703.01340 (2017).
[52] YOSINSKI, J., CLUNE, J., BENGIO, Y., AND LIPSON, H. How
transferable are features in deep neural networks? In Advances
in neural information processing systems (2014), pp. 3320–3328.
[53] ZHANG, C., BENGIO, S., HARDT, M., RECHT, B., AND
VINYALS, O. Understanding deep learning requires rethinking
generalization. arXiv preprint arXiv:1611.03530 (2016).
Appendix
A The StingRay Attack
Algorithm 1 shows the pseudocode of StingRay’s two
general-purpose procedures. STINGRAY builds a set I
with at least Nmin and at most Nmax attack instances. In
the sample crafting loop, this procedure invokes GET-
BASEINSTANCE to select appropriate base instances for
the target. Each iteration of the loop crafts one poison
instance by invoking CRAFTINSTANCE, which modiﬁes
the set of allowable features (according to FAIL’s L di-
mension) of the base instance. This procedure is speciﬁc
to each application. The other application-speciﬁc ele-
ments are the distance function D and the method for
injecting the poison in the training set:
the crafted in-
stances may either replace or complement the base in-
stances, depending on the application domain. Next, we
describe the steps that overcome the main challenges of
targeted poisoning.
Application-speciﬁc instance modiﬁcation. CRAFTIN-
STANCE crafts a poisoning instance by modifying the set
of allowable features of the base instance. The procedure
selects a random sample among these features, under the
constraint of the target resemblance budget. It then al-
ters these features to resemble those of the target. Each
crafted sample introduces only a small perturbation that
may not be sufﬁcient to induce the target misclassiﬁca-
tion; however, because different samples modify differ-
ent features, they collectively teach the classiﬁer that the
features of t correspond to label yd. We discuss the im-
plementation details of this procedure for the four appli-
cations in Section 4.2.
Crafting individually inconspicuous samples. To en-
sure that the attack instances do not stand out from the
1316    27th USENIX Security Symposium
USENIX Association