0.880
0.994
1
Website B
Precision Recall
0.835
0.980
0.982
1
Website C
Precision Recall
0.729
0.927
0.946
1
Training-Testing Temporal Split. We ﬁrst ensure the
temporary training constraint [49], which means training data
should be strictly temporally precedent to the testing data. We
split the August data by using the ﬁrst two weeks of data
for training and the later two weeks for testing. Given our
feature encoding is sliding-window based, we never use the
“future” data to predict the “past” events (for both bots and
benign data). We did not artiﬁcially balance the ratio of bot
and benign data, but kept the ratio observed from the data.
Bootstrapping the Slide Window. The sliding-window has
a bootstrapping phase. For the ﬁrst few days in August 2018,
there is no historical data. Suppose the sliding-window size
w = 7 days, we bootstrap the system by using the ﬁrst 7 days
of data to encode the IP-sequences formed in the ﬁrst 7 days.
On day 8, the bootstrapping is ﬁnished (sliding window is
day 1 – day 7). On day 9, the window starts to slide (day 2 –
day 8). The bootstrapping does not violate temporary training
constraints since the bootstrapping phase is ﬁnished within the
training period (the ﬁrst two weeks in August).
Testing starts on day 15 (sliding window is day 7 - day
14). The window keeps sliding as we test on later days. For
our experiment, we have tested different window sizes w. We
pick window size w =7 to balance the computation complexity
and performance (additional experiments on window size are
in Appendix B). The results for w =7 are shown in Table V.
Note that feature encoding does not require any labels. As
such, we used all the data (bots and benign) to estimate the
entity frequency distribution in the time window.
Model Performance. We compute the precision (the frac-
tion of true bots among the detected bots) and recall (the
fraction of all true bots that are detected). F1 score com-
bines precision and recall to reﬂect the overall performance:
F 1 = 2 × P recision × Recall/(P recision + Recall).
As shown in Table V, the precision and recall of the LSTM
model are reasonable, but not extremely high. For example,
for website B, the precision is 0.888 and the recall is 0.877.
The worst performance appears on C where the precision is
0.789 and the recall is 0.730. Since our model is focused on
advanced bots that already bypassed the rules, it makes sense
that they are difﬁcult to detect.
Table VI illustrates the value of the machine learning models
to complement existing rules. Now we consider both simple
bots and advanced bots, and examine the percentage of bots
that rules and LSTM model detected. If we use rules alone
(given the rules are highly conservative), we would miss a
large portion of all
the bots. If we apply LSTM on the
remaining data (after the rules), we could recover most of these
bots. The overall recall of bots can be improved signiﬁcantly.
For website B, the overall recall is booted from 0.835 to
0.980 (15% improvement). For website C, the recall is boosted
from 0.729 to 0.927 (30% improvement). For website A, the
improvement is smaller since the rules already detected most
of the bots (with a recall of 0.880 using rules alone). We
also show the precision is only slightly decreased. We argue
that this trade-off is reasonable for web services since the
CAPTCHA system can further verify the suspicious candidates
and reduce false positives.
Training with Limited Data. The above performance looks
promising, but it requires a large labeled training dataset. This
requires aggressive CAPTCHA delivery which could hurt the
benign users’ experience. As such, it is highly desirable to
reduce the amount of training data needed for model training.
We run a quick experiment with limited training data
(Table VII). We randomly sample 1% of the training set in
the ﬁrst two weeks for model training, and then test the model
on the same testing dataset in the last two weeks of August.
Note that we sample 1% from both bots and benign classes.
We repeat the experiments for 10 times and report the average
F1 score. We show that limiting the training data indeed hurts
the performance. For example, using 1% of the training data,
B’s F1 score has a huge drop from 0.883 to 0.446 (with a very
high standard deviation). C has a milder drop of 5%-6%. Only
A maintains a high F1 score. This indicates that the advanced
bots in A exhibit a homogeneous distribution that is highly
different from benign data (later we show that such patterns
do not hold over time).
On one hand, for certain websites (like A), our LSTM model
is already effective in capturing bot patterns using a small
portion of the training data. On the other hand, however, the
result shows the LSTM model is easily crippled by limited
training data when the bot behavior is more complex (like B).
V. DATA SYNTHESIS USING ODDS
In this section, we explore the usage of synthesized data to
augment the model training. More speciﬁcally, we only synthe-
size bot data because we expect bots are dynamically changing
and bot labels are more expensive to obtain. Note that our
goal is very different from the line of works on adversarial
retraining (which aims to handle adversarial examples) [50],
[51]. In our case, the main problem is the training data is
too sparse to train an accurate model in the ﬁrst place. We
B. Overview of the Design of ODDS
At a high level, ODDS contains three main steps. Step
1 is a prepossessing step to learn a latent representation of
the input data. We use an LSTM-autoencoder to convert the
input feature vectors into a more compressed feature space.
Step 2: we apply DBSCAN [54] on the new feature space
to divide data into clusters and outliers. Step 3: we train the
ODDS model where one generator aims to ﬁt the outlier data,
and the other generator ﬁts the clustered data. A discriminator
is trained to (a) classify the synthetic data from real data, and
(b) classify bot data from benign data. The discriminator can
be directly used for bot detection.
Step 1 and Step 2 are using well-established models, and
we describe the design of Step 3 in the next section. Formally,
M = {X1, . . . , XN} is a labeled training dataset where Xi
is an IP-sequence. Xi = (x1, x2, . . . , xT ) where xt ∈ Rd
denotes the original feature vector of the tth request in the
IP-sequence.
LSTM-Autoencoder Preprocessing.
This step learns a
compressed representation of the input data for more efﬁcient
data clustering. LSTM-Autoencoder is a sequence-to-sequence
model that contains an encoder and a decoder. The encoder
computes a low-dimensional latent vector for a given input,
and the decoder reconstructs the input based on the latent
vector. Intuitively, if the input can be accurately reconstructed,
it means the latent vector is an effective representation of the
original input. We train the LSTM-Autoencoder using all the
benign data and the benign data only. In this way, the au-
toencoder will treat bot data as out-of-distribution anomalies,
which helps to map the bot data even further away from the be-
nign data. Formally, we convert M to V = {v1, v2, . . . , vN},
where v is a latent representation of the input data. We use
Bv to represent the distribution of the latent space.
Data Clustering.
In the second step, we use DBSCAN [54]
to divide the data into two parts: high-density clusters and
low-density outliers. DBSCAN is a density-based clustering
algorithm which not only captures clusters in the data, but also
produces “outliers” that could not form a cluster. DBSCAN
has two parameters: sm is the minimal number of data points
to form a dense region; dt is a distance threshold. Outliers
are produced when their distance to any dense region is larger
than dt. We use the standard L2 distance for DBSCAN. We
follow a common “elbow method” (label-free) to determine
the number of clusters in the data [55]. At the high-level, the
idea is to look for a good silhouette score (when the intra-
cluster distance is the smallest with respect to the inter-cluster
distance to the nearest cluster). Once the number of clusters
is determined, we can automatically set the threshold dt to
identify the outliers. DBSCAN is applied to the latent vector
space V. It is well known that the “distance function” that
clustering algorithms depend on often loses its usefulness on
high-dimensional data, and thus clustering in the latent space
is more effective. Formally, we use DBSCAN to divide Bv
into the clustered part Bc and the outlier part Bo.
Fig. 3: Illustrating data synthesis in the clustered data region (left)
and the outlier data region (right).
design a data synthesis method called ODDS. The key novelty
is that our data synthesis is distribution-aware — we use
different generalization functions based on the characteristics
of “outliers” and ”clustered data“ in the labeled data samples.
A. Motivation of ODDS
Training with limited data tends to succumb to overﬁtting,
leading to a poor generalizability of the trained model. Regu-
larization techniques such as dropout and batch normalization
can help, but
they cannot capture the data invariance in
unobserved (future) data distributions. A promising approach
is to synthesize new data for training augmentation. Generative
adversarial network (GAN) [52] is a popular method to
synthesize data to mimic a target distribution. For our problem,
however, we cannot apply a standard GAN to generate new
samples that resemble the training data [53], because the input
bot distribution is expected to be non-representative. As such,
we look into ways to expand the input data distribution (with
controls) to the unknown regions in the feature space.
A more critical question is, how do we know our “guesses”
on the unknown distribution is correct. One can argue that it is
impossible to know the right guesses without post-validations
(CAPTCHA or manual veriﬁcation). However, we can still
leverage domain-speciﬁc heuristics to improve the chance of
correct guesses. We have two assumptions. First, we assume
the benign data is relatively more representative and stable
than bots. As such, we can rely on benign data to generate
“complementary data”, i.e., any data that is outside the benign
region is more likely to be bots. Second,
the assumption
is the labeled bot data is biased: certain bot behaviors are
well captured but other bot behaviors are under-represented or
even missing. We need to synthesize data differently based on
different internal structures of the labeled data. In “clustered
regions” in the feature space, we carefully expand the region
of the already labeled bots and the expansion becomes less
aggressive closer to the benign region. In the “outlier” region,
we can expand the bot region more aggressively and uniformly
outside of the benign clusters.
Figure 3 illustrates the high level idea of the data synthesis
in clustered regions and outlier regions. In the following, we
design a specialized GAN for such synthesis. We name the
model “Outlier Distribution aware Data Synthesis” or ODDS.
1-d View2-d ViewdensityxxydensityxxyBotBotBenignBenignSynthesizedSynthesized(cid:26) 1
high-density regions. More speciﬁcally, the clustered data is a
mixture of benign and bot samples. α is the term to control
whether the synthesized bot data is closer to real malicious
data pm or closer to the benign data pb. We deﬁne this
clustered bot distribution C as:
if pb(˜v) >  and ˜v ∈ Bv
C(˜v) =
if pb(˜v) ≤  and ˜v ∈ Bv
αpb(˜v) + (1 − α)pm(˜v)
where τ2 is the normalization term.
between pG2 and C. The objective function as follows:
LKL(pG2||C) =
To learn this distribution, we minimize the KL divergence
[log pb(˜v)]1[pb(˜v) > ]
−H(pG2 ) + E
˜v∼PG2
1
τ2
pb(˜v)
− E
˜v∼PG2
[(αpb(˜v) + (1 − α)pm(˜v))]1[pb(˜v) ≤ ]
The feature matching loss Lfm2
in generator 2 is to ensure
the generated samples and the real clustered samples are not
too different. In other words, the generated samples are more
likely to be located in the clustered region.
Lfm2
= || E
˜v∼PG2
f (˜v) − E
v∼Bc
f (v)||2
where f is the hidden layer of the discriminator.
The complete objective function for the second generator is
deﬁned as:
LKL(PG2||C) + Lfm2
min
G2
Discriminator.
The discriminator aims to classify synthe-
sized data from real data (a common design for GAN), and
also classify benign users from bots (added for our detection
purpose). The formulation of the discriminator is:
min
D
E
v∼pb
[log D(v)] + E
[log(1 − D(˜v))] + E
v∼pb
˜v∼pG1
+ E
˜v∼pG2
+ E
v∼pm
[log(1 − D(v))]
[log(1 − D(˜v))]
[D(v) log D(v)]
The ﬁrst three terms are similar to those in a regular GAN
which are used to distinguish real data from synthesized
data. However, a key difference is that we do not need the
discriminator to distinguish real bot data from synthesized
bot data. Instead, the ﬁrst three terms seek to distinguish real
benign data from synthesized bot data, for bot detection. The
fourth conditional entropy term encourages the discriminator
to recognize real benign data with high conﬁdence (assuming
benign data is representative). The last term encourages the
discriminator to correctly classify real bots from real benign
data. Combining all the terms, the discriminator is trained to
classify benign users from both real and synthesized bots.
Note that we use the discriminator directly as the bot
detector. We have tried to feed the synthetic data to a separate
classiﬁer (e.g., LSTM, Random Forest), and the results are not
as accurate as the discriminator (see Appendix C). In addition,
using the discriminator for bot detection also eliminates the
extra overhead of training a separate classiﬁer.
Fig. 4: The data ﬂow of ODDS model. “Positive (P)” represents bot
data; “Negative (N)” represents benign data.
C. Formulation of ODDS
As shown in Figure 4, ODDS contains a generator G1 for
approximating the outlier distribution of Bo, another generator
G2 for approximating the clustered data distribution Bc, and
one discriminator D.
Generator 1 for Outliers.
To approximate the real out-
liers distribution pG1, the generator G1 learns a generative
distribution O that is complementary from the benign user
representations. In other words,
if the probability of the
generated samples ˜v falling in the high-density regions of
benign users is bigger than a threshold pb(˜v) > , it will
be generated with a lower probability. Otherwise, it follows a
uniform distribution to ﬁll in the space, as shown in Figure 3
(right). We deﬁne this outlier distribution O as:
O(˜v) =
1
pb(˜v)
if pb(˜v) >  and ˜v ∈ Bv
if pb(˜v) ≤  and ˜v ∈ Bv
τ1
C
(cid:26) 1
where  is a threshold to indicate whether the generated sam-
ples are in high-density benign regions; τ1 is a normalization
term; C is a small constant; Bv represents the whole latent
feature space (covering both outlier and clustered regions).
To learn this outlier distribution, we minimize the KL
divergence between pG1 and O. Since τ1 and C are constants,
we can omit them in the objective function as follows:
LKL(pG1||O) = −H(pG1) + E
˜v∼PG1
where H is the entropy and 1 is the indicator function.
[log pb(˜v)]1[pb(˜v) > ]
We deﬁne a feature matching loss to ensure the generated