## PostgreSQL HLL 近似计算算法要点    
### 作者                        
digoal                        
### 日期                        
2020-10-11                        
### 标签                        
PostgreSQL , HLL                        
----                        
## 背景      
https://github.com/citusdata/postgresql-hll                     
了解HLL的算法:     
随着往hll value里面添加value的增加, hll的结构随之变化:  从空(EMPTY), 到原始值(EXPLICIT), 到稀疏映射(SPARSE), 到bit pack映射(FULL)(与bloom类似).       
空和原始值都可以完成精确计算, 稀疏映射bit pack映射都是近似映射.    
进入近似映射后, hll里面注册的值越多, 判断单个值是否在hll内的准确度越低.      
与bloom一样:      
- true不一定为true. (因为多个其他值可能侵占了你add的value的bit pack.)    
- false一定为false.     
所以如果判断结果是: 某个value不在这个hll中, 那肯定不在. 但是判断结果是某个value在这个hll中, 你不能当真.     
```    
postgres=> create table abc(id int8[],  hll_ids hll);    
CREATE TABLE    
postgres=> insert into abc select array_agg(id),     
hll_add_agg(hll_hash_bigint(id),16,7,0,0)     
from (select (random()*20000000000::int8)::int8 as id from generate_series(1,1000000)) t;    
INSERT 0 1    
postgres=> select sum (case when c1=c2 then 1 else 0 end)     
from     
(    
select t2.id,    
  hll_add(hll_ids, hll_hash_bigint(t2.id))=hll_ids as c1,     
  t1.id @> array[t2.id] as c2     
  from     
  ( select generate_series(1,10000)::int8 id) t2,   abc t1     
  )     
t;    
 sum     
-----    
 491    
(1 row)    
```    
如果需要非常精准的存在判断:  可以选择roaringbitmap(精确)  
[《PostgreSQL pg_roaringbitmap - 用户画像、标签、高效检索》](../201911/20191118_01.md)    
[《PostgreSQL 大量IO扫描、计算浪费的优化 - 推荐模块, 过滤已推荐. (热点用户、已推荐列表超大)》](../202006/20200601_01.md)  
[![Build Status](https://travis-ci.org/aggregateknowledge/postgresql-hll.svg?branch=master)](https://travis-ci.org/aggregateknowledge/postgresql-hll)    
Overview    
========    
This Postgres module introduces a new data type `hll` which is a [HyperLogLog](https://research.neustar.biz/2012/10/25/sketch-of-the-day-hyperloglog-cornerstone-of-a-big-data-infrastructure/) data structure. HyperLogLog is a **fixed-size**, set-like structure used for distinct value counting with tunable precision. For example, in 1280 bytes `hll` can estimate the count of tens of billions of distinct values with only a few percent error.    
In addition to the algorithm proposed in the [original paper](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf), this implementation is augmented to improve its accuracy and memory use without sacrificing much speed. See below for more details.    
This `postgresql-hll` extension was originally developed by the Science team from Aggregate Knowledge, now a part of [Neustar](https://research.neustar.biz). Please see the [acknowledgements](#acknowledgements) section below for details about its contributors.     
Algorithms    
----------    
A `hll` is a combination of different set/distinct-value-counting algorithms that can be thought of as a hierarchy, along with rules for moving up that hierarchy. In order to distinguish between said algorithms, we have given them names:    
### `EMPTY` ###    
A constant value that denotes the empty set.    
### `EXPLICIT` ###    
An explicit, unique, sorted list of integers in the set, which is maintained up to a fixed cardinality.    
### `SPARSE` ###    
A 'lazy', map-based implementation of HyperLogLog, a probabilistic set data structure. Only stores the indices and values of non-zero registers in a map, until the number of non-zero registers exceeds a fixed cardinality.    
### `FULL` ###    
A fully-materialized, list-based implementation of HyperLogLog. Explicitly stores the value of every register in a list ordered by register index.    
Motivation    
----------    
Our motivation for augmenting the original HLL algorithm went something like this:    
* Naively, a HLL takes `regwidth * 2^log2m` bits to store.    
* In typical usage, `log2m = 11` and `regwidth = 5`, it requires 10,240 bits or 1,280 bytes.    
* That's a lot of bytes!    
The first addition to the original HLL algorithm came from realizing that 1,280 bytes is the size of 160 64-bit integers. So, if we wanted more accuracy at low cardinalities, we could just keep an explicit set of the inputs as a sorted list of 64-bit integers until we hit the 161st distinct value. This would give us the true representation of the distinct values in the stream while requiring the same amount of memory. (This is the `EXPLICIT` algorithm.)    
The second came from the realization that we didn't need to store registers whose value was zero. We could simply represent the set of registers that had non-zero values as a map from index to values. This map is stored as a list of index-value pairs that are bit-packed "short words" of length `log2m + regwidth`. (This is the `SPARSE` algorithm.)    
Combining these two augmentations, we get a "promotion hierarchy" that allows the algorithm to be tuned for better accuracy, memory, or performance.    
Initializing and storing a new `hll` object will simply allocate a small sentinel value symbolizing the empty set (`EMPTY`). When you add the first few values, a sorted list of unique integers is stored in an `EXPLICIT` set. When you wish to cease trading off accuracy for memory, the values in the sorted list are "promoted" to a `SPARSE` map-based HyperLogLog structure. Finally, when there are enough registers, the map-based HLL will be converted to a bit-packed `FULL` HLL structure.    
Empirically, the insertion rate of `EMPTY`, `EXPLICIT`, and `SPARSE` representations is measured in 200k/s - 300k/s range, while the throughput of the `FULL` representation is in the millions of inserts per second on relatively new hardware ('10 Xeon).    
Naturally, the cardinality estimates of the `EMPTY` and `EXPLICIT` representations is exact, while the `SPARSE` and `FULL` representations' accuracies are governed by the guarantees provided by the original HLL algorithm.    
* * * * * * * * * * * * * * * * * * * * * * * * *    
Usage    
=====    
"Hello World"    
-------------    
        --- Make a dummy table    
        CREATE TABLE helloworld (    
                id              integer,    
                set     hll    
        );    
        --- Insert an empty HLL    
        INSERT INTO helloworld(id, set) VALUES (1, hll_empty());    
        --- Add a hashed integer to the HLL    
        UPDATE helloworld SET set = hll_add(set, hll_hash_integer(12345)) WHERE id = 1;    
        --- Or add a hashed string to the HLL    
        UPDATE helloworld SET set = hll_add(set, hll_hash_text('hello world')) WHERE id = 1;    
        --- Get the cardinality of the HLL    
        SELECT hll_cardinality(set) FROM helloworld WHERE id = 1;    
Now with the silly stuff out of the way, here's a more realistic use case.    
Data Warehouse Use Case    
-----------------------    
Let's assume I've got a fact table that records users' visits to my site, what they did, and where they came from. It's got hundreds of millions of rows. Table scans take minutes (or at least lots and lots of seconds.)    
    CREATE TABLE facts (    
        date            date,    
        user_id         integer,    
        activity_type   smallint,    
        referrer        varchar(255)    
    );    
I'd really like a quick (milliseconds) idea of how many unique users are visiting per day for my dashboard. No problem, let's set up an aggregate table:    
    -- Create the destination table    
    CREATE TABLE daily_uniques (    
        date            date UNIQUE,    
        users           hll    
    );    
    -- Fill it with the aggregated unique statistics    
    INSERT INTO daily_uniques(date, users)    
        SELECT date, hll_add_agg(hll_hash_integer(user_id))    
        FROM facts    
        GROUP BY 1;    
We're first hashing the `user_id`, then aggregating those hashed values into one `hll` per day. Now we can ask for the cardinality of the `hll` for each day:    
    SELECT date, hll_cardinality(users) FROM daily_uniques;    
You're probably thinking, "But I could have done this with `COUNT DISTINCT`!" And you're right, you could have. But then you only ever answer a single question: "How many unique users did I see each day?"    
What if you wanted to this week's uniques?    
    SELECT hll_cardinality(hll_union_agg(users)) FROM daily_uniques WHERE date >= '2012-01-02'::date AND date = '2012-01-01' AND    
          date <  '2013-01-01'    
    GROUP BY 1;    
Or how about a sliding window of uniques over the past 6 days?    
    SELECT date, #hll_union_agg(users) OVER seven_days    
    FROM daily_uniques    
    WINDOW seven_days AS (ORDER BY date ASC ROWS 6 PRECEDING);    
Or the number of uniques you saw yesterday that you didn't see today?    
    SELECT date, (#hll_union_agg(users) OVER two_days) - #users AS lost_uniques    
    FROM daily_uniques    
    WINDOW two_days AS (ORDER BY date ASC ROWS 1 PRECEDING);    
These are just a few examples of the types of queries that would return in milliseconds in an `hll` world from a single aggregate, but would require either completely separate pre-built aggregates or self-joins or `generate_series` trickery in a `COUNT DISTINCT` world.    