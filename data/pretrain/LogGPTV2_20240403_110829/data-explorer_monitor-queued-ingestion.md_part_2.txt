> [!NOTE]
> According to the default [batching policy](kusto/management/batching-policy.md), the default batching time is five minutes. Therefore, if the batch isn't sealed by other triggers, the batch will be sealed after five minutes.
When you see a long latency until data is ready for query, analyzing **Stage Latency** and **Discovery Latency** can help you understand whether the long latency is because of long latency in Azure Data Explorer, or is upstream to Azure Data Explorer. When the latency is in Azure Data Explorer itself, you can also detect the specific component responsible for the long latency.
### Stage Latency (preview)
Let's first look at the stage latency of our queued ingestion.
For an explanation of each stage, see [Batching stages](#batching-stages).
1. In the **Metrics** pane in Azure Monitor, select **Add Metric**.
1. Select *Stage Latency* as the **Metric** value and *Avg* as the **Aggregation** value.
1. Select the **Apply splitting** button and choose *Component Type* to segment the chart by the different ingestion components.
1. Select the **Add filter** button, and filter on the data sent to the *GitHub* database. After selecting the filter values, click away from the filter selector to close it.
Now the chart shows the latency of ingestion operations that are sent to GitHub database at each of the components through ingestion over time:
:::image type="content" source="media/monitor-batching-ingestion/stage-latency-by-component-chart.png" alt-text="Screenshot of the Metrics pane in Azure portal showing a chart of stage latency for ingestion from the github database aggregated by avg and split by component type." lightbox="media/monitor-batching-ingestion/stage-latency-by-component-chart.png":::
We can tell the following information from this chart:
* The latency at the *Event Hubs Data Connection* component is approximately 0 seconds. This makes sense, because **Stage Latency** only measures latency from when a message is discovered by Azure Data Explorer.
* The longest time in the ingestion process (approximately 5 minutes) passes from when the *Batching Manager* component received data to when the *Ingestion Manager* component received data. In this example, we use the default batching policy for the *GitHub* database. As noted, the latency time limit for the default batching policy is 5 minutes, so this most likely indicates that nearly all the data was batched by time, and most of the latency time for the queued ingestion was due to the batching itself.
* The storage engine latency in the chart represents the latency until data is stored in the *Azure Data Explorer Storage Engine* and is ready for query. You can see that the average total latency from the time of data discovery by Azure Data Explorer until it's ready for query is 5.2 minutes.
### Discovery Latency
If you use ingestion with data connections, you may want to estimate the latency upstream to Azure Data Explorer over time, as long latency may also occur before Azure Data Explorer gets the data for ingestion. For that purpose, you can use the **Discovery Latency** metric.
1. Select **+ New chart**.
1. Select *Discovery Latency* as the **Metric** value and *Avg* as the **Aggregation** value.
1. Select the **Apply splitting** button and choose *Component Type* to segment the chart by the different data connection component types. After selecting the splitting values, click away from the split selector to close it.
:::image type="content" source="media/monitor-batching-ingestion/discovery-latency-by-component-type-chart.png" alt-text="Screenshot of the Metrics pane in Azure portal showing a chart of discovery latency for ingestion from the GitHub database aggregated by avg and split by component type." lightbox="media/monitor-batching-ingestion/discovery-latency-by-component-type-chart.png":::
* You can see that for most of the duration the discovery latency is close to 0 seconds, indicating that Azure Data Explorer got data just after data enqueue. The highest peak of around 300 milliseconds is around February 13 at 14:00, indicating that at this time the Azure Data Explorer cluster received the data around 300 milliseconds after data enqueue.
## Understand the batching process
In the second stage of the queued ingestion flow, the *Batching Manager* component optimizes the ingestion throughput by batching the data it receives based on the ingestion [batching policy](kusto/management/batching-policy.md).
The following set of metrics helps you understand how your data is being batched during ingestion:
* **Batches Processed**: The number of batches completed for ingestion.
* **Batch Size**: The estimated size of uncompressed data in a batch aggregated for ingestion.
* **Batch Duration**: The duration of each individual batch from the moment the batch is opened until batch sealing.
* **Batch Blob Count**: The number of blobs in a completed batch for ingestion.
### Batches processed
Let's start with an overall view of the batching process by looking at the **Batches processed** metric.
1. In the **Metrics** pane in Azure Monitor, select **Add Metric**.
1. Select *Batches Processed* as the **Metric** value and *Sum* as the **Aggregation** value.
1. Select the **Apply splitting** button and choose *Batching Type* to segment the chart based on the reason the batch was sealed. For a complete list of batching types, see [Batching types](kusto/management/batching-policy.md#sealing-a-batch).
1. Select the **Add filter** button and filter on the batches sent to the *GitHub* database. After selecting the filter values, click away from the filter selector to close it.
The chart shows the number of sealed batches with data sent to the *GitHub* database over time, split by the *Batching Type*.
:::image type="content" source="media/monitor-batching-ingestion/batches-processed-by-batching-type-chart.png" alt-text="Screenshot of the Metrics pane in Azure portal showing a chart of batches processed for ingestion from the GitHub database aggregated by sum and split by batching type." lightbox="media/monitor-batching-ingestion/batches-processed-by-batching-type-chart.png":::
* Notice that there are 2-4 batches per time unit over time, and all batches are sealed by time as estimated in the [Stage Latency](#stage-latency-preview) section where you can see that it takes around 5 minutes to batch data based on the default batching policy.
### Batch duration, size, and blob count
Now let's further characterize the processed batches.
1. Select the **+ Add Chart** button for each chart to create more charts for the **Metric** values *Batch Duration*, *Batch Size*, and *Batch Blob Count*.
1. Use *Avg* as the **Aggregation** value.
1. As in the previous example, select the **Add filter** button, and filter on the data sent to the *GitHub* database.
:::image type="content" source="media/monitor-batching-ingestion/batch-count-duration-size-charts.png" alt-text="Screenshot of the Metrics pane in Azure portal showing charts of Batch blob count, Batch duration and Batch size metrics, for ingestion from the github database aggregated by avg and split by batching type." lightbox="media/monitor-batching-ingestion/batch-count-duration-size-charts.png":::
From the *Batch Duration*, *Batch Size*, and *Batch Blob Count* charts we can conclude some insights:
* The average batch duration is five minutes (according to the default batching policy). You should take this into account when looking at the total ingestion latency.
* In the *Batch Size* chart, you can see that the average size of batches is around 200-500 MB over time. The optimal size of data to be ingested is 1 GB of uncompressed data, and this size is also defined as a seal condition by the default batching policy. As there's not 1 GB of data to be batched over time, we don't see any batches sealed by size.
* The average number of blobs in the batches is around 160 blobs over time, which then decreases to 60-120 blobs. Based on the default batching policy, a batch can seal when the blob count is 1000 blobs. As we don't arrive at this number, we don't see batches sealed by count.
## Compare events received to events sent for ingestion
When applying event hub, IoT hub, or Event Grid ingestion, it can be useful to compare the number of events received by Azure Data Explorer to the number of events sent from the eventing source to Azure Data Explorer. The metrics **Events Received**, **Events Processed**, and **Events Dropped** allow you to make this comparison.
### Events Received
1. In the **Metrics** pane in Azure Monitor, select **Add Metric**.
1. Select *Events Received* as the **Metric** value and *Sum* as the **Aggregation** value.
1. Select the **Add filter** button above the chart and choose the **Property** value *Component Name* to filter the events received by a specific data connection defined on your cluster. In this example, we filter on the *GitHubStreamingEvents* data connection. After selecting the filter values, click away from the filter selector to close it.
Now the chart shows the number of events received by the selected data connection over time:
:::image type="content" source="media/monitor-batching-ingestion/events-received-chart.png" alt-text="Screenshot of the Metrics pane in Azure portal showing a chart of the events received during ingestion from the GitHub database aggregated over time." lightbox="media/monitor-batching-ingestion/events-received-chart.png":::
* In this chart, the *GitHubStreamingEvents* data connection receives around 200-500 events per time unit over time.
### Events Processed and Events Dropped
To see if any events were dropped by Azure Data Explorer, use the **Events Processed** and **Events Dropped** metrics.
1. On the chart you have already created, select **Add metric**.
1. Select *Events Processed* as the **Metric** value and *Sum* as the **Aggregation** value.
1. Select **Add metric** again and select *Events Dropped* as the **Metric** value and *Sum* as the **Aggregation** value.
The chart now shows the number of Events that were received, processed, and dropped by the *GitHubStreamingEvents* data connection over time.
:::image type="content" source="media/monitor-batching-ingestion/events-received-processed-dropped-chart.png" alt-text="Screenshot of the Metrics pane in Azure portal showing a chart with graphs of the events received, processed, and dropped during ingestion from the GitHub database aggregated over time." lightbox="media/monitor-batching-ingestion/events-received-processed-dropped-chart.png":::
* Almost all the received events were processed successfully by the data connection. There is one dropped event, which is compatible with the failed ingestion result due to bad request that we saw when [viewing the ingestion result metric](#view-the-ingestion-result).
### Compare events received in Azure Data Explorer to outgoing messages from event hub
You may also want to compare the number of events received to the number of events that were sent from event hub to Azure Data Explorer, by comparing the **Events Received** and **Outgoing Messages** metrics.
1. On the chart you have already created for **Events Received**, select **Add metric**.
1. Select **Scope** and in the **Select a scope** dialog, browse for, and select the namespace of the event hub that sends data to your data connection.
   :::image type="content" source="media/monitor-batching-ingestion/select-a-scope.png" alt-text="Screenshot of the Select a scope dialog in the Azure portal, showing a search for the github4demo in the list of event hubs namespaces." lightbox="media/monitor-batching-ingestion/select-a-scope.png":::
1. Select **Apply**
1. Select *Outgoing Messages* as the **Metric** value and *Sum* as the **Aggregation** value.
Click away from the settings to get the full chart that compares the number of events processed by the Azure Data Explorer data connection to the number of events sent from the event hub.
:::image type="content" source="media/monitor-batching-ingestion/all-event-metrics-chart.png" alt-text="Screenshot of the Metrics pane in Azure portal showing a chart with graphs for all of the events received, processed, dropped and during ingestion from the GitHub database aggregated over time." lightbox="media/monitor-batching-ingestion/all-event-metrics-chart.png":::
* Notice that all the events that were sent from event hub were processed successfully by the Azure Data Explorer data connection.
* If you have more than one event hub in the event hub namespace, you should filter the **Outgoing Messages** metric by the **Entity Name** dimension to get only data from the desired event hub in your event hub namespace.
> [!NOTE]
> There's no option to monitor outgoing message per consumer group. The **Outgoing Messages** metric counts the total number of messages that were consumed by all consumer groups. So, if you have a few consumer groups in your event hub, you may get a larger number of **Outgoing Messages** than **Events Received**.
## Related content
* [Monitor Azure Data Explorer performance, health, and usage with metrics](using-metrics.md)
* [Monitor Azure Data Explorer ingestion, commands, queries, and tables using diagnostic logs](using-diagnostic-logs.md)
* [Use Azure Monitor Insights](/azure/azure-monitor/insights/data-explorer)