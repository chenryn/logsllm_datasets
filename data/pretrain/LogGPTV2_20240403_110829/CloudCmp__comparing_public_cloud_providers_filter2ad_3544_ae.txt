u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
C1
C4
 50
 100
 150
 200
Response Time (ms)
(a) Send
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
C1
C4
 50
 100
 150
 200
Response Time (ms)
(b) Retrieve
C1
C4
 100
 200
 300
 400
 500
Queue Propagation Delay (ms)
(c) Propagation delay
Figure 9: The cumulative distributions of the response time to send or retrieve a message from a queue, and the propagation delay of a queue.
C1
C2
C4
)
s
m
(
i
e
m
T
d
a
o
n
w
o
D
l
 400
 350
 300
 250
 200
 150
 100
 50
 0
 14000
 12000
)
s
m
C1
C2
C4
 10000
i
(
e
m
T
d
a
o
n
w
o
D
l
 8000
 6000
 4000
 2000
 0
1
1
8
4
16
2
# of Concurent Operations
(a) 1KB Blob
8
4
2
16
# of Concurrent Operations
(b) 10MB Blob
32
32
Figure 8: The blob downloading time from each blob service under
multiple concurrent operations. The number of concurrent requests
ranges from 1 to 32. Note that the x-axes are on a logarithmic scale.
results with non-Java clients to eliminate overheads due to client
side inefﬁciency. As we will soon show, C1 and C2’s blob service
throughput is close to their intra-datacenter network bandwidth (see
Figure 10), suggesting that the bottleneck in throughput is unlikely
within the blob service itself. This is also true for C4’s blob service
throughput of a large instance, which more than doubles that of the
single core instance that we were using earlier. It appears that the
impact on an instance’s throughput due to other VMs colocated on
the machine is non trivial in C4.
In summary, we observe that client implementation and con-
tention from other VMs or along network paths signiﬁcantly impact
the perceived storage service performance (across the three cloud
platforms we study). Therefore, a more efﬁcient client implemen-
tation or less contention may improve the results in this paper.
We tested the consistency property of the blob services, and did
not ﬁnd inconsistency problems in the three providers. The charg-
ing models are similar for all three providers and are based on the
number of operations and the size of the blob. We omit these results
for brevity.
5.2.3 Queue Storage
We compare the queue services of C1 and C4. Similar to ta-
ble services, we only show the results from our Java-based client.
Cloud
Provider Center Name
Data
C1
C2
C4
C1.DC1
C1.DC2
C1.DC3
C2.DC1
C2.DC2
C4.DC1
C4.DC2
C4.DC3
C4.DC4
C4.DC5
C4.DC6
Location
Region
US
US
Europe
North Virginia
North California
Ireland
Dallas/Fort Worth, Texas US
US
Chicago, Illinois
US
Chicago, Illinois
Europe
Amsterdam, Netherlands
San Antonio, Texas
US
Asia
Singapore
Europe
Dublin, Ireland
Hong Kong
Asia
Table 6: The geographical location of the cloud data centers.
Figure 9(a) and 9(b) show the distributions of the response time of
sending and retrieving a message. The queue services are designed
to transfer only small messages up to 8KB. Thus, we choose a mes-
sage size of 50B in our measurement. The results show that both
services have large variations in response time. C4 is slightly faster
at sending messages while C1 is faster at retrieving messages. We
test the scalability of the queue services, up to 32 concurrent mes-
sages, and no signiﬁcant performance degradation is found, mostly
due to the small message size.
It is interesting to note that the response time of the queue ser-
vice is on the same order of magnitude as that of the table and blob
services. This suggests that although the queue service is simpler
and designed to be more efﬁcient compared to the other storage
services, the performance gain is insigniﬁcant. One may use the
table or blob service to implement a simple signaling framework
with similar functionality as the queue service without much per-
formance degradation.
We then measure the propagation delay of a queue. The delay
is deﬁned as the time between when a message is sent to an empty
queue and when it is available to be retrieved. A queue with shorter
propagation delay can improve the responsiveness of an applica-
tion. Figure 9(c) shows the distribution of the propagation delay
of the two services. We see that roughly 20% of the messages for
C1 take a long time (>200ms) to propagate through the queue,
while C4’s queue service has a similar median propagation delay
but lower variation. Finally, both services charge similarly–1 cent
per 10K operations.
5.3 Intra-cloud Network
In this section, we compare the intra-cloud network performance
of different providers. As of April 27th, when we conducted this
measurement, we found a total of 11 data center locations from
three providers: 6 for C4, 3 for C1, and 2 for C2. Similar to in-
stance types, we name each data center as provider.DCi. Table 6
summarizes the geographic locations of the data centers. We do not
10 1000
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
P
C
T
 800
 600
 400
 200
 0
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
.
1
1
C
.
2
1
C
.
3
1
C
.
1
2
C
.
2
2
C
.
1
4
C
.
2
4
C
.
3
4
C
.
4
4
C
.
5
4
C
.
6
4
C
 0
 0
 50
C1
C2
C3
C3 actual
C4
 150
 100
 200
Round Trip Time (ms)
 250
 300
Figure 10: The intra-datacenter TCP throughput between two in-
stances in all data centers we measure.
 500
)
s
p
b
M
 400
Figure 12: This ﬁgure shows the cumulative distribution of the op-
timal round trip time (RTT) to the instances deployed on a cloud
provider from 260 global vantage points. For C3 we also show the ac-
tual RTT from a vantage point to the instance returned by the cloud’s
DNS load balancing.
(
t
u
p
h
g
u
o
r
h
T
P
C
T
 300
 200
 100
 0
C1
C2
C4
Figure 11: The TCP throughput between two different US data cen-
ters of a cloud provider.
consider C3’s intra-cloud network performance because it does not
allow direct communication between instances.
When measuring the intra-cloud bandwidth and latency, we
choose the instance types that can at least fully occupy one CPU
core. This is to avoid the network performance degradation due to
CPU time sharing introduced by virtualization [35].
5.3.1 Intra-datacenter Network
Figure 10 shows the TCP throughput between two instances in
the same data center. In rare cases, a pair of instances are colocated
on the same machine and obtain a throughput larger than 1Gbps
which is the speed of the NIC. We ﬁlter out such pairs as they do
not measure actual network performance. We combine the results
for intra-zone and inter-zone cases, because the difference between
them is not signiﬁcant. From the ﬁgure, we see that the network
bandwidth of providers differs signiﬁcantly. C1 and C4 provide
very high TCP throughput which is close to the limit of the NIC
(1Gbps) and the variation is low. C2 has much lower throughput,
probably due to throttling or under-provisioned network.
In terms of latency, all data centers achieve low round trip time
(< 2ms) for all pairs of instances we test. This result is not sur-
prising, because the instances located in the same data center are
physically proximate. We omit the results to save space.
5.3.2 Inter-datacenter Network
Next, we show the performance of network paths between data
centers of the same provider. Because most providers focus on the
US market (and some only have US data centers), we only show
the results for data centers within the US. Figure 11 shows that
the throughput across datacenters is much smaller than that within
the datacenter for all providers. Both C1 and C4 have their me-
dian inter-datacenter TCP throughput higher than 200Mbps, while
C2’s throughput is much lower. Further, the variation in throughput
across datacenters is higher since the wide-area trafﬁc may have to
compete with much other trafﬁc.
We also compare the above result with the inter-datacenter
throughput obtained by a vanilla client that uses one TCP ﬂow
with the default send and receive window sizes conﬁgured by the
provider. All providers see a smaller throughput with this vanilla
client. This is because the network paths across data centers have
a high bandwidth-delay product (e.g., 50ms × 800Mbps = 5MB)
and the default window sizes are not conﬁgured appropriately. We
ﬁnd that the degradation is less with Linux instances, since mod-
ern Linux kernels auto-tune the size of the TCP receive buffer [11]
which can grow up to 4MB in the default setting. However the
degradation is far worse in the non-Linux instances since either
auto-tuning is not turned on or is conﬁgured poorly.
We ﬁnd that the latencies between data centers largely corre-
spond to the geographical distance between the data centers. The
latencies across providers are incomparable because their data cen-
ters are at different locations. Hence, we omit these results.
5.4 Wide-area Network
In this section, we compare the wide area network performance
of different providers. Figure 12 shows the distribution of the
optimal wide-area latency observed from a diverse set of vantage
points. We also show the actual latency of C3 which is the latency
to the instance returned by its DNS load balancing system. From
the ﬁgure, we can see that both the optimal and actual latency distri-
butions of C3 are lower than that of other providers. This could be
explained by the provider’s widely dispersed presence – we observe
48 unique IP addresses simultaneously serving our test application.
These IP addresses likely correspond to the virtual IPs of front-end
load balancers at distinct locations. Furthermore, the gap between
the optimal and actual latency of C3 is less than 20ms, suggesting
that its load balancing algorithm works very well in practice.
C1 and C4 have similar latency distributions:
they are worse
than C3, but much better than C2. A main difference is that C1
has a larger fraction of vantage points that have an optimal la-
tency higher than 100ms. Closer examination of our data reveals
that these high latency vantage points are mostly in Asia and South
America, where C1 does not have a presence (Table 6).
C2 has the worst latency distribution because it has the smallest
number of data centers. The ﬂat curve between 50ms and 100ms
corresponds to the latency differences between two groups of van-
tage points: the North American nodes and those in other conti-
11 100
)
s
(
i
e
m