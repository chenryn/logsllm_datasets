### Journal Coalition Scheme

The modified metadata during a single transaction commit typically occupies less than 10% of a 4KB block. This implies that the modified metadata can be packed into a single journal block, with significant space remaining. Based on this observation, we propose the journal coalition scheme, which consolidates control information and all modified metadata into fewer blocks (Fig. 10(a)). This reduces the total data volume in the data persistence path and mitigates the overhead of excessive control blocks (Section III.C.(2)).

The journal coalition scheme leverages the reliable write feature of eMMC hardware to maintain system reliability. When metadata blocks are written back to the file system region for checkpointing (Section II.B), the OS must use the eMMC driver to set the reliable write bit in CMD23, ensuring the atomicity of programming on the flash medium. Otherwise, there is a risk of data corruption. For example, if the system crashes while writing back a dirty metadata block M2, and the eMMC uses a normal write, the sudden crash could corrupt the data in block M2. Since only the modified portion of M2 is logged in the journal coalition block, the recovery thread cannot fully correct the contents of M2 (Fig. 10(b)). In contrast, if reliable write is used (M1 in Fig. 10(a)), the programmed metadata block will retain either the old or new data after a crash, allowing the recovery thread to apply the modified bytes without data corruption concerns.

When a metadata block to be logged is newly allocated, it cannot be merged into the journal coalition block because the entire block is considered modified. Our scheme handles this by writing an index structure in the journal coalition block and logging the metadata block to the next available slot in the journal region. In such cases, the journal thread submits a separate commit block at the end of the transaction commit, similar to the baseline journal scheme.

In summary, by leveraging the reliable write feature of eMMC hardware, the journal coalition scheme reduces data volume without introducing corruption hazards, preserving system reliability at the same level as the baseline system. The overhead of the journal coalition scheme comes from the reliable writes used in the checkpoint stage, which will be evaluated in Section VI.

### Flush-Ahead Commit

Figures 7 and 8 highlight the performance overhead of global barriers. However, we cannot simply remove cache flushing from the data persistence path for two reasons: (1) persistence, ensuring all previously submitted data, descriptors, and metadata blocks are persistent in non-volatile storage, and (2) ordering, ensuring the commit block is the last block written to the flash medium. In eMMC storage, there are less expensive methods to achieve persistence and ordering than buffer cache flushes.

As described in Section II.A, FUA (Force Unit Access) is a fine-grained method for persisting data to non-volatile storage. It achieves better performance in data persistence by direct persistence compared to flush-at-end, as shown in Fig. 8. However, in real scenarios, blocks may not be persisted in the order they are submitted due to reordering by both the OS and the device. Section V.B presents techniques to prevent reordering in the OS. To prevent reordering in the eMMC device, packed write is used because requests within a packed command are completed in the order they appear in the packed header. Combining FUA and packed write can achieve both data persistence and ordering. By leveraging these features, we propose the flush-ahead commit to mitigate the overhead of global barriers.

Figure 11 shows the time sequence comparison between the current scheme and the flush-ahead commit. In Fig. 11(b), the flush is triggered at the first step to make previously submitted data blocks durable in the flash medium. The remaining blocks are then persisted using direct persistence, except they are parceled in a packed command. This allows the flushing time to overlap with the execution time of descriptor compositing. Additionally, the synchronous wait time in step K of Fig. 11(a) is not necessary in the flush-ahead scheme because the ordering between the last commit block and previous blocks is guaranteed by the packed command.

### Implementation

This section describes the implementation details of the journal coalition and flush-ahead commit. Our implementation consists of approximately 1500 lines of modifications in the Linux kernel v3.10, spanning the file system, generic block driver, and eMMC driver layers.

#### A. Implementation of Journal Coalition

The journal coalition requires two supports from the I/O subsystem:
1. File system-level support for tracking modified segments in each metadata block.
2. eMMC driver support for setting the reliable write bit in CMD23.

Whenever the Ext4 file system modifies a metadata block, it calls `ext4_journal_get_write(create)_access` to link the modified block to the list of dirty metadata blocks. We instrumented the Linux source code to identify the size and offset of the modified segments in each metadata block, bookkeeping this information via a bitmap associated with the modified metadata block.

Additionally, we added a `REQ_REL` flag in the Linux I/O subsystem to pass the request for reliable write from the file system layer to the eMMC driver layer. As discussed in Section IV.A, the metadata block must be checkpointed to the file system region using the reliable write feature of the eMMC. This is achieved by modifying the `jbd2_log_do_checkpoint` function, which sends the logged metadata block to the main file system space when the journal region is running out of space. Specifically, we inserted code to set the `REQ_REL` flag for each write request triggered by this function. In the eMMC driver layer, the driver code detects the `REQ_REL` flag and enables the reliable write bit in CMD23 if the flag is set.

#### B. Implementation of Flush-Ahead Commit

The flush-ahead commit requires that the commit block is the last block to be packed when the eMMC driver performs a packed write. However, the I/O scheduler typically merges or reorders requests from the upper layer for I/O throughput, resulting in a different sequence of requests at the eMMC driver. To transfer the ordering information from the journal thread to the eMMC driver, we crafted three helper functions in the generic block layer:

- `blk_start_order_plug(struct blk_plug *plug);`
- `blk_insert_barrier(struct blk_plug *plug);`
- `blk_finish_order_plug(struct blk_plug *plug);`

These functions are based on the plug mechanism provided by the generic block layer, which prevents I/O requests from being processed by lower-level block drivers (e.g., the eMMC driver) before fully merging with adjacent requests. If the order between two I/O requests is required, their submission must be done within an active plug. Besides these helper functions, we added a variable named `ordered_list` in the I/O request data structure to chain ordered I/O requests. The detailed steps are as follows:

1. Buffer cache flush is invoked as soon as the dirty user data blocks are submitted to the block driver.
2. The journal thread is woken up, and `blk_start_order_plug` is invoked to set up a plug.
3. Descriptor blocks and all dirty metadata blocks are submitted to the generic block driver, where an I/O request is made for the submitted blocks. Due to the plug, this request will not be processed by the eMMC driver until `blk_finish_order_plug` is invoked.
4. `blk_insert_barrier` is invoked to inject an ordering hint between the previous submission and the following commit block.
5. The journal thread submits the commit block to the block driver, creating another I/O request in the generic block layer. Since this request is submitted after `blk_insert_barrier`, it is linked to the ordered list of the request submitted before the injection of the ordering hint.
6. `blk_finish_order_plug` is invoked to release the requests in the plug to the eMMC driver. The eMMC driver checks whether there are requests chained on the ordered list of each request. If the list is not empty, the eMMC driver performs a packed write for the request and each request chained on its list.

### Evaluation

In this section, we evaluate the reduction in data volume and data persistence time by applying the optimizations. We also evaluate the performance of mobile workloads on the baseline system (Ext4 with ordered journal mode), the optimized system (Ext4 with optimized journaling), and a system without reliability guarantees (Ext4 without journal, which has the best performance). Additionally, we evaluate the performance of SQLite databases, as SQLite is widely used by Android apps and generates many synchronous writes. Finally, we analyze and evaluate the overhead on checkpoint and recovery. All workloads are executed on the mobile system used in Section III.

#### A. Journal Block Traffic

In this subsection, we apply the journal coalition scheme alone to the optimized system. We run mobile workloads and measure the journal block ratio in every data persistence path. As shown in Table 4, across all eight workloads, the journal block ratio decreases by 57%-68%. Given that the data amount produced by applications remains the same during each run, the decreased journal block ratio indicates a reduction in total data volume in the optimized system. This benefits the system in three ways:

1. It leads to shorter synchronous waits for writing back in-memory descriptor and metadata blocks (the wait between steps FF and G in Section II.B).
2. The wait time for buffer cache flush decreases, as fewer blocks are in the buffer cache and waiting to be flushed to the flash medium.
3. As shown in Fig. 13, by applying the journal coalition, the normalized data volume decreases by 28%-47%, almost approximating the data volume in the "no journal" case. These reductions translate to an average 15.2% improvement over the baseline system in normalized performance (Fig. 13).
4. Due to the reduced data volume, NAND flash wears out more slowly than in the baseline system.

**Table 4. Comparison of Average Journal Block Ratio**

| Apps          | Baseline | Optimized |
|---------------|----------|-----------|
| Facebook      | 0.73     | 0.27      |
| Twitter       | 0.79     | 0.25      |
| Google Map    | 0.66     | 0.28      |
| Gmail         | 0.72     | 0.26      |
| Netflix       | 0.79     | 0.25      |
| Angry Birds   | 0.78     | 0.25      |
| Chrome        | 0.73     | 0.25      |
| Amazon        | 0.79     | 0.25      |

#### B. Data Persistence Time

In this subsection, we run mobile workloads on the system where the flush-ahead scheme is applied. The decrease in average data persistence time for each mobile app is shown in Fig. 14. Compared to the baseline system, the data persistence time decreases by 6%-19% across all eight apps. As shown in Fig. 13, the saved data persistence time translates to an average 6.7% boost over the baseline system.

**Figure 14. Normalized Runtime of Data Persistence**

#### C. Performance Boost on Mobile Workloads

In this section, we run mobile workloads on the system with both optimizations and compare the performance with the baseline system. Figure 12 provides a breakdown comparison of the average runtime of each step in the data persistence path. The wait time for writing back descriptors and metadata blocks decreases from 23% to 12% due to the reduction in journal blocks. Compared to the baseline system, the optimized system spends an extra 16% runtime on waiting for the completion of the forwarded cache flush. However, the total wait time for cache flush and commit block decreases from 50% to 37%. As shown in Fig. 13, these runtime decreases translate to a 5.4%-31% (average 22%) improvement over the baseline system.

Additionally, we evaluate the system without journaling. It provides the best performance but lacks reliability guarantees.