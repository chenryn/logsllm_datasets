 ; 
<	 ; <
; 
<
 = '(("&%#$ 
which is less than 10% of a 4KB block.  
The result implies that the modified metadata during one 
transaction commit can be packed into a single journal block, 
and  even  so,  there  is  still  large  space  left  in  that  journal 
block.  Based  on  this  observation,  we  propose  the  journal 
coalition scheme,  which compacts control  information  and 
all modified metadata into fewer blocks (Fig. 10(a)). It will 
reduce  the  total  data  volume  in  data  persistence  path. 
Meanwhile,  this  method  can  also  mitigate  the  overhead  of 
excessive number of control blocks (Section III.C.(2)). 
Journal coalition scheme leverages the reliable write fea-
ture of eMMC hardware to preserve the system reliability. 
When metadata blocks are written back to file system region 
for  check-pointing  (Section  II.B),  the  OS  needs  direct 
eMMC driver to set reliable write bit in CMD23 to ensure 
the atomicity of programming on flash medium. Otherwise, 
there will be hazard of data corruption. Considering the case
shown  in  Fig.  10(a),  system  crashes  when  the  dirtied 
metadata block M2 is being written back to the file system 
region. If eMMC is programming the metadata block with 
normal  write,  the  sudden  system  crash  would  corrupt  the 
data in block M2. Since only the modified portion of M2 are 
logged in journal coalition block, the recovery thread cannot 
correct the entire contents of M2 (Fig. 10(b)). In contrast, if 
reliable write is utilized in metadata write-back (M1 in Fig. 
10(a)),  the  being  programmed  metadata  block  will  retain 
(a) Journal Coalition Scheme. Blue frame: journal commit phase. Orange 
frame: metadata checkpoint phase 
(b) Recovery Failure. M2 is corrupted since reliable write is not applied 
in metadata checkpoint. 
Figure 10: Scheme of Leveraging eMMC Reliable Write 
either old data or new data after system crashes. In this way, 
the recovery thread can apply the modified bytes to the cor-
responding  metadata  without  worrying  about  data  corrup-
tion.
Note that when the metadata block to be logged is newly 
allocated,  it  cannot  be  merged  into  journal  coalition  block 
since the entire block is considered as modified. Our scheme 
can  also  handle  this  case  by  writing  an  index  structure  in 
journal  coalition  block  and  logging  the  metadata  block  to 
the next available slot in journal region. In this case, journal 
thread submits a separate commit block at the end of trans-
action commit as the baseline journal scheme does. 
    In  summary,  by  leveraging  reliable  write  feature  of 
eMMC hardware, journal coalition scheme can reduce data 
volume without introducing corruption hazard, so that sys-
tem  reliability  can  be  preserved  at  the  same  level  as  the 
baseline  system.  The  overhead  of  journal  coalition  derives 
from the reliable writes utilized in the checkpoint stage.  We 
will evaluate this overhead in Section VI. 
79
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 
	
	
	

		
	
	



	



	


			
	

		


	
	


	

	



			

Figure 11: Time Sequence Comparison 
B. Flush-Ahead Commit 
    Fig. 7 and Fig. 8 highlight the performance overhead of 
global barrier. But we cannot simply remove flushing cache 
from  the  data  persistence  path  for  two  reasons:  (1)  persis-
tence: ensuring all previously submitted data, descriptor and 
metadata blocks are persistent in non-volatile medium, and 
(2) ordering: ensuring the commit block is the last block to 
be written in flash medium. Nevertheless, in eMMC storage, 
there are less expensive method to achieve persistence and 
ordering than buffer cache flush. 
As  described  in  Section  II.A,  FUA  is  a  fine-grained 
method to persist data to non-volatile storage. It can achieve 
better performance of data persistence by direct persistence 
than  flush-at-end  as  shown  in  Fig.  8.  But  in  real  scenario, 
blocks are not persistence in the order of they get submitted. 
Both of OS and device may reorder them. Section V.B pre-
sents the technique to prevent the reordering in OS. To pre-
vent  the  reordering  in  the  eMMC  device,  packed  write  is 
leveraged here because the requests parceled in one packed 
command are completed in the order that they appear in the 
packed header. Therefore, combining FUA and packed write 
can  achieve  data  persistence  and  ordering  all  together.  By 
leveraging  these  two  features,  we  propose  flush-ahead 
commit for mitigating the overhead of global barrier. 
Fig.  11  shows  the  time  sequence  comparison  between 
current scheme and flush-ahead commit. In Fig. 11(b), flush 
is triggered at the first step to make the previously submitted 
data  blocks  durable  in  flash  medium.  Then,  the  remaining 
blocks  are  persisted  in  the  way  like direct  persistence,  ex-
cept  that  these  blocks  are  parceled  in  a  packed  command. 
By doing so, flushing time can be overlapped with the exe-
cution  time  of  descriptor  compositing.  Also,  the  synchro-
nous wait time in step  K of Fig. 11(a) is not necessary in 
flush-ahead  scheme  since  the  ordering  between  the  last 
commit block and the previous blocks is guaranteed by the 
packed command. 
V.IMPLEMENTATION 
This section describes the implementation details of jour-
nal  coalition  and  flush-ahead  commit.  Our  implementation 
consists  of  ~1500  lines  of  modifications  in  Linux  kernel 
v3.10,  which  spreads  over  the  file  system,  generic  block 
driver and eMMC driver layers.  
80
A. Implementation of Journal Coalition 
    Journal coalition requires two supports from I/O subsys-
tem: 1) file system level support for bookkeeping modified 
segments in each metadata block, and 2) eMMC driver sup-
port for setting reliable write bit in CMD23. 
Whenever Ext4 file system modifies a metadata block, it 
calls  ext4_journal_get_write(create)_access 
the 
modified block to the linked list of dirtied metadata blocks.
We  instrumented  Linux  source  code  at  places  where  these 
two functions are called to identify the size and offset of the 
modified segments in each metadata block. The information 
is then bookkept via a bitmap associated with the modified 
metadata block. 
link 
to 
by 
the 
achieved 
modifying 
Furthermore, we add a REQ_REL flag in Linux I/O sub-
system to pass the request of reliable write from file system 
layer  to  the  eMMC  driver  layer.  As  discussed  in  Section 
IV.A,  the  metadata  block  has  to  be  check-pointed  to  file 
system region by reliable write offered by eMMC. This can 
be 
function 
jbd2_log_do_checkpoint,  which  sends  the  logged  metadata 
block to main file system space when journal region is run-
ning out of space. To be more specific, we inserted codes to 
set REQ_REL flag upon each write request triggered by this 
function. In the eMMC driver layer, the driver code detects 
the REQ_REL flag upon each request from upper layers. In 
case  that  REQ_REL  flag  is  set,  the  eMMC  driver  enables
reliable write bit in CMD23. 
B. Implementation of Flush-Ahead Commit 
    Flush-Ahead  commit  requires  that  the  commit  block  is 
the last block to be packed when eMMC driver performs a
packed  write.  However,  I/O  scheduler  typically  merg-
es/reorders  requests  submitted  from  upper  layer  for  I/O 
throughput. The requests arrived at eMMC driver will be in 
a  sequence  different  from  that  journal  thread  submitted 
them.  To  allow  the  ordering  information  to  be  transferred 
from journal thread to eMMC driver, we crafted three helper 
functions in generic block layer: 
blk_start_order_plug(struct blk_plug *plug); 
blk_insert_barrier(struct blk_plug *plug); 
blk_finish_order_plug(struct blk_plug *plug); 
The  development  of  these  functions  is  based  on  the  plug 
mechanism  provided  by  generic  block  layer,  which  is  uti-
lized to prevent the I/O requests processed by the lower lev-
el  block  drivers  (i.e.  eMMC  driver)  before  fully  merging 
with the requests on adjacent addresses. If the order between 
two  I/O  requests  is  required,  the  submission  of  these  two 
requests  must  be  done  within  an  active  plug.  Beside  these 
helper  functions,  we  add  a  variable  named  ordered_list  in 
the data structure of I/O request for chaining the ordered I/O 
requests. More details are listed below:  
    1) buffer cache flush is invoked as soon as the dirtied user 
data  blocks  are  submitted  to  the  block  driver.  2)  journal 
thread  is  woken  up  and  helper  blk_start_order_plug is  in-
voked  to  setup  a  plug.  3)  descriptor  block  and  all  dirtied 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 
100%
80%
60%
40%
20%
others
wait commit block
wait writeback
wait flush done
compose descriptor 
data writeback 
0%
Baseline Optimized
Figure 12. Breakdown on Data Persistence of 
Baseline System and Optimized System 
e
c
n
a
m
r
o
f
r
e
P
d
e
z
i
l
a
m
r
o
N
1.5
1.3
1.1
0.9
0.7
2
Data Volume
1.6
e
m
u
l
o
V
a
t
a
D
d
e
z
i
l
a
m
r
o
N
1.2
0.8
0.4
B F J C N B F J C N B F J C N B F J C N B F J C N B F J C N B F J C N B F J C N
0
Figure 13. Normalized Performance Comparison of Real Workloads. B: baseline, F: flush ahead, 
J: journal coalition, C: J+F, N: no journal 
is 
metadata  blocks  are  submitted  to  generic  block  driver, 
where an I/O request is made for the submitted blocks. By 
the means of plug, this request won’t be touched by eMMC 
driver  before  blk_finish_order_plug 
invoked.  4)  
blk_insert_barrier  is  invoked  to  inject  ordering  hint  be-
tween  the  previous  submission  and  the  following  commit 
block.  5)  journal  thread  submits  the  commit  block  to  the 
block driver. Another I/O request is made in generic block 
layer. But since this request is submitted after the invoking 
of blk_insert_barrier, it will be linked to the ordered list of 
the request submitted before the injection of ordering hint. 
Since I/O scheduler is not aware the existing of this list, the 
order  of  requests  chained  on  ordered  list  can  preserve.  6) 
blk_finish_order_plug  is  invoked  to  release  the  requests  in 
plug  to  eMMC  driver.  The  latter  will  detect  whether  there 
are requests chained on ordered list of each request. If it’s 
not  empty  for  one  request,  eMMC  driver  performs  packed 
write for this request and each request chained on its list.
VI.EVALUATION 
    In  this  section,  we  first  evaluate  the  reduction  on  data 
volume and data persistence time by applying optimizations. 
Then, we evaluate the performance of mobile workloads on 
baseline  system  (Ext4  with  ordered  journal  mode),  opti-
mized system (Ext4 with optimized journaling) and system 
without  reliability  guarantee  (Ext4  without  journal,  which 
has the best performance). In addition, we evaluate the per-
formance of SQLite database, since SQLite is widely used 
by  Android  apps  and  brings  lots  of  synchronous  writes  to 
system.  At  last,  we  analyze  and  evaluate  the  overhead  on 
checkpoint and recovery. All workloads are executed on the 
mobile system used in Section III. 
A. Journal Block Traffic 
In  this  subsection,  journal  coalition  scheme  alone  is  ap-
plied  to  optimized  system.  We  run  the  mobile  workloads 
and measure the journal block ratio in every data persistence 
path. As shown in Table 4, across all eight workloads, the 
journal block ratio decreases by 57%-68%. Considering the 
data  amount  produced  by  applications  remains  the  same 
during each repeated run, the decreased journal block ratio 
means  the  total  data  volume  is  reduced  in  the  optimized 
system. It can benefit the whole system in three-fold. First, 
it leads to shorter synchronous wait for writing back the in-
memory  descriptor  and  metadata  blocks  (the  wait  between 
steps FF and G in Section II.B). Second, the wait time for 
buffer cache flush will decrease, since less blocks are in the 
buffer  cache  and  waiting  for  flushing  out  to  flash  medium 
than baseline system. As shown by the data volume lines in 
Fig. 13, by applying journal coalition in system, the normal-
ized  data  volume  decreases  28%-47%  and  almost  approxi-
mates  the  data  volume  in  the  case  of  “no  journal”.  These 
data  reduction  translates  to  average  15.2%  improvement 
over the baseline system on normalized performance shown 
in  Fig.  13.  Third,  due  to  the  reduced  data  volume,  NAND 
flash are worn out slower than the baseline system. 
Table 4. Comparison of Average Journal Block Ratio 
Apps 
Baseline
Optimized 
Facebook 
Twitter 
Google Map 
0.73
0.27 
Gmail 
0.79
0.25 
0.66
0.28 
Netflix 
0.79
0.26 
0.72
0.27 
Angry Birds 
0.78
0.25 
Apps 
Baseline
Optimized 
B. Data Persistence Time 
Chrome 
0.73 
0.25 
Amazon 
0.79 
0.25 
Baseline
Flush-Ahead
1.2
1
0.8
0.6
0.4
0.2
0
e
c
n
a
m
r
o
f
r
e
P
d
e
z
i
l
a
m
r
o
N
In this subsection, we 
run  mobile  workloads 
on  the  system  where 
Flush-Ahead  scheme  is 
applied.  The  decrease 
of  average  data  persis-
tence  time  in  each  mo-
bile app is shown in Fig. 
14.  Compared  to  the 
baseline 
by 
hiding  the  flush  time  and  exploiting  the  benefits  from 
packed  write,  the  data  persistence  time  decreases  6%-19% 
across  all  eight  apps.  As  shown  in  Fig.  13,  the  saved  data 
persistence  time  translates  to  average  6.7%  boost  over  the 
baseline system. 
C. Performance Boost on Mobile Workloads 
Figure 14. Normalized Runtime of 
Data Persistence 
system, 
In this section, we run mobile workloads in system with
both optimizations, and compare the performance with base-
81
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 
line system. Fig. 12 is a breakdown comparison on average 
runtime of each step in the data persistence path. The obser-
vation is that the wait time for writing back descriptor and 
metadata blocks decreases from 23% to 12% due to the re-
duction of journal blocks. Compared to the baseline system, 
the optimized system spends extra 16% runtime on waiting 
for completion of the forwarded cache flush. But the sum of 
wait time for cache flush and commit block decreases from 
50% to 37%. As shown in Fig. 13, these runtime decreases 
translate to 5.4%-31% (average 22%) improvement over the 
baseline system.  
In  addition,  we  evaluate  the  system  without  journal.  It 