File Copy 256 bufsize 500 maxblocks
File Copy 4096 bufsize 8000 maxblocks
Pipe Throughput
Pipe-based Context Switching
Process Creation
Shell Scripts (1 concurrent)
Shell Scripts (8 concurrent)
System Call Overhead
System Benchmarks Index Score
C. Performance
We use UnixBench to compare the performance overhead
before and after enabling our system. Table III lists all results.
As the results show, CPU benchmarks such as Dhrystone
(testing integer and string operations) and Whetstone (testing
ﬂoat point arithmetic performance) incur negligible overhead.
Other benchmarks like shell scripts, pipe throughput, and
system call also trigger little overhead. The pipe-based context
switching does incur a 61.53% overhead in the case of one
parallel copy, but it decreases to 1.63% for 8 parallel copies.
We anticipate that inter-cgroup context switching involves
enabling/disabling the performance event monitor, whereas
intra-cgroup context switching does not involve any such
overhead. This could explain why 8 parallel copies can maintain
a similar performance level with the power-based namespace
disabled. In addition, context switching only contributes to a
very small portion of the whole-system performance overhead,
so there is trivial impact for the normal use. As demonstrated
in the last row of Table III, the overall performance overheads
for the UnixBench are 9.66% for one parallel copy and 7.03%
for 8 parallel copies, respectively. Our system’s performance
depends heavily on the implementation of perf event cgroup
and could improve with the advancement of a performance
monitoring subsystem.
VII. DISCUSSION
A. Synergistic Power Attacks without the RAPL Channel
We also notice that servers in some container clouds are not
equipped with RAPL or other similar embedded power meters.
Those servers might still face power attacks. Without power-
capping tools like RAPL, those servers might be vulnerable to
host-level power attacks on a single machine. In addition, if
power data is not directly available, advanced attackers will try
to approximate the power status based on the resource utilization
information, such as the CPU and memory utilization, which is
still available in the identiﬁed information leakages. It would be
better to make system-wide performance statistics unavailable
to container tenants.
B. Complete Container Implementation
The root cause for information leakage and synergistic
power attack is the incomplete implementation of the isolation
mechanisms in the Linux kernel. It would be better to introduce
more security features, such as implementing more namespaces
and control groups. However, some system resources are
still difﬁcult to be partitioned, e.g., interrupts, scheduling
Fig. 8: The accuracy of our
energy modeling approach to
estimate the active power for
the container from aggregate
event usage and RAPL.
Fig. 9: Transparency: a ma-
licious container (Container
2) is unaware of the power
condition for the host.
power usage with the ground truth obtained through RAPL.
The power consumption is equal to the energy consumption
per second. Due to the restriction of the security policy of
the Docker container, we select a subset of SPECCPU2006
benchmarks that are feasible to run inside the container and
have no overlap with the benchmarks used for power modeling.
The error ξ is deﬁned as follows:
ξ =
|(ERAPL − Δdiﬀ ) − Mcontainer|
ERAPL − Δdiﬀ
,
(4)
where ERAPL is the power usage read from RAPL on the
host, and Mcontainer is the modeled power usage for the same
workload read within the container. Note that both the host
and container consume power at an idle state with trivial
differences. We use a constant Δdiﬀ as the modiﬁer reﬂecting
the difference in power consumption at an idle state for a host
and a container. The results, illustrated in Figure 8, show that
our power modeling is accurate as the error values of all the
tested benchmarks are lower than 0.05.
B. Security
We also evaluate our system from the security perspective.
With the power-based namespace enabled, the container should
only retrieve the power consumed within the container and be
unaware of the host’s power status. We launch two containers
in our testbed for comparison. We run the SPECCPU2006
benchmark in one container and leave the other one idle. We
record the power usage per second of both containers and the
host. We show the results of 401.bzip2 in Figure 9. All other
benchmarks exhibit similar patterns.
When both containers have no workload,
their power
consumption is at the same level as that of the host, e.g., from
0s to 10s. Once container 1 starts a workload at 10s, we can ﬁnd
that the power consumption of container 1 and the host surges
simultaneously. From 10s to 60s, container 1 and the host have
a similar power usage pattern, whereas container 2 is still at a
low power consumption level. Container 2 is unaware of the
power ﬂuctuation on the whole system because of the isolation
enforced by the power-based namespace. This indicates that
our system is effective for isolating and partitioning power
consumption for multiple containers, and thus it can neutralize
the synergistic power attacks.
246
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
information, and temperature. People also argue that
the
complete container implementation is no different from a virtual
machine, and loses all the container’s advantages. It is a trade-
off for containers to deal with. The question of how to balance
security, performance, and usability in container clouds needs
further investigation.
VIII. RELATED WORK
In this section, we list some research efforts that inspire
our work and highlight the differences between our work and
previous research. We mainly discuss research works in the
following three areas:
A. Performance and Security Research on Containers
Since containers have recently become popular, researchers
are curious about the performance comparison between con-
tainers and hardware virtualization. Felter et al. compared
the performance of Docker and KVM by using a set of
benchmarks covering CPU, memory, storage, and networking
resources [14]. Their results show that Docker can achieve
equal or better performance than KVM in all cases. Spoiala
et al. [34] used the Kurento Media Server to compare the
performance of WebRTC servers on both Docker and KVM.
They also demonstrated that Docker outperforms KVM and
could support real-time applications. Morabito et al. [30]
compared the performance between traditional hypervisor and
OS-level virtualization with respect to computing, storage,
memory, and networks. They conducted experiments on Docker,
LXC, and KVM and observed that Disk I/O is still the bottleneck
of the KVM hypervisor. All of these works demonstrate that
container-based OS-level virtualization can achieve a higher
performance than hardware virtualization. Besides performance,
the security of a container cloud is always an important research
area. Gupta [20] gave a brief overview of Docker security.
Bui [11] also performed an analysis on Docker containers,
including the isolation problem and corresponding kernel
security mechanisms. They claimed that Docker containers
are fairly secure with default conﬁgurations. Grattaﬁori et
al. [17] summarized a variety of potential vulnerabilities
of containers. They also mentioned several channels in the
memory-based pseudo ﬁle systems. Previous research efforts
on the performance and security of containers encourage us
to investigate more on how containers can achieve the same
security guarantees as hardware virtualization, but with trivial
performance trade-offs. We are among the ﬁrst to systematically
identify the information leakage problem in containers and
investigate potential container-based power attack threats built
upon these leakage channels.
B. Cloud Security and Side/Covert Channel Attacks
Cloud security has received much attention from both
academia and industry. Co-residence detection in the cloud
settings is the most closely related research topic to our
work. Co-residence detection was ﬁrst proposed by Ristenpart
et al. [31]. They demonstrated that an attacker can place a
malicious VM co-resident with a target VM on the same sever
and then launch side-channel and covert-channel attacks. Two
previous works [36], [42] show that it is still practical to achieve
co-residence in existing mainstream cloud services. To verify
co-residence on the same physical server, attackers typically
leverage side channels or covert channels, e.g., one widely
adopted approach is to use cache-based covert channels [22],
[35], [41]. Multiple instances locating on the same package
share the last-level caches. By using some dedicated opera-
tions, such as cﬂush [44], attackers can detect co-residence
by measuring the time of cache accessing. Liu et al. [27]
demonstrated that l3 cache side-channel attacks are practical
for cross-core and cross-VM attacks. Zhang et al. conducted
real side-channel attacks on the cloud [46], [47] and proposed
several defense mechanisms to mitigate those attacks [40], [45],
[48]. In particular, they demonstrated that cross-tenant side-
channel attacks can be successfully conducted in PaaS with co-
resident servers [47]. Besides the cache-based channel, memory
bus [38] and memory deduplication [39] have also proved to
be effective for covert-channel construction. Different from
existing research efforts on side/covert channels, we discover a
system-wide information leakage in the container cloud settings
and design a new methodology to quantitatively assess the
capacity of leakage channels for co-residence detection. In
addition, compared to the research on minimizing the kernel
attack surface for VMs [18], we proposed a two-stage defense
mechanism to minimize the space for information leakages and
power attacks on container clouds.
System status information, such as core temperature and
system power consumption, have also been used to build
side/covert channels. Thiele et al. [10], [28] proposed a thermal
covert channel based on the temperature of each core and tested
the capacity in a local testbed. Power consumption could also be
abused to break AES [23]. In our work, we do not use the power
consumption data as a covert channel to transfer information.
Instead, we demonstrate that adversaries may leverage the host
power consumption leakage to launch more advanced power
attacks.
C. Power Modeling
When hardware-based power meter is absent, power mod-
eling is the approach to approximating power consumption.
Russell et al. [32] and Chakrabarti et al. [12] proposed
instruction-level power modeling. Their works indicate that
the number of branches affects power consumption. There
are several works of approximating power consumption for
VMs. Both works [21], [24] demonstrate that VM-level power
consumption can be estimated by CPU utilization and last-level
cache miss. Mobius et al. [29] broke the power consumption
of VM into CPU, cache, memory, and disk. BITWATTS [13]
modeled the power consumption at a ﬁner-grained process
level. Shen et al. [33] proposed a power container to account
for energy consumption of requests in multi-core systems.
Our defense against the synergistic power attack is mainly
inspired by the power modeling approach for VMs. We propose
a new power partitioning technique to approximate the per-
container power consumption and reuse the RAPL interface,
thus addressing the RAPL data leakage problem in the container
settings.
IX. CONCLUSION
Container cloud services have become popular for provid-
ing lightweight OS-level virtual hosting environments. The
emergence of container technology profoundly changes the eco-
system of developing and deploying distributed applications in
247
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
the cloud. However, due to the incomplete implementation
of system resource partitioning mechanisms in the Linux
kernel, there still exist some security concerns for multiple
container tenants sharing the same kernel. In this paper, we
ﬁrst present a systematic approach to discovering information
leakage channels that may expose host information to containers.
By exploiting such leaked host information, malicious container
tenants can launch a new type of power attack that may
potentially jeopardize the dependability of power systems in
the data centers. Additionally, we discuss the root causes of
these information leakages and propose a two-stage defense
mechanism. Our evaluation demonstrates that the proposed
solution is effective and incurs trivial performance overhead.
REFERENCES
[1] Burstable Performance Instances. https://aws.amazon.com/ec2/instance-
types/#burst.
[2] Containers Not Virtual Machines Are
http://www.linuxjournal.com/content/containers.
the
Future Cloud.
[3] ElasticHosts: Linux container virtualisation allows us to beat AWS on
price. http://www.computerworlduk.com/news/it-leadership/elastichosts-
linux-container-virtualisation-allows-us-beat-aws-on-price-3510973/.
IBM Cloud metering and billing. https://www.ibm.com/developerworks/c-
loud/library/cl-cloudmetering/.
[4]
[5] OnDemand Pricing Calculator.
offering/pricing-calculator/on-demand.
http://vcloud.vmware.com/service-
[6] Perf Wiki. https://perf.wiki.kernel.org/index.php/Main Page.
[7] Prime95 Version 28.9. http://www.mersenne.org/download/ .
[8] Travel nightmare for ﬂiers after power outage grounds Delta.
http://money.cnn.com/2016/08/08/news/companies/delta-system-outage-
ﬂights/.
[9] L. A. Barroso and U. H¨olzle. The Case for Energy-Proportional
Computing. Computer, 2007.
[10] D. B. Bartolini, P. Miedl, and L. Thiele. On the Capacity of Thermal
Covert Channels in Multicores. In ACM EuroSys, 2016.
[11] T. Bui. Analysis of Docker Security. arXiv preprint arXiv:1501.02967,
2015.
[12] C. Chakrabarti and D. Gaitonde. Instruction Level Power Model of
Microcontrollers. In IEEE ISCAS, 1999.
[13] M. Colmant, M. Kurpicz, P. Felber, L. Huertas, R. Rouvoy, and A. Sobe.
Process-level Power Estimation in VM-based Systems. In ACM EuroSys,
2015.
[14] W. Felter, A. Ferreira, R. Rajamony, and J. Rubio. An Updated
Performance Comparison of Virtual Machines and Linux Containers. In
IEEE ISPASS, 2015.
[15] K. Ganesan, J. Jo, W. L. Bircher, D. Kaseridis, Z. Yu, and L. K. John.
System-level Max Power (SYMPO) - A Systematic Approach for Esca-
lating System-level Power Consumption using Synthetic Benchmarks.
In ACM PACT, 2010.
[16] K. Ganesan and L. K. John. MAximum Multicore POwer (MAMPO)
- An Automatic Multithreaded Synthetic Power Virus Generation
Framework for Multicore Systems. In ACM SC, 2011.
[17] A. Grattaﬁori. NCC Group Whitepaper: Understanding and Hardening
Linux Containers, 2016.
[18] Z. Gu, B. Saltaformaggio, X. Zhang, and D. Xu. FACE-CHANGE:
Application-Driven Dynamic Kernel View Switching in a Virtual
Machine. In IEEE/IFIP DSN, 2014.
[19] P. Guide.
Intel R(cid:2) 64 and IA-32 Architectures Software Developers
Manual, 2011.
[20] U. Gupta. Comparison between security majors in virtual machine and
linux containers. arXiv preprint arXiv:1507.07816, 2015.
[21] Z. Jiang, C. Lu, Y. Cai, Z. Jiang, and C. Ma. VPower: Metering Power
Consumption of VM. In IEEE ICSESS, 2013.
[22] M. Kayaalp, N. Abu-Ghazaleh, D. Ponomarev, and A. Jaleel. A High-
Resolution Side-Channel Attack on Last-Level Cache. In IEEE DAC,
2016.
248
[23] P. Kocher, J. Jaffe, and B. Jun. Differential Power Analysis. In Annual
International Cryptology Conference, 1999.
[24] B. Krishnan, H. Amur, A. Gavrilovska, and K. Schwan. VM Power
Metering: Feasibility and Challenges. ACM SIGMETRICS Performance
Evaluation Review, 2011.
[25] C. Lefurgy, X. Wang, and M. Ware. Power Capping: A Prelude to
Power Shifting. Cluster Computing, 2008.
[26] C. Li, Z. Wang, X. Hou, H. Chen, X. Liang, and M. Guo. Power Attack
Defense: Securing Battery-Backed Data Centers. In IEEE ISCA, 2016.
[27] F. Liu, Y. Yarom, Q. Ge, G. Heiser, and R. B. Lee. Last-Level Cache
Side-Channel Attacks are Practical. In IEEE S&P, 2015.
[28] R. J. Masti, D. Rai, A. Ranganathan, C. M¨uller, L. Thiele, and S. Capkun.
Thermal Covert Channels on Multi-core Platforms. In USENIX Security,
2015.
[29] C. Mobius, W. Dargie, and A. Schill. Power Consumption Estimation
Models for Processors, Virtual Machines, and Servers. IEEE Transactions
on Parallel and Distributed Systems, 2014.
[30] R. Morabito, J. Kj¨allman, and M. Komu. Hypervisors vs. Lightweight
Virtualization: A Performance Comparison. In IEEE IC2E, 2015.
[31] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey, You, Get Off
of My Cloud: Exploring Information Leakage in Third-Party Compute
Clouds. In ACM CCS, 2009.
J. T. Russell and M. F. Jacome. Software Power Estimation and
Optimization for High Performance, 32-bit Embedded Processors. In
IEEE ICCD, 1998.
[32]
[33] K. Shen, A. Shriraman, S. Dwarkadas, X. Zhang, and Z. Chen.
Power Containers: An OS Facility for Fine-Grained Power and Energy
Management on Multicore Servers. ACM ASPLOS, 2013.
[34] C. C. Spoiala, A. Calinciuc, C. O. Turcu, and C. Filote. Performance
comparison of a WebRTC server on Docker versus Virtual Machine. In
IEEE DAS, 2016.
[35] E. Tromer, D. A. Osvik, and A. Shamir. Efﬁcient Cache Attacks on
AES, and Countermeasures. Journal of Cryptology, 2010.
[36] V. Varadarajan, Y. Zhang, T. Ristenpart, and M. Swift. A Placement
Vulnerability Study in Multi-Tenant Public Clouds. In USENIX Security,
2015.
[37] Q. Wu, Q. Deng, L. Ganesh, C.-H. Hsu, Y. Jin, S. Kumar, B. Li,
J. Meza, and Y. J. Song. Dynamo: Facebooks Data Center-Wide Power
Management System. IEEE ISCA, 2016.
[38] Z. Wu, Z. Xu, and H. Wang. Whispers in the Hyper-space: High-speed
[39]
Covert Channel Attacks in the Cloud. In USENIX Security, 2012.
J. Xiao, Z. Xu, H. Huang, and H. Wang. Security Implications of
Memory Deduplication in a Virtualized Environment. In IEEE/IFIP
DSN, 2013.
[40] Q. Xiao, M. K. Reiter, and Y. Zhang. Mitigating Storage Side Channels
Using Statistical Privacy Mechanisms. In ACM CCS, 2015.
[41] Y. Xu, M. Bailey, F. Jahanian, K. Joshi, M. Hiltunen, and R. Schlichting.
An Exploration of L2 Cache Covert Channels in Virtualized Environ-
ments. In ACM CCSW, 2011.
[42] Z. Xu, H. Wang, and Z. Wu. A Measurement Study on Co-residence
Threat inside the Cloud. In USENIX Security, 2015.
[43] Z. Xu, H. Wang, Z. Xu, and X. Wang. Power Attack: An Increasing
Threat to Data Centers. In NDSS, 2014.
[44] Y. Yarom and K. Falkner. FLUSH+ RELOAD: A High Resolution, Low
Noise, L3 Cache Side-Channel Attack. In USENIX Security, 2014.
[45] Y. Zhang, A. Juels, A. Oprea, and M. K. Reiter. HomeAlone: Co-
residency Detection in the Cloud via Side-Channel Analysis. In IEEE
S&P, 2011.
[46] Y. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Cross-VM Side
Channels and Their Use to Extract Private Keys. In ACM CCS, 2012.
[47] Y. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Cross-Tenant
Side-Channel Attacks in PaaS Clouds. In ACM CCS, 2014.
[48] Y. Zhang and M. K. Reiter. D¨uppel: Retroﬁtting Commodity Operating
Systems to Mitigate Cache Side Channels in the Cloud. In ACM CCS,
2013.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply.