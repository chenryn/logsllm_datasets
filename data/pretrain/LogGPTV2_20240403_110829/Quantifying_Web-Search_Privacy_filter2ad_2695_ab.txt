In this case, we quantify privacy as to what extent the
obfuscation function leads to the deformation of the struc-
ture of the queries when partitioned by the linkage attack.
This is measured by computing how many pairs of query
events from the target user end up in diﬀerent clusters (i.e.
the false negatives of the attack). Similarly, we measure the
false positives of the attack by computing how many pairs of
query events from the target user and fake events end up in
the same cluster. These errors reﬂect diﬀerent privacy gains
of the user.
2.5.2 Semantic Privacy
As another objective, assume that the user wants to pro-
tect the privacy at a higher semantic level (instead of query
level discussed above). Assume that we can map a set of
query events to a semantic proﬁle of interests (for example,
quantifying that a user is a businessman or a technical per-
son, captured through some histogram over a set of semantic
topics). By constructing such a semantic proﬁle for the sets
SU , S1 and S2, we measure the eﬀectiveness of the obfusca-
tion mechanism by quantifying the diﬀerences between the
semantic proﬁles of SU compared to that of the two clusters
S1 and S2. The higher the distance is, the less semantic in-
formation about the user is leaked to the adversary. Similar
approach could be applied for other higher level aspects of
the user proﬁle, for example, temporal aspects of the user
search activity. The semantic privacy metric reﬂects how
much information about the user’s proﬁle is in adversary’s
estimate, i.e., the mutual information between user’s proﬁle
2This metric has also been used in other domains, e.g., in
location privacy [33].
AOL Dataset
Total number of users
Total number of queries
657,426
21,011,340
Our Sampled Dataset
Total number of users
Total number of queries
Average number of queries per user
100
73,984
828.62 (±133.54)
Our TMN Dataset
Total number of users
Total number of queries
Average number of queries per user
100
90,851
908.51 (±394.81)
Table 1: Query dataset statistics
and the estimated proﬁle. This is similar to the information-
theoretic mutual information metric, with the diﬀerence that
we do not need to estimate the probability distribution over
the proﬁles (which is very diﬃcult if not impossible [9]).
2.5.3 Relative Privacy
We quantify privacy with respect to other objectives in
a similar manner. One important remark here is that the
above two metrics quantify what we call as “absolute” pri-
vacy obtained from the obfuscation mechanism.
In fact,
there is an inherent randomness in the browsing behavior
of each user itself without any obfuscation. It is important
to capture the additional relative value of privacy and ran-
domness added by the obfuscation mechanism. We call this
the “relative” privacy added by the obfuscation mechanism
and is quantiﬁed as follows for a given metric. Consider an
adversary that applies a linkage attack on SO even though
there was no obfuscation applied (i.e. SO = SU ) to obtain
partition C(SU , L, k). Now, comparing the privacy metrics
on the output of linkage attacks C(SO, L, k) and C(SU , L, k)
allows us to capture the “relative” privacy oﬀered by the ob-
fuscation for a given metric. More precisely, we compare SO
and SU by computing the false negative (positive) metric as
the fraction of query pairs that are (are not) in the same par-
tition in SU but are not (are) in the same partitions in SO.
This captures the relative error introduced to the clustering
attack due to obfuscation, and it allows to compare diﬀerent
obfuscation mechanisms in a fair manner by removing the
eﬀect of the user’s proﬁle on her privacy.
3. USER SEARCH BEHAVIOUR
In this section, we model the web search behaviour of users
in terms of the content they search for and the contextual
information associated to their web searches. We ﬁrst de-
scribe the web-search query dataset that we used for our
experiments. Then, we explain how we model a query event
based on its features.
3.1 Dataset
The most extensive, freely accessible, real world web search
query dataset is the AOL dataset [27] from 2006. This
dataset contains about 21 million queries from nearly 650,000
users during a three month period. However, only about 850
users issue more than 500 queries over these three months.
To have a more realistic dataset, that reﬂects the behaviour
Feature
Description
Query comparison Description
Behavioural features
TimeQuery
DayWeek
TimeDay
NumClicks
Timestamp
Weekday number
Hour of day
Number of landing pages clicked
Semantic features
Frequency of terms in the query
TFQuery
Frequency of terms in landing pages
TFLandingPage
Number of terms in the query
NumQueryTerms
Number of characters in the query
NumQueryChar
TFQueryAdult
Frequency of adult terms in the query
TFLandingPageAdult Frequency of adult terms in the land-
NumSpellingErrors
TopicODP
CitiesQuery
CountriesQuery
TFURL
ing pages
Number of misspelled terms
Set of ODP categories of the top 8
result pages
Cities mentioned in the query
Countries mentioned in the query
Keywords in URLs of the top 8 result
pages
QueryTermPopularity Frequency of the query terms in AOL
dataset
Table 2: Features extracted from the query events
of today’s Internet users, we have chosen 100 highly active
users from which we have between 400 to 1100 queries (800
to 1000 for most of the users). These users on average issue
about 8 queries per day. Table 1 presents some statistics
about these datasets. Note that while our analysis relies on
a much smaller set of users than the original AOL dataset,
we show in the Appendix that 100 users are representative
enough to estimate the overall distribution of the attack per-
formance, and hence to quantitatively compare the eﬀective-
ness of diﬀerent obfuscation mechanisms.
Every query is associated with a unique user identiﬁer. In
addition to the query terms, the dataset contains the exact
time at which the user submitted the query and possibly the
link of the webpage clicked by the user on the search result
page, referred to as the landing page. As the landing pages
might not exist anymore due to the dynamic nature of the
web, we simulated the AOL queries on Google search engine
and populated the dataset with up-to-date content regarding
the landing pages. We note that this approach has some
practical limitations. For technical reasons, our web crawler
failed to retrieve some landing pages for which we could not
get the page full content.
In our dataset, we ignore the
queries associated with such landing pages. Therefore, we
only consider queries where we have the full set of features.
In order to gather realistic TMN data, we developed a
Firefox plugin which takes real user queries from 100 users
of the AOL dataset and issues them to the Google search
engine, as a real user would do. In parallel, we have been
running TMN in the browser and recorded the generated
obfuscation queries. In order to simulate the three months
period of the AOL dataset in a reasonable amount of time,
we ran TMN during about one week and then scaled up
the timestamps of the queries in order to match the three
months period. The number of fake TMN queries are se-
lected such that they are approximately equal to the number
of user’s real queries. Table 1 presents some statistics about
the TMN dataset.
Behavioural feature similarities
D Time
S WeekWeekend
S SameDaytime
D NumberClicks
Diﬀerence of timestamps
Whether both in weekend or weekday
Same 2 hour window of a day
Diﬀerence of clicked landing pages
JaccardC of the query terms
JaccardC of landing pages
TFIDF of landing pages
TFIDF of landing pages symmetric
Diﬀerence of query terms len
JaccardC of query adult terms
Both queries have adult terms
Queries adult terms bool diﬀerence
Semantic feature similarities
S QueryTerms
S LandPagTerms1
S LandPagTerms2
S LandPagTerms3
D QueryTermLen
D QueryCharacterLen Diﬀerence of query characters len
S AdultTerms1
S AdultTerms2
S AdultTerms3
S LandPagAdultTerms JaccardC of landing page adult terms
D SpellingErrors
S SpellingError1
S SpellingError2
S Level2Cat
S Level3Cat
D TreeDistance
S City
S Country
S Location1
S Location2
S UrlTerms
D QueryTermWeight
D EditDistance
Diﬀerence of spelling errors
Both queries have spelling error
Queries spelling error bool diﬀerence
Same ODP level 2 category
Same ODP level 3 category
Average ODP tree distance
Queries have same city
Queries have same country
Both queries have location terms
Queries location info bool diﬀerence
JaccardC of top 8 landing page URLs
Diﬀerence in query term weights
Levenshtein distance of queries
Table 3: (Similarity/Diﬀerence) Relations between
queries
3.2 Features
We extracted 16 features from the queries as listed in Ta-
ble 2. The features can be grouped into semantic features
and behavioural features [36]. For instance, features such as
the time of query, day of week or time of day are features
describing the behaviour of the user. Moreover, the number
of clicks per query characterizes how many times usually a
user clicks on a link on the result page(s).
For the semantic features, we extract some features that
are used in Natural Language Processing (NLP). We com-
pute the term-frequency and also the topics associated with
the result pages obtained according to the Open Directory
Project (ODP), an openly available hierarchical ontology [2].
When processing the terms in the query and the result pages,
we stem the terms by applying the Lancaster stemmer [3].
We use ODP for categorising queries into diﬀerent seman-
tic categories/topics [10]. The ODP dataset contains about
4.2 million web-sites. The categories are organized within
a tree, having the root as common top category. There are
about 600,000 categories as the leaves in the ODP dataset.
We categorize a query into one or multiple ODP categories,
in the following way. Given a query, we retrieve the top 8
landing pages. For each landing page URL, we search the ex-
act URL in the ODP dataset (e.g., www.domain.com/abc).
If we do not ﬁnd any match, we extract the domain of the
URL and search for the categories associated with the do-
main in the ODP dataset (e.g. www.domain.com). There
might be multiple categories associated with a URL. Having
a total of 73,984 unique queries across 100 users, we found
Feature relation
D Time
D QueryTermWeight
S UrlTerms - Jaccard
D EditDistance
S LandPagTerms2
S LandPagTerms1
S LandPagTerms3
D QueryCharacterLen
S QueryTerms
D QueryTermLen
D TreeDistance
D NumberClicks
S SameDaytime
D SpellingErrors
S LandPagAdultTerms
S SpellingError1
Importance
100
24
22
20
17
16
15
13
11
8
7
5
4
4
4
1
Table 4: Relative importance of the features in link-
age function LU SR,
linkage function learned
for attack against an obfuscation mechanism using
queries from another user.
i.e.
Feature relation
D QueryTermWeight
D Time
D EditDistance
D TreeDistance
S LandPagTerms3
S LandPagTerms1
S LandPagTerms2
D NumberClicks
D QueryTermLen
S UrlTerms - Jaccard
D QueryCharacterLen
D SpellingErrors
S SpellingError1
S Level2Cat
S SpellingError2
S LandPagAdultTerms
Importance
100
56
32
31
26
20
19
18
14
12
10
7
6
4
4
4
Table 5: Relative importance of the features in link-
age function LT M N ,
linkage function learned
for the attack against an obfuscation using auto-
generated queries (TMN).
i.e.
at least one ODP category for 73,391 queries, accounting for
99.2% of the queries.
The queries may contain sensible and identifying informa-
tion about the users. This includes the geographic informa-
tion such as the name of a city or country, or whether the
queries contain particular terms such as adult terms. We ex-
tract location information from the queries by searching the
query terms in a city/country dataset [4]. If the query only
contains city information, we match the city to the country
as well. We also retrieve a list of adult terms and tagged each
query with its respective adult terms, if existing. Moreover,
since the language of most queries is English, we count the
number of misspelled words per query using a dictionary.
Another piece of information that we extract from the
queries is the popularity of the query terms with respect
to the queries that other users search. We compute how
trendy/popular each term is by counting its frequency in the
whole AOL dataset. Then we compute the overall popularity
of a query by summing these frequencies.
3.3 Query similarities