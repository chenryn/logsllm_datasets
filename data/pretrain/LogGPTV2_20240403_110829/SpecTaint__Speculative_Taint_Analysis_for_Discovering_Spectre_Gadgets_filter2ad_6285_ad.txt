0
8
FP*
0
0
0
0
0
0
FP
17
43
9
79
215
55
FN
0
1
1
0
3
0
FN
0
13
8
6
10
3
TP
3
12
8
7
7
10
TP
3
0
1
1
0
7
TP
2
7
8
5
4
6
SpecFuzz
FN
1
6
1
2
6
4
Precision
0.105
0.140
0.471
0.059
0.018
0.098
FP
1
17
6
14
3
16
Spectre
Total #
15
RH Scanner
12
oo7
15
SpecFuzz
15
SpecTaint
15
TABLE II: The number of detected Spectre gadgets from the
Spectre Sample Dataset.
SpecFuzz
Reproduce
SpecTaint
Tainted branch
Unique
JSMN
16
17
1
1
0
Brotli
68
43
17
13
6
HTTP
9
9
6
6
1
LibHTP
91
79
14
9
0
YAML
140
215
3
0
0
SSL
589
55
16
13
4
TABLE III: The number of detected gadgets in Real-world V1
Dataset. Reproduce shows the results that were reproduced
using the open-sourced SpecFuzz implementation. Tainted
branch means the gadgets are detected during speculative
execution over tainted branches. Unique means the gadgets
are detected only by SpecTaint.
two conditional branches, and thus missed the inserted gadget
at line 5. Second, SpecFuzz stops the simulation and rolls
back to a previously saved state once an invalid memory
access or other exceptions are captured. Thus, it will miss
gadgets located after the invalid memory access or exception.
For example, as presented in Listing 3, SpecFuzz detected an
out-of-bounds access at line 3, then it stopped the simulated
speculative execution and restored. As a result, it failed to
detect the inserted gadget in the “else” branch.
Table I also shows that RH Scanner missed many inserted
gadgets. This is because it failed to explore the execution paths
of target programs due to the limitation of the static path
exploration it uses. It also produced a large number of detection
results due to the simplistic syntax-based gadget modeling. In
this experiment, since there are too many results reported by
RH Scanner and we only focus on inserted gadgets, we did not
inspect all detection results and only consider inserted gadgets
to be true positives. In conclusion, the evaluation results show
that SpecTaint outperforms state-of-the-art tools in terms of
precision and recall when analyzing real-world programs.
D. Baseline Evaluation on Real-world V1 Dataset
We further conducted a baseline comparison with another
dynamic analysis approach SpecFuzz [39]. In this experiment,
we evaluate two aspects of dynamic Spectre gadget detection
tools. The ﬁrst is the effectiveness, which aims at comparing
the number of detected gadgets. Since there is no ground
truth in this dataset, we manually analyze detected gadgets
and consider exploitable gadgets to be true positives based on
human knowledge. The second is efﬁciency. This is to evaluate
Program
JSMN
Brotli
HTTP
LibHTP
YAML
SSL
TP
1
5
2
3
0
2
Precision
SpecFuzz
FP
16
38
7
76
215
53
0.059
0.116
0.222
0.038
0
0.036
Precision
SpecTaint
FP
0
6
4
11
3
10
0.647
0.333
0.214
1
0
0.375
TP
1
11
2
3
0
6
TABLE IV: True positive, false positive and precision of
gadgets detected by SpecFuzz and SpecTaint on Real-world
V1 dataset.
the performance of gadget detection tools and runtime perfor-
mance of target programs after patching detected gadgets.
. . .
f o r
1
2
4
5
7
3 {
6 }
( k = 1 ; k r a w b u f f e r . p o i n t e r [ k ] ;
v a l u e = ( v a l u e num transforms )
t a i n t e d
t∗ word = &words−>d a t a [ o f f s e t ] ;
c o n s t u i n t 8
i n t
i f
[ 0 ] ) {
l e n = i ;
( t r a n s f o r m i d x == t r a n s f o r m s−>c u t O f f T r a n s f o r m s
memcpy(&s−>r i n g b u f f e r [ pos ] , word ,
l e n = B r o t l i T r a n s f o r m D i c t i o n a r y W o r d (&s−>
t ) l e n ) ;
( s i z e
} e l s e {
r i n g b u f f e r [ pos ] , word ,
t r a n s f o r m i d x ) ;
len ,
t r a n s f o r m s ,
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
21
22
23
24
25
}
. . .
}
. . .
17 }
/ /
u i n t 8
{
i n t
c o n s t u i n t 8
t r a n s f o r m s ,
. . .
i d x = 0 ;
26
27 }
r e a d and l e a k s e c r e t u s i n g t r a n s f o r m i d x a s
18
19 # d e f i n e BROTLI TRANSFORM PREFIX ID( T ,
( ( T )−>t r a n s f o r m s
i n d e x .
[ ( ( I ) ∗ 3 ) + 0 ] )
I )
(&(T )−>
20 # d e f i n e BROTLI TRANSFORM PREFIX( T ,
I )