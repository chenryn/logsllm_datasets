those classes X,
it
∀C
1
∈
Proof: By Lemma
pre(R) and
∀i.0 ≤ i < len(vtbl(C)) newInd[C][i] is well deﬁned
and set only once at line 19. Lets denote by m the length
of ivtbl at the time that newInd[C][i] is set. Since in
that same iteration vtbl(C)[i] is appended at the end of
ivtbl, and ivtbl only grows, it follows that at the end of
the algorithm ivtbl[m] = vtbl(C)[i]. However at the
end of the algorithm newInd[C][i] = m − addrPtM[C]
(line
algorithm
ivtbl[newInd[C][i]+addrPtM[C]]=vtbl(C)[i].
19). Therefore,
end
the
the
of
at
Next, to establish requirement (3) we ﬁrst prove a small
helper lemma:
Lemma 3. For ∀B, D ∈ pre(R) where B is a superclass of
D if at the beginning of an iteration of the outer loop (line 16)
posM[B] < len(vtbl(B)) then posM[B] = posM[D].
Proof: This follows by induction on the number of
iterations of the outer loop. First posM[B] and posM[D]
are both initialized to 0. Next, lets assume that at the be-
ginning of some iteration of the outer loop posM[D] =
posM[B]. Since posM[B] < len(vtbl(B)) and since
len(vtbl(B)) ≤ len(vtbl(D)) (because D is derived from
B) it follows that posM[D] < len(vtbl(D)). Therefore,
since both posM[B] < len(vtbl(B)) and posM[D] <
len(vtbl(D)) at
then both
posM[B] and posM[D] are incremented in that iteration of
the start of the outer loop,
9
newInd[D][i] = len(ivtbl)D − addrMap[D] =
len(ivtbl)B + (order.indexOf(D)−
order.indexOf(B)) − order.indexOf(D) =
len(ivtbl)B − order.indexOf(B) = newInd[B][i]
The remainder follow from the fact that newInd[D][i]
and newInd[B][i] are set only once by Lemma 1.
To rehash, we have shown that our interleaving algorithm
is correct by establishing 3 properties - it preserves all of the
original vtable entries (Corollary 1), it correctly computes the
new indices for vtable entries in the IVT (Lemma 2), and
ﬁnally it does not break the assumptions on layout preservation
between base and derived classes (Lemma 4).
IX.
IMPLEMENTATION
We implemented our technique in the LLVM [19] compiler
framework. Our implementation required change to about 50
lines of C++ code and the addition of another 6820 lines of
C++ code. Our change is split between the Clang compiler
frontend (approx. 900 LOC), and several new link-time passes
we added (approx 5900 LOC). Our tool supports separate
compilation by relying on the link time optimization (LTO)
mechanism built in LLVM [21]. To support LTO all interme-
diate object ﬁles are emitted in LLVM’s higher-level IR, which
Name
astar
omnetpp
xalancbmk
soplex
povray
chrome
#LOC
11684
47903
547486
41463
155177
1M
1
111
958
29
28
1
961
11253
557
120
#Classes
#Callsites Avg. #T/C
1
21.1235
5.96188
4.00359
1.74167
51.0186
20294
129054
Fig. 20: Tool Workﬂow
retains additional semantic information necessary at link time
to reconstruct the global class hierarchy and identify all sites
in the code that must be modiﬁed. The workﬂow of our tool
is presented in Figure 20.
First source ﬁles are fed through our modiﬁed Clang
compiler, which adds some semantic information for use by
later passes:
• Metadata describing the structure of vtable groups and
local class hierarchies
Placeholder instructions for vptr checks
Placeholder instructions for new vtable indices
•
•
Note that placeholder instructions are necessary, as the pre-
cise ranges for vptr checks and new indices (for interleaving)
depend on the global class hierarchy which is not known at
compile time.
Work at link time is done as part of the LLVM gold plugin,
and is spread in several steps:
•
•
•
CHA - gather vtable metadata; construct global class
hierarchy; decompose it in primitive vtable trees
VTBL - build the new ordered/interleaved vtables
INST - replace placeholder instructions based on the
previous 2 steps
X. EVALUATION
We evaluate our approach on several C++ benchmarks
including several C++ programs from SPEC2006 as well as
the Chrome browser. For the Chrome browser we evaluate
performance overhead on several standard industry benchmark-
ing suites including sunspider, octane, kraken and some of
the more intensive parts of Chrome’s performance tests suite.
Figure 21 lists our benchmarks along with statistics like line
count, number of classes, and number of static sites in the code
where a virtual method is called. These benchmarks represent
realistic large bodies of C++ code (e.g. Chrome contains over
1M lines of C++ code). Chrome especially exercised many
corner cases of the C++ ABI, including combinations of virtual
and multiple inheritance, all of which we handled. All of the
benchmarks ran unmodiﬁed, with the exception of xalancbmk
which contained a CFI violation: a class is cast to its sibling
class and methods are invoked on it. We are not the only ones
to report this violation – LLVM-VCFI also reports it. Because
the layouts of the two sibling objects in this benchmark are
similar enough, the violation went by unnoticed before. We
ﬁxed this CFI violation in xalancbmk (4 lines of code) before
running it through our experimental setup.
10
Fig. 21: Benchmark names along with several statistics:
#LOC is number of lines of code; #Classes is the number of
classes with virtual methods in them; #Callsites is the
number of virtual method calls; and #T/C is the average
number of possible vtables per virtual method call site,
according to the static type system.
A. Runtime Overhead
Figure 22 shows the percentage runtime overhead of our
approach, with the baseline being LLVM O2 with link time
optimizations turned on. The bars marked OVT correspond to
checks based on Ordered VTables, while IVT bars correspond
to checks based on Interleaved VTables. For comparison, we
also include the runtime of the very recent LLVM 3.7 forward-
edge CFI implementation [22] (columns labeled LLVM-VCFI).
This LLVM work has not been published in an academic
venue, but as far as we are aware, it is the fastest technique
to date that offers similar precision to ours. For each static
type, LLVM-VCFI emits a bitset that encodes the set of valid
address points for that type. Dynamic checks are then used to
test membership in these bitsets. By default, LLVM-VCFI also
checks downcasts, which we don’t do. As a result, we disabled
this option in LLVM-VCFI in our experimental evaluation.
Runtimes for each benchmark are averaged over 50 repetitions.
Interleaving achieves an average runtime overhead of
1.17% across all benchmarks, and 1.7% across the Chrome
benchmarks. Note that this is almost 2 times faster compared
to LLVM-VCFI, which achieves 1.97% on average for all
benchmarks, and 2.9% on Chrome benchmarks. Additionally,
the average overhead of ordered vtables is 1.57%, which is
higher than interleaved vtables, but lower than LLVM-VCFI.
Given that interleaving and ordering employ the same runtime
checks, the faster runtime of interleaving stems from better
memory caching performance due to the removed padding.
One of the benchmarks (soplex) exhibits a small (<-1%)
negative overhead for all 3 compared techniques. We believe
this to be due to alignment or memory caching artifacts.
There are two benchmarks where, according to the exact
numbers, LLVM-VCFI appears slightly faster than OVT: astar
and omnetpp. We talk about each of the two benchmarks
in turn. All of the overheads in astar are extremely small.
This is because there is a single virtual method call site in
this benchmark. If we look at the exact numbers for that
benchmark, LLVM-VCFI has an overhead of about 0.1%
and IVT has a slightly larger overhead, about 0.13%. The
difference of 0.03% is so small that it is comparable to same-
run variations caused by noise. Regarding omnetpp, LLVM-
VCFI has overhead of 1.17% and IVT has overhead of 1.18%
– again the difference of 0.01% is so small that it is in the
noise of same-run variations.
C++C++C++CLANGIRIRIRLinker1 0 1 00 1 1 1Fig. 22: Percentage runtime overhead. The baseline is LLVM O2 with link-time optimizations. LLVM-VCFI is the recent
state-of-the-art LLVM 3.7 forward-edge CFI implementation; OVT is our ordering approach; IVT is our interleaving approach,
which provides the best performance.
To better understand the source of runtime overhead in the
interleaved approach, we disable our checks while maintaining
the interleaving layout. We ﬁnd that interleaving alone, without
any checks, causes roughly 0.5% overhead across all bench-
marks. This tells us that out of the total 1.17% overhead in
the interleaving approach, 0.5% is due to caching effects from
the vtable layout itself, and the additional 0.67% is caused by
the checks.
B. Size Overhead
Figure 23 presents binary size overhead, again for LLVM-
VCFI, OVT and IVT. On average, OVT has the highest
increase in binary size, about 5.9% – this is because in addition
to adding checks to the binary, OVT also adds a lot of padding
to align vtables (and the vtables are also stored in the binary
text). LLVM-VCFI has the next largest binary size overhead,
at about 3.6%. LLVM-VCFI’s binary size overhead comes
from checks that are added to the binary, vtable aligning and
from the bitsets that are stored in the binary. Finally, IVT
has the smallest binary size overhead, at about 1.7%. The
only overhead in IVT are the checks – there is no alignment
overhead and no additional data structures that need to be
stored.
C. Range Check Optimization Frequency
To better understand how frequently the range-check opti-
mizations from Section VI are applied, Figure 24 shows the
breakdown of these different optimizations as a percentage of
all virtual method call sites. Each bar represents a benchmark,
and the shaded regions in each bar show the percentage of
virtual method call sites that are optimized in a particular
way. More speciﬁcally, regions shaded no_check represent
the percentage of call sites in which the check was optimized
away due to a statically veriﬁed vptr. On average, these account
for about 1.5% of all virtual method call sites. Regions shaded
as eq_check represent the percentage of call sites for which
the range check was optimized to a single equality. On average
these account for a surprisingly large percentage of all call
sites - approximately 26%, indicating that this optimization
is particularly valuable. Finally the regions shaded range
represent the remaining call sites, where a full range check
was needed, on average approximately 72%.
11
sunspiderkrakenoctanehtml5ballslinelayoutastaromnetppxalansoplexpovraymeanRuntime Overhead %−10123456LLVM−VCFIOVTIVTpaper [16] shows that when programs are proﬁled on one set of
inputs, but then run on a different set if inputs, the overhead
increases even more. VTV[35] has around 4% overhead on
SPEC2006, whereas our overhead on the same subset of
SPEC2006 (471.omnetpp, 473.astar, 483.xalancbmk) is about
0.9%.
vfGuard[30] and vtInt[42] both have higher overhead (18%
and 2% respectively). The main focus of those techniques is
identifying vtables and virtual call sites in stripped binaries,
and extracting a class hierarchy. Because this is a very hard
task, these techniques inevitably will have less precise informa-
tion than our approach, which naturally leads to less precision
in the checks.
As mentioned and ﬁxed in a recent paper [13], there are
some corner cases that certain vtable integrity techniques,
including LLVM-VCFI, do not handle precisely. We have a
way in our system to handle these cases, using multiple range
checks, as was mentioned in Section VII. Because LLVM-
VCFI does not handle these cases, we have disabled multiple
range checks in our system when collecting the numbers
reported in this section. If we enable these checks,
thus
getting more precise protection, we would handle the corner
cases mentioned in [13] – although note that an empirical
comparison of precision with [13] is difﬁcult due to the large
difference in tested versions of chrome (version 32 vs. version
42). If we enable multiple range checks, our overhead for
Chrome remains the same, whereas our overall overhead for
all SPEC2006 benchmarks goes from about 0.45% to 0.89%.
Finally, while an improvement from 2% to 1% may seem
small initially, it is important to realize that it corresponds to
a halving of the overhead, without the need for proﬁle guided
optimizations and also with minimal memory overhead.
E. Security Analysis
Our technique enforces the C++ type system constraints at
each dynamic dispatch site. Namely we guarantee that if in
the source code a given dynamic dispatch is performed on a
variable of static type C, then the vtable used at runtime will
be the vtable for C or a subclass of C. In column 5 of Figure 21
we list the average number of possible vtables for a dynamic
dispatch site in each benchmark. As such, we believe our
defense is effective against attack such as Counterfeit Object-
Oriented Programing[32].
Since our technique is implemented at the LLVM IR level,
it
is possible for the later register allocator to decide to
spill a checked vptr value on the stack. In this case, if the
attacker can additionally mount a stack overwrite attack, this
opens up the possibility for a time-of-check-time-of-use attack.
The stack can be protected through a variety of mechanisms,
including [6], [20] (although recent work [5] has shown that
many of these stack defenses are not as strong as originally
thought). To fully overcome these kinds of attacks, we believe
we would need to implement our approach at a lower-level in
the LLVM compiler, to achieve explicit control over register
allocation, and prevent register spilling of vptr values between
the time they have been checked and the time they are used.
Finally, similarly to LLVM-VCFI our technique does not
currently handle C++ pointers to member function. Pointers to
Fig. 23: Percentage binary size overhead. The baseline is
LLVM O2 with link-time optimizations.
Fig. 24: Callsite optimization breakdown
D. Overhead comparison with existing work
Our experimental evaluation in the prior sections com-
pared against the state-of-the-art LLVM 3.7 forward-edge CFI
(LLVM-VCFI), which is the most efﬁcient implementation of
similar precision to our work. Here we broaden the scope of
our comparison to also include performance numbers reported
in other academic papers. In this broader setting, our tech-
nique achieves lower runtime and memory overhead compared
to all vtable protection techniques of comparable precision.
SafeDispatch [16] and LLVM-VCFI both achieve roughly 2%
overhead on the Chrome and SPEC benchmarks we evaluated,
compared to our overhead of about 1.17%. SafeDispatch also
requires proﬁle-guided optimizations, and the SafeDispatch
12
omnetppxalansoplexpovraychromemeanSize Overhead %051015OVTLLVM−VCFIIVTomnetppxalancbmksoplexpovraychromemeanStatic callsite optimization breakdown0102030405060708090100no_checkeq_checkrangemember functions are a C++ construct containing an index to
a method in a vtable. When used for dynamic dispatch, the
stored index is used to look up the target method in the vtable
of the object on which we are invoking. In the case of Ordered
VTables it is possible to check that the index is contained
in the vtable of the target class with a single range check.
This however still leaves considerable freedom for an attacker,
and might be insufﬁcient. In the case of Interleaved VTables
handling member pointers is further complicated by the fact
that vtables are broken up in multiple ranges. Member pointers
are currently left as future work as discussed in Section XII.
XI. RELATED WORK
The cost of control-ﬂow hijacking attacks has motivated a
rich body of research on prevention and mitigation. We can
broadly split related work into 4 groups – vtable protection,
general CFI enforcement, Software Fault Isolation (SFI) and
other mitigation techniques.
A. VTable Protection
Closest to our work are techniques focusing on vtable pro-
tection and forward-edge control-ﬂow enforcement. SafeDis-
patch [16] and VTV [35] both present compiler-based trans-
formations that achieve similar precision to us. SafeDispatch
incurs higher overhead – 2.1% on Chrome and SPEC2006
(vs. 1.1% for us across all benchmarks), and requires proﬁle