p(x) · [PHf|Ms(x)PMs(x)+
PHf|Mf (x)PMf (x)]
(7)
We do not show the similar development for the “Parallel
detection” case, which leads to equation (3) for the detec-
tion task.
The model as described has three parameters for each
possible case that may be presented to the system. Since
each case is in some respect different from all others, it is
infeasible to estimate these parameters. What can be done
is to estimate the failure probabilities for classes of sim-
ilar demands. So, in a controlled trial of the system, we
would ﬁrst subdivide the cases into classes. Equation 7 is
still valid, if we interpret each x as representing a whole
class of demands:
PHf =
(cid:1)
x∈{classes of cases}
p(x) · [PHf|Ms(x)PMs(x)+
PHf|Mf (x)PMf (x)]
(8)
For each class of cases, we need to estimate the param-
eters PHf|Ms(x), PMs(x), PHf|Mf (x). We would think
that two demands belong to the same class if they are equiv-
alent under all respects that signiﬁcantly affect the difﬁculty
of dealing with them correctly, both for the reader and for
the CADT algorithms. I.e., the two demands are practically
indistinguishable from the viewpoint of the failure probabil-
ities they produce: all the demands in the same class have
similar values of pMf (x), pHf|Mf (x), and pHf|Ms(x)1. In
practice, we would try to classify the cases according to var-
ious characteristics that are easy to identify and produce a
feasibly small number of classes.
5 Extrapolation from controlled trials to re-
sults in the ﬁeld
In deciding about the adoption of the CADT in a new
environment, or on possible design changes to the CADT
or its use, it will be necessary to rely on measurements
taken under different conditions from those for which the
prediction is sought. For instance, the measurements may
be taken in a controlled trial, which may by necessity be
run under different conditions (especially, different demand
proﬁle) than those of actual clinical use. Equation (8):
PHf =
(cid:1)
x∈{classes of cases}
p(x) · [PHf|Ms(x)PMs(x)+
PHf|Mf (x)PMf (x)]
is the key to this kind of extrapolation.
Changes in the conditions of use of the CADT or its char-
acteristics can be represented by changing the values of pa-
rameters in this equation. Consider ﬁrst the direct effects of
such changes in the conditions of use, for instance:
1. the frequencies of the various kinds of cases may
change. Provided that a useful classiﬁcation of the
cases into classes is found, such that equation 8 is a
suitable description of the probability of wrong deci-
sion, these changes would be represented by changes
of the parameters p(x) (one for each category x) in the
equation;
2. the readers have varying levels of ability (represented
by the parameters PHf|Ms(x) and PHf|Mf (x)). The
trial data can indicate the range of these abilities, show
whether there are strong discrepancies between hu-
mans, and if these affect different categories of de-
mands differently (as is believed to be the case);
3. the behaviour of the readers, and thus PHf|Ms(x)
and PHf|Mf (x), will evolve over time as they learn
more about the behaviour of the CADT, e.g., becom-
ing more complacent about relying on its prompts, or
more skilled in detecting its failures;
1Identical values of pM f (x), pHf|M f (x), and pHf|M s(x) for all x
in a class are a sufﬁcient condition for one to be able to reuse, for pre-
dictions in a new environment of use of the CADT, the parameter values
estimated, for classes of demands, in another environment. One can also
deﬁne less restrictive sufﬁcient conditions for this re-use to be sound. In
any case, whether these conditions hold is essential for deciding how to use
these models, and determining it with conﬁdence will often be difﬁcult, as
usual in this kind of statistical analysis.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply. 
4. the probabilities of success or failure of the CADT,
PMs(x) and PMf (x) = 1 − PMs(x), may change due
e.g.
to maintenance practices, systematic differences
in ﬁlm characteristics, better detection algorithms, dif-
ferent tuning of the detection algorithms (which may
be decided as a consequence of measuring their per-
formance), and so on.
There may be indirect effects as well. For instance, a
large enough change in the prevalence of cancers (or of dif-
ferent categories of cancers) in the population of cases (item
1 in the list above), may change the readers’ failure proba-
bilities, PHf|Ms(x) and PHf|Mf (x). To guess the range of
magnitude of the changes, we would need to rely on mea-
surements of similar effects on human behaviours in simi-
lar tasks. The equation will show the corresponding ranges
of uncertainty in the predicted probability of system fail-
ure. Another example is that changing the frequency of
failures of the CADT, PMf , may affect the tendency of
readers to depend on its outputs more or less in formulating
their own decisions, thus indirectly changing PHf|Ms(x)
and PHf|Mf (x) as well. Changing the probability of false
positive failures of the CADT, not considered in this arti-
cle, may similarly affect the degree to which readers trust it.
The adaptation of readers to perceived failures of a CADT
is discussed e.g. in [6], with references to previous litera-
ture. Other pertinent literature from research on cognition
and HCI (cf for instance [4, 10]) can inform conjectures
about these changes.
We now illustrate some of these extrapolation steps
on a numerical example. The parameter values we use
do not necessarily represent the performance of any real
CADT, but their orders of magnitude are realistic, as is
their variation between classes of cases. It is essential to
consider the dependence of the probability of failure on
the type of demand. In this example, we assume that there
are essentially just two categories of cases, which we have
labelled here as “easy” and “difﬁcult”. An experimenter
has run a trial on the human-machine system and obtained
the estimates shown in the table below for the model
parameters. We assume for the sake of simplicity that
narrow enough conﬁdence intervals can be obtained for all
parameters.
classes
of cases
easy
difﬁcult
Model parameters
demand
proﬁles
Trial Field PMf PMs PHf|Mf PHf|Ms
0.8
0.2
0.07
0.41
0.9
0.1
0.93
0.59
0.18
0.9
0.14
0.4
The two categories of cases are represented in propor-
tions of 80% “easy” and 20% “difﬁcult”. Suppose that the
actual proportions in the ﬁeld are 90% vs 10%. Clearly,
to predict failure probabilities in the ﬁeld one will need to
adjust for this different “demand proﬁle“.
The next table shows the difference between the depend-
ability that will be observed in the ﬁeld and that in the trial.
So far, we do not need our complete model which identiﬁes
the roles of the machine and the human in system failure:
all that we are doing is using, in the bottom line, weighted
sums of the probabilities of system failures for the two cat-
egories of cases.
Probability of system failure
easy cases
difﬁcult cases
0.143
0.605
all cases
Trial
0.235
Field
0.189
Suppose now that designers wish to improve the CADT,
reducing its probability of false negative failure. They
would want to predict the effects of this improvement
on the dependability of the human-machine system, and
to decide where they should concentrate efforts towards
improvement. Apart from cost constraints, for pattern
detection problems it is often possible to reduce greatly (for
either a human or an automated algorithm) the probability
of false negative failures if one is willing to accept a
corresponding increase in false positive failures. However,
the latter may make the CADT useless. Our hypothetical
designers consider two alternative improvements to the
CADT: a reduction by 10 of the failure probability PMf
for the “easy” (and frequent) cases, or a similar reduction
limited to the “difﬁcult” (and rarer) cases. The following
table gives the results of applying equation (8) to these
scenarios, both for the “Trial” and for the “Field” proﬁles:
Probability of system failure
im-
CADT
proved for easy
cases
CADT
proved
difﬁcult cases
im-
for
easy cases
difﬁcult cases
0.140
0.605
0.143
0.421
all cases
Trial
0.233
Field
0.187
Trial
0.198
Field
0.171
The probabilities of failures under the two demand pro-
ﬁles (Trial and Field) differ, as would be expected. As for
the effects of reducing the CADT’s failure probabilities for
the “easy” or for the “difﬁcult” cases, we see that neither
produces large improvements (compared to the bottom line
of the previous table). The “easy” cases are the majority, so
one might expect that reducing the CADT’s failure proba-
bilities by a factor of 10 on these cases would greatly im-
prove the overall situation. This is not so, however. The
failure probability for the “Field” proﬁle is only reduced
to 0.187, vs the 0.189 of the unimproved CADT. The rea-
son is in the difference between the values of PHf|Mf and
PHf|Ms for these cases: only 0.04. Although the improved
CADT gives correct outputs much more often, this improve-
ment does not greatly affect the reader’s performance on
Instead, the difference PHf|Mf − PHf|Ms
those cases.
is larger for the “difﬁcult” cases, and accordingly reduc-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply. 
ing the CADT’s failure probability for these cases yields
greater improvement in overall probability of failure (0.171
vs 0.189, for the “Field” proﬁle; 0.198 vs 0.235, for the
“Trial” proﬁle).
This example illustrate some properties of this model,
which we discuss in the next section.
6 An analysis of human-machine interaction
6.1 Importance index
If we deﬁne t(x) = PHf /Mf (x) − PHf /Ms(x), we can
rewrite equation (8) as
PHf =
(cid:1)
x
(cid:1)
x
p(x) · [PHf|Ms(x)PMs(x) + PHf|Mf (x)PMf (x)] =
p(x)·[PHf|Ms(x)·(1−PMf (x))+PHf|Mf (x)PMf (x)] =
(cid:1)
x
p(x) · [PHf|Ms(x) + PMf (x) · t(x)]
(9)
This says that the probability of reader failure (false neg-
ative decision) is given by the probability of him/her failing
given correct output by the CADT, plus the probability of
the CADT not giving correct output (failing), multiplied by
a term t(x).
The term t(x) seems a useful general measure of how
much the decision support offered by an automated system
affects a human user’s success or failure. t(x) is an “im-