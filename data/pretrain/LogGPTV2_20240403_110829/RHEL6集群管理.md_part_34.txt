-   ::: para
    执行 `ethtool eth1 | grep link`{.command}
    命令检查该以太网链接是否可用。
    :::
-   ::: para
    在每个节点中使用 `tcpdump`{.command} 命令检查网络流量。
    :::
-   ::: para
    确定您没有设定防火墙规则阻断节点间的沟通。
    :::
-   ::: para
    确定该集群用于内部节点间沟通的接口没有使用捆绑模式 0、1 和 2
    以外的模式。（从红帽企业版 Linux 6.4 开始支持捆绑模式 0 和 2。）
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-troubleshoot-CA.html#s1-cluster-norejoin-CA}9.3. 无法在 Fence 或者重启后重新加入集群的节点 {.title}
:::
::: para
如果您从节点在 fence 或者重启后无法重新加入该集群，请检查以下方面：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    使用 Cisco Catalyst 切换通过其流量的集群可能会有这个问题。
    :::
-   ::: para
    请确定所有集群节点都使用同一版本的 `cluster.conf`{.filename}
    文件。如果在任何一个节点中的 `cluster.conf`{.filename}
    文件有所不同，则那些节点在 fence 后就无法加入该集群。
    :::
    ::: para
    从红帽企业版 Linux 6.1
    开始，您可以使用以下命令确认在主机的集群配置文件中指定的所有节点都有相同的集群配置文件：
    :::
    ``` screen
    ccs -h host --checkconf
    ```
    ::: para
    有关 `ccs`{.command} 命令的详情请参考 [第 5 章 *使用
    [**ccs**]{.application}
    命令配置红帽高可用性附加组件*](#ch-config-ccs-CA.html){.xref} 和
    [第 6 章 *使用 [**ccs**]{.application} 管理 Red Hat
    高可用性附加组件*](#ch-mgmt-ccs-CA.html){.xref}。
    :::
-   ::: para
    请确定您在要加入该集群的节点中为集群服务配置了
    `chkconfig on`{.command}。
    :::
-   ::: para
    请确定没有阻断该节点与集群中的其他节点沟通的防火墙规则。
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-troubleshoot-CA.html#s1-clustcrash-CA}9.4. 集群守护进程崩溃 {.title}
:::
::: para
RGManager 有一个监控进程可在主 `rgmanager`{.command}
进程意外失败时重启该主机。这就可以 fence 该集群节点，且
`rgmanager`{.command} 可在另一台主机中恢复该服务。监控守护进程探测到主
`rgmanager`{.command}
崩溃时，它就会重启该集群节点，同时活动的集群节点将探测到该集群节点已离开，并将其从该集群中逐出。
:::
::: para
较小数字的*进程 ID*（PID）是 watchdog 进程，可在其子进程（有较大 PID
数字的进程）崩溃时起作用。使用 `gcore`{.command} 捕获 PID 较大进程的
core 可帮助对崩溃的守护进程进行故障排除。
:::
::: para
安装所需软件包捕获和查看 core，并保证 `rgmanager`{.command} 和
`rgmanager-debuginfo`{.command} 是同一版本，否则捕获的应用程序 core
可能不可用。
:::
``` screen
$ yum -y --enablerepo=rhel-debuginfo install gdb rgmanager-debuginfo
```
::: section
::: titlepage
## [⁠]{#ch-troubleshoot-CA.html#s2-clustcrash-runtime}9.4.1. 在运行时捕获 `rgmanager`{.command} Core {.title}
:::
::: para
它启动时有两个 `rgmanager`{.command} 进程。您必须捕获有较高 PID 的那个
`rgmanager`{.command} 进程。
:::
::: para
以下是执行 `ps`{.command} 命令时显示两个 `rgmanager`{.command}
进程的输出结果示例。
:::
``` screen
$ ps aux | grep rgmanager | grep -v grep 
root    22482  0.0  0.5  23544  5136 ?        S
## [⁠]{#ch-troubleshoot-CA.html#s2-clustcrash-atcrash}9.4.2. 守护进程崩溃是捕获 Core {.title}
:::
::: para
默认情况下 `/etc/init.d/functions`{.filename} 脚本会阻断由
`/etc/init.d/rgmanager`{.filename} 所调用守护进程的 core
文件。对于要生成应用程序 core
的守护进程，您必须启用那个选项。必须在所有需要捕获应用程序 core
的集群节点中执行这一步骤。
:::
::: para
要在 rgmanager 守护进程崩溃时生成 core 文件，请编辑
`/etc/sysconfig/cluster`{.filename} 文件。*`DAEMONCOREFILELIMIT`*
参数可让该守护进程在该进程崩溃时生成 core 文件。使用 `-w`{.option}
选项可阻止 watchdog 进程运行。如果 `rgmanager`{.command} 崩溃，watchdog
守护进程负责重启该集群节点。如果 watchdog 守护进程正在运行，则不会生成该
core 文件，因此一定要禁用它方可捕获 core 文件。
:::
``` screen
DAEMONCOREFILELIMIT="unlimited"
RGMGR_OPTS="-w"
```
::: para
重启 rgmanager 激活新配置选项：
:::
``` screen
service rgmanager restart
```
::: {.note xmlns:d="http://docbook.org/ns/docbook"}
::: admonition_header
**备注**
:::
::: admonition
::: para
如果在这个集群节点中正在运行集群服务，那么它会以非良好状态离开运行的服务。
:::
:::
:::
::: para
如果 `rgmanager`{.command} 进程崩溃生成 core
文件，那么该文件将可以被写入。
:::
``` screen
ls /core*
```
::: para
输出结果应类似如下：
:::
``` screen
/core.11926
```
::: para
重启 `rgmanager`{.command} 捕获应用程序 core 前，请移动或删除 /
目录中的所有旧 core 文件。应重启出现 `rgmanager`{.command}
崩溃的集群节点，或者在捕获 core 文件后 fence
该节点以保证没有运行监视进程。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-troubleshoot-CA.html#s2-clustcrash-gdb}9.4.3. 记录 `gdb`{.command} Backtrace 会话 {.title}
:::
::: para
捕获 core 文件后，您可以使用 `gdb`{.command}，即 GNU Debugger
查看其内容。要在受影响系统的 core 文件中记录 `gdb`{.command}
脚本会话，请运行以下命令：
:::
``` screen
$ script /tmp/gdb-rgmanager.txt
$ gdb /usr/sbin/rgmanager /tmp/rgmanager-.core.
```
::: para
这样将启动 `gdb`{.command} 会话，同时 `script`{.command}
会将其记录到适当的文本文件中。同时在 `gdb`{.command} 中运行以下命令：
:::
``` screen
(gdb) thread apply all bt full
(gdb) quit
```
::: para
按 `ctrl-D`{.command} 停止脚本会话，并将其保存到文本文件中。
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-troubleshoot-CA.html#s1-clusterhang-CA}9.5. 集群服务挂起 {.title}
:::
::: para
当集群服务尝试 fence 某个节点时，该集群服务会停止，直到成功完成 fence
操作。因此，如果您使用集群控制的存储或者服务挂起，且集群节点显示不同的集群成员，或者当您尝试
fence 某个节点时集群挂起，您需要重启节点进行恢复时，请检查以下方面：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    该集群可能尝试 fence 某个节点，且 fence 操作可能已经失败。
    :::
-   ::: para
    查看所有节点中的 `/var/log/messages`{.filename}
    文件，看看是否有失败的 fence
    信息。如果有，重启集群中的那些节点，并正确配置 fencing。
    :::
-   ::: para
    确认没有出现如 [第 9.8 节
    "双节点集群的每个节点都报告第二个节点无法工作"](#ch-troubleshoot-CA.html#s1-twonodeproblem-CA){.xref}
    所示的网络分割。同时确认节点间可进行沟通，网络正常工作。
    :::
-   ::: para
    如果有节点离开该集群，剩余的节点可能不足构成集群。集群需要有一定量的节点方可操作。如果删除节点导致该节点没有足够量的节点，则服务和存储将会挂起。您可以调整预期的票数或者在该集群中保持所需节点数。
    :::
:::
::: {.note xmlns:d="http://docbook.org/ns/docbook"}
::: admonition_header
**注意**
:::
::: admonition
::: para
您可以使用 `fence_node`{.command} 命令或者 [**Conga**]{.application}
手动 fence 某个节点。详情请查看`fence_node`{.command} man page 和
[第 4.3.2 节
"使节点离开或者加入集群"](#ch-mgmt-conga-CA.html#s2-node-leave-join-conga-CA){.xref}。
:::
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-troubleshoot-CA.html#s1-clustservicenostart-CA}9.6. 无法启动集群服务 {.title}
:::
::: para
如果无法启动某个集群控制的服务，请检查以下方面。
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    可能在 `cluster.conf`{.filename}
    文件的服务配置部分有语法错误。您可以使用 `rg_test`{.command}
    命令验证您配置文件中的语法。如果有任何配置或者语法错误，`rg_test`{.command}
    会告诉您哪里出了问题。
    :::
    ``` screen
    $ rg_test test /etc/cluster/cluster.conf start service servicename 
    ```
    ::: para
    有关 `rg_test`{.command} 命令的详情请参考 [第 C.5 节
    "调整并测试服务和资源顺序"](#ap-ha-resource-behavior-CA.html#s1-clust-rsc-testing-config-CA){.xref}。
    :::
    ::: para
    如果配置无误，可提高资源组管理器的日志级别，然后阅读日志信息，确定是什么导致无法启动该服务。您可以在
    `cluster.conf`{.filename} 文件的 `rm`{.literal} 标签中添加
    `loglevel="7"`{.literal}
    参数提高日志级别。然后您可以根据启动、停止、迁移集群的服务增加信息日志的详细程度。
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-troubleshoot-CA.html#s1-clustnomigrate-CA}9.7. 无法迁移集群控制的服务 {.title}
:::
::: para