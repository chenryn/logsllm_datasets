(cid:16)
(cid:14)
(cid:24)
(cid:18)
(cid:21)
(cid:20)
(cid:1)
(cid:1)
(cid:11)
(cid:17)
(cid:26)
(cid:14)
(cid:22)
(cid:16)
(cid:10)
(cid:21)
(cid:21)
(cid:19)
(cid:1)
(cid:4)(cid:18)
(cid:5)(cid:14)(cid:12)(cid:16)(cid:17)(cid:10)(cid:15)(cid:11) (cid:7)(cid:16)(cid:19)(cid:15)(cid:11)(cid:10)(cid:18)(cid:13)(cid:16)(cid:15)(cid:1)
(cid:2)(cid:6)(cid:13)(cid:4)(cid:9)(cid:13)(cid:3)
(cid:2)(cid:7)(cid:3)(cid:11)(cid:6)(cid:5)(cid:1)(cid:4)(cid:3)(cid:12)(cid:6)(cid:5)(cid:1)(cid:10)(cid:9)(cid:1)(cid:12)(cid:13)(cid:3)(cid:8)(cid:6)(cid:12)
(cid:12)(cid:9)(cid:3)(cid:12)(cid:7)(cid:2)(cid:12)(cid:8)(cid:2)(cid:12)(cid:6)
(cid:1)
(cid:13)
(cid:22)
(cid:14)
(cid:20)
(cid:23)
(cid:14)
(cid:15)
(cid:24)
(cid:18)
(cid:21)
(cid:20)
(cid:11)
(cid:17)
(cid:26)
(cid:14)
(cid:22)
(cid:16)
(cid:10)
(cid:21)
(cid:21)
(cid:19)
(cid:1)
(cid:13)(cid:22)(cid:14)(cid:20)(cid:23)(cid:14)(cid:15)(cid:24)(cid:18)(cid:21)(cid:20)(cid:1)
(cid:5)(cid:17)(cid:17)(cid:23)
(cid:5)(cid:18)
(cid:8)(cid:7)
(cid:13)
(cid:5)(cid:7)(cid:19)(cid:9)(cid:8)(cid:1)(cid:10)(cid:13)(cid:15)(cid:1)
(cid:4)(cid:18)(cid:17)(cid:18)(cid:15)(cid:9)(cid:1)
(cid:6)(cid:16)(cid:9)
⎧⎪⎨
⎪⎩
rL
i slj
rM
i smj
rK
i skj
Rj
i =
j ∈ L
j ∈ M
j ∈ K
Fig. 2: Algorand Reward sharing mechanism.
distribution related funds (Algos) are deposited. All rewards
for each round of the Algorand protocol are expected to be
disbursed (or transferred) from this public key. To bootstrap
the new cryptocurrency, the Algorand foundation implemented
a ceiling of 1.75 billion Algos to be disbursed from the
Foundation Reward Pool. Per the foundation, in each round
Ri Algos are added to the Foundation Reward Pool until
the ceiling of 1.75 billion is reached. The projected total
rewards for the ﬁrst 12 periods are suggested by the Algorand
Foundation [34] as 10 (period 1), 13, 16, 19, 22, ..., 38, and 38
Millions of Algos (period 12), respectively [35]. Each reward
period spans 500 thousands blocks. For example, in the ﬁrst
reward period, 10 millions Algos would be distributed, which
is equal to approximately 20 Algos for each round, if in each
round a block could be successfully added to the ledger.
The reward sharing proposal suggests that in each round
this reward Ri should be distributed among Algorand users
in proportion to their current system stake, irrespective of
their roles (i.e.,
leaders or committee members). In other
words, users with higher stake receive a larger portion of the
allocated foundation reward Ri in each round. The transaction
fees accumulated from the transactions in the added blocks
during the bootstrap phase are saved or deposited into the
Transaction Fee Pool. This pool is not planned to be used
for reward disbursement until the 1.75 billion Algos ceiling
of the Foundation Reward pool is met. In summary, currently
only the Foundation Reward Pool is being used for the per-
round reward (or incentive disbursement). Out of the Ri Algos
disbursed to the Foundation Reward Pool per round i, let us
assume that Bi (where, Bi ≤ Ri) Algos are actually disbursed
among the system users. Initially Bi is expected to be equal
to Ri. Let us assume that the total value of stake in the system
is SN . Thus, SN = SL + SM + SK. Here, SL, SM , and SK
are the total stake values of leaders, committee members, and
all other online nodes in round i, respectively. These values
are changing in each round, but for the sake of presentation
we write SN instead of SN (i). Then, the rewards assigned to
a leader node lj in round i, Rj
SN , where slj is
the stake of leader lj. In summary, the reward distribution is:
i , would be Bislj
⎧⎪⎨
⎪⎩
rL
i slj
rM
i smj
rK
i skj
j ∈ L
j ∈ M
j ∈ K,
Rj
i =
(2)
457
i = rM
i = rK
SN . We now analyze if the
where rL
incentives provided by this mechanism is enough to guarantee
cooperation (in the consensus process) by rational nodes.
i = ri = Bi
C. To be Cooperative or Not: Problem Motivation
Let us assume that an Algorand node is cooperative when
it plays its role, i.e., performing all the assigned tasks, and
consequently, accepting all the associated costs. In contrast,
a defecting node only remains online but does not perform
any of its assigned tasks, except sortition computation to join
the network (i.e., paying cost cso). In this case, if appropriate
countermeasures (e.g., punishment mechanisms) are not de-
ployed, the defecting nodes may end up earning rewards by
simply relying on other nodes to honestly perform their tasks
and not contributing anything towards the block proposal, ver-
iﬁcation, and consensus tasks. Considering this deﬁnition for
cooperative and defecting behavior, we can divide Algorand
node behaviors into the following four categories:
• Honest nodes: These nodes always cooperate. They are also
altruistic and cooperate even when the reward is not more than
the cost of cooperation.
• Honest but Selﬁsh nodes: These nodes cooperate and defect
depending on the amount of received incentives versus the cost
for their actions. Hence, they are selﬁsh and will cooperate if
and only if the reward is more than the cost of cooperation.
• Malicious nodes: They arbitrarily cooperate or defect. In
addition to this, they may inject malicious transactions and
blocks, or arbitrarily compromise other nodes.
• Faulty nodes: These nodes are ofﬂine due to system
malfunction (and not by choice) and do not contribute anything
to the network operation.
In this paper, we assume that all network nodes behave in an
honest but selﬁsh manner. Moreover, in this preliminary work,
we assume that nodes do not arbitrarily behave maliciously or
become faulty. In other words, nodes make a strategic decision
to cooperate (participate) or defect (not participate) solely by
maximizing their own interests/incentives. They neither make
any arbitrary protocol participation decision, nor maliciously
modify the protocol to maximize their interests/incentives. To
get an insight into the robustness of the proposed Algorand
reward sharing approach against selﬁsh (or rational) node/user
behavior, we conduct preliminary simulation experiments.
Our simulator, written in Python, is based on the Algorand
discrete event simulator by Deka et al. [36] and implements all
Algorand protocol modules, including, Sortition, Reduction,
and BinaryBA(cid:2). We are also able to simulate network delays
and various synchrony conditions, as well as, customize dif-
ferent network parameters such as total number of nodes and
the distribution of network message delays in our simulator.
Within this simulation framework, we also implemented the
reward sharing protocol proposed by the Algorand Foundation
(described earlier), which computes a per-round reward to
be shared among the nodes. We simulate each round of the
Algorand block proposal and consensus protocol, as outlined
in Section II , and execute the reward sharing algorithm at the
end of each round to compute and distribute rewards.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:36:15 UTC from IEEE Xplore.  Restrictions apply. 
We simulate the Algorand with 100 different seed numbers
and let the protocol operate for 100 rounds in each simulation.
In each simulation instance, we randomly select defecting
nodes (i.e., honest and selﬁsh nodes who chose to defect given
their payoff) by means of a uniform distribution. We consider
the total number of defecting nodes in the network in steps of
5%, 15%, and 25% of all the nodes in the Algorand network.
Moreover, we distribute the stakes among all nodes with a
uniform distribution between 1 to 50 Algos. Note that we
compute trimmed mean which ignores 20% top and bottom
data to compute the mean values of these 100 simulations.
In our simulation each node sends the messages to 5 other
nodes that are randomly selected from the network. We ﬁrst
analyze the impact of defecting nodes on the block creation
process. The corresponding number of nodes who extracted
ﬁnal, tentative, or no blocks from the network messages (i.e.,
votes) are plotted in Fig. 3. As Fig. 3-(a) shows, even with
a low defection rate of 5% the number of tentative blocks is
increasing in the network. Moreover, about 7% of nodes do
not receive any block. When the number of defecting nodes is
increasing the Algorand network fails to inform most of the
nodes about the ﬁnal blocks.
For example, as shown in Fig. 3-(b), with 15% defection
rate, most of the nodes do not reach any consensus on a ﬁnal
block after round #30. In other words, with 15% defection the
network may transition to a weak synchrony or even an asyn-
chrony state in some rounds and it prevents some nodes from
receiving network messages (e.g., votes and block proposals).
However, by reaching a strong synchrony state after a long
period of asynchrony (i.e. weak synchrony assumption), nodes
who have extracted tentative blocks can ﬁnalize their blocks.
This effect has been highlighted in Fig. 3-(b) in the proximity
of rounds 17 through 20. As shown in the ﬁgure, in round
#17 the asynchrony of the network has caused an increase in
the number of nodes that have extracted tentative blocks from
the network. But in round #18, network becomes synchronous
again and consequently a majority of the Algorand nodes are
able to extract the ﬁnalized blocks. We also need to clarify
that these defecting nodes may control more than threshold h
(i.e. Algorand honest assumption as deﬁned in Section II-B1)
of stakes in the network. This happens if there are more
nodes with high values of stakes in the list of defecting nodes.
Defection of these nodes can amplify the network synchrony
problem in the Algorand network and consequently the block
creation process. Finally, the results show that even with 25%
defection the network fails in the ﬁrst few rounds.
In summary, the above simulation results show that without
an incentive-compatible reward sharing approach that fos-
ters cooperation, rational nodes will be inclined to defect
from the block creation and consensus process resulting in
an asynchrony state, thereby failing to add new blocks. In
the following section, we outline a game-theoretic model to
analyze the effect of defection in the Algorand network and
propose a solution to prevent defection in the network.
IV. GAME MODEL AND INCENTIVE ANALYSIS
To obtain insight into the strategic behavior of nodes in
Algorand, we model
their interaction using a static non-
cooperative game. We ﬁrst focus on the interaction between
nodes that are supposed to interact and create blocks in each
round. Let us assume that each round i of Algorand is a static
game GAl where network nodes are players. We assume that
all strategies are hard-wired in each node. In other words, each
node does not change his chosen strategy during round i of
the game. They also choose their strategies simultaneously.
In our Algorand game GAl users must decide whether to
cooperate and contribute to make a new block or not. The
game GAl is formally deﬁned as a triplet (P, S, U), where
P is the set of players, S is the set of strategies and U
is the set of payoff values. The set of players P includes
leaders L, committee members M, and all other users K, i.e.,
K. An Algorand node can take an action (si)
P = L
from the set S = {C, D, O}, where C, D, and O represent
(i) Cooperate, (ii) Defect, and (iii) Ofﬂine, respectively. As
we discussed in previous section, cooperative nodes follow all
deﬁned tasks, while defecting nodes are only online but do not
perform their assign tasks. Moreover, a node can play ofﬂine
in round i (i.e., plays O), in which it runs sortition computing
but it becomes ofﬂine and does not receive any reward. Given
the above assumption, the following lemma shows that the O
strategy is always strictly dominated by D strategy.
Lemma 1. In GAl, strategy O is strictly dominated by playing
defection (D).
(cid:6)
(cid:6)
M
Proof. A user always obtains greater payoffs by playing D
instead of O, for all possible strategy proﬁles of other users
(i.e., opponents). In fact, a user can obtain the reward by
playing D in the current version of Algorand, but it’s payoff
would be −cso, if it plays O.
Given the result in Lemma 1, we are not going to consider
strategy O in our analysis as it will not be chosen by any
rational player. In the following section, we present our results
for the analysis of GAl, based on the proposed reward sharing
by the Algorand Foundation [34].
A. Analysis of GAl
In GAl, we deﬁne strategy proﬁles All − D and All − C,
where all nodes choose to play C and D, respectively. We
apply the Nash Equilibrium (NE) concept to analyze GAl. At
the NE proﬁle no player can unilaterally change his strategy to
increase his utility. The following theorem shows the existence
of an all defection strategy (All − D) NE for GAl.
Theorem 1. In each round i of GAl with N players (nL
leaders, nM committee members, and nK remaining nodes),
all-defection strategy proﬁle (All − D) is a Nash equilibrium.
Proof. Let us consider a strategy proﬁle where all Algorand
nodes defect, where there is no incurred costs such as cL, cM ,
or cK for all nodes. Hence, the payoff for each node would
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:36:15 UTC from IEEE Xplore.  Restrictions apply. 
458
(a) Defection Rate: 5%
(b) Defection Rate: 15%
(c) Defection Rate: 25%
Fig. 3: The percentage of nodes who extracted the tentative and ﬁnal blocks with different rate of defection. In each scenario,
defecting nodes will not cooperate if the beneﬁt is not more than the cost of cooperation for them.
be ui = −cso as there is no block added to the chain and they
cannot earn any Algo. In this case:
1) None of the Algorand leaders lj can increase their
payoff unilaterally by changing their strategies. Because,
the cooperative leader can not gain any reward without the
contribution of at least SST EP committee members in each
step of BA(cid:2) protocol and SF IN AL members for the ﬁnal
committee, as discussed in Section II. In other words, the
payoff of a leader who deviates from D to C would be
i (C) =− cL, which is always smaller than his defecting
ulj
payoff (i.e., ulj
i (D) = −cso).
2) Similarly, a cooperative committee member mj cannot
obtain any reward without the contribution of leaders and
sufﬁcient number of committee members. In this case, payoff
of a committee member who has deviated is umj
i = −cM .
3) With similar justiﬁcation, we can prove that all other
online nodes kj will not be able to increase their payoffs
unilaterally by deviating from D to C, as its payoff would
i (C) = −cK. Hence, All − D strategy
be decreased to ukj
proﬁle is a NE in GAl.
In fact, in such distributed protocols one would like to