### 5.2 Theoretical Results and Proofs

The following result is derived by substituting (20) and (21) into (3) and (19), along with the earlier proofs of Theorems 3.1 and 4.1.

**Theorem 5.1.** Under user distortion, the estimators (3) and (19) can be written as:
\[
\alpha_{i}^{t+1} = \frac{1}{m} \sum_{j=1}^{m} \beta_{ij}^{t},
\]
\[
\pi_{iv}^{t+1}(y) = \frac{\sum_{j=1}^{m} \beta_{ij}^{t} \mathbb{1}_{u_j^v = y}}{m \alpha_i^{t+1}},
\]
where \( \beta_{ij}^{t} = p(\omega_i | u_j, \theta_u^t, \alpha^t) \). Furthermore, this is the EM algorithm for \((\theta_u, \alpha)\).

### 5.3 Discussion

To evaluate the result of Theorem 5.1, we construct a new database D3, shown in Table 7, by switching from RTOs to user features. Note that the Linux signature ties Novell in DF and MSS, while Windows does the same in TTL. For simplicity, we use simulation scenarios where \(\phi_{iv} = \phi_v\) for all \(i\), and \(\phi_v\) is the probability that feature \(v\) remains at its default value. This simplifies the matrix \(\phi_{iv}\) to a vector \(\phi_v\), which is easier to follow across different tables.

The initial PMFs \(\pi_{iv}^0\) of the EM algorithm are set to include 90% of the mass on the default value and split the remainder uniformly across the viable alternatives. Since it is believed [42] that the order of non-NOP options cannot be changed without rewriting the TCP/IP stack of the OS, we initialize \(\pi_{i4}^0\) to allow only candidates compatible with the original \(u_{i4}\). For example, MST is feasible for Linux but not for the other two signatures in Table 7. Note that any single option (M, S, W) and the empty set are valid for all three OSes.

We use two models for generating noisy observations. The first model, called RAND, picks uniformly from the space of possible values observed in our Internet scan, except OPT is limited to compatible subsets/supersets of the original. We have 5,695 candidates for Win, four for TTL, two for DF, 266 for OPT, and 1,082 for MSS. Decisions are made independently for each feature \(v\) and each observation \(j\), modeling users "tweaking" their OS without coordinating with each other or sharing a common objective. Even though RAND can generate 13.1 billion unique combinations \(u_j\), only a small subset is encountered by the classifier in our simulations.

The second model, called PATCH, selects an alternative vector of features \(u_i''\) for each OS \(\omega_i\) and switches the default value \(u_{iv}\) to \(u_{iv}''\) with probability \(1 - \phi_v\), again independently for each \(v\). This represents the deployment of software patches that change one of the features to an updated value. The probability for a host to use multiple patches is the product of corresponding \((1 - \phi_v)\)'s. For example, modification to both Win and OPT affects \((1 - \phi_1)(1 - \phi_4)\) fraction of hosts. Vectors \(u_i''\) are non-adversarial and do not attempt to confuse the classifier. We construct them by flipping the DF flag, setting OPT to M, and adding \(i\) to all remaining fields (modulo the max field value). The result is given in Table 8.

To estimate the vector \(\phi_v^t\), we use a weighted average of feature non-modification across all OSes, i.e., \(\phi_v^t = \sum_{i=1}^{n} \alpha_i^t \phi_{iv}^t\). Our next scenario S3 is detailed in Table 9, and the corresponding outcome is given in Table 10. We omit vector \(\alpha^\infty\) since it matches the ground-truth \(\alpha\) very accurately. Due to the new treatment of non-default features in (18), the first iteration of EM in Table 10 is superior to Hershel+. However, both are much worse than the last iteration. It should be noted that the second case S32 modifies Win, TTL, and MSS in 100% of the samples. Identifiability in such conditions is helped by the fact that OPT is constrained to a subset of the original string, making a certain fraction of randomly generated values feasible for only one OS. This allows EM to learn to ignore (Win, TTL, MSS) and focus decisions on (DF, OPT). Furthermore, when guessing is involved, EM uses its knowledge of \(\alpha\) to correctly pick the most-likely OS. It is also interesting that S33 is classified with 100% accuracy once EM gets a grasp on the new values in Table 8 and their probability of occurrence.

### 6. Complete System

#### 6.1 Reset Packets

Because the loss of RST packets causes the corresponding user features (i.e., ACK/RST flags, ACK sequence number, window size [42]) to be wiped out, there is a dependency between the distortion applied by the network and the user. As a result, this case should be handled separately. The first modification needed is to increase the length of network vectors \(d_i\) and \(d_j'\) to accommodate the RST timestamp. The second change is to add RST values into user features. Since it is currently believed that RST fields are unmodifiable independently of each other [42], they can be combined into a single integer and appended to user vectors \(u_i\) and \(u_j'\) in position \(b + 1\).

There are four possible scenarios for handling RST packets, as shown in Table 11, each with a certain probability \(\zeta_{ij}^t\):

| RST Present in \(d_i\) | RST Present in \(d_j'\) | Action | Multiplier \(\zeta_{ij}^t\) |
|------------------------|------------------------|--------|---------------------------|
| Yes                    | Yes                    | -      | 1                         |
| Yes                    | No                     | Ignore RST in \(d_j'\) | 1                         |
| No                     | Yes                    | -      | \(\pi_t(u_{i,b+1})\)      |
| No                     | No                     | -      | -                         |

When both the observation and candidate signature contain an RST, the only multiplier needed is the probability that the received feature was produced by that OS. If the sampled OS has an RST, but the signature does not, this indicates possible interference from an intermediate device (e.g., IDS after expiring connection state, scrubbers). In this case, it is likely meaningless to use the temporal characteristics of the RST, which is why we omit it from \(d_j'\) before computing the loss and delay probabilities. However, multiplication by \(\pi_t(u_{i,b+1})\) is still warranted to assign a proper weight to this mismatch. The third row of the table corresponds to packet loss, which is handled automatically in \(p_i^t(\gamma)\), i.e., no additional actions or multipliers are needed. Finally, the last row is identical to the setup assumed in preceding sections.

#### 6.2 Final Model

We now combine the developed network, user, and RST models into a single framework. Redefining (12) as:
\[
p_{ij\tau\gamma}^t = \alpha_i^t \zeta_{ij}^t \prod_{v=1}^{b} \pi_{iv}^t(u_j^v) f_T^t(\tau) p_i^t(\gamma) \prod_{r=1}^{|d_j'|} f_\Delta^t(\delta_{ij\tau\gamma r}),
\]
allows us to compute \(\beta_{ij\tau\gamma}^t\) still via (13), as well as reuse (14)-(17). However, (23) requires an update to:
\[
\pi_{iv}^{t+1}(y) = \frac{\sum_{j=1}^{m} \sum_{\tau, \gamma} \beta_{ij\tau\gamma}^t \mathbb{1}_{u_j^v = y}}{m \alpha_i^{t+1}},
\]
where \(v = 1, 2, \ldots, b + 1\). The final classifier, which we call Faulds, is applied after EM has converged and is given by:
\[
p(\omega_i | x_j', \theta^\infty, \alpha^\infty) = \sum_{\tau, \gamma} \beta_{ij\tau\gamma}^\infty.
\]

It is easy to generalize our earlier results to cover the complete model, as given in the next theorem without proof.

**Theorem 6.1.** Under both network and user distortion, the estimator (13)-(17), (24)-(25) is the EM algorithm for \((\theta, \alpha)\).

#### 6.3 Scaling the Database

Due to the large number of features it combines, Faulds is not challenged by the previous toy databases. We therefore switch to a more realistic set of signatures created by Plata in [41]. We call this database D4 and note that it contains 420 stacks, among which some have the same exact RTO vector and others overlap in all features.

---

This version of the text is more structured, coherent, and professional, with clear section headings and improved readability.