We model the capability of an adaptive intruder by deﬁn-
ing some conﬁdence bounds. We assume an intruder can de-
viate ˆp− δl, ˆp + δu from the expected ˆp value. Also, based
on our conﬁdence in the detector algorithm and how hard
we expect it to be for an intruder to evade the detector, or to
create non-relevant false positives (this also models how the
1The usual arrow notation: a ← DM (y) implies that DM can be a
probabilistic algorithm.
normal behavior of the system being monitored can produce
new -previously unseen- false alarms), we deﬁne α and β as
bounds to the amount of variation we can expect during the
IDS operation from the false alarms and the detection rate
(respectively) we expected, i.e. the amount of variation from
( ˆPFA, ˆPD) (although in practice estimating these bounds is
not an easy task, testing approaches like the one described
in [33] can help in their determination).
The intruder also has access to an oracle Feature(·,·)
that simulates an event to input into the IDS. Feature(0,ζ)
outputs a feature vector modeling the normal behavior of
the system that will raise an alarm with probability ζ (or
a crafted malicious feature to only raise alarms in the case
Feature(0,1)). And Feature(1,ζ) outputs the feature vec-
tor of an intrusion that will raise an alarm with probability
ζ.
Deﬁnition 2 A (δ,α,β) − intruder is an algorithm I that
can select its frequency of intrusions p1 from the interval
δ = [ ˆp−δl, ˆp+δu]. If it decides to attempt an intrusion, then
with probability p2 ∈ [0,β], it creates an attack feature x that
will go undetected by the IDS (otherwise this intrusion is
detected with probability ˆPD). If it decides not to attempt an
intrusion, with probability p3 ∈ [0,α] it creates a feature x
that will raise a false alarm in the IDS
I(δ,α,β)
Select p1 ∈ [ ˆp− δl, ˆp + δu]
Select p2 ∈ [0,α]
Select p3 ∈ [0,β]
I ← Bernoulli(p1)
If I = 1
B ← Bernoulli(p3)
x ← Feature(1,(min{(1− B), ˆPD}))
B ← Bernoulli(p2)
x ← Feature(0,max{B, ˆPFA})
Else
Output (I,x)
where Bernoulli(ζ) outputs a Bernoulli random variable
with probability of success ζ.
Furthermore, if δl = p and δu = 1− p we say that I has
♦
the ability to make a chosen-intrusion rate attack.
We now formalize what it means for an evaluation
scheme to be robust. We stress the fact that we are not ana-
lyzing the security of an IDS, but rather the security of the
evaluation of an IDS, i.e. how conﬁdent we are that the IDS
will behave during operation similarly to what we assumed
in the evaluation.
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:52:29 UTC from IEEE Xplore.  Restrictions apply. 
5.1. Robust Expected Cost Evaluation
We start with the general decision theoretic framework
of evaluating the expected cost (per input) E[C(I, A)] for an
IDS.
Deﬁnition 3 An evaluation method that claims the ex-
pected cost of an I DS is at most r is robust against a
(δ,α,β)− intruder if the expected cost of I DS during the
attack (Eδ,α,β[C[I, A)]) is no larger than r, i.e.
i,a
C(i, a)×
Eδ,α,β[C[I, A)] = ∑
Pr[ (I,x) ← I(δ,α,β); A ← I DS(x) : I = i, A = a ] ≤ r
♦
Now recall that the traditional evaluation framework
by using equation (6). So by
ﬁnds an evaluation value r
we are basically ﬁnding the best performance of
ﬁnding r
is
an IDS and claiming the IDS is better than others if r
smaller than the evaluation of the other IDSs. In this sec-
tion we claim that an IDS is better than others if its expected
value under the worst performance is smaller than the ex-
pected value under the worst performance of other IDSs. In
short
∗
∗
∗
Traditional Evaluation Given
IDSs
{I DS 1,I DS 2, . . . ,I DS n} ﬁnd the best expected
cost for each:
set
of
a
∗
i =
r
min
β
,P
D
)∈ROCi
(Pα
FA
E[C(I, A)]
(15)
Declare that the best IDS is the one with smallest ex-
pected cost r
∗
i .
Robust Evaluation Given
a
of
set
IDSs
{I DS 1,I DS 2, . . . ,I DS n} ﬁnd the best expected
cost for each I DS i when being under the attack of a
(δ,αi,βi)− intruder2. Therefore we ﬁnd the best IDS
as follows:
=
Eδ,αi,βi[C(I, A)]
max
rrobust
i
min
)∈ROC
βi
,P
D
αi,βi
i
(P
αi
FA
I(δ,αi,βi)
(16)
Several important questions can be raised by the above
framework.
In particular we are interested in ﬁnding the
least upper bound r such that we can claim the evaluation of
I DS to be robust. Another important question is how can
we design an evaluation of I DS satisfying this least upper
bound? Solutions to these questions are partially based on
game theory.
2Note that different IDSs might have different α and β values. For
example if I DS 1 is an anomaly detection scheme then we can expect that
the probability that new normal events will generate alarms α1 is larger
than the same probability α2 for a misuse detection scheme I DS 2.
Lemma 2 Given an initial estimate of the base-rate ˆp, an
initial ROC curve obtained from D, and constant costs
C(I, A), the least upper bound r such that the expected cost
evaluation of I DS is r-robust is given by
)(1− ˆpδ) + R(1, ˆP
α
r = R(0, ˆP
FA
) ˆpδ
(17)
β
D
where
R(0, ˆPα
FA
) ≡ [C(0,0)(1− ˆPα
FA
) +C(0,1) ˆPα
FA
]
is the expected cost of I DS under no intrusion and
β
R(1, ˆP
D
) ≡ [C(1,0)(1− ˆP
β
D
β
) +C(1,1) ˆP
D
]
(18)
(19)
is the expected cost of I DS under an intrusion, and ˆpδ,
β
ˆPα
FA and ˆP
D are the solution to a zero-sum game between
the intruder (the maximizer) and the IDS (the minimizer),
whose solution can be found in the following way:
1. Let (PFA, PD) denote any points of the initial ROC ob-
tained from D and let ROC(α,β) be the ROC curve de-
= PD(1 − β)
ﬁned by the points (Pα
FA
and Pα
FA
β
, P
= α + PFA(1− α).
D
β
), where P
D
2. Using ˆp + δu in the isoline method, ﬁnd the optimal
operating point (xu, yu)in ROC(α,β) and using ˆp − δl
in the isoline method, ﬁnd the optimal operating point
(xl, yl) in ROC(α,β).
∗, y
3. Find the points (x
∗) in ROC(α,β) that intersect the
line
+ x
y = C(1,0)−C(0,0)
C(1,0)−C(1,1)
C(0,0)−C(0,1)
C(1,0)−C(1,1)
(under the natural assumptions C(1,0) > R(0,x
C(0,0), C(0,1) > C(0,0) and C(1,0) > C(1,1)).
there are no points that intersect this line, then set x
∗ = 1.
y
∗) >
If
∗ =
4. If x
∗ ∈ [xl, xu] then ﬁnd the base-rate parameter p
such that the optimal isoline of Equation (9) intercepts
ROC(α,β) at (x
and
β
ˆP
D
∗) and set ˆpδ = p
∗
∗
= y
ˆPα
FA
∗, y
= x
∗
∗
,
.
5. Else if R(0, xu) < R(1,yu) ﬁnd the base-rate param-
eter pu such that the optimal isoline of Equation (9)
intercepts ROC(α,β) at (xu, yu) and then set ˆpδ = pu,
ˆPα
= yu. Otherwise, ﬁnd the base-rate
FA
parameter pl such that the optimal isoline of Equation
(9) intercepts ROC(α,β) at (xl, yl) and then set ˆpδ = pl,
ˆPα
FA
β
= xu and ˆP
D
β
= xl and ˆP
D
= yl.
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:52:29 UTC from IEEE Xplore.  Restrictions apply. 
The proof of this lemma is very straightforward. The ba-
sic idea is that if the uncertainty range of p is large enough,
the Nash equilibrium of the game is obtained by selecting
the point intercepting equation (3). Otherwise one of the
strategies for the intruder is always a dominant strategy of
the game and therefore we only need to ﬁnd which one is
it: either ˆp + δu or ˆp− δl. For most practical cases it will
be ˆp + δu. Also note that the optimal operating point in
the original ROC can be found by obtaining ( ˆPFA, ˆPD) from
( ˆPα
FA
β
, ˆP
D
).
5.2. Robust IDOC Evaluation
Similarly we can now also analyze the robustness of the
evaluation done with the IDOC curves. In this case it is also
easy to see that the worst attacker for the evaluation is an
intruder I that selects p1 = ˆp− δl, p2 = α and p3 = β.
ˆPPV , ˆPD) corresponding to ˆp
Corollary 3 For any point (
in the IDOC curve, a (δ,α,β)− intruder can decrease the
detection rate and the positive predictive value to the pair
(
ˆPPV
), where ˆPβ = ˆPD(1− β) and where
δ,α,β
β
, ˆP
D
δ,α,β =
ˆPPV
D p− Pβδ
β
P
(1− p) + δPα
FA
− δP
β
D
β
D p + Pα
P
FA
(20)
5.3. Example: Minimizing the Cost of a
Chosen Intrusion Rate Attack
We now present an example that shows the generality of
lemma 2 and also presents a compelling scenario of when
does a probabilistic IDSs make sense. Assume an ad hoc
network scenario similar to [20, 36, 32, 4] where nodes
monitor and distribute reputation values of other nodes’ be-
havior at the routing layer. The monitoring nodes report
selﬁsh actions (e.g. nodes that agree to forward packets in
order to be accepted in the network, but then fail to do so) or
attacks (e.g. nodes that modify routing information before
forwarding it).
Now suppose that there is a network operator consider-
ing implementing a watchdog monitoring scheme to check
the compliance of nodes forwarding packets as in [20].
The operator then plans an evaluation period of the method
where trusted nodes will be the watchdogs reporting the
misbehavior of other nodes. Since the detection of misbe-
having nodes is not perfect, during the evaluation period
the network operator is going to measure the consistency of
reports given by several watchdogs and decide if the watch-
dog system is worth keeping or not.
During this trial period, it is of interest to selﬁsh nodes
to behave as deceiving as they can so that the neighboring
watchdogs have largely different results and the system is
not permanently established. As stated in [20] the watch-
dogs might not detect a misbehaving node in the presence
of 1) ambiguous collisions, 2) receiver collisions, 3) limited
transmission power, 4) false misbehavior, 5) collusion or 6)
partial dropping. False alarms are also possible in several
cases, for example when a node moves out of the previous
node’s listening range before forwarding on a packet. Also
if a collision occurs while the watchdog is waiting for the