end times because of occasional instances where events were not
marked as resolved until long after their apparent resolution.
3.4
Identifying failures with impact
As previously stated, one of our goals is to identify failures
that potentially impact end-users and applications. Since we did
not have access to application monitoring logs, we cannot precisely
quantify application impact such as throughput loss or increased re-
sponse times. Therefore, we instead estimate the impact of failures
on network trafﬁc.
To estimate trafﬁc impact, we correlate each link failure with
trafﬁc observed on the link in the recent past before the time of
failure. We leverage ﬁve minute trafﬁc averages for each link that
failed and compare the median trafﬁc on the link in the time win-
dow preceding the failure event and the median trafﬁc during the
failure event. We say a failure has impacted network trafﬁc if the
median trafﬁc during the failure is less than the trafﬁc before the
failure. Since many of the failures we observe have short durations
(less than ten minutes) and our polling interval is ﬁve minutes, we
do not require that trafﬁc on the link go down to zero during the
failure. We analyze the failure impact in detail in Section 5.
Table 3 summarizes the impact of link failures we observe.
We separate links that were transferring no data before the failure
into two categories, “inactive” (no data before or during failure) and
“provisioning” (no data before, some data transferred during fail-
ure). (Note that these categories are inferred based only on trafﬁc
observations.) The majority of failures we observe are on links that
are inactive (e.g., a new device being deployed), followed by link
failures with impact. We also observe a signiﬁcant fraction of link
Table 3: Summary of logged link events
Category
All
Inactive
Provisioning
No impact
Impact
No trafﬁc data
Percent
100.0
41.2
1.0
17.9
28.6
11.3
Events
46,676
19,253
477
8,339
13,330
5,277
failure notiﬁcations where no impact was observed (e.g., devices
experiencing software errors at the end of the deployment process).
For link failures, verifying that the failure caused impact to
network trafﬁc enables us to eliminate many spurious notiﬁcations
from our analysis and focus on events that had a measurable impact
on network trafﬁc. However, since we do not have application level
monitoring, we are unable to determine if these events impacted
applications or if there were faults that impacted applications that
we did not observe.
For device failures, we perform additional steps to ﬁlter spuri-
ous failure messages (e.g., down messages caused by software bugs
when the device is in fact up). If a device is down, neighboring de-
vices connected to it will observe failures on inter-connecting links.
For each device down notiﬁcation, we verify that at least one link
failure with impact has been noted for links incident on the device
within a time window of ﬁve minutes. This simple sanity check sig-
niﬁcantly reduces the number of device failures we observe. Note
that if the neighbors of a device fail simultaneously e.g., due to a
correlated failure, we may not observe a link-down message for that
device.
For the remainder of our analysis, unless stated otherwise, we
consider only failure events that impacted network trafﬁc.
4. FAILURE ANALYSIS
4.1 Failure event panorama
Figure 3 illustrates how failures are distributed across our mea-
surement period and across data centers in our network. It shows
plots for links that experience at least one failure, both for all fail-
ures and those with potential impact; the y-axis is sorted by data
center and the x-axis is binned by day. Each point indicates that the
link (y) experienced at least one failure on a given day (x).
All failures vs. failures with impact. We ﬁrst compare the view of
all failures (Figure 3 (a)) to failures having impact (Figure 3 (b)).
Links that experience failures impacting network trafﬁc are only
about one third of the population of links that experience failures.
We do not observe signiﬁcant widespread failures in either plot,
with failures tending to cluster within data centers, or even on in-
terfaces of a single device.
Widespread failures: Vertical bands indicate failures that were
spatially widespread. Upon further investigation, we ﬁnd that these
tend to be related to software upgrades. For example, the vertical
band highlighted in Figure 3 (b) was due to an upgrade of load bal-
ancer software that spanned multiple data centers. In the case of
planned upgrades, the network operators are able to take precau-
tions so that the disruptions do not impact applications.
Long-lived failures: Horizontal bands indicate link failures on a
common link or device over time. These tend to be caused by prob-
lems such as ﬁrmware bugs or device unreliability (wider bands
indicate multiple interfaces failed on a single device). We observe
horizontal bands with regular spacing between link failure events.
In one case, these events occurred weekly and were investigated
35312000
10000
8000
6000
4000
2000
r
e
t
n
e
c
a
t
a
d
y
b
d
e
t
r
o
s
s
k
n
i
L
12000
10000
8000
6000
4000
2000
r
e
t
n
e
c
a
t
a
d
y
b
d
e
t
r
o
s
s
k
n
i
L
0
Oct-09
Dec-09
Feb-10
Apr-10
Jul-10
Sep-10
0
Oct-09
Time (binned by day) 
(a) All failures
Dec-09
Feb-10
Apr-10
Jul-10
Sep-10
Time (binned by day) 
(b) Failures with impact
Figure 3: Overview of all link failures (a) and link failures with impact on network trafﬁc (b) on links with at least one failure.
e
r
u
l
i
a
f
f
o
y
t
i
l
i
b
a
b
o
r
P
0.25
0.2
0.15
0.1
0.05
0
0.219 
0.214 
0.111 
0.039  0.045 
0.030  0.027  0.020 
0.005 
ToR-1 ToR-2 ToR-3 AggS-2 ToR-5 LB-2 ToR-4 AggS-1 LB-1
Figure 4: Probability of device failure in one year for device
types with population size of at least 300.
Device type 
in independent NOC tickets. As a result of the time lag, the op-
erators did not correlate these events and dismissed each notiﬁca-
tion as spurious since they occurred in isolation and did not impact
performance. This underscores the importance of network health
monitoring tools that track failures over time and alert operators to
spatio-temporal patterns which may not be easily recognized using
local views alone.
Table 4: Failures per time unit
Failures per day: Mean Median
3.0
Devices
Links
18.5
5.2
40.8
95% COV
1.3
14.7
136.0
1.9
4.2 Daily volume of failures
We now consider the daily frequency of failures of devices
and links. Table 4 summarizes the occurrences of link and device
failures per day during our measurement period. Links experience
about an order of magnitude more failures than devices. On a daily
basis, device and link failures occur with high variability, having
COV of 1.3 and 1.9, respectively. (COV > 1 is considered high
variability.)
Link failures are variable and bursty. Link failures exhibit high
variability in their rate of occurrence. We observed bursts of link
0.25
0.2
e
r
u
l
i
0.15
0.1
0.05
0
a
f
f
o
y
t
i
l
i
b
a
b
o
r
P
0.176 
0.095 
0.095 
0.054 
0.026 
0.028 
TRUNK MGMT
CORE
LB
Link type 
ISC
IX
Figure 5: Probability of a failure impacting network trafﬁc in
one year for interface types with population size of at least 500.
failures caused by protocol issues (e.g., UDLD [9]) and device is-
sues (e.g., power cycling load balancers).
Device failures are usually caused by maintenance. While de-
vice failures are less frequent than link failures, they also occur in
bursts at the daily level. We discovered that periods with high fre-
quency of device failures are caused by large scale maintenance
(e.g., on all ToRs connected to a common AggS).
4.3 Probability of failure
We next consider the probability of failure for network ele-
ments. This value is computed by dividing the number of devices
of a given type that observe failures by the total device population
of the given type. This gives the probability of failure in our one
year measurement period. We observe (Figure 4) that in terms of
overall reliability, ToRs have the lowest failure rates whereas LBs
have the highest failure rate. (Tables 1 and 2 summarize the abbre-
viated link and device names.)
Load balancers have the highest failure probability. Figure 4
shows the failure probability for device types with population size
of at least 300. In terms of overall failure probability, load balancers
(LB-1, LB-2) are the least reliable with a 1 in 5 chance of expe-
riencing failure. Since our deﬁnition of failure can include inci-
dents where devices are power cycled during planned maintenance,
we emphasize here that not all of these failures are unexpected.
Our analysis of load balancer logs revealed several causes of these
354 
e
g
a
t
n
e
c
r
e
P
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
failures
downtime
66% 
38% 
28% 
18% 
15% 
2% 
LB-1
LB-2
9% 
5% 
8% 
4% 
4% 
0.4% 
ToR-1
LB-3
Device type 
ToR-2
AggS-1
e
g
a
t
n
e
c
r
e
P
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
70% 
58% 
26% 
9% 
TRUNK
LB
failures
downtime
5% 
2% 
6% 
5% 
5% 
1% 
1% 
12% 
MGMT
ISC
Link type 
CORE
IX
Figure 6: Percent of failures and downtime per device type.
Figure 7: Percent of failures and downtime per link type.
Table 5: Summary of failures per device (for devices that expe-
rience at least one failure).
Device type Mean Median
LB-1
1.0
1.0
LB-2
1.0
ToR-1
1.0
LB-3
1.0
ToR-2