scenario where a randomly sampled batch is too imbalanced
(for example, no damaging tweet sampled in the batch). A
separate BERT model is ﬁne-tuned for the challenger using
the loss function in Equation (4). Note that no balancing is
required here since all the input tweets to the challenger model
are non-damaging. We note that explaining the exact strategy
employed by BERT models to classify text is an active research
topic and complementary to our efforts. However, we highlight
that our challenger does not use any information about either
the adversary’s exact model or its parameters.
Budget constraints: We allow a limited budget of Bstatic =
200 deleted tweets for the static adversary and set τ = 1,
i.e., the static adversary only trains during the ﬁrst out of
the ten intervals. Similarly for the adaptive adversary, we
allow a ﬁxed recurring budget of Badapt = 200 deleted tweets
every interval. There are no budget restraints for random and
oracle challengers (having no access and black-box access
respectively). However, we restrict the D2 challenger to have
the same (recurring) query budget as the adaptive adversary’s
recurring budget to keep the game fair, i.e., Bg = Badapt = 200.
We simulate the game described in Algorithm 3 with
an adversary and a challenger, both implemented as BERT
language models, with 10 different random seeds. We repeat
the experiments for k = 1, 2, 5 where k denotes the number
of decoy posts added per damaging deletion.
10
No access: The top row of Figure 4 shows the performance
of the three adversaries (random, static, and adaptive) in the
presence of the random challenger. We observe that although
the F-score of both the static and the adaptive adversary
decreases for all values of k, the reduction is not signiﬁcant
(only 7 percentage points for k=1 compared to the no-
challenger case) In fact, both the adversaries still perform much
better than the random adversary. This shows that protection
of damaging deletions in the no-access scenario is possible but
severely limited.
Black-box access: The middle row of Figure 4 shows the
performance of the adversaries in the presence of an oracle
challenger. Not surprisingly, this approach is very effective
at
lowering the (static and adaptive) adversaries’ F-scores
(close to random for k=1, 2; i.e., 20 and 35 percentage point
reduction in the case of k=1 for the static and adaptive
adversary respectively compared to the no-challenger case).
We also observe a major difference between the static and
the adaptive adversaries in the presence of a competitive chal-
lenger. The static adversary retains the same recall performance
(as in the no-challenger case) but loses drastically in precision,
i.e., it classiﬁes a large number of decoy posts as damaging.
On the other hand, the adaptive adversary tries to adapt to the
presence of decoy posts and becomes highly conservative –
retains the same precision performance (as in the no-challenger
case) but suffers heavily in the recall performance, i.e., it
classiﬁes a large number of damaging posts as non-damaging.
Restricted black-box access: The bottom row of Figure 4
shows the performance of the adversaries in the presence of
the D2 challenger. The performance of the D2 challenger is
comparable to the oracle challenger. The adversaries’ F-scores
in the presence of the D2 challenger is close to 45% for the
case of k=1 (20 and 30 percentage point reduction for the
static and adaptive adversaries respectively compared to the no-
challenger case). We also observe a precision-recall trade-off
separating the static and the adaptive adversary (i.e., the static
adversary loses in precision, whereas the adaptive adversary
loses in recall) similar to the one described in the presence of
an oracle challenger .
Overall, we conclude that the D2 challenger is able to
successfully raise the bar for the adversaries in identifying
damaging deletions without requiring an unmonitored black-
box access with inﬁnite query budget.
The increase of decoy posts (k) results in lower adversarial
performance with diminishing returns. While examining
each row of Figure 4 individually, we see that the performance
of the adversaries always decreases as k, the number of decoy
deletions per damaging deletion, increases. However, we also
observe that k = 1 is enough to reduce the F-scores of the
adversaries to 45% (close to the random adversary). Since the
goal of most social platforms is to retain as many posts as
possible, it would not be in the platform’s best interests to use
much larger values of k or to delete the entire volunteered set.
Observation of damaging and decoy posts. In Table I in the
Appendix, we show damaging tweets (as labeled by the AMT
workers) and decoy tweets (chosen by the D2 challenger from
a set of non-deleted tweets). We observe that even though the
decoy tweets typically seem to have sensitive words, they do
not possess content damaging to the owner.
Fig. 3: F-score of different adversaries (random, static, adaptive)
when no privacy preserving deletion mechanism is in place.
Shaded areas represent 95% conﬁdence intervals.
D. Results
Figures 3 and 4 show the F-scores (with 95% conﬁdence
intervals), precision and recall for different adversaries over 10
time intervals. We make the following key observations.
Detection of damaging deletions in social media platforms
is a serious concern. We start by considering the case where
no privacy-preserving deletion mechanism is in place (i.e., no
challenger to inject decoy deletions). In such a scenario, we
compare the efﬁciency of different types of adversaries ten
intervals shown in Figure 3.
The random adversary labels the posts based on the prior
distribution of the deleted tweets (around 42% damaging and
58% non-damaging every interval). As expected, the adversary
achieves a 42% precision and 58% recall resulting in an F-
score of about 48% in each interval.
in the ﬁrst
interval,
As shown in Figure 3,
the static
adversary achieves a 17 percentage points (i.e., a 35%) increase
in its F-score compared to the random adversary, and remains
almost constant over the rest of the intervals. On the other
hand, the adaptive adversary receives new training data every
interval and trains its classiﬁer continually, and hence is able
to increase its F-score even further by about 10 percentage
points (56% increase compared to the random adversary) at
the end of the 10th interval.
This shows that even normal users of social media plat-
forms, not only celebrities and politicians, are vulnerable to
the detection of their damaging deletions. Furthermore, the
adversaries can automate this attack on a large-scale with an
insigniﬁcant amount of overhead (access to a small dataset of
posts with the corresponding labels), highlighting the neces-
sity for a much-needed privacy-preserving mechanism for the
users’ damaging deletions in today’s social platforms.
Injecting decoy deletions decreases the adversarial perfor-
mance. As explained in Sections III and IV, we consider
three challengers corresponding to the three types of accesses
to the adversary’s model – no access, black-box access, and
restricted black-box access. In the following, we compare the
performance of the adversaries in the presence of the respective
challengers against the absence of any challenger case above.
11
12345678910Intervals0.20.40.60.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75RecallRandom adversaryStatic adversaryAdaptive adversary(a) Random challenger (k = 1)
(b) Random challenger (k = 2)
(c) Random challenger (k = 5)
(No access.) Adversaries (random, static and adaptive) in the presence of random challenger with k = 1, 2, 5.
(d) Oracle challenger (k = 1)
(e) Oracle challenger (k = 2)
(f) Oracle challenger (k = 5)
(Black-box access.) Adversaries (random, static and adaptive) in the presence of oracle challenger with k = 1, 2, 5.
(g) D2 challenger (k = 1)
(h) D2 challenger (k = 2)
(i) D2 challenger (k = 5)
(Restricted black-box access.) Adversaries (random, static and adaptive) in the presence of D2 challenger with k = 1, 2, 5.
Fig. 4: F-score (with 95% conﬁdence intervals), precision and recall for the three adversaries (random, static and adaptive)
in the presence of different challengers corresponding to different accesses with k = 1, 2, 5. Key observation: D2 challenger
fools the adversaries almost as well as the oracle challenger but with a restricted black-box access.
VI. DISCUSSION
volunteered posts collected up until this point.
A. Adversarial Deception Tactics
The adversary can use different techniques to sabotage the
challenger. Here, we mention some prominent systems attacks
and their effects on the challenger.
Denial of Service attack. One of such attacks could be a
simple Denial of Service (DoS), where the attacker submits
requests for many damaging deletions to consume all
the
volunteer posts. First, we remind that the volunteered posts are
a renewable resource, not a ﬁnite resource, as the users create,
volunteer and delete posts in each time interval. Regardless, a
DOS attack is possible wherein the adversary can use up all
A standard way to avoid such attacks is to limit the number
of damaging deletions that can be protected for each user in
one time interval (we assume that the adversary can have
many adversarial users to help with the DoS attack but is
not allowed to use bots [26], [30], [37], [40], [82], [86]). As is
clear from Section IV-D, the challenger’s defense is dependent
on the distribution and number of volunteered posts. If there
are more adversarial users than volunteers, then the adversary
can win the game.
We implemented the DoS attack as follows: in every inter-
val, the adversary deletes as much as the standard deletions.
We observed that the F-score did not change in this situation.
12
12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75Recall12345678910Intervals0.20.30.40.50.60.70.8F-score2468100.250.50.75Precision246810Intervals0.250.50.75RecallVolunteer Identiﬁcation attack. In a volunteer identiﬁcation
attack, the adversary deletes a bunch of posts and uses the
process of doing so to identify individuals who volunteer posts
to the challenger for deletion. First, we note that in each
time interval there is a large number of posts being deleted
(> 100 million tweets daily [62]). Thus the posts deleted by
the adversary (to try to identify volunteers) and the corre-
sponding decoy deletions are mixed with other (damaging/non-
damaging/decoy) deletions. In such a case, identifying the
volunteers is equivalent
to separating the decoy deletions
from the damaging deletions; reducing to the original task.
Additionally, the challenger does not delete the decoy posts at
the same time as the original damaging deletion but does so
in batches spread out within the time interval.
Further, the volunteers can also have damaging deletions of
their own. Even if an adversary is able to identify volunteers,
the adversary still needs to ﬁgure out which of the volunteer’s
deletions are decoys. If the adversary ignores all posts from
volunteers, then a simple protection for the users is to become
a volunteer, which helps our cause.
In this attack,
Adversary disguising as volunteer.
the
adversary can take the role of a volunteer (or hire many
volunteers) to offer posts to the challenger. Subsequently, the
challenger may select the adversary’s posts as decoys in the
later intervals; however, these posts do not provide deletion
privacy as the adversary will be able to discard these decoy
posts easily. This effect can be mitigated with the help of more
genuine volunteers and increasing the number of decoys per
damaging deletion. This points to a more fundamental problem
with any crowdsourcing approach: if the number of adversarial
volunteers is more than the number of genuine volunteers, the
approach fails.
Differentiating between different damaging categories.
In
this work, all
the damaging posts are treated the same.
However, in practice, the damaging posts fall into different
categories, and some may be more harmful to the users than
others. As a result, the adversary can focus on those categories
more carefully. In such a case, the challengers outputs and loss
function need to be modiﬁed—the challenger needs to output a
weight per damaging category for each decoy post (indicating
the likelihood of fooling the adversary as a damaging post
of that category). The challenger would also have to balance
the different categories of decoy posts to keep the same
distribution of categories as in the real damaging posts.
B. Obtaining volunteered posts from users
Volunteer posts are a signiﬁcant component of our system.
We identify that there are already deletion services which en-
able users to delete their content in bulk (e.g., “twitWipe” [12]
and “tweetDelete” [10] for Twitter, “Social Book Post Man-
ager” [2] for Facebook, “Cleaner for IG” [14] for Instagram,
“Nuke Reddit History” [15], and multiple bots on RequestABot
subreddit for Reddit). Our system can beneﬁt from these bulk
deletions to construct
the volunteered posts pool. In such
a scenario, whenever a user bulk-deletes it will mark its
damaging posts and the remaining posts will be considered
as “volunteered” with a guarantee that they will be deleted
within a ﬁxed time period.
We contacted the deletion services mentioned above and
shared our proposal, Deceptive Deletions, for the privacy of
users’ damaging deletions. The responses that we received
have been positive. They attest that, with Deceptive Deletions,
an attacker that observes the deletion of users in large numbers
will have a harder time ﬁguring out which of the deleted posts
contain sensitive material.
Nevertheless, other strategies could be more effective,
for instance, one based on costs and rewards. Under such
a strategy, each user seeking privacy for his/her damaging
deletions is required to pay a cost for the service, whereas the
users that volunteer their non-damaging posts to be deleted
by the challenger (at any future point in time) are rewarded7.
The costs and rewards can be monetary or can be in terms
of the number of posts themselves (i.e., a user has to volun-
teer a certain number of her non-damaging posts to protect
her damaging deletion). Nevertheless, in an ideal world, the
volunteered set could also be obtained from altruistic users
who offer their non-damaging posts for the protection of other
users’ deletions.
Finally, we emphasize that (as observed in Section V-D)
even when there is one decoy post for each damaging post
(k = 1),
the task of the adversary becomes signiﬁcantly
harder. Further, as we state in Appendix A, the percentage
of damaging deletions versus the non-damaging ones is sig-
niﬁcantly lower (i.e., 18% to 82%). Therefore, we can reckon
that obtaining the pool of volunteer posts is realizable.
C. Rate Limiting The Adversary’s Data Access
In this work, we consider a very powerful adversary in
terms of data access—it is capable of taking snapshots of
the entire platform at different times to identify deleted posts
(see Section III-B). However, in practice, platforms can use
rate-limiting techniques to restrict access of the adversary to
the users’ proﬁle. Client-side strategies [63], [64], deferred
responding [17], and the common limitations on source IP
address, user, and API key [17], [19] are some of the well-
known practices. A more sophisticated approach is to use
computational puzzles, where the adversary can only access the
data after successfully computing a puzzle given by the data
platform. Sample domains include data breach mitigation [57],
[85], DDOS [34], [52], spam-prevention [39], and practical
cryptocurrencies [59]. These types of data limiting restrictions
are interesting future work and will only improve our results.
In such a case, the adversary will not be able to observe all
the users’ proﬁles constantly, or it will have blackout periods
of the users’ proﬁles (not observing the deletions).
D. Deceptive Learning Game vs GANs
Recall that in our setting, the task of the challenger is
to select posts from a pre-deﬁned volunteered set Dv. An
alternative approach is to use generative models [35], [44],
[55], [73], [92] to generate fake texts —see Zhang et al. [93]
for a recent survey and Radford et al. [73] for the state-of-the-
art— enabling the challenger to generate decoy posts instead
of selecting them from a pre-deﬁned set. However, we note
that such generative models might not be favorable or even
effective in practical systems.
7Other distributed systems use similar concept such as BitTorrent [53], [79].
13