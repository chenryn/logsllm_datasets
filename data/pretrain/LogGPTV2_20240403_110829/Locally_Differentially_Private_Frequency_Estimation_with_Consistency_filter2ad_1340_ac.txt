the original distribution f(cid:48), the vector Àúf is a set of independent
random variables, where each component Àúfv follows Gaussian
distribution with mean f(cid:48)
v . The likelihood
of Àúf given f(cid:48) is thus
v and variance œÉ(cid:48)2
(cid:104)Àúf|f(cid:48)(cid:105)
Pr
(cid:104)Àúf|f(cid:48)(cid:105)
(cid:89)
‚âà(cid:89)
1(cid:112)2œÄœÉ(cid:48)2
=
v
¬∑ e
v
v
(cid:104) Àúfv|f(cid:48)
v
(cid:105)
Pr
‚àí (f(cid:48)
v‚àí Àúfv )2
2œÉ(cid:48)2
v =
1(cid:112)2œÄ(cid:81)
v œÉ(cid:48)2
v
‚àí(cid:80)
v
¬∑ e
(f(cid:48)
v‚àí Àúfv )2
2œÉ(cid:48)2
v
.
(9)
To differentiate from [19], we call it MLE-Apx.
‚Ä¢ MLE-Apx: First use standard FO, then compute the MLE
with constraints (summing-to-one and non-negativity) to
recover the values.
In Appendix B, we use the KKT condition [21], [20] to obtain
an efÔ¨Åcient solution. In particular, we partition the domain D
into D0 and D1, where D0 ‚à© D1 = ‚àÖ and D0 ‚à™ D1 = D. For
v ‚àà D0, f(cid:48)
v = 0; for v ‚àà D1,
f(cid:48)
v =
q(1 ‚àí q)xv + Àúfv(p ‚àí q)
p ‚àí q ‚àí (p ‚àí q)(1 ‚àí p ‚àí q)xv
(10)
where
xv =
(cid:80)
Àúfv(p ‚àí q) ‚àí (p ‚àí q)
x‚ààD1
(p ‚àí q)(1 ‚àí p ‚àí q) ‚àí |D1|q(1 ‚àí q)
We can rewrite Equation (10) as
f(cid:48)
v = Àúfv ¬∑ Œ≥ + Œ¥,
where
Œ≥ =
Œ¥ =
p ‚àí q
p ‚àí q + (p ‚àí q)(1 ‚àí p ‚àí q)xv
p ‚àí q + (p ‚àí q)(1 ‚àí p ‚àí q)xv
q(1 ‚àí q)xv
Hence MLE-Apx appears to represent some hybrid of Norm-
Sub and Norm-Mul. In evaluation, we observe that Norm-Sub
and MLE-Apx give very close results, as Œ≥ ‚àº 1. Furthermore,
6
Method
Base-Pos
Post-Pos
Base-Cut
Norm
Norm-Mul
Norm-Cut
Norm-Sub
MLE-Apx
Power
PowerNS
Description
Convert negative est. to 0
Convert negative query result to 0
Convert est. below threshold T to 0
Add Œ¥ to est.
Convert negative est. to 0, then multiply Œ≥ to positive est.
Convert negative and small positive est. below Œ∏ to 0.
Convert negative est. to 0 while adding Œ¥ to positive est.
Convert negative est. to 0, then add Œ¥ to positive est.
Fit Power-Law dist., then minimize expected squared error
Apply Norm-Sub after Power
TABLE I
SUMMARY OF METHODS.
Non-neg
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes
Sum to 1
Complexity
No
No
No
Yes
Yes
Yes
Yes
No
Yes
Almost
O(d)
N/A
O(d)
O(d)
O(d)
O(d)
O(d)
‚àö
O(d)
‚àö
n ¬∑ d)
n ¬∑ d)
O(
O(
in variance is dominated by the
when the fv component
other component (as in Equation (5)), the CLS formulation
is equivalent to our MLE formulation.
E. Least Expected Square Error
Jia et al. [17] proposed a method in which one Ô¨Årst
assumes that the data follows some type of distribution (but
the parameters are unknown), then uses the estimates to Ô¨Åt the
parameters of the distribution, and Ô¨Ånally updates the estimates
that achieve expected least square.
‚Ä¢ Power: Fit a distribution, and then minimize the expected
squared error.
(cid:105)
E(cid:104)
Formally, for each value v, the estimate Àúfv given by FO
is regarded as the addition of two parts: the true frequency
fv and noise following the normal distribution (as shown
in Equation (6)). The method then Ô¨Ånds f(cid:48)
v that minimizes
. To solve this problem, the authors esti-
mate the true distribution fv from the estimates Àúf (where Àúf is
the vector of the Àúfv‚Äôs).
(fv ‚àí f(cid:48)
v)2| Àúfv
In particular, it is assume in [17] that the distribution follows
Power-Law or Gaussian. The distributions can be determined
by one or two parameters, which can be Ô¨Åtted from the
estimation Àúf. Given Pr [x] as the probability fv = x from
the Ô¨Åtted distribution, and Pr [x ‚àº N (0, œÉ)] as the pdf of x
drawn from the Normal distribution with 0 mean and standard
deviation œÉ (as in Equation (6)), one can then minimize the
objective. SpeciÔ¨Åcally, for each value v ‚àà D, output
(cid:105) ¬∑ Pr [x] ¬∑ x
(cid:105) ¬∑ Pr [y] dy
dx.
(11)
(cid:104)
(cid:104)
(cid:90) 1
Pr
(cid:82) 1
0
0 Pr
f(cid:48)
v =
( Àúfv ‚àí x) ‚àº N (0, œÉ)
( Àúfv ‚àí y) ‚àº N (0, œÉ)
We Ô¨Åt Pr [x] with the Power-Law distribution and call the
method Power. Using this method requires knowledge and/or
assumption of the distribution to be estimated. If there are
too much noise, or the underlying distribution is different,
forcing the observations to Ô¨Åt a distribution could lead to
poor accuracy. Moreover, this method does not ensure the
frequencies sum up to 1, as Equation (11) only considers the
frequency of each value v independently. To make the result
consistent, we use Norm-Sub to post-process results of Power,
since Power is close to CLS, and Norm-Sub is the solution to
CLS. We call it PowerNS.
‚Ä¢ PowerNS: First use standard FO, then use Power to recover
the values, Ô¨Ånally use Norm-Sub to further process the
results.
F. Summary of Methods
In summary, Norm-Sub is the solution to the Constraint
Least Square (CLS) formulation to the problem. Furthermore,
when the fv component
in variance is dominated by the
other component (as in Equation (5)), the CLS formulation
is equivalent to our MLE formulation. In that case, Norm-Sub
is equivalent to MLE-Apx.
v1
‚â§ f(cid:48)
Table I gives a summary of the methods. First of all, all
of the methods preserve the frequency order of the value,
v2 iff Àúfv1 ‚â§ Àúfv2. The methods can be classiÔ¨Åes
i.e., f(cid:48)
into three classes: First, enforcing non-negativity only. Base-
Pos, Post-Pos, Base-Cut, and Power fall
in this category.
Second, enforcing summing-to-one only. Only Norm is in this
class. Third, enforcing the two requirement simultaneously.
Norm-Mul, Norm-Cut, Norm-Sub, and PowerNS satisfy both
requirements.
V. EVALUATION
As we are optimizing multiple utility metrics together, it
is hard to theoretically compare different methods. In this
section, we run experiments to empirically evaluate these
methods.
At the high level, our evaluations show that different meth-
ods perform differently in different settings, and to achieve
the best utility, it may or may not be necessary to exploit all
the consistency constraints. As a result, we conclude that for
full-domain query, Base-Cut performs the best; for set-value
query, PowerNS performs the best; and for high-frequency-
value query, Norm performs the best.
A. Experimental Setup
Datasets. We run experiments on two datasets (one synthetic
and one real).
‚Ä¢ Synthetic Zipf‚Äôs distribution with 1024 values and 1
million reports. We use s = 1.5 in this distribution.
‚Ä¢ Emoji: The daily emoji usage data. We use the average
emoji usage of an emoji keyboard 1, which gives the total
count of n = 884427 with d = 1573 different emojis.
Setup. The FO protocols and post-processing algorithms
are implemented in Python 3.6.6 using Numpy 1.15; and
all the experiments are conducted on a PC with Intel Core
i7-4790 3.60GHz and 16GB memory. Although the post-
processing methods can be applied to any FO protocol, we
1http://www.emojistats.org/, accessed 12/15/2019 10pm ET
7
(a) Base (Post-Pos)
(b) Base-Pos
(c) Base-Cut
(d) Norm
(e) Norm-Mul
(f) Norm-Cut
(g) Norm-Sub
(h) Power
(i) PowerNS
Fig. 1. Log-scale distribution of the Zipf‚Äôs dataset Ô¨Åxing  = 1, the x-axes indicates the sorted value index and the y-axes is its count. The blue line is the
ground truth; the green dots are estimations by different methods.
focus on simulating OLH as it provides near-optimal utility
with reasonable communication bandwidth.
Metrics. We evaluate three scenarios 1) estimate the fre-
quency of every value in the domain (full-domain), 2) estimate
the aggregate frequencies of a subset of values (set-value),
and 3) estimate the frequencies of the most frequent values
(frequent-value).
We use the metrics of Mean of Squared Error (MSE). MSE
measures the mean of squared difference between the estimate
and the ground truth for each (set of) value. For full-domain,
we compute
(cid:88)
v‚ààD
MSE =
1
d
(fv ‚àí f(cid:48)
v)2.
For frequent-value, we consider the top k values with highest
fv instead of the whole domain D; and for set-value, instead
of measuring errors for singletons, we measure errors for sets,
that is, we Ô¨Årst sum the frequencies for a set of values, and
then measure the difference.
Plotting Convention. Unless otherwise speciÔ¨Åed, for each
dataset and each method, we repeat the experiment 30 times,
with result mean and standard deviation reported. The standard
deviation is typically very small, and barely noticeable in the
Ô¨Ågures.
Because there are 11 algorithms (10 post-processing meth-
ods plus Base), and for any single metric there are often
multiple methods that perform very similarly, resulting their
lines overlapping. To make Figures 4‚Äì8 readable, we plot
8
results on two separate Ô¨Ågures on the same row. On the left,
we plot 6 methods, Base, Base-Pos, Post-Pos, Norm, Norm-
Mul, and Norm-Sub. On the right, we plot Norm-Sub with the
remaining 5 methods, MLE-Apx, Base-Cut, Norm-Cut, Power
and PowerNS. We mainly want to compare the methods in the
right column.
B. Bias-variance Evaluation
Figure 1 shows the true distribution of the synthetic Zipf‚Äôs
dataset and the mean of the estimations. As we plot the count
estimations (instead of frequency estimations), the variance is
larger (a n2 = 1012 multiplicative factor than the frequency
estimations). We thus estimate 5000 times in order to make the
mean stabilize. In Figure 2, we subtract the estimation mean by
the ground truth and plot the difference, which representing
the empirical bias. It can be seen that Base and Norm are
unbiased. Base-Pos introduces systematic positive bias. Base-
Cut gives unbiased estimations for the Ô¨Årst few most frequent
values, as their true frequencies are much greater than the
threshold T used to cut off estimation below it to 0. As the
noise is close to normal distribution, the possibility that a high-
frequency value is estimated to be below T is exponentially
small. The similar analysis also holds for the low-frequency
values, whose estimates are unlikely to be above T . On the
other hand, for values in between, the two biases compete with
each other. At some point, the two effects cancel out with
each other, leading to unbiased estimations. But this point is
dependent on the whole distribution, and thus is hard to be
found analytically. For Norm-Cut, the similar reasoning also











































