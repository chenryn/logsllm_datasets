pair we consider in our evaluation, we measure the end-to-end
protocol execution time and the total amount of communication.
Comparisons with prior work. We compare the performance
of CRYPTGPU against FALCON [6] and CRYPTFLOW [5]. To
our knowledge, these are the only privacy-preserving machine-
learning frameworks that have demonstrated the ability to
handle neural networks at the scale of AlexNet on large datasets.
Since our primary focus is on the scalability of our approach
and not on the performance on shallow networks (where GPUs
are unlikely to shine compared to optimized CPU protocols),
we focus our comparisons with FALCON and CRYPTFLOW.
• For CRYPTFLOW (which supports private inference for
ResNet), we use the performance numbers reported in their
paper (which also operate in a LAN environment).
• For FALCON (which supports private inference and private
training for LeNet, AlexNet, and VGG-16), we collect bench-
marks using their provided reference implementation [43].
We run the FALCON system on three compute-optimized
AWS instances (c4.8xlarge) in the Northern Virginia
region.3 Each instance runs Ubuntu 18.4 and has 36 Xeon
E5-2666 v3 (2.9 GHz) CPUs and 60 GB of RAM. We
measure the network bandwidth between machines to be
1.16GB/s with an average latency of 0.2ms.
For the main benchmarks, we also measure the computational
cost using PyTorch on plaintext data (with GPU acceleration).
Private inference. Table I summarizes the performance of
CRYPTGPU’s private inference protocol on the models and
datasets described in Section IV-A. For shallow networks and
small datasets (e.g., LeNet on MNIST or AlexNet on CIFAR),
FALCON outperforms CRYPTGPU. However, as we scale to
progressively larger datasets and deeper models (e.g., VGG-
16 on Tiny ImageNet), then CRYPTGPU is faster (3.7× on
VGG-16). The performance on small datasets is not unexpected;
after all, if the computation is sufﬁciently simple, then the
extra parallelism provided by the GPU is unlikely to beneﬁt.
Moreover, the use of more efﬁcient cryptographic building
blocks (which may not be “GPU-friendly”) can allow a CPU-
based approach to enjoy superior performance.
The setting where we would expect the GPU-based approach
to perform well is in the setting of large datasets and deeper
models. For instance, at the scale of ImageNet, CRYPTGPU is
able to perform private inference over the ResNet-152 network
(containing over 150 layers and over 60 million parameters)
in just over 25 seconds. This is about 2.2× faster than
CRYPTFLOW, which to our knowledge, is the only protocol for
private inference that has demonstrated support for the ResNet
family of networks on the ImageNet dataset. For the ResNet
family of networks, the running time of CRYPTGPU scales
linearly with the depth of the network.
Compared to plaintext inference on the GPU, there still
remains a signiﬁcant 1000× gap in performance. This un-
derscores the importance of designing more GPU-friendly
cryptographic primitives to bridge this gap in performance.
Batch private inference. We can also leverage GPU paral-
lelism to process a batch of images. This allows us to amortize
3Note that we use different instances for our comparison because CRYPTGPU
is GPU-based while FALCON is CPU-based.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
81028
LeNet (MNIST)
AlexNet (CIFAR)
VGG-16 (CIFAR)
AlexNet (TI)
VGG-16 (TI)
FALCON
CRYPTGPU
Plaintext
Time
0.038
0.38
0.0007
Comm. (MB)
2.29
3.00
—
Time
0.11
0.91
0.0012
Comm. (MB)
4.02
2.43
—
AlexNet (ImageNet)
VGG (ImageNet)
CRYPTFLOW
CRYPTGPU
Plaintext
Time
—
1.52
0.0013
Comm. (GB)
—
0.24
—
Time
—
9.44
0.0024
Comm. (GB)
—
2.75
—
Time
1.44
2.14
Comm. (MB)
40.45
56.2
Time
0.34
0.95
Comm. (MB)
16.23
13.97
0.0024
ResNet-50 (ImageNet)
—
0.0012
ResNet-101 (ImageNet)
—
Time
25.9
9.31
0.011
Comm. (GB)
6.9
3.08
—
Time
40*
17.62
0.021
Comm. (GB)
10.5*
4.64
—
Time
8.61
2.30
Comm. (MB)
161.71
224.5
0.0024
ResNet-152 (ImageNet)
—
Time
60*
25.77
0.031
Comm. (GB)
14.5*
6.56
—
*Value estimated from [5, Fig. 10]
TABLE I: Running time (in seconds) and total communication of private inference for different models, datasets, and systems
in a LAN setting. The “TI” dataset refers to the Tiny ImageNet dataset [15]. The plaintext measurements correspond to the
cost of inference on plaintext data on the GPU. Performance numbers for CRYPTFLOW are taken from [5]. Performance
numbers for FALCON are obtained by running its reference implementation on three compute-optimized AWS instances in a
LAN environment (see Section IV-B). As discussed in Section IV-A, when testing the performance of CRYPTGPU, we replace
max pooling with average pooling in all of the networks.
k = 1
k = 64
AlexNet
VGG-16
Time
0.91
2.14
Comm.
0.002
0.056
Time
1.09
11.76
Comm.
0.16
3.60
TABLE II: Running time (in seconds) and total communication
(in GB) for batch private inference on CIFAR-10 using a batch
size of k.
k = 1
k = 8
Time
9.31
17.62
25.77
Comm.
3.08
4.64
6.56
Time
42.99
72.99
105.20
Comm.
24.7
37.2
52.5
ResNet-50
ResNet-101
ResNet-152
TABLE III: Running time (in seconds) and total communication
(in GB) for batch private inference on ImageNet using a batch
size of k.
the cost of private inference. Table II shows the time and
communication needed for private inference over a batch of
64 images on the CIFAR-10 dataset. Here, the amortized cost
of private inference on a single image using AlexNet drops
from 0.91s to 0.017s (a 53× reduction). With VGG-16, batch
processing reduces the per-image cost from 2.14s to 0.18s (a
12× reduction).
Table III shows the time and communication needed for
private inference on ImageNet using the ResNet networks
with a batch of 8 images. Here, we see a 1.9× reduction in
the amortized per-image private inference cost for each of
ResNet-50, ResNet-101, and ResNet-152. The cost reduction
compared to those on the CIFAR-10 dataset (Table II) is smaller.
This is likely due to the smaller batch sizes in play here (8 vs.
64). Supporting larger batch sizes is possible by either using
multiple GPUs or using GPUs with more available memory.
Nonetheless, irrespective of the model/input size, we observe
that batch private inference allows us to amortize the cost of
private inference protocol. Communication in all cases scales
linearly with the batch size.
Private training. We expect GPUs to have a larger advantage
in the setting of private training (just like modern deep learning,
training is much more challenging than inference and thus, more
reliant on hardware acceleration). We measure the time needed
for a single iteration of private backpropagation (Appendix A)
on a batch size of 128 images for several dataset/model
conﬁgurations and summarize our results in Table IV (together
with measurements for the equivalent plaintext protocol). We
only compare with FALCON because CRYPTFLOW does not
support private training. We note that the public implementation
of the FALCON system [43] does not include support for
computing the cross-entropy loss function for backpropagation.
However, given the gradients for the output layer, the provided
implementation supports gradient computation for intermediate
layers. Thus, our measurements for the FALCON system only
includes the cost of computing the gradients for intermediate
layers and not for the output layer; this provides a lower bound
on the running time of using FALCON for private training. Our
system supports the full backpropagation training algorithm.
Our system achieves a considerable speedup over FALCON
in multiple settings, especially over larger models and datasets.
For instance, to train AlexNet on Tiny ImageNet, a single
iteration of (private) backpropagation completes in 11.30s with
CRYPTGPU and 6.9 minutes using FALCON. For context, pri-
vately training AlexNet on Tiny ImageNet (100,000 examples)
would just take over a week (≈ 10 days) using CRYPTGPU
while it would take over a year (≈ 375 days) using FALCON
(assuming 100 epochs over the training set).
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
91029
LeNet (MNIST)
Time
Comm.
AlexNet (CIFAR-10)
Time
Comm.
FALCON*
CRYPTGPU
Plaintext
14.90
2.21
0.0025
0.346
1.14
—
62.37
2.91
0.0049
0.621
1.37
—
VGG-16 (CIFAR-10)
Time
360.83†
12.14†
0.0089
Comm.
1.78†
7.55†
—
AlexNet (TI)
Time
Comm.
415.67
11.30
0.0099
2.35
6.98
—
VGG-16 (TI)
Time
359.60‡
13.89‡
0.0086
Comm.
1.78‡
7.59‡
—
*The provided implementation of FALCON does not support computing the gradients for the output layer, so the FALCON measurements only include the time
for computing the gradients for intermediate layers. All measurements for FALCON are taken without batch normalization.
†Using a smaller batch size of 32 images per iteration (due to GPU memory limitations). We make the same batch size adjustment for FALCON.
‡Using a smaller batch size of 8 images per iteration (due to GPU memory limitations). We make the same batch size adjustment for FALCON.
TABLE IV: Running time (in seconds) and total communication (in GB) for a single iteration of private training with a batch
size of 128 images for different models, datasets, and systems in a LAN setting. The “TI” dataset refers to the Tiny ImageNet
dataset [15]. The plaintext measurements correspond to the cost of training on plaintext data on the GPU. Performance numbers
for FALCON are obtained by running its reference implementation [43] on three compute-optimized AWS instances in a LAN
environment (see Section IV-B). As discussed in Section IV-A, when testing the performance of CRYPTGPU, we replace max
pooling with average pooling in all of the networks.
On the larger VGG-16 network, our system is constrained