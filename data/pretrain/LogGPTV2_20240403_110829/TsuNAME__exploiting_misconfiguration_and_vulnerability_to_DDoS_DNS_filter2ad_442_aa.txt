# Title: TsuNAME: Exploiting Misconfiguration and Vulnerability to DDoS DNS

## Authors:
- Giovane C. M. Moura (SIDN Labs)
- Sebastian Castro (InternetNZ)
- John S. Heidemann (USC/ISI)
- Wes Hardaker (USC/ISI)

## Abstract
The Domain Name System (DNS) is a critical component of the Internet, facilitating every web request and email exchange. DNS failures can lead to catastrophic outages, affecting major websites and services. This paper introduces TsuNAME, a vulnerability in which some recursive resolvers can significantly amplify queries, potentially resulting in a denial-of-service (DoS) attack on DNS services. TsuNAME is caused by cyclical dependencies in DNS records, leading to repeated queries that, combined with insufficient caching and application-level retries, can overwhelm authoritative servers. Although cyclic dependencies have been known, the scale of amplification has not been fully understood. We document real-world events in .nz (a country-level domain), where two misconfigured domains resulted in a 50% increase in overall traffic. Through experiments, we demonstrate an amplification factor of up to 500×. In response to our disclosure, several DNS software vendors, including Google Public DNS and Cisco OpenDNS, have implemented mitigations. To help operators of authoritative DNS services, we developed CycleHunter, an open-source tool that detects cyclic dependencies and prevents attacks. Using CycleHunter, we evaluated approximately 184 million domain names across 7 large top-level domains (TLDs), identifying 44 cyclic dependent NS records used by 1,400 domain names. The TsuNAME vulnerability is weaponizable, as an adversary can create cycles to attack the infrastructure of parent domains. Documenting this threat and its solutions is crucial for ensuring it is fully addressed.

## ACM Reference Format:
Giovane C. M. Moura, Sebastian Castro, John S. Heidemann, and Wes Hardaker. 2021. TsuNAME: exploiting misconfiguration and vulnerability to DDoS DNS. In ACM Internet Measurement Conference (IMC '21), November 2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 21 pages. https://doi.org/10.1145/3487552.3487824

## 1. Introduction
The Internet’s Domain Name System (DNS) [27] provides one of the core services of the Internet by mapping hostnames, applications, and services to IP addresses and other information. Every web page visit requires a series of DNS queries, and large DNS failures can cause severe disruptions, even to major websites and other Internet infrastructure. For example, the October 2016 DoS attack against Dyn [5] made many prominent websites such as Twitter, Spotify, and Netflix unreachable to many customers [40]. Another DoS attack against Amazon’s DNS service in October 2019 affected a large number of services [61].

The DNS can be seen as a hierarchical and distributed database, where DNS records are stored in and distributed from authoritative servers. All information about an end domain name in the DNS is served by authoritative servers for that domain. This information is typically retrieved by recursive resolvers, which answer questions originally posed by users and their applications. Resolvers are usually operated by a user’s ISP or public DNS resolvers like Google [15], Cloudflare [1], Quad9 [43], Cisco OpenDNS [38], and others.

The configuration of authoritative servers and their records is prone to several types of errors [2, 25, 27, 39, 55]. Here, we focus on loops where records required for resolution point at each other. Cyclic dependencies occur when resolving a name requires the resolution of another name, which in turn refers back to the first [39]. Loops can involve CNAMEs (§ 3.6.2, [27]) or NS records (§ 2, [25]). For example, if the NS record for example.org points to example.com and vice versa, then any attempt to resolve a name within either domain will fail because the IP address for both servers cannot be confirmed.

The first contribution of this paper is to report that, in the wild, cyclic dependencies can result in a query cascade that greatly increases traffic to authoritative servers. We call this amplification TsuNAME (inspired by the destructive potential of a tsunami) and describe the factors contributing to it in §2. TsuNAME amplification has occurred multiple times in the real world. §3 shows an event on February 1, 2020, in .nz, where a configuration error (not an intentional attack) in two domains, each having cyclic nameservers (NS records), resulted in a 50% increase in aggregate traffic volume (from 800M to 1.2B daily queries, as shown in the shaded area in Figure 1). While these servers handled the increased load, the large amplification highlights the risk a malicious attack could pose. Other TLDs have experienced even greater increases: §6 shows a European ccTLD that experienced a 10× increase in traffic due to TsuNAME.

These accidental events raise the question of what a motivated attacker could do to exploit this problem. An intentional attack could leverage multiple cycles to amplify moderate client traffic to overwhelm authoritative servers. Additionally, since DNS providers often host multiple domains on shared infrastructure, other services could suffer collateral DoS damage (we discuss this threat model in Appendix F). This threat poses a significant concern for any domains and registration points, such as TLDs and ccTLDs, which often host domains providing essential services to their users, such as government websites, banking, and online shopping.

The potential of DNS cycles was documented in the initial specification in 1987 [27] and was called out as an implementation risk in 1993 [25]. Nevertheless, cyclic dependencies were noted as a continuing risk in 2004 [39], and they still cause huge traffic volumes even in 2020 (§3). Our contribution is not to document cyclic dependencies as a new problem but to show that their impact can be significant even today and that this impact can be amplified by modern recursive resolver architectures.

DNS cycles cause a problem when an end-user’s query results in excessive traffic to authoritative DNS servers. However, the amount of traffic depends on the interaction of several components of the DNS system:

1. The injection rate of client queries into the cycle.
2. Stub resolvers' query retries and parallel queries [62].
3. The number of independent caches in recursive resolver services [44, 53].
4. Whether recursive resolvers cache negative replies.
5. Whether recursive resolvers send additional requests for an unresolved pending request.
6. Recursive resolver limits on the number of queries made when responding to an incoming request.

We carefully examine components (3) to (6) in this paper, as they are easier to mitigate as part of the infrastructure.

### The Threat
The risk of TsuNAME is that it is "weaponizable." The authoritative servers of any zone with third-party registration are at risk. An adversary can register two or more domains, reconfigure them to create a cyclic dependency, and then inject client traffic from a botnet. TsuNAME causes recursive resolvers to amplify injected traffic; in some prior events, by more than 500×. Registering domains without cycles is easy, cheap, and benign. The adversary can then activate the attack by reconfiguring to create a cycle and triggering traffic from a botnet—none of these steps are visible to the authoritative operator until the attack takes effect.

Although we (§3) and others (§6) encountered the problem by accident, we want to prevent its intentional use. We expand on this threat in Appendix F.

### Recommendation
Our recommendation is the same as in 1987 [27] (and repeated in 1993 [25]): resolver implementations must "bound the amount of work... so a request can’t get into an infinite loop or start a chain reaction... even if someone has incorrectly configured some data."

We add one prescription and one recommendation to this guideline: all recursive resolvers must implement negative caching, so they do not repeatedly retry a cyclic question. Negative caching means recording (and replying) with error replies, replying immediately to a query from a new client rather than repeating the amplification. This design choice would have mitigated the problems we report on below and has been deployed by Google Public DNS (§4.5) and other vendors (§5.3).

We also recommend that operators of parallel resolution systems share caches between resolvers and strongly encourage recursive resolvers to avoid making duplicate requests for the same content when one request is still being resolved. (Authoritative servers typically already "cache" results in memory; additional caching does not reduce their network traffic.)

### Figure 1
Daily queries to .nz for all domains (top) and the cyclic dependent domains (bottom two lines).

### 2. The TsuNAME Problem
#### The Cause
The fundamental problem in TsuNAME is a cyclic dependency that amplifies traffic sent to authoritative servers. When two zones have name server (NS) records that point at each other, a recursive resolver trying to resolve a name in that zone will loop between the two, trying to break the cycle. These queries mean that a single query from a client can result in recursive resolvers placing many queries against the authoritative servers, potentially overwhelming their capacity. Amplification in TsuNAME is this end-to-end effect, with several contributing factors described below—a more complex process than traditional DNS amplification, where a short query directly creates a large reply [23].

We encourage zone operators to scan for cycles using our tool to detect accidental cycles (§5).

While cycles were recognized as a problem more than 30 years ago, the problem is pressing today because the evolution of the system has amplified query rates: the "happy eyeballs" algorithm doubles queries [62], and large public resolvers often use parallelism across many machines without shared caches [44]. Recommendations from 1987 and 1993 emphasize the role of the single recursive resolver without considering the interactions in today’s DNS ecosystem.

### 3. TsuNAME's Impact in .nz
On February 1, 2020, two domains in .nz (DomainA and DomainB) had their NS records misconfigured to be cyclically dependent. DomainA’s NS records were set to ns[1,2].DomainB.nz, while DomainB’s NS records pointed to ns[1,2].DomainA.nz. This misconfiguration increased the query volume to .nz’s authoritative servers by 50%. Figure 1 shows this problem, as measured at the authoritative servers of .nz. The .nz operators manually fixed this misconfiguration more than two weeks later on February 17, 2020, returning query volumes to normal.

We describe this problem here, reproduce it in §4, and discuss detection in §5. While this traffic increase was large, .nz servers are overprovisioned and handle other bursts (such as those on February 20 and 23 in Figure 1), but a malicious attack could create much greater traffic volumes using more domains and more initial traffic.

#### 3.1 Query Sources
During the sixteen-day period of this TsuNAME event (February 1–17, 2020), there were 4.07 billion combined queries for DomainA and DomainB, with a daily average of 269 million. Figure 2a and Table 1 show the top 10 ASes by query volume during the event period. The overwhelming majority (99.99%) of all traffic originated from Google (AS15169), with only 324,000 queries originating from 579 other ASes. Queries from Google outnumbered the other sources by four orders of magnitude.

For comparison, Figure 2b shows the top 10 ASes for both domains during the "normal" periods when there was no cyclic dependency, spanning over the 16 days before and after the TsuNAME period (January 24–30 and February 18–28, 2020). During this "normal" period, Google sent no more than 100,000 daily queries for both DomainA and DomainB. During the TsuNAME period, however, Google’s query volume multiplied 5,453× (Figure 2c). No other AS had a traffic growth greater than 100× in the same period.

#### 3.2 Scrutinizing Google Queries
To understand why Google was responsible for so many queries, we consider: How long and how many times should resolvers retry when resolving domains with cyclic dependencies? And how aggressive should they be when finding answers?

Previous research has shown that resolvers will hammer unresponsive authoritative servers [34], generating up to 8× typical query rates, depending on the DNS records’ time-to-live (TTL) value. But in the case of cyclic dependencies, authoritative servers are responsive, and resolvers bounce from one authoritative server to another, asking the same sequence of questions repeatedly. This difference allows recursive resolvers to generate even more traffic than with unresponsive servers.

Given that Google was responsible for virtually all queries during the .nz TsuNAME event for the cyclic dependent domains (§3.1), we isolate and study the queries from Google. Table 1 shows the breakdown of the query names and types from Google during the .nz event. We see that most queries to .nz are for A and AAAA records for each of the two domain’s own NS records (NS records store the authoritative server names of a domain, while A [27] and AAAA records [57] store each server’s IPv4 and IPv6 addresses, respectively). With a cyclic dependency, these queries can never be resolved, as each authoritative server refers resolvers to the other. The NS records for the zones, however, were readily available within the parent .nz zone, which explains the lower volume of queries compared to the A/AAAA records.

#### 3.2.1 Interquery Interval
How frequently did GDNS resolvers send .nz queries for these domains during the TsuNAME event? We estimate this query rate by measuring the inter-query interval for queries with the same query name and type from GDNS to the .nz authoritative servers on one day (February 6, 2020).

Figure 3 shows the results (for space constraints, we show only results for the queries highlighted in the green rows of Table 1).

| Query Name | Query Type | Queries (v4) | Queries (v6) |
|------------|------------|--------------|--------------|
| ns1.DomainA.nz | A | 10.9M | 3.0M |
| ns1.DomainA.nz | AAAA | 281.3M | 281.4M |
| ns1.DomainA.nz | A | 281.2M | 281.4M |
| ns1.DomainA.nz | AAAA | 237.9M | 237.7M |
| ns1.DomainA.nz | A | 237.7M | 237.5M |
| ns1.DomainA.nz | AAAA | 13.0M | 4.3M |
| ns1.DomainA.nz | A | 266.1M | 266.2M |
| ns1.DomainA.nz | AAAA | 266.1M | 266.1M |
| ns1.DomainA.nz | A | 222.6M | 222.5M |
| ns1.DomainA.nz | AAAA | 222.5M | 222.3M |

### Figures
- **Figure 1**: Daily queries to .nz for all domains (top) and the cyclic dependent domains (bottom two lines).
- **Figure 2**: Top 10 ASes querying for Domains A and B, for .nz. TsuNAME period: February 1–17, 2020; normal period: January 24–30 and February 18–28, 2020.
- **Figure 3**: Interquery intervals for queries from GDNS to .nz authoritative servers on February 6, 2020.

### Conclusion
This paper documents the TsuNAME vulnerability, which can be exploited to launch DDoS attacks on DNS services. By leveraging cyclic dependencies, attackers can significantly amplify traffic, overwhelming authoritative servers. We provide real-world examples, demonstrate the potential for large-scale amplification, and offer recommendations for mitigating this threat. The development of CycleHunter, an open-source tool for detecting cyclic dependencies, is a key step in preventing future attacks. By sharing our findings and working with DNS software vendors, we aim to ensure that the TsuNAME vulnerability is fully addressed.