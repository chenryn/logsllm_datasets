29th USENIX Security Symposium    2691
conﬁgurations, and at a sampling frequency of 16kHz. Each
replay conﬁguration is different with respect to recording de-
vice type, playback device type, and recording environment.
Recording environments include balconies, bedrooms, can-
teens, homes, and ofﬁces. 26 playback devices were used, in-
cluding 12 high-quality professional audio equipment such as
active studio monitors and studio headphones (e.g., Genelec
8020C and Behringer Truth B2030A). Such devices would
introduce much less acoustic distortion than smaller, in-built
loudspeakers. Nine playback devices were in-built speakers
from various smartphones, tablets, and laptops. 5 devices
were medium-quality, portable speakers (e.g., Creative A60
speakers). 25 recording devices were used, including 12 high-
quality recording devices such as studio-quality condenser
microphones or hand-held recorders (e.g., Zoom H6 recorder
with Behringer ECM8000 mic). There were 61 replay conﬁg-
urations used.
The ASVspoof dataset is partitioned into training set, de-
velopment set, and evaluation set (see Table 2). We trained
Void on the training and development sets, and tested Void’s
performance against the evaluation set, which is compliant
with the ASVspoof competition rules (see [9]).
Table 2: Description of ASVspoof 2017 dataset [8].
Partition
Training
Development
Evaluation
Total
# Speakers Live-human Replay
1,507
950
12,008
14,465
1,507
760
1,298
3,565
10
8
24
42
The training set and developing set combined consists of
2,267 live-human samples and 2,457 attack samples. The eval-
uation set consists of 1,298 live-human samples and 12,008
attack samples – this proportion of attack samples in the eval-
uation set is much larger (see Table 1).
7 Evaluation
7.1 Experiment setup
For evaluation, we used the two datasets described in Sec-
tion 6. As for the ﬁrst attack dataset that we collected, to
reduce any bias that might be associated with the process of
randomly splitting the datasets into training and testing sets,
we used 10 fold cross-validation: the training samples were
partitioned into 10 equal-sized sets with similar class distri-
butions. As for the ASVspoof dataset, we trained Void using
both the train and developing sets, and evaluated Void against
the evaluation set – this is how the competition measured the
performance of submitted solutions.
To measure the performance of Void, we rely on the stan-
dard speaker veriﬁcation metrics, which are “false accep-
tance rates” (FAR) and “false rejection rates” (FRR). The
four possible classiﬁcation decisions are presented in Table
3. “True acceptance” (TA) and “true rejection” (TR) refer
to correctly detecting live-human voice and loudspeaker, re-
spectively. “False acceptance” (FA) is when a loudspeaker
is mis-classiﬁed as live-human voice, and “false rejection”
(FR) is when live-human voice is mis-classiﬁed as loud-
speaker. We measure equal error rates (EERs), representing
error rates for which FAR and FRR are equal. Receiver op-
erating characteristic (ROC) curve and area under the curve
(AUC) were also used for comparison of various thresholds.
For computing EER, we used the Bosaris toolkit (https:
//sites.google.com/site/bosaristoolkit/) that was
suggested in the 2017 ASVspoof competition [7].
Table 3: Four possible classiﬁcation decisions.
Live-human
Replay attack
Accept
Reject
True Acceptance
False Rejection
False Acceptance True Rejection
Our experiments were conducted on a powerful server
equipped with two Intel Xeon E5 (2.10GHz) CPUs, 260GB
RAM and NVIDIA 1080Ti GPU, running 64-bit Ubuntu
16.04 LTS operating system. Our latency and model com-
plexity results were measured based on this server setting.
7.2 Optimal classiﬁcation method for Void
To determine the optimal classiﬁcation method, we ﬁrst eval-
uated the performance of ﬁve different classiﬁcation methods
that are popularly used in security systems: k-Nearest Neigh-
bor (kNN), Random forest, SVM with linear kernel (SVM
linear), and SVM with RBF kernel (SVM RBF). All of those
classiﬁers were tested using the ASVspoof dataset.
Table 4 shows the detection accuracy of four classiﬁcation
models (k-Nearest Neighbor (kNN), Random forest, SVM
with linear kernel (SVM linear), and SVM with RBF kernel
(SVM RBF)) for the ASVspoof dataset.
Table 4: Detection accuracy of four classiﬁcation algo-
rithms for the ASVspoof dataset [7].
Algorithm
SVM RBF
kNN
Random forest
SVM linear
EER (%)
11.6
23.4
15.8
19.1
Among classiﬁcation algorithms tested, SVM RBF pro-
duced the best EER results (11.6%) while providing training
and testing times comparable with other classiﬁcation algo-
rithms. Therefore, we recommend the use of SVM RBF. All
2692    29th USENIX Security Symposium
USENIX Association
subsequent evaluations were conducted using the SVM RBF
classiﬁer.
7.3 Attack detection accuracy
We show the ROC curve and AUC in Figure 8 to demonstrate
the classiﬁcation performance of Void under various threshold
settings. Void achieved an AUC of 0.99 and 0.94 for our
dataset and ASVspoof dataset, respectively. Even though the
live-human to replay attack sample ratios are low in both
datasets, the strong ROC curve and AUC results indicate
that Void would likely achieve low error rates when more
balanced datasets are used (see Figure 8). Void achieved an
EER of 0.3% and 11.6% for our dataset and ASVspoof dataset,
respectively1. We note that this EER result (11.6%) would
rank Void as the second best solution (EER 12.34%) in the
ASVspoof 2017 competition [10].
CQCC as the main features, and 512-component Gaus-
sian Mixture Model (GMM) as the classiﬁcation algorithm.
CQCC-GMM achieved 23% EER on the ASVspoof evalu-
ation set [7] – demonstrating signiﬁcantly larger EER com-
pared to Void.
STFT-LCNN. To evaluate the best performing model from
the ASVspoof competition, we implemented the Light Convo-
lutional Neural Network (LCNN) structure described in [30]
and used STFT as the main features – this is one of the
two deep learning models used. We contacted the authors
from [30] and used the exact LCNN hyper-parameters and
STFT parameters they recommended. Their model consists
of 5 convolutional layers, 4 network in network layers, 10
max-feature-map layers, 4 max-pooling layers, and 2 fully
connected layers as described in [30]. STFT-LCNN achieved
7.4% EER on the ASVspoof evaluation set [7] according to
the EER result presented in [30]2.
7.4 Latency and model complexity results
We compare Void against CQCC-GMM and STFT-LCNN
with respect to the latency and model complexity (see Table 5).
Feature extraction time (“Extraction”) represents the average
time taken to extract features from a single voice sample.
Training time (“Training”) refers to the time taken to train
a model (using the extracted features). Testing time refers
to the average time taken to extract features from a single
voice sample and perform classiﬁcation using those features.
Memory size, in megabytes, refers to the average memory
used by each model to classify a given sample.
As for the space complexity, we count the number of fea-
tures extracted from a single voice sample. The number of
features used by Void is just 97, compared to 14,020 features
used by our CQCC-GMM implementation and 84,770 fea-
tures used by STFT-LCNN. In consequence, Void only used
1.988 megabytes of memory on average to classify a given
voice sample. CQCC-GMM used 173.707 megabytes and
STFT-LCNN used 304.176 megabytes of memory.
As for the execution time overheads, on average, Void took
0.283 seconds for training, and 0.035 seconds for testing.
Void outperformed all other solutions with respect to both the
training time and testing time. The average testing time for
STFT-LCNN was 0.27 seconds.
These observations clearly indicate that Void is a much
more efﬁcient, faster, and lighter solution compared to other
solutions. Void is the only solution that would satisfy the
strict latency, and model and feature complexity requirements
described in Section 3.1.
2Although we used the same hyper-parameters and model layouts descried
in [30], our own implementation achieved 12.7% EER – higher than the 7.4%
EER presented in [30].
Figure 8: Accuracy results of Void.
To compare Void against existing solutions from the
ASVspoof competition with respect to latency, space com-
plexity, and accuracy, we implemented (used existing code if
available) the two classiﬁcation models described below, and
evaluated them using the ASVspoof evaluation set. Table 5
summarizes those evaluation results.
Table 5: Average training/testing times, number of fea-
tures used, average memory used, and performance of
classiﬁcation models on the ASVspoof dataset [7].
Measure
Void CQCC-GMM [7]
Time
Memory
Accuracy
Extraction (sec.)
Training (sec.)
Testing (sec.)
# Features
Memory size (MB)
EER
0.035
0.283
0.035
97
1.988
11.6%
0.059
6,599.428
0.062
14,020
173.707
23.0%
STFT-LCNN [30]
3e−4
15,362.448
0.270
84,770
304.176
7.4%
CQCC-GMM. This is the baseline approach recom-
mended in the 2017 ASVspoof competition [7] that uses
1We additionally provide precision, recall, and F1-score measures for our
dataset because the numbers for live-human samples and attack samples are
imbalanced; the precision, recall, and F1-score are 95.8%, 85.2%, and 89.2%,
respectively.
USENIX Association
29th USENIX Security Symposium    2693
False positive rate00.20.40.60.81True positive rate00.20.40.60.81Our datasetASVspoof 2017 datasetAUC: 0.94EER: 11.6%AUC: 0.99EER: 0.3%7.5 Using Void as an ensemble solution
Our discussions with several speech recognition engineers at a
large IT company revealed that ﬁlter bank and MFCC are the
only two spectral features used for speech recognition. Since
MFCC would be extracted and available anyway (and would
not require any additional feature extraction time), we imple-
mented an ensemble solution that consists of MFCC-GMM
and Void, and evaluated its accuracy against the ASVspoof
evaluation set. MFCC-GMM alone achieves 25.5% EER on
the evaluation set, and uses 8,053 features – it is much lighter
than CQCC. Its average testing time was around 0.03 seconds.
We used a logistic regression model to compute the opti-
mal weight for each model: 0.7 for Void, and 0.3 for MFCC-
GMM. This ensemble approach achieved an EER of 8.7%,
further demonstrating the effectiveness of Void and its poten-
tial beneﬁts when combined with other lightweight models.
Again, our ensemble solution would have ranked second in
the ASVspoof competition, and not too far from the best so-
lution that achieved an EER of 6.74%. The total testing time
would be around 0.06 seconds per voice sample.
7.6 Effects of variances
In this section, we analyze the effects of four key variances –
distances between target devices and attack devices, human
gender, loudspeaker types and cross data training – on the
performance of Void. We trained a single classiﬁer using our
own dataset; the train set comprised of 9,000 live-human sam-
ples and 9,000 replay attack samples. We used this classiﬁer
to evaluate Void’s performance under distance and gender
variances.
7.6.1 Attack source distances
To analyze the effects of varying distances between attacker
and target device, voice samples were collected using three
different distances: 15cm, 130cm, and 260cm. For testing,
we used the remaining replayed samples, randomly choosing
1,920, 1,919, 1,920 samples, respectively, from each of the
3 categories (15cm, 130cm, 260cm), and 1,209 live-human
samples. We did not experiment with distances that are too
far from target devices since attackers would have to use very
loud volumes, which would be easily noticed.
Table 6: Effects of variances on detection accuracy.
Distance
Diversity Dimension Test samples RoC Acc.(%)
99.6
99.7
99.9
98.9
98.9
15cm
130cm
260cm
Male
Female
Gender
1,920
1,919
1,920
1,940
2,062
0.99
0.99
0.99
0.99
0.99
Prec.(%) Rec.(%)
99.16
99.58
100
99.24
99.49
98.51
98.18
98.01
98.07
97.76
F1(%) EER(%)
98.93
0.72
0.85
98.87
0.15
98.99
0.69
98.66
98.62
0.97
Evaluation results are presented in Table 6. We show that
all F1 scores are greater than 98%, and all EERs are less
than 1%. For 15cm, Void achieved 99.6% attack detection
rate and an EER of 0.72%. For 130cm, Void achieved 99.7%
attack detection rate and an EER of 0.85%. For 260cm, Void
achieved 99.9% attack detection rate and an EER of 0.15%.
Those results demonstrate that distance variations have mini-
mal impact on the performance of Void.
7.6.2 Gender
Since female voices have typically higher fundamental fre-
quencies than male voices [21, 22], the power distribution
patterns may also vary between males and females. To an-
alyze the effects of changing gender, we tested Void sepa-
rately on (1) 1,940 male live-human voice and attack samples,
and (2) 2,062 female live-human voice and attack samples.
We selected attack samples that were replayed using the V-
MODA speaker with 15cm recording distance. Ten fold cross-
validation was used to evaluate Void classiﬁers.
Again, gender variances did not really inﬂuence Void’s
performance (see Table 6): accuracy and F1 scores are greater