### 29th USENIX Security Symposium

#### Audio Replay Configurations and Sampling
The audio replay configurations were sampled at a frequency of 16 kHz. Each configuration varied in terms of the recording device type, playback device type, and recording environment. The environments included balconies, bedrooms, canteens, homes, and offices. A total of 26 playback devices were used, including 12 high-quality professional audio equipment such as active studio monitors and studio headphones (e.g., Genelec 8020C and Behringer Truth B2030A). These devices introduced significantly less acoustic distortion compared to smaller, built-in loudspeakers. Nine playback devices were built-in speakers from various smartphones, tablets, and laptops, while five were medium-quality portable speakers (e.g., Creative A60 speakers). Additionally, 25 recording devices were utilized, including 12 high-quality recording devices such as studio-quality condenser microphones or hand-held recorders (e.g., Zoom H6 recorder with Behringer ECM8000 mic). In total, 61 different replay configurations were employed.

#### ASVspoof Dataset Partitioning
The ASVspoof dataset is divided into three parts: a training set, a development set, and an evaluation set (see Table 2). We trained Void on the combined training and development sets and tested its performance against the evaluation set, adhering to the ASVspoof competition rules (see [9]).

**Table 2: Description of ASVspoof 2017 Dataset [8]**
| Partition   | Training | Development | Evaluation | Total |
|-------------|----------|-------------|------------|-------|
| # Speakers  | 1,507    | 1,507       | 10         | 3,024 |
| Live-human | 1,507    | 1,507       | 10         | 3,024 |
| Replay     | 950      | 760         | 8          | 1,718 |

- **Training and Development Sets**: Combined, these sets include 2,267 live-human samples and 2,457 attack samples.
- **Evaluation Set**: This set includes 1,298 live-human samples and 12,008 attack samples, reflecting a much higher proportion of attack samples.

### Evaluation

#### Experiment Setup
For evaluation, we used two datasets described in Section 6. To minimize bias associated with random splitting, we employed 10-fold cross-validation for the first attack dataset. For the ASVspoof dataset, we trained Void using both the training and development sets and evaluated it against the evaluation set, following the competition's methodology.

#### Performance Metrics
We measured Void's performance using standard speaker verification metrics: false acceptance rate (FAR) and false rejection rate (FRR). The four possible classification decisions are summarized in Table 3.

**Table 3: Four Possible Classification Decisions**
| Actual        | Predicted    | Decision                |
|---------------|--------------|-------------------------|
| Live-human   | Accept       | True Acceptance (TA)    |
| Live-human   | Reject       | False Rejection (FR)    |
| Replay Attack| Accept       | False Acceptance (FA)   |
| Replay Attack| Reject       | True Rejection (TR)     |

We also calculated equal error rates (EERs), representing the error rates where FAR and FRR are equal. Receiver operating characteristic (ROC) curves and area under the curve (AUC) were used for threshold comparisons. EER was computed using the Bosaris toolkit (https://sites.google.com/site/bosaristoolkit/), as suggested in the 2017 ASVspoof competition [7].

#### Experimental Environment
Experiments were conducted on a server equipped with two Intel Xeon E5 (2.10GHz) CPUs, 260GB RAM, and an NVIDIA 1080Ti GPU, running 64-bit Ubuntu 16.04 LTS. Latency and model complexity results were based on this setup.

#### Optimal Classification Method for Void
We evaluated five popular classification methods: k-Nearest Neighbor (kNN), Random Forest, SVM with linear kernel (SVM Linear), and SVM with RBF kernel (SVM RBF). All classifiers were tested using the ASVspoof dataset.

**Table 4: Detection Accuracy of Four Classification Algorithms for the ASVspoof Dataset [7]**
| Algorithm    | EER (%) |
|--------------|---------|
| SVM RBF     | 11.6    |
| kNN         | 23.4    |
| Random Forest| 15.8   |
| SVM Linear  | 19.1    |

SVM RBF produced the best EER results (11.6%) with comparable training and testing times. Therefore, we recommend using SVM RBF for subsequent evaluations.

#### Attack Detection Accuracy
The ROC curve and AUC for Void under various threshold settings are shown in Figure 8. Void achieved AUCs of 0.99 and 0.94 for our dataset and the ASVspoof dataset, respectively. Despite the low live-human to replay attack sample ratios, the strong ROC and AUC results indicate that Void would likely achieve low error rates with more balanced datasets. Void achieved EERs of 0.3% and 11.6% for our dataset and the ASVspoof dataset, respectively. This EER result (11.6%) would rank Void as the second-best solution (EER 12.34%) in the ASVspoof 2017 competition [10].

**Figure 8: Accuracy Results of Void**

#### Comparison with Existing Solutions
To compare Void with existing solutions, we implemented and evaluated the following models using the ASVspoof evaluation set:

- **CQCC-GMM**: Achieved 23% EER on the ASVspoof evaluation set [7].
- **STFT-LCNN**: Achieved 7.4% EER on the ASVspoof evaluation set [30].

**Table 5: Average Training/Testing Times, Number of Features Used, Average Memory Used, and Performance of Classification Models on the ASVspoof Dataset [7]**
| Measure      | Void       | CQCC-GMM [7] | STFT-LCNN [30] |
|--------------|------------|--------------|-----------------|
| Extraction (sec.) | 0.035    | 0.059        | 3eâˆ’4            |
| Training (sec.)   | 0.283    | 6,599.428    | 15,362.448      |
| Testing (sec.)    | 0.035    | 0.062        | 0.270           |
| # Features       | 97        | 14,020       | 84,770          |
| Memory size (MB) | 1.988    | 173.707      | 304.176         |
| EER             | 11.6%     | 23.0%        | 7.4%            |

#### Using Void as an Ensemble Solution
In discussions with speech recognition engineers, we found that filter bank and MFCC are the primary spectral features used. Since MFCC extraction is already available, we implemented an ensemble solution combining MFCC-GMM and Void. This ensemble achieved an EER of 8.7%, demonstrating the effectiveness of Void when combined with other lightweight models. The total testing time was around 0.06 seconds per voice sample.

#### Effects of Variances
We analyzed the effects of four key variances: distances between target and attack devices, human gender, loudspeaker types, and cross-data training. We trained a single classifier using our own dataset, which included 9,000 live-human samples and 9,000 replay attack samples.

**7.6.1 Attack Source Distances**
Voice samples were collected at three different distances: 15 cm, 130 cm, and 260 cm. For testing, we used 1,920, 1,919, and 1,920 samples, respectively, from each distance category, along with 1,209 live-human samples.

**Table 6: Effects of Variances on Detection Accuracy**
| Distance (cm) | Test Samples | ROC | Acc. (%) | Prec. (%) | Rec. (%) | F1 (%) | EER (%) |
|---------------|--------------|-----|----------|-----------|----------|--------|---------|
| 15            | 1,920        | 0.99| 99.6     | 99.16     | 98.51    | 98.93  | 0.72    |
| 130           | 1,919        | 0.99| 99.7     | 99.58     | 98.18    | 98.87  | 0.85    |
| 260           | 1,920        | 0.99| 99.9     | 100       | 98.01    | 98.99  | 0.15    |

**7.6.2 Gender**
We tested Void separately on male and female voice samples. Gender variations did not significantly influence Void's performance, with accuracy and F1 scores remaining high (see Table 6).

These results demonstrate that Void is a robust and efficient solution, capable of handling various variances and achieving high detection accuracy.