allel computation. Memory operations performed by
atomic instructions (namely, atomic memory operations)
are guaranteed to complete uninterrupted, because ac-
cesses to the affected memory regions by other proces-
sors or devices are temporarily blocked from execution.
4.2.2 Analysis
Atomic memory operations, by their design, generate
system-wide observable contentions in the target mem-
ory regions they operate on. And this particular feature
of atomic memory operations caught our attention. Ide-
ally, contention generated by an atomic memory oper-
ation is well bounded, and is only evident when the
affected memory region is accessed in parallel. Thus,
atomic memory operations are not exploitable for cross–
VM covert channels, because VMs normally do not im-
plicitly share physical memory. However, we have found
out that the hardware implementations of atomic mem-
ory operations do not match the idealistic speciﬁcation,
and memory contentions caused by atomic memory op-
erations could propagate much further than expected.
Early generations (before Pentium Pro) of x86 proces-
sors implement atomic memory operations by using bus
lock, a dedicated hardware signal that provides exclusive
access of the memory bus to the device who asserts it.
While providing a very convenient means to implement
atomic memory operations, the sledgehammer-like ap-
proach of locking the memory bus results in system-wide
memory contention. In addition to being exploitable
for covert channels, the bus-locking implementation of
atomic memory operations also causes performance and
scalability problems.
Modern generations (before Intel Nehalem and AMD
K8/K10) of x86 processors improve the implementa-
tion of atomic memory operations by signiﬁcantly re-
ducing the likelihood of memory bus locking. In par-
ticular, when an atomic operation is performed on a
memory region that can be entirely cached by a cache
line, which is a very common case, the corresponding
cache line is locked, instead of asserting the memory bus
lock [10]. However, on these platforms, atomic mem-
ory operations can still be exploited for covert chan-
nels, because the triggering conditions for bus-locking
are not eliminated. Speciﬁcally, when atomic opera-
tions are performed on memory regions with an exotic4
conﬁguration—unaligned addresses that span two cache
lines, atomicity cannot be ensured by cache line locking,
and bus lock signals are thus asserted.
Remarkable architecture evolutions have taken place
in the latest generations (Intel Nehalem and AMD
K8/K10) of x86 processors, one of which is the removal
of the shared memory bus. On these platforms, instead
of having a uniﬁed central memory storage for the entire
system, the main memory is divided into several pieces,
each assigned to a processor as its local storage. While
each processor has direct access to its local memory,
it can also access memory assigned to other processors
via a high-speed inter-processor link. This non-uniform
memory access (NUMA) design eliminates the bottle-
neck of a single shared memory bus, and thus greatly
improves processor and memory scalability. As a side
effect, the removal of the shared memory bus has seem-
ingly invalidated memory bus covert channel techniques
at their foundation. Interestingly, however, the exploit
of atomic memory operation continues to work on the
newer platforms, and the reason for this requires a bit
more in-depth explanation.
On the latest x86 platforms, normal atomic memory
operations (i.e., operating on memory regions that can be
4The word “exotic” here only means that it is very rare to encounter
such an unaligned memory access in modern programs, due to auto-
matic data ﬁeld alignments by the compilers. However, manually gen-
erating such an access pattern is very easy.
ϯϬϬϬ
ϮϱϬϬ
ϮϬϬϬ
ϭϱϬϬ
ϭϬϬϬ
ϱϬϬ
Ϳ
Ɛ
Ŷ
;

Ǉ
Đ
Ŷ
Ğ
ƚ
Ă
>

Ɛ
Ɛ
Ğ
Đ
Đ

Ϭ
Ϭ
Ϳ
Ɛ
Ŷ
;

Ǉ
Đ
Ŷ
Ğ
ƚ
Ă
>

Ɛ
Ɛ
Ğ
Đ
Đ

ϰϬϬϬ
ϯϱϬϬ
ϯϬϬϬ
ϮϱϬϬ
ϮϬϬϬ
ϭϱϬϬ
ϭϬϬϬ
ϱϬϬ
Ϭ
ϮϬϬ
ϰϬϬ
ϲϬϬ
ϴϬϬ
ϭϬϬϬ
Ϭ
ϮϬϬ
ϰϬϬ
ϲϬϬ
ϴϬϬ
ϭϬϬϬ
^ĞƋƵĞŶƚŝĂů^ĂŵƉůĞƐKǀĞƌdŝŵĞ;ɑȌ

^ĞƋƵĞŶƚŝĂů^ĂŵƉůĞƐKǀĞƌdŝŵĞ;ɑȌ


(a) Intel Core2, Hyper-V, Windows Guest VMs


(b) Intel Xeon (Nehalem), Xen, Linux Guest VMs

Figure 3: Timing-based Memory Bus Channel Bandwidth Tests
cached by a single cache line) are handled by the cache
line locking mechanism similar to that of the previous
generation processors. However, for exotic atomic mem-
ory operations (i.e., operating on cache-line-crossing
memory regions), because there is no shared memory bus
to lock, the atomicity is achieved by a set of much more
complex operations: all processors must coordinate and
completely ﬂush in-ﬂight memory transactions that are
previously issued. In a sense, exotic atomic memory op-
erations are handled on the newer platform by “emulat-
ing” the bus locking behavior of the older platforms. As
a result, the effect of memory access delay is still observ-
able, despite the absence of the shared memory bus.
4.2.3 Veriﬁcation
With the memory bus exploit, we can easily build a mem-
ory bus covert channel by adapting our timing-based
cache transmission scheme with minor modiﬁcations, as
shown in Algorithm 3.
Compared with Algorithm 2, there are only two dif-
ferences in the memory bus channel protocol. First, we
substitute the set of cache lines (CLines) with the mem-
ory bus as the transmission medium. Similar to the cache
lines, the memory bus can also be put in two states, con-
tended and contention-free, depending on whether ex-
otic atomic memory operations are performed. Second,
instead of trying to evict contents of the selected cache
lines, the sender changes the memory bus status by per-
forming exotic atomic memory operations. And corre-
spondingly, the receiver must make uncached memory
accesses to detect contentions.
We demonstrate the effectiveness of the memory bus
channel by performing bandwidth estimation experi-
ments, similar to the one in Section 4.1, on two sys-
tems running different generations of platforms, hyper-
visors and guest VMs. Speciﬁcally, the ﬁrst system uses
an older shared memory bus platform and runs Hyper-V
with Windows guest VMs, while the second system uti-
lizes the newer platform without a shared memory bus
and runs Xen with Linux guest VMs. As Figure 3 shows,
the x-value of each sample point is the observed mem-
ory access latency by the receiver, and the trend lines are
created by plotting the moving average of two samples.
According to the measurement results, on both systems,
39 bits can be transmitted over a period of 1 millisec-
ond, yielding a raw bandwidth of over 38 kilobits per
second. Although an order of magnitude lower in band-
width than our cache channel, the memory bus channel
enjoys its unique advantage of working across different
physical processors. And notably, the same covert chan-
nel implementation works on both systems, regardless of
the guest operating systems, hypervisors, and hardware
platform generations.
4.3 Whispering into the Hyper-space
We have demonstrated that the memory bus channel is
capable of achieving high speed data transmission on vir-
tualized systems. However, the preliminary protocol de-
scribed in Algorithm 3 is prone to errors and failures in a
realistic environment, because the memory bus is a very
noisy channel, especially on virtualized systems running
many non-participating workloads.
Figure 4 presents a realistic memory bus channel
sample, taken using a pair of physically co-resident
VMs in the Amazon EC2 cloud. From this ﬁgure, we
can observe that both the “contention free” and “con-
tended” signals are subject to frequent interferences. The
“contention free” signals are intermittently disrupted by
workloads of other non-participating VMs, causing the
memory access latency to moderately raise above the
baseline. In contrast, the “contended” signals experience
much heavier interferences, which originate from two
sources: scheduling and non-participating workloads.
The scheduling interference is responsible for the peri-
odic drop of memory access latency. In particular, con-
text switches temporarily de-schedule the sender process
from execution, and thereby brieﬂy relieving memory
bus contention. The non-participating workloads exe-
ϯϬϬϬ
ϮϱϬϬ
ϮϬϬϬ
ϭϱϬϬ
ϭϬϬϬ
ϱϬϬ
Ϳ
Ɛ
Ŷ
;

Ǉ
Đ
Ŷ
Ğ
ƚ
Ă
>

Ɛ
Ɛ
Ğ
Đ
Đ

Ϭ
ϭϲϮ
ϮƉĞƌ͘DŽǀ͘ǀŐ͘;ŽŶƚĞŶƚŝŽŶ&ƌĞĞͿ
ϮƉĞƌ͘DŽǀ͘ǀŐ͘;ŽŶƚĞŶĚĞĚͿ
ϭϳϮ
ϭϴϮ
ϭϵϮ
ϮϬϮ
^ĞƋƵĞŶƚŝĂů^ĂŵƉůĞƐKǀĞƌdŝŵĞ;Ȍ


Figure 4: Memory Bus Channel Quality Sample in EC2

cuted in parallel with the sender process worsen memory
bus contention and cause the spikes in the ﬁgure, while
non-participating workloads executed concurrently with
the sender process reduce memory bus contention, and
result in the dips in the ﬁgure. All these interferences can
degrade the signal quality in the channel, and make what
the receiver observes different from what the sender in-
tends to generate, which leads to bit-ﬂip errors.
Besides the observable interferences shown in Fig-
ure 4, there are also unobservable interferences, i.e., the
scheduling interferences to the receiver, which can cause
an entirely different phenomenon. When the receiver is
de-scheduled from execution, there is no observer in the
channel, and thus all data being sent is lost. And to
make matters worse, the receiver could not determine
the amount of information being lost, because the sender
may also be de-scheduled during that time. As a result,
the receiver suffers from random erasure errors.
Therefore, three important issues need to be addressed
by the communication protocol in order to ensure reli-
able cross–VM communication: receiving conﬁrmation,
clock synchronization, and error correction.
Receiving Conﬁrmation: The random erasure errors
can make the transmitted data very discontinuous, signif-
icantly reducing its usefulness. To alleviate this problem,
it is very important for the sender to be aware of whether
the data it sent out has been received.
We avoid using message based “send-and-ack”, a
commonly employed mechanism for solving this prob-
lem, since this mechanism requires the receiver to ac-
tively send data back to the sender, reversing the roles
of sending and receiving, and subjects the acknowledg-
ment sender (i.e., the data receiver) to the same problem.
Instead, we leverage the system-wide effect of memory
bus contention to achieve simultaneous data transmis-
sion and receiving conﬁrmation. Here the sender infers
the presence of receiver by observing increased memory
access latencies generated by the receiver.
The corresponding changes to the data transmission
protocol include:
1. Instead of making uncached memory accesses, the
receiver performs exotic atomic memory operations,
just like the sender transmitting a one bit.
2. Instead of sleeping when transmitting a zero bit, the
sender performs uncached memory accesses. In ad-
dition, the sender always times its memory accesses.
3. While the receiver is in execution, the sender should
always observe high memory access latencies; oth-
erwise, the sender can assume the data has been par-
tially lost, and retry at a later time.
Clock Synchronization: Since the sender and receiver
belong to two independent VMs, scheduling differences
between them tend to make the data transmission and
detection procedures de-synchronized, which can cause
a signiﬁcant problem to pure timing-based data mod-
ulation. We overcome clock de-synchronization by us-
ing self-clocking coding—a commonly used technique
in telecommunications. Here we choose to transmit data
bits using differential Manchester encoding, a standard
network coding scheme [28].
Error Correction: Even with self-clocking coding, bit-
ﬂip errors are expected to be common. Similar to re-
solving the receiving conﬁrmation problem, we again
avoid using acknowledgment-based mechanisms. As-
suming only a one-way communication channel, we re-
solve the error correction problems by applying forward
error correction (FEC) to the original data, before apply-
ing self-clocking coding. More speciﬁcally, we use the
Reed-Solomon coding [17], a widely applied block FEC
code with strong multi-bit error correction performance.