title:An Empirical Analysis of Target-Resident DoS Filters
author:Michael P. Collins and
Michael K. Reiter
An Empirical Analysis of Target-Resident DoS Filters
(Extended Abstract)
Michael Collins
∗
†
Michael K. Reiter
Abstract
Numerous techniques have been proposed by which an
end-system, subjected to a denial-of-service ﬂood, ﬁlters
the offending trafﬁc. In this paper, we provide an empirical
analysis of several such proposals, using trafﬁc recorded at
the border of a large network and including real DoS traf-
ﬁc. We focus our analysis on four ﬁltering techniques, two
based on the addresses from which the victim server typ-
ically receives trafﬁc (static clustering and network-aware
clustering), and two based on coarse indications of the path
each packet traverses (hop-count ﬁltering and path identi-
ﬁers). Our analysis reveals challenges facing the proposed
techniques in practice, and the implications of these is-
sues for effective ﬁltering. In addition, we compare tech-
niques on equal footing, by evaluating the performance of
one scheme under assumptions made by another. We con-
clude with an interpretation of the results and suggestions
for further analysis.
1. Introduction
Denial-of-service (DoS) attacks are among the most
prominent types of attack on the internet today, and those
that overwhelm a victim by ﬂooding it with large num-
bers of packets constitute a substantial fraction of all
DoS attacks. Today, such DoS attacks are conducted
by large-scale networked applications capable of ﬂood-
ing a victim from thousands of compromised computers.
In addition to the number of attacking computers, DoS at-
tacks vary on a number of axes (e.g., see [19, 11]). One
axis of interest here is whether trafﬁc is spoofed or not, i.e.,
∗ CERT Network Situational Awareness and Department of Electrical
and Computer Engineering, Carnegie Mellon University, Pittsburgh,
Pennsylvania, USA; mcollins@cert.org. CERT is part of the
Carnegie Mellon Software Engineering Institute, sponsored by the
U.S. Department of Defense.
† Department of Electrical and Computer Engineering, Department of
Computer Science, and CyLab, Carnegie Mellon University, Pitts-
burgh, Pennsylvania, USA; PI:EMAIL
whether the source addresses of attack packets bear the cor-
rect IP address of the attacking machine. Spoofed traf-
ﬁc hinders the ability to identify the originating hosts—a
problem for which “traceback” has been widely stud-
ied to address (e.g., [4, 25, 17, 1, 7, 26]). However,
spooﬁng is of less utility when the attack is launched in-
directly through compromised computers, since these
computers do not expose the location of the original at-
tacker, and can interfere with the attack trafﬁc reaching
its target due to increased deployment of ingress ﬁlter-
ing [9].
Given the inertia of the deployed network infrastructure,
several proposed DoS defenses have advocated a victim-
centered approach in which the victim host (or an ingress
router at the border of the victim’s network) ﬁlters DoS
trafﬁc, with no or small change to the networking infras-
tructure itself. Here we focus on two classes of such tech-
niques. In the ﬁrst, which we call address based, the victim
proﬁles the source IP addresses of trafﬁc it receives under
normal conditions to produce a model of “expected” traf-
ﬁc. It then applies this model to the IP addresses of incom-
ing trafﬁc during times of abnormally heavy load or other-
wise anomalous trafﬁc characteristics (which may indicate
a DoS attack) to ﬁlter out packets with addresses not con-
sistent with the model [14]. In the second, which we call
path based, the victim gleans from each packet an indica-
tor of the path the packet traversed, either the length (hop
count) of the path [13] or a “path marker” that is created by
the routers on the path to the victim [31], and ﬁlters trafﬁc
based on whether this indicator is consistent with a previ-
ously formed model of either the network itself or of paths
that attackers are known to use.
In this paper we undertake an empirical analysis of these
ﬁlters, based on data collected at the border of a large
(greater than one million host) network. Data was col-
lected in the form of NetFlow records from multiple bor-
der routers; since the routers cover all known interfaces be-
tween the client network and the internet proper, the col-
lection system provides access to all incoming trafﬁc. In the
course of collecting this data, we have recorded several DoS
attacks, both spoofed and non-spoofed, in situ. This permits
us to analyze the trafﬁc targeted at the victim before, dur-
Proceedings of the 2004 IEEE Symposium on Security and Privacy (S&P’04)  
1081-6011/04 $ 20.00 © 2004 IEEE 
ing, and after the DoS and, in particular, to simulate the be-
havior of the aforementioned proposed ﬁltering techniques
against these attacks.
In this extended abstract, we report on this analysis using
a representative DoS attack. In an effort to generalize some-
what from this one attack, however, we simulate this attack
against ten different servers for which we have records of
normal trafﬁc (one of which is the actually attacked server),
to determine the ﬁlters’ abilities to distinguish attack ad-
dresses from normal addresses for each of these servers.
Moreover, since path-based ﬁlters are sensitive to the topol-
ogy of routes to the attacked server, we also evaluate the at-
tack (and normal) trafﬁc as it would have been “seen” at
each of these ten servers, had it traveled to the server over
one of 26 different, real route topologies. We thus believe
that our analysis offers insight beyond the behavior of these
ﬁlters against this particular attack on one server. Still, we
caution the reader from inferring too much from our study:
as is any empirical study, ours is inﬂuenced by the partic-
ulars of the environment in which it is conducted. For this
reason, we intend to replicate this analysis to multiple at-
tacks in the full version of this paper.
Throughout this paper, the primary measures by which
we quantify the efﬁcacy of a ﬁltering scheme are the false
positive rate (the percentage of normal addresses from
which trafﬁc is ﬁltered out) and the false negative rate (the
percentage of attacker addresses from which trafﬁc is ac-
cepted, or in a spoofed attack, the percentage of such traf-
ﬁc accepted). We use these measures to clarify the impact
of several factors on the efﬁcacy of these ﬁltering tech-
niques, many of which are not adequately explored in
previous work. For example, each of these ﬁltering tech-
niques requires a learning period in which normal or attack
trafﬁc is modeled. We evaluate the impact of this learn-
ing period on the effectiveness of these ﬁltering techniques.
In addition, different techniques employ different assump-
tions during their learning phases, e.g., that it is possible
to identify a packet as an attack packet based on its con-
tents. A component of our analysis is to evaluate the effect
of such an assumption even for techniques that did not pre-
viously employ it, thereby comparing these techniques on
equal footing. This analysis leads us to preliminary com-
parison of these approaches and recommendations for
further study.
The remainder of this paper is structured as follows. We
survey related work in Section 2, and we describe the ﬁlter-
ing techniques that we evaluate in Section 3. We describe
our data set in Section 4. Section 5 constitutes the main por-
tion of our analysis. We interpret our results in Section 6,
and describe future work in Section 7.
2. Related Work
The dearth of widely accessible network-level traces that
are appropriate for evaluating DoS ﬁlters has forced most
analyses to simulate attacker locations and trafﬁc character-
istics artiﬁcially (e.g., as in [31, 13]), or to utilize less suited
data sources (e.g., HTTP logs and worm trafﬁc, as in [14]).
To our knowledge, our study is the ﬁrst direct comparison
of target-resident DoS ﬁlters in which they are trained on
recorded network trafﬁc and tested against a recorded DoS
event. The use of a recorded attack frees our analysis from
assumptions about trafﬁc characteristics or attacker loca-
tions that are often required in studies lacking appropriate
data.
Outside the focus of evaluating ﬁltering techniques,
though within the related realm of characterizing net-
work trafﬁc anomalies and DoS attacks, a number of stud-
ies have been performed utilizing recorded data sets.
Moore et al. utilize “backscatter” resulting from spoofed at-
tacks to evaluate the prevalence of these attacks and to
characterize them on several axes [20]. Barford et al. uti-
lize NetFlow records (as we do, see Section 4) collected
on the University of Wisconsin–Madison network to pro-
vide statistical descriptions of outages, ﬂash crowds, DoS
attacks and measurement failures [2, 3]. Hussain et al. uti-
lize a large data set capturing numerous attacks to char-
acterize and classify DoS attacks, and to explain the rea-
sons underlying their behavior [11]. We reiterate that our
goal is different from these studies: we strive to evaluate ﬁl-
tering techniques on representative DoS attacks, rather than
to characterize DoS attacks more broadly.
More distantly related are empirical studies of common-
case network packet trafﬁc (e.g., [5, 21, 22, 30]), in con-
trast to the anomalous events we consider here; of network
fault detection methods, i.e., that focus on accurate detec-
tion of anomalies induced by failures (e.g., [15, 10, 29]),
rather than ﬁltering DoS attacks as we consider here; and of
malicious network activity other than the DoS variety that
we consider here (e.g., [28, 33]).
As a ﬁrst effort at comparing techniques, our analysis fo-
cuses on a few techniques within a narrow segment of pro-
posed DoS defenses, namely those that ﬁlter at the target
with no change (or, in one case we consider, a small change)
to routers within the network. We have not considered nu-
merous, more ambitious approaches to ﬁltering DoS trafﬁc
within the network (e.g., [18, 27, 12, 23]). It is possible that
our results can form a baseline measure against which these
more ambitious approaches can be compared.
3. Filtering Techniques
In this section we summarize the DoS ﬁlters that we eval-
uate. We describe these techniques in the context of IPv4
Proceedings of the 2004 IEEE Symposium on Security and Privacy (S&P’04)  
1081-6011/04 $ 20.00 © 2004 IEEE 
and use corresponding terminology, in particular the use of
CIDR notation, i.e., /x to denote the x high-order bits of
IPv4 addresses. We note, however, that these approaches
can extend to IPv6 and other networking regimes.
(NAC)
Each of these techniques operates on the basis of a ﬁl-
tering attribute that the target extracts from each packet it
receives. Recall that we informally separate ﬁltering tech-
niques into address based and path based. Address based
ﬁltering utilizes only the source address of a packet to gen-
erate its ﬁltering attribute value. The address based tech-
niques that we explore in this work are the following:
[14] NAC character-
Network-aware clusters
izes networks by grouping IP addresses into clusters.
These clusters are not uniform size, but rather are de-
rived from CIDR blocks determined by examining BGP ta-
bles [16]. More precisely, when a target receives a packet
with source address s, it clusters this packet by examin-
ing BGP tables to determine the longest (most speciﬁc) pre-
ﬁx that matches s; i.e., this preﬁx is the attribute value of
this packet.
Static clusters (SC) [16] SC is a simpler form of cluster-
ing discussed in [16], though the authors discard this tech-
nique because static clusters are not an accurate reﬂection
of the administrative relationships between networks. How-
ever, it is the simplest of the techniques we consider. SC
uses clusters deﬁned by a ﬁxed constant x ∈ [1, 32]. For ex-
ample, if x = 16, then upon receiving a packet from
source address 192.143.14.7, the corresponding cluster (at-
tribute value) is 192.143.14.7/16 = 192.143. ∗ .∗.
Path based techniques work on the premise that packets
from the same network and sent to the same destination will
typically travel the same path. As such, these techniques ex-
tract a (possibly coarse) indicator of the path taken by the
packet and use this as the ﬁltering attribute value. We con-
sider the following path based ﬁltering attributes:
Hop counts (HCF) [13] If packets from the same network
and sent to the same destination travel the same path, then
the distance (hops) the packets travel will be the same. HCF
estimates the hop count of each packet it receives based on
its time-to-live ﬁeld (TTL). (The TTL permits only an es-
timation of the hop count, since initial TTL values differ
across platforms.) The ﬁltering attribute for this algorithm
is the pair consisting of the /24 preﬁx of the received packet,
and its hop count estimate.1
Path identiﬁers (PI) [31] This is the only technique
we study that presumes changes to routers in the net-
work. In this approach, each network router contributes to
a ﬁxed-size marker ﬁeld (e.g., 16 bits) in the IP header of
1
[13] considered ﬁner-grained clustering of the IP address component
of the attribute value, as well, though this will not be important to our
analysis.
each packet that traverses the router. The router’s contri-
bution is b bits (e.g., b = 1 or b = 2) that are computed
deterministically from a combination of the router’s IP ad-
dress and the previous router’s address. The router inserts
these bits into the marker ﬁeld either in a location deter-
mined by the TTL value of the packet [31] or by shifting
the current marker value and slotting this router’s contri-
bution in the low-order bits vacated by the shift [32]. As
such, packets that traverse identically the same path will
bear the same marker ﬁeld upon receipt; this marker ﬁeld
is the ﬁltering attribute. It is important to note that be-
cause of limited space for the marker ﬁeld, which is
created by co-opting portions of the IPv4 header, dis-
tinct paths can induce the same marker value. In particu-
lar, for a k-bit marker ﬁeld, any path ending with the same
k/b routers will bear the same marker.
Each of the ﬁltering attributes described above are used
as input to train a model of network trafﬁc and then to ﬁlter
based upon it. During learning, the target develops a model
of network trafﬁc based upon the attribute values of pack-
ets received. Afterward, the target ﬁlters trafﬁc based upon
this model and packets’ attribute values, with the goals of
eliminating as much DoS trafﬁc as possible and accepting
as much legitimate trafﬁc as possible.
A coarse characterization of ﬁlters that we will ﬁnd use-
ful later is whether the learning algorithm learns on the basis
of only legitimate (negative) trafﬁc—which we call a nor-
malcy learning algorithm—or if it presumes to be given la-
beled examples of both legitimate (negative) trafﬁc and at-
tack (positive) trafﬁc—which we call an attacker learning
algorithm. The distinction between normalcy and attacker
learning has signiﬁcant ramiﬁcations to both the feasibil-
ity of the learning algorithm and to the posture of the ﬁl-
tering algorithm. First, an attacker learning algorithm re-
quires some means of correctly distinguishing between le-
gitimate packets and attack packets, presumably via an au-
tomatic classiﬁer if learning is to involve trafﬁc of any sig-
niﬁcant quantity. So, in some measure an attacker learning
algorithm already solves the ﬁltering problem, and thus con-
stitutes a powerful assumption. Second, an attacker learning
algorithm permits the ﬁlter to deny a packet on the grounds
that it possesses the same attribute value as attack packets
seen during learning, provided that the attacker does not (as
in a non-spoofed attack) or cannot (as in PI) forge attribute
values. Though different ﬁlters were proposed using differ-
ent types of learning—NAC, SC, and HCF presume nor-
malcy learning; PI assumes attacker learning—one aspect
of our analysis is to consider the performance of, e.g., NAC,
against non-spoofed attacks when given the beneﬁt of an at-
tacker learner.
Proceedings of the 2004 IEEE Symposium on Security and Privacy (S&P’04)  
1081-6011/04 $ 20.00 © 2004 IEEE 
4. Attacks
In this section we describe the attack that we use to eval-
uate the DoS ﬁltering techniques described in Section 3. We
begin with an overview of the system we use to collect the
trafﬁc, and then describe the attack that we utilize for eval-
uation.
4.1. Attack collection system
Our data collection was performed using a system called
SiLK (System for Internet-level Knowledge), developed by
the CERT. SiLK is a data collection and analysis system for
monitoring large volumes of network trafﬁc. SiLK provides
tools to analyze trafﬁc over large networks (currently over
1 million nodes) and long periods of time (several months).
By installing SiLK on a client network, we have been able
to collect network trafﬁc before, during and after attacks.
The SiLK system collects CISCO NetFlow records
and converts them into a space-efﬁcient format. Net-
Flow is a collection system developed by CISCO to cap-
ture ﬂows, i.e., summaries of trafﬁc consisting of packets
closely grouped over time [6]. These ﬂow records con-
sist of a characteristic 5-tuple of (source IP address, des-
tination IP address, protocol, source port, destination
port2). CISCO NetFlow is a commercially available pack-
age for reporting this ﬂow trafﬁc using a well-deﬁned data
format.
While NetFlow is a trafﬁc analysis tool, it provides fea-
tures that make it desirable for security analysis. In particu-
lar, NetFlow records are collected at the router: by collect-
ing ﬂows it is possible to see all trafﬁc crossing a network’s
border, including trafﬁc which the router drops because of
ACL violations and overﬂows. Flow analysis is used else-
where for security purposes, e.g., [24].
SiLK optimizes NetFlow data for rapid analysis. The
records are compressed to a more space-efﬁcient form and
then recorded in ﬂat ﬁles that are accessed using a special-
ized collection of tools. Given the volume of data collected
by SiLK (in excess of 60 GB and 300 million records per
day on the network we monitor), traditional database solu-
tions would require a far more expensive implementation to
provide the same access speed.
DoS attacks are unpredictable; the best way to acquire
data on a DoS attack is to instrument a sufﬁciently large
network and lie in wait. SiLK’s primary strength is its ca-
pacity to manage large volumes of trafﬁc data over long pe-
riods of time, making a long wait feasible.
Protocol
Packet Size
Duration
Bytes
Packets
Source addresses
UDP
34 bytes
6h, 1m
22,009,778,272
687,805,571
3,102
Table 1. Trafﬁc characteristics of attack
4.2. Attack data set
For the purposes of this paper, we discuss a represen-
tative DoS attack from the SiLK data. This attack targeted
a web server, but the attack was not itself HTTP-speciﬁc
and was not aimed at a speciﬁc service: it struck at a range
of destination ports that are largely unrelated to speciﬁc ser-
vices. This attack was aimed at either the IP stack or the net-
work infrastructure surrounding the targeted machine. The
attack was identiﬁed after the fact: analysts notiﬁed us of
the basic characteristics of the event, and relevant trafﬁc was
then extracted from the SiLK data warehouse.
The basic characteristics of the attacks are given in Ta-
ble 1, and progression of the attack over time is shown Fig-
ure 1. The attack packets originated from a limited set of
IP addresses: throughout the attack, 3102 previously un-
seen addresses repeatedly sent 32-byte UDP packets at ran-
dom destination ports. While it is impossible to say deﬁni-
tively whether these attacks were spoofed, there is evidence
to suggest that the attacker addresses are authentic. First, the
addresses all originated from used address spaces.3 Second,
the ephemeral port assignments for the attackers are consis-
tent with single addresses: for each address, the port num-
bers increase linearly and then cycle back to an initial port.4
During the course of the attack, certain addresses appear
and disappear from the set of attackers; see Figure 1(iii). We
hypothesize three reasons for these changes: data lost by the
router dropping reports (see below), user intervention to re-
move DoS attackers, and attacker intervention to add addi-
tional attackers. At this point, there appears to be no consis-
tent pattern to why an address appears or disappears: when
new addresses appear, address changes do not appear to be
grouped uniformly together in time and addresses that stop
do not appear to be from the same networks.
As shown in Figure 1(ii), approximately half of the traf-
ﬁc hitting the server was dropped by the router. This exces-
sive drop rate is likely a result of the severity of the attack.
2
For ICMP trafﬁc, type and code are stored in the destination port ﬁeld.