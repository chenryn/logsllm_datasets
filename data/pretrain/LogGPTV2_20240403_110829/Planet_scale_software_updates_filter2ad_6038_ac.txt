their laptop in the middle of the night), thus, they add a very small
error to our AOM estimation.
4.3 Frequency of computer updates
We now study how up-to-date computers are kept around the world.
To this extent, we examine a Windows XP patch from June 2005
(Set-II).4 The minimum size of a requested delta for this patch is
22 KBytes, while the maximum was 800KBytes. Smaller deltas
correspond to updates for recent versions of the ﬁle, while larger
deltas update older versions. In Fig. 10, we plot the CDF of re-
quested delta sizes in different parts of the world. Jumps in the
graph correspond to requested deltas. For instance, we see a ﬁrst
jump around 20KB, which corresponds to all computers that are
regularly updated and need few changes. Similarly, jumps at larger
deltas relate to less updated computers.
We can see that there is a large difference on how updated com-
puters are kept around the world. For instance, US and Japan users
keep their machines highly updated, with 90% of users updating
from the inmediate previous version, while this happens only for
50− 70% of users in China or France. The fact that users are more
or less updated, is likely a combination of download speeds and the
amount of time they spend on-line.
From this plot, we can also determine how many deltas per ﬁle
should be made available for download to satisfy a large number
of users. This is an interesting question that impacts the design of
an on-line patch system. Providing a large number of deltas per
ﬁle increases the chances that nodes do not have to download the
complete ﬁle. However, it increases system’s complexity and re-
duces the access reference locality of any given ﬁle, since requests
are spread across many deltas. From our data, we can observe that
90% or more of all users can be satisﬁed by very few deltas of re-
cent versions (e.g. 2-3). Thus, a very small set of deltas is sufﬁcient
to patch most users, thereby, decreasing the need for publishing and
archiving a large number of older deltas per ﬁle.
Always Patched Users: We have also calculated the percentage
of users that are patched with the latest critical updates. To this
end, we have analyzed the behavior of 300 Million users that pe-
riodically query for updates (Set-I), and monitored the percentage
4This is a security patch addressing HTML help vulnerability; see
http://support.microsoft.com/kb/896358.
Figure 9: Estimated percentage of distinct IPs classiﬁed as AOM
per country versus the total number of distinct IPs per country. The
estimates are upper bounds with 95% conﬁdence. The larger the
number of distinct IPs per country, the tighter the estimate would
be to the uknown parameter.
test) for each AS. 2 Overall, for the ﬁrst day of our trace the null
hypothesis cannot be rejected for 52% of ASes. However, these
ASes only amount to less than 0.1% of the distinct IP’s observed
over the day! We obtained the same percentages by running the KS
test for each of the other two days. Thus, for 50% of the ASes that
acount for more than 90% of our IP population, the uniformity of
query arrivals is rejected.
We also examined the magnitude of the query rate burstiness by
evaluating the maximum workload of a hypothetical server that
serves AS-aggregate of queries and quantiﬁed that, indeed, the bursti-
ness is in many cases larger than if the AS-aggregate query rates
were uniform in time.
4.2 Estimated Always on-line Machines
Always Online Machines (AOM) are of speciﬁc interest since they
can be instantaneously patched using an ideal push patching sys-
tem (e.g.
for instance a ubiquitous multicast channel joining all
users). The polling instants of an AOM user can be modeled as
a renewal stochastic process with inter-polling times independent
and identically distributed, uniformly in [18, 22] hours.3
We use an estimation technique based on the aggregate counts of
queries to estimate an upper bound on the number of AOM users
with a ﬁxed conﬁdence, which is a good estimate for the unknown
number of AOM users in aggregates of many users, provided that
over some time intervals the number AOM users dominates, i.e.
the number of other users can be neglected (as it would be over
night periods for aggregates of users with geographical proximity).
The estimation technique uses the fact that the number of fresh IPs
observed from AOM users over a time interval is a binomial ran-
dom variable and uses known large deviation bound that holds for
binomial random variables. For space reasons, the details of the es-
2The KS statistic is deﬁned as the maximum absolute deviation of
an empirical CDF and a candidate CDF assumed under null hy-
pothesis. If, instead of using the absolute deviation, we use either
the most positive or the most negative deviation, the KS statistic is
called one-sided.
3This polling rule may be regarded a good design choice as it is
a random walk on a 24 hour clock, with inter-polling times hav-
ing a density so that a host polling instant converges to a uniform
distribution over a day.
Figure 10: Distribution of the number of requests for different delta
sizes across different countries (US, FR, CHN, JPN, KR). Smaller
deltas indicate more updated computers. All deltas correspond to
the same ﬁle.
Figure 11: Requests per delta included in SP2. Notice three major
spikes, corresponding to people requesting SP2 installation on top
of XP-RTM, XP-SP1, or XP-SP1 plus all security patches released
to date.
of them that need to patch at any point in time (see Figure 1). We
have observed that for the most part, the number of users that query
and are not fully updated with all previous patches (e.g. require an
update) is less than 10%. We have also tracked what happens in-
mediately after the release of a security patch during the second day
of the trace. We have observed that the number of users requiring
the security patch rapidly decreases to less than 20% of the total
users by the end of the second day.
In summary, the results indicate that a large percentage of the
population is highly updated. Such encouraging results are in sharp
contrast with the results obtained by analyzing the state of comput-
ers prior to the release of SP2, when the automate update service
was not turned On by default. To highlight this, Fig. 11 shows the
timestamp of those deltas requested during the SP2 distribution;
older timestamps indicate users updating from older versions. It
shows that the number of users that were updated with the most
recent patches was less than 5%, with 22% of users updating from
SP1 versions and 60% from XP RTM versions. This emphazises
the importance of automatic patching schemes, which do not rely
on manual intervention.
Figure 12: CCDF of the number of requests per IP address.
5. PATCH DISSEMINATION STRATEGIES
We now investigate alternative update-delivery strategies and com-
pare them to standard central server solutions. Using analytical and
empirical results we evaluate the potential of (i) caching, (ii) peer-
to-peer, and (iii) peer-to-peer with locality. Our purpose is not to
evaluate in detail the various strategies, but to study some ’what if’
scenarios and to provide qualitative results.
To evaluate the alternative strategies, we assume that hosts are
partitioned into groups, which we call subnets. For example, all
hosts that belong to the same ISP, or the same Autonomous System
(AS) belong to the same subnet; in the rest of this section, subnets
may be thought of ASes or ISP networks. We shall make the as-
sumption that inter-subnet trafﬁc (e.g. cross ISP) is expensive and
intra-subnet is preferential. We shall study the effect of the alterna-
tive policies in reducing the server load and the inter-subnet trafﬁc.
5.1 Caching
In this section we ﬁrst estimate the beneﬁts of a patching distribu-
tion system that uses the currently deployed Web caches. We then
estimate the potential beneﬁts of an ideal full cache deployment
with caches at each AS.
Assume that caches are deployed in a subset C of subnets. The
reduction in the workload of the download servers can be computed
using the assignment of hosts to subnets, and the number of patches
per host. Denote with Ni,j the number of hosts from subnet j that
need update i. The number of requests directed to the central server
from a subnet j ∈ C is at least 1, if there exists a host in j that needs
i. Indeed, a lower bound of the number of downloads D from the
central server is:
D ≥X
X
1Ni,j >0 +
Ni,j
j∈C
i
the number of downloads from the central server isP
where the equality holds for inﬁnite-capacity caches. Without caching,
P
j ∈| C
i
and, hence, caching reduces the load at the server by a factor α:
i Ni,j,
j
„
α = µ ·
1 − 1
S
X
X
«
P
P
P
P
with
µ =
P
P
j∈C
j
P
P
i Ni,j
i Ni,j
and S =
j∈C
i Ni,j
j∈C
i 1Ni,j >0
.
In other words, µ is the fraction of updates needed in subnets cov-
ered by caches and S is simply the mean number of updates per
subnet over subnets that deploy caches. A similar approach can be
used to derive the beneﬁt in terms of bytes.
Estimated caching deployment: To calculate the beneﬁt of using
the currently deployed web caches for update dissemination, we
345678x 10500.10.20.30.40.50.60.70.80.91Patch Delta Size (Bytes)FDCdelta size CDF for different countriesUSFRCHNJPNKRJan01Jan02Jan03Jan04Jan0500.511.522.5x 105Delta TimeStampRequests per Delta10010210410610−810−610−410−2100Number of requests per IP1−Pr[X>x]need to estimate the parameters µ and S. Using Set-I, we identify
IP addresses from which we have received multiple requests in a
period of 17hr. Since each (AOM) machine polls the update servers
at most once during a period of 17hr, those IP addresses are shared
by multiple machines and likely represent caches, NAT devices,
ﬁrewalls, modem pools, etc. It is impossible to identify the type
of the device using our data; instead, we assume that an IP address
belongs to a cache if the number of requests received from that IP
is above a threshold. Our estimate is an upper bound on the number
of currently deployed large caches, since we falsely classify large
NATed points as caches.
We have used Set-I and counted the number of polls and the num-
ber of distinct IP addresses observed. Fig. 12 plots the complemen-
tary CDF of the number of machines per IP address. We observe
that it follows a heavy-tailed distribution, with a few IPs having a
very large number of machines (as large as 240K machines).
We have experimented with various thresholds for identifying
caches and our estimates for µ, S, and α are given in Table 3.
For thresholds over 10 users/IP (which should exclude most home
NATed environments) the estimate of the percentage of requests
from users behind caches, µ is 20-29%. Note that larger thresholds
(e.g. 50 users/IP), do not change the estimate signiﬁcantly. Al-
though the percentage of queries that can be satisﬁed from caches
is non-negligible, the central servers still need to handle 210-240M
users, hence, the beneﬁt of using existing caches is rather limited.
Threshold
α
# Caches
25,351,342
5,504,963
1,718,094
656,903
209,765
Users Covered
186,766,792
115,942,495
88,419,310
69,701,692
59,933,202
µ
S
62%
7.37
38% 21.06
29% 51.46
23% 106.11
20% 285.72
> 2
> 5
> 10
> 25
> 50
53.58%
36.20%
28.44%
22.78%
19.92%
Table 3: Estimate of the load reduction α at the server by using
existing caches. We assume that an IP address belongs to a cache if
we have received more than threshold requests from that IP. We also
assume the distribution of a single ﬁle, hence S =
.
Data from Set-I.
P
P
j∈C 1Nj >0
j∈C Nj
Full cache deployment: We now study the impact of an ideal
caching deployment with a cache at each subnet (µ = 1). We
have assumed that each subnet represents an AS. Even though this
scenario is not representative of the current Internet, it allows us to
estimate the potential beneﬁt of deploying more caches. Observe
that if every subnet has a cache, then the server needs to serve at
most as many copies of an update as the number of subnets, there-