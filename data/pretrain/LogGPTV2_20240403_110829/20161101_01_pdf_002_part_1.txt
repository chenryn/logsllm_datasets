Introduction to Hacking PostgreSQL
Neil Conway
PI:EMAIL
May 21, 2007
Outline
1 Prerequisites
Why Should You Hack On PostgreSQL?
What Skills Will You Need?
What Tools Should You Use?
2 The Architecture of PostgreSQL
System Architecture
Components of the Backend
3 Common Code Conventions
Memory Management
Error Handling
4 Community Processes
5 Sample Patch
6 Conclusion
Outline
1 Prerequisites
Why Should You Hack On PostgreSQL?
What Skills Will You Need?
What Tools Should You Use?
2 The Architecture of PostgreSQL
System Architecture
Components of the Backend
3 Common Code Conventions
Memory Management
Error Handling
4 Community Processes
5 Sample Patch
6 Conclusion
Contribute to the community
We need more reviewers
Understand how PostgreSQL works more deeply
Become a better programmer
The PostgreSQL source is a good example to learn from
Commercial opportunities
Why Hack On PostgreSQL?
Possible Reasons
Databases are fun!
Understand how PostgreSQL works more deeply
Become a better programmer
The PostgreSQL source is a good example to learn from
Commercial opportunities
Why Hack On PostgreSQL?
Possible Reasons
Databases are fun!
Contribute to the community
We need more reviewers
Become a better programmer
The PostgreSQL source is a good example to learn from
Commercial opportunities
Why Hack On PostgreSQL?
Possible Reasons
Databases are fun!
Contribute to the community
We need more reviewers
Understand how PostgreSQL works more deeply
Commercial opportunities
Why Hack On PostgreSQL?
Possible Reasons
Databases are fun!
Contribute to the community
We need more reviewers
Understand how PostgreSQL works more deeply
Become a better programmer
The PostgreSQL source is a good example to learn from
Why Hack On PostgreSQL?
Possible Reasons
Databases are fun!
Contribute to the community
We need more reviewers
Understand how PostgreSQL works more deeply
Become a better programmer
The PostgreSQL source is a good example to learn from
Commercial opportunities
Skills
Essential
Some knowledge of C
Fortunately, C is easy
Some familiarity with Unix and basic Unix programming
Postgres development on Win32 is increasingly feasible
Helpful, but not essential
Unix systems programming
DBMS internals
Autotools-foo
Performance analysis
...depending on what you want to hack on
Indexing The Source
A tool like tags, cscope or glimpse is essential when
navigating any large code base
“What is the definition of this function/type?”
“What are all the call-sites of this function?”
src/tools/make [ce]tags
Development Tools
The Basics
$CC, Bison, Flex, CVS, autotools
Configure flags: enable-depend, enable-debug,
enable-cassert
Consider CFLAGS=-O0 for easier debugging (and faster builds)
With GCC, this suppresses some important warnings
Development Tools
The Basics
$CC, Bison, Flex, CVS, autotools
Configure flags: enable-depend, enable-debug,
enable-cassert
Consider CFLAGS=-O0 for easier debugging (and faster builds)
With GCC, this suppresses some important warnings
Indexing The Source
A tool like tags, cscope or glimpse is essential when
navigating any large code base
“What is the definition of this function/type?”
“What are all the call-sites of this function?”
src/tools/make [ce]tags
Profiling
gprof is the traditional choice; various bugs and limitations
Use --enable-profiling to reduce the pain
callgrind works well, nice UI (kcachegrind)
oprofile is good at system-level performance analysis
DTrace
Other Tools
A debugger is often necessary: most developers use gdb
Or a front-end like ddd
Even MSVC?
ccache and distcc are useful, especially on slower machines
valgrind is useful for debugging memory errors and memory
leaks in client apps
Not as useful for finding backend memory leaks
Other Tools
A debugger is often necessary: most developers use gdb
Or a front-end like ddd
Even MSVC?
ccache and distcc are useful, especially on slower machines
valgrind is useful for debugging memory errors and memory
leaks in client apps
Not as useful for finding backend memory leaks
Profiling
gprof is the traditional choice; various bugs and limitations
Use --enable-profiling to reduce the pain
callgrind works well, nice UI (kcachegrind)
oprofile is good at system-level performance analysis
DTrace
Authoring SGML
I don’t know of a good SGML editor, other than Emacs
Writing DocBook markup by hand is labour-intensive but not
hard: copy conventions of nearby markup
make check does a quick syntax check
make draft is useful for previewing changes
SGML Documentation
Understatement
The DocBook toolchain is less than perfect
SGML Documentation
Understatement
The DocBook toolchain is less than perfect
Authoring SGML
I don’t know of a good SGML editor, other than Emacs
Writing DocBook markup by hand is labour-intensive but not
hard: copy conventions of nearby markup
make check does a quick syntax check
make draft is useful for previewing changes
Patch Management
Most development is done by mailing around patches
echo "diff -c -N -p" >> ~/.cvsrc
cvs diff > ~/my_patch-vN.patch
interdiff is a useful tool: “exactly what did I change
between v5 and v6?”
Remote cvs is slow: setup a local mirror of the CVS
repository
cvsup, csup, rsync, svnsync (soon!)
To include newly-added files in a CVS diff, either use a local
CVS mirror or cvsutils
For larger projects: akpm’s Quilt, or a distributed VCS
Postgres-R uses Monotone
Recommended: git tree at repo.or.cz/w/PostgreSQL.git
Text Editor
If you’re not using a good programmer’s text editor, start
Teach your editor to obey the Postgres coding conventions:
Hard tabs, with a tab width of 4 spaces
Similar to Allman/BSD style; just copy the surrounding code
Using the Postgres coding conventions makes it more likely
that your patch will be promptly reviewed and applied
Useful Texts
SQL-92, SQL:1999, SQL:2003, and SQL:200n
http://www.wiscorp.com/SQLStandards.html (“draft”)
There are some books and presentations that are more
human-readable
There’s a samizdat plaintext version of SQL-92
SQL references for Oracle, DB2, ...
A textbook on the design of database management systems
I personally like Database Management Systems by
Ramakrishnan and Gehrke
Books on the toolchain (C, Yacc, autotools, ...) and
operating system kernels
Outline
1 Prerequisites
Why Should You Hack On PostgreSQL?
What Skills Will You Need?
What Tools Should You Use?
2 The Architecture of PostgreSQL
System Architecture
Components of the Backend
3 Common Code Conventions
Memory Management
Error Handling
4 Community Processes
5 Sample Patch
6 Conclusion
2 Attach to shared memory segment (SysV IPC), initialize
shared data structures
3 Fork off daemon processes: autovacuum launcher, stats
daemon, bgwriter, syslogger
4 Bind to TCP socket, listen for incoming connections
For each new connection, spawn a backend
Periodically check for child death, launch replacements or
perform recovery
The Postmaster
Lifecycle
1 Initialize essential subsystems; perform XLOG recovery to
restore the database to a consistent state
3 Fork off daemon processes: autovacuum launcher, stats
daemon, bgwriter, syslogger
4 Bind to TCP socket, listen for incoming connections
For each new connection, spawn a backend
Periodically check for child death, launch replacements or
perform recovery
The Postmaster
Lifecycle
1 Initialize essential subsystems; perform XLOG recovery to
restore the database to a consistent state
2 Attach to shared memory segment (SysV IPC), initialize
shared data structures
4 Bind to TCP socket, listen for incoming connections
For each new connection, spawn a backend
Periodically check for child death, launch replacements or
perform recovery
The Postmaster
Lifecycle
1 Initialize essential subsystems; perform XLOG recovery to
restore the database to a consistent state
2 Attach to shared memory segment (SysV IPC), initialize
shared data structures
3 Fork off daemon processes: autovacuum launcher, stats
daemon, bgwriter, syslogger
The Postmaster
Lifecycle
1 Initialize essential subsystems; perform XLOG recovery to
restore the database to a consistent state
2 Attach to shared memory segment (SysV IPC), initialize
shared data structures
3 Fork off daemon processes: autovacuum launcher, stats
daemon, bgwriter, syslogger
4 Bind to TCP socket, listen for incoming connections
For each new connection, spawn a backend
Periodically check for child death, launch replacements or
perform recovery
Inter-Process Communication
Most shared data is communicated via a shared memory
segment
Signals, semaphores, and pipes also used as appropriate
Stats collector uses UDP on the loopback interface
Subprocesses inherit the state of the postmaster after fork()
Daemon Processes
Types of Processes
autovacuum launcher: Periodically start autovacuum workers
bgwriter: Flush dirty buffers to disk, perform periodic checkpoints
stats collector: Accepts run-time stats from backends via UDP
syslogger: Collect log output from other processes, write to file(s)
normal backend: Handles a single client session
Daemon Processes
Types of Processes
autovacuum launcher: Periodically start autovacuum workers
bgwriter: Flush dirty buffers to disk, perform periodic checkpoints
stats collector: Accepts run-time stats from backends via UDP
syslogger: Collect log output from other processes, write to file(s)
normal backend: Handles a single client session
Inter-Process Communication
Most shared data is communicated via a shared memory
segment
Signals, semaphores, and pipes also used as appropriate
Stats collector uses UDP on the loopback interface
Subprocesses inherit the state of the postmaster after fork()
Disadvantages
Shared memory segment is statically-sized at startup
Managing arbitrarily-sized shared data is problematic
Some shared operations can be awkward: e.g. using multiple
processors to evaluate a single query
Consequences
Advantages
Address space protection: significantly harder for misbehaving
processes to crash the entire DBMS
IPC and modifications to shared data are explicit: all state is
process-private by default
Consequences
Advantages
Address space protection: significantly harder for misbehaving
processes to crash the entire DBMS
IPC and modifications to shared data are explicit: all state is
process-private by default
Disadvantages
Shared memory segment is statically-sized at startup
Managing arbitrarily-sized shared data is problematic
Some shared operations can be awkward: e.g. using multiple
processors to evaluate a single query
2 Backend enters the “frontend/backend” protocol:
1 Authenticate the client
2 “Simple query protocol”: accept a query, evaluate it, return
result set
3 When the client disconnects, the backend exits
Backend Lifecycle
Backend Lifecycle
1 Postmaster accepts a connection, forks a new backend, then
closes its copy of the TCP socket
All communication occurs between backend and client
Backend Lifecycle
Backend Lifecycle
1 Postmaster accepts a connection, forks a new backend, then
closes its copy of the TCP socket
All communication occurs between backend and client
2 Backend enters the “frontend/backend” protocol:
1 Authenticate the client
2 “Simple query protocol”: accept a query, evaluate it, return
result set
3 When the client disconnects, the backend exits
Stages In Query Processing
Major Components
1 The parser - lex & parse the query string
2 The rewriter - apply rewrite rules
3 The optimizer - determine an efficient query plan
4 The executor - execute a query plan
5 The utility processor - process DDL like CREATE TABLE
Produces a “raw parsetree”: a linked list of parse nodes
Parse nodes are defined in include/nodes/parsenodes.h
Typically a simple mapping between grammar productions and
parse node structure
The Parser
Lex and parse the query string submitted by the user
Lexing: divide the input string into a sequence of tokens
Postgres uses GNU Flex
Parsing: construct an abstract syntax tree (AST) from
sequence of tokens
Postgres uses GNU Bison
The elements of the AST are known as parse nodes
The Parser
Lex and parse the query string submitted by the user
Lexing: divide the input string into a sequence of tokens
Postgres uses GNU Flex
Parsing: construct an abstract syntax tree (AST) from
sequence of tokens
Postgres uses GNU Bison
The elements of the AST are known as parse nodes
Produces a “raw parsetree”: a linked list of parse nodes
Parse nodes are defined in include/nodes/parsenodes.h
Typically a simple mapping between grammar productions and
parse node structure
Semantic Analysis
In the parser itself, only syntactic analysis is done; basic
semantic checks are done in a subsequent “analysis phase”
parser/analyze.c and related code under parser/
Resolve column references, considering schema path and
query context
SELECT a, b, c FROM t1, t2, x.t3
WHERE x IN (SELECT t1 FROM b)
Verify that referenced schemas, tables and columns exist
Check that the types used in expressions are consistent
In general, check for errors that are impossible or difficult to
detect in the parser itself
Rewriter, Planner
The analysis phase produces a Query, which is the query’s
parse tree (Abstract Syntax Tree) with additional annotations
The rewriter applies rewrite rules, including view definitions.
Input is a Query, output is zero or more Querys
The planner takes a Query and produces a Plan, which
encodes how the query should be executed
A query plan is a tree of Plan nodes, each describing a
physical operation
Only needed for “optimizable” statements (INSERT, DELETE,
SELECT, UPDATE)
The planner arranges the operations into a plan tree that
describes the data flow between operations
Tuples flow from the leaves of the tree to the root
Leaf nodes are scans: no input, produce a stream of tuples
Joins are binary operators: accept two inputs (child nodes),
produce a single output
The root of the tree produces the query’s result set
Therefore, the executor is “trivial”: simply ask the root plan
node to repeatedly produce result tuples
Executor
Each node in the plan tree describes a physical operation
Scan a relation, perform an index scan, join two relations,
perform a sort, apply a predicate, perform projection, ...
Executor
Each node in the plan tree describes a physical operation
Scan a relation, perform an index scan, join two relations,
perform a sort, apply a predicate, perform projection, ...
The planner arranges the operations into a plan tree that
describes the data flow between operations
Tuples flow from the leaves of the tree to the root
Leaf nodes are scans: no input, produce a stream of tuples
Joins are binary operators: accept two inputs (child nodes),
produce a single output
The root of the tree produces the query’s result set
Therefore, the executor is “trivial”: simply ask the root plan
node to repeatedly produce result tuples
scan types: Seq scan, index scan, bitmap index scan
join order: Inner joins are commutative: reordered freely
join types: Sort-merge join, hash join, nested loops
aggregation: Hashed aggregation, aggregation by sorting
predicates: Predicate push down, evaluation order
rewrites: Subqueries and set operations → joins,
outer joins → inner joins, function inlining, ...
Query Optimization
SQL is (ostensibly) a declarative query language
The query specifies the properties the result set must satisfy,
not the procedure the DBMS must follow to produce the result
set
For a typical SQL query, there are many equivalent query plans
Query Optimization