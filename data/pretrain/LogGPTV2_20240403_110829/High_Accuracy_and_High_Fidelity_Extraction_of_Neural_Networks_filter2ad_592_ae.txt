### Gradient Estimation and Weight Extraction Errors

The gradient estimation procedure may produce an inaccurate estimate of the gradient, leading to a significant error in one of the extracted weights \( \hat{A}^{(0)}_{ij} \). Introducing an error into just one of the weights of the first matrix \( \hat{A}^{(0)} \) should not, in theory, induce significant further errors. However, this error affects the bias vector extraction, as the extracted bias \( \hat{B}^{(0)}_i \) will have an error proportional to the error in \( \hat{A}^{(0)}_{ij} \). When the bias is incorrect, it impacts every subsequent calculation, even those where the erroneous edge is not directly involved.

To resolve this issue, either the failure rate of the gradient estimation must be reduced from 1 in 10,000 to practically zero, or a complex error-recovery procedure must be implemented. In the following section, we introduce an improvement that almost completely mitigates this problem.

### Fidelity of Functionally-Equivalent Extraction Attack

| Parameters | Fidelity | Queries |
|------------|----------|---------|
| 25,000     | 100%     | 217.2   |
| 50,000     | 100%     | 220.2   |
| 12,500     | 100%     | 218.2   |
| 100,000    | 99.98%   | 219.2   |

**Table 6:** Fidelity of the functionally-equivalent extraction attack across different test distributions on an MNIST victim model. Results are averaged over five extraction attacks. For small models, we achieve perfect fidelity; larger models have near-perfect fidelity on the test data distribution but begin to lose accuracy at 100,000 parameters.

### Challenges in Extending the Attack to Deeper Networks

The attack is specific to two-layer neural networks, and extending it to deeper networks presents several challenges. In deep networks, the critical point search step (Section 6.3) results in critical points from multiple layers, making it nontrivial to determine which layer a critical point belongs to. Without this knowledge, we cannot control the inputs to the neurons, which is necessary for weight recovery (Section 6.4). Even with knowledge of the layer, the inputs of neurons beyond the first layer are the outputs of other neurons, providing only indirect control. Additionally, small numerical errors in the first layer's extraction propagate through finite differences computations in subsequent layers, causing increasingly larger errors. Therefore, extending the attack to deeper networks requires solving the following: producing critical points for specific layers, recovering weights without direct input control, and significantly reducing numerical errors.

### Hybrid Strategies

Until now, our strategies for model extraction have been either purely learning-based or purely direct. We now demonstrate that there is a continuous spectrum of hybrid strategies that can leverage both the query efficiency of learning-based methods and the fidelity of direct extraction.

#### 7.1 Learning-Based Extraction with Gradient Matching

Milli et al. show that gradient matching improves extraction by optimizing the objective function:
\[
\sum_{i=1}^n H(O(x_i), f(x_i)) + \alpha \left\| \nabla_x O(x_i) - \nabla_x f(x_i) \right\|^2_2,
\]
assuming the adversary can query the model for \( \nabla_x O(x) \). This requires more model access than we allow, but it illustrates how direct recovery intuition can enhance extraction. Preliminary experiments indicate that this technique can improve fidelity on small datasets (e.g., increasing fidelity from 95% to 96.5% on Fashion-MNIST). We leave scaling and removing the model access assumption for future work.

#### 7.2 Error Recovery through Learning

Recall that the functionally-equivalent extraction attack's fidelity degrades with increasing model size due to low-probability errors in the first weight matrix, which induce incorrect biases and propagate to subsequent layers. We introduce a method for performing a learning-based error recovery routine. While a fully learning-based attack leaves too many free variables, fixing many variables to values extracted through direct recovery allows us to learn the remaining variables.

Formally, let \( \hat{A}^{(0)} \) be the extracted weight matrix and \( \hat{B}^{(0)} \) be the extracted bias vector for the first layer. Previously, we used least squares to solve for \( \hat{A}^{(1)} \) and \( \hat{B}^{(1)} \) assuming perfect extraction of the first layer. Here, we relax this assumption and perform gradient descent to minimize:
\[
\mathbb{E}_{x \in D} \left\| f_\theta(x) - W_1 \text{ReLU}(\hat{A}^{(0)} x + \hat{B}^{(0)} + W_0) + W_2 \right\|,
\]
where we use a single trainable parameter to adjust the bias term of the first layer and then solve for the remaining weights via gradient descent with training data. This hybrid strategy substantially increases the fidelity of the extracted model, as shown in Table 8. In the worst-performing example (with only direct extraction), the extracted 128-neuron network had 80% fidelity agreement with the victim model. With learning-based recovery, the fidelity agreement jumps to 99.75%.

| Parameters | Transferability |
|------------|-----------------|
| 50,000     | 100%            |
| 100,000    | 100%            |
| 200,000    | 100%            |
| 400,000    | 100%            |

**Table 8:** Transferability rate of adversarial examples using the extracted neural network from our Section 7 attack.

#### 7.2.1 Transferability

Adversarial examples often transfer between models, especially when the models are similar. We expect that adversarial examples generated on our extracted model will fool the remote oracle nearly always. To measure transferability, we run 20 iterations of PGD with \( \ell_\infty \) distortion set to commonly used values: 0.1 for MNIST and 0.03 for CIFAR-10. The attack achieves functionally equivalent extraction (modulo floating-point precision errors), resulting in a 100% transferability success rate for all extracted models.

### Related Work

Defenses against model extraction fall into two categories: limiting information per query and differentiating adversaries from benign users. Limiting information includes perturbing probabilities, removing some class probabilities, or returning only the class output. Another approach involves sampling from a distribution over model parameters. Differentiating adversaries focuses on analyzing query patterns. Non-adaptive attacks bypass pattern-based detection and are weakened by information limiting. Our functionally-equivalent attack is broken by these measures, and we leave defense-aware attacks for future work.

Queries can also reveal hyperparameters or architectural information, and side-channel attacks can do the same. These are orthogonal but compatible with our work, as information about a model, such as assumptions in Section 6, empowers extraction. Watermarking neural networks has been proposed to identify extracted models, and cryptographic protocols like obfuscation and secure multi-party computation offer potential defenses.

### Conclusion

This paper explores model extraction attacks on neural networks, focusing on accuracy and fidelity. Our learning-based methods effectively attack large models, reducing the error rate by 10%. Direct functionally-equivalent extraction returns a neural network with 100% fidelity on test samples and adversarial examples. We propose a hybrid method to recover from numerical instability in the functionally-equivalent extraction attack. Our work highlights open problems in model extraction, such as reducing required capabilities and scaling functionally-equivalent extraction.

### Acknowledgements

We thank Ilya Mironov for lengthy and fruitful discussions, Úlfar Erlingsson for positioning the work, and Florian Tramèr for his comments on an early draft.

### References

[References listed here, formatted as in the original text]