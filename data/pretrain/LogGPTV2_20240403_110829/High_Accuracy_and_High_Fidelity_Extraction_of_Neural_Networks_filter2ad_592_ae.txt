gradient estimation procedure obtains an incorrect estimate
of the gradient and therefore one of the extracted weights ˆA(0)
i j
is incorrect by a non-insigniﬁcant margin.
Introducing an error into just one of the weights of the
ﬁrst matrix ˆA(0) should not induce signiﬁcant further errors.
However, because of this error, when we solve for the bias
vector, the extracted bias ˆB(0)
i will have error proportional to
the error of ˆA(0)
i j . And when the bias is wrong, it impacts every
calculation, even those where this edge is not in use.
Resolving this issue completely either requires reducing
the failure rate of gradient estimation from 1 in 10,000 to
practically 0, or would require a complex error-recovery pro-
cedure. Instead, we will introduce in the following section an
improvement which almost completely solves this issue.
# of Parameters
Fidelity
Queries
25,000
50,000
12,500
100,000
100% 100% 100% 99.98%
217.2
220.2
218.2
219.2
Table 6: Fidelity of the functionally-equivalent extraction
attack across different test distributions on an MNIST victim
model. Results are averaged over ﬁve extraction attacks. For
small models, we achieve perfect ﬁdelity extraction; larger
models have near-perfect ﬁdelity on the test data distribution,
but begins to lose accuracy at 100,000 parameters.
Difﬁculties Extending the Attack. The attack is speciﬁc to
two layer neural networks; deeper networks pose multiple
difﬁculties. In deep networks, the critical point search step of
Section 6.3 will result in critical points from many different
layers, and determining which layer a critical point is on is
nontrivial. Without knowing which layer a critical point is on,
we cannot control inputs to the neuron, which we need to do
to recover the weights in Section 6.4. Even given knowledge
of what layer a critical point is on, the inputs of any neuron
past layer 1 are the outputs of other neurons, so we only
have indirect control over their inputs. Finally, even with the
ability to recover these weights, small numerical errors occur
in the ﬁrst layer extraction. These cause errors in every ﬁnite
differences computation in further layers, causing the second
layer to have even larger numerical errors than the ﬁrst (and
so on). Therefore, extending the attack to deeper networks
will require at least solving each of the following: producing
critical points belonging to a speciﬁc layer, recovering weights
for those neurons without direct control of their inputs, and
signiﬁcantly reducing numerical errors in these algorithms.
7 Hybrid Strategies
Until now the strategies we have developed for extraction
have been pure and focused entirely on learning or entirely
on direct extraction. We now show that there is a continuous
spectrum from which we can draw attack strategies, and these
hybrid strategies can leverage both the query efﬁciency of
learning extraction, and the ﬁdelity of direct extraction.
7.1 Learning-Based Extraction with Gradient
Matching
Milli et al. demonstrate that gradient matching helps extrac-
tion by optimizing the objective function
H(O(xi), f (xi)) + α|∇xO(xi)− ∇x f (xi)|2
2,
n
∑
i=1
assuming the adversary can query the model for ∇xO(x). This
is more model access than we permit our adversary, but is an
1356    29th USENIX Security Symposium
USENIX Association
# of Parameters
Fidelity
Queries
50,000
100%
219.2
100,000
200,000
400,000
100% 99.95% 99.31%
220.2
222.2
221.2
Table 7: Fidelity of extracted MNIST model is improved with
the hybrid strategy. Note when comparing to Table 6 the
model sizes are 4× larger.
example of using intuition from direct recovery to improve
extraction. We found in preliminary experiments that this
technique can improve ﬁdelity on small datasets (increasing
ﬁdelity from 95% to 96.5% on Fashion-MNIST), but we leave
scaling and removing the model access assumption of this
technique to future work. Next, we will show another com-
bination of learning and direct recovery, using learning to
alleviate some of the limitations of the previous functionally-
equivalent extraction attack.
7.2 Error Recovery through Learning
Recall from earlier that the functionally-equivalent extraction
attack ﬁdelity degrades as the model size increases. This is
a result of low-probability errors in the ﬁrst weight matrix
inducing incorrect biases on the ﬁrst layer, which in turn
propagates and causes worse errors in the second layer.
We now introduce a method for performing a learning-
based error recovery routine. While performing a fully-
learning-based attack leaves too many free variables so that
functionally-equivalent extraction is not possible, if we ﬁx
many of the variables to the values extracted through the di-
rect recovery attack, we now show it is possible to learn the
remainder of the variables.
Formally, let ˆA(0) be the extracted weight matrix for the ﬁrst
layer and ˆB(0) be the extracted bias vector for the ﬁrst layer.
Previously, we used least squares to directly solve for ˆA(1)
and ˆB(1) assuming we had extracted the ﬁrst layer perfectly.
Here, we relax this assumption. Instead, we perform gradient
descent optimizing for parameters W0..2 that minimize
(cid:13)(cid:13) fθ(x)−W1ReLU( ˆA(0)x + ˆB(0) +W0) +W2
(cid:13)(cid:13)
Ex∈D
That is, we use a single trainable parameter to adjust the
bias term of the ﬁrst layer, and then solve (via gradient descent
with training data) for the remaining weights accordingly.
This hybrid strategy increases the ﬁdelity of the ex-
tracted model substantially, detailed in Table 8. In the worst-
performing example from earlier (with only direct extraction)
the extracted 128-neuron network had 80% ﬁdelity agreement
with the victim model. When performing learning-based re-
covery, the ﬁdelity agreement jumps all the way to 99.75%.
# of Parameters
Transferability
50,000
100%
100,000
100%
200,000
100%
400,000
100%
Table 8: Transferability rate of adversarial examples using the
extracted neural network from our Section 7 attack.
7.2.1 Transferability
Adversarial examples transfer: an adversarial example [47]
generated on one model often fools different models, too.
Transferability is higher when the models are more similar [7].
We should therefore expect that we can generate adversar-
ial examples on our extracted model, and that these will fool
the remote oracle nearly always. In order to measure transfer-
ability, we run 20 iterations of PGD [48] with (cid:96)∞ distortion
set to the value most often used in the literature: for MNIST:
0.1, and for CIFAR-10: 0.03.
The attack achieves functionally equivalent extraction
(modulo ﬂoating point precision errors in the extracted
weights), so we expect it to have high adversarial example
transferability. Indeed, we ﬁnd we achieve a 100% transfer-
ability success rate for all extracted models.
8 Related Work
Defenses for model extraction have fallen into two camps:
limiting the information gained per query, and differentiating
extraction adversaries from benign users. Approaches to lim-
iting information include perturbing the probabilities returned
by the model [11,13,49], removing the probabilities for some
of the model’s classes [11], or returning only the class out-
put [11, 13]. Another proposal has considered sampling from
a distribution over model parameters [13,50]. The other camp,
differentiating benign from malicious users, has focused on
analyzing query patterns [51, 52]. Non-adaptive attacks (such
as supervised or MixMatch extraction) bypass query pattern-
based detection, and are weakened by information limiting.
We demonstrate the impact of removing complete access to
probability values by considering only access to top 5 prob-
abilities from WSL in Table 2. Our functionally-equivalent
attack is broken by all of these measures. We leave considera-
tion of defense-aware attacks to future work.
Queries to a model can also reveal hyperparameters [53] or
architectural information [14]. Adversaries can use side chan-
nel attacks to do the same [18, 25]. These are orthogonal to,
but compatible with, our work—information about a model,
such as assumptions made in Section 6, empowers extraction.
Watermarking neural networks has been proposed [54, 55]
to identify extracted models. Model extraction calls into ques-
tion the utility of cryptographic protocols used to protect
model weights. One unrealized approach is obfuscation [56],
where an equivalent program could be released and queried as
USENIX Association
29th USENIX Security Symposium    1357
many times as desired. A practical approach is secure multi-
party computation, where each query is computed by running
a protocol between the model owner and querier [57].
9 Conclusion
This paper characterizes and explores the space of model
extraction attacks on neural networks. We focus this paper
speciﬁcally around the objectives of accuracy, to measure the
success of a theft-motivated adversary, and ﬁdelity, an often-
overlooked measure which compares the agreement between
models to reﬂect the success of a recon-motivated adversary.
Our learning-based methods can effectively attack a model
with several millions of parameters trained on a billion images,
and allows the attacker to reduce the error rate of their model
by 10%. This attack does not match perfect ﬁdelity with the
victim model due to what we show are inherent limitations of
learning-based approaches: nondeterminism (including only
the nondeterminism on the GPU) prohibits training identical
models. In contrast, our direct functionally-equivalent extrac-
tion returns a neural network agreeing with the victim model
on 100% of the test samples and having 100% ﬁdelity on
transfered adversarial examples.
We then propose a hybrid method which uniﬁes these two
attacks, using learning-based approaches to recover from nu-
merical instability errors when performing the functionally-
equivalent extraction attack.
Our work highlights many remaining open problems in
model extraction, such as reducing the capabilities required
by our attacks and scaling functionally-equivalent extraction.
Acknowledgements
We would like to thank Ilya Mironov for lengthy and fruitful
discussions regarding the functionally equivalent extraction
attack. We also thank Úlfar Erlingsson for helpful discussions
on positioning the work, and Florian Tramèr for his comments
on an early draft of this paper.
References
[1] E. Strubell, A. Ganesh, and A. McCallum, “Energy and
policy considerations for deep learning in nlp,” arXiv
preprint arXiv:1906.02243, 2019.
[2] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhut-
dinov, and Q. V. Le, “Xlnet: Generalized autoregressive
pretraining for language understanding,” in Advances in
neural information processing systems, 2019, pp. 5754–
5764.
[3] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable
effectiveness of data,” 2009.
[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei, “Imagenet: A large-scale hierarchical image
database,” in 2009 IEEE conference on computer vision
and pattern recognition.
Ieee, 2009, pp. 248–255.
[5] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to
sequence learning with neural networks,” in Neural in-
formation processing systems, 2014, pp. 3104–3112.
[6] A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior,
and K. Kavukcuoglu, “Wavenet: A generative model for
raw audio.” SSW, vol. 125, 2016.
[7] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B.
Celik, and A. Swami, “Practical black-box attacks
against machine learning,” in Proceedings of the 2017
ACM on Asia conference on computer and communica-
tions security. ACM, 2017, pp. 506–519.
[8] D. Lowd and C. Meek, “Adversarial learning,” in Pro-
ceedings of the eleventh ACM SIGKDD international
conference on Knowledge discovery in data mining.
ACM, 2005, pp. 641–647.
[9] R. Shokri, M. Stronati, C. Song, and V. Shmatikov,
“Membership inference attacks against machine learn-
ing models,” in 2017 IEEE Symposium on Security and
Privacy (SP).
IEEE, 2017, pp. 3–18.
[10] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz,
and M. Backes, “Ml-leaks: Model and data independent
membership inference attacks and defenses on machine
learning models,” arXiv preprint arXiv:1806.01246,
2018.
[11] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ris-
tenpart, “Stealing machine learning models via pre-
diction apis,” in 25th {USENIX} Security Symposium
({USENIX} Security 16), 2016, pp. 601–618.
[12] T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets:
Stealing functionality of black-box models,” in Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2019, pp. 4954–4963.
[13] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha,
and S. Yan, “Model extraction and active learning,”
CoRR, vol. abs/1811.02054, 2018. [Online]. Available:
http://arxiv.org/abs/1811.02054
[14] S. J. Oh, M. Augustin, B. Schiele, and M. Fritz, “To-
wards reverse-engineering black-box neural networks,”
arXiv preprint arXiv:1711.01768, 2017.
[15] S. Pal, Y. Gupta, A. Shukla, A. Kanade, S. K. Shevade,
and V. Ganapathy, “A framework for the extraction
of deep neural networks by leveraging public data,”
1358    29th USENIX Security Symposium
USENIX Association
CoRR, vol. abs/1905.09165, 2019. [Online]. Available:
http://arxiv.org/abs/1905.09165
in Proceedings of the European Conference on Com-
puter Vision (ECCV), 2018, pp. 181–196.
[16] J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F.
de Souza, and T. Oliveira-Santos, “Copycat cnn: Steal-
ing knowledge by persuading confession with random
non-labeled data,” in 2018 International Joint Confer-
ence on Neural Networks (IJCNN).
IEEE, 2018.
[17] C. Song and V. Shmatikov, “Overlearning reveals sensi-
tive attributes,” arXiv preprint arXiv:1905.11742, 2019.
[18] S. Hong, M. Davinroy, Y. Kaya, S. N. Locke, I. Rackow,
K. Kulda, D. Dachman-Soled, and T. Dumitra¸s, “Secu-
rity analysis of deep neural networks operating in the
presence of cache side-channel attacks,” arXiv preprint
arXiv:1810.03487, 2018.
[19] S. Milli, L. Schmidt, A. D. Dragan, and M. Hardt,
“Model reconstruction from model explanations,” arXiv
preprint arXiv:1807.05185, 2018.
[20] V. Nair and G. E. Hinton, “Rectiﬁed linear units im-
prove restricted boltzmann machines,” in Proceedings
of the 27th international conference on machine learn-
ing (ICML-10), 2010, pp. 807–814.
[21] Y. E. Nesterov, “A method for solving the convex pro-
gramming problem with convergence rate o (1/kˆ 2),” in
Dokl. akad. nauk Sssr, vol. 269, 1983, pp. 543–547.
[22] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient
methods for online learning and stochastic optimization,”
Journal of Machine Learning Research, vol. 12, no. Jul,
pp. 2121–2159, 2011.
[23] D. P. Kingma and J. Ba, “Adam: A method for stochastic
optimization,” arXiv preprint arXiv:1412.6980, 2014.
[24] G. Hinton, O. Vinyals, and J. Dean, “Distilling the
knowledge in a neural network,” arXiv preprint
arXiv:1503.02531, 2015.
[25] L. Batina, S. Bhasin, D. Jap, and S. Picek, “Csi neu-
ral network: Using side-channels to recover your ar-
tiﬁcial neural network information,” arXiv preprint
arXiv:1810.09076, 2018.
[26] P. Kocher, J. Jaffe, and B. Jun, “Differential power anal-