D
4,017
89K

D
25,287
566K

D
13,384
311K

D
7.9M
338,581

28.9M 1,097,993 D

D
15.4M 396,555

D
2.2M
115,980

D
99
15K

D
4,091
122K

D
178,339
6.1M

D
185K
7,367
























Table 9: Closed-source binaries tested successfully with ZAFL. L/W = Lin-
ux/Windows; D/I = position-dependent/independent; Sym = binary is non-
stripped; Opt = whether register liveness-aware optimization succeeds.
speed. While liveness false positives introduce overhead from
the additional instructions needed to save/restore registers,
liveness false negatives may leave live registers erroneously
overwritten—potentially breaking program functionality. If
ZAFL’s liveness analysis (§ 5.2.4) cannot guarantee correct-
ness, it conservatively halts this optimization to avoid false
negatives, and instead safely inserts code at basic block starts.
To assess the impact of skipping register liveness-aware
optimization, we replicate our overhead evaluation (§ 7.3.4)
to compare ZAFL’s speed with/without liveness-aware instru-
mentation. As Figure 6 shows, liveness-unaware ZAFL faces
31% more overhead across all eight benchmarks. While 13–
16% slower than AFL-Dyninst on bsdtar and sfconvert,
ZAFL’s unoptimized instrumentation still averages 25% and
193% less overhead than AFL-Dyninst and AFL-QEMU, re-
spectively. Thus, even in the worst case ZAFL generally out-
performs other binary-only fuzzing instrumenters.
Figure 6: A comparison of ZAFL’s runtime overhead with and without register
liveness-aware instrumentation optimization (lower is better).
As Table 8 and Table 9 show, we successfully apply
liveness-aware instrumentation for all 44 Linux benchmarks.
We posit that with further engineering, the same robustness is
achievable for Windows binaries.
7.5.3
Instruction Recovery
Recovery of the original binary’s full
instructions is
paramount to static rewriting. It is especially important for
binary-only fuzzing, as false positive instructions misguide
coverage-guidance; while false negatives introduce coverage
blind-spots or break functionality. Further, precise instruc-
tion recovery heads fuzzing-enhancing transformation, as it is
necessary to know where/how to modify code (e.g., targeting
cmp’s for sub-instruction proﬁling (§ 6.2.1)).
Binary
Total
Insns
IDA Pro
Binary Ninja
ZAFL
Unrecov
Reached
FalseNeg
Unrecov
Reached
FalseNeg
Unrecov
Reached
FalseNeg
0
268K
idat64
nconvert 458K
nvdisasm 162K
pngout
unrar
1681 0
5342 2
105K 3117 0.68% 3569 0
0
180
16.8K 645
0
37.8K 1523 0
0
0
0
0
0
958
0
33.0K 0
0
112.5 0.67% 1724 0
0
3814 21.4 0.01% 0
752
1941 138.2 0.37% 40
0
0
0
0
0
Table 10: Instruction recovery statistics for IDA Pro, Binary Ninja, and
ZAFL, with ground-truth disassembly from LLVM-10’s objdump. Reached
= mean unrecovered instructions reached by fuzzing (hence, erroneously-
unrecovered); FalseNeg = erroneously-unrecovered instructions over total.
We evaluate ZAFL’s instruction recovery using ground-
truth disassemblies of binary .TEXT sections generated by
objdump, which is shown to achieve ∼100% accuracy [5]
(speciﬁcally, we use the version shipped in LLVM-10 [53]).
To see how ZAFL fairs with respect to the state-of-the-art in
binary analysis, we also evaluate disassemblies of the com-
mercial tools IDA Pro 7.1 and Binary Ninja 1.1.1259. As all
three only recover instructions they deem “reachable”, we
compute false negative recovery rates from the mean number
USENIX Association
30th USENIX Security Symposium    1695
bsdtarcert-basicclean_textjasperreadelfsfconverttcpdumpunrtfAVG.Benchmark1.01.21.41.61.82.02.22.42.6Overhead Rel. to BaselineLiveness-AwareLiveness-Unawareof unique unrecovered instructions that are actually reached
among ﬁve 24-hour fuzzing campaigns per benchmark.
Table 10 lists the total instructions; and total and reached
unrecovered instructions per our ﬁve closed-source bench-
marks.3 As we observe zero false positives for any tool on
any benchmark, we focus only on false negatives. Though
all three achieve near-perfect accuracy, ZAFL is the only to
maintain a 0% false negative rate among all benchmarks, as
IDA and Binary Ninja erroneously unrecover an average of
0–0.68% of instructions. While static rewriting is fraught
with challenges—many of which require further engineering
work to overcome (§ 8.3)—these results suggest that ZAFL’s
common-case instruction recovery is sound.
7.5.4 Control-ﬂow Recovery
Preserving the original binary’s control-ﬂow is critical to
fuzzing’s coverage-guidance. Excessive false positives add
noise that misguide fuzzing or overwhelm its seed scheduling
processes; while false negatives may cause fuzzing to over-
look entire code regions or bug-triggering paths. To examine
ZAFL’s control-ﬂow recovery, we run all test cases generated
over ﬁve 24-hour trials for our eight open-source benchmarks
on both a ZAFL- and a ground-truth LLVM-instrumented
binary, and log when each report new coverage.
Binary
bsdtar
cert-basic
clean_text
jasper
readelf
sfconvert
tcpdump
unrtf
Mean
Coverage TPR
Coverage TNR
Coverage Accuracy
97.28%
96.67%
96.39%
98.82%
99.98%
98.71%
96.51%
94.17%
97.30%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
100.00%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
>99.99%
100.00%
Table 11: ZAFL’s fuzzing code coverage true positive and true negative rates,
and accuracy with respect to the LLVM compiler over 5×24-hour trials.
As Table 11 shows, ZAFL’s coverage identiﬁcation is near-
identical to LLVM’s: achieving 97.3% sensitivity, ∼100%
speciﬁcity, and ∼100% accuracy. While ZAFL encounters
some false positives, they are so infrequent (1–20 test cases
out of 1–20 million) that the total noise is negligible. In in-
vestigating false negatives, we see that in only 7/40 fuzzing
campaigns do missed test cases precede bug-triggering paths;
however, further triage reveals that ZAFL eventually ﬁnds re-
placement test cases, thus, ZAFL reaches every bug reached by
LLVM. Thus, we conclude that ZAFL succeeds in preserving
the control-ﬂow of compiler-generated code.
8 Limitations
Below we brieﬂy discuss limitations unique to ZAFL, and
others fundamental to static binary rewriting.
3We omit results for our eight open-source benchmarks as all three tools
achieve a 0% false negative instruction recovery rate on each.
Improving Baseline Performance
8.1
Our performance evaluation § 7.3.4 shows ZAFL’s baseline
(i.e., non-tracing) overhead is around 5%. We believe that our
rewriter’s code layout algorithm is likely the biggest contribut-
ing factor to performance and have since tested experimental
optimizations that bring baseline overhead down to ∼1%. But
as ZAFL’s full fuzzing performance is already near modern
compiler’s, we leave further optimization and the requisite
re-evaluation to future work.
8.2 Supporting New Architectures, Formats,
and Platforms
Our current ZAFL prototype is limited to x86-64 C/C++ bi-
naries. As our current static rewriting engine handles both
32- and 64-bit x86 and ARM binaries (as well as prototype
32-bit MIPS support), we believe supporting these in ZAFL is
achievable with future engineering work.
Extending to other compiled languages similarly depends
on the rewriter’s capabilities. We have some experimental
success for Go/Rust binaries, but more ZAFL-side engineering
is needed to achieve soundness. We leave instrumenting non-
C/C++ languages for future work.
While ZAFL is engineered with Linux targets in mind, our
evaluation shows it also supports many Windows applications;
few other static binary rewriters support Windows binaries.
Though we face some challenges in precise code/data disam-
biguation and at this time are restricted to Windows 7 64-bit
PE32+ formats, we expect that with future rewriter-level en-
hancements, ZAFL will achieve broader success across other
Windows binary formats and versions.
8.3 Static Rewriting’s Limitations
Though static rewriting’s speed makes it an attractive choice
over dynamic translation for many binary-only use cases and
matches what compilers do, static rewriting normally fails
on software crafted to thwart reverse engineering. Two such
examples are code obfuscation and digital rights management
(DRM) protections—both of which, while uncommon, ap-
pear in many proprietary and commercial applications. While
neither ZAFL nor its rewriter currently support obfuscated
or DRM-protected binaries, a growing body of research is
working toward overcoming these obstacles [12, 90]. Thus,
we believe that with new advances in binary deobfuscation
and DRM-stripping, ZAFL will be able to bring performant
binary-only fuzzing to high-value closed-source targets like
Dropbox, Skype, and Spotify.
Another grey area for static binary rewriters is deprecated
language constructs. For example, C++’s dynamic exception
speciﬁcation—obsolete as of C++11—is unsupported in ZAFL
and simply ignored. We recognize there are trade-offs be-
tween static binary rewriting generalizability and precision,
1696    30th USENIX Security Symposium
USENIX Association
and leave addressing such gaps as future work.
Most modern static binary rewriters perform their core
analyses—disassembly, code/data disambiguation, and indi-
rect branch target identiﬁcation—via third-party tools like
Capstone [67] and IDA [39], consequently inheriting their
limitations. For example, if the utilized disassembler is not
up-to-date with the latest x86 ISA extension, binaries con-
taining such code cannot be fully interpreted. We posit that
trickle-down dependency limitations are an inherent prob-
lem to modern static binary rewriting; and while perfection
is never guaranteed [59, 69], most common roadblocks are
mitigated with further heuristics or engineering.
9 Related Work
Below we discuss related works in orthogonal areas static
rewriting, fuzzing test case generation, hybrid fuzzing, and
emergent fuzzing transformations.
9.1 Static Binary Rewriting
Static rewriters generally differ by their underlying method-
ologies. Uroboros [87], Ramblr [86], and RetroWrite [26]
reconstruct binary assembly code “reassembleable” by com-
pilers. Others translate directly to compiler-level intermediate
representations (IR); Hasabnis et. al [40] target GCC [34]
while McSema [25], SecondWrite [4], and dagger [15] focus
on LLVM IR. GTIRB [38] and Zipr [46] implement their own
custom IR’s. We believe static rewriters with robust, low-level
IR’s are best-suited to supporting ZAFL.
9.2
Improving Fuzzing Test Case Generation
Research continues to improve test case generation from
many perspectives. Input data-inference (e.g., Angora [18],
VUzzer [68], TIFF [49]) augments mutation with type-
/shape characteristics. Other works bridge the gap between
naive- and grammar-based fuzzing with models inferred stat-
ically (e.g., Shastry et. al [71], Skyﬁre [84]) or dynamically
(e.g., pFuzzer [58], NAUTILUS [6], Superion [85], AFLS-
mart [66]). Such approaches mainly augment fuzzing at the
mutator-level, and thus complement ZAFL’s compiler-quality
instrumentation in binary-only contexts.
Another area of improvement
is path prioritization.
AFLFast [14] allocates mutation to test cases exercising deep
paths. FairFuzz [54] focuses on data segments triggering
rare basic blocks. VUzzer [68] assigns deeper blocks high
scores to prioritize test cases reaching them; and QTEP [88]
similarly targets code near program faults. ZAFL’s feedback-
enhancing transformations result in greater path discovery,
thus increasing the importance of smart path prioritization.
9.3 Hybrid Fuzzing
Many recent fuzzers are hybrid: using coverage-guided
fuzzing for most test cases but sparingly invoking more heavy-
weight analyses. Angora [18] uses taint tracking to infer muta-
tion information, but runs all mutatees in the standard fuzzing
loop; REDQUEEN [7] operates similarly but forgoes taint
tracking for program state monitoring. Driller’s [74] concolic
execution starts when fuzzing coverage stalls; QSYM’s [92]
instead runs in parallel, as do DigFuzz’s [94] and SAV-
IOR’s [19], which improve by prioritizing rare and bug-
honing paths, respectively. While this paper’s focus is ap-
plying performant, compiler-quality transformations to the
standard coverage-guided fuzzing loop, we imagine leverag-
ing ZAFL to also enhance the more heavyweight techniques
central to hybrid fuzzing.
9.4 Emergent Fuzzing Transformations
LLVM [53] offers several robust “sanitizers” useful for soft-
ware debugging . In fuzzing, sanitizers are typically reserved
for post-fuzzing crash triage due to their performance bloat;
but recently, several works achieve success with sanitizers
intra-fuzzing: AFLGo [13] compiles binaries with Address-
Sanitizer for more effective crash-ﬁnding; Angora [18] builds
its taint tracking atop DataFlowSanitizer [78]; and SAV-
IOR [19] uses UndeﬁnedBehaviorSanitizer to steer concolic
execution toward bug-exercising paths. We thus foresee in-