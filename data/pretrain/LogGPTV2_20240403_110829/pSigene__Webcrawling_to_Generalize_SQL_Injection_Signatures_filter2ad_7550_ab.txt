the feature sources is presented in Table II.
The SQL reserved words are used as features since they
represent identiﬁers or functions, necessary to create SQL
queries like in SQLi attacks. Examples of reserved words
used to create the feature set for SQLi attacks include
SELECT, DELETE, CURRENT_USER, and VARCHAR. For
this paper, we limited the feature set to only include the re-
served words for the MySQL database management system,
thus excluding special-purpose keywords used in Microsoft
SQL and other non MySQL databases.
We also looked at existing signatures for features since
the signatures are the result of a usually long optimization
process, so it could be assume that these signatures have
components (strings inside a signature) that can be used as
features to help identify attacks. We did not use a whole
signature as a single feature, but rather divided the signature
into logical components and each component then was used
as a feature. To achieve this, we used metacharacters such
as parentheses () and the alternation operator | that delimit
logical groups and branches inside a regular expression.
As an example,
let’s consider a signature taken from
the ModSecurity Core Rule Set
(CRS), deﬁned as a
regular expression with seven case insensitive groups,
joined by the alternation operator: (?:)|
. . . |(?:is\s+null)|
(?:like\s+null)|
In this case, we created seven
. . . |(?:).
features, one for each regex group in the signature.
Our choice allowed for our system to also consider
the relative position of SQL tokens among them, when
creating the features. As an example, feature =[0-9%]+
only considers a number if it is preceeded by the = character.
All features included in the set were of numeric type, each
one measuring the number of times a feature was found
in an attack sample. The resulting feature set used in the
experiments had 159 entries (from an initial set of 477),
after removing those features that were not found in any of
the samples used in the training phase of the system. The
removed features also corresponded to cases for attacks to
non-MySQL databases (not considered in our experiments)
or because of multiple features looking for similar SQLi
strings (overlapping features).
70 (out of 159) entries in the resulting feature set per-
formed as binary features. That is, the value for each of
these features was either one (conﬁrming the existence of
the corresponding SQL token or string in a sample) or a
zero (non existence) in each of the attack samples.
The process of creating the feature set might at ﬁrst blush
seem intensely manual. But in our experience, the process
was automatable for the most part. Both the reserved words
and the fragmentation of the existing signatures (rows 1 and
2 in Table II) could be automated since they follow from
unambiguous rules. In the case of analyzing the reference
documents, this was partially automated and served more to
validate features created with the other sources. Additionally,
we believe that
the feature space was exhausted so the
creation of the feature set should be considered a one-time
task, for each kind of attack (such as SQLi).
We also considered using only binary features, i.e., the
binary ﬂag whether a feature is present or absent in a sample,
rather than its count. However, this did not produce good
results.
Each attack sample that provides the input to the cluster-
ing algorithm later used is characterized by its values for
the 159 features. The resulting data is organized in a matrix
where the samples are the rows of the matrix and the features
are the columns. The size of the matrix was then 30,000 by
159 and can be classiﬁed as sparse because 85% of its cells
were populated with zeroes. About 6% of its cell values
were ones.
C. Creating Clusters for Similar Attack Samples
We use the biclustering technique [30] to analyze our
matrix, which is popularly used in gene expression data
analysis. The objective of this technique is to identify
blocks in the sample dataset built by a subset of features to
characterize a subset of samples. Given a set of m rows and
n columns (i.e., an m×n matrix), the biclustering algorithm
generates biclusters - a subset of rows which exhibit similar
behavior across a subset of columns. To achieve this, the
biclustering technique ﬁrst clusters the rows (samples) of
484848
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:01:01 UTC from IEEE Xplore.  Restrictions apply. 
the matrix and then clusters the columns (features) of the
row-clustered data.
To formalize the concept of bicluster, a sample set D is
given as a |N| × |F| matrix where N is the set of samples
and F is the set of features. The elements dij of the matrix
indicate the relationship between sample i and feature j.
Then, a bicluster BRC is a block that includes a subset of
the rows R ⊆ N and a subset of columns C ⊆ F , sharing
one or more similarity properties.
The objective of using biclustering is to identify subsets
of attack samples which share similar values for a subset
of features. Each subset of samples (cluster) may use dif-
ferent sets of features. We want to create a signature for
each bicluster and the biclustering technique allows using
different features for different biclusters. This enables us to
create compact and distinctive signatures for the wide variety
of SQLi attacks. The biclusters are nonoverlapping (i.e., no
two biclusters have spatial overlap) and nonexclusive (i.e.,
two biclusters may use overlapping set of features) (Figure
2). The heatmap shows eleven clusters that are formed, by
visual analysis of the color patterns. A contiguous region
with one color pattern constitutes one cluster. Note that not
all features are used in the cluster formation; thus, there are
some gaps for the feature dimension when you consider all
the clusters. Note also that not all samples are covered in a
cluster, indicating that some attack samples are considered
so different that they do not ﬁt within any cluster. This may
indicate that our training set has some noise in it. Being able
to deal with some noise in a training set is an important
property for any machine learning algorithm and we are
heartened to see that that is the case with pSigene.
We use a simple approach to achieve the biclustering
technique, performing a two-way hierarchical agglomerative
clustering (HAC) algorithm, using the Unweighted Pair
Group Method with Arithmetic Mean (UPGMA). The way
biclustering worked is ﬁrst it did a clustering of the samples
and then within each cluster, it clustered by the features.
Thus, it identiﬁed what were the discriminating features for
each cluster.
Table III
FEATURES INCLUDED IN SIGNATURE 6
FEATURE
NUMBER
25
37
53
36
28
32
FEATURE (Regular Expression)
=
=[-0-9\%]*
|r?like|sounds\s+like|regex
([ˆa-zA-Z&]+)?&|exists
[\?&][ˆ\s\t\x00-\x37\|]+?
\)?;
its results will guide us to pick the multiple clusters as we
explain below.
The results from applying the biclustering technique to the
webcrawled dataset are presented as a heat map in Figure
2. On each axis, the corresponding dendrograms are also
shown. The heat map shows the graphical representation of
the reordering of the matrix |N|×|F| into a set of bi-clusters.
Each bicluster is represented as an area of similar color
as the heat map simultaneously exposes the hierarchical
cluster structure of both rows and columns, as explained in
[10]. Each column in the matrix is standardized as follows:
the statistical mean and standard deviation of the values is
computed. The mean is then subtracted from each value and
the result divided by the standard deviation. As a value is
closer to the mean, it is shown with the black color in the
heat map. The highest and the lowest values are shown in red
and green, respectively. Figure 2 also shows the dendrograms
produced by the HAC algorithm for both rows (sample set)
and columns (feature set).
To validate the accuracy of the HAC algorithm, we also
calculated the cophenetic correlation coefﬁcient for each
dendrogram. The cophenetic correlation for a cluster tree
is deﬁned as the linear correlation coefﬁcient between the
cophenetic distances obtained from the tree, and the original
distances (or dissimilarities) used to construct the tree. Thus,
it is a measure of how faithfully the tree represents the
dissimilarities among observations. In our experiments, we
found the cophenetic correlation coefﬁcient value of 0.92, a
promisingly high number.
Ultimately, the above-mentioned explorations of the de-
sign space required visual inspection of multiple heatmaps
rather than the alternative: use of multiple security experts
and an almost zen master-like grasp of regular expressions.
D. Creation of Generalized Signatures
From each bicluster bj, we create a signature Sigbj which
characterizes the samples in that bicluster, plus is more
generalized. Speciﬁcally, in our solution, a signature Sigbj
is a logistic regression model built to predict whether an
SQL query is an attack similar to the samples in cluster bj.
Logistic regression is a very popular classiﬁcation method
since the output values for the hypothesis function, lay in
the range between 0 and 1. These values are interpreted as
the estimated probability that a sample belongs to a class.
494949
The UPGMA algorithm produces a hierarchical
tree,
usually presented as a dendrogram, from which clusters
can be created. It works in a bottom-up (agglomerative)
approach by ﬁrst partitioning the sample set of size N into
N clusters, each one containing a single sample. Then, the
Euclidean pairwise distance is calculated among the initial,
single sample clusters in order to merge the two closest ones.
After the ﬁrst round of paired clusters ﬁnishes, UPGMA
is used to recursively merge the clusters. At each step, the
nearest two clusters are combined into a higher-level cluster.
The distance between any two clusters A and B is taken to
be the average of all distances between pairs of objects ”x”
in A and ”y” in B. This biclustering process is repeated
until a single cluster containing all the samples is formed.
Note that this is just the termination point from biclustering;
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:01:01 UTC from IEEE Xplore.  Restrictions apply. 
	


	
	













	
Figure 2. Heat map with two dendrograms of the matrix data representing the samples dataset. The 30,000 attack samples are the rows and the 159
features are the columns. The heat map also shows the eleven biclusters selected. Signatures were not produces for biclusters 9 and 10 as they were ’black
holes’ (did not contained sufﬁcient training data).
Each bicluster bj is deﬁned by a set of features Fj and
a set of samples Sj. We then create the corresponding
signature Sigbj of each bicluster by using the features as the
variables in the hypothesis function and training this function
with the samples from the bicluster, as well as normal trafﬁc.
pSigene calculates the parameters Θj (which is a vector
of individual parameter values), using the labeled data of
attack samples from cluster bj as well as benign network
trafﬁc data. The intuition behind the calculation of Θj is
that it should minimize the errors in the labeled training set.
Having calculated Θj, let us see how pSigene would work
during the operational phase (the test phase). When a sample
i is available to pSigene, to determine if it belongs to attack
class j, it calculates the value of the hypothesis function:
hθ(Fij) = g(ΘT
j Fij)
where Fij represents the values of sample i for the feature
set Fj. We use for g the sigmoid function which is deﬁned
as:
g(z) =
1
1 + e
−z
This gives a value between 0 and 1 and is interpreted as
the probability that the test sample i belongs to attack class
j.
We used the Preconditioned Conjugate Gradients (PCG)
method [11] to ﬁnd the optimal parameters Θ of the regres-
sion model for each bicluster.
As an example of how we used logistic regression in the
SQL injection attack scenario, consider a bicluster b6 ob-
tained after running the biclustering algorithm. This bicluster
505050
has a set S6 of 2, 741 samples and a set F6 of the features
listed in Table III. After training with the set S6 (attack
class) and one day of non-malicious trafﬁc (other class), we
compute the parameters Θ6 of the generalized signature for
bicluster S6:
6 = −3.761054 + 0.262131f6,25 + 0.262131f6,37
ΘT
+ 0.261463f6,53 + 0.261584f6,36
− 0.117270f6,28 + 0.708324f6,32
III. EVALUATION
We implemented pSigene using the popular Bro IDS and
evaluated it along the signatures in three other IDSes by
using SQL attack samples and benign web trafﬁc. We also
compared our technique to another proposed algorithm for
signature generation.
A. SQLi Signature Sets
We analyzed three different sets of SQLi signatures, taken
from two popular open-source IDS (Snort and Bro) and
one web application ﬁrewall (ModSec). A summary of the
different signatures used in the evaluation is presented in
Table IV. The fact that some of the SQLi rules are disabled
by default in some of the IDSes may indicate the perception
that there exists overlaps between rules. The high usage of
regex is because it holds the promise that a regex will be able
to match a wide set of attacks. This observation motivated
us to build on regex’s in choosing the features in pSigene.
A description of each signature set follows:
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:01:01 UTC from IEEE Xplore.  Restrictions apply. 
COMPARISON BETWEEN DIFFERENT SQLI RULESETS.
Table IV
RULES
DISTRIBUTION
VERSION
NUMBER
SQLi RULES
SQLi RULES
ENABLED
USAGE
OF REGEX
Bro
Snort Rules
Emerging
Threats
ModSecurity
2.0
2920
7098
2.2.4
6
79
4231
34
100%
61%
0%
100%
100%
82%
99%
100%
Bro A network analysis framework, Bro can be used as
a signature-based IDS. It comes with a set of signatures to
perform low-level pattern matching. We analyzed the 6 SQLi
rules present on Bro v2.0 [28] to detect SQLi attacks. All
six of the rules make extensive usage of regular expressions.
Snort / Emerging Threats (ET) Snort is an open source
network IDS that performs packet-level analysis and comes
with its own ruleset. Emerging Threats is an open source
project that publishes detection rulesets for IDS such as
Snort. For our experiments, we merged Snort version 2920
[40] and ET version 7098 rulesets [12]. Over 98% of those
rules use simple regular expressions.
ModSecurity (shortened as “ModSec”) is a web applica-
tion ﬁrewall (WAF) used to protect Apache web servers from
attacks such as SQLi. The OWASP ModSecurity Core Rule
Set (ModSec CRS) project is an open-source initiative to
provide the signatures used by ModSecurity to detect attacks
to web applications. We analyzed CRS version 2.2.4.
Snort and Bro use a deterministic approach to handle
the signatures. In other words, these systems produce an
alert only if all the requisites deﬁned in a signature are
met. In contrast, ModSecurity takes a probabilistic approach
and uses a scoring scheme where signatures are weighted
and can contribute to determine the level of anomaly for a
particular trace.
The average length of the 6 signatures (regular expres-
sions) found in Bro was 247.7 characters (max: 429, min:
27). Meanwhile, regular expressions in ModSecurity had an
average length of 390.2 characters (max: 2917, min: 28) and
in Snort were 27.1 (max: 40.1, min: 5).
For all the experiments presented in this paper, we con-
sidered only those signatures that used or included regular
expressions, which was the overwhelming majority of the
rules. We did this to allow for a fairer comparison between
pSigene and the other IDSes, since pSigene uses regular
expressions for its features.
B. Datasets
For training, we used the 30,000 attack samples collected
by crawling public sources (as detailed in Section II-A) and
240,000 HTTP samples for normal trafﬁc. We used three
test datasets to evaluate the performance of the different