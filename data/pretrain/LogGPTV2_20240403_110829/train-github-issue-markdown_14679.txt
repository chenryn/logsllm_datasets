pytorch 1.0  
DOC err:torch.nn.CrossEntropyLoss  
if weight=NONE  
loss(x,class)=−x[class]+log(j∑​exp(x[j])) ,The result is correct.
if weight!=NONE  
loss(x,class)=weight[class]*(−x[class]+log(j∑​exp(x[j]))) ,The result is
wrong?
* * *
## import torch  
import torch.nn.functional as F  
a=[[-5.,-6.],[3.,2.],[3.,2.]]  
c=[1,0,1]  
b=[3.,2.]  
input=torch.tensor(a,requires_grad=True)  
target = torch.tensor(c)  
weight = torch.tensor(b)  
output = F.cross_entropy(input,target,weight,reduce=True,size_average=True)  
z=output.sum()  
z.backward()  
print(output,input.grad)
tensor(0.8847, grad_fn=) tensor([[ 0.2089, -0.2089],  
[-0.1153, 0.1153],  
[ 0.2089, -0.2089]])