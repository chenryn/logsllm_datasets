### Increasing Computation Density and Its Impact on Hardware Reliability

Increasing the computation density by adding more components increases the risk of hardware failures [4]. To address the limitations in parallel programming, consolidating Virtual Machines (VMs) onto a single hosting platform is an alternative to increasing computation density. This study examines whether increasing the number of VMs leads to reliability issues by comparing the trend of average failure rates across VMs with different consolidation levels. Specifically, we consider the average weekly failure rates. Since the consolidation level experienced by VMs changes over time due to VMs being turned off and migrated, we propose estimating it by the average monthly consolidation level of a VM computed over a one-year observation period.

#### Analysis of VM and Failure Distributions

First, we analyze the distribution of VMs and their failure rates across consolidation levels ranging from 1 to 32. We observe that the number of VMs increases with the consolidation level, from 0.6% for a consolidation level of 1 (i.e., the VM does not share the hosting platform with other VMs) to 30% and 32% for consolidation levels of 16 and 32, respectively. Figure 9 depicts the average failure rates and the corresponding 25th and 75th percentiles. The data clearly shows that the failure rate decreases significantly with the level of consolidation. This can be attributed to the fact that the underlying machines hosting more VMs are high-end systems equipped with more reliable components and built-in fault-tolerance features. This observation is consistent with our previous findings on the relationship between Physical Machine (PM) utilization levels and failure rates. Additionally, combining the increasing failure rates with the increasing CPU utilization of VMs, we conclude that PMs hosting a decent number of underutilized VMs result in low failure rates for both PMs and VMs. Our findings confirm that virtualization not only resolves resource over-provisioning issues in today’s data centers but also potentially enhances system reliability.

#### Impact of VM On/Off Frequency

The impact of wear and tear on hardware components is well-documented [16], and frequent on/off cycles can reduce the lifespan of PMs. In contrast, VMs are designed for elastic resource provisioning and can be managed on demand, i.e., turned on or off as needed by users. Indeed, VMs experience frequent on/off cycles [17]. We are interested in understanding how robust VMs are against frequent on/off cycles, similar to PMs.

To this end, we compare the weekly failure rates of VMs experiencing different levels of weekly on/off frequencies. For each VM, we collect its average weekly failure rate over a one-year observation period and its average weekly on/off frequency, computed over a two-month observation period. Note that to track the on/off frequency, we use fine-grained 15-minute data points, for which we have only two months of data. Given this limitation, we assume that the observed VM on/off frequencies are consistent throughout the entire year.

Figure 10 shows the VM failure rates with respect to the average weekly on/off frequency. We observe that the number of VMs decreases with the on/off frequency, with 60% of all VMs being turned on/off at most once per month and only 14% being powered on/off 8 times per month. When focusing on the majority of VMs, i.e., those with an average on/off frequency of less than twice per month, we see an increasing trend in average failure rates from 0.002 to 0.0035. However, when considering the entire range of on/off frequencies, the average failure rates vary without any clear trend. This suggests that the VM on/off frequency has a certain impact on VM reliability, especially from no on/off to 2 on/offs per month. However, our findings do not indicate that very frequent on/offs of VMs will significantly deteriorate their reliability.

### Summary and Conclusion

We summarize our key findings as follows:

- **Differences in PM/VM Failures**: VMs have lower failure rates and lower recurrent failure probabilities than PMs. Inter-failure times of VMs follow a Gamma distribution, similar to PMs. Software inter-failure times are the shortest, compared to hardware/infrastructure-related ones. The average repair time for VM failures is almost half that of PMs and follows a Log-normal distribution. Hardware failures take the longest to repair. VM failures show a weak positive trend with age.
  
- **Impact of Resource Usage and Capacity on PM/VM Failures**: Resource utilization is more critical than capacity for PM failures, particularly CPU utilization. Key resource attributes affecting VM failures are CPU utilization and the number of disks, while disk capacity has the least impact. Most PMs show increasing failure rates with CPU utilization, while most VMs show a decreasing trend.
  
- **Impact of VM Management on VM Failures**: VM failure rates decrease with the consolidation level. Systems consolidating a fair number of underutilized VMs result in lower failure rates. Frequent turning on/off of VMs does not seem to have a significant impact on VM failures.

We conducted a failure analysis on PMs and VMs hosted on commercial data centers using one-year-long data collected over 10K servers. Our analysis highlights the differences and similarities in PM and VM failure patterns and their correlations with resource capacity and usage. While some of our findings on PMs confirm previous studies, our findings on VMs provide fresh perspectives and insights. Overall, VMs have lower failure rates than PMs, and increasing the computation intensity by VM unit does not increase the failure rate, contrary to PMs.

### Acknowledgements

This work was partly funded by the EU Commission under the FP7 GENiC project (Grant Agreement No 608826).

### References

[1] L. Barroso, J. Dean, and U. Hölzle, “Web search for a planet: The Google cluster architecture,” IEEE Micro, vol. 23, no. 2, pp. 22–28, Mar. 2003.

[2] E. B. Nightingale, J. R. Douceur, and V. Orgovan, “Cycles, cells and platters: An empirical analysis of hardware failures on a million consumer PCs,” in Proceedings of the Sixth Conference on Computer Systems, ser. EuroSys ’11, 2011, pp. 343–356.

[3] K. V. Vishwanath and N. Nagappan, “Characterizing cloud computing hardware reliability,” in Proceedings of the 1st ACM Symposium on Cloud Computing, ser. SoCC ’10, 2010, pp. 193–204.

[4] B. Schroeder and G. A. Gibson, “A large-scale study of failures in high-performance computing systems,” in Proc. DSN, 2006, pp. 249–258.

[5] N. El-Sayed and B. Schroeder, “Reading between the lines of failure logs: Understanding how HPC systems fail,” in Proc. DSN, 2013, pp. 1–12.

[6] “http://www.informationweek.com/data-center-outages-generate-big-losses/d/d-id/1097712?”

[7] W. Jiang, C. Hu, Y. Zhou, and A. Kanevsky, “Are disks the dominant contributor for storage failures - a comprehensive study of storage subsystem failure characteristics,” ACM Transactions on Storage, vol. 4, no. 3, 2008.

[8] E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large disk drive population,” in Proc. FAST, 2007, pp. 17–28.

[9] B. Schroeder, S. Damouras, and P. Gill, “Understanding latent sector errors and how to protect against them,” in Proc. FAST, 2010, pp. 71–84.

[10] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. K. Sahoo, “Blue Gene/L failure analysis and prediction models,” in Proc. DSN, 2006, pp. 425–434.

[11] D. Yuan, J. Zheng, S. Park, Y. Zhou, and S. Savage, “Improving software diagnosability via log enhancement,” ACM Trans. Comput. Syst., vol. 30, no. 1, p. 4, 2012.

[12] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild: A large-scale field study,” Commun. ACM, vol. 54, no. 2, pp. 100–107, 2011.

[13] R. K. Sahoo, A. Sivasubramaniam, M. S. Squillante, and Y. Zhang, “Failure data analysis of a large-scale heterogeneous server environment,” in Proc. DSN, 2004, pp. 772–781.

[14] HP OpenView, “http://support.openview.hp.com/.”

[15] IBM Monitoring, Tivoli, “http://ibm.com/software/tivoli/products/monitor/.”

[16] K. Trivedi, Probability and Statistics with Reliability, Queuing, and Computer Science Applications. Wiley, 2001.

[17] R. Birke, A. Podzimek, L. Y. Chen, and E. Smirni, “State-of-the-practice in data center virtualization: Toward a better understanding of VM usage,” in Proc. DSN, 2013.