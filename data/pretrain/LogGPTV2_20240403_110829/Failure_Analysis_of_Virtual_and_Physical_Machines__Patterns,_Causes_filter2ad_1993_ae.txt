Increasing the computation density by component counts
increases the risk of hardware failures [4]. To overcome the
limitations on parallel programming, consolidating VMs onto
the same hosting platform serves as an alternative to increasing
the computation density. We study whether increasing the
number of VMs leads to reliability issues, by comparing
the trend of the average failure rates computed across VMs
associated with different consolidation levels. Speciﬁcally, we
consider the average weekly failure rates. As the consolidation
level experienced by VMs changes over time because of VMs
being turned off and migrated, we propose to estimate it by
the average monthly consolidation level of a VM computed
over a one year observation period.
First, we look at the VM and failure distributions across
consolidation levels ranging from 1 to 32. We note that
the number of VMs increases with the consolidation level,
from 0.6% corresponding to 1 (i.e., the VM does share the
hosting platform with other VMs) to 30% and 32% for 16
and 32, respectively. We depict the average failure rates and
the corresponding 25th and 75th percentiles in Fig. 9. Clearly,
10101010
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
 3
 2.5
 2
 1.5
 1
 0.5
 0
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
PM
VM
PM
VM
 2
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
 1.5
 1
 0.5
0-10
20-30
40-50
60-70
80-90
10-20
30-40
50-60
70-80
90-100
(a) CPU (%)
VM
0-10
20-30
40-50
60-70
10-20
30-40
50-60
70-80
90-100
80-90
 0
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
0-10
20-30
40-50
60-70
80-90
10-20
30-40
50-60
70-80
90-100
(b) Memory (%)
VM
2-4
4-8
8-16
32-64
128-256
512-1K
2K-4K
16-32
64-128
256-512
1K-2K
(c) Disk (%)
(d) Network (Kbps)
Fig. 8. Weekly failure rate across PMs and VMs relative to CPU, memory, disk and network usage.
the failure rate decreases signiﬁcantly with the level of consol-
idation. This can be explained by the fact that the underlying
machines that host more VMs are high-end systems equipped
with more reliable components and built-in fault-tolerance
features. This observation is also backed by our previous
ﬁnding on the relationship between PM’s utilization level and
failure rates. Furthermore, combining increasing failure rates
with increasing CPU utilization of VMs, we conclude that PMs
hosting a decent number of VMs that are underutilized result
into low failure rates for both PMs and VMs. Our ﬁnding is a
nice conﬁrmation that virtualization can not only resolve the
resource over-provisioning issues in today’s datacenters, but
also potentially increase system reliability.
B. VM On/Off Frequency
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
VM
1-2
2-4
4-8
8-16
16-32 32-64
Fig. 9.
Impact of VM consolidation level on weekly failure rate.
The impact of wear and tear on hardware components is
well known [16], and the frequency of turning PMs on/off
can even deteriorate their lifespan. In contrast to PMs, VMs
are designed to facilitate elastic resource provisioning and thus
can be managed on demand, i.e., turning on/off when required
by users. Indeed, VMs experience frequent on/offs [17]. We
are therefore interested in understanding how robust VMs are
against frequent on/offs in a similar way as for PMs.
To this end, we compare the weekly failure rates of VMs
that experience different levels of weekly on/off frequencies.
For each VM, we collect its average weekly failure rate over
a one-year observation period, and its average weekly on/off
frequency, computed over a two-month observation period.
Note that to trace the on-off frequency, we need to screen
ﬁne-grained 15-minute data points, for which we only have
two months worth of data. Because of this limitation, our
assumption is that the VM on/off frequencies observed are
consistent throughout the entire year.
In Fig. 10, we depict the VM failure rates with respect
to the average weekly on/off frequency. We ﬁrst note that the
number of VMs decreases with the on/off frequency, with 60%
of all VMs being turned on/off at most once per month and
11111111
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
0
0
1
x
e
t
a
R
e
r
u
l
i
l
a
F
y
k
e
e
W
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
0
0.5-1
1-2
2-4
4-8
8-16 16-32
Fig. 10.
Impact of monthly on-off frequency on weekly failure rate.
only 14% of them being powered on/off 8 times per month.
When focusing on the majority of VMs, i.e., those with an
average on/off frequency of less than twice per month, we can
see an increasing trend of average failure rates from 0.002
to 0.0035. However, when considering the entire range of
on/off frequencies, one can observe that the average failure
rates indeed vary, however without any obvious trend. This
observation implies that the VM on/off frequency has a certain
impact on the VM’s reliability, especially from no on/off to 2
on/offs per month. However, our ﬁndings do not suggest that
very frequent on/offs of VMs will deteriorate their reliability.
VII. SUMMARY AND CONCLUSION
We summarize our key ﬁndings in the following:
• Differences in PM/VM failures – VMs have lower fail-
ure rates and lower recurrent failure probabilities than
PMs. Inter-failure times of VMs are well captured by
the Gamma distribution, showing a similar behavior as
for PMs. Software inter-failure times are the shortest,
compared with hardware/infrastructure-related ones. The
average repair time of VM failures is lower than for
PM by almost a factor of two and follows a Log-normal
distribution. Hardware failures take longest to repair. In
addition to a stronger time dependency, VM failures
also exhibit higher spatial dependency. The relationship
between VM failures and their age does not follow a
bathtub-like function. Moreover, VM failures show a
weak positive trend with age.
• Impact of resource usage and capacity on PM/VM fail-
ures – Resource utilization are more critical than capac-
ities for PM failures, in particular CPU utilization. The
key resource attributes affecting VM failures are the CPU
utilization and the number of disks, whereas the disk
capacity has the least impact. The majority of PMs show
surprisingly increasing failure rates with respect to CPU
utilization, while most VMs show a decreasing trend.
• Impact of VM management on VM failures – VM fail-
ure rates decrease with the consolidation level. Systems
consolidating a fair number of underutilized VMs result
in lower failure rates. Frequent turning on/off of VMs do
VM
not seem to have a signiﬁcant impact on VM failures.
We conducted a failure analysis on PMs and VMs hosted
on commercial datacenters, using one-year-long data collected
over 10K servers. Our analysis highlights the differences and
similarities of PM and VM failure patterns, and correlations
with resource capacity and usage. In addition to identifying
the dominant resources affecting VM failures, we also shed
light on the impact of VM resource management on VM
reliability. Yet while some of our ﬁndings on PMs conﬁrm
those reported on physical systems in previous studies, some,
particularly related to VMs, provide fresh perspectives and
insights. Overall, VMs have lower failure rates than PMs, and
show a surprising trend that, in contrast to PMs, increasing the
computation intensity by VM unit does not increase failure
rate.
ACKNOWLEDGEMENTS
This work has been partly funded by the EU Commission
under the FP7 GENiC project (Grant Agreement No 608826).
REFERENCES
[1] L. Barroso, J. Dean, and U. H¨olzle, “Web search for a planet: The
Google cluster architecture,” IEEE Micro, vol. 23, no. 2, pp. 22–28,
Mar. 2003.
[2] E. B. Nightingale, J. R. Douceur, and V. Orgovan, “Cycles, cells
and platters: An empirical analysis of hardware failures on a million
consumer PCs,” in Proceedings of the Sixth Conference on Computer
Systems, ser. EuroSys ’11, 2011, pp. 343–356.
[3] K. V. Vishwanath and N. Nagappan, “Characterizing cloud computing
hardware reliability,” in Proceedings of the 1st ACM Symposium on
Cloud Computing, ser. SoCC ’10, 2010, pp. 193–204.
[4] B. Schroeder and G. A. Gibson, “A large-scale study of failures in high-
performance computing systems,” in Proc. DSN, 2006, pp. 249–258.
[5] N. El-Sayed and B. Schroeder, “Reading between the lines of failure
logs: Understanding how HPC systems fail,” in Proc. DSN, 2013, pp.
1–12.
[6] “http://www.informationweek.com/data-center-outages-generate-big-
losses/d/d-id/1097712?”
[7] W. Jiang, C. Hu, Y. Zhou, and A. Kanevsky, “Are disks the dominant
contributor for storage failures - a comprehensive study of storage
subsystem failure characteristics,” ACM Transactions on Storage, vol. 4,
no. 3, 2008.
[8] E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large
disk drive population,” in Proc. FAST, 2007, pp. 17–28.
[9] B. Schroeder, S. Damouras, and P. Gill, “Understanding latent sector
errors and how to protect against them,” in Proc. FAST, 2010, pp. 71–
84.
[10] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. K. Sahoo,
“Blue Gene/L failure analysis and prediction models,” in Proc. DSN,
2006, pp. 425–434.
[11] D. Yuan, J. Zheng, S. Park, Y. Zhou, and S. Savage, “Improving software
diagnosability via log enhancement,” ACM Trans. Comput. Syst., vol. 30,
no. 1, p. 4, 2012.
[12] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild:
A large-scale ﬁeld study,” Commun. ACM, vol. 54, no. 2, pp. 100–107,
2011.
[13] R. K. Sahoo, A. Sivasubramaniam, M. S. Squillante, and Y. Zhang, “Fail-
ure data analysis of a large-scale heterogeneous server environment,” in
Proc. DSN, 2004, pp. 772–781.
[14] HP OpenView, “http://support.openview.hp.com/.”
[15] IBM
Monitoring,
Tivoli
“http://ibm.com/software/tivoli/
products/monitor/.”
[16] K. Trivedi, Probability and Statistics with Reliability, Queuing, and
Computer Science Applications. Wiley, 2001.
[17] R. Birke, A. Podzimek, L. Y. Chen, and E. Smirni, “State-of-the-practice
in data center virtualization: Toward a better understanding of VM
usage,” in Proc. DSN, 2013.
12121212
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply.