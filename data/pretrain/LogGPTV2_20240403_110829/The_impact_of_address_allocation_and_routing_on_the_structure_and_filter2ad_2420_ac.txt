calibrate ARAM’s performance, we used routing tables from Route
Views [27]. We processed the routing tables in two important ways.
First, we removed historic routing table entries (those preﬁxes that
belonged to address space that had been allocated prior to the es-
tablishment of the registries). ARAM does not attempt to model
these historic preﬁxes. Since our goal is to extrapolate current allo-
cation and routing practice, we argue that the effect of these histor-
ical preﬁxes will soon become negligible. Indeed, in 19974, these
historical preﬁxes formed almost 45% of the routing table. As of
this writing they account for almost 25% of the number of preﬁxes.
Second, we removed preﬁxes longer than /24. Most ISPs ﬁlter pre-
ﬁxes longer than /24, although some such preﬁxes do appear in the
Route Views database (possibly because the Route Views’ peers do
not apply ﬁltering rules to their feeds).
2.3.2 Validating ARAM
In this section, we describe the results of several kinds of valida-
tion tests we performed on ARAM. Recall that the goal of ARAM
is to plausibly capture the relationships between preﬁxes. This no-
tion is somewhat difﬁcult to precisely deﬁne, and we deﬁne instead
three indirect metrics that capture various aspects of the relation-
ships.
Our ﬁrst metric is the normalized RMS error in the preﬁx length
distribution. This metric computes the root mean square error be-
tween the preﬁx length distribution generated by a model and the
preﬁx length distribution in real routing tables. We normalize the
RMS error by the number of preﬁxes in each table, resulting in a
“normalized RMS error per preﬁx” metric.
Our second metric is the depth ratio, the ratio of the number of
preﬁxes at depth zero to that of the number of preﬁxes at depth one
in a routing table. It attempts to capture a different facet of preﬁx
relationships than the length distribution—how the less speciﬁcs
4While we would have liked to study the evolution of the routing
table soon after the deployment of CIDR, we have been unable to
do so due to the lack of periodic routing table snapshots from that
era.
and more speciﬁcs relate to each other.
Our ﬁnal metric is a slightly more ﬁne-grained measure of pre-
ﬁx relationships. The unibit trie density captures the number of
nodes in the unibit trie [36] per preﬁx in the routing table. Unibit
trie density is inversely related to preﬁx density; when preﬁxes are
more closely clustered together in a routing table, fewer unibit trie
nodes are required per preﬁx. This metric captures the relationship
between parent preﬁxes and spawned preﬁxes. For example, when
the difference in the lengths of a parent preﬁx and a spawned pre-
ﬁx is large, the number of unibit trie nodes is more than when the
difference is small. Similarly, if of three preﬁxes, one is the parent
of the other two, the unibit trie is likely to be smaller than a table
where these preﬁxes were randomly scattered across the address
space.
Using these metrics, we not only compare ARAM to actual rout-
ing tables, but we also calibrate ARAM against5 a random scatter
model. This scatter model simply scales the preﬁx length distribu-
tion of the current routing table by a constant factor to get the preﬁx
length distribution of a larger (or smaller) table. It then “scatters”
these preﬁxes randomly among the 113 /8s that are currently re-
served for use by IANA.
Our performance comparisons use 20 routing tables dating back
to November 1997, approximately6 one every three months.
2.3.2.1 Preﬁx Length Distribution.
Figure 1 plots the normalized RMS error per preﬁx for ARAM
and scatter. Notice that ARAM’s RMS error is uniformly low (less
than 6% over the 5-year measurement period). This is remarkable
because ARAM is not explicitly engineered to produce a speciﬁc
preﬁx length distribution (it only has coarse-grained preﬁx length
ranges encoded in it). What is more remarkable, however, is that
in producing an ARAM to match a routing table at time T, we used
the number of allocations that had been made by the registries at
T. Furthermore, in generating the matching tables, we used one set
of values for the split and spawn parameters. (This is testament
to the fact that, statistically speaking, the incidence of splitting and
5Another natural model to compare ARAM to would have been
RTG [28]. For various logistical reasons this hasn’t been possi-
ble. Despite our efforts, the source code for RTG did not run to
completion when asked to generate large tables. Furthermore, the
RTG paper does not include enough details to reproduce the preﬁx
generation part of the generator. We are working with the authors
of [28] to resolve some of these issues. In the meantime, we resort
to a qualitative comparison between the models (Section 4).
6In one case, incomplete data in the Route Views database pre-
vented us from taking samples exactly three months apart.
spawning hasn’t changed much over the 5 years.) To give the reader
a visual sense of the closeness of the match, Figure 2 compares the
preﬁx length distributions in actual routing tables to those gener-
ated by ARAM.
Scatter has lower RMS error than ARAM, in general. This is
because scatter, by design, attempts to match the preﬁx length dis-
tribution. The reason the RMS error is non-zero in some cases is
that we generated all the scatter tables using the preﬁx length dis-
tribution from one instance of the actual routing table.
2.3.2.2 Depth Ratio.
Figure 3 plots the depth ratio for ARAM, scatter and the routing
table. ARAM clearly matches the depth ratio of each instance of
the routing table, but this is not surprising since ARAM is implic-
itly engineered (given its split and spawn parameters) to generate a
number of preﬁxes at depth zero and depth one that is comparable
to those in a real routing table.
However, it is important to note that scatter gets the depth dis-
tribution completely wrong, and it is not hard to see why. Because
preﬁxes are scattered randomly among the /8s, it is less likely that
a preﬁx will be at depth one (i.e., a more speciﬁc) especially for
smaller routing tables. This metric starkly illustrates (as does the
next) how ARAM clearly outperforms scatter.
2.3.2.3 Unibit Trie Density.
Finally, Figure 4 plots the unibit trie density for ARAM, scat-
ter, and the real routing tables for our twenty measurement points.
Scatter grossly overestimates the density, because its preﬁxes are
essentially uncorrelated with respect to where they are placed in
the address space.
However, a real routing table exhibits signiﬁcant relationships
between preﬁxes, resulting in smaller unibit tries for the same pre-
ﬁx length. ARAM performs well; for current routing tables, ARAM
is off by less than 7% in this metric. This is encouraging, especially
considering the simpliﬁcations we make in the model (Section 2.2).
2.3.3 Validating ARAM’s Design Assumptions
Having validated the performance of ARAM, we now quantita-
tively justify many of our design decisions.
2.3.3.1 Allocation Size Distribution.
An important component of ARAM is that the preﬁx length dis-
tributions of allocations is well modeled by a function of the form
y = xk (Section 2.2). We used k = 3.4. The R-squared value
for this value of k is 0.92. The ﬁt deviates from the actual data at
two preﬁx lengths: /16 and /19. Pre-CIDR allocations of class B
address space account for the deviation at /16. The deviation at /19
is essentially a boundary effect; until recently, the RIRs used /19 as
the minimum allocation size
An exponential also ﬁts the data well, although, as we have dis-
cussed before, there is an intuitive justiﬁcation for the algebraic
form (that larger ISPs get smaller preﬁxes, and ISP sizes can be
expected to follow a power-law).
There is one caveat to this distribution. ARAM assumes that the
maximum size of an allocation is /10 since currently there are no
allocations whose preﬁx length is smaller than 10. ARIN allocates
(at a given time) blocks no larger than a /13, and the existing larger
allocations that one sees in the databases are the result of contigu-
ous allocations made over time. We expect this policy to continue,
since a larger maximum allocation size reduces an RIR’s /8 signif-
icantly (at a given time, each RIR only has a small number of /8s
that it allocates from). The other RIRs may change their policies to
have a maximum allocation size too.
ARAM, for simplicity, starts allocating preﬁxes from a fresh /8
when the current /8’s utilization reaches 80%. In practice, /8s can
have a utilization ranging from 80% to 99%. We learnt from AP-
NIC [30] and RIPE [31] that they use their /8s up to 80% before
asking IANA for more. We also learnt from ARIN [29] that they
do not make fresh allocations from a /8 whose usage has crossed
80%; such a used /8 is only used for growing allocations which
have already been made from it (recall that RIRs try to make con-
tiguous allocations). This is a practice followed in general, by all
RIRs. However the usage or non-usage of allocations would not
have a major effect on trie properties because only a small percent-
age of preﬁxes are intact preﬁxes. ARAM also leaves one partially
used (i.e., with less than 80% utilization) /8 per RIR. In practice,
there may be more than one such /8 per RIR; IANA may some-
times choose to give two new /8s to an RIR at once [29].
2.3.3.2 Modeling Intact and Split Preﬁxes.
ARAM represents the fraction of preﬁxes that split using a sin-
gle parameter Fsplit. This is a simpliﬁcation. In practice, we see
that the fraction of split allocations decreases with decreasing allo-
cation size. We could have used a distribution as an input, but in
the absence of a clearer understanding of why splitting varies with
preﬁx length, doing this would not have helped us understand how
routing practice contributes to extrapolations. Obtaining this un-
derstanding is left for future work, but a possible explanation for
the downward trend for Fsplit is that larger allocations split more
frequently because they need ﬁner grain control over trafﬁc.
We earlier gave some reasons for allocation splitting: geographic
distribution of preﬁxes, and load sharing. One relatively minor, but
interesting regional variation is caused by a small number of LIRs
in the RIPE region (e.g., the National Institute for R&D in Infor-
matics of Romania) which do not provide Internet connectivity to
their assignees. Their allocations appear as split allocations in the
routing table.
Recall that our splitting rule (Section 2.2) sometimes splits an
allocation of length /i into preﬁxes of length /(i + 2) and /(i + 3).
Actual routing practice is much less cleaner than our rule would
suggest (details omitted for brevity), and the incidence and extent
of splitting varies with allocation size.
2.3.3.3 Modeling Spawned Preﬁxes.
In Section 2.2, we assumed that spawned preﬁxes are in the range
/19 to /24, largely because ISP assignments fall into that range. The
assignment records bear this out. Less than 0.4% of all assignments
registered in the whois databases (Section 2.3.1) are larger than
/19. Even the largest ISPs rarely make assignments larger than /19.
ARIN and APNIC require that they be consulted before any as-
signment larger than a /19 is made. ARIN allows extra-large ISPs
to consult only when an assignment larger than /18 is to be made.
RIPE has no such maximum assignment size. We learnt that there
are few LIRs who would make assignments larger than /19 in the
RIPE region [31].
ARAM also assumes that the spawned preﬁxes are randomly
scattered within a parent’s address space. We empirically observed
that assignments made by ISPs which appear in the routing table
follow no particular pattern across the ISPs’ address space. This
is consistent with the intuition that customers may generally multi-
home at will, and ISPs generally do not attempt to make multihom-
ing assignments from a separate block (there are some exceptions).
Our choice of single parameters Fspawn and Cspawn for the fre-
quency and extent of spawning is clearly a ﬁrst order approxima-
tion. Fspawn actually decreases with increasing preﬁx length of the
spawning preﬁx, but without an understanding of the reasons be-
o
i
t
a
r
h
t
p
e
D
16
12
8
4
0
Real routing table
Scatter
ARAM
0
10
20
30
40
50
60
70
Months since Nov.1997
Figure 3: Depth Ratio
x
i
f
e
r
p
r
e
p
s
e
d
o
n
e
i
r
t
t
i
i
b
n
u
f
o
r
e
b
m
u
N
8
7
6
5
4
3
2
Real routing table
Scatter
ARAM
0
10
20
30
40
50
60
70
Months since Nov.1997
Figure 4: Unibit Trie Density
hind this variation, we were loathe to include a richer parametriza-
tion in the model.
Finally, our spawning rule is biased towards producing a larger
number of longer preﬁxes. Thus, ARAM produces a large number
of /23s and /24s (see Figure 2 for example). The larger number of
/24s produced by ARAM is consistent with the data and is caused
by a boundary effect; ISPs ﬁlter their routing tables at /24, and
many small organizations that want to multi-home advertise /24 7.
However, the bias shown by the spawning algorithm towards /23s
is an artifact of the simplistic choice of spawning rule, and deviates
from the data a little. Overall, however, as we have shown above,
the routing tables produced by ARAM qualitatively match several
years’ worth of routing tables.
Finally, ARAM generates preﬁxes only at depth zero and depth
one (recall that depth of a preﬁx is the number of less speciﬁc pre-
ﬁxes in the routing table). Over the last 5 years, consistently, fewer
that 10% of the routing table preﬁxes have a depth greater than one
(detailed data omitted for brevity).
3. EVALUATING THE SCALABILITY OF
IPV4 LOOKUP TECHNIQUES
In the introduction we said that router vendors need a model that
is able to generate realistic tables of a million or more entries in
order to evaluate the performance of algorithmic solutions for IP
lookup. The validation in the last section provides some conﬁ-
dence that ARAM generates realistic tables. In this section we ap-
ply ARAM to produce larger tables in order to test the scalability of
a speciﬁc algorithmic solution when compared to Ternary CAMs.
3.1 TCAMs and Multibit Tries
TCAMs (Ternary Content Addressable Memories) can store the
values 0, 1 and X (a “don’t care” value). The ability to store don’t
care values makes TCAMs apt for IP preﬁx lookups. Essentially,
TCAMs can compare a given destination address to all stored pre-
ﬁxes in parallel and return the longest match in one memory access.
To store a w bit preﬁx, a TCAM would use w cells (i.e., each cell
stores one bit of information). For example, IP addresses would re-
quire 32 cells per preﬁx. Variations in routing table structure have
no effect on the memory requirements of TCAMs.
By contrast, algorithmic solutions are based on storing preﬁxes
in tries. High-speed algorithmic solutions store tries in SRAM.
7This fact is also recognized in RIR policy. ARIN recently ratiﬁed
a policy, which allows multihoming as a justiﬁcation for a customer
to get a /24 from an LIR (i.e., the requirement that 25% of an as-
signment be used immediately is waived) [32]
Multibit trie algorithms process multiple bits in the trie in a single
memory access and are therefore faster. However a multibit trie in-
creases the memory required for the lookup structure. To offset this
storage increase, modern implementations that store the forwarding
table in SRAM generally compress multibit trie nodes.
In this paper, we study the scalability of the Tree Bitmap [12]
compression algorithm. While the Tree Bitmap algorithm is basi-
cally a reﬁnement of the seminal Lulea [11] algorithm, it appears
to be more popular because it allows fast updates. CAMs have fast
updates [45], and, to keep up with the competition, vendors prefer
implementing algorithmic solutions with fast updates. In any case,
we believe that similar scaling results will hold for any compressed
multibit trie implementation, of which the Tree Bitmap algorithm
can be considered a representative.
An example of a multibit trie is shown in Figure 5, where the
root node is a multibit node of stride (the number of bits processed
in a single memory access) two. The Tree Bitmap algorithm uses
bitmaps to encode the multibit trie compactly. The bitmaps are of
two kinds internal and external. These two bitmaps are sufﬁcient to
encode a multibit node. The internal bitmap (2stride − 1 bits long)
records the preﬁxes present in a multibit node. The external bitmap
(2stride bits long) records which child nodes of the multibit node