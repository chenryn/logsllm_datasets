We will approximate fx by a low-degree polynomial by approximating each fxi by a low-degree
polynomial. We do the latter using a standard technique based on Chebychev polynomials:
Fact 7.6. For all t ∈ N and α > 0,there exists a univariate (real) polynomial g of degree at most
t log(1/α)(cid:1) such that g(0) = 0 and for all i ∈ {1, . . . , t}, 1 − α ≤ g(i) ≤ 1 + α. Moreover, g
s = O(cid:0)√
can be constructed in time poly(t, log(1/α) and all of the coeﬃcients of g have magnitude at most
2s.
Given g as in the fact and a row w ∈ X, consider the following function:
hw(y) = g
wjyj
(8)
√
where g is from Fact 7.6. hw is a multivariate polynomial of degree O(
most C = dO(
t·log(1/α)) coeﬃcients of magnitude at most M = dO(
t·log(1/α)).
√
√
By construction we have that for all w ∈ X and all y ∈ X of Hamming weight at most t,
t · log(1/α)). It has at
 d(cid:88)
j=1
 ,
Thus, if we deﬁne
we have that:
|hw(y) − fw(y)| ≤ α .
n(cid:88)
i=1
hx =
1
n
hxi,
|hx(y) − fx(y)| ≤ α.
61
To obtain diﬀerential privacy, we can now add Laplace noise to each coeﬃcient of hx. Each
coeﬃcient is an average of the corresponding coeﬃcients of the hxi’s, so has has global sensi-
tivity at most 2M/n. By the Laplace Mechanism and Basic Composition, it suﬃces to add noise
Lap(2M C/εn) to each of the C coeﬃcients for the resulting vector of coeﬃcients to be diﬀerentially
private. With high probability, none of the coeﬃcients will have noise more than (log C)· 2M C/εn,
√
which will add up to an error of at most C · log C · 2M C/εn = dO(
t)/(εn) when evaluating on any
input y.
Now we turn to a diﬀerent approach, which runs in polynomial time and can answer nearly n4
low-order marginals.
Theorem 7.7 (marginals via SDP projection [44]). Let t ∈ N be an even constant. For all
n, d ∈ N, ε, δ > 0, there is a polynomial time (ε, δ)-diﬀerentially private algorithm that takes a
dataset x ∈ ({0, 1}d)n and answers all counting queries in Qconj
(d) on x to within additive error
(cid:16) ˜O(dt/4) ·(cid:112)log(1/δ)/εn
(cid:17)1/2
t
.
α =
The most interesting case of this theorem is t = 2, when the error is ( ˜O(
matching the lower bound of Theorem 5.23 up to a factor of poly(log d, log(1/δ)) [21].
Proof sketch. The starting point for the algorithm is a beautiful geometric approach of Nikolov,
Talwar, and Zhang [84] that was used to prove the Hereditary Discrepancy Upper Bound (Theo-
rem 5.10). We will use an instantiation of their algorithm that provides near-optimal error bounds
in terms of |Q|, like the Private Multiplicative Weights algorithm, but for (cid:96)2 or (cid:96)1 error rather than
(cid:96)∞.
We know that adding independent noise of magnitude O((cid:112)|Q|/εn) to the answers to all the
counting queries in a family Q provides privacy, but gives useless results (that lie outside [0, 1]) when
|Q| > n2. Remarkably, it turns out that by simply projecting these answers back to be consistent
with some dataset yields highly accurate results.
To formalize this, recall the convex body K used in the packing characterization of sample
complexity (Theorem 5.15). That is, K = ConvexHull({aw : w ∈ X}), where aw = (q(w))q∈Q in
the vector in RQ giving all the query answers on row w ∈ X. Recall that for any dataset x ∈ X, the
tuple of answers on x is ax = (1/n)(cid:80)n
i=1 axi ∈ K.
This leads to the following algorithm M(x, Q):
d)·(cid:112)log(1/δ)/εn)1/2,
√
1. Calculate the exact answers
2. Add Gaussian noise to the coordinates of y:
y = ax = (q(x))q∈Q ∈ K.
O((cid:112)|Q| · log(1/δ))
˜y = y +
εn
· N(0, 1)|Q|.
(This can be shown to achieve (ε, δ)-diﬀerential privacy, and is more convenient than Laplace
noise for the geometric arguments we are about to make.)
3. Project back to K: Let
ˆy = argminz∈K (cid:107)z − ˜y(cid:107)2.
This step maintains (ε, δ)-diﬀerential privacy by post-processing.
62
Let’s analyze the error introduced by this algorithm. Consider the line (cid:96) through y and ˆy, and
let p be the orthogonal projection of ˜y onto (cid:96). On (cid:96), p must be on the ray from ˆy to inﬁnity. (If p
were on the segment between y and ˆy, then p would be a point in K closer to ˜y than ˆy. If p were
on the ray from y to inﬁnity, then y would be a point in K closer to ˜y than ˆy.)
(cid:107)y − ˆy(cid:107)2
2 = (cid:104)ˆy − y, ˆy − y(cid:105)
≤ (cid:104)ˆy − y, p − y(cid:105)
= (cid:104)ˆy − y, ˜y − y(cid:105)
≤ (|(cid:104)ˆy, ˜y − y(cid:105)| + |(cid:104)y, ˜y − y(cid:105)|)
≤ 2 max
z∈K
|(cid:104)z, ˜y − y(cid:105)|
(because p is on the ray from ˆy to inﬁnity)
(because ˜y − p is orthogonal to ˆy − y)
(triangle inequality)
Taking expectations, and writing ˜y − y = O((cid:112)|Q| · log(1/δ)/εn) · g for g ∼ N(0, 1)|Q|, we have:
E(cid:2)(cid:107)y − ˆy(cid:107)2
(cid:3) ≤ O
2
(cid:16)(cid:112)|Q| · log(1/δ)
(cid:17)
(cid:20)
εn
· E
g
max
z∈K
|(cid:104)z, g(cid:105)|
.
(cid:21)
The quantity
(cid:96)∗(K) def= E
g
max
z∈K
|(cid:104)z, g(cid:105)|
is known as the Gaussian mean width of the polytope K, an important and well-studied quantity
in convex geometry.
Let’s upper bound it for K deﬁned by an arbitrary set Q of counting queries. For every choice
of g, the maximum of |(cid:104)z, g(cid:105)| over z ∈ K will be obtained at one of the vertices of K. Recalling the
deﬁnition of K, we have
|(cid:104)z, g(cid:105)| = max
w∈X
max
z∈K
|(cid:104)aw, g(cid:105)|.
By rotational symmetry of Gaussians, the random variable (cid:104)aw, g(cid:105) is distributed as N(0,(cid:107)aw(cid:107)2).
We have (cid:107)aw(cid:107)2 ≤(cid:112)|Q| since aw is a {0, 1} vector. Thus, with probability at least 1 − β over g, we
have |(cid:104)aw, g(cid:105)| ≤ O((cid:112)|Q| · log(1/β)). Taking a union bound over w ∈ X, we have
with probability at least 1 − β, for every β > 0. This implies that:
|(cid:104)aw, g(cid:105)| ≤ O((cid:112)|Q| · log(|X|/β)).
(cid:21)
(cid:20)
(cid:3) ≤ |Q| · O((cid:112)log |X| · log(1/δ))
|(cid:104)aw, g(cid:105)|
= E
g
max
w∈X
(cid:21)
.
≤ O((cid:112)|Q| · log |X|).
εn
max
w∈X
(cid:20)
E
g
max
z∈K
|(cid:104)z, g(cid:105)|
E(cid:2)(cid:107)y − ˆy(cid:107)2
2
Putting it all together, we have:
63
So if we look at the average error (averaged over the |Q| queries), we have:
E
E
coins of M, q ∈ Q
|yq − ˆyq|2
(cid:19)1/2
(cid:20) 1
(cid:21)(cid:19)1/2
|Q| · (cid:107)y − ˆy(cid:107)2
(cid:33)1/2
(cid:32)(cid:112)log(1/δ)
(cid:112)|Q| · εn
· (cid:96)∗(K)
(cid:33)1/2
(cid:32)(cid:112)log |X| · log(1/δ)
coins of M
2
εn
.
E
coins of M, q ∈ Q
[|yq − ˆyq|] ≤
=
(cid:18)
(cid:18)
= O
≤ O
This exactly matches the (optimal) bound from the Private Multiplicative Weights algorithm,
except that we only achieve error on average for a random query from Q. However, it can be
generalized to obtain small average-case error on any given distribution of queries (just weight the
coordinates in RQ according to the distribution), and then combined with a diﬀerentially private
algorithm for “boosting” [42] to obtain small error on all queries with high probability (paying a
factor of polylog(|Q|) in the error).
Our interest in this algorithm, however, is that it does not appear to generate synthetic data,
and thus is not subject to the computational complexity lower bounds of Theorem 6.12. Converting
the output ˆy to synthetic data would amount to decomposing ˆy into a convex combination of the
|X| vertices of K, which could take time proportional to |X|. Unfortunately, this same reason means
that the “Project back to K” step might take time proportional to |X|, as the given description of
K is in terms of its |X| vertices. Indeed, projection onto a convex set is known to be polynomially
equivalent to optimizing linear functions on the set, and as we will see below, optimizing over K is
NP-hard for the cases we are interested in.
marginals with t > 2, the theorem follows by reduction to 2-way marginals. (Create (cid:0) d
Let’s see how to make this process more eﬃcient for the case of 2-way marginals. For t-way
(cid:1) ≤ dt/2
t/2
variables representing the conjunctions on every subset of t/2 variables; and then every t-way
conjunction in the original variables can be written as a 2-way conjunction in the new variables.)
Actually, releasing conjunctions of width at most 2 is equivalent to releasing parities of width
at most 2, so let us focus on the latter problem. It will also be useful to work in ±1 notation, so
the parity function qij : {±1}d → {±1} on variables i and j is given by qij(v) = vivj. Thus we see
that
K = ConvexHull({v ⊗ v : v ∈ {±1}d}).
Unfortunately, projecting onto and optimizing over K is known to be NP-hard, so we will take a
cue from approximation algorithms and look at a semideﬁnite programming relaxation.
It is NP-hard to do this optimally. So instead, we’ll ﬁnd a nicer L “close” to K (where K ⊆ L)
and optimize over L. We need to ensure that the Gaussian mean width of L is comparable to that
of K (or at least the bound we used on the Gaussian mean width of K).
First, we will relax to:
L0 = ConvexHull({v ⊗ v(cid:48) : v, v(cid:48) ∈ {±1}d}).
64
vectors whose entries have magnitude at most 1, and the bound was linear in(cid:112)log |X| =
To bound the Gaussian mean width of K, we only used the fact that K is the convex hull of |X| = 2d
d. L0
√
is now the convex hull of 22d such vectors, so we only lose a constant factor in our bound.
Optimizing over L0 is still NP-hard, but it has polynomial-time approximation algorithms.
Indeed, if we relax L0 to
L = {V ∈ Rd2
: ∃{ui}d
i=1,{u(cid:48)
j}d
j=1 unit vectors with Vij = (cid:104)ui, u(cid:48)
j(cid:105)},
then we can optimize linear functions on L by semideﬁnite programming, and consequently we can
project onto L. Moreover, Grothendieck’s Inequality (see [70]) says that the maximum of any linear