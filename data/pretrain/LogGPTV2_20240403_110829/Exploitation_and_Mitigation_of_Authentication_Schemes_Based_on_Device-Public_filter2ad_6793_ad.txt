backend did not send any SMS or phone call. This was possible
because these apps used the content of a hidden file stored in the
external storage to authenticate users upon re-installation.
Therefore, in the version of the apps we have analyzed, an
attacker controlling an app on the victim’s device could authenticate
to the remote backend on behalf of the victim by:
After these operations, the vulnerable app running on the attacker’s
device is automatically logged in as the victim, without even having
the victim’s device receiving an authentication text message. This
exploit gives attackers full use of the victim’s account, allowing
them to send messages on the victim’s behalf, and to receive all
future messages sent to the victim.
Vendor Reaction. Upon discovery of these vulnerabilities, we
contacted both vendors in August 2015. Both vulnerabilities
were quickly acknowledged and, after working with the vendors,
mitigations were deployed.
Specifically, both WhatsApp and Viber removed reliance on
static identifiers and publicly-accessible files. However, they still
rely on the content of a received text message (or, in case of Viber,
the caller’s number of an authentication phone call) for their
primary means of authentication.
Interestingly, after our first notification, Viber was initially
changed in a way that was ineffective against our attack. In
particular, the file in the external storage remained, but just
spoofing it was not enough. We discovered that Viber was changed
to also check that other device’s identifiers matched the ones
used during a previous registration. However, as we discussed
through the paper, an attacker could just query the values of the
different device identifiers using an attacker-controlled app on
the victim’s device (see Section 3.2) and then spoof them on an
attacker-controlled device, as implemented in the “ID Injector”
(see Section 3.3). We note that this attack was working until Viber
issued another update, in September 2017, removing reliance on
the content of publicly-accessible files to perform authentication.
In addition, after our notification to the vendors (but likely inde-
pendently from our disclosure), in 2016 both apps implemented new
cryptographic measures limiting an attacker’s ability to impersonate
a user when an account is stolen (via ours or other attacks). In partic-
ular, both apps implemented an end-to-end encryption mechanism,
based on the usage of a per-user key pair. This functionality allows
users to authenticate and encrypt exchanged messages. For instance,
suppose that two users A and B communicate together. This system
encrypts the communication channel between A and B using their
keys. Moreover, a user, for instance A, can check the value of B’s pub-
lic key and, in case B’s public key changes, A would be notified (how-
ever, A and B could still communicate together). The same notifica-
tion would be shown after the aforementioned attack is performed
because the per-user key is stored in an app-private location, so it
cannot be stolen and transferred to the attacker-controlled device.
(1) Copying the content of a specific file (stored in the external
storage of the victim’s device) to an attacker-controlled
device.
(2) In case of WhatsApp, spoof the value of the Google email
account. To achieve this, an attacker can use a malicious
6.2 Free-to-play Games
The other 38 identified vulnerable applications are games, in which
an attacker can perform an identity-transfer attack to, steal the vic-
tim’s virtual currency or in-game objects. In these apps, the system
Exploitation and Mitigation of Authentication Schemes. . .
ACSAC 2017, December 4–8, 2017, San Juan, PR, USA
detected, for instance, the fact that graphical interfaces used to enter
the user’s name, or to show game tutorials and welcome messages
were skipped during the Vulnerability Detection and Exploit Ver-
ification phases (see Figure 2). This indicated that the app authenti-
cated with the remote backend and was able to obtain the user’s state.
After an identity-transfer attack was performed, we noticed
three different kinds of behaviors on the victim’s device, when
attacked while the victim is using them. Some of the apps show
to the user a generic error message after the attack, such as
“Connection Timeout.” Others show a message informing the users
that “another device” accessed their account. Finally, some of them
do not show any information to the victim.
All these games offer in-app purchases, and the virtual currency
used is derived from real money, using the Google’s In-App billing
API (IAB). For this reason, these vulnerabilities are particularly wor-
risome, since they can represent actual financial loss to the victims
and the apps’ developers. One surprising result was that the account
transferred during the Exploit Verification phase can include virtual
currency purchased through Google’s In-App Billing API [19].
Notably, this is not an explicit attack against the In-App Billing
API, as explored in previous work [30], but rather that its use in
conjunction with the discovered vulnerabilities makes this data
vulnerable as well. This is due to how the IAB API is implemented
and how it is typically used by developers. In particular, even
though the IAB mechanism offers to store information about a
user’s purchases (in a way which is secure under our threat model),
it cannot be easily used as a store of the user’s current account
balance, since precise accounting is not possible.
Therefore, developers need to store the virtual currency balance
differently, in the (potentially unsafe) app’s backend. In case of
applications vulnerable to identity-transfer attacks, this means the
user’s paid-for currency is as easy to steal as any other information
in the user’s account. This is particularly of concern, given the
already-established trend in malware on other platforms targeting
online game accounts [12].
7 PROPOSED DEFENSES
We propose two defenses against the attack studied in this paper:
one aimed at creating secure device identifiers, and another
aimed at safeguarding SMS-based authentication. A fully working
prototype implementation of our defenses is publicly available [4],
as an Xposed framework’s module.
7.1 Securing the SMS Channel
Design. All installed apps on a device (that request the proper
permission) can request to be notified of the content of incoming
SMS messages, even when these messages are only intended for
use by a particular app. As we shown, this behavior is particularly
problematic when received SMS messages contain authentication
codes destined for only a particular app.
Our proposed solution, similar to one discussed in [29], works
by delivering authentication SMS only to the apps intended
to receive them. Specifically, we propose a convention that
authentication-related SMS should be pre-pended with the string
AUTHCODE: app_cert_fingerprint, where app_cert_fingerprint is
the fingerprint of the certificate used to sign the destination app.
The OS would then route the message only to the main SMS reader
app, and the app bearing the included fingerprint. This improves on
Mulliner et al.’s previous solution by not requiring the OS to be noti-
fied ahead of time about how incoming messages should be routed.
For example, consider an app named “Foo Messaging” signed
with a certificate whose fingerprint is 0d5af23c. In this case, users
enter their phone number into the app, which is sent to the app’s
backend. As a response, the app’s backend sends an SMS message
with the content AUTHCODE: 0d5af238c Verification Code: 34782.
When received, the OS would then only notify the user’s default
SMS reader and “Foo Messaging” about the new message.
To improve usability, we propose that the default messaging app,
by default, hides this routing information. Alternatively, the default
messaging app could replace it with an indication of the app the
message was delivered to. This functionality can be implemented
without any modification to existing apps. However, it would
require a small modification to the app’s backends to prepend the
app’s fingerprint to the outgoing SMS.
We note that the recently-released Android version 8 introduces
a new API called createAppSpecificSmsToken. This API “creates
a single use app specific incoming SMS request for the calling
package” [16]. When using this API, an app would first get a secret
token from the operating system. Then the app’s backend would
send an SMS containing that specific token to the user’s device and
the SMS will be subsequently automatically routed by the OS to
the correct app (through the token) and it will not be made readable
by any other apps (or visible by the user).
While it may seem that this new feature mitigates the weakness
of the usage of SMS to authenticate user, we argue that, on the
contrary, it eases the attack we have described in this paper. In fact,
while the usage of this API would stop an attacker from stealing
and replaying authentication codes when the user attempts to
authenticate, the attacker can just attempt their own authentication
(simulating a user re-installing the app on the same device), at
which point the SMS will be sent and routed to the attacker’s app
and it can thus be easily stolen. From the conceptual point of view,
the attack works because the app’s backend does not have enough
information to determine whether the app receiving the SMS is the
legitimate app or the attacker’s app.
There are two additional aspects that make apps using this new
API more vulnerable to the attacks presented in this paper. First,
neither the user nor the legitimate app will notice the incoming
SMS message (triggered by the attacker), since it will be routed only
to the attacker’s app. Second, the attacker’s app does not need to
require the READ_SMS permission when receiving messages using
this API, thus making this malicious app stealthier.
Implementation. There are two ways an app can access SMS in
Android: an app can ask to the operating system to be notified when
a new message is received, or an app can access the list of received
messages. Thus, to implement our defense, we modified both the
Android InboundSmsHandler component, responsible to notify apps
of incoming messages, and the SmsProvider component, mediating
apps’ accesses to received SMS. Globally, our modifications consist
of approximately 100 LOC added to the original Android code.
The added code introduces an average slowdown of 5.381ms every
time an SMS is received and a slowdown of 2.064ms every time an
ACSAC 2017, December 4–8, 2017, San Juan, PR, USA
A. Bianchi et al.
app queries the operating system for received SMS. We consider
both slowdowns as negligible, given the fact that, receiving a text
message it is not a frequent event.
called. The standard Android API caches this value after the first
time an app access it, thus we consider this slowdown as negligible.
7.2 Secure Device IDs
Design. The most common and easily obtained device-public
identifier is the ANDROID_ID, which is intended to be used to
allow apps and their backends to differentiate Android devices.
In our defense we modify the API used to access the AN-
DROID_ID, so that it returns a Private Device ID (PDID) different
for every app (more precisely, different for every app’s signing
certificate), instead of the original device-wide value. Specifically,
the first time a device boots (or after a factory reset) a random Secret
ID (SID) is generated. The Private Device ID is then derived from
the Secret ID using the signing certificate included with each app,
which uniquely identifies its developer. In this way, the semantics
of the ANDROID_ID are preserved, apps from different developers
cannot steal each other’s identifiers, but no convenience is lost for
a developer with multiple apps on the same device. Moreover, the
PDID does not change after app’s re-installation.
computed
follows:
where:
HMAC(SID, caller_app_cert_fingerprint)
caller_app_cert_fingerprint is the certificate fingerprint of
the app calling the API and HMAC is a cryptographically secure keyed-
hash message authentication code (e.g., HMAC-SHA256) in which
SID is used as “key” and caller_app_cert_fingerprint as “message.”
Specifically,
The security of this method is bolstered by the fact that
(1) Upon installation, Android verifies that an app has been
the
PDID
is
as
correctly signed.
framework API [17].
(2) The operating system can securely identify the caller of a
(3) No API is provided to get the value of SID.
For these reasons, as long as the developer’s private key remains
uncompromised, the privacy of the PDID to an app is maintained.
We implemented this modification as complete transparent
replacement of the current API used to get the ANDROID_ID. In this
way this defense could be deployed without requiring code changes
to existing apps. This will necessarily interfere with advertising
libraries, which seek to use the ANDROID_ID to track the usage
of multiple apps on the same device. However, as explained in
Section 3.2, the only identifier that advertisement libraries are
supposed to use to track users is the Google Advertisement ID. A
possible alternative implementation would be providing the PDID
to apps trough a separate API.
It is interesting to note that, concurrently (and independently)
to the development of this work, Google changed the behavior of
the ANDROID_ID to follow our proposed modification. Although
the implementation details differ, the functionality achieved by
this change is the same. This modification is available starting from
Android version 8 [22].
Implementation. We implemented this defense by modifying
the Android SettingsProvider, the operating system component
responsible to deliver the ANDROID_ID value to the running
apps. Our modifications consist of approximately 70 LOC added
to the original Android code. The added code introduces an average
slowdown of 1.497ms when the API to get the ANDRODID_ID is
8 LIMITATIONS AND FUTURE WORK
While we were able to find a surprising number of vulnerable apps,
our system is far from perfect. There are a few conceptual ways in
which our system might miss vulnerable apps. The most important
one is the inability to influence a change in the user’s state stored by
the app’s backend server. For instance, in some games, the dynamic
analysis system would need to effectively play the game and, for
example, score points or spend virtual currency.
An important source of error in dynamic analysis is the
non-determinism inherent in today’s operating systems and
apps. Some apps explicitly perform random behaviors, which
our Invariant Generation step attempts to remove, but it is by no
means perfect. For example, if the non-deterministic behaviors
are time-dependent or influenced by network delays, they may
produce the same result during the Invariant generation, but not
during the other phases. Some previous work has been done to
try to have fully deterministic replay of actions (see Section 9.2),
but the current state-of-the-art does not handle all the source of
indeterminism that our system has to deal with.
One other source of future improvement is in the number of
identifiers spoofed and transferred by our system. We used a large
set of known identifiers for which we could locate Android APIs,
but apps could conceivably invent their own identifiers based on
collections of obscure system properties, or implement other means
of fingerprinting devices. Finding all possible ways this can happen
is an open problem.
We would also like to explore the use of network traffic as part
of the Invariant generation, to attempt to more precisely determine
when a backend is saving and retrieving user state. In particular, a
way to assist with the network traffic analysis, as well as other data
sources and sinks, is to use a taint-tracking-based analysis system,
such as TaintDroid [11]. Unfortunately, we have noticed that many