---
author: A. Jesse Jiryu Davis , Guido van Rossum
category: 软件开发
comments_data: []
count:
  commentnum: 0
  favtimes: 1
  likes: 0
  sharetimes: 0
  viewnum: 10374
date: '2017-03-06 10:31:00'
editorchoice: false
excerpt: 在最后一个阶段，我们将使用 Python 标准库“asyncio”中功能完整的协程， 并通过异步队列完成这个网络爬虫。
fromurl: http://aosabook.org/en/500L/pages/a-web-crawler-with-asyncio-coroutines.html
id: 8267
islctt: true
largepic: /data/attachment/album/201703/04/160319vrfrrsl2x2lrlpzp.jpg
permalink: /article-8267-1.html
pic: /data/attachment/album/201703/04/160319vrfrrsl2x2lrlpzp.jpg.thumb.jpg
related: []
reviewer: ''
selector: ''
summary: 在最后一个阶段，我们将使用 Python 标准库“asyncio”中功能完整的协程， 并通过异步队列完成这个网络爬虫。
tags:
- asyncio
- 协程
- 回调
- 异步
- Python
- 爬虫
thumb: false
title: 一个使用 asyncio 协程的网络爬虫（三）
titlepic: true
translator: qingyunha
updated: '2017-03-06 10:31:00'
---
> 
> 本文作者：
> 
> 
> A. Jesse Jiryu Davis 是纽约 MongoDB 的工程师。他编写了异步 MongoDB Python 驱动程序 Motor，也是 MongoDB C 驱动程序的开发领袖和 PyMongo 团队成员。 他也为 asyncio 和 Tornado 做了贡献，在  上写作。
> 
> 
> Guido van Rossum 是主流编程语言 Python 的创造者，Python 社区称他为 BDFL （仁慈的终生大独裁者 (Benevolent Dictator For Life)）——这是一个来自 Monty Python 短剧的称号。他的主页是 [http://www.python.org/~guido/](http://www.python.org/%7Eguido/) 。
> 
> 
> 
![](/data/attachment/album/201703/04/160319vrfrrsl2x2lrlpzp.jpg)
### 使用协程
我们将从描述爬虫如何工作开始。现在是时候用 asynio 去实现它了。
我们的爬虫从获取第一个网页开始，解析出链接并把它们加到队列中。此后它开始傲游整个网站，并发地获取网页。但是由于客户端和服务端的负载限制，我们希望有一个最大数目的运行的 worker，不能再多。任何时候一个 worker 完成一个网页的获取，它应该立即从队列中取出下一个链接。我们会遇到没有那么多事干的时候，所以一些 worker 必须能够暂停。一旦又有 worker 获取一个有很多链接的网页，队列会突增，暂停的 worker 立马被唤醒干活。最后，当任务完成后我们的程序必须马上退出。
假如你的 worker 是线程，怎样去描述你的爬虫算法？我们可以使用 Python 标准库中的[同步队列](https://docs.python.org/3/library/queue.html)。每次有新的一项加入，队列增加它的 “tasks” 计数器。线程 worker 完成一个任务后调用 `task_done`。主线程阻塞在 `Queue.join`，直到“tasks”计数器与 `task_done` 调用次数相匹配，然后退出。
协程通过 asyncio 队列，使用和线程一样的模式来实现！首先我们[导入它](https://docs.python.org/3/library/asyncio-sync.html)：
```
try:
    from asyncio import JoinableQueue as Queue
except ImportError:
    # In Python 3.5, asyncio.JoinableQueue is
    # merged into Queue.
    from asyncio import Queue
```
我们把 worker 的共享状态收集在一个 crawler 类中，主要的逻辑写在 `crawl` 方法中。我们在一个协程中启动 `crawl`,运行 asyncio 的事件循环直到 `crawl` 完成：
```
loop = asyncio.get_event_loop()
crawler = crawling.Crawler('http://xkcd.com',
                           max_redirect=10)
loop.run_until_complete(crawler.crawl())
```
crawler 用一个根 URL 和最大重定向数 `max_redirect` 来初始化，它把 `(URL, max_redirect)` 序对放入队列中。（为什么要这样做，请看下文）
```
class Crawler:
    def __init__(self, root_url, max_redirect):
        self.max_tasks = 10
        self.max_redirect = max_redirect
        self.q = Queue()
        self.seen_urls = set()
        # aiohttp's ClientSession does connection pooling and
        # HTTP keep-alives for us.
        self.session = aiohttp.ClientSession(loop=loop)
        # Put (URL, max_redirect) in the queue.
        self.q.put((root_url, self.max_redirect))
```
现在队列中未完成的任务数是 1。回到我们的主程序，启动事件循环和 `crawl` 方法：
```
loop.run_until_complete(crawler.crawl())
```
`crawl` 协程把 worker 们赶起来干活。它像一个主线程：阻塞在 `join` 上直到所有任务完成，同时 worker 们在后台运行。
```
    @asyncio.coroutine
    def crawl(self):
        """Run the crawler until all work is done."""
        workers = [asyncio.Task(self.work())
                   for _ in range(self.max_tasks)]
        # When all work is done, exit.
        yield from self.q.join()
        for w in workers:
            w.cancel()
```
如果 worker 是线程，可能我们不会一次把它们全部创建出来。为了避免创建线程的昂贵代价，通常一个线程池会按需增长。但是协程很廉价，我们可以直接把他们全部创建出来。
怎么关闭这个 `crawler` 很有趣。当 `join` 完成，worker 存活但是被暂停：他们等待更多的 URL，所以主协程要在退出之前清除它们。否则 Python 解释器关闭并调用所有对象的析构函数时，活着的 worker 会哭喊到：
```
ERROR:asyncio:Task was destroyed but it is pending!
```
`cancel` 又是如何工作的呢？生成器还有一个我们还没介绍的特点。你可以从外部抛一个异常给它：
```
>>> gen = gen_fn()
>>> gen.send(None)  # Start the generator as usual.
1
>>> gen.throw(Exception('error'))
Traceback (most recent call last):
  File "", line 3, in 
  File "", line 2, in gen_fn
Exception: error
```
生成器被 `throw` 恢复，但是它现在抛出一个异常。如过生成器的调用堆栈中没有捕获异常的代码，这个异常被传递到顶层。所以注销一个协程：
```
    # Method of Task class.
    def cancel(self):
        self.coro.throw(CancelledError)
```
任何时候生成器暂停，在某些 `yield from` 语句它恢复并且抛出一个异常。我们在 task 的 `step` 方法中处理注销。
```
    # Method of Task class.
    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except CancelledError:
            self.cancelled = True
            return
        except StopIteration:
            return
        next_future.add_done_callback(self.step)
```
现在 task 知道它被注销了，所以当它被销毁时，它不再抱怨。
一旦 `crawl` 注销了 worker，它就退出。同时事件循环看见这个协程结束了（我们后面会见到的），也就退出。
```
loop.run_until_complete(crawler.crawl())
```
`crawl` 方法包含了所有主协程需要做的事。而 worker 则完成从队列中获取 URL、获取网页、解析它们得到新的链接。每个 worker 独立地运行 `work` 协程：
```
    @asyncio.coroutine
    def work(self):
        while True:
            url, max_redirect = yield from self.q.get()
            # Download page and add new links to self.q.
            yield from self.fetch(url, max_redirect)
            self.q.task_done()
```
Python 看见这段代码包含 `yield from` 语句，就把它编译成生成器函数。所以在 `crawl` 方法中，我们调用了 10 次 `self.work`，但并没有真正执行，它仅仅创建了 10 个指向这段代码的生成器对象并把它们包装成 Task 对象。task 接收每个生成器所 yield 的 future，通过调用 `send` 方法，当 future 解决时，用 future 的结果做为 `send` 的参数，来驱动它。由于生成器有自己的栈帧，它们可以独立运行，带有独立的局部变量和指令指针。
worker 使用队列来协调其小伙伴。它这样等待新的 URL：
```
    url, max_redirect = yield from self.q.get()