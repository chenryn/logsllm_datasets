Anomaly scores for speci(cid:2)c packets are based on devi-
ations from expected values in a predetermined pro(cid:2)le.
Once a threshold is exceeded, an alarm is generated. They
based their experimental analysis on a prototype that pro-
cessed both HTTP and DNS network traf(cid:2)c.
Granger et al. [10] present a software architecture to en-
able self-securing network interfaces to examine packets
as they move between network links and host software,
detecting and potentially blocking malicious activity. This
host-based approach includes a detection technique that
enables detection of worm propagation. The technique in-
volves shadowing a host·s DNS table and checking the IP
address of each new connection against it. The basic con-
struct of this approach is that its abnormal for a host to
make a large number connection attempts without DNS
activity. We have extended it to a network-based solu-
tion that incorporates additional network information (i.e.
whitelists, embedded IP addresses in HTTP packets) that
hosts use to initiate new connections.
Williamson [27] devised a method to limit or throttle the
rate of malicious mobile code by determining if a system
tries to connect to new addresses. If so, the connection is
delayed in order to slow the propagation of the malicious
code. By not dropping the connection, a balance is struck
between reducing the impact of false alarms and limiting
the spread of malicious activity in the network.
Jung et al. [11] developed an algorithm called Thresh-
old Random Walk (TRW), to identify malicious remote
hosts. They based this algorithm on the observation that
scanners are more likely to access hosts and services that
do not exist than legitimate remote hosts. If the connec-
tion is determined to succeed the random walk is driven
upwards, failure to succeed drives the random walk down.
By giving legitimate network traf(cid:2)c a higher probability
to succeed than attack traf(cid:2)c, a determination can be made
on whether a series of connection attempts is a scan.
Weaver et al. [25] developed a scan detection and sup-
pression algorithm based on a simpli(cid:2)cation of the TRW.
They use caches to track the activity of both new connec-
tions and IP addresses to reduce the random walk calcula-
tion in the TRW. This simplication made the algorithm
suitable for implementation in both hardware and soft-
ware. Their technique allows a scanning host to be de-
tected and stopped in fewer than 10 scans with a low false
positive rate.
Two commercial scanning detectors offer an alternate
approach to detect worm propagation. Forescout [1] and
Mirage [2] networks use a technique called dark-address
detection. These detectors either have knowledge of or
route unoccupied address spaces within an internal net-
work and detects when systems attempt to connect to these
unused spaces.
Silicon Defense developed the CounterMalice worm de-
fense solution [7] to proactively identify and automati-
cally block worm activity in an internal network. The so-
lution divides the network in cells and prevents the worms
from spreading between the cells. Staniford’s analysis of
worm propagation within an enterprise network revealed
that such outbreaks exhibit a phased structure with an epi-
demic threshold [20]. If the network contains a low den-
sity of susceptible systems or if the threshold is low, con-
tainment systems can be effective in containing the worm
outbreak. If these parameters are too large however, the
containment systems will not be able to stop the spread
of the worm until a signi(cid:2)cant number of systems are in-
fected.
Zou et al. [28] model requirements for the dynamic
quarantine of infected hosts. They demonstrate that epi-
demic thresholds exist for differing detection and response
times. This work provides a benchmark against which the
ef(cid:2)ciency of any new proposed detection algorithm should
take into account. We believe that our scanning worm de-
tection approach has the required ef(cid:2)ciency to stop worm
propagation before epidemic thresholds are reached.
8 Concluding Remarks
The DNS-based worm propagation detection approach
is an effective way to combat scanning worm infec-
tion within appropriate enterprise networks (see Section
1). Depending on the network environment and secu-
rity policy however, the number of protocols added to
the whitelist may potentially limit the applicability of this
technique as a stand alone detector. In these scenarios, this
detection method could be used as an additional detection
signal in concert with other worm detection schemes in-
stead of being used as the primary detection technique.
During evaluation, our prototype was successful in de-
tecting scanning worm propagation in the Internal De-
partmental Network cell of our enterprise network. We
have demonstrated that this network-based detection ap-
proach can be used in certain network environments to
offer signi(cid:2)cant improvement in detection speed over ex-
isting scanning worm propagation detection methods. Re-
gardless of the scanning rate, the detection algorithm is
able to detect scanning worm propagaton in a single scan-
ning attempt. It relies on a network service found in ev-
ery network (i.e. DNS), and being anomaly-based, has the
ability to detect emerging worms. We have developed a
full implementation of our approach in a software proto-
type that runs on non-specialized commodity hardware.
We plan to make the software available to the public.
Finally, we believe that this detection approach could
be easily modi(cid:2)ed to detect additional classes of mali-
cious activity including: covert channel detection, mass-
mailing worms, automated scanning tools, and remote to
local worm propagation.
In fact, we have already ex-
tended these ideas in an analogous worm detection im-
plementation based on the Address Resolution Protocol
as noted in Section 2.
Acknowledgements
We thank Anil Somayaji and the anonymous review-
ers for comments which signi(cid:2)cantly improved this pa-
per. The second author is supported in part by NSERC
(Natural Sciences and Engineering Research Council of
Canada) and MITACS (Mathematics of Information Tech-
nology and Complex Systems) grants. The third author
is the Canada Research Chair in Network and Software
Security, and is supported in part by an NSERC Discov-
ery Grant, the Canada Research Chairs Program, and MI-
TACS.
References
[1] Forescout. Wormscout. http://www.forescout.com/
wormscout.html.
[2] Mirage Networks. http://www.miragenetworks.com.
[3] Optixpro trojan horse.
http://securityresponse1.
symantec.com/sarc/sarc.nsf/html/backdoor.optixpro
.12.html.
[4] Secure Shell Protocol (secsh). http://www.ietf.org/
html.charters/secsh-charter.html; accessed on Octo-
ber 24, 2004.
[5] tcpdump/libpcap
www.tcpdump.org.
public
repository.
http://
[6] August was Worst Month Ever for Viruses, Worms.
Technet News, September 2003.
[7] Worm containment in the internal network. Techni-
cal report, Silicon Defense, 2003.
[8] M. Crispin.
Internet Message Access Proto-
col. March 2003. http://www.ietf.org/rfc/rfc3501.
txt?number=3501; accessed October 22, 2004.
[9] D. Ellis, J. Aiken, K. Attwood, and S. Tenaglia. A
behavioral approach to worm detection. In To Ap-
pear in Proceedings of The Workshop on Rapid Mal-
code, 2003.
[10] G. Granger, G. Economou, and S. Bielski. Self-
securing network interfaces: What, why and how.
Technical report, Carnegie Mellon Iniversity, CMU-
CS-02-144, May 2002.
[11] J. Jung, V. Paxson, A. Berger, and H. Balakrishman.
Fast portscan detection using sequential hypothesis
testing. In 2004 IEEE Symposium on Security and
Privacy, 2004.
[12] C. Kruegel, T. Toth, and E. Kirda. Service speci(cid:2)c
anomaly detection for intrusion detection. Technical
report, TU-1841-2002-28, 2002.
[13] D. Mills. Network Time Protocol (Version 3). RFC,
March 1992. http://www.ietf.org/rfcs/rfc1305.txt?
number=1305; accessed October 24, 2004.
[14] D. Moore, V. Paxson, S. Savage, C. Shannon,
S. Staniford, and N. Weaver.
Inside the slammer
worm. In IEEE Magazine of Security and Privacy,
pages 33(cid:150)39, July/August 2003.
[15] D. Moore, C. Shannon, G. Voelker, and S. Sav-
age. Internet quarantine: Requirements for contain-
ing self-propagating code.
In Proceedings of the
2003 IEEE Infocom Conference, San Francisco, CA,
April 2003.
[16] R. Pethia. Attacks on the Internet 2003. Congres-
sional Testimony, Subcommittee on Telecommunica-
tions and the Internet, November 2003.
[17] D. Plummer.
olution Protocol.
http://www.ietf.org/rfc/rfc0826.txt?number=
accessed on October 24, 2004.
An Ethernet Address Res-
RFC, November 1982.
826;
[18] J. Postel
and J. Reynolds.
File Trans-
fer Protocol
RFC, October 1985.
http://www.ietf.org/rfc/rfc959. txt?number=959; ac-
cessed October 24, 2004.
(FTP).
[19] C. Shannon and D. Moore. The spread of the witty
worm. Technical report, CAIDA, March 2004.
[20] S. Staniford. Containment of scanning worms in en-
terprise networks. In Journal of Computer Science,
to appear, 2004.
[21] S. Staniford, V. Paxson, and N. Weaver. How to 0wn
the internet in your spare time. In Proceedings of the
11th USENIX Security Symposium, August 2002.
[22] N. Weaver.
Potential strategies for high speed
2002.
pdf;
active worms: A worst case analysis.
http://www.cs.berkeley.edu/(cid:152)nweaver/worms.
last accessed October 20, 2004.
[23] N. Weaver and D. Ellis. Re(cid:3)ections on Witty. ;login:
The USENIX Magazine, 29(3):34(cid:150)37, June 2004.
[24] N. Weaver, V. Paxson, S. Staniford, and R. Cunning-
ham. A taxonomy of computer worms. In The First
ACM Workshop on Rapid Malcode (WORM), Octo-
ber 2003.
[25] N. Weaver, S. Staniford, and V. Paxson. Very fast
containment of scanning worms. In Proceedings of
the 13th USENIX Security Symposium, 2004.
[26] D. Whyte, E. Kranakis, and P. C. van Oorschot. Arp-
based detection of scanning worms in an enterprise
network. Technical report, School of Computer Sci-
ence, Carleton University, October 2004.
[27] M. Williamson. Throttling viruses: Restricting prop-
agation to defeat malicious mobile code. In Annual
Computer Security Applications Conference, 2002.
[28] C. Zou, L. Gao, W. Gong, and D. Towsley. Moni-
toring and early warning for Internet worms. In In
Proceedings of the 10th ACM Conference on Com-
puter and Communications Security, 2003.
A Background
A.1 Worm Propagation Methods
A thorough discussion of worm classi(cid:2)cations based
on worm target discovery and selection strategies, carrier
mechanisms, activation, payloads, and types of attackers
is found in [24]. Worms are typically classi(cid:2)ed based on
two attributes: methods used to spread and the techniques
used to exploit vulnerabilities. Most worms propagate by
using indiscriminate scanning of the Internet to identify
vulnerable systems. As revealed by Slammer, the faster a
worm can locate systems the more rapid the infection rate.
Staniford et al.’s study [21] used empirical data from ac-
tual worm outbreaks to reveal a common effective prop-
agation strategy, random constant spread (RCS) model,
wherein a worm randomly scanning through the entire In-
 systems, searching for vulner-
ternet address space, of 2
able systems.
Traditional Propagation Methods. The limiting fac-
tors which dictate how fast a worm can spread are: (1) the
rate of scanning used to detect vulnerable systems, (2) the
population of vulnerable systems, (3) the time required
to infect vulnerable systems, and (4) their resistance to
countermeasures [22]. The spread of a random scanning
worm can be described in three phases: the slow spread-
ing phase, fast spreading phase, and slow (cid:2)nishing phase
[28].
In the slow spreading phase, the worm is building up an
initial base of infected systems. Although it is infecting
systems at an exponential rate, the small initial population
limits the propagation speed. Once a certain threshold of
infected hosts is reached, the worm begins the fast spread-
ing phase. Models derived from actual worm data indicate
that this threshold is approximately 10,000 systems [21].
Worms use a number of different scanning strategies to
propagate. Scans can be focused on speci(cid:2)c groups of
systems (e.g. subnet scanning) that are phyically or log-
ically connected together. Some scanning strategies use
information such as URL caches, peer-to-peer connec-
tions, trusted network connections, and email addresses
harvested from their victims (e.g.
topological scanning)
to target potentially susceptible hosts [21].
In addition to scanning, worms have also used mass
email and network shares to propagate. Using built-in
emailers, worms can harvest email addresses from exist-
ing email address books, the inbox of the email client, and
web page caches. Copies of the worm are then sent to
all the harvested email entries. Through shared network
drives, systems often have access to directories on other
systems. By placing itself in a shared system, the worm
can use this shared access to infect other systems. The
worm can also take a more active role and change permis-
sions on directories or add guest accounts.
Hyper Virulent Worm Propagation Strategies.
Weaver et al. [21] describe a number of possible hyper
virulent worm propagation strategies that includes hit-list
and permutation scanning. A hit-list is a list of vulnerable
systems targeted for infection. Typically, a hit-list is gen-
erated by previous reconnaissance activities such as: net-
work scanning, web surveys, DNS queries, and web spi-
ders. A hit-list is used to allow a worm to rapidly spread
in the (cid:2)rst few minutes. This increases its virulence and
its chances of survival. Permutation scanning is a strategy
to increase scanning ef(cid:2)ciency. Random scanning can be
inef(cid:2)cient because many addresses may be probed multi-
ple times. Here, worms share a common pseudo random
permutation of the Internet address space. Infected sys-
tems start scanning at a (cid:2)xed point in the permutation.
If the worm detects an infected system, it simply picks a
new random point in the permutation and begins scanning
again. This prevents reinfection and imposes a measure
of coordination on the worm. Until the Witty worm [19],
these strategies have not appeared in the wild.
A.2 DNS Review
DNS is a globally distributed hierarchical database that
provides a mapping between numeric IP addresses and al-
phanumeric domain names. Whenever access to service
occurs that uses a domain name to locate a server, DNS is
used. A domain name is a human friendly pseudonym for
a systems IP address.
DNS queries are performed on behalf of the user by a
resolver, an application installed on the user’s local sys-
tem to query the local DNS server whose location is spec-
i(cid:2)ed during the system’s network connection con(cid:2)gura-
tion. The resolver contacts the local DNS server with the
domain name provided by an application. If the local DNS
server doesn’t know the IP address for the requested do-
main name, it queries external DNS servers to resolve the
domain name. If the external DNS servers do not know
the information for the domain name, they respond to the
querying local DNS server with the address of an author-
itative DNS server higher up the chain. A server is con-
sidered authoritative about a domain if it can respond to a
query with certainty that the name exists.
If a system or user has a priori knowledge of the IP ad-
dress of another system it needs to access, a DNS query
can be avoided. However, the majority of accesses to re-
mote systems are initiated by specifying the domain name
in a client application. DNS resource records are the dis-
crete data structures used to store information about the
structure and content of the entire domain name space.
There exists a variety of DNS resource records. The re-
source record of interest with respect to our detection ap-
proach is the authoritative resource record or A record,
which maps a fully quali(cid:2)ed domain name to an IP ad-
dress. The mapping between domain names and numeric
IP addresses can change over time as new services are
added or as networks change. Each DNS record has an
associated Time to Live (TTL) value, the number of sec-
onds that the mapping will be guaranteed to be valid. The
TTL dictates how long the resource record will be kept in
the DNS server’s cache. Caching resource records enables
a DNS server to reduce the number of requests it needs to
make to other name servers. Although the TTL value can
be as low as a few seconds, in practice the default TTL is
one day.
TTL values are associated with all DNS replies. A TTL
value provides a mechanism to allow resource records to
expire so that the information they contain can be updated
periodically in case changes to the network topology are
made.