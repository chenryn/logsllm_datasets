### Detailed Explanation of CGC Dataset Results

In this section, we provide a detailed explanation of the annotations used to solve the 10 targets in the CGC dataset, as presented in Section V-F (see Table VIII for details). The exact time spent on solving each individual target is challenging to measure precisely, as small changes often require subsequent fuzzing that can last from minutes to hours without human intervention. Therefore, we can only provide rough estimates of the time spent on the implementation. For the shorter examples (less than 1 hour), many were solved with just a few minutes of human attention. Most of the examples required maximizing a single loop counter, index, or pointer, combined with various string comparison techniques. However, some cases, such as NRFIN 00004, NRFIN 00041, and CROMU 00020, involved multiple steps. To illustrate these more complex solutions, we will discuss the techniques used in three case studies.

#### Table VIII: Solving CGC Challenges
| Target        | LOC | Effort  | Comment                         |
|---------------|-----|---------|---------------------------------|
| CROMU 00011   | 2   | < 1h    | strcmp, maximize index          |
| NRFIN 00030   | 1   | < 1h    | maximize index                  |
| NRFIN 00004*  | 5   | < 5h    | strcmp*, maximize index         |
| NRFIN 00076†  | 1   | < 1h    | strcmp                          |
| NRFIN 00041*  | 4   | < 1h    | checksum*                       |
| CROMU 00020*  | 3   | < 5h    | challenge response*             |
| NRFIN 00005†  | 1   | < 1h    | strcmp                          |
| NRFIN 00012†  | 2   | < 5h    | strcmp                          |
| NRFIN 00038†  | 1   | < 1h    | strcmp                          |
| NRFIN 00049†  | 1   | < 1h    | strcmp                          |

*Indicates that the solution is discussed in detail.
†Indicates that the `strcmp` could be solved using a properly chosen dictionary.

### Case Studies

#### A. NRFIN 00004 (HeartThrob)
This program uses a fully unrolled prefix tree (trie) to perform string comparisons. The function has approximately 30,000 different branches, leading to a large number of possible paths. The fuzzer explores all inputs equally, filling the bitmap with useless inputs. After diagnosing the problem, it took less than 20 minutes to build a script that extracts relevant strings from the trie and disables coverage feedback within the function. Using these strings, we achieved the necessary coverage. Finally, we used the `IJON-MAX` primitive to trigger an out-of-bounds (OOB) crash.

#### B. NRFIN 00041 (AIS-Lite)
This program uses an encoded checksum to guard the bug. After manually removing the checksum check, the bug was found quickly. To obtain a valid input without understanding the format or the checksum, we used an `IJON_CMP` annotation on the checksum check to produce a valid input (while still using the patched target). AFL's crash exploration mode ensured that the fuzzer did not remove the cause of the crash while fixing the checksum. The fixed input triggers the crash in the unmodified binary. This approach is similar to those used by T-FUZZ [43] and REDQUEEN [7], but instead of leveraging symbolic execution or colorization, we used the fuzzer itself to provide the fixed input.

#### C. CROMU 00020 (Estadio)
Similar to the previous case study, we used a "patch, check, and fix the crashing input afterwards" approach to solve this target. However, instead of a checksum, a series of challenge-response messages were required to trigger the bug.

### References
[References listed as provided, with no changes.]

This revised version provides a clearer and more structured explanation of the results, making it easier to follow the methodology and understand the specific challenges and solutions for each target.