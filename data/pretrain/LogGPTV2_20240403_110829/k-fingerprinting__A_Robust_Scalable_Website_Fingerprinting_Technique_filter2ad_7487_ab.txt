gerprints collected during training and is classiﬁed as a
monitored page if and only if all k ﬁngerprints agree on
classiﬁcation, as described in Section 3.1, otherwise it is
classiﬁed as an unmonitored page.
We deﬁne the following performance measures for the
attack:
• True Positive Rate (TPR). The probability that a
monitored page is classiﬁed as the correct monitored
page.
• False Positive Rate (FPR). The probability that an un-
monitored page is incorrectly classiﬁed as a monitored
page.
• Bayesian Detection Rate (BDR). The probability that
a page corresponds to the correct monitored page given
that the classiﬁer recognized it as that monitored page.
Assuming a uniform distribution of pages BDR can be
found from TPR and FPR using the formula
T PR.Pr(M)
(T PR.Pr(M) +FPR. Pr(U))
where
Pr(M) = |Monitored|
|Total Pages|
, Pr(U) =1 − P(M).
Ultimately BDR indicates the practical feasibility of the
attack as it measures the main concern of the attacker, the
probability that the classiﬁer made a correct prediction.
4 Data gathering
We collect two data sets: one via Tor7 DSTor, and an-
other via a standard web browser, DSNorm. DSNorm
consists of 30 instances from each of 55 monitored
web pages, along with 7,000 unmonitored web pages
chosen from Alexa’s top 20,000 web sites [1]. We
collected DSNorm using a number of Amazon EC2
7The most recent version at the time of collection was used, Tor
Browser 5.0.6.
instances8, Selenium9 and the headless browser
PhantomJS10. We used tcpdump11 to collect network
traces for 20 seconds with 2 seconds between each web
page load. Monitored pages were collected in batches
of 30 and unmonitored web pages were collected suc-
cessively. Page loading was performed with no caches
and time gaps between multiple loads of the same web
page, as recommended by Wang and Goldberg [38]. We
chose to monitor web pages from Alexa’s top 100 web
sites [1] to provide a comparison with the real world cen-
sored web pages used in the Wang et al. [39] data set12.
DSTor was collected in a similar manner to DSNorm but
was collected via the Tor browser. DSTor consists of two
subsets of monitored web pages: (i) 100 instances from
each of the 55 top Alexa monitored web pages and (ii)
80 instances from each of 30 popular Tor hidden ser-
vices. A Tor hidden service is a website that is hosted
behind a Tor client’s Onion Proxy, which serves as the
interface between application and network. Tor hidden
services allow both a client accessing the website and the
server hosting the website to remain anonymous to one
another and any external observers. We chose hidden ser-
vices to ﬁngerprint based on popularity as listed by the
.onion search engine Ahmia13. The unmonitored set
is comprised of the top 100,000 Alexa web pages, ex-
cluding the top 55. We chose to ﬁngerprint web pages
as listed by Alexa as these constitute the most popular
web pages in the world over extended periods of time,
and hence provide a more realistic dataset than choosing
pages at random and/or using transiently popular website
links as included in Panchenko et al.’s recent work [28].
By including website visits to trending topics we argue
that this diminishes the ability to properly measure how
effective a website ﬁngerprinting attack will perform in
general.
For comparison to previous work, we evaluated our at-
tack on one of the previous largest website ﬁngerprinting
data sets [39], which collected 90 instances from each of
100 monitored sites, along with 5000 unmonitored web
pages. The Wang et al. monitored web pages are various
real-world censored websites from UK, Saudi Arabia and
China providing a realistic set of web pages an attacker
may wish to monitor. The unmonitored web pages are
chosen at random from Alexa’s top 10,000 websites –
with no intersection between monitored and unmonitored
web pages.
8https://aws.amazon.com/ec2/
9http://www.seleniumhq.org/
10http://phantomjs.org/
11http://www.tcpdump.org/
12We used TCP/IP packets for ﬁnal classiﬁcation over abstracting
to the Tor cell layer [38], preliminary experiments showed no consis-
tent improvements from using one data layer for classiﬁcation over the
other.
13http://www.ahmia.fi/
1190  25th USENIX Security Symposium 
USENIX Association
4
This allows us to validate k-ﬁngerprinting on two
different data sets while allowing for direct compari-
son against the state-of-the-art k-Nearest Neighbor at-
tack [39]. We can also infer how well the attack works
on censored web pages which may not have small land-
ing pages or be set up for caching like websites in the top
Alexa list. Testing k-ﬁngerprinting on both real-world
censored websites and top alexa websites indicates how
the attack performs across a wide range of websites.
For the sake of comparison, according to a study by
research ﬁrm Nielsen [3] the number of unique websites
visited per month by an average client in 2010 was 89.
Another study [17, 26] collected web site statistics from
80 volunteers in a virtual ofﬁce environment. Trafﬁc was
collected from each volunteer for a total of 40 hours. The
mean unique number of websites visited per volunteer
was 484, this is substantially smaller than the world sizes
we consider in our experiments.
5 Feature selection
Our ﬁrst contribution is a systematic analysis of fea-
ture importance. Despite some preliminary work by
Panchenko et al. [27], there is a notable absence of fea-
ture analysis in the website ﬁngerprinting literature. In-
stead features are picked based on heuristic arguments.
All feature importance experiments were performed with
the Wang et al. data set [39] so as to allow direct com-
parison with their attack results.
We train a random forest classiﬁer in the closed-world
setting using a feature vector comprised of features in
the literature, and labels corresponding to the moni-
tored sites. We use the gini coefﬁcient as the purity
criterion for splitting branches and estimate feature im-
portance using the standard methodology described by
Breiman [2, 6, 13]. Each time a decision tree branches
on a feature the weighted sum of the gini impurity index
for the two descendant nodes is higher than the purity of
the parent node. We add up the gini decrease for each in-
dividual feature over the entire forest to get a consistent
measure of feature importance.
Figure 1 illustrates the effect of using a subset of fea-
tures for random forest classiﬁcation. We ﬁrst train a
random forest classiﬁer to establish feature importance;
and then train new random forests with only subsets of
the most informative features in batches of ﬁve. As we
increase the number of features we observe a monotonic
increase in accuracy; however there are diminishing re-
turns as we can achieve nearly the same accuracy after
using the 30 most important features. We chose to use
150 features in all following experiments since the dif-
ference in training time when using fewer features was
negligible.
Figure 2 identiﬁes the top-20 ranked features and illus-
trates their variability across 100 repeated experiments.
Figure 1: Accuracy of k-ﬁngerprinting in a closed-world
setting as the number of features is varied.
As seen in Figure 1 there is a reduction in gradient when
combining the top 15 features compared to using the top
10 features. Figure 2 shows that the top 13 features are
comparatively much more important than the rest of the
top 20 features, hence there is only a slight increase in
accuracy when using the top 15 features compared to us-
ing the top 10. After the drop between the rank 13 and
rank 14 features, feature importance falls steadily until
feature rank 40, after which the reduction in feature im-
portance is less prominent14. Note that there is some
interchangeability in rank between features, we assign
ranks based on the average rank of a feature over the 100
experiments.
Feature Importance
From each packet sequence we extract the following fea-
tures:
• Number of packets statistics. The total number of
packets, along with the number of incoming and out-
going packets [12, 27, 39]. The number of incoming
packets during transmission is the most important fea-
ture, and together with the number of outgoing packets
during transmission are always two of the ﬁve most im-
portant features. The total number of packets in trans-
mission has rank 10.
• Incoming & outgoing packets as fraction of total
packets. The number of incoming and outgoing pack-
ets as a fraction of the total number of packets [27].
Always two of the ﬁve most important features.
• Packet ordering statistics. For each successive in-
coming and outgoing packet, the total number of pack-
ets seen before it in the sequence [7, 27, 39]. The stan-
dard deviation of the outgoing packet ordering list has
rank 4, the average of the outgoing packet ordering list
has rank 7. The standard deviation of the incoming
packet ordering list has rank 12 and the average of the
14The total feature importance table is shown in Appendix A.
5
USENIX Association  
25th USENIX Security Symposium  1191
packets feature list. The outgoing packets feature list
split into 20 evenly sized subsets and sum each sub-
set. This creates a new list of features. Similarly to the
concentration feature list, the alternative concentration
feature list are regularly in the top 20 most important
features and bottom 50 features. Note though concen-
tration features are never seen in the top 15 most im-
portant features whereas alternative concentration fea-
tures are, – at rank 14 and 15, – so information is
gained by summing the concentration subsets.
• Packet inter-arrival time statistics. For the total, in-
coming and outgoing packet streams extract the lists
of inter-arrival times between packets. For each list
extract the max, mean, standard deviation, and third
quartile [5]. These features have rank between 40 and
70.
• Transmission time statistics. For the total, incom-
ing and outgoing packet sequences we extract the ﬁrst,
second, third quartile and total transmission time [39].
These features have rank between 30 and 50. The total
transmission time for incoming and outgoing packet
streams are the most important out of this subset of
features.
• Alternative number of packets per second features.
For the number of packets per second feature list we
create 20 even sized subsets and sum each subset. The
sum of all subsets is the 9th most important feature.
The features produced by each subset are in the bottom
50 features - with rank 101 and below. The important
features in this subset are the ﬁrst few features with
rank between 66 and 78, that are calculated from the
ﬁrst few seconds of a packet sequence.
We conclude that the total number of incoming pack-
ets is the most informative feature. This is expected as
different web pages have different resource sizes, that
are poorly hidden by encryption or anonymization. The
number of incoming and outgoing packets as a fraction
of the total number of packets are also informative for
the same reason.
The least important features are from the padded con-
centration of outgoing packets list, since the original con-
centration of outgoing packets lists were of non-uniform
size and so have been padded with zeros to give uni-
form length. Clearly, if most packet sequences have been
padded with the same value this will provide a poor cri-
terion for splitting, hence being a feature of low impor-
tance. Packet concentration statistics, while making up
the bulk of “useless features” also regularly make up a
few of the top 30 most important features, they are the
ﬁrst few items that are unlikely to be zero.
In other
words, the ﬁrst few values in the packet concentration
list do split the data well.
Packet ordering features have rank 4, 7, 12 and 13,
indicating these features are a good criterion for classiﬁ-
Feature Description
Number of incoming packets.
Number of outgoing packets as a fraction of the total number
of packets.
Number of incoming packets as a fraction of the total number
of packets.
Standard deviation of the outgoing packet ordering list.
Number of outgoing packets.
Sum of all items in the alternative concentration feature list.
Average of the outgoing packet ordering list.
Sum of incoming, outgoing and total number of packets.
Sum of alternative number packets per second.
Total number of packets.
Packet concentration and ordering features list.
The total number of incoming packets stats in ﬁrst 30 packets.
The total number of outgoing packets stats in ﬁrst 30 packets.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11-18.
19.
20.
Figure 2: The 20 most important features.
incoming packet ordering list has rank 13.
• Concentration of outgoing packets. The packet se-
quence split into non-overlapping chunks of 20 pack-
ets. Count the number of outgoing packets in each of
the chunks. Along with the entire chunk sequence, we
extract the standard deviation (rank 16), mean (rank
11), median (rank 64) and max (rank 65) of the se-
quence of chunks. This provides a snapshot of where
outgoing packets are concentrated [39]. The features
that make up the concentration list are between the 15th
and 30th most important features, but also make up the
bulk of the 75 least important features.
• Concentration of incoming & outgoing packets in
ﬁrst & last 30 packets. The number of incoming and
outgoing packets in the ﬁrst and last 30 packets [39].
The number of incoming and outgoing packets in the
ﬁrst thirty packets has rank 19 and 20, respectively.
The number of incoming and outgoing packets in the
last thirty packets has rank 50 and 55, respectively.
• Number of packets per second. The number of pack-
ets per second, along with the mean (rank 44), standard
deviation (rank 38), min (rank 117), max (42), median
(rank 50).
• Alternative concentration features. This subset of
features is based on the concentration of outgoing
1192  25th USENIX Security Symposium 
USENIX Association
6
cation. Packet ordering features exploit the information
leaked via the way in which browsers request resources
and the end server orders the resources to be sent. This
supports conclusions in [7, 39] about the importance of
packet ordering features.
We also found that the number of incoming and out-
going packets in the ﬁrst thirty packets, with rank 19 and
20, were more important than the number of incoming
and outgoing packets in the last thirty packets, with rank
50 and 55.
In the alternative number packets per sec-
ond feature list the earlier features were a better criterion
for splitting than the later features in the list. This sup-
ports claims by Wang et al. [39] that the beginning of
a packet sequence leaks more information than the end
of a packet sequence.
[5]
we found packet inter-arrival time statistics, with rank
between 40 and 70, only slightly increase the attack ac-
curacy, despite being a key feature in their work.
In contrast to Bissias et al.
6 Attack on hardened defenses
For direct comparison we tested our random forest classi-
ﬁer in a closed-world setting on various defenses against
the k-NN attack and the more recent CUMUL [28] at-
tack using the Wang et al. data set [39]. Note that most
of these defenses require large bandwidth overheads that
may render them unusable for the average client. We test
against the following defenses:
• BuFLO [12]. This defense sends packets at a constant
size during ﬁxed time intervals. This potentially ex-
tends the length of transmission and requires dummy
packets to ﬁll in gaps.
• Decoy pages [27]. This defense loads a decoy page
whenever another page is loaded. This provides back-
ground noise that degrades the accuracy of an attack.
This is essentially a defense that employs multi-tab
browsing.
• Trafﬁc morphing [40]. Trafﬁc morphing shapes a
client’s trafﬁc to look like another set of web pages.
A client chooses the source web pages that they would
like to defend, as well as a set of target web pages that
they would like to make the source processes look like.
• Tamaraw [35]. Tamaraw operates similarly to Bu-