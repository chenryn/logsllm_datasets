title:Performance Characterization of a Commercial Video Streaming Service
author:Mojgan Ghasemi and
Partha Kanuparthy and
Ahmed Mansy and
Theophilus Benson and
Jennifer Rexford
Performance Characterization of a Commercial Video
Streaming Service
Mojgan Ghasemi
Princeton University
Partha Kanuparthy
Yahoo Research∗
Ahmed Mansy
Yahoo
Theophilus Benson
Duke University
Jennifer Rexford
Princeton University
Abstract
Despite the growing popularity of video streaming over the Inter-
net, problems such as re-buffering and high startup latency continue
to plague users. In this paper, we present an end-to-end charac-
terization of Yahoo’s video streaming service, analyzing over 500
million video chunks downloaded over a two-week period. We
gain unique visibility into the causes of performance degradation
by instrumenting both the CDN server and the client player at the
chunk level, while also collecting frequent snapshots of TCP vari-
ables from the server network stack. We uncover a range of perfor-
mance issues, including an asynchronous disk-read timer and cache
misses at the server, high latency and latency variability in the net-
work, and buffering delays and dropped frames at the client. Look-
ing across chunks in the same session, or destined to the same IP
preﬁx, we see how some performance problems are relatively per-
sistent, depending on the video’s popularity, the distance between
the client and server, and the client’s operating system, browser,
and Flash runtime.
1.
INTRODUCTION
Internet users watch hundreds of millions of videos per day [6],
and video streams represent more than 70% of North America’s
downstream trafﬁc during peak hours [5]. A video streaming ses-
sion, however, may suffer from problems such as long startup de-
lay, re-buffering events, and low video quality that negatively im-
pact user experience and the content provider’s revenue [25, 14].
Content providers strive to improve performance through a variety
of optimizations, such as placing servers closer to clients, content
caching, effective peering and routing decisions, and splitting the
video session (i.e., the HTTP session carrying the video trafﬁc) into
ﬁxed-length chunks in multiple bitrates [9, 37, 20, 23, 32]. Multi-
ple bitrates enable adaptive bitrate algorithms (ABR) in the player
to adjust video quality to available resources.
Despite these optimizations, performance problems can arise any-
where along the end-to-end delivery path shown in Figure 1. The
poor performance can stem from a variety of root causes. For exam-
ple, the backend service may increase the chunk download latency
∗Work done at Yahoo. Current afﬁliation: Amazon Web Services.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC 2016, November 14-16, 2016, Santa Monica, CA, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987481
H
T
T
P
G
E
T
Client
ABR
Playback 
buffer
Demux
Decode
Render
Screen
CDN PoPs
Backend 
Service
Figure 1: End-to-End video delivery components.
on a cache miss. The CDN servers can introduce high latency when
accessing data from disk. The network can introduce congestion or
random packet losses. The client’s download stack may handle data
inefﬁciently (e.g., slow copying of data from OS to the player via
the browser and Flash runtime) and the client’s rendering path may
drop frames due to high CPU load.
While ABR algorithms can adapt to performance problems (e.g.,
lower the bitrate when throughput is low), understanding the lo-
cation and root causes of performance problems enables content
providers to take the right corrective (or even proactive) actions,
such as directing client requests to different servers, adopting a
different cache-replacement algorithm, or further optimizing the
player software. In some cases, knowing the bottleneck can help
the content provider decide not to act, because the root cause is
beyond the provider’s control—for example, it lies in the client’s
browser, operating system, or access link. The content provider
could detect the existence of performance problems by collecting
Quality of Experience (QoE) metrics at the player, but this does
not go far enough to identify the underlying cause. In addition, the
buffer at the player can (temporarily) mask underlying performance
problems, leading to delays in detecting signiﬁcant problems based
solely on QoE metrics.
Instead, we adopt a performance-driven approach for uncovering
performance problems. Collecting data at the client or the CDN
alone is not enough. Client-side measurements, while crucial for
uncovering problems in the download stack (e.g., a slow browser)
or rendering path (e.g., slow decoder), cannot isolate network and
provider-side bottlenecks. Moreover, a content provider cannot col-
lect OS-level logs or measure the network stack at the client; even
adding small extensions to the browsers or plugins would compli-
cate deployment. Server-side logging can ﬁll in the gaps [38], with
care to ensure that the measurements are sufﬁciently lightweight in
production.
In this paper, we instrument the CDN servers and the video player
of a Web-scale commercial video streaming service, and join the
measurement data to construct an end-to-end view of session per-
formance. We measure per-chunk milestones at the player, which
499Location Findings
CDN
Network
Client
1. Asynchronous disk reads increase server-side delay.
2. Cache misses increase CDN latency by order of magnitude.
3. Persistent cache-miss and slow reads for unpopular videos.
4. Higher server latency even on lightly loaded machines.
1. Persistent delay due to physical distance or enterprise paths.
2. Higher latency variation for users in enterprise networks.
3. Packet losses early in a session have a bigger impact.
4. Bad performance caused more by throughput than latency.
1. Buffering in client download stack can cause re-buffering.
2. First chunk of a session has higher download stack latency.
3. Less popular browsers drop more frames while rendering.
4. Avoiding frame drops needs min of 1.5 sec
sec download rate.
5. Videos at lower bitrates have more dropped frames.
Table 1: Summary of key ﬁndings.
runs on top of Flash (e.g., the time to get the chunk’s ﬁrst and last
bytes, and the number of dropped frames during rendering), and the
CDN server (e.g., server and backend latency), as well as kernel-
space TCP variables (e.g., congestion window and round-trip time)
from the server host. Direct measurement of the main system com-
ponents help us avoid relying on inference or tomography tech-
niques that would limit the accuracy; or requiring other source of
“ground truth” to label the data for machine learning [13]. In this
paper, we make the following contributions:
1. A large-scale instrumentation of both sides of the video deliv-
ery path in a commercial video streaming service over a two-week
period, studying more than 523 million chunks and 65 million on-
demand video sessions.
2. End-to-end instrumentation that allows us to characterize the
player, network path, and the CDN components of session perfor-
mance across multiple layers of the stack, per-chunk. We show
an example of how partial instrumentation (e.g., player-side alone)
would lead to incorrect conclusions about performance problems.
Such conclusions could cause the ABR algorithm to make wrong
decisions.
3. We characterize transient and persistent problems in the end-
to-end path that have not been studied before; in particular the
client’s download stack and rendering path, and show their impact
on QoE.
4. We offer a comprehensive characterization of performance
problems for Internet video, and our key ﬁndings are listed in Ta-
ble 1. Based on these ﬁndings, we offer insights for video content
providers and Internet providers to improve video QoE.
2. CHUNK PERFORMANCE MONITORING
Model. We model a video session as an ordered sequence of HTTP(S)1
requests and responses over a single TCP connection between the
player and the CDN server—after the player has been assigned to
a server. The session starts with the player requesting the manifest,
which contains a list of chunks in available bitrates (upon errors
and user events such as seeks, manifest is requested again). The
ABR algorithm — tuned and tested in the wild to balance between
low startup delay, low re-buffering rate, high quality and smooth-
ness — chooses a bitrate for each chunk to be requested from the
CDN server. The CDN service maintains a FIFO queue of arrived
requests and maintains a thread pool to serve the queue. The CDN
uses a multi-level distributed cache (between machines, and the
main memory and disk on each machine) to cache chunks with
1Both HTTP and HTTPS protocols are supported at Yahoo; for
simplicity, we use HTTP instead of HTTPS in the rest of the paper.
Player
OS
CDN
Backend
HTTP GET
{DDS 
{
DFB 
{DLB 
Cache Miss
DCDN 
+DBE {} DBE
e
m
T
i
Download 
Stack
Wide-Area
Network
Backend
Connection
Figure 2: Time diagram of chunk delivery. Solid lines are in-
strumentation while dashed lines are estimates.
an LRU replacement policy. Upon a cache miss, the CDN server
makes a corresponding request to the backend service.
The client host includes two independent execution paths that
share host resources. The download path “moves” chunks from
the NIC to the player, by writing them to the playback buffer. The
rendering path reads from the playback buffer, de-muxes (audio
from video), decodes and renders the pixels on the screen—this
path could use either the GPU or the CPU. Note that there is a
stack below the player: the player executes on top of a Javascript
and Flash runtime, which in turn is run by the browser on top of the
OS.
2.1 Chunk Instrumentation
We collect chunk-level measurements because: (1) most deci-
sions affecting performance are taken per-chunk (e.g., caching at
the CDN, and bitrate selection at the player), although some metrics
are chosen once per session (e.g., the CDN server), (2) sub-chunk
measurements would increase CPU load on client, at the expense of
rendering performance (Section 4.4), and (3) client-side handling
of data within a chunk can vary across streaming technologies, and
is often neither visible nor controllable. For example, players im-
plemented on top of Flash use a progress event that delivers data to
the player, and the buffer size or frequency of this event may vary
across browsers or versions.
We capture the following milestones per chunk at the player and
the CDN service: (1) When the chunk’s HTTP GET request is sent,
(2) CDN latency in serving the chunk, in addition to backend la-
tency for cache misses, and (3) the time to download the ﬁrst and
last bytes of the chunk. We denote the player-side ﬁrst-byte delay
DF B and last-byte delay DLB. Figure 2 summarizes our notation.
We divide a chunk’s lifetime into the three phases: fetch, download,
and playout.
Fetch Phase. The fetch process starts with the player sending an
HTTP request to the CDN for a chunk at a speciﬁed bitrate until the
ﬁrst byte arrives at the player. The byte transmission and delivery
traverse the host stack (player, Flash runtime, browser, userspace
to kernel space and the NIC)—contributing to the download stack
latency. If the content is cached at the CDN server, the ﬁrst byte
is sent after a delay of DCDN (the cache lookup and load delay);
otherwise, the backend request for that chunk incurs an additional
delay of DBE. Note that the backend and delivery are always
pipelined. The ﬁrst-byte delay DF B includes network round-trip
time (rtt0), CDN service latency, backend latency (if any), and
client download stack latency:
DF B = DCDN + DBE + DDS + rtt0
(1)
500We measure DF B for each chunk at the player. At the CDN ser-
vice, we measure DCDN and its constituent parts: (1) Dwait: the
time the HTTP request waits in the queue until the request headers
are read by the server, (2) Dopen: after the request headers are read
until the server ﬁrst attempts to open the ﬁle, regardless of cache