or two ﬁrst entries (which depends on the reachability of
the address).
In the rest of the paper we call such nodes
responsible nodes for the address.
The actual transmission of the scheduled ADDR messages
does not happen immediately. Every 100 milliseconds one
neighbour is randomly selected from the list oﬀ all peer’s
neighbours and the queue for outgoing ADDR messages is
ﬂushed for this node only. We call the node chosen at the
beginning of a 100 milliseconds round trickle node and the
procedure as a whole as trickling.
Consider an example on Fig. 2. Assume that node n0 gets
an ADDR message with one address A0 from node n3 and that
node n0 schedules to forward it to nodes n1 and n2 (i.e. these
nodes are responsible nodes for address A0). In round 1 node
n1 is chosen as a trickle node and the address is forwarded
to this node while the delivery to n2 is still pending. After
100 milliseconds in round 2 n3 is chosen as the trickle node
thus no actual transmission happens at this stage. After
another 100 milliseconds in round 3 n2 is chosen as the trickle
node and address A0 is ﬁnally sent to it. Choosing a trickle
node causes random delays at each hop during an address
propagation.
Figure 2: Trickling of ADDR messages
Finally for each connection, a Bitcoin peer remembers ad-
dresses that were forwarded over this connection. Before
a peer forwards an address, it ﬁrst checks if the same ad-
dress was already sent over the connection. This history is
cleared every 24 hours. An important note is that the his-
tory of sent addresses is kept per connection and not per IP,
i.e. if a Bitcoin peer reconnects, its history will be cleared.
The total number of addresses a Bitcoin peer can store is
limited by 20480. Whenever new addresses arrive at a peer
they replace old ones (according to speciﬁc rules which are
outside of the scope of this paper). In addtition when a peer
receives a GETADDR messages it sends back 23% of the number
of addresses it stores but not more than 2500 addresses.
Peer discovery.
After the startup a Bitcoin peer discovers its own IP ad-
dresses, which includes not only its network interfaces ad-
dresses but also the IP address as it is seen from the Internet
(in the majority of cases for NAT users it resolves to an IP
address of the peer’s ISP). In order to discover the latter,
the peer issues a GET request to two hard-coded web-sites
which reply with the address. For each address obtained
by the discover procedure, the peer assigns a score. Local
interfaces initially get score 1, the external IP address gets
score of 4 (in case the external IP address coincides with one
of the local addresses the scores a summed). When a client
establishes an outgoing connection to a remote peer, they
ﬁrst exchange VERSION messages and the client advertises its
address with the highest score. The remote peer then uses
the addresses propagation algorithm described above. The
client repeats the same procedure for the remaining 7 out-
going connections.
Transaction propagation.
Forwarding a transaction from one peer to another in-
volves several steps. First the sender transmits an INVEN-
TORY message with the hash of the transactions. Second, the
receiver runs several checks on the transaction and if the
checks pass, it requests the actual transaction by sending a
GETDATA message. The sender then transmits the transaction
in a TRANSACTION message. When the receiver gets the trans-
action he advertises it to its peers in an INVENTORY message.
When a client generates a transaction he schedules6 it for
forwarding to all of its neighbours. It then computes a hash
of a value composed of the transaction hash and a secret salt.
If the computed hash has two last bits set to zero the trans-
action is forwarded7 immediately to all the 8 entry nodes.
Otherwise a queue of a neighbour for outgoing transactions
is ﬂushed when the neighbour becomes the trickle node (the
same as with ADDR messages). Obviously 1
4 of all transaction
are forwarded immediately in average.
When a transaction is received it is scheduled for the de-
livery to all peer’s neighbours as described above. As with
ADDR messages, a Bitcoin peer maintains history of forwarded
transactions for each connection.
If a transaction was al-
ready sent over a connection it will not be resent again. A
Bitcoin peer keeps all received transaction in a memory pool.
If the peer received a transaction with the same hash as one
in the pool or in a block in the main block chain, the received
transaction is rejected.
3. DISCONNNECTING FROM TOR
In this section we explain the ﬁrst phase of our attack.
We show how to prohibit the Bitcoin servers to accept con-
nections via Tor and other anonymity services. This results
in clients using their actual IP addresses when connecting
to other peers and thus being exposed to the main phase of
our attack, which correlates pseudonyms with IP addresses.
6By scheduling we mean that the node puts the transaction
into the outgoing queue of the corresponding connection.
7More precisely the peer sends an INVENTORY message with
the hash of the transaction.
N1(trickle)N2N3N2N1N3 (trickle)Round 1Round 2N2(trickle)N1N3Round 3N0N0N0This phase is quite noticeable, so a stealthy attacker may
want to skip it and deanonymize only non-Tor users.
In the further text we discuss Tor, but the same method
applies to other anonymity services with minor modiﬁca-
tions. Brieﬂy, the Tor network [7] is a set of relays (5397 for
the time of writing) with the list of all Tor relays publicly
available on-line. Whenever a user wants to establish a con-
nection to a service through Tor, he chooses a chain of three
Tor relays. The ﬁnal node in the chain is called Tor Exit
node and the service sees the connection as it was originated
from this Tor Exit node.
To separate Tor from Bitcoin, we exploit the Bitcoin built-
in DoS protection. Whenever a peer receives a malformed
message, it increases the penalty score of the IP address
from which the message came (if a client uses Tor, then the
message will obviously come from one of the Tor exit nodes).
When this score exceeds 100, the sender’s IP is banned for
24 hours. According to the bitcoind implementation, there
are many ways to generate a message which would cause
penalty of 100 and an immediate ban, e.g. one can send
a block with empty transactions list8 (the size of such a
message is 81 bytes). It means that if a client proxied its
connection over a Tor relay and sent a malformed message,
the IP address of this relay will be banned.
This allows to separate any target server from the entire
Tor network. For that we connect to the target through as
many Tor nodes as possible. For the time of writing there
were 1008 Tor exit nodes. Thus the attack requires estab-
lishing 1008 connections and sending a few MBytes in data.
This can be repeated for all Bitcoin servers, thus prohibit-
ing all Tor connections for 24 hours at the cost of a million
connections and less than 1 GByte of traﬃc. In case an IP
address of a speciﬁc Bitcoin node can be spoofed, it can be
banned as well.
As a proof of concept we used the described method to
isolate our Bitcoin node from a set of Tor exit relays.
4. LEARNING TOPOLOGY
Suppose that we have ruled out the case that the Bit-
coin users, which we deanonymize, use Tor. Now we target
clients, i.e. nodes that do not accept incoming connections,
but have 8 outgoing connections to entry nodes. In this sec-
tion we show how to learn these entry nodes.
The method is based on the fact that whenever a client
C establishes a connection to one of its entry nodes, it ad-
vertises its address Ca as it is seen from the Internet (see
section 2). If the attacker is already connected to an entry
node, with some probability (which depends on the num-
ber of the attacker’s connections) the address Ca will be
forwarded to him. This suggests the following strategy:
1. Connect to W Bitcoin servers, where W is close to the
total number of servers.
2. For each advertised Ca, log the set E(cid:48) of servers that
forwarded Ca to attacker’s machines and designate it
as the entry node subset E(cid:48)
Ca .
There are two problems with this method. First, the entry
node might send the client’s address to some non-attacker’s
8We validated this for Bitcoin core client version 0.8.6. For
bitcoin clients with version older than 0.9.0 one can send a
loose coinbase transaction.
peer. Second a client does not connect to all his entry nodes
simultaneously, but there is a time gap between connections.
In both cases, the advertised address reaches attacker’s ma-
chines via peers that are not entry nodes, which yields false
(noisy) entries in E(cid:48)
Ca .
Noise reduction technique.
Our strategy of ﬁltering noise assumes that either the
client’s IP was already used in the Bitcoin network, which
is quite common for the clients behind NAT or the client’s
public IP is contained in a known list of IP addresses (e.g.
within an IP range of a major ISP) which an attacker can
use. If an attacker knows Ca, he restricts its propagation
using the following fact:
• If the address had already been sent from A to B, it
will not be forwarded over this connection again;
This suggests broadcasting Ca (or all the addresses under
investigation) to all servers we are connected to. We sug-
gest repeating this procedure every 10 minutes (see details
below), though there could be other options. The adversary
expects that when the client reconnects, the entry nodes
will forward Ca to him, and even if they don’t, the address
propagation will stop before it reaches the adversary via a
non-entry node.
Eventually the attacker obtains the fraction paddr of client’s
entry nodes. The exact value of paddr depends on the num-
ber of attacker’s connections, and it is computed for some
parameters in Section 8.1. For instance, if an attacker es-
tablishes 35 connections to each potential entry node, which
all had 90 connections beforehand, then he identiﬁes 4 entry
nodes out of 8 on average.
Here are some details. When the attacker advertises the
Ca, each Bitcoin server chooses two responsible nodes to for-
ward the address. The attacker than establishes a number
of connections to each server in the network hoping that her
nodes will replace some of the responsible nodes for address
Ca. When client C connects to one of its entry nodes e1,
it advertises its address. If one of attacker’s nodes replaced
one of the responsible nodes, then the attacker will learn
that client C might be connected to node e1. If the respon-
sible nodes did not change address Ca will not be propagated
further in the network.
Since the attacker advertised Ca to node e1, responsible
nodes of e1 might be replaced by some non-attacker nodes
and the attack might fail.
In Section 8 we show that the
probability of this event is actually quite low given that the
attacker re-sends its list of addresses frequently enough.
5. DEANONYMIZATION
We have prohibited Bitcoin servers from accepting Tor
connections and showed how to ﬁnd the entry nodes of clients.
Now we describe the main phase of the deanonomyzation at-
tack.
The main phase consists of four steps:
1. Getting the list S of servers. This list is regularly
refreshed.
2. Composing a list C of Bitcoin clients for deanonymiza-
tion.
3. Learning entry nodes of clients from C when they con-
nect to the network.
4. Listening to servers from S and mapping transactions
to entry nodes and then to clients.
Eventually we create a list I = {(IP, Id, P K)}, where IP
is the IP address of a peer or its ISP, Id distinguishes clients
sharing the same IP, and P K is the pseudonym used in a
transaction (hash of a public key). Let us explain the steps
in detail.
Step 1. Getting the list of servers.
This phase of the attack is rather straightforward. An
attacker ﬁrst collects the entire list of peers by quering all
known peers with a GETADDR message. Each address P in
the response ADDR message can be checked if it is online by
establishing a TCP connection and sending a VERSION mes-
sage. If it is, P is designated as a server. An attacker can
initiate the procedure by querying a small set of seed nodes
and continue by querying the newly received IP addresses.
The adversary establishes m connections to each server (we
suggest 50 for the size of the current Bitcoin network).
Step 2. Composing the deanonymization list.
The attacker selects a set C of nodes whose identities
he wants to reveal. The addresses may come from various
sources. The attacker might take IP ranges of major Inter-
net service providers, or collect addresses already advertised
in the Bitcoin network. Finally, she might take some entries
from the list of peers she obtained at Step 1.
Step 3. Mapping clients to their entry nodes.
Now the attacker identiﬁes the entry nodes of the clients
that are connecting to the network. Equipped with the list
C of addresses, the attacker runs the procedure described in
Section 4. Let us estimate how many entry nodes are needed
to uniquely identify the client.
Let us denote the set of entry nodes for P by EP . We
stress that it is likely that EP1 (cid:54)= EP2 even if P1 and P2 share
the same IP address. For each P advertising its address in
P ⊆ EP . Since
the network the attacker obtains a set of E(cid:48)
there are about 8 · 103 possible entry nodes out of 105 total
peers (servers and clients together), the collisions in E(cid:48)
P are
unlikely if every tuple has at least 3 entry nodes:
105 · 105
(8 · 103)3 (cid:28) 1.
Therefore, 3 entry nodes uniquely identify a user, though
two nodes also do this for a large percent of users.
An attacker adds EP to its database and proceeds to Step
4.
Step 4. Mapping transactions to entry nodes.
This step runs in parallel to steps 1-3. Now an attacker
tries to correlate the transactions appearing in the network
with sets E(cid:48)
P obtained in step 2. The attacker listens for
INVENTORY messages with transaction hashes received over all
the connections that she established and for each transaction
T she collects RT — the ﬁrst q addresses of Bitcoin servers
that forwarded the INVENTORY message. She then compares
E(cid:48)
P with RT (see details below), and the matching entries
suggest pairs (P, T ). In our experiments we take q = 10.
There could be many variants for the matching procedure,
and we suggest the following version.
• The attacker composes all possible 3-tuples from all
P and looks for their appearances in RT . If there
sets E(cid:48)
is a match, he gets a pair (R, T );
• If there is no match, the attacker consider 2-tuples and
then 1-tuples. Several pairs {(Pi, T )} can be suggested
at this stage, but we can ﬁlter them with later trans-
actions.
We made several experiments and collected some statis-
tics to estimate the success of the attack. In our experiments
on the testnet we established 50 connections to each server,
obtained 6 out of 8 entry nodes on average, and the 3-tuples
were detected and linked to the client in 60% of transac-
tions (Section 7). In the real network, where we can estab-
lish fewer connections on average, our pessimistic estimate
is 11% (Section 8), i.e. we identify 11% of transactions.
Finally, let us consider the approach where we identify
clients by 2-tuples in the top-10. As detailed in Appendix A
(and brieﬂy stated in Section 7), for 35% of transactions the
right client would be identiﬁed. However, each transaction
might generate several false positives.
To estimate the false positive rate, we ﬁrst calculate the
average number of 2-tuples among the entry nodes we catch.
For paddr = 0.34 each 2-tuple is detected with probability
0.115, so out of 28 possible 2-tuples we detect 3.2 on average.
Each top-10 suggests 45 2-tuples, and there are 225 2-tuples
at all (all tuples are unordered). If we work with a database
of N clients, each transaction suggests N 27.3−25 = N/217.7
candidate clients.
If we track all 100,000 clients, we get
the false positive rate around 0.28, which is slightly smaller
than the probability 0.35 to detect the right client for a
transaction.
In other words, for each suggested client the
probability that he is the right one is about 55%.
Remark 1.
Step 4 of the attack depends on that some entry nodes of
a client are among the ﬁrst to forward the INVENTORY message
with the transaction’s hash. The intuition behind it is that it
takes a number of steps for a transaction to propagate to the
next hop. Fig. 3 shows steps that are required for a trans-
action to be propagated over two hops and received at peer
A. When a transaction is received by a node it ﬁrst runs a
number of checks and then schedules the transmission. The
actual transmission will happen either immediately (for 25%
of transations) or with a random delay due to trickling (see
Section 2). The time needed for an INVENTORY message to
be forwarded to the attacker’s node through node Entry is
the sum of propagation delays of 4 messages (2xINVENTORY,
1xGETDATA, 1xTRANSACTION) plus the time node Entry needs to
run 16 checks and possibly a random trickling delay. On the
other hand the time needed for the same INVENTORY message
to be forwarded to the attacker’s node through peer A con-
sists of 7 messages (3xINVENTORY, 2xGETDATA, 2xTRANSACTION),
32 checks, and two random delays due to trickling. Finally
since the majority of connections to a peer are coming from
clients, one more hop should be passed before the trans-
action reaches an attacker’s node through a wrong server.
Measurements of transaction propagation delays are given
in Appendix C.
Based on this we expect that if a transaction generated by