results in a median of one agent per node (details omitted
for space reasons). Note that clients may use unassigned
guards, but will be unable to collect free tickets from them.
Since agents limit distribution to unique IP addresses,
users behind NAT boxes will compete for handouts and ag-
gregate performance for NAT users will suﬀer. Note that if
IPv6 is universally adopted, the distribution scheme will re-
quire modiﬁcation since each client can generate several IPv6
addresses [37]. We accept an adversary capable of joining
multiple IPv4 nodes since it increases Tor’s anonymity set.
Agent Collusion Using guards as distribution agents in-
troduces a chance for collusion. An adversary could join a
relay to Tor, become a guard, and distribute tickets to a col-
luding client. To mitigate this problem, the bank will only
allow an agent to distribute tickets if that agent has e.g. ob-
tained the “stable” ﬂag in Tor [50]. An agent cannot cheat
until it has contributed signiﬁcant resources.
The bank also limits distribution to each agent. Band-
width measurements may be used to estimate the number
of clients an agent is servicing, and the number of tickets
the agent is allowed to distribute. Since agents are also
guard nodes and ticket distribution is based on contributed
bandwidth, the number of tickets they distribute directly
correlates with the number of tickets they earn by relaying
traﬃc. This can be used to bound the advantage agents gain
by not honestly distributing tickets to clients.
Suppose agent A has bandwidth fraction b. Each agent
has two non-agent guard nodes, and A receives 1
3 of the
tickets they spend when they select A as their entry node to
Tor. Each selection occurs with probability b, so A receives
3 of the tickets just by being a guard. If A additionally
2·b
keeps the tickets A is supposed to distribute, the most tickets
A can receive is 5·b
3 , about 2.5 times as many tickets. This
is the worst-case: tickets re-spent by relays will lower this
bound. Future work should consider auditing agents’ ticket
distribution to detect dishonesty.
Ticket Economy Our ticket distribution strategy continu-
ously introduces new tickets into the system that will eventu-
ally be exchanged at the bank. Continuous ticket exchanges
impose a bandwidth constraint on the bank (see Section 4.1).
Therefore, we must bound the total number of tickets that
exist in the system to allow the bank to handle all exchanges.
To bound the total number of tickets in the system, the
bank imposes a ticket tax on users when exchanging tick-
ets. The tax rate is adjustable based on the bank’s band-
width constraints and estimate of the total number of tick-
ets currently in the system. The bank’s estimate considers
the number of tickets exchanged during previous exchange
intervals (tickets not exchanged expire automatically).
In
practice, the bank can probabilistically fail each ticket ex-
change to reach the desired tax rate, but this consumes
bandwidth resources for tickets that will be taxed. Alter-
natively, the bank could reveal random numbers that repre-
sent a hash output range during every exchange period, and
tickets whose hash value falls in this range can be considered
taxed and invalid. Then clients can discover which of their
tickets have been taxed without contacting the bank. The
anonymity implications involved with taxing and bounding
tickets are discussed in Section 4.2.
3.4 Differentiated Service
BRAIDS employs diﬀerentiated services and a scheduler
based on the proportional diﬀerentiation model introduced
by Drovolis et al.
[16, 17, 18, 19]. The model states that
performance for each service class (in terms of measurable
metrics like queueing delay) should be relatively propor-
tional to parameters conﬁgured by the network operator.
Let qi(t, t + τ ) be a performance metric measured during
the interval (t, t + τ ) for monitoring time scale τ . The pro-
portional diﬀerentiation model creates quality diﬀerentia-
tion parameters ci for each class of service i and introduces
constraints such that:
qi(t, t + τ )
qj(t, t + τ )
=
ci
cj
where c1 < c2 < . . . < cn. We write the delay ratio be-
tween these classes as c1 : c2 : . . . : cn. This means that
the performance metric under consideration should always
maintain the proportions deﬁned by the quality diﬀerentia-
tion parameters, during any monitoring timescale.
i (t) = ai(t)
ci
i(t) · (1 − f ) + p(cid:48)(cid:48)
We deﬁne the performance metric qi to be the queue-
ing delay of class i; the delay parameters between each
class are adjustable. Drovolis et al. contribute schedulers
that approximate proportional delay diﬀerentiation under
heavy loads. BRAIDS utilizes the Hybrid Proportional De-
lay (HPD) scheduler, which is a combination of the Waiting
Time Priority (WTP) and the Proportional Average Delay
(PAD) schedulers. Each Tor cell is time-stamped upon ar-
rival at the relay and placed in the queue associated with
the cell’s class of service. When the relay makes a schedul-
ing decision at time t, WTP computes the priority of only
the cells at the head of each class i’s queue as p(cid:48)
i(t) = wi(t)
,
ci
where wi(t) is the waiting time of the cell computed using
the time-stamp from above. PAD computes class priorities
as p(cid:48)(cid:48)
, where ai(t) is the total average delay in-
curred by service class i before time t. HPD weights these
i (t) · f , where f is
priorities as pi(t) = p(cid:48)
an adjustable fraction. The cell with the highest computed
priority pi(t) is scheduled. In BRAIDS, the HPD scheduler
computes at most six priorities for each scheduling decision.
HPD allows us to diﬀerentiate performance of paying and
non-paying clients by adjusting the ci parameters. We then
divide client traﬃc into three distinct service classes: (1)
Low-latency for web browsing clients, (2) High-throughput
for ﬁle sharing clients, and (3) Normal for non-paying clients.
These classes will be proportionately delayed as low-latency :
high-throughput : normal.
Low-latency Service Users who wish to browse the web
typically want fast response but not high throughput. There-
fore, we schedule low-latency traﬃc with the highest priority.
We rate-limit low-latency traﬃc for each circuit to prevent
users from sending high traﬃc loads and overwhelming the
low-latency class; traﬃc exceeding a threshold limit over a
monitoring timescale will be demoted to the normal class.
We suggest a threshold equal to the number of free tick-
ets each user receives during a spending interval (discussed
in Section 4.1). Throttling is necessary to prevent high-
throughput clients from “abusing the pipe” for web users,
which is currently a well-known problem in Tor [32].
High-throughput Service Conversely, clients with high
throughput requirements (e.g. BitTorrent users) tolerate
higher-latency service. Therefore, we increase scheduling
delays relative to the low-latency class but do not throttle
323traﬃc. As a result, high-throughput traﬃc has a diminish-
ing eﬀect on low-latency traﬃc.
Normal Service Since not all users will be able or willing
to deploy relay services, we do not require clients to make
payments in order to use BRAIDS. Instead, clients who have
expended their free ticket allowance, or choose not to pay
for service, receive both the lowest priority and, in turn, the
highest scheduling delays.
Diﬀerentiating service results in two interesting conse-
quences:
it provides incentives to run relays, since users
in higher service classes receive lower delay; and it allows
for incremental deployability by placing traﬃc from legacy
clients in the normal service class. Note that the extent of
the performance gain between service classes depends on the
chosen delay parameters.
4. ANALYSIS AND DISCUSSION
4.1 Parameter Selection
Ticket Validity Intervals Recall that ticket validity inter-
vals [-,du), [du,dv), and [dv,dw) are global uniform intervals
in which tickets may be spent and exchanged and are broad-
casted by the bank (see Section 3.1). We explore both the
frequency and relative time that each interval occurs.
To prevent unspendable ticket periods, tickets that are
received in spending interval i are exchanged in spending
interval i + 1 (exchange interval i overlaps spending interval
i + 1). Time in each exchange interval is shared between
relay-exchange and client-exchange such that the fraction
of time alotted for relay exchange corresponds with the ex-
pected fraction of tickets relays possess (which the bank can
estimate based on exchanges in previous intervals).
Using the interval strategy just described, the bank will
only exchange half of all tickets in the system during every
spending interval and users can only spend half of their tick-
ets at one time. Following this approach, tickets received in
spending interval i are exchanged in spending interval i + 1
and spent in spending interval i + 2. All tickets not ex-
changed during an exchange interval will expire, so if relays
are oﬄine for the duration of an exchange interval, they will
lose roughly half of their tickets.
Longer spending and exchange intervals means relays must
wait longer to use tickets, but shorter intervals means tickets
expire faster. We suggest a compromise of 24 hour spending
and exchange intervals, noting that further exploration of
exchange intervals is desirable.
Ticket Worth Recall that several cells may be transferred
through Tor for each ticket. The number of cells transferred
for each ticket has an important impact on the bank’s CPU
and bandwidth consumption. Since we limit the amount of
data users can download for free, higher ticket worth means
the bank has to exchange fewer tickets, reducing both CPU
and bandwidth requirements. However, users then have
fewer tickets overall which reduces the number of indepen-
dent circuits that can be paid simultaneously. We suggest
that users receive 3 tickets every 10 minutes so they may uti-
lize a prioritized circuit at any time. We note that in practice
these tickets will likely be freely distributed in batches at a
higher time granularity (e.g. every hour).
Cryptographic and Bandwidth Costs Each relay must
perform a SHA1 hash and an AES encryption/decryption
for each cell it transfers. BRAIDS introduces an additional
task – veriﬁcation of a ticket. We implemented the partially
AMD Athlon
(3 GHz)
Intel Core2
(2.67 GHz)
AES+SHA1
PBS verify
PBS bank
AES+SHA1
PBS verify
PBS bank
Mean
9.139
531.287
413.244
6.226
1496.813
1193.233
Median
8.616
530.885
412.069
5.859
1496.613
1192.782
Std. D.
2.493
8.342
7.297
1.307
10.844
9.472
Table 1: Cryptographic time per cell for Tor relays com-
pared with BRAIDS PBS veriﬁcations, in microseconds.
Also shown is the bank’s time per signature.
blind signature scheme of Abe et al.
[1] using GMP [23]
for arbitrary precision arithmetic. We measure both the
amount of time a bank spends producing a signature, and
the amount of time a relay spends verifying a single ticket.
We also compute the time to perform the SHA1 and AES
operations required by Tor.
Table 1 shows the results of our Linux benchmarks on 3
GHz AMD 64 Athlon X2 6000 and 2.67 GHz Intel Core 2
Duo 6750 CPUs. We report the mean, median, and stan-
dard deviation of times, in microseconds, for each opera-
tion described above. As expected, a signature veriﬁcation
takes signiﬁcantly longer than AES and SHA1 operations
currently performed by relays. However, the value of each
ticket can be selected such that a ticket need not be sent
for every cell and expensive ticket veriﬁcation costs can be
amortized. An appropriate value would result in a greater
cost for AES and SHA1 than for veriﬁcations to prevent the
signature scheme from becoming a bottleneck. Our bench-
marks suggest a single ticket be worth 128 KB of data so that
a veriﬁcation need only be performed for every 256 cells.
Given our Intel benchmarks, a relay performs roughly 666
veriﬁcations per second while the bank may perform over
833 signatures per second per processor core. Each relay
may therefore upload at a rate of 666 Mbps while stream-
lining veriﬁcation procedures, and the bank may sustain an
aggregate 833 Mbps of prioritized traﬃc through Tor. Re-
call that the bank is oﬄine and may be distributed among
multiple machines for additional processing resources.
To compute bandwidth costs, suppose a user receives η
free tickets during each spending interval. Not including pro-
tocol overhead, which can be minimized by batching ticket
exchanges, each ticket exchange consumes 488 bytes of band-
width (the partially blind signature from Abe et al.
[1] re-
quires multiple messages between the client and the bank).
In aggregate, the bank distributes η·µ tickets per day, where
µ is the total number of users receiving free tickets. Ticket
exchanges are taxed such that after ρ spending intervals, η·µ
tickets are eliminated from the economy. The total number