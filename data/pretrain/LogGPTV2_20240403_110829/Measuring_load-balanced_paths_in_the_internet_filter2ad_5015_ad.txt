links for resilience and traﬃc engineering.
6.2 Diamond length
Recall that Sec. 4.2 deﬁnes the length of a diamond as
the maximum number of hops between its divergence point
and convergence point. We deﬁne the diamond length for a
load-balanced path to be the length of the longest diamond
found in that path.
Fig. 8 shows the distribution of the diamond lengths for
load-balanced paths between all source-destination pairs in
the MIT dataset. The Y axis, in log scale, represents the
80000
60000
40000
20000
 2  4  6  8  10  12  14  16
Min-width
 8
 10
 12
 2
 4
Length
 6
per-flow
per-destination
s
r
i
a
p
n
o
i
t
a
n
i
t
s
e
d
-
e
c
r
u
o
s
#
 1e+06
 100000
 10000
 1000
 100
 10
 1
2
4
6
8
10
12
14
16
Length
Figure 8: Diamond length distributions (MIT).
number of source-destination pairs that have a given dia-
mond length. All sources have similar statistics, except for
the nyu trace in which 56% of the per-ﬂow load-balanced
paths (26,016 out of the 43,112 per-ﬂow load-balanced paths)
traverse a diamond of length 4. This diamond, which is lo-
cated in the Broadwing network, skews the nyu distribution.
Overall, diamonds tend to be short, with a signiﬁcant por-
tion being of length 2.
6.2.1 Short diamonds
Of a total of 394,165 source-destination pairs with per-
ﬂow load balancing, 35% have a diamond of length two.
Per-destination diamonds also tend to be short. Of 555,693
paths with per-destination load balancing, 26% of them have
a diamond of length two. Diamond length is most skewed
towards the short end for per-packet load balancing (not
shown), with 90% of paths having a diamond of length two.
As discussed earlier, diamonds of length two should typi-
cally correspond to multiple links between two routers. Op-
erators use load balancing between two routers not only for
load sharing, but also as active backup in case of single link
failures.
6.2.2 Long diamonds
Load-balanced paths with longer diamonds are less fre-
quent. For instance, fewer than 1% of per-destination load-
balanced paths have diamonds longer than 8. We observe
per-ﬂow diamonds of lengths up to 15 and per-destination
Figure 9: Diamond length and min-width of per-ﬂow
load-balanced paths.
diamonds with up to 17 hops. The longest per-packet load-
balanced paths have lengths up to 6.
Per-destination diamonds tend to be longer than per-ﬂow.
Around 37% of load-balanced paths traverse a per-ﬂow di-
amond of length greater than 3; this percentage is 46% for
per-destination diamonds. There are few long per-packet
diamonds (only 3% have a length greater than 3).
We looked at the 25 cases of per-packet diamonds of length
5 and 6 in detail. Most of them appear in core networks
in Asian ISPs. Given the general practice of avoiding per-
packet load balancing in core networks, perhaps these are
cases of misconﬁgured load balancing.
If so, then we see
how Paris traceroute could help operators detect such mis-
conﬁgurations.
6.2.3 Length and width
We now study the relationship between the min-width
and length. Fig. 9 presents the number of per-ﬂow load-
balanced paths in the MIT dataset with a given diamond
length and min-width. The vertical axis represents the num-
ber of source-destination pairs whose diamond length and
min-width are given by the horizontal axis.
As discussed in Sec. 6.1, there may be several diamonds
for the same source-destination pair.
If so, we select the
min-width and length of the diamond with the smallest
min-width. There is a clear peak in the number of load-
balanced paths with length and min-width equal to two
(load-balanced paths between 17% of the 394,165 source-
destination pairs with per-ﬂow load balancing are in this
category). Load-balanced paths between 53% of source-
destination pairs traverse a diamond with a length less or
equal to 2 and min-width 2 or 3. This result conﬁrms that
the vast majority of the diamonds are both short and nar-
row.
There are no wide and long diamonds. There is a biparti-
tion of the remaining diamonds into two categories. The ﬁrst
category contains wide but short diamonds. It is extremely
rare to observe wide diamonds (whose width is greater than
2) with more than 3 hops. The second one corresponds to
narrow but long parallel paths. In this case, the min-width
is always 2. Wide but short diamonds probably correspond
to multiple links between routers. Operators may introduce
new links between routers to upgrade capacity. Long and
narrow diamonds likely correspond to paths between the
ingress and egress routers in a network, which are useful for
traﬃc engineering.
6.3 Diamond asymmetry
We say that a diamond is asymmetric when one can reach
its convergence point with diﬀerent hop counts. There might
be some concern that asymmetric diamonds are the result of
misconﬁguration. But the equal-cost multipath mechanisms
of OSPF and IS-IS require only that paths have the same
cost in terms of link weight, not hop count [19, 20]. Network
operators can conﬁgure two paths of diﬀerent hop counts to
have the same sum of link weights. In addition, some new
mechanisms [27] allow load-balancing over paths with small
cost diﬀerences.
From the point of view of performance, asymmetry might
cause delay diﬀerences. We study this correlation in Sec. 7.
Asymmetry might also be related to path reliability, with
longer paths, traversing as they do more IP interfaces, being
potentially more susceptible to failure. We do not study this.
Fig. 10 presents the number of source-destination pairs
in the MIT dataset that have per-ﬂow and per-destination
load-balanced paths with a given asymmetry. The Y axis is
per-flow
per-destination
s
r
i
a
p
n
o
i
t
a
n
i
t
s
e
d
-
e
c
r
u
o
s
#
 1e+06
 100000
 10000
 1000
 100
 10
 1
 0
 2
 6
 4
Asymmetry
 8
 10
Figure 10: Asymmetry distributions (MIT).
in log scale.
Most paths with per-ﬂow load balancing, 79%, traverse
symmetric diamonds. Paths under per-destination load bal-
ancing, are slightly more symmetric: 85% of the 555,693
paths under per-destination load balancing traverse sym-
metric diamonds. That still leaves a signiﬁcant fraction of
destinations that can be reached with diﬀerent numbers of
hops.
On the other hand, over 95% of the paths with per-packet
load balancing (not shown) traverse a symmetric diamond.
This is consistent with the observation that the majority of
such diamonds are short, and thus have less possibility for
asymmetry. Nonetheless, the 5% of such paths with asym-
metry are a concern for TCP connections, in so far as asym-
metry implies diﬀerent delays and thus greater chances for
packet reordering.
6.3.1 Low asymmetry
When asymmetry is present, it is typically low. For in-
stance, out of 84,222 per-ﬂow load-balanced paths with asym-
metric diamonds, 69,334 (82%) only diﬀer by one hop. For
per-destination load-balanced paths, this fraction is 79%
(66,721 out of 84,744 pairs with asymmetric diamonds), and
for per- packet, 65% (923 out of 1,406 pairs).
6.3.2 High asymmetry
Per-ﬂow diamonds with more than 3 hops of diﬀerence are
extremely rare (614 source-destinations pairs). For instance,
the paris trace contains only 63 such diamonds, and the chi1-
gblx trace, 162. We found 2,549 such per-destination, and
only 11 such per-packet load-balanced paths.
We examined the per-ﬂow diamond with the maximum
asymmetry, which has 8 hops diﬀerence. One path traverses
8 routers between the divergence and convergence points
while the other directly reaches the end of the diamond. We
believe that the latter path is an MPLS tunnel, maybe even
traversing the same routers as the one traversed on the ﬁrst
path. This hypothesis is supported by the observation that
routers in the diamond append MPLS extensions [28] to the
ICMP responses. This example suggests that some of the
shorter diamonds may also result from MPLS tunnels.
For per-destination load balancing, there are 71 cases of
asymmetry between 8 and 10. We examined some of these
diamonds with very distinct paths. For instance, there is
one asymmetric diamond that spans the US and Europe
in Cogent’s network. By inspecting the interface names,
we concluded that the parallel paths each traverse diﬀer-
ent numbers of points of presence (PoPs), which causes the
asymmetry. Depending upon which address is probed in-
side the destination preﬁx, packets either cross the ocean
through a link via London or another via Paris.
7. DELAYS
This section characterizes round-trip times (RTTs) under
per-ﬂow load balancing. We focus on per-ﬂow load balancers
because we cannot control the paths under per-packet split-
ting and we cannot strictly compare delays to diﬀerent des-
tinations under per-destination load balancing.
RTTs measure the forward and the return path combined;
therefore we need to ensure that all probes to measure de-
lays of paths in the forward direction follow the same return
path (even if they traverse a load balancer). Otherwise, our
conclusions could be mistaken: delay diﬀerences between
load-balanced paths in the forward direction could be due
to delay variation in the reverse direction. This section ﬁrst
describes how to control the return path under load balanc-
ing. Then, it presents our measurement setup and results.
7.1 Controlling the return path
Classic traceroute fails to maintain the ﬂow identiﬁer on
the reverse path, so probe responses may take diﬀerent paths
when they traverse a per-ﬂow load balancer. Probe re-
sponses are ICMP messages (usually, Time Exceeded or Port
Unreachable) from routers or end-hosts. Per-ﬂow load bal-
ancers use the ICMP checksum, which depends on the ICMP
header and data, as part of the ﬂow identiﬁer for ICMP
packets [15]. Routers construct an ICMP Time Exceeded
message with the IP and UDP headers of the probe, but
usually not the UDP data [29]. Classic traceroute system-
atically varies the checksum of the probe responses, because
it varies the UDP data of the probe, which in turn impacts
the UDP checksum of the probe. Ping also fails to measure
accurate delays under load balancing, because it varies the
ICMP sequence number, which has an impact on the ICMP
checksum. Ping probes may take diﬀerent paths not only
on the forward path, but also on the reverse path.
Unlike ping and classic traceroute, Paris traceroute main-
tains the forward ﬂow identiﬁer, but its original implemen-
tation [3] varies the return ﬂow identiﬁer. We develop a
method to control the return path:
1. Build a probe with the desired forward ﬂow identiﬁer
(for UDP, we set the UDP port numbers);
2. Predict the ICMP checksum value by constructing the
predicted ICMP response; and
3. Find the appropriate value we have to put in the UDP
data of the probe to yield the desired ICMP checksum.
Unfortunately, there are some challenges that may prevent
us from correctly predicting the ﬂow identiﬁer of the probe
responses. First, to predict the response content we have
to know the TTL value of the probe when it reaches the
router. The TTL is generally equal to 1, but in some cases
it can have a diﬀerent value (for instance, in the presence
of routers that forward packets whose TTL has reached the
minimum value [3]). Second, we found some routers which
ﬁll some ICMP ﬁelds with junk bytes (fortunately, these are
always the same byte values for a given router), whereas
they should be ﬁlled with zeros. Since the ICMP checksum
also depends on those ﬁelds, we cannot know the content
of those ﬁelds until we have probed the router. Third, we
found rare routers that include IP (security related) options
in the ICMP responses. We solve the ﬁrst two issues by
sending a ﬁrst “scout” probe to discover the parameters one
has to use to build the next probe. The third issue cannot be
addressed, because per-ﬂow load balancers behave like per-
packet when they forward packets with IP options, which
prevents any attempt to control the path responses may
take.
We use this technique to maintain the return ﬂow iden-
tiﬁer for our RTT measurements. In future work, we plan
to vary the return ﬂow identiﬁer in a controlled manner to
detect load-balanced paths on the reverse path.
7.2 Measuring RTTs
We measure RTTs using the version of Paris traceroute
that maintains the return ﬂow identiﬁer. Per-packet load
balancers can still make the return path vary, but as dis-
cussed in Sec. 5, this type of load balancer is less frequently
seen (less than 2% of the paths in our traces). Furthermore,
we looked at the paths between all our sources (recall that
we updated the MIT list by adding all our source nodes) and
did not observe any per-packet load balancers. Given that
they tend to be deployed in edge networks, we believe that
there were no per-packet load balancers close to our sources,
thus our measurements were not aﬀected by per-packet load
balancing on the return path.
We take the following steps to conduct our measurements:
Selection of destinations: We launch Paris traceroute
from 10 sources towards all destinations in the MIT list. For
each source, we select the ﬁrst 5,000 destinations reachable
through a per-ﬂow load balancer.
Selection of ﬂow identiﬁers: We use the MDA in a
ﬁrst phase as an input to guide the second phase of the
experiments. For each destination, we select at most n ﬂow
identiﬁers belonging to diﬀerent ﬂow classes (a ﬂow class is
a set of ﬂows which take exactly the same IP-level forward
path). We chose n = 6, because the results in Sec. 6 show
that only 4% of the per-ﬂow diamonds have a max-width
larger than 6.
Selection of interface to probe: We probe intermedi-
ate hops instead of the destinations themselves. We select
the last responding interface that all the selected paths to