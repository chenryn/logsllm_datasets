### 6.1 Observations from DSL Home Internet Connections

We observed that many large and popular targeted attack campaigns were submitted from DSL home internet connections. However, it is important to note that we did not observe the development phase of these attacks; the samples were already in the wild (even if undetected and unknown to the public) before they were submitted to our sandbox. For this experiment, we considered the entire dataset without applying any filtering or clustering strategies. Our goal was not to track the development of APT samples but to highlight that these samples were available to researchers long before they were publicly discovered.

The key takeaway from this experiment is the unfortunate fact that these samples went unnoticed. As a community, there is a need for an early warning system to report suspicious samples to security researchers. Such a system could prevent these threats from remaining undetected and could mitigate months or even years of damage to targeted companies.

### 6.2 Case Studies

In this section, we provide detailed descriptions of three development scenarios. While our system identified many more interesting cases, space limitations necessitate a brief overview. This overview offers valuable insights into the different ways attackers use (and misuse) public sandboxes and how security analysts can leverage the information collected by our system to investigate each case, reconstructing both the attacker's behavior and their final goals.

#### Example I: Anti-sandbox Malware

The first example involves a malware author who introduced anti-sandbox functionality to a Trojan application. The cluster related to this example contains three samples. The timeline (summarized in Figure 2) suggests a possible development, as the difference between submission time and compile time is very small.

A quick analysis of the static features shows that the three samples are very similar, sharing the same strings and imphash. However, the first sample has 21 functions, while the last two have 22 functions. Our report indicates that the first and second samples differ in two functions: the start function and a new CloseHandle function. This information, extracted automatically by our system, provides a starting point for a closer analysis.

Upon opening the executables in IDA Pro, we quickly identified the modified start function and the new CloseHandle function (see Figure 3). The start function was modified to include an additional basic block and a call to the new CloseHandle function. This block uses the rdtsc x86 instruction to read the Timestamp Counter Register (TSC), which contains the number of CPU cycles since the last reset. The assembly snippet is called twice to check the time difference. After the first rdtsc instruction, there is a call to CloseHandle using the timestamp as a handler (likely an invalid handler). These tricks are used to detect the Anubis Sandbox environment, as the delay introduced by its checks during program execution can be detected. If the time difference is greater than 0E0000h, the program terminates by calling the ExitProcess function.

The last sample in the cluster was submitted only to tune the threshold and showed no significant differences from the second sample. The control flow graph analysis performed by our system confirmed a high similarity between the first two samples, consistent with the minor modifications found in the disassembled code. The behavioral features extracted by our system also confirmed our hypothesis: the first sample ran until the analysis timeout, while the second one terminated after only five seconds.

The malicious intent is further supported by other cluster metadata. The first sample was unknown to VirusTotal, while the last one was identified as a common Trojan application. This suggests that the original sample, without the timing check, was never used in the wild. The fact that all three samples were submitted days before the trojan was first observed in the wild strongly supports the conclusion that the person who submitted them was indeed the malware author.

#### Example II: Testing a Trojan Dropper

The second cluster consists of five samples, with the first four written in Delphi and the last one in Visual Basic. This is unusual, as the two programming languages are quite different and unlikely to generate similar binaries.

The cluster timeline does not provide useful information, as all Delphi samples share the same compilation time (June 20, 1992). Only the Visual Basic sample had a consistent compilation time with the submission. The submission times, however, are interesting, as all samples were submitted within a few hours, suggesting a possible development. Two IP addresses were involved: one for the Delphi samples and one for the Visual Basic version. The static features of the first four samples show very little differences, indicating they are likely variations of the same program.

On average, they share 169 out of 172 functions and 7 out of 8 PE sections. The changes indicate that the attacker added some thread synchronization code to a function responsible for injecting code into a different process. The control flow graph similarity reported by our tool was over 98%, confirming the small differences observed between versions. Once satisfied, the author submitted a completely different sample, this time in Visual Basic. Despite the obvious differences in most static analysis features, the fuzzyhash similarity with the fourth Delphi sample was 100%. A rapid analysis showed that the Visual Basic application embedded the entire binary of the fourth Delphi program. The behavior report confirmed that, once executed, the Visual Basic Trojan dropped the embedded executable, which was then injected into a target process.

None of the Antivirus software used by VirusTotal recognized the first four samples as malicious. However, the last one was flagged by 37 out of 50 AVs as a trojan dropper malware.

Our system's advantage is its ability to automatically reconstruct the entire picture despite the different IP addresses (all located in the same geographical area). We were also able to propagate certain metadata, such as the author's username, from one sample to others where that information was missing. This ability to retrieve and propagate metadata between different samples is very useful during an investigation.

Another interesting aspect is that after the process injection, the program used a well-known dynamic DNS service (no-ip) to resolve a domain name. The IP address returned by the DNS query pointed to the same machine used by the author to submit the sample, suggesting the attacker was testing the attack before releasing it.

#### Example III: Probe Development

In this example, we show an attacker fingerprinting the analysis environment and successfully creating an antisandbox check. The cluster consists of two samples, both submitted from France within 23 hours by the same IP address. The two samples have the same size, number of functions (164), and sections (4). The only differences are in the _start function and two sections (.text and .rdata).

At first glance, this cluster did not seem very interesting. However, the inter-cluster connections pointed to six loosely correlated samples submitted by the same author in the same week. These files, not included in the core cluster due to lower binary similarity, were designed to collect information or test anti-virtualization/emulation tricks. For instance, one binary implemented known techniques to detect a virtual machine monitor, another retrieved the computer name, and another detected inline hooking.

Putting all the pieces together, it is clear that the author was preparing probes to assess various aspects of the sandbox environment. This example highlights the value of inter-cluster edges in understanding and linking different submissions that, while different at the binary level, are part of the same organized campaign.

### 6.3 Malware Samples in the Wild

Out of the 3038 clusters reported as malware development candidates by our machine learning classifier, 1474 (48%) contained binaries detected by antivirus signatures as malicious (according to VirusTotal). A total of 228 files in these clusters were later detected in the wild by Symantecâ€™s antivirus engine. The average time between submission to our sandbox and detection in the wild was 135 days, indicating it took between four and five months for the antivirus company to develop a signature and for the file to appear on end-users' machines. Some of these binaries were detected on more than 1000 different computers in 13 different countries, proving that while these may not be sophisticated malware, they have a negative impact on thousands of users.

### 7. Limitations

We acknowledge that once this research is published, malware authors may take countermeasures to evade such analysis systems, such as using private malware checkers and avoiding public sandboxes. This is a problem that applies to many analysis techniques, including botnet detection, intrusion prevention, and malware analysis. Despite this, we believe it is important to describe our findings to enable other researchers to work in this area and propose more robust methodologies in the future.

Moreover, after completing our study, it was noticed that some known malware development groups were testing their creations on VirusTotal. This confirms that what we have found is not an isolated case but a widespread phenomenon affecting other online analysis systems. Since the interaction between malware developers and public sandboxes is no longer a secret, there is no reason to withhold our findings.

We are aware that our methodology is not perfect and can be evaded. However, the key message is that malware authors are abusing public sandboxes to test their code, and currently, a sophisticated analysis is not needed to find them. This is the first paper to identify these cases, and our approach was sufficient to detect thousands of them. More research is needed to develop more precise monitoring and early warning systems to analyze the large amounts of data collected by public services daily.

### 8. Related Work

While extensive research has been conducted on malware analysis and detection, few studies have focused on datasets collected by public malware dynamic analysis sandboxes. The most comprehensive study in this direction was conducted by Bayer et al. [24], who analyzed two years of Anubis [10] reports and provided statistics on malware evolution and prevalent types of malicious behaviors.

Lindorfer et al. [43] conducted the first study on malware development by studying the evolution of eleven known malware families over time. They documented the updating process and code changes for different versions of each family. In contrast, our study focuses on detecting malware authors on a large scale as they interact with the sandbox.

In a different paper, Lindorfer et al. [44] proposed a technique to detect environment-sensitive malware by executing each sample multiple times on different sandboxes and comparing the normalized reports to detect discrepancies.

Another related area studies the phylogeny of malware using approaches from biology. While partially related, our study aimed to detect suspicious submissions that may be part of malware development activity rather than understanding the relationship between different malware species.

Jang et al. [34] studied how to infer software evolution by analyzing program binaries, using both static and dynamic analysis features. Their focus was on benign programs, with some experiments on 114 malicious software with known lineage. Compared to our work, they used a smaller set of features designed to infer software lineage, while we used a richer set to distinguish malware developments from variations of the same samples collected in the wild.

Other approaches have been proposed in the literature, but the goals and methodologies differ from ours.