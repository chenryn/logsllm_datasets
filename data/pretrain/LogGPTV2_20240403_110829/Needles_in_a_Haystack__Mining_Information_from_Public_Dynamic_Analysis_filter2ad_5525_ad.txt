mitted from DSL home Internet connections. However,
we cannot claim that we observed the development phase
of these large and popular targeted attacks campaigns as
in all cases the samples were already observed in the
wild (even though undetected and no one was publicly
aware of their existence) before they were submitted to
our sandbox. It is important to note that for this exper-
iment we considered the entire dataset, without apply-
ing any ﬁltering and clustering strategy. In fact, in this
case we did not want to spot the development of the APT
samples, but simply the fact that those samples were sub-
mitted and available to researchers long before they were
publicly discovered.
We believe the sad message to take away from this ex-
periment is that all those samples went unnoticed. As a
community, there is a need for some kind of early warn-
ing system to report suspicious samples to security re-
searches. This could prevent these threats from ﬂying
under the radar and could save months (or even years) of
damage to the companies targeted by these attacks.
6.2 Case studies
In the rest of this section we describe in more details
three development scenarios. While our system identi-
ﬁed many more interesting cases, due to space limitation
we believe the following brief overview provides a valu-
able insight on the different ways in which attackers use
(and misuse) public sandboxes. Moreover, it also shows
how a security analyst can use the information collected
by our system to investigate each case, and reconstruct
both the author behavior and his ﬁnal goal.
In the ﬁrst example, the malware author introduced an
anti-sandbox functionality to a Trojan application. In this
case the analyst gathers intelligence information about
the modus operandi of the attacker and about all the de-
velopment phases.
In the second scenario, we describe a step by step de-
velopment in which the attacker tries to collect informa-
tion from the sandbox. This information is later used
to detect the environment and prevent the execution of
a future malware in the sandbox.
In the last example,
Figure 2: Anti-sandbox check - Timeline
we show how an attacker uses the sandbox as a testbed
to verify the behavior of the malware. In this case, the
author generated the binary using one of the many ded-
icated builder applications that can be downloaded from
the Internet or bought on the black market.
Example I: Anti-sandbox Malware
The cluster related to this example contains three sam-
ples. The timeline (summarized in Figure 2) already sug-
gests a possible development. In fact, the difference be-
tween the submission time and the compile time is
very small.
A quick look at the static features of the cluster shows
that the three samples are very similar, and share the
same strings as well as the same imphash (the import
hash [20, 21] recently introduced also by VirusTotal).
However, the ﬁrst sample is composed of 21 functions,
while the last two samples have 22 functions. Our report
also shows how the ﬁrst and the second samples differ for
two functions: the author modiﬁed the function start,
and introduced a new function CloseHandle. This in-
formation (so far extracted completely automatically by
our system) is a good starting point for a closer analysis.
We opened the two executables in IDA Pro, and
quickly identiﬁed the two aforementioned functions
(snippet in Figure 3). It was immediately clear that the
start function was modiﬁed to add an additional ba-
sic block and a call to the new CloseHandle function.
The new basic block uses the rdtsc x86 instruction to
read the value of the Timestamp Counter Register (TSC),
which contains the number of CPU cycles since the last
reset. The same snippet of assembly is called two times
to check the time difference. After the ﬁrst rdtsc in-
struction there is a call to CloseHandle, using the times-
tamp as handler (probably an invalid handler). These two
well known tricks are here combined to detect the Anubis
Sandbox environment – due to the delay introduced by
its checks during program execution. The Anubis Sand-
box’s core is slower in looking up the handlers table, and
this time discrepancy is the key to detect the analysis en-
vironment. In this case the difference has to be less than
0E0000h, or the program would immediately terminate
by calling the ExitProcess function.
The last sample in the cluster was submitted only to
tune the threshold and for this reason there were no im-
portant differences with the second sample. The control
1066  24th USENIX Security Symposium 
USENIX Association
10
16:59:1316:59:3317:05:2117:06:0617:13:2617:14:16tSubmission timeCompile timeSample 1Sample 1Sample 2Sample 3Figure 3: Anti-sandbox check - Start function comparison
ﬂow graph analysis performed automatically by our sys-
tem report a very high similarity between the ﬁrst two
samples, in line with the little modiﬁcations we found in
the disassembled code. Finally, the behavioral features
extracted by our system conﬁrm our hypothesis: the ﬁrst
sample was executed until the analysis timeout, but the
execution of the second one terminated after only ﬁve
seconds.
The behavior described so far suggest malicious in-
tents. This is also conﬁrmed by other cluster metadata.
For instance, while the ﬁrst sample in the cluster was
unknown to VirusTotal, the last one was clearly identi-
ﬁed as a common Trojan application. This suggests that
the original sample, without the timing check, has never
been used in the wild. Once more, the fact that all three
samples have been submitted days before the trojan was
ﬁrst observed in the wild strongly supports the fact that
the person who submitted them was indeed the malware
author.
Example II: Testing a Trojan Dropper
The second cluster we want to describe is composed
of ﬁve samples. Our report indicates that the ﬁrst four
are written in Delphi and the last one is written in Visual
Basic. This is already a strange fact, since the two pro-
gramming languages are quite different and it is unlikely
that they could generate similar binaries.
In this case the cluster timeline does not provide use-
ful information as all the Delphi samples share exactly
the same compilation time: 20th of June, 1992. Only the
Visual Basic sample had a compilation time consistent
with the submission. On the contrary, the submission
times provide an interesting perspective. All the samples
have been submitted in few hours and this might indi-
cate a possible development. In addition, there are two
IP addresses involved: one for the four Delphi samples
and one for the ﬁnal Visual Basic version. The static fea-
tures of the ﬁrst four samples show very little differences,
suggesting that these are likely just small variations of
the same program.
In average, they share 169 out of
172 functions and 7 out of 8 PE sections. By inspect-
ing the changes, we notice that the attacker was adding
some threads synchronization code to a function respon-
sible for injecting code into a different process. The con-
trol ﬂow graph similarity reported by our tool was over
98%, conﬁrming the small differences we observed be-
tween each versions. Once the author was happy with the
result, she submitted one more sample, this time com-
pletely different from the previous ones. Despite the ob-
vious differences in most of the static analysis features,
the fuzzyhash similarity with sample 4 was 100%. A
rapid analysis showed that this perfect match was due
to the fact that the Visual Basic application literally em-
bedded the entire binary of the fourth Delphi program.
In addition, the behavior report conﬁrmed that, once ex-
ecuted, the Visual Basic Trojan dropped the embedded
executable that was later injected inside a target process.
None of the Antivirus software used by VirusTotal rec-
ognized the ﬁrst four samples as malicious. However,
the last one was ﬂagged by 37 out of 50 AVs as a trojan
dropper malware.
It is important to stress that a clear advantage of our
system is that it was able to automatically reconstruct the
entire picture despite the fact that not all samples were
submitted from the same IP address (even though all lo-
cated in the same geographical area). Moreover, we were
able to propagate certain metadata extracted by our sys-
tem (for example the username of the author extracted
from the binary compiled with Visual Studio) from one
sample to the others in which that information was miss-
ing. This ability to retrieve and propagate metadata be-
tween different samples can be very useful during an in-
vestigation.
Another very interesting aspect of this malware devel-
USENIX Association  
24th USENIX Security Symposium  1067
11
First SampleSecond Sampleopment is the fact that after the process injection, the pro-
gram used a well known dynamic DNS service (no-ip)
to resolve a domain name. The IP address returned by the
DNS query pointed exactly to the same machine that was
used by the author to submit the sample. This suggests
that the attacker was indeed testing his attack before re-
leasing it, and this information could be used to locate
the attacker machine.
We identiﬁed a similar connect-back behavior in other
1817 clusters. We also noticed how most of these clus-
ters contain samples generated by known trojan builders,
like Bifrost [8] or PoisonIvy [9]. While this may seem to
prove that these are mostly unsophisticated attacks, Fire-
Eye [22] recently observed how the Xtremerat builder [7]
(which appeared in 28 of our clusters) was used to pre-
pare samples used in several targeted attacks.
Example III: Probe Development
In this last example we show an attacker ﬁngerprint-
ing the analysis environment and how, at the end, she
manages to create her own successful antisandbox check.
The cluster consists of two samples, both submitted from
France in a time span of 23 hours by the same IP ad-
dress. The two samples have the same size, the same
number of functions (164), and of sections (4). There
is only one function (_start) and two sections (.text
and .rdata) presenting some differences. The two pro-
grams perform the same actions, they create an empty
text ﬁle and then they retrieve the ﬁle attributes through
the API GetFileAttributes. The only differences are
on the API version they use (GetFileAttributesA or
GetFileAttributesW) and on the ﬁle name to open.
At a ﬁrst look, this cluster did not seem very inter-
esting. However the inter-cluster connections pointed
to other six loosely correlated samples submitted by the
same author in the same week. As explained in Section 4,
these ﬁles have not been included in the core cluster be-
cause the binary similarity was below our threshold. In
this case, these samples were all designed either to col-
lect information or to test anti-virtualization/emulation
tricks. For instance, one binary implemented all the
known techniques based on idt, gdt and ldt to de-
tect a virtual machine monitor [48, 47, 42]. Another one
simply retrieved the computer name, and another one
was designed to detect the presence of inline hooking.
Putting all the pieces together, it is clear that the author
was preparing a number of probes to assess various as-
pects of the sandbox environment.
This example shows how valuable the inter-clusters
edges can be to better understand and link together differ-
ent submissions that, while different between each other
at a binary level, are likely part of the same organized
“campaign”.
6.3 Malware Samples in the Wild
As we already mentioned at the beginning of the sec-
tion, out of 3038 clusters reported as malware develop-
ment candidates by our machine learning classiﬁer, 1474
(48%) contained binaries that were detected by the an-
tivirus signatures as malicious (according to VirusTotal).
A total of 228 of the ﬁles contained in these clusters
were later detected in the wild by the Symantec’s an-
tivirus engine. The average time between the submission
to our sandbox and the time the malware was observed
in the wild was 135 days – i.e., it took between four and
ﬁve months for the antivirus company to develop a signa-
ture and for the ﬁle to appear on the end-users machines.
Interestingly, some of these binaries were later detected
on more than 1000 different computers in 13 different
countries all around the world (obviously a lower bound,
based on the alerts triggered on a subset of the Syman-
tec’s customers). This proves that, while these may not
be very sophisticated malware, they certainly have a neg-
ative impact on thousands of normal users.
7 Limitations
We are aware of the fact that once this research is pub-
lished, malware authors can react and take countermea-
sures to sidestep this type of analysis systems. For in-
stance, they may decide to use “private” malware check-
ers, and avoid interacting with public sandboxes alto-
gether. First of all, this is a problem that applies to many
analysis techniques ranging from botnet detection, to in-
trusion prevention, to malware analysis. Despite that, we
believe that it is important to describe our ﬁndings so that
other researchers can work in this area and propose more
robust methodologies in the future.
Moreover, as we mentioned in the introduction, af-
ter we completed our study someone noticed that some
known malware development groups were testing their
creation on VirusTotal [52, 27]. This conﬁrms that what
we have found is not an isolated case but a widespread
phenomenon that also affects other online analysis sys-
tems. Second, now that the interaction between malware
developers and public sandboxes is not a secret anymore,
there is no reason that prevents us from publishing our
ﬁndings as well.
We are aware of the fact that our methodology is not
perfect, that it can be evaded, and that cannot catch all de-
velopment cases. However, we believe the key message
of the paper is that malware authors are abusing public
sandboxes to test their code, and at the moment we do
not need a very sophisticated analysis to ﬁnd them. Since
this is the ﬁrst paper that tries to identify these cases, we
found that our approach was already sufﬁcient to detect
1068  24th USENIX Security Symposium 
USENIX Association
12
thousands of them. Certainly more research is needed in
this area to develop more precise monitoring and early
warning system to analyze the large amounts of data au-
tomatically collected by public services on a daily basis.
8 Related Work
While there has been an extensive amount of research on
malware analysis and detection, very few works in the
literature have studied the datasets collected by public
malware dynamic analysis sandboxes. The most compre-
hensive study in this direction was conducted by Bayer et
al. [24]. The authors looked at two years of Anubis [10]
reports and they provided several statistics about mal-
ware evolution and about the prevalent types of malicious
behaviors observed in their dataset.
Lindorfer et al. [43] conducted the ﬁrst study in the
area of malware development by studying the evolution
over time of eleven known malware families. In partic-
ular, the authors documented the malware updating pro-
cess and the changes in the code for a number of dif-
ferent versions of each family. In our study we look at
the malware development process from a different angle.
Instead of studying different versions of the same well
known malware, we try to detect, on a large scale, the au-
thors of the malware at the moment in which they interact
with the sandbox itself. In a different paper, Lindorfer et
al. [44] proposed a technique to detect environment sen-
sitive malware. The idea is to execute each malware sam-
ple multiple times on several sandboxes equipped with
different monitoring implementations and then compare
the normalized reports to detect behavior discrepancies.
A similar research area studies the phylogeny [30] of
malware by using approaches taken from the biology
ﬁeld. Even if partially related to our work, in our study
we were not interested in understanding the relationship
between different species of malware, but only to detect
suspicious submissions that may be part of a malware
development activity.
In a paper closer to our work, Jang et al. [34] studied
how to infer the software evolution looking at program
binaries. In particular, the authors used both static and
dynamic analysis features to recover the software lin-
eage. While Jang’s paper focused mostly on benign pro-
grams, some experiments were also conducted on 114
malicious software with known lineage extracted from
the Cyber Genome Project [12]. Compared to our work,
the authors used a smaller set of static and dynamic fea-
tures especially designed to infer the software lineage
(e.g., the fact that a linear development is characterized
by a monotonically increasing ﬁle size). Instead, we use
a richer set of features to be able to distinguish mal-
ware developments from variations of the same samples
collected on the wild and not submitted by the author.
While our approaches share some similarities, the goals
are clearly different.
Other approaches have been proposed in the litera-