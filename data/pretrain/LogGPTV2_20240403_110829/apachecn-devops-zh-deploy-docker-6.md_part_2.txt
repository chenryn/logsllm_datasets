$ # Now we will specify a custom DNS and DNS search prefix and see what the same file looks like
$ docker run --rm \
 -it \
 --dns 4.4.4.2 \
 --dns-search "domain.com" \
 ubuntu \
 /bin/cat /etc/resolv.conf 
search domain.com
nameserver 4.4.4.2
```
如您所见，容器中的设置已经更改，以匹配我们的参数。在我们的例子中，任何域名解析都将流向`4.4.4.2`服务器，任何不合格的主机名都将首先被解析为`.domain.com`。
# 覆盖网络
我们只是在[第 4 章](4.html)、*扩展容器、*中简单地提到了这一点，但是为了让我们的容器与 Swarm 服务发现一起工作，我们不得不创建这种类型的网络，尽管我们并没有花太多时间解释它是什么。在 Docker Swarm 的上下文中，一台机器上的容器无法到达另一台机器上的容器，因为它们的网络在穿过网络时会直接路由到下一跳，而桥接网络会阻止每个容器到达同一节点上的邻居。为了在这种多主机设置中无缝地将所有容器连接在一起，您可以创建一个覆盖网络，该网络跨越集群中的任何 Swarm 节点。可悲的是，这种类型的网络只在 Docker Swarm 集群中可用，所以一般来说，它在编排工具上的可移植性有限，但是您可以使用`docker network create -d overlay network_name`创建一个。由于我们已经在[第 4 章](4.html)、*扩展容器*中介绍了一个使用这种类型网络的部署示例，因此您可以在那里查看它的运行情况。
Caution! Overlay networks do not communicate data securely by default with other nodes, so using the `--opt encrypted` flag when creating one is highly encouraged where network transport cannot be trusted fully. Using this option will incur some processing cost and will require you to allow port `50` communication within your cluster, but in most cases, it should be worth it turning it on.
# Docker 内置网络映射
在前面的章节中，我们主要使用具有默认网络设置的容器，在大多数情况下，这些容器使用`bridge`网络，因为这是默认设置，但是这不是唯一可以用于容器的网络类型。以下是可用网络连接的列表，几乎所有连接都可以通过`docker run --network`参数进行设置:
*   `bridge`:如前几章所述，这种类型的网络在主机上创建一个独立的虚拟接口，用于与容器进行通信，容器可以与主机和互联网进行通信。通常，在这种类型的网络中，容器间的通信是被阻止的。
*   `none`:禁用容器的所有网络通信。这对于只包含工具而不需要网络通信的容器非常有用。
*   `host`:使用主机的网络栈，不创建任何虚拟接口。
*   ``:连接到命名网络。当您创建网络并希望将多个容器放在同一个网络分组中时，此标志非常有用。例如，这对于将多个聊天容器(如 Elasticsearch)连接到它们自己的独立网络中非常有用。
*   ``:这允许您连接到指定容器的网络栈。就像`--pid`标志一样，这对于调试运行的容器非常有用，而无需直接附加到它们，尽管网络可能需要根据所使用的网络驱动程序使用`--attachable`标志来创建。
Warning! Using the `host` networking switch gives the container full access to local system services and as such is a liability when used in any context other than testing. Use extreme caution when this flag is used, but luckily, there are only very few cases (if any) where there will be a legitimate use for this mode.
# Docker 通信端口
除非您正在运行 Docker Swarm，否则您可能永远不需要担心 Docker 使用什么端口进行通信，但是如果您在现场遇到此类配置或者您希望在集群中进行此类部署，这是一个相对较好的参考点。列表很短，但是每个端口对于大多数 Swarm 集群的运行都非常重要:
```
2377 TCP - Used for Swarm node communication
4789 UDP - Container ingress network
7946 TCP/UDP - Container network discovery
50 IP - Used for secure communication of overlay networks if you use "--opt encrypted" when creating the overlay network
```
# 高可用性管道
以前，我们大部分时间都在处理集群中节点之间基于套接字的通信，这通常对大多数人来说是有意义的，并且几乎在每种编程语言中都有围绕它构建的工具。因此，它是人们将传统基础设施转换为容器时通常会使用的第一个工具，但是对于处理纯数据处理的大型和超大型应用来说，由于超出处理管道剩余部分的特定阶段的容量而导致的背压，它根本无法正常工作。
如果您将每个集群服务想象成一组连续的转换步骤，那么基于套接字的系统将经历类似以下步骤的循环:
*   打开监听插座。
*   永远循环执行以下操作:
    *   等待上一阶段套接字上的数据。
    *   处理这些数据。
    *   将处理后的数据发送到下一级的套接字。
但是，如果下一阶段已经达到最大容量，最后一步会发生什么？大多数基于套接字的系统要么抛出一个异常，使该特定数据的处理流水线完全失败，要么阻止执行继续进行，并不断重试将数据发送到下一阶段，直到成功为止。因为我们不希望处理管道失败，因为结果不是错误，也不希望让我们的工作人员等待下一个阶段解除阻塞，所以我们需要能够以有序结构保存阶段输入的东西，以便前一个阶段可以继续处理它自己的一组新输入。
# 容器消息传递
对于我们刚刚讨论的场景，其中单个处理阶段的背压导致级联回流停止，消息队列(通常也称为发布/订阅消息系统)在这里为我们提供所需的精确解决方案。消息队列通常将数据作为消息存储在**先进先出**、**先进先出** ( **先进先出**)队列结构中，并通过允许发送方将所需输入添加到特定阶段的队列(“入队”)和允许工作方(侦听器)触发该队列中的新消息来工作。当工作人员处理消息时，队列对其他工作人员隐藏消息，当工作人员完成并成功时，消息将从队列中永久删除。通过以异步方式对结果进行操作，我们可以允许发送者继续处理他们自己的任务，并完全模块化数据处理管道。
要查看正在运行的队列，假设我们有两个正在运行的容器，并且在很短的时间内，消息 **A** 、 **B** 、 **C** 和 **D** 作为来自某个假想处理步骤的输入相继到达(红色表示队列顶部):
![](img/2f0c276b-0978-45de-aadf-b0f12dbf4727.png)
在内部，队列跟踪它们的顺序，最初，两个容器队列侦听器都没有注意到消息，但是很快，它们得到通知，有新的工作要做，因此它们按照接收消息的顺序获得消息。消息队列(取决于具体的实现)将这些消息标记为其他侦听器不可用，并为工作进程设置完成超时。在本例中**消息 A** 和**消息 B** 已由可用的工人标记为待处理:
![](img/97e3a523-300b-4ec3-9648-c0ec1f8d1417.png)
在这个过程中，让我们假设**容器** 1 发生了灾难性的故障，它刚刚死亡。**消息 A** 队列上的超时没有完成就过期了，因此队列将它放回顶部，并使它再次可供监听器使用，同时我们的另一个容器继续工作:
![](img/c21ddcf4-4f2a-4bb3-966f-ed9555fd9006.png)
随着**消息 B** 成功完成，**容器 2** 通知队列任务完成，队列将其从列表中完全删除。这样一来，容器现在接受最上面的消息，它原来是未完成的**消息 A** ，并且过程像以前一样继续:
![](img/4d9fb591-c1b1-4bc4-b381-3bd7e6de1e25.png)
虽然这个集群阶段一直在处理故障和过载，但是将所有这些消息放入队列的前一个阶段一直在处理它的专用工作负载。我们目前的阶段也没有丢失任何数据，尽管我们的处理能力有一半在随机时间点被强制移除。
工人的新伪代码循环现在有点像这样:
*   在队列中注册为侦听器。
*   循环永远执行以下操作:
    *   等待队列中的消息。
    *   处理队列中的数据。
    *   将处理后的数据发送到下一个队列。
有了这个新系统，如果流水线中有任何类型的处理变慢，那些过载阶段的队列将开始变大，但是如果早期阶段变慢，队列将缩小，直到它们变空。只要最大队列大小可以处理消息量，过载的阶段可以处理平均需求，您就可以确定管道中的所有数据最终都会被处理，您的扩展阶段触发器就像注意到不是由 bug 引起的更大的队列一样简单。这不仅有助于减少管道阶段扩展的差异，而且如果集群中的部分发生故障，这也有助于保留数据，因为队列会在故障期间增长，然后随着基础架构恢复到完全工作状态而清空，所有这些都不会丢失数据。
如果这种好处还不够积极的话，考虑一下现在可以保证数据已经被处理了，因为队列保存着数据，所以如果一个工作人员死了，队列会(正如我们之前看到的)将消息放回队列中，以便有可能被另一个工作人员处理，这与基于套接字的处理不同，后者在这种情况下会无声地死去。处理密度的增加、容错能力的提高以及对突发数据的更好处理使得队列对容器开发人员极具吸引力。如果您的所有通信也是通过队列完成的，那么这些工作人员可能甚至不需要服务发现，只需要告诉他们队列管理器在哪里，因为队列正在为您执行发现工作。
不出所料，大多数队列都是以开发成本为代价的，这就是为什么它们没有像人们预期的那样被广泛使用。在大多数情况下，您不仅需要将自定义队列客户端库添加到您的工作代码中，而且在许多类型的部署中，您还需要一个进程或某个守护进程，它将是处理消息的主要队列仲裁器。事实上，我可能会说，单独选择消息传递系统本身就是一项研究任务，但是如果您正在寻找快速答案，通常 Apache Kafka([https://kafka.apache.org/](https://kafka.apache.org/))、Rabbtmq([https://www.rabbitmq.com/](https://www.rabbitmq.com/))和 Redis 支持的自定义实现([https://redis.io/](https://redis.io/))似乎在内部消息传递队列从最大部署到最小部署的集群环境中更受欢迎。
As with all things we have been covering so far, most cloud providers offer some type of service for this (AWS SQS, Google Cloud Pub/Sub, Azure Queue Storage, and so on) so that you don't have to build it yourself. If you are OK with spending a little bit more money, you can utilize these and not worry about hosting the daemon process yourself. Historically, messaging queues have been hard to maintain and manage properly in house, so I would venture to say that many, if not most, cloud systems use these services instead of deploying their own.
# 实现我们自己的消息队列
抛开理论不谈，让我们看看如何构建自己的小队列发布者和侦听器。对于这里的例子，我们将使用一个基于 Redis 的更简单的消息传递系统，叫做`bull`([https://www.npmjs.com/package/bull](https://www.npmjs.com/package/bull))。首先，我们将编写运行整个系统的代码，为了让事情变得简单，我们将对消费者和生产者使用相同的映像。
在新目录中，创建以下内容:
As a reminder, this code is also in the GitHub repository and you can view it or clone it from [https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_6/redis_queue](https://github.com/sgnn7/deploying_with_docker/tree/master/chapter_6/redis_queue) if you do not want to type the full text.
# package.json
这个文件几乎只是我们旧示例的副本，增加了`bull`包并更改了名称:
```
{
  "name": "queue-worker",
  "version": "0.0.1",
  "scripts": {
    "start": "node index.js"
  },
  "dependencies": {
    "bull": "^3.2.0"
  }
}
```
# index.js
`index.js`是一个单文件应用，要么每 1.5 秒向队列发送一个时间戳，要么根据调用参数从队列中读取。队列位置由`QUEUE_HOST`环境变量定义:
```
'use strict'
const Queue = require('bull');
const veryImportantThingsQueue = new Queue('very_important_things',
                                           { redis: { port: 6379,
                                                      host: process.env.QUEUE_HOST }});
// Prints any message data received
class Receiver {
    constructor () {
        console.info('Registering listener...');
        veryImportantThingsQueue.process(job => {
            console.info('Got a message from the queue with data:', job.data);
            return Promise.resolve({});
        });
    }
}
// Sends the date every 1.5 seconds
class Sender {
    constructor () {
        function sendMessage() {
            const messageValue = new Date();
            console.info('Sending a message...', messageValue);
            veryImportantThingsQueue.add({ 'key': messageValue });
        }
        setInterval(sendMessage, 1500);
    }
}
// Sanity check
if (process.argv.length `);
}
// Start either receiver or sender depending of CLI arg
console.info('Starting...');
if (process.argv[2] === 'sender') {
    new Sender();
} else if (process.argv[2] === 'receiver') {
    new Receiver();
} else {
    throw new Error(`Usage: ${process.argv.slice(0, 2).join(' ')} `);
}
```
# Dockerfile
这里没有什么特别的:这个文件几乎是我们旧的 Node.js 应用的精简版本:
```
FROM node:8
# Make sure we are fully up to date
RUN apt-get update -q && \
 apt-get dist-upgrade -y && \
 apt-get clean && \
 apt-get autoclean
# Container port that should get exposed
EXPOSE 8000
ENV SRV_PATH /usr/local/share/queue_handler
# Make our directory
RUN mkdir -p $SRV_PATH && \