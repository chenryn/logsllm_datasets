contacting the people that know the victim. Out of the 89
victim-impersonator pairs in our dataset, only two accounts
met our test for potentially being social engineering attacks.
3.1.3 Doppelgänger bot attacks
Our analysis of impersonation attacks above suggests that
most attacks are not directed towards popular celebrities
or attempting to trick a victim’s friends. This observa-
tion raises the question: What then might be motivating an
attacker to launch an impersonation attack targeting non-
celebrity users?
One hypothesis is that these impersonation attacks are
merely an attempt by attackers creating fake identities to
evade detection by Twitter Sybil and spam defense systems.
By simply copying the proﬁle attributes of existing Twit-
ter users, attackers could create new real-looking fake Twit-
ter identities that might escape traditional spam defenses
4Twitter has an account veriﬁcation program for highly pop-
ular users of its service.
5Less than 0.01% (0.007%) of Twitter users have more than
1000 (10,000) followers.
145that analyze features of individual identities. Such identi-
ties could be used proﬁtably to promote other Twitter users
and content in the system (selling fake followers and fake
content promotions is a growing business in Twitter). We
refer to such attacks as doppelg¨anger bot attacks.
To check our hypothesis that many impersonating ac-
counts are involved in follower fraud we analyzed whom
they are following. Since the number of doppelg¨anger bots
in our Random Dataset is limited to less than a hun-
dred, we investigated the doppelg¨anger bots in the BFS
Dataset, where we have identiﬁed tens of thousands of
victim-impersonator pairs using a BFS crawl of the Twitter
network starting with four seed doppelg¨anger bot accounts
(see Table 1). We found that the impersonating accounts in
the BFS Dataset follow a set of 3,030,748 distinct users.
Out of the users followed, 473 are followed by more than
10% of all the impersonating accounts. We checked if the
473 most followed accounts are suspected of having bought
fake followers in the past using a publicly deployed follower
fraud detection service [34]. Among those users for which
the service could do a check, 40% were reported to have
at least 10% fake followers. 6 The fact that a large frac-
tion of impersonating accounts follow the same small set of
users and that the users they follow are suspected of hav-
ing bought fake followers strongly points to the possibility
of impersonating accounts being involved in follower fraud.
3.2 Analyzing doppelgänger bot attacks
In this section, our goal is to better understand doppel-
g¨anger bot attacks with the ultimate goal of detecting dop-
pelg¨anger bots. To this end, we focus on understanding the
characteristics (i.e., various reputation metrics and activi-
ties) of doppelg¨anger bots and their victims. Our investi-
gation reveals important diﬀerences between the character-
istics of victim accounts and doppelg¨anger bots, which we
leverage in a later section to detect doppelg¨anger bots. The
doppelg¨anger bot attacks analyzed in this section are from
the BFS Dataset.
3.2.1 Characterizing victim accounts
To understand who the doppelg¨anger bot attacks are tar-
geting we proceed by asking several questions related to the
reputation and activity of victim accounts. We focus our
analysis on the diﬀerences in the statistical distributions of
properties of victim accounts and randomly chosen Twitter
users.
Reputation of victim accounts.
How popular are the victim accounts? Figure 2a shows
the CDF of the number of followers of victim accounts. The
median number of followers is only 73, which shows that
most doppelg¨anger bots do not target famous people but
ordinary users.
How inﬂuential are the victim accounts? Figures 2b
and 2c show the CDF of klout scores and the number
of expert lists where a victim account appears in, respec-
tively. 40% of victim accounts appear in at least one list
6For a point of comparison, we also checked whom the avatar
accounts from avatar-avatar pairs follow. There are only
four accounts followed by 10% of the avatar accounts and
they correspond to Justin Bieber, Taylor Swift, Katy Perry
and Youtube, all of which are well-known celebrity / corpo-
rate accounts followed widely on Twitter.
(a) Number of followers
(b) Klout score
(c) Number of lists the account
appears in
(d) Account creation date
(e) Number of followings
(f) Number of retweets
(g) Number of tweets favorited
by the user
(h) Number of mentions
(i) Number of tweets
(j) Date of the last tweet
Figure 2: CDFs of diﬀerent features for characterizing the
reputation and activity of impersonating and victim ac-
counts as well as Twitter accounts picked at random.
10010210400.51Number of followersCDF  ImpersonatorVictimRandom05010000.51Klout scoreCDF  ImpersonatorVictimRandom10010210400.51Number of listsCDF  ImpersonatorVictimRandom20042009201500.51Account creation dateCDF  ImpersonatorVictimRandom10010210400.51Number of followingsCDF  ImpersonatorVictimRandom10010200.51Number of retweetsCDF  ImpersonatorVictimRandom10010200.51Number of tweets favorited by the userCDF  ImpersonatorVictimRandom10010210400.51Number of mentionsCDF  ImpersonatorVictimRandom10010210400.51Number of tweetsCDF  ImpersonatorVictimRandom2010201500.51Date of the last tweetCDF  ImpersonatorVictimRandom146and 30% of victim accounts have klout scores higher than
25 (For comparison purposes, researchers like Dina Papa-
giannaki – @dpapagia – and Jon Crowcroft – @tforcworc –
have klout scores of 26 and 45 respectively, while Barack
Obama –@barackobama–has a klout score of 99). The ﬁg-
ures also show that the inﬂuence scores of victims are notice-
ably higher than those of random Twitter users, which indi-
cates that many victim accounts, while not exactly celebri-
ties, correspond to professional users with good reputation
in Twitter.
How old are the victim accounts? Figure 2d shows that
victims have generally older accounts than random Twitter
users. The median creation date for victim accounts is Octo-
ber 2010 while the median creation date for random Twitter
users is May 2012. Attackers target users that have been for
a long time in the system.
Activity of victim accounts.
How active are the victim accounts? Figures 2e through
2j show the CDFs of various activity metrics for victims.
They show that victims are considerably more active than
random Twitter users. For instance, Figure 2i shows the
CDF of the number of tweets per victim account. The me-
dian number of tweets is 181. In contrast, the median num-
ber of tweets for random users is 0, while the median num-
ber of tweets for random users that have at least one post is
20. Similarly, Figure 2j shows that 75% of victim accounts
posted at least one tweet in 2013, while only 20% of random
Twitter users posted at least one tweet in 2013. So the vic-
tims tend to be fairly active Twitter users that have been
active recently too.
In summary, the victims of doppelg¨anger bot attacks are
active users with a level of reputation and inﬂuence that
is signiﬁcantly higher than a random Twitter user (though
they are not necessarily celebrities). Our ﬁnding shows that
attackers target, inadvertently or not, users who put a sig-
niﬁcant amount of eﬀort into building an online image with
a good reputation.
3.2.2 Characterizing doppelgänger bot accounts
To understand how doppelg¨anger bot impersonating accounts
behave we analyze their reputation and activity.
Reputation of doppelgänger bot accounts.
Figures 2a through 2d compares the CDFs of diﬀerent rep-
utation metrics for doppelg¨anger bots, their victims, and
random Twitter accounts. The plots show that: (1) the
number of followers and klout score of impersonating ac-
counts is lower than the number of followers and klout score
of victim accounts but higher than the ones of random ac-
counts (Figures 2a and 2b); (2) impersonating accounts do
not appear in any lists of other accounts (Figure 2c); and (3)
most impersonating accounts were created recently, during
2013 (Figure 2d). At a high level, the reputation of doppel-
g¨anger bots is clearly lower than the reputation of victim ac-
counts, however, it is higher than the reputation of random
Twitter users. In the next section, we will show that these
diﬀerences have important implications for the detectability
of doppelg¨anger bots. Thus, impersonating accounts do not
have suspicious markers of reputation.
Activity of doppelgänger bot accounts.
We refer again to Figures 2e through 2j. The plots show
that: (1) the number of followings, retweets and favorites of
impersonating accounts is higher than victim and random
accounts (Figures 2e, 2f, and 2g). This ﬁnding is consis-
tent with our hypothesis that these accounts are involved
in illegal promotion of content and users; (2) However, the
number of times doppelg¨anger bots mention other users is
unusually low, which is consistent with the intuition that
doppelg¨anger bots would not not wish to draw attention to
themselves and their activities (Figure 2h); and (3) imper-
sonating accounts do not show excessive markers of activity:
the median number of users an impersonating account fol-
lows is 372, which is not very high compared with the median
number a victim account follows which is 111 and imperson-
ating accounts do not tweet excessively (Figure 2i), but are
very active, i.e. their last tweet is in the month we crawled
them (Figure 2j). These markers of activity not only support
our hypothesis that doppelg¨anger bots might be involved in
illegal promotion of content, but they also suggest that at-
tackers may be trying to emulate normal user behavior and
avoid being detected with abnormal (excessive) tweeting or
following behavior.
3.3 Detecting doppelgänger bot attacks
Even if doppelg¨anger bots do not seem to intentionally want
to harm their victims, they can still harm unintentionally the
online image of their victims. For example, in our dataset a
doppelg¨anger bot of a tech company tweeted “I think I was
a stripper in a past life”, which is clearly not the image the
tech company wants to promote. Even worse, Twitter took
in average 287 days to suspend these accounts.7 Thus, the
online image of victims was potentially harmed for several
months.
The natural question that follows is why do these accounts
go undetected for such long periods of time? In this section,
we analyze the challenges in detecting doppelg¨anger bots;
in particular our analysis shows that traditional sybil detec-
tion schemes are not able to accurately detect them. The
low accuracy can be potentially explained by the fact that
doppelg¨anger bots operate under the radar and the accounts
do not have any characteristics that makes them look sus-
picious in absolute. We show that the key to detect doppel-
g¨anger bots is to study their characteristics relative to their
corresponding victim accounts.
Using traditional sybil-detection schemes: reasoning
about the absolute trustworthiness of accounts.
Doppelg¨anger bots are a special type of sybil accounts, thus
we investigate whether we can detect them using existing
methods to detect sybil accounts. At a high level, many tra-
ditional sybil-detection schemes exploit ground truth about
good and bad users to create patterns of good and bad be-
havior. The schemes then decide whether an account is sybil
or not by comparing its behavior to these patterns [3, 40,
29]. We emulate such behavioral methods by training a SVM
classiﬁer with examples of doppelg¨anger bots (bad behav-
ior) and random Twitter accounts (good behavior) using
the methodology in [3].
7We know with an approximation of one week when Twitter
suspended the impersonating accounts and we know from
the Twitter API when the account was created.
147We consider all doppelg¨anger bots from the BFS Dataset
as positive examples (16,408 accounts), and we pick 16,000
random Twitter accounts as negative examples. We use 70%
of examples for training and 30% for testing and we train the
classiﬁer with all features that characterize the reputation
and the activity of a single account we presented in §2.4.
Because doppelg¨anger bots are only a small fraction of all
accounts in Twitter, our classiﬁcation problem has a high
class imbalance (i.e., there are many more negative exam-
ples than positive examples). Such scenarios require clas-
siﬁers that can operate at a very low false positive rate.
The smallest false positive rate our SVM classiﬁer achieves
is 0.1% for a 34% true positive rate (the true positive rate
drops to zero for lower false positive rates). A 0.1% false
positive rate is, however, very high in our scenario. For ex-
ample, if we take the 1.4 million random accounts (in the
Random Dataset), a classiﬁer with 34% true positive rate
for a 0.1% false positive rate will detect 40 actual doppel-
g¨anger bots (34%×122) while mislabeling 1,400 legitimate
accounts as doppelg¨anger bots. This accuracy is clearly un-
satisfying.
A plausible reason why these schemes are not optimal is
precisely because attackers intentionally create real-looking
accounts and emulate the behavior of legitimate users so that
they are harder to detect by current sybil account detection
systems.
Distinguish doppelgänger bots from victim accounts:
reasoning about the relative trustworthiness of accounts.
The previous section showed that, given a set of accounts, it
is hard to detect which ones are doppelg¨anger bots. Here we
try to approach the problem from a diﬀerent perspective and
we ask a diﬀerent question: given a victim-impersonator pair
can we pinpoint the impersonating account? Our intuition
is that, if it is too hard to reason about the trustworthiness
of an account in absolute, it might be easier to reason about
its trustworthiness relative to another account.
To answer the question we refer back to Figure 2 that
presents the CDFs for diﬀerent features of impersonating,
victim and random accounts. We can see that the charac-
teristics of victim accounts in aggregate are very diﬀerent
from the characteristics of impersonating accounts. More
precisely, victim accounts have a much higher reputation
(number of followers, number of lists and klout scores) than
impersonating accounts and they have a much older account
creation date. In fact, in all the victim-impersonator pairs
in the BFS Dataset and Random Dataset, none of the
impersonating accounts have the creation date after the cre-
ation date of their victim accounts and 85% of the victim
accounts have a klout score higher than the one of the im-
personating accounts. Thus, to detect the impersonating ac-
count in a victim-impersonator pair with no miss-detections,
we can simply take the account that has the more recent cre-
ation date. This reasoning opens up solutions to detect dop-
pelg¨anger bots, however, it does not solve the whole problem
because we still have to detect whether a pair of accounts
is a avatar-avatar pair or victim-impersonator pair. This is
the focus of section §4.
How well humans can detect doppelgänger bots.
In this section, we investigate how well humans are able to
detect doppelg¨anger bots. We focus on two questions: (1)
If humans stumble upon a doppelg¨anger bot, are they able
to detect that the account is fake?
(i.e., the question of
assessing the absolute trustworthiness of accounts) – this
scenario is speciﬁc to a recruiter that knows the name of
the person and searches for his accounts in diﬀerent social
networks to learn more about him and stumbles upon the
doppelg¨anger bot; and (2) If humans have access to both
the impersonating and the victim account, are they able to
detect the impersonating account better? (i.e., the question
of assessing the relative trustworthiness of accounts). The
ﬁrst question will show the severity of the doppelg¨anger bot
attacks problem by analyzing whether humans are tricked
into believing the doppelg¨anger bots represent the real per-
son. The second question will show whether humans are also
better at detecting impersonating accounts when they have
a point of reference.