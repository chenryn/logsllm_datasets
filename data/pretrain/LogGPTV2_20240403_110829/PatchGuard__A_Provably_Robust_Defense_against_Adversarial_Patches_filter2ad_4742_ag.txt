[46] Vikash Sehwag, Chawin Sitawarin, Arjun Nitin Bhagoji,
Arsalan Mosenia, Mung Chiang, and Prateek Mittal. Not
all pixels are born equal: An analysis of evasion attacks
under locality constraints. In Proceedings of the 2018
ACM SIGSAC Conference on Computer and Communi-
cations Security (CCS), pages 2285–2287, 2018.
[47] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
In 3rd International Conference on Learning Represen-
tations (ICLR), 2015.
[48] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,
and Alexander A. Alemi. Inception-v4, inception-resnet
and the impact of residual connections on learning. In
Proceedings of the Thirty-First AAAI Conference on
Artiﬁcial Intelligence (AAAI), pages 4278–4284, 2017.
[49] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going
deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1–9, 2015.
[50] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In 2nd
International Conference on Learning Representations
(ICLR), 2014.
[51] Simen Thys, Wiebe Van Ranst, and Toon Goedemé.
Fooling automated surveillance cameras: Adversarial
patches to attack person detection. In IEEE Conference
on Computer Vision and Pattern Recognition Workshops
(CVPR Workshops), pages 49–55, 2019.
[52] Florian Tramer, Nicholas Carlini, Wieland Brendel, and
Aleksander Madry. On adaptive attacks to adversarial
example defenses. In 2020 USENIX Security and AI
Networking Summit (ScAINet), 2020.
[53] Eric Wong and J. Zico Kolter. Provable defenses against
adversarial examples via the convex outer adversarial
In Proceedings of the 35th International
polytope.
Conference on Machine Learning (ICML), pages 5283–
5292, 2018.
[55] Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and
Prateek Mittal. Patchguard: Provable defense against ad-
versarial patches using masks on small receptive ﬁelds.
arXiv preprint arXiv:2005.10884, 2020.
[56] Weilin Xu, David Evans, and Yanjun Qi. Feature squeez-
ing: Detecting adversarial examples in deep neural net-
works. In 25th Annual Network and Distributed System
Security Symposium (NDSS), 2018.
[57] Dong Yin, Ramchandran Kannan, and Peter Bartlett.
Rademacher complexity for adversarially robust gen-
eralization. In Proceedings of the 36th International
Conference on Machine Learning (ICML), pages 7085–
7094, 2019.
[58] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Ad-
versarial examples: Attacks and defenses for deep learn-
ing. IEEE Transactions on Neural Networks and Learn-
ing Systems, 30(9):2805–2824, 2019.
[59] Zhanyuan Zhang, Benson Yuan, Michael McCoyd, and
David Wagner. Clipped bagnet: Defending against
sticker attacks with clipped bag-of-features. In 3rd Deep
Learning and Security Workshop (DLS), 2020.
A Provable Adversarial Training
In order to improve the provable robust accuracy, we train a
BagNet with a mask over the region with the largest true class
evidence. This training mimics the procedure of our provable
analysis in Algorithm 2, and we call it provable adversarial
training. In Table 14, we report the results for Mask-BN-17
with and without provable adversarial training against a 2%
pixel patch on ImageNet/ImageNette and a 2.4% pixel patch
on CIFAR-10. We can see from the table that provable ad-
versarial training signiﬁcantly improves provable robustness.
We note that we do not do provable adversarial training for
DS-ResNet because it is too expensive to computing its all
local features during the training. Further details of model
training are available in our technical report [55].
Table 14: Effect of provable adversarial training on Mask-BN-17
Dataset
Accuracy
Conventional training
Provable adv. training
ImageNet
ImageNette
CIFAR-10
clean
54.4%
54.6%
robust
13.3%
26.0%
clean
93.9%
95.0%
robust
83.8%
86.7%
clean
82.6%
83.9%
robust
31.7%
47.3%
2252    30th USENIX Security Symposium
USENIX Association
Figure 6: Toy example of 1-D convolution computation
Figure 7: Example of computing window size.
B Details of Receptive Fields
Local features focus on the center of the receptive ﬁeld.
In Section 3.1, we mentioned that a particular local feature
focuses exponentially more on the center of its receptive ﬁeld.
We provide the intuition for this argument in Figure 6. The
left part of the ﬁgure illustrates a 1-D example of convolu-
tion computation in which the input has ﬁve cells and will go
through two convolution layers with a kernel size of 3 to com-
pute the ﬁnal output. Each cell in the hidden layer (i.e., the
output of the ﬁrst convolution layer) looks at 3 input cells, and
the output cell looks at three hidden cells. We count the num-
ber of times each cell is looked at when computing the output
cell and plot it in the ﬁgure. As we can see, the center cell of
the input layer receives the most attention (being looked at 3
times). Moreover, as the number of layers increases (a similar
example for 3 convolution layers is plotted in the right part of
Figure 6), the difference in attention between the center cell
and the rightmost/leftmost cell will increase exponentially.
Therefore, a particular feature focuses exponentially more on
the center of its receptive ﬁeld, and an adversary controlling
the center cell will have a larger capacity to manipulate the
ﬁnal output features.
Computing the Window Size. One crucial step of our robust
masking defense is to determine the window size, and we
show in Section 3.4 and Equation 1 that the window size w
can be computed as w = (cid:100)(p + r−1)/s(cid:101), where p is the upper
bound on the patch size, r is the size of receptive ﬁeld, and
s is the stride of receptive ﬁeld. In Figure 7, we provide the
intuition for Equation 1. In this example, we assume the stride
s = 4, and the size of the receptive ﬁeld r = 3. We distinguish
the centers of receptive ﬁelds, the other cells in the receptive
ﬁelds, and the other cells with different colors. Note that we
choose a large stride s such that adjacent receptive ﬁelds
Table 15: Top-k accuracies of Mask-BN-17 on ImageNet
Patch size
Accuracy
Top-1
Top-2
Top-3
Top-4
Top-5
1% pixels
2% pixels
3% pixels
clean
55.1%
65.9%
71.3%
74.6%
77.0%
robust
32.2%
48.3%
52.2%
53.9%
54.8%
clean
54.6%
65.5%
70.8%
74.2%
76.6%
robust
26.0%
43.8%
48.7%
51.3%
52.9%
clean
54.1%
64.9%
70.2%
73.7%
76.2%
robust
19.7%
38.2%
44.1%
47.4%
49.6%
do not overlap for a better visual demonstration; the derived
equation is applicable to smaller s or larger r. In Figure 7,
we want to determine the largest patch size p such that the
patch only appears in n but not n+1 receptive ﬁelds. We plot
the boundary of the largest patch with red dash line in the
ﬁgure. The left part of the patch covers the rightmost cells of
receptive ﬁeld 0, and the right part does not appear in receptive
ﬁeld n. Based on Figure 7, we can compute p = n· s− r + 1.
Next, we can substitute n with w, use (cid:100)·(cid:101) for generalization to
any patch size, and ﬁnally get w = (cid:100)(p + r− 1)/s(cid:101). We note
that the network architectures [4, 27] used in this paper have
s = 8 for BagNet and s = 1 for DS-ResNet.
C Tighter Provable Analysis
conservative Mask Size
for Over-
Recall that the mask window size is a tunable security param-
eter. Robust masking can prove robustness for any patch is
smaller than the mask. In this section, we discuss a tighter
version of Lemma 1 when the defender overestimates the
worst-case patch size and use a larger mask window size.
Let W be the set of all possible malicious windows and V
be the set of all possible detected windows whose sizes are
larger than malicious windows. We can have the following
generalized Lemma.
Lemma 2. Given a malicious window w ∈ W , a class
¯y ∈ Y , and the set of all possible detected windows V , the
clipped and masked class evidence of class ¯y (i.e., s ¯y) can
be no larger than SUM(ˆu ¯y (cid:12) (1− v∗
w =
argmaxv∈Vw SUM(ˆu ¯y (cid:12)v) and Vw = {v ∈ V |SUM(w(cid:12)v) =
SUM(w)}.
w))/(1− T ), where v∗
In this lemma, Vw is the set of possible mask windows
that cover the entire malicious window w, and v∗
w is the mask
window in Vw with the largest class evidence. This bound
reduces to the bound of Lemma 1 when the sizes of malicious
window and mask window (i.e., the output of subprocedure
DETECT) are the same: Vw = {w} and thus v∗
w = w. The
proof of Lemma 2 is in the same spirit as that of Lemma 1
and is available in our technical report [55].
USENIX Association
30th USENIX Security Symposium    2253
D Additional Top-k Analysis
Top-k provable robustness. In Algorithm 2, we compare the
maximum wrong class evidence maxy(cid:48)∈Y (cid:48)(sy(cid:48)) with the lower
bound of true class evidence sy to determine the feasibility
of an attack. To determine the top-k provable robustness,
we create a set S ← {y(cid:48) ∈ Y (cid:48)|sy(cid:48) > sy} for wrong classes
whose evidence is larger than the lower bound of the true
class evidence sy. If the set size |S| is smaller than k, we
assume the image is robust to the top-k attack.
Additional results for ImageNet. We report the top-k clean
accuracy and provable robust accuracy in Table 15. Notably,
Mask-BN achieves a 77.0% clean and 54.8% provable robust
top-5 accuracy against a 1% pixel patch for the extremely
challenging 1000-class classiﬁcation task.
E Additional Discussion on Multiple Patches
In this paper, we focus on the threat of the adversary arbitrarily
corrupting one contiguous region. In this section, we discuss
how PatchGuard can deal with multiple adversarial patches.
Merge multiple patches into one large patch. The most
straightforward way to approach multiple patches is to con-
sider a larger contiguous region that contains all patches. The
analysis in the paper can be directly applied. However, when
the multiple patches are far away from each other, the merged
single region can be too large to have decent model robustness.
When this is the case, we have the following alternatives.7
Mask individual local features. Our robust masking pre-
sented in Section 3.4 masks the feature window with the
highest class evidence. As an alternative, we can mask α
individual local features with top-α highest class evidence.
Such individual feature masking is agnostic to the number
of patches, and we can easily re-prove Lemma 1 to have the
same upper bound of wrong class evidence. However, the
lower bound of true class in this masking mechanism is re-
duced compared with window masking, and this will lead to
a drop in provable robust accuracy.
Use alternative secure aggregation. As discussed in Sec-
tion 6.2, a promising direction of future work is to explore
parameter-free secure aggregation mechanisms. We note that
we have already seen concrete examples of alternative ag-
gregation that can deal with multiple patches in Section 6.1,
where we discuss how to reduce PatchGuard to CBN and DS.
F Additional Discussion on the Limits of
PatchGuard
In Section 5, we evaluate our defense against 1-3% pixel
patches. In this section, we take Mask-BN-17 on ImageNette
7We note that when patches are far away from each other, the problem
is closer to the global adversarial example with a L0 constraint, which is
orthogonal to our work.
Figure 8: Performance of Mask-BN-17 on ImageNette against vari-
ous patch sizes.
Figure 9: Visualization of large occlusion with a 96×96 pixel block
on the 224×224 image.
for a case study to analyze the defense performance when
facing a much larger patch. We report the performance of
Mask-BN-17 against various patch sizes in Figure 8. Note
that the image is in the shape of 224×224; a 32×32 square
patch takes up 2% pixels. As shown in the ﬁgure, the clean
accuracy of Mask-BN-17 drops slowly as the patch size in-
creases. When the patch size is as large as 192 pixels, the
patch will appear in the receptive ﬁeld of all local features of
BagNet-17 and our defense reduces to a random guess (10%
accuracy for the 10-class classiﬁcation task). Similarly, the
provable robust accuracy drops as the patch becomes larger.
We note that this drop also results from the limitation of classi-
ﬁcation problem itself when the patch is large. In Figure 9, we
visualize ﬁve images and occlude them with a 96×96 pixel
block. As shown in the ﬁgure, a large pixel block covers the
entire salient object and makes the classiﬁcation almost im-
possible (recall that our threat model allows the adversary to
put a patch at any location of the image). Notably, our defense
still achieves a 91.1% clean accuracy and 50.7% provable
robust accuracy against this large 96×96 patch. We believe
this analysis further demonstrates the strength of our defense.
2254    30th USENIX Security Symposium
USENIX Association