# Title: Cross-Domain Collaborative Anomaly Detection: So Far Yet So Close

## Authors:
- Nathaniel Boggs<sup>1</sup>
- Sharath Hiremagalore<sup>2</sup>
- Angelos Stavrou<sup>2</sup>
- Salvatore J. Stolfo<sup>1</sup>

<sup>1</sup> Department of Computer Science, Columbia University  
<sup>2</sup> Department of Computer Science, George Mason University

**Contact Information:**
- {boggs, sal}@cs.columbia.edu
- {shiremag, astavrou}@gmu.edu

---

### Abstract

Web applications have become the primary means of accessing vital and sensitive services, such as online payment systems and databases containing personally identifiable information. Unfortunately, the need for ubiquitous and often anonymous access exposes web servers to adversaries. Network-borne zero-day attacks pose a critical and widespread threat to web servers, which cannot be mitigated by signature-based intrusion detection systems. To detect previously unseen attacks, we correlate web requests containing user-submitted content across multiple web servers, using local Content Anomaly Detection (CAD) sensors. The cross-site information exchange happens in real-time, leveraging privacy-preserving data structures. We filter out high-entropy and rarely seen legitimate requests, reducing the amount of data and time an operator must spend sifting through alerts. Our results are based on a fully working prototype using eleven weeks of real-world data from production web servers. During this period, we identified at least three application-specific attacks not belonging to existing classes of web attacks, as well as a wide range of traditional attack classes, including SQL injection, directory traversal, and code inclusion, without using human-specified knowledge or input.

**Keywords:** Intrusion Detection, Web Security, Anomaly Detection, Attacks, Defenses, Collaborative Security

---

### 1. Introduction

Web applications are the primary means of access to the majority of popular Internet services, including commerce, search, and information retrieval. Online web portals have become a crucial part of our everyday activities, ranging from bank transactions and web email to social networking, entertainment, and news. However, this reliance on ubiquitous and, in most cases, anonymous access has made web services prime targets for attacks of varying sophistication. Newly crafted attacks, often termed "zero-day," pose a significant challenge, compromising thousands of web servers before signature-based defenses can recognize them [31]. Although recent research indicates that Anomaly Detection (AD) sensors can detect a class of zero-day attacks, current AD systems experience limitations that prevent them from becoming practical intrusion detection tools.

In this paper, we propose a new defense framework where Content Anomaly Detection (CAD) sensors, rather than traditional IDS systems, share content alerts to detect widespread, zero-day attacks. Unlike pure alert correlation and fusion [29], we exchange abnormal content across sites to reduce the high false positive rate of local CAD systems. We leverage local CAD sensors to generate accurate, reliable alert streams, where false positives are minimized through a process of alert validation. This ensures that false positives rarely reach human operators. We implement information exchange mechanisms enabling collaborative detection of attacks across administrative domains. We believe such collaboration, if done in a controlled and privacy-preserving manner, will significantly elevate costs for attackers while incurring low costs for defenders. Our system has several core capabilities: high-quality, verified alert streams that focus on detecting and learning from zero-day attacks and previously unseen attack instances; scalable alert processing; and modular multi-stage correlation. Figure 1 illustrates the overall architecture.

Intuitively, inbound web requests fall into three categories: legitimate low-entropy requests, legitimate high-entropy or rarely seen requests, and malicious requests. Legitimate low-entropy requests are the most accurately modeled by CAD systems. Therefore, each individual CAD sensor will label previously seen, low-entropy requests as normal and will not exchange them with other CAD sensors. Legitimate high-entropy or rare requests will often show up as abnormal to the local CAD sensor and will therefore be exchanged. Since remote sites do not have similar content due to the high-entropy nature or rarity of these requests, no matches will be identified, and thus no alerts will be raised. On the other hand, malicious requests will appear as abnormal in many local CAD models. Therefore, when exchanged, they will match other sites and alerts will be raised. The more sites participating, the better the coverage and the faster the response to widespread web attacks. Space and structural constraints due to the HTTP protocol and specific web application parsing limit the ability for an attacker to fully exploit polymorphism techniques, analyzed in [22], so each zero-day attack should exhibit similar content across the attacked web services.

In our experimental evaluation, we use eleven weeks of traffic captured from real-world, production web servers located in different physical and network locations. We do not inject any artificial or additional data. All attacks and statistics described are observed on live networks. We measured the detection and false positive changes from adding an additional server to the sharing system. Most interestingly, we confirm the theory presented by [4] that false positives tend to repeat across sites. Additionally, as most of the false positives occur early and often, we show that CAD systems can benefit greatly from a reasonable cross-site training period. This reduces the number of false positives to 0.03% of all normalized web requests. Furthermore, we quantify the similarity of the produced CAD models from each site over long periods of time. Using these models, we provide an analysis of how aggregate normal and abnormal data flows compare between sites and change over time. Moreover, we furnish results regarding the threshold of matching content and the effects of increasing the set of participating collaborating sites. Finally, we are the first to present a real-world study of the average number of alerts a human operator has to process per day. We show that the alert sharing and correlation of alerts reduce the human workload by at least an order of magnitude.

---

### 2. Related Work

Anomaly Detection (AD) techniques have been employed in the past with promising results. Alexsander Lazarevic et al. compare several AD systems in Network Intrusion Detection [12]. For our analysis, we use the STAND [5] method and Anagram [30] CAD sensor as our base CAD system. The STAND process shows improved results for CAD sensors by introducing a sanitization phase to scrub training data. Automated sensor parameter tuning has been shown to work well with STAND in [6]. Furthermore, the authors in [24] observe that replacing outdated CAD models with newer models helps improve the performance of the sensor as the newer models accurately represent the changes in network usage over time. Similarly, in [30], the authors proposed a local shadow server where the AD was used as a filter to perform dynamic execution of suspicious data.

In all of the above works, due to limited resources within a single domain, a global picture of the network attack is never examined. Furthermore, Intrusion Detection Systems (IDS) that leverage machine learning techniques suffer from well-known limitations [20]. In the past, there has been a lot of criticism for Anomaly Detection techniques [25], especially focusing on the high volume of false positives they generate. With our work, we dispel some of this criticism and show that we can improve the performance of CAD systems by sharing content information across sites and correlating the content alerts.

Initially, Distributed Intrusion Detection Systems (DIDS) dealt with data aggregated across several systems and analyzed them at a central location within a single organization. EMERALD [23] and GrIDS [18] are examples of these early scalable DIDS. Recent DIDS systems have focused on collaborative intrusion detection systems across organizations. Kr√ºgel et al. developed a scalable peer-to-peer DIDS, Quicksand [9, 10], and showed that no more messages than twice the number of events are generated to detect an attack in progress. DShield [27] is a collaborative alert log correlation system. Volunteers provide DShield with their logs, which are centrally correlated, and an early warning system provides "top 10"-style reports and blacklists to the public gratis. Our work differs in that we rely on the actual user-submitted content of the web request rather than the source IP. More general mechanisms for node "cooperation" during attacks are described in [2, 1].

DOMINO [33], a closely related DIDS, is an overlay network that distributes alert information based on the hash of the source IP address. DShield logs are used to measure the information gain. DOMINO differs from our technique as it does not use AD to generate alerts. DaCID [7] is another collaborative intrusion detection system based on the Dempster-Shafer theory of evidence of fusing data. Another DIDS with a decentralized analyzer is described by authors in [34].

Centralized and decentralized alert correlation techniques have been studied in the past. The authors in [26] introduce a hierarchical alert correlation architecture. In addition to scalability in a DIDS, privacy preservation of data sent across organizations is a concern. Privacy preservation techniques that do not affect the correlation results have been studied. A privacy-preserving alert correlation technique, also based on the hierarchical architecture [32], scrubs the alert strings based on entropy. We expand Worminator [15], a privacy-preserving alert exchange mechanism based on Bloom filters, which had previously been used for IP alerts. Furthermore, Carrie Gates et al. [8] used a distributed sensor system to detect network scans, albeit with limited success. Finally, there has been extensive work in signature-based intrusion detection schemes [19, 16]. These systems make use of packet payload identification techniques that are based on string and regular expression matching for NIDS [28, 14]. This type of matching is only useful against attacks for which some pattern is already known [11].

---

### 3. System Evaluation

#### 3.1 Data Sets

We collected eight contiguous weeks of traffic between October and November 2010 of all incoming HTTP requests to two popular university web servers: www.cs.columbia.edu and www.gmu.edu. To measure the effects of scaling to multiple sites, we added a third collaborating server, www.cs.gmu.edu, resulting in an additional three weeks in December 2010 of data from all three servers. The second data set allows us to analyze the effects of an additional web site on the overall detection rate and network load. This enables us to show the change in the amount of alert parsing a human operator would have to deal with in a real-world setting and to analyze models of web server request content. All detected attacks are actual attacks coming from the internet to our web servers and are confirmed independently using either IDS signatures [17, 16] developed weeks after the actual attacks occurred and manual inspection when such signatures were not available. However, this does not preclude false negatives that could have been missed by both signature-based IDS and our approach. The number of processed packets across all of our datasets is over 180 million incoming HTTP packets. Only 4 million of them are deemed suspicious because our normalization process drops simple web requests with no user-submitted variables.

#### 3.2 Normalized Content

Our system inspects normalized content rather than packet header attributes such as frequency or source IP address. We process all HTTP GET requests and extract all user-defined content (i.e., user-specified parameters) from the URI across all request packets. Putting aside serious HTTP protocol or server flaws, the user-specified argument string appears to be the primary source of web attacks. We use these user-specified argument strings to derive requests that are deemed abnormal and can be used for correlating data across servers serving different pages. Additionally, we normalize these strings to more accurately compare them [4, 21]. We also decode any hex-encoded characters to identify potential encoding and polymorphic attacks. Any numeric characters are inspected but not retained in the normality model to prevent overtraining from legitimate but high-entropy requests. We convert all letters to lowercase to allow accurate comparisons and drop content less than five characters long to avoid modeling issues. Figure 2 illustrates this process.

Moreover, we perform tests analyzing POST request data as well. POST requests are approximately 0.34% of the total requests. However, our experiments show that the current CAD sensor does not accurately train with data that exhibits large entropy typical in most POST requests. We leave the development of a new CAD sensor that can accurately model POST requests for future work and focus on analyzing GET requests, which dominate the web traffic we observe (99.7%).

#### 3.3 Content Anomaly Detector and Models

In cross-site content correlation, each site builds a local model of its incoming requests using a Content Anomaly Detection (CAD) sensor. In our experiments, we leverage the STAND [5] optimizations of the Anagram [30] CAD sensor, although any CAD sensor with a high detection rate could be used with our approach. However, we apply the CAD sensors to normalized input instead of full packet content to obtain more accurate results. Moreover, we fully utilize all the automatic calibration described in [5].

---