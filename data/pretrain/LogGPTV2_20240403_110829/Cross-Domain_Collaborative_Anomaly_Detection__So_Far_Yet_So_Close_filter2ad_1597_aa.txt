title:Cross-Domain Collaborative Anomaly Detection: So Far Yet So Close
author:Nathaniel Boggs and
Sharath Hiremagalore and
Angelos Stavrou and
Salvatore J. Stolfo
Cross-domain Collaborative Anomaly Detection:
So Far Yet So Close (cid:63)
Nathaniel Boggs1, Sharath Hiremagalore2, Angelos Stavrou2, and Salvatore J.
Stolfo1
1 Department of Computer Science, Columbia University
2 Department of Computer Science, George Mason University
{boggs,sal}@cs.columbia.edu
{shiremag,astavrou}@gmu.edu
Abstract. Web applications have emerged as the primary means of ac-
cess to vital and sensitive services such as online payment systems and
databases storing personally identiﬁable information. Unfortunately, the
need for ubiquitous and often anonymous access exposes web servers to
adversaries. Indeed, network-borne zero-day attacks pose a critical and
widespread threat to web servers that cannot be mitigated by the use
of signature-based intrusion detection systems. To detect previously un-
seen attacks, we correlate web requests containing user submitted content
across multiple web servers that is deemed abnormal by local Content
Anomaly Detection (CAD) sensors. The cross-site information exchange
happens in real-time leveraging privacy preserving data structures. We
ﬁlter out high entropy and rarely seen legitimate requests reducing the
amount of data and time an operator has to spend sifting through alerts.
Our results come from a fully working prototype using eleven weeks of
real-world data from production web servers. During that period, we
identify at least three application-speciﬁc attacks not belonging to an
existing class of web attacks as well as a wide-range of traditional classes
of attacks including SQL injection, directory traversal, and code inclusion
without using human speciﬁed knowledge or input.
Keywords: Intrusion Detection, Web Security, Anomaly Detection, At-
tacks, Defenses, Collaborative Security
1
Introduction
Web applications are the primary means of access to the majority of popular
Internet services including commerce, search, and information retrieval. Indeed,
(cid:63) This work is sponsored in part by US National Science Foundation (NSF) grant CNS-
TC 0915291 and AFOSR MURI grant 107151AA “MURI: Autonomic Recovery of
Enterprise-wide Systems After Attack or Failure with Forward Correction.” The
views and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of the U.S. Government.
2
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
Fig. 1. Architecture
online web portals have become a crucial part of our everyday activities with
usage ranging from bank transactions and access to web email to social network-
ing, entertainment, and news. However, this reliance on ubiquitous and, in most
cases, anonymous access has turned web services into prime targets for attacks
of diﬀerent levels of sophistication. Newly crafted attacks, often termed “zero-
day,” pose a hard to address challenge compromising thousands of web servers
before signature-based defenses are able to recognize them [31]. Although recent
research indicates that Anomaly Detection (AD) sensors can detect a class of
zero-day attacks, currently, AD systems experience limitations which prevent
them from becoming a practical intrusion detection tool.
In this paper, we propose a new defense framework where Content Anomaly
Detection (CAD) sensors, rather than traditional IDS systems, share content
alerts with the aim of detecting wide-spread, zero-day attacks. Contrary to pure
alert correlation and fusion [29], we exchange abnormal content across sites as a
means to reduce the inherent high false positive rate of local CAD systems. We
leverage local CAD sensors to generate an accurate, reliable alert stream where
false positives are consumed through a process of alert validation; false positives
rarely make their way in front of a human operator. We implement information
exchange mechanisms enabling the collaborative detection of attacks across ad-
ministrative domains. We believe such collaboration, if done in a controlled and
privacy preserving manner, will signiﬁcantly elevate costs for attackers at a low
cost for defenders. Our system has a number of core capabilities: high-quality,
veriﬁed alert streams that focus on detecting the presence of and learn from zero-
day attacks and previously unseen attack instances; scalable alert processing; and
modular multi-stage correlation. Figure 1 illustrates the overall architecture.
Intuitively, inbound web requests fall into three categories: legitimate low
entropy requests, legitimate high entropy or rarely seen requests, and malicious
requests. Legitimate low entropy requests are the most accurately modeled by
CAD systems. Therefore, each individual CAD sensor will label previously seen,
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
3
low entropy requests as normal and will not exchange them with other CAD
sensors. Legitimate high entropy or rare requests will often show up as abnormal
to the local CAD sensor and will therefore be exchanged. Since remote sites
do not have similar content due to the high entropy nature or rarity of these
requests, no matches will be identiﬁed, and thus no alerts will be raised. On
the other hand, malicious requests will appear as abnormal in many local CAD
models. Therefore, when exchanged, they will match other sites and alerts will
be raised. The more sites participating the better the coverage and the faster
the response to wide-spread web attacks. Space and structural constraints due
to HTTP protocol and speciﬁc web application parsing limit the ability for an
attacker to fully exploit polymorphism techniques, analyzed in [22], so each zero-
day attack should exhibit similar content across the attacked web services.
In our experimental evaluation, we use eleven weeks of traﬃc captured from
real-world, production web servers located in diﬀerent physical and network
locations. We do not inject any artiﬁcial or additional data. All attacks and
statistics described are observed on live networks. We measured the detection
and false positive changes from adding an additional server in the sharing system.
Most interestingly, we conﬁrm the theory presented by [4] that false positives
tend to repeat across sites. Additionally, as most of the false positives occur
early and often, we show that CAD systems can beneﬁt greatly from a reasonable
cross-site training period. This reduces the number of the false positives to 0.03%
of all the normalized web requests. Furthermore, we quantify the similarity of
the produced CAD models from each site over long periods of time. Using these
models we provide an analysis of how aggregate normal and abnormal data
ﬂows compare between sites and change over time. Moreover, we furnish results
regarding the threshold of the matching content and the eﬀects of increasing the
set of participating collaborating sites. Finally, we are the ﬁrst to present a real-
world study of the average number of alerts a human operator has to process per
day. Moreover, we show that the alert sharing and correlation of alerts reduces
the human workload by at least an order of magnitude.
2 Related Work
Anomaly Detection techniques have been employed in the past with promising
results. Alexsander Lazarevic et al. compares several AD systems in Network
Intrusion Detection [12]. For our analysis, we use the STAND [5] method and
Anagram [30] CAD sensor as our base CAD system. The STAND process shows
improved results for CAD sensors by introducing a sanitization phase to scrub
training data. Automated sensor parameter tuning has been shown to work well
with STAND in [6]. Furthermore, the authors in [24] observe that replacing
outdated CAD models with newer models helps improve the performance of the
sensor as the newer models accurately represent the changes in network usage
over time. Similarly, in [30] the authors proposed a local shadow server where
the AD was used as a ﬁter to perform dynamic execution of suspicious data.
In all of the above works, due to limited resources within a single domain, a
4
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
global picture of the network attack is never examined. Furthermore, Intrusion
Detection Systems that leverage machine learning techniques suﬀer from well-
known limitations [20]. In the past, there has been a lot of criticism for Anomaly
Detection techniques [25] especially focusing on the high volume of the false
positives they generate. With our work we dispel some of this criticism and we
show that we can improve the performance of CAD systems by sharing content
information across sites and correlating the content alerts.
Initially, Distributed Intrusion Detection Systems (DIDS) dealt with data ag-
gregated across several systems and analyzed them at a central location within a
single organization. EMERALD [23] and GrIDS [18] are examples of these early
scalable DIDS. Recent DIDS systems dealt with collaborative intrusion detec-
tion systems across organizations. Kr¨ugel et al. developed a scalable peer-to-peer
DIDS, Quicksand [9, 10] and showed that no more messages than twice the num-
ber of events are generated to detect an attack in progress. DShield [27] is a
collaborative alert log correlation system. Volunteers provide DShield with their
logs where they are centrally correlated and an early warning system provides
“top 10”-style reports and blacklists to the public gratis. Our work diﬀers in that
we rely on the actual user submitted content of the web request rather than on
source IP. More general mechanisms for node “cooperation” during attacks are
described in [2, 1].
DOMINO [33], a closely related DIDS, is an overlay network that distributes
alert information based on hash of the source IP address. DShield logs are used to
measure the information gain. DOMINO diﬀers from our technique as it does not
use AD to generate alerts. DaCID [7] is another collaborative intrusion detection
system based on the Dempster Shafer theory of evidence of fusing data. Another
DIDS with a decentralized analyzer is described by authors in [34].
Centralized and decentralized alert correlation techniques have been studied
in the past. The authors in [26] introduce a hierarchical alert correlation archi-
tecture. In addition to scalability in a DIDS, privacy preservation of data send
across organizations is a concern. Privacy preservation techniques that do not
aﬀect the correlation results have been studied. A privacy preserving alert corre-
lation technique, also based on the hierarchical architecture [32] scrubs the alert
strings based on entropy. We expand Worminator [15] a privacy preserving alert
exchange mechanism based on Bloom ﬁlters, which had previously been used for
IP alerts. Furthermore, Carrie Gates et al. [8] used a distributed sensor system
to detect network scans albeit showing limited success. Finally, there has been
extensive work in signature-based intrusion detection schemes [19] [16]. These
systems make use of packet payload identiﬁcation techniques that are based on
string and regular expression matching for NIDS [28]
[14]. This type of
matching is only useful against attacks for which some pattern is already known.
[11]
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
5
Fig. 2. A normalization example from a conﬁrmed attack. The ﬁrst line of the original
GET request is shown. We use the output of the normalization function for all future
operations.
3 System Evaluation
3.1 Data Sets
We collected contiguous eight weeks of traﬃc between October and Novem-
ber 2010 of all incoming HTTP requests to two popular university web servers:
www.cs.columbia.edu and www.gmu.edu. To measure the eﬀects of scaling to mul-
tiple sites, we added a third collaborating server, www.cs.gmu.edu. This resulted
in an additional three weeks in December 2010 of data from all three servers.
The second data set allows us to analyze the eﬀects of an additional web site to
the overall detection rate and network load. To that end, we are able to show the
change in the amount of alert parsing a human operator would have to deal with
in a real-world setting and analyze models of web server request content. All
attacks detected are actual attacks coming from the internet to our web servers
and are conﬁrmed independently using either IDS signatures[17, 16] developed
weeks after the actual attacks occurred and manual inspection when such sig-
natures were not available. However, that does not preclude false negatives that
could have been missed by both signature-based IDS and our approach. The
number of processed packets across all of our datasets are over 180 million in-
coming HTTP packets. Only 4 million of them are deemed as suspicious because
our normalization process drops simple web requests with no user submitted
variables.
3.2 Normalized Content
Our system inspects normalized content rather than packet header attributes
such as frequency or source IP address. We process all HTTP GET requests
and we extract all user-deﬁned content (i.e. user speciﬁed parameters) from the
URI across all request packets. Putting aside serious HTTP protocol or server
ﬂaws, the user speciﬁed argument string appears to be primary source of web
attacks. We use these user-speciﬁed argument strings to derive requests that are
deemed abnormal and can be used for correlating data across servers serving dif-
ferent pages. Additionally, we normalize these strings in order to more accurately
compare them [4, 21]. We also decode any hex-encoded characters to identify po-
tential encoding and polymorphic attacks. Any numeric characters are inspected
6
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
and but not retained in the normality model to prevent overtraining from legit-
imate but high entropy requests. Also, we convert all the letters to lowercase to
allow accurate comparisons and drop content less than ﬁve characters long to
avoid modeling issues. Figure 2 illustrates this process.
Moreover, we perform tests analyzing POST request data as well. POST re-
quests are approximately 0.34% of the total requests. However, our experiments
show that the current CAD sensor does not accurately train with data that ex-
hibits large entropy typical in most POST requests. We leave the development
of a new CAD sensor that can accurately model POST requests for future work
and we focus on analyzing GET requests, which dominate the web traﬃc we
observe (99.7%).
3.3 Content Anomaly Detector and Models
In cross-site content correlation, each site builds a local model of its incoming
requests using a Content Anomaly Detection (CAD) sensor. In our experiments,
we leverage the STAND [5] optimizations of the Anagram [30] CAD sensor al-
though any CAD sensor with a high detection rate could be used with our
approach. However, we apply the CAD sensors on normalized input instead of
full packet content as they originally operated on in order to obtain more accu-
rate results. Moreover, we fully utilize all of the automatic calibration described