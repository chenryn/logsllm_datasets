of network links, which is inherently difﬁcult. As another al-
ternative, Ripple could dynamically rebuild spanning trees by
changing the root switches, so that the attacker cannot predict
which links are part of the spanning tree.
Spooﬁng synchronization packets. A strong adversary that
has knowledge of synchronization packets could potentially
inject spoofed packets into the network to “poison” the switch
views. A classic defense is to use cryptography, where a
message carried a MAC (Message Authentication Code) for
source authentication; the MAC could also include times-
tamps or sequence numbers to prevent replay attacks. How-
ever, as a practical challenge, today’s P4 programming model
does not support cryptographic operations natively. To solve
this, there are two possible approaches. (a) Crypto modules
can be integrated to programmable data planes as “extern”
hardware modules and invoked by P4 programs [1]. (b) Re-
cent work has designed different cryptographic primitives
using P4 [31, 32], including AES [21]; another related project
explicitly considers authenticating inter-switch communica-
tion in the data plane [56]. These techniques are all useful
building blocks for packet authentication.
6 Evaluation
We have evaluated Ripple in order to answer three research
questions: a) How well does the Ripple compiler work? b)
How much overhead do the Ripple defense programs incur? c)
How effective can Ripple defend against link-ﬂooding attacks,
especially in the presence of adaptive adversaries?
Next, we discuss potential ways that an attacker might disrupt
the Ripple defense, and outline self-defense techniques. As
discussed in Section 2, link-ﬂooding adversaries are typically
at the Internet edge, so in addition to launching link-ﬂooding
attacks, these adversaries can also inject crafted packets to
manipulate Ripple. Attackers that can actively compromised
network switches, eavesdrop, or modify existing trafﬁc are
outside the threat model.
Disrupting the synchronization protocol. The synchroniza-
tion protocol in Ripple propagates panoramic variables across
the network. If an attacker can disrupt this protocol—e.g., by
6.1 Prototype and setup
Software and hardware prototypes. We have developed our
Ripple compiler in ∼ 6000 lines of code in C++. It currently
supports the bmv2 [11] switch backend, which is a widely used
software P4-16 switch model [25,27,35,42,45]. Our compiler
takes in a Ripple policy, a network topology, and emits a P4
program for each switch. We have also developed a hardware
prototype by converting one of the generated P4-16 programs
to P4-Toﬁno—a special P4 dialect for Intel/Barefoot Toﬁno
hardware switches—in 1600 line of P4 code.
USENIX Association
30th USENIX Security Symposium    3873
Baseline defenses. To understand the beneﬁts of Ripple, we
have evaluated it against three SDN-based defenses as base-
line systems. SDN-S and SDN-R are representative of classic
SDN setups: SDN-S samples trafﬁc from OpenFlow switches
at a prespeciﬁed sampling rate to the controller; the controller
runs classiﬁcation algorithms on the trafﬁc sample, and in-
stalls OpenFlow rules to reroute suspicious trafﬁc. SDN-R,
on the other hand, does not perform sampling or classiﬁca-
tion; rather, it collects link load data from all switches, and
computes rerouting decisions for all ﬂows at congested links.
In addition, we have created a third baseline SDN++ to give
SDN defenses an extra advantage—it enhances OpenFlow
switches with an extra module that can run classiﬁcation al-
gorithms in the data plane without involving a controller. We
use SDN++ as a baseline to demonstrate the “upperbound” of
centralized defenses; in practice, such a module is only imple-
mentable in P4 switches. In all cases, the SDN controller uses
SOL [33], a state-of-the-art trafﬁc engineering framework, for
trafﬁc engineering and computing rerouting decisions.
Attacks. We use similar strategies as in Crossﬁre [40] for bot
distribution, ﬂow density, and attack target links. Attackers
generate Crossﬁre, Coremelt, and SPIFFY ﬂows in differ-
ent experimental setups. Normal users employ regular TCP
connections for ﬁle downloads. One of the main evaluation
metrics is the ability for a defense to mitigate attacks and
recover normal user throughput.
Experimental platforms. Most existing work on link-
ﬂooding defense [39, 40, 53] use ﬂow-level simulation, where
trafﬁc patterns are simulated at a coarse, ﬂow-level granular-
ity for scalable evaluation. We adopt the same strategy by
extending an existing ﬂow-level simulator for Ripple [20]. In
addition, we have also evaluated Ripple in two other platforms
to understand the ﬁne-grained behaviors that ﬂow-level simu-
lators cannot capture. Concretely, we have used packet-level
simulation using a version of ns3 [8] that is integrated with
bmv2 support, which can faithfully simulate how P4 switches
process every single packet. Since ﬁne-grained simulation
comes at the cost of higher simulation time, packet-level sim-
ulation is only feasible on smaller networks. Both packet- and
ﬂow-level simulators run in a Ubuntu 18.04 server with six
Intel Xeon E5-2643 Quad-core 3.40 GHz CPUs and 128 GB
RAM. To demonstrate real hardware feasibility, we have also
used a Wedge 100BF hardware switch, whose bandwidth is
100Gbps per port and 1.6 Tbps in aggregate. We ﬂash the
switch hardware with the manually converted P4-Toﬁno pro-
gram for this evaluation. In the following subsections, when
reporting a set of results we also clarify which platform(s) the
experiments have been conducted on.
6.2 Overhead
Our ﬁrst set of experiments measures the overhead of Ripple
defense programs. Most of the results are obtained using the
P4-Toﬁno defense program on a real hardware switch.
Resources
Detection Classiﬁcation Mitigation
Protocol
All
12
10.68
43.75
25.00
6
3.65
14.58
2.78
Stages
VLIWs (%)
ALU (%)
Hash unit (%)
SRAM (%)
6
2.86
18.75
4.17
4.17
6
5.99
29.17
15.28
11.98
3
1.82
10.42
4.17
5.38
4.48
15.62
Table 2: Resource utilization on the Toﬁno hardware switch
(policy: Coremelt).
(a) Latency (nanoseconds)
(b) Throughput (Gbps)
Figure 4: Ripple incurs extra latency on the order of nanosec-
onds, and it achieves linespeed throughput.
Figure 5: Trafﬁc overheads of Ripple’s distributed protocol at
different synchronization periods.
Hardware utilization. Table 2 shows the hardware re-
source utilization for each program component. As we can
see, the classiﬁcation pipeline incurs the highest resource uti-
lization, because it is the most complex component of the
policy. Overall, the defense program uses 10.68% VLIWs
(Very Long Instruction Words) and 43.75% ALUs (Arith-
metic Logical Units) for header computation, 25% of the
CRC hash units, as well as 15.62% SRAM (Static RAM).
All these hardware resources are spread across 12 hardware
stages. We note that more recent switch models (e.g., Toﬁno
2) have higher resource provisions for all types of resources.
Another important takeaway is that the defense program is
implementable in today’s programmable switch hardware.
Latency. Next, we evaluate the extra latency incurred by
the Ripple defense, using a baseline switch program “Fwd”,
which is a minimal P4 program that only forwards trafﬁc with-
out any other processing. As Figure 4(a) shows, the Ripple
defense program incurs 139 nanoseconds of latency compared
with the baseline. Interestingly, we found that the classiﬁca-
tion component incurs the least latency overhead, and the
detection component incurs the most overhead. This is be-
cause the classiﬁcation component is dominated by a set of
3874    30th USENIX Security Symposium
USENIX Association
0100200300400500Fwd.Detect.Class.Mitig.Proto.AllLatencyDefense050100Fwd.Detect.Class.Mitig.Proto.AllPer-port throughputDefense 0 0.5 1 1.5 2 2.550100150200250300350400450500 0.2 0.4 0.6 0.8 1 1.2 1.4Multicast (kBps)Spanning tree (MBps)Synchronization period (ms)Multicast modeSpanning tree modePolicy
LoC of policies LoC of P4 Compilation time
Crossﬁre
Coremelt
SPIFFY
Multi-vector
13
9
18
18
1509
924
1516
1910
68ms
37ms
69ms
85ms
Table 3: Ripple captures state-of-the-art defenses within 20
lines of code; the compiler works efﬁciently and generates P4
programs for each policy within one second. Multi-vector is
a combination of Crossﬁre and Coremelt.
Topo Name ANS CRL Bell Canada
SurfNet UUNet
#switches
#links
18
25
33
38
48
65
50
68
49
84
Table 4: Topology setups used in large-scale simulation. All
topologies are from Topology Zoo [7].
6.4 Defense effectiveness
Next, we evaluate the effectiveness of the defenses on three
topologies with increasing sizes and trafﬁc complexities. We
use the packet-level simulator for the small network; we use
ﬂow-level simulator for medium and large networks because
ﬁne-grained simulation does not scale to large setups. Table 4
shows the topology setups that we have used for evaluation.
Figures 7(a)-(i) present the defense effectiveness of all
tested systems, as measured by the throughput degradation the
attack causes over time. We normalize the aggregate through-
put of normal users over that before the attack, so a higher
percentage indicates a stronger defense, and 100% means a
full recovery. We also plot a “no defense” baseline that shows
the attack impact without deploying any defense. There are
four key takeaways: (1) Compared to SDN-R, which reroutes
all ﬂows from the congested links, Ripple achieves a simi-
lar level of throughput recovery but it acts much faster. This
is because Ripple directly reroutes trafﬁc in the data plane
without a central controller. As the network becomes larger,
the advantage of Ripple also becomes more prominent. (2)
SDN-S only samples and reroutes 1% ﬂows, so it acts faster
than SDN-R; compared to SDN-S, Ripple recovers throughput
much more effectively. This is because the SDN controller
only sees heavily downsampled trafﬁc. The defense decisions
cannot take action on the majority of malicious ﬂows, as they
are not included in the samples. (3) SDN++ is the most pow-
erful SDN variant, and it can recover throughput with similar
effectiveness as SDN-R. It also responds faster, as classiﬁ-
cation is done in the extra switch module and the controller
performs trafﬁc engineering on reported suspicious ﬂows. (4)
Overall, Ripple outperforms all three SDN baselines.
We quantify the effectiveness of a defense system by mea-
suring the attack impact on normal user throughput. For each
defense, we measure the throughput degradation ratio per unit
time, and compute the aggregate degradation until throughput
recovers to a stable state. This aggregate A denotes the attack
Figure 6: SDN-R reroutes all trafﬁc, and normal user ﬂows
experience an average path length increase of 31%.
register operations, which are parallelized by switch hard-
ware; on the other hand, the detection component involves
sequential processing. Overall, the extra latency is negligible,
as network RTTs are typically on the order of milliseconds in
the Internet core.
Throughput. Next, we evaluate the throughput of Ripple
using the on-switch hardware packet generator, which can gen-
erate full linespeed trafﬁc (100Gbps per port). Our baseline
program is still “Fwd”. As Figure 4(b) shows, the through-
puts of Ripple and of the baseline are very close, at about
99.52 Gbps per port. This is because of the pipelined na-
ture of the switch hardware, which is designed to mask small
latency increases by massive parallelism.
The above results demonstrate that Ripple defenses are
practical on today’s hardware switches, and that they incur
relatively low overhead. Next, we turn to measure the trafﬁc
overhead due to the Ripple distributed protocol using packet-
level simulation:
Trafﬁc overhead. Figure 5 presents the results for different
synchronization periods for a single link. For both synchro-
nization modes (spanning tree vs. multicast), the overheads
are low enough to be practical. Concretely, the multicast mode
only propagates link utilization metrics, and it incurs 2.1 KBps
overhead at a period of 50 ms. The spanning tree mode prop-
agates all other metric types and generates more trafﬁc: the
overhead is 1.4 MBps at 50 ms. More frequent synchroniza-
tion also leads to higher overhead. Overall, the overhead is
low since today’s network linkspeeds are 40-100Gbps.
6.3 The Ripple Compiler
Table 3 shows the number of lines of code that Ripple uses
to capture state-of-the-art policies. The policy programs are
much more concise than the generated P4 programs. Ripple
also works efﬁciently, generating switch programs within
one second in all cases. We have manually veriﬁed that the
programs can successfully mitigate Crossﬁre, Coremelt, and
SPIFFY attacks by deploying them to the ns3 simulator and
evaluating them against real attacks.
USENIX Association
30th USENIX Security Symposium    3875
 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14CDF (%)Hop countBefore reroutingAfter rerouting(a) SDN-R (small network, packet-level)
(b) SDN-R (medium network, ﬂow-level)
(c) SDN-R (large network, ﬂow-level)
(d) SDN-S (small network, packet-level)
(e) SDN-S (medium network, ﬂow-level)
(f) SDN-S (large network, ﬂow-level)
(g) SDN++ (small network, packet-level)
(h) SDN++ (medium network, ﬂow-level)
(i) SDN++ (large network, ﬂow-level)
Figure 7: Ripple can mitigate attacks faster than all SDN baselines, and it recovers normal throughputs effectively. Small:
customized topology with 10 switches and 15 links; Medium: Bell Canada; Large: UUNet. Table 4 summarizes the topologies.
impact, and a larger A means that the attack is more effective.
We found that under the Ripple defense, we have A = 0.17 in
the worst-case scenario; this can be interpreted as “the attack
degrades the throughput for 17% for 1 second”. For SDN
baselines, on the other hand, we have A = 10, 30, and 15 for
SDN++, SDN-S, and SDN-R on average, respectively, which
are orders of magnitude larger. As another interesting ﬁnding,
SDN-R performs worse than SDN++ and SDN-S in terms of
protecting normal user ﬂows. This is because the latter two
defenses use a classiﬁer to identify and then only reroute po-
tentially suspicious ﬂows; user ﬂows still follow the original
routing. In contrast, SDN-R reroutes all ﬂows, which leads to
higher hop counts and increased latency for user ﬂows (Fig-