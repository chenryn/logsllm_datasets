print out the trigger and stick it on, for example, a trafﬁc sign.
Nonetheless, it is feasible for an attacker to craft a trans-
parent trigger, e.g., printing the trigger using a plastic with
a certain transparency. Therefore, we have tested STRIP
detection capability under ﬁve different trigger transparency
settings: 90%, 80%, 70%, 60% and 50%, shown in Fig. 14 in
Appendix A. We employ CIFAR10 and trigger b—shown in
Fig. 7 (b)—in our evaluations.
Table. V in Appendix A summarizes the classiﬁcation rate
of clean images, attack success rate of trojaned images, and
detection rate under different
transparency settings. When
training the trojaned model, we act as an attacker and stamp
triggers with different transparencies to clean images to craft
3The batch-size is 32.
Section VI-C—targeting the same label in CIFAR10. Given
the trojaned model, the classiﬁcation rate of clean images is
86.12%. As for any trigger, its attack success rate is 100%.
Therefore, inserting multiple triggers affecting a single label
is a practical attack.
We then employ STRIP to detect these triggers. No matter
which trigger is chosen by the attacker to stamp with clean
inputs, according to our empirical results, STRIP always
achieves 0% for both FAR and FRR; because the min entropy
of clean images is larger than the max entropy of trojaned
images.
E. Source-label-speciﬁc (Partial) Backdoors
Although STRIP is shown to be very effective in detecting
input-agnostic trojan attacks, STRIP may be evaded by an ad-
versary employing a class-speciﬁc trigger—an attack strategy
that is similar to the ‘all-to-all’ attack [8]. More speciﬁcally,
the targeted attack is only successful when the trigger is
stamped on the attacker chosen/interested classes. Using the
MNIST dataset as an example, as attacker poisons classes 1
and 2 (refereed to as the source classes) with a trigger and
changes the label to the targeted class 4. Now the attacker can
activate the trigger only when the trigger is stamped on the
source classes [8]. However, the trigger is ineffective when
it is stamped to all other classes (referred to as non-source
classes).
Notably, if the attacker just intends to perform input-speciﬁc
attacks,
the attacker might prefer the adversarial example
attack—usually speciﬁc to each input, since the attacker is
no longer required to access and tamper the DNN model
or/and training data, which is easier. In addition, a source-
label-speciﬁc trojan attack is harder to be performed in certain
scenarios such as in the context of federated learning [10],
because an attacker is not allowed to manipulate other classes
owned by other participants.
is trained,
Although such class-speciﬁc backdoor attack is out
the
scope of our threat model detailed in Section IV-B, we test
STRIP robustness against it. In this context, we use trigger
b and CIFAR10 dataset. As one example case, we set source
classes to be ‘airplane’ (class 0), ‘automobile’ (class 1), ‘bird’
(class 2), ‘cat’ (class 3), ‘deer’ (class 4), ‘dog’ (class 5)
and ‘frog’ (class 6). Rest classes are non-source classes. The
targeted class is set to be ‘horse’ (class 7). After the trojaned
model
its classiﬁcation rate of clean inputs is
85.56%. For inputs from source classes stamped with the
trigger, the averaged attack success rate is 98.20%. While
for inputs from non-source classes such as ‘ship’ (class 8)
and ‘truck’ (class 9) also stamped with the trigger, the attack
success rates (misclassiﬁed to targeted class 7) are greatly
reduced to 19.7% and 12.4%, respectively. Such an ineffective
misclassiﬁcation rate for non-source class inputs stamped with
the trigger is what the partial backdoor aims to behave, since
they can be viewed as clean inputs from the class-speciﬁc
4The attacker needs to craft some poisoned samples by stamping the trigger
with non-source classes, but keeps the ground-truth label. Without doing so,
the trained model will be input-agnostic.
9
backdoor attack perspective. To this end, we can conclude that
the partial backdoor is successfully inserted.
We apply STRIP on this partial backdoored model. Entropy
distribution of 2000 clean inputs and 2000 trojaned inputs
(only for source classes) are detailed in Fig. 12. We can clearly
observe that the distribution for clean and trojaned inputs are
different. So if the defender is allowed to have a set of trojaned
inputs as assumed in [20], [21], our STRIP appears to be able
to detect class-speciﬁc trojan attacks; by carefully examining
and analysing the entropy distribution of tested samples (done
ofﬂine) because the entropy distribution of trojaned inputs does
look different from clean inputs. Speciﬁcally, by examining
the inputs with extremely low entropy, they are more likely to
contain trigger for partial backdoor attack.
Figure 12. Entropy distribution of clean and trojaned inputs for partial
trojaned model. Trigger b and CIFAR10 dataset.
Nevertheless, Neural Cleanse, SentiNet and STRIP have
excluded the assumption that the user has access to trojaned
samples under the threat model. They thereby appear to be
ineffective to detect source-label-speciﬁc triggers—all these
works mainly focus on the commonplace input-agnostic trojan
attacks. Detecting source-label-speciﬁc triggers, regarded as
a challenge, leaves an important future work in the trojan
detection research.
F. Entropy Manipulation
STRIP examines the entropy of inputs. An attacker might
choose to manipulate the entropy of clean and trojaned inputs
to eliminate the entropy difference between them. In other
words, the attacker can forge a trojaned model exhibiting
similar entropy for both clean and trojaned samples. We refer
to such an adaptive attack as an entropy manipulation.
An identiﬁed speciﬁc method to perform entropy manipu-
lation follows the steps below:
1) We ﬁrst poison a small fraction of training samples
(speciﬁcally, 600) by stamping the trigger c. Then, we
(as an attacker) change all the trojaned samples’ labels
to the attacker’s targeted class.
2) For each poisoned sample, we ﬁrst randomly select N
images (10 is used) from training dataset and superim-
pose each of N images (clean inputs) with the given poi-
soned (trojaned) sample. Then, for each superimposed
trojaned sample, we randomly assign a label to it and
include it into the training dataset.
The intuition of step (2) is to cause predictions of perturbed
trojaned inputs to be random and similar to predictions of
0.00.51.01.50.0000.0250.0500.0750.100Probability (%)normalized entropywithout trojanwith trojanperturbed clean inputs. After training the trojaned model
using the above created poisoned dataset, we found that the
classiﬁcation rate for clean input is 86.61% while the attack
success rate is 99.95%. The attack success rate drops but is
quite small—originally it was 100% as detailed in Table II.
The attacker can still successfully perform the trojan attack.
As shown in Fig. 13, the entropy distribution of clean and
trojaned inputs are similar.
However, when the entropy distribution of the clean inputs
is examined, it violates the expected normal distribution 5.
In addition, the entropy appears to be much higher. It is
always more than 3.0, which is much higher than that is
shown in Fig. 8 (d). Therefore, such an adaptive attack can be
detected in practice by examining the entropy of clean inputs
(without reliance on trojaned inputs) via the proposed strong
perturbation method. Here, the abnormal entropy distribution
of the clean inputs indicates a malicious model.
Figure 13. Entropy distribution of clean and trojaned inputs under entropy
manipulation adaptive attack. CIFIAR10 and trigger c are used.
VII. RELATED WORK AND COMPARISON
Previous poisoning attacks usually aim to degrade a clas-
siﬁer’s accuracy of clean inputs [28], [29]. In contrast, trojan
attacks maintain prediction accuracy of clean inputs as high
as a benign model, while misdirecting the input to a targeted
class whenever the input contains an attacker chosen trigger.
A. Attacks
In 2017, Gu et al. [8], [30] proposed Badnets, where
the attacker has access to the training data and can, thus,
manipulate the training data to insert an arbitrarily chosen
trigger and also change the class labels. Gu et al. [8] use
a square-like trigger located at the corner of the digit image
of the MNIST data to demonstrate the trojan attack. On the
MNIST dataset, the authors demonstrate an attack success
rate of over 99% without impacting model performance on
benign inputs. In addition, trojan triggers to misdirect trafﬁc
sign classiﬁcations have also been investigated in [8]. Chen
et al. [6] from UC Berkeley concurrently demonstrated such
backdoor attacks by poisoning the training dataset.
Liu et al. [16] eschew the requirements of accessing the
training data. Instead, their attack is performed during the
5We have also tested such an adaptive attack on the GTSRB dataset, and
observed the same abnormal entropy distribution behavior of clean inputs.
10
model update phase, not model
training phase. They ﬁrst
carry out reverse engineer to synthesize the training data, then
improve the trigger generation process by delicately designing
triggers to maximize the activation of chosen internal neurons
in the neural network. This builds a stronger connection
between triggers and internal neurons, thus, requiring less
training samples to insert backdoors.
Bagdasaryan et al. [10] show that federated learning is
fundamentally vulnerable to trojan attacks. Firstly, participants
are enormous, e.g., millions, it is impossible to guarantee that
none of them are malicious. Secondly, federated learning is
designed to have no access to the participant’s local data and
training process to ensure the privacy of the sensitive training
data; therefore, participants can use trojaned data for training.
The authors demonstrate that with controll over no more than
1% participants, an attacker is able to cause a global model
to be trojaned and achieves a 100% accuracy on the trojaned
input even when the attacker is only selected in a single round
of training—federated learning requires a number of rounds to
update the global model parameters. This federated learning
trojan attack is validated through the CIFAR10 dataset that we
also use in this paper.
B. Defenses
Though there are general defenses against poisoning at-
tacks [31], they cannot be directly mounted to guard against
trojan attacks. Especially, considering that the user has no
knowledge of the trojan trigger and no access to trojaned sam-
ples, this makes combating trojan attacks more challenging.
Works in [32], [33] suggest approaches to remove the trojan
behavior without ﬁrst checking whether the model is trojaned
or not. Fine-tuning is used to remove potential trojans by
pruning carefully chosen parameters of the DNN model [32].
However, this method substantially degrades the model accu-
racy [17]. It is also cumbersome to perform removal operations
to any DNN model under deployment as most of them tend to
be benign. Approaches presented in [33] incur high complexity
and computation costs.
Chen et al. [20] propose an activation clustering (AC)
method to detect whether the training data has been trojaned
or not prior to deployment. The intuition behind this method
is that reasons why the trojaned and the benign samples
receive same predicted label by the trojaned DNN model are
different. By observing neuron activations of benign samples
and trojaned samples that produce same label in hidden layers,
one can potentially distinguish trojaned samples from clean
samples via the activation difference. This method assumes
that the user has access to the trojaned training samples in
hand.
Chou et al. [11] exploit both the model interpretability and
object detection techniques, referred to as SentiNet, to ﬁrstly
discover contiguous regions of an input image important for
determining the classiﬁcation result. This region is assumed
having a high chance of possessing a trojan trigger when it
strongly affects the classiﬁcation. Once this region is deter-
mined, it is carved out and patched on to other held-out images
that are with ground-truth labels. If both the misclassiﬁcation
1.52.02.53.00.00.20.40.6Probability (%)normalized entropywithout trojanwith trojan11
COMPARISON WITH OTHER TROJAN DETECTION WORKS.
Table IV
Work
Activation Clustering (AC) by Chen et al. [20]
Neural Cleanse by Wang et al. [17]
SentiNet by Chou et al. [11]
STRIP by us
Black/White
-Box Access1
Run-time
Computation
Cost
White-box
Black-box
Black-box
Black-box
No
No
Yes
Yes
Moderate
High
Moderate
Low
Time
Overhead
Moderate
High
Moderate
Low
Trigger Size
Dependence
Access to
Trojaned Samples
Detection
Capability
No
Yes
Yes
No
Yes
No
No
No
F1 score nearly 100%
100%2
5.74% FAR and 6.04% FRR
0.46% FAR and 1% FRR3
1 White-box requires access to inner neurons of the model.
2 According to case studies on 6 infected, and their matching original model, authors [17] show all infected/trojaned and clean models
can be clearly distinguished.
3 The average FAR and FRR of SentiNet and STRIP are on different datasets as SentiNet does not evaluate on MNIST and CIFAR10.
rate—probability of the predicted label is not the ground-truth
label of the held-out image—and conﬁdence of these patched
images are high enough, this carved patch is regarded as an
adversarial patch that contains a trojan trigger. Therefore, the
incoming input is a trojaned input.
In Oakland 2019, Wang et al. [17] propose the Neural
Cleanse method to detect whether a DNN model has been
trojaned or not prior to deployment, where its accuracy is
further improved in [15]. Neural Cleanse is based on the intu-
ition that, given a backdoored model, it requires much smaller
modiﬁcations to all input samples to misclassify them into
the attacker targeted (infected) label than any other uninfected
labels. Therefore, their method iterates through all labels of
the model and determines if any label requires a substantially
smaller amount of modiﬁcation to achieve misclassiﬁcations.
One advantage of this method is that
the trigger can be
discovered and identiﬁed during the trojaned model detection
process. However, this method has two limitations. Firstly,
it could incur high computation costs proportionally to the
number of labels. Secondly, similar to SentiNet [11],
the
method is reported to be less effective with increasing trigger
size.
C. Comparison
We compare STRIP with other three recent trojan detection
works, as summarized in Table IV. Notably, AC and Neural
Cleanse are performed ofﬂine prior to the model deployment to
directly detect whether the model has been trojaned or not. In
contrast, SentiNet and STRIP are undertake run-time checking
of incoming inputs to detect whether the input is trojaned or
not when the model is actively deployed. STRIP is efﬁcient
in terms of computational costs and time overhead. While AC
and STRIP are insensitive to trojan trigger size, AC assumes
access to a trojaned sample set.
We regard SentiNet to be mostly related to our approach
since both SentiNet and STRIP focus on detecting whether
the incoming input has been trojaned or not during run-time.
However, there are differences: i) We do not care about the
ground-truth labels of neither the incoming input nor the
drawn images from the held-out samples, while [11] relies on
the ground-truth labels of the held-out images; ii) We intro-
duce entropy to evaluate the randomness of the outputs—this
is more convenient, straightforward and easy-to-implement
in comparison with the evaluation methodology presented
in [11]; iii) STRIP evaluations demonstrate its capability of