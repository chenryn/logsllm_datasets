title:Falcon Codes: Fast, Authenticated LT Codes (Or: Making Rapid Tornadoes
Unstoppable)
author:Ari Juels and
James Kelley and
Roberto Tamassia and
Nikos Triandopoulos
2013 IEEE International Symposium on Information Theory
LT-AF Codes: LT Codes with Alternating Feedback
Ali Talari and Nazanin Rahnavard
Oklahoma State University, Stillwater, OK 74078,
Emails: {ali.talari, nazanin.rahnavard}@okstate.edu
Abstract—LT codes are capacity achieving and ﬂexible rateless
codes that need a single-bit feedback to inform the encoder of
the successful decoding. However, this weak feedback channel
remains unused when the transmission is in progress. In addi-
tion, although LT codes are asymptotically capacity achieving,
their performance signiﬁcantly degrades for short block lengths.
Consequently, we propose LT Codes with Alternating Feedback
(LT-AF Codes) that considerably improve the performance of LT
codes for short-block lengths when belief propagation decoder is
in use. In our proposed scheme, the decoder alternatively issues
two types of feedbacks based on the dependencies of the still
undecoded received output symbols and the number of decoded
input symbols. We propose two methods to form the latter type of
feedback with a trade-off in their complexity and performance.
I. INTRODUCTION
LT codes [1] are the ﬁrst practical realization of Rateless
codes [1–3] with capacity-achieving performance on erasure
channels. LT codes are universal in the sense that they are
simultaneously near optimal for every erasure channel with
varying or unknown erasure rate ε ∈ [0, 1) [1]. These codes
is issued by the decoder
require only one feedback that
(receiver) to inform the encoder (transmitter) of a successful
LT decoding. However, in LT coding the available feedback
channel remains under utilized during the transmission.
Further, as the data-block length decreases the performance
of LT codes signiﬁcantly deteriorates [1, 4, 5]. Therefore, ex-
isting work have proposed to employ feedbacks to inform the
encoder of the number of successfully decoded input symbols
(source packets) [6–8], a suitable input symbol to be sent for
enhancing the decoding [9], or the index of some recovered
input symbols [10]. In this paper, we take a step further and
propose to alternatively generate two types of aforementioned
feedbacks and propose LT codes with alternating feedbacks
(LT-AF codes). We propose two novel methods to analyze the
decoders buffer and to request a suitable input symbol that
greedily makes the highest progress in the decoding process
as a feedback. In contrast to other existing work, we design LT-
AF codes with a realistic feedback channel assumption, where
the feedback channel can have unknown or varying erasure
rate εf b ∈ [0, 1).
The paper is organized as follows. In Section II, we provide
a brief review on LT codes. In Section III we review the
existing rateless codes with feedback. In Section IV, we
propose and analyze LT-AF codes. Section V reports the
performance of LT-AF codes. Finally, Section VI concludes
the paper.
II. REVIEW ON LT CODES
First, without loss of generality and for simplicity, let us
assume that the input and output symbols are binary symbols,
while they may contain thousands of bits. LT codes [1] have
simple encoding and decoding procedures as follows.
(cid:2)k
LT encoding: In LT encoding of k input symbols, ﬁrst an
output symbol degree d is chosen from the Robust-Soliton
(RS) degree distribution [1] {μ1, μ2, . . . , μk}, where μi is the
probability that d = i (also shown by its generator polynomial
i=1 μixi). Next, d input symbols are chosen
μ(x) =
uniformly at random from k input symbols and are XORed to
generate an output symbol. We refer to the d contributing input
symbols in forming an output symbol as its neighbors. This
procedure can be potentially repeated inﬁnite number of times;
however, it is stopped upon the reception of the single-bit
feedback. The RS distribution μ(.) is obtained by combining
the ideal-Soliton (IS) distribution ρ(.) and distribution τ (.)
given by
ρ(i) =
k
1
i(i−1)
i = 1,
i = 2, . . . , k,
(1)
(cid:3) 1
⎧⎨
⎩ R
ik
R
(cid:2)k
and
− 1,
,
τ (i) =
k ln( R
δ )
0
i = 1, . . . , k
R
i = k
R
i = k
R + 1, . . . , k,
√
respectively, where R = c ln( k
k, and δ and c are two
δ )
tuneable parameters [1]. It is easy to see that the average
i=1 iρ(i) =
degree of output symbols with IS distribution is
ρ(cid:2)(1) = H(k) ≈ ln k, where ρ(cid:2)(x) is the ﬁrst derivative
of ρ(x) with respect to its variable x, and H(k) is the kth
Harmonic number [1]. Finally, RS degree distribution μ(.) is
obtained by
(cid:2)k
μ(i) =
ρ(i) + τ (i)
β
, i = 1, . . . , k,
(2)
where β =
i=1 ρ(i) + τ (i).
LT decoding: Rateless decoding is iteratively performed
upon arrival of a new output symbols. The decoder ﬁnds
an output symbol such that the value of all but one of its
neighboring input symbols is known. It recovers the value of
the unknown input symbol by bitwise XOR operations and
removes all the edges incident to that output symbol. This
process is repeated until no such an output symbol exists. The
set of output symbols that are reduced to degree one is called
the ripple. If the ripple becomes empty the decoding stops
and waits for new output symbols of degree one to proceed
the decoding. If all k input symbols are recovered, it issues a
single-bit feedback indicating success of the decoding.
As shown in [1] the only condition for a successful LT
decoding is the delivery of a certain number of output
978-1-4799-0446-4/13/$31.00 ©2013 IEEE
2646
2013 IEEE International Symposium on Information Theory
symbols since they are statistically independent. Let γsucc be
the required coding overhead to have a successful decoding
with high probability (w.h.p.), i.e., γsucc × k coded symbols
are enough to decode k input symbols w.h.p. Further, let
γ, 0 ≤ γ ≤ γsucc, denote the received coding overhead
(meanwhile the transmission is in progress), i.e., γ × k is the
number of received output symbols at receiver.
Although LT codes with RS distribution μ(.) are asymptot-
ically capacity achieving, i.e., γsucc → 1 as k → ∞ [1], when
k is ﬁnite γsucc becomes signiﬁcantly larger than 1 [1, 4, 5],
which may result in an inefﬁcient FEC coding. Therefore, we
exploit the feedback channel to obtain a much smaller γsucc
for a ﬁnite k in LT-AF coding.
III. RELATED WORK
Previously, many works have investigated LT codes with
feedback [6–9, 11, 12]. Authors in [6] proposed shifted LT
(SLT) codes and have shown that when n input symbols have
been recovered at the decoder, the degree of each arriving
output symbol decreases by an expected k−n
fraction (due to
k
earlier recovery of their neighboring input symbol). Therefore,
they propose to shift the RS distribution such that its average
k
degree μ(cid:2)(1) is increased by
k−n , where μ(cid:2)(x) is the ﬁrst
derivative of μ(x) with respect to its variable x. With this
setup, arriving output symbols at decoder always maintain an
RS degree distribution regardless of the value of n. SLT codes
considerably improve the performance of LT codes. We make
some changes to the idea of distribution shifting proposed
in SLT codes and employ it in the design of LT-AF codes,
while showing that LT-AF codes considerably outperform SLT
codes.
In contributions [7] and [8], Growth codes and RT-oblivious
codes have been proposed, respectively, which have basically
the same structure. In these algorithms, as n increases and
reaches to certain thresholds a feedback indicating that de-
coder has achieved the corresponding threshold is initiated.
Therefore, the encoder gradually increases the degree of output
symbols on-the-ﬂy based on the feedbacks such that
the
instantaneous decoding probability of each delivered output
symbol is maximized. Since Growth and RT-oblivious codes
only consider the instantaneous recovery probability of each
output symbol upon reception, they do not perform as well as
SLT and LT-AF codes.
Authors in [9] propose to employ IS degree distribution ρ(.)
for LT coding. They have proposed to start decoding when an
overhead of γ = 1 has been delivered to the decoder. When
the decoding halts during the decoding process and some input
symbols are remaining unrecovered, a randomly selected input
symbol that is a neighbor of an output symbol of degree
two is requested from the encoder. Despite the advantages of
algorithm proposed in [9], in this scheme many feedbacks are
issued back-to-back as soon as γ exceeds 1.
Authors in [11] redesigned the LT coding degree distribu-
tions when a few feedback opportunities are available based
on k. They show that incorporating feedback to LT codes
can signiﬁcantly decrease both the coding overhead and the
encoding/decoding complexity. In [11], the point at which the
feedback is initiated should be optimized, while in LT-AF
codes no optimization is required. Further, new optimization
of degree distribution is not necessary in LT-AF codes.
IV. LT-AF CODES
Let Ωk,n(.) denote the degree distribution of LT-AF codes
for a data-block of length k when n input symbols are
already recovered at decoder. We adopt the idea of SLT codes
[6], and propose to shift Ωk,n(.) based on n (see Section
III). Therefore, we allow the decoder to issue the ﬁrst type
of feedback referred to as f b1, which is used to keep the
encoder updated with the current value of n. The encoder
also generates an output symbol of degree-one (containing
a randomly selected input symbol) as acknowledgment as
described in detail later.
Although IS distribution given by (1) is solely designed
for the theoretical analysis of RS distribution, we slightly
modify and employ it at the encoding phase of LT-AF codes in
combination with two types of feedback. The IS distribution
is tuned for γsucc = 1 such that at each decoding iteration in
expectation exactly one input symbol is recovered and only
one output symbols is reduced to degree 1 and is added
to the ripple. The single output symbol in the ripple with
degree one can decode one input symbol in the next iteration.
Since on average only a single degree one output symbol is
generated for k output symbols (note that ρ(1) = 1
k ), the IS
distribution would ideally realize an optimal coding/decoding,
i.e., complete recovery of k input symbols from k output
symbols and γsucc = 1.
However, due to inherent randomness and uncertainties in
the output symbol generation there is a high probability that
an output symbol does not reduce to degree one when an
input symbol is recovered. Consequently, the ripple becomes
empty and the decoding stops although undecoded output
and unrecovered input symbols are remaining. Despite this,
we propose to employ IS distribution in LT-AF coding and
exploit the feedback channel and request a suitable input
symbol (which is an output symbol of degree 1) so that the
decoding can continue. Therefore, we allow the decoder to
request desired input symbols employing the second type of
feedback referred to as f b2.
Moreover, to design Ωk,n(.) we propose to modify the
IS distribution such that the encoder does not generate any
degree-one output symbol. With this setup, we may exploit
the degree-one output symbols as acknowledgments from the
encoder to the reception of feedbacks. Therefore, the encoder
generates a degree-one output symbol if and only if it has
received a f b1 or f b2. Consequently, the lack of the arrival of
an output symbol at the decoder with degree one after issuing
a f b1 or f b2 clearly indicates a feedback loss. Consequently,
all feedback packet losses are identiﬁed by the decoder and a
feedback retransmission is performed. Therefore, the decoding
recovery rate of LT-AF codes does not considerably degrade
at high feedback channel loss rates εf b ∈ [0, 1) in contrast
to existing work [6–10]. We should note that a degree-one
output symbol generated at the encoder after a f b1 contains
a randomly selected input symbol. However, such an output
2647
2013 IEEE International Symposium on Information Theory
(cid:2)k
symbol would contain the requested input symbol (selected by
decoder) after a f b2.
Let Ωk,n(x) =
i=1 Ωk,n,ixi, where Ωk,n,d is the prob-
ability of selecting degree d to generate an LT-AF output
symbol. Since we do not allow the encoder to generate any
degree-one symbol, we set Ωk,n,1 = 0. Further, considering
the distribution shifting idea from [6] we deﬁne Ωk,n,d as
follows.
(cid:7)
Ωk,n,d =
0
k−1 ρk−n(i) d = 2, 3, . . . , k,(cid:8) i
1−
k
d = 1,
(cid:9) = d,
n
k
(3)
where (cid:8).(cid:9) returns the closest integer to its argument, ρk(.) is
the IS distribution for a data-block of length k given by (1),
and k
k−1 is the normalizing factor to have
d Ωk,n,d = 1.
(cid:2)
Lemma 1: The average degree of a check node generated
employing Ωk,n(.) distribution is
(cid:8)
i
iΩ(i) = Ω(cid:2)
k,n(1) ≈ k2 ln (k − n)
(k − n)(k − 1)
which gives
,
(4)
Therefore,
k,n1 (1) − Ω(cid:2)
Ω(cid:2)
k,0(1) =
the ﬁrst f b1 is issued for a value of n1 that
√
ln k. Using Lemma 1 we have
ln (k − n0)
√
ln k,
k − n0
k − 1
=
k
ln (k − n1) − k
(6)
k
k − n1
k
k − 1
=
1
k
which gives
ln (k − n1)
k − n1
Next, let Ai(k) = 1
(cid:12)
. (7)
ln(k − ni−1)
.
Since Ai(k) > −π,∀k, i employing Lambert’s W function,
we have
√
k − n0
ln k +
√
ln k + k
k − 1
(cid:11)
ln (k − n0)
k−ni−1
k−1
k
k
k
k
(cid:13)
(cid:14)
k,n(x) is the ﬁrst derivative of Ωk,n(x) with respect
where Ω(cid:2)
to its variable x.
Proof: The average degree of IS distribution ρ(.) is ln k
k,0(1) ≈
(see Section II). Therefore, it is easy to see that Ω(cid:2)
k
k−1 ln k, since for n = 0 no shift occurs in IS distribution
and degree-one check nodes are not generated. Generalization
for Ω(cid:2)
k,n(1) is straightforward. Considering that the average
degree of IS degree distribution for a data-block length of k−n
is ln (k − n) and the shift of degree distribution increases the
average degree by a factor of
k
k−n , (4) is obtained.
A. Generating f b1
Obviously, the decoder is not always aware of n unless
its knowledge about n is updated by a f b1. Initially, the
encoder assumes n = 0 and employs the degree distribu-
tion Ωk,0(.) to generate output symbols. Let nr denote the
most recent reported value of n employing a f b1. Similar
k,nr (1) ≥ √
to [6], we propose the encoder to generate a f b1 when
k,n(1) − Ω(cid:2)
√
ln k, i.e., average degree of Ωk,n(.)
Ω(cid:2)
ln k. Let ni be the threshold that for
increases by at least
n ≥ ni the ith f b1 is generated. In the following lemma we
give the expression for ni.
Lemma 2: In LT-AF codes with data-block length k, ni
the threshold of n for which ith f b1 is issued is recursively
obtained as follows.
W−1 (−Ai(k))
k +
√
ln k + k
Ai(k)
(cid:12)
, i > 0,
ln (k − ni−1)
(5)
k
k
k−1
where Ai(k) = 1
, n0 =
0, and Wm(.) is the mth root of Lambert W-Function (the
Lambert W-Function is deﬁned as the inverse function of
f (x) = xex [13]).
k−ni−1