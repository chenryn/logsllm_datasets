title:Haetae: Scaling the Performance of Network Intrusion Detection with
Many-Core Processors
author:Jaehyun Nam and
Muhammad Jamshed and
Byungkwon Choi and
Dongsu Han and
KyoungSoo Park
Haetae: Scaling the Performance of Network
Intrusion Detection with Many-Core Processors
Jaehyun Nam1(B), Muhammad Jamshed2, Byungkwon Choi2, Dongsu Han2,
and KyoungSoo Park2
1 School of Computing, KAIST, Daejeon, South Korea
2 Department of Electrical Engineering, KAIST,
{namjh,ajamshed,cbkbrad,dongsu han,kyoungsoo}@kaist.ac.kr
Daejeon, South Korea
Abstract. In this paper, we present the design and implementation of
Haetae, a high-performance Suricata-based NIDS on many-core proces-
sors (MCPs). Haetae achieves high performance with three design choices.
First, Haetae extensively exploits high parallelism by launching NIDS
engines that independently analyze the incoming ﬂows at high speed as
much as possible. Second, Haetae fully leverages programmable network
interface cards to oﬄoad common packet processing tasks from regular
cores. Also, Haetae minimizes redundant memory access by maintaining
the packet metadata structure as small as possible. Third, Haetae dynam-
ically oﬄoads ﬂows to the host-side CPU when the system experiences a
high load. This dynamic ﬂow oﬄoading utilizes all processing power on
a given system regardless of processor types. Our evaluation shows that
Haetae achieves up to 79.3 Gbps for synthetic traﬃc or 48.5 Gbps for real
packet traces. Our system outperforms the best-known GPU-based NIDS
by 2.4 times and the best-performing MCP-based system by 1.7 times. In
addition, Haetae is 5.8 times more power eﬃcient than the state-of-the-art
GPU-based NIDS.
Keywords: Many-core processor · Network intrusion detection system ·
Parallelism · Oﬄoading
1 Introduction
High-performance network intrusion detection systems (NIDSes) are gaining
more popularity as network bandwidth is rapidly increasing. As traditional
perimeter defense, NIDSes oversee all the network activity on a given net-
work, and alarm the network administrators if suspicious intrusion attempts
are detected. As the edge network bandwidth of large enterprises and campuses
expands to 10+ Gbps over time, the demand for high-throughput intrusion detec-
tion keeps on increasing. In fact, NIDSes are often deployed at traﬃc aggrega-
tion points, such as cellular core network gateways or near large ISP’s access
networks, whose aggregate bandwidth easily exceeds a multiple of 10 Gbps.
Many existing NIDSes adopt customized FPGA/ASIC hardware to meet
the high performance requirements [4,13]. While these systems oﬀer monitoring
c(cid:2) Springer International Publishing Switzerland 2015
H. Bos et al. (Eds.): RAID 2015, LNCS 9404, pp. 89–110, 2015.
DOI: 10.1007/978-3-319-26362-5 5
90
J. Nam et al.
throughputs of 10+ Gbps, it is often very challenging to conﬁgure and adapt such
systems to varying network conditions. For example, moving an FPGA appli-
cation to a new device requires non-trivial modiﬁcation of the hardware logic
even if we retain the same application semantics [25]. In addition, specialized
hardware often entails high costs and a long development cycle.
On the other hand, commodity computing hardware, such as multi-core
processors [3,15] and many-core GPU devices [2,9], oﬀers high ﬂexibility and low
cost because of its mass production advantage. In addition, recent GPU-based
NIDSes [23,34] enable high performance, comparable to that of hardware-based
approaches. However, adopting GPUs leads to a few undesirable constraints.
First, it is diﬃcult to program GPU to extract the peak performance. Since GPU
operates in a single-instruction-multiple-data (SIMD) fashion, the peak perfor-
mance is obtained only when all computing elements follow the same instruction
stream. Satisfying this constraint is very challenging and often limits the GPU
applicability to relatively simple tasks. Second, large number of GPU cores con-
sume a signiﬁcant amount of power. Even with recent power optimization, GPUs
still use a signiﬁcant portion of the overall system power. Finally, discrete GPUs
incur high latency since packets (and their metadata) need to be copied to GPU
memory across the PCIe interface for analysis. These extra PCIe transactions
often exacerbate the lack of CPU-side memory bandwidth, which degrades the
performance of other NIDS tasks.
Recent development of system-on-chip many-core processors [8,16] has brid-
ged the technology gap between hardware- and software-based systems. The
processors typically employ tens to hundreds of processing cores, allowing highly-
ﬂexible general-purpose computation at a low power budget without the SIMD
constraint. For example, EZchip TILE-Gx72 [16], the platform that we employ
in this paper, has 72 processing cores where each core runs at 1 GHz but con-
sumes only 1.3 Watts even at full speed (95 watts in total). With massively
parallel computation capacity, a TILE platform could signiﬁcantly upgrade the
performance of NIDS.
In this paper, we explore the high-performance NIDS design space on a TILE
platform. Our guiding design principle is to balance the load across many cores
for high parallelism while taking advantage of the underlying hardware to min-
imize the per-packet overhead. Under this principle, we design and implement
Haetae, our high-performance NIDS on TILE-Gx72, with the following design
choices. First, we run a full NIDS engine independently on each core for high
performance scalability. Unlike the existing approach that adopts the pipelining
architecture [24], our system removes all the inter-core dependency and mini-
mizes CPU cycle wastes on inter-core communication. Second, we leverage the
programmable network interface cards (NICs) to oﬄoad per-packet metadata
operations from regular processing cores. We also minimize the size of packet
metadata to eliminate redundant memory access. This results in signiﬁcant sav-
ings in processing cycles. Finally, Haetae dynamically oﬄoads the network ﬂows
to host-side CPU for analysis when the system experiences a high load. We ﬁnd
that the host oﬄoading greatly improves the performance by exploiting available
computing cycles of diﬀerent processor types.
Haetae: Scaling the Performance of Network Intrusion Detection
91
We implement Haetae by extending open-source Suricata [14] optimized for
TILE-Gx72 processors. Our evaluation shows that Haetae achieves 79.3 Gbps
for large synthetic packets, a factor of 1.7 improvement over the MCP-based
Suricata. Our system outperforms Kargus [23], the best-known GPU-based
NIDS, by a factor of 2.4 with 2,435 HTTP rules given by Snort 2.9.2.1 [29]
that Kargus used. With real traﬃc traces, the performance of Haetae reaches
48.5 Gbps, which is 1.9 times higher throughput than that of the state-of-the-art
GPU-based NIDS. In terms of power eﬃciency, Haetae consumes 5.8 times less
power than the GPU-based NIDS.
While we focus on the development of Haetae on TILE-Gx72 in this paper,
we believe that our design principles can be easily ported to other programmable
NICs and many-core processors as well.
2 Background
In this section, we provide a brief overview of many-core processors using EZchip
TILE-Gx72 as a reference processor. We then describe the operation of a typical
signature-based NIDS.
2.1 Overview of EZchip TILE-Gx
Figure 1 shows the architecture of the EZchip TILE-Gx72 processor with
72 processing cores (called tiles in the TILE architecture). Each tile consists
of a 64-bit, 5-stage very-long-instruction-word (VLIW) pipeline with 64 regis-
ters, 32 KB L1 instruction and data caches, and a 256 KB L2 set-associative
cache. TILE-Gx72 does not provide a local L3 cache, but the collection of all L2
caches serves as a distributed L3 cache, resulting in a shared L3 cache of 18 MB.
Fast L3 cache access is realized by a high-speed mesh network (called iMesh),
DDR3 controller
P
F
S
s
p
b
G
0
1
/
1
t
h
g
i
E
2
E
P
I
P
m
1
E
P
I
P
m
DDR3 controller
MiCA
UART,
USB, JTAG,
I2C,SPI
I
O
R
T
0
.
2
e
I
C
P
Flexible
I/O
DDR3 controller
MiCA
DDR3 controller
64-bit Processor
Register File
3 Execution
pipeline
Cache
L1-I Cache
L1-D Cache
I-TLB
D-TLB
L2-D Cache
Terabit
Switch
Fig. 1. Overall architecture of TILE-Gx72 processor
92
J. Nam et al.
which provides lossless routing of data and ensures cache coherency among dif-
ferent tiles. The power eﬃciency comes from relatively low clock speed (1 to 1.2
GHz), while a large number of tiles provide ample computation cycles.
The TILE-Gx72 processor contains special hardware modules for network
and PCIe interfaces as well. mPIPE is a programmable packet I/O engine that
consists of ten 16-bit general-purpose processors dedicated for packet processing.
mPIPE acts as a programmable NIC by directly interacting with the Ethernet
hardware with a small set of API written in C. mPIPE is capable of performing
packet I/O at line speed (up to 80 Gbps), and its API allows to perform direct
memory access (DMA) transactions of packets into the tile memory, inspect
packet contents, and perform load-balancing. The primary goal of the mPIPE
module is to evenly distribute incoming packets to tiles. Its packet processors
help parse packet headers and balance the traﬃc load across all tiles: a feature
that closely resembles the receive-side scaling (RSS) algorithm available in mod-
ern NICs. The mPIPE processors can be programmed to check the 5-tuples of
each packet header (i.e., source and destination IP addresses, source and des-
tination ports, and protocol ID) and to consistently redirect the packets of the
same TCP connection to the same tile.
Besides the mPIPE module, the TILE-Gx72 processor also has the TRIO
hardware module, which performs bidirectional PCIe transactions with the host
system over an 8-lane PCIev2 interface. The TRIO module maps its memory
region to the host side after which it handles DMA data transfers and buﬀer
management tasks between the tile and host memory. TRIO is typically used by
the host system to manage applications running in a TILE platform. Since the
TILE platform does not have direct access to block storage devices, some TILE
applications also use TRIO to access host-side storage using FUSE. In this work,
we extend the stock TRIO module to oﬄoad ﬂow analyzing tasks to the host
machine for Haetae.
The TILE processors are commonly employed as PCIe-based co-processors.
TILEncore-Gx72 is a PCIe device that has the TILE-Gx72 processor and eight
10 GbE interfaces [5], and we call it TILE platform (or simply TILE-Gx72) in
this paper.
2.2 Overview of the Suricata NIDS
We use a TILE-optimized version of Suricata v1.4.0 [14] provided by EZchip. We
refer to it as baseline Suricata (or simply Suricata) in this paper. Baseline Suri-
cata uses a stacked multi-threaded model where each thread is aﬃnitized to a
tile, and it runs a mostly independent NIDS engine except for ﬂow table manage-
ment and TRIO-based communication. It follows a semi-pipelining architecture
where a portion of NIDS tasks are split across multiple tiles. The incoming traf-
ﬁc is distributed to the tiles, and each tile has the ownership of its share of the
traﬃc. In this work, we extend baseline Suricata to support the design choices
we make for high NIDS performance.
Haetae: Scaling the Performance of Network Intrusion Detection
93
Incoming packets to Suricata go through the following ﬁve NIDS modules.
1. The receive module reads packets through packet I/O engines. In com-
modity desktop and server machines, such packet I/O engines may include
PF RING [11], PSIO [20], and DPDK [7]. Haetae, on the other hand, uses
EZchip’s mPIPE module for network I/O communication. After receiving a
batch of packets from the mPIPE module, the NIDS allocates memory for
each ingress packet and initializes the corresponding packet data structure.
2. The decode module parses packet headers and ﬁlls the relevant packet sub-
structures with protocol-speciﬁc metadata. As a last step, it registers the
incoming packets with the corresponding ﬂows.
3. The stream module handles IP defragmentation and TCP segment reassem-
bly. It also monitors IP-fragmented and TCP-segmented evasion attacks as
mentioned in [21].
4. The detect module inspects the packet contents against attack signatures
(also known as rules). This phase performs deep packet inspection by scan-
ning each byte in the packet payloads. It ﬁrst checks if a packet contains
possible attack strings (e.g., multi-string matching) and if so, more rigorous
regular expression matching is performed to conﬁrm an intrusion attempt.
This two-stage pattern matching allows eﬃcient content scanning by avoid-
ing regular expression matching on the innocent traﬃc.
5. Finally, the output module logs the detection of possible intrusions based
on the information from the matched signatures.
3 Approach to High Performance
In this section, we identify the performance bottlenecks of baseline Suricata on
the TILE platform and describe our basic approach to addressing them.
3.1 Performance Bottlenecks of Suricata
A typical performance bottleneck of a signature-based NIDS is its pattern match-
ing. However, for TILE-Gx72, we ﬁnd that parallel execution of pattern matching
may provide enough performance while per-packet overhead related to metadata
processing takes up a large fraction of processing cycles.
To demonstrate this, we measure the performance of a multi-pattern match-
ing (MPM) algorithm (Aho-Corasick algorithm [17], which is the de-facto multi-
string matching scheme adopted by many software-based NIDSes [14,23,29,34]).
Figure 2(a) shows the performance of the MPM algorithm on the TILE platform
without packet I/O and its related NIDS tasks. For the experiment, we feed in
newly-created 1514B TCP packets with random payloads from the memory to
the pattern matching module with 2,435 HTTP rules from the Snort 2.9.2.1 rule-
set. We observe that the performance scales up linearly as the number of cores
grows, peaking at 86.1 Gbps with 70 cores. The pattern matching performance
is reasonable for TILE-Gx72 that has eight 10G network interfaces.
94
J. Nam et al.
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
100
90
80
70
60
50
40
30
20
10
0
Aho-Corasick with internal input
All modules with external input
86.1 
45.1 
60
70
1.2 
0.9 
1
10
30
20
50
Number of cores