reconcile the unified policy intents of different administrators of
SDMZ sites. SDMZs have no fine-grained flow management, i.e.,
filtering, steering or revoking of flows according to dynamic project
requirements or security states of the SDMZ network. In addition,
SDMZs do not offer the necessary context to enable an association
between flows, projects, and data. Below, we identify four key
requirements for an SDMZ security framework.
Fine-grained Dataflow Security Policies: SDMZ policy
2.1.1
requirements differ from that of typical enterprise networks. Below,
we summarize three broad classes of policies that are germane to
the SDMZ network infrastructure:
Policy 1. Dataflow Policies: The nature of SDMZ experiments in-
volve the transport and computation of project datasets with diverse
sensitivities. Ideally, they should be data centric rather than IP flow
2
Site1 SDMZ Network	Policy Framework	Site2 SDMZ Network	SDMZ Core	Project-specific Abstractions	Project-specific Enforcement rules	Lack of Isolated Abstractions and 	Unified Policy Specification among Projects within and across Sites	Lack of Isolation in Policy Enforcement among Projects within the Shared Infrastructure	Project1: ALLOW DTN1 -> Internet	Project2:  DENY DTN1 -> Internet	Project1: Host  & Network specifics 	Project2: Host & Network specifics 	Site 1 LAN	LAN Edge Firewall	Site 2 LAN	LAN Edge Firewall	Elephant Flows	Firewall Bypass Architecture	centric. For example, a single experiment may include both public
data and data involving personal information. Policies should have
the ability to express fine-grained controls over where data can be
transmitted or received based on the type and sensitivities of the data.
Examples: (Ex. 1) - Sensitive data derived from experiments of
project P1 in site S1 is to be only shared among nodes running P1.
If projects P1 and P2 are co-resident in an SDMZ node, P2 users
or applications may not exfiltrate P1 data to other nodes. (Ex. 2)
- Application binaries that are not white-listed are not allowed to
access sensitive files or send packets of size greater than X bytes
using protocols such as DNS and NTP. (Ex. 3) - Sensitive data derived
from project P1{experiment2} in Site1 (e.g., D2) is not to be shared
with Site2, Site3, their collaborators and blacklisted countries,
including the transformed output data (i.e., derivatives D2/*). Also,
not allowed to be accessed by any application or user which/who
collaborates with Site2 and Site3 i.e., S1{D2/*} !→ {S2,S3}).
Policy 2. Temporal and Spatial Policies. As the SDMZ is a federation
of shared and independently managed resources, the operator
should be capable of defining resource utilization policies based on
time, network address space, or geography.
Examples: (Ex. 1) Sensitive and confidential science data produced
by project P1 is not allowed to be accessed or transmitted before
9 AM and after 6 PM, i.e., in the absence of data administrators, to
prevent malicious data access. (Ex. 2) Export-controlled scientific
data derived from project P2 is not to be transmitted to any
ITAR-restricted countries.
Policy 3. Network Security Policies. Policies should be adaptable to
address the dynamic security state of the network.
Example: Notify admin and quarantine hosts to prevent any
sensitive data transfers outside DTNs if there is evidence of a
successful brute-force attack.
Infrastructure Abstraction & Policy Specification: As illus-
2.1.2
trated in Figure 2, existing policy frameworks [2, 19, 36] do not
provide the required isolation between project users and site
administrators. The infrastructure details, abstracted for specifying
the policies, need to be effectively isolated for protecting the network
infrastructure details from getting exposed to other unintended
SDMZ users. The SDMZ network infrastructure involves users (i.e.,
researchers who are non-administrative users) who should identify
the necessary network resources and security services required for
enforcing the data-specific policies of their projects. These policy
rules are manually inserted (and often statically configured) into
routers and monitoring devices.
2.1.3 Conflict-free Policy Enforcement: Furthermore, the policies
must provide necessary project-level isolation while enforcing the
policies on sites where: (i) each project spans across multiple sites,
and (ii) multiple projects share the same host DTN and network
infrastructure. Conflicting access control policies ‘or’ QoS policies (i.e.,
to share the network resources) involving multi-project traffic from a
shared DTN should be effectively de-conflicted before insertion onto
enforcement devices. While traditional enforcement mechanisms
require dedicated DTNs and network infrastructure elements per
project,thisrestrictionisinefficientandimpedestheabilitytodynam-
ically manage SDMZ networks. Consider for example following two
policies from two different projects Project 1 and 2 as shown below:
Figure 3: Lack of context in detecting missed network and
application-level attacks with clustered IDS.
Project 1: HostDTN[1-10]: GridFTP → ALLOW → Internet.
Project 2: HostDTN[3-7]: GridFTP → DENY → Internet.
From above two policies specified in Site1, the first policy from
Project 1 allows FTP application data to be sent to the Internet from
hosts 1 – 10, while the same type of traffic from a subset of hosts
are DENIED as part of Project 2.
2.1.4 Contextual Awareness: Consider the case of SDMZ data-
transfer applications [10] (e.g., GridFTP, bbftp, bbcp), which are
multi-point, and multi-stream applications where a single dataflow
can be transferred in parallel as multiple data streams on to multiple
data nodes. Consolidating or correlating the distributed, parallel TCP
streams (i.e., either clear or opaque traffic) is difficult as the TCP port
numbers used in the data transfer is dynamically negotiated using
GridFTP’s secure control messages. As shown in Figure 3, various
attacks such as network- or application-level DDoS, data exfiltration
andbrute-forceattackscouldgoundetectedwithaclusteredIDSsolu-
tion. To dynamically allow experimental data from various sites to be
properly filtered and steered according to security conditions, each
site which originates the data should provide additional contextual
information. When tier0/1 DoE sites with advanced security services
detect security vulnerabilities they should share these details with
the site that originates the data for collaborative protection.
3 The CoordiNetZ System Framework
We introduce a coordinated and context-aware security framework
(CoordiNetZ) that is designed to address the SDMZ requirements
for enforcing security to dataflows in multi-tenant, multi-project,
and multi-administrative environments. The key elements of the
CoordiNetZ system include:
1) Tree-based Infrastructure Abstraction Engine: employs abstraction-
mappings, which automatically generate isolated tree-based abstrac-
tions (i.e., required to specify policies) of the infrastructure that is spe-
cific to each administrator’s or user’s role and scope of control (§4.1).
2) Graph-based policy specification: allows specification of graph-
based policies with simple drag-and-drop syntax of nodes from
abstraction trees supplied to each administrator (§4.2).
3) Conflict Detection & Resolution: facilitates composition technique
for conflict detection and resolution among policies that are
produced by different project and site administrators (§4.3).
4) Inter-Site & Intra-Site Context-Aware Tagging: associates policies
with context-aware tagging2, which is required for dynamically
2Necessary context required for enforcing security to dataflows is provided by host
DTNs and other protection mechanisms (e.g., IDS) deployed in the SDMZ.
3
(cid:23)(cid:33)(cid:29)F (cid:5)(cid:4)(cid:6)(cid:1)(cid:22)(cid:13)16(cid:1)(cid:22)(cid:33)H(cid:29)(cid:16)D(cid:41)H(cid:1)(cid:13)(cid:23)2(cid:41)(cid:22)(cid:33)H(cid:29)(cid:6)/(cid:13)(cid:22)(cid:6)/(cid:13)(cid:22)(cid:7)/(cid:13)(cid:22)2(cid:23)F7:(cid:33)H(cid:33)DC7(cid:35)(cid:1)DF(cid:1)(cid:22)(cid:13)2 (cid:26)7(cid:41)(cid:29):(cid:1)0D7:(cid:1)(cid:11)7(cid:35)7C(cid:27)(cid:29)F(cid:16)D(cid:41)H(cid:1)(cid:13)(cid:23)2(cid:41)(cid:22)(cid:33)H(cid:29)(cid:7)-7(cid:35)(cid:41)(cid:29)(cid:1)2(cid:29)(cid:31)7H(cid:33)(cid:44)(cid:29)(cid:10)(cid:1)07(cid:27)(cid:34)(cid:1)D(cid:30)(cid:1)(cid:12)DCH(cid:29)LH(cid:1)HD(cid:1)/(cid:13)(cid:22)(cid:1)(cid:30)DF(cid:1):(cid:29)H(cid:29)(cid:27)H(cid:33)DC(cid:1)D(cid:30)(cid:1)2(cid:29)H(cid:45)DF(cid:34)(cid:1)(cid:13)(cid:13)D(cid:22)(cid:2)(cid:1)2(cid:29)H(cid:45)DF(cid:34)(cid:1)(cid:22)(cid:27)7C(cid:41)(cid:2)(cid:1)(cid:11)FIH(cid:29) (cid:30)DF(cid:27)(cid:29)(cid:1)7C:(cid:1)(cid:13)7H7(cid:1),L(cid:30)(cid:33)(cid:35)HF7H(cid:33)DC(cid:1)7HH7(cid:27)(cid:34)(cid:41)(cid:9)(cid:5)(cid:5)(cid:34)EE(cid:41)(cid:9)(cid:5)(cid:5)(cid:34)EE(cid:41)(cid:8)(cid:5)(cid:5)(cid:1)(cid:34)EE(cid:41)(cid:7)1EE(cid:41)1I(cid:35)H(cid:33) ED(cid:33)CH(cid:2)(cid:1)1I(cid:35)H(cid:33) (cid:41)HF(cid:29)7B(cid:1)(cid:29)(cid:35)(cid:29)E(cid:32)7CH(cid:1)(cid:30)(cid:35)D(cid:45)(cid:41)(cid:1)D(cid:44)(cid:29)F(cid:1):MC7B(cid:33)(cid:27)7(cid:35)(cid:35)M(cid:1)(cid:31)(cid:29)C(cid:29)F7H(cid:29):(cid:1)-(cid:23)(cid:21)(cid:1)HF7C(cid:41)EDFH(cid:1)EDFH(cid:41)(cid:1)(cid:5)(cid:5)(cid:18)(cid:8)(cid:1)(cid:2)(cid:1)(cid:5)(cid:9)(cid:22)(cid:9)(cid:1)(cid:4)/(cid:9)(cid:15)(cid:25)(cid:21)(cid:13)(cid:21)(cid:1)(cid:8)(cid:11)2(cid:23)(cid:13)(cid:10)(cid:11)(cid:1)(cid:12)(cid:18)2(cid:1)(cid:7)2(cid:18)(cid:14)(cid:11)(cid:10)(cid:22)(cid:1) (cid:1)(cid:6)(cid:24)(cid:19)(cid:11)2(cid:13)(cid:16)(cid:11)/(cid:22)(cid:1)(cid:5)(cid:9)(cid:22)(cid:9)that dictate the user’s, application’s, or process’ ability to access
the data and send it over the network.
4) SciFlow: The SciFlow module, runs as a daemon to continuously
monitor for flows generated from a specific interface inside the host
and triggers SciMon to gather user and process attributes, file I/O,
and application binary information associated with this network
flow. Flow records gathered by SciMon and SciFlow are sent to the
CNZ Controller for further processing (see Figure 17 in Appendix
A.3). The fields that are extracted from the host and network flows
are customized per CNZ Controller’s policies.
5) Stateless Microservices: Our security-based microservices are
based on the DPDK platform [17]. We implement each security
functional capability as a light-weight stateless microservice
with their states externally stored to shared memory [27]. The
micro-services based functions that we implemented include
tag-based filtering, rate limiting, spoof protection, connection
tracking based on IP tuples and tags.
Figure 4: The CoordiNetZ dataflow policy specification and enforcement architecture with two users specifying policy intents
for securing sensitive data using security services provided by tier 0/1 SDMZ transit sites.
filtering dataflows on the basis of associated security conditions.
We develop technique to allocate tags to policies associated with
each project based on the graph edge coloring approach while
considering the limited tag size supported in IPv6 with 20 bits Flow
Label packet header (§5).
3.1 System Components
Figure 4 illustrates the major components of CoordiNetZ and
their integration points within the SDMZ. Its purpose is to enable
users among a broad range of roles (e.g., project scientists, site
administrators, project data administrators) to express their policies
using a tree-based abstraction, and then enforce these policies on
large data science projects that are hosted across independently
managed SDMZ institutions.
CoordiNetZ integrates following enhancements to SDMZ:
1) CNZ Coordinator: CNZ Coordinator acts as the centralized manager
for specifying cross-site project policies from multiple users, both
through policy files and a graph-based user interface. It implements
the following key capabilities: (a) intent-based framework, (b)
tag-based policy enforcement, (c) manages tag space allocation
mechanism for assigning the range of tags across projects, and (d)
builds abstraction trees based on stats from the CNZ Controller. It
uses Openstack Horizon UI [35] for building graph-based policy
specification. The abstraction engine of CNZ Coordinator built on
Openstack Congress engine [7], with datalog rule generator module
developed to generate infrastructure abstraction trees.
2) CNZ Controller: The CNZ Controller acts as a mediator between
the DTNs and the CNZ Coordinator. It analyzes each host’s DTN
records for malicious data flows and consolidates data-flow records
and DTN state information required for building the abstraction
trees at the CNZ Coordinator. It translates site-specific policies
provided by the CNZ Coordinator into host- and network-specific
rules. Our custom built CNZ Controller handles the data records from
host DTNs and REST APIs are used for exchanging and triggering
the policies between the CNZ Controller and SDN controllers.