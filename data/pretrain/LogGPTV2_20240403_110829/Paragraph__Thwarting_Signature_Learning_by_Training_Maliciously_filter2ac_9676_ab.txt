pool into the target-class samples in such a way as to make it impossible for the
learner to generate a classiﬁer that incurs both few false positives and few false
negatives.
In this work we demonstrate highly effective attacks of both types that assume only a
delusive adversary—one who provides the learner with correctly labeled training data,
but who manipulates the features in the target-class samples to mislead the learner. We
further demonstrate how an adversary with the ability to poison the suspicious pool, the
innocuous pool, or both, can more easily perform inseparability attacks.
Having sketched these strategies broadly, we now turn to describing the attacks based
on them in detail.
Paragraph: Thwarting Signature Learning by Training Maliciously
87
3 Attacks on Conjunction Learners
One way of generating a classiﬁer is to identify a set of features that appears in every
sample of the target class. The classiﬁer then classiﬁes a sample as positive if and only
if it contains every such feature.
We construct two types of red herring attacks against learners of this type. We use
the Polygraph conjunction learner as a concrete example for analysis [15]. In the Poly-
graph conjunction learner, the signature is the set of features that occur in every sample
in the malicious training pool.2 In Section 5 we discuss the effectiveness of these at-
tacks against Hamsa [9], a recently proposed Polygraph-like system. We show that the
attacks described here are highly effective, even under the optimistic assumption that
the malicious training pool contains only target-class samples.
In Section 3.3, we show that even in a highly optimistic scenario, a polymorphic
worm that Polygraph could stop after only .5% of vulnerable hosts are infected can use
these attacks to improve its infection ratio to 33% of vulnerable hosts.
3.1 Attack I: Randomized Red Herring Attack
Attack Description. The learner’s goal is to generate a signature consisting only of
features found in every target-class sample. In the Randomized Red Herring attack,
the attacker includes unnecessary, or spurious, features in some target-class samples,
with the goal of tricking the learner into using those features in its signature. As a
result, target-class samples that do not include the set of spurious features that are in
the signature are able to evade the signature.
The attacker ﬁrst chooses a set of α spurious features. The attacker constructs the
target-class samples such that each one contains a particular spurious feature with prob-
ability p. As a result, the target-class samples in the learner’s malicious pool will all
have some subset of the α spurious features in common, and those spurious features
will appear in the signature. The signature will then have false negatives, because many
target-class samples will not have all of those features.
Analysis. We ﬁrst ﬁnd how selection of αand p affect the expected false negative rate.
Theorem 1. The expected false negative rate F[s] for a signature generated from s
target-class samples, where each target-class sample has probability p of including
each of α spurious features, is F[s] = 1− pαps.
Derivation. The expected number of spurious features that will be included in a signa-
ture after collecting s samples is σ= αps. The chance of all σof those spurious features
being present in any given target-class samples is pσ. Hence, the expected false negative
rate of the signature is y = 1− pσ, which we rewrite as y = 1− pαps.
The attacker has two parameters to choose: the number of spurious features α, and
the probability of a spurious feature occurring in a target-class sample p. The attacker
2 In Section 5, we show that the hierarchical clustering algorithm used by Polygraph to tolerate
noise does not protect against these attacks.
88
J. Newsome, B. Karp, and D. Song
will use as high an α as is practical, often limited only by the number of additional
bytes that the attacker is willing to append.
The ideal value of p is not clear by inspection. A higher p results in more spu-
rious features incorporated into the signature, but it also means that the spurious
features that do get included in the classiﬁer are more likely to occur in other target-
class samples. We ﬁnd the best value of p by ﬁnding the roots of the derivative:
dy
d p = −αpαps+s−1(sln(p) + 1). There are two roots. p = 0 minimizes false negatives
(it is equivalent to not performing the attack at all), and p = e
negatives.
− 1
s maximizes false
Theorem 2. The value of p that maximizes the false negative rate in the Randomized
Red Herring attack is: p = e
− 1
s .
The p that generates the highest false negative rate depends on the number of target-
class samples seen by the learner, s. Hence, the optimal value of p depends on the exact
goals of the attacker. For a worm author, one way to choose a value of p would be to set
a goal for the number of machines to compromise before there is an effective classiﬁer,
and calculate the number of positive samples that the learner is likely to have gathered
by that time, based on the propagation model of his worm and the deployment of the
learner, and then set p to a value that ensures there are still a large number of false
negatives at that time.
y
e
t
a
r
e
v
i
t
a
g
e
n
e
s
a
F
l
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Theory p=.900
Theory p=.995
Theory p=.999
Eval p=.900
Eval p=.995
 200
 400
 600
 800
 1000
Number of training samples s
Fig. 2. Randomized Red Herring attack. α= 400
We implemented a version of the Randomized Red Herring attack based on this
model. We took a real buffer-overﬂow exploit against the ATPhttpd web server [19],
ﬁlled the attack-code with random bytes to simulate polymorphic encryption and ob-
fuscation, and replaced the 800 bytes of padding with 400 unique two-byte spurious
features. Speciﬁcally, we set each two-byte token to the binary representation of its
offset with probability p, and to a random value with probability 1 − p. Note that
Paragraph: Thwarting Signature Learning by Training Maliciously
89
the number of spurious features used here is conservative. In this attack, the 800 padding
bytes were already used, because they were necessary to overﬂow the buffer. The at-
tacker could easily include more bytes to use as spurious features. For example, he
could include additional HTTP headers for the sole purpose of ﬁlling them with spuri-
ous features.
Figure 2 shows the predicted and actual false negative rates as the number of train-
ing samples increases, for several values of p. We used values that maximized the false
negative rate when s = 10 (p = .900), when s = 200 (p = .995), and when s = 500
(p = .999). For each data point, we generate s worm samples, and use the Polygraph
conjunction learner to generate a classiﬁer. We then generate another 1000 worm sam-
ples to measure the false negative rate. There are two things to see in this graph. First,
our experimental results conﬁrm our probability calculations. Second, the attack is quite
devastating. Low values of p result in very high initial false negatives, while high val-
ues of p prevent a low-false-negative signature from being generated until many worm
samples have been collected.
3.2 Attack II: Dropped Red Herring Attack
Attack description. In the Dropped Red Herring attack, the attacker again chooses a
set of αspurious features. Initially, he includes all αfeatures in every target-class sam-
ple. As a result, the target-class samples in the learner’s malicious training pool will
all have all α spurious features, and all α spurious features will be included in the
signature.
Once the signature is in place, all the attacker needs to do to evade the signature is
to stop including one of the spurious features in subsequent target-class samples. The
signature will have a 100% false negative rate until the learner sees a target-class sample
missing the spurious feature, and deploys an updated signature that no longer requires
that feature to be present. At that point, the attacker stops including another spurious
feature. The cycle continues until the attacker has stopped including all of the spurious
features.
Attack analysis. For sake of comparison to the Randomized Red Herring attack, as-
sume that the attacker stops including a single spurious feature the instant that an up-
dated signature is deployed. Also assume that the learner deploys a new signature each
time it collects a new worm sample, since each successive sample will have one fewer
spurious feature than the last. In that case, the classiﬁer will have 100% false negatives
until α positive samples have been collected.
Theorem 3. The false negative rate F[s] for the signature generated after s target-class
samples have been collected is
(cid:2)
F[s] =
100% i f s < α
0% i f s ≥ α
With these assumptions, the Dropped Red Herring attack is compared to the Random-
ized Red Herring attack in Figure 3. When the attack is executed in this way, and there
90
J. Newsome, B. Karp, and D. Song
y
e
t
a
r
e
v
i
t
a
g
e
n
l
e
s
a
F
d
e
t
c
e
p
x
E
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Randomized p=.900
Randomized p=.995
Randomized p=.999
Dropped
 200
 400
 600
 800
 1000
Number of training samples
Fig. 3. Dropped Red Herring compared to Randomized Red Herring, α= 400
are a moderate number of spurious features, the attack can be quite devastating. The
generated signatures are useless until all α features have been eliminated from the sig-
nature.
While the Dropped Red Herring attack is far more effective than the Randomized
Red Herring attack (until the learner has dropped all α spurious features from the sig-
nature), the Randomized Red Herring attack has one important advantage: it is simpler
to implement. The Dropped Red Herring attack must interact with the signature learning
system, in that it must discover when a signature that matches the current target-class
samples has been published, so that it can drop another feature, and remain unﬁltered.
There is no such requirement of the Randomized Red Herring attack. This is not to say
that the Dropped Red Herring attack is impractical; the attacker has signiﬁcant room
for error. While dropping a feature prematurely will ‘waste’ a spurious feature, there is
little or no penalty for dropping a feature some time after an updated signature has been
deployed.
3.3 Attack Effectiveness
We show that even with an optimistic model of a distributed signature generation sys-
tem, and a pessimistic model of a worm, employing the Randomized Red Herring or
Dropped Red Herring attack delays the learner enough to infect a large fraction of vul-
nerable hosts before an accurate signature can be generated.
We assume that the learner is monitoring L addresses. Each time the worm scans
one of these addresses, the learner correctly identiﬁes it as a worm, and instantaneously
updates and distributes the signature. At that point, any scan of any vulnerable host
has probability F[s] of succeeding (the false negative rate of the current signature).
There are several optimistic assumptions for the learner here, most notably that up-
dated signatures are distributed instantaneously. In reality, distributing even a single
Paragraph: Thwarting Signature Learning by Training Maliciously
91
 1e+06
 900000
 800000
 700000
 600000
 500000
 400000
 300000
 200000
 100000
 0
No defense
Dropped RH
Randomized RH (p=.999)
Randomized RH (p=.995)
Randomized RH (p=.900)
Maximally Varying Polymorphic
 0
 1000  2000  3000  4000  5000  6000  7000  8000
Worm samples seen by learner (s)
d