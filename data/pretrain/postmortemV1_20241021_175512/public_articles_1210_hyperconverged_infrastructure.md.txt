Hyperconverged Infrastructure
October 2017
HPE has recently released a Dummies book describing its hyperconverged
infrastructure. Hyperconverged Infrastructure for Dummies can be found at
https://www.hpe.com/us/en/resources/integrated-systems/hyperconverged-infrastructure-
dummies.html?pp=false&jumpid=sc_fhmt17uanr_aid-510318809
The book describes HPE’s hyperconverged infrastructure product, SimpliVity. Hyperconverged
infrastructure combines compute, storage, networking, and data services into a single physical system.
The software that enables hyperconvergence runs on standard x86 systems. Its distributed architecture
lets you cluster multiple systems within and between sites that can be managed through a single
interface.
To understand hyperconvergence, we will discuss virtualization, the software-defined data center, and
cloud computing.
Virtualization
Today, most services running in a data center run in a virtual environment. Applications run as virtual
machines, many of which are hosted on a single physical server. Administrators consider the virtual
environment for running new applications rather than building a new physical environment.
Virtualization helps organizations consolidate many of their servers to run on a common platform on top
of a hypervisor, which allocates resources of the physical server to the virtual machines (VMs). Before
virtualization, it was not uncommon to find a data center populated with hundreds of physical servers
running at 15% capacity. As a result of virtualization, organizations enjoy a much higher return on their
investment.
IT departments used to be required to maintain separate groups of people to manage separate hardware
resources – servers, storage, networking. Furthermore, different workloads created resource challenges
that pushed them to develop infrastructure environments on a per-server basis. Virtual desktops, for
instance, have vastly different resource usage patterns than server virtualization projects. Even worse,
the devices often required separate management consoles.
Virtualization is heavily dependent upon storage. Many VMs are running on a single host and accessing
storage for their own needs. The database system has to jump all over the disk to service the combined
load of the VMs. Continued consolidation of VMs contributes to random I/O workloads, each with its own
pattern for reading and writing to storage. Highly random I/O streams adversely affect overall
performance as VMs contend for disk resources.
1
© 2017 Sombers Associates, Inc., and W. H. Highleyman

Virtual desktops are a particular problem. Sometimes they only need ten or twenty input/output operations
per second (IOPS). However, when they are being brought up, IOPS can skyrocket due to the boot
storms and login storms. This typically occurs at the beginning of the day.
The best outcome in any environment is to eliminate writes to disk before they ever happen. In a
hyperconverged environment, many operations do not have to touch disk because of caching in RAM.
In the modern data center, disk capacity is not an issue. Capacity has become plentiful as vendors
release bigger drives. However, performance has barely improved. With the addition of disaster recovery,
the demand for WAN bandwidth has increased. Given this reality, the data center infrastructure needs to
optimize for performance and latency, not capacity and throughput.
Inline deduplication provides the level of efficiency needed. It consists of only reading the data, applying
deduplication, and writing the data as it is being transferred to another destination.
In a hyperconverged environment, backup and replication are applied directly to individual applications (or
VMs).
The Software-Defined Data Center
A software-defined data center (SDDC) employs a high degree of virtualization. Storage, servers, and
even WANs are virtualized. This eliminates resources that are traditionally locked within a single-purpose
device and creates a shared-resource pool for applications. Instead, the SDDC uses commodity x86
hardware. Virtualization abstracts the hardware components of the data center and overlays them with a
common software layer.
There are many advantages to an SDDC:
• Predictability – Services operate in a predictable way at a predictable cost.
• Scalability – The data center can’t be a limiting factor when expansion becomes necessary.
• Utilization – Because a hyperconverged data center is built on common components, high
utilization rates are easy to achieve.
• Personnel – A company can operate a data center with fewer personnel because there are no
traditional resource islands.
• Provisioning – An SDDC offers agility and flexibility, which reduces provisioning times for new
services.
Resource islands are inherently inefficient. The broader the IT environment, the easier it is to achieve
operational economies of scale. Don’t think about each individual resource as its own island. Instead,
focus on the overall scale of all resources.
IT staff wants to lower the risk in their operations. Applications must be highly available, and data must be
safe. As more diverse hardware is installed, achieving these goals becomes more difficult. Companies
can reduce these risks by adopting a hyperconverged infrastructure. Then It can quickly and easily deploy
new applications and services in response to business demands.
How the Cloud Is Changing IT
Major cloud service providers are changing expectations of how a data center should operate. The best
architectural design elements from clouds have been brought to the hyperconverged world and packaged
for affordability.
2
© 2017 Sombers Associates, Inc., and W. H. Highleyman

Major clouds are based on commodity hardware. It is the software in a hyperconverged environment that
provides the services. This includes recovering from commodity hardware failures. Scalability is easily
obtained by simply adding more commodity resources under the umbrella of the software. Thus,
scalability can be achieved in small bite-sized pieces.
Hyperconvergence brings cloud-type consumption to IT without compromising performance, reliability, or
availability. Rather than making huge buys every few years, IT simply adds small building blocks of
infrastructure to the data center as needed.
Converged Infrastructure
Convergence and SDDCs are aimed at reducing the infrastructure clutter, complexity, and cost of data
centers. Converged infrastructure products combine the server and storage components in a single box.
They provide a single localized resource pool, offering simplified management and faster time to
deployment.
However, converged infrastructure has some limitations. Resource ratios, such as CPU:storage:network,
are fixed, making them less flexible than some organizations require. The products cannot always be
used by existing infrastructure.
Hyperconverged Infrastructure
Hyperconvergence delivers simplification and savings by consolidating all required functionality into a
single infrastructure stack running on commodity x86 servers. Hyperconvergence brings many features
that make legacy services obsolete in some IT environments:
• Data protection via backup and replication.
• Deduplication.
• Wide-area network (WAN) optimization.
• Solid-state drive arrays.
• Public cloud gateways.
Advantages of a Hyperconverged Infrastructure
Hyperconvergence brings several advantages to an application:
• Software Focus - Since hyperconvergence is software based, it provides the flexibility required to
meet current and future business needs without having to rip and replace infrastructure
components.
• Centralized System Management – Since all components are combined in a single shared
resource pool, IT can manage resources across individual nodes as a single federated system.
• Enhanced Agility – All resources in all physical data centers reside under a single administrative
umbrella. Therefore, workload migration is quite straightforward.
• Scalability and Efficiency – Hyperconvergence is a scalable building-block approach that allows
IT to expand by adding units in small step sizes.
3
© 2017 Sombers Associates, Inc., and W. H. Highleyman

• Cost-Effective Infrastructure – Hyperconverged systems have a low cost of entry and a low cost
of ownership.
• Easy Automation – All resources are combined under central management tools. IT doesn’t have
to worry about structures from different manufacturers since everything is encapsulated in one
environment.
• Focus on VMs – Hyperconverged systems use VMs as the basic constructs of the environment. It
is easy to move workloads around to different data centers.
• Shared Resources – Hyperconvergence enables an organization to deploy may kinds of
applications in a single shared resource pool. Hyperconvergent systems include many kinds of
storage – from flash to hard disks – in each appliance.
• Data Protection – Backup, recovery, and disaster recovery are built in.
How to Apply Hyperconvergence
Existing infrastructure does not have to be replaced in order for hyperconvergence to be of immediate
value.
• Consolidating servers and data centers – Hyperconvergent products integrate seamlessly with
the existing environment.
• Modernizing technology – The implementation of hyperconvergent is non-disruptive. New
architectures can be phased in while old ones are phased out.
• Deploying new tier-1 applications – Deploy new workloads in a hyperconverged environment to
gain its inherent operational benefits.
• Deploying VDI – Deploy virtual desktop infrastructure (VDI) in a hyperconverged infrastructure.
• Managing sites remotely – In a hyperconverged environment, the entire infrastructure from local
to remote resources is controlled by a single management system.
• Performing testing and development – Create a test and development environment so that bad
code isn’t released into production. Hyperconvergence supports test and development with
management tools that create logical separations between these functions and production.
• Modernizing backup and implementing disaster recovery – Hyperconvergence is a simple way to
achieve backup and disaster recovery goals.
HPE SimpliVity
HPE’s SimpliVity hyperconverged infrastructure delivers the
performance, resiliency, and data protection that today’s systems
require. It provides a pay-as-you-go building block approach to IT
infrastructure, workload-centric management, efficient optimization of
capacity and performance, and built-in data protection and resiliency.
HPE SimpliVity delivers all infrastructure and data services for
virtualized workloads on a pair of its Proliant servers. HPE SimpliVity provides a hyperconverged
infrastructure with more agility, efficiency, and resiliency at less cost and complexity. For more
information, go to www.hpe.com/info/hc.
4
© 2017 Sombers Associates, Inc., and W. H. Highleyman
