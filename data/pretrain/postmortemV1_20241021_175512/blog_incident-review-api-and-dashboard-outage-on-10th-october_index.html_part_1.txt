# Incident review: API and Dashboard outage on 10 October 2017 

[Written by Chris Sinjakli

This post represents the collective work of our Core Infrastructure
team\'s investigation into our [API and Dashboard outage] on 10 October 2017.

As a payments company, we take reliability very seriously. We hope that
the transparency in technical write-ups like this reflects that.

We have included a high-level summary of the incident, and a more
detailed technical breakdown of what happened, our investigation, and
changes we\'ve made since.

## Summary 

On the afternoon of 10 October 2017, we experienced an outage of our API
and Dashboard, lasting 1 hour and 50 minutes. Any requests made during
that time failed, and returned an error.

The cause of the incident was a hardware failure on our primary database
node, combined with unusual circumstances that prevented our database
cluster automation from promoting one of the replica database nodes to
act as the new primary.

This failure to promote a new primary database node extended an outage
that would normally last 1 or 2 minutes to one that lasted almost 2
hours.

## Our database setup 

Before we start, it\'s helpful to have a high-level view of how we store
data at GoCardless.

All of our most critical
data[1](https://gocardless.com/blog/incident-review-api-and-dashboard-outage-on-10th-october#fn1)
 is stored in [Postgres](https://www.postgresql.org/).

We run Postgres in a cluster of 3 nodes, with a primary, 1 synchronous
replica and 1 asynchronous replica. This means that we always have at
least 2 copies of every piece of data by the time we respond
successfully to an API request.

To manage the promotion of a new primary node in the event of machine
failure, we run a piece of software
called [Pacemaker](https://clusterlabs.org/pacemaker.html)
 on each node in the cluster. Clients, such as our Ruby
on Rails applications, connect to the primary
node[2](https://gocardless.com/blog/incident-review-api-and-dashboard-outage-on-10th-october#fn2)
 using a virtual IP address (VIP) that is also managed by
Pacemaker.

Put together, it looks a little like this:

![
basic-cluster.png](https://images.ctfassets.net/40w0m41bmydz/7HEMJWmOpHjvl7K94SCzdJ/368054f07ebf59e4657b7ab8ed85652b/postgres-outage-oct-2017_cluster-intro_basic-cluster.png?w=1920&h=1080&q=50&fm=png)

When the primary node fails, the cluster notices.

![
cluster-with-failure.png](https://images.ctfassets.net/40w0m41bmydz/279FOm2RvHYo7Yq1nANAcO/d4beed97a99b1706057065d21fae464e/postgres-outage-oct-2017_cluster-intro_cluster-with-failure.png?w=1920&h=1080&q=50&fm=png)

It promotes the synchronous replica, which is guaranteed to have a copy
of every write (e.g. new payment) that the primary accepted. It also
sets up the old asynchronous replica as the new synchronous replica.

![
cluster-after-promotion.png](https://images.ctfassets.net/40w0m41bmydz/6ImEWdDB236HJSBMwlrgtO/458da2b0396be0ad086b9471df13d264/postgres-outage-oct-2017_cluster-intro_cluster-after-promotion.png?w=1920&h=1080&q=50&fm=png)

Once the VIP is moved across, applications can carry on their work.

![
cluster-after-vip-move.png](https://images.ctfassets.net/40w0m41bmydz/6VGrfu37GLRdES67gFjgbB/0e605a7d7908a1ac7a38489d2e31ec03/postgres-outage-oct-2017_cluster-intro_cluster-after-vip-move.png?w=1920&h=1080&q=50&fm=png)

A Site Reliability Engineer (SRE) then adds a new replica back into the
cluster.

![
fully-healed-cluster.png](https://images.ctfassets.net/40w0m41bmydz/6TiK5XQupNQCiyltn2M5Z3/bb9949cb1c821a912cbbfa82d39805e2/postgres-outage-oct-2017_cluster-intro_fully-healed-cluster.png?w=1920&h=1080&q=50&fm=png)

## Incident timeline 

So, how did all this go wrong?

All times in this section are in British Summer Time (UTC+1).

-   [15:09]: Our monitoring detects the total outage of
    our API and Dashboard; engineers begin to investigate.

-   [15:11]: We see evidence of a disk array failure on
    the primary. We are unsure why the cluster hasn\'t already failed
    over to the synchronous replica.

-   [15:17]: We power off the broken primary Postgres
    node. This is done so that we don\'t have a machine in the cluster
    that is online, but with a broken disk array. We believe that with
    only the synchronous and asynchronous nodes online, the cluster
    software will promote a new primary. It quickly becomes clear that
    this is not the case.

-   [15:18]: We clear the error counts (

    `crm resource cleanup`) in Pacemaker to prompt it to
    rediscover the state of the Postgres instances. Doing so has no
    effect. The cluster will not promote the synchronous replica.

    We spend the next hour trying a variety of approaches to promote a
    new primary.

    Our last few attempts centre around editing the configuration on the
    synchronous replica to try to promote it ourselves. We put the
    Pacemaker cluster into maintenance mode
    (`crm configure property maintenance-mode=true`),
    remove the configuration flag that tells Postgres to be a replica
    from `recovery.conf`, and bring the cluster out of
    maintenance mode. Every time, it brings the replica back with its
    original `recovery.conf`, and we are left with no
    primary.

-   [16:18]: We decide that our attempts at this approach
    have run on for too long, and that we need to try something else. We
    set the cluster into maintenance mode one last time, configure the
    synchronous replica to be a primary, and start Postgres ourselves.

    Since the cluster also manages the VIP, and we\'ve set it into
    maintenance mode, we reconfigure our backend applications to connect
    to the actual IP address of the new primary.

-   [16:46]: Our manually promoted primary Postgres node
    is working, and the configuration changes are being rolled out to
    our applications.

-   [16:59]: Our monitoring systems confirm that our API
    and Dashboard are back up.

## The immediate fallout 

Having brought our systems back online, our next priority was to restore
the database cluster to its usual level of redundancy. This meant
bootstrapping a third node as an asynchronous replica.

Since Pacemaker was still in maintenance mode, there would be no
automatic failover if a machine were to fail now. We believed we were
likely to be running this manually-managed setup for a
while[3](https://gocardless.com/blog/incident-review-api-and-dashboard-outage-on-10th-october#fn3)
, and in the event of our primary failing, we wanted to
be able to promote the synchronous replica as quickly as possible.

We decided to introduce another VIP, to be managed manually by the
infrastructure team. In the event of our primary failing, we would
promote the synchronous replica and move the VIP over ourselves.

As this was an incident triggered by a disk array failure, out of
caution we spent some time verifying the integrity of our data. After
running every test we could think of, we found no evidence of data
corruption.

Once that was done and we felt safe with the GoCardless services running
as they were, we started planning our next steps.

## The following weeks 

The day after the incident, the whole team sat down to discuss two
issues:

-   Why did our Pacemaker cluster fail to elect a new primary database
    node?

-   How do we move back to having Pacemaker managing our cluster, now
    that we\'re in this manually-managed state?

During that discussion, we decided that we\'d taken too much manual
intervention on our existing database cluster to be confident in
bringing back the Pacemaker automation there. We decided that we\'d
provision a new cluster, replicate data into it, and switch traffic
over.

We split up into two subteams - one trying to reproduce the failure, and
the other working out how we\'d move over to a new database cluster with
minimal disruption.

### Reproducing the failure 

To be confident in a new cluster, we needed to understand why the
existing one didn\'t promote a new primary, and make changes to fix that
issue.

For the most part, we did this by analysing the logs of the components
involved in the failure. From that analysis, we pulled out several
factors that looked like they could be relevant to reproducing the
issue:

-   The [RAID](https://en.wikipedia.org/wiki/RAID)
     controller logged the simultaneous loss of 3 disks
    from the array. All subsequent read and write operations against it
    failed.

-   The Linux kernel set the filesystem backed by that controller into
    read-only mode. Given the state of the array, even reads weren\'t
    possible.

-   The Pacemaker cluster correctly observed that Postgres was unhealthy
    on the primary node. It repeatedly attempted to promote a new
    primary, but each time it couldn\'t decide where that primary should
    run.

-   On the synchronous replica - the one that should have become the new
    primary - one of Postgres\'s subprocesses crashed around the time of
    the disk array failure on the primary. When this happens, Postgres
    terminates the rest of its subprocesses and restarts.

-   After that restart, the synchronous replica kept trying to restore
    a [Write-Ahead
    Log](https://www.postgresql.org/docs/current/static/wal-intro.html)
     (WAL) file through
    the `restore_command`. On each attempt, it failed with
    a message stating that the file was invalid (more on this later).

A lot to unpick, right?

Given the complexity involved, it was clear that we\'d only get to an
answer in a reasonable amount of time if we could repeatedly break a
cluster in slightly different ways and see if we could get it to break
in the same way our production cluster did on 10 October.

Fortunately, as part of some unrelated work we\'d done recently, we had
a version of the cluster that we could run inside Docker containers. We
used it to help us build a script that mimicked the failures we saw in
production. Being able to rapidly turn clusters up and down let us
iterate on that script quickly, until we found a combination of events
that broke the cluster in just the right way.

#### A red herring: the invalid WAL file 

One of the log entries that stood out, and was a real cause of concern
for a while, was the synchronous replica failing to restore a WAL file
through its `restore_command`.

A quick bit of background for those not familiar with Postgres: the
Write-Ahead Log is how Postgres records everything you ask it to write
(e.g. `INSERT`, `UPDATE`, `DELETE` queries).
This log provides strong guarantees of those writes not being lost if
Postgres crashes, and is also used to keep replicas in sync with the
primary.

There are two ways the WAL can be used: through streaming replication,
and
through `archive_command` and `restore_command`.

In streaming replication, replicas establish an ongoing connection to
the primary, which sends them any WAL it generates. If you specify that
the replication should be synchronous, it waits for the replica to
confirm that it\'s received the WAL before returning from the query that
generated it.

With `archive_command`, Postgres lets you specify a shell
command that will be executed every time a chunk of WAL is generated on
the primary. It makes the file name available to the command you
specify, so that you can choose what to do with the file.
Similarly, `restore_command` runs on replicas, and passes
you the name of the next WAL file the database expects to replay, so you
can copy it from wherever you archived it to.
