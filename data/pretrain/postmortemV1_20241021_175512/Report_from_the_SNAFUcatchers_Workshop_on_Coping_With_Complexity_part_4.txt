    that an anomaly was in progress and that it was being actively
    managed. Some organizations had detailed, formal process plans for
    these communications.

#### Shared artifacts 

Most anomaly response takes place via individual display screens,
although common workspace promotes locally shared viewing of screens.
There are few public artifacts in these workspaces. When present, these
are usually graphic displays of coarse-grained system activity.

The increasing popularity of distributed operations for 24/7 available
systems as well as work-from-home may have reduced the value of (as well
as investment in) traditional shared artifacts, eg. whiteboards, while
increasing the value of new sharing methods (see Communications above).
In some cases, the metrics dashboards which internal teams use to
monitor system health are also shared externally with end users.

#### The consequences of escalating consequences 

Generally, the longer a disruption lasts, the more damage is done.
Disturbances propagate and cascade; consequences grow. Even when the
initiating event is a full-fledged outage, the weight of that event
increases as subsidiary and related processes react or fail to react.
(The air travel disruptions from IT outages are convenient examples.)
Knowing this, organizations have in place plans for managing the likely
or foreseeable consequences of SNAFUs. For minor SNAFUs these plans
include methods for bringing resources to bear, notification chains to
alert more senior managers and customer relations people, etc. At the
other extreme are business continuity plans for use eg. after
environmental disasters. The extent, scope, and level of detail varies
across organizations.^[2](http://snafucatchers.github.io/#foot_2)^

IT specialists responding to a disruption try to restore functionality
as quickly as possible in order to limit the damage. Their attention is
focused on understanding the disruption and devising and enacting
countermeasures. There is pressure to restore the IT function quickly.

There is also pressure to gauge the scope of the SNAFU, its likely and
possible trajectories, and the risks associated with countermeasures
that might be employed. Significantly, the people best able to make such
assessments are necessarily the ones trying to understand the anomaly.

Although they are linked, these two activities can come into conflict.
If the specialists are allowed to work without being asked for updates
and projections, their efforts can be concentrated on understanding and
fixing the broken system but the rest of the organization is then
hamstrung by the lack of information about how the problem-solving
process is proceeding. On the other hand, if the specialists are
constantly being dunned for explanations and estimates of the time to
repair they are likely to make little progress on understanding the
anomaly, devising countermeasures, etc.

The postmortem discussions revealed that organizations seek ways to
avoid burdening their technical staff with demands for updates and
projections, especially in the early stages of anomaly response. For
example, the postmortem descriptions included comments such as \"\[the
managers\] could see we were busy and stayed away from our
workstations\".

In highly regulated settings the plans for dealing with anomalies
sometimes have quite sharp thresholds or \'edges\' where new
requirements come into play. Here the technical staff may experience
strong pressure to make declarations about the nature of anomalies even
though they do not fully understand what is happening or what it will
take to resume \'normal\' operations. Uncertainty and escalating
consequences combine to turn the operational setting into a pressure
cooker and workshop participants agreed that such situations are
stressful in ways that can promote significant risk taking.

Process control studies, notably of nuclear power plant operations, show
that, at some point in the evolution of an anomaly, escalating
consequences can shift the main focus of work away from trying to
understand and fix the problem towards trying to alleviate its
consequences. Woods has called this the \"shift to disturbance
management\" ([Woods & Hollnagel,
2006](http://snafucatchers.github.io/#ref_43); [Woods,
1994](http://snafucatchers.github.io/#ref_37)). For 24/7 operations this
might mean taking some dramatic action such as moving operations to a
backup facility. Such decisions usually have formalized organizational
definitions (e.g. \"declaring\" a disaster). They are highly charged and
entail exceptional commitments and risks. Because these situations are
so rare and involve transfers during a disruption their success is not
assured. The degree of commitment involved may make retreat from the
decision practically impossible \-- there may be no way to recover if
the transfer is unsuccessful, making the decision an \'all in\'
commitment.

Although they are not usually charged with making such dramatic
decisions, IT staffs understand how passing time and escalating
consequences may lead to decisions that eliminate their own capacity to
resolve the SNAFU and this creates enormous pressure to gain resolution
quickly. It seems likely that what are later regarded as unwise
decisions by IT staff are actually efforts to forestall the escalation
of consequences to the point where the shift to disturbance management
will occur. One rationale for improving the quality of postmortems is to
obtain better insight into the way that escalating consequences increase
the pressure on IT staff and how to better inform their approach to
these difficult situations.

#### Managing risk 

Correcting the technical problem almost always requires specific actions
and these actions entail exposure to risk. Entering console commands,
restarting services, making changes to and deploying code, altering
network settings, and the myriad other changes that may be required to
address the outage all present some risk.

The workshop presentations described the workers\' concern for the risk
they were accepting by making changes to the working system. They
explicitly assessed the risk associated with different approaches and
sought ways to test potential solutions. They recognized that what
seemed to be a fix for one problem might generate additional problems.
Their sense of jeopardy increased with uncertainty about the source of
the problem; the risk of taking action was judged much greater when the
cause of the disruption was unknown or speculative.

#### Goal sacrifice 

In each case the practitioners involved were called on to sacrifice one
or more goals in order to achieve some other goal. Sacrificing lower
level goals for higher level ones is a common theme in managing process
control situations ([Allspaw,
2015](http://snafucatchers.github.io/#ref_2)). Under \'normal\'
operating conditions many goals can be active simultaneously and the
workers need to do little to maintain a balance between competing or
mutually exclusive goals.

During disturbances, however, achieving important (\"high level\") goals
may require abandoning less important (\"low level\") ones. This
sacrifice decision can be controversial if the lower level goal is,
e.g., keeping operational costs down. Sometimes the sacrifice requires
incurring damage, even severe damage, in order to prevent an even
greater catastrophe.^[3](http://snafucatchers.github.io/#foot_3)^

Two things stand out. First, although organizations often purport to
have shared values and common goals, sacrifice is always difficult.
Sacrifices typically take place under high pressure and uncertainty when
the available data is sparse or conflicted. Second, sacrifice decisions
are readily criticized afterwards and, this is ironically the case,
especially when they are successful. A sacrifice that preserves some
desired goal at a high cost may be characterized as a failed decision
because that bad outcome did not occur. Thus, a decision not to open for
business because the technical system is not proven to be in the correct
state sets up a potential criticism argument that the decider was being
too cautious and that the loss experienced was needless.

## 3.5 Observations on the postmortem process 

All the participating firms have established processes for after-event
reviews. In keeping with the current rubric, their term for these is
postmortem. How the postmortem is prepared and conducted varies. Some of
the common features of postmortem processes were identified in the
workshop. A summary of these follows.

Preparation for postmortem is usually the responsibility of one or two
people with experience in conducting postmortems. They gather machine
generated logs and available chat histories and interview key
participants. Depending on the nature of the event they may develop a
simple or elaborate presentation for the postmortem meeting which is
held a few days or few weeks after the event. The postmortem itself is a
meeting that lasts about an hour. The facilitators present the event and
a skeletal timeline using a video projector and presentation software
such as PowerPoint. Attendees are invited to speak and they fill in
technical details, sequential actions, and \"what were we thinking
about\" comments.

The number of people involved in the postmortem varies widely, ranging
from about 5 to over 50. Smaller sessions are attended mostly by
technical staff but larger ones include people affected by the event or
with specific concerns, e.g. managers, administrators, and non-IT
workers. The meeting sometimes generates specific action items related
to the event.

The postmortems are prosaic to the point of being narrow. They
concentrate on technical details and establishing the evolution of the
event from its onset through the immediate recovery. Little attention is
given to contributors other than the existing code and architecture.
Inquiry into broader issues such as the contribution of management
structures, production pressures, staffing levels, the pace of
development and deployment, and distribution of resources is
exceptional.

The conduct of postmortems varies widely, even within a single firm.
More significant events are handled differently than less significant
ones: more time is devoted to the preparation, more control is exercised
over the initial presentation, and the tenor of the discussion is more
restrained and can even be restricted. postmortems for events that
produce large economic losses or engage regulatory bodies are more
scripted, sometimes to the point of being little more than staged events
at which carefully vetted statements are made and discussion of certain
topics is deliberately avoided. The implications of this observation are
discussed more fully in part 4 (Themes).

Postmortems use language similar to that found in other after-accident
investigations: \"root cause\", \"contributor\", \"defense\",
\"trigger\", etc. None of the postmortem processes employed formal
investigative tools, e.g. fishbone diagrams, probabilistic risk
assessment.

In most cases the postmortem findings are recorded as text or hypertext
documents that are labeled, indexed, and categorized for future
reference. The preparation of these is the responsibility of the
facilitators.

There is some pressure to derive specific action items and assign these
to individuals or groups. Facilitators commented that this pressure
sometimes leads to abbreviated presentations of the timeline and limits
the analysis. One firm has divided the postmortem into two meetings,
separated by about 10 days, with the first meeting devoted to timeline
and analysis and the second reserved for identifying, discussing, and
deciding on corrective actions.

Commonly, the action items are transferred to a \'to do\' list that may
be reviewed later.^[4](http://snafucatchers.github.io/#foot_4)^ Access
to the description of the event, the postmortem discussion notes, and
the action item list is typically considered private to some subset of
the people involved and high management although wider distribution is
sometimes available.

Postmortems may be collected and preserved. Access to these collections
is sometimes narrowly limited, sometimes quite broad. The collection
itself is sometimes described as \'the morgue\', a term consistent with
the medical term \'postmortem\' and also an [old newspaper
term](https://en.wikipedia.org/wiki/Morgue_file) for the collection of
prior publications organized by topic.

# 4. Themes 

The workshop presentations and discussions expanded around related and
interlocking themes. Among these are postmortems, blame and sanctions,
controlling the costs of coordination, visualization during anomaly
management, strange loops, and dark debt. Using the cases as a starting
point, these are described in more detail below.

## 4.1 Capturing the value of anomalies through postmortems 

Anomalies are unambiguous but highly encoded messages about how systems
really work. Postmortems represent an attempt to decode the messages and
share them. Patterns in the message content, in the frequency and timing
of the messages, and in the general themes that the messages touch upon
are information about the system that cannot be obtained by other
methods. Complexity and change \-- the two are closely linked \-- make
it impossible to maintain a complete and accurate understanding of the
system. Anomalies are indications of the places where the understanding
is both weak and important. Anomalies are a class of untyped pointers to
deficiencies in our understanding that matter. We can, if we choose to
do so, if we have sufficient skill in decoding, use those pointers to
identify regions worthy of study. Doing this is not simple nor is it
always obviously rewarding. But with diligent practice it is possible.

Some of what we know about postmortems comes from the study of reactions
to failure in other domains. Results from those studies are consistent
with what we know about postmortems in IT settings and this applies to
both the opportunities and challenges that come when trying to build
competence and consistency in the approach to postmortems.

As in other domains, postmortems are private or semi-private events
intended to identify and capture important factors and features
associated with anomalies. The basic premise is that postmortems promote
learning from (bad) experiences so that (1) such experiences can be made
less likely if their sources are eliminated or reduced and that (2) the
ability to manage such experiences can be made better if responses to
them are better prepared for, e.g. by technical means, better training.
The fact that Allspaw and others use the term in IT settings is the
result of recognition of a similarity with the processes used in medical
settings and a deliberate attempt to evoke the conditions and
assumptions that apply there.

One participant observed: \"We see repeatedly that postmortems generate
I-didn\'t-know-that-the-system- worked-that-way experiences.\"
Conducting postmortems informs and recalibrates people\'s models of the
how the system works, their understandings of how it is vulnerable, and
what opportunities are available for exploitation. \"Collectively, our
skill isn't in having a good model of how the system works, our skill is
in being able to update our model efficiently and
appropriately.\"^[5](http://snafucatchers.github.io/#foot_5)^

It is clear from study of after-event reviews that postmortems serve
multiple purposes and engage multiple interests. Although they are often
characterized as narrowly technical in nature, postmortems are deeply
social events that have important functions in the organization. They
are conducted in ritual fashion and, like other rituals, engage
communities in particular ways, assert authority, and represent status.
We will first examine some of the technical aspects of postmortems and
then suggest how their social value and meaning are established and
managed within the organizational context.

### 4.1.1. Technical issues in postmortems: 

Postmortems can point out unrecognized dependencies, mismatches between
capacity and demand, mis-calibrations about how components will work
together, and the brittleness of technical and organizational processes.
They can also lead to deeper insights into the technical,
organizational, economic, and even political factors that promote those
conditions. Postmortems bring together and focus significant expertise
on a specific problem for a short period. People attending them learn
about the way that their systems work and don\'t work. Postmortems do
not, in and of themselves, make change happen; instead, they direct a
group's attention to areas of concern that they might not otherwise pay
attention to. Interestingly, the presence and nature of postmortems
serves as a signal about the health and focus of the organization and
