This regional incident was detected by our availability monitors, and we
were on the investigation bridge within 13 minutes of customer impact.
We understood the issue to the action that was performed erroneously and
determined a way to reverse it. Another option would have been to
rebuild entirely new caches - but it was determined that this rebuild
would take much longer than fixing the caches in-place, so we proceeded
to formulate the method to revive the caches in-place.

On applying this initial mitigation, the caches came back up, which
resulted in a partial recovery of the incident at 06:18 UTC. While
success rates improved significantly at this point (\~60%) the recovery
was considered \'partial\' due to two reasons. Firstly, a timing issue
in applying mitigation caused gateways in one of the two clusters to
cache incorrect cache connection strings. Secondly, the metadata caches
were not receiving updates for changes that happened while the caches
were unavailable.

The first issue was mitigated by restarting all the gateway nodes in the
cluster, which needed to be done at a measured pace to avoid overloading
the recovering metadata caches. As the restarts progressed, we saw
success rates continue to improve, steadily reaching 97% around 07:58
UTC, once all restarts had completed. At this point connections to any
database that had not undergone changes (i.e., service tier updates)
during the incident would have been successful.

The last step was to determine which persistent cache entries were stale
(missed updates) and refresh them to a consistent state. We developed
and executed a script to refresh cache entries, with the initial
refreshes being done manually while the script was being developed. The
success rate recovered to 99.9% for the region at 11:10 UTC. We then
proceeded to identify and mitigate any residual issues, and also started
the process to confirm recovery with customers and downstream impacted
Azure services.

Based on login success rate telemetry, the incident mitigation time was
determined to be 13:30 UTC. Mitigation communications were sent out to
all impacted customers at 19:16 UTC, after a thorough validation that no
residual impact remained.

**How are we making incidents like this less likely or less impactful?**

We are implementing a number of service repairs as a result of this
incident, including but not limited to:

Completed:

-   Programmatically blocking any further executions of the action that
    led to the metadata caches becoming unavailable.

In progress:

-   Implementing stronger guardrails on impactful operations to prevent
    human errors like the one that triggered this incident.
-   Implementing in-memory caching of connection routing metadata in
    each gateway process, to further increase resiliency and
    scalability.
-   Implementing throttling on telemetry readers to prevent ingestion
    from falling behind.
-   Removing dependency of automatic-failover on telemetry system.
-   Investigating other service resiliency repairs as determined by our
    internal retrospective of this incident, which is ongoing.

**How can our customers and partners make incidents like this less
impactful?**

Customers who had configured active geo-replication and failover groups
would have been able to recover by performing a forced-failover to the
configured geo-replica.

More guidance for recovery in regional failure scenarios is available
at:
[https://docs.microsoft.com/en-us/azure/azure-sql/database/disaster-recovery-guidance](https://docs.microsoft.com/en-us/azure/azure-sql/database/disaster-recovery-guidance)

**How can we make our incident communications more useful?**

We are piloting this \"PIR\" format as a potential replacement for our
\"RCA\" (Root Cause Analysis) format.

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/3TBL-PD8](https://aka.ms/AzPIR/3TBL-PD8)

## June 2022

## 29 

[06/29/2022]

Post Incident Review (PIR) - Wide Area Network - Multiple Regions

Tracking ID: YKDK-TT8


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/YKDK-TT8](https://aka.ms/AIR/YKDK-TT8)*

**What happened?**

*Across 28 and 29 June, 2022, two independent Azure networking service
incidents occurred simultaneously. While very few customer subscriptions
were impacted by both, the overlapping nature of the incident timelines
meant that we previously delivered a Preliminary Post Incident Review
(PIR) that addressed both issues. After thorough investigations, we are
publishing a Final PIR for each incident independently. Below is the
Final PIR for Incident YKDK-TT8, summarizing our EMEA-impacting Wide
Area Network (WAN) issue -- we have separately published a Final PIR for
Incident YVTL-RS0, summarizing our multi-region Software Load Balancer
(SLB) issue.*

Between 02:40 UTC and 20:14 UTC on 29 June 2022, a subset of customers
experienced intermittent network failures and packet loss due to an
issue with a router in the Microsoft Wide Area Network (WAN) in the
London, UK area. We have determined that one of several WAN routers
handling traffic transiting this area experienced a partial hardware
failure, which caused it to route improperly 0.4% of the packets it
normally handles. This caused intermittent periods of packet loss for a
subset of customers whose traffic went through that router, which
included a subset of the traffic being served to and from four regions
-- UK West, UK South, Europe West and Europe North.

Some customers using ExpressRoute and/or VPN Gateway were impacted by
this, as their traffic traversed this router. Customers affected by this
would have experienced impact in the form of lower transmission
throughput, long delays in connections and, in the most extreme cases,
connection failures.

**What went wrong, and why?**

Upon investigating this issue, we identified an issue with part of our
WAN infrastructure located in London, U.K.. One specific router in
London intermittently started dropping network packets on one line card.
This impacted 0.4% of the packets traversing it and caused intermittent
network failures for a subset of customer traffic flowing to/from the UK
West, UK South, Europe West, and Europe North regions.

Since end users experienced this issue intermittently, this was
characterized as a 'gray' failure so alerting was not immediately able
to identify that there was an issue. On this single faulty router, only
four of its 140 physical ports had a physical failure, which was causing
a section of the router to malfunction intermittently without producing
an actionable error message.

For Azure VPN Gateway customers, on-premises customer traffic goes
through the WAN routers on their path before reaching Azure VPN Gateway,
so intermittent packet drops on the core router caused connectivity to
be disrupted for Azure VPN scenarios.

**How did we respond?**

When first investigating the impact of this issue, we initially invested
significant effort attempting to identify software-based causes, because
failures were being observed across 17 regions. Router issues typically
only impact a small number of regions, and we identified a number of
concurrent software changes that had been deployed recently.

As the investigation continued, engineers were able to separate the
impact from an unrelated Software Load Balancer (SLB) incident, and at
approximately 19:15 UTC the remaining impacted traffic was triangulated
to WAN routers located near London -- with related failures only
impacting a small subset of traffic across four regions, not 17.

By 20:00 UTC, engineers were able to identify the partially
malfunctioning router. Within the next 15 minutes, the engineers removed
the unhealthy router from the network -- at which point redundant
routers took over the traffic, and fully resolved the issue.

As mentioned above, this specific partial hardware failure was not
producing any specific message in the router that could be tracked or
monitored, and the overall low intensity and intermittence of this issue
(\<0.4% of packets dropped, because of 4 faulty ports out of 140) caused
our traffic-quality alerting to miss this issue, so the investigation
had to resort to manual inspection of router's behavior and
second-degree signals.

**How are we making outages like this less likely or less impactful?**

Already completed:

-   We enhanced our WAN traffic-quality alerting systems to identify
    more accurately these low-intensity but long-duration issues, to
    improve triangulation in future.
-   We have worked with our hardware vendor to improve their error
    messages when partial failures like this happen, and are consuming
    this new signal into our existing automatic alerts and automatic
    mitigations.

Work in progress:

-   We are replacing the faulty components on the failing London router.
    This will be brought back into rotation slowly and safely, as we
    have sufficient redundant capacity with other routers in the
    network.
-   We are investing in improved VPN Gateway end-to-end diagnostics to
    help identify and mitigate this class of issue more quickly.

In the longer term:

-   We are planning enhancements to ExpressRoute traffic-quality
    monitoring, to better identify impact to individual customers during
    these kinds of scenarios.

**How can we make our incident communications more useful?**

We are piloting this \"PIR\" template as a potential replacement for our
\"RCA\" (Root Cause Analysis) template.

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/YKDK-TT8](https://aka.ms/AzPIR/YKDK-TT8)

\

## 28 

[06/28/2022]

Post Incident Review (PIR) - Azure Software Load Balancer - Multiple
Regions

Tracking ID: YVTL-RS0


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/YVTL-RS0](https://aka.ms/AIR/YVTL-RS0)*

**What happened?**

*Across 28 and 29 June, 2022, two independent Azure networking service
incidents occurred simultaneously. While very few customer subscriptions
were impacted by both, the overlapping nature of the incident timelines
meant that we previously delivered a Preliminary Post Incident Review
(PIR) that addressed both issues. After thorough investigations, we are
publishing a Final PIR for each incident independently. Below is the
Final PIR for Incident YVTL-RS0, summarizing our multi-region Software
Load Balancer (SLB) issue -- we have separately published a Final PIR
for Incident YKDK-TT8, summarizing our EMEA-impacting Wide Area Network
(WAN) issue.*

Between 05:26 UTC on 28 June 2022 and 04:00 UTC on 1 July 2022, a subset
of customers experienced intermittent network failures and packet loss
while trying to connect to resources behind a Software Load Balancer
(SLB). This incident impacted customers whose Load Balancer frontend IPs
(VIPs) were in either of two specific scenarios:

\(1\) Azure Firewall customers with more than 15 Destination IP
addresses (DIPs) behind an Azure Load Balancer, combined with a
requirement for both upstream and downstream packets of a single flow to
always land on the same DIP. This scenario impacted services in 14
regions.

\(2\) A service endpoint or private link endpoint connection to a
service in the region with more than 15 DIPs -- for example, SQL and/or
Storage. Impacted customers experienced intermittent connectivity
failures and packet loss while trying to connect to their resources.
This scenario impacted services in three regions.

**What went wrong, and why?**

The Software Load Balancer (SLB) is our software-based routing and load
balancing service for all network traffic within our datacenters and
clusters. At the highest level, it is a set of servers called
multiplexers or MUXs that create an SLB scale unit. Each scale unit
hosts many VIPs (Virtual IPs) to serve traffic to multiple DIPs
(Destination IPs, these are the IPs of the resources behind the VIP).
The MUXs then direct traffic to the DIPs that are in service behind the
VIP, which are dynamically monitored utilizing periodic probes to
monitor health of DIPs.

Upon investigating this issue, we identified a change in our latest code
deployment which caused some MUX instances to store the backend DIPs in
a different order. The change only occurred for load balancers with more
than 15 DIPs in the backend pool. The order of the DIPs is important, as
it is tied directly to the process used to select the backend server
that receives a particular connection. The ordering change impacts
traffic that passes through more than one MUX. There are two scenarios
where this can occur -- the first is when traffic passes through a
firewall in both the incoming and return paths; the second is when a
particular connection moves from one MUX to another MUX, due a change in
routing path through the physical network.

The first scenario impacted Azure Firewall customers, but only those
with more than 15 firewall instances behind a Load Balancer. That's
because Azure Firewall is a stateful firewall that leverages Azure Load
Balancer behind the scenes, scaling horizontally to handle traffic load.
Since flows are pinned to instances for firewall processing, the load
balancer issue meant that packets for a given flow would be received at
different instances and be dropped, impacting customer traffic. The
number of instances behind the Firewall's internal Load Balancer are
determined by a customer's traffic pattern. Not all Firewall's internal
Load Balancers have this many instances, which led to delays in
correlation between customer reported issues and the behavior of the
platform.

The second scenario occurred due to a change in the flow identifier for
an IPv6 packet routing header used for Private Link and service endpoint
traffic. Traffic that used this technology would have seen drops as the
connection moved from one MUX to another MUX. The new MUX would forward
the connection to a different server in the backend pool, which would
then reject the request.

When we deploy updates to our cloud, we use a system called Safe
Deployment Practices (SDP). SDP is how we manage change automation so
that all code and configuration updates go through well-defined stages,
to catch regressions and bugs before they reach customers or -- if they
do make it past the early stages -- impact the smallest number possible.
Although telemetry observed a small increase in errors (and a number of
customer support requests had been raised) the intermittent nature of
this failure meant that our health checks did not correlate these
signals to the recent change. This meant that the code had propagated to
14 regions (of our 60+ regions) before we correlated customer impact to
this specific deployment.

**How did we respond?**

We engaged multiple investigation and mitigation workstreams in
parallel. While we worked to understand the issue, we initially used
targeted operations to mitigate VIPs manually for specific customers
that were identified as impacted.

Simultaneously, we developed a hotfix that would address the bug that
could be applied without causing further impact to customer traffic.
After the hotfix was tested, it was rolled out using a phased approach
as per our Safe Deployment Practices. 

Additionally, we created automation that would scan all VIPs every five
minutes and mitigate any VIP that was in an inconsistent state to
minimize the impact to customers while the hotfix rolled out. This
script continued to run until we had our sustainable mitigation
completely rolled out.

**How are we making outages like this less likely or less impactful?**

Already completed:

-   We have restored consistent ordering for our backend instances,
    returning to the pre-incident state. The order of these is tied
    directly to the process used to select the backend server that
    receives a particular connection.

Work in progress:

-   To address SLB scenario #1 above, we are investing in additional
    monitoring for Azure Firewall to detect cases when upstream and
    downstream packets on a single flow do not reach the same DIP.
