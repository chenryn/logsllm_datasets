Azure Container Apps, Azure ML, Azure Managed HSM, Azure Confidential
VMs, Azure Database Services (MySQL - Flexible Server, Postgres-
Flexible Server, PostgreSQL -- Hyperscale) to experience provisioning
failures globally. This issue impacted customers that relied on
provisioning of certificates as part of provisioning of an Azure
resource. This write-up is a Post Incident Review (PIR) we are providing
to summarize what went wrong, how we responded, and the steps Microsoft
is taking to learn from this and improve.

**What went wrong, and why?**

The requesting authority for Azure Key Vault (the underlying platform,
on which all the described services rely for the creation of certificate
resources) was experiencing high latency and volume of requests. This
resulted in provisioning failures for the impacted services, as those
services were not able to acquire certificates within the expected time.
During the incident, a backend service that Azure Key Vault relies on,
became unhealthy due to an unexpected spike in traffic during scheduled
hardware maintenance, which caused a build-up of requests in the queue,
resulting in high latencies to fulfil new certificate creation
requests. 

**How did Microsoft respond?**

We developed and deployed a hotfix to increase the throughput, created
new queues for request processing, and drained the queue of accumulated
requests to alleviate the overall latency and process requests as
expected.

**How is Microsoft making incidents like this less likely, or at least
less impactful?**

• In the short-term, we are implementing request caps and partitioning
the request queues, to help prevent lasting failures in the service in
similar scenarios.

• We are also reviewing the backend capacity and gaps in the maintenance
process that led to the loss of availability during this maintenance
operation.

• Based on our learning from this incident, we are implementing
improvements to our health monitoring and operational guidance that
would help reduce the time to detect similar issues and allow us to
address similar issues before customers experience impact.

• In the longer term, we are working to add fine-grained distributed
throttling and portioning, to add additional isolation layers to the
backend of this service, which will minimize impact in similar
scenarios.

• Finally, we will work to add more Availability Zones and fault domains
in all layers of the stack, along with automatic failover to the
service, to help prevent disruption to customer workloads.

**How can we make our incident communication more useful?**

We are piloting this \"PIR\" format as a potential replacement for our
\"RCA\" (Root Cause Analysis) format.

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/YLBJ-790](https://aka.ms/AzPIR/YLBJ-790)

## 12 

[08/12/2022]

Post Incident Review (PIR) - Azure Communication Services - Multiple
Regions

Tracking ID: YTYN-5T8


**What happened?**

Between 18:13 UTC on 12 Aug 2022 and 03:30 UTC on 13 Aug 2022, customers
using Azure Communication Services (ACS) may have experienced
authentication failures, or failures using our APIs. As a result,
multiple scenarios may have been impacted including SMS, Chat, Email,
Voice & Video scenarios, Phone Number Management, and Teams-ACS Interop.

**What went wrong, and why?**

[An Azure resource provider provides the ability for customers to create
and maintain resources, in this case, for ACS. The ACS resource provider
utilizes backend Cosmos DB instances for resource metadata persistence.
Prior to the incident, an increased volume of data-plane related
requests was made by the resource provider to the database, which met
database throughput limits. At 18:00 UTC on 12 Aug 2022, to meet the
increased demand of requests, the database processing capacity was
increased. This change in database capacity inadvertently exposed a
latent code bug for the resource provider, which resulted in a
functional difference in the number of database results being returned
against what could be processed by the resource provider. ACS is a
globally distributed service and the metadata being retrieved was
required for routing calls across different regions for the
authentication process. This resulted in ACS authentication failures,
and subsequently caused SMS, Chat, Voice & Video, Phone Number
Management, and Teams-ACS Interop scenarios to
fail. ]{style="color: rgb(51, 51, 51)"}

**How did we respond?**

Automated alerting indicated several failures for different ACS API
requests made by customers. We immediately investigated with multiple
engineering teams, however understanding the nature of the issue took
time because specific fields used for debugging Cosmos DB issues were
not being logged for successful queries. Due to the service
configuration, a rollback of the change to the database instance would
not have been supported. Once the underlying issue was identified, we
developed code fixes to resolve the issue. We validated and deployed the
fix using our Safe Deployment Practices, in phases. The hotfix was fully
rolled out at 03:30 UTC on 13 Aug 2022, with customers reporting
successful operation shortly thereafter.

**How are we making incidents like this less likely or less impactful?**

Completed:

-   We\'ve completed code updates to address the latent bug and help
    ensure the resource provider can process all results in similar
    scenarios.
-   We\'ve added additional logging of backend database requests for the
    ACS resource provider, to ensure improved traceability in future.
-   We have added additional gates for [database configuration updates
    and hardening for our processes when applying such updates. We have
    mirrored all production configuration templates in our
    pre-production environment to allow validation of configuration
    updates before they get deployed to
    production.]{style="color: rgb(51, 51, 51)"}
-   We have completed additional Failure Mode Analyses (FMA) across
    different ACS features. We have created repair items for a resilient
    service architecture to improve failure recovery time.

**How can we make our incident communications more useful?**

We are piloting this \"PIR\" format as a potential replacement for our
\"RCA\" (Root Cause Analysis) format.

You can rate this PIR and provide any feedback using our quick
3-question
survey: [https://aka.ms/AzPIR/YTYN-5T8](https://aka.ms/AzPIR/YTYN-5T8)

## July 2022

## 29 

[07/29/2022]

Post Incident Review (PIR) -- Network Connectivity Issues

Tracking ID: 7SHM-P88


**What happened?**

Between 08:00 UTC and until 13:20 UTC on 29 July 2022, customers may
have experienced connectivity issues such as network drops, latency,
and/or degradation when attempting to access or manage Azure resources
in multiple regions.

The most significant impact would have been experienced in the following
regions -- Brazil South, Canada Central, East Asia, East US, East US 2,
France Central, Japan East, Korea Central, North Central US, South
Africa North, South Central US, Southeast Asia, West Europe, and West
US. Customers in other regions may have seen an intermittent impact when
accessing resources across the Microsoft wide area network (WAN).

**What went wrong, and why?**

Starting at 08:00 UTC on 29 July, the Azure WAN began to experience a
sudden and significant increase of traffic, upwards of 60 Tbps in
additional traffic compared to the normal levels of traffic carried on
the network.

While the event was detected immediately and automated remediation was
triggered, the substantial increased bursts of traffic occurring
throughout the event affected the ability of automated mitigations to
continue providing the necessary relief to the network. WAN routers then
became overloaded and dropped network packets, which resulted in network
connectivity issues experienced by some customers.

This event included impact to both intra-region and cross-region traffic
over various network paths, which included ExpressRoute.

**How did we respond?**

We have several detection and mitigation algorithms that were triggered
automatically around 08:00 UTC when an increased burst of traffic
occurred. The volume of traffic surges continued to substantially
increase, reaching 10-15 times greater than any traffic volume
experienced on the network prior.

While the mitigation mechanisms were successfully triggered to load
balance and throttle the traffic bursts to help prevent impact, the
significance of traffic on the WAN routers resulted in these mechanisms
to take longer to alleviate the traffic surges and restore traffic back
to normal levels.

By 13:20 UTC, traffic levels returned to normal as network telemetry
confirmed packet drops had reduced to standard levels, which is when
customers would have seen resource and service network health restored.

**How are we making incidents like this less likely or less impactful?**

We are implementing service repairs because of this incident, including
but not limited to:

Already completed: 

-   Additional alerting for specific packet drops signature caused by
    significant traffic bursts.
-   Improvements to network device capabilities to help reduce packet
    drops when handling significant traffic bursts.
-   Changes to the network design for traffic spike detection to help
    reduce the time to mitigate for similar events.
-   Improve network incident response playbook to better streamline
    preventative actions performed for similar events.
-   Apply additional layers of network throttling to help protect the
    network reliability when increased traffic surges occur. 

**How did we communicate with impacted customers?**

Starting around 11:00 UTC, we began to receive some reports of a
potential emerging issue. As signals continued to gradually increase, we
posted an initial statement to the Azure status page at 11:52 UTC.

Delays in communications via Service Health in the portal were primarily
due to challenges gauging the extent of impact and affected regions as
limited telemetry of the networking event developing did not clearly
indicate a viable scope of impact. Though other signals via internal and
external reports indicated a likely platform event ongoing, the
disparity of signals deterred targeted notifications until a broad
networking issue was determined.

Communications were sent via Azure Service Health for Azure services
that started to report impact, where were later determined to be
affected by the networking event. With further analysis and evidence of
regional impact confirmed, broad targeted communication was sent to
customers region-wide for the identified affected regions by 13:28 UTC. 

Between 13:28 and 15:32 UTC, communications were sent to the customers
of additional impacted regions identified.

By 15:32 UTC, we began reporting recovery via the status page and
Service Health, but monitoring and preventative workstreams persisted,
which we continued to report until the necessary preventative
workstreams were completed by 19:52 UTC.

**How can we make our incident communications more useful?**

We are piloting this \"PIR\" format as a potential replacement for our
\"RCA\" (Root Cause Analysis) format.

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/7SHM-P88](https://aka.ms/AzPIR/7SHM-P88)

## 21 

[07/21/2022]

Post Incident Review (PIR) - SQL Database - West Europe

Tracking ID: 3TBL-PD8


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/3TBL-PD8](https://aka.ms/AIR/3TBL-PD8)*

**What happened?**

Between 03:47 UTC and 13:30 UTC on 21 Jul 2022, customers using SQL
Database and SQL Data Warehouse in West Europe may have experienced
issues accessing services. During this time, new connections to
databases in this region may have resulted in errors or timeouts.
Existing connections would have remained available to accept new
requests, however if those connections were terminated and then
re-established, they may have failed.

New connections to the region and related management operations began
failing from 03:47 UTC, partial recovery began at 06:12 UTC, with full
mitigation at 13:30 UTC. Although we initially did not declare
mitigation until 18:45 UTC, a thorough impact analysis confirms that
failure rates had returned to pre-incident levels earlier. No failures
that occurred after 13:30 UTC were directly as a result of this
incident.

During this impact window, several downstream Azure services that were
dependent on the SQL Database service in the region were also impacted -
including App Services, Automation, Backup, Data Factory V2, and Digital
Twins.

Customers that had configured active geo-replication and failover groups
would have been able to recover by performing a forced-failover to the
configured geo-replica - more information can be found here
[https://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view](https://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view)

**What went wrong, and why?**

For context, connections to the Azure SQL Database service are received
and routed by regional gateway clusters. Each region has multiple
gateway clusters for redundancy - traffic is distributed evenly between
the clusters under normal operations, and automatically rerouted in case
one of the clusters becomes unhealthy. Each gateway cluster has a
persisted cache of metadata about each database in the system, that is
used for connection routing. These caches are used for scaling-out
gateway nodes, to avoid contention on a single source of metadata. There
are multiple caches per gateway cluster and each node will fetch data
from any of the caches that is available. The West Europe region has two
gateway clusters, and each of these clusters has two persisted metadata
caches.

An operator error led to an incorrect action being performed in close
sequence on all four persisted metadata caches. The action resulted in a
configuration change that made the caches unavailable to the regional
gateway processes. This resulted in all regional gateway processes in
West Europe becoming unable to access connection routing metadata,
leading to the regional incident from 03:47 UTC. New connections would
have failed as the gateways were not able to read routing metadata, but
connections that were already established would have continued to work.
Management operations on server and database resources would also have
been impacted, as some workflows also rely on connection routing.

A secondary impact of the issue was that our internal telemetry service
in the West Europe region became overloaded with queries. This caused
the telemetry ingestion to fall behind by a few hours and telemetry
queries were also timing out. The telemetry issues contributed to delays
in automatically notifying impacted customer subscriptions via Azure
Service Health.

As some customers were receiving automatic notifications of impact
within 15 minutes, we assumed that the notification pipeline was working
as designed. It was later in the event when we understood that
communications were not reaching all impacted subscriptions. As a
result, we broadened our communications to every customer in the region
and published an update on the Azure status page.

Additionally, Automatic failover for anyone who had setup failover
groups with auto-failover configuration was also impacted due to
telemetry issues (manual failover was not impacted).

**How did we respond?**

