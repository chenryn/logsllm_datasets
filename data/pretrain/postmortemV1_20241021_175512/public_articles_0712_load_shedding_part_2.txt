The reality is that this single node actually has 6 CPU's capacity, so it already has 50% more than
should be required to do the work. If we double this up to 2 sites; we have sized the system at a max
capacity of 400 (4 CPU’s), but in really 99.9%+ of the time we actually have a capacity of 1200 (12
CPU’s)-3timesthatrequiredtoprocessourmaximumexpectedworkload.
Likewise we have a separate HSM pool associated with each active NonStop system. A single HSM
pool(with1devicefailed)canprocessanentirepeakday.
For telecoms we operate a policy that all of a customer’s bandwidth requirement can fit down a single
physical circuit at again no more than 80% utilised. However for resiliency we have 2 full size telecoms
circuitstoeachactiveNonStop,(4intotal)foreachcustomer.
So in normal running we have triple the NonStop capacity required to process the peak day which is
already significantly higher than the “average” day, more than double the required HSM capacity, and
quadruple the telecoms capacity required. Finally our service operates a QoS system where
Synchronous payments (customer waiting 15 sec round trip SLA) always take priority over
Asynchronous payments (pre-scheduled payments).In theory all synchronous payments get serviced
withintheSLAand15seccustomertimeoutscenario,andasynchronousonescantakealittlelongerto
process.
So, as has been stated before, this is mostly a policy and budgets decision. What is the capacity and
availability model the business is prepared to pay for in order to mitigate service risk? The availability
model we use ensures significant additional capacity is always available anytime..
Thismayseemwastefulbutisnowherenearaswastefulasaconventionalproduction/DRmodelwhere
theextracapacityisdormantandcannoteasilybebroughtonline,especiallywithoutcustomerimpact.
Summary
Ourauthor,PaulGreen,addshisowninsightsintothisissue:
Great discussion; thank you, everyone. Let me relate some real-world situations that I've been involved
in. Let me add that I'm specifically interested in examining this issue from the point of view of the
system architect, not the end-user. I fully agree that a system that can't handle the full incoming load is
less than 100% available. I also fully agree that in the real world, we must do everything possible to
avoidthissituation.HatsofftoDamian,whoworksforanorganizationthatiswillingtoinvestthemoney
toensurethatsufficientcapacity isalwaysavailable. AllIcansayisthatmostofthecustomersthatI've
dealtwithovermycareerwouldn'tdreamoffundingsuchadeluxeinfrastructure.
2AvoidingCapacityExhaustion,AvailabilityDigest;July2012.
http://www.availabilitydigest.com/public_articles/0707/workload_forecasting.pdf
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

If I divide my customers into groups based on their approach to dealing with overload situations, they
fallintoseveralbroadcategories:
Category #1. "Avoid the problem". This group sets a goal of always having enough capacity to handle
any workload, usually has a fairly predictable workload, and usually diligently monitors their systems
and takes action far enough in advance to avoid the problem. So far, so good. However, if they do get
overloaded, they have no defense. Generally speaking, they run around with their hair on fire, utterly
surprised and in a panic because this was never supposed to happen. Their solution is usually to rush
outandbuymorehardware(onlytoseethesituationrepeatin2-3years).
Category #2. "Bounce the application". This group has one universal solution for any application issue.
Restart it. If that doesn't work, restart the operating system. Keep doing this until the problem goes
away. One of my customers has a hard-coded 4-second timeout between all clients and servers. If this
timeout is ever reached, code is invoked to restart the application. While the technique is brutal, it has
thebenefitthatyoucaneasilypredictthedurationoftherecoverytime.
Category #3. "Protect the online application, defer batch". This group has two classes of inbound
transactions:thosethat mustbe processed in real time(online), and thosethatcantakea few hours or
more (batch). The batch system simply recursively uses the online system to do its work. So when the
online system is at peak load, they hold off entering batch transactions. This is a priority scheme in
whichtheendusersdecidewhichservicetopurchase(onlineorbatch) basedontheirneedsandprice.
Asbefore,successgenerallyridesonpropermanualintervention.
Category #4. "Muddle through". This group muddles along with no clear strategy. They try to have
enough capacity, but aren't methodical about it. They have a bunch of ad-hoc, manual techniques to
use when things get rough. If they can deflect blame onto someone else, they do so. These clients are
usuallyunder-fundedandunder-resourced.
Category #5. "Competent and Paranoid". Very rare creature. This group always has enough hardware
capacity to handle any reasonable load and most unreasonable loads. They methodically track usage
and accurately predict future requirements, and they are diligent about staying ahead of the demand.
Further, they obsessively run accurate benchmarks of simulated traffic so that they know how their
systems (hardware and software) behave under real-world situations. They know their maximum
throughput,andthey know (becausethey'vesimulated it) whathappens ifalinetoanHSMgoes down,
or alinetoVisagoes down,or aCPU goes offline,or any ofthereal-worldthings thatcan gobaddoes
go bad. They know because they injected errors in their lab. They have thought about the problem. If
they have a weakness, it is that they still require too much manual intervention. But otherwise, they are
thebestofthebunch.
IhopewecanconvinceIT shops toadoptadefense-in-depthapproach.Buying enoughhardwareearly
andoftenenoughisnecessarybutnotsufficient.Therestisuptotheapplicationarchitects.
I rather like the idea of introducing some randomness into the basket of possible solutions to load
shedding. I think there is an analogy that can be made to the packet flow control algorithms of the
Internet. I'm not an expert in packet flow control, so I won't try to go into details, but as I understand it,
many devices today have what is called a "tail drop" or "tail truncate" algorithm -- they capture packets
untiltheir buffers fillup,andthenthey dropallincomingpackets.Theproblemwithsuchanalgorithm is
that it produces bursty traffic and introduces long latencies into the transmission of data. A new
algorithmhas beenproposedinwhichthereceiver randomly discards packets as its queuefills up. The
longer the queue, the more packets get dropped. The act of discarding packets provides a clue to the
sender that he's transmitting too fast. His algorithm will slow him down. Communication continues and
isneverinterrupted.Itisfairbecauseeveryoneistreatedthesameway.Thepipeisstillusedefficiently
(high degree of bandwidth utilization). Latency is kept low. Well-behaved senders are not penalized. It
seems tomethattheseare someofthesamequalities we'dliketoseein any load-shedding algorithm.
5
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

(If you are interested in reading about the newly-proposed load-shedding algorithm for TCP/IP, do an
Internetsearchfor"ControllingQueueDelay"byKathleenNicholsandVanJacobson.)
I'll bet the flow-control algorithms (if any) used in POS/ATM terminals still date from the time of low-
speed, dial-up lines. But now that everything is online, why not come up with a consistent, unified
approach? Instead of sending the request and starting up a 60-second timer that will abort the request
(which is what many devices still do), [why not] have them retry sooner but back off an increasing
amount of time each time they time out? If the whole network worked this way, then when the central
server(s)gotsaturated,the systemwouldautomatically findtheappropriateinput ratethatwouldmatch
the processing capacity of the system. If the servers were only slightly overloaded, then most requests
would still get processed on the first try, but some clients, chosen at random, would take a little longer.
Astheserversgetmoreoverloaded,moreclientsaredelayed.
Worksmarter,notharder.
Concluding Thoughts
Clearly, dealing with overloaded application processing systems is a real problem that deserves serious
and sustained effort. We believe it is important to factor these concerns into both the initial design of the
system and the ongoing maintenance. While the precise methods will vary from one application to
another, due to the unique properties of each application, we can still draw some important lessons from
thisdiscussion.
First, every effort must be made to design a system that cannot become overloaded under any
reasonable (or even moderately unreasonable) situation. Second, all affected parties should be involved
in the up-front design of any load-shedding algorithms: senior management, end-users, application
designers, and system vendors. Each of these parties has a unique role and set of insights. Third, to the
greatest extent possible, the design should permit the software to dynamically take care of itself. While
there will always be a need for manual intervention, automated interventions can be designed and tested
under a wide variety of simulated conditions; and thus an organization can be assured that many
situations will be quickly and effectively managed. Indeed, advance testing of system behavior, capacity
and response time is fundamental to success of continuously-available systems.Fourth, there is a strong
need for continuous monitoring, logging, and in some cases, publication of key system parameters.
Certainly, operations staff requires detailed knowledge of system behavior. But in addition, users can
often modify their own behavior if they know the system is overloaded; at the very least, they will
appreciate the disclosure of anticipated response times. (Think of the difference between a call center
that informs you of your anticipated wait time and one that does not do so). Lastly, building a culture that
strivestoprovideserviceunderallpossiblecircumstancesensuresthatanorganizationwillsucceed.
Acknowledgements
Wewould like to thank the following Continuous AvailabilityForum members for their contributions to this
topic:
JonSchmidt
GerhardSchwartz
ThomasBurg
RobertGezelter
WolfgangBreidbach
TomWright
DamianWard
AlanSmith
MikeFyson
RandallBecker
6
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
|---|--|
| 0 |  |
| 1 |  |
