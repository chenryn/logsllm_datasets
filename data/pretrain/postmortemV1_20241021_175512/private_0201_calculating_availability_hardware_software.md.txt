Calculating Availability – Hardware/Software Faults
January2007
The Diminishing Effect of Hardware Failures
In our previous articles, we have discussed the basic availability equation for active/active
systems, how the equation is affected by different repair strategies, and how it is affected by the
requirement to recover a node and subsequently restore it to service once it has been repaired.1
In all of these analyses, we assumed that the active/active system failure was caused by the
failure of s+1 nodes due to hardware faults, where s is the number of spare nodes in the
application network. Somecomponent needed to be repaired in at least one node, and that node
hadtoberecoveredsothattheactive/activesystemcouldberestoredtoservice.
However, in actual practice today, active/active system failures are rarely caused by dual
hardware failures. Hardware has simply become much more reliable than software and system
operators. If a system fails, it is most likely that one or more nodes failed due to a software fault
oranoperatorerror.
Forinstance,iftheprobabilityofanodefailureduetoahardwarefaultis10%,theprobabilityofa
system failure caused by two node failures, each taken down by a hardware fault, is only 1%.
Otherfactorscontributetotheother99%ofallsystemfailures.
Consequently, an active/active system failure is highly likelyto have been caused byat least one
node having been taken down by a failure other than hardware – perhaps a software fault, an
operator error, or anenvironmentalfault of somekind.Inthis case,thesystem will be restored to
service without the need to repair a hardware component. Only one of the nodes needs to be
recoveredinordertoallowtheactive/activesystemtoberestoredtoservice.
Node recovery may require rebooting the system, reloading applications, opening the database,
and testing that the node is, in fact, operational. System restoration may require resynchronizing
the databases in the application network. These activities are likely to be much faster than a
hardware repair – hours rather than days, resulting in a much faster system restoration than if a
hardwarecomponentneedstoberepaired.
Thisarticleshowsthemodificationstobemadetotheavailabilityequationstoaccountforthefact
thatonlysomesystemfailuresrequireahardwarerepair.2
1
CalculatingAvailability–RedundantSystems,AvailabilityDigest;October,2006.
CalculatingAvailability–RepairStrategies,AvailabilityDigest;November,2006.
CalculatingAvailability–TheThreeRs,AvailabilityDigest;December,2006.
2
TheimpactofhardwarefaultsonsystemavailabilityisdealtwithindetailinBreakingtheAvailabilityBarrier:Achieving
CenturyUptimeswithActive/ActiveSystems,byPaulJ.Holenstein,Dr.BillHighleyman,andDr.BruceHolenstein.
1
©2007SombersAssociates,Inc.,andW.H.Highleyman

Availability Analysis Reviewed
The equations which we developed earlier that considered repair strategies, the recovery of
nodes,andtherestorationtoserviceofanactive/activesystemare
r/(s1)R
F f(1a)s1 forparallelrepair (1)
r/(s1)
rR
F f(1a)s1 forsequentialrepair (2)
r
Parallel repair implies that there is a different service technician simultaneously working on each
failed node. Sequential repair means that there is only one service technician working on one
nodeatatime.Theparametersintheseequationsare
F istheprobabilityoffailureoftheactive/activesystem.
a istheavailabilityofanode.
r istherepairtimeforanode(hardwarerepairplusrecovery).
R isthesystemrestorationtime.
s isthenumberofsparenodesprovidedforthesystem.
f isthenumberofwaysthats+1outofnnodescanfail.
n isthenumberofnodesintheactive/activesystem.
Systemavailabilityis,ofcourse,1-F.
The Impact of Occasional Hardware Faults
Inthe above equations, r is thetimetorepair the node andtorecover itsothatit canbereturned
to service. Should a node fail due to a problem other than a hardware fault, there are still
activities that must be performed before a node can be recovered and returned to service. These
activitiesmightincludethefollowing:
 thetimetodecidethatthenodehasreallyfailed.
 the time to determine the cause of the problem – is it a hardware fault that needs to be
repairedorasoftwareoroperatorerrorthathasdonenootherdamagetothesystem.
 rebootingthenodeandrestartingitsapplications.
 openingthedatabasebytheapplications.
 testingthenodetoensurethatitappearstobeworkingproperly.
Thetimetoperformthesetasksiswhatwehavereferredtoastherecoverytimeofthenode.
Letus modifyour definition of theaboveparameters.Let r be therepair time,if any, andr’be the
recoverytime:
r isthetimetorepairahardwarefault.
r’ isthetimetorecoveranode(includingtheinitialdecisiontimes).
Letusfurtherdefinehasbeingtheprobabilitythatanodewillbetakendownbyahardwarefault:
h istheprobabilityofanodefailureduetoahardwarefault.
2
©2007SombersAssociates,Inc.,andW.H.Highleyman

All node failures require a recovery time of r’. In addition, h of those node failures require a
hardwarerepairtimeofr.Thus,thenodalmeantimetorecover,mtr,is
mtr r'hr (3)
In our earlier equations expressed by Equations (1) and (2) above, the nodal mtr was the repair
and recovery time, r. All we need to do is to update that value with our new value of mtr as
expressedinEquation(3).Thisyields
(r'hr)/(s1)R
F f(1a)s1 forparallelrepair (4)
(r'hr)/(s1)
(r'hr)R
F f(1a)s1 forsequentialrepair (5)
(r'hr)
Noteonceagainthedifferencesbetweenr,r’,andR:3
r isthetimetorepairahardwarefaultinanode.
r’ is the time to recover a node (including the initial decision times) excluding
repair time, if any. It is the time spent by personnel at the node site from the
time that the nodal fault occurred to the time that the node is ready to be
returnedtoserviceintheactive/activenetwork,exclusiveofhardwarerepair.
R isthetimetorestoretheactive/activesystem toserviceonceoneof itsfailed
nodes has been recovered. This time might include resynchronizing the
database copies in the application network, entering backlogged
transactions, and any other activity that must be performed at a full system
levelbeforeserviceisreturnedtotheusers.
ThePointofNoReturn
As hardware becomes more and more reliable, there comes a point of no return beyond which
further improvements in hardware availability provide no improvement in system availability. This
is because the rate of system failures caused by software faults and operator errors is orders of
magnitude greater than the rate of such failures caused by hardware faults. The NonStop triple
modular redundancy (TMR) NSAA configuration which offers seven 9s of hardware availability is
acaseinpoint.4
As hardware becomes more reliable, the probability of a hardware fault, h, approaches zero; and
thetermr’+hrapproachesr’.Inthelimit,Equations(4)and(5)approach
r'/(s1)R
F f(1a)s1 forparallelrepair (6)
r'/(s1)
r'R
F f(1a)s1 forsequentialrepair (7)
r'
3
Equations (4) and (5) have been derived rather intuitively. A formal analysis shows that these relationships are valid
provided that the repair time is much larger than the recovery or restore times. For the detailed analysis, contact
editorest.com.Caution:Formathnutsonly.
4
R.Buckle,W.Highleyman,TheNewNonStopAdvancedArchitecture:AMassiveJumpinProcessorReliability,The
Connection;September/October,2003.
3
©2007SombersAssociates,Inc.,andW.H.Highleyman

Equations (6) and(7) representthelimitingfailure probabilities thatcan be achieved if there is no
probability of a hardware fault. These limits are reached if the contribution of hardware repair
time,hr,tonodalmtrisverymuchlessthanthecontributionofnodalrecoverytime,r’.
For instance,assumethatthenodalrecoverytimeis 4hours andthattherepair timeis 24hours.
If the probabilityof a hardware failure is 0.1% (.001), then hardware faults add only.024 hours to
the four hours of recovery time for a total mtr of 4.024 hours. Reducing the probability of
hardwarefailureevenfurtherwillhavelittleimpactonsystemavailability.
Summary
Not all node failures are caused by hardware faults. In fact, with today’s hardware reliability,
hardware faults generally contribute much less to node failures than do software faults or
operatorerrors.
As hardware becomes more and more reliable as it has over the last several years, there comes
a point at which further improvements in hardware reliability will have little impact on system
availability. For instance, the hardware reliability of the triple modular redundancy configuration
forHP’snewNonStopservershasreachedthislimitingpoint.
4
©2007SombersAssociates,Inc.,andW.H.Highleyman
