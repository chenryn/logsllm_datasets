Using an Availability Benchmark
October2008
In several previous articles, we have talked about availability being the poor cousin to
performance when it comes to benchmarks. In fact, for over two decades, the IT industry has
dependeduponperformancebenchmarkslikeTPC-Cfrom theTransactionProcessingCouncilto
make informed decisions concerning system purchases. However, in this era of 24/7
requirements for our mission-critical systems, there is still no way to compare the availabilities of
diversesystems.
Consulting firms such as Gartner Group and Standish Group have published the results of
extensive surveys of the availability experience for broad classes of systems, but there is not yet
arecognizedmeanstocomparetheavailabilitiesofspecificsystems.
In today’s world of gigabit/second processors and petabyte storage systems, is not availability
equally important? After all, a system that is down has zero availability. And downtime in many
applications has apricetagmeasuredas hundreds of thousands of dollars per hour. Over thelife
spanofasystem,itsdowntimecostcanoftenbemultiplesofitsoriginalcost.
In our mostrecent article on this subject, entitled Adding Availabilityto Performance Benchmarks
(September, 2007), we suggested that recovery time is a useful and measurable metric
appropriate as an availability benchmark. But how does management use this measure to aid
theminapurchasedecision?Wesuggestaformalmethodtodosointhisarticle.
A Review
The solution is to add an availability measure to the current TPC-C transaction-processing
benchmark. But how do we measure availability when many current systems are showing failure
intervalsofyears?
Inmanyof our previous Digest articles, we have addressed this issue. In Let’s Get an Availability
Benchmark (June, 2007), we reviewed the current transaction-processing benchmarks and
discussed the problems ofadding availabilityto these benchmarks.Wediscussed manypossible
tradeoffsbetweenavailabilityandperformanceinAvailabilityversusPerformance(August,2007).
Yes–availabilitynotonlycostsmoneybutalsocostsperformance.
InAdding AvailabilitytoPerformanceBenchmarks (September,2007), we attemptedtobreak the
availability-benchmark barrier by pointing out that there were several metrics for availability –
availability itself (the proportion of time that a system is up), its failure interval (MTBF – the mean
time before failure), and system recovery time (MTR – the mean time to recover). These are
relatedbythewell-knownequation
MTBF
Availbility  A 
MTBFMTR
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

As we think about it, the measure of reliability is in the eye of the beholder. Any one of these
parametersmaybechosenasthemeasureofreliability.Forinstance:1
 In the Hubble satellite, mean time before failure, MTBF, is of paramount importance.
It is critical for a system contained in a satellite to be up as long as possible
(hopefully, years) because once it fails, it is generally not repairable. (Most satellites
havebackupsystems,butwilltheybeoperationalwhentheprimarysystemfails?)
 In a system measuring oil flow in a pipeline, availability, A, is the most important
measure because when the system is down, oil flow is unknown. Any downtime
resultsinrevenuelossforoildelivered.
 Inacommercialtransaction-processingsystem,wesubmitthatmeantimetorecover,
MTR, is the most important measure. After all, if the recovery from a fault is so fast
thatnoonenoticesthattherehasbeenafault,ineffect,therehasbeennofault.
We expand on the reasoning behind recovery time as the availability metric and how it would be
usedasabenchmarkdeliverableinthenextsections.
The Importance of Recovery Time in Transaction Processing Systems
The importance of recovery time in transaction-processing systems can best be shown by
example. Assume that we have a system with an availability of four 9s (it is up 99.99% of the
time). There are any number of combinations of recovery time, MTR, and failure interval, MTBF,
thatleadtoanavailabilityoffour9s.Letuslookatsomecases:
 Case 1: The system is down one second every three hours. The users probably will
notevennotice this outage as it is withintheresponse timethat theyexpectfrom the
system.
 Case2: Thesystem isdownoneminuteeveryweek.Users willprobablybeaffected
by the outage but will grudgingly accept it. After all, “Computers do fail.” (How often
doyourebootyourPC?)
 Case 3: The system is down one week every 200 years. The users will be delighted
with the system until they all lose their jobs because the company goes out of
business. If you say “So what? The system will be gone long before then,” you are
missingthepoint.Thatbadweekcouldbenextweek.
Based on these examples, we submit that availability and failure intervals are not nearly as
important as recovery time in these sorts of transaction-processing systems. It does not matter
howmuchofthetimeasystem isuporhowoftenafailureoccurssolongasthefailureintervalis
so short that the users don’t notice it or at least are not inconvenienced. Therefore, it is recovery
timethatcountsintransaction-processingsystems.
Recovery Time as an Availability Benchmark
So how do we measure system recovery time? This is a task better left to the benchmark
standards people. There are many kinds of faults that can take down a system, such as
processor faults, software faults, database faults, network faults, operator errors, and others. To
subjectasystem toacomplete mix offaults andmeasureits recoverytimemightwellbe overkill.
1AchievingCenturyUptimesPart7:WhatistheAvailabilityBarrier,Anyway?,Connection;November/December,2007.
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

After all, most faults simply cause the system to crash; and any crash recovery might be a
reasonabletest.
From arecovery-timeviewpoint,therearetwotypes of faults–thosethatrequire hardwarerepair
followed by a system recovery (loading applications, recovering the database, testing, etc.) and
thosethatonlyrequirerecovery(suchas anapplicationfaultor operator error).Onemightdesign
a set of tests to create a simplemix of these two types of faults. For instance, the recoverytimes
might be measured for the cases of pulling and replacing a processor, pulling and replacing one
or more disks (as required to create a disk crash), killing an application, and halting the system
viaan operator command.Allfaults wouldbeinduced whilethesystem was under thefull loadof
the performance benchmark. The resulting recovery times would be averaged according to an
agreeduponweighting.Theresultwouldbethepublishedrecoverytime.
However, there is still a leap from knowing the recovery times of competing systems to making a
purchasedecision.
Using Recovery Time as an Availability Benchmark
To get a further insight into this, let us consider a comparison between two systems. System A
costs $1,000per tps (transactions per second),andSystem Bcosts $2,000per tps.Furthermore,
System A has a recovery time of one hour; and System B has a recovery time of three minutes
(1/20ofanhour).
If 100transactions per secondmustbesupported,andif thecosttotheenterpriseof downtimeis
$100,000 per hour, System A will cost $100,000 plus $100,000 per failure. The cost of System B
will be $200,000 plus $5,000 per failure. We don’t know what the failure intervals are for the two
systems, but we do know that after the second failure, System B has a significantly lower cost
than System A. System B is therefore the preferred choice according to these criteria providing
multiplesystemfailuresareexpectedoverthelifeofthesystem.
The very high reliability of today’s hardware and operating systems gives us a reasonable
approximation for the number of failures that asystem willsuffer over its lifetime.This is because
thebulkofsystemfailurestodayarenotcausedbyhardwareoroperatingsystemfaults.Theyare
caused by operator errors, application program errors, and environmental faults. We know from
experiencethatindustry-standardserversseem tofailaboutonceortwiceper yearandthatfault-
tolerant servers seem to fail about once every four or five years. We might want to tune these
MTBF values; but as we shall see, these numbers will give us a basis to compare system costs
withdowntimeincluded.
For our benchmark purposes, the total cost of a system is its initial cost and its operating costs
plus its downtime cost over the life of the system. Let us call the initial cost and operating costs
the “system cost.” System cost includes the initial cost of the system plus the costs of
maintenance,operators,sites,energy,insurance,etc.,overthelifeofthesystem.
If we know the hourly cost of downtime and the system recovery time, we can calculate the
downtime cost of a single failure. For instance, if downtime cost per hour is $100,000, and if the
system recovery time is two hours, then the cost of a single failure is $200,000. We call this the
“failurecost.”
Thus,ifthereareNfailuresoverthelifeofthesystem,thetotalsystem costis
(totalcost)=(systemcost)+N*(failurecost)
Let us consider two differentsystems,Systems Aand B, withdifferentsystem costs and different
costs of failure. System A is relatively inexpensive but has a relatively long recovery time.
ComparedtoSystemA,SystemBismoreexpensivebuthasashorterrecoverytime.
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

Over the life of the system, there will be some number of failures for which the costs of the two
systems will be equal. We call this number of failures the failure index. Let’s calculate this
number.
Thefailureindexisthatnumberoffailuresthatmakesthetotalcostsequal.Settingthetotalcosts
forsystemsAandBequal,wehave
(systemcostA)+N*(failurecostA)=(systemcostB)+N*(failurecostB)
SolvingforN,wehave
(systemcostB)-(systemcostA)
N=failureindex=
(failurecostA)-(failurecostB)
N is the failure index for which we are looking. If there are N failures over the life of the system,
the total costs for System A and System B are the same. If the actual number of failures is
greater thanthefailure index,then System B, with its lower cost of failure, will be less expensive.
If the actual number of failures is less than the failure index, then System A, with its lower initial
cost,willbelessexpensive.
totalcost
choose
SystemA
SystemB
choose
System B
failureindex
SystemA
N
expectednumberoffailures
CalculatingtheFailureIndex
Consider two systems as follows over a five-year life span. System A is an industry-standard
server (ISS), and System B is a two-node cluster of industry-standard servers. For this example,
letusassumethefollowingsystemcostsandrecoverytimesfromasingle-nodefailure:
SystemCost RecoveryTime
SystemA-ISS $100,000 4hours(repairtime)
SystemB–ISSCluster $500,000 3minutes(failovertime)
Assuming a nodal failure rate of one failure per year, we would expect both System A and
SystemBtofailaboutfivetimesoveritsfive-yearlife.(ThoughSystemBhastwonodes,itisonly
the failure of the primary node that will take the system down.) Let us consider two cases for the
cost of downtime - $5,000 per hour and $100,000 per hour. The per-failure cost for each system
foreachofthesecasesis
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

DowntimeCost $5,000/hour $100,000/hour
Per-failurecost–SystemA $20,000 $400,000
Per-failurecost–SystemB $250 $5,000
Forthe$5,000perhourdowntimecost,thefailureindexis
500-100
N= 20
20-0.25
The number of expected failures for either system over the five-year system life is significantly
lessthanthefailureindex.Therefore,System Aisthesystem withtheleastexpectedcost.Atfive
failures, System A would cost (100,000 + 5 * 20,000) = $200,000 versus (500,000 + 5 * 250) =
$501,250forSystemBwithitstenfailures.
Forthe$100,000perhourdowntimecostcase,thefailureindexis
500-100
N= 1
400-5
Both systems will fail significantly more times than just once over their five-year life. Therefore,
System Bisthepreferredsystem.Atfivefailures,System Awouldcost(100,000+5*400,000)=
$2,100,000.SystemBwouldcost(500,000+5*5,000)=$525,000.Insummary,
Downtimecost $5,000/hour $100,000/hour
Totalcost–SystemA $200,000 $2,100,000
Totalcost–SystemB $501,250 $525,000
We have seen here that we can make good comparative judgments about the cost of downtime
withoutknowingexactlywhatthesystem availabilityis.Recoverytimeisagoodmetricforsystem
reliability. Even though System B costs five times as much as System A to purchase, it can be
muchlessexpensiveinthelongrunifdowntimecostsarehigh.
Summary
Too often, a manager is judged by his short-term performance. He may be held, for instance, to
quarterly budgets. Consequently, he may choose a system because of its lower initial cost and
ignore the long-term consequences of the cost of downtime. He is now a hero and leaves the
long-term damage to his successor. An availability benchmark would help to eliminate this
problem.
System reliability as measured by recovery time is only one of the parameters involved in a
system choice. Performance (which dictates a system’s size) and cost are other important
benchmarks. We submit that the cost of downtime is an equally important metric. The use of
recovery-timemeasurements incorporates the cost of downtime in the system choice. After all, in
many applications, it is the cost of downtime that is the predominant cost over the life of the
system. Perhaps equally important, it is recovery time upon which the users of a transaction-
processingsystemjudgeitseffectiveness.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman
