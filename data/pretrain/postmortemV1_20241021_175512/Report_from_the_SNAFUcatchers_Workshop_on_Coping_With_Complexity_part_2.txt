anomalies is a way of getting at the messy details that so often make
work hard ([Nemeth et al.
2004](http://snafucatchers.github.io/#ref_25)).

The research base for analyzing the \"Coping With Complexity\" project
described here is extensive. Its central tenet is the need to fully
understand how experts cope with the challenges that arise in running
dynamic process environments. All such environments generate new
instances of old problems and new types of problems that tax the
abilities of the people charged with keeping the processes going. A full
understanding of how experts cope with the complexity confronting them
is essential to engineering the processes to be more resilient. Doing
this often requires some form of process tracing ([Woods,
1993](http://snafucatchers.github.io/#ref_39)). Post-anomaly
investigations and formal postmortems rely on process tracing. The nexus
of process tracing as a means to understanding coping with complexity is
visible in Allspaw\'s [Trade-Offs Under Pressure: Heuristics And
Observations Of Teams Resolving Internet Service
Outages](http://snafucatchers.github.io/#ref_2) (2015).

Developing means to establish how people understood what was happening,
how they explored the possible sources, how they weighed alternative
corrective actions and made sacrifice decisions, how they deployed
resources, managed side effects, compensated for deteriorating
conditions, revised their problem understandings, and coordinated with
others is paramount if we are to enhance the resilience of these
important systems.

## 2.3 The above-the-line/below-the-line framework 

The framework for this report is a systems view of internet-facing
business information infrastructure.

A typical description of the \"system\" for this infrastructure is shown
in Figure 1. It includes internally-developed code, externally-sourced
software and hardware such as databases, routers, load balancers, etc.
and is provided as a product or service to customers, both internal and
external to the organization.

![](./stella_report_files/figure1.jpg){height="572" border="0"}

Figure 1. One view of The System

This mental picture is specific and contextual to the business. It
typically has lots of components, and while there might be similarities
to the components across organizations (databases, web applications,
published and acting APIs, etc.) they all hang together and interact in
specific ways that are unique to the business. This view gets a
significant amount of attention and focus from software engineers,
devops people, network and system admins and others.

![](./stella_report_files/figure2.jpg){height="357" border="0"}

Figure 2 - A more systems-oriented view of The System

This view of The System includes tools that teams use, modify, build,
maintain, update, and repair (Figure 2). These are instruments and
components used by engineers to manipulate the product or service that
customers use. The first view of The System (Fig. 1) is entirely
contained in this view. Added are deployment tools, confidence building
tools (code review, tests, etc.) as well as tools for monitoring,
observability, telemetry, alerting, etc. which are used to validate and
watch the behavior of the product or service.

![](./stella_report_files/figure3.jpg){height="513" border="0"}

Figure 3 - The System includes the makers, modifiers, watchers, and
compensators

All working business enterprises rely on people to build, maintain,
troubleshoot, and operate the technical components of the system (Figure
3). These people do the cognitive work needed to track the way these
artifacts function and how they fail, what is happening and what can
happen next, which risks are looming and which are receding, and what
changes are coming. All these facets are incorporated into an internal
representation that is sometimes called a \"mental model\" ([Woods et al
2010](http://snafucatchers.github.io/#ref_42), Chapter 6). Each internal
representation is unique \-- note that the \'models\' in the bubbles
above the silhouette heads above are similar in some respects but not
identical. Building and keeping current a useful representation takes
effort. As the world changes representations may become stale. In a fast
changing world, the effort needed to keep up to date can be daunting.

![](./stella_report_files/figure4.png){border="0"}

Figure 4 - An inclusive view of The System

Figure 4 shows another, more inclusive view of The System. The people
engaged in observing, inferring, anticipating, planning,
troubleshooting, diagnosing, correcting, modifying and reacting to what
is happening are shown with their individual mental representations.
These representations allow the people to do their work \-- work that is
undertaken in pursuit of particular goals. To understand the
implications of their actions requires an understanding of the cognitive
tasks they are performing and, in turn, an understanding of what
purposes those cognitive tasks serve.

The green line is the line of representation. It is composed of terminal
display screens, keyboards, mice, trackpads, and other interfaces. The
software and hardware (collectively, the *technical artifacts*) running
below the line cannot be seen or controlled directly. Instead, every
interaction crossing the line is mediated by a representation. This is
true as well for people in the using world who interact via
representations on their computer screens and send keystrokes and mouse
movements.

A somewhat startling consequence of this is that *what is below the line
is inferred from people\'s mental models of The System* (in the Figure 1
sense).

This is not to say that what is below the line is imaginary. But the
artifacts there cannot be perceived or manipulated directly. Instead,
people use mental models of what, although hidden, they infer must be
there to interpret what they see on the screens and to predict what the
effect of typing a character or clicking a mouse will be.

This framework may seem awkward. It appears to insert an unwanted
intermediary between us and the system that we all \"know\" is running
somewhere inside the computer. Reflection will demonstrate, however,
that Figure 4 shows what must be true: what lies below the line is never
directly seen or touched but only accessed via representations.

An important consequence of this is that people interacting with the
system are critically dependent on their mental models of that system
\-- models that are sure to be incomplete, buggy ([Woods et al.
2010](http://snafucatchers.github.io/#ref_42), page 104-5), and quickly
become stale. When a technical system surprises us, it is most often
because our mental models of that system are flawed.

Two broad challenges arise from Figure 4\'s representation (!) of The
System:

1.  Individuals in a variety of roles must somehow develop and maintain
    good enough mental representations of the technical artifacts to be
    able to comprehend and influence the behavior of those artifacts. In
    a changing world, their knowledge of what is below the line will go
    stale.
2.  Individuals must somehow develop and maintain a good enough
    understanding of how others understand the artifacts that they can
    cooperate.

# 3. Cases 

## 3.1 Catching the Apache SNAFU 

A software build process intended to bring up a new single server for
testing purposes failed because the version of Apache httpd included in
the system repository was incompatible with one of the new applications.
Furthermore, the default setting pushed an upgrade across the fleet,
instead of an isolated install on the single machine.

The Chef recipe for building a server was modified to force the newest
version to be pulled and installed. A regular 10 min Chef update process
found this new recipe and began updating the several hundred servers
running httpd. These new versions of Apache failed to start, leading to
degraded system performance.

The effect was recognized quickly and remedial restarts of httpd were
successful, although it took several hours to resolve all the effects of
the upgrade. The Chef rollout was staggered at 10 minute intervals to
avoid a thundering herd effect. This created additional pressure for the
team working to resolve the issue.

The performance was gradually declining because the rollout of the
non-working version of Apache was staggered but as time passed there
were fewer and fewer servers keeping the site up. The team were
confronting the question \"how many machines could we lose as we
diagnose the issue?\"

Post-event review showed that the system was viable during the anomaly
because, while a few servers did have automatic Chef update processes,
the updating processes themselves were broken, and therefore they
continued to run the old version of httpd. Recovery efforts were
complicated by loss of system monitoring tools that also depended on
Apache.

Package maintenance routines for the system repository, Chef recipes and
the Chef system, and the mistaken belief that installing a single server
could not have system-wide side effects interacted to produce the
anomaly. The irony that the system was able to \'limp along\' on a
handful of servers that continued to run because they were not
\'properly\' configured was not lost on the operators.

## 3.2 Catching the Travis CI SNAFU 

A distributed continuous integration (CI) service, Travis CI Enterprise,
used daily by hundreds of teams in the company to build and test
projects bogged down and became unresponsive. Several sources of this
behavior were considered and evaluated.

Eventually the investigations revealed that the asynchronous (RabbitMQ)
message server was failing. The failure triggered an alert that multiple
build worker nodes were unable to handle build jobs. The team began
investigating the issue by looking at the cloud console for stale or
terminated worker instances. A few were found and deleted but the
performance issues continued.

Eventually the troubleshooting group killed all the running virtual
machine instances and containers. The CI service was restarted. The
system then appeared to be stable with nominal performance. Examination
of the CI service logs suggested that worker container processes were
being killed. Increasing the quota regulating the number of concurrent
workers did not resolve the performance issue.

Users reported that builds were not restarting. At this point the team
noticed that the queue in the Travis CI messaging system for build
restarts was growing rapidly. Travis CI was restarted which seemed to
solve the issue. Shortly after, a second alert triggered for the same
initial problem (workers not working). The same steps were taken
(killing builders and workers in GCE and it improved the situation but
did not solve it.

It was then noticed that RabbitMQ had errors in the Travis CI worker
process logs as well as unusual behavior in other Travis CI internal
queues. Concurrently, the team noticed that the Database-as-a-Service
provider which hosts their RabbitMQ service had had a capacity-related
service incident in the same datacenter where their RabbitMQ service was
hosted. The console showed that the RabbitMQ instance was healthy and
the team attempted to confirm this with the Database-as-a-Service
provider, and they were assured the problems were not with the database
or messenger service. Convinced this was the source of their issues, the
team began planning to migrate to a new instance of RabbitMQ.
Ordinarily, the team accomplishes such configuration changes using
Travis CI. In this case, the team used a previously designated fall-back
mechanism to make the change since Travis CI was not functional.

This migration was successful and after processing backlogged queues the
system began running as expected. A support ticket had been opened with
the Database-as-a-Service provider but the team failed to receive any
feedback to aid their diagnostics or recovery.

## 3.3 Catching the Logstash SNAFU 

Startup of a normally reliable multiple server system was exceptionally
slow and even simple terminal commands (e.g. ls) entered on the primary
host took minutes to complete. Diagnostic exploration of the primary
server did not reveal unusual behaviors in that server or in the other
servers. The internal network appeared to be operating normally. That
network was reconfigured to permit starting the system via a backup
server. Initially the poor performance problem appeared to be resolved
but the backup too began to slow.

Contingency plans for a major outage were started. Further examination
of the primary server showed that a downstream ELK stack\'s Logstash
program was behaving erratically and only occasionally able to process
messages from the primary server. This lead to the primary server\'s
kernel TCP/IP buffer to fill, causing back pressure on the primary
system and causing the system logging facility to stutter.

This communication issue would not normally have had the impact it did
except for the fact that the primary server also had the 'snoopy'
keylogger application installed. The 'snoopy' keylogger was in place to
capture operator's commands in order to 1) produce a high-resolution
audit trail to be used for compliance purposes, and 2) provide timeline
data that can be reviewed during post-incident reviews.

This meant that every command executed on the server would hang as it
was intercepted by snoopy who then attempted to write to syslog. Snoopy
would cause the command to hang until a timeout threshold was reached
and the syslog write attempt was aborted.

Killing Logstash relieved the communications pressure and allowed the
primary server to complete the system startup. The possibility of a
downstream source of system \'constipation\' was not considered until
late in the anomaly response.

## 3.4 Observations on the cases 

The cases are from businesses whose primary activity is providing
information processing services. These businesses normally interact with
customers exclusively via their \'sites\' via network. Other than using
the internet for service delivery, the businesses and services they
supply have little in common; the computer languages, supporting
applications, organization, and even regulatory environments are
dissimilar.

Despite these differences, the anomalies and the reactions to them do
have common features.

### 3.4.1 Features of the anomalies 

-   The anomalies are examples of complex systems failures
    ([Cook,1998](http://snafucatchers.github.io/#ref_4)).
    -   Each anomaly arose from unanticipated, unappreciated
        interactions between system components.
    -   There was no \'root\' cause. Instead, the anomalies arose from
        multiple latent factors that combined to generate a
        vulnerability.
    -   The vulnerabilities themselves were present for weeks or months
        before they played a part in the evolution of an anomaly.
    -   The events involved both external software/hardware (e.g. a
        server or piece of application from a vendor) and on
        locally-developed, maintained, and configured software (e.g.
        programs developed \'in-house\', automation scripts,
        configuration files).
    -   The vulnerabilities were activated by specific events,
        conditions, or situations.
    -   The activators were minor events, near-nominal operating
        conditions, or only slightly off-normal situations.
-   The anomaly consequences cascaded over time.
    -   The consequences propagated across technical components and
        outwards to have direct and indirect impacts on customers and
