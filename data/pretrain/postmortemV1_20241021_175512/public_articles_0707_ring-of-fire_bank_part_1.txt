Ring-of-Fire Bank Beats Earthquakes with Active/Active
July2012
Not many banks avoided exposure to the recent subprime crisis and speculative real-estate mortgage
meltdown. One bank that did, due to its rational credit policies, remained the number one lender in its
area while other financial institutions severely restricted credit to their customers. The bank continues to
steerclearofmanyofthecreditchallengesfacingthefinancialindustry.
However, it was not as well prepared for Mother Nature. The bank is located on the Pacific Rim Ring of
Fire and lost its data center for several hours during an earthquake. When the failover to the backup
system didnot go as planned,the bank said,“Never again!” Itstarted onthe path tomove its datacenter
operationstoacontinuouslyavailableactive/activeenvironment.
The Bank’s Background
History
The bank was established over 150 years ago and has grown to the point where it now has over $15
billion USD in assets. It provides all of the traditional services to its customers, including checking and
savingsaccounts,creditcardsanddebitcards,mortgagefinancing,consumerandcommercialloans,and
online banking. Its conservative credit philosophy is derived from its parent bank, one of the top five
banks in the world, according to Forbes magazine. Standard & Poor’s ranks the parent bank sixth in the
worldintermsofcreditstrength.
DataCenters
The bank operates two HP NonStop data centers that are located about 50 miles apart. Prior to the
earthquake, one data center served as the production site and the other as a passive backup site. The
backup database was kept synchronized with the production database via a replication product that
supported only active/passive architectures in which the target environment had to be an exact mirror of
the source. Additionally, the replication product limited the target application to read-only operations.
Unfortunately, the third-party banking application used by the bank could not run in read-only mode.
Hence, testing of the target system required a complete outage of the source environment and the
consequentlossofapplicationservicestousersduringthetestperiod.
As many companies have discovered, testing failover to a backup data center is a risky, inconvenient,
and costly proposition. In order to properly test its backup system, the bank had to bring down its
production system, denying application services to its customers. Applications had to be loaded and
started on the backup system, and the user network had to be switched. Furthermore, the replication
product used by the bank did not maintain a transactionally-consistent copy of a production database on
the backup system during replication. Therefore, the bank’s backup database had to be brought into
consistencybeforethebackupsystemscouldbetestedandplacedintouse.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

The failover process is complex. Knowledgeable staff must be assembled, typically in the middle of the
night or over a weekend, to handle any problems that may arise. Failover, if successful, can take more
than an hour, during which time the applications are down. Even worse, it is entirely possible that the
failover itself will fail and that the backup system may be unable to be brought into service. This failover
faultdelaystheoverallfailovertestevenfurther.
Therefore, the bank followed what is unfortunately a common practice. Although it periodically performed
failover testing, its tests often ended up incomplete. The process to switch to the backup site and then to
switch back to the production site typically ran into many small problems that collectively added up to a
painful and slow sequence. Many failover tests simply did not complete or did not complete successfully
intheallowedtestingtimeframe.
In practice, many companies that rely on active/passive architectures confront the same issues. In the
end, they rely on faith and hope that their backup systems will come up in a reasonable amount of time
should the primarysystems fail. Even if a company’s data center is well-designed for redundancy and its
staff well-trained in the proper failover procedures, and the staff practices these procedures successfully
within a failover window that is acceptable to the business, other external factors such as the
management decision time to fail over often significantly extend the failover process. The result can be a
failoverthatevenifsuccessfulseverelyviolatesthedisaster-recoveryservicelevelagreement(SLA).
Mother Nature Strikes
One fateful day, the bank’s backup preparedness faced a crucial test. An earthquake struck and caused
the production systems and their networks to fail. The bank initiated its disaster-recovery plan. As might
have been predicted, the bank suffered a failover fault. It could not bring its backup systems into
operation. The most critical outages were those of its online banking services and its ATM/POS network.
WithnoATMsorPOSdevicesworking,muchofthearea’sretailactivitycametoahaltataterriblycritical
time since people needed to buysupplies and to take other actions to survive the damage caused bythe
earthquake.
The problem was further aggravated by the facts that the production system had lost power and that the
IT staff had been evacuated from the primary data center due to concerns about structural damage.
Hourspassedbeforethebank’sstaffcouldreentertheproductiondatacenterandbringuptheproduction
systeminordertorestoreATMandPOSservicestothecommunity.
The Search for Continuous Availability
This disastrous experience led the bank to realize that its approach to disaster recovery was
unacceptable. Failover was slow and unreliable and was difficult to test, and failover testing always
caused an application outage. Therefore, the bank initiated an in-depth study of where it currently was
andwhereitwantedtobe.
Where it wanted to be was obvious. The bank needed to eliminate unplanned and planned downtime for
its critical applications. It did not want its online banking services, its ATM and POS networks, and its
othercriticalservicesevertobedownagain.Itwantedthemtobecontinuouslyavailable.
The bank concluded that it had to eliminate the problem of failing over to a system whose operational
statemightbequestionable.Ithadtohaveabackupsystem thatwasknowntobeworkingandthatcould
betestedfrequentlywithoutimpactingtheapplicationusers.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

TheLimitationsoftheBank’sArchitecture
The bank came to understand that its backup approach was constrained by the data replication
technology that it had adopted and the network infrastructure it had deployed. Data replication is
fundamental to disaster recovery and high availability, as it is the mechanism that provides an up-to-date
copy of the production database at a remote site. Without a synchronized remote copy of the database,
therecanbenorecoveryfromalostproductionsystemsincethereisnodatafortheapplicationtouse.
Manyreplication products are available, and they offer different advantages. The problem the bank faced
was that it had chosen a replication product and network infrastructure that did not support the
functionality needed to ensure rapid and reliable failover. The limitations of the bank’s architecture
included:
 The replication product was very restrictive. It required the backup system to be configured
exactly the same as the production system. Any changes made to one environment could
cause replication outages if these changes were not also made to the other environment.
Many of the bank’s failover testing problems were caused by a change that had been made
totheproductionsystembutnottothebackupsystem.
 The target database provided bythe replication product was transactionallyinconsistent, and
the replication product prevented applications from opening the database in update mode.
Therefore, applications on the backup system could not be up and running with the database
mountedforfastfailover.
 The replication product provided only unidirectional replication. The bank could never move
to a configuration in the future in which both systems were actively processing transactions,
informing each other as to the database changes that they were making. Active/active
architectures or their lesser brethren sizzling-hot stand-by configurations, in which all
processing nodes are available to process transactions, are required to achieve continuous
availability.
The fact that applications could not be running on the backup system limited the bank to an
active/passive configuration, in which the backup system was idle except for being a replication target.
Therefore, failover testing was a lengthy process. The production system had to be stopped, the backup
database had to be brought into a consistent state, and reverse replication had to be configured. (This
optional but useful step allows the newly promoted node to send all of its changes back to the original
production node to simplify resynchronizing it.) The backup applications then had to be started, the
network switched, and the system tested before it could be put into service. This process typically
required several hours, usually in the middle of the night, during which time all application services were
unavailable.
Compounding this challenge was the fact that configuration errors could not be detected until the backup
applications wereupandrunning.If found,theseerrors hadtobecorrected;otherwise,thetesthadtobe
terminated. Between the complexity of the failover process and the problem of configuration errors,
failovernotonlywasdifficultandtime-consuming,butitalsowasunreliable.
These problems had an additional effect on recoveryfrom an unplanned outage such as the earthquake.
Becausefailover was solengthyandunreliable,the managementdecisiontofail over rather thantotryto
recover the failed production system was a difficult call and added additional time to the failover
sequence.
TheRoadtoAvailabilityImprovement
The bank decided that it had to move from disaster recovery to disaster tolerance. Disaster recovery
means that the IT systems recover from a disastrous event and continue operating, even if that requires
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
