Windows Server Failover Clustering
April2010
Windows Server Failover Clustering (WSFC) is the successor to Microsoft Cluster Service
(MSCS).WSFC andits predecessor,MSCS,offer highavailabilityfor criticalapplications suchas
email, databases, and line-of-business applications by implementing a redundant cluster of
Windowsserversthatprovideasingle-systemimagetotheusers.
MSCS has been Microsoft’s solution to building high-availability clusters of Windows servers
sinceit was firstintroduced withWindows NT Server 4.0.MSCShas beensignificantlyenhanced
and simplified and renamed WSFC with the release of Windows Server 2008. WSFC for
WindowsServer2008R2hasseenevenfurtherenhancementstoWindowsclustering.
What is a Windows Cluster?
Microsoftdefinesaclusterasfollows:1
“A failover cluster is a group of independent
computers, or nodes, which are physically clients
connected by a local-area network (LAN) or a
wide-area network (WAN) and that are
programmatically connected by cluster software.
publicnetwork
The group of nodes is managed as a single
system and shares a common namespace. The
clusterservice
group usually includes multiple network
connections and data storage connected to the NodeA NodeB
nodes via storage area networks (SANs). The private
network
failover cluster operates by moving resources
Application Application
between nodes to provide service if system 1 2
componentsfail.”
The nodes in a Windows cluster are Windows SAN
servers that are physically interconnected by a
redundant private network for node monitoring and
shared
failover. The nodes have access to acommonset of
storage
redundant disk resources through a storage area
network (SAN). The cluster service is the software that programmatically connects the nodes in
theclusterandprovidesasingle-systemviewtotheclientsthatareusingthecluster.
The clients are unaware that theyare dealing with a cluster. The cluster appears to them to be a
singleWindowsserver.Ineffect,theapplicationisrunninginavirtualserver.
1
FailoverClusteringinWindowsServer2008R2,MicrosoftWhitePaper;April2009.
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

An application runs in only one node at a time. However, the redundancy built into the cluster
provides protection against any single component failure. Should a server, communication link,
storage link, or application fail, the failure is automatically detected by the cluster service, which
will move the failed application to a surviving node. Users may experience temporary degraded
performancebutwillnotcompletelyloseaccesstotheirapplications.
All hardware used in a WSFC cluster must be certified by Microsoft in order to obtain Microsoft
support for the cluster. Certified hardware is listed in Microsoft’s Hardware Compatibility List
(HCL). Furthermore, Microsoft highly recommends that all nodes in a cluster be identically
configured.
Resource Groups
Fundamental to the operation of a cluster is the notion of resources and resource groups.2 A
resourceisahardwareorsoftwarecomponentthatis managedbytheclusterservice.Resources
include application executables, disks, logical storage units, IP addresses, network names, and
network interface cards (NICs). Every resource has a resource monitor that allows it to report its
statustotheclusterserviceandthatallowstheclusterservicetoquerytheresourcestatusandto
send directives to the resource to bring it online or take it offline.. The most important function of
the resource monitor is to monitor the health of its resource and to report health changes to the
clusterservice.
A resource group is the group of resources that comprise an application. It includes all resources
needed for an application. A resource group typically includes a set of application executables,
one or more logical storage units (identified via LUNs, or logical unit numbers), an IP address,
andanetworkname.ClientsknowtheapplicationonlybyitsIPaddressornetworkname.
The cluster service treats a resource group as an atomic unit. A resource group can only be
running in one node at a time. That node is said to own the resources of a resource group
currently assigned to it. A node can be running (can own) several resource groups at any one
time.Thatis,anodecanbesupportingseveralapplicationssimultaneously.
Failover
Shouldan application be impactedbya hardware or asoftwarefault, thecluster servicecantake
oneofseveralactions:
 Itcanattempttorestarttheapplicationonitsowningnode.
 Itcanmovetheresourcegrouptoanothernodeinthecluster.
 If the problem is a node failure, it can move all resource groups currently owned by that
nodetoothernodesinthecluster.
Eachapplicationcan have apreferencelist indicating inwhichnode itprefers torunandto which
nodes it should fail over in preference order. It also species dependencies, indicating for each
resource what other resources must first be available. When cluster service detects a failure, it
determines to which node to move a failed resource group based on several factors, such as
nodalloadandpreference. Theresourcesoftheresourcegroupbeingmovedarethenstartedon
thenewnodeintheorderspecifiedbytheresource-groupdependencies.
Whenanodeisrestoredtoserviceandrejoinsthecluster,allresourcegroupsthathavespecified
therestorednodeastheirpreferrednodearemovedbacktothatnode.
2ServerClusters:ArchitectureOverviewforWindowsServer2003,MicrosoftWhitePaper;March2003.
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Since clients know their application only by its IP address or network name, and since these are
the resources that are transferred to the new node upon failure, cluster component failures are
transparent to the clients. They simply keep on using the application even though it is now
running on a different node. One caveat is that session state and memory-resident application
state will be lost following a failover. Therefore, a cluster provides high-availability but not fault
tolerance.
Notethataresourcegroup canonlybeownedbyone nodeatatime.Thatmeans thatLUNs can
onlybe accessed byone node at a time. However, all nodes musthave a connection to all LUNs
that theymayhave to own following a failure. This requirement is satisfied byhaving all LUNs be
residentinthesharedstorageprovidedbytheSAN.
Though applications generally do not need to be modified to run in a cluster, “cluster-aware”
applications can often take advantage of additional facilities built into the resource monitors for
extendedhigh-availabilityandscalabilityfeatures.
Quorum
In addition to the resources that can be owned by nodes, a cluster has a veryimportant common
resource – the quorum. The quorum is a cluster configuration database that is hosted on shared
storage and that is therefore accessible to all nodes. The configuration database includes such
information as which servers are currently members of the cluster, which resources are installed
in the cluster, and the current state of each resource. A node can participate in a cluster only if it
cancommunicatewiththequorum.
Thequorumhastwomainfunctions:
Consistency
The quorum is a definitive repository of all configuration information related to the cluster. It
provides each physical server with a consistent view of how the cluster is currently configured. It
also provides the configuration information required by a node being returned to the cluster or by
anewnodebeingaddedtothecluster.
Arbitration
As in any multinode application network, a cluster is subject to the split-brain syndrome. If a
network fault breaks the cluster so that there are two or more isolated groups of nodes, and if no
action were taken, each of the isolated groups might conclude that it is the surviving remnant of
theclusterandwilltakeownershipoftheresourcegroupsownedbythenodesthatitconsidersto
have failed. Resource groups are now owned by multiple nodes in the cluster, leading to
database corruption as independent and uncoordinated updates are made to the databases of
theaffectedapplications.
Split-brainoperationmustbeavoided.Thisisafunctionprovidedbythequorum.Itwilldetectthat
the cluster has been broken and will select the surviving cluster according to majority. Majority
means that the surviving group of nodes selected to carry on the cluster functions must contain
more than half of the nodes configured for the cluster. If there are n nodes, the surviving group
must contain at least n/2+1 nodes. All of the other nodes will be removed from cluster
membership, and this new configuration will be noted in the quorum-configuration database. The
survivinggroupissaidtohave“quorum.”Ifnogrouphasquorum,thecluster isdown;anditmust
waitfornodestorejointhecluster.
This leaves the problem of a cluster with an even number of nodes. If the cluster is evenly split,
neither group has quorum; and the cluster is down. To avoid this, the quorum database can be
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

given a vote so that there are effectively an odd number of nodes, allowing a quorum to be
established.
The Cluster Service
Theclusterserviceisacollectionofsoftwarecomponentsthatrunoneachnodeandthatperform
cluster-specific activity. Cluster-service components interact with each other over the private
networkinterconnectingtheclusternodes.Thecomponentsincludethefollowing:
NodeManager
TheNodeManagerrunsoneachnodeandmaintains alistofallnodesthatbelongtothecluster.
It monitors the health of the nodes by sending heartbeat messages to each node. If it does not
receive a response to a heartbeat message after a number of tries, it multicasts to the entire
clusteramessagerequestingthateachmemberverifyitsviewofthecurrentclustermembership.
Databaseupdatesarepauseduntiltheclustermembershiphasstabilized.
