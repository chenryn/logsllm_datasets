## May 2021

## 10 

[05/10/2021]

Microsoft Azure Portal - Intermittent Portal Access Issues

Tracking ID: GVD7-RDZ


**Summary of Impact:** Between 15:24 UTC and 17:55 UTC on 10 May 2021,
customers may have experienced intermittent 500-level errors or an
intermittent latency when accessing the Azure portal. Azure services
were not affected. 

**Preliminary Root Cause:** The Azure portal frontend endpoints in the
US North Central region experienced an increase in CPU usage, causing
some instances to not serve traffic as fast as expected.

**Mitigation:** We rerouted traffic around the unhealthy region and
scaled out CPU resources in adjacent regions to handle the increase in
traffic.

**Next steps:** We sincerely apologize for the impact to affected
customers. We will continue to investigate to establish the full root
cause and prevent future occurrences. Stay informed about Azure service
issues by creating custom service health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation. 

## 4 

[05/04/2021]

Azure Speech Service - West Europe - Mitigated

Tracking ID: LLL3-LTZ


**Summary of Impact:** Between 06:45 UTC and 11:35 UTC on 04 May 2021, a
subset of customers using Azure Speech Service in West Europe may have
experienced failures with online transcription, batch transcription,
custom speech, and translation.

**Preliminary Root Cause:** We have determined that during recent
deployment a part of the code lost access to KeyVault, preventing the
App Service that Azure Speech Service is dependent on from running as
expected.\
\
**Mitigation:** We have restored the access to the KeyVault to mitigate
this issue and enable the App Service to run as expected in turn
bringing Azure Speech Service back to healthy state.\
\
**Next steps:** We will continue to investigate to establish the full
root cause and prevent future occurrences. Stay informed about Azure
service issues by creating custom service health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation.

## April 2021

## 30 

[04/30/2021]

Issues accessing Azure Portal - HTTP 500-level Errors / Performance
issues - Mitigated

Tracking ID: 0TK3-HPZ


**Summary of Impact**: Between 07:30 and 08:45 UTC on 30 Apr 2021, a
subset of customers may have experienced intermittent HTTP 500 errors or
general latency when trying to access the Azure Portal. There was no
impact to Azure services during this time and retries to the portal may
have been successful for some customers.\
\
**Preliminary Root Cause**: At the start of business hours in the UK
region, the Azure portal frontend endpoints in UK South began scaling up
their instances to support the daily traffic. Our initial investigation
show that the scaling process kicked in as expected but instances didn't
serve traffic as fast as expected, leading to degraded customer
experience.\
\
**Mitigation**: The issue was self-healed once the new instances were
able to service traffic. Even though our telemetry shows that the
traffic patterns for the duration of the incident are similar to those
observed during the past week, we provisioned additional instances and
also increased the maximum instance count to be used for future scaling
operations. \
\
**Next steps**: We will continue to investigate to establish the full
root cause and prevent future occurrences. Stay informed about Azure
service issues by creating custom service health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation.

## 20 

[04/20/2021]

RCA- Intermittent 503 errors accessing Azure Portal

Tracking ID: HNS6-1SZ


**Summary of Impact:** Between approximately 10:30 and 12:11 UTC, and
again between 13:49 and 14:09 UTC on 20 Apr 2021, a subset of customers
may have experienced intermittent HTTP 503 errors when trying to access
the Azure Portal. There was no impact to Azure services during this
time, and retires to the portal may have been successful for some
customers.\
\

**Root Cause:** The Azure portal frontend resources in UK South was
taken out of rotation for maintenance the previous day, at 2021-04-19
19:08 UTC. For operational reasons related to an issue with that
maintenance, the region was left out of rotation for a longer period
than anticipated. This shifted traffic from UK South to UK West. This
scenario was within acceptable operational limits, as the volume of
Azure Portal traffic for that part of the world was declining at the end
of the working day there.\
\
The next day, the increase in traffic cause our instances in UK West to
automatically scale-up, and it soon reached the maximum allowed number
of instances, and stopped scaling up further. The running instances
became overloaded, causing high CPU and disk activity, to a point where
the instances became unable to process requests and began returning HTTP
503 errors.\
\

**Mitigation:** At 12:11 UTC, we removed the region from the global
Azure Portal rotation to restore functionality. In tandem we also
scaled-out resources in other regions to ensure there was no impact
related to the load rebalancing.\
\
As part of the mitigation troubleshooting, the UK West instances were
initially reimaged and retuned to rotation, as the impact from the UK
south traffic was not fully understood, and thus it was believed this
would resolve the issue. They were briefly brought online at 13:49 UTC,
but the lack of scale caused a recurrence of the issue. UK West was
taken out of rotation again at 14:09 UTC. pending a full RCA review.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Return UK South and UK West to rotation with increased autoscaling
    limits. \[COMPLETED\]
-   Ensure autoscaling rules for adjacent regions are adjusted in the
    event of a region being taken out of rotation.\
-   Raise internal alerts to a higher severity to ensure an earlier
    response.\
-   Raise default thresholds for autoscaling to account for growth of
    Portal.\
-   Improve monitoring to take region out of rotation automatically
    (failures weren\'t consistent enough to reach the threshold for our
    alerts).\
-   Alert if a region is running at the maximum auto-scale limits.\
    \

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 1 

[04/01/2021]

RCA - DNS issue impacting multiple Microsoft services

Tracking ID: GVY5-TZZ


**Summary of Impact:** Between 21:21 UTC and 22:00 UTC on 1 Apr 2021,
Azure DNS experienced a service availability issue. This resulted in
customers being unable to resolve domain names for services they use,
which resulted in intermittent failures accessing or managing Azure and
Microsoft services. Due to the nature of DNS, the impact of the issue
was observed across multiple regions. Recovery time varied by service,
but the majority of services recovered by 22:30 UTC.

**Root Cause:** Azure DNS servers experienced an anomalous surge in DNS
queries from across the globe targeting a set of domains hosted on
Azure. Normally, Azure's layers of caches and traffic shaping would
mitigate this surge. In this incident, one specific sequence of events
exposed a code defect in our DNS service that reduced the efficiency of
our DNS Edge caches. As our DNS service became overloaded, DNS clients
began frequent retries of their requests which added workload to the DNS
service. Since client retries are considered legitimate DNS traffic,
this traffic was not dropped by our volumetric spike mitigation systems.
This increase in traffic led to decreased availability of our DNS
service.

**Mitigation:** The decrease in service availability triggered our
monitoring systems and engaged our engineers. Our DNS services
automatically recovered themselves by 22:00 UTC. This recovery time
exceeded our design goal, and our engineers prepared additional serving
capacity and the ability to answer DNS queries from the volumetric spike
mitigation system in case further mitigation steps were needed. The
majority of services were fully recovered by 22:30 UTC. Immediately
after the incident, we updated the logic on the volumetric spike
mitigation system to protect the DNS service from excessive retries.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Repair the code defect so that all requests can be efficiently
    handled in cache.
-   Improve the automatic detection and mitigation of anomalous traffic
    patterns.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey at
[https://aka.ms/AzurePIRsurvey](https://aka.ms/AzurePIRsurvey)
.

## March 2021

## 18 

[03/18/2021]

RCA - Azure Key Vault - Intermittent failures

Tracking ID: 5LJ1-3CZ


**Summary of Impact: **Between 23:00 UTC on 18 Mar 2021 and 02:15 UTC on
19 Mar 2021, a subset of customers experienced issues and/or encountered
error message \"InternalServerErrors\" when accessing their vaults in
West Europe and North Europe regions. These errors were directly
impacting customers performing operations on the Control Plane or Data
Plane for Azure Key Vault or for supported scenarios that used Customer
Managed Keys for encryption at rest for Azure resource providers, in
which case those resources were unavailable.

**Timeline:**

-   18 Mar 2021 23:00 UTC - First Impact Observed in West Europe
-   18 Mar 2021 23:10 UTC - West Europe Key Vault service fails over to
    North Europe
-   19 Mar 2021 00:00 UTC - North Europe Vaults impacted by same issue
-   19 Mar 2021 01:50 UTC - Mitigations completed by deploying new VMs.
    North Europe fully recovered
-   19 Mar 2021 02:15 UTC - West Europe fully recovered

**Root Cause (updated 27 Apr 2021): **Azure Key Vault\'s microservice
that handles storage transactions in the West Europe region was impacted
by network resource exhaustion that started at 18 Mar 2021 at 23:00 UTC.
This was triggered by a surge of requests to the Data Plane for a
specific type of resource which resulted in excessive operations to
access the backend storage and over-utilization of network resources as
a result of a code defect. The particular microservice was also
under-provisioned in the West Europe and North Europe regions and had
limited capacity to handle the increased load, which caused
exceptionally high CPU usage. This is typically prevented by caching and
service limits which will throttle the requests, but in this particular
case there was a gap in our cache implementation and the lower capacity
allocated to the service resulted in the incident in the West Europe
region. As a result of this, the service health monitoring automatically
failed over West Europe traffic to North Europe at 23:10 UTC. In North
Europe, the same conditions led to the service degrading and eventually
experiencing an outage on 19 Mar 2021 at 00:00 UTC.

**Mitigation:** As a first measure to remediate the situation,
underlying Virtual Machines (VMs) supporting Azure Key Vault were
rebooted. However, the CPU usage continued to be high in the VMs.
Engineers then deployed new VMs with higher capacity to handle the
increased CPU usage and redirected traffic to them. Once this was
completed both regions recovered. Also as a preliminary measure to
prevent recurrence in other regions, the capacity was reviewed and
increased globally.

**Next Steps (updated 27 Apr 2021):** We apologize for the impact to
affected customers. We are continuously taking steps to improve the
Microsoft Azure Platform and our processes to help ensure such incidents
