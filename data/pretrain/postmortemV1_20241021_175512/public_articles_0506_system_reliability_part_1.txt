What Is Reliability?
June2010
Andy Bailey, Stratus’ Availability Architect, started a very active thread entitled “What does fault
tolerant mean to you?” on our LinkedIn Continuous Availability Forum. Andy opened the thread
withtheobservation:
“I’velongbeenawarethatmisuseoftheterm‘FaultTolerance’(FT)islayingcompaniesopen
tobusinessriskandfinanciallossesfromsystemdowntime.”1
The range of responses support the general feeling that terms such as ‘fault-tolerant,’ ‘high
availability,’ ‘continuous availability,’ ‘mission critical,’ and others serve the purpose of the writer,
not the industry. Even characterizing system reliability in terms of ‘nines’ has its shortcomings
sincewhatisbeingincludedasdowntimemustbecarefullydefined.
Inthisarticle,wesuggestasimplemethodtoquantifysystemreliability,eliminatinganyambiguity
or suggestion of marketing intent in the above terms.Our result is a table that compares different
highly-reliable technologies for IT systems. We encourage you to challenge and to comment
aboutoursuggestionontheContinuousAvailabilityForumbyaddingtoAndy’sthread.
Slushy Terminology
Intheliterature,thetermsthatareusedtodescribecomputersystem reliabilityarewhatwemight
describe as “slushy.” There are no agreed-upon definitions – the user of such terms is free to
match them to his purpose, often with no attempt at a formal definition. There are certainly no
legaldefinitionsthatwouldstandupincourt.
Andythrowsdownthegauntletwiththefollowingpublishedstatement:2
“Fault tolerant computing is the ability to provide highly demanding enterprise application
workloads with 99.999% (five nines) system uptime or better, zero failover time and no data
loss.… 60% of IT users don’t understand the difference between availability, high availability,
and software-based fault tolerance, none of which meet the definition of fault-tolerance, and
hardware-basedfault-tolerance,whichdoes.”
Herearesomeoftheobservationspostedtothisthread:3
 [Continuously available] systems are available virtually all of the time – generally
99.999% of the time (about 30 seconds of downtime). Extensive use of independent
1SeeAndy’sAvailabilityAdvisorblogathttp://availabilityadvisor.com/.
2 Stratus Technologies takes zero tolerance stance on misuse of term ‘fault tolerant,’ November 14, 2009
(http://www.stratus.com/news/2009/documents/20091112.pdf).
3 Thanks to Randall Becker, George Ludgate, Tuomo Stauffer, Richard Buckle, and Moore Ewing for their comments
postedtothisthread.
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

components allows these systems to operate virtually without any unplanned outages.
Plannedoutagesdooccurforupgrades,butthewindowfortheseoutagesisverysmall.
 [A fault-tolerant system] is a system that can survive any single fault. Of course, that
bypasses the issue of what “survive” means. If you recover from the fault in five minutes,
doyousurviveit?
 I distinguish between fault-tolerant systems and continuous availability systems, such as
active/active systems. A true FT [fault-tolerant] system survives any single failure within
the system. A CA [continuously available] system provides continuous uptime to its
users, no matter what. The CA system may hiccup while it fails over, but if the hiccup is
so short that no one notices, the system can be considered to be continuously available.
The big difference is that a CA system keeps going, no matter what – even in the
presence of a site failure. An FT system is not inherently a CA system because it does
notprotectagainstasitefailure.
 A lot of my time is spent trying to understand the technical definitions companies are
using intheir literature.Do theymakethis easy–NO! …Wherearewein having precise
definitionsofouravailabilitytermsforhardware,softwareandentiresystems?
 It'samarketingterm anditdepends.Tandems(ok,HPNonStop)aregreatforrecovering
intransactionenvironments,reallytolerateafaultandcanbebuilt[to]tolerateevenmore
than two-way faults. But - fault tolerant can be thought many ways, what is a fault? We
lostsomedealsbecauseasecurityincidentwasthoughttobeafault…
 Gartner Group, The Standish Group and even Jim Gray back in the old Tandem days
included anything that can take a system down as a fault - from a hardware failure to an
applicationbug.
 The problem may be the short-term horizon managers now work under. In many
companies, the quarterly finances are far more important than the long-term effect of
decisions. … If I have a failure twice a year, and 90% of those are recovered within my
four-hour RTO [recovery time objective], an SLA [service level agreement] violation will
occur on the average only once every five years. I'll probably be long-gone by then,
promotedintohighermanagementbecauseImanagedcostssowell.
 Afaulttolerantsystem isonethatcansurviveasinglepointoffailure.Butshouldwecast
the net wider, and include any outage – planned and unplanned? … In the end, if you
think your system failed, then it did! If you believed you had an outage, you did!
Perception is everything. Users looking at a blank screen for any period other than a
screenrefreshcycle,arewitnessinganoutage,surely!
 In general I think there is consensus that as long as a 'fault' doesn't materially affect a
user then it could be interpreted as ft - to me this also includes no loss of any in-flight
data.
 It further comes back to the question of what is Fault Tolerant. In my opinion, FT means
that the corporation is insulated from any reasonable infrastructure fault (including
applications/ITofferingsin'infrastructure',likephones,andpower).
 An interesting pair of terms … is “fault tolerance” and “failure tolerance.” … Fault
tolerance[is defined]as “hardwarethatis built withredundantcomponents toensurethat
processing survives failure of an individual component.” … Failure tolerance [is defined]
as “an application that can continue even when failures such as node or site outages
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

occur.”Iinterpretthisasfailuretolerancerequiresfailovertoabackup,butfaulttolerance
doesnot.…Clearly,failuretolerancerequiresanadditionalparameterandthatisfailover
time.…Allthisleavesthequestionoftheeliminationofplanneddowntimeupintheair.
 No technology … can prevent stupidity, can fix bad design, can always work, is suitable
for everything … is the best … solution for everything, etc. But, again … anytime a
user/customer sees the system "down" - it is down, it doesn't matter where the problem
is.
 Most terms lack a legal, formal, tight definition and even when they do there is little the
owner can do if the public decide to use it differently. A working definition might be "a f-t
systemisonewithnoSPOFwhichcausesthesystemtoceasetofunctioncorrectly".Any
suchsystem will be composedof multiplesubsystemsallof whichmayuse verydifferent
techniques to remove SPOFs. The problem arises when technical terms become
marketingslogans.
 The most "legal" definition typically comes from the service level agreements. … In a
contract,onemight put "MissionCriticalis defined as being available x% of the timeover
tyears"toprovidethatdefinition.
 Fortermslike"MissionCritical"orfault-tolerant",Idon'tthink thereneedstobealegal,or
even a formal, definition. Neither has any place in the requirements specification of a
development. Both relate to the real requirement "Availability". One [mission critical] is a
vague description of an availability requirement, the other [fault-tolerant] of an approach
bywhichitwillbeachieved.
Thefinalcommentseemstosumupthesentiment.Reliabilitytermsinusetodayareeithervague
definitionsofreliabilityrequirementsorarevaguedescriptionsofareliabilityapproach.
How Do We Quantify Reliability?
We submit that ‘reliability’ is a measure of the user experience. Therefore, at the risk of
attempting to define terms that are not inherently definable, we specifyfor purposes of this paper
that ‘reliability’ is a measure of a user’s experience with respect to the provision of IT services to
him.Itisthismeasurethatwewanttoquantify.
RecoveryistheKey
ShimonPeresmadetheobservationthat
“if a problem has no solution, it may not be a problem, but a fact, not to be solved, but to be
copedwith …”
Failureisafact.Recoveryishowwecopewithit.
We are stuck with component failures. We can’t control failure rates (except to buy better
components). What we can control is how we get back into business when a component fails –
how do we recover from the fault? Reducing recovery times by a factor of ten has the same
impactonavailabilityimprovementasincreasingfailureintervalsbyafactoroften.
Therearetwoaspects totherecoveryof anIT system –recoveryof services and recoveryof lost
data. For these two aspects, there are well-defined terms in the industry to quantify them – RTO
(Recovery Time Objective) and RPO (Recovery Point Objective). Though technically they are
objectives to be set forth in an SLA, we can also view them as descriptors of system reliability.
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

RTO is the time that services are expected to be down. RPO is the amount of data that may be
lostandmustberecovered.
So why not characterize the reliability of systems by their expected RTOs and RPOs? True, one
couldstillplaywithfailurescenariosandchoosethosethatgiveabettermarketingimage.Butwe
can scope RTOs and RPOs to a fairly narrow range and thus obtain reasonable comparisons
betweensystems.
ButReliabilityisMoreThanRPOandRTO
If we accept that reliability is a measure of the user experience, simply measuring a system’s
