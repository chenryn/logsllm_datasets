The Great 2003 Northeast Blackout and the $6 Billion Software
Bug
March2007
August11,2003.TheBlasterwormwreakedhavocamongPCsaroundtheworld.Terrorism?
Three days later, on August 14, Northeast North America went dark. Was this a continuation of
theterroristcyberattack?Itcertainlylookedthatway.
However, as it turned out, the cause of the great 2003 Northeast Blackout was anything but
sinister. The Blackout was, in fact, triggered on a hot day by an untrimmed tree in Ohio and was
aidedbyahungalarmsystem.
Power Grids
Mostoftheworld’selectricgenerationsystemsbelongtosomesortofpowergridfromwhichthey
can draw power if their demand exceeds their capacity. In addition, theycan supply power to the
gridiftheyaregeneratinganexcessamountofpower.
A typical power plant generates electricity by coal-fired, oil-fired, or nuclear steam-generating
plants. The steam produced is used to drive motors, which in turn drive the generators that
produce electricity. (Other techniques involve driving motors with water power, wind power, or
geologicalthermalpower.Largesolararraysgeneratepowerwithoutmovingparts.)
highvoltage
transmissionline
substation
distribution
station
substation
motor g ae tn oe rr- substation
steam
boiler fuel
plant
ElectricalDistributionSystem
1
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

The power generated by the generating station is distributed via high-voltage transmission lines
tosubstationsaroundtheutility’sservicearea.Thesubstationsstepdownthehighvoltagetothat
required by the utility’s consumers and distribute this power to homes, businesses, and
industries.
A generation and distribution facility is connected to its neighbors to share in power demands
fromitscustomers.Thisinterconnectionformstheelectricalpowerdistributiongrid.1
Each power generating system is heavily protected
with devices such as circuit breakers, which can trip to
isolate failed or overloaded components such as
generators, transformers, or transmission lines. The
status of all components, including the state of the
circuit breakers, is monitored by sophisticated SCADA
(Supervisory Control and Data Acquisition) computer
systems. These systems are typically redundant
systems. In addition to status monitoring, they also
incorporate sophisticated models to predict the proper
responsetofaultconditions.
If all else fails, a power generating plant will go into “safe mode” to prevent overload damage. In
this mode, it can be shut down in an orderly manner. It typically takes a day or more to return a
generatingplanttoservice.
There are strict protocols in place which govern the procedures to be followed should a power
company encounter problems. Specifically, if theyhave to shut down a generating plant, whether
for planned maintenance or due to an unplanned failure, they must coordinate this action with
their neighboring grid members since this will impose additional load on their neighbors’
generating capabilities, which must be managed. If there is not good coordination, a failed
generating plant can overload a neighboring plant and take it down, which will overload its
neighbors and take them down, and on and on until there is a massive area electrical outage.
ThatiswhathappenedonAugust14,2003.
The All-Important Monitoring System
Our story centers on FirstEnergy, a major producer of electric power in Ohio. FirstEnergy uses a
Unix-basedredundantGEXA/21SCADAtransmissionmanagementsystem writteninCandC++
to control power generation and distribution via its high-voltage transmission system.2 The XA/21
provides reliable management of generated power in response to system disturbances. It
monitors system conditions against operating limits and develops corrective and preventative
strategies.
The XA/21 is a proven system with over 100 worldwide installations and millions of hours of
operationalexperience.Itisasystemrespectedbythepowergeneratingutilities.
The Cascading Power Failure
Power lines sag in hot weather.Theyalsosag duetotheheat generated bytheelectrical current
whichtheyarecarrying.Thehigh-voltagetransmissionlines(upto500,000volts) canblastatree
to its roots. This takes a tremendous amount of power and instantly overloads the transmission
1
ElectricalSystemOverview,GlobalSecurity.org;April27,2005.
2www.gepower.com.
2
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

lines.Therefore,itis imperativethattrees under these transmissionlines bekept trimmedsothat
theywillnotcomeincontactwiththetransmissionlines.
Thepolicyof FirstEnergywas totrim trees everyfive years.However,theydidnot always stick to
thisschedule;andtheresultwasthatsometreesunderitstransmissionlineshadgrowntootall.
August 14 was a hot day, pushing 90 degrees Fahrenheit in the Ohio area. Air conditioners and
fans were imposing heavy demand on generating capacity. Between the heat of the day and the
heat generated by the electrical current, transmission lines were seriously sagging. System logs
showedthefollowingsequenceoffaults:3
At1:31 p.m.,thepower generating plantatEastlake, Ohio,shutdown.This plant had ahistoryof
maintenanceproblems.
At2:02p.m.,thefirst345kilovolttransmissionlinecameintocontactwithatreeandfailed.
At 2:14 p.m., the GE XA/21 SCADA alarm subsystem failed. However, the SCADA system was
otherwise working; and the controllers were not aware of this failure. Therefore, it was not
repaired.
At 2:27 p.m., a second transmission line fell into the trees and was shut down. The controllers
received no notification of this nor of any of the problems which were to follow due to the errant
SCADAsystem.
At 3:32 p.m., power shifted by the first failure caused another transmission line to sag into the
treesandfail.
The power controllers were trying to understand these failures and failed to inform system
controllers in nearby states. Therefore, FirstEnergy’s power neighbors were not given the
opportunitytoplanforincreasedpowerdemandscausedbytheOhiofailures.
At3:39p.m.,anothertransmissionlinefailed.
At 3:41 p.m. and 3:46 p.m., two circuit breakers connecting FirstEnergy’s grid with the outside
world tripped while sixteen more transmission lines failed. This turned out to be the last chance
thatcontrollershadtosavetheirgridiftheyhadtheycutpowertoClevelandatthattime.
At 4:06 p.m., after another transmission line failed, an uncontrollable power surge was created;
and chaos ensued. Ohio drew two gigawatts of power from Michigan. By 4:10 p.m., the eastern
Michigan grid disconnected from the western part of the state; and Cleveland separated from the
Pennsylvania grid. Many more transmission lines failed in Ohio and Michigan, blocking the
eastward flow of power. Generators went down, creating huge power deficits. To compensate,
3.7 gigawatts of power surged from the East through Ontario to Michigan, a sudden tenfold
increaseinpowertransmission.ThistrippedtheEastCoastbreakers,andtheblackoutwason.
Within a minute, the flow reversed to send two gigawatts eastward from Michigan through
Ontario.Itflippedagaininjustahalfasecond.
By 4:10 p.m., international connections began failing. Western Ontario separated from the East,
and the first Ontario plants went offline in response to the unstable system. New York separated
fromNewEngland,OntarioseparatedfromthewesternNewYorkgrid,andthelastlinesbetween
MichiganandOhiofailed.WindsorandOntariodroppedoffthegrid.
3
NortheastBlackoutof2003,Wikipedia.
3
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

By 4:13 p.m., the cascading failures ended. 508 generating stations at 256 power plants,
including 22 nuclear power plants in the U.S. and Canada, had gone offline. Over 40 million
peopleintheU.S.andCanadawerewithoutpower.Thefinancialimpactranintobillions.Itwasn’t
untilthenexteveningthatpartialpowerwasrestored.
TheNortheastBeforethe TheNortheastAfterthe
Blackout Blackout
(NOAAsatellitephotographs.Theremaininglightsarecommunities,residences,and
businesseswithlocalpowergenerationcapabilities.)
This blackout was more than a lights-out inconvenience. Sewage spilled into waterways. Train
service in the Northeast Corridor, including those provided by Amtrak, Long Island Railroad, and
Metro-North, was shut down. Planes couldn’t fly because passenger screening equipment was
down, baggage couldn’t be delivered, and electronic ticketing systems could not be accessed.
Gas stations couldn’t pump fuel. Oil refineries shut down. Cell phones and laptops quit working
when their batteries ran out. Miners were marooned underground. The U.S./Canadian border
shut down because of the lack of electronic border check systems. The backup diesel generator
for the 1,900 room Marriott Hotel in New York wouldn’t start even though it had been tested
weekly. Guests had to walk down and sleep under the stars.4 It is thought that the Ontario
government fell in October elections because of the blackout. Financial losses were estimated to
besixbilliondollars.
The Monitoring System that Wasn’t
Allthis disaster took was ahotday,atalltree,andabalkymonitoringsystem.If anyoneof these
fault links had not occurred, the failure chain would have been broken; and the Northeast
Blackoutof2003wouldnothaveoccurred(atleast,notonAugust14).
Wecan’tcontrol hot days. FirstEnergycouldhavetrimmedtheir trees.But whydidtheredundant
GEXA/21systemfail?
As logs were reviewed, it was found that the first thing that had happened was that alarm
conditions were no longer being processed. This created an ever-growing queue of alarm
conditions awaiting processing. Response times for operator requests grew from one second to
oneminute.Withinthirtyminutes,thisqueuebecamesolongthatitcausedtheprimarysystem to
4
R.LaPedis,Lessonslearnedfromthe2003northeasternblackout,TheConnection;March/April2004.
4
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

shut down and pass control to its backup system. Equally overwhelmed by the large queue of
alarmevents,thebackupsystemitselfeventuallyfailed.
Where should one look to find the problem? There were four millions of lines of code in the
system, and millions of hours of operation had never induced this problem. GE Energy made the
trackingandsolutionofthisproblemitshighestpriority.
field
devices
power
controllers
GEXA/21SCADASystem
Thefocusofthecodesearchwasthemillionlinesofcodecomprisingthealarmsystem.5Ittook a
half-dozen experts eight weeks,buteventuallytheywereable torecreatetheproblem byslowing
the system way down. It turned out to be a race condition with an opportunity window measured
in milliseconds or less. Two programs were able to get write access to the same data structure
simultaneously. Their updates contaminated the structure. The result was that the alarm program
wasputintoaninfiniteloop.Itcouldneitherprocessalarmsnorreportthatitwasinfailure.
Because alarms could not be processed, the queue of events that had to be monitored for alarm
conditions could not be processed. The queue grew without limit, and after about thirtyminutes it
had consumed all available memory. At this point, the primary server crashed; and the backup
took over. It, too, was overwhelmed by the monster queue; and it crashed. At this point, the
operatorsrealizedthattherewasaproblemwiththeirSCADAsystem;butitwasfartoolate.
Thissoftwarebugcostpowercustomersanestimatedsixbilliondollars.
GEEnergyhassincesentapatchcorrectingthisbugtoallofitscustomersaroundtheworld.
Lessons Learned
There aremanylessons to be learned from this disaster. Some had to do with power generation,
butmanyhadtodowithdataprocessing,whichisourfocus.
SeparateYourBackupSystem
Perhaps the greatestlessonthat was learnedbythe2003NortheastBlackoutis thesamelesson
that was learned after 9/11. Provide a great deal of space between your systems, whether you
are running active/backup, active/hot standby, or active/active. If your active system was in New
York Cityand your backup system was inCleveland, you wereout of luck.If your backupsystem
wasinChicagoorSanFrancisco,youwereokay.
5
TrackingtheBlackoutBug,SecurityFocus;April7,2004.
5
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

How far is enough? One never knows. After 9/11, the regulatory bodies wanted to mandate a
minimum separation of 300 miles for financial institutions. This recommendation hasn’t been
rigorously followed and may not have been enough anyway to avoid an outage with a blackout
this size. The only thing that is certain is that the further your systems are separated, the safer
youare.
MindYourBackupPower
An alternative to system separation is to provide backup power at your sites. These UPS
(uninterrupted power supply) systems are usually diesel motor generators. However, there are
countless stories of UPS systems not starting. Though frequent testing of these systems is a
must, the Marriott experiencementioned above shows that even that maynot be enough. Others
have found that their diesel fuel has congealed if it is not replaced periodically. Still others have
foundthattheydidnothaveenoughfuelonhandtooperateforthedurationonthepoweroutage.
IftheNortheastBlackouthadlastedfordays,couldyougetyourgeneratorsrefueled?
Make sure that you have enough battery backup to carry your system through the minutes that it
may take to start a balky generator. Finally, with all of these problems, if your system is really
mission-critical and depends on diesel generator backups, consider having two such systems in
caseonewon’tstart.
Don’tTrustYourSystems
Another key lesson is “Don’t trust your systems.” In this case, the power controllers trusted their
systemstotheextentthattheyignoredphonecallsfromthefieldandothercenterswarningabout
worsening conditions in the field. As a result, they did not know of the extensive tripping of
electrical facilities in their grid; and they made no emergency alternative procedures to monitor
theirgrid.Evenworse,perhaps,wasthattheydidnotalerttheirneighboringgridsoftheproblems
theywerefacing.
Computers fail. Hardware fails. Software fails. Event notification fails. Failover fails. A computer
monitioringsystemisagreataid,andinmanycasesitisrequiredinordertobeabletoeffectively
run a facility such as a power utility. However, there must be procedures in place to alert
operatorstoafailureoftheirsystem independentofthesystem itself.Thatis,onecannotdepend
upon a sick system to tell you that it is sick. Furthermore, there must be procedures in place to
allowoperationstocontinueintheeventofamonitoringfailureortosmoothlyterminateoperation
ifnecessary.
SoftwareisNeverBug-Free
Another lesson is that software is never bug-free. Here is a system with millions of hours of
operational experience, and it still had a lurking bug in it. Is this the last bug? It reminds us of the
software bug detector once promoted. It is a little box with a green light that plugs into your
computer. When the last software bug has been fixed, the green light illuminates. Upon further
inspection,openinguptheboxfindsitempty–thegreenlightiswiredtonothing.
Accepting that software is never bug-free, it then becomes incumbent upon the programming
team towritedefensivecode.Defensivecodedefends againstsituations thatcan causedamage,
such as multiple write-access to the same data structure. This particular case is a good example
of the benefits of object-oriented programming. If the system data structures had been
encapsulated in an object which could protect its data, this kind of problem could have been
effectivelyeliminated.
6
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

Defensive code also continually checks the health of the system components. If there had been
builtintothesystem amonitorthatperiodicallyqueriedthevarioussoftwaremodules,suchasthe
alarm module, a lack of response would have identifiedthis problem almostimmediately. Though
the alarm system was looping, the Unix operating system provided occasional time slices to the
rest of the system (thus, the increase in operator response time from one second to one minute).
Thesystemcouldhavesoundedanalarm.
In a companion article, we described microrebooting6 as a technique for quickly repairing a
software system by restarting only the failed submodule. This is an excellent example of a
situationinwhichthistechniquewouldhavebeenuseful.
Postscript
As bad as this blackout was, it was not the worst that could happen. Just a month later, on
September 28 at 3 a.m., a larger blackout in Italy affected 56 million people for nine hours. This
blackout was unaided by computer error. It was caused when a storm took out two high-voltage
transmission lines feeding power to Italy from Switzerland. The cascading effect of this failure
causedthetransmissionlinestoItalyfromFrancetotrip,plungingItalyintodarkness.7
Within a year, other major blackouts occurred in London, Denmark and Sweden, Greece,
Bahrain,Chile,Croatia,andJordan.8Thesesupportthepredominantlessonlearnedasdescribed
above–separateyoursystemsforreliablebackup.
6
MicrorebootingforFastRecovery,AvailabilityDigest;March,2007.
7
2003ItalyBlackout,Wikipedia.
8
ResourcesforUnderstandingElectricPowerReliability,PSERC;undated.
7
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman
