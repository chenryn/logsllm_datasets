
Under normal circumstances when we apply HA and GA updates to a cluster,
the update takes many hours because we honor deployment availability
constraints called Update Domains (UDs). Instead of pushing the older HA
out using the standard deployment functionality, we felt confident
enough with the tests to opt for a "blast" update, which simultaneously
updated to the older version the HA on all servers at the same time.

Unfortunately, in our eagerness to get the fix deployed, we had
overlooked the fact that the update package we created with the older HA
included the networking plugin that was written for the newer HA, and
the two were incompatible. The networking plugin is responsible for
configuring a VM's virtual network and without its functionality a VM
has no networking capability. Our test of the single server had not
included testing network connectivity to the VMs on the server, which
was not working. Figure 4 depicts the incompatible combination.

![](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2012/03/2133.Outage4.webp)

*Figure 4. Servers running the incompatible combination of HA and HA
networking plugin*

At 2:47 AM PST on the 29^th^, we pushed the incompatible combination of
components to those seven clusters and every VM, including ones that had
been healthy previously, causing them to become disconnected from the
network. Since major services such as Access Control Service (ACS) and
Windows Azure Service Bus deployments were in those clusters, any
application using them was now impacted because of the loss of services
on which they depended.

We quickly produced a corrected HA package and at 3:40 AM PST tested
again, this time verifying VM connectivity and other aspects of VM
health. Given the impact on these seven clusters, we chose to blast out
the fix starting at 5:40 AM PST. The clusters were largely operational
again by 8:00 AM PST, but a number of servers were in corrupted states
as a result of the various transitions. Developers and operations staff
worked furiously through the rest of the day manually restoring and
validating these servers. As clusters and services were brought back
online we provided updates to the dashboard, and posted the last
incident update to the Windows Azure dashboard that all Windows Azure
services were healthy at 2:15 AM PST, March 1^st^.

### Improving the Service

After an incident occurs, we take the time to analyze the incident and
ways we can improve our engineering, operations and communications.  To
learn as much as we can, we do the root cause analysis but also follow
this up with an analysis of all aspects of the incident.  The three
truths of cloud computing are: hardware fails, software has bugs and
people make mistakes.  Our job is to mitigate all of these unpredictable
issues to provide a robust service for our customers.  By understanding
and addressing these issues we will continue to improve the service we
offer to our customers.

The analysis is organized into four major areas, looking at each part of
the incident lifecycle as well as the engineering process that preceded
it:

-   **Prevention** -- how the system can avoid, isolate, and/or recover
    from failures
-   **Detection** -- how to rapidly surface failures and prioritize
    recovery
-   **Response** -- how to support our customers during an incident
-   **Recovery** -- how to reduce the recovery time and impact on our
    customers

#### Prevention

-   **Testing.**  The root cause of the initial outage was a software
    bug due to the incorrect manipulation of date/time values.  We are
    taking steps that improve our testing to detect time-related bugs. 
    We are also enhancing our code analysis tools to detect this and
    similar classes of coding issues, and we have already reviewed our
    code base.
-   **Fault Isolation.**  The Fabric Controller moved nodes to a Human
    Investigate (HI) state when their operations failed due to the Guest
    Agent (GA) bug.  It incorrectly assumed the hardware, not the GA,
    was faulty.  We are taking steps to distinguish these faults and
    isolate them before they can propagate further into the system.
-   **Graceful Degradation.**  We took the step of turning off service
    management to protect customers' already running services during
    this incident, but this also prevented any ongoing management of
    their services.  We are taking steps to have finer granularity
    controls to allow disabling different aspects of the service while
    keeping others up and visible.

#### Detection

-   **Fail Fast.**  GA failures were not surfaced until 75 minutes after
    a long timeout.  We are taking steps to better classify errors so
    that we fail-fast in these cases, alert these failures and start
    recovery.

#### Response

-   **Service Dashboard**.  The Windows Azure Dashboard is the primary
    mechanism to communicate individual service health to customers. 
    However the service dashboard experienced intermittent availability
    issues, didn't provide a summary of the situation in its entirety,
    and didn't provide the granularity of detail and transparency our
    customers need and expect.  
    -   **Intermittent availability**:  This dashboard is run on two
        different internal infrastructures, Windows Azure and
        Microsoft.com, to deal with the catastrophic failure of either
        system.  It is also geo-replicated to deal with geographic
        specific incidents.  However, the dashboard experienced
        intermittent availability issues due to exceptionally high
        volume and fail-over/load balancing that was taking place.    We
        have taken steps to correct this and ensure more robust service
        in the future.
    -   **Situation summary**: The service dashboard provides
        information on the health status of 60+ individual services at
        the sub-region level.  While this is valuable in understanding
        individual service status, the lack of summary information made
        it difficult for customers to understand the situation
        holistically.  Customers have asked for a summarized view on the
        dashboard to quickly gain a comprehensive understanding of the
        scope and severity of the outage.  We are taking steps to make
        this change.
    -   **Detail and transparency**: Although updates are posted on an
        hourly basis, the status updates were often generic or repeated
        the information provided in the last couple of hours.  Customers
        have asked that we provide more details and new information on
        the specific work taking place to resolve the issue.  We are
        committed to providing more detail and transparency on steps
        we're taking to resolve an outage as well as details on progress
        and setbacks along the way.
-   **Customer Support. ** During this incident, we had exceptionally
    high call volumes that led to longer than expected wait times.  
    While we are staffed to handle high call volumes in the event of an
    outage the intermittent availability of the service dashboard and
    lack of updates through other communication channels contributed to
    the increased call volume.  We are reevaluating our customer support
    staffing needs and taking steps to provide more transparent
    communication through a broader set of channels.
-   **Other Communication Channels**.  A significant number of customers
    are asking us to better use our blog, Facebook page, and Twitter
    handle to communicate with them in the event of an incident.  They
    are also asking that we provide official communication through email
    more quickly in the days following the incident.  We are taking
    steps to improve our communication overall and to provide more
    proactive information through these vehicles.  We are also taking
    steps to provide more granular tools to customers and support to
    diagnose problems with their specific services.

#### Recovery

-   **Internal tooling. ** We developed and modified some of our
    internal tooling to address this incident.  We will continue to
    invest in our tools to help speed recovery and make recovery from
    intermediate states more predictable. 
-   **Dependency priorities.**  We are also examining our processes to
    make sure dependencies are factored into recovery to ensure that all
    Windows Azure infrastructure services, such as ACS and Windows Azure
    Service Bus, are recovered first to reduce the impact on customers. 
-   **Visibility.**  We are looking at how we can provide better
    visibility into recovery steps and provide customers with visibility
    into the intermediate progress being made.

#### Service Credits

Microsoft recognizes that this outage had a significant impact on many
of our customers. We stand behind the quality of our service and our
Service Level Agreement (SLA), and we remain committed to our
customers.  Due to the extraordinary nature of this event, we have
decided to provide a 33% credit to all customers of Windows Azure
Compute, Access Control, Service Bus and Caching for the entire affected
billing month(s) for these services, regardless of whether their service
was impacted.  These credits will be applied proactively and will be
reflected on a billing period subsequent to the affected billing
period.  Customers who have additional questions can contact
[support](https://azure.microsoft.com/support/?WT.mc_id=cmp_pst001_blg_post0072){target="_blank"
rel="noopener"} for more information.

### Conclusion

We will continue to spend time to fully understand all of the issues
outlined above and over the coming days and weeks we will take steps to
address and mitigate the issues to improve our service.  We know that
our customers depend on Windows Azure for their services and we take our
SLA with customers very seriously.  We will strive to continue to be
transparent with customers when incidents occur and will use the
learning to advance our engineering, operations, communications and
customer support and improve our service to you.

Sincerely,

Bill Laing and the Windows Azure Team


