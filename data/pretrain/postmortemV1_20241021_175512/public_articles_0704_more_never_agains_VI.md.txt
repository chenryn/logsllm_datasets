More Never Agains VI
April2012
The first quarter of 2012 has had its share of catastrophic outages. We have already reported on the
discovery of Oracle’s ticking time bomb,1 on the continuing string of outages suffered by Australia’s “big
four”banks,2andonthemultidayfailureofMicrosoft’s Azurepubliccloud.3Wesummarizebelowsomeof
theothersthathavemadeheadlinesinthepastfewmonths.
Zoho SaaS Service Downed for Ten Hours by Abrupt Power Failure
Over five-million users take advantage of SaaS-provider Zoho’s online collaborative, business, and
productivity applications; but applications were not available during the day on Friday, January 20,2012.
Zoho’s collocated data center, managed by Equinix in Silicon Valley, California, suffered an “abrupt”
power failure about 8AM PST.4This caused everydatabase cluster and everyserver to instantlyfail. The
manual effort to recover and test these systems overwhelmed its recovery team. Zoho runs over one-
hundred database clusters, and 40% of them had synchronization errors. Consequently, service was not
fullyrestoreduntilafter6PM.
An “abrupt” power failure evidently meant that many levels of redundancy failed. Specifically, the data
center’s UPS (uninterruptible power supply) did not kick in and give servers and databases time to shut
downgracefully.
Fortunately, no data was lost as a result of the outage. Zoho mirrors all data in a secondaryEquinix data
center in New York to protect against data loss. However, there is not enough capacity in the New York
datacentertorunallZohoservices.
Topreventsuchaprobleminthefuture,ZohoisinstallingitsownUPSandismovingtodatabaseservers
withinternalbatteriessothattheycanshutdowngracefullyevenintheeventofanabruptpoweroutage.
It is also considering increasing the capacity of its secondary data center so that it can take over
applicationservicesintheeventofatotaloutageofitsprimarydatacenter.
1Oracle’sTickingTimeBomb,AvailabilityDigest;February2012.
http://www.availabilitydigest.com/public_articles/0702/oracle_scn.pdf
2Australia’sPainfulBankingOutages,AvailabilityDigest;March2012.
http://www.availabilitydigest.com/public_articles/0703/australian_bank_outages.pdf
3WindowsAzureCloudSuccumbstoLeapYear,AvailabilityDigest;March2012.
http://www.availabilitydigest.com/public_articles/0703/azure.pdf
4OurFridayOutageandActionsWeAreTaking,ZohoBlogs;January23,2010.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Tokyo Stock Exchange Suspends Trading
Just before the market opened on Thursday, February 2, 2012, trading in 241 securities for 222
companies was halted at the Tokyo Stock Exchange (TSE), the world’s third largest exchange.5 The
suspension included blue-chip stocks such as those for Sony and Hitachi as well as other instruments
such as exchange-traded funds, real-estate investment trusts, and convertible bonds. Trading in all
suspendedsharesresumedfortheafternoonsession.
The outage hit during the middle of the earnings season, a particularly active timefor trading. February2
turnedouttobethethird-heaviesttradingdayoftheyear.
According to the TSE, one of the eight servers the exchange uses to distribute stock data failed, and its
backupdidnottakeover.Thefailureaffectedabout10%oftheexchange’s2,459listedsecurities.
In early 2010, the exchange replaced its aging trading system following a series of disruptions in 2005,
2006, and 2008. The new Arrowhead high-speed trading platform has had generally favorable reviews
fromtheinvestmentcommunity.Thiswasitsfirstoutagesinceitwentintoservice.
FOUR African Undersea Cables Chopped
Over the years, we have reported on many undersea data-cable breaks that have taken down Internet
accessfordays,particularlyontheAfricancontinent.IthappenedagaininFebruary,2012.Butthistimeit
was four cables cut in two different regions.6 Internet transfer rates were brought to their knees in nine
countriesinEasternandSouthernAfricaastrafficwasreroutedoversurvivingcables.
On February 15th, three cables in the Red Sea - the Europe India Gateway (EIG), the Southeast Asia
Middle East Western Europe-3 (SMW-3), and the Eastern Africa Submarine Cable System (ESSAY) -
wereallseveredatthesametime.Tendays later,onFebruary25th,it wassuspectedthatashipdropped
anchor of the coast of Kenya, cutting The East African Marine Systems (TEAMS) cable. TEAMS was
alreadycarryingmuchofthetrafficnormallyroutedoverthethreepreviouslybrokencables.
Withfourcablescutinsuchashorttime,thereweresuspicionsofsabotage.However,thethreeRedSea
cables lay in relatively shallow water about 650 feet below the surface and could have been cut by a
passingshipsimilartothefatesuspectedoftheTEAMScableoffKenya.Repairstook aboutthree weeks
beforefullInternetservicewasrestoredtotheaffectedcountries.
Server Provisioning Disabled in Ninefold Cloud
Ninefoldis amajor Australiancloudprovider.OnThursday,February17,2012,it sufferedamajor outage
thatcausedallvirtual-serverprovisioninginitscloudtobedisabledforfivehours.7
According to Ninefold, a Network File System (NFS) server experienced a failure; but it automatically
restarted and resumed normal operation in a few minutes. However, a number of physical host servers
with their virtual machines (VMs) that were performing operations with the NFS server became
unresponsive to further provisioning. Eventually, some customer VMs on these host servers became
unavailable;andtheycouldnotberestartedonotherphysicalhosts.
Though the failure did not affect themajorityof physical hosts or virtual servers, Ninefold determined that
themostexpedientwaytoreturnservicetotheaffectedcustomerswastoterminateprovisioningservices
for all customers. Consequently, no customer could provision new VMs for the five-hour period of the
outage.
5TokyoTacklesTradingGlitch,WallStreetJournal;February2,2012.
6EpicnetoutageinAfricaasFOURunderseacableschopped,TheRegister;February28,2012.
7Ninefoldoutagedisablesserverprovisioning,CRN;February21,2012.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Ninefold had previously suffered a host server outage in August of last year and another incident a few
monthsearlier inMay.Itis expectingtolaunchasecondavailabilityzoneinanotherdatacenterinMayof
2012 and is working out the details to see if the two data centers can be run in an active/active
configuration.
U.S. Defense Information Systems Agency Down Again
The U.S. Defense Information Systems Agency (DISA) provides IT services for the Department of
Defense(DoD)andassuchhasaresponsibilityforensuringnetwork reliability.Itwillsoonberesponsible
for hosting the U.S. Army’s entire email and collaboration system in a private cloud. However, DISA is
facingitsownresiliencyproblems.
On Thursday, March 1, 2012, DISA users in Washington, D.C. and in some Midwest regions lost access
to the Internet for three hours because of problems at three of the DoD (Department of Defense)
gateways that interface the DISA web site with commercial Internet access points.8 DISA worked with
commercialvendorsandpartnerstoreroutetrafficuntilthenetworkwasrestored.
Network issues are not new to government agencies. Last November, a Storage Area Network fault took
down DISA’s web site and blocked DoD personnel from accessing applications. It took five days to
resolvetheproblem.CIO.gov,thewebsitefortheWhiteHouse’sCIOCouncil,hadseveraloutagesinthe
pastfewmonths.InOctober,a series of faults with theUSAJobs.gov web site promptedaCongressional
investigation.
BATS Halts Trading in its Own Shares
BATS Global Markets is the third-largest equities exchange operatorboth in the U.S. and globally. In Europe,
BATSChi-XEuropeisthelargestpan-Europeanequitiesmarketoperator.
On Friday, March 23, 2012, BATS encountered a software bug that caused an infinite loop and made open
customer orders inaccessible for more than two hours for stocks with symbols in the range of “A” to “BFZZZ.”9
Not only did this halt trading in Apple (AAPL) and BATS, but BATS stock plunged from $16 a share to 2 cents
injustninesecondsduringitsIPOauction,BATS’long-awaitedinitialpublicoffering,whichhadjustbegun.
BATS resolved the problem by noon for all stocks except its own. BATS management determined that the
delayofmorethantwohourshaderodedinvestorconfidence,anditelectedtocancelitsIPO.
We talk about the penalties of outages in terms of cost of downtime, erosion of confidence of customers and
suppliers,andbadpublicity.Hereisanewanddevastatingcost–theabortingofanIPO.
California’s Child Services Loses 800,000 Social Security Numbers
The California Department of Child Services needed to test its ability to cope with a disaster. Part of the
test was to ensure that it could manage its data remotely. To run this test, it shipped data regarding
800,000 adults andchildrenfrom Sacramento,California, to Boulder, Colorado,onfour computer-storage
devices.Thetestwentwellbutendedupinadisasterofitsownmaking.
The storage devices were supposed to be shipped back to Sacramento via Iron Mountain at the end of
March; but since Iron Mountain did not offer air services, the devices were shipped by FedEx instead.
FedEx picked them up but never delivered them. Suddenly, names, addresses, social security numbers,
8DISASuffersAnotherNetworkOutage,InformationWeek;March5,2012.
9
BATSGlobalMarketsdoesdamagecontrol,doesn’tplannewIPO,TheBusinessJournals;March26,2012.
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

drivers’ licenses, health insurance providers, employers, and other information on parents and their
childrenweregone.10
According to a state official, the state’s belief is that the container holding the devices was not properly
secured and fell out during transport. So far, there has been no indication that any information contained
on the devices has been improperly accessed or used. Everyone who may possibly have been affected
by this incident has been notified; and credit reporting agencies, the state Attorney General, and the
state’sOfficeofPrivacyProtectionarecloselymonitoringthesituation.
Dutch Vodafone Network Destroyed by Fire
A fire in a Vodafone network facility in Rotterdam terminated cell-phone service for more than a million
customers for a week.11 The fire onWednesday, April 4, 2012, destroyed the local network infrastructure
and put 700 transmission towers out of commission. Mobile users were unable to place calls, send
messages,usevoicemail,oraccessmobileInternet.
Affected were customers in Rotterdam, The Hague, and surrounding urban areas. Vodafone, the largest
mobile-phone operator in the world and the second largest in The Netherlands, said in a statement that
restoringserviceafterthefirewasoneofthemostcomprehensiveandcomplexoperationsinitshistory.It
was not until the following Tuesday, April 10th, that Vodafone confirmed that its network was “almost
stable.”
Fortunately, no one was in the building at the time; so there were no injuries. The government plans to
hold talks with all of the country’s mobile operators for possible network sharing in the face of future
outagessuchasthis.
Summary
Softwarebugsseemtobethebigproblem inthisseriesofoutages.Oraclehaditsticking-time-bombbug.
Azurehadaleap-yearbug.BATSwentdownwithaninfinite-loopbug.
Equally important were recovery faults. At Zoho, the UPS failed following a power failure. At the Tokyo
Stock Exchange, a backup data-distribution server failed to take over. At Ninefold, host servers failed
followingtherecoveryofanNFSserver.
Theseoutages haveacommoncharacteristic –testing.Itseems thatnomatter howmuchsystem testing
we do, there are always remaining problems. How much testing is justified? That, of course, depends
uponthecostassociatedwithanoutage.
Thenthereweretheoutagesthatnotestingwouldhaveprevented–afireandaFedExloss.
10Misplaceddataleave800,000Californiansexposed,CNETNews;March30,2012.
11VodafoneNetworkInHollandAlmostStableAfterInterruptionCausedByFire,WallStreetJournal;April10,2012.
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com