technical artifacts themselves.

Postmortems are not magic. They can be done well. They can also be done
badly.^[6](http://snafucatchers.github.io/#foot_6)^ They can fail to
identify the important underlying factors, misdirect attention away from
sensitive topics, reinforce organizational boundaries, and undermine
efforts to improve. At their worst, postmortems can be reduced to
nothing more than formal, perfunctory performances that give the
organization a thin patina that deflects criticism and preserves the
status quo.

There is no consensus about what makes postmortems work or even what
approaches are most likely to lead to success. The presence of skilled
facilitators \-- most often people with technical chops who have devoted
time and effort to learn how to manage these meeting and have practiced
doing so \-- certainly contributes to success.

The story of an anomaly is often complicated and it is almost always
necessary to compress the narrative of the event in order to allow
enough time for comments and exploration by the attendees. It can be
hard to enumerate all the pathways that were explored and all the
choices that were considered. This compression cannot be made lossless
but it can preserve the more important signals present in the story.
Commonly the timeline contains more detail than the discussion addresses
and it offers opportunities for the participants to draw attention to
specifics that were not included in the initial presentation. The
timeline figures or tables often become iconic representations of the
event.

There is some risk that the discussion at a well-attended postmortem
will be dominated by a few speakers or will become a narrowly focused on
unproductive debate. Conversely, there is sometimes hesitation to
participate, especially when the underlying issues are obvious but
intractable, or because of fear of reprisal. Expert facilitators are
especially helpful in these situations by redirecting attention to
productive, learning-focused themes.

Management buy-in is important. Even so, the workshop participants were
unanimous that postmortems are essential contributors to their work and
produce technical and organizational improvement. There was also
agreement that preparation for and conduct of postmortems requires
significant effort and that there are few useful tools to aid those
preparing and presenting postmortems, let alone those attempting to
catalog and make available their results.

Participants expressed interest in making postmortems better, more
insight-generating, more easily accomplished, and more effective in
improving their organizational and technical environment. They also
noted pressures to reduce the resources needed for postmortems and
increase their \'value\' to the organization. The pressure to identify
and maximize \'value\' leads to efforts to reduce investment in
postmortems.

### 4.1.2. Social issues in postmortems: 

Because they involve detailed examination of events, the circumstances
that produced them, and the responses to them, postmortems may bring
sensitive, contentious, and organizationally dangerous issues out in the
open. Postmortems can reveal dysfunctions, poor performance, mixed
messages, conflicts between stated intentions and incentives, etc.

Although apparently technically focused, postmortems are inherently
social events. Especially for events with significant consequences,
there are incentives to direct attention towards some issues and away
from others. When large losses incur attention of senior management the
tenor and content of the postmortem may shift away from freewheeling
discourse to a more closed ended, narrowly technical discussion.
Postmortems may become "stage plays" intended to assert organizational
control, ratify management decisions, or localize and truncate the
inquiry into circumstances and contributors. In most cases, these shifts
are obvious to the more technically sophisticated staff. Repeated
experience with these manipulations can generate secondary learning from
events, i.e. learning that the organizational imperative is to maintain
face, to stave off inquiry into sensitive topics, and to avoid
entanglement with powerful outside entities.

Postmortems sometimes serve as demonstrations of due diligence. Such
demonstrations may be used to ward off outside attention and
intervention. Difficult (or dangerous) issues within the organization
are often not addressed or addressed by encoding social features as
technical ones. This is particularly true for efforts to produce social
control when the organization is in turmoil or disintegrating. The
resort to constructing policies and procedures is sometimes evidence of
this.

There have been many expressions of interest in the social and
psychological effects of post-anomaly reviews. Much of this interest
revolves around avoiding \'blaming\' the technical workers closely
associated with the anomaly. Facilitators acknowledge that their role is
to deflect criticism of individual performances and concentrate
attention on technical contributors. Ironically, there is much less
written about the technical aspects of post-anomaly investigation than
about the need to avoid \"blame and shame\" for individuals. This is one
indication of how fraught the post-accident setting is.

How does the learning from postmortems get spread across the
organization? In almost all settings that we know of, postmortem
processes are isolated and events are handled one-at-a-time and
independently of others. There is little opportunity for review of other
postmortems and reflection about the patterns across multiple
postmortems are distinctly rare. Some firms have libraries of
postmortems but there appear to be few people who have library cards and
even fewer prone to check out a volume and peruse it. In some settings,
this leads to large collections of inert knowledge. One person quipped
that the library of incidents is a write-only memory.

A related problem is the way that the learning from postmortems is
shared or not shared beyond the postmortem meeting itself. Learning is
truncated at organizational boundaries \-- at the departmental,
divisional, corporate, and enterprise boundary the postmortem results
become progressively more opaque and less useful. At the extreme, the
publicly available reports about events are pale and stale when compared
to what we understand to the many issues, problems, decisions, and
tradeoffs that led to those events. Whether this is an essential feature
of organizations is unclear but it is prominent wherever we look. Is it
possible to pool these experiences and the results of deep, incisive
examination of the anomalies?

We do not presently know how to prepare for and support distributed
postmortem activities. Postmortems are presently treated like
proprietary code. This suggests that there may be ways to play off the
open source movement and the public code repository theme. Perhaps it is
possible: play off of git and its prominence in code management, using
it to manage both the postmortem data and the discourse that constitutes
analysis and assessment of that data.

Investing in adaptive capacity is hard to do and even harder to sustain.
It is clear that organizations under pressure find it hard to devote the
resources needed to do frequent, thoughtful postmortems. Shortchanging
investments in adaptive capacity in order to devote more effort to
production may yield immediate benefits by taking on additional systemic
brittleness. This is one example of the kinds of tradeoffs that are
common in complex systems working settings (see also [Hoffman & Woods,
2011](http://snafucatchers.github.io/#ref_11)). \[See also the
discussion on technical and dark debt at
[4.6](http://snafucatchers.github.io/#4_6_Dark_Debt)\]

## 4.2 Blame versus sanction in the aftermath of anomalies 

The notion of a \'blameless postmortem\' has become popular in the
industry at the same time as \'accountability\' and \'just culture\'.
The rubric surrounding these topics can be difficult to parse. There is
some agreement that a critical but non-judgmental review of events can
produce useful insights. This is not the only function of after-accident
reviews. Legal and business issues are often entangled with anomalies
and anomaly response. Organizational needs dominate after events and
different firms approach events quite differently. The postmortems of
SNAFUcatchers partners differ
substantially.^[7](http://snafucatchers.github.io/#foot_7)^ A few
related observations are listed below.

1.  [Blameless]{.underline} and [sanctionless]{.underline} are often
    conflated. *Blame* is the attribution of an undesired outcome to a
    specific source, e.g. \"the picnic was ruined by the rain\". Blame
    implies a causal connection between the target and the outcome. A
    *sanction* is a penalty levied on a specific individual, e.g. \"I
    got a sixty dollar fine for parking too close to the corner.\"
    Organizations often assert that their reviews are \"blameless\"
    although in many instances they are, in fact, *sanctionless*. As a
    practical matter, it is difficult to forego sanctions entirely.
2.  [Accountability]{.underline} is often nice-speak for blaming and
    sanctioning. It\'s use signals organizational willingness to take
    action against people and that the blame and sanction are
    justifiable. As such it is a means for maintaining a benevolent
    appearance while retaining the authority to levy sanctions.
3.  There is a strong correlation between the severity of an outcome,
    blame, and sanctions. When severity is low it is easy to adopt a
    \"no blame\" stance; it is much harder to do this when the cost of
    an accident is high. Organizations rarely use the same processes for
    small and large consequence events. Frequent minor events are often
    handled by formal organizational structures, e.g. \"incident
    reporting\" or \"tracking\" systems. Major events are often handled
    separately, frequently under direct, high-level management
    supervision.
4.  Blaming persists because it satisfies a variety of needs. Fixing
    blame can be used to represent organizational diligence, especially
    when an event becomes public. Describing an event as caused by
    \"human error\" is organizationally useful because it localizes the
    fault in an individual and absolves the rest of the organization
    from responsibility (cf. Cook & Nemeth, 2010). Localizing cause in
    an individual also provides relief from the sense of precariousness
    that follows catastrophes (Cook & Woods, 2006).

The primary motivation for eliminating sanctions and reducing blame is
improving the quality of information about problems that would otherwise
remain hidden. A \"no blame\" approach to managing incidents and
accidents is predicated on the idea that the knowledge obtained from
open, rapid, and thorough examination of these events is worth more than
the gain from castigating individuals. Although many organizations claim
to be \"no blame\", creating a blame free environment remains for most
organizations an aspiration rather than an accomplishment.

## 4.3 Controlling the costs of coordination during anomaly response 

The SNAFUcatchers cases and many others show that controlling the costs
of coordination is a critical need during the high-tempo,
high-consequence conditions often produced by anomalies. An escalating
anomaly can outstrip the resources of a single responder quickly. There
is much to do and significant pressure to act quickly and decisively. To
marshal resources and deploy them effectively requires a collection of
skills that are related to but different from those associated with
direct problem solving. But to be effective, these resources must be
directed, tracked, and redirected. These activities are themselves
demanding.

As an anomaly response evolves it draws in more and more people.
Managing this crowd is sometimes difficult. Those involved in the
anomaly response face a quandary: The people joining the circle are
potentially valuable resources that might speed the diagnosis and repair
but the effort needed to bring them \"up to speed\" with what has
happened and what needs to be done takes attention away from going
further in diagnosis and repair. Especially if the anomaly cascades or
resists repair, failure to bring these new people fully into the
response can make for big problems. But few anomalies (none, actually)
announce how they will expand or remain! The dilemma facing those
already involved is whether they should stay focused on the anomaly in
order to maximize their chances of quick diagnosis and repair or devote
some of their effort to bringing others up to speed so that they can
participate in that work.

Beyond simply bringing new people \"up to speed\", coordination of work
is necessary and costly. It is common, for example, for individuals to
be tasked to examine something or do something. In one workshop case an
individual was tasked to manually kill processes being spawned by an
errant bit of code and ended up doing this by creating a script to look
for such processes and kill them. Having such a person available unloads
some of the necessary work but this unloading comes at the cost of
having to identify the task, selecting someone to do the task,
specifying what is to be done and, later, giving some attention to the
report back from that person. The overhead seems small compared to the
benefit obtained but this is precisely the point: It only makes sense to
assign tasks in this way for those tasks that are well bounded, can be
accomplished by an individual, and for which a suitable person is both
available and not already working on a higher priority task. Moreover,
distributing tasks in this way imposes the additional burden of keeping
track of progress on the tasks and the effects that this progress is
having on the management of the anomaly. The situation is made even more
difficult when the anomaly is developing over time. Shifts in the
pattern of failure may make a particular thread of activity superfluous
or even hazardous. This imposes additional workload on the parties.

Controlling the costs of coordination is both important and challenging.
The controlling the costs of coordination issue cuts across the entire
landscape of devops and complex systems. A few significant points are
discussed below.

### 4.3.1 Offloading work to low-tempo periods 

Anomalies tend to be sporadic and there are usually long periods in
between them. Although the tempo of activity during an anomaly can be
very high, the *average* tempo of work is
low.^[8](http://snafucatchers.github.io/#foot_8)^ This encourages people
to find ways to shift some of the costs of coordination to the non-busy
times. Maintaining call ladders that identify who to call for escalating
situations is an example. Effort spent during non-busy times can
sometimes pay off during the busy times by reducing the workload then.

By their own nature, however, high-tempo work situations tend to resist
such approaches. It is easy to imagine that one or another resource will
be useful during an unfolding anomaly and then to find that effort spent
on that resource does not pay off well. Building these sorts of
structures requires a good deal of knowledge about how high-tempo
situations unfold and what is likely to be useful during those periods.
Many aids are developed based on assumptions about how anomalies present
themselves and about what will be useful that later turn out to be
incorrect. Checklists and decision trees that seem crisp and clear in
the office may be unhelpful or even misleading during real events.

### 4.3.2 Providing expertise on demand 

The SNAFUcatchers consortium members reflect a range of approaches to
the general problem of providing access to expertise. Some have
operational requirements for on-site presence of the most expert
personnel during peak periods. Others use call-rota systems that bring
experts in \'cold\' when first responders determine that they need help.
SNAFUcatcher organizations confront the same underlying issue: They all
need to have some means to bring higher levels of expertise to bear on
difficult or escalating problems.

Routine problems can be screened or dealt with by people with less than
the highest levels of expertise. Fielding false alarms from Nagios,
responding to annoyance-level alerts (\"the XYZ disk is 80% full and at
the current rate of growth will be full within 30 days\"), identifying
transients as transients are things that can be readily handled and,
typically, are managed on the fly by first-level responders. More
challenging anomalies benefit from higher levels of expertise, often
deeper but sometimes broader. All the organizations have access to large
amounts of expertise and all of them draw regularly on this resource.

A relevant research finding is that to be immediately productive in
anomaly response, experts may need to be regularly in touch with the
underlying processes so that they have sufficient context to be
effective quickly. Easy problems are quickly solved without expert help.
The people who initially confront an anomaly investigate it and
intervene to the degree that they are permitted. In most cases the
anomaly is resolved.

Experts are usually called upon when the initial responses fail and so
experts typically confront difficult problems. Anomalies that persist
despite the initial response are qualitatively different \-- steps have
been taken, lines of inquiry pursued, diagnostics and workarounds
attempted. Coupled to an anomaly that is itself cascading, the
