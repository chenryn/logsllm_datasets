©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

SHA-1 seems to be the more predominant hash key in use today. It was designed by the U.S.
National Security Agency and was published as a federal standard for security applications and
protocols.5
To improve hashing efficiency, some deduplication products use a weak hash key to detect
potential duplicate chunks. If one is found, the more secure hash key is then used to determine
whether there is a match or not. This can reduce processing requirements significantly, as the
more secure hash key only has to be calculated for a subset of chunks rather than for every
chunk.
ServerDeduplication
Deduplication can occur at the receiving end (server deduplication), at the sending end (client
deduplication), or as a cooperative effort between the sender and the receiver (client/server
deduplication).
With server deduplication, the entire file is sent to the receiving end. There it is deduplicated
eitherviainlineprocessingorpostprocessing.
Server deduplication is typically done via a deduplication appliance, which is a self-contained
hardware system containing the deduplication and restoration logic and the disk storage for
deduplicated files. Use of such an appliance offloads the deduplication processing load from
existingsystemsandmakesdeduplicationtotallytransparenttotheapplications.
Manydeduplicationappliancesacceptdatastreamsfrommanysourcessothatdeduplicateddata
from several systems coexists in one place. Many appliances also support replication of the
deduplicated data to remote DR sites. This approach requires a fraction of the bandwidth that full
replicationwouldrequire.
ClientDeduplication
With client deduplication, the deduplication is done on the system hosting the primary database.
ThededuplicateddatacanthenbedirectlyreplicatedtoaremoteDRsite.Ifperformancepermits,
thededuplicated datacanalsoreplacetheoriginalfull copyof thedatafor application-processing
purposes.
Client deduplication is necessarily software-based. It has the advantage of eliminating the need
for a deduplication appliance, which represents a single point of failure in the data stream. Also,
licensing costs for the deduplication software may be less expensive than the cost of an
appliance.
However,deduplicationagentsonallsendingsystemsmustnowbelicensedandmanaged.
Client/ServerDeduplication
With client/server deduplication, the sending site and the receiving site cooperate to bring both
processing efficiency and network efficiency to deduplication. This method generally involves
deduplicationappliancesatboththesendingandthereceivingsites.
Rather than sending the entire file to the receiving site, with client/server deduplication the
sending site first calculates the hashes on each chunk of the data stream and sends the file as a
sequence of hash keys to the receiving site. The receiving site compares these hash keys to its
5ThenewerSHA-2generateshashkeysupto512bitsinlength.
5
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

hashkeytableandresponds witha list of hashkeys for which it has norecord. Thesendingsite
thenneedsonlytosendtheuniquechunkstothereceivingsite.
Processing efficiency is achieved since the hash computations are offloaded from the application
systems. Furthermore, the network is utilized efficiently since only changed data must be sent
(plustherelativelysmalloverheadofsendinghashkeys).
Another advantage of this approach is that data from many remote branch locations can use the
same deduplicated central storage site for backup and disaster recovery. Each location takes
advantageofthetotalityofdataacrosstheenterprisefordeduplication.
DataCollisions
A concern with data deduplication is that of data collisions. A data collision occurs if two different
chunks generate the same hash key. In this case, the database is corrupted and may not be
easily recoverable without going back to a previous backup. The problem is that the corruption
may not be detected until the data is restored, which could happen a long time after the
corruptionoccurred.
The bigger the database and the higher the rate of change, the more likely data collisions
become. The key to minimizing data collisions is a very large hash space. The more hash keys
thereare,thelesslikelyaredatacollisions.
A feel for the current state of this technology can be obtained by considering the 160-bit key
generated by the SHA-1 algorithm. This generates 2160 = 1.5x1048 unique hash keys. This is a
prettybignumber.
In Appendix A, we look at the collision rate for a petabyte database that is being changed at a
rate of one-million updates per second. We find that we can expect a collision about once every
billion trillion years. This is a pretty long time; the age of the universe is only about 14 billion
years.
We can expect a virtually uncountable number of total disk-system failures before we ever see a
deduplication data collision. However, that being said, the probabilityof a data collision is still not
zero.
CompressionandEncryption
Both data compression and data encryption can be used with data deduplication. However, the
relationshipiscritical.
Compression tends to eliminate similar sequences in the data. Therefore, a compressed file may
not fare very well under deduplication. For this reason, compression should be applied to the
deduplicatedfile,nottheoriginalfile.Typicalcompressionratiosforadeduplicatedfiletendtorun
inthe2:1range.
Similarly, the purpose of encryption is to completely randomize a file. Therefore, deduplication is
not effective on an encrypted file. Encryption should be made on the deduplicated file, not the
originalfilebeforededuplication.
Cost
Of course, the cost of deduplication appliances varies with the vendor. However, in 2008, the
typical cost for a deduplication appliance was reported to be about $1 per deduped gigabyte of
6
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

storage.6 Twenty terabytes of deduped storage cost about $20,000. At a 20:1 compression ratio,
thisisequivalentto400gigabytesoforiginalundeduplicateddata.
A Little Deduplication History
The concept, and indeed the technology, of data deduplication is not new. An early form of
deduplication is the data compression methods that have been around for decades.
Deduplication by duplicating only files that have changed (single-instance storage, or SIS) has
beenaroundforawhile.
Probably one of the first instances of data deduplication as we know it today is the Unix utility
rsync. Introduced in 1996, rsync minimizes transmission bandwidth requirements. It uses a 128-
bit hash key provided by the MD4/MD5 hash algorithm and what has been described above as
client/server deduplication. Though it only acts on a file-by-file basis (not the entire database), it
sends the hash keys of a file to be transferred to the remote server. The remote server responds
withthelistof hashkeys thatitdoes nothave inits hashtable,andthenonlythe newchunks are
sent.
Implementations
Many deduplication products exist in the market today – just Google “data deduplication.” Some
examplesare:
HP’sStoreOnce
IBM’sProtecTIER
EMC’sDataDomain
Quantum’sDXiseries
FalconStor’sVirtualTapeLibrary
Thisisjustasmallsamplingofavailableproducts.Whitepapersfrom thesevendorsprovidedthe
informationforthisarticle,alongwiththeWikipediasubjectspreviouslyreferenced.
Summary
Data deduplication is an old technologythat is just now going mainstream. Usedwith appropriate
data stores, deduplication can significantly reduce the amount of disk storage needed for
disaster-recoverydatabases and for near-term archiving of database backups. It can also reduce
thenetworkcapacityrequiredforreplicatingnonrelationaldatabases.
Deduplicationdoesnotreplacediskstorageforonlinedatathatisneededtosupportapplications,
nor does it replace magnetic tape for long-term archival of point-in-time backups. However, used
properly, it can reduce the data-center footprint of data-storage subsystems with the resultant
reductioninthecostsofhardware,power,cooling,anddata-centerspace.
6DataDe-duplicationforDummies;WileyPublishing,Inc.(forQuantumCorp.);2008.
7
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Appendix A– Data-Collision Rates
Aconcern withdatadeduplicationis datacollisions,in whichtwodifferentchunksresolveintothe
same hash key. The minimization of data collisions depends upon a sparsely populated hash
space in which only a small proportion of all hash keys actually reference a chunk. The more
sparsethehashspace,thelesslikelyitisthattwochunkswillresolvetothesamehashkey.
The popular hashing algorithm for data deduplication is SHA-1, which generates a 160-bit hash
key.Thereare2160=1.5x1048possiblehash-keyvaluesforthisalgorithm.
Consider deduplicating a one-petabyte (1015 bytes) database that is being updated one-million
times per second.Furthermore,assumethatbackups thatareone-year oldare rolledtotapeand
deletedfromdisk.Whatisthedata-collisionratethatoccursduringdeduplication?
Assumethatthechunk sizeis 4,000 bytes.Thenumber of uniquechunks inthe database is then
1015/4,000=2.5x1011chunks.
Furthermore, new chunks are being generated at a rate of 1,000,000 (106) chunks per second.
Taking the worst case in which each new chunk will be referenced and therefore must be stored,
there will be (106 chunks per second)(3.2x107 seconds per year) = 3.2x1013 new chunks created
per year which must be stored before they are deleted following a tape backup.7 Thus, the total
numberofchunksstoredisapproximately2.5x1011+3.2x1013=3.2x1013.
Theprobabilitythatahashkeywillbeusedis
probabilitythathashkeyisused=(numberofchunks)/(numberofhashkeys)
=3.2x1013/1.5x1048=2.1x10-35
This is also the probability that a new chunk will cause a data collision. If chunks are changing at
arateof1,000,000(106)chunkspersecond,therateofcollisionsis
collisionrate=(chunkchangerate)(probabilityofadatacollision)
=(106)(2.1x10-35)=2.1x10-29collisions/second.
Theyearlycollisionrateis
collisionrate=(2.1x10-29collisions/second)(3.2x107seconds/year)
=6.7x10-22collisionsperyear.
Thetimebetweencollisionsisthen
timebetweencollisions=1/6.7x10-22=1.5x10-21years.
Thisisacollisionabouteverybilliontrillionyears.
7Notethatthisis100timesthesizeofthebasicdatabase.Thisis,ofcourse,astrangeexample.Ifthededupeddataover
ayearis100timesthesizeofthebasicdatabase,itmaybebettertosimplystoreweeklyfullbackups.Thiswouldrequire
onlyfiftytimesthestoragespaceoftheoriginaldatabase.However,thisexampleservestoillustratethepointthatdata
collisionswillhardlyeveroccur.
8
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
