Salesforce Takes a Dive
July2016
In May, 2016, Salesforce suffered an outage of a portion of its SaaS (software as a
service) cloud that lasted almost six days. This failure probably ranks as one of the
worstcloudfailuresever,beatingtherecentcloudfailuresofsuchgiantsasAmazonandMicrosoft.
The root cause of the outage was multi-faceted. The failure of a circuit breaker in its Washington data
centerwascompoundedbyalatent,unknownbuginitsstoragearraysinitsChicagodatacenter.
Salesforce’s recovery was hampered by an inability to fail over from it Washington data center to its
backup data center in Chicago. We have noted many times in the Digest that failover to another data
center is an iffy process if it has not been thoroughly tested. It appears that Salesforce did not give
failovertestingtheattentionitdeserved.
Salesforce
Salesforce was founded in 1999. It is a cloud computing company headquartered in San Francisco,
California, U.S.A. It provides software as a service to its customers via its CRM (customer relationship
management)product.
CRM provides the facilities that companies need to analyze and manage customer interactions and data
throughout the customer lifecycle. Its goal is to improve business relationships with customers, assisting
in customer retention, and driving sales growth. CRM systems can provide customer-facing staff with
detailedcorporateinformation,personalinformation,purchasehistory,buyingpreferences,andconcerns.
Salesforce Instances
Salesforce groups customer systems together in instances that link data centers and databases together
in organized groups. There are about three dozen instances serving North America (NA instances), eight
in Europe (EU instances), five in the Asia-Pacific region (AP instances), and 50 other “sandbox” (CS)
instances.
The NA14 Instance Outage
On Monday, May 9, 2016, at 5:46 pm PDT time, the Salesforce technology team noticed a service
disruption of its NA14 instancethatprimarilyservedcustomers ontheU.S. west coast.Customers onthe
NA14instancewereunabletoaccessSalesforceservices.
TheFirstRootCause–AFaultyCircuitBreaker
The problem was traced to a power failure caused by a faulty circuit breaker in the Washington, D.C.
(WAS)datacenter.
1
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Salesforce uses redundant intelligent circuit breakers in its data centers to segment power from the data
centeruniversalpowersupplytodifferentrooms.Oneofthesecircuitbreakersfailed.However,thefailure
created an uncertain power condition. The backup circuit breaker could not confirm the state of the
problembreaker,andthisledtotheredundantbreakernotclosingtoactivatethebackupfeed.
TheSecondRootCause–AnInoperableBackupSystem
In order to restore service as quicklyas possible, the Salesforce technical team decided to perform a site
switch, moving the active NA14 instance from theWAS data center to its Chicago (CHI) data center. The
switchover was completed at 7:39 pm, and service for the NA14 customers was restored. The technical
teamthenstartedtorecovertheWASNA14instanceviaalocalbackup.
The NA14 instance performed properly through most of the night. But at 5:41 am on May 10th, the team
noticed a degradation in performance of the NA14 instance. At 6:31 am, the degradation escalated to a
servicedisruptionastheresultofadatabaseclusterfailureontheNA14instanceintheCHIdatacenter.
The database failure resulted in file discrepancies in the NA14 database in the CHI data center. These
discrepancies were replicated to the WAS database, thus corrupting it. All attempts to repair the
discrepancies in the CHI data center failed. Salesforce was now in a position that it could not use the
NA14 instance in either data center, and it could not update the WAS data center from the CHI data
center.
TheSalesforceteam wasabletorestoretheCHIdatabase.However,theNA14instancecontinuedtorun
intheCHIdatacenter ina degradedstatethroughWednesday,May11.Theteam haltedseveralinternal
jobsandstaggeredinitialcustomeractivitycomingintotheinstancetotrytosmooththeworkload.
Withhelpfrom thestoragearrayvendor,anunknownfirmwarebugwasfoundinthestoragearray.It was
exposed by the increased traffic volume coming into the array from the backlog of customer traffic that
had built up during the time of the initial disruption. The bug increased the time to write to the array. As a
consequence, the database experienced timeout conditions, and database writes were unable to
completesuccessfully.
This ultimately caused a file discrepancy in the database, and the database cluster failed and could not
be restarted. It was these file discrepancies that been replicated to the WAS database before the CHI
databasefailed,makingtheWASdatabaseunusable.Atthispoint,neitherNA14instancecouldbeused.
The Salesforce team decided to restore the WAS data center from a local backup of the NA14 instance.
However, all data that had been entered by customers since the switchover to the CHI data center was
lost.
The team used redo logs from the CHI data center to replay the lost data at WAS. Unfortunately, this
actioncouldnot becompletedbeforethestartof peak customer activityonWednesday, May11th.Rather
than impacting another day of customer activity, the team decided to halt the replay of the redo log. All
datafrom2:53amto6:29amonMay10thwasnotappliedtotherecoveredNA14instance.
OnThursdaymorning,May12th,Salesforcepostedamessagetoitsstatuswebpagesaying:
“The NA14 instance continues to operate in a degraded state. Customers can access the Salesforce
service, but we have temporarily suspended some functionality such as weekly exports and sandbox
copyfunctionality.”
“The service disruption was caused by a database failure on the NA14 instance, which introduced a
file integrity issue in the NA14 database. The issue was resolved by restoring NA14 from a prior
backup, which was not impacted by the file integrity issues. We have determined that data written to
theNA14instancebetween9:53UTCand14:53onMay10,2016couldnotberestored.”
2
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Salesforce CEO Marc Benioff apologized for the outage on Twitter, providing customers with his email
address.
All activity was finally restored on Sunday, May 15th, almost a week after the initial problem was
discovered.
Lessons Learned
The initial cause of this outage was the failure of a redundant intelligent circuit breaker in the WAS data
center. However, the service should have been restored by switching over to the CHI data center. The
outageshouldhavelastednomorethanthetwohoursrequiredtobringuptheCHIdatacenter.
However, a firmware bug in the CHI data center corrupted the CHI NA14 database and then crashed it.
This corruption was replicated to the WAS database. Now neither NA14 instances could be used. It took
daystoreturntheNA14instancetofullservice.
AswehaveemphasizedinseveralDigestarticles,youcan’tcountonthesuccessofafailoverunless you
have thoroughly tested it. This includes testing the backup system under full load. This clearly was not
done by Salesforce; otherwise, they would have found the latent firmware bug. Failover testing is a risky
and expensive activity. However, it is certainly better than taking a multi-day outage. Salesforce’s
experienceprovesthispoint.
Postscript
Salesforce later notified its customers that it had been able to restore the lost transaction data from the
timebetween2:53am and6:29am onMay10from aseparatecopyoftheNA14 instance.Thisdatawas
availabletocustomersuponrequestsothattheycouldmanuallyre-enterthedata.
Acknowledgements
Informationforthisarticlewastakenfromthefollowingsources:
SalesforceoutagepersistsacrossUS,CEOwadesin,ZDNet;May11,2016.
Salesforce experienced an outage and service disruption to the NA14 instance, sending customers to
Twitter to complain and organizations to evaluate the best way to work with cloud software providers,
InformationWeek;May12,2016.
RCMforNA14DisruptionofService,Salesforcedocument;May16,2016.
CircuitbreakerfailurewasinitialcauseofSalesforceserviceoutage,ZDNet;May18,2016.
3
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com