How Does Google Do It (part 2)
November2012
Google has transformed our relationship with information. No longer do we go to the library to do our
research or consult an encyclopedia. We type in a query for Google and instantly (often before we have
even finished typing our query), Google gives us a long list of postings that can be found on the Web.
True, this information has not been vetted; so we have to be careful about its accuracy and authenticity.
But this has not slowed our adoption of the new, powerful information access technology that has rapidly
evolvedoverthelastdecade.
Today, Google indexes twenty million web pages every day to support its searches. It handles three
billion daily search queries. It provides free email to 425 million Gmail users. This level of activity is not
supported by your everyday data center. It requires a massive network of close to a million computers
spreadallaroundtheworld.
HowGoogledoesthis has beenacloselyguardedsecret,andmuchofitstillis. WereportedonGoogle’s
technology for massive parallel computation in an earlier Availability Digest article entitled “How Does
GoogleDoIt?”1
Google has now opened its kimono a little bit more. Strongly committed to green practices, Google has
mademajorinroadsonenergysavingsinitsdatacentersandfeelsthattheextremesecurityregardingits
progress in this area undercuts that commitment. So in 2009, Google opened its doors to provide an
insightintoitsenergy-conservationpractices.2
Just recently, in October, 2012, Google provided a more detailed peek into its data centers and allowed
the observations to be published.3 We look at these reports in this article along with some other insights
thathavebeenpublishedpreviouslyintheAvailabilityDigest.
Google’s Compute Capacity
Google is now one of the world’s largest computer manufacturers. It builds all of its own servers and
much of its networking equipment. No one seems to know exactly how many servers Google deploys in
its worldwide data centers, but estimates indicate that there are several hundred thousand servers,
perhapsapproachingamillion.
Google builds servers that are tailored for its use. They would not make good general-purpose
computers. Theyare 2U (3.5”) rack-mounted servers with no cases – theyslide right into the racks.They
1HowDoesGoogleDoIt?,AvailabilityDigest;February2008.
http://www.availabilitydigest.com/public_articles/0302/google.pdf
2Googleuncloaksonce-secretserver,CNet;April1,2009.
3StevenLevy,GoogleThrowsOpenDoorstoItsTop-SecretDataCenter,Wired;October17,2009.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

have no graphics boards (no monitors are driven by these servers) and no wireless capability. They are
x86-based,butconjectureaboundsthatGooglemaysomedaymakeitsownchips.
Centralpowersuppliesprovideonlytwelve-voltpowertotheracksratherthanthefive-voltandtwelve-volt
feeds of commercial rack power supplies. The five-volt sources are generated on the mother board. This
allows Google to run its power supplies efficiently at near peak power, and power can be distributed with
smaller power buses because of the reduced amperage that they must carry due to the higher voltage.
Bothofthesefactorsreducetheheatgeneratedbyaserver.
Each server has its own twelve-volt battery to provide backup power in the event of a data center power
outage. This is more efficient than a large, centralized UPS, which must have its own cooling system. In
addition, reliability is increased because a battery failure will take down only one server rather than an
entiredatacenter.
Since 2005, Google’s data centers have been built from standard shipping containers, or pods. Each pod
isoutfittedwith1,160serversandprovidesitsowncooling.
Google’s system capability goes far beyond its data centers. Stung by some early failures of telecom
operators, Google has bought up many abandoned fiber-optic networks for pennies on the dollar. In
addition, it has started laying its own fiber. It has now built a mighty empire of glass circling the world in
fiber.
Concernedaboutnetwork capacityafter acquiring YouTube,Googlebeganto establishmini-data centers
to store popular videos. The mini-data centers are often connected directly to ISPs like Comcast and
AT&T. If you stream a video, you may not be receiving it from one of Google’s giant data centers but
ratherfromamini-datacenterjustafewmilesaway.
Managing All That Data
Search for “availability” on Google. How does Google give you 801,000,000 web references in 190
milliseconds? Through an extremely efficient, large-scale index of petabytes (that’s millions of gigabytes)
ofwebdata.
A major problem Google faces is how to process the massive amount of worldwide data in a time that
makes the indices useful. Google has solved this problem by building an equally massive parallel
computing facility driven by its MapReduce application. MapReduce distributes applications crunching
terabytes of data across hundreds or thousands of commodity PC-class machines to obtain results in
seconds.4 MapReduce takes care of partitioning the input data, scheduling the program’s execution
across the machines, handling balky and failed machines, balancing the load, and managing
intermachinecommunications.
MapReduce comprises asetof Maptasks andaset of Reducetasks.ManyMap tasks arecreated,each
working on a portion of the input data set. The Map tasks parse the input data and create intermediate
key/value pairs. These are passed to the Reduce tasks, which merge and collate the results of the
variousMaptaskstogeneratethefinalkey/valuepairs.
4HowDoesGoogleDoIt?,AvailabilityDigest;February2008.
http://www.availabilitydigest.com/public_articles/0302/google.pdf
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

input Reduce
Maptask
data task
intermediate final
key/value key/value
pairs pairs
TheMapReduceFunction
For instance, consider the search for the word “availability.” The task is to determine the number of
entities in the input data (the web-page indices) that contain the word “availability.” The input data is
partitionedacrossthousandsofMaptasks.EachMaptask generates akey/valuepairforeachwordinits
portion of the input data set. This key/value pair is simply the word (the key) followed by the number “1”
(thevalue).Eachkey/valuepairiswrittentoanintermediatekey/valuestore.
The Reduce function reads the intermediate key/value store and merges by key all of the intermediate
key/value pairs that have been generated by the Map tasks. In this simple case, the Reduce function will
simplyadd up the number of key/value pairs for each “availability” keyand will store the final count in the
finalkey/valuestore.Thefinalkey/valuewillbe“availability/801,000,000.”
Of course, the actual search is a little more complex than this. As part of the search, Google evaluates
the relevance of the web page to the search criteria and organizes the list of references in priority order.
Thisiswhattheusersees.
A typical configuration might have 200,000 Map tasks and 5,000 Reduce tasks spread among 2,000
worker machines. If a computer fails during the search task, its Map and Reduce tasks are simply
assignedtoothercomputersandrepeated.
MapReduce has been so successful that an open-source version, Hadoop, has become an industry
standard.
Energy Efficiency
Google is focused on energy efficiency of its data centers. Data centers consume 1.5% of all the energy
intheworld.Google’sfocusisevidentinseveralareasinwhichithasreducedpowerandcosts.
One area is the servers themselves. We have noted earlier that Google goes so far as to distribute only
twelve volts to its servers, generating the five-volt sources on the motherboard, so that smaller power
busesgeneratinglessheatcanbeused.
The local batteries associated with each server preclude the need for a UPS system. A UPS system has
its own power requirements during normal operations and generates its own share of heat that must be
removedfromthedatacenter.
Cooling the data center is perhaps the second biggest consumer of energy next to the computing
equipment itself. Google has done much to reduce the energy needed for cooling, which we discuss in
thenextsection.
Power Usage Effectiveness (PUE) is ameasureof howefficientlyadatacenter uses power.It is theratio
ofthetotalpowerusedbythedatacenter,includingcooling,lighting,andotheroverhead,ascomparedto
the amount of power used by just the computing equipment. For a long time, a PUE of 2.0 was
considered reasonable for a data center (that is, the power used bythe computing equipment was half of
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

the total power consumed by the data center). Google has blown through that number. It typically
achievesaPUEof1.2foritsdatacenters.ItsbestshowingisaPUEof1.12–thetotalpoweroverheadis
only12%ofthatusedbythecomputingequipment.
Cooling a Massive Data Center
In its early data centers, Google had a radical insight into data-center cooling. It found that a data center
did not have to be cooled to arctic temperatures by giant air conditioners such that system operators
needed to wear sweaters. Rather, it made extensive experiments and found that system reliability was
quite sufficient if data centers were maintained at about 80° F (T-shirts and shorts weather).5 This
realization allowed Google to dramatically reduce the amount of energy needed for cooling – the primary
factorinthePUE.
The cooling facilities for the computing equipment are built into the pods. The servers are organized into
aisles. The front aisles, or cold aisles, are the access aisles for computer operators. The servers are
removed and inserted via the front aisles. All of the cables and plugs are accessible to the front aisles.
Thefrontaislesarekeptatabalmytemperatureof81°F.
The rear aisles are the hot aisles. The hot aisles are a tightly enclosed space – access is blocked with
metalbaffles ateither end - and canreachtemperatures of 120° F.(If maintenancepersonnel mustenter
a hot aisle, the servers are first shut down; and the aisle is allowed to cool.) The heat generated by the
computerequipmentisabsorbedbywater-filledcoilsandispumpedoutsidethebuildingtocool.
This is where another advance has been made by Google. Rather than cooling water with energy-
consuming chillers, it uses cooling towers and allows the water to drizzle down. Some water evaporates
andcoolstheremainingwater.Thewaterisreplacedfromalocalsourcesuchasariverorcanal.
Thus, Google’s data centers do not use large, energy-hungry computer room air conditioners. They
basically just have to pump some water. Google has taken this a step further in some of its new data
centers. It is locating data centers in cooler climes and is using the outside cold air to cool these data
centers to 80° F.6 It only has to turn on its air conditioners for the few days in the year when the outside
airtemperaturerisestothepointthatthisaireconomizertechniquewon’twork.
Reliability
Google is very concerned about reliability. From a server viewpoint, its applications are all distributed
across hundreds or thousands of servers.If one server fails,another one automaticallypicks upthe load.
Thefailed server is pulled from the rack, and a new one is inserted. The new server is then automatically
addedback intotheserver pool.This is differentfrom today’s virtual environments –there is nomigration
of a virtual machine from one physical server to another. Recovery time is unnoticeable as the other
hundredsorthousandsofserversinvolvedinthedistributedprocessingcontinueonwiththeirtasks.
Securityofdatais amajorconcern.Notonlyisdatabackedupatseverallevels,butfaileddisk drivesare
physicallydestroyedtopreventthecompromiseofdatastoredonthem.
Google deploys a Site Reliability Engineering team. Comprising normal Google engineers who spend
most of their time writing production code, the SREs are like a geek SEAL team. Every year, the
appointed SREs get leather jackets with military-style insignia. Their job is to wage a simulated (and
sometimes real) war against Google to try to take down portions of it and to gauge the response of
Google’sincidentmanagers.ThewariscalledDIRT–Disaster-RecoveryTesting.
5TheBrainoftheBeast:GoogleRevealsTheComputersBehindTheCloud,NPR;October17,2012.
6DataCenterCoolingNature’sWay,AvailabilityDigest;May2010.
http://www.availabilitydigest.com/public_articles/0505/cooling.pdf
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Prior attacks have included causing leaks in water pipes, staging protests outside the gates of a Google
facilityinanattempttodistractattentionfrom intruders tryingtostealdisksfrom servers,andcuttingmost
of Google’s fiber connections to Asia. If the incident response team cannot figure out the fixes, the attack
isabortedsoasnottoinconveniencerealusers.
Summary
Googlemanages its massivedata-processingrequirements bybuilding large datacenters that behaveas
asinglecomputer. Applications are distributed across the entireserver floor.Its datacenters areso large
that Google provides its maintenance staff with personal transportation devices – foot scooters, which
don’tincreasethePUE.
Google has a particular interest in energy efficiency. It achieves industry-leading PUEs with particular
attention to cooling, server design, battery backup, and other initiatives such as foot scooters to minimize
theenergyoverheadrequiredtooperateamajordatacenter.
5
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com