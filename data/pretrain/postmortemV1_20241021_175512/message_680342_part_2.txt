portion of the RDS databases hosted in the affected Availability Zone
became inaccessible. Throughout the course of the event, customers were
able to create new RDS instances and access existing RDS instances in
the unaffected Availability Zones in the region.

Amazon RDS provides two modes of operation: Single Availability Zone
(Single-AZ), where a single database instance operates in one
Availability Zone; and Multi Availability Zone (Multi-AZ), where two
database instances are synchronously operated in two different
Availability Zones. For Multi-AZ RDS, one of the two database instances
is the "primary" and the other is a "standby." The primary handles all
database requests and replicates to the standby. In the case where a
primary fails, the standby is promoted to be the new primary and is
available to handle database requests after integrity checks are
completed.

Single-AZ database instances are exposed to disruptions in an
Availability Zone. In this case, a Single-AZ database instance would
have been affected if one of the EBS volumes it was relying on got
stuck. During this event, a significant number of the Single-AZ
databases in the affected zone became stuck as the EBS volumes used by
them were affected by the primary EBS event described above. In the case
of these Single-AZ databases, recovery depended on waiting for the
underlying EBS volumes to have their performance restored. By 1:30PM
PDT, a significant number of the impaired Single-AZ RDS instances were
restored as the volumes they depended on became unstuck. By 3:30PM PDT,
the majority of the affected database instances were restored, and by
6:35PM PDT, almost all of the affected Single-AZ RDS instances were
restored.

During the course of the event, almost all of the Multi-AZ instances
were promoted to their standby in a healthy Availability Zone, and were
available to handle database requests after integrity checks were
completed. However, a single digit percentage of Multi-AZ RDS instances
in the affected Availability Zone did not failover automatically due to
two different software bugs. The first group of RDS instances that did
not failover as expected encountered an uncommon stuck I/O condition,
which the automatic failover logic did not handle correctly. These
instances required operator action and were fully restored by 11:30 AM
PDT. We have developed a fix for this bug and are in the process of
rolling it out. The second group of Multi-AZ instances did not failover
automatically because the master database instances were disconnected
from their standby for a brief time interval immediately before these
master database instances' volumes became stuck. Normally these events
are simultaneous. Between the period of time the masters were
disconnected from their standbys and the point where volumes became
stuck, the masters continued to process transactions without being able
to replicate to their standbys. When these masters subsequently became
stuck, the system blocked automatic failover to the out-of-date
standbys. We have already been working on a fix for this issue which
will allow the standby to be favored immediately when its master is in
an impaired Availability Zone. Due to the subtle nature of the issues
involved, we are still in the process of completing this fix and
carefully testing it, but are on track to deploy it fully by December.
Database instances affected by this condition were restored once the
associated EBS volumes had performance restored. While we are
disappointed with the impact to these Multi-AZ instances, we are
confident that when we complete the deployment of these two bug fixes,
the root cause of the Multi-AZ failures we observed during this event
will be addressed. It is the top priority of the team to complete these
fixes and get them deployed to the fleet.

Customers affected by the Multi-AZ RDS issues did not get the
availability they or we expected. If an application is Multi-AZ and has
enough resources running to continue operating if one Availability Zone
is lost, then that application component should remain available (with
minimal service disruption). Accordingly, AWS will issue service credits
to customers whose RDS Multi-AZ instances took longer than 20 minutes to
fail over to their secondary copies, equal to 10 days of charges for
those affected Multi-AZ instances. Affected customers do not need to
take any action; the credits will be automatically applied to their AWS
account prior to their October 31 bill being calculated.

**Impact on Amazon Elastic Load Balancing (ELB)**

This event also affected the Amazon Elastic Load Balancing (ELB)
service. Each ELB load balancer uses one or more load balancer instances
to route traffic to customers' EC2 instances. These ELB load balancer
instances use EBS for storing configuration and monitoring information,
and when the EBS volumes on these load balancer instances hung, some of
the ELB load balancers became degraded and the ELB service began
executing recovery workflows to either restore or replace the affected
load balancer instances. Customers can use ELB with applications that
the run in either single or multiple Availability Zones.

For customers using an ELB load balancer with an application running in
a single Available Zone, ELB provisions load balancer instances in the
Availability Zone in which the application is running (effectively
creating a Single-AZ load balancer). During this event, a number of
Single-AZ load balancers in the affected Availability Zone became
impaired when some or all of the load balancer instances used by the
load balancer became inaccessible due to the primary EBS issue. These
affected load balancers recovered as soon as the ELB system was able to
provision additional EBS volumes in the affected Availability Zone, or
in some cases, when the EBS volumes on which particular load balancers
relied, were restored. By 1:10PM PDT, the majority of affected Single-AZ
load balancers had recovered, and by 3:30PM PDT, most of the remaining
load balancers had also been recovered. Recovery of the last remaining
load balancers was then slowed by an issue encountered by the ELB
recovery workflows. ELB uses Elastic IP addresses (EIPs) to reliably
route traffic to load balancer instances. EIPs are consumed as new load
balancers are created and as existing load balancers are scaled. The
increased demand for EIPs from the ELB recovery workflows (and the
overall increase of customer activity during this period) caused ELB to
consume all of the EIPs that were available to it. This stalled the
recovery workflows and delayed recovery of the final affected load
balancers. The team continued to manually recover the remaining impaired
load balancers and was able to remediate the EIP shortage at 9:50PM PDT.

We are working on a number of improvements to shorten the recovery time
of ELB for all customers. We will ensure that we have additional EIP
capacity available to the ELB system at all times to allow full recovery
of any Availability Zone issue. We are already in the process of making
a few changes to reduce the interdependency between ELB and EBS to avoid
correlated failure in future events and allow ELB recovery even when
there are EBS issues within an Availability Zone. Finally, we are also
in the process of a few additional improvements to our recovery
workflows that will be released in the coming weeks that will further
improve the recovery time of ELB load balancers during any similar
event.

For customers using an ELB load balancer with an application running in
multiple Availability Zones, ELB will provision load balancer instances
in every Availability Zone in which the application is running. For
these multiple Availability Zone applications, ELB can route traffic
away from degraded Availability Zones to allow multiple Availability
Zone applications to quickly recover. During this event, customers using
ELB with applications running in multiple Availability Zones that
included the affected Availability Zone may have experienced elevated
error rates during the early parts of the primary event as load balancer
instances or the EC2 instances running the customer's application were
affected by the EBS issue. By 11:49AM PDT, the ELB service shifted
customer traffic away from the impaired Availability Zone for most load
balancers with multiple Availability Zone applications. This allowed
applications behind these load balancers to serve traffic from their
instances in other, unaffected Availability Zones. Unfortunately, a bug
in the traffic shifting functionality incorrectly mapped a small number
of the affected load balancers and therefore didn't shift traffic
correctly. These load balancers continued to send a portion of the
customer requests to the affected Availability Zone until the issue was
identified and corrected at 12:45PM PDT. We have corrected the logic in
the ELB traffic shifting functionality so this error will not occur in
the future. We are also working to improve the sensitivity of the
traffic shifting procedure so that traffic is more quickly failed away
from a degraded Availability Zone in the future. Over time, we will also
expose this traffic shifting functionality directly to ELB customers so
that they have the ability to control the routing of their requests to
the Availability Zones in which they run their applications. Finally, we
will work on helping our customers understand and test the impact of
this traffic shift so that they can be sure their applications can scale
to handle the increased load caused by failing away from an Availability
Zone.

**Final Thoughts**

We apologize for the inconvenience and trouble this caused for affected
customers. We know how critical our services are to our customers'
businesses, and will work hard (and expeditiously) to apply the learning
from this event to our services. While we saw that some of the changes
that we previously made helped us mitigate some of the impact, we also
learned about new failure modes. We will spend many hours over the
coming days and weeks improving our understanding of the event and
further investing in the resiliency of our services.

Sincerely,\
The AWS Team\


