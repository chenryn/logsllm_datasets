multiterabytes;andits systems supportedover 350,000users from 18,700companies generatingmillions
uponmillionsofqueriesandtransactionsperday.
Therefore, Salesforce.com decided to upgrade to the latest version of Oracle. However, Salesforce.com
almost immediately began having problems with its new Oracle RAC cluster. The cluster would crash, or
itwouldnotfailover whenaserver failed.Problemscontinuedforfourmonths,withmanyoutages lasting
forhours.OracleassignedateamofseniorengineerstoSalesforce.comtoresolvetheissues.
To make matters worse, except for its premium customers, Salesforce.com did not communicate with its
customers the status of each outage. Even worse, it characterized these outages as “minor incidents.”
This led to an outpouring of anger in the form of several blogs. Salesforce.com has since taken a major
step to improve customer communications by establishing a tracking facility called
“trust.Salesforce.com.”8
Rackspace,amajorhostingserviceforthousands ofwebsites,wentdownforreasons
that would be hard to anticipate – a truck driver’s heart attack caused his truck to hit a
transformer that powered the Rackspace data center. The transformer exploded, and
the data center went black. In spite of triply-redundant power backup, this incident started a sequence of
eventsthatresultedinmanyofthewebsitesthatRackspacehostedtogodownforhours.
7FireSuppressionSuppressesWestHostforDays,AvailabilityDigest;May2010.
http://www.availabilitydigest.com/public_articles/0505/westhost.pdf
8On-DemandSoftwareUtilityHitsAvailabilityBump,AvailabilityDigest;October2007.
http://www.availabilitydigest.com/public_articles/0210/salesforce.pdf
4
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Asplanned, whenpowerfirstfailed,Rackspace’semergencydieselbackupgeneratorkickedin.Thedata
center came back to life and continued in operation with but a brief interruption. This allowed Rackspace
operators to switch to their secondary power source – a completely separate utility line feeding the
building.Atthispoint,theemergencygeneratorhaddoneitsjobandwasshutdown.
However, fifteen minutes later, the secondary power source shut down. This time, the emergency
personnel requested the power utility to shut down power while they were trying to free the trapped truck
driversoastoavoidelectrocutionofnotonlythetruckdriverbutalsooftheemergencyworkers.
Again, the emergency generator started up and continued to power the data center; and the data center
was once again operational with little impact on the hosted web sites. However, the dual outages caused
the data-center chillers to recycle, a process that took about half an hour. With temperatures in the data
center dangerously rising, management had no choice but to shut down its servers. It took until the
followingdaytogetthehostedwebsitesbackonline.9
Hostway is one of the largest web-site hosting services in the world. When
Hostway attempted to move 3,700 servers of the Miami data center of a recently
acquired company to its own facilities in Tampa, Florida, the servers suffered multiple failures in transit.
The result was that the web sites of 500 customers, many of them small online stores, were down for
days.Aweeklater,severalcustomerswerestilldown.
The move was planned to take place over a weekend, and Hostway notified impacted customers that
theirwebsiteswouldbedownfor12to15hours.
According to plan, Hostway began the process of powering down, disconnecting, packing, loading,
moving, repositioning, reconnecting, and testing the servers prior to returning them to service along with
the web sites that they hosted. Unfortunately, it found that many hard drives were destroyed due to
physicalshockduringthemovingprocess.
Hostway lost about 500 servers due to the disk failures. A week later, some servers had yet to be
restored.
Most of the customers that were affected were using dedicated hosting services. Each failed server
represented one customer. Therefore, about 500 customers experienced multiple-day outages. Many of
themweresmallonlinestoresthatwereliterallyoutofbusinessduringthattime.10
The Planet is one of the world’s largest providers of dedicated servers. Its
servers are used bythousands of web-hosting companies supporting millions of
websites.
Early one evening, an explosion took down The Planet’s Houston data center. A short circuit in a high-
voltage wireconduit set atransformer onfire, which thencaused an explosionof battery-acid fumesfrom
theUPSbattery-backupsystem.
Theexplosion was strongenoughto blow downthree walls surroundingthe electricalequipmentroom on
the first floor of the data center. It blew apart the power-transfer switch that transferred the data center
from utility power to backup diesel-generator power, thus knocking out power to the entire data center.
Fortunately,noonewasinjured.
9Rackspace–AnotherHostingServiceBitestheDust,AvailabilityDigest;December2007.
http://www.availabilitydigest.com/public_articles/0212/rackspace.pdf
10Hostway’sWebHostingServiceDownforDays,AvailabilityDigest;September2007.
http://www.availabilitydigest.com/public_articles/0209/hostway.pdf
5
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Though no servers or networking equipment were damaged, 9,000 servers leased by 7,500 customers
were brought down due to the power outage. More than one-million retail sites were affected by the
explosion,denyingservicetomillionsofInternetusers.
For safety reasons, the fire department evacuated the building and directed that the backup generators
could not be turned on. It wasn’t until after 10 pm that staff were allowed back into the building to assess
thedamage.Fourdaysaftertheexplosion,fullservicetoallcustomershadyettoberestored.11
The blogging service JournalSpace suddenly went out of business when it lost
its entire database of blogs that had not been backed up and could not be
recovered.Thousandsofbloggerslostyearsoftheirwork.
The database’s demise was due to the malicious act of a disgruntled employee – even worse, the IT
manager. JournalSpace claims that it had caught the IT manager stealing from the company. They
summarilyfiredhim;buthedidaslash-and-burnonhis wayout,overwritingtheentireblogdatabasewith
garbage.
It was the IT manager’s responsibility to ensure that a backup copy of the database was periodically
taken and preserved. However, though he dutifully backed up the HTML code for the site on a remote
server, his backup strategy for the blog database was to use a RAID 2 mirrored disk. If one disk failed,
thedatabasewasstillavailableonthemirror.
Upper management should have known that this was not a backup strategy at all. True, it protected
against a hard-disk failure. But it did not protect against a site disaster – or a malicious act of overwriting
theentiredatabase.
In a panic, the JournalSpace management attempted to reconstruct the data on the overwritten disks.
Unfortunately,thediskswereunrecoverable.JournalSpacecloseditsdoorsdayslater.12
Sidekick is a smart phone produced byDanger Incorporated that is offered by
T-Mobile. The Sidekick service stored all of its subscribers’ data, including
address books, calendars, photos, to-do lists, and email messages in its server complex, a large Oracle
RACcluster.Sidekickprovidednowayforitssubscriberstobackuptheirdataoffline.
Danger and its Sidekick smartphone were acquired by Microsoft. One day, Microsoft set out to upgrade
the storage area network used by the RAC cluster. Unfortunately, it decided that the upgrade was so
minorthatitdidnotneedtobackupthedatabasebeforetheupgrade.
Bad decision. The upgrade did not go well. Both the primary and backup databases were corrupted.
Sidekicksubscribersreportedlosingalloftheirstoreddata–photos,addressbooks,andall.
Microsoftstartedamajorefforttorecoverthelostdata.However,amonthlater,itwasonlyabletorestore
some of the data. Most subscriber data was irretrievably lost. Microsoft has since taken steps to improve
thereliabilityofSidekickstoragetoensurethatthisdisasterwillnotberepeated.13
11ThePlanetBlowsUp,AvailabilityDigest;September2008.
http://www.availabilitydigest.com/public_articles/0309/planet_explosion.pdf
12WhyBackUp?,AvailabilityDigest;April2009.
http://www.availabilitydigest.com/public_articles/0404/journalspace.pdf
13Sidekick:YourDataisin‘Danger’,AvailabilityDigest;November2009.
http://www.availabilitydigest.com/public_articles/0411/sidekick.pdf
6
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Lessons Learned
Many clouds provide SLAs guaranteeing three 9s of availability or better. But a multi-day outage, as has
been experienced by many of the largest clouds, can reduce cloud availability for the year to two 9s. A
several-day outage is extremely painful to an organization even for non-critical applications. It is
intolerable for critical applications, many of which cannot withstand outages lasting more than several
minutes.
The SLAs are often not much help. They typically offer a month’s rebate on the charges paid by the
customerfor their cloudservices.For asmallonlinestorethathas losthours or days of revenue,this can
amounttoapaltryfewdollarsascompensation.
Thelessonsprovidedbythesecloud-disasterexamplesareclear:
 Clouds are not yet suitable for critical applications unless they can be run in a redundant mode
suchasAmazon’sAvailabilityZones.Thiscapabilityisbeginningtobeofferedbymoreclouds.
 Anotherredundantmodesuitableforcriticalapplicationsistohavethecapabilitytomovethem to
in-house servers in the event of a cloud failure. However, for many companies, the advantage of
the cloud is that they can decommission their data centers so that there are no in-house servers
onwhichtoruntheircriticalapplicationsifnecessary.
 In some cases, a subscription to the services of a disaster-recovery service or a Recovery-as-a-
Service (RaaS) cloud provider14 can provide servers-on-demand to recover from a cloud failure.
However, recovery times using these services is measured in hours, not minutes. This approach
maynotbesuitableforcriticalapplicationsrequiringminimaldowntime.
 Datastoredinthecloudmustbebackedupoutsideofthecloudincasethecloudlosespartorall
of a company’s data. This is required if critical applications are to be backed up outside of the
cloudsothatthebackupsystem has access totheapplicationdatabaseif thecloudis down..For
non-criticalapplications,anydatathatcannotbereconstructedissubjecttoloss.
 Clouds are subject to being taken offline by Distributed Denial of Service (DDoS) attacks.15
Though a large cloud has the bandwidth to withstand a large attack, the technology is now
publicly available to launch attacks of such unprecedented volume that even large clouds may
succumb.
 Thebottom line is that no matter how you use acloudto host your applications, youmusthave a
tested Business Continuity Plan detailing how you will continue application services when they
arenolongeravailableviayourITassets.Thisoftenwillentailmanualoperationsofsomesort.
Cloudcomputingis becominganimportantresourcefor increasingnumbers ofcompanies.Unfortunately,
cloud utilities do not yet provide the availability offered by electrical utilities and telephone services. Until
that time, careful thought and planning must go into any decision to utilize cloud services for your
applications,whethertheyarecriticalornot.
14HP’sCloudRecovery-as-a-Service(Raas),AvailabilityDigest;June,2012.
http://www.availabilitydigest.com/public_articles/0706/raas.pdf
15AnatomyofaDDoSAttack,AvailabilityDigest;April2013.
http://www.availabilitydigest.com/public_articles/0804/ddos_anatomy.pdf
7
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
