It’s Official! Leap Day Caused the Windows Azure Outage
May2012
In our March, 2012, Never Again article entitled “Windows Azure Cloud Succumbs to Leap Year,” we
related how Microsoft’s Windows Azure Platform as a Service (PaaS) cloud went down for a day and a
half as the result of what appeared to be a leap year software bug. At the time, the conjecture was that
validitydates for SSL(SecureSockets Layer) certificates werecalculated erroneously.As itturns out,the
conjecturewasprettyclose.
Following inthe path of Googleand Amazon, whohavebeen verytransparent in describingpubliclywhat
happenedduringmajor outages, Microsofthas releaseda detailedtimeline of exactlywhat went wrong in
this major outage and thesometimes frantic efforts torestoreservice to its customers.Therather lengthy
descriptionisfoundinaMicrosoftblog1authoredbynoneotherthanBillLaing,Microsoft’sVicePresident
ofWindowsServersandSolutions.
In this article, we summarize the events related in his blog. But first, let us look at some architectural
aspects of the Azure cloud, as related by Mr. Laing, that are important to understand for purposes of this
outage.
The Azure Cloud Architecture
As in any cloud, applications running in the Azure cloud consist of virtual machines (VMs) running on
physicalserversinMicrosoftdatacenters.MicrosoftmanagessixAzurecentersaroundtheworld.
AzureClusters
Each Azure center contains thousands of physical
servers. Servers are grouped into clusters of about 1,000
servers each. Each cluster is independently managed by
a redundant software facility called the Fabric Controller
(FC).
The FC manages the life cycle of applications running in
the cluster, including provisioning the physical resources
needed by the applications, deploying applications in
VMs in the cluster, updating applications and guest operating systems, scaling out applications, and
monitoring the health of the physical servers and other hardware in the cluster. If a server should fail, the
1SummaryofWindowsAzureServiceDisruptiononFeb29th,2012,MSDNBlogs;March9,2012.
http://blogs.msdn.com/b/windowsazure/archive/2012/03/09/summary-of-windows-azure-service-disruption-on-feb-29th-2012.aspx
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

FC reincarnates the server’s virtual machines on healthy servers within the cluster. This is called service
healing.
Agents
A physical server is tightly integrated with the VMs running
on it through the use of a guest agent (GA) bound into the
operating system image of each VM. Each physical server
has a host agent (HA) that the FC leverages to
communicate with the GAs that the HA’s server is hosting.
For instance, the HA deploys application secrets such as
SSL certificates that an application uses to secure HTTPS
end points. The HA also sends heartbeats to each GA on
itsservertodetermineiftheGA’sVMishealthy.
Important to understanding the cause of the Azure outage is that when a VM is initialized, its GA creates
atransfercertificateusing public-keyencryptiontoprotectapplicationsecretsthat willbetransmittedover
physical or logical networks. The first step that a GA takes when connecting with its HA is to pass to the
HA its public key in a transfer certificate. The HA can then encrypt secrets readable only by the GA
becausetheGAhastheprivatekey.
TheGAgeneratesanewtransfercertificatewhenever:
 aVMiscreated(thatis,whenanapplicationisdeployed).
 whenanapplicationisscaledout.
 whenadeploymentupdatesitsoperatingsystem.
 when the FC reincarnates a VM that was running on a server that the FC has determined to be
unhealthy(servicehealing).
ServiceHealing
The HA is responsible for ensuring that new VMs come into service properly.When a VM is first created,
the HA waits up to 25 minutes to receive a transfer certificate from the VM. If a transfer certificate is not
receivedinthattime,theHAwillreinitializetheVMandwillwaitagain.
If after three such attempts the HA still has not heard from the VM, it assumes that the physical server
maybedown.Atthispoint,itwillinformtheFCthattheserverisfaulty.TheFCwillreincarnatetheVMon
another physical server, and it will place the suspected server into Human Interaction (HI) mode for
operatorintervention.TheFCwillservice-healtheotherVMsrunningonthefaultyserver bymovingthem
tootherhealthyserversinthecluster.
The Leap-Day Bug
TheSoftwareBug
Wecan now understand what happened to take down Azure.When the GA creates a transfer certificate,
it gives it a one-year validity range. It uses midnight of the current day as the valid-from date and simply
adds one to the year to get the valid-to date. Consequently, certificates created on February 29, 2012,
hadanexpirationdateofFebruary29,2013.
This is an invalid date, and the certificate creation failed. On leap day, no GA could create a transfer
certificate.Therefore,nonewlyinitiatedGAwasabletoconnecttoitsHA,anditsVMwasterminated.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

CascadingOutages
This caused the HA to declare the physical server faulty. In response, the FC not only tried to reinitialize
the VM on another server; but it also moved all other VMs on the presumably faulty server to other
servers in the cluster. Now all these VMs tried to generate transfer certificates, all failed to do so, and all
weremovedtostillotherservers.TheserveroutagesrapidlycascadedthroughouttheentireAzurecloud.
This disastrous effect was partly mitigated by an HI threshold in the FC designed to prevent a software
bug from propagating throughout the cluster. If a certain number of servers in a cluster transit to the HI
state, the FC moves the entire cluster to the HI state. It stops service healing so that VMs are no longer
moved between physical servers. It suspends other service management functions such as creating new
VMs, applying updates, and scaling out so that the operators have an opportunity to take control of the
cluster and to repair the problem before it progresses further. During this time, VMs in the cluster that
were working before the cluster was moved to the HI state continue working; they just can’t be modified
byservicemanagement.
TheTimeLine
As one would now expect, the Leap Day bug started precisely at 00:00 UST (Universal Standard Time)
on February 29, 2012, which was 4 PM PST (Pacific Standard Time) on February 28th. GAs in newly
launchedVMs wereunabletocreatetransfercertificates.Precisely75minutes later(afterthreeretries25
minutesapart),at5:15PM PST,theHIthresholdinsomeclusterswasexceeded andtheseclusters went
into HI state. This problem was compounded by the fact that operations staff was just in the midst of a
rollout of new versions of the FC, the HA, and the GA. This ensured that the clusters involved in the
rollout would hit the Leap Day bug immediately. The bug worked its way more slowly through other
clusters.
From here on, staff efforts to identify and rectify the outage tracked the following time line (all times are
PST):
6:38PM,February28:Thecauseofthebugwasidentified.
6:55 PM: The operators disabled service management to stop the cascading. At this time, no users
could deploy new applications; nor could they stop, update, or scale existing applications. However,
alreadydeployedapplicationscontinuedtorunproperly.
10:00PM:AtestplanfortheupcomingcorrectedGAwasprepared.
11:20PM:ThecorrectiontotheGAcodewascompleted.
1:50AM,February29:ThecorrectedGAcodewassuccessfullytestedusingatestcluster.
2:11AM:ThefixwasthenrolledouttosomeofMicrosoft’sownclustersandranproperly.
5:23AM:Thefixwasrolledouttomostclusters,andtheseclusterswererestoredtoservice.Aseach
clusterwasupdated,servicemanageabilityinthatclusterwasrestored.
TheSecondaryOutage
“Most clusters” are the operable words. An additional problem occurred with seven clusters that had just
startedtheir rollouts of theupdatedFC,HA,andGAcomponents.Someservers intheseclusters hadthe
old GA and others had the updated GA, all with the Leap Day bug. Servers in these clusters were all
rolled back to the original HA code but with the new and corrected GA code. Unfortunately, no one was
aware that the new networking plugin that was installed along with the new GA code was incompatible
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

with the old HA. The networking plug-in configures the network for the VM. Consequently, no VMs in
theseclustershadnetworkingcapabilities.
The updating of these seven clusters was completed with the incompatible combination by 2:47 AM on
February 29th, only to find that all of the VMs in these clusters, even those that had been healthy, had
become disconnected from the network because of the network plugin incompatibility. This situation was
fixedandthecorrectupdateoftheseclusterswascompletedby8:00AM
However, a number of servers were left in a corrupted state because of all these transitions. The
developers and operations staff worked feverishly to manually restore and validate these servers, but it
was not until2:15 AM,March1, adayanda half after the outage,that allclusters werefinallyrestored to
service.
Improvements for the Future
Microsoft is undertaking several initiatives to preclude such outages in the future. Theyare improving the
fourphasesoftheincidentlifecycle–prevention,detection,response,andrecovery.
Prevention: Code analysis and testing will be improved, especially with respect to time-related bugs. The
FC will be improved to recognize the difference between hardware and software bugs so that the cluster
HI state will not be entered due to a software bug. Service management will be reconstructed to make it
morefinelygranular so that onlythose portions of the services that need to be disabled in an emergency
willbe.
Detection: Taking 75 minutes to recognize a problem is too long. There will be improvements to allow
fast-failcapabilities.
Response: The dashboard capacity will be increased to handle anticipated outage loads, and better
summary information will be provided. Customer support staffing will be improved to eliminate long hold
times in emergencies; and better use will be made of other channels such as blogs, Facebook, and
Twitter.
Recovery: Internal tools will be improved. Better control of dependency priorities will be implemented so
that necessary services are brought up before other services that depend upon them. Better customer
visibilityintotherecoveryprogresswillbeprovided.
In addition to its heartfelt apology for the inconvenience felt by Azure customers, Microsoft has issued a
33%credittoeverycustomerforitsaffectedbillingmonths(Februaryforall,Marchforsome).
Summary
What a catastrophe a little software bug can make. It is not much of a step to add to the statement
“increment year by one” the statement “if Feb. 29, decrement day by one.” This error should have been
caught in code reviews and testing, an improvement that Microsoft is now making. Shortcutting these
important steps in the development phase can potentially cost orders of magnitude more than what is
saved.
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
|---|--|--|--|--|--|
| 0 |  |  |  |  |  |
| 1 |  |  |  |  |  |
| 2 |  |  |  |  |  |
| 3 |  |  |  |  |  |
| 4 |  |  |  |  |  |
| 5 |  |  |  |  |  |
| 6 |  |  |  |  |  |
| 7 |  |  |  |  |  |
|---|--|--|--|--|--|
| 0 |  |  |  |  |  |
| 1 |  |  |  |  |  |
| 2 |  |  |  |  |  |
| 3 |  |  |  |  |  |
| 4 |  |  |  |  |  |
| 5 |  |  |  |  |  |
| 6 |  |  |  |  |  |
| 7 |  |  |  |  |  |