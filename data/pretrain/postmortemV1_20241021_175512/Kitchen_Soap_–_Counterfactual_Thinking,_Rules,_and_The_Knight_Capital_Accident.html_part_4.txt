>
> [*Is the number of the emails (97 of them) important? 97 sounds like a
> lot, doesn't it? If it was one, and not 97, would the paragraph read
> differently? What if there were 10,000 messages
> sent? *]
>
> [*How many engineers right now are receiving alerts on their phone
> (forget about emails) that they will glance at and think that they are
> part of the normal levels of noise in the system, because thresholds
> and error handling are not always precisely
> tuned?*]

C. Controls and Supervisory Procedures

SMARS

20\. Knight had a number of controls in place prior to the point that
orders reached SMARS. In particular, Knight's customer interface,
internal order management system, and system for internally executing
customer orders all contained controls concerning the prevention of the
entry of erroneous orders.

21\. However, Knight did not have adequate controls in SMARS to prevent
the entry of erroneous orders. For example, Knight did not have
sufficient controls to monitor the output from SMARS, such as a control
to compare orders leaving SMARS with those that entered it. Knight also
did not have procedures in place to halt SMARS's operations in response
to its own aberrant activity. Knight had a control that capped the limit
price on a parent order, and therefore related child orders, at 9.5
percent below the National Best Bid (for sell orders) or above the
National Best Offer (for buy orders) for the stock at the time that
SMARS had received the parent order. However, this control would[ not
prevent the entry of erroneous orders in circumstances in which the
National Best Bid or Offer moved by less than 9.5
percent]{style="background-color: #ffff00;"}. Further, it did not apply
to orders--such as the 212 orders described above--that Knight received
before the market open and intended to send to participate in the
opening auction at the primary listing exchange for the stock.

> [*Note: Anomaly detection and error-handling criteria have two
> origins: the imagination of their authors and the history of surprises
> that have been encountered already. A significant number of
> thresholds, guardrails, and alerts in any technical organization are
> put in place only after it's realized that they are needed. Some of
> these realizations come from negative events like outages, data loss,
> etc. and some of them come from "near-misses" or explicit
> re-anticipation activated by feedback that comes from real-world
> operation.*]
>
> [*Even then, real-world observations don't always produce new
> safeguards. How many successful trades had Knight Capital seen in its
> lifetime while that control allowed "the entry of erroneous orders in
> circumstances in which the National Best Bid or Offer moved by less
> than 9.5 percent." How many successful Shuttle launches saw
> degradation in O-ring integrity before the Challenger accident? This
> 'normalization of deviance' (Vaughn, 1997) phenomenon is to be
> expected in all socio-technical organizations. Financial trading
> systems are no exception. History matters.*]

Capital Thresholds

> *[Note: Nothing in this section had much value in explanation or
> prevention.]*

Code Development and Deployment

26\. Knight did not have written code development and deployment
procedures for SMARS (although other groups at Knight had written
procedures), and [Knight did not require a second technician to review
code deployment]{style="background-color: #ffff00;"} in SMARS. Knight
also did not have a written protocol concerning the accessing of unused
code on its production servers, such as a protocol requiring the testing
of any such code after it had been accessed to ensure that the code
still functioned properly.

> [*Note: Again, does a review guarantee safety? Does testing prevent
> malfunction?*]

Incident Response

27\. On August 1, Knight did not have supervisory procedures concerning
incident response. More specifically, Knight [did not have supervisory
procedures to guide its relevant personnel when significant issues
developed]{style="background-color: #ffff00;"}. On August 1, Knight
relied primarily on its technology team to attempt to identify and
address the SMARS problem in a live trading environment. Knight's system
continued to send millions of child orders while its personnel attempted
to identify the source of the problem. In one of its attempts to address
the problem, Knight uninstalled the new RLP code from the seven servers
where it had been deployed correctly. This action worsened the problem,
causing additional incoming parent orders to activate the Power Peg code
that was present on those servers, similar to what had already occurred
on the eighth server.

> [*Note: I would like to think that most engineering organizations that
> are tasked with troubleshooting issues in production systems
> understand that diagnosis isn't something you can **prescribe**.
> Successful incident response in escalating scenarios is something that
> comes from real-world  practice, not a document. Improvisation and
> intuition play a significant role in this, which obviously cannot be
> written down beforehand. *]
>
> [*Thought exercise: you just deployed new code to production. You
> become aware of an issue. Would it be surprising if one of the ways
> you attempt to rectify the scenario is to roll back to the last known
> working version? The SEC release implies that it would
> be.*]

D. Compliance Reviews and Written Description of Controls

> [*Note: I'm skipping some sections here as it's just more about
> compliance. *]

Post-Compliance Date Reviews

32\. Knight conducted periodic reviews pursuant to the WSPs. As
explained above, the WSPs assigned various tasks to be performed by SCG
staff in consultation with the pertinent business and technology units,
with a senior member of the pertinent business unit reviewing and
approving that work. These reviews did not consider whether Knight
needed controls to limit the risk that SMARS could malfunction, nor did
these reviews consider whether Knight needed controls concerning code
deployment or unused code residing on servers. Before undertaking any
evaluation of Knight's controls, SCG, along with business and technology
staff, had to spend significant time and effort identifying the missing
content and correcting the inaccuracies in the written description.

33\. [Several previous events presented an opportunity for Knight to
review the adequacy of its controls in their
entirety]{style="background-color: #ffff00;"}. For example, in October
2011, Knight used test data to perform a weekend disaster recovery test.
After the test concluded, Knight's LMM desk mistakenly continued to use
the test data to generate automated quotes when trading began that
Monday morning. Knight experienced a nearly \$7.5 million loss as a
result of this event. Knight responded to the event by limiting the
operation of the system to market hours, changing the control so that
this system would stop providing quotes after receiving an execution,
and adding an item to a disaster recovery checklist that required a
check of the test data. Knight did not broadly consider whether it had
sufficient controls to prevent the entry of erroneous orders, regardless
of the specific system that sent the orders or the particular reason for
that system's error. Knight also did not have a mechanism to test
whether their systems were relying on stale data.

> [*Note: That we might be able to cherry-pick opportunities in the past
> where signs of doomsday could have (or should have) been seen and
> heeded is consistent with textbook definitions of The Hindsight Bias.
> How organizations learn is influenced by the social and cultural
> dynamics of its internal structures. Again, Diane Vaughn's writings is
> a place we can look to for exploring how path dependency can get us
> into surprising places. But again: this is not the SEC's job to speak
> to that.  *]

E. CEO Certification

34\. In March 2012, Knight's CEO signed a certification concerning Rule
15c3-5. The certification did not state that Knight's controls and
procedures complied with the rule. Instead, the certification stated
that Knight had in place "processes" to comply with the rule. [This
drafting error was not intentional, the CEO did not notice the error,
and the CEO believed at the time that he was certifying that Knight's
controls and procedures complied with the
rule.]{style="background-color: #ffff00;"}

> [*Note: This is possibly the only hint at local rationality in the
> document. *]

F. Collateral Consequences

35\. There were collateral consequences as a result of the August 1
event, including significant net capital problems. In addition, many of
the millions of orders that SMARS sent on August 1 were short sale
orders. Knight did not mark these orders as short sales, as required by
Rule 200(g) of Regulation SHO. Similarly, Rule 203(b) of Regulation SHO
prohibits a broker or dealer from accepting a short sale order in an
equity security from another person, or effecting a short sale in an
equity security for its own account, unless it has borrowed the
security, entered into a bona-fide arrangement to borrow the security,
or has reasonable grounds to believe that the security can be borrowed
so that it can be delivered on the date delivery is due (known as the
"locate" requirement), and has documented compliance with this
requirement. Knight did not obtain a "locate" in connection with
Knight's unintended orders and did not document compliance with the
requirement with respect to Knight's unintended orders.

VIOLATIONS\
A. Market Access Rule: Section 15(c)(3) of the Exchange Act and Rule
15c3-5

> [*Note: I'm going skip a bit because it's not much more than a
> restating of rules that the SEC deemed were
> broken....*]

<div>

<div>

Accordingly, pursuant to Sections 15(b) and 21C of the Exchange Act, it
is hereby ORDERED that:

</div>

</div>

A. Respondent [Knight cease and desist from committing or causing any
violations and any future violations of Section
15(c)(3)]{style="background-color: #ffff00;"} of the Exchange Act and
Rule 15c3-5 thereunder, and Rules 200(g) and 203(b) of Regulation SHO.

> [*Note: Translated -- you must stop immediately all of the things that
> violate rules that say you must "reasonably design" things. So don't
> **unreasonably** design things anymore. *]

The SEC document does what it needs to do: walk through the regulations
that they think were violated, and talk about the settlement agreement.
Knight Capital doesn't have to admit they did anything wrong or
suboptimal, and the SEC gets to tell them what to do next. That is,
roughly:

1.  Hire a consultant that helps them not unreasonably design things
    anymore, and document that.
2.  Pay \$12M to the SEC.

Under no circumstances should you take this document to be an
explanation of the event or how to prevent future ones like it.

</div>

##  What questions remain unanswered?

Like I mentioned before, this SEC release doesn't help explain ~~why~~
***how*** the event came to be, or make any effort towards prevention
other than require Knight Capital to pay a settlement, hire a
consultant, and write new procedures that can predict the future. I do
not know anyone at Knight Capital (or at the SEC for that matter) so
it's very unlikely that I'll gain any more awareness of accident details
than you will, my dear reader.

But I can put down a few questions that I might ask if I was
facilitating the debriefing of the accident, which could possibly help
with gaining a systems-thinking perspective on explanation. Real
prevention is left to an exercise to the readers who also work at Knight
Capital.

-    The engineer who deployed the new code to support the RLP
    integration had confidence that all servers (not just seven of the
    eight) received the new code. What gave him that confidence? Was it
    a dashboard? Reliance on an alert? Some other sort of feedback from
    the deployment process?
-   The BNET Reject E-mail Messages: Have they ever been sent before? Do
    the recipients of them trust their validity? What is the background
    on their delivery being via email, versus synchronous alerting? Do
    they provide enough context in their content to give an engineer
    sufficient criteria to act on?
-   What were the signals that the responding team used to indicate that
    a roll-back of the code on the seven servers was a potential
    repairing action?
-   Did the team that were responding to the issue have solid and clear
    communication channels? Was it textual chat, in-person, or over
    voice or video conference?
-   Did the team have to improvise any new tooling to be used in the
    diagnosis or response?
-   What metrics did the team use to guide their actions? Were they
    infrastructural (such as latency, network, or CPU graphs?) or
    market-related data (trades, positions, etc.) or a mixture?
-   What indications were there to raise awareness that the eighth
    server didn't receive the latest code? Was it a checksum or
    versioning? Was it logs of a deployment tool? Was it differences in
    the server metrics of the eighth server?
-   As the new code was rolled out: what was the team focused on? What
    were they seeing?
-   As they recognized there was an issue: did the symptoms look like
    something they had seen before?
-   As the event unfolded: did the responding team discuss what to do,
    or did single actors take action?
-   Regarding non-technical teams: were they involved with directing the
    response?
-   Many many more questions remain, that presumably (hopefully) Knight
    Capital has asked and answered themselves.

## The Second Victim

What about the engineer who deployed the code...the one who had his
hands on the actual work being done? How is *he* doing? Is he receiving
support from his peers and colleagues? Or was he fired? The financial
trading world does not exactly have a reputation for empathy, and given
that there is no voice given to the people closest to the work (such as
this engineer) informing the story, I can imagine that symptoms
consistent with traumatic stress are likely.

Some safety-critical domains have put together structured programs to
offer support to individuals that are involved with high-tempo and
high-consequence work. Aviation and air traffic control has seen good
success with CISM ([Critical Incident Stress
Management](http://www.skybrary.aero/index.php/Critical_Incident_Stress_Management_in_ATM "CISM"){target="_blank"
rel="noopener"}) and it's been embraced by organizations around the
world.

As web operations and financial trading systems become more and more
complex, we will continue to be surprised by outcomes of what looks like
"normal" work. If we do not make effort to support those who navigate
this complexity on a daily basis, we will not like the results.

## Summary

1.  The SEC does not have responsibility for investigation with the
    goals of explanation or prevention of adverse events. Their focus is
    regulation.
2.  Absent a real investigation that eschews counterfactuals, puts
    procedures and rules into context, and encourages a narrative that
    holds paramount the voices of those closest to the work: we cannot
    draw any substantial conclusions. [This means armchair accident
    investigation ripe with
    indignation.](http://pythonsweetness.tumblr.com/post/64740079543/how-to-lose-172-222-a-second-for-45-minutes "Indignant Hindsight"){target="_blank"
    rel="noopener"}

So please don't use the SEC Release No. 70694 as a post-mortem document,
because it is not.


