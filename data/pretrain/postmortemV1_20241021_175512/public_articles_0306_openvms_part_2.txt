isdifferentfrom thatheldbythemaster,thecurrentsegmentistransferredfrom themastertothe
new member. The comparison is repeated following the segment write to ensure that the
segment has not been modified during the copy. If so, it is recopied. If this occurs several times
(i.e., the segment is being heavily updated), HBVS will acquire a lock temporarily pausing I/Os to
theshadowsetsothatitcancopythesegmentaccurately.
A “fence” on the new member separates the part that has been copied from the part that has not
been copied. The part that has been copied is available to participate in read requests from the
clusterapplications.Thepartfollowingthefenceisnotavailable.
OpenVMS also provides a mini-copy for faster recovery should a member be removed from the
shadow set for operational purposes, such as providing a snapshot to be written to tape for
backup. In this case, a bit map of all segments on the disk is cleared before the disk is taken
offline. Then, as processing continues, each segment that is modified is noted in the bit map.
When the member is brought back online, only those segments so noted in the bit map need to
bewrittentothereturningmember.Theinitialcomparisonisnotdoneinthiscase.
Thecopyprocesstoaninitializeddisk canbeacceleratedbywritingabackupcopyofthecurrent
database to the member to be created. This is relatively fast compared to the copy operation.
Next, the copy as described above is performed. Only those segments that have changed since
thebackupneedbewritten,thusspeedingupthecopysignificantly.
6
©2008SombersAssociates,Inc.,andW.H.Highleyman

ShadowMemberMerge
When a node that is accessing a shadow set leaves the cluster unexpectedly, there is some
chancethatsomeofthewritesitwasperformingtotheshadowsetmayhavecompletedonsome
members but not on others, causing a discrepancy in their contents. To ensure that the data on
all shadow-set members is identical, the shadow-set is resynchronized via a merge operation.
One of the current shadow-set members is designated the master, and it is used to bring the
othermember(s)intocurrency.
The merge operation is similar to the copy
operation except that the entire contents of
the disk, even those segments after the
fence, are available for reading. However, if a merge good
fence
segment after the fence is to be read, it must
bad
be merged first by comparing it to the master
and by refreshing it if necessary. This is
current member
required to assure read consistency – that is, members being
the same record contents will be returned on recovered
subsequent reads no matter which shadow
setmemberisused.
Mini-merges can also be used to recover after node failures. To do this, bit-map recording is
started during normal operation. Areas of the shadow set that are written to by nodes in the
cluster are recorded bysetting bits in the bit map. After a while, a new bit map is started; and the
old bit map is discarded. Using this method, HBVS knows what specific areas of the disk have
been written to recently; and when a node fails, only those areas written to recently need to be
mergedinsteadoftheentirecontentsoftheshadowset.
Shadow copies and merges can be sped up by partitioning the data and by performing parallel
copiesormergesonthepartitions.
Quorum
Should a cluster failure occur that isolates part of the cluster from the rest of the cluster, the
Connection Manager must reorganize the cluster components into a single cluster by discarding
one of the separated parts. It accomplishes this through a quorum procedure. Otherwise, the
system could function in “split-brain” mode. Each partial cluster would be processing its own
transactions against that part of the disk subsystem to which it had access. The disk contents
woulddiverge,leadingtodatabasecorruption.
Basically, each processor (and a quorum disk if desired) can
be given one or more votes. A quorum requires a majority of
site
votes. For instance, in a three-site system, each site might 1vote
haveonevote.Inthiscase,quorumistwovotes.Shouldone x
site be disconnected from the other two sites, the
site
disconnectedsitewillhaveavoteofone.Thetwoconnected
1vote
sites, as a cluster, will have a vote of two. Thus, the
connected sites will have quorum and will continue
processing.Thesinglesitewillnothavequorum andwillhalt surviving site
cluster 1vote
processing. The quorum decision and subsequent
reconfiguration are the responsibility of the cluster’s
ConnectionManager.
This configuration is often used in a two-site system with a third small quorum site. Alternatively,
aquorum disk,notamemberofashadowset,canbe connectedtotheSANandcanprovidethe
7
©2008SombersAssociates,Inc.,andW.H.Highleyman

same function. The two operational sites maintain their status on the quorum site or quorum disk
forusebytheConnectionManager.
If only two sites are used, one site can
be declared the primary site and the primary secondary
x
other site the secondary site by giving site site
2votes 1vote
the primary site two votes and the
secondary site one vote. Again, in this
surviving
configuration,quorumistwovotes.
cluster
Should the secondary site fail, or should adjust
quorum
the communication link between the
sites fail, the primary site will continue in primary secondary
site site
operation since it has quorum. However,
2votes 1vote
should the primary site fail, the
secondary site will pause processing survivingclusterafter
since it does not have quorum. Manual quorumadjustment
intervention is needed to change the quorum value to that of the votes held bythe surviving site.
At this point, the secondary system will take over processing. In a lights-out operation, the lights-
outsiteshouldbetheprimarysitesinceitmaybedifficulttogettoittomanuallyadjustquorum.
There are more complex configurations that are handled. For instance, consider two sites with
two nodes each. Site 1’s processors are given two votes each, and Site 2’s processors are given
one vote each. Clearly, Site 1 is the primary site. However, should a Site 1 processor fail, then
both sites have two votes; and at this point there is no longer a distinction between the sites in
terms of the number of votes. In this case, the site with the most operating nodes – Site 2 in this
example–willbeconsideredthenewprimarysite.
In a two-site system in which each site has an equal number of votes, the failure of one site or of
thecommunicationlink requiresmanualinterventiontogiveoneofthesites quorum sothatitcan
take over processing. Alternatively, the cluster’s Connection Manager can be configured to
automaticallymake a choice; or a third site with an additional node to provide a tie-breaking vote
canbeadded.
When a node fails and quorum is maintained, it takes about three seconds to fail over while the
locktablesarerebuilt.
Channel Latency
A concern with any synchronous replication method is channel latency. Synchronous operations
must wait until all systems involved have reported completion. If some of these systems are
remote,significantdelayscanbeintroducedtotheoperations.
The propagation times over networks vary widely depending upon the communication medium
and the intervening equipment. However, a useful rule of thumb is that signal propagation will be
about one-half the speed of light. This means a round-trip delay of about 2 milliseconds per 100
miles.
OpenVMS split-site clusters are typically configured over campus clusters using fibre-channel
links that are up to 100 kilometers in length. This gives a round-trip time of a little over a
millisecond that is added to each write operation. For typical applications that are write-once,
read-many,experiencehasshownthatthisamountofdelayisquitetolerable.
There is no fundamental limit to the distance between two OpenVMS cluster sites. In fact, by
using SAN channel extenders that extend fibre channels over IP, intersite distances of hundreds
8
©2008SombersAssociates,Inc.,andW.H.Highleyman

of miles can be achieved. The OpenVMS Cluster Software SPD (Software Product Description)
specifies a maximum supported distance of 500 miles based on potential concerns about
application performance. However, OpenVMS engineering recently demonstrated a cluster using
SCS over IP that operates over sites separated by 8,000 miles – from Nashua, New Hampshire,
USA,toBangalore,India.
Shadow-setmemberseparationof 60,000miles has beentestedbysimulationandfoundtowork
(60,000 miles was the limit of the test equipment). Why go more than halfway around the world?
Maybe to use satellite channels. Also, some circuitous routing, especially after a network path
failure,couldextendthepathlengthindeterminately.5
However, performance rapidlydegrades as intersite distances increase. One set of tests showed
thefollowingperformanceovervaryingdistances:6
Simulated One-Way Transactions Degradation
Distance Network PerSecond
(mi/km) Latency
(msec.)
0/0 0 6,500 0
372/600 3 2,000 3.3
621/1,000 5 1,250 5.2
The performance problems of clusters separated by great distances are further compounded by
the amount of internode traffic required to support HBVS. Though a minimum 10 megabit/sec.
channel is recommended, heavy block traffic may require significantly greater bandwidth. The
bandwidth of even the fastest feasible long-distance channels may throttle performance, adding
even more to the performance degradation caused by channel latency. Moreover, the intersite
bandwidth of channels required to support greater distances is often determined by the shadow-
setcopyratherthanbytheapplicationwriteload.
Clearly, one should not run with OpenVMS cluster sites over long distances unless the
application can tolerate this sort of performance degradation. That being said, there is one
customerthatisrunningaclusterwitha3,000mileintersitedistance.
The effects of channel latency also bring up another issue. It is common practice when
negotiating an SLA (Service Level Agreement) with a communications carrier to include
bandwidth and error-rate guarantees. When using synchronous replication, it is also important to
negotiate channel-latency guarantees. There have been cases where a link failure caused traffic
to be rerouted over a long, circuitous route. While the bandwidth and error-rate guarantees were
stillmet,theincreasedchannellatenciesbroughttheapplicationstotheirknees.
This leaves the problem of metro clusters and continental clusters - providing a disaster recovery
site that is far from the operational sites. OpenVMS clusters solve this problem with
asynchronous replication. Using any one of a number of asynchronous replication engines, one
site can be chosen to replicate its database to a backup database any distance away. The
replicated database cannot be used actively with the production databases, and some data in
transit may be lost following a production-site failure. However, this use of asynchronous
replicationprovidesthestandarddisaster-recoveryprotectioncommonintheindustrytoday.
5ArecentbreakinanunderseacablebetweenEgyptandItalycausedtraffictoberoutedaroundtheworld.SeeWhat?
NoInternet?,AvailabilityDigest;February,2008.
6 Disaster Tolerance Proof of Concept: OpenVMS Host-Based Volume Shadowing and Oracle9i RAC over Extended
Distances,HPWhitePaper.
9
©2008SombersAssociates,Inc.,andW.H.Highleyman

asynchronous
synchronous
replication
cluster writes cluster backup
node1 node2 cluster
60miles 1,000miles
Reliable Transaction Router (RTR)
Another alternative for providing longer distance protection without the performance penalties of
synchronous disk replication is to use HP Reliable Transaction Router software (RTR) to do
transaction replication. With this software, a transaction is replicated to two different backend
servers,eachofwhichcouldbeafullOpenVMSdisaster-tolerantclusterinitsownright.Ineffect,
the two servers form a server shadow set rather than a disk shadow set. One site is the primary
site, and one is the secondary site. With RTR, transaction replication is synchronous. The
transaction does not commit until it has been successfully applied to the database in the primary
systemandhasbeensafe-storedinthesecondarysystem.
UsingRTRshadowing,ifoneofthebackendserverclustersitesfails,theapplicationcontinuesto
operate uninterrupted and without data loss. If one site suffers an outage, the remaining site
records the transactions missed by the absent site and can replay them to resynchronize the
backendserver cluster afterthesecondsitereturnsto serviceAtthispoint,redundancyhasbeen
restored;andprocessingcontinueswithfullprotection.
By replicating at the transaction level instead of at the disk-block level, experience at RTR
customer sites has shown that higher intersite distances can be tolerated with less adverse
performanceimpact.Thisapproachalsoavoidsanyriskofdatacollisions.
Though this technique consumes twice the system capacity during normal operation, that
capacityisgenerallyneededanywaytocontinueprocessingintheeventofasitefailure.
Summary
OpenVMS clusters are the “founding fathers” of cluster technology, having been introduced in
1984. Analyst firm Illuminata has called OpenVMS clusters the “gold standard” for commercial
clusters,notingtheircapabilitytoprotectagainstdisastersthataffectanentiresite.7
In a TCO (Total Cost of Ownership) study, research firm TechWise Research compared
OpenVMS clusters with cluster technology from IBM AIX System p5 clusters and Sun Solaris
Sun-Fireclusters.8TheyfoundthatOpenVMSclusters averaged2hoursofdowntimeper yearas
compared to over six hours per year for IBM clusters and over 9 hours per year for Sun clusters.
A major factor in this comparative reliability was the operating systems. OpenVMS and its cluster
software experienced less than 0.2 hours per year of downtime as compared to over four hours
per year of downtime for IBM’s AIX and Sun’s Solaris, including their cluster software. OpenVMS
IntegrityclustersofferedthelowestTCOforanyoftheconfigurationsconsidered.Anearlierstudy
byTechWiseResearchfoundasimilardisparitybetweenOpenVMSclustersandHPUXclusters.9
OpenVMSfilesynchronizationis synchronous,whereasmostactive/activeinstallations todayuse
bidirectional asynchronous replication. Consequently, OpenVMS clusters trade off performance
7Illuminata,Inc.,DisasterTolerantUnix:RemovingtheLastSinglePointofFailure,
http://h71000.www7.hp.com/openvms/whitepapers/Illuminata.pdf;August9,2002.
8TechWiseResearchInc.,QuantifyingtheTotalCostofOwnershipforEntry-LevelandMid-RangeServerClusters;June,
2007.
9TechwiseResearch,Inc.,QuantifyingtheValueofAvailability;June,2000.
10
©2008SombersAssociates,Inc.,andW.H.Highleyman

over long distances to eliminate the problems of asynchronous replication - data collisions and
lossofdatafollowingasitefailure.
The flip side of this coin is that bidirectional asynchronous replication supports active/active
nodes that can be thousands of miles apart provided that the problems of data loss following a
nodefailureandofdatacollisionscanbetolerated.
Toachievedisaster tolerancewhensites mustbeseparatedbygreatdistances,onecanbuildan
OpenVMS multisite cluster in which the operational sites are separated by distances
commensuratewithsynchronous replicationandthenuse unidirectionalasynchronous replication
toaremotesite.Theremotesite cannotactivelyparticipateinthe applicationunless ittakes over
followingthefailureoftheoperationalsites.
Alternatively, Reliable Transaction Router software may be utilized in conjunction with OpenVMS
clusters to do synchronous replication at the transaction level to two geographically-separated
servers.Thisislesssensitivetodistance.
OpenVMS clusters are an interesting mix of cluster and active/active technologies. Although they
are structured logically as multinode clusters accessing a local file system, they can be split into
geographically-separated multinode sites; and the nodes can all be executing common
applications as an active/active network. OpenVMS clusters can be put to many additional uses,
such as acting as continuously available file servers with clients accessing data through
mechanismssuchas FTPandNFS,thus protectingdataonbehalf of less capablesystems.With
these as significant advantages, the author can think of no disadvantages of OpenVMS clusters
overtoday’scontemporaryclustertechnology.
11
©2008SombersAssociates,Inc.,andW.H.Highleyman

