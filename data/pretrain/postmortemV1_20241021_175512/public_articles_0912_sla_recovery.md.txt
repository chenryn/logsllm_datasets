How Does Failover Affect Your SLA?
December2014
Service Level Agreements (SLAs) usually include a limit on the amount of downtime that
is tolerable for an application. Each application typically has its own SLA requirements.
The SLA may exclude certain types of downtime, such as for periodic maintenance (planned downtime).
It may also apportion the downtime calculation over the number of users impacted. For instance, if an
hourofdowntimeaffectedonlyhalftheusers,thentheoutagecountsasahalfhourofdowntime.
Specifying Downtime
Often, rather than specifythe allowable downtime, the SLA instead will stipulate the required uptime as a
certainnumberof9s.Forinstance,arequirementforanuptimeofthree9smeansthattheapplicationwill
be available 99.9% of the time. That is, the SLA will allow only 0.001% of downtime annually. This is
equivalenttobeingdown8.76hoursper year.
Theequivalenceofuptimemeasuredin9stodowntimeper yearisasfollows:
Two9s 87.6hoursperyear
Three9s 8.76hoursperyear
Four9s 0.876hoursper year(52.6minutesperyear)
Five9s 5.26minutesper year
Six9s 0.526minutesperyear(31.5secondsperyear)
Seven9s 3.15secondsperyear
Eight9s 0.315secondsper year
An easywayto remember this table is that five 9s equals fiveminutes. Multiplybyfactors of tenor divide
byfactorsoftentoobtainapproximatevaluesfortheotherentries.
Field experience shows that today’s systems offer inherent availabilities in the range of three to four 9s.
Windows and Linux servers generally deliver availabilities of 0.999 to 0.9995. Fault-tolerant systems like
HP NonStop and Stratus ftServers provide availabilities of 0.9999. This is not to saythat the hardware or
operating systems of these platforms have such availabilities. Platform availabilities can range in the six
to seven 9s. Instead, it may be other factors that reduce availability – application faults, operator errors,
powerandcoolingoutages,etc.
Improving Availability via a Backup System
Clearly, if an availability greater than the inherent system availability is required, there must be a means
tocontinueoperationinthe presenceof asystem failure.This isoftenaccomplishedviaabackupsystem
to which application processing can fail over in the event of a production system failure. However,
bringing the backup system online takes some time. How does that time affect the overall system
availability?Weexplorethisquestionbelow.
1
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Your New SLA
Let us consider an illustrative example. You head your company’s IT department. Meeting the availability
requirementsoftheapplicationSLAsis yourresponsibility.
You are currently running your critical applications on a NonStop server. You are confident that
your applications will exhibit high availability – after all, NonStop servers are fault-tolerant. They
will survive any single fault in the system. Your experience is that your NonStop server will fail
about once every five years and will take about four hours to return to service. This represents
anaveragedowntimeof0.8hoursperyearorfour9sofavailability.
Your company is launching some new mission-critical applications, the SLAs for which call for an
availability of six 9s. This represents 30 seconds of downtime per year or 2.5 minutes of downtime every
five years (our assumed mean time between failures, or MTBF). Clearly, you need a second system to
backupyouractiveproductionsystem.
Active/Backup Systems
active backup
An active/backup system comprises two nodes – a production node and a
backup node. The production node normally runs the application. The backup failover
node may be performing other work, such as that used for development.
Should the production node fail, application processing is moved to the backup
node.Thisiscalledfailover.
Will an active/backup system provide the availabilityrequired by the new SLA? Your applications are still
up if the production node fails because the backup takes over. However, your applications are down if
bothnodesfail.
What is the probability of a dual-node failure? The probability that one node
with four 9s availability will fail is1 (1 – 0.9999) = 0.0001 = 10-4. The probability
thatbothnodes willfail is 10-4x 10-4 =10-8.Thus,theredundantsystem has an
availabilityofeight9s,easilymeetingyournewSLA.Right?
Think again. You can achieve eight 9s if the backup node can take over
instantly.Butitcan’t!
RecoveryTime
It takes a while for a backup node to take over processing. This is called recovery time. During the
recovery, applications are down. Downtime becomes greater; availability becomes less. It is therefore
imperativetoreducerecoverytime.
Whatmustbedonetobringupthebackupnode?Let’s assumethatthebackup
database has been kept up-to-date via data replication (tape backup can take
hourstodaystorestorethedatabase).Thefirststepis todecidewhetherornot
to fail over. What caused the production node to fail? Is it better to wait for it to
get repaired, or will it be faster to bring the backup node online? This often
requiresamanagementdecisionandaddstotherecoverytime.
Ifthedecisionismadetofailover:
 Anyworkthatthebackupnodeisdoing(suchasdevelopment)hastobeshutdown.
1Notethattheexponentofthefailureprobabilityisthenumberof9sofavailability.
2
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

 Applicationshavetobeloaded.
 Networkshavetobereconfigured.
 Thedatabasehastobemounted.
 Thenewproductionnodehastobetested.
Theaboveproceduresdescribeacoldstandby.Typicalcold-standbyrecoverytimeisonetothreehours.
Recoverytimecanbeminimizedby:
 preloadingtheapplicationsofthebackupnode(thisiscalledahotstandby).
 scriptingbackupprocedurestominimizemanualefforts(anderrors).
 ensuringthatthebackupdatabaseiscurrentandconsistent(useasuitabledatareplicationengine).
 practice,practice,practice.
By using these techniques, recovery time typically can be reduced to anywhere from ten minutes to one
hour.
FailoverFaults
However, one other factor that affects the availability of an active/backup
system is failover faults. A failover fault occurs if the backup node cannot be
broughtonline.Inthiscase,wehaveadual-nodefailure.
Therearemanycausesoffailoverfaults:
 Backupfailure–thebackupnodehasfailed,andnoonenoticed.
 Configurationdrift–changesmadetotheactivenodedidnotmakeittothebackupnode.
 Scripterror–afailoverscriptwaswrongorout-of-date.
 Operatorerror–anerrorwasmadeinamanualpartofthefailoverprocedure.
Failoverfaultscanbeminimizedbyperiodictesting.However,manycompaniesconsiderthistobearisky
and expensive procedure. Applications are down during failover testing.What if things go wrong, and the
active node cannot be brought back online? As a consequence, failover testing is typically done during
off-hours.Inaddition,theavailabilityofallseniorstaffmustbeensuredincasethefailoverfails.
Because of these factors, failover testing is often not thoroughly performed. Some companies never test
failover – they depend upon faith and hope.2 Without periodic testing, failover faults are all too likely to
occurandrepresentaseriousimpedimenttohighavailability.
An Availability Analysis
Let us analyze the impact of recovery time and failover faults on availability. We are going to use a little
math, but we will keep it to a minimum. If you are mathematically challenged, ignore the math. You will
stillbeabletounderstandtheresults.
Inanactive/backupsystem,therearethreewaysinwhichanapplicationcanbedown:
 Boththeactiveandbackupnodeshavefailed.
 Theapplicationsareintheprocessofbeingrecoveredonthebackupnode.
 Afailoverfaulthasoccurred.
2AnexcellentcounterexampleisMayoClinic.Onaquarterlybasis,Mayo failsoverandrunsonthealternatesystemuntilthenext
failovertime(switch-and-stay,agoodbestpractice).Asaresult,Mayoachievesarecoverytimeoffifteenminutesandhasvirtually
nofailoverfaults.SeeTacklingSwitchoverTimes,AvailabilityDigest;October2006–
http://www.availabilitydigest.com/public_articles/0101/tackling_switchover_times.pdf
3
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Letuscalculatetheexpectedamountofdowntimeforanapplication.Let:
f betheprobabilityoffailureofanode.
mtbf bethemean(average)timebetweenfailuresforanode.
mtr bethemean(average)timetorecovertothebackup.
d betheprobabilityofafailoverfault.
Case1:Dual-NodeFailure
Theprobabilitythatonenodewillfailisf.Theprobabilitythatbothnodeswillfailisfxf=f2:
probabilityofadualnodefailure=f2
In our example, a node has an availabilityof four 9s. Therefore, the probability of failure of a node is (1 –
0.9999)=0.0001=10-4;andtheprobabilityofadual-nodefailureis10-4x10-4=10-8:
This represents an average downtime of 0.3 seconds per year or 1.5 seconds every five years (our
assumed mtbf). Note that this is an average. With a nodal downtime of four hours, the system will be
downforfourhoursevery72,000years.Dual-nodefailuresarenotverysignificant.
Case2:RecoveryTime
In our example, the active node fails once every five years (mtbf). Applications will be down during the
time it takes to recover to the backup node (mtr). Therefore, the probability that an application will be
downduringrecoverytothebackupnodeismtr/mtbf:
probabilityofbeingdownduringrecovery=mtr/mtbf
Let us consider a recovery time of thirty minutes. We will be down thirty minutes every five years while
applicationsarerecoveringtothebackupnode.
Case3:FailoverFaults
In our example, the active node fails on the average of once every five years (mtbf). The probability that
the active node has failed is f. The probability that there will be a failover fault when the active node fails
is d. Therefore, the probability of a failover fault is the probability that the active node will fail AND the
probabilitythatafailoverfaultwilloccur:
probabilityofafailoverfault=fxd
In our example, the probability of the active node failing, f, is 10-4. Let the probability of a failover fault
following the failure of the active node be ten percent (0.1). One out of ten failovers will fail. This means
that there will be a failover fault every fifty years on the average. (Note that this implies that effective
failovertestinghasnotbeendone.)
Thus, the probability of a failover fault is 10-4 x 10-1 = 10-5. This is five 9s. Five 9s is a downtime of five
minutesperyearorfifteenminuteseveryfiveyears.
SummaryoftheAnalysis
Tosummarizetheanalysis,wehave:
probabilityofadualnodefailure=f2
probabilityofbeingdownduringrecovery=mtr/mtbf
probabilityofafailoverfault=fxd
4
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

probabilityofapplicationdowntime=f2+(mtr/mtbf)+(fxd)
Tosummarizeourexample:
perfiveyears
downtimeduetodual-nodefailures: 2.5seconds
downtimeduetorecovery: 30minutes
downtimeduetofailoverfaults: 15minutes
totaldowntime 45minutes
Anaverageof45minutesofdowntimeeveryfiveyearsrepresentsanavailabilityof0.99998.
We have reduced our average downtime from four hours every five years to 45 minutes every five years
(good!). We have increased our availability from four 9s to almost five 9s (good!). We have missed our
newSLAofsix9s(bad!).
SowhatcanwedotomeetournewSLA?Theanswerisanactive/activesystem.
Active/Active Systems
An active/active system has two or more nodes. Every
node is actively processing transactions. Every node
has the same view of the application database.
Whenever a node makes a change to its copy of the
application database, that change is immediately
replicatedtotheothernodesinthesystem.
If a node fails, all transactions are routed to the
surviving node (or nodes). Recovery time, mtr, can be
as little as several seconds - typically, new connections and sessions must be established with the
survivingnode(s).However,mtrcanbeeffectivelyzeroifclientsmaintainsessionswithallnodes.
It is known that all nodes are operational; after all, they are actively processing transactions. Therefore,
therewillbenofailoverfaults;anddisequaltozero.
Returningtoourexpressionforapplicationdowntime:
probabilityofapplicationdowntime=f2+(mtr/mtbf)+(fxd)
Letmtr befifteenseconds (theapplication is downfifteenseconds everyfive years duringrecoverytime).
d=0(therearenofailoverfaults).f,thenodalavailability,is10-4.Then
f2=10-8
mtr/mtbf=9x10-8
f2+mtr/mtbf=10x10-8=10-7
Thisisanavailabilityofseven9s.Congratulations!Wemeetouravailabilityofsix9s.
Another advantage of active/active systems is thatthereis noplanned downtime.If nodalmaintenance is
required, simply shift all traffic to one node and upgrade the idle node. Then repeat this process for the
othernode.
Active/active systems provide true continuous availability. If a node fails, no one notices. If a data center
blowsup,noonenotices.
5
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Summary
The availability of an active/backup system is strongly affected by recovery times and failover faults.
Thesefactorsareeliminatedwithactive/activesystems.
If you require application availability in excess of five 9s, consider an active/active architecture.
Active/active systems minimize recovery times and eliminate failover faults. There are manyexamples of
active/activesystemsthathavebeeninservicefordecadeswithoutanoutage.
6
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com