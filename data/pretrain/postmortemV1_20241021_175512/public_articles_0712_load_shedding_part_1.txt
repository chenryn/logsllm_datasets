Load Shedding
December2012
PaulGreen
SeniorTechnicalConsultant
StratusTechnologies,Inc.
Dr.BillHighleyman
AvailabilityDigest
A recent thread in our LinkedIn Continuous Availability Forum covered a very important topic – load
shedding.1 What do you do if your system approaches full capacity? What do you do in an active/active
system if you loseonenodeandthesurvivingnodes mustcarrythefull load?Whatdo you dofollowinga
failoverifyourbackupsystemissmallerthanyourproductionsystem?
If youwanttomaintainareasonablelevelofservice,youmayhavetoshedsomeoftheloadthatisbeing
carriedbythesystem.Butwhichload?
Oneofus(Green)startedthediscussionbyposingthefollowingquestion:
What is the appropriate load-shedding policy when a continuously-available
systembecomesoverloaded?
All processing systems, whether manual or automated, have an upper limit on the
number of transactions they can process per second. As designers of such systems,
we project the capacity needed over a specific period of time and then engineer the system so that it
satisfiesthoserequirements.Astherequirementschangeovertime,were-engineerthesystemtomeet
thenewprojections.
But unanticipated situations happen, whether from a natural event (a hurricane), human error, partial
failure of the system itself, or simply a sudden and unanticipated increase in demand beyond the
capacity ofthesystem.Therefore,itseems thatweneedtospecifically designfor thecaseinwhichthe
incomingtransactionrateexceedstheprocessingcapacityofthesystem.
While internal queuing can handle momentary spikes, it can't handle prolonged spans of time. What
policy should the system follow when it is overloaded? Should it simply let the requests queue up
externally? Should it deny some requests and accept others? Some of these choices depend upon the
nature of the application. All of them have impacts on society. All of them have unintended
consequences.
1 http://www.linkedin.com/groupItem?view=&gid=2586333&type=member&item=185759401&qid=3770f805-5895-4054-b4d0-
a7eab6fa4fc5&trk=group_most_popular-0-b-ttl&goback=%2Egmp_2586333
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

In my experience, many organizations simply side-step this issue by trying to always have enough
capacity. Fine, but as engineers, don't we owe it to our clients to be aware of this problem and try to
solve it anyway? I can assure you that it happens with enough frequency to warrant thoughtful
consideration.
We had dozens of meaningful comments on the subject. An important point that was made was that an
overloadedsystemthatcannotprovideusefulresponsetimesisafailingsystem:
From an end user's point of view, any overload situation is just another system outage. The person (or
machine) waiting for a service and not receiving it could not care less whether some hardware is
broken,somesoftwareran intothewoods,or thereis "just"toomuchdemandonthesystem.Asystem
that is not sufficiently configured to support the current incoming load is just no longer a continuously
availablesystem.
But “failures” due to degraded response times do happen in the real world. Arguably, overloaded
situationsaremorecommonthantotalapplicationoutages.
Anothercommentatornotedthatloadsheddingisnotjustatechnicalconsideration:
While it is tempting to consider load shedding a technical decision, in reality it is a policy decision.
Whatever the decision about how to shed load (either in response to capacity shortfalls or to a security
incident), it needs to be along the lines of policies already decided at the business management level.
Cut-offs often have legal and responsibility implications, quantification and management of which is a
seniormanagementresponsibility.
It's a senior management decision whether they want their system to be continuously available under
highloadconditions ornot.Itcanbeaclearbusinessdecisiontolethighloadbringthebusinessdown.
Butthisshouldbeaconsciousdecision,notleadingtosurprisedfacesandpanickingwhenthisactually
happens.
Some suggested that there be algorithms to discard less important transactions so that others could
complete.
I suggest that the app throw away or reject the transactions that it knows that it won't be able to get to.
A queue scanner, for example, could notate and drop or reject transactions that it knows won't be able
tofinish.Thatwayitcouldkeepthequeue'trimmed'andperhapsgivethetransactiontheopportunityto
quicklyresubmititselftoanotherprocessor.
It may be possible to put priority on incoming requests (i.e. "looking" vs. "booking" requests). If so,
rejectlow-priorityrequestswithaclearerrormessage:"systembusy,pleaseretrylater".Iknowthistext
soundslame,butitseemsbetterthanjustdumptimeoutsforallrequests.
Because an overload is very unlikely, the thing we did was [to] think about priorities. The critical online
applicationhasthehighestprioritiesandthenon-criticalonlineandbatch-applicationshavesignificantly
lower priorities. In addition our monitoring checks for processes causing too much load, and it reduces
thepriorityforsuchprocessesautomatically.
I'll relate one design element that we had in a law enforcement message switching application. That
systemprocessedmanydifferenttypesofinquiries-somecritical,someroutine.Wedesignedtheinitial
message parser to reject all messages of a given type if system resources were being taxed. The user
defined the order in which we would start rejecting traffic. That let the user continue to process the
higher priority workload during abnormal conditions. We quickly learned that we had to add a delay to
the "reject" message to prevent the users from immediately resending and taxing the system even
more.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Ithink the lesson learnedhereis that you have to plan carefully what load toshed.Unfortunately,there
are many monitors on the NonStop that are themselves very heavy consumers and probably should be
curtailed or reduced in capability. I would even go so far as to say that ongoing Measure (a good
practice)shouldhavedataamountsreducedandintervalslengthenedduringyour"shed."
Onepersonthoughtthatmovingexcessloadtoacloudmaybeapossiblesolution:
One argument for cloud computing is the theory that there should always be enough capacity available
in the background to match the current demand. There is no reason why a continuously available
system could not be part of a cloud, or even that continuously available systems might be the stuff that
thecloudisbuiltof.
In the NonStop world, there is a mechanism called Persistent Cloud Services that can be used to
provision such extra capacity for high load situations. But care needs to be taken - when moving load
into the cloud, you'll see extra latency (potentially disturbing); and you may also get hurt if you don't
have fully dedicated physical servers assigned to you, but rather rely on virtual servers which in turn
might run on physical servers that get overloaded. If you have a big business peak shortly before
Christmas, you might not be the only one - and the cloud you are relying on just might get overloaded
too. The cloud-service providing business is a pretty tough one, margins are low, and those who do
actuallybelievethattheirproviderwillgenerouslyinvestinamplecapacitynotneededfortherestofthe
yearmaybeinforasurprise.
However,Iamdubiousaboutcloudsolutions,tooriskyandoutofourcontrol.
CapacityplanningismorethanCPUworkload:
Veryoften,capacityplanningisdonebylookingattheCPUloadonly.Butwehavetolookateverything
that could reach the limits. If your communication lines do not have the necessary bandwidth, you will
never have a chance to differentiate between the messages. And problems with line capacity can
sometimes have very mysterious symptoms. A few years ago, we had protocol errors on a leased line;
and that was the result of a bandwidth problem. The bandwidth was increased and the problems were
gone.
Or think of such things as HSMs (Hardware Security Managers). Today we need a lot of HSM capacity
as we have to regard rules like PCI/DSS. What happens if an HSM goes down? Will the remaining
HSMsbeabletohandletheload?
Sogoodandeffectivecapacity planninginvolves allthecomponents needed.Istillprefer tolook atthat
datamyselftoverifythateverythingisok.Idonotthink thereisanyproductavailablewiththe"lookand
feel"experiencewillgiveus.
WhatcanwedoaboutDistributedDenialofServiceattacks?
A classic example that remains a problem is a web site under a DDoS attack in which the attacker is
not yet identified (and hence the false requests can't be distinguished from the true ones). Distributed
Denial of Service attacks are indeed a very special situation and ought to be rejected already at the
network level. It would be a very tough requirement to size an OLTP system for the coordinated
simultaneous attack of millions of botnet PC's hijacked by cybercriminals. But systems ought to be
sizedtohandlethemaximumconceivablegenuineworkload.
If you usually share the load between your systems, a DDoS attack will hit all the systems. So at the
very beginning we already decided against such a load-sharing. In our active/active architecture, both
systemshavetheirowncommunicationlineswithdifferentaddresses(X.25andTCP/IP).Thecustomer
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

can use whatever system he chooses, but he has to be able to switch the communication to the other
system.SoaDDOSattackwouldhavetodealwith2systems.
Theultimateistoneverrunoutofcapacity.DamianWardofVocaLinkespousesthisphilosophy:2
We run a 100% available service that uses 2 HP NonStop servers deployed in an Active/Active
configuration.Ourbusinessis“riskaverse,”soweoperateapolicywhereapeakdaycanbeprocessed
by a single NonStop system with 1 CPU down. Additionally, in this configuration, no single CPU can be
operatingatgreaterthan80%utilisation.
Imagine a 6 CPU system, (capacity 600), and remove a CPU from the theoretical model giving 5 cpu’s
(capacity 500). Then multiply the 5 remaining CPU's by 80% which actually only leaves us with a
capacityof400(4CPU'sworthinthemodel)todowork.
