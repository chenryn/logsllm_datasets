# Details of the Cloudflare outage on July 2, 2019 

07/12/2019

![John
    Graham-Cumming](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=64,height=64/https://blog.cloudflare.com/content/images/2017/03/url-2.jpg)

19 min read

Almost nine years ago, Cloudflare was a tiny company and I was a
customer not an employee. Cloudflare had launched a month earlier and
one day alerting told me that my little site,
[jgc.org](https://jgc.org/), didn't seem to have working DNS any more.
Cloudflare had pushed out a change to its use of [Protocol
Buffers](https://developers.google.com/protocol-buffers/) and it had
broken DNS.

I wrote to Matthew Prince directly with an email titled "Where's my
dns?" and he replied with a long, detailed, technical response (you can
read the [full email exchange
here](https://gist.github.com/jgrahamc/6bb02a6f7c3799a1590b3cdb901f8e08))
to which I replied:

    From: John Graham-Cumming
    Date: Thu, Oct 7, 2010 at 9:14 AM
    Subject: Re: Where's my dns?
    To: Matthew Prince

    Awesome report, thanks. I'll make sure to call you if there's a
    problem.  At some point it would probably be good to write this up as
    a blog post when you have all the technical details because I think
    people really appreciate openness and honesty about these things.
    Especially if you couple it with charts showing your post launch
    traffic increase.

    I have pretty robust monitoring of my sites so I get an SMS when
    anything fails.  Monitoring shows I was down from 13:03:07 to
    14:04:12.  Tests are made every five minutes.

    It was a blip that I'm sure you'll get past.  But are you sure you
    don't need someone in Europe? :-)

To which he replied:

    From: Matthew Prince
    Date: Thu, Oct 7, 2010 at 9:57 AM
    Subject: Re: Where's my dns?
    To: John Graham-Cumming

    Thanks. We've written back to everyone who wrote in. I'm headed in to
    the office now and we'll put something on the blog or pin an official
    post to the top of our bulletin board system. I agree 100%    
    transparency is best.

And so, today, as an employee of a much, much larger Cloudflare I get to
be the one who writes, transparently about a mistake we made, its impact
and what we are doing about it.

### The events of July 2

On July 2, we deployed a new rule in our WAF Managed Rules that [caused
CPUs to become
exhausted](https://blog.cloudflare.com/cloudflare-outage/) on every CPU
core that handles HTTP/HTTPS traffic on the Cloudflare network
worldwide. We are constantly improving WAF Managed Rules to respond to
new vulnerabilities and threats. In May, for example, we used the speed
with which we can update the WAF to [push a
rule](https://blog.cloudflare.com/stopping-cve-2019-0604/) to protect
against a serious SharePoint vulnerability. Being able to deploy rules
quickly and globally is a critical feature of our
[WAF](https://www.cloudflare.com/learning/ddos/glossary/web-application-firewall-waf/).

Unfortunately, last Tuesday's update contained a regular expression that
backtracked enormously and exhausted CPU used for HTTP/HTTPS serving.
This brought down Cloudflare's core proxying, CDN and WAF functionality.
The following graph shows CPUs dedicated to serving HTTP/HTTPS traffic
spiking to nearly 100% usage across the servers in our network.

![CPU utilization in one of our PoPs during the
incident](https://blog.cloudflare.com/content/images/2019/07/cpu-goes-boom.png)

This resulted in our customers (and their customers) seeing a 502 error
page when visiting any Cloudflare domain. The 502 errors were generated
by the front line Cloudflare web servers that still had CPU cores
available but were unable to reach the processes that serve HTTP/HTTPS
traffic.

![](https://blog.cloudflare.com/content/images/2019/07/502-bad-gateway.png)

We know how much this hurt our customers. We're ashamed it happened. It
also had a negative impact on our own operations while we were dealing
with the incident.

It must have been incredibly stressful, frustrating and frightening if
you were one of our customers. It was even more upsetting because we
haven't had a [global
outage](https://blog.cloudflare.com/todays-outage-post-mortem-82515/)
for six years.

The CPU exhaustion was caused by a single WAF rule that contained a
poorly written regular expression that ended up creating excessive
backtracking. The regular expression that was at the heart of the outage
is
`` (?:(?:\"|'|\]|\}|\\|\d|(?:nan|infinity|true|false|null|undefined|symbol|math)|\`|\-|\+)+[)]*;?((?:\s|-|~|!|{}|\|\||\+)*.*(?:.*=.*))) ``

Although the regular expression itself is of interest to many people
(and is discussed more below), the real story of how the Cloudflare
service went down for 27 minutes is much more complex than "a regular
expression went bad". We've taken the time to write out the series of
events that led to the outage and kept us from responding quickly. And,
if you want to know more about regular expression backtracking and what
to do about it, then you'll find it in an appendix at the end of this
post.

### What happened

Let's begin with the sequence of events. All times in this blog are UTC.

At 13:42 an engineer working on the firewall team deployed a minor
change to the rules for
[XSS](https://www.cloudflare.com/learning/security/threats/cross-site-scripting/)
detection via an automatic process. This generated a Change Request
ticket. We use Jira to manage these tickets and a screenshot is below.

Three minutes later the first PagerDuty page went out indicating a fault
with the WAF. This was a synthetic test that checks the functionality of
the WAF (we have hundreds of such tests) from outside Cloudflare to
ensure that it is working correctly. This was rapidly followed by pages
indicating many other end-to-end tests of Cloudflare services failing, a
global traffic drop alert, widespread 502 errors and then many reports
from our points-of-presence (PoPs) in cities worldwide indicating there
was CPU exhaustion.

![](https://blog.cloudflare.com/content/images/2019/07/pager-duty-1345.png)

![](https://blog.cloudflare.com/content/images/2019/07/pager-duty-1346.jpg)

Some of these alerts hit my watch and I jumped out of the meeting I was
in and was on my way back to my desk when a leader in our Solutions
Engineering group told me we had lost 80% of our traffic. I ran over to
SRE where the team was debugging the situation. In the initial moments
of the outage there was speculation it was an attack of some type we'd
never seen before.

![](https://blog.cloudflare.com/content/images/2019/07/pager-duty-1348.jpg)

Cloudflare's SRE team is distributed around the world, with continuous,
around-the-clock coverage. Alerts like these, the vast majority of which
are noting very specific issues of limited scopes in localized areas,
are monitored in internal dashboards and addressed many times every day.
This pattern of pages and alerts, however, indicated that something
gravely serious had happened, and SRE immediately declared a P0 incident
and escalated to engineering leadership and systems engineering.

The London engineering team was at that moment in our main event space
listening to an internal tech talk. The talk was interrupted and
everyone assembled in a large conference room and others dialed-in. This
wasn't a normal problem that SRE could handle alone, it needed every
relevant team online at once.

At 14:00 the WAF was identified as the component causing the problem and
an attack dismissed as a possibility. The Performance Team pulled live
CPU data from a machine that clearly showed the WAF was responsible.
Another team member used strace to confirm. Another team saw error logs
indicating the WAF was in trouble. At 14:02 the entire team looked at me
when it was proposed that we use a 'global terminate', a mechanism built
into Cloudflare to disable a single component worldwide.

But getting to the global WAF termination was another story. Things
stood in our way. We use our own products and with our
[Access](https://www.cloudflare.com/products/cloudflare-access/) service
down we couldn't authenticate to our internal control panel (and once we
were back we'd discover that some members of the team had lost access
because of a security feature that disables their credentials if they
don't use the internal control panel frequently).

And we couldn't get to other internal services like Jira or the build
system. To get to them, we had to use a bypass mechanism that wasn't
frequently used (another thing to drill on after the event). Eventually,
a team member executed the global WAF termination at 14:07 and by 14:09
traffic levels and CPU were back to expected levels worldwide. The rest
of Cloudflare\'s protection mechanisms continued to operate.

Then we moved on to restoring the WAF functionality. Because of the
sensitivity of the situation we performed both negative tests (asking
ourselves "was it really that particular change that caused the
problem?") and positive tests (verifying the rollback worked) in a
single city using a subset of traffic after removing our paying
customers' traffic from that location.

At 14:52 we were 100% satisfied that we understood the cause and had a
fix in place and the WAF was re-enabled globally.

### How Cloudflare operates

Cloudflare has a team of engineers who work on our WAF Managed Rules
product; they are constantly working to improve detection rates, lower
false positives, and respond rapidly to new threats as they emerge. In
the last 60 days, 476 change requests have been handled for the WAF
Managed Rules (averaging one every 3 hours).

This particular change was to be deployed in "simulate" mode where real
customer traffic passes through the rule but nothing is blocked. We use
that mode to test the effectiveness of a rule and measure its false
positive and false negative rate. But even in the simulate mode the
rules actually need to execute and in this case the rule contained a
regular expression that consumed excessive CPU.

![](https://blog.cloudflare.com/content/images/2019/07/change-request.png)

As can be seen from the Change Request above there's a deployment plan,
a rollback plan and a link to the internal Standard Operating Procedure
(SOP) for this type of deployment. The SOP for a rule change
specifically allows it to be pushed globally. This is very different
from all the software we release at Cloudflare where the SOP first
pushes software to an internal dogfooding network point of presence
(PoP) (which our employees pass through), then to a small number of
customers in an isolated location, followed by a push to numerous
customers and finally to the world.

The process for a software release looks like this: We use git
internally via BitBucket. Engineers working on changes push code which
is built by TeamCity and when the build passes, reviewers are assigned.
Once a pull request is approved the code is built and the test suite
runs (again).

If the build and tests pass then a Change Request Jira is generated, and
the change has to be approved by the relevant manager or technical lead.
Once approved deployment to what we call the "animal PoPs" occurs: DOG,
PIG, and the [Canaries](https://en.wikipedia.org/wiki/Sentinel_species).

The DOG PoP is a Cloudflare PoP (just like any of our cities worldwide)
but it is used only by Cloudflare employees. This dogfooding PoP enables
us to catch problems early before any customer traffic has touched the
code. And it frequently does.

If the DOG test passes successfully code goes to PIG (as in "Guinea
Pig"). This is a Cloudflare PoP where a small subset of customer traffic
from non-paying customers passes through the new code.

If that is successful the code moves to the Canaries. We have three
Canary PoPs spread across the world and run paying and non-paying
customer traffic running through them on the new code as a final check
for errors.

![Cloudflare software release
process](https://blog.cloudflare.com/content/images/2019/07/animal-deploy-1.png)

Once successful in Canary the code is allowed to go live. The entire
DOG, PIG, Canary, Global process can take hours or days to complete,
depending on the type of code change. The diversity of Cloudflare's
network and customers allows us to test code thoroughly before a release
is pushed to all our customers globally. But, by design, the WAF doesn't
use this process because of the need to respond rapidly to threats.

### WAF Threats

In the last few years we have seen a dramatic increase in
vulnerabilities in common applications. This has happened due to the
increased availability of software testing tools, like fuzzing for
example (we just posted a new blog on fuzzing
[here](https://blog.cloudflare.com/a-gentle-introduction-to-linux-kernel-fuzzing/)).

![Source:
https://cvedetails.com/](https://blog.cloudflare.com/content/images/2019/07/Number-of-CVEs-per-year.png)

What is commonly seen is a Proof of Concept (PoC) is created and often
published on GitHub quickly, so that teams running and maintaining
applications can test to make sure they have adequate protections.
Because of this, it's imperative that Cloudflare are able to react as
quickly as possible to new attacks to give our customers a chance to
patch their software.

A great example of how Cloudflare proactively provided this protection
was through the deployment of our protections against the SharePoint
vulnerability in May ([blog
here](https://blog.cloudflare.com/stopping-cve-2019-0604/)). Within a
short space of time from publicised announcements, we saw a huge spike
in attempts to exploit our customer's Sharepoint installations. Our team
continuously monitors for new threats and writes rules to mitigate them
on behalf of our customers.

The specific rule that caused last Tuesday's outage was targeting
Cross-site scripting (XSS) attacks. These, too, have increased
dramatically in recent years.

![Source:
https://cvedetails.com/](https://blog.cloudflare.com/content/images/2019/07/Number-of-XSS-CVEs-by-year.png)

The standard procedure for a WAF Managed Rules change indicates that
Continuous Integration (CI) tests must pass prior to a global deploy.
That happened normally last Tuesday and the rules were deployed. At
13:31 an engineer on the team had merged a Pull Request containing the
change after it was approved.

![](https://blog.cloudflare.com/content/images/2019/07/change-details.png)

At 13:37 TeamCity built the rules and ran the tests, giving it the green
light. The WAF test suite tests that the core functionality of the WAF
works and consists of a large collection of unit tests for individual
matching functions. After the unit tests run the individual WAF rules
are tested by executing a huge collection of HTTP requests against the
WAF. These HTTP requests are designed to test requests that should be
blocked by the WAF (to make sure it catches attacks) and those that
should be let through (to make sure it isn't over-blocking and creating
false positives). What it didn't do was test for runaway CPU utilization
by the WAF and examining the log files from previous WAF builds shows
that no increase in test suite run time was observed with the rule that
would ultimately cause CPU exhaustion on our edge.

With the tests passing, TeamCity automatically began deploying the
change at 13:42.

![](https://blog.cloudflare.com/content/images/2019/07/build-process.png)

### Quicksilver

Because WAF rules are required to address emergent threats they are
