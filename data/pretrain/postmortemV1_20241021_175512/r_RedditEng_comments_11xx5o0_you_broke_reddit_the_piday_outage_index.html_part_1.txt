# You Broke Reddit: The Pi-Day Outage 

Cute error image friends, we love them.

Been a while since that was our 500 page, hasn't it? It was cute and
fun. We've now got our terribly overwhelmed Snoo being crushed by a pile
of upvotes. Unfortunately, if you were browsing the site, or at least
trying, during the afternoon of March 14th during US hours, you may have
seen our unfortunate Snoo during the [314-minute
outage](https://www.reddit.com/r/shittychangelog/comments/11rhs8z/in_celebration_of_pi_day_we_took_the_site_down/) Reddit
faced (on Pi day no less!) Or maybe you just saw the homepage with no
posts. Or an error. One way or another, Reddit was definitely broken.
But it wasn't you, it was us.

Today we're going to talk about the Pi day outage, but I want to make
sure we give our team(s) credit where due. Over the last few years,
we've put a major emphasis on improving availability. In fact, there's a
[great blog post from our
CTO](https://www.reddit.com/r/RedditEng/comments/10egj46/seeing_the_forest_in_the_trees_two_years_of/)
talking about our improvements over time. In classic Reddit form, I'll
steal the image and repost it as my own.

[](https://preview.redd.it/you-broke-reddit-the-pi-day-outage-v0-oc33dikt26pa1.png?width=1282&format=png&auto=webp&s=ea5d2d0565fb1245ad7d0a75e243b1506197cbe3 "Image from r/RedditEng - Reddit daily availability vs current SLO target.")

Reddit daily availability vs current SLO target.

As you can see, we've made some pretty strong progress in improving
Reddit's availability. As we've emphasized the improvements, we've
worked to de-risk changes, but we're not where we want to be in every
area yet, so we know that some changes remain unreasonably risky.
Kubernetes version and component upgrades remain a big footgun for us,
and indeed, this was a major trigger for our 3/14 outage.

# TL;DR 

-   Upgrades, particularly to our Kubernetes clusters, are risky for us,
    but we must do them anyway. We test and validate them in advance as
    best we can, but we still have plenty of work to do.

-   Upgrading from Kubernetes 1.23 to 1.24 on the particular cluster we
    were working on bit us in a new and subtle way we'd never seen
    before. It took us hours to decide that a rollback, a high-risk
    action on its own, was the best course of action.

-   Restoring from a backup is scary, and we hate it. The process we
    have for this is laden with pitfalls and must be improved.
    Fortunately, it worked!

-   We didn't find the extremely subtle cause until hours after we
    pulled the ripcord and restored from a backup.

-   Not everything went down. Our modern service API layers all remained
    up and resilient, but this impacted the most critical legacy node in
    our dependency graph, so the blast radius still included most user
    flows; more work remains in our modernization drive.

-   Never waste a good crisis -- we're resolute in using this outage to
    change some of the major architectural and process decisions we've
    lived with for a long time and we're going to make our cluster
    upgrades safe.

# It Begins 

It's funny in an ironic sort of way. As a team, we had just finished up
an internal postmortem for a previous Kubernetes upgrade that had gone
poorly; but only mildly, and for an entirely resolved cause. So we were
kicking off another upgrade of the same cluster.

We've been cleaning house quite a bit this year, trying to get to a more
maintainable state internally. Managing Kubernetes (k8s) clusters has
been painful in a number of ways. Reddit has been on cloud since 2009,
and started adopting k8s relatively early. Along the way, we accumulated
a set of bespoke clusters built using the kubeadm tool rather than any
standard template. Some of them have even been too large to support
under various cloud-managed offerings. That history led to an
inconsistent upgrade cadence, and split configuration between clusters.
We'd raised a set of pets, not managed a herd of cattle.

The Compute team manages the parts of our infrastructure related to
running workloads, and has spent a long time defining and refining our
upgrade process to try and improve this. Upgrades are tested against a
dedicated set of clusters, then released to the production environments,
working from lowest criticality to highest. This upgrade cycle was one
of our team's big-ticket items this quarter, and one of the most
important clusters in the company, the one running the Legacy part of
our stack (affectionately referred to by the community as Old Reddit),
was ready to be upgraded to the next version. The engineer doing the
work kicked off the upgrade just after 19:00 UTC, and everything seemed
fine, for about 2 minutes. Then? Chaos.

[](https://preview.redd.it/you-broke-reddit-the-pi-day-outage-v0-ngnso6qw26pa1.png?width=1438&format=png&auto=webp&s=3784126af329d398e056fc9c8b3b04a0300e31eb "Image from r/RedditEng - Reddit edge traffic, RPS by status. Oh, thatâ€™s... not ideal. ")

Reddit edge traffic, RPS by status. Oh, that's\... not ideal.

All at once the site came to a screeching halt. We opened an incident
immediately, and brought all hands on deck, trying to figure out what
had happened. Hands were on deck and in the call by T+3 minutes. The
first thing we realized was that the affected cluster had completely
lost all metrics (the above graph shows stats at our CDN edge, which is
intentionally separated). We were flying blind. The only thing sticking
out was that DNS wasn't working. We couldn't resolve records for entries
in Consul (a service we run for cross-environment dynamic DNS), or for
in-cluster DNS entries. But, weirdly, it was resolving requests for
public DNS records just fine. We tugged on this thread for a bit, trying
to find what was wrong, to no avail. This was a problem we had never
seen before, in previous upgrades anywhere else in our fleet, or our
tests performing upgrades in non-production environments.

For a deployment failure, immediately reverting is always "Plan A", and
we definitely considered this right off. But, dear Redditor...
Kubernetes has no supported downgrade procedure. Because a number of
schema and data migrations are performed automatically by Kubernetes
during an upgrade, there's no reverse path defined. Downgrades thus
require a restore from a backup and state reload!

We are sufficiently paranoid, so of course our upgrade procedure
includes taking a backup as standard. However, this backup procedure,
and the restore, were written several years ago. While the restore had
been tested repeatedly and extensively in our pilot clusters, it hadn't
been kept fully up to date with changes in our environment, and we'd
never had to use it against a production cluster, let alone *this*
cluster. This meant, of course, that we were scared of it -- We didn't
know precisely how long it would take to perform, but initial estimates
were on the order of hours... of *guaranteed* downtime. The decision was
made to continue investigating and attempt to fix forward.

# It's Definitely Not A Feature, It's A Bug 

About 30 minutes in, we still hadn't found clear leads. More people had
joined the incident call. Roughly a half-dozen of us from various
on-call rotations worked hands-on, trying to find the problem, while
dozens of others observed and gave feedback. Another 30 minutes went by.
We had some promising leads, but not a definite solution by this point,
so it was time for contingency planning... we picked a subset of the
Compute team to fork off to another call and prepare all the steps to
restore from backup.

In parallel, several of us combed logs. We tried restarts of components,
thinking perhaps some of them had gotten stuck in an infinite loop or a
leaked connection from a pool that wasn't recovering on its own. A few
things were noticed:

-   Pods were taking an extremely long time to start and stop.

-   Container images were also taking a very long time to pull (on the
    order of minutes for \<100MB images over a multi-gigabit
    connection).

-   Control plane logs were flowing heavily, but not with any truly
    obvious errors.

At some point, we noticed that our [container network
interface](https://www.cni.dev/), Calico, wasn't working
properly. Pods for it weren't healthy. Calico has three main components
that matter in our environment:

-   calico-kube-controllers: Responsible for taking action based on
    cluster state to do things like assigning IP pools out to nodes for
    use by pods.

-   calico-typha: An aggregating, caching proxy that sits between other
    parts of Calico and the cluster control plane, to reduce load on the
    Kubernetes API.

-   calico-node: The guts of networking. An agent that runs on each node
    in the cluster, used to dynamically generate and register network
    interfaces for each pod on that node.

The first thing we saw was that the calico-kube-controllers pod was
stuck in a ContainerCreating status. As a part of upgrading the control
plane of the cluster, we also have to upgrade the container runtime to a
supported version. In our environment, we use CRI-O as our container
runtime and recently we'd identified a low severity bug when upgrading
CRI-O on a given host, where one-or-more containers exited, and then
randomly and at low rate got stuck starting back up. The quick fix for
this is to just delete the pod, and it gets recreated and we move on. No
such luck, not the problem here.

[](https://preview.redd.it/you-broke-reddit-the-pi-day-outage-v0-hdqjfez236pa1.png?width=624&format=png&auto=webp&s=77d1a31c6d4f42cb1b83f9e1eb2df86dd893e361 "Image from r/RedditEng - This fixes everything, I swear!")

This fixes everything, I swear!

Next, we decided to restart calico-typha. This was one of the spots that
got interesting. We deleted the pods, and waited for them to restart...
and they didn't. The new pods didn't get created immediately. We waited
a couple minutes, no new pods. In the interest of trying to get things
unstuck, we issued a rolling restart of the control plane components. No
change. We also tried the classic option: We turned the whole control
plane off, all of it, and turned it back on again. We didn't have a lot
of hope that this would turn things around, and it didn't.

At this point, someone spotted that we were getting a lot of timeouts in
the API server logs for write operations. But not specifically on the
writes themselves. Rather, it was timeouts calling the [admission
controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/) on the
cluster. Reddit utilizes several different admission controller
webhooks. On this cluster in particular, the only admission controller
we use that's generalized to watch all resources is [Open Policy Agent
(OPA)](https://www.openpolicyagent.org/). Since it was down anyway,
we took this opportunity to delete its webhook configurations. The
timeouts disappeared instantly... But the cluster didn't recover.

# Let 'Er Rip (Conquering Our Fear of Backup Restores) 

We were running low on constructive ideas, and the outage had gone on
for over two hours at this point. It was time to make the hard call; we
would make the restore from backup. Knowing that most of the worker
nodes we had running would be invalidated by the restore anyway, we
started terminating all of them, so we wouldn't have to deal with the
long reconciliation after the control plane was back up. As our largest
cluster, this was unfortunately time-consuming as well, taking about 20
minutes for all the API calls to go through.

Once that was finished, we took on the restore procedure, which nobody
involved had ever performed before, let alone on our favorite single
point of failure. Distilled down, the procedure looked like this:

1.  Terminate two control plane nodes.

2.  Downgrade the components of the remaining one.

3.  Restore the data to the remaining node.
