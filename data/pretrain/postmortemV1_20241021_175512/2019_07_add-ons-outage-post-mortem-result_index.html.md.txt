# Add-Ons Outage Post-Mortem Result 

*Editor's Note: July 12, 1:52pm pt -- Updated Balrog update frequency
and added some more background.*

As I mentioned in my previous post, we've been conducting a
[post-mortem](https://hacks.mozilla.org/2019/05/technical-details-on-the-recent-firefox-add-on-outage/) on the add-ons outage. Sorry this took so long to get
out; we'd hoped to have this out within a week, but obviously that
didn't happen. There was just a lot more digging to do than we expected.
In any case, we're now ready to share the results. This post provides a
high level overview of our findings, with more detail available in
[Sheila Mooney's incident
report](https://wiki.mozilla.org/Add-ons/Expired-Certificate) and [Matt Miller & Peter Saint-Andre's technical
report](https://wiki.mozilla.org/Add-ons/Expired-Certificate-Technical-Report).

## Root Cause Analysis

The first question that everyone asks is "how did you let this happen?"
At a high level, the story seems simple: we let the certificate expire.
This seems like a simple failure of planning, but upon further
investigation it turns out to be more complicated: the team responsible
for the system which generated the signatures knew that the certificate
was expiring but thought (incorrectly) that Firefox ignored the
expiration dates. Part of the reason for this misunderstanding was that
in a previous incident we had disabled end-entity certificate checking,
and this led to confusion about the status of intermediate certificate
checking. Moreover, the Firefox QA plan didn't incorporate testing for
certificate expiration (or generalized testing of how the browser will
behave at future dates) and therefore the problem wasn't detected. This
seems to have been a fundamental oversight in our test plan.

The lesson here is that: (1) we need better communication and
documentation of these parts of the system and (2) this information
needs to get fed back into our engineering and QA work to make sure
we're not missing things. The technical report provides more details.

## Code Delivery

As I mentioned previously, once we had a fix, we decided to deliver it
via the Studies system (this is one part of a system we internally call
"Normandy"). The Studies system isn't an obvious choice for this kind of
deployment because it was intended for deploying experiments, not code
fixes. Moreover, because Studies permission is coupled to Telemetry,
this meant that some users needed to enable Telemetry in order to get
the fix, leading to Mozilla temporarily over-collecting data that we
didn't actually want, [which we then had to clean
up](https://blog.mozilla.org/blog/2019/05/09/what-we-do-when-things-go-wrong/).

This leads to the natural question: "isn't there some other way you
could have deployed the fix?" to which the answer is "sort of." Our
other main mechanisms for deploying new code to users are dot releases
and a system called "Balrog". Unfortunately, both of these are slower
than Normandy: Balrog checks for updates every 12 hours (though there
turns out to have been some confusion about whether this number was 12
or 24), whereas Normandy checks every 6. Because we had a lot of users
who were affected, getting them fixed was a very high priority, which
made Studies the best technical choice.

The lesson here is that we need a mechanism that allows fast updates
that isn't coupled to Telemetry and Studies. The property we want is the
ability to quickly deploy updates to any user who has automatic updates
enabled. This is something our engineers are already working on.

## Incomplete Fixes

Over the weeks following the incident, we released a large number of
fixes, including eight versions of the system add-on and six dot
releases. In some cases this was necessary because older deployment
targets needed a separate fix. In other cases it was a result of defects
in an earlier fix, which we then had to patch up in subsequent work. Of
course, defects in software cannot be completely eliminated, but the
technical report found that at least in some cases a high level of
urgency combined with a lack of available QA resources (or at least
coordination issues around QA) led to testing that was less thorough
than we would have liked.

The lesson here is that during incidents of this kind we need to make
sure that we not only recruit management, engineering, and operations
personnel (which we did) but also to ensure that we have QA available to
test the inevitable fixes.

