time. This procedure is tested daily at minimum, so the recovery time
frame was well understood, however until this incident we have never
needed to fully rebuild an entire cluster from backup and had instead
been able to rely on other strategies such as delayed replicas.

### 2018 October 22 00:41 UTC[](#2018-october-22-0041-utc) 

A backup process for all affected MySQL clusters had been initiated by
this time and engineers were monitoring progress. Concurrently, multiple
teams of engineers were investigating ways to speed up the transfer and
recovery time without further degrading site usability or risking data
corruption.

### 2018 October 22 06:51 UTC[](#2018-october-22-0651-utc) 

Several clusters had completed restoration from backups in our US East
Coast data center and begun replicating new data from the West Coast.
This resulted in slow site load times for pages that had to execute a
write operation over a cross-country link, but pages reading from those
database clusters would return up-to-date results if the read request
landed on the newly restored replica. Other larger database clusters
were still restoring.

Our teams had identified ways to restore directly from the West Coast to
overcome throughput restrictions caused by downloading from off-site
storage and were increasingly confident that restoration was imminent,
and the time left to establishing a healthy replication topology was
dependent on how long it would take replication to catch up. This
estimate was linearly interpolated from the replication telemetry we had
available and the status page was
[updated](https://twitter.com/githubstatus/status/1054264047250608130)
to set an expectation of two hours as our estimated time of recovery.

### 2018 October 22 07:46 UTC[](#2018-october-22-0746-utc) 

GitHub published a [blog
post](https://blog.github.com/2018-10-21-october21-incident-report) to
provide more context. We use GitHub Pages internally and all builds had
been paused several hours earlier, so publishing this took additional
effort. We apologize for the delay. We intended to send this
communication out much sooner and will be ensuring we can publish
updates in the future under these constraints.

### 2018 October 22 11:12 UTC[](#2018-october-22-1112-utc) 

All database primaries established in US East Coast again. This resulted
in the site becoming far more responsive as writes were now directed to
a database server that was co-located in the same physical data center
as our application tier. While this improved performance substantially,
there were still dozens of database read replicas that were multiple
hours delayed behind the primary. These delayed replicas resulted in
users seeing inconsistent data as they interacted with our services. We
spread the read load across a large pool of read replicas and each
request to our services had a good chance of hitting a read replica that
was multiple hours delayed.

In reality, the time required for replication to catch up had adhered to
a power decay function instead of a linear trajectory. Due to increased
write load on our database clusters as users woke up and began their
workday in Europe and the US, the recovery process took longer than
originally estimated.

### 2018 October 22 13:15 UTC[](#2018-october-22-1315-utc) 

By now, we were approaching peak traffic load on GitHub.com. A
discussion was had by the incident response team on how to proceed. It
was clear that replication delays were increasing instead of decreasing
towards a consistent state. We'd begun provisioning additional MySQL
read replicas in the US East Coast public cloud earlier in the incident.
Once these became available it became easier to spread read request
volume across more servers. Reducing the utilization in aggregate across
the read replicas allowed replication to catch up.

### 2018 October 22 16:24 UTC[](#2018-october-22-1624-utc) 

Once the replicas were in sync, we conducted a failover to the original
topology, addressing the immediate latency/availability concerns. As
part of a conscious decision to prioritize data integrity over a shorter
incident window, we kept the service [status
red](https://twitter.com/githubstatus/status/1054408042836606977) while
we began processing the backlog of data we had accumulated.

### 2018 October 22 16:45 UTC[](#2018-october-22-1645-utc) 

During this phase of the recovery, we had to balance the increased load
represented by the backlog, potentially overloading our ecosystem
partners with notifications, and getting our services back to 100% as
quickly as possible. There were over five million hook events and 80
thousand Pages builds queued.

As we re-enabled processing of this data, we processed \~200,000 webhook
payloads that had outlived an internal TTL and were dropped. Upon
discovering this, we paused that processing and pushed a change to
increase that TTL for the time being.

To avoid further eroding the reliability of our status updates, we
remained in degraded status until we had completed processing the entire
backlog of data and ensured that our services had clearly settled back
into normal performance levels.

### 2018 October 22 23:03 UTC[](#2018-october-22-2303-utc) 

All pending webhooks and Pages builds had been processed and the
integrity and proper operation of all systems had been confirmed. The
site status was [updated to
green](https://twitter.com/githubstatus/status/1054508689560870912).

## Next steps[](#next-steps)

### Resolving data inconsistencies[](#resolving-data-inconsistencies)

During our recovery, we captured the MySQL binary logs containing the
writes we took in our primary site that were not replicated to our West
Coast site from each affected cluster. The total number of writes that
were not replicated to the West Coast was relatively small. For example,
one of our busiest clusters had 954 writes in the affected window. We
are currently performing an analysis on these logs and determining which
writes can be automatically reconciled and which will require outreach
to users. We have multiple teams engaged in this effort, and our
analysis has already determined a category of writes that have since
been repeated by the user and successfully persisted. As stated in this
analysis, our primary goal is preserving the integrity and accuracy of
the data you store on GitHub.

### Communication[](#communication)

In our desire to communicate meaningful information to you during the
incident, we made several public estimates on time to repair based on
the rate of processing of the backlog of data. In retrospect, our
estimates did not factor in all variables. We are sorry for the
confusion this caused and will strive to provide more accurate
information in the future.

### Technical initiatives[](#technical-initiatives)

There are a number of technical initiatives that have been identified
during this analysis. As we continue to work through an extensive
post-incident analysis process internally, we expect to identify even
more work that needs to happen.

1.  Adjust the configuration of Orchestrator to prevent the promotion of
    database primaries across regional boundaries. Orchestrator's
    actions behaved as configured, despite our application tier being
    unable to support this topology change. Leader-election within a
    region is generally safe, but the sudden introduction of
    cross-country latency was a major contributing factor during this
    incident. This was emergent behavior of the system given that we
    hadn't previously seen an internal network partition of this
    magnitude.
2.  We have accelerated our migration to a new status reporting
    mechanism that will provide a richer forum for us to talk about
    active incidents in crisper and clearer language. While many
    portions of GitHub were available throughout the incident, we were
    only able to set our status to green, yellow, and red. We recognize
    that this doesn't give you an accurate picture of what is working
    and what is not, and in the future will be displaying the different
    components of the platform so you know the status of each service.
3.  In the weeks prior to this incident, we had started a company-wide
    engineering initiative to support serving GitHub traffic from
    multiple data centers in an active/active/active design. This
    project has the goal of supporting N+1 redundancy at the facility
    level. The goal of that work is to tolerate the full failure of a
    single data center failure without user impact. This is a major
    effort and will take some time, but we believe that multiple
    well-connected sites in a geography provides a good set of
    trade-offs. This incident has added urgency to the initiative.
4.  We will take a more proactive stance in testing our assumptions.
    GitHub is a fast growing company and has built up its fair share of
    complexity over the last decade. As we continue to grow, it becomes
    increasingly difficult to capture and transfer the historical
    context of trade-offs and decisions made to newer generations of
    Hubbers.

### Organizational initiatives[](#organizational-initiatives)

This incident has led to a shift in our mindset around site reliability.
We have learned that tighter operational controls or improved response
times are insufficient safeguards for site reliability within a system
of services as complicated as ours. To bolster those efforts, we will
also begin a systemic practice of validating failure scenarios before
they have a chance to affect you. This work will involve future
investment in fault injection and chaos engineering tooling at GitHub.

## Conclusion[](#conclusion)

We know how much you rely on GitHub for your projects and businesses to
succeed. No one is more passionate about the availability of our
services and the correctness of your data. We will continue to analyze
this event for opportunities to serve you better and earn the trust you
place in us.


