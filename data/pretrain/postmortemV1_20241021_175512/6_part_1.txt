## August 2021

## 11 

[08/11/2021]

RCA - Connection errors for resources leveraging Azure Front Door and
Azure CDN

Tracking ID: 0MQY-NPG


**Summary of Impact:** Between 06:30 UTC and 09:30 UTC on 11 Aug 2021, a
subset of customers leveraging Azure Front Door and Azure CDN Standard
from Microsoft in Japan East, Japan West, Korea South, Korea Central
and/or West US regions may have experienced intermittent HTTPS request
connectivity failures when trying to reach their applications. During
the incident, the average global error rate was \~2.5% and the peak
global error rate was \~5%.

**Root Cause: **Azure Front Door and Azure CDN Standard from Microsoft
serve traffic through edge locations around the world. We were in the
process of rolling out a software update to prevent the use of TLS
session resumption keys which were older than specific thresholds. The
update followed the Azure safe deployment process and was rolling out in
phases until it reached the impacted locations. A subset of edge
locations in Korea, Japan, and West US were running with stale TLS
resumption keys, and the rolled-out update triggered the mechanism to
prevent the reuse of stale keys. However, a code defect in the
rolled-out version resulted in a race condition where a few servers in
the impacted locations tried to revert to a full TLS handshake. This
race condition resulted in these servers dropping HTTPS requests.

**Mitigation: **Our monitoring detected this issue and alerted the
service team. To mitigate, we removed unhealthy edge locations from
serving traffic, which routed traffic to healthy edge locations. We also
rolled back the update that caused the regression.

**Next Steps:[ ]{.underline}**We apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure Platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Deploy the fix for the regression that caused the race condition.
-   Enhancements to monitoring to ensure alerting if TLS resumption is
    off or if the session resumption key is older than threshold.
-   Enhancements in staging environments to account for additional stale
    TLS resumption failure modes.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our
survey: [https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## July 2021

## 28 

[07/28/2021]

RCA - Azure Network Infrastructure service availability issues - Brazil
Southeast

Tracking ID: LNZM-TZG


**Summary of Impact:** Between 13:48 UTC and 15:20 UTC on 27 July 2021,
a subset of customers experienced issues connecting to their services in
the Brazil Southeast region.

**Root Cause:** We determined that a degradation in connectivity was
caused by packet loss when the metadata on one of our regional-level
routers was updated incorrectly. As part of a planned network
configuration refresh, an update was being performed on the
regional-level routers in the Brazil Southeast region. The
regional-level tier of the network is designed with redundancy to allow
a subset of the routers at that network tier to be taken off-line (not
serving customer traffic) for updates.

During the update, our automated network configuration system applied an
incorrect IPv4 network prefix (IP Range) to a regional-level router that
was taken off-line. Restoring traffic to this regional-level router
resulted in packet loss for some of the Azure services in the region.
The incorrect network prefix caused traffic from this region to Azure
destinations in other regions and a subset of internet regions to be
dropped.

**Mitigation:** The device with incorrect prefixes was removed from
service. This mitigation took longer than expected because automated
safety checks were failing for the entire region, and some human
intervention was required to proceed with the traffic rollback. 

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   We have audited all ongoing deployments to identify this error
    pattern and enhancing our validation checks to prevent such a
    combination of errors, including improved detection logic.
-   We are continuously enhancing our alert logic to help identify
    issues faster and force rollbacks without human intervention.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 23 

[07/23/2021]

RCA - Service availability issue - Brazil South

Tracking ID: 5TGJ-1SZ


**Summary of impact:** Between 19:00 UTC and 21:28 UTC on 23 Jul 2021, a
subset of customers in Brazil South may have experienced issues
connecting to a number of Azure services hosted in this region,
including, but not limited to, App Service, ExpressRoute, Azure Search,
and VPN Gateway.

**Root cause:** On 23 Jul 2021, at 19:00 UTC, we received an alert from
our health monitoring that there were network connectivity issues in the
Brazil South region. On initial investigation, we identified a problem
in the physical network infrastructure. Upon further investigation, we
found that the physical network links to some of the devices within a
single cluster were disrupted during a planned maintenance activity.
During this maintenance activity, a technician was executing a change
that impacted the incorrect set of cables. The work that was executed on
site led to the disruption of services.

**Mitigation:** The Data Center Operations team worked at restoring some
of the physical links which restored network connectivity to operational
threshold by 21:20 UTC. By 21:28 UTC most of the impacted services were
recovered.

Data Center Operations team then continued to restore the remaining
subset of cables and completed this activity at 00:52 UTC on 24 Jul
2021, to bring the services back through normal connectivity.

**Next steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Reviewing existing network datacenter operations cabling processes
    and standards.
-   Revisiting and retraining our data center staff on processes.

**Provide feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 21 

[07/21/2021]

RCA - Azure Cognitive Services

Tracking ID: 4LRL-V8G


**Summary of impact:** Between approximately 07:00 UTC and 13:55 UTC on
21 Jul 2021, customers using Cognitive Services may have experienced
difficulties connecting to resources. Impacted customers may have
experienced timeouts or 401 authorization errors.

\

**Root Cause:** We determined that a subset of cognitive services
backend compute clusters were impacted during this incident, causing
them to be unable to service customer requests. Our investigation
revealed that this issue was triggered by a pending (deployed, but
awaiting reboot) OS security update which had introduced a networking
change on the updated clusters. The networking update unintentionally
caused a networking state which was not supported, and thus when the
update was staged, the machines lost network connectivity and could no
longer handle requests.

This issue had two separate impact streams as there was a loss of
available compute clusters due to the update, and also there was impact
to the control plane functionality, thus preventing the dynamic addition
of resources to meet with demand. In addition, the standard
auto-mitigation workstream of rebooting impacted nodes could not
execute, as the nodes could not be contacted due to the networking
connectivity issues on the individual nodes. The ultimate outcome was a
significant impact to our ability to service Cognitive Services requests
for some customers due to loss of compute nodes, and impairment of our
ability to auto-mitigate using the standard frameworks.

\

**Mitigation:** Our internal monitoring detected the problem and alerted
the on-call engineers. We initially mitigated customer impact by
recycling the impacted nodes of the control plane which restored the
self-healing capabilities of the system. This mitigation had to be
applied on multiple parts of the service as the issue affected several
internal subsystems (e.g., authentication, speech-to-text,
post-processing, text-to-speech). We then validated that we were able to
service customer requests.

Separately, an updated security patch was produced that reverted the
unintentional change in behavior such that future critical security
patches for this component will not trigger this failure pattern again.

\

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Improvement of the validation mechanisms of update testing to ensure
    similar issues are detected pre-deployment
-   Improvement of the patch staging/pre-reboot process to reduce the
    footprint of potential impact during updates
-   Improvement of the cluster network connectivity detection and
    automatic remediation/reset processes

\

Provide Feedback: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 21 

[07/21/2021]

RCA - Azure Cognitive Services - Azure Government

Tracking ID: 4VWK-LPZ


**Summary of impact:** Between approximately 03:00 EDT and 09:55 EDT on
21 Jul 2021, customers using Cognitive Services may have experienced
difficulties connecting to resources. Impacted customers may have
experienced timeouts or 401 authorization errors.

\

**Root Cause**: We determined that a subset of cognitive services
backend compute clusters were impacted during this incident, causing
them to be unable to service customer requests. Our investigation
revealed that this issue was triggered by a pending (deployed, but
awaiting reboot) OS security update which had introduced a networking
change on the updated clusters. The networking update unintentionally
caused a networking state which was not supported, and thus when the
update was staged, the machines lost network connectivity and could no
longer handle requests.

This issue had two separate impact streams as there was a loss of
available compute clusters due to the update, and also there was impact
to the control plane functionality, thus preventing the dynamic addition
of resources to meet with demand. In addition, the standard
auto-mitigation workstream of rebooting impacted nodes could not
execute, as the nodes could not be contacted due to the networking
connectivity issues on the individual nodes. The ultimate outcome was a
significant impact to our ability to service Cognitive Services requests
for some customers due to loss of compute nodes, and impairment of our
ability to auto-mitigate using the standard frameworks.

\

**Mitigation**: Our internal monitoring detected the problem and alerted
the on-call engineers. We initially mitigated customer impact by
recycling the impacted nodes of the control plane which restored the
self-healing capabilities of the system. This mitigation had to be
applied on multiple parts of the service as the issue affected several
internal subsystems (e.g., authentication, speech-to-text,
post-processing, text-to-speech). We then validated that we were able to
service customer requests.

Separately, an updated security patch was produced that reverted the
unintentional change in behavior such that future critical security
patches for this component will not trigger this failure pattern again.

\

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Improvement of the validation mechanisms of update testing to ensure
    similar issues are detected pre-deployment
-   Improvement of the patch staging/pre-reboot process to reduce the
    footprint of potential impact during updates
-   Improvement of the cluster network connectivity detection and
    automatic remediation/reset processes

\

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 12 

[07/12/2021]

RCA - Azure Front Door and Azure CDN - Connectivity issues and increased
latency

Tracking ID: 0TYG-DPG


**Summary of impact**: Between approximately 14:55 UTC and 19:50 UTC on
July 12th, 2021, a subset of customers primarily in the US and Canada
experienced increased latency and connection timeouts when connecting to
Azure Front Door and Azure CDN Standard from Microsoft. Retries may have
been successful on the resources/environments that experienced issues.
The peak failure rate was approximately 70% in some East US metro
