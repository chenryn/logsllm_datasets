Major ISP Migrates from Sybase to NonStop with No Downtime
November2008
Overview
A major international Internet Service Provider (ISP) offers email services, Internet access, and
other functions to millions of globalcustomers.Several million of thesecustomersmaybe logged
in at any one time. Maintaining continuity of service to its customers is a mandatory requirement
fortheISP.
Its rapid growth has led to capacity strains in the ISP’s IT
active
infrastructure. One such issue recently occurred in its login 16-server
Linux/Sybase
subsystem. When a customer logged in, his login request
Partitioned
was sent to the ISP’s Login Request Complex. There his Database
user profile was accessed and validated and his session
established. Though comprising a large farm of redundant existing
servers running many instances of Sybase on Linux, this login
request SybaseData
login subsystem had reached the limits of its capacity. s Replication
Further additions to capacity were going to be very
expensive. 16-server backup
Linux/Sybase
The ISP therefore decided to architect and build an entirely Partitioned
Database
new loginsubsystem. Itperformedaseries of functionality,
performance, and load volume torture tests using many of
the major commercially available database engines,
includingSybase, Oracle, and NonStop's SQL/MP. The
TheLoginRequestComplex
tests were run under the configuration and tuning guidance
of each vendor. As the load was scaled up to the ISP's load levels, one of the competitors could
not complete the tests. Another limped along under full load, not able to complete queries within
thedesiredSLA. Theclear winner was NonStop's SQL/MP,runningontheHPIntegrityNonStop
hardwareinanactive/activeconfiguration-itcompletedalltestssatisfactorily.
Not only would the single-system image presented by the active/active system make the Login
Request Complex significantly more manageable, but the rapid failover time offered by the
active/activesystem (seconds) would also ensurethat theloss of anode wouldnotbenoticedby
the users that it was servicing at the time of failure. Furthermore, if capacity were added in the
future, only the capacity needed would be purchased, rather than twice the capacity as required
bytheredundantLinux/Sybasearrays.
The problem then became how to migrate from the old Login Request Complex to the new
NonStop system without impacting the ISP’s customers. The goal was to perform an online
migration of the application with zero (or minimal) application downtime, often referred to as Zero
Downtime Migration, or ZDM. By the judicious use of the Shadowbase data-replication engine
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

from Gravic, Inc. (www.gravic.com), the ISP was able to gracefully migrate all of its customers
overaperiodoftimetothenewNonStopsystemwithnointerruptioninservice.
The ISP Login Request Complex
When a user wants to log in to the ISP, his login request is sent to the ISP’s Login Request
Complex. Until recently, this complex comprised a primary farm and a standby farm, each with
sixteen login servers. The farms were geographically separated for the purpose of disaster
tolerance and were interconnected by a WAN. The servers were large Linux systems running
Sybasedatabasemanagers.Eachserverwasverypowerful,comprisingmultipleprocessors.The
loginserversstoredtheprofilesforalloftheISP’s users.Asauserloginrequestwasreceivedby
theseservers,ausersessionwassetupwithhisorherprofile.
For each server in the primary farm, there was a similarly configured backup server in the
standby farm. The databases of the standby servers were kept synchronized with the primary
databasesviaSybasedatareplication.
The users were partitioned across the Login Request Complex according to their user names
(screen names). Intelligent routers directed a user login request to the appropriate server in the
Login Request Complex. During normal operation, login requests were routed to the primary
farm.Shouldaserver inthatfarmfail,its loginrequests werethenpassedtoitscompanioninthe
standbyfarmbytheintelligentroutinginfrastructure.
The Challenge of Migration
The ISP’s customer base was rapidly growing. To accommodate this growth, the Login Request
Complex had to be expanded. This could have been done by adding more servers to the Login
RequestComplexandbyrepartitioningtheusersacrosstheincreasednumberofservers.
However, these servers were very large and expensive. Significant licensing expenses added to
thecost.Thisalreadyhighexpenditurewasfurthercompoundedbythecostsofmanagingsucha
largecomplexofservers.
Additionally, each of the databases on the individual servers were standalone and not integrated;
and operational transactions had to be individually generated and assigned to the appropriate
database instance. For example, to generate a query that aggregated the information across all
of the databases, one query per database had to be generated and individually run. Then the
results had to be manually aggregated. Worse, the amount of data per database instance had
grownsolargethatqueriesthatscannedthelargesttablesoftentimedoutbeforecompleting.
More problematic was that of transaction updates. If a transaction updated data across the
databases (for example, an insert on one and a delete on another), the application had to
programmatically submit two transactions, one per database instance. If one of the transactions
failed, the application had to programmatically note this and “undo” the one that was successful.
This led to complex application transactions and backout logic when updating multiple instances,
and the databases often became corrupt when the application itself failed or a system fault
occurred.
Therefore, the ISP decided to move its login processing to a new Login Request Complex
organized as a four-node, geographically-distributed active/active system.1 HP NS16200
NonStop servers were to be used as the nodes in the active/active network. Shadowbase was
chosentokeepthenodesintheactive/activesystemsynchronized.
1WhatisActive/Active?,AvailabilityDigest;October,2006.
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

Thequeryandupdateproblemsposedbythemultiple,independentSybasedatabasesdescribed
abovearesolvedbytheactive/activeLoginRequestComplex.TheNonStopactive/activesystem
presents a single-system view with the database partitioned over many disks. Thus, single
queriesandtransactionsthatspantheentiredatabasecanbeissued.
The problem facing the ISP was how to migrate from the existing Linux/Sybase servers to the
new NonStop active/active system without impacting service to its customers – a Zero Downtime
Migration.
active
16-server
Linux/Sybase
Partitioned
Database
login
requests SybaseData migrate
Shadowbase
Replication
16-server backup
Linux/Sybase
Partitioned
Database
NonStopActive/Active
LoginRequestComplex
TheChallengeofMigration
The Migration Solution
Working with the Shadowbase engineers, a Zero Downtime Migration (ZDM) solution was
formulated.2 Briefly, temporary servers would be set up to capture all changes to the Sybase
databases in the Login Request Complex. This change capture function would then be activated
while the Sybase database contents were loaded into the NonStop databases via an ETL
(extract, transform, and load) utility. Once the load was completed, the changes that had
accumulated during the load would be sent to the NonStop servers to synchronize them with the
Sybase servers. At this time, the users could be switched to the NonStop servers for login
service.
To minimize any potential negative impact to the users, this process would be carried out in
phases. There would always be a failback path to the Sybase Login Request Complex should a
migration step fail. The migration would proceed over a period of several months, after which the
Sybaseserverswouldbedecommissioned.
Additionally,thecustomerdictatedthatnoadditionalreplicationorapplicationsoftwareother than
theexistingSybasereplicationproductbeinstalledontheproductionservers.Thismeantthatthe
Sybase replication server had to be used to replicate the database information off of the existing
servers. Since Sybase does not directlysupport replication to NonStop targets, Shadowbase had
toprovidetheintermediate“glue”toconnectthesystems.
Step1–CaptureChanges
The first step in the implementation of this plan was to provide for the capture of Sybase
databasechanges.This was achievedbyinstallingaserver farm offourteensystems runningthe
Linux operating system, thereby creating the Change Capture Complex. However, the only
2 B. D. Holenstein, W. H. Highleyman, P. J. Holenstein, Chapter 8, Eliminating Planned Outages with Zero Downtime
Migrations, Breaking the Availability Barrier II: Achieving Century Uptimes with Active/Active Systems, AuthorHouse;
2007.
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

application running ontheseservers was the Shadowbasereplication engine.There were no ISP
applications,andSybasewasnotrequired.
Asaresult,theprocessingloadontheserverswasnotlarge;andsmaller,lessexpensiveservers
couldbeused.TheserverscomprisedaminimalconfigurationofonlytwoCPUseach.
Shadowbase was used to capture changes on the Sybase servers and to replicate them to the
Change Capture Complex. There the changes were queued until needed to synchronize the
NonStop servers. Replication was provided for both the primary and standby Sybase farms to
accommodate times when one or more primary Sybase servers were down. However, all
changeswerecapturedbythecommonChangeCaptureComplex.
active ChangeCapture
16-server Complex
Linux/Sybase
Partitioned
Database shadowbase changes
login
requests SybaseData 14-server .....
Replication Linuxwith Shadowbase
Shadowbase
16-server backup
Linux/Sybase
Partitioned
Database shadowbase changes
NonStopActive/Active
LoginRequestComplex
Step1-CaptureChanges
Step2–LoadActive/ActiveSystemDatabases
With the Change Capture Complex installed and tested and the new login applications ready on
the NonStop servers, now was the time to begin the actual migration. An Extract, Transform,and
Load (ETL) utility was used to copy the data from the actively running Sybase servers to the
NonStopservers(Step2a).
extract xform load
active ChangeCapture
16-server
Complex
Linux/Sybase
Partitioned
Database shadowbase changes
login
requests SybaseData 14-server .....
Replication Linuxwith Shadowbase
Shadowbase
16-server backup
Linux/Sybase
Partitioned
Database shadowbase changes
NonStopActive/Active
LoginRequestComplex
Step2a-LoadActive/ActiveSystemDatabase
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

Atthesametime,anychangesthatweremadetotheSybasedatabasesfollowingtheinitiationof
the ETL process were replicated by Shadowbase to the Change Capture Complex, where they
werequeuedforlateruse.
At the end of the load process, the database for the ISP’s users was now on the NonStop
servers. However, the NonStop database was inconsistent and out-of-date since copying was
taking place while the source database was active and being changed by the ISP’s applications.
Row changes that arrived at the Sybase Login Request Complex after those rows had been
loaded were not reflected in the NonStop database. Therefore, the NonStop database still had to
bebroughtintosynchronizationwiththeSybasedatabase.
This was the role of the Change Capture Complex. When the load had been completed, the
changes that had been made during the load and that were queued in the Change Capture
Complex were now replicated to the NonStop servers via Shadowbase (Step 2b). Of course,
during this process, changes were still arriving at the Sybase servers; and these were also
queued by the Change Capture Complex and ultimately replicated to the NonStop servers. This
continual updating of the NonStop database from the original Login Request Complex continued
throughoutthemigrationtoensurethatthetwodatabaseswereproperlysynchronized.
active ChangeCapture
16-server Complex
Linux/Sybase
Partitioned
Database shadowbase changes
existing
login
requests S Ry eb pa ls ice aD tioat na L1 i4 n- us xer wv ie tr h shadowbase Shadowbase .....
Shadowbase
16-server backup
Linux/Sybase
Partitioned
Database shadowbase changes
NonStopActive/Active
LoginRequestComplex
Step2b-UpdateDatabase
Because there may have been cases in which old changes were applied to new data,
Shadowbase’s “fuzzyreplication” was used.If anupdateweremadetoarowthatdidn’texist,the
update was turned into an insert. If an insert were to be made, but the row already existed, the
insert was changed to an update. If a row were to be deleted, but it did not exist, the delete was
ignored. Ultimately, however, only the latest changes were reflected in the NonStop database;
andthedatabasewasproperlysynchronizedwiththeSybasedatabases.
Linux NonStop
Step3–VerificationandValidationoftheNonStopDatabase Sybase SQL/MP
Once the copy operation had been completed, it was imperative source target
data data
to ensure that the NonStop database was, in fact, a consistent base base
copy of the Sybase databases. The databases had to be
compare
compared, and any differences had to be repaired by bringing
compare
the NonStop database into conformance with the Sybase rewrite
databases.
Linux
V&V
The ISP wrote its own verification and validation (V & V) utility to
perform this task (Step 3). The utility ran on a separate Linux
Step3-
system that compared rows within key ranges to obtain higher
VerificationandValidation
5
©2008SombersAssociates,Inc.,andW.H.Highleyman

speed through parallel operation. Comparisons were bidirectional so that extra rows and missing
rowsonthetargetcouldbedetected.
The complexity of this step was compounded by the fact that the structure of the Sybase
databases and the NonStop database were different. Shadowbase made this conversion
automatically as changes were replicated from the Change Capture Complex to the NonStop
LoginRequestComplexaccordingtorulestheISPembeddedintoShadowbase.
However, the initial load of the NonStop database was not done with the Shadowbase loaders.
Therefore, the ISP had to incorporate the data-conversion rules into the ETL utilitythat was to do
theloading.
Furthermore, the same rules had to be incorporated into the customer’s V & V utility. As a result,
many issues in the NonStop database and in the V & V verification logic were found. They
included incorrectly mapping fields and improperly converting the Sybase data types into
NonStop SQL/MP data types. Consequently, corrections had to be made to the ETL loader and
theV&Vutility;andtheloadhadtobererunmultipletimes.3.
However, when all this had been done, the copy accuracy of the process and the data was
verified.
Step4–MigrateUsers
At the conclusion of the process, all changes had been sent to the NonStop servers. Introducing
thenewNonStopactive/activesystemintoservicewasdoneinacontrolledfashion(Step4).
At first, only read requests were routed to the NonStop servers (Step 4a). All update requests
were still routed to the original Linux/Sybase Login Request Complex. After experience was
gained with this activity, new users were assigned to the NonStop system, which handled both
readrequestsandupdaterequestsforthenewusers(Step4b).
existinglogin existinglogin
changerequests readrequests
shadowbase shadowbase
Sybase Change NonStop
Login Capture Login
Complex Complex Complex
Step4a-MoveReadRequeststoNonStopLoginComplex
existinglogin
readrequests
existinglogin and
changerequests newusers
shadowbase shadowbase
Sybase Change NonStop
Login Capture Login
Complex Complex Complex
Step4b-MoveNewUserstoNonStopLoginComplex
3ThecustomerlateracknowledgedthatitwouldhavebeenaloteasierandfastertohaveusedaShadowbaseloaderto
avoidtheseproblems.IfaShadowbaseloaderhadbeenusedinstead,theconversionruleswouldhavebeenconsistent.
6
©2008SombersAssociates,Inc.,andW.H.Highleyman

Finally,theoriginalusersweremovedtothenewcomplex;andallprocessingbytheLinux/Server
farmswasdiscontinued(Step4c).
login
requests
Sybase Change NonStop
Login Capture Login
Complex Complex Complex
Step4c-MoveAllLogonRequeststoNonStopLoginComplex
The Result
As described above, the migration proceeded cautiously. After several months, all users were
successfully migrated; and the Sybase Login Request Complex and the Change Capture
Complexweredecommissioned.
login login
requests requests
Each node in the NonStop active/active system acts as a
master node for 25% of the ISP’s users. When a user
request is received, it is routed to its master node by
intelligent routers for processing. Since only the master
node can update its portion of the database, there is no
possibility for data collisions. Should that node fail, the
Shadowbase
intelligent routers reassign that subset of users to one of
the surviving nodes, which becomes the new master for
thatpartofthedatabase.
Shouldadditionalcapacitybeneeded,itcanbesuppliedby
adding additional nodes to the active/active network.
login login
Likewise, load balancing is straightforward. If one node
requests NonStopActive/Active requests
becomes heavily loaded, some of the users who are being LoginRequestComplex
servicedbythisnodecanbemovedtoothernodes.
TheResult
Summary
When companies have user communities measured in the tens of millions of active users, any
consideration of migrating a core system to another platform can be daunting, especially if it is to
be achieved with no application service outage. By using data-replication technology, this ISP
wasabletosuccessfullymigrateseveralhundredmillionuseraccountstoanewNonStopsystem
with no impact on user service. Shadowbase from Gravic provided the necessary services to
performthismigrationandtoimplementthenewactive/activesystem.
7
©2008SombersAssociates,Inc.,andW.H.Highleyman
