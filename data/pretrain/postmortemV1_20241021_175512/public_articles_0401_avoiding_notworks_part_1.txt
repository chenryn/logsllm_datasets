Avoiding Notworks
January2009
Anetworkthatdoesn’tworkisa“notwork.”
The Impact of a Notwork
Active/ActiveSystems
Inanactive/activesystem,1inwhichtwoormorenodes cooperateinacommonapplicationusing
a distributed database, network faults can wreak havoc. One set of nodes can become isolated
from others, requiring that the isolated nodes be taken down if split-brain operation is to be
avoided. If the isolated nodes are not taken down, perhaps data-collision resolution2 will be
requiredwhenthenetworkisreturnedtoservice.
If synchronous replication is being used between the active/active nodes, network errors or
increasedchannellatencycancauseapplicationstotimeoutandcrash.
Active/BackupConfigurations
Network faults can cause similar problems even in an active/backup configuration, in which a
backup system is available to take over the role of the active system should it fail. The backup
system must have access to a reasonably current application database copy that it can use if it
has to take over processing. To the extent that the backup database copy is not current,
transactions will be lost upon failover. The degree of acceptable data loss is specified by the
RecoveryPointObjective,orRPO.
If the backup database is being kept current via virtual tape or by data replication, both of which
replicate changes from the active system to the backup system over the network (as opposed to
the use of magnetic tape), a network failure will cause the backup database to get further and
furtherbehind,thusperhapsviolatingtheRPOspecification.
UserAccess
In all of the above scenarios, an available network is required if users are to be moved from an
inoperative node or system to a functional one. If the network should fail and then a node in an
active/active system or the active system in an active/backup pair should fail, users cannot be
reconnectedtoanoperatingnodeorsystem tocontinuetheirservice.Theseusersarenowdown
aswell.
In this article, we review at a high level some of the steps that should be taken to ensure that
long-haul wide-areanetworks usedinanactive/active system donotbecomethelimitingfactor in
1WhatisActive/Active?,AvailabilityDigest;October2006.
2AsynchronousReplicationEngines,AvailabilityDigest;November2006.
1
©2009SombersAssociates,Inc.,andW.H.Highleyman

system availability or performance. Many of these considerations also apply to local area
networksthatconnectuserstotheirlocalprocessingnodes.
Network Availability
Split-BrainOperation
Split-brain operation may be acceptable for those cases in which collisions cannot occur or are
notimportant.
In some active/active applications, data collisions do not occur. An example of such an
applicationisaninsert-onlyapplication.Sincenodataisbeingupdated,collisionscannotoccur.
These applications can work perfectly well in split-brain mode. Should the network fail, operation
continues unaffected until the network is restored. The only ability that is lost is the ability to run
queries against a current database until the network is restored and the database copies are
resynchronized. If query processing is not critical, network availability is not critical. Besides,
queriescanbesatisfiedfromthelocaldatabasethoughtheymayreturnstaleresults.
If data collisions can happen, they can be avoided by partitioning the database so that each
partition is owned by one and only one node. Only the owning node can update its partition.
Therefore, there can be no data collisions. So long as all transactions that update a partition are
generated by local users, the absence of the network only affects queries that need data in
partitions resident in remote nodes. Of course, if a user initiates a transaction that needs to be
senttoaremoteowningnodeforprocessing,thattransactionwillfail.
In some applications, data collisions are unimportant. During split-brain mode, each node will
miss some updates. However, when the network is restored, it may be acceptable to realize that
eventuallyallcollisionswillberesolvedsimplybynewupdatesthatwilloverwritethestalevalues.
BackboneNetworksDoFail
If split-brain operation is to be avoided, the availability of the interconnecting network becomes
paramount.Specifically,sinceitisacomponentintheavailabilityequation,itsavailabilitymustbe
better than the desired availability of the system. If the active/active system is to have an
availability of six 9s, the network should have an availability of seven 9s or so (three seconds of
downtimeperyear).
Network connections today are highly reliable. Should a link fail in a communications channel,
traffic is often simply rerouted around that link; and the fault is transparent to the users of that
channel.
However, networks do fail. A common cause is the “last mile” that connects the network
provider’s backbone to the user. This is often a single cable that can break because of natural or
man-madeactions(atreelimbfalling,abackhoedigging).
However,thenetwork backbonecanalsofail. Underseafibre-optic cables seem tobeparticularly
vulnerable, being cut bydragging anchors or broken byseismic activity. Submarine cables under
the Mediterranean Sea account for the majority of communications between Europe and the
MiddleEast,causingNorthAfricatobeparticularlyhardhit.
On Wednesday, January 30, 2008, North Africa, the Middle East, and India experienced a
massive Internet outage due to several submarine cables being cut. The outage created an
Internet notwork that was destined to last for several days or even weeks in some cases.3 Then
3What?NoInternet?,AvailabilityDigest;February,2008.
2
©2009SombersAssociates,Inc.,andW.H.Highleyman

on December 19, 2008, disaster struck again when three Mediterranean cables were cut.4 Days
later, efforts were still in progress to restore communication service to Egypt and some Middle
Eastcountries.
One of the biggest interruptions of international telecommunication services occurred in
December, 2006, when a magnitude 7.1 earthquake broke nine submarine cables between
Taiwan and the Philippines and cut connections between southeast Asia and the rest of the
world.Ittook49daystorestorefullcapacity.
But submarine cables are not the only culprits. In April of 2007, the Blackberry network went
downforoveradayduetoafaultyupgrade.5ItsnetworkalsowentdowninFebruaryof2003and
twiceinJuneof2007.
OnDecember 8,2008,aTimeWarner cableoutagedeniedInternetservices toseveralthousand
users for over a day.6 On December 28, 2008, AT&T service throughout the Eastern United
States was downfor mostof thedayduetoapower failure.7Evenmajor carriersarenot immune
fromnotworks.Andthelistgoeson.
What will you do when you lose network connectivity? The only way to ensure network reliability
isredundancy.
Redundancy
It is easy to say that your network should be redundant. However, it is another matter to ensure
redundancy.
Communication-LinkIndependence
The “last mile” is perhaps the easiest part. The data center should be connected to two
independent communication feeds that are electrically and physically separated. Electrical
separation begins in the data center. Redundant network equipment in the communications room
powered by separate power feeds and connected to separate network ports on the computers
shouldbeprovided.
Physical separation means that the communication feeds should be routed into the data center
over separate paths. There have been too manycases of a backhoe severing all communication
lines coming into a data center through a common conduit (the same can be said for separate
powersources).
But beyond the “last mile” is the network backbone. Separate communication links into the data
centerdonothelpifthebackbonenetwork isdown.Itisequallyimportanttohavetheseseparate
communication links be fed by different communication carriers so that if one loses its backbone
network,theothercanstillbeprovidingcommunications.
Redundancy issues don’t stop here. Often, a communication carrier will lease lines from another
common carrier. If the two carriers that you have selected lease lines from the same third-party
carrier,andifthatcarrierlosesitsbackbonenetwork,thenbothofyourcarriersmaybedown.
Andtheproblemsgoon.Thereisonecaseinwhichacompanytook alloftheseprecautions.But
what it didn’t know was that both carriers routed cables over a common bridge crossing a major
river.Afloodcausedthebridgetobuckle,andbothnetworksfailed.
4SecondInternetOutageHitsEgypt,DailyNewsEgypt;December21,2008.
5BlackberryGetsJuiced,AvailabilityDigest;May2007.
6TimeWarnerCableInternetOutageFixed,OCRegister;December11,2008.
7WinterStormCausesMajorAT&TOutages,InformationWeek;December29,2008.
3
©2009SombersAssociates,Inc.,andW.H.Highleyman

CommunicationLinkUsage
Given completely independent communication links that cannot suffer a common failure, there is
still an issue of how they are used. One possibility is to have the communication links configured
as an active/standby pair. All traffic is routed over the active link. Should it fail, traffic is rerouted
overthebackuplink.
But whatif thebackuplink turns outtobenonoperational?Communicationhas beeninterrupted,
and in an active/active system we are faced with the decision of running in split-brain mode or of
having to down one or more isolated nodes. This is called a failover fault and happens all too
frequently.
A better configuration from an availability viewpoint is to use the redundant channels in a load-
sharing configuration. In this case, both channels are active; and traffic is split between the two
channels. Of course, either channel must be able to carry the entire communication load should
theotherchannelfail.
Theadvantages of this configurationaretwofold.First, itis knownthatbothchannels are working
because each is actively carrying traffic. Therefore, there are no failover faults. Should one
channelfail,alltrafficissimplyreroutedovertothesurvivingchannel.
Second, in normal operation, each channel is carrying only half the traffic even though it is
capable of carrying all traffic. Therefore, the channels are less loaded than the active channel in
anactive/backupconfiguration,thusimprovingcommunicationperformance.
Loadsharingistheactive/activeequivalentofapplicationprocessing.Allchannels areactive,and
failoverissimplyreroutingtrafficoveraknowngoodchannel.
