OpenVMS Active/Active Split-Site Clusters
June2008
HP OpenVMS clusters offer a dramatic improvement over contemporary cluster technology.
Nodes in an OpenVMS cluster run in an active/active mode in which multiple nodes across
multiple sites cooperate in a common application against a common, distributed file system. The
recent “Disaster Proof” video from HP,1 in which a data center was blown up, showed that
OpenVMS had the fastest recovery time of all the clustering technologies used (OpenVMS,
HPUX,NonStop,WindowsandLinux).
In our earlier article on clusters,2 it was pointed out that contemporary clusters do not run in an
active/active mode in our sense because a disk volume can be mounted only on one node at a
time (unless Oracle RAC is used), and only that node can participate in the application.
Consequently, when a node fails, the application has to be started on another node, the volume
remounted and repaired, and the users switched. This leads to failover times for contemporary
clustersmeasuredinminutesormore.
Like active/active systems, OpenVMS clusters recover in seconds because once a failure is
detected, all that must be done to continue operation is to switch the subset of users who were
connected to the failed node to surviving nodes at any of the sites. Furthermore, no data is lost
following a failure (a Recovery Point Objective, or RPO, of zero is achieved) because the
applicationfilesystemcopiesareupdatedsynchronously.
OpenVMS Cluster Overview
An HP OpenVMS cluster is a shared-everything cluster that can have up to 96 nodes3 distributed
over one or more geographically-separated sites. Redundant data storage is organized as
shadow sets using HBVS (Host-Based Volume Shadowing). Each disk (or presented storage
device in a fibre channel disk array, where each presented device could itself be a RAID 0+1
entity) can be a member of a shadow set, and a shadow set can have up to three members. All
diskmembersofashadowsetareexactcopiesofeachother.
The three members of a shadow set maybe distributed across as manyas three of the nodes in
the cluster. When fibre channel disk arrays in a storage area network are used, all nodes have
simultaneous access to all shadow set members. (The three-member limit is currently being
increased with an architectural limit of sixteen members and with support for up to six members
anticipated). A cluster can support up to 500 disks in multiple-member sets or up to 10,000 disks
insingle-membersets.
11http://h20219.www2.hp.com/enterprise/cache/523434-0-0-0-121.html.
2Active/ActiveVersusClusters,AvailabilityDigest;May2007.
3Actually,thesystemisdesignedtoaccommodateupto255nodesbuthasbeenqualifiedforonly96nodes.Although
largerclustersarenotsupported,somecustomershaverunclusterswith150nodesormore.
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

clients
ethernet
SCS SCS
node node node
storage
area shadow
files network files files set
site1 site2 site3
An application can be distributed across several nodes, and users can use any instance of an
application on any node. To support shared-everything, OpenVMS clusters provide a Distributed
Lock Manager that manages lock requests and a Connection Manager that manages the
reconfiguration of the cluster should there be a failure. The Connection Manager ensures that
therewillnotbetwoclustersubsetsactivelyprocessinginasplit-brainmode.
Shadow sets are managed via the Host-
Users
Based Volume Shadowing (HBVS) facility
of OpenVMSclusters.Withinashadowset,
writes are synchronous. That is, a write is Application Application Application
applied to all disks in a shadow set
simultaneously; and a completion status is Node Node Node Node Node
returned to the application only when all
writes have completed. If a member of a DistributedLockManager
shadow set fails, it is removed from the
shadow set until it can be repaired, at ConnectionManager
which time it will be synchronized with the QuorumScheme
shadow set. Thus, no data is lost in the SharedResources
eventofafailure. (files,disks,tapes)
Readsaredirectedtothedisk thatcanrespondthefastest.Thisisdeterminedbyseveralfactors,
such as the communication latency of the path to the disk, the number of reads currentlyqueued
tothedisk,andthespeedofthestoragearraycontainingthediskvolume.
The nodes in the cluster and the disks in a shadow set can be heterogeneous. A cluster can
compriseamix of AlphaServers or IntegrityServers as nodes,or itcancontainamix of VAX and
AlphaServer nodes (or VAX and Integrity Server nodes for migration purposes). The supported
mixed-versionclusters permit“rollingupgrades” sothatacluster can bemaintainedandhavethe
operatingsystemupgradedwhileotherclustermemberscontinuetoprovideservice.
The disk storage arrays in a shadow set can be a mix of disk technologies, including SCSI disks
and fiber-connected arrays organized into a storage area network. For instance, a shadow
member can be expanded on-the-fly to a larger disk. The virtual disk represented by the shadow
set will have a size equal to the smallest disk in the shadow set. When all disks have been
expanded, the shadow set virtual disk can be expanded to provide an increase in storage
capacity.
Host-Based Volume Shadowing is not itself transaction-oriented. It is file-oriented,managing files
under RMS(theRecordManagementSystem). OpenVMSclusters includeDECdtm,adistributed
transaction manager, to provide transaction semantics. DECdtm can coordinate the actions of
resource managers such as Oracle, Rdb (an Oracle relational database for OpenVMS), or OpenVMS
RMSJournaling(forRMSfiles).
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

Internode and Intersite Communications
Communications within the cluster are managed by SCS, the System Communication Services.
The nodes within the cluster are interconnected via high-speed channels such as FDDI or gigabit
Ethernetorganizedas aLAN.Eachnodecancommunicatedirectlywitheveryothernode –there
is no routing through a node. These channels are used to relay lock information, disk-block
contents, and Connection Manager quorum information between nodes. They are also used to
allowthenodes tocommunicate witheachother for purposes of heartbeats (“hello”messages) to
monitorthehealthofthenodes.
The SCS protocol is a high-throughput, low-latency layer 2 protocol running directly on the LAN.
Withrecentchangesinwide-areanetworkcapabilitiesandthepredominanceofTCP/IP-managed
services, there is work under way to implement the SCS protocol over a TCP/IP network rather
thanrequiringanextendedLANlayer2networkforclustercommunications.
Typically, dual extended LANs are provided. SCS will use multiple LANs in a load-sharing
arrangement. If multiple interconnects of different types are used, SCS will choose the best
channeltouseandwillfailovertotheotherchannelshouldtheprimarychannelfail.
The shadow set disk arrays are interconnected by dual fiber channels forming a storage area
network (SAN). A shadow set presents a virtual image of its members to the applications running
in the nodes. Any node can write to or read from any shadow set over the storage area network
andseesthatshadowsetsimplyasasingledisk.
Shouldanodefailor communications belostbetweenapair of nodes,thecluster is reconfigured
by the Connection Manager based on a quorum computation, as described later. The cluster
portion that has quorum (a majority of votes) survives, and the rest of the cluster is taken out of
service until a repair can be made. As a consequence, there can be no split-brain operation nor
cantherebeatug-of-warbetweenclusternodesattemptingtogaincontrolofthecluster.
User access to the clusters is typically implemented over a separate network, thus avoiding
contentionwiththeclusterinterconnects.
Host-Based Volume Shadowing (HBVS)
HBVS is the OpenVMScluster facilitythatmanages shadow sets. Acopyof HBVSruns on every
node in the cluster, and the HBVS instances communicate with each other over high-speed links
to coordinate locks and cache. All read and write requests are directed to the application’s local
copyofHBVS,whichprovidestheshadow-setservices.
All writes are at the 512-byte block level. When an application updates a record, that record is
sent to its local copy of RMS or to the database manager being used. RMS or the database
manager will first obtain a cluster-wide lock on the record to be rewritten. If the block containing
the record is not currently in the node’s local cache, HBVS will read the block into that cache. It
willupdatetheblock andwill writeitsimultaneouslytoallmembers of theshadow set.Onlywhen
allwriteshavecompletedisthewritecompletionstatusreturnedtotheapplication.
When a read request is received, HBVS will determine which shadow set member can respond
mostquickly.Thisdeterminationisbaseduponseveralfactors,suchasthelengthofreadqueues
for the disk, the speed of the disk’s storage array, and the communication latencyseparating the
diskfromthenodemakingtherequest.
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

clients
gigabitethernet
clusternode clusternode
application application
read write read write
HBVSshadowsetservices HBVSshadowsetservices
dualfibre
channel
dualSAN
read
write write write
Host-Based
VolumeShadowing
ShadowSetMembers
Both reads and writes can be synchronous or asynchronous. For asynchronous operations, the
application is notified of completionbyanAST,anasynchronous trapthat is aspecialinterrupt to
runningcode.
ThedatabasemanagerorthefilesystembeingusedsharesseveralresponsibilitieswithHBVS:
 To synchronize writes to all members of a shadow set, the database manager or the file
systemusesdistributedlocksanddistributedlocalcacheprovidedbyOpenVMS.
 HBVSwritessynchronouslytoallofthediskmembersofashadowset.
 HBVS optimizes reads by reading from the shadow-set member that will respond the
fastest.
 HBVS resynchronizes a disk when that disk is being returned to service so that it can
rejoinitsshadowset.
 HBVSsynchronizesnewdisksthatarebeingaddedtoashadowset.
DistributedLockManagement
OpenVMS synchronizes write activities among the nodes in the cluster via distributed lock
management. OpenVMS supports several levels of locks, including read locks, write locks, and
exclusivelocks.4
4 W. E. Snaman, D. W. Thiel, The VAX/VMS Distributed Lock Manager, Digital Technical Journal, No. 5; September,
1987.
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

A lock request maybe non-queued, queued synchronous, or queued asynchronous. If the lock is
requested without queuing, and if the lock is unavailable, the request is simplydenied. If queuing
is requested, the lock request is queued behind other earlier lock requests until it can be granted
in turn. If synchronous lock queuing is requested, the application is paused until the lock can be
granted. If asynchronous lock queuing is requested, the application continues processing and is
notified byan asynchronous trap (AST) when the lock has been granted. The AST can invoke an
applicationroutinespecifiedbythelockrequest.
To handle distributed lock
3.requestlock
management, OpenVMS first defines
Directory Nodes. A Directory Node Application Resource 4.queue
Manager lock
contains the node IDs of the nodes
5.grantlock
that are currently acting as Resource
Managers, if any, for a tree of 1.request
resource objects. When a node lock 2.Resource
Manager
wishes to request a lock, ithashes the nodeID
name of the resource that it wishes to
lock to obtain the Directory Node for Directory
DistributedLock
that resource. It then sends its lock Node Management
request to the appropriate Directory
Node.
If there is a node that is currently acting as a Resource Manager, the Directory Node will return
the node ID of the Resource Manager to the requesting node. The requesting node will then
resend its lock request to the Resource Manager, which will queue the lock request. When the
lockcanbegranted,theResourceManagersoinformstherequestingnode.
If the Directory Node is the Resource Manager for the resource, it will manage the lock request
directly.
If the Directory Node receives a lock request for which there is no currently assigned Resource
Manager, the requesting node is told to become the Resource Manager for this resource tree.
The Directory Node creates a directory entry noting this association. Thereafter, the requesting
node need not make anyfurther requests to any node. It handles lock requests for the resources
whichitismanaginglocally.
In this way, if an application is locking a resource frequently, there is no messaging traffic. To
extend this concept, if another node becomes more active on a resource, the role of Resource
Managerispassedtothatnode.
When a Resource Manager has no more lock requests for the resource that it is managing, it
notifies the Directory Node, which will remove this entry from its directory. An application can
acquire a null lock on a resource, a lock that has no impact except to prevent the Resource
Managerforthatresourcefromdisappearing.
Using these algorithms, most lock requests are handled locally. Some require one internode
request to a remote Resource Manager. At most, two internode requests are required, one to the
Directory Node and one to the remote Resource Manager. As a rule of thumb, a local lock
request takes about three to five microseconds, depending upon the speed of the server
hardware;andaremotelockrequesttakesabout180microsecondsoveraFastEthernetLAN.
The Distributed Lock Manager allows an application to acquire a lock and hold it if it is going to
use the resource for an extended period of time. However, if another lock request is received for
that resource, the application holding the lock is notified via an AST so that it can release the
lock.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman

The Distributed Lock Manager provides cluster-wide deadlock detection and will reject a lock
requestthatwillleadtoadeadlock.
DistributedCacheManagement
Each node maintains its own local block cache. Associated with each block is a sequence
number that is carried with the cached blocks. Whenever a block is written to disk, its sequence
numberisincremented.Atthistime,onlythewritingnodeandthestoragearraysintheSANhave
theupdatedblockintheirlocalcaches.
When a node wants to read a block, a lock request is generated. The returnedmessage granting
the lock also contains the current sequence number for the block. If the node’s local cache copy
matches the current sequence number, it can use its copy of the block. If the sequence number
haschanged,thenodemustfetchthecurrentblockfromdisk.
In a later implementation, the Extended File Cache (XFC) was introduced. In this system, if a
nodehas acopyof ablock inits localcache,ithas registeredaninterestinthat block.Whenever
a block is modified, the modifying node, via a bit map, notifies each node that has registered an
interest in the block of the block modification via an AST. Each node then marks its copy of the
blockinitslocalcacheasbeinginvalid.
ShadowMemberCopy
When a new disk (or one that has been out of
service for a long time) is to be added to a shadow
set, the entire contents of a current member of the copy good
fence
shadow set must be copied onto the new member.
bad
The currentmembers are designated as the source
members, and their contents will be copied to the current member
member-to-be. One of the source members is members being
chosentobethemaster. created
The copy begins by reading a 127-block segment (the default value) from the master and from
the new member being created. If the segment of the new member matches the contents of the
segment held by the master, the next segment is tested. If a segment found on the new member
