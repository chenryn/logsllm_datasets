The AWS Outage
WecannowturnourattentiontotheAWSoutageofOctober22.Intypical(andwonderful)Amazonstyle,
a detailed description of the outage, its symptoms, its causes, and its cures was subsequently published
byAmazon.2Thefollowingdescriptionofeventsistakenfromthatdescription.
TheMemoryLeak
At 10 AM on Monday, October 22, 2012, AWS operations staff noted that a small number of EBS
volumes in one Availability Zone in Amazon’s Northeast region had become stuck and were unable to
process further I/O requests. As designed, they began to fail over to healthy EBS servers in the
Availability Zone. As time went on, more and more EBS volumes got stuck. Ultimately, there were not
enough servers for the stuck EBS volumes to fail over to. EC2 instances were losing their EBS storage;
andtheirapplicationsfailed,takingdownamultitudeofcustomers.
The team made adjustments to control the failover rate by throttling calls to AWS services, and the
system began to slowly recover volumes. However, the large surge in failover/recovery activity made it
difficultfortheteamtodeterminetherootcauseoftheproblem.
Finally, the problem was identified. It had been initiated several days previous to the outage when a data
collection server failed. The AWS data collection servers collect information from the various systems in
AWS to guide maintenance activities. Their operation is important but not time sensitive. They are
designedtotoleratelateandlostdata.
The server was replaced, and a DNS record was modified to reroute traffic from the failed server to the
newserver.Letthebugsbegin.
Bug Number 1 - The DNS change did not propagate to all of Amazon’s internal DNS servers. The EBS
servers use a data collection agent to send health information to the data collection servers. For those
EBS servers that did not get the DNS update, the data collection agent continued to attempt to contact
the failed data collection server, but to no avail. Because of thefault-tolerant design of the data collection
serversandtheiragents,thisdidnotcauseanissue;andalarmswerenotgenerated.
2AmazonWebServicesMessage680342,http://aws.amazon.com/message/680342.
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Bug Number 2 – However, this action triggered a latent memoryleak in the data collection agents on the
EBS servers. The memory leak slowly consumed the servers’ system memory until the servers could no
longer function. At this point, they became stuck and attempted to fail over to healthy servers. Initially,
these appeared to be normal failover events and caused no alarm on the part of the operations staff.
However, as more and more EBS servers began to fail, the failover activity increased to the point that
therewerenotenoughhealthyEBSserverstowhichtofailover.Atthispoint,applicationsbegantofail.
Bug Number 3 – The EBS servers monitor their use of memory and are configured to raise an alarm if a
memory leak is detected. However, they make very dynamic use of memory; and it is hard to establish
meaningfulthresholds.Asaresult,thememoryleakswerenotidentified;andnoalarmsweregenerated.
A little after 3 PM, the operations staff determined the cause of the problem and began restoring affected
volumes by freeing the excess memory consumed by the misbehaving data collection agents. By 4:15
PM, most volumes were recovered and were functioning properly. The Amazon cloud’s Availability Zone
hadbeendownforfivehours.
However,thebugsdidnotendhere.
TheImpactonEC2andEBSAPIs
Many customers complained that, even though their applications were up and running, they couldn’t
manage them. AWS provides an extensive API (application programming interface) to configure and
manage applications, and these APIs were executing painfully slowly. In some cases, their poor
performancemadethemuseless.
Before the memory-leak problem was understood, the operations staff moved to reduce the load on the
system in a desperate attempt to allow the system more time to return to stability. They did this by
throttlingtherateatwhichAPIscouldbeexecuted.
A level of throttling is implementedin AWStoprevent callers from overwhelming services. An example of
suchacallerisarunawayapplicationthatretriesafailedrequestasfastasitpossiblycan.
Bug Number 4 - As it turned out, the team was too aggressive in its throttling restrictions, resulting in an
inability to keep up with the normal API activity. There was no way to fine-tune throttling by customer,
which may have allowed the team to give priority to critical applications. Rather, throttling could only be
doneonanaggregatebasisacrosstheentireAZ.
After 2:30 PM, the team significantly reduced the level of throttling and freed up API execution. This
problem did not affect applications that were running across multiple AZs and that had failed over to their
backupAZs.
TheImpactontheRelationalDatabaseService(RDS)
RDS uses EBS for its database and transaction log storage. RDS can be configured as a single-AZ
database in which case it is running in only one AZ. Alternatively, it can be configured as a multi-AZ
database in which a primary database is running in one AZ and its backup is running in another AZ. In
this case,if theprimarydatabasefails,thestandbydatabaseis promotedtoprimaryandis availableafter
itsintegritychecksarecompleted.
The single-AZ RDS databases became stuck if they depended upon a stuck EBS volume. However,
multi-AZ databases were able to recover to their standby databases and continue operating – almost. A
smallportionofRDSmulti-AZRDSinstancesdidnotrecoverbecauseoftwoadditionalsoftwarebugs.
Bug Number 5 – Somemulti-AZ RDS databases encountered an uncommon stuck I/O condition and had
tobefailedovermanually.
5
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Bug Number 6 – Some primary databases had been disconnected from their backups just prior to the
failure.Therefore,theirbackupsweremissingsomedataupdates.Thesystemblockedfailovertotheout-
of-date standbys. The primary RDS databases could not be restored until their EBS volumes were
restored.
ImpactonElasticLoadBalancer(ELB)
The Elastic Load Balancer routes traffic to the customer EC2 instances. It uses Elastic IP addresses
(EIPs) for routing within and across AZs, and it uses EBS for storing its configuration and monitoring
information.
Because of the EBS problems, the ELBs became degraded, and the ELB service began executing
recoveryworkflowstoreplacethedegradedELBinstances.Manywererecoveredby3:30PM.Butthen:
Bug Number 7 – A bug in the ELB recovery workflow reared its head. The recovery service ran out of
EIPs.Thisstalledtherecoveryworkloads.
Bug Number 8 – To make matters worse, a bug in the traffic-shifting functionality in the ELB caused the
ELB to not shift some traffic properly. The result was that some traffic continued to be routed to the
affectedAZratherthantothebackupAZs
The ELB problems were finally resolved by 9:50 PM, and AWS returned to normal. It had been hobbled
foralmosttwelvehours.
BugFixesandEnhancements
Amazonimmediatelyinitiatedeffortstocorrectthemanydeficienciesthatithadfoundduringthisoutage:
Bug1:TheDNSservershavebeenmodifiedtoensurethatchangesareproperlypropagated.
Bug 2: The memory leak in the EBS server data collection agent has been plugged. Furthermore, limits
onresourceusageoflow-priorityprocesseshavebeenimplemented.
Bug3:Themonitoringofmemoryusagehasbeenimproved.
Bug 4: Throttling has been changed to be on a per-customer basis rather than on an overall aggregate
basis.
Bug5:Themulti-AZRDSstuckI/Oconditionhasbeenfixed.
Bug6: If theprimarydatabaseis inanaffectedAZ,failover tothestandbydatabaseis nowallowedeven
ifitisoutofsynchronizationwiththeprimarydatabase.
Bug 7: The ELB recoveryprocess has been modified to ensure that there are sufficient EIPs to support a
recovery.
Bug 8: The traffic-shifting bug in the ELB has been fixed. Furthermore, the ELB has been modified to
morequicklyshifttraffic;andcustomerscannowcontroltheroutingoftraffictospecificAZs.
Amazon has offered a wide range of credits to compensate customers for the problems caused by the
AWSoutage.
6
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Summary
As can be seen from the Amazon AWScloud, clouds are extremelycomplex. Bugs will always be lurking
in their depths, but with experience they will be slowly flushed out over time and eradicated. Of course,
newfeatureswillalwaysaddnewbugs,sothiswillbeanever-endingexperience.
Amazon’s approach to bug reporting is refreshing as it is very transparent. It helps customers to know
exactlywhathappenedandwhatisbeingdonetocorrecttheproblem.
Nonetheless, the complexity of clouds and the propensityof bugs as evidenced in this outage emphasize
the need to have an effective and tested business continuity plan to guide you when your critical cloud-
basedapplicationssuddenlybecomeunavailable.
Acknowledgement
Ourthankstooursubscriber,KeithEvans,forpointingustothisincident.
7
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
