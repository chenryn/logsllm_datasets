Penguin Computing Offers Beowulf Clustering on Linux
January2007
Clustering can provide high availabilityand supercomputer-scalable high-performance computing
at commodity prices. The original Linux clustering software was Beowulf (www.beowulf.org).
Though available as open source, Beowulf clustering is offered as a
supported product by Penguin Computing (www.penguincomputing.com)
alongwithalineofserverssupportinghigh-performancecomputing.
What makes Penguin unique is that the original developer of Beowulf,
DonaldBecker,isnowChiefTechnologyOfficerofPenguinComputing.
An Overview of Clustering
A computer cluster is a group of loosely coupled computers that work together closely so that in
many respects they can be viewed as though they are a single computer.1 The group of
standalone computers are linked together by software and by high-speed networks. The primary
advantagesofclusteringarehighperformanceandhighavailabilityatalowcost.
The capability to achieve high performance makes clusters very suitable to high-performance
computing(HPC).Sincetheworkloadisspreadamongthecomputersinthecluster,aclustercan
be scaled to achieve very high performance, especially if the applications can take advantage of
parallel processing. Supercomputing capabilities in the ten-gigaflop range and more were
commonlyachievedas earlyas themid-1990s. 2Today’s clusters provideteraflops of processing
power.
In addition, since clusters are highly redundant, they are suitable for applications requiring high
availability (HA). It is possible to structure them to achieve very high availabilities with automatic
failover and load balancing. Should any processor in the cluster fail, in principle its role can be
automaticallyassumedbyasurvivingprocessor,providingthatprocessorstatecanbepreserved.
Another benefit of clusters is their inherent scalability. Processors can be added or deleted to
adjusttheavailablecapacity.Coupledwithanappropriatesystemmanagementutility,changesin
thehardwareconfigurationofaclustercaneasilybemade.
High-performance computing coupled with high availability makes clusters suitable for massive
computational tasks such as data mining, business intelligence, biotechnology, modeling, and
simulation. High availability is extremelyimportant in manyof these applications as theycan take
weekstorun.
1
www.wikipedia.org.
2
Seehttp://www.beowulf.org/overview/history.html.
1
©2007SombersAssociates,Inc.,andW.H.Highleyman

Since clusters run on commodity hardware, even large clusters can be a fraction of the cost of
equivalentsupercomputersandmainframes.3Asaconsequence,“departmentalsupercomputers”
arenowpossible.
Beowulf
Beowulf was the original Linux cluster facility. Beowulf was developed in 1993 at the NASA
GoddardSpaceCenterbyDonaldBeckerandDr.ThomasSterling.
This development effort showed that commodity clusters could do the work of multimillion dollar
supercomputersatafractionofthecost.
ThebenefitsofBeowulfinclude:
 running on Linux, an open source operating system that can be acquired
inexpensively.
 theuseofcommodityhardware.
 the ability to deploy departmental HPC systems to avoid the waiting times required
formanysupercomputers.
Beowulf is supported by the open source community, which communicates via periodic
conferences,andtheBeowulf websitereferencedearlier Thatwebsitealsolists thecompanies,
includingPenguinComputing,thatofferasupportedversionofBeowulf.
Additionalsources of informationincludethe whitepaper BreakingNewGround: TheEvolutionof
Linux Clustering, available at http://www.scyld.com/breaking_new_ground.pdf, and the 1999
book,How ToBuilda Beowulf: AGuidetothe Implementationand Application ofPC Clusters,by
ThomasSterling,JohnSalmon,DonaldBecker,andDanielSavarese.
Scyld ClusterWare
Penguin Computing offers Beowulf clustering as its
AApppplliiccaattiioonnss
Scyld ClusterWare product, coupling it with the
company’s line of cluster servers. It obtained Scyld GGlloobbaallSSttoorraaggeeaannddSShhaarreeddFFiilleeSSyysstteemm
(pronounce “skilled”) with the acquisition of Scyld HHiigghhAAvvaaiillaabbiilliittyyFFeeaattuurreess
Computing in 2003. Scyld Computing was founded by
JJoobbSScchheedduulliinngg
DonaldBecker,thedeveloperofBeowulfandthecurrent RReessoouurrcceeMMaannaaggeemmeenntt
CTOofPenguin,in1993. SSyysstteemmMMaannaaggeemmeennttTToooollss
UUssuuaaggeeMMoonniittoorriinngg
HHaarrddwwaarreeMMoonniittoorriinnggaannddMMaaiinntteennaannccee
ThePenguinStack
CClluusstteerriinnggLLiibbrraarriieess
KKeerrnneellEExxtteennssiioonnss
Penguin’s ScyldClusterWaresupports allof thelevels of
the cluster stack. With Penguin’s line of cluster systems
HHiigghh--SSppeeeeddIInntteerrccoonnnneecctt
for the hardware level come the Linux operating system
and the development tools needed to develop a LLiinnuuxxDDiissttrriibbuuttiioonn
CCoommppiilleerraannddDDeevveellooppmmeennttTToooollss
clusteringsolution.
HHaarrddwwaarreePPllaattffoorrmm
Penguin’s cluster systems also come complete with a
high-speed interconnect. This interconnect can either be ThePenguinStack
3
However,onemustbecognizantofthepitfallsof“buildingyourown”cluster.SeeourarticleintheDecemberissueof
theAvailabilityDigest, Can10,000ChickensReplaceYourTractor?
2
©2007SombersAssociates,Inc.,andW.H.Highleyman

gigabyteEthernetorInfiniband.
Thestandard Linux operatingsystem is augmented byPenguin withkernel extensions to support
clusteringaswellaswiththerequiredsetofopensourceclusteringlibraries.
The Scyld TaskMaster Suite provides the functions of job scheduling, resource management,
usagemonitoring,systemmanagementtools,andhardwaremonitoringandmaintenance.
Finally, Scyld ClusterWare includes support for a variety of file systems. Its high-availability
featuresprovidethereliabilityneededforthesesystems.
AllofthisissupportedbyservicesofferedbyPenguin.
ClusterArchitecture
AScyldClusterWareclustercontainsagroupofcomputenodesmanagedbyaMasternode.The
Master node assigns jobs to the compute nodes based on a scheduling policy. The compute
nodes are lightweight nodes with a memory-resident operating system for maximum
performance.
compute compute compute
compute compute compute
comnpoudtees comnpoudtees comnpoudtees
nodes nodes nodes
nodes nodes nodes
redundant
filesystem
high-speed
interconnect
switch
Master Master
Master Master
(active) (active)
(active) (active)
system users
administration
The compute nodes and the Master node are interconnected by a high-speed, low-latency
network. Also accessible through this network are the cluster file systems. These could be direct
attached storage, storage area networks (SANs), or network attached storage (NAS). Redundant
databasessuchasRAIDaresupported.
Inadditiontojobscheduling,theMasternodeisresponsibleforoverallclustermanagement.
TheMasterNode–TheSinglePointofClusterControl
ScyldClusterWarerunsonaMasternodeinthecluster.TheMasternodeactsasthesinglepoint
of control for the entire cluster. Using Penguin’s Scyld TaskMaster Suite, the Master node
providesallofthefunctionsofclustermanagement,including:
 Schedulingjobs.
 Addinganddeletingcomputenodes.
3
©2007SombersAssociates,Inc.,andW.H.Highleyman

 Managingapplications.
 Installingnewapplicationsandnewversionsofexistingapplications.
 Clustermonitoringandadministration.
Forsystemswhichrequirehighavailability,aScyldclustercanbeconfiguredwithtwoMaster
nodes,oneactingastheactivenodeandtheotherasabackupnode.
Multiple Master nodes can be configured, each with its own compute nodes. These Masters can
interact, therebycoordinatingtheir respective workloads.Ineffect,acluster of clusters is created.
Computenodes can bemigratedfrom oneMaster to another according topolicies establishedby
theuser.TheMasterscanbackupeachother,andeachMastercanfailovertoitsbackupshould
itfail.
Scyld ClusterWare comes bundled with the CentOS Linux distribution. Penguin also offers
support for Red Hat Linux. Also included in the bundle is a toolkit that includes open source
libraries such as the MPI messaging library, Ganglia web-based monitoring, cluster file systems,
compilers (GCC, C, C++, Fortran), and a user interface for the monitoring of cluster status and
resourceutilization.
ScyldTaskMasterSuite
The Scyld TaskMaster Suite is the heart of cluster control. It virtualizes the cluster to provide a
single-system image to the operator. This reduces the complexity and administration burden of
clusteredcomputing.AnypersonwiththeskillstomanageastandbaloneLinuxsystemcaneasily
manageaPenguinBeowulfcluster.
Among its primary activities is job scheduling. By default, jobs are scheduled to run in the least
loadedcomputenode.Alternatively,ajobcanbeassignedtoaparticularcomputenodeifspecial
services are required. More generally, job scheduling is determined by a scheduling policy
established by the user. If desired, reservations of system capacity can be made for jobs to be
run in the future. Scyld TaskMaster provides a simulation function for “what if” testing of
schedulingpolicies.
Inaddition,ScyldTaskMasterprovidesthefollowingfunctions:
 Addingordeletingcomputenodesondemandwithinseconds.
 Runningandmanagingapplicationsandensuringthatallversionsareup-to-date.
 Monitoringallcomponentsinthecluster,withstatusreports.
 Accessing jobs, nodes, statistics, policies, and available resources (now and in the
future).
 Supporting lights-out management of remote facilities through remote power
managementandsystemhealthmonitoring.
 Providing event triggers to automate maintenance tasks, to adjust scheduling policies,
andtosendnotifications.
Agraphicalinterfaceforclusteradministrationprovidesaccesseitherlocallyorthroughtheweb.
A primary goal of Scyld TaskMaster is to optimize cluster utilization, with a goal of 90% to 99%
utilization.
4
©2007SombersAssociates,Inc.,andW.H.Highleyman

HighAvailability
Scyld ClusterWare and the Penguin servers provide the functions required to achieve high
availability. The Master nodes can be backed up by passive standbys so that the system
continues in operation should a Master fail. Furthermore, there can be multiple Master nodes
backingupeachother.
Shouldacomputenodefail,itsworkloadcanbemovedtoanothercomputenode.
Compute nodes and system and application processes are looselycoupled through a messaging
systemsothatafaultinonecomponentwillnotdirectlytakedownanothercomponent.
Penguin clusters can be provided with redundant power supplies, and all hardware components
arehot-swappable.Componentswappingrequiresnotools.
High-PerformanceComputing
Scyld ClusterWare is optimized for high-performance computing. The compute nodes exist only
torun applications as specifiedbytheMaster node. Computenodes are lightweight, havingbeen
stripped of unnecessary software and overhead. The operating environment in the compute
nodes is stateless and is fully memory-resident. Libraries are automatically cached just in time
(JIT)astheyareneeded.
Because the compute nodes are lightweight, they can be flexibly added or deleted in seconds.
Thisallowsvirtuallyinstantaneousadjustmentofacluster’scapacitytomeetcurrentworkloads.
The Scyld TaskMaster ensures that all operating system components and application
componentsarealwaysatthelatestversionlevel.Thereisnoversionskew.
Neither the compute nodes nor the Master nodes contain any unnecessary software. Therefore,
theyarevirtuallyimpervioustooutsidemaliciousattacks.
Penguin Systems
The Penguin Application-Ready Clusters include two configurations – the Penguin Performance
Cluster and the Penguin High Density Cluster. Both are available with either Intel Xeon or AMD
Opteron microprocessors.The Opteron microprocessors can be provided as either single core or
dualcore.
AllclusterscomecompletewithScyldClusterWareandallotherrequiredsoftwareinstalled.
PerformanceCluster
A Penguin Performance Cluster includes a full range of hot-swappable
SATA or SCSI disks and hardware RAID controllers. Direct attached
storage, network attached storage, and storage area networks are
supported.Alldiskunitsarehot-swappablewithouttools.
Each rack includes redundant power supplies and high-speed
interconnects using gigabyte Ethernet, Infiniband, or Myrinet. All power
supplies, cooling fans, and blades are hot-swappable without the need
5
©2007SombersAssociates,Inc.,andW.H.Highleyman

fortools.
The cluster blades are a 2U rackmount form factor. Each blade can contain two Xeon or two
Opteron(singleordualcore)processors.
A cluster node can contain from 32 to 256 compute nodes, and the cluster itself can scale to
thousandsofcomputenodes.Eachnodecomesequippedforlights-outoperation.
HighDensityCluster
TheHighDensityClusterissimilartothePerformanceClusterexceptforthebladeconfiguration..
It packages 24 processors (Xeon or Opteron single or dual core) in 4U of rack space. A single
42Urackcancontainupto480computenodesandprovidestwoteraflopsofprocessingpower.
Penguin Computing
Penguin, located in San Francisco, was founded in 1998. It
acquiredScyldComputingin2003.
Its Chairman and CEO is Enrico Pesatori. Donald Becker is its
CTO, and Pauline Nist is Penguin’s Senior Vice President of
ProductDevelopmentandManagement.
Summary
ThebenefitsofScyldClusterWarearemany:
 The entire cluster is managed as a single virtual machine. There is only a single point of
management.
 ItrequiresinstallationonlyontheMasternodes.
 ThereisvisibilityintotheentireclusterfromtheMasternode.
 Automaticjobschedulingisprovidedbasedonuser-suppliedpolicies.
 Computetimeslotscanbereserved.
 Workloadscanbeprioritized.
 Computenodescanbeadded,deleted,orreprovisionedinseconds.
 Itishighlyscalabletothousandsofcomputenodes.
 Itcanbeconfiguredtobehighlyavailable.
 Because of the use of lightweight software, clusters are highly secure and virtually
impervioustomaliciousattack.
 Accountingisprovidedforsharedusage.
6
©2007SombersAssociates,Inc.,andW.H.Highleyman
