# We had issues with Monzo on 29th July. Here\'s what happened, and what we did to fix it. 

This blog post was accurate when we published it -- head to
[monzo.com](http://monzo.com/){rel="noopener noreferrer"
target="_blank"} or your Monzo app for the most up to date information.

On the 29th July from about 13:10 onwards, you might have had some
issues with Monzo. As we shared in [an
update](https://monzo.com/blog/2019/07/30/we-had-issues-with-monzo-yesterday)
at the time, you might not have been able to:

-   Log into the app

-   Send and receive payments, or withdraw money from ATMs

-   See accurate balances and transactions in your app

-   Get in touch with us through the in-app chat or by phone

This happened because we were making a change to our database, that
didn\'t go to plan.

We know that when it comes to your money, problems of any kind are
totally unacceptable. We\'re really sorry about this, and we\'re
committed to making sure it doesn\'t happen again.

We fixed the majority of issues by the end of that day. And since then,
we\'ve been investigating exactly what happened and working on plans to
make sure it doesn\'t happen again.

In the spirit of transparency, we\'d like to share exactly what went
wrong from a technical perspective, and how we\'re working to avoid it
in the future.

## Some background

### We use Cassandra to store our data, and have multiple copies of everything

We use a database called
[Cassandra](https://en.wikipedia.org/wiki/Apache_Cassandra) to store our
data. Cassandra is an open-source, highly-available, distributed data
store. We use Cassandra to spread the data across multiple servers,
while still serving it as one logical unit to our services.

We run a collection of 21 servers (which we call a cluster). And all the
data we store is replicated across three out of the 21 servers. This
means if something goes wrong with one server or we need to make a
planned change, the data is entirely safe and still available from the
other two.

Cassandra uses an element called the **partition key** to decide which
three servers in the cluster of 21 are responsible for a particular
piece of data.

![A diagram showing a piece of data stored across three
servers](https://images.ctfassets.net/ro61k101ee59/0K9zDvgmPxo7vd7RJS3xb/a8c0d642a744a48f610057484f828ea7/_inc-platform-issues_-_Google_Slides.png?w=1280&q=90)\
Here we have a piece of data represented by the pink square which is set
to the value T

When we want to read data, we can go to any of the servers in the
cluster and ask for a piece of data for a specific partition key. All
reads and writes from our services happen with quorum. This means at
least two out of three servers need to acknowledge the value before data
is returned from or written to Cassandra.

The entire cluster knows how to translate the partition key to the same
three servers that actually hold the data.

![Diagram showing us reading the data from three
servers](https://images.ctfassets.net/ro61k101ee59/1HvwaK1B4xCuCrABd3AIrK/2eff99b007d306bb62f478909cbdc279/Untitled.png?w=1280&q=90)

## We were scaling up Cassandra to keep apps and card payments working smoothly

As more and more people start using Monzo, we have to scale up Cassandra
so it can store all the data and serve it quickly and smoothly. We last
scaled up Cassandra in October 2018 and projected that our current
capacity would tide us over for about a year.

But during this time, lots more people started using Monzo, and we
increased the number of microservices we run to support all the new
features in the Monzo app.

![A graph showing the microservices we use going up over
time](https://images.ctfassets.net/ro61k101ee59/61pdYUleMaupifO14nlvXG/4a705c1ea61e66de89162437324611d2/Using_Envoy_as_a_Service_Proxy__Devops_Exchange_London__-_Google_Slides.png?w=1280&q=90)\
We run a microservice architecture, and the number of services we run is
growing as we offer more features.

As a result, during peak load, our Cassandra cluster was running closer
to its limits than we\'d like. And even though this wasn\'t affecting
our customers, we knew if we didn\'t address it soon we\'d start seeing
an increase in the time it\'d take to serve requests.

For some of our services (especially those we use to serve real time
payments), taking longer to serve requests would mean we were slowing
down the apps and card payments. And nobody wants to wait for a long
time when they\'re at the front of the queue to pay for their shopping!

So we planned to increase the size of the cluster, to add more compute
capacity and spread the load across more servers.

![A diagram showing a before and after comparison of adding extra
servers](https://images.ctfassets.net/ro61k101ee59/6HMPWU13eTzCXiTSJ19Cdg/392cf825e5b5443e5d5df52ea499e557/_inc-platform-issues_-_Google_Slides__1_.png?w=1280&q=90)\
Adding more servers means we can spread the data more evenly and support
a larger amount of queries in the future.

## What happened on 29th July 2019

This is the timeline of events that happened on the day. All times are
in British Summer Time (BST), on the 29th of July 2019.

**13:10** We start scaling Cassandra by adding six new servers to the
cluster. We have a flag set which we believe means the new servers will
start up and join the cluster, but otherwise remain inactive until we
stream data to them.

We confirm there\'s been no impact using the metrics from both the
database and the services that depend on it (i.e. server and client side
metrics). To do this, we look for any increase in errors, changes in
latency, or any deviations in read and write rates. All of these metrics
appear normal and unaffected by the operation.

**13:14** Our automated alerts detect an issue with Mastercard,
indicating **a small percentage of card transactions are failing**. We
tell our payments team about the issue.

**13:14** We receive reports from Customer Operations (COps) that the
tool they use to communicate with our customers isn\'t working as
expected. This meant **we weren\'t able to help customers through in app
chat,** leaving customers who\'d got in touch with us waiting.

**13:15** We declare an incident, and our daytime
[on-call](https://monzo.com/blog/2018/09/20/on-call) engineer assembles
a small group of engineers to investigate.

**13:24** The engineer who initiated the change to the database notices
the incident and joins the investigation. We discuss whether the
scale-up activity could have caused the issue, but discount the
possibility as everything appears healthy. There\'s no increase in
errors, and the read and write rates look like they haven\'t changed.

**13:24** Our payments team identify a small code error in one of our
Mastercard services, where a specific execution path wasn\'t gracefully
handling an error case. We believe this is the cause of the Mastercard
issue, so get to work on a fix.

**13:29** We notice that our internal edge is returning HTTP 404
responses.

Our internal edge is a service we\'ve written and use to access internal
services (like our customer operations tooling and our deployment
pipeline). It does checks to make sure that we only give access to Monzo
employees, and forwards requests to the relevant place.

This means our internal edge couldn\'t find the destination it was
looking for.

**13:32** We receive reports from COps that **some customers are being
logged out of both the Android and iOS apps.**

**13:33** **We update our public** [**status
page**](https://status.monzo.com) **to let our customers know about the
issue.** This also shows up as a banner in our apps.

*\"We\'re working to fix a problem that is causing some card
transactions to fail, or the Monzo app to display incomplete or old
information. If affected, you may also be unable to login or receive
bank transfers.\"*

**13:33** The Mastercard fix is ready to deploy, but we notice our
deployment tooling is also failing.

**13:39** We deploy the Mastercard fix using a planned fallback
mechanism. And **we see an immediate improvement in card transactions
succeeding**.

**13:46** We identify our internal edge isn\'t correctly routing
internal traffic. Beyond authentication and authorisation, it validates
that it\'s \'internal\' by inspecting the request and using our
configuration service to match against our private network range.

A configuration service is a Monzo microservice that provides a simple
request-response (RPC) interface for other services to store and
retrieve key-value pairs representing configuration. Internally, it uses
Cassandra to store its data.

We conclude that either the configuration service is down, or our
network range has changed. We quickly rule out the latter and focus on
the service.

**13:48** We try to get data from our configuration service directly and
realise it\'s returning a 404 (not found) response for the key we
attempted to retrieve. We\'re confused, as we believe that if the
configuration service wasn\'t working, it\'d have a much wider impact
than we were seeing.

**13:53** From the metrics, we see successful read and writes to the
configuration service, which is surprising given we\'ve just seen it
fail to retrieve data. It feels like we\'re seeing conflicting evidence.

**13:57** We search for some other keys in the configuration service and
realise they\'re still there.

**14:00** We bypass the configuration service interface and query
