HP’s NonStop Blades
August2008
HP has ported its fault-tolerant NonStop server to its HP c-Class BladeSystem. Named the HP
IntegrityNB50000cBladeSystem,afully-configuredsystemcancontainuptosixteenprocessors,
the same as HP’s largest contemporary NonStop servers. Based on dual-core Itanium
processors, the new multicore architecture is called NSMA, the NonStop Multicore Architecture.
An NSMA system delivers twice the power of the HP NS16000, until recently HP’s largest
NonStopserver,inhalfthefootprint.
Existing applications can be ported seamlessly to the new bladed system. Using standard
NonStop management facilities, NSMA nodes can be added to existing NonStop clusters
comprisingotherIntegrity(NS-series)andS-seriesNonStopservers.
Overview
The NSMA system uses HP’s c7000 c-Class blade processors running the NonStop operating
system.AnNSMAsystem comprisestwotosixteenbladeprocessors.Eachbladeis drivenbyan
Inteldual-coreItaniummicroprocessorwithupto48gigabytesofmemory.
The standard NonStop fault-tolerant architecture has
been ported to the BladeSystem. Processes mayrun
rossecorpedalb rossecorpedalb rossecorpedalb rossecorpedalb
as checkpointed process pairs or as persistent
processes. For checkpointed pairs, one process is
the active process. The other process, which is
running in a separate processor, is the backup
process, whose state is kept current via
checkpointing for instant takeover. Should the active
process or its processor fail, its backup will take over
processingwithoutthelossofanycontext.
Alternatively, processes can run as persistent ServerNet
processes under a checkpointed monitor that will
restart a process in an operating processor should
the process or the processor in which it was running
fail. CLIM CLIM CLIM CLIM
A major modification to the NonStop operating
system to support the NSMA system implements a
SAS
new process scheduler that is multicore-aware. Its
Disk IPLinks
responsibility is to allocate processes to cores for
properloadbalancing.
NonStopMulticoreArchitecture
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

NSMA uses a new I/O subsystem called a CLIM (Cluster I/O Module). The CLIM is a duplexed-
pair of Proliant servers that interfaces SAS disks and IP channels to the blade processors via a
dual ServerNet fabric. The CLIM also supports XP storage and other NonStop disk and tape
devicesaswellasothercommunicationchannels.
NSMA systems may join heterogeneous clusters comprising other NSMA systems, Integrity
NonStop servers, and S-series NonStop servers. All are managed by the same system
managementfacilitiesusedbyotherNonStopsystems.
NSMA Hardware Architecture
TheProcessor
NonStopBlade
The new NSMA system uses HP’s standard c7000 c-Class
blade processor. This blade fits vertically into a 10U
enclosure1 that can hold up to eight blades. Thus, a full CPU
sixteen-processorNSMAsystemcomprisestwoenclosures. IPU IPU
0 1
Ac7000c-Classbladeconfigurationincludes:
 anIntelItanium 9100dual-coreprocessorrunningat
1.66 gigahertz (each core is called an Instruction memory
ProcessingUnit,orIPU).
 18megabytesofcachememory.
 8–48gigabytesofmainmemory,inincrementsof8 ServerNet
gigabytes. Interface
In addition to this standard blade configuration, an NSMA
blade carries one additional card – a ServerNet card for
ServerNet
connecting to the dual ServerNet fabric There are two
fabric
redundant ServerNet fabrics for a NonStop BladeSystem.
NonStopBlade
The fabrics provide 12 or 24 duplexed I/O links per
enclosure. Therefore, a full NSMA system with two enclosures can provide up to 48 ServerNet
I/O connections. There is no additional cabinet height consumed by the ServerNet interconnect.
The ServerNet switches are embedded inside the blade enclosure at the back and are contained
inoneFieldReplaceableUnit(FRU).
A NonStop blade comprises one logical processor in a NonStop uptoeight
BladeSystem. There can be up to sixteen logical processors processorblades
(blades) in a system. Should one fail, its processing functions are
takenoverbyoneormoreotherlogicalprocessors.
Even though the Integrity microprocessor that drives the blade
has two independent IPUs, the fault zone is the blade itself. If one
oftheIPUsfails,thebladefails.Thereisnoattempttocontinueto
operate with only one IPU. Therefore, if a blade fails, the
processesrunninginthatbladewillfailovertootherblades.
Enclosure
NSMA uses the standard HP c7000 c-Class enclosure to house
the blade processors. This is a 10U chassis that can hold up to
sixpowersupplies
(prepopulated)
1A“U”isarackunitandis1.75incheshigh.
AnNSMAEnclosure 2
©2008SombersAssociates,Inc.,andW.H.Highleyman

eightbladeprocessors.Itcomes prepopulated withsix power supplies (2,250 watts each) and10
fans. Though only some of these are required to power and cool the enclosure, this provides
sparesand,moreimportantly,roomforgrowthasquad-andeight-corechipsbecomeavailable.
The enclosure is designed to minimize single points of failure. The midplane interconnecting the
processor blades and theServerNetfabric is notactive.Furthermore,onlypoint-to-point links are
used–therearenobusses.
NSMA cabinets are 42U high. However, only one processor enclosure may be put into a cabinet
becauseofpower,cooling,andweightrestrictions.
CLIM–TheI/OSystem
The CLIM (Cluster I/O Module) is a
newly-developed storage and
communication interface for NSMA. It
supports SAS (serial-attached SCSI)
disks and IP interconnects. It also
supports certain legacy disk and
communication systems as well as
HP’sXPStorage. CLIM
The CLIM platform is an HP Proliant DL385 rack-mounted server with a 2U height. It is driven by
a 1.8 gigahertz dual-core Opteron processor with four gigabytes of memory and contains
redundantpowersuppliesandfans.
Since a NonStop BladeSystem can provide up to 48 ServerNet I/O links, it can support up to 44
CLIMsinadditiontothetworequiredstorageCLIMs(whichusetwoServerNetportseach).
A CLIM contains eight PCI card slots. Five of these may be used for Host Bus Adapters (HBAs).
The SAS disks and fibre channel links connect to the CLIM via the HBAs, which are described
later. Via the HBAs, a storage CLIM can be configured to host four SAS ports or two SAS ports
and two fiber channel ports. The communication CLIM can support up to 5 copper gigabit-per-
secondEthernetportsor3copperand2fibergigabit-per-secondEthernetports
The CLIM functions are implemented via Linux. No customer application software can run on the
CLIM.Itappearstotheoutsideworldsimplyasadevicecontroller.
CLIMs are usually configured in redundant load-sharing pairs. CLIM health is monitored by
heartbeats sent to the NonStop blade processors. Should one CLIM fail, connectivity is
maintained to the disk and communication devices by failing over all connections to surviving
CLIMs.
StorageCLIM
The storage CLIM supports dual-
ported SAS disks. The disks are a 2½
inch form factor, and 25 disks can fit
into a 2U rack-mounted MSA70
enclosure.2
MSA70SASDiskArray
A storage CLIM pair can drive four MSA70 SAS disk storage arrays. Thus, a CLIM pair can
control up to 100 SAS disks. The disks are usually configured as mirrored pairs, resulting in 50
diskmirrorsperCLIMpair.
2Asopposedto14disksina3Uenclosureforfibre-channelconnecteddisks.
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

TheSASdiskscaneitherbe72gigabyte,15,000rpmdisksor146gigabyte,10,000rpmdisks.
The SAS disk arrays connect to the CLIM pair via host bus adapters. HBA slots can be used for
disk-array connections using one of two options. Either each MSA70 disk array may be
connectedtoeachCLIMdirectly,ortwoMSA70disk arraysmaybedaisy-chainedandconnected
as a chain to CLIM HBAs. For a full complement of four disk arrays, direct connections require
fourHBAsoftheallottedfiveoneachCLIM.DaisychainingrequiresonlytwoHBAconnections,
SAS SAS
SAS SAS
CLIM CLIM CLIM CLIM
SAS SAS
SAS SAS
MSA70DirectConnection MSA70DaisyChaining
100SASDisks 100SASDisks
FourHBAsperCLIM TwoHBAsperCLIM
All HBA links in a CLIM may be active simultaneously, each carrying three gigabytes per second
oftraffic.Thus,afullypopulatedCLIMusingdirectHBAconnectionscantransferdataatarateof
up to twelve gigabytes per second. To support this data rate, each storage CLIM by default uses
two ServerNet ports. Thus, a fully-configured NonStop BladeSystem with 22 storage CLIMs can
containover160terabytesofmirroredstorage.
The SAS disks are significantly faster than the fibre-channel connected disks used in the
NonStopIntegrityseries.Foronething,eachdiskcontainsonboardcachethatcanbeconsidered
part of the storage device since it is mirrored. The Write Cache Enabled (WCE) option allows
writes to complete to cache rather than having to be written to disk. Measured comparative
performanceimprovementswithWriteCacheEnabledaretabulatedbelow:
ReadSequential 70%
ReadRandom 20%
WriteSequential 500%
WriteRandom 35%
If WCE is used, in-cabinet UPS is required to prevent the loss of cached data following a power
outage. The standard in-cabinet UPS system can provide power for about five minutes. This can
optionallybeextendedtotenminutes.
The storage CLIM replaces FCSAs (Fibre Channel ServerNet Adapters) in an IOAME ((I/O
Adapter Module Enclosure) used in HP’s current Integrity NonStop servers, though these
adapters are still supported by NSMA via ServerNet. Integrity and S-series databases can be
migratedonlinetotheNonStopBladeSystem.
Fibre-channel connected XP storage is directly supported by the CLIM, as are SAS and fibre-
channeltapesystems.MigratinganXPstorageunittoaNonStopBladeSystemissimplyamatter
ofreconnectingittoaCLIMpair.
CommunicationCLIM
As withstorageCLIMs,communicationCLIMsarenormallyconfiguredasredundantload-sharing
pairsforfaulttolerance.EachcommunicationCLIMcansupportfiveGigabitEthernetports,either
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

as fivecopper ports or as threecopper ports andtwofibre-channelports.Onecopper port is built
intotheCLIM.TheotherfourportsareconnectedviaEthernetNICs.
A communication CLIM by default uses one ServerNet port, though two ports per CLIM may be
configured for very high traffic. Thus, a fully configured NonStop BladeSystem with 44
communicationCLIMscantheoreticallysupportupto220GigabitEthernetchannels.
TheCLIMsupportsbothIPv4andIPv6withIPSecurity(IPSec).
The communication CLIM also supports HP NonStop SWAN (ServerNet Wide Area Network)
concentratorsthathandlebisync,async,X.25,andSDLCcommunicationinterfaces.
Power
The cabinet power and cooling are designed to support the systems of the future as quad-core
andeight-coremultiprocessorsbecomeavailable.
Allcabinetsinac7000system haveapairofpowerdistributionunits(PDUs)thatsupplypowerto
the CLIMs and SAS disks. These PDUs can supply 8.6 kilowatts of I/O power. Each CLIM
requires 250 watts, and each MSA70 SAS array requires 225 watts. Therefore, a fully populated
I/Ocabinetwith20I/Ounitswillconsumeabout5kilowattsofpower.
The processor blades are not powered by the in-cabinet PDUs. Rather, they derive their power
from the six in-chassis power supplies, four of which are in standby mode ready to come into
service if the power draw increases. These 2,250-watt power supplies are fed from an
independent redundant pair of three-phase input feeds. Three-phase power is required because
ofthehighpowerdensitycreatedbythebladepackaging.
A blade consumes about 350 watts. Therefore, a fully populated blade enclosure will consume
2.8 kilowatts. Fan power can potentially drive the enclosure power requirements beyond 3.8
kilowatts, which can be supported by just two of the six in-chassis power supplies. This power
consumption will undoubtedly increase as more cores become available on the chips. A c7000
enclosurewithredundantpowersuppliescansupply6.75kilowattsofpowertotheenclosure.
In the event of an external power failure, an in-cabinet UPS can supply two three-phase output
feeds for about five minutes. One feed powers the cabinet PDU, and one feed powers the c7000
enclosure.TheUPScanoptionallybeconfiguredtosupplycabinetpowerforuptotenminutes.
NSMA Fault Tolerance
Thefault-toleranthardware characteristics of theNSMAarchitecturehavebeendescribedabove.
Not only are there multiple processors in the system, but the CLIMs provide redundant access to
network links and data storage devices. The redundant high-speed ServerNet fabric provides
fault-tolerant communication between all devices. The system is architected so that it will survive
anysinglehardwarefailureaswellassomemultiplefailures.
Thisishardwareprotection.Butwhataboutprotectingthesystemandapplicationprocesses3that
are running on the fault-tolerant platform? What if a processor fails and takes down all of the
processesinit?Whatifaprocessaborts?
The NonStop server provides two primary mechanisms to protect processes against failure –
checkpointedprocesspairsandpersistentprocesses.
3Aprocessisaprogramrunninginaprocessor.Theremaybeseveralinstancesofaprogramrunningasdifferentnamed
processes.
5
