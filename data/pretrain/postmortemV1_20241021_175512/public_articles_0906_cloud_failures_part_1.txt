Can You Trust Your Public Cloud?
June2014
Joyent is the latest cloud to bite the dust. It follows a long series of public-cloud failures
by even the largest cloud providers – Amazon, Google, Microsoft, Salesforce, and
Rackspace,tonameafew. Joyent’s failure emphasizes theneedtobe preparedfor your
public-cloud services to suddenly disappear for hours and sometimes for days. Even worse, your data
mightdisappear.
Inthis article, wereviewmanyof thepublic-cloudfailures andsee what wecanlearnabouttrustingcloud
servicesandaboutprotectingourapplicationsanddatafromtheirfaults.
Joyent Cloud is Joyent’s hosting service. It is designed to complete with Amazon’s EC2
cloud, providing Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) for
large enterprises. Though Joyent is aimed mainly at the online social-network game market, it hosted
TwitterinTwitter’searlydays.
A Joyent system administrator was updating software on some new servers when he accidentally typed
the wrong command. Instead of causing just those servers to reboot, he commanded all of the servers in
the data center to reboot. Everything went down. Due to the heavy loading imposed upon the boot
infrastructureandthecontrolplane,ittookhoursfortheserverstoreboot.
The fundamental problem was that the tool that the administrator was using did not have enough input
validation to prevent this from happening. There was no “Are you sure you want to reboot all servers”
query.
Joyent did not fire the administrator – they pointed out that his mortification was punishment enough.
However,theyhaveinitiatedamajorprojecttoenhancealladministrativetoolssothatsomethinglikethis
scenariowillneverhappenagain.1
Shades of Y2K! Microsoft’s Windows Azure Cloud went down for over a day on
Wednesday, February 29, 2012. Starting around midnight as the clock ticked over to
Leap Day, various subsystems of the Azure Cloud started to fail one-by-one. Soon,
applicationsformanycustomersbecameunresponsive.
Theoutagesresultedfromadate-relatedproblem inthevirtualmachine(VM)securitycertificatesthatare
used in the Azure Cloud to authenticate VMs to their physical hosts. Unless a VM has a valid certificate,
thehostsystemwillnotallowittorun.
1Fat-fingeredadmindownsentireJoyentdatacenter,AvailabilityDigest;June2014.
http://www.availabilitydigest.com/public_articles/0906/joyent.pdf
1
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

The Azure Cloud VM certificates are good for one year. The certificate-creation program simply
incremented by one the year of the current date to get the certificate’s expiration date. This meant that a
certificate created on Leap Day, February 29, 2012, had an expiration date of February 29, 2013.
Unfortunately,therewasnosuchdate;andthecertificatewasdeemedinvalid.
VMs carrying this invalid certificate could not connect with their hosts. This led the Azure Cloud to
determine that these physical host servers were faulty, and they were taken out of service. All of their
VMs were moved to other hosts, which suffered the same fate and were taken out of service. The failure
ofhostserverscascadedthroughouttheAzureCloud,ultimatelytakingitdown.
It was not until 8 AM Thursday morning, thirty-two hours later, that Microsoft reported the completion of
recoveryefforts.2
What should have been a ten-minute outage at a major Google data center hosting the
Google App Engine service turned into a two-and-a-half hour ordeal simply because of
erroneousfailoverdocumentation.
Following a power outage, Google’s backup power kicked in. However, due to a faulty power switch,
about 25% of the servers in the data center did not receive backup power and subsequently went down.
Their applications failed over to surviving servers in the data center, which were not configured to handle
thatmuchadditionalload;andtheyfailed.Theserverfailurescascadedthroughoutthedatacenter.
The decision was made to fail over to the backup data center, which had been newly reconfigured. The
documentedprocedureswerefollowedbutledtoanunsuccessfulfailover.
It then looked like the primary data center had returned to operation, so processing was returned to it.
This turned outtobeanerroneous observation since theprimarydatacenter with only75% of its servers
operatingstillcouldnothandlethefullload,anditonceagainfailed.
Finally, knowledgeable technical personnel were reached; and the backup data center was brought
successfully online. A post mortem discovered that parts of the documentation of the new failover
procedures incorrectly referred to the old data-center configuration rather than to the upgraded
configuration. Clearly, the newly documented failover procedures had not been tested; nor had the staff
beentrained.Otherwise,theseerrorswouldhavebeenfound.3
Amazon is one of the largest cloud-service providers in the world. Its Amazon Web Services
(AWS) comprises Elastic Compute Cloud (EC2) and the Simple Storage Service (S3). S3 is an
onlinestorageservicethat acustomercanusetostoreandretrieveanunlimitednumberofdata
objectsthroughasimpleweb-servicesinterface,accessingeachdataobjectviaitsuniqueURL.
Access to S3 is controlled by the AWS Authentication service. Early one morning, system operators
started seeing unusually elevated levels of authentication requests at one of their locations that provide
AWS Authentication services. This heavy activity ultimately overloaded the S3 infrastructure at that site,
and Amazon was unable to process any further requests at that location. Further requests were rerouted
tootherAWSservicesites,buttheincreasedloadcausedthemtofailalso.S3servicesweredown.
It took over three hours to reconfigure enough capacity to handle the increased authorization load and to
return S3 services to Amazon’s customers. During this time, thousands of web sites that depended upon
S3anditscompanionEC2serviceweredown.
2WindowsAzureCloudSuccumbstoLeapYear,AvailabilityDigest;March2012.
http://www.availabilitydigest.com/public_articles/0703/azure.pdf
3PoorDocumentationSnagsGoogle,AvailabilityDigest;April2010.
http://www.availabilitydigest.com/public_articles/0504/google_power_out.pdf
2
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

An outcryarose from thecustomer communityabout Amazon’s lack of communication duringthe outage.
Customers complained that they had no way to know whether the problem was due to their own
equipment or to Amazon’s services. There were neither email notifications nor updates on Amazon’s
AWS blog. In response to these customer complaints, Amazon developed a service-health dashboard
indicatingthestatusofitsvariousservices.4
Once again for Amazon. Amazon has gone to great lengths to ensure the availabilityof its cloud
services. It has broken its cloud infrastructure into multiple regional, geographically-separated
data centers; and within each region it provides independent AvailabilityZones. A customermay
runacriticalapplicationinmultipleAvailabilityZoneswithinaregiontoensureavailability.
However, a maintenance error took down an entire Availability Zone. The outage began with an upgrade
to increase the capacityof the primarynetwork of one of the AvailabilityZones. The first step was to shift
all traffic off of the primary network to one of the other high-speed primary links. Unfortunately, a
technician error caused all traffic to be rerouted instead to a much slower secondary network. The
secondarynetwork could nothandle the traffic.Consequently,the computenodes in the AvailabilityZone
were effectively isolated from their storage subsystems. So far as the nodes were concerned, their
databasereplicasweredown.
The nodes started searching the Availability Zone for other storage subsystems that they could use to
remirror their data. However, being isolated from the other storage subsystems, many nodes were left
continuouslysearchingtheAvailabilityZoneforfreestoragespace.
This remirroring storm exposed in the Amazon software an unknown bug that caused a node to fail if it
was trying to close a large number of replication requests. This caused more nodes to fail and increased
the intensity of the remirroring storm. Many customers who were not running applications redundantly
acrossAvailabilityZonesweredownforthefourdaysthatittooktorestoreservicetotheregion.5
One evening, companies and individuals around the world began to lose their web
sites and email services. An estimated fifteen million web sites and an untold number
of email accounts suffered failure and did not recover until six hours later. Most of the
web sites were those of individuals or small businesses. Small businesses lost massive amounts of
revenueinlostsales.
This catastrophe was caused by an outage incurred by GoDaddy. GoDaddy is a major Internet domain
registrar and web-hosting company.It is thelargestregistrar inthe world, having registeredmorethan 45
milliondomainnames,fourtimesmorethanitsnearestcompetitor.
Initial conjecture was that GoDaddy had been taken down by a DDoS attack. In fact, one attacker took
credit for the attack. However, GoDaddy was busy getting to the root cause of the failure and finally
announced that the outage was notcaused bya DDoSattack atall. Rather, it was an internal networking
problemthatcausedcorruptionofroutingtablesdirectingtraffictoitsDNSservers.
With no access to its DNS servers, domain URLs managed by GoDaddy could not be converted to IP
addresses; and those web sites and email domains therefore could not be reached. Not only were the
web sites and email addresses hosted by GoDaddy inaccessible, but so were those hosted by other
companiesbutwhosedomainnameswerehostedbyGoDaddyonitsDNSservers.6
4HowMany9sinAmazon?,AvailabilityDigest;July2008.
http://www.availabilitydigest.com/public_articles/0307/amazon.pdf
5Amazon’sCloudDownedbyFatFinger,AvailabilityDigest;May2011.
http://www.availabilitydigest.com/public_articles/0605/amazon_ebs.pdf
6GoDaddyTakesDownMillionsofWebSites,AvailabilityDigest;September2012.
http://www.availabilitydigest.com/public_articles/0709/godaddy.pdf
3
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

It’s not a good idea to test a fire-suppression system by triggering it. But that’s what
happened to WestHost, a major web-hosting provider. The accidental release of a
blastoffire-suppressantgasseverelydamagedmostofitsdatastores.
TheWestHostfire-suppressionsystem worksbyreleasingInergengas.Comprisingcommonatmospheric
gases, Inergen is environmentallyfriendlyand breathable bypeople; but it reduces the oxygen content of
airtoalevelthatdoesnotsupportcombustion.
The WestHost data center underwent a standard yearly test of its Inergen fire-suppression system.
Unfortunately, a test technician did not remove one of the actuators that activated the system. When the
system was re-armed following the test, the actuator fired and triggered the release of a large blast of
Inergengasdesignedtoputoutafire.
Hundreds of disk-storage systems were severely damaged. WestHost operations immediately came to a
halt, and it was days before full service was restored. Recovery efforts put WestHost’s customers out of
commissionforuptosixdays.Unfortunately,thebackupdiskswereinthesamedatacenterandsuffered
thesamedamage.MuchofWestHost’scustomers’datawasirretrievablylost.
Subsequent tests by Siemens, the manufacturer of the fire-suppression system, and Tyco, the maker of
Inergen gas, showed that it was not the sudden increase in gas pressure but the blast of the fire sirens
thatdamagedthedisks.Amongtheirrecommendations–aimthesirensawayfromthediskenclosures.7
Salesforce.comisasoftwareon-demandutilityprovidingCustomerRelationship
Managementsoftwareservicestoitscustomers.Itsusersdependuponcriticalcustomerand
salesdataheldbytheSalesforce.comdatacenterstoruntheirdailybusinesses.Asautility,
theSalesforce.comservicesareexpectedtobealwaysavailable.
Salesforce.com is an Oracle RAC user along with Oracle’s TimesTen In-Memory database. However,
Salesforce.com was pushing Oracle to its limits. At the time, Salesforce.com’s database measured in the
