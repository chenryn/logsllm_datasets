Incident Review: What
Comes Up Must First Go
Down
—
Incident review facilitated by Nathan Lincoln,
write-up by Fred Hebert

Table of contents
Theincident
Thesetup
Earlymorningpages
Businesshoursinvestigation
Beinginabadplace
Analysis
Howadatabasedies
Singlepointsoffailureandouroboroses
Incidentresponseispersonal
Correctiveactions
Conclusion
honeycomb.io 1

OnJuly25th,2023,weexperiencedatotalHoneycomboutagespanningingest,querying,
triggers,SLOs,andourAPI.Thecondensedtimelineisthatfollowing10minutesofpartially
degradedingestion,wesawarapidfailurecascadethattookdownmostofourservices.The
outageimpactedalluser-facingcomponentsfrom1:40p.m.UTCto2:48p.m.UTC,duringwhich
nodatacouldbeprocessedoraccessed.
Theuserinterfacerecoveredaround2:48p.m.UTC,andqueryingrecoveredunevenlyacrossall
partitionsandteams.Duringthistime,requestsmayhavecontainedoutdatedinformation,or
simplyfailed.
Ingestioncamebackuparound3:15p.m.UTC,atwhichpointwecouldacceptincomingtraffic
andAPIrequests.Querycapacity,alongwithtriggersandSLOalerting,keptregainingaccuracy
andsucceededformoreandmoreusers,untilservicewasfullyrestoredat3:35p.m.UTC.
Ourtotaloutagetimewasroughlyonehourand10minutes,withanextra28minutesforingest
(10minutespartial,18minutestotal),and47minutesofdegradedqueryingandalerting
(triggersandSLOs)forroughlytwohoursofincidenttime.
Thisoutageisourbiggest(ormosttotal)sincewe’vehadpayingcustomers,buttheevents
behinditareunremarkable.Inthisreview,wewillcovertheincidentitself,andthenwe’llzoom
backoutforananalysisofmultiplecontributingelements.Finally,we’llgooverourresponse
andtheaftermath.
The incident
The setup
TheeventsstartedonMondayJuly24th,lateinthedayforourwestcoastengineers,whenwe
switchedbetweentwoclustersofRetriever(ourstorageandqueryengine).Wefoundapotential
bugontheclusterthatrepresentsthenextsoftwareversiontorunthere,sowedecidedtogo
backtotheoldversionandfixtheproblemonthenew,unusedclusterinthemorning.Thisisa
routinechangewe’vedonemultipletimesinthepast,anditfeltlikethesafestoption.
Shortlyafter,ourinternalSLOforShepherd(ouringestservice)startedslowlyburning.This
happenedinshortspikesattheendofeachhour;however,sincetheservicedoesn’tnormally
experienceheavyloadatnight,ittookmultiplehourstomakeasignificantenoughdentforour
engineerstobenotified.
honeycomb.io 2

Adifferentgroupofengineerslookedintotheissue,whichseemedlocalized:thecallstorefresh
theirlocalin-memorycacheappearedslow,andaliveprofilingcheckpointedatcontention
aroundsomemutexes.Eventually,theycametotheconclusionthatthedatabaseitselfwas
slow,whichbubbledouttotheingestservice.Therehadbeenaneedforingestscale-uparound
thattime,butthewritevolumedidn’tmatcheither.Theylookedintonewcodethatshipped
earlierinthedayandfoundnothingthatcouldexplainadatabaseoringestissue.
Atabout8:00p.m.localtime,theengineersdecidedthatsincetheproblemwasjusta
marginallyslow,inconsistentperformanceburnwithalackofgoodexplanation,thiswasa
minorissuethatcouldbeinvestigatedinthemorning.
Thisisajudgmentcallweoftenrecommendtoengineerstoensuretheyarewellrestedwhen
dealingwithincidents.
Early morning pages
Around4:00a.m.PDT,ouron-callengineers—differentfromthosewhoswitchedflagsorwho
investigatedthenightbefore—receivedpageralertsforingestissuesandperformance.This
timearound,thealertspagedbecauseofthemorningeastcoastscale-upiningestvolume,
whichamplifiedtheperformanceissues.
AstheengineerslookedintothealertsandtheSlacklogsfromthenightbefore,theynoticedit
wasacontinuationofthesameproblem.They,too,concludedthatthiswasanintermittent
issuewithoutsignificantcustomerimpactthatcouldwaitforinvestigationduringbusiness
hours.
Business hours investigation
Inthemorning,duringeastcoastworkinghours,oneofourengineerssawtheissuelogsfrom
thenightbefore.Theywentoverthesamethreadsothershadlookedinto,sawaquerythat
lookedatallthedatabasecallsallofourservicesdid,andre-ranitonawidertimeline(24hours
insteadofoneminute)toseeifanypatternheldupovertime.Thispointedoutabigchange:
honeycomb.io 3

Queryshowingthedatabasecallsoveraoneminutewindow,hopingtohighlightspecificcostlyoperationsthatstackup
atthetimesymptomsareobserved.
honeycomb.io 4

Thesamequeryovera24hourperiod,hopingtoshowwidetrendsineventsovertimetohighlightchanges.
Asasidenote,thisisapracticeourengineersrecommend:goingfromanarrowtoawideview
oftenrapidlyinvalidatesorconfirmspotentialinvestigationpathsbyshowingwhetherpatterns
arespecifictothecurrentinvestigation,ornormalandmisleading.
Goingfromanarrowviewtoawideoneimmediatelyrevealedadrasticdropinsomedatabase
calls(theblueones).ThesecallsarenamedSetDatasetColumnsLastWrittenandtheycome
fromourRetrieverservice,andupdatesthelasttimewereceiveddataforanyfield,forany
dataset,foranycustomer.
honeycomb.io 5

Twothingsstoodoutrightaway:
1. Thischangewasrelatedtotheshiftininfrastructurefromthenightbefore.Theengineer
whonoticedthepatternwasawareofthechange,andknewthatshiftingbetween
clustersalsoshiftedwhichsetofserverswouldupdatethesefields.
2. Thisfieldwasusedbyourcachingmechanismtoknowwhichdatasetschemaswe
neededtoactivelyrefresh,andwasusedtopre-warmcachesforthewholeingest
service.
Puttogether,thesethingsprovidedaconvenientexplanation:somethingabouttheseupdates
ontheoldqueryengineclusterfailed,whichunderminedthecache.Thisexplainedtheingest
performanceissues.
Thedependencycyclelooksabitlikethis:
High-levelarchitecturediagramshowinghowusereventsareusedbythewritertoupdateschemas,andthatfieldis
requiredforcachebackfilling,whichifnotdone,hasreadfallthroughtothedatabase.
Theissuestillwasn’tconsideredcriticalatthispoint,sincetherewasonlyaslightdegradation
inperformance.Shepherdsmaintaintheirownin-memorycache,andweassumedthatthe
latencyspikeshappenedwhentheybypassedtheschemastoretorunexpensivequeries,but
otherwise,thingswerestable.Thiswasstillweird,though.We’vemadetheswitchbetween
storageclustersmultipletimesbeforewithoutaproblem.Soasourengineerscameonline
honeycomb.io 6

accordingtotheirrespectivetimezones,theylookedintowhatcouldbebehindthisdropof
writestobringthemback.
Sincethere’saslightvariationininfrastructurebetweentheclusters,oneofthetheorieswe
entertainedwasthatmaybe,throughrecentchanges,somepermissionsorwritemechanisms
hadgonewrongontheoldinfrastructure.Wechangedwhichclusterhandledthewritesbackto
thenewonebyflippingafeatureflag.Thiswouldletusgetthefieldsupdatedwithoutrunning
theriskofhavingqueriesencounterthebugwestillhadtoaddress.
We’veusedthatflagmanytimesbefore,soweknewthatflippingittemporarilystopsallthe
writes:thetimestampforanyindividualentryisonlyupdatedevery10minutesorso,and
switchingthewritescreatesasynchronizationpointbyresettingtimersuniformly.Itusually
takesroughly10minutesbeforeseeingthemcomebackatfullvolume.Afterthese10minutes,
however,theystilldidn’tcomeback.Thingsstartedtofeelevenweirder.Werestartedacouple
hosts,butdidn'tnoticeanimprovement.
Anengineerstartedtodigintothefeatureflag’simplementation,andnoticedasubtlebug:
wheneverweswitchtheflag,thegoroutinethatwritestheupdatesreturnsinsteadofcontinuing
overthecurrentiteration.Therefore,whicheverhostsdidthewritinginatimedloopstopped
doingso,butmoreimportantly,nevertriedagaineveniftheflagwasswitchedback.
Thekeydistinctionwasthetimescalesandwhenweusedtheflag.Afullrebootwasrequired
forwritestomigrateovertoaclusterwhosewriteshadbeenturnedoffbefore.Theflawwas
presentintheflagallalong,buthiddenthroughourdeploymentmechanismoftenrunningin
parallel.
Sincedeployscausedallhostsonallservicestorestart,anddeploysoftenhappenedright
aroundthetimeweswitchedflags,itpaperedoverthebehaviorandmadetheflaglooklikeit
workedasexpected.Everyoneknewitworkedfine.
Thissurprisedpeopleintheincident;somewerefamiliarwiththiscode,hadmodifiedor
revieweditinthepast,andthat“exitandnevercomeback”behaviorwasn’tsomethingthatthey
evernoticed.Aswelearnedthroughtheincidentreviewprocess,literallynooneinthe
organization,includingthepeoplewhowrotethecodeanduseditthemost,hadanyideathat
thisishowitactuallyworked.
honeycomb.io 7

Thisinsightletusfindmoredata.Thefollowingqueryshowsthehistoryoftheattemptedwrites
percluster(purplebeingnew,orangebeingold):
YoucanseeeverydeploymentpriortoJuly25th(inUTC)causingbumpsofattemptedwriteson
theorangeline,andthenstoppingasthecodeloopsexits.Thepurpleclusterisfullyused.
However,whenweswitchedclustersonJuly24th,wedidsoatthetailendofadeployment
windowandmostbutnotalltheRetrieverhostsstoppedwriting.Fortheentirenight,thefew
trailinghostsontheoldclusterwrotetheirdata,sothecustomerswhosendusthemosttraffic
andspanmanypartitionswereabletokeeptheirownschemas’cachewarm.
Whenweswitchedwritesacrossclusterstobringdatabackinthemorning,westoppedthe
writesandkeptthecachefromrefreshing.Themomentweunderstoodthis,wewrotea
commandtorollourqueryengine’sclusterasquicklyaspossiblewithoutinterferingwithother
criticalfeatures(queryingandalerting).
Butmereminutesbeforewegottoit,thingswentbad:
Queryvolumebeforetheincident,andafter,whenalltheworkstartedpilingup.
Thebiggreenlineisnewschemachangeslaggingandpilingup,whichisaverybadthing.They
shouldbegoingmuchfaster,andinlowervolume.
Withintwominutes,ourend-to-endalerts(whichmonitorourabilityforwritestogothroughthe
systemandgetqueried)firedandpagedus,andamassiveonslaughtofdatabaseconnection
errors(forhavingtoomanyconnections)happened.Wenoticedallcriticalserviceswere100%
down.
honeycomb.io 8

Being in a bad place
Inthesetwominutes,wewentfromaleisurelymorninginvestigationintoafull-blownincident
response.
Weknewfrompastexperiencethatingestoutageslikelyrequireweirdmanualcircuit-breaking
ofalltrafficjusttocomeback;thehostscan’tcomeupontheirown.Inthiscase,wealso
proactivelysetupthesecircuitbreakersanddeniedalltrafficwitha5xxerror,becausewe
assumedthedatabaseissuecamefromtoomanyschemaupdateswhileoverloaded.
Theplan,inshort,wastobringingestback.But,bringingingestbackwithoutacachewould
makeitgodownagain,eitherthroughoverloadordatabaseconnectionsaturation.Wefirsthad
tomakesurethedatabasewasreadytotakeconnections,thenrestartedalloftheRetriever
hostsonthequeryenginetoprovidethedataforthecache.
Aswecuttrafficoff,agroupofengineerswenttoseeiftheycouldsavethedatabaseby
removingwhatevertransactionswerestuck.Ifwecouldtakeafewextraminutestosalvagethe
currenthost,theoverallrecoverywouldbefaster.
This,however,wasunproductive.Wecouldn’tkillconnectionsaseverythingwouldhang—even
commandslike SHOW ENGINE INNODB STATUS,whichcalltononeoftheusualtables,would
neverreturn.Itappearedthataconfluenceofheavyreadload,increasingwrites,andoverall
stressinthedatabaselockeditup.
Atthispoint,wedecidedtocallitquitsonkillingconnectionstosavethedatabase;weweren’t
makingprogressinreasonabletime.Instead,wedecidedtofailovertoareplica.
Asitturnsoutthough,thedatabasecamerightbackup.Mostofourservicescamebacknearly
instantly.Front-endqueriesworked,Shepherds(theingestservice)werebootedup,andour
queryengineappearedmostlyfunctional,itjusthadnofreshdata.
Butwecouldn’tbringtrafficbackjustyetbyremovingourcircuit-breaking.Theflagswere
flippedthepreviousnight,whichmeantfreshnessdatabehindthecachewassparse,andsince
noupdatesatallhadhappenedsincethestartoftheactualoutage,thecachewasempty.
Retrieverwasupandcouldmarkrecentlywrittendata,butwasn’tgettingtraffic:allowingtraffic
toShepherdwithoutacacheisrisky,anduntilitflowsthrough,wecan’tupdatethecache.
honeycomb.io 9

Wesplitintotwoinvestigativepaths:oneofmakingtheschemacacheservicereloadalonger
historyofdata,andonetomanuallydoSQLsurgeryinourdatabasetoforceadatareload.
Unfortunately,thelastexperimentwehadrunwiththeschemareloadingservicetomakeit
forcereloadmoredatawasunsuccessful,andtheengineerswhoownedtheservicewerenotup
yet.
SincewemadeprogressontheSQLsurgeryfront,wedecidedtogowiththatapproach.We
manuallyupdatedthe‘lastwritten’timestampofallschemasthathadseentrafficinthelastday
andmarkeditasnow.Thisintroducedincorrectdatathatisusedintwoplaces:thepartsofour
UIthattellyouhowfreshdatais,andthequeryassistant,whichlooksintotimewindowsgreater
