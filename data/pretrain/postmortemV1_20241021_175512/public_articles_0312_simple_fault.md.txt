Innocuous Fault Leads to Weeks of Recovery
December2008
The bank’s ultimate horror started with a single disk failure on one node of a three-node,
geographically-distributed system. Through a sequence of unimaginable events, this presumably
innocuous faultspread through allthree processing nodes run bythebank,taking them all down.
TheinternationalbanksuddenlyfoundthatitsPOSandATMserviceshadcometoahalt.
It would take weeks to recover, and full recovery was impossible. Significant amounts of data
were lost forever, though some of it was recoverable from other incompatible systems. Manual
reconciliationofdisputescarriedonformonths.
The Bank
Established in 1835, the bank that suffered this disaster is an international bank operating in
several countries. It is the largest bank in its home country and is a recognized leader in credit-
anddebit-cardtransactions.Onpeakdays,ithandlesover10,000,000cardtransactions.
The System
Thebank’ssystem thathandlesATMandPOStransactionscomprisesthreenodes.Theyinclude
a production node (PRD), a disaster-recovery node (DR), and a development node (DEV).
Because the system is in an active earthquake and volcanic zone, the nodes are geographically
separatedbetweentwosites.
The production node, PRD, is located at one site. The DR and DEV nodes are resident at a
second site 1,000 kilometers away. All nodes are large multi-CPU systems. The PRD and DR
nodeseachhavethirtymirroredpairsofdiskstoholdtheproductionfilesandtables.
A Simple Disk Fault
On February 17th, the unimaginable happened. A manageable disk problem grew into a triple
systemfailuresituationthattookmonthsfromwhichtocompletelyrecover.
February17
20:39: One of sixty data disks on the PRD system fails with a hard disk error. It is the mirror of
one of the mirrored pairs. There is no impact on system operation. The system vendor is notified
toobtainadiskreplacement.
23:55:Thevendor’scustomerengineer(CE)arrivesonsitewithareplacementdisk.
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

February18
01:01: The faulty disk is replaced. However, one of the CPUs halts with an unrecoverable error.
Again,thereisnoimpactonsystemoperationastheotherCPUsassumethetransactionload.
01:45: A bank technician finds a reference to this problem in the vendor’s documentation and
callsthevendor’sCustomerSupport.
02:00: Customer Support verifies that the workaround described in the documentation is
applicabletothissystem andrecommendsthatthereferencedworkaroundbeappliedtothePRD
system.
02:30-03:15: The workaround is applied to the PRD system and, after a brief test, to the DR and
DEVsystems.MISTAKE!
03:30:SectorchecksumerrorsoccurontheDRsystem,whichotherwisecontinuestooperate.
03:45:BusinessapplicationsonthePRDsystembegintoshowfaults.
04:08:ThePRDsystemfreezes.
04:25-05:20:ThePRDsystemiscold-loaded.However,checksumerrorsdominatethelogs.
05:40:ThedecisionismadetofailovertotheDRsystem.
06:40:ThecommunicationsnetworkisswitchedtotheDRsystemandistestedandready.
09:00-10:40:TheproductionapplicationsarestartedandarerunningontheDRsystem.
11:00-19:00:TheDRsystemexperiencessectorandblockchecksumerrors.
19:05: Customer Support provides a facility to back out the workaround. The workaround is
backedoutontheDRmachine.
20:00-23:00: The ATM batch run is successful on the DR system, but the POS batch run
inexplicablyfails.
00:00:Theexhaustedstaffisorderedhometorestuntil09:00thenextday.
February19
05:15:Customer Support verifies thatthecorruptionissues areaworkaroundproblem andpages
thebank’sstafftoapplythebackouttotheothersystemsimmediately.
05:55:TheworkaroundisbackedoutonthePRDsystem.
09:00:Thebank’ssupportstaffarrives,andtheenormityoftheissuesbecomesapparent.
12:20:TheprimarypartitionsofthePOSlogfilesaremovedtoasparediskforsafekeeping.
ThePRDandDEVsystemsareunusableandarequarantined.
TheproductionapplicationscontinuetorunontheDRmachine.
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

The Crash Analysis
As the crisis developed, it began to become clear what was happening. The workaround to cure
the original processor fault was an undocumented utility that had been used successfully in the
past. Although it had been blessed by Customer Service for use with the bank’s version of the
operatingsystem,itturned outthatthis was erroneous.Rather,the workaround was thecause of
the sector checksum errors. The likelihood of corruption turned out to be relative to the length of
timethattheworkaroundwasinplace.
Unfortunately, the workaround had been applied to all three of the nodes in the system without
first verifying that it worked properly. This caused all of their disks to become corrupted. Even
worse,asdatabaseupdateswerebeingmadetotheactivesystem,thecorrupteddatawasbeing
replicatedtoitsbackup,resultinginnowaytorecoverthedata.
Furthermore, when checksum errors reached a certain threshold, the disks automatically went
into write-verify mode. In this mode, every block write was read back into memory and verified.
Thisprocesssloweddowndiskactivitytremendously,causingapplicationstotimeout.
Even worse, as system processes were allocated disk extents with corrupted segments, their
failurecausedCPUhalts.
It was later determined that there was no way to recover the corrupted data. The result of this
disaster wasthatmuchofthetransactiondataforthePOSandATMsystemswasunrecoverable.
Itwouldtakefourmonthstorepairthedamage.
The Recovery
Therewerefourstepsintherecoveryprocess:
 Identifythediskcorruption
 Identifythedatacorruption
 Recoverthebusinessdata
 Recovertheplatforms
DiskCorruption
Fortunately, there was a system utilitythat could verify the sanityof a disk. This utilitydiscovered
thefollowingproblems:
 PRD–Ofthe60disksonthesystem(30mirroredpairs),40werecorrupted.
 DR–Onephysicaldiskwascorrupted.
 DEV–Onephysicaldiskwascorrupted.
 Threemirroredvolumeswereunrecoverablebecausetheirdefectstableswerefilled.
DataCorruption
The multiple disk failures caused the loss of much of the business-transaction data. The
unrecoverablefilesandtableswereidentified.
DataRecovery
The recovery of lost data was the biggest problem facing the bank. The vendor created a utility
thatcouldignorechecksumerrors.Badfilesandtables wereread withthisutility, butalldata was
foundtobemeaningless.
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

Much of the POS transactional data was recoverable from the saved POS files. This process,
however, took weeks. Settlement and payment functions using this data were successful.
Merchants were settled from totals accumulated in surviving files, though the transactional detail
waslargelylost.
Some lost data could be partially retrieved from other incompatible systems operated by the
bank.Stillotherdatawasabletoberetrievedfromthebank’sinterchangepartners.
Remainingdisputesweresettledmanuallyoveranextendedperiodoftime.
Resolvingdata discrepancies was aresource-intensiveprocess.Reflecting thesevere load on its
personnel,thebankimposedamaximumfifteen-hoursperworkdayforitspeople.
PlatformRecovery
It was found that the corrupted disks could not be used without scrubbing them with a data-
clearingutility.Ittook untilMarch6thtocleanseallofthePRDdisks.Evenwithdisk cleansing,the
system vendor had to supply two new mirrored volumes to be installed on PRD. A system disk
was created and installed on PRD on March 8th. At this point, the PRD system could finally be
returnedtoservice.
During the time that the PRD system was down, a backup system was rented to provide
redundancyfortheDRsystem.
Lessons Learned
As a result of this multi-month incident, the bank learned several things. Some of these were that
thebankdidmanythingsright.Itdidonethingmassivelywrong.
GoodDisasterRecoveryProcedures
Once the decision was made to fail over to the DR system, the failover went smoothly, even in
the face of continuing data-corruption problems. The staff realized that a real failover is a lot
different than a failover exercise. For one thing, there is no preparation time during which the
failoverisplannedandallpertinentstaffisavailable.
For another,there is nofallback capabilityshould thefailover fail.Thefailover hadto work,or the
entire system would be down. Up to this point, a disaster-recovery system was a good idea
thoughabitofanuisance.Itnowhasproventobealifesaver.
The bank’s excellent failover documentation and checklists proved to be the backbone of the
successful failover. Frequent testing ensured not only that these were up-to-date but that the DR
systemwasanexactreplicateofthePRDsystemthatitwasbackingup.
EfficientServiceManagement
A single point of contact had been set up with a service manager to resolve incidents and
complaints raised by the users of the system. This facilitated speedy assistance from other bank
areasandallowedthetechnicalstafftodoitsjob.
EffectiveCommunicationProcedures
Two open conference calls with the technical staff, bank management, and vendor support staff
ranforthedurationoftherecovery.
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

A management conference line was established to update key stakeholders on the resolution
status.
Mediaexposurewaslimitedtoasmall,nondescriptarticleregardingapplicationtimeouts.
VendorSupport
Once the problem was escalated to the vendor on February 17th, the vendor Customer Support
staff maintained an open conference bridge around the clock to support problem resolution. The
residentvendorengineersworkedtirelesslywiththebank’stechnicalstafftoresolveproblems.
The vendor even supplied additional on-site manpower for several days to allow bank technical
staffsometimeoff.
TheHardLesson–Test,Test,Test
The primary error lay on the shoulders of both the vendor’s Customer Support staff and the
bank’s technical staff. This was the rapid replication of the “workaround” to all systems in the
application network.Theworkaroundwas supposedtocureaprocessor haltthat occurredonthe
PRDsystem shortlyafterthefirstdefectivedisk wasreplaced.Whatcausedthis problem wasnot
determined,butinhindsightitprobablywasalocalizedproblemwiththefailedCPU.
Firstly, the workaround was undocumented. Though it had been used many times in the past, it
appears that it was not thoroughly tested with the operating system version being run by the
bank.Aswaspainfullyseen,theworkarounddidnotworkonthebank’ssystem.
Theonerouseffectsoftheworkaroundtookawhiletomanifestthemselves.Disksectorerrorsdid
not seem to appear until the workaround had run for a while, or least the operations staff did not
seethem.
There seems to be no reason that the workaround had to be made to the DR and DEV systems
since their CPUs were not exhibiting this problem. In fact, this problem had never been seen up
tothispoint,sowhybothertoinstallitonthosesystems?
But given that decision, the workaround was not thoroughly tested. It was given a cursory test,
and the conclusion was that it worked – ship it. Unfortunately, the problems created by the
workarounddidnotsurfaceuntilithadbeenrunningforawhileonallsystems.
In hindsight, the main lesson to be learned from this incident is rather obvious. When rolling an
upgrade through a redundant system, unless it is an emergency, take your time; and thoroughly
test it in production on one node. Testing might take days or weeks to achieve sufficient
confidencetorollitouttotheothernodes.Inthis way,iftheupgradeexhibits problems,abackup
nodecantakeoveroperations;andthefaultyupgradecanberolledbackandcorrected.
Of course, in this case, the faulty workaround was also causing corrupt data to be replicated to
thebackupDRsystem.Maybethebankwasdoomedfromthestart.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman
