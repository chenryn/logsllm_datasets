failure and for which we had not been able to take a snapshot. At 3:00
PM PDT, the team began restoring these. Ultimately, 0.07% of the volumes
in the affected Availability Zone could not be restored for customers in
a consistent state.

Impact on Amazon Relational Database Service (RDS)

In addition to the direct effect this EBS issue had on EC2 instances, it
also impacted the Relational Database Service ("RDS"). RDS depends upon
EBS for database and log storage, and as a result a portion of the RDS
databases hosted in the primary affected Availability Zone became
inaccessible.

Customers can choose to operate RDS instances either in a single
Availability Zone ("single-AZ") or replicated across multiple
Availability Zones ("multi-AZ"). Single-AZ database instances are
exposed to disruptions in an Availability Zone. In this case, a
single-AZ database instance would have been affected if one of the EBS
volumes it was relying on got "stuck". In the primary affected
Availability Zone, a peak of 45% of single-AZ instances were impacted
with "stuck" I/O. This was a relatively-bigger portion of the RDS
population than the corresponding EBS volume population because RDS
database instances make use of multiple EBS volumes. This increases
aggregate I/O capacity for database workloads under normal conditions,
but means that a "stuck" I/O on any volume for a single-AZ database
instance can make it inoperable until the volume is restored. The
percentage of "stuck" single-AZ database instances in the affected
Availability Zone decreased steadily during the event as the EBS
recovery proceeded. The percentage of "stuck" single-AZ database
instances in the affected Availability Zone decreased to 41.0% at the
end of 24 hours, 23.5% at 36 hours and 14.6% at the end of 48 hours, and
the rest recovered throughout the weekend. Though we recovered nearly
all of the affected database instances, 0.4% of single-AZ database
instances in the affected Availability Zone had an underlying EBS
storage volume that was not recoverable. For these database instances,
customers with automatic backups turned on (the default setting) had the
option to initiate point-in-time database restore operations.

RDS multi-AZ deployments provide redundancy by synchronously replicating
data between two database replicas in different Availability Zones. In
the event of a failure on the primary replica, RDS is designed to
automatically detect the disruption and fail over to the secondary
replica. Of multi-AZ database instances in the US East Region, 2.5% did
not automatically failover after experiencing "stuck" I/O. The primary
cause was that the rapid succession of network interruption (which
partitioned the primary from the secondary) and "stuck" I/O on the
primary replica triggered a previously un-encountered bug. This bug left
the primary replica in an isolated state where it was not safe for our
monitoring agent to automatically fail over to the secondary replica
without risking data loss, and manual intervention was required. We are
actively working on a fix to resolve this issue.

**Preventing the Event**

The trigger for this event was a network configuration change. We will
audit our change process and increase the automation to prevent this
mistake from happening in the future. However, we focus on building
software and services to survive failures. Much of the work that will
come out of this event will be to further protect the EBS service in the
face of a similar failure in the future.

We will be making a number of changes to prevent a cluster from getting
into a re-mirroring storm in the future. With additional excess
capacity, the degraded EBS cluster would have more quickly absorbed the
large number of re-mirroring requests and avoided the re-mirroring
storm. We now understand the amount of capacity needed for large
recovery events and will be modifying our capacity planning and alarming
so that we carry the additional safety capacity that is needed for large
scale failures. We have already increased our capacity buffer
significantly, and expect to have the requisite new capacity in place in
a few weeks. We will also modify our retry logic in the EBS server nodes
to prevent a cluster from getting into a re-mirroring storm. When a
large interruption occurs, our retry logic will back off more
aggressively and focus on re-establishing connectivity with previous
replicas rather than futilely searching for new nodes with which to
re-mirror. We have begun working through these changes and are confident
we can address the root cause of the re-mirroring storm by modifying
this logic. Finally, we have identified the source of the race condition
that led to EBS node failure. We have a fix and will be testing it and
deploying it to our clusters in the next couple of weeks. These changes
provide us with three separate protections against having a repeat of
this event.

**Impact to Multiple Availability Zones**

EC2 provides two very important availability building blocks: Regions
and Availability Zones. By design, Regions are completely separate
deployments of our infrastructure. Regions are completely isolated from
each other and provide the highest degree of independence. Many users
utilize multiple EC2 Regions to achieve extremely-high levels of fault
tolerance. However, if you want to move data between Regions, you need
to do it via your applications as we don't replicate any data between
Regions on our users' behalf. You also need to use a separate set of
APIs to manage each Region. Regions provide users with a powerful
availability building block, but it requires effort on the part of
application builders to take advantage of this isolation. Within
Regions, we provide Availability Zones to help users build
fault-tolerant applications easily. Availability Zones are physically
and logically separate infrastructure that are built to be highly
independent while still providing users with high speed, low latency
network connectivity, easy ways to replicate data, and a consistent set
of management APIs. For example, when running inside a Region, users
have the ability to take EBS snapshots which can be restored in any
Availability Zone and can programmatically manipulate EC2 and EBS
resources with the same APIs. We provide this loose coupling because it
allows users to easily build highly-fault-tolerant applications.

This event had two distinct impacts. First, there was an impact to
running applications in the affected Availability Zone because affected
EBS volumes became "stuck". Because of the architecture of the EBS
service, the impact to running instances was limited to the affected
Availability Zone. As a result, many users who wrote their applications
to take advantage of multiple Availability Zones did not have
significant availability impact as a result of this event. Some
customers reported that they had "stuck" EBS volumes in Availability
Zones other than the impacted Availability Zone on Thursday. While our
monitoring clearly shows the effect of the re-mirroring storm on the EBS
control plane and on volumes within the affected Availability Zone, it
does not reflect significant impact to existing EBS volumes within other
Availability Zones in the Region. We do see that there were slightly
more "stuck" volumes than we would have expected in the healthy
Availability Zones, though still an extremely small number. To put this
in perspective, the peak "stuck" volume percentage we saw in the Region
outside of the affected Availability Zone was less than 0.07%. We
investigated a number of these "stuck" volumes. The slightly-elevated
number of "stuck" volumes in these non-impacted zones was caused by the
delays in recovering from normal re-mirrors because of the increased
latencies and error rates of the EBS control plane described above;
there is always a background rate of volume re-mirroring going on. We
also believe that the work described below to further insulate the EBS
control plane will prevent even this slightly-elevated rate if something
similar happened.

While users' applications taking advantage of multiple Availability Zone
("multi-AZ") architectures were able to avoid impact from this event,
there was definitely an impact on the EBS control plane that affected
the ability to create and manipulate EBS volumes across the Region. One
of the advantages of EC2 is the ability to rapidly replace failed
resources. When the EBS control plane was degraded or unavailable, it
made it difficult for customers with affected volumes to replace their
volumes or EBS-booted EC2 instances in other healthy Availability Zones.
Preventing this from reoccurring is a top priority.

Even though we provide a degree of loose coupling for our customers, our
design goal is to make Availability Zones indistinguishable from
completely independent. Our EBS control plane is designed to allow users
to access resources in multiple Availability Zones while still being
tolerant to failures in individual zones. This event has taught us that
we must make further investments to realize this design goal. There are
three things we will do to prevent a single Availability Zone from
impacting the EBS control plane across multiple Availability Zones. The
first is that we will immediately improve our timeout logic to prevent
thread exhaustion when a single Availability Zone cluster is taking too
long to process requests. This would have prevented the API impact from
12:50 AM PDT to 2:40 AM PDT on April 21st. To address the cause of the
second API impact, we will also add the ability for our EBS control
plane to be more Availability Zone aware and shed load intelligently
when it is over capacity. This is similar to other throttles that we
already have in our systems. Additionally, we also see an opportunity to
push more of our EBS control plane into per-EBS cluster services. By
moving more functionality out of the EBS control plane and creating
per-EBS cluster deployments of these services (which run in the same
Availability Zone as the EBS cluster they are supporting), we can
provide even better Availability Zone isolation for the EBS control
plane.

**Making it Easier to Take Advantage of Multiple Availability Zones**

We also intend to make it easier for customers to take advantage of
multiple Availability Zones. First, we will offer multiple Availability
Zones for all of our services, including Amazon Virtual Private Cloud
("VPC"). Today, VPC customers only have access to a single Availably
Zone. We will be adjusting our roadmap to give VPC customers access to
multiple Availability Zones as soon as possible. This will allow VPC
customers to build highly-available applications using multiple
Availability Zones just as EC2 customers not using a VPC do today.

A related finding from this event is we need to do a better job of
making highly-reliable multi-AZ deployments easy to design and operate.
Some customers' applications (or critical components of the application
like the database) are deployed in only a single Availability Zone,
while others have instances spread across Availability Zones but still
have critical, single points of failure in a single Availability Zone.
In cases like these, operational issues can negatively impact
application availability when a robust multi-Availability Zone
deployment would allow the application to continue without impact. We
will look to provide customers with better tools to create multi-AZ
applications that can support the loss of an entire Availability Zone
without impacting application availability. We know we need to help
customers design their application logic using common design patterns.
In this event, some customers were seriously impacted, and yet others
had resources that were impacted but saw nearly no impact on their
applications.

In order to work more closely with our customers and partners on best
practices for architecting in [the
cloud](/what-is-cloud-computing/){adhocenable="false"}, we will be
hosting a series of free webinars starting Monday, May 2. The first
topics we will cover will be Designing Fault-tolerant Applications,
Architecting for the Cloud, and Web Hosting Best Practices. We
anticipate adding more topics to the series over the next few weeks, and
will continue to do these on a frequent ongoing basis. The webinars over
the next two weeks will be hosted several times daily to support our
customers around the world in multiple time zones. We will set aside a
significant portion of the webinars for detailed Q&A. Follow-up
discussions for customers or partners will also be arranged. These
webinars, as well as a series of whitepapers on best practices for
architecting for the AWS cloud, are available in a new Architecture
Center on the AWS website. We'll also continue to deliver additional
services like S3, SimpleDB and multi-AZ RDS that perform multi-AZ level
balancing automatically so customers can benefit from multiple
Availability Zones without doing any of the heavy-lifting in their
applications.

**Speeding Up Recovery**

We will also invest in increasing our visibility, control, and
automation to recover volumes in an EBS cluster. We have a number of
operational tools for managing an EBS cluster, but the fine-grained
control and throttling the team used to recover the cluster will be
built directly into the EBS nodes. We will also automate the recovery
models that we used for the various types of volume recovery that we had
to do. This would have saved us significant time in the recovery
process. We will also look at what changes we can make to preserve
volume functionality during periods of degraded cluster operation,
including adding the ability to take a snapshot of a "stuck" volume. If
customers had this ability, they would have been able to more easily
recover their applications in other Availability Zones in the Region.

**Improving Communication and Service Health Tools During Operational
Issues**

In addition to the technical insights and improvements that will result
from this event, we also identified improvements that need to be made in
our customer communications. We would like our communications to be more
frequent and contain more information. We understand that during an
outage, customers want to know as many details as possible about what's
going on, how long it will take to fix, and what we are doing so that it
doesn't happen again. Most of the AWS team, including the entire senior
leadership team, was directly involved in helping to coordinate,
troubleshoot and resolve the event. Initially, our primary focus was on
thinking through how to solve the operational problems for customers
rather than on identifying root causes. We felt that that focusing our
efforts on a solution and not the problem was the right thing to do for
our customers, and that it helped us to return the services and our
customers back to health more quickly. We updated customers when we had
new information that we felt confident was accurate and refrained from
speculating, knowing that once we had returned the services back to
health that we would quickly transition to the data collection and
analysis stage that would drive this post mortem.

That said, we think we can improve in this area. We switched to more
regular updates part of the way through this event and plan to continue
with similar frequency of updates in the future. In addition, we are
already working on how we can staff our developer support team more
expansively in an event such as this, and organize to provide early and
meaningful information, while still avoiding speculation.

We also can do a better job of making it easier for customers to tell if
their resources have been impacted, and we are developing tools to allow
you to see via the APIs if your instances are impaired.

**Service Credit for Affected Customers**

For customers with an attached EBS volume or a running RDS database
instance in the affected Availability Zone in the US East Region at the
time of the disruption, regardless of whether their resources and
application were impacted or not, we are going to provide a 10 day
credit equal to 100% of their usage of EBS Volumes, EC2 Instances and
RDS database instances that were running in the affected Availability
Zone. These customers will not have to do anything in order to receive
this credit, as it will be automatically applied to their next AWS bill.
Customers can see whether they qualify for the service credit by logging
into their AWS Account Activity page.

**In Conclusion**

Last, but certainly not least, we want to apologize. We know how
critical our services are to our customers' businesses and we will do
everything we can to learn from this event and use it to drive
improvement across our services. As with any significant operational
issue, we will spend many hours over the coming days and weeks improving
our understanding of the details of the various parts of this event and
determining how to make changes to improve our services and processes.

Sincerely,\
The AWS Team\


