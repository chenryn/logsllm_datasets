Choosing a Business Continuity Solution
Part 1 – Availability Fundamentals
July2011
Business continuity encompasses those activities that an enterprise performs to maintain
consistency and recoverability of its operations and services. The availability of application
services provided by an enterprise’s IT infrastructure is only one of many facets of business
continuity, albeit an extremely important one. Application availability depends upon the ability of
IT services to survive any fault, whether it is a server failure, a network fault, or a data-center
disaster. An enabling technology for achieving high availability and even continuous availability
for application services is data replication. Selecting the right data-replication technology to
achieveyourbusinesscontinuitygoalsisthefocusofthisfour-partseries.1
Availability doesn’t come for free. Every organization has a variety of IT applications, the
importance of which ranges from noncritical to mission-critical. Some applications can be down
forhoursorevenfordayswithlittleimpactonoperations.Downtimeforotherapplicationsmaybe
quite costly but survivable. A handful of applications cannot be down at all without loss of critical
services,lossoflife,orwithoutotherwisewreakingmajorhavoconoperations.
Fortunately, a range of data-processing architectures exists for meeting every application-
availability need. However, a common characteristic of such architectures is that the more
availability they provide, the more costly they become and the more complex they are to
implementandtomanage.
This series of articles provides management with an understanding of data-replication
technologies. Management can then make informed decisions concerning the availability
approachtobeappliedtoeachapplicationformaximumreturnonthecompany’sinvestment.
Inthisfirstpart,wereviewmanyofthefundamentalconceptsthathelpusdefineavailability.
Availability Fundamentals
Congratulations! If you are reading this article, you have taken an important step toward solving
your availabilityneeds.Webegin with thefundamental requirements tobemetbyanysystem for
whichavailabilityisanissue.
1ThisseriesofarticlesisareprintofaGravic,Inc.,whitepaperandispublishedwiththepermissionofGravic.Seethe
Gravicwebsiteforotherwhitepapersatwww.gravic.com/shadowbase/whitepapers.
1
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Redundancy
Single points of failure should be eliminated. This means that everycomponent that is necessary
to support an application should be backed up by an equivalent (not necessarily identical)
component. Redundancy applies to processors, data storage, networks, sites, power, cooling,
andothers.
In some cases, a company may elect not to back up certain components that are considered
highly unlikely to fail.2 In such cases, the company is willing to take the consequences of the
(presumably)infrequentfailureofthosecomponents.
Isolation
Redundant components should be isolated so that the failure of one does not affect its backup.
For instance, depending upon two power feeds into a data center from the same power grid
violates isolation since if one fails due to a grid fault, the other fails. Likewise, dual networks from
thesamecommunicationcarrierviolateisolation.
Another violation of isolationis inherent inthose architectures thatrequire a backupsystem tobe
identical to the primary system, even down to the hardware, thesoftware versions, and the
database schema. One problem associated with the requirement for identical facilities is that a
buginonesystemmaysimplyreoccurintheothersystemfollowingafailover.Anotheristhatitis
often difficult for system administrators to know if changes made in one system do, in fact, make
it to the other system.These problems often occur in active/passive system configurations and in
clusters.
Truly isolated redundancy implies that the backup facilitydoes not have to be identical to the
primary facility. What is necessary to satisfy the isolation criterionis that a problem or a
configurationchangeaffectingonesystemdoesnotimpacttheothersystem.
Dispersion
Redundant components must not be collocated. They must be geographically dispersed so that
somesingleeventcannotdisablebothoftheredundantcomponents.
An important single point of failure is the data center. Reports occur regularly of entire data
centers going down for hours or days. Disasters such as fires,floods, tornados, and earthquakes
as well as more subtle events such as employee malfeasance, hackers, and even law-
enforcementconfiscationofdata-centerequipmentcancausedata-centeroutages.
The only solution to this problem is to have two or more data centers located sufficientlyfar from
each other so that a disaster at one site does not affect other sites. Fault-tolerant architectures
suchasasingleHPNonStopsystem,clusters,and virtualmachinesarehighlyredundantbutare
inherently not dispersible. Though they form a solid basis for a highly-available or continuously-
available data center, true high availability means that they must be replicated in a distant data
center.
The distance by which data centers must be separated depends upon the threats. In Europe, for
instance, where there is no expectation of earthquakes or hurricanes, data-center separations of
tens or hundreds of kilometers are often considered adequate to protect against local disasters
2“Highlyunlikelytofail”doesnotmeanfailure-free.InSeptember,2006,ValerieWilsonwonthemillion-dollarNewYork
StateLotteryforthesecondtime.Herchancesofdoingthiswereonein3.6trillion.Thatismorethantwelve9s.Butit
happened.Bewareoflettingahighnumberof9slullyouintoafalsesenseofsecurity.Thequestionisnot,“Canitfail?”
Thequestionis“Whenwillitfail?”Moreimportantly,whatareyouwillingtodotoprepareforsuchanevent?Afterall,
thathighlyunlikelyeventmayjusthappentomorrow.
2
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

such as fires, floods, and power outages. In other, more disaster-prone areas, hundreds or
thousandsofmilesarefrequentlyrequired.
Additionally, local regulations may play a role in dictating distances. For instance, European
countries generally dictate their own distance requirements within the country for critical
applications. In the United States, federal regulations call for a minimum distance measured in
thehundredsofmilesforcriticalfinancialapplications.
Failover
A redundant component is useless if it cannot readily assume the duties of its failed counterpart.
Thisrequiresmultiplecapabilitiestobepartofthearchitecture:
 Faultdetectiontorapidlyidentifyfaults.
 Failover decision to determine whether it is better to tryto restorethefailedcomponent
(forinstance,torebootaprocessor)ortofailovertoitsbackup.
 Failoveractiontoperformthefailoverifthisisthedesiredaction.
 Verification to validate that the backup component is providing the required service to
thesystem.
To the extent that these capabilities are automated, failover will be faster. It also typically will be
more reliable and less error-prone. However, in many cases, one or more capabilities are
deemed too complex for automation and are left to the system operators and their management.
Forthesecases,establishedprocedures,thoroughdocumentation,andperiodicoperatorpractice
areimportant.
Testing
Perhaps the availabilityrequirementthat is mostviolatedis periodic testing of the failover plan.In
manyarchitectures,failover testingrequires stoppingprocessingandfailingover tothebackup,a
course of action that might take down user access to the application for hours. Added to this is
the possibility that the failover will fail, further increasing the outage. For this reason, many
companiesdon’ttesttheirfailoverplansoronlydosorarelyorinpart.Whenarealfailureoccurs,
andtheymustfailover,alltheycandoistohopeforthebest.
Aswillbeseen,somearchitectureslendthemselvestocontinuousbackup-system testingwithout
taking the system offline so that the operations staff always knows that the backup system is
workingproperly.
The Components of Availability
Many factors influence the availability of data-processing systems. Factors range from planned
outages to unplanned outages. In this white paper, we will discuss the use of data-replication
techniquesforreducingoreliminatingbothplannedandunplannedoutages.
PlannedOutages
A processing system or a node in a redundant processing system may have to be taken down
periodically for planned maintenance. If the system is not redundant, the users are down during
this planned maintenance interval. However, if the system is redundant, another node in the
systemcancontinuetoprovideprocessingservices.3
3UsingShadowbasetoEliminatePlannedDowntimeviaZeroDowntimeMigrations,AGravicWhitePaper.
3
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Often, for nonredundant systems, there is a maintenance window during which the system is idle
and during which such upgrades can be performed. Perhaps the window is at night or over the
weekend. But will the maintenance activities be completed on time? Will the system come up
properlyattheendofthemaintenancewindow?
Planned outages are often necessary for hardware upgrades. Also, many software upgrades
require that the system be taken down. They include upgrades to the operating system,
applications, and database-management systems. Certain database operations such as
rebalancing indices, modifying the database schema, or making consistent tape backups may
alsohavetobeperformedonaquiescentsystem.
Typically,adata-centersitemovenecessitatesthatallsystemsbetakendown.
UnplannedOutages
Anunplannedoutagerequireseitheranimmediaterecoveryofthefailedcomponentorfailoverto
a redundant component, if there is one. Any number of faults can cause an unplanned outage,
including hardware failures, software bugs, operator errors, environmental faults such as power
or air conditioning, employee malfeasance, hackers, denial-of-service attacks, and even data-
centerdestructionduetoadisasterofsomesort.
There are two major availability concerns when an unplanned outage occurs – the availability of
the processing infrastructure and the availability of the application data. Having all of the servers
upandrunningdoes nogoodunlesstheyhavevaliddatauponwhichtooperate. Welook nextat
processingavailabilityanddataavailability.
Application-Services Availability and the Recovery Time Objective (RTO)
The availability of the processing infrastructure – servers, networks, power, etc. – varies widely
depending upon the redundant architecture used. We will explore these architectures later. First,
letuslookathowwecharacterizetheavailabilityofprocessingservices.
Uptime
The typical way to measure availability is to predict (or measure) the percentage of time that the
system is up– its uptime. Uptimeis oftenreported in “nines”(9s).4For instance, if asystem is up
99.9% of the time, its uptime (thus, its availability) is quoted as “three 9s.” The relationship
between9sanddowntimeisshowninTable1.
Uptime Downtime/year
one9 876hours
two9s 88hours
three9s 9hours
four9s 50minutes
