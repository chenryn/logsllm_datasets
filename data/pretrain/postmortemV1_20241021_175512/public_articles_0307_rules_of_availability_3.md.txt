Rules of Availability – Part 3
July2008
We conclude in this article a review of our “Rules of Availability,” as published in our series of
books entitled Breaking the Availability Barrier.1We have chosen those rules that are particularly
applicable as best practices to achieve continuous availability with redundant systems and with a
focusonactive/activesystems.
The Importance of Recovery Time
There has been a paradigm shift in the approach to continuous availability. Almost four decades
of development have resulted in fault-tolerant systems that today are so reliable that they hardly
ever fail due to hardware or operating system faults. Any further efforts to improve these
reliabilities are overshadowed byother faults such as application errors and operator errors. As a
result, we have seen a shift from trying to prevent failures through fault-tolerant system design to
accepting that failures will occur and to being able to recover from these failures so quickly that
noonenotices. Inotherwords,Letitfail,butfixitfast.
Rule 41: Active/active systems can provide the availability of a primary/standby pair with less
equipmentandlesscost.
When you back up your data-processing system with a passive/standby system, you must
purchase 200% capacity – 100% for your primary system and an equivalent 100% for your
backup system. But half of this capacity goes unused. However, the probability of a total system
outageisextremelysmallsinceitrequiresthefailureofbothnodes.
In contrast, an active/active system can be configured with multiple smaller nodes. For
instance,consider afive-nodeactive/activesystem witheachnodecarrying 25% of theload. You
onlyhave to purchase 125% of capacityrather than200%. Itstill takes atwo-nodefailure to drop
below100% capacity, but inthis case 75% of capacityis still available(rather thanno capacityin
the primary/standby case). Furthermore, the time required to return service to the affected users
istypicallymeasuredinsecondsratherthaninhours.
Rule 42: In a system with s spares, reducing subsystem mtr by a factor of k will reduce system
MTR by a factor of k and will increase system MTBF by a factor of ks, thus increasing system
reliabilitybyafactorofks+1.
This rule states the importance of being able to repair a node in a redundant system quickly. For
instance, consider a dual-node system with one spare (the normal case). Assume that a dual-
1BreakingtheAvailabilityBarrier:SurvivableSystemsforEnterpriseComputing,AuthorHouse;2004.
BreakingtheAvailabilityBarrierII:AchievingCenturyUptimeswithActive/ActiveSystems,AuthorHouse;2007.
BreakingtheAvailabilityBarrierIII:Active/ActiveSystemsinPractice,AuthorHouse;2007.
Theserulesmayalsobefoundat http://www.gravic.com/breaking_the_availability_barrier_rules.html.
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

node failure will take down the system once every five years (the system MTBF). Furthermore,
assumethatanodehasa meantimetorepair(nodalmtr)offourhours.Shouldbothnodesfail,it
will take an average of four hours to return the system to service (assuming exponential repair
times and only one repair person who works only on one node at a time). This is the system
MTR.
If the nodal mtr can be cut in half to two hours, the system MTR will be cut in half to two hours.
Because the window during which the second node can fail (the time that one node is down) is
now halved, the system MTBF will be doubled to ten years. The probability of a failure of the
system is MTR/MTBF.Therefore,theprobabilityof afailurehas beenreducedbyafactor offour.
Thereliabilityofthenewsystemhasbeenincreasedbyafactoroffourbycuttingthenodalrepair
timeinhalf.
Rule43:Ifs+1subsystems failandarebeingrepairedsimultaneously,andifthereturntoservice
ofanyoneofthesesubsystemswillreturnthesystemtoservice,thesystemMTR ismtr/(s+1).
In a system with s spares, it takes thefailure of s+1 nodes to cause the system to fail. The repair
of anyone of thesenodes willreturnthesystem toservice.If thereare s+1repair people,onefor
eachsparenode,thenontheaveragethere willbe s+1repairsmadeduringthetimethatittakes
to repair one node. Assuming that repair times are random with each nodal repair taking an
average time of mtr, then the rate of repairs is (s+1)/mtr; and the average system repair time,
MTR(whichequalsthetimetorepairthefirstnode),ismtr/(s+1).
For instance, if there is one spare in the system, the system MTR will be cut in half if two repair
peopleareavailableratherthanjustone.
Rule45: Ifyoubreak amonolithic systemintok smaller nodes with nospares,thesystemwillbe
morereliablethantheoriginalmonolithicsystemprovidedthateachnodeis morethanktimes as
reliableasthemonolithicsystem.
In many systems such as multiprocessor systems, the smaller the system, the more reliable it is
since there are fewer ways for it to fail (less parts, hence more reliability). For instance, consider
alargesingle-sparedsystem with8subsystems.Inthis case,thereare 8x7/2 =28 ways in which
twonodescanfail(thenumberoffailuremodes)andtakedownthesystem.
Now let us split this system into two four-node systems. Each node only has six failure modes
(therearesix ways that twonodes canfailin afour-nodesystem).However,thereare two nodes
inthesystem;andthefailureof anynode is assumedtotakedownthesystem.Thus,therearea
total of 2x6=12 failure modes in the split system. The dual-node system is more than twice as
reliableasthemonolithicsystem.
Rule 46: Don’t underestimate your failover time. It may well be the most important factor in
perceivedavailability.
If the nodes in your distributed system are so reliable that a dual-node failure is highly unlikely,
then user downtime reduces to the time that it takes to recover services for the users who have
been affected by an outage. This is the failover time, and it becomes the predominant factor in
the reliabilityequation. In the limit, if the failover time is so short that the user has not noticed the
outage,thenineffecttherehasbeennooutage.
Rule 47: A small probability of a failover fault may cause a disproportionate decrease in system
availability. Moving from active/backup to active/hot-standby or active/active with frequent testing
cansignificantlyreducetheprobabilityoffailoverfaults.
Rule 46 assumes that the failover will work. However, there are many reasons that a failover will
fail. We call this a failover fault. Most failover faults are related to inadequate testing of the
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

failover procedures. In the example given in our book, a 1% chance of a failover fault (that is, a
failover works 99 times out of a 100 – wouldn’t that be nice) results in a 500% decrease in
availability!
Active/active systems substantially eliminate the chance of a failover fault because it is known
that the backup system is operational. After all, it is actively processing transactions at the time
thatanothernodefails.
Rule48:Pickyournodelocationsinanactive/activesystemcarefullytominimizethechancethat
environmentalhazardswilloutweightheavailabilityofthenodes.
What is the advantage of using highly reliable systems in a data center if there is a significant
probabilitythat the data center maybe taken down by outside events? Tryto locate data centers
outsideofhurricaneandtornadozones,floodplains,earthquakefaultlines,andsoon.
Rule 49: Replication engines that violate referential integrity on the target database are rarely, if
ever,suitableforactive/activeimplementations.
If a data-replication engine does not ensure referential integrityof the target database, there may
beasignificanttimerequiredto bringthetarget databaseinto astate of fullconsistencyfollowing
afailureofthenodethatitisbackingup.Thiscanleadtoverylongfailovertimes.
Active/active systems, of course, require a data-replication engine that guarantees referential
integrity and transactional consistency at the target database since the target database is being
activelyused.
Rule50: (Corollaryto Rule 49) - Hardwarereplication is notsuitablefor active/activeapplications
becauseitdoesnotprovidereferentialintegrity.
Database systems in which physical block replication (such as replicating cache block changes)
are replicated by the hardware storage controllers do not provide referential integrity since there
is no sequential control over the replicated blocks. Indices may be replicated that have no
associateddatarows,childrenmayhavenoparents,andsoforth.
Rule 53: ZDM (zero downtime migration) eliminates planned application downtime, therefore
improvingapplicationavailability.
A real killer of availability is planned system downtime for upgrades, migrations, backups, batch
runs,and soforth.This overrides failover as aconsideration since users canbe outof servicefor
hours. Active/active systems eliminate planned downtime because nodes can be taken out of
service one at a time, upgraded or used in whatever way is necessary, and then returned to
service.Whilethenodeisoutofservice,usersareroutedtootheroperatingnodes.
Total Cost of Ownership
Rule 54: (Standish Law) - In order to calculate a meaningful predicted Total Cost of Ownership,
onemustfirstproperlysizethesystem.
Although this seems obvious, a major obstacle to proper system sizing is often the system
vendor.Each vendor willmaximizethecapabilityof his offerings (oftencalled puffery),whichmay
lead to an undersizing of the system. In addition, reliability claims may be excessive and thus
resultinunrealisticallylowestimatesofthecostofdowntime.
It is important to fully understand the performance and availability parameters of any system that
youareconsideringsothatinitialandoperatingcostestimatescanbeaccurate.
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

Rule 56: To minimize insurance costs, reduce threat by increasing mtbee (the mean time
between endorsed events), reduce vulnerability by using appropriate countermeasures, and
minimize the value of an event by minimizing MTR. An active/active system is an effective
countermeasurethatreducesbothvulnerabilityandeventvalue.
The use of active/active systems often can significantlyreduce business insurance costs. This is
because the extremeavailabilityof an active/activesystem ensures againstbusiness disruptions.
Extending an active/active system so that it is geographically dispersed further protects against
businessinterruptionscausedbysiteandareadisasters.
Rule 57: (Corollary to Rule 39) - The use of fault-tolerant nodes in an active/active system can
reduce TCO by reducing the number of nodes required, the facilities’ costs, the licensing costs,
andthepersonnelcosts.
Rule 39 states that the use of active/active systems will generally require fewer nodes if fault-
tolerant nodes are used rather than industry standard high-availability nodes to achieve a
specified system availability. In most cases, at least one less node of the same capacity will be
requiredwhenfault-tolerantnodesareusedduetoreducedsparingrequirements.
Performance
Rule 58: (Latency Rule) - Replication engine latency is largely governed by disk-queuing and
communicationdelays,notbyprocessingtimes.
When analyzing the performance of data-replication engines, it is found that processing time is
negligible compared to other delays. Predominant among these other delays are the access time
andpollingdelaysassociatedwithdiskstorageandcommunicationdelays.
Disk-storage delays can be minimized by reducing the number of disk-queuing points in the
replication engine. Communication delays (aside from channel latency) are primarily caused by
the time to buffer large blocks of data for efficient channel utilization. In this case, replication
latencycanbetradedforefficientchannelutilization.
Rule 61: Active/active systems made up of fault-tolerant nodes will benefit from lights-out
operations. Active/active systems using high-availability (but not fault-tolerant) nodes may have,
infact,pooreravailabilityinalights-outoperationthantheindividualnodes.
This is especiallytrueif therearemultiple lights-out nodes.Letus assumethattherepair timefor
a lights-out node is 20 hours rather than two hours for a staffed node. In effect, a lights-out node
willhaveoneless9ofavailabilitythanastaffednode.
Consider a five-node active/active system in which four nodes are lights-out nodes monitored by
one staffed node. The system will survive the loss of any two nodes. If four-9s, fault-tolerant
servers are used, the resulting availability is a little less than six 9s. However, if three-9s, high-
availability servers are used, the net availability is a little over three 9s – about the same as if a
singlenodewereused.
Rule 62: (Paul’s Law) - If synchronous replication is used, the Applier does not have to enforce
the order of commits so long as the source referential integrity is at least as strong as that at the
target.
As opposed to asynchronous replication, a synchronous replication engine guarantees that the
order of transactions applied to the target database is the same as that at the source database.
Therefore, if the source database is ensuring referential integrity, the target database will ensure
thesamelevelofreferentialintegrity.
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

The one case where this might be a problem is if the target database enforces a higher level of
referentialintegritythanthesourcedatabase.Inthiscase,referentialintegrityviolationsthatwere
notenforcedbythesourcedatabasecouldoccuratthetargetdatabase.
Rule 63: The difference in application latency between coordinated commits and dual writes is
mostapparentwhenfastresponsetimesarerequired.
The difference in application latency between dual writes (synchronizing at the write level) and
coordinated commits (synchronizing at the transaction level) is measured in the tens or hundreds
of milliseconds depending upon the distance between the two nodes. Dual writes are faster for
short distances and short transactions, and coordinated commits are faster for long distances
and/orlongtransactions.
If users are expecting subsecond response times, the choice of the synchronization technique
may be important. However, if users are expecting response times measured in seconds (for
instance, for web applications), the difference between these techniques so far as the user
experienceisconcernedmaynotbesignificant.
Application Active/Active-Ready
Rule 64: (Werner’s Law) - When trying to distribute an application over multiple nodes, identify
each global resource and local context the application uses; and carefully consider the
consequences of this in a multinode approach. Identify each application decision that is made on
system or database state information (as opposed to database contents information), and make
surethisiscompatiblewithyourmultinodeapproach.
Although in principle an application may be made active/active by running it in multiple nodes
whose databases are synchronized by data replication, not all applications may be active/active-
ready. For instance, if sequential or random numbers are generated, multiple nodes may
generate identical numbers. Important memory-based context may not be replicated. Duplicate
mini-batchesmayberunateachofthenodes.Distributeddeadlocksmayoccur.
Therefore, an application must be thoroughly analyzed; and any required modifications must be
madebeforeattemptingtorunitinanactive/activemode.
In Memoriam
The database pioneer Jim Gray, lost at sea on January 28, 2007, contributed greatly to the
technologythattodayallowsactive/activesystemstobebuilt.Afterreviewingthefirstbook inthis
series,hecommented,“Ilovedthelaws!”Wehopethatyouhaveenjoyedthem andhavelearned
fromthemaswell.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman
