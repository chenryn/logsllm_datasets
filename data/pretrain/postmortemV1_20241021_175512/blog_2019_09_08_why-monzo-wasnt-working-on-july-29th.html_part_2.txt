Cassandra directly. We confirm that the key used by the internal edge is
in fact missing from Cassandra.

**14:02** At this point we believe we\'re facing an incident where
*some* of our data isn\'t available, so we turn our focus to Cassandra.

**14:04** Despite our earlier fix for Mastercard, we can see it\'s still
not fully healthy and the payments team keeps working on it. This means
that a **small number of card payments are still failing**.

**14:08** We query whether the new Cassandra servers have taken
ownership of some parts of the data. We don\'t think this is possible
given our understanding of what\'s happened so far, but we keep
investigating.

**14:13** We issue a query for some data in Cassandra and confirm the
response is coming from one of the new servers. At this point, we\'ve
confirmed that something we thought was impossible, had in fact
happened.

**The new servers had joined the cluster, assumed responsibility for
some parts of the data (certain partition keys to balance the load), but
hadn\'t yet streamed it over. This explains why some data appeared to be
missing.**

![A diagram showing the new servers in the cluster, missing the
data](https://images.ctfassets.net/ro61k101ee59/wcWFwS4HBG1DC8ANWIZUM/6e94d3ec6b43cc82932d67551b477197/Untitled__1_.png?w=1280&q=90)

**14:18** We begin decommissioning the new servers one-by-one, to allow
us to return data ownership safely into the original cluster servers.
Each node takes approximately 8-10 minutes to remove safely.

**14:28** We remove the first node fully, and we notice an immediate
reduction in the number of 404s being raised.

Our internal customer support tooling starts working again, so **we can
respond to customers using in-app chat**.

**15:08** We removed the final Cassandra node, and the immediate impact
is over. **For the majority of customers, Monzo starts working again as
normal.**

**15:08 â†’ 23:00** We keep working through all the data that had been
written while the six new servers were actively serving reads and
writes. We do this through a combination of replaying events which we
store externally, and running internal reconciliation processes which
check for data consistency.

**23:00** We confirm that **all customers are now able to access their
money, use Monzo as normal, and contact customer support if they need
to.**

## We misunderstood the behaviour of a setting

The issue happened because we expected the new servers to join the
cluster, and stay inactive until we carried out a further action. But in
fact, when we added new ones they immediately took part in the reading
and writing of data, despite not actually holding any data. The source
of the misunderstanding was a single setting (or \'flag\') that controls
how a new server behaves.

In Cassandra, there\'s a flag called `auto_bootstrap` which configures
how servers start up and join an existing cluster. The flag controls
whether data is automatically streamed from the existing Cassandra
servers onto a new server which has joined the cluster. Crucially, it
also controls the querying pattern to continue serving read requests to
the older servers until the newer servers have streamed all the data.

![Diagram showing what we expected to happen, with data coming from the
old, active
nodes](https://images.ctfassets.net/ro61k101ee59/2wzoTzjfnpfhpGsBAMZLyj/95e80f23d856cce44b21e3e4e2a6721c/Untitled__2_.png?w=1280&q=90)\
What we expected: new nodes join the cluster in an inactive mode, and
get assigned a portion of the data. They remain in this state until we
actively stream data into them, one-by-one.

In the majority of cases, it\'s recommended to leave the default of
`auto_bootstrap` to `true`. With the flag in this state, servers join
the cluster in an \'inactive\' state, have a portion of the data
assigned to them, and remain inactive in data reading until the data
streaming process finishes.

But when our last scale up in October 2018 was complete, we\'d set the
`auto_bootstrap` flag to `false` for our production environment. We did
this so that if we lost a server in our cluster (for example, due to
hardware failure) and had to replace it, we\'d restore the data from
backups (which would be significantly faster and put less pressure on
the rest of the cluster) rather than rebuild it from scratch using the
other servers which had the data.

![Diagram showing us restoring data from a
backup](https://images.ctfassets.net/ro61k101ee59/3JyHW5jzf8UdJOKlc5bgC1/8777b9b4586466b17a848734c89ed239/_inc-platform-issues_-_Google_Slides__2_.png?w=1280&q=90)

During the scale-up activity, we had no intention to stream the data
from backups. With `auto_bootstrap` set to false, we expected the six
new servers would be added to cluster, agree on the partitions of data
they were responsible for, and remain inactive until we initiated the
rebuild/streaming process on each server, one-by-one.

But this wasn\'t the case. It turns out that in addition to the data
streaming behaviour, the flag also controls whether new servers join in
an active or inactive state. Once the new servers had agreed on the
partitions of data they were responsible for, they assumed full
responsibility without having any of the underlying data, and began
serving queries.

Because some of the data was being served from the new servers which
didn\'t have any data yet, that data appeared to be missing.

So when some customers opened the app, for example, transactions that
should have existed couldn\'t be found, which caused their account
balances to appear incorrectly.

Once the issue had been resolved, we were able to fully recover the data
and correct any issues.

## To stop this happening again, we\'re making some changes

There are a few things we can learn from this issue, and fix to makes
sure it doesn\'t happen again.

### We\'ve identified gaps in our knowledge of operating Cassandra

While we routinely perform operations like rolling restarts and server
upgrades on Cassandra, some actions (like adding more servers) are less
common.

**We\'d tested the scale-up on our test environment, but not to the same
extent as production.**

Instead of adding six servers, we tested with one.

To gain confidence in the production rollout, we brought a new server
online and left it in the initial \'no data\' state for several hours.
We did this across two clusters.

We were able to confirm that there was no impact on the rest of the
cluster or any of the users of the environment. And at this point we
were happy that the initial joining behaviour of `auto_bootstrap` was
benign as we expected. So we continued to stream the data to the new
server, monitored throughout, and confirmed there were no issues with
data consistency or availability.

What we failed to account for was quorum (the three servers agreeing on
a value). With only one new server, it wouldn\'t matter if it joined the
cluster fully and didn\'t hold any data. In this case, we\'d have
agreement from the other two servers in the cluster.

But when we added six servers to production, the data ownership had two
or three members reallocated to the new nodes, meaning we didn\'t have
the same guarantee because the underlying data didn\'t reallocate.

### We\'ve already fixed the incorrect setting

We\'ve already fixed our use of `auto_bootstrap`. And we\'ve also
reviewed and documented all our decisions around the other Cassandra
settings. This means we have more extensive runbooks -- the operational
guides we use to provide step-by-step plans on performing operations
such as a scale up or a restart of Cassandra. This\'ll help fill the
gaps in our knowledge, and make sure that knowledge is spread to all our
engineers.

Another key issue that delayed our actions was the lack of metrics
showing Cassandra as a primary cause of the issue. So we\'re also
looking at exposing more metrics and adding potential alerting for
strong shifts in metrics like \'row not found\'.

### We\'ll split our single Cassandra cluster into smaller ones to reduce the impact one change can have

For a long time, we\'ve run a single Cassandra cluster for all our
services. Each service has its own dedicated area (known as a keyspace)
within the cluster. But the data itself is spread across a shared set of
underlying servers.

The single cluster approach has been advantageous for engineers building
services -- they don\'t have to worry about which cluster to put a
service on, and we only have to operationally manage and monitor one
thing. But a downside of this design is that a single change can have a
far-reaching impact, like we saw with this issue. We'd like to reduce
the likelihood that any single activity can impact more than one area of
Monzo.

With one cluster, it\'s also much harder to pinpoint the source of
failure. In this instance we took almost an hour from the first alert to
the point where we pulled the information into a single coherent picture
that highlighted Cassandra at fault. With smaller and more constrained
system configurations, we believe this would have been a much less
complex issue to deal with.

In the long term, we plan to split up our single large cluster into
multiple smaller ones. This will drastically reduce the likelihood and
impact of repeat issues like this one, and make it safer for us to
operate at scale. We want to make sure we get it right though; doing it
in such a way that doesn\'t introduce too much operational complexity,
or slow our engineers down at releasing new features to customers.

![Diagram showing one large cluster being split into six separate
ones](https://images.ctfassets.net/ro61k101ee59/6mQyqjtoPHmpYw2ICpxfoF/c01ddf9d9874ebdba42e6232332c2682/Untitled__3_.png?w=1280&q=90)

------------------------------------------------------------------------

We're really sorry this happened, and we're committed to fixing it for
the future. Let us know if you found this debrief useful, and share any
other questions or feedback with us too.

