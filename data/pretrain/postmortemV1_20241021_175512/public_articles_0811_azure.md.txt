Windows Azure Downed by Single Point of Failure
November2013
Clouds are expected to be highly redundant and resilient to any single failure. There is
alwaysanothercomponentthatcantakeoverintheeventofafailure.Right?
Wrong! As it turns out, this is not always true. The Microsoft Azure cloud has a
single point of failure, and this component failed at the end of October, 2013. The
failure caused a worldwide partial compute outage. While the glitch did not prevent
cloud applications from running, it took down certain cloud management functions
foradayandahalf.Specifically,newapplicationscouldnotbeputintoservice.
The Failure
The Azure cloud is distributed over eight worldwide regions – East, North Central, South Central, and
West U.S.; North andWest Europe; and East and Southeast Asia. Each of these regions is fault-isolated
fromtheotherregionssothataproblemthatimpactsoneregionpresumablywillnotaffectotherregions.
However, at 2:35 AM UTC on Wednesday, October 30, 2013, Azure users were greeted with the
followingominousmessage:
“We are experiencing an issue with Compute in North Central US, South Central US, North Europe,
Southeast Asia, West Europe, East Asia, East US and West US. We are actively investigating this
issue and assessing its impact to our customers. Further updates will be published to keep you
apprisedoftheimpact.Weapologizeforanyinconveniencethiscausesourcustomers.”
At 10:30 AM later that day, the Microsoft team reported that the problem had been addressed and that
the company was in the process of correcting the problem. The fault lay in a management service called
Swap Deployment. Swap Deployment allows developers to move cloud applications under development
to production by swapping a virtual IP address. Microsoft noted that Swap Deployment operations could
cause errors and suggested that service management functions be delayed until the problem was fixed.
Though the problem did not affect anyapplications currently in production, new applications could not be
deployed.
The frustration being felt by Azure developers is reflected in the following post to Microsoft’s TechNet
website:
“IamgettingthefollowingmessagewhenItrytodoanythingwithmycloudservice:
W“ indowsAzureiscurrentlyperforminganoperationwith
1
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

x-ms-requestid9eaaa9d657ad2e838b4c55dec6fdc4b9 on this deployment that requires exclusive
access.
“This happens whenI tryto delete theproduction deployment or delete the cloud service itself.When
I try to deploy a new staging deployment, it says my staging slot is filled, but it shows nothing is
deployedtostaging.
“Is this a common occurrence? The client I'm working with is getting nervous over the technical
issues.I'mtheonewhoconvincedhimtogowithAzureforhostingwhenwearegoingintoproduction
nextyearandI'dliketobeabletoassurehim thathisisjustatemporaryhiccupindeployment.Itisn't
affectingthetestsitesothathelpsbutnotbeingabletodeployanewversionisanissue.”
Microsoft’sanswertothispleawaslessthanhelpful:
“The Azure Management API (AMAPI) can be temperamental. There’s not much you can do about
this one other than let the operation internally timeout. I have seen hosted service deployments be
stuckinatransitioningstateforextendedperiodsoftime(4-8hours)althoughthisisuncommon.
“You should open a support incident with Microsoft. Even if it resolves itself faster than they get to
you,itisgoodtoletthemknowthatthisishappening.”
The user responded that he had opened up a support ticket but was horrified to find that this counted as
one of his two allowed tickets per year under his MSDN subscription, even though the fault was
Microsoft’s.
Finally, at 10:45 AM the next day, Thursday, October 31st, a day and a half later, Microsoft posted the
followingnotice:
““As of10:45 AM PST, the partial interruption affecting Windows Azure Compute has been resolved.
Running applications and compute functionality was unaffected throughout the interruption. Only the
Swap Deployment operations were impacted for a small number of customers. As a precaution, we
advisedcustomerstodelaySwapDeploymentoperationsuntiltheissuewasresolved. Allservicesto
impactedaccountshavebeenrestored.”
The Cause
The Azure cloud provides both a staging environment and a
production environment for an application. The staging environment
lets users test their systems before putting them into production. The
production environment provides all of the facilities needed by the
applicationtointeractproperlywiththeoutsideworld.
Moving an application from a staging environment to a production
environment is done by the management utility Swap Deployment.
Swap Deployment initiates a virtual IP address swap between the
stagingandproductionenvironmentsforapplications.
The culprit in the Swap Development utility was a module called Red
Dog Front End (RDFE). RDFE provides the publicly exposed
management portal and the service management API. User requests
are fed through RDFE to the fabric front end, which disperses
requests through aggregators and load balancers to Fabric
Controllers. The Fabric Controllers in each data center direct the
cloud’svirtualmachinesandotherresources.
Whenanapplication(aserviceinAzureterms) is tobedeployed,it is
2
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

passed to the Azure Portal Service. The Portal Service invokes RDFE to properly format the service, and
the service is then sent to the Fabric Controller in an appropriate data center based on where the user
has requested that his application run. Each data center comprises myriad servers organized into
independent fault domain clusters. Each cluster contains about 1,000 servers. The Fabric Controller will
instantiate versions of the application running as virtual machines in at least two clusters to maintain high
availability(theAzureSLAcallsforanavailabilityof99.95%).
Theproblem arose whenMicrosoftmadeanupdateto RDFE.Thechange was testedon asmallnumber
of nodes within a single cluster and worked perfectly. The Azure developers then pushed the change out
to all of the data centers worldwide, and that is when the problem exposed itself. Though existing
applications continued to work, the Swap Deployment management facility was broken; and new
applicationscouldnotbedeployedinanyofthedatacentersworldwide.
Due to the way that Azure is built, there can only be one RDFE in the entire cloud. Therefore, the RDFE
is a single point of failure. The developers could not run one instance of an RDFE and then deploy it to
otherinstancesonlyafterithadprovenitselfinproduction.
Summary
Thoughthisoutagedidnotaffectexistingproductionapplications,itcertainlywasirritatingtoheavyusers.
Imagine having a tight deadline to get an application into service, only to be blocked by an outage such
as this. Regardless of whom it affected, a worldwide outage may certainly damage confidence in
Microsoft’sabilitytomanagealargedistributednetwork.
It was only a little over a year and a half ago that the entire Azure cloud went down for over thirty hours,
compute capacity and all. This problem was due to a software bug in the way that Microsoft developers
calculatedLeapDay.1
These two outages lead to an interesting observation. No matter the system, there is one single point of
failure, and that is software. A software bug that is allowed to go into production can infect every system
inthecloud.
Acknowledgements
Materialforthisarticlewastakenfromthefollowingsources:
WindowsAzureComputecloudgoesTITSUPPLANET-WIDE,TheRegister;October30,2013.
Whoopsie:WindowsAzurestumblesagain,Gigaom;October31,2013.
Microsoft’s Windows Azure cloud hit by worldwide management interruption, PC World; October 31,
2013.
MicrosoftAzureSwapDeploymentFeatureRestoredAfterGlobalOutage,TheWhir;October31,2013.
Microsoft’sWindowsAzureHitWithGlobalComputePerformanceGlitch,CRN;October31,2013.
TheTRUTHbehindMicrosoft’sAzureglobalcloudmega-cock-up,TheRegister;November8,2013.
Inside Windows Azure: The Cloud Operating System, Presentation, 2011 Microsoft Tech-Ed –COS 301;
May16,2011.
1WindowsAzureCloudSuccumbstoLeapDay,AvailabilityDigest;March2012.
http://www.availabilitydigest.com/public_articles/0703/azure.pdf
3
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com