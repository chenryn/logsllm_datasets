Poor Documentation Snags Google
April2010
What should have been a ten-minute outage at a major Google data center hosting the Google
App Engine turned into a two-and-a-half hour ordeal simply because of faulty failover
documentation. Of course,the fact that the failover procedures were incorrectlydocumented also
impliesthattheywerenevertestedandthatthestaffwasnevertrained.
Kudos to Google for its transparency during and after the outage. It published a detailed post-
mortemstudyexplainingminute-by-minuteexactlywhathappened,whattheunderlyingcausesof
theoutagewere,andwhatitplanstodotoavoidthissituationinthefuture.
The incident is an excellent example of a failure chain. Many failures occur because of a
sequenceof events.If anyoneeventdoes nothappen,thefailurechainis broken;andthefailure
does not occur. In this case, the failure chain included a power failure, a backup power fault,
recent failover enhancements, faulty failover documentation of the new procedures, and the
unavailability of the knowledgeable technical people who could have untangled the
documentation.Ifanyoneoftheseeventshadnothappened,thismajoroutage wouldhavebeen
aminorinconvenience.
The Google App Engine
The Google App Engine is a compute cloud service for users to develop and host their web
applications in Google’s data centers. The App Engine virtualizes applications across not only
multiple servers but also across multiple data centers to ensure that no fault will take down the
applications. Shouldaserver failduetoequipmentproblems or evenduetoanentiredatacenter
outage, the applications on that server will be rapidly migrated to a surviving server, where they
willcontinuetorun.OrsoGooglethought.
The Data Center Power Failure
ThePowerGoesOut
On Wednesday, February 24, 2010, a Google data center hosting the Google App Engine
suffered a major power outage.1 The power was down for about thirty minutes, but Google’s
backuppowerkickedinandcontinuedtopowerthedatacenter.
The only problem was that for some reason (never disclosed), about 25% of the servers did not
receive backup power and subsequently went down. This caused their applications to fail over to
thesurvivingservers inthe datacenter,whichwerenotconfiguredtohandlethatmuchadditional
load;andtheserverfailurescascaded.
1WhenthePowerGoesOutatGoogle,DataCenterKnowledge;March8,2010.
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Failover
This unusualfailure pattern was never envisioned bytheGoogletechnicalstaff.Ifaserver failed,
its applications were migrated to surviving servers. It was never anticipated that a large group of
servers would fail and would overload the remaining servers, thus in essence taking down the
entiredatacenter.
Should a server fail, failover to a surviving server in the same data center is automatic and
transparent.However, if adatacenter fails,failover to abackupdatacenter is acomplex process
requiring careful analysis and manual intervention. The decision-making process in this situation
was made all the more complex by the failure mode that had never been thought through. The
result was that there was no established procedure to determine that the data center had failed
and when it might recover. The fact that the failure of a server group could take down an entire
datacenterwasarevelationtothetechnicalstaff.
Thisoversightwascompoundedbythefactthatrecentwork hadjustbeencompletedtoenhance
the data center failover procedures. Multihoming was introduced so that access to the Google
App Engine could be provided by a virtual IP address over multiple communication links serving
manyservers.Ifaserveroranetwork linkfailed,trafficcouldbeeasilyreroutedbythenetwork to
a surviving server in the same or different data center. This change was intended to significantly
simplifyfailoverprocedures.
Unfortunately, parts of the documentation of the new failover procedures incorrectly referred to
the old data center configuration rather than the upgraded configuration. Clearly, the newly
documented failover procedures had not been tested; nor had the staff been trained. Otherwise,
theseerrorswouldhavebeenfound.
TheFailoverFault
Thus, there was a great deal of confusion about how to handle the multiple server failures. The
first decision was to fail over the data center. The documented procedures were followed but led
toanunsuccessfulfailover.
It then looked like the primarydata center had returned to operation because of its reduced load,
so processing was returned to it. This turned out to be an erroneous observation since the
primary data center could still not handle the full load, and it once again failed. Finally,
knowledgeable technical personnel were reached; and the backup data center was brought
successfully online. Two-and-a-half hours had passed since the initial failure. If things had gone
properly,itwouldhavetakenonlyabouttenminutestofailovertothebackupdatacenter.
Post-Mortem
As it does with any outage, Google staff thoroughly analyzed the sequence of events that led to
this extended outage. To Google’s credit, it published the results of its post-mortem analysis for
all to see. The post-mortem showed the following timeline of the outage and the corrective
actionsthatGoogleplanstotaketoavoidthissituationinthefuture.
TheTimeline
Google’spublishedtimeline2fortheeventsofthemorningofFebruary24thshowedthefollowing:
2Post-mortemforFebruary24th,2010outage,GooglePost-Mortem,March4,2010.
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

7:48 AM – Internal monitoring graphs first began to show elevated errors in the primary data
center.
7:53 AM – The on-call staff was notified that the primary data center had suffered a power
outageandthatabout25%oftheservershadnotreceivedbackuppower.
8:01 AM – The primary on-call engineer determined that the Google App Engine was down.
He paged product managers and engineering leads, requesting them to handle
communicationwiththeusersabouttheoutage.
8:22 AM – Though power had been restored to the data center, it was determined that many
servers were down and that the surviving servers were not able to handle the traffic. Major
clustershadlostenoughmachinesthattheywerenotabletocarrytheload.Theon-callteam
agreedtoinvoketheunexpectedfailoverprocedureforanunplanneddatacenteroutage.
8:40 AM – Two conflicting sets of procedures were discovered. The team attempted to
contact the specific engineers responsible for procedure changes so that the situation could
beresolved.
8:44 AM – The primary on-call engineer attempted to move all traffic in a read-only state to
the backup data center. Unexpected configuration problems from this procedure prevented
theread-onlybackupfromworkingproperly.
9:08 AM – New data seemed to indicate that the primarydata center had recovered.With no
clear policy, the team was not aware that based on historical data, the primary data center
was unlikely to have recovered to a usable state. An attempt was made to move traffic back
to the primary data center while the read-only problems in the backup data center were
debugged.
9:18 AM – The primary on-call engineer determined that the primary data center had not
recovered. Traffic was failed back to the backup data center, and the unplanned failover
procedurewasreinitiated.
9:35 AM – An engineer familiar with the unplanned failover procedure was finally reached
and began providing guidance about the procedure. Traffic was moved in read-only mode to
thebackupdatacenter.
9:48 AM – Read-only mode became operational. Applications that handled read-only mode
workedproperlybutinareducedoperationalmode.
9:53 AM – Relevant engineers were now online. The correct procedure document was
confirmed.Theactualfailoverprocedureforreadsandwritesbegan.
10:08 AM – The unplanned failover procedure completed with no problems. The App Engine
wasrestoredtoservice.
10:19 AM – a post to the App Engine downtime-notify group let users know that the App
Enginewasoperatingproperly.
ModifiedProcedures
The post-mortem analysis concluded with the following plan to avoid such a situation in the
future:
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

1. Additional drills will be scheduled for all on-call staff to review production procedures,
including“rareandcomplicatedprocedures.”
2. A monthly audit of operation documents will be implemented. All documents out-of-date
willbemarked“deprecated.”
3. A clear policy about taking intrusive actions during a failure will be established to allow
theoperationsstafftoactconfidentlyandwithoutdelayinemergencysituations.
4. TwodifferentAppEngineconfigurationswillbeoffered:
a. The current low-latency option that has lower availability during unexpected
failures
b. A new, higher-availability option using synchronous replication that will have
higherlatencyduringnormaloperation.
KudosforGoodCommunication
The rapid and transparent communication from Google about its problems did not go unnoticed
amongitsusers.Therewasaraftoffavorableresponsesonmanyblogs:
“The amount of time and thought put into this post-mortem is staggering, and the
takeawaysareusefultoeveryorganization.”
“…atemplateforwhattheentireindustryshoulddosothatcollectivelywecanlearnfrom
eachother’smistakesinsteadofhavingtolearnthepainfullessonsindividually.”3
“Knowing what happened gives a large number of customers peace of mind, even if it is
sometimespainfulforinternalcustomers/employeestoadmittofault.”
Lessons Learned
The primary lesson to be learned from this experience is to try to identify all failure modes. Then
foreachsuchmode,plan,test,document,andtrain.
Another lesson is to ensure that all procedures are properly documented and the appropriate
personnel trained in the new procedures. Documentation and training should both be an
importantpartofchangemanagement.
A second lesson is one on which we have frequently commented. Be transparent, and
communicate effectively with your users during the resolution of a problem. There is a great deal
offorgivenessoutthereifyousimplykeepusersinformedastowhatishappening.
3Moretothepoint,ifyouhaveastorytosharethatwillhelpothers,evenifitmustbepublishedanonymously,letusknow
sothatwecanpublishitasaNeverAgainarticleforthebenefitofall.
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com