Superstorm Sandy Survivors
June2013
Superstorm Sandy, with its high winds and severe storm surge, hit the New Jersey and New
York shores and lower Manhattan with devastating force on Monday, October 29, 2012. It
flooded streets, tunnels, and subway lines in New York City. It disrupted power in and around
the city for weeks and cut communications when they were most needed. Sandy leveled homes,
businesses,andevenentirecommunitiesalongtheshoreline.
Melissa Delaney, writing for Biztech Magazine, has published an insightful series of case studies
describing how three companies in Superstorm Sandy’s path weathered the disaster relatively
unscathed.1 These companies relied on multilayered disaster-recovery plans and business continuity
strategies. They remained in operation even though they never anticipated the length of the outage, the
disruptiontopowerandcommunications,andthelengththatsomeoftheirfacilitiesremaineddark.
In this article, we review Delaney’s case studies. They provide valuable guidance for other companies
thatarereviewingtheirbusiness-continuityplans inthelightofrecentdisasterssuchasHurricaneSandy,
theOklahomaCitytornadoes,andtheCaliforniawildfires.
Structure Tone
http://twitter.com/?status=SuperstormSandySurvivorshttp://bit.ly/19iO4gX
Structure Tone is an international construction management firm headquartered at 770 Broadway in New
YorkCity.IthasofficesthroughouttheUnitedStatesandinEngland,Ireland,andHongKong.
Six years ago, the company set out to unifyits disparate IT systems so that it could implement a broader
setofconstruction-industrybusinessapplicationsandcouldensurethatitscorporatesystemswerehighly
reliable and resilient in the face of disasters. As part of this upgrade, it centralized its IT operations in its
NewYorkCityheadquarters.
It created a virtualized environment of 27 HP Proliant servers running 170 virtual machines under
VMware.ItcentralizedallofitsdatastorageontoanEMCClariionCX4-120storageareanetwork(SAN).
Structure Tone set up a backup data center in Dallas, Texas. It used VMware’s vCenter Site Recovery
Manager toprovide failover between sites and EMC’s RecoverPoint appliances to replicate databetween
the production and backup SANs. Replication was scheduled every fifteen minutes so that data at the
backupsitewasnevermorethanfifteenminutesold.
All of its offices are interconnected via a redundant Multiprotocol Label Switching (MPLS) mesh network
so that the offices can back up each other. Structure Tone’s northeast offices are backed up at the New
YorkCitydatacenter,anditsTexasofficesarebackedupbyitsDallascenter.
1MelissaDelaney,How3CompaniesDisaster-ProofedTheirITAheadofSuperstormSandy,Biztech;May29,2013.
1
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

TheprimarydatacentersinNewYork CityandDallasback eachotherup.Intheeventofproblemsatthe
production center in New York, the New York servers can be pointed to the SAN in Dallas. If the New
Yorkserversaredown,theentireIToperationcanbefailedovertoDallas.
To improve resiliency, Structure Tone uses EMC’s Avamar deduplication system to provide tape image
backup to virtual disk. Prior to this upgrade, all data was backed up daily to magnetic tape. This meant
that up to a day’s worth of data could be lost, and it could take two to three days to recover a failed
system.With the Avamar system, no more than five minutes of data will be lost; and a failed system can
berecoveredinahalfhour.
As Sandy approached, Structure Tone sent its staff home at 1 PM on Monday, Oct. 29th. Little did
managementrealizethattheywouldnot beabletoget back intotheir New York offices untila week later,
Saturday,November3rd,becauseofthefloodingandthepoweroutagesinManhattan.
However, the company’s disaster planning paid off. They had allowed themselves sixty minutes to fail
overtotheDallasdatacenter.WhenpowerwentoutattheirNewYorkCityheadquarters,theycompleted
thefailoverinthirtyminutesandprovidednear-seamlessservicestotheirITusers.
Structure Tone found that communications with its employees and customers over the next week was its
primary problem. Even though the company was successful in rapidly recovering their IT systems, its
biggestproblemwasgettingpeopleconnected.
Structure Tone had planned for this. Before Sandy hit, it emailed staff with conference bridge numbers
that executive teams and department heads used to communicate with each other in the days following
thestorm.
However, the company had made some employee mobility assumptions that didn’t hold. It had supplied
all users of its IT services with a notebook computer to communicate remotely with the company.
However, many employees viewed the storm warnings as being overhyped and did not bring their
notebookshomewiththem.
Those that did should have been able to connect to Structure Tone’s IT services through its Cisco
Systems virtual private network (VPN), but many employees had lost their home Internet services. The
cellularsystemswerefloodedwithtrafficandwerevirtuallyuseless.
The company set up mini-command centers at some larger job sites, where it distributed notebooks to
provideIT services to its employees.Withpower inlower Manhattanoutfor days below 14thStreet,many
ofthecompany’sclientshadsevereflooding.UsingsmartphonetextingandVPNaccesstocommunicate
with its field teams, Structure Tone helped many clients assess damage and perform any necessary
recoverywork.
Atalanta
AtalantaisanationalfooddistributerheadquarteredinElizabeth,NewJersey.ItsIT environmentincludes
HPservers,aCiscoSystemsnetwork,andaCitrixvirtualizationenvironment.
As part of its Disaster Recovery Plan, Atalanta has established a hot-site backup environment in a
colocation facility about a thirty-minute drive from its headquarters. Atalanta’s IT infrastructure is
replicated in a private cloud at the backup site. Atalanta periodicallybacks up its production environment,
includingitsdataandbusinessapplications,totheprivatecloud.
As Sandy approached, Atalanta closed its main office before the storm hit. One of the company’s
executives later remarked that as he dodged billboards blowing around the highway on his drive to work
thenextday,hehadneveranticipatedanythinglikethedestructionwroughtbySandy.
2
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Atalanta’s DR site sits at a higher elevation than most of the towns around it, and it fared well during the
storm.Atalantasuccessfullymoveditsoperationstothisbackupsite,andbusinesswentonasusual.The
deliveryoffoodproductstowarehousesandcustomersaroundthecountryproceededuninterrupted.
The company rented space in a hotel near its headquarters so that employees could work there.
However,thosefewemployeesthathadcommunicationsathomeworkedfromhome.
As withStructureTone,Atalanta’smajorproblem was communications withmanyofits employees.Many
phone lines were down for weeks.Tomaintain employee communications, the companyrerouted calls to
a sister company in Los Angeles. When another sister company in Paramus, New Jersey, regained
power,itmoveditsVoIPphonesystemthere.
Atalanta used text messaging to employee smart phones to get around the network loading that
prevented efficient voice communications, and it sent emails to both personal and work accounts to
adviseemployeesthatthecompanywasstillupandrunningandtoinformthemofproceduralchanges.
Atalanta has a bring-your-own-device policy. Though this policy requires additional security and
management, it became invaluable in ensuring the mobility of its employees. Atalanta states that this
policyhelpedimmenselyinthedisaster.
ConnectOne Bank
ConnectOne Bank is a local New Jersey bank with eight branches. It is headquartered in Englewood
Cliffs,NewJersey,andmaintainsitsproductionITsystemsatthatlocation.
The production systems are mirrored at its branch in Hackensack, New Jersey. At this branch, the bank
maintains the same operations center and the same phone systems as at its production site. However,
thesitesarepoweredbydifferentpowergridsandtelephonenetworks.
Inan emergency,telephonecalls areautomaticallyreroutedto the other office.Ifboth the productionand
backuplocationslosetelephoneservice,eachbranchhasbackupcellphonenumbers.
When Sandy hit, the phones went out in the bank’s Englewood Cliffs headquarters. To make matters
worse, staff had trouble accessing core IT services. Therefore, the bank’s operations were moved to its
backupsiteinHackensack;andalltelephonecallswerereroutedtothatbranch.
All of the bank’s branches were opened the day after the storm. Englewood Cliffs had lost power but
remained in service with an emergency generator. All branches had emergency generators, and
ConnectOne had contractors onsite to ensure that the generators were operating properly. The bank
instituted relaxed policies such as extending the time to cover overdrafts to accommodate the challenges
thatitscustomerswerefacing.
AspartoftheDRplan,communicationproblemswereanticipated.Adisaster-recoveryteam dialedintoa
conference call each morning to discuss the situation and to determine the next steps to implement a full
recovery.TheDRteam wasbrokenintogroupsthatfocusedondifferentaspectsofthebusiness,suchas
IT,retail,compliance,operations,andloans.
According to Elizabeth Magennis, executive vice president and chief lending officer of ConnectOne
Bank, communitybanks were able to rise to the challenge, responding better than their larger peers.
“We’re smaller organizations that are simply more nimble than larger ones,” she says. “We probably
haveabetterhandleonwhathappensinanypartofthebank.”
3
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Summary
JohnMcLennan, the CIO of Atalanta,summedupthe situation nicelywhen hesaid“Think bigger. Mother
Natureisprettypowerful.”
There are many lessons to be learned from Sandy. One is the importance of a well-thought-out and
testeddisaster-recoveryplan.
Another is that in a disaster, communication with employees, contractors, suppliers, and customers can
be difficult or even nonexistent. Even though each of the companies in these case studies succeeded
nicely in recovering their IT systems, many had difficulty with communications. The experience following
the 9/11 attacks showed how critical communications are to a DR plan. Despite extensive planning
followingthe9/11disaster,manycompaniesstillstruggledwithcommunications.
In her article, Melissa Delaney suggests several points that should be taken into account in a DR plan.
Her“Don’tForgetList”includes:
Createatieredapproach
A company’s operations contain a wide range of services, some much more critical than others. These
shouldbe organized into tiers for Service Level Agreement(SLA) purposes. For instance, StructureTone
includedEnterpriseResourcePlanning(ERP),filestorage,email,andmarketingdatainitsfirsttier witha
thirty minute SLA. Included in its second tier with a 24-hour SLA were Customer Resource Management
(CRM),administration,andfinancialplanning.
Lookathowclientswillbeaffectedbyadisaster
There may be opportunities to help customers out in a disaster and win customer loyalty. For instance,
ConnectOne gave clients extra time to make deposits if their accounts were overdrawn. They also let
customerschargetheircellphones,tablets,andnotebookcomputersandaccessWi-Fiattheirbranches.
Buildinthetimeneededtogetoperationsfullybacktonormal
When Structure Tone was able to get back into its New York City office a week after Sandy hit, it used
EMC’s RecoverPoint and VMware’s Site Recovery Manager to fail back IT services from its Dallas data
center to its New York City data center. It recovered its server farm within three hours, which began
running using the Dallas SAN. It resynchronized all data between the SANs within 24 hours, and IT
operationswerebacktonormal.
Carveouttimetotestyourplan
An untested DR plan is nothing more than notes on paper. It must be periodically updated and tested.
Likely,everytimeitistested,somethingthathasfallenthroughthecrackswillbediscovered.
4
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com