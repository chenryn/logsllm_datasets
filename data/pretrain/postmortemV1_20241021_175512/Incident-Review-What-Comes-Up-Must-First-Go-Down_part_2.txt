thanadayandwouldhavebeenunaffectedduringthetimeoftheincident.
Exampledataforwhichweintroducedinaccuraciesinordertobringserviceback.
honeycomb.io 10

Thisworked,andonthenextscan,theschemacacheupdaterreloadedallthedataweneeded.
Atthatpoint,westilldidn’tknowhowwellthedatabasemighttakethetrafficorifitneededto
warmup,andaprioroutagerevealedatendencyforouruserstohavelotsofdatabufferedthat
they’dsendallatonce.Weusedshorttimeperiodswithourcircuitbreakertoletintraffic,then
chokeditback,andanalyzed.
Welettrafficthroughforafewseconds,thenturneditoff.CPUandmemoryusagewerehigh,
whichwebelievedtobenormalwhenfirstwarmingup.Afterapausetoletitallsettle,welet
moretrafficgothroughforhalfaminuteorso,andthistimeitlookedstable.Wemanagedto
confirmthatRetrieverswereupdatingthelastupdatedschemavalues,andsoalltheingredients
wereinplaceforafullsuccess.Welettrafficgothroughinathirdcircuit-breakingreset,which
repairedingest.
Wethenworkedtorestorequeryingcapacity.Itseemedpartiallyfunctional,andoneofour
engineersrestartedhostsonarotationwhiletheseeffortswereongoingtomakesuretheflag
wasactiveandthateverythingwasup.Somehostswerelaggingbehindandrequiredextra
restartssincethedatabasefaultsmadethemfailsomeinternalchecks.
Atthispoint,thewholefleetwashealthy,andallwehadlefttodowastokeeprestartingthe
inactiveRetrieverhosts andinvestigatethemorespecificfailuremodeswe’veseeninour
database.
Analysis
Asthedustsettledandweranourincidentinvestigationandreview,multipleinterestingthreads
cameup.Onewastofigureouthowourdatabasediedsohard,whichnaturallyledtoa
discussionaboutitscentralroleinourinfrastructure.Wealsohadalongdiscussiononhowthe
responsefelt,oncewewerestuckinit.Finally,wehadalotoftalksaboutwhatexactlywecan
doforthistypeofoutage.
How a database dies
Theprecisedatabasefailuremodeweencounteredisdifficulttoexplain.Afterdiggingintoall
sortsofmetricsandlogs,wemanagedtofigureoutthatatthecoreofthedatabasefailurewas
adeadlockinMySQL’sinternals,andnotinourtransactions.
There’snoclearpathonhowwespecificallygottothatdeadlock,butwethinkthatsincethisis
aconcurrencyissue,it’snon-deterministic,andthechancesofhittingtheseincreaseswiththe
honeycomb.io 11

amountofcontentionandparallelismgoingon.Themoreloadedthedatabaseis,thehigherthe
likelihoodofhittingtheserarebugs.Here’swhatweobserved:
Asourcachemostlystoppedworking,theamountofreads(inpurple)weranshotupdrastically,
overloadedthedatabase,andthenafewwrites(inorange)—adecentamount,butnothingoutof
theordinary—seeminglytippeditoverintoarareraceconditionintheMySQLinternals,locking
threadafterthreadofthedatabase,untiloperationspiledup.
Eventually,allworkingthreadswereexhausted,thedatabaseranoutofconnections,andour
servicesdiedwhentheywereunabletogettheinformationtheyneededtorun.Thisinternal
deadlockfurtherexplainswhyweweren’tabletofreeupconnections:anythingwedidthattried
tointeractwithanystuckthreadgotstuckwaitingonthesamelockaswell.
Partoftheheavywriteworkloadcomesfromsequencesofinsertionsintoourschemastorage
whennewfieldsareencounteredbycustomers.Thisisgenerallyabitdemandingforthe
database,buttheheavyreadloadmovedusintoaratherdangerousanduncomfortable
situation.Engineersdescribedthisas“playinggolfinastorm,”denotingtheprobabilisticnature
oftheissue,butalsohowoursystemwasatthesametimeputinavulnerableposition.
Ourcacheimplementationisperceivedasbeingload-bearingandstructural:thesystemisn’t
safewithoutit.Thisisknowntobealessthanidealsituation,particularlywhenthisdatabaseis
socentraltothesystem,whichbringsustoournextpointonsinglepointsoffailure.
honeycomb.io 12

Single points of failure and ouroboroses
WerunhalfadozendifferentMySQLdatabases,allhostedviaRDS.Althoughwehavemany,we
dohavea“main”databasethatcontainsmanytypesofdata,requiredforeverythingrelatedto
useraccounts,permissions,configurationvaluesaroundteams,SLOsandtriggers,andfinally,
environmentanddataschemas—tonamethemainones.Noneofthesecontainactual
customerevents,whicharestoredviaourqueryengine,buttheyarenecessaryfortheoverall
systemtowork.
Mostfeaturesaredirectlytiedtoenvironmentsanddataschemas,andsothisdatabase
naturally“attracts”moredatatoit,asallservicestendtorequireatleastsomeofthat
informationtofunction.ThisrepresentsanobviousSinglePointofFailure(SPOF),which
everyoneintheorganizationisawareofandcontinuouslyworkingon;theotherdatabaseswe
haveweremostlysplitawayfromthemainoneovertimeasanattempttoremoveperformance
bottlenecksandreducetheblastradiusforcertainfeatures.
Ourcacheprotectsthisdatabase,andtheingestionofdataisrequiredtokeepthecacheas
warmaspossible.Coldstartsarechallengingbutmanageable;it’srunninghotwithoutthe
cachethatcausesissues.
Asanadditionalfactor,thecacheissharedbymultipleservices,whichallkeepanin-memory
cacheandrefreshitfromthelayeredone,andfallthroughtothedatabasewhenthere’san
issue.Tomaintaincorrectness,asingleprocessbackfillsthecachefromthedatabase;ifthe
layeredcacheisn’taround,processescanstillupdatetheirown,andintheory,thesystemcan
runforalongtime,aslongasthedatabasedoesnothardlock—whichissadlywhathappened.
Thiscreatesasystemwheremanydependenciesareindirect,andthingsgoingwronginone
endoftheenvironmentcanendupdegradingserviceforanother.Manyofthesearesetup
becausetheyaremoreeconomicalandlessrisky.
Infact,thatcomplexityishardlyavoidable.Locallygooddecisionsofteninteractinunexpected
ways:
● Thequeryengineupdatedthetimestampstothedatabasebecauseitalreadyhadtosee
allthedata,anditensuredconsistencybetweenwhattheinterfacereportsandwhatthe
queriesreturn.
honeycomb.io 13

● Thesharedcacheimplementationwasinpartscalingwork,andinpartanefforttoclean
upandnormalizeallschemausage—afirststepinmakingitmoremanageableto
untanglethe“main”databaseusage.
● Webelievedthefailoveroftheprimarydatabaseinstancetobecostlybecausewehad
seeninadequateperformanceissuesonreplicaswhenfailingover,formanyhoursthat
follow.
● Wenowknowthatthereplicashavingreliabilityandperformanceissueswereadisjoint
failuremode,butthesepriorproblemsarewhatmadeuswanttoswitchthewrites
acrossqueryengineclustersduringmigrations,andultimatelyfedintothisoutage.
● Weweremigratingbacktoavoidabugwiththenewclusterversionthatwethought
couldcorruptdata.
● Frequentdeploysmitigatedanon-obviousbugaroundaspecificfeatureflagwhich
everyonethereforebelievedtobesafe.
● Duringtheearlyinvestigation,wehadpauseddeploymentstopreventinterferencewith
newcodeandhostrestarts.
Asitturnsout,preventingabugbyswitchingtotheoldinfrastructuresetupthestage.This
investigationmadeussuspendotherwisefrequentdeployments,whichmadeitaguaranteethat
therestartsaccidentallyrequiredforthequeryengineflagswouldn’thappenbyaccident.Finally,
recoveryonthedatabasewasabitslowerbecausepriornear-misseswithourdatabasesledus
tobelieveperformancecouldbebadforalongertimethanwhatweobservedinpractice.
Itisabitironichowfeatureflags,frequentdeploys,suspendingdeploysduringincidents,and
learningfrompriornear-missesalltechnicallycontributedtothisincident,whilebeingsomeof
themosttrustedpracticeswehavetomakeoursystemsafer.
Incident response is personal
Wemadeaspecificcallduringtheincidenttogodownhard,onlysothatwecouldcomeback
upfaster.Whatweknewatthetimewasthattheloadonthedatabasewithoutacachewasthe
likelyproblem,thatthecacherequiredRetrievertoberebootedtowork,andthatwecouldn’t
rebootRetrieveruntilthedatabasewasbackup,whichwouldn’tbesafetoassumeunless
ingestwastakenoutofrotation.
honeycomb.io 14

Underthatpattern,theassumptionwasthatanypartialfailurewherewebroughtingestbackup
asfastaswecouldwouldnotactuallygetusoutoftheincident.Whenthissortof
self-sustainingsituationhappens,themosteffectivecourseofactionisoftentointerruptit
entirelytorestartitinabetterstate.
Thisstrategyfeltsafer,faster,andalsomorestraightforwardatthetimebecauseofrecent
experiencewithincidents,suchasissueswithourdatabasereplicas(togetmorefamiliar
arounddatabasedebuggingandfailovers),afullShepherdoutage(toexercisethecircuit
breaking),andexperimentswithbootingfreshclustersofHoneycombinnon-production
environments(whichdemonstratedwedon’thavecirculardependenciesthatpreventusfrom
coldrestarting).
Allthesefragmentsofpastresponseswereputtogethertohandlethisonebiggerincident.
Engineersmentionedduringthereviewthattheresponsefelteffectiveandwellorganized
internally,especiallywhencomparedtosmalleroutagesinthepast.
Themomenttheteaminvestigatingtheissuenoticedeverythingturnedintoanoutage,the
wholeresponsereorganizedinamatterofminutes.Itwentfromabrainstorm-centered
approach,wherepeoplelookedatvariousthingsandsuggestedpossibleapproaches,toone
thatwasalotmorefocusedonaction.
Becauseresponderskneweveryoneinvolvedandtheirrespectiveskillsets:
● Weself-organizedfollowingknownpatternsfortheoverallresponse
● Weidentifiedanengineerwhohaddonetheingestcircuitbreakingandwasaroundtodo
itasfastaspossibletobreakoffthecrashloop
honeycomb.io 15

● Anotherengineerself-assignedtheroleoflookingintothedatabaseissuesand
connectionproblems
● WebroughtinmoreengineerswhounderstoodMySQLreallywelltoworkonthatissue
whenitsurfaced
● Westartedmulti-prongedsideinvestigationsintohowtomosteffectivelybackfillthe
cache,hedgingourbetssincenotallapproacheswereguaranteedtowork
Sincetheyhadalsodealtwithalotofincidentsorhigh-stakesnear-missestogetherinthepast,
participantsalreadyknewtoself-reporttheirwork.Theself-appointedincidentcommanderwas
awareoftheirowntendencytoveeroffintodebuggingandmonitoringworkandbothtook
measurestostayfocusedandhadcoworkersremindingthemofit,whichopenedthedoorto
sideinvestigationsbeingkeptintheloopmoreeffectively.
Finally,peoplewhosetaskswereundercontrolpassivelylistenedtoongoinginvestigations(e.g.,
idlinginZoomcalls)whileourengineerhandlingcommsmadefrequentupdatesinshared
channels.Everyonealreadyhadcontextintotheincident.Whenallthepieceswereinplace,we
couldjoinforcestomergethecachebackfill,circuitbreakingreopening,anddatabasehealth
monitoringintoasingleoperation.
Intheend,theincidentwashighpace,butitdidn’tfeelconfusing.Wehadagoodbalanceof
orchestrationandself-organization.
Corrective actions
Forthedurationoftheoutage,nonewdatacouldbeingested.Ifourusersorcustomersdidnot
bufferthisdatatodisksomewheretoreplayitlater,itisgoneforgood.Thishasknock-on
effectsduringtheincidentonalertingandquerying,buttheingestiongapstaysforaslongas
there’sdataretention.Assuch,majoringestoutageslikethathaveawaytostretchintime
beyondtheperiodduringwhichthesystemisdown.
Thistypeoffailuremodeissomethingbothweandourcustomersunderstandispossible
basedonhowweareonlypresentinoneregion(althoughwedoruninmultipleredundant
zones).ThepresenceofSPOFs,whilepossiblylessvisibletopeopleoutsideofHoneycomb,is
wellknown—andafrequentconcerninternally.Migrationsawayfromsuchpatternsare
consideredsaferwhendonegradually.Movingfastrequiresstoppingallactivity,evenurgent
scalingwork,andrushingcancauseevenmoremajoroutages.
Theconsequenceofthisisthatnobodyhasseriouslysuggested“blowupthedatabaseand
removethesinglepointoffailure”asanoutcomeofthisincident,becauseit’sbothimpractical
honeycomb.io 16

asanactionitem,andalsosomethingthatisalwaysonpeople’smindsandgraduallybeing
workedonwhendoingtheirengineeringjob.BreakingapartSPOFsispartofalessexplicitset
oflongtermobjectivesthatwebakeintodesigns,buthavingthissortoftrade-offbemade
moreexplicitintheaftermathofanincidentreviewissomethingwearediscussingwithin
engineeringteams.
We’reexploringfuturearchitecturesthatmaymitigatetheimpactofsuchincidentsandtheir
implications.Wearelookingatwaystostrengthenthecachefurther,mechanismstolowerthe
amountofcontentionweputonourdatabaseduring“update”stormsfromnewschemas,and
tostabilizetheperformancecostsofsomeoperationswithinoursystem.Wearestudyingthem
alongothermeasures,includingthoserelatedtoinstrumentationorexperimentationtobetter
detectandhandlesuchedgecases.
Intheshortterm,thechancesofanincidenthappeningwiththisspecificfailuremodehave
beendrasticallyreduced,mostlybecauseourmigrationiscompleteandwehavealready
removedallcodethatallowswritesbehindthecachetobedisabled.Theexpectedresponse
timeisalsolikelytobemuchfastersincewe’veupdatedourunderstandingofdatabasefailures
andmakerecoveryfasterforunlikelylockupsofthiskind.Inthemeantime,wearecarefully
evaluatingtheoptionsabovetoseewhichrepresentthebesttrade-offs.
Conclusion
Thisincidentissomethingwewishwouldn’thappen,butknowthatfromtimetotime,we’llhave
tomanagethem—nomatterhowhardweplanforthem.Oneofthemostintriguingthemesthat
cameupfrommanyofourrecentincidentsandnear-missesishowoftenoneoftheir
contributingfactorshasbeentryingtopreventasubtlebugwithpossiblybadconsequences
fromhappening—onlytoendupwithabiggerunforeseenoutageattheend.
Theprocessaroundincidentsisgenerallypavedwithgoodideasforimprovements,thebestof
intentions,andfascinatingsurprises.Wehopethatwe’vebeenabletoillustratehowthesecame
tointeractwitheachotherinthisspecificsituation,andthattheyconveythecomplexityand
richnessbehindthisoutage.
Someofthepatternsinincidentresponsementionedheremayfeelfamiliarwithsomeofour
readers.Ifyou’veseensimilarthingsandwanttodiscussthemwithus,reachouttousin
Pollinators,ourSlackcommunity.
honeycomb.io 17
