Microsoft’s Azure Cloud Goes Down - Again
December2014
Public clouds are gaining increasing acceptance by both large and small companies to
host their applications. Companies can avoid the upfront costs of servers, storage,
networks,andcoolingas wellastheongoingcostsofdatacenter space,power,andstaffing.Alltheypay
isafeefortheiruseofthefacilitiesownedbythecloudprovider.
Cloud computing is certainly acceptable for ordinary applications. But is it reliable enough to host a
company’s mission-critical applications? Research by Infosys suggests that about four in five enterprises
plantomovetheirmission-criticalworkloadsintothepubliccloud.
However, there continue to be examples of massive cloud failures that have taken down applications for
hoursandinsomecasesevendays.Ifacompanydecidestohostamission-criticalapplicationinapublic
cloud, it must have plans as to how to continue to offer the services of the application should the cloud
fail.
A case in point is the recent global failure of Microsoft’s Azure cloud, a failure lasted for eleven hours. It
addstoastringofAzureoutagesoverthelastfewyears.
The Azure Cloud Failure of November 2014
Similar to Amazon, Microsoft has built multiple Azure availability regions around the globe. Each region
can comprise one or more facilities. A company can host an application in multiple regions so that if a
region fails, the application can continue to run in another region. As of this date, Microsoft is operating
nineteenAzureregionsaroundtheworld.
Around 1 AM UTC (Coordinated Universal Time, or about 8 PM Eastern Standard Time in the U.S.), on
Tuesday,November18,2014,2Microsoft’sAzurecloudwentdark intwelveofitsnineteenglobalregions.
The regions affected included Central US, East US, East US 2, North Central US, South Central US,
WestUS,NorthEurope,WestEurope,EastAsia,SoutheastAsia,JapanEast,andJapanWest.
Many Azure services were affected, including Azure Storage, Virtual Machines, SQL, Visual Studio,
HDInsights (Hadoop for Big Data Analytics), Active Directory, Azure Backup Services, and the Service
HealthDashboard.1Hostedwebsites wentdown, includingsomeofMicrosoft’s own websites.Access to
Office 365 and Xbox Live, the popular multiplayer game site, was interrupted. Microsoft’s MSN news and
websitewasinaccessible.
Of particular importance was the loss of the Service Health Dashboard. The Dashboard posts the status
of all Azure services and informs customers of problems and expected times to solution. Without the
1 See Update on Azure Storage Service Interruption, Microsoft Blog; November 19, 2014, for a complete list of Azure services
affected.
1
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Dashboard, Microsoft could not even tell its customers that it was having a problem. After about three
hours,MicrosoftresortedtoTwitterinanattempttokeepcustomersinformed.
Itwasnotuntil11:45AMUTC,elevenhourslater,thatAzurewasbroughtbackonline.
Microsoft warned customers who had been affected that they “will see a data gap during the impacted
window.” “Data gap” was not defined, but probably meant that updates that were attempted during the
outagemayhavetoberepeated.Dataintegrityofcustomerdatabasesmayalsohavebeenanissue.
Inapostmortem,MicrosoftidentifiedafailedupgradetoitsAzureStoragesystemastheculprit.
Azure Storage Services
Azure Storage services provide scalable, durable, and highly available storage for applications and for
virtualmachinesrunningonAzure.AzureStorageofferssupportforblobs,tables,queues,andfiles:
 Blobstoragestoresanytypeoftextorbinarydata,suchasadocument,mediafile,orvideos.
 Table storage stores structured datasets. It allows rapid development and fast access to large
quantitiesofdata.
 Queue storage provides reliable messaging between application components for workflow
processing.
 Filestorage offerssharedstoragefor legacyapplications.Azurevirtualmachines andcloudservices
cansharefiledataacrossapplicationcomponents.
AnAzurecustomer storageaccountcancontainup to 500terabytes of data.Data inastorageaccountis
replicated to ensure durability and highly availability. There are several options for replicating storage
accountdata:
 Locallyredundantstoragemaintainsthreecopiesofthedatawithinasinglefacilityinasingleregion.
 Zone-redundant storage maintains three copies of the data replicated across two to three facilities,
eitherwithinasingleregionoracrosstworegions.
 Geo-redundant storage (the default) maintains six copies of the data replicated three times within the
primary region and three times in a secondary region hundreds of miles away from the primary
region.
 Read-access geo-redundant storage allows read access to data at the secondary region in the event
thattheprimaryregionbecomesunavailable.
What Caused the Failure?
TheAzureoutagewascausedbytwofactors-afaultyupgradeandanimproperdeployment.
The upgrade was being made by Microsoft to its Azure Storage service. The upgrade was intended to
improve the performance of the service. It had been extensively tested for several weeks in a subset of
customer-facing storage services for Azure tables. Microsoft calls this procedure “inflighting.” It is
intendedto identifyissues beforebroad deployment. Theinflightingdemonstratedanotable improvement
in storage performance. With this result in hand, Microsoft proceeded to roll out the upgrade to all of its
regions.
2
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Unfortunately, a decision was made to roll out the configuration change to all regions simultaneously.
Microsoft’s standard protocol was not followed, which specified that changes should be rolled out in
incrementalbatches.Thus,allregionsreceivedtheupdatewithinashortperiodoftime.
As soon as some of the regions came online with the Azure Storage performance upgrade, problems
arose. The Storage blob front-ends went into an infinite loop and could take on no further traffic. This
issue had gone undetected during the testing of the update. Other services built on top of blob storage
begantoexperienceoutages.
By the time Microsoft staff had identified the problem, twelve of its nineteen regions were affected. The
Azurecloudwentdownaroundmostoftheglobe.
Microsoft immediately started rolling back the Storage upgrade. However, this required restarting the
AzureStoragefront-ends.Restartingthefront-endssignificantlydelayedtherecovery.
It was not until eleven hours later that the failed upgrade was fully rolled back and the Azure cloud was
returnedtofullfunctionality.
Microsoft’s Mitigation Steps
Following its post-mortem, which in the spirit of full transparency was published in detail, Microsoft
initiatedthefollowingstepstoimprovethereliabilityofAzure:
 It fixed the infinite-loop bug in the CPU improvement logic for blob front-ends before the upgrade
wasredeployed.
 Itimproveditsrecoverymethodstominimizerecoverytime.
 Itensuredthatitsdeploymenttoolsenforcethestandardprotocolofapplyingproductionchangesin
incrementalbatches.
 ItimproveditsServiceHealthDashboardinfrastructuretomakeitmoreresilient.
Azure’s History of Outages
This was the second large-scale Azure outage in less than three months. The earlier outage caused
significant Azure downtime in the U.S. Japan, and Brazil. In some cases, the outage lasted nearly a
week.
On March 13, 2009, a network issue caused applications to crash in a beta version of Azure; and the
recoveryservicebecameoverwhelmedandcrashed.Azurewasdownforalmostaday.2
On February 29, 2012, the entire Azure cloud was struck with a leap-dayprogramming error. Azure went
downfor32hours.3
OnOctober30,2013,afailedupgradethatwasprematurelydeployedacrossallregionstook outasingle
point of failure and prevented any new applications from being deployed. It took a day and a half to
correcttheproblem.4
2MoreNeverAgainsIII,AvailabilityDigest;July2009.
http://www.availabilitydigest.com/public_articles/0407/more_never_agains_3.pdf
3WindowsAzureCloudSuccumbstoLeapYear,AvailabilityDigest;March2012.
http://www.availabilitydigest.com/public_articles/0703/azure.pdf
4WindowsAzureDownedbySinglePointofFailure,AvailabilityDigest;November2013.
http://www.availabilitydigest.com/public_articles/0811/azure.pdf
3
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Microsoft specifies an availability of 99.9% in its SLA for all of its services.. This represents about eight
hoursofdowntimeperyear.Inthelastthreeyears,AzureoutageshaveblownthroughitsSLA.
Summary
According to one blogger, Microsoft’s compensation for violating its SLA is likely to be service credits. He
notes that this is tantamount to saying “We’re sorry for the crummy service – here’s some more crummy
serviceascompensation.”
Azure’s availability history certainly raises serious questions about the use of public clouds for mission-
critical applications. Microsoft is competing with cloud platforms offered by other major companies such
as Amazon, Google, and IBM. Lydia Leong, a VP and distinguished analyst at Gartner, notes that
“Microsoft’s disastrous inability to keep Azure outages confined to a single region is a major red flag for
enterprisesconsideringAzure.”
This incident indicates that Microsoft’s inflighting approach may not be totally effective. Why was the
several-week inflight test conducted only on Azure Storage tables?What about blobs, queues, and files?
Maybeitwasluckythatonlyblobstoragewasaffected.
In any event, it is a good bet that Microsoft is tuning up its inflight procedures and will ensure that
upgrades are rolled out in a controlled fashion in the future. In fact, a problem with an upgrade to the
AzuredatabasethefollowingweekonNovember24thwascontainedbydoingjustthat.
Acknowledgements
Materialforthisarticlewastakenfromthefollowingsources:
Azureoutageraisesquestionsaboutpubliccloudformission-criticalapps,BusinessCloudNews;
November19,2014.
MicrosoftExplainsWhatWentWronginLatestGlobalAzureOutage,CRN;November19,2014.
UpdateonAzureStorageServiceInterruption,MicrosoftBlog;November19,2014.
What Happened? Microsoft’s Fritz Discusses Azure Outage and Building Azure Sites, Visual Studio
Magazine;November19,2014.
Microsoft Delivers A Post Mortem – The Reasons Behind the Global Azure-Alypse, Forbes; November
20,2014.
Microsoft Says 11-Hour Azure Outage Was Caused by System Update, Entrepreneur, November 20,
2014.
Azure outage Tuesday produced disruptions to MSN website, Office 365, Xbox Live, and third-party
services,aswellaspossibledataintegrityproblems,InformationWeek;undated.
IntroductiontoMicrosoftAzureStorage,MicrosoftDocumentation.
4
©2014SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com