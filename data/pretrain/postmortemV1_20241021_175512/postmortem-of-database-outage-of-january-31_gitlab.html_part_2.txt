1.  Copy over the LVM snapshot
2.  Copy over the PostgreSQL data directory

In both cases the amount of data to copy would be roughly the same.
Since copying over and restoring the data directory would be easier we
decided to go with this solution.

Copying the data from the staging to the production host took around 18
hours. These disks are network disks and are throttled to a really low
number (around 60Mbps), there is no way to move from cheap storage to
premium, so this was the performance we would get out of it. There was
no network or processor bottleneck, the bottleneck was in the drives.
Once copied we were able to restore the database (including webhooks) to
the state it was at January 31st, 17:20 UTC.

On February 1st at 17:00 UTC we managed to restore the GitLab.com
database without webhooks. Restoring webhooks was done by creating a
separate staging database using the LVM snapshot, but without triggering
the removal of webhooks. This allowed us to generate a SQL dump of the
table and import this into the restored GitLab.com database.

Around 18:00 UTC we finished the final restoration procedures such as
restoring the webhooks and confirming everything was operating as
expected.


In the spirit of transparency we kept track of progress and notes in a
[publicly visible Google
document](https://docs.google.com/document/d/1GCK53YDcBWQveod9kfzW-VCxIABGiryG7_z_6jHdVik/pub).
We also streamed the recovery procedure on YouTube, with a peak viewer
count of around 5000 (resulting in the stream being the #2 live stream
on YouTube for several hours). The stream was used to give our users
live updates about the recovery procedure. Finally we used Twitter
(<https://twitter.com/gitlabstatus>) to inform those that might not be
watching the stream.

The document in question was initially private to GitLab employees and
contained name of the engineer who accidentally removed the data. While
the name was added by the engineer themselves (and they had no problem
with this being public), we will redact names in future cases as other
engineers may not be comfortable with their name being published.


Database data such as projects, issues, snippets, etc. created between
January 31st 17:20 UTC and 23:30 UTC has been lost. Git repositories and
Wikis were not removed as they are stored separately.

It\'s hard to estimate how much data has been lost exactly, but we
estimate we have lost at least 5000 projects, 5000 comments, and roughly
700 users. This only affected users of GitLab.com, self-managed
instances or GitHost instances were not affected.


Since GitLab uses GitLab.com to develop GitLab the outage meant that for
some it was harder to get work done. Most developers could continue
working using their local Git repositories, but creating issues and such
had to be delayed. To publish the blog post [\"GitLab.com Database
Incident\"](/blog/2017/02/01/gitlab-dot-com-database-incident/) we used
a private GitLab instance we normally use for private/sensitive
workflows (e.g. security releases). This allowed us to build and deploy
a new version of the website while GitLab.com was unavailable.

We also have a public monitoring website located at
<https://dashboards.gitlab.com/>. Unfortunately the current setup for
this website was not able to handle the load produced by users using
this service during the outage. Fortunately our internal monitoring
systems (which dashboards.gitlab.com is based on) were not affected.


To analyse the root cause of these problems we\'ll use a technique
called [\"The 5 Whys\"](https://en.wikipedia.org/wiki/5_Whys). We\'ll
break up the incident into 2 main problems: GitLab.com being down, and
it taking a long time to restore GitLab.com.

**Problem 1:** GitLab.com was down for about 18 hours.

1.  **Why was GitLab.com down?** - The database directory of the primary
    database was removed by accident, instead of removing the database
    directory of the secondary.
2.  **Why was the database directory removed?** - Database replication
    stopped, requiring the secondary to be reset/rebuilt. This in turn
    requires that the PostgreSQL data directory is empty. Restoring this
    required manual work as this was not automated, nor was it
    documented properly.
3.  **Why did replication stop?** - A spike in database load caused the
    database replication process to stop. This was due to the primary
    removing WAL segments before the secondary could replicate them.
4.  **Why did the database load increase?** - This was caused by two
    events happening at the same time: an increase in spam, and a
    process trying to remove a GitLab employee and their associated
    data.
5.  **Why was a GitLab employee scheduled for removal?** - The employee
    was reported for abuse by a troll. The current system used for
    responding to abuse reports makes it too easy to overlook the
    details of those reported. As a result the employee was accidentally
    scheduled for removal.

**Problem 2:** restoring GitLab.com took over 18 hours.

1.  **Why did restoring GitLab.com take so long?** - GitLab.com had to
    be restored using a copy of the staging database. This was hosted on
    slower Azure VMs in a different region.
2.  **Why was the staging database needed for restoring GitLab.com?** -
    Azure disk snapshots were not enabled for the database servers, and
    the periodic database backups using `pg_dump` were not working.
3.  **Why could we not fail over to the secondary database host?** - The
    secondary database\'s data was wiped as part of restoring database
    replication. As such it could not be used for disaster recovery.
4.  **Why could we not use the standard backup procedure?** - The
    standard backup procedure uses `pg_dump` to perform a logical backup
    of the database. This procedure failed silently because it was using
    PostgreSQL 9.2, while GitLab.com runs on PostgreSQL 9.6.
5.  **Why did the backup procedure fail silently?** - Notifications were
    sent upon failure, but because of the Emails being rejected there
    was no indication of failure. The sender was an automated process
    with no other means to report any errors.
6.  **Why were the Emails rejected?** - Emails were rejected by the
    receiving mail server due to the Emails not being signed using
    DMARC.
7.  **Why were Azure disk snapshots not enabled?** - We assumed our
    other backup procedures were sufficient. Furthermore, restoring
    these snapshots can take days.
8.  **Why was the backup procedure not tested on a regular basis?** -
    Because there was no ownership, as a result nobody was responsible
    for testing this procedure.


We are currently working on fixing and improving our various recovery
procedures. Work is split across the following issues:

1.  [Overview of status of all issues listed in this blog post
    (#1684)](https://gitlab.com/gitlab-com/infrastructure/issues/1684)
2.  [Update PS1 across all hosts to more clearly differentiate between
    hosts and environments
    (#1094)](https://gitlab.com/gitlab-com/infrastructure/issues/1094)
3.  [Prometheus monitoring for backups
    (#1095)](https://gitlab.com/gitlab-com/infrastructure/issues/1095)
4.  [Set PostgreSQL\'s max_connections to a sane value
    (#1096)](https://gitlab.com/gitlab-com/infrastructure/issues/1096)
5.  [Investigate Point in time recovery & continuous archiving for
    PostgreSQL
    (#1097)](https://gitlab.com/gitlab-com/infrastructure/issues/1097)
6.  [Hourly LVM snapshots of the production databases
    (#1098)](https://gitlab.com/gitlab-com/infrastructure/issues/1098)
7.  [Azure disk snapshots of production databases
    (#1099)](https://gitlab.com/gitlab-com/infrastructure/issues/1099)
8.  [Move staging to the ARM environment
    (#1100)](https://gitlab.com/gitlab-com/infrastructure/issues/1100)
9.  [Recover production replica(s)
    (#1101)](https://gitlab.com/gitlab-com/infrastructure/issues/1101)
10. [Automated testing of recovering PostgreSQL database backups
    (#1102)](https://gitlab.com/gitlab-com/infrastructure/issues/1102)
11. [Improve PostgreSQL replication documentation/runbooks
    (#1103)](https://gitlab.com/gitlab-com/infrastructure/issues/1103)
12. [Investigate pgbarman for creating PostgreSQL backups
    (#1105)](https://gitlab.com/gitlab-com/infrastructure/issues/1105)
13. [Investigate using WAL-E as a means of Database Backup and Realtime
    Replication
    (#494)](https://gitlab.com/gitlab-com/infrastructure/issues/494)
14. [Build Streaming Database
    Restore](https://gitlab.com/gitlab-com/infrastructure/issues/1152)
15. [Assign an owner for data
    durability](https://gitlab.com/gitlab-com/infrastructure/issues/1163)

We are also working on setting up multiple secondaries and balancing the
load amongst these hosts. More information on this can be found at:

-   [Bundle pgpool-II 3.6.1
    (!1251)](https://gitlab.com/gitlab-org/omnibus-gitlab/merge_requests/1251)
-   [Connection pooling/load balancing for PostgreSQL
    (#259)](https://gitlab.com/gitlab-com/infrastructure/issues/259)

Our main focus is to improve disaster recovery, and making it more
obvious as to what host you\'re using; instead of preventing production
engineers from running certain commands. For example, one could alias
`rm` to something safer but in doing so would only protect themselves
against accidentally running `rm -rf /important-data`, not against disk
corruption or any of the many other ways you can lose data.

An ideal environment is one in which you *can* make mistakes but easily
and quickly recover from them with minimal to no impact. This in turn
requires you to be able to perform these procedures on a regular basis,
and make it easy to test and roll back any changes. For example, we are
in the process of setting up procedures that allow developers to test
their database migrations. More information on this can be found in the
issue [\"Tool for executing and reverting Rails migrations on
staging\"](https://gitlab.com/gitlab-com/infrastructure/issues/811).

We\'re also looking into ways to build better recovery procedures for
the entire GitLab.com infrastructure, and not just the database; and to
ensure there is ownership of these procedures. The issue for this is
[\"Disaster recovery for everything that is not the
database\"](https://gitlab.com/gitlab-com/infrastructure/issues/1161).

Monitoring wise we also started working on a public backup monitoring
dashboard, which can be found at
<https://dashboards.gitlab.com/dashboard/db/postgresql-backups>.
Currently this dashboard only contains data of our `pg_dump` backup
procedure, but we aim to add more data over time.

One might notice that at the moment our `pg_dump` backups are 3 days
old. We perform these backups on a secondary as `pg_dump` can put quite
a bit of pressure on a database. Since we are in the process of
rebuilding our secondaries the `pg_dump` backup procedure is suspended
for the time being. Fear not however, as LVM snapshots are now taken
every hour instead of once per 24 hours. Enabling Azure disk snapshots
is something we\'re still looking into.

Finally, we\'re looking into improving our abuse reporting and response
system. More information regarding this can be found in the issue
[\"Removal of users by spam should not hard
delete\"](https://gitlab.com/gitlab-org/gitlab-ce/issues/27581).

If you think there are additional measures we can take to prevent
incidents like this please let us know in the comments.



Pushing to the default branch will automatically update the merge
request so that it\'s aware of there not being any differences between
the source and target branch. At this point you can safely close the
merge request.


There are 3 options to resolve this:

1.  Close the MR and create a new one
2.  Push new changes to the merge request\'s source branch
3.  Rebase/amend, and force push to the merge request\'s source branch


Go to your project, then \"Pipelines\", \"New Pipeline\", use \"master\"
as the branch, then create the pipeline. This will create and start a
new pipeline using your master branch, which should result in your
website being updated.


Most likely they were, but the database is not aware of this. To solve
this, create a new pipeline using the right branch and run it.


Pushing new commits should automatically solve this. Alternatively you
can try force pushing to the target branch.


Project details are stored in the database. This meant that this data
was lost for projects created after 17:20. We ran a procedure to restore
these projects based on their Git repositories that were still stored in
our NFS cluster. This procedure however was only able to restore
projects in their most basic form, without associated data such as
issues and merge requests.


