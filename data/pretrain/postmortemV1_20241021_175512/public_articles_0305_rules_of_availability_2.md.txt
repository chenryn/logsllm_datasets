Rules of Availability – Part 2
May2008
We continue in this article a review of our sixty-four “Rules of Availability,” as published in our
series of books entitled Breaking the Availability Barrier.1 We chose those rules that are
particularly applicable as best practices to achieve continuous availability with redundant
systems,withafocusonactive/activesystems.
The Facts of Life
Rule 18: Redundant hardware systems have an availability of five to six nines. Software and
peoplereducethistofourninesorless.
Even Microsoft touts five nines of availability in their ads. But they are referring to availability at
the system level, not at an operational level. The extreme in hardware availability is achieved by
NonStop Integrity TMR (triple modular redundancy) servers that theoretically can achieve seven
ninesofhardwareavailability.
Hardware availability has ceased to be a significant part of the system availability equation. It is
application bugs and people errors that now control availability. This means that thorough
applicationtesting,documentedoperationalprocedures,andgoodoperator trainingrepresentthe
pathtoimprovingyoursystemavailability.
Rule19:(Bartlett’sLaw)-Whenthingsgowrong,peoplegetstupider.
This is probably one of the most important availability rules of all, and this rule plays a major
factor in Rule 18 above.When a system crashes, thepressure on the operations staff can rise to
untenable levels. Not only are the operations people trying to determine what caused the
problem, but management is suddenly very much involved and is demanding answers and a
course of action. What is it that went wrong? Is it best to try to reboot the system? How long will
thattake?Shouldafailover beattemptedtothebackupsystem?How long,inturn,willthattake?
Will the backup really work?When was the last time that it was tested?What is it that is going to
getmybossofoffmybacksothatIcantendtotheproblem?
With pressures like these, it is no wonder that people make mistakes. Have you ever felt your
brain turn to mush when everything seems to be going wrong? Have your fingers become too fat
forthekeyboard?
1BreakingtheAvailabilityBarrier:SurvivableSystemsforEnterpriseComputing,AuthorHouse;2004.
BreakingtheAvailabilityBarrierII:AchievingCenturyUptimeswithActive/ActiveSystems,AuthorHouse;2007.
BreakingtheAvailabilityBarrierIII:Active/ActiveSystemsinPractice,AuthorHouse;2007.
Theserulesmayalsobefoundat http://www.gravic.com/breaking_the_availability_barrier_rules.html.
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

Times like this are when well-thought-out, tested, documented, and automated operating
proceduresareamust.Thebestwaytoavoidmistakesunderstressistoreducethestress.Ifthe
operating staff has a clear set of procedures that can guide them through the crisis, if they are
adequately trained in those procedures, and if the procedures are highly automated, people will
get“less stupider” andmakefewer mistakes.As aresult, your applicationavailabilitywillcertainly
improve.
Rule 20: Conduct periodic simulated failures to keep the operations staff trained and to ensure
thatrecoveryproceduresarecurrent.
This rule is really a corollary to Rule 19. The best way to reduce the effect of human errors on
availability is to ensure that all involved are well-trained. Simulated periodic failures are the best
waytoachievethistraining.
Simulated failures themselves come with a price. For one thing, the system will be down during
the recovery period, whether the recovery is a reboot or a failover. For another, if recovery is to
be achieved by failing over to a backup system, will that system really come online without a
problem?Mustthesystemthenbefailedbacktotheprimarysystemoncethetestisover?
Theseproblemsmeanthatthesystem willbedownforagoodbitofthesimulatedfailure.Isthere
a window during which the users of the applications running on the system can be without
service? What happens if a fifteen-minute failover test turns into a four-hour outage? For these
reasons,manycompaniesdonotpracticerecoveryprocedures.
A good solution to this dilemma is the use of active/active systems. It is always known that the
backup system is working because it is in production. All that needs to be done is to move the
affecteduserstothesurvivingsystem.Thiscanoftenbedoneinseconds.
Rule23:(Niehaus’Law)-Changecausesoutages.
When it comes to outages caused by people or software, a common culprit is change.
Modifications to applications are a notorious source of software bugs that can cause an
applicationoutage,ifnotasystemoutage.Furthermore,changedapplicationscanmeanchanges
inoperatingproceduresthatcanleadtooperatorerrors.
Good change control procedures are imperative to achieve high availability. There should be
sufficient software documentation so that the impact of a change on other software modules can
be accurately determined. Changes should be carefully reviewed and critiqued by all affected
personnel. Functional changes, software changes, and operating procedure changes should be
clearly documented. Most importantly, any change should be thoroughly tested; and the
operationsstaffshouldbeadequatelytrainedinanychangestooperatingprocedures.
Rule 24: Following the failure of one subsystem, failover faults cause the system to behave as if
itcomprisesn-1remainingsubsystemswithdecreasedavailability.
A failover fault occurs when failover to a backup component is unsuccessful. It does not matter
whether the backup component is a process in a checkpointed process pair or a system in an
active/backupconfiguration.
Failover faults are insidious. First of all, it is very difficult to exhaustively test a system to ensure
that it will always fail over successfully under any failure condition. That is because one cannot
predict every failure scenario, and many failure scenarios cannot be easily simulated. As a
consequence,thepossibilityofafailoverfaultisalwayspresent.
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

Secondly, failover faults have a magnified impact on system availability. For instance, it can be
shown that under reasonable conditions a 1% chance of a failover fault can translate into a 40%
reductioninsystemavailability!
Again, active/active systems bring a failover fault advantage to system availability because the
likelihood of a failover fault is highly unlikely. It is always known that the backup is working
becauseitisactivelyprocessingtransactions.
Rule 26: (The Golden Rule) - Design your systems for fast recovery to maximize availability, to
reducetheeffectoffailoverfaults,andtotakefulladvantageofsystemsplitting.
Nomatterwhatthereasonforasystem failure,itisobviousthatthefasterthatthesystem canbe
returned to service, the better will be the system availability. Recovery times can be improved by
contracting for a higher level of service, by stocking critical parts at the data center, and by
ensuringthatthereareenoughtrainedtechniciansoncalltoeffectparallelrepair(multiplesystem
failuresarerepairedsimultaneouslyinsteadofoneatatime).
It can be shown that reducing the average time to repair a node in a distributed system by a
factor of k willreduce its average timetorestoreservicebyafactor of k, will increaseits average
timebeforefailurebyafactorofk,andwillimproveitsavailabilitybyafactorofk2.
Rule 27: Rapid recovery of a system outage is not simply a matter of command line entries. It is
anentirebusinessprocess.
Adding to Rule 26, system recovery is a process that encompasses the entire enterprise. It
requires close cooperation and efficient decision making of the entire operating staff and their
management. It requires not only that each system component has a backup but also that every
critical person has a backup. There must be a means to contact important personnel on an
emergency basis. It requires good operator training. It requires good documentation of the
recovery procedures for the system, the applications, the database, the network, and the
environmentalcontrols; anditrequires thefrequenttestingof theseprocedures.Itrequires a plan
forcontinuingthebusinessoftheenterpriseintheabsenceofITsupport.
Rule 28: RPO and RTO are both a function of the data replication technology used to maintain
databasesinsynchronism.
Synchronousreplicationguaranteesthatnocommittedtransactionswillbelostfollowingasystem
failure. This is an RPO (recovery point objective) of zero. With asynchronous replication, the
amount of data that may be lost is that which is in the replication pipeline at the time of failure.
Thus,thelower thelatencyof thereplicationengine,theless data willbelost.Replicationlatency
is a function of disk queuing points in the replication engine as well as process and
communicationqueuesandtheirserviceintervals.
Recovery time, which is the focus of RTO (recovery time objective), depends upon the
configuration of the backup system. If the backup is a cold standby whose databases are kept
current via data replication, bringing the databases into a state of consistency, loading the
applications, connecting them to the databases, and testing the system before putting it into
operationcantakehours insomeinstances.If thestandbysystem isalreadyupandrunning,this
time can be shortened to perhaps minutes. Using an active/active configuration can reduce
recoverytimetoseconds.
Rule29:Youcanhavehighavailability,fastperformance,orlowcost.Pickanytwo.
The design of an active/active system brings with it many choices that lead to compromises.
Virtually any decision that will increase availability, increase performance, or decrease cost will
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

adversely affect the other attributes. A good example is the disk farm. Consider a four-node
active/active system. If each node has its own complete mirrored database, this is the ultimate in
availability. Performance will also be enhanced because all applications have local access to the
database.However,thecostoftheduplicateddiskfarmsmaybeexcessive.
If each node instead has an unmirrored database, availability will suffer; but cost will be
significantlyreduced, and performance will be unaffected. If we further restrict the system so that
only two of the nodes have a copy of the database, and if each of those copies is unmirrored,
cost will be further reduced at the expense of lower availability and lower performance (some
nodeswillnowhavetoreachacrossthenetworktogettoadatabasecopy).
Rule30:Asystemthatisdownhaszeroperformance,anditscostmaybeincalculable.
Availability remains the dominant factor in many critical applications. Remember that a system
that is down has zero performance; and the cost of that downtime may be untenable when
measured not only in dollars but in customer retention, publicity, regulatory infractions, and
perhapseventhelossoflifeorproperty.Thecostofanactive/activesystem isdeterminednotby
a cost objective so much as it is determined by the performance and availability requirements of
theapplication.
Referential Integrity
Rule 34: Database changes must be applied to the target database in natural flow order to
maintainreferentialintegrity.
A major factor in system recovery time following a node failure is the recovery of the database.
Thismustbedonebeforeanyapplicationscanberun.
If a replication engine that ensures data integrity of the target database is used, this means that
the only database recovery operations that need to be applied are the backing out of incomplete
transactions. Database integrity implies that all related transactions must be applied to the target
databaseinthesameorderthattheywereappliedtothesourcedatabase.
Any replication technique that does not guarantee referential integrity can lead to a corrupted
database. In this case, the database must be repaired, which can take a time that is very much
longerthanthatrequiredtosimplyrollbackuncompletedtransactions.
Rule 35: A serializing facility that will restore natural flow is required following all data replication
threads and before the target database in order to guarantee that the database will remain
consistentanduncorrupted.
Rule 34 becomes critical in a multithreaded replication engine. Replication engines often use
multiple threads to increase performance. If updates or transactions can be sent down different
threads,theymayarriveatthetargetdatabaseinanorderthatisdifferentfromtheorderinwhich
they were applied to the source database. Therefore, some sort of serialization facility is
necessaryatthetargetdatabasetoreorderupdatessothattheirordermatchesthatofthesource
databaseupdates.
Achieving Century Uptimes
Rule36:Toachieveextremereliabilities,letitfail;butfixitfast.
Thisisthemantraofactive/activesystems.Iftherecoveryfromafailureissofastthatusersdon’t
notice it,then in effectnofailure has occurred. Anactive/activesystem cantypicallyrecover from
a node failure in a time measured in subseconds to seconds. All that is required is that users on
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

the failed node be switched over to a surviving node or that the failed node be removed from the
poolofnodesthatcanaccepttransactions.
Rule 37: All of the purchased capacity in an active/active system is usable. There is no need for
anidlestandbysystem.
Every node in an active/active system is actively processing transactions. There is no spare
system sitting passively by waiting to be put into service following a system failure. Even if spare
capacity is built into the system to handle a failure, that capacity can be put to good use during
peaktimessuchasholidays.
Rule 38: Providing that the nodes in an active/active system are geographically distributed,
disastertolerancecomesforfree.
Adding to Rule 37, if the nodes in an active/active network are sufficiently geographically
distributed, no single disaster will bring down multiple nodes to cause a total outage. The system
isinherentlydisaster-tolerant.
Rule 39: (NonStop Maxim) - Century uptimes in an active/active system will generally require
fewernodesiffault-tolerantnodesareusedratherthanhigh-availabilitynodes.
In fact, studies have shown that using nodes each with an availability of four nines (such as
NonStop servers) will typically require at least one less node in an active/active network than
using nodes with an availability of three nines (such as industry-standard servers) to achieve a
specifiedlevelofavailabilityandcapacityfollowinganodefailure.
Rule 40: (Darwin’s extension to Murphy’s Law) - Eventually, a disaster will befall every
enterprise;andonlythosethatarepreparedwillsurvive.
This should be the ultimate wakeup call for all enterprises to protect their IT assets from disaster.
Consider the number of companies that were put out of business by the 9/11 terrorist attacks.
Those that could continue operations even in the face of this disaster survived. Many others
failed.
Disasters as serious as this are just waiting to happen, from natural occurrences such as
earthquakestoman-madedisasterssuchasbiologicalattacksanddirtynuclearbombs.
What’s Next?
In our last part of this series, we will review the final excerpt of our sixty-four Rules of Availability.
Takenasawhole,theyrepresentabodyofbestpracticesforachievinghighavailabilityandeven
continuousavailability.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman
