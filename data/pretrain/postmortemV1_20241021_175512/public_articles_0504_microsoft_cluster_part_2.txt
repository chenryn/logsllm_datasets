Ifanodedoes notrespond,itistakenoutofservice;anditsactiveresourcegroupsaremovedto
otheroperatingnodesaccordingtothepreferencesofeachresourcegroup.
DatabaseManager
The Database Manager runs on each node and maintains the cluster configuration database.
This database contains information on all physical and logical entities in the cluster, such as the
cluster itself, node membership, resource types and descriptions, and resource groups. This
informationisusedtotrackthecurrentstateoftheclusterandtodetermineitsdesiredstate.
The Database Managers cooperate to ensure that a consistent view of the cluster is maintained
at each node. The Database Manager on the node making a configuration change initiates the
replicationofitsupdatetotheothernodes.Replicationisatomicandserialandusesaone-phase
commit.Ifanodecannotmakeanupdate,itistakenoutofservice.
Changesarealsowrittentothequorumresourceasalogfornode-recoverypurposes.
FailoverManager
The Failover Managers, which run on each node, work together to arbitrate ownership of
resource groups following a component failure. They are responsible for initiating failover of
resourcegroups andfor starting andstoppingresources accordingtothedependencies specified
byeachresourcegroup.
If a resource fails, the Failover Manager in that node might tryto stop and restart the resource. If
this doesn’t correct the problem, the Failover Manager stops the resource, which will trigger a
failover of the failed resource group to another node. In the event of a resource-group move, the
FailoverManagerwillupdatetheconfigurationdatabaseviatheDatabaseManager.
Failover maybe triggered inresponsetoanunplannedhardwareor applicationfault,or itmaybe
triggeredmanuallybytheclusteradministratorsothatanodecanbeupgraded.Inthelattercase,
the shutdown is orderly. If failover is triggered by a component failure, shutdown can be sudden
and disruptive. Should this happen, extra steps are required to evaluate the state of the cluster
and the integrityof the application database before the failed resource groups can be returned to
serviceonasurvivingnode.
When a node is returned to service and rejoins the cluster, the Failover Manager manages the
failback of resource groups. It decides which resource groups to move to the recovered node
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

based on preferences. Moving resource groups to a recovered node can be restricted to certain
hourstopreventmassmovementsduringpeakactivitytimes.
The Failover Manager is also responsible for the backup and restoration of the quorum logs and
othercriticalfiles.
Multisite Clusters
Thoughhigh-availabilityclustersreducetheimpactofsingle-componentfailures,theclusterisstill
vulnerable to site-location disasters such as fires and floods. The only protection against site
disasters is to have another cluster located far enough away that it is unlikely that any one
disaster willaffectbothclustersites.Thiscanbeachieved withgeographicallydispersedmultisite
clusters,3 in which interconnected clusters are located at two or more geographically-separate
sites.
Inamultisiteconfiguration,datareplicationmustbeusedtoensurethatbothsites haveanup-to-
date view of all files and databases. Data disks maybe optionallymirrored either asynchronously
or synchronously, though the applications must be able to deal with some data loss if
asynchronous replication is used.However, the quorum disk mustbereplicatedsynchronouslyto
ensureaconsistentviewofthedistributedcluster atanypointintime.Ifasynchronousreplication
of application databases is used, distance is not a problem since the response time of a quorum
update does not directlyaffect the performance of the applications. Clusters can be separated by
hundreds of miles. If the application databases are synchronously replicated, the distance
separatingthesitesislimited.
private
site1
network site2
node1 node2 node3 node4
storage storage
controller controller
1 2
disk mirror disk
1 2
disk mirror disk
3 4
Replication may either be software-based at the host level or hardware-based at the SAN
controller level.However, if SAN block replicationis used, itmustbeguaranteed thatthe order of
writesispreservedtomaintaintargetdatabaseconsistency.
3GeographicallyDispersedClusters,MicrosoftTechNet;2010.
5
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

TheWSFCclusterserviceandclusteredapplications areunawareofgeographicalseparation.All
cluster functions are performed in the same way no matter where the cluster members are
located. Microsoft does not provide a replication product for multisite clusters. Third-party
products must be used, such as the NeverFail ClusterProtector,4 which provides synchronous
replication,faultdetection,andremote-sitefailoverservices.
Geographically-dispersed cluster configurations supported by Microsoft appear in the Microsoft
HardwareCompatibilityList.
WSFC Enhancements over MSCS
WSFChasbeensignificantlyenhancedoverMSCSinmanyareas.
ClusterAdministration
A major challenge historically with clusters has been the complexity of building, configuring, and
managing clusters. WSFC hides the clustering “nuts and bolts” behind a new GUI interface, the
Failover Cluster Management snap-in for the Microsoft Management Console. Microsoft claims
that a cluster expert is no longer needed to successfully deploy and maintain a cluster. These
functionscannowbeperformedbyanITgeneralist.
The Failover Cluster Management administration tool is task-oriented rather than resource-
oriented and simplifies administration via several new wizards. For instance, with MSCS, in order
to create a highly available file share, the administrator had to create a group, create a disk
resource,create anIPaddress,createa network name,configure heartbeatmessages,establish
a preferred node list, and specify resource dependencies. With WSFC, all the administrator has
todoistospecifyanetworkname.TheHighAvailabilityWizarddoestherest.
With Failover Cluster Management, an administrator can maintain multiple clusters in the
organization.ClusterscanbemanagedremotelyviatheRemoteServerAdministrationTools.
For those experienced cluster administrators who want to further tune the cluster configuration,
the MSCS cluster.exe commands are still available and allow full access to all MSCS
administrative capabilities. However, the cluster.exe commands will be replaced with new
WindowsPowerShellCmdletsinlaterversions.
Scalability
Under MSCS, the maximum number of nodes that could be in a cluster was eight. WSFC has
increasedthislimittosixteenx64nodesinacluster.
Security
Kerberos is now used for user authentication. All communication between nodes is signed and
maybeencrypted.
4NeverFailClusterProtector,http://extranet.neverfailgroup.com/download/DS-cluster-08-09-4page-lo.pdf.
6
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Networks
Microsoft strongly encourages the use of redundant, separate, and distinctly routed networks for
providing fault tolerance in the private network connecting the nodes. If this is not provided,
WSFC will generate a warning message; and the cluster may not be accepted by Microsoft for
support.
Inthecaseofredundantnetworks,thefastestnetworkwillbegivenpriorityforinternaltraffic.
Under MSCS, the maximum allowed latency over the private network was 500 milliseconds. This
was due to heartbeat limitations. Heartbeat intervals were not configurable. In WSFC, heartbeat
parameters are configurable; and the latency restriction has been removed. In addition, rather
than broadcasting heartbeats, WSFC now uses TCP/IP connections to improve heartbeat
reliability.IPv6aswellasIPv4issupported.
Under MSCS, the cluster members at both sites in a geographically-dispersed multisite cluster
had to be on the same subnet. This meant that the private network interconnecting the two sites
had to be a VLAN (virtual LAN) stretched over a WAN communications link. This restriction has
been removed by WSFC. The cluster members at each site can now be on different subnets
connectedbyasimple(redundant)WANlink.NoVLANneedstobecreated.
Hyper-VIntegration
WSFC is integrated with Microsoft’s Hyper-V virtualization services. Any node in the cluster may
host virtual machines, and virtual machines may be failed over individually or en masse. Live
migration of virtual machines commanded bythe administrator occurs within milliseconds with no
perceiveddowntimeandwithnolostconnections.
Validation
The Validate a Configuration Wizard can be run to ensure that a cluster configuration will be
supported by Microsoft. It validates all hardware components against Microsoft’s Hardware
Compatibility List and validates the cluster configuration. It is useful not only when a cluster is
created,butitcanalsobeusedtoperiodicallyvalidatetheclusterconfiguration.
RollingUpgrades
Nodes may be upgraded by removing them one at a time from the cluster, upgrading them, and
then returning them to the cluster. However, migration from MSCS clusters is not specifically
supported.TheMigrationWizardisavailabletohelpthesemigrations.
Summary
The WSFC cluster service monitors cluster health and automatically moves applications from a
failed node to surviving nodes, bringing high availabilityto critical applications.WSFC also brings
highavailabilitytoMicrosoft’sHyper-Vvirtualizationservices.
WSFC brings many enhancements to the stalwart MSCS clustering services. With WSFC, up to
sixteen Windows servers can be organized into a multisite, geographically-dispersed cluster with
cluster sites separated by hundreds of miles. A convenient GUI administrator tool supported by
severalwizardsremovestheneedforaclusterspecialisttoconfigureandmanagethecluster.
WSFC makes cluster technology even more attractive to small businesses and large enterprises
alike.
7
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
