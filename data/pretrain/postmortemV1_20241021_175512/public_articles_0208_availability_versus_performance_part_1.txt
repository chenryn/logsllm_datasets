Availability versus Performance
August2007
Youcanhavehighavailability,fastperformance,orlowcost.Pickanytwo.
Thus says Rule 29 of Breaking the Availability Barrier.1 Accordingly, if you want to improve
systemavailability,eitherperformanceorcost(orboth)isgoingtosuffer.
There are many ways available today to improve system availability if one is willing to spend
more money. However, are there also ways in which we can trade higher availability for less
performanceratherthanforhighercost?Therecertainlyare,andmanyofthesetechniquesarein
usetodayinhigh-availabilitysystems.Furthermore,thereismuchresearchgoingoninthisarea.
In the following article, we first summarize some of the availability/cost tradeoffs available today.
Wethenfocusonthelessvisibleavailability/performancetradeoffs.
Trading Availability for Cost
MultinodeArchitectures
We have written extensively about the use of active/active systems2 and clusters3 to improve
availability. These architectures use two or more independent processing nodes to back each
otherupintheeventofafailureofoneoftheprocessingnodesintheapplicationnetwork.
Node failures will happen, but these architectures strive to achieve high availability by reducing
the time that users are denied service following a failure. Multinode architectures all improve
application availability by reducing recovery time following a node failure – seconds for
active/activesystemsandminutesforclusters.4
Such systems improve availability at the penalty of extra cost. Rather than purchasing just one
data processing system, two or more systems must be purchased and linked together as nodes
in an application network. The additional cost is usually justified by a very high cost of downtime
measuredintermsofdollars,thelossofcustomers,orthelossoflifeand/orproperty.
1
Highleyman, W. H., Holenstein, P.J., Holenstein, B. D., Breaking the Availability Barrier: Survivable Systems for
EnterpriseComputing,AuthorHouse;2004.
2
WhatisActive/Active?,AvailabilityDigest;October,2006.
3
Active/ActiveversusClusters,AvailabilityDigest;May,2007.
4
Anadditionalmultiplenodearchitecture–active/standbypairs–alsoprotectsagainstthefailureoftheactivenodebut
withrecoverytimesmeasuredtypicallyinhours.Anactive/standbypairiscommonlyusedtoprovidecontinuationofdata
processingservicesintheeventofanonrecoverablefailureoftheprimarysystemduetoadisasterofsomesort.
1
©2007SombersAssociates,Inc.,andW.H.Highleyman

Single-NodeAvailability
But what about single systems? Are there tradeoffs in these systems that would improve
availabilityforcost?
Yes, of course. There are hardware techniques for improving the availability of a single node,
rangingfrom dualpower supplies to RAID arrays for datastorage. Attheextreme,afault-tolerant
system such as that commercially available from HP (their NonStop systems) or Stratus (their
ftServers) can be used. All of these solutions for improving single-system high availability come
withadditionalcosts.Systemcostisbeingtradedforhigheravailability.
Trading Availability for Performance
Acommontechnique for improving availabilityat the expenseof performanceis simplyto run the
processors at a slower rate to reduce processor errors. This is an example of a hardware
approachfortradingperformanceforavailability.
The availability of a single system, whatever its structure, can also be greatly affected by the
software architecture. There are many software techniques for improving availability, but most
haveanegativeimpactonperformance.
Are these techniques worthwhile? Should we be degrading performance simply so that we can
achievehigheravailability?
Most software systems today, whether they be applications, operating systems, or database
systems, are optimized for performance. This performance optimization is augmented by the
performance improvements of the underlying hardware. Hardware improvements have been
approximatelyfollowingMoore’slaw–doublingeveryeighteenmonths.5
While performance has been following this rapid improvement, system availability has remained
fairly static. Short of fault-tolerant systems and application-network architectures such as
active/active systems and clusters, single-system availability has increased by a factor of about
10 (from two 9s to three 9s) over the last 40 years and has leveled off over the last decade.
Meanwhile, performance has increased by a factor of thousands and continues to improve. As
the need for higher availability grows due to the 24x7 requirements for IT systems, maybe it is
time to start trading single-system performance for availability. This can be done most readily at
the software level by using techniques that contain or reduce failure rates and that either
decreasesystemrecoverytimeorincreasethetimebetweensoftwarefaults.
Many of these high-availability software techniques are, in fact, in use in fault-tolerant systems
today. But they have not made it into the class of industry standard servers which are the
systems inmostcommonuse.Welook atseveral examples of software techniques for improving
availability. While hardly an exhaustive list, these examples demonstrate that the quest for high
performancehasoftenbeenattheexpenseofavailability.
DatabaseRecovery
Typically, when a row or record is updated in the database of a typical modern-day server, the
change is made in cache. There it sits until at some later time it is flushed to disk via a least-
recently-used or equivalent algorithm.Shouldthesystem fail,updates still incachenever makeit
to disk. The result is that the database is left in an inconsistent state. Record updates are
missing.Indexblocksmaynotbeupdated.Childrecordsorrowsmayhavenoparents.
5
Thisisthecommonquote.GordonMooreactuallysaidthattransistordensitywoulddoubleeverytwoyears.
2
©2007SombersAssociates,Inc.,andW.H.Highleyman

Before the system can be returned to operation, the database must be recovered to a consistent
state, typically by running fsck in a UNIX environment or chkdsk in a Windows environment. If a
large database is severely corrupted, this could take hours if it is even possible. In some cases,
the database may have to be reconstructed from backup tapes, which results in the loss of all
dataupdatessincethelastbackup.Inanyevent,recoverytimecanbeextensive.
This extensive recovery time can have a significant impact on availability. Consider an industry-
standard server that has an MTBF of 4,000 hours (about six months). We include failures from
any cause – hardware, operating system, applications, environmental (air conditioning, power,
etc.),andoperatorerrors.
Industry standard servers typically exhibit measured availabilities of three 9s (99.9%). If such a
server fails on the average every 4,000 hours, this means that its average recovery time is four
hours (availability = uptime/(uptime+downtime) = 4,000/4,004 = .999). If the server supports a
large database that requires on the average an hour to be recovered, then database recovery
represents25%ofalldowntime.
Clearly,thisisafruitfulareaforimprovedrecoverytimeandthereforehigheravailability.
CacheWrite-Thru
Onesolutiontothedatabaserecoveryproblemistowriteupdatestodiskjustassoonastheyare
made to cache (cache write-thru). This, of course, will really slow down a system unless it is a
write-seldom, read-often application since disk writes are orders of magnitude slower than
memory writes. However, database corruption following a failure is significantly reduced. Instead
of losing all of the updates that are still in cache (which might be the last several minutes of
updates),onlythe updates in progress at thetimeof thefailuremaybe lost.Theresult is a much
slower system with a much faster recovery time. By using cache write-thru, recovery time (and
thereforeavailability)canbesignificantlyimprovedattheexpenseofperformance.
Unfortunately, the statement that cache write-thru “will reallyslow the system down” is perhaps a
gross understatement in many instances. Consider a system in which performance is governed
by disk activity and in which a high cache hit rate is achieved (this is typical of enterprise
computing systems). This means that only a small portion of disk accesses have to wait for the
physical disk. If cache write-thru is used, then 100% of all updates must bemade directlyto disk.
If the application is write-intensive, performance may be decreased by a factor of two or three or
more.
The decrease in performance is so drastic that cache write-thru is generally not used. However,
for write-seldom, read-often applications, in which the performance penalty is acceptable, cache
write-thru can significantly improve availability at the cost of some performance by significantly
reducingthesystemrecoverytimefollowingafailure.
Log-BasedRecovery
An alternative to cache write-thru is the use of log-based recovery. With this technique, all
changes tothedatabasearewrittentoalog,whichisimmediatelywrittentodisk.Sincethelogis
serial in nature, its writes are quite fast compared to the random writes of database updates. The
actualrandomdataisthenwrittenfromcachewhenitisconvenientforthesystemtodoso.
Log-basedrecoveryiscommonlyusedintransaction-processingsystems,butitcanbeemployed
evenifupdatesarenotappliedasapartofcommittedtransactions.Basically,alogofallchanges
is written to disk. The log maybe a transaction log created bya transaction monitor, or it maybe
achangelogcreatedbytheapplicationorbydatabasetriggers.
3
©2007SombersAssociates,Inc.,andW.H.Highleyman

Should the system fail, certain transactions (or updates) will not have been completed. The log is
used to roll back uncompleted transactions or updates from the database and to roll forward
completedtransactionsorupdatesthathavenotyetmadeittothedatabase.
Log-based recovery imposes a much smaller penalty (the creation of the log) on the running
application while costing somewhat more in recoverytime (the replaying of the log) as compared
tocachewrite-thru.Asaresult,itisagoodcompromiseapproachtoimproverecoverytimeatthe
expenseofperformance.Log-basedrecoveryisincommonusagetoday,andanyapplicationthat
must have high availability and that cannot use cache write-thru because of its severe
performancepenaltyshouldbeimplementedtouselog-basedrecovery.
SharedMemory
A common way to improve performance in single systems is to provide communication between
processing modules via shared memory. Since the modules share the same memoryspace, one
communicates with the other by simply changing memory-resident variables, the fastest way to
passinformation.
However, this means that an errant process can corrupt memory and can bring down the entire
system. What may also occur is that erroneous processing may have corrupted the database or
mayhavesenterroneousinformationtotheusersortoothersystems.
