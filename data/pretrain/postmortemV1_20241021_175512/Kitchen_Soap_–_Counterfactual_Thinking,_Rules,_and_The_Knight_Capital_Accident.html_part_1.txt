# Kitchen Soap[Thoughts on systems safety, software operations, and sociotechnical systems.]

#  Counterfactual Thinking, Rules, and The Knight Capital Accident

In between reading copious amounts of indignation surrounding whatever
is suboptimal about healthcare.gov, you may or may not have noticed the
SEC statement regarding the Knight Capital accident that took place in
2012.

This [Release No.
70694](http://www.sec.gov/litigation/admin/2013/34-70694.pdf) is a
document that contains many details about the accident, and you can read
what looks like on the surface to be an in-depth analysis of what went
wrong and how best to prevent such an accident from happening in the
future.

***You may believe this document can serve as a 'post-mortem' narrative.
It cannot, and [should not.]***

Any 'after-action' or 'postmortem' document (in my domain of web
operations and engineering) has two main goals:

1.  To provide an **explanation** of how an event happened, as the
    organization (including those closest to the work) best understands
    it.
2.  To produce artifacts (recommendations, remediations, etc.) aimed at
    both **prevention** and the improvement of detection and response
    approaches to aid in handling similar events in the future.

You need #1 in order to work on #2. If you don't understand how the
event unfolded, you can't make gains towards prevention in the future.

The purpose of this post is to outline how the release is not something
that can or should be used for explanation or prevention.

The Release No. 70694 document does not address either of those concerns
in any meaningful way.

What it *does* address, however, is exactly what a regulatory body is
tasked to do in the wake of a known outcome: contrast how an
organization was or was not in compliance with the rules that the body
has put in place. Nothing more, nothing less. In this area, the document
is concise and focused.

You can be forgiven for thinking that the document could serve as an
explanation, because you can find some technical details in it. It looks
a little bit like a timeline. What is interesting is not what details
are covered, but what details are *not *covered, including the
organizational
[sensemaking](https://en.wikipedia.org/wiki/Sensemaking "Sensemaking"){target="_blank"
rel="noopener"} that is part of every complex systems failure.

If you are looking for a real postmortem of the Knight Capital accident
in *this* post, you're going to be disappointed. At the end of this
post, I will certainly attempt to list some questions that I might pose
if I was facilitating a debriefing of the event, but no real
investigation can happen without the individuals closest to the work
involved in the discussion.

However, I'd like to write up a bit about why it should **not** be
viewed as what is traditionally known (at least in the web operations
and engineering community) as a postmortem report. Because frankly I
think that is more important than the specific event itself.

But before I do that, it's necessary to unpack a few concepts related to
learning in a retrospective way, as in a postmortem...

## **Counterfactuals**

Learning from events in the past (both successful and unsuccessful) puts
us into a funny position as humans. In a process that is genuinely
interested in learning from events, we have to rectify our *need to
understand* with the reality that we will never get a complete picture
of what has happened in the past. Regulatory bodies such as the SEC
(lucky for them) don't have to get a complete picture in order to do
their job. They have only to point out the gap between how "work is
prescribed" versus "work is being done" (or what [Richard Cook has
said](http://www.youtube.com/watch?v=2S0k12uZR14)  "the system as
imagined" versus "the system as found.")

In many circumstances (as in the case of the SEC release), what this
means is to point out the things that people and organizations
**didn't** do in the time preceding an event. This is usually done by
using "counterfactuals", which means literally "counter the facts."

In the language of my domain, using counterfactuals in the process of
explanation and prevention is an *anti-pattern, *and I'll explain why.

One of the potential pitfalls of postmortem reports (and debriefings) is
that the language we use can cloud our opportunities to learn what took
place and the context people (and machines!) found themselves in.
[Sidney
Dekker](http://sidneydekker.com/ "Sidney Dekkers"){target="_blank"
rel="noopener"} says this about using counterfactuals:

> "They make you spend your time talking about a reality that did not
> happen (but if it had happened, the mishap would not have happened)."
> (Dekker, 2006, p. 39)

What are examples of counterfactuals? In ordinary language, they look
like:

-   "they shouldn't have..."
-   "they could have..."
-   "they failed to..."
-   "if only they had...!"

Why are these statements woefully inappropriate for aiding explanation
of what happened? Because stating what you think should have happened
doesn't explain people's (or an organization's) behavior.
Counterfactuals serve as a massive distraction, because it brings
sharply into focus what didn't happen, when what is required for
explanation is to understand *why people did what they did. *

People do what makes sense to them, given their focus, their goals, and
what they perceive to be their environment. This is known as the *local
rationality principle,* and it is required in order to tease out [second
stories, which in turn is required for learning from
failure](http://www.kitchensoap.com/2013/09/30/learning-from-failure-at-etsy/ "Second Stories"){target="_blank"
rel="noopener"}. People's local rationality is influenced by many
dynamics, and I can imagine some of these things might feel familiar to
any engineers who operate in high-tempo organizations:

-   <div>

    **Multiple conflicting goals**

    </div>

    -   <div>

        E.g., "Deploy the new stuff, and do it quickly because our
        competitors may beat us! Also: take care of all of the details
        while you do it quickly, because one small mistake could make
        for a big deal!"

        </div>

-   <div>

    **Multiple targets of attention**

    </div>

    -   <div>

        E.g., "When you deploy the new stuff, make sure you're looking
        at the logs. And ignore the errors that are normally there, so
        you can focus on the right ones to pay attention to. Oh, and the
        dashboard graph of errors...pay attention to that. And the
        deployment process. And the system resources on each node as you
        deploy to them. And the network bandwidth. Also: remember, we
        have to get this done quickly."

        </div>

</div>

<div>

David Woods put counterfactual thinking in context with how people
actually work:

> *"After-the-fact, based on knowledge of outcome, outsiders can
> identify "critical" decisions and actions that, if different, would
> have averted the negative outcome. Since these "critical" points are
> so clear to you with the benefit of hindsight, you could be tempted to
> think they should have been equally clear and obvious to the people
> involved in the incident. These people's failure to see what is
> obvious now to you seems inexplicable and therefore irrational or even
> perverse. In fact, what seems to be irrational behavior in hindsight
> turns out to be quite reasonable from the point of view of the demands
> practitioners face and the resources they can bring bear." (Woods,
> 2010)\
> *

Dekker concurs:

> *"You construct a referent world from outside the accident sequence,
> based on data you now have access to, based on facts you now know to
> be true. The problem is that these after-the-fact-worlds may have very
> little relevance to the circumstances of the accident sequence. They
> do not explain the observed behavior. You have substituted your own
> world for the one that surrounded the people in question." (Dekker,
> 2004, p.33)*
>
> "Saying what people failed to do, or implying what they could or
> should have done to prevent the mishap, has no role in understanding
> human error."  (Dekker, 2004, p.43)

The engineers and managers at Knight Capital did not set out that
morning of August 1, 2012 to lose \$460 million. If they did, we'd be
talking about sabotage and not human error. They did, however, set out
to perform some work successfully (in this case, roll out what they
needed to participate in the Retail Liquidity Program.)

If you haven't picked up on it already, the use of counterfactuals is a
manifestation of one of the most studied cognitive bias in modern
psychology: [The Hindsight
Bias](http://en.wikipedia.org/wiki/Hindsight_bias). I will leave it as
an exercise to the reader to dig into that.

## Outcome Bias

Cognitive biases are the greatest pitfalls in explaining surprising
outcomes. The weird cousin of The Hindsight Bias is [Outcome
Bias](http://en.wikipedia.org/wiki/Outcome_bias "The Outcome Bias"){target="_blank"
rel="noopener"}. In a nutshell, it says that we are biased to "judge a
past decision by its ultimate outcome instead of based on the quality of
the decision at the time it was made, given what was known at that
time." (Outcome Bias, 2013)

In other words, we can be tricked into thinking that if the result of an
accident is truly awful (like people dying, something crashing, or, say,
losing \$460 million in 20 minutes) then the decisions that led up to
that outcome must have been *reeeeeealllllllyyyy* bad. Right?

This is a myth debunked by a few decades of social science, but it
remains persistent. No decision maker has omniscience about results, so
the severity of the outcome cannot be seen to be proportional to the
quality of thought that went into the decisions or actions that led up
to the result. Why we have this bias to begin with is yet another topic
that we can explore another time.

But a possible indication that you are susceptible to The Outcome Bias
is a quick thought exercise on results: if Knight Capital lost only
\$1,000 (or less) would you think them to be more or less prudent in
their preventative measures than in the case of \$460 million?

If you're into sports, maybe
[this](http://measureofdoubt.com/2011/11/16/coach-smiths-gutsy-call/ "Coach Smith's Gutsy Call"){target="_blank"
rel="noopener"} can help shed light on The Outcome Bias.

## **Procedures **

Operators (within complex systems, at least) have procedures and rules
to help them achieve their goals safely. They come in many forms:
checklists, guidelines, playbooks, laws, etc. There is a distinction
between procedures and rules, but they have similarities when it comes
to gaining understanding of the past.

First let's talk about procedures. In the aftermath of an accident, we
can (and will, in the SEC release) see many calls for "they didn't
follow procedures!" or "they didn't *even have* a checklist!" This sort
of statement can nicely serve as a counterfactual.

What is important to recognize is that procedures are but
only **one** resource people use to do work. If we only worked by
following every rule and procedure we've written for ourselves, by the
letter, then I suspect society would come to a halt. As an aside,
["work-to-rule"](http://en.wikipedia.org/wiki/Work-to-rule "Work To Rule"){target="_blank"
rel="noopener"} is a tactic that labor organizations have used to
demonstrate the issues that onerous rules and procedures can rob people
of their adaptive capacities, and therefore bring business to an
effective standstill.

Some more thought exercises to think with on procedures:

-   How easy might it be to go to your corporate wiki or intranet to
    find a procedure (or a step within a procedure) that was once
    relevant, but no longer is?
-   Do you think you can find a procedure somewhere in your group that
    isn't specific enough to address every context you might use it in?
-   Can you find steps in existing procedures that feel safe to skip,
    especially in if you're under time pressure to get something done?
-   Part of the legal terms of using Microsoft Office is that you **read
    and understand the End User License Agreement**. You did that before
    checking "I agree", right? Or did you *violate that legal
    agreement?!* (don't worry, I won't tell anyone)

Procedures are important for a number of reasons. They serve as
institutional knowledge and guidelines for safe work. But, like wikis,
they make sense to the authors of the procedure the day they wrote it.
They are written to take into account all of the scenarios and contexts
that the author can imagine.

But since that imagination is limited, many procedures that are thought
to ensure safety are context-sensitive and they require interpretation,
modification, and adaptation.

There are multiple issues with procedures as they are navigated by
people who do real work. Stealing from Dekker again:

1.  "First, a mismatch between procedures and practice is not unique to
    accident sequences. Not following procedures does not necessarily
    lead to trouble, and safe outcomes may be preceded by just as
    (relatively) many procedural deviations as those that precede
    accidents (Woods et al., 1994; Snook, 2000) This turns any
    "findings" about accidents being preceded by procedural violation
    into mere
    tautologies..."[ ]{style="font-family: AdvTimes; font-size: medium;"}
2.  "Second, real work takes place in a context of limited resources and
    multiple goals and pressures." [\
    ]{style="font-family: AdvTimes; font-size: medium;"}
3.  "Third, some of the safest complex, dynamic work not only occurs
    despite the procedures--such as aircraft line maintenance--but
    without procedures altogether." The long-studied High Reliability
    Organizations have examples (in domains such as naval aircraft
    carrier operations and nuclear power generation) where procedures
    are eschewed, and instead replaced by less static forms of learning
    from practice:

    > ''there were no books on the integration of this new hardware into
    > existing routines and no other place to practice it but at sea.
    > Moreover, little of the process was written down, so that the ship
    > in operation is the only reliable manual''. Work is ''neither
    > standardized across ships nor, in fact, written down
    > systematically and formally anywhere''. Yet naval air- craft
    > carriers--with inherent high-risk operations--have a remarkable
    > safety record, like other so-called high reliability organizations
    > (Rochlin et al., 1987; Weick, 1990; Rochlin, 1999). "
4.  "Fourth, procedure-following can be antithetical to safety."
     -- Consider the case of the 1949 US Mann Gulch disaster
    where firefighters who perished were the ones sticking to the
    organizational mandate to carry their tools everywhere. Or Swissair
    Flight 111, when captain and co-pilot of an aircraft disagreed
    on [whether or not to follow the prescribed
    checklist](http://online.wsj.com/article/SB913760693252632000.html) for
