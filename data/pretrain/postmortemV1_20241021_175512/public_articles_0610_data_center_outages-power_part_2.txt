1998,ithasgrownatarateinexcessof50%per year.
Rackspace is very conservative in its power management. It uses triplexed power sources –two
independentpowersourcesandadieselgeneratortoprovidepowerduringaswitchover.
In the early evening of Monday, November 12, 2007, at 6:30 PM, Rackspace suddenly lost power to its
Dallasdatacenter.UnbeknownsttoRackspace,atruckerhadpassedoutandrammedatransformerthat
fedthedatacenter.Thetransformerexploded,andthedatacenterwentblack.6
As planned, Rackspace’s emergency diesel backup generator kicked in; and the data center came back
tolifeandcontinuedinoperation withbutabrief interruption.This allowedRackspaceoperators toswitch
to their secondary power source – a completelyseparate utility line feeding the building. At this point, the
emergency generator had done its job and was shut down. Triple modular power redundancy had paid
off.
However, fifteen minutes later, the secondary power source shut down. This time, the blackout was
requested by the emergencypersonnel trying to free the trapped truck driver so as to avoid electrocution
of not only the truck driver but also the emergency workers. Things were happening so fast at the scene
of the accident that Rackspace was not notified by the electric utility of the intent to shut off the data-
centerpower.
Again, the emergencygenerator started up and continued to power the data center. The diesel generator
was designed to power the data center indefinitely (so long as fuel was available), and the data center
wasonceagainoperationalwithlittleimpactonthehostedwebsites.
But a serious and unanticipated problem became apparent. With each interruption in power, the air-
conditioning chillers had to recycle. It would take them about a half hour to recycle before they were
effectively cooling the data center again. The chillers were down for about fifteen minutes as a result of
the first power outage, and they would have been back on line in another fifteen minutes, a delay
accounted for in the data-center design. However, with the second interruption in power, the chillers had
toonceagainrecycle.
With thousands of powered servers pumping out heat, the temperature in the data center was rapidly
climbing to a dangerous level. Management realized that this extended time without air conditioning
would cause the servers to overheat and could cause significant damage to the hardware. Therefore,
management reluctantly decided to shut down all of the servers in the data center to protect them. The
Dallasdatacenterwasnowcompletelynonoperational.
Oncepowerandcoolingwererestored,allofthethousandsofservershadtoberestoredtoservice.Most
ofthewebsiteswereupbythefollowingday,Tuesday.However,theyhadbeendownforhours.
The Rackspace failure could have been avoided by proper disaster planning. Nowhere, evidently, in
Rackspace’s business-continuity planning was the concept of data-center redundancy. The N+1
redundancy that Rackspace had built into its emergency UPS (uninterruptible power supply system) and
even into its HVAC (heating, ventilation, and air conditioning) system and whatever redundancy that it
hadinitsserverfarmscametonaughtwhentheentiredatacenterwastakendown.
6Rackspace–AnotherHostingServiceBitestheDust,AvailabilityDigest;December2007.
http://www.availabilitydigest.com/public_articles/0212/rackspace.pdf
4
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Triple Redundancy Failure on the Space Station
In June, 2007, a triply-redundant attitude and environmental-control computer provided by Russia failed
ontheInternationalSpace Station(ISS).7Hadthis beenamissiontoMars,itwouldhavebeenfatal.Only
thespacestation’s proximitytoEarth,whichputitinrangeof supportandresupplymissions,preventeda
tragedy.Thoughtheproblemwascircumventedinafewdays bythespace-stationcrew,ittook weeksfor
thestationcrewandgroundengineerstodeterminethesourceoftheproblem.
The crew quickly determined that the failure was caused by the simultanous loss of power to all three
computers. Power had been shut off by a surge-protection unit designed to protect the computers from
power surges beyond the capabilities of their own power filters. A NASA internal technical report
describing this failure said, “On 13 June, a complete shutdown of secondary power to all (three) central
computer and terminal computer channels occurred, resulting in the loss of capability to control ISS
Russiansegmentsystems.”
RussianofficialswerequicktoblameNASAfor“zappingtheircomputers”with“dirty”28-voltpowerfroma
newly-installed solar array. This was the first of many bad guesses by top Russian program managers
andwoulddistractengineerstryingtogettotherealsourceoftheproblem.
Inthemeantime,thecomputers hadto befixed – and fast.Thestationcrew assumedthatsomeexternal
interference such as noise in the 28-volt power supply was responsible for generating false commands
inside the computers’ power-monitoring system and caused it to send shut-down commands to all three
computers. Based on this reasoning, the crew bypassed the power monitoring system to two of the
computers by using jumper cables. These two computers were now subject to damage bypower surges,
butbynowthepowersystemhadsettledintoasteadystateandwasgeneratingcleanpower.
The astronauts spent their time disassembling the power control boxes and the associated cabling in
order tolook forclues thatmightleadtothecauseof failure.Thoughmultiplescopes andprobes failedto
findtheproblem,theireyesandfingersdid.
What they discovered was that the connection pins from the power-monitoring unit were wet and
corroded. Continuity checks showed that the command lines in the cable coming off the unit had failed.
Even worse, one of the command lines had shorted. It was the power-off command line that went to all
three computers. The shorted condition created the disastrous power-off command. The jumper cables
had bypassed the false power-off command and had allowed the computers to function properly once
again.
But what had caused the corrosion? Water condensation. The problem was traced to a malfunctioning
dehumidifier dripping condensate water. The situation was aggravated by a stream of cold air from
another location on the dehumidifier that at times cooled the cables below the dew point at which
moisture could condense. As temporaryfurther protection, the crew rigged a thermal barrier between the
computers and the dehumidifier. The thermal barrier was built using a surplus reference manual and
ordinarygraytape.
Oncethe problem was understood, itbecameclear thatthesystem sufferedfromafataldesignflaw.The
supposedly triply-redundant design included a single point of failure – the external power monitoring unit
that, by itself, could turn off all of the computers. Should it fail (as it did due to condensation), the triply-
redundantsystemwasdown.
7TripleRedundancyFailureontheSpaceStation,AvailabilityDigest;November2007.
http://www.availabilitydigest.com/public_articles/0211/iss_tmr_failure.pdf
5
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

And Then There Was the Great Northeast Blackout
On August 14, 2003, much of the Northeast United States and neighboring Canada lost power. It would
beseveraldaysbeforepowerwasfullyrestored.8
Howdidsuchadisasterhappen?Throughachainofeventsthatstartedwithatree.
Power lines sag in hot weather. They also sag due to the heat generated by the electrical current that
they are carrying. A high-voltage transmission line can blast a tree to its roots. This takes a tremendous
amountof power andinstantlyoverloads thetransmissionline.Therefore,itis imperative that trees under
thesetransmissionlinesbekepttrimmedsothattheywillnotcomeincontactwiththetransmissionlines.
Thepolicyof FirstEnergyin Ohio was totrim trees everyfive years.However,theydidnotalways stick to
this schedule; and the result was that some trees under its transmission lines had grown too tall. August
14th was a hot day, pushing 90 degrees Fahrenheit in the Ohio area. Air conditioners and fans were
imposing heavy demand on generating capacity. Between the heat of the day and the heat generated by
the electrical current, transmission lines were seriously sagging. One apparently zapped a tree that was
too tall. This caused the transmission line to shut down, putting more load on the remaining transmission
lines.
These transmission lines sagged even further due to the increased load. Over the next two hours, two
moretransmissionlinesweretakenoutofservicebytreecontact.Problemsrapidlyincreaseduntilagiant
power surge took down the entire Northeast electric grid. 508 generating stations at 256 power plants,
including22nuclearpower plantsintheU.S.andCanada,wentoffline.Over40millionpeopleintheU.S.
andCanadawerewithoutpower.Thefinancialimpactranintobillions.Itwasn’tuntilthenexteveningthat
partialpowerwasrestored.
Wherewerethe operators duringthis time?Itturns outthattheGEmonitoring system inthe Ohiocontrol
room had failed due to a software bug, and the operators were blissfully unaware of the problem. GE
Energysubsequentlysentapatchcorrectingthisbugtoallofitscustomersaroundtheworld.
Summary
None of these events was foreseeable. This speaks to the needs of a good Business Continuity Plan.
During the Risk Assessment phase, it is perhaps not so useful from an IT perspective to try to identify all
of the events for which strategies must be planned.9 Rather, the Risk Assessment should focus on the
results of the events. It doesn’t make anydifference whether a fire or a flood or an explosion takes down
a data center. What is important is that the data center is down. How will business continue if it loses its
datacenter,nomatterhowthathappened?
Inournextpartofthisseries,wewilllook atnetwork problemsthatimpactedIT operations.Again, we will
findthatmanyofthesecouldnothavebeenimagined.
8TheGreat2003NortheastBlackoutandthe$6BillionSoftwareBug,AvailabilityDigest;March2007.
http://www.availabilitydigest.com/private/0203/northeast_blackout.pdf
9Ofcourse,otheraspectsoftheBCP,suchaspersonnelmanagement,communication,andsoonmaydependuponthenatureof
theevent.
6
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
