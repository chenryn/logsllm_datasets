Lightning Downs Amazon – Not!
September2011
A power failure in the evening of Sunday, August 7, 2011, took down an Availability Zone in Amazon’s
Dublin data center, which houses Amazon’s European region for its Elastic Compute Cloud. Thousands
of users in dozens of European countries found that they had no access to their applications nor to their
data. It was days before service was restored. The power utilityreported that the power loss was caused
by a lightning strike that caused a massive transformer in an electrical substation outside of Dublin,
Ireland,toexplode.
Why should a power failure cause days of havoc? Where were the backup generators? As it turns out,
there were several factors that led to a failure chain totally unanticipated by Amazon. Factors included
hardware faults, software bugs, and human errors. Lightning was not one of them. As is Amazon’s
practice, it was very forthcoming with updates on the status of the outage via its Service Health
Dashboard. However, the complexity of the failure chain was evident in some of the confusion exhibited
byAmazonasittriedtogivearunningcommentaryonthesituation.
Amazon’s Availability Zones
Before delving further into this disaster, let us briefly review Amazon’s Availability Zones, since they
playedaroleinsomeoftheproblemsaswellastheresolutionoftheproblems.1
Regions
Amazon’s Elastic Compute Cloud (EC2) is arguably the leading service today for deploying custom
applications in a cloud environment. Amazon has gone to great lengths to ensure the availability of its
cloud services. It has broken its cloud infrastructure into five geographically dispersed regional data
centers - US East (Virginia), US West (Northern California), EU (Ireland), Asia Pacific (Singapore), and
AsiaPacific(Japan).
AvailabilityZones
Within each region, Amazon provides independent Availability Zones. An Availability Zone (AZ) is a data
center that is independent of all other data centers in the region, though the AZ data centers within a
regionarecollocatedintheregionaldatacenter.TheU.S.-EastRegiondatacenter,forinstance,hasfour
Availability Zones. Should an AZ fail, the others continue uninterrupted. A customer can run a critical
application in multiple Availability Zones within a region to ensure availability. If desired, an application
canalsohavearedundantinstanceinotherregions.
1Amazon’sCloudDownedbyFatFinger,AvailabilityDigest;May2011.
http://www.availabilitydigest.com/public_articles/0605/amazon_ebs.pdf
1
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

ElasticBlockStore(EBS)
Amazon offers two storage services – S3 (Simple Storage Service) and EBS (Elastic Block Store). EBS
offers persistent block storage that can be attached to an EC2 instance. The storage then can be used
like any block storage device, such as a SAN. The contents of each EBS volume are replicated to
multiple backup EBS volumes. An EBS volume can be accessed by only one EC2 instance, and they
bothmustbeinthesameAvailabilityZone.
To provide high availability, a customer can run several copies of an EC2 instance in different Availability
Zones within a region. Amazon will replicate data between the EBS volumes in different AZs. Remote
EC2 instances can also be run in other regions to provide disaster recovery. However, data replication
betweenEBSvolumesistheresponsibilityofthecustomerinthesecases.
SimpleStorageService(S3)
S3, on the other hand, is independent of EC2. It provides Storage as a Service features through web
interfacessuchasSOAPandREST.Itcanbeusedtostoreandretrievedatafrom anywhereontheweb.
Itishugelyscalableandstoresdataredundantlyforhighavailability.
Remirroring
A factor that contributed to Amazon’s problems was a process called remirroring. When an EBS volume
loses connectivity to one of its replica volumes, it searches for a new volume to which it can replicate its
data. This is called remirroring. To do this, the primary volume searches its EBS cluster for another
volume that has enough storage capacity to act as a replica. The transfer of volume data is then initiated
tothenewreplica.
To ensure consistency, access to volumes that are being remirrored are blockeduntil the remirroring has
been completed and a primary replica is identified. While this is happening, EC2 instances attempting to
accessthisdataareblocked–inEC2terms,theyarestuck.
The EU Region Outage
At 10:41 AM PDT (6:41 PM Dublin time) on Sunday, August 7th, an Availability Zone in Amazon’s
European data center in Dublin lost power. The local power utility blamed the power loss on a lightning
strike that caused an explosion of a 100 kilovolt, 10 megawatt transformer in a power substation outside
of Dublin. Even worse, the surge from the lightning strike appeared to have damaged the controllers for
thebackupgeneratorsfortheAZ,whichconsequentlyfailedtocomeup.TheAvailabilityZonewentdark.
AnearlypostbyAmazonsaid:
“The transient electric deviation caused by the explosion was large enough that it propagated to a
portion of the phase control system that synchronizes the backup generator plant, disabling some of
them. Power sources must be phase-synchronized before they can be brought online to load.
Bringingthesegeneratorsonlinerequiredmanualsynchronization.”
Availability zones in a region are generally powered by independent sources, so other AZs in the
Europeanregionwereunaffected.
By 1:49 PM PDT, some power was restored and enough network capacity was available to allow access
to the AZ. Amazon then focused on bringing the failed EC2 instances and their EBS volumes back on
line,butprogresswasslowerthananticipated.
.
Then to Amazon’s horror, they discovered that an error in Amazon Web Services cleanup software
resulted in some customers having data deleted from their backup data snapshots, preventing data
2
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

recovery in those instances in which data was lost. In order to restore the snapshots, Amazon had to
copy all the data from each inconsistent EBS node to S3, where they could rebuild the volume and
convert it to snapshot format. This was a massive task – each EBS volume can hold up to a terabyte of
data. Amazon had to truck in additional servers to aid in the recovery process, an effort that was
compounded because it was nighttimeinDublin. EarlyTuesdaymorning, Amazon announced that half of
the volumes that had been in an inconsistent state had been recovered. Affected EC2 customers had
nowbeendownforalmosttwodays.
Finally, in the evening of Wednesday, August 10th, 98% of the European EC2 services were restored.
Somecustomershadbeendownforoverthreedays.
WhatReallyHappened?
It seems that ESB Networks, the Irish electricity provider, had a later version of events. It said that the
problem arose following a fault in one of its CityWest substations, noting that an alternate power source
shouldhavekicked-inautomaticallyinlessthanasecond.
The utility added that there was no report of an explosion or fire. And to compound matters, it said there
was norecord of alightningstrikeintheDublinarea atthe time.Itsaidthatits original assessmentthat a
lightningstrikewastoblamewaswrong:
“ESB Networks can confirm that at 18:16 on Sunday August 7th, a number of customers in CityWest
lost electricity supply. …In this case, the problem was the failure of a 110kV transformer in the
CityWest 110kV substation. The cause of this failure is still being investigated at this time but our
initial assessmentof lightning as the cause has now been ruled out. Thisinitial supply disruption
lasted forapproximately 1 hour as ESB Networks worked to restore supply.There was an ongoing
partial outagein the area until 11pm. The interruption affected about 100 customers in the Citywest
area, including Amazon and a number of other data centres. Another Amazon data centre served by
ESB in South County Dublin was not directly affected by the outage, though it did experience a
voltagedipwhichlastedforlessthanonesecond.”
As is usuallythecase,onceall problems hadbeenresolved,Amazonpublished adetailedaccountof the
outageandtheeffortstorecoverfromit.2Itisquiteanextensiveaccountandissummarizedasfollows.
10:41 PM PDT: The AZ lost power. “Normally, when utility power fails, electrical load is seamlessly
picked up by backup generators. Programmable Logic Controllers (PLCs) assure that the electrical
phase is synchronized between generators before their power is brought online. In this case, one of
the PLCs did not complete the connection of a portion of the generators to bring them online …With
noutilitypower,andbackupgeneratorsforalargeportionofthisAvailabilityZonedisabled,therewas
insufficient power for all of the servers in the Availability Zone to continue operating. Uninterruptable
Power Supplies (UPSs) that provide a short period of battery power quickly drained3 and we lost
power to almost all of the EC2 instances and 58% of the EBS volumes in that Availability Zone. We
also lost power to the EC2 networking gear that connects this Availability Zone to the Internet and
connectsthisAvailabilityZonetotheotherAvailabilityZonesintheRegion.”
11:05 AM PDT: “We were seeing launch delays and API errors in all EU West Availability Zones.
There were two primary factors that contributed to this. First, our EC2 management service … has
servers in each Availability Zone. The management servers which receive requests continued to
route requests to management servers in the affected Availability Zone. Because the management
servers in the affected Availability Zone were inaccessible, requests routed to those servers failed.
Second, the EC2 management servers receiving requests were continuing to accept RunInstances
2SummaryoftheAmazonEC2,AmazonEBS,andAmazonRDSServiceEventintheEUWestRegion,Amazonmessage.
http://aws.amazon.com/message/65648/
