Help! My Data Center is Down!
Part 6 – The Human Factor
March2012
In many respects, a company’s data center is part of its life blood. Significant investments are made to
ensurethatcorporatedatacentersneverfail.Unfortunately,theydo.
Industry studies have shown that the human factor plays a role in about 70% of data-center failures. In
some cases, it is a careless error on the part of an operator. In others, it is out-and-out malfeasance.
Often, an otherwise controllable situation caused by some hardware or software fault is elevated to a full
crisisbyahumanaction.
Inourpreviousarticles on data-centerfailures,wefocusedonfailures duetopower,storagesubsystems,
network faults, and upgrades gone wrong. In this article, we look at some human contributions to data-
center outages.1 The stories are all true and are taken from the Never Again archives of the Availability
Digest.
Carelessness
A good many outages are caused by careless operator errors that occur with a perfectly good
operationalsystem.
TheCoffeePotFiasco
A particularlyamusing incident (though not so at the time) was when a coffee pot took down a small data
center. After running its active/activenetwork successfullyfor several years on its existingequipment,the
company decided to upgrade to the next version of the system that it was using for its two nodes. This
wasamajorupgradeinvolvingnewhardware.
As best practices dictate, the system at each node was powered by a separate circuit protected by an
uninterruptible power supply (UPS). When the new system was rolled in at one of the nodes, the
operations staff found that all of the UPS power connectors were being used. So as not to delay the
upgrade, the new system was temporarily connected to the facility’s unprotected power. The plan was to
correctthisprobleminshortorderbyaddinganadditionalconnectortotheUPSoutput.
However, the required power connector change was forgotten. As time went on, the load on the
unprotectedcircuitsgraduallyincreasedasthecompanygrew.Onefatefulday,anemployeeperformeda
normal, everydaytask. He or she plugged in the coffee pot to make fresh coffee. This was the straw that
broke the camel’s back. The coffee pot blew the circuit breaker, taking down everything that was on that
1OurthankstoTheConnectionforgivinguspermissiontoreprintthisseriesofarticles.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

circuit. This included dropping primary power to the upgraded system, which had never been moved to
theUPScircuit.
The system kept on running for a while on its internal UPS. Fast action on the part of the staff at the site
restored the primary power in just 35 seconds – an admirable feat. Unfortunately, the system’s internal
UPS only lasted for 30 seconds. The node shut down and suffered a 30-minute outage until it was
brought back online. Though this was a major fault, the fact that the applications were running
active/active meant that the other system immediately assumed all of the load; and users were not
affected.
TheCaseoftheFlyingCable
The company’s fault-tolerant system depended upon communications with the outside world. Therefore,
its communication interface was totally redundant. It had dual communication processors connected to
dual LANs driving redundant communication lines. Each set of equipment was powered by an
independentexternalpowersource.
The communication subsystem hadn’t failed for over a decade and seemed solid. Not quite! One day, a
technician was pulling cables through the false flooring when a cable end came loose and went flying. It
collided with the power strip supplying one side of the redundant network equipment. The on/off switches
on the power strips had been disabled so that they would not be accidentally turned off. However, the
recessedcircuitbreakerswereactiveforsafetyreasons.
As luck would have it, the corner of the cable went into the recess of one of the circuit breakers and
tripped it. But no problem because the other side of the communications system would carry on, right?
Wrong. Long ago, with no one noticing, all of the communication equipment had been plugged into only
onepowerstrip;andthatistheonethatgothit.Thatendedadecadeofperfectavailability.
IgnoringaSoftwareBugCausesTrainWreck
An international long-haul railroad uses a fault-tolerant system to track its trains. Controllers monitor train
movementsandcancontrolthetrainsbypositioningswitchesandchangingsignalstates.
The system had been in operation for over a decade with its normal set of problems, all of which had
been corrected except for one known problem. There was one particular set of parallel tracks, one
northbound and one southbound, that would lose the southbound train display if trains were on each set
oftracks.Thetraincontrollerswerewellawareofthisproblembuthadneverreportedit.Instead,theyjust
rememberedwhenatrainhadbeenlost.Sincetrafficwaslight,thishardlyeverhappened.
But one day it did happen while the controller’s attention was diverted. When he returned to his console,
he saw that the track was unoccupied; and he cleared the next train onto that track. Unfortunately, the
trackwasnotunoccupied–theghosttrainwasstillthere.
Also, unfortunately, at the same time the engineer of the trailing train was “otherwise occupied” and did
notseethetraininfrontof him.Theresultingcrashcausedsignificantdamagetothetrains andthetrack.
Fortunatelynoonewasinjuredbecauseofthelowspeedofthecollision.
ConsoleCommandTakesDownActive/ActiveSystem
You have to work hard to take down an active/active system. However, one way to do this is for an
operatortoerroneouslyenteraseriesofcommandsthatadverselyaffectallsystemsinthenetwork.
Just such an incident happened to a two-node active/active system that had run for years without an
outage.Infact,thesystemhadundergonemanyrollingupgradeswithoutaplannedoutageofanysort.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

The upgrade started normally. The operations staff moved the users off of Node A in preparation for
upgrading it. Once satisfied that all users were now properly being handled by Node B, the system
manager brought up the maintenance console for Node A; and the command to stop the Node A system
wasentered.
To the system manager’s horror, the entire system suddenly shut down. As it turned out, he had not
brought up the maintenance console for Node A. He had brought up the maintenance console for Node
B. Oops! He had shut down the wrong node. Consequently, he had stopped the operational system; and
alluserswereoutofservice.
The$38BillionKeystroke
To protect oil revenues for the citizens of the state of Alaska, the Alaska Permanent Fund was set up to
receive and invest proceeds from the sale of Alaskan oil. The fund has paid a yearly dividend to all
Alaskanresidents.In2006,thefundbalancehadreached$38billion.
On a fateful day in July, 2006, a computer technician working on a disk drive at the Department of
Revenue mistakenly deleted the oil fund database. This was not a big problem because the data also
existed on a redundant backup disk. However, under the pressure of the moment, the technician also
managedtoreformatthebackupdisk.
Not to despair. Like all good data centers, this data was backed up on magnetic tape. The only data that
would be lost would be those transactions entered since the last update. The tapes were retrieved from
storage. Only then did the magnitude of the disaster become apparent. The tapes were unreadable. The
tripleredundancythatwasbuiltintothesystemwasnotenough.
Over the next several days, employees and consultants tried vainlyto salvage the data. The terrible truth
finally had to be accepted. The last nine months of transaction history had been lost. This included
800,000 scanned images of paper applications. Fortunately, there was a fourth level of backup – the
paper documents themselves, stored in over 300 cardboard boxes. Each of the 800,000 documents had
to be rescanned, sent through quality control, written to the database, and linked to the appropriate
person’saccount.
It took 70 people working nights and weekends almost two months to complete the recovery at a cost to
thestateof$200,000.
DataCenterTakenDownbyNoise
It’s not a good idea to test a fire-suppression system by triggering it. But that’s what happened to
WestHost, a major web-hosting provider headquartered in Utah. On Saturday, February 20, 2010, the
WestHost data center underwent a standard yearly test of its Inergen fire-suppression system.
Unfortunately, a third-party test technician failed to follow the published pre-test check list and did not
removeone of theactuators thatactivates thesystem.Whenthesystem was re-armedfollowing the test,
theactuatorfiredandtriggeredthereleaseofthelargeblastofInergengasdesignedtoputafireout.
No one seemed to know at the time whether it was the pressure blast or the gas itself, but hundreds of
servers and disk storage systems were severely damaged. Even worse, the backup disk drives were in
the same facility as the servers, and many of the backup disks were destroyed. Some RAID drives were
recoverable,andtheirserverswerebroughtback intoservice.Thoughdatarecoveryexpertswereableto
restore data from some failed drives, other data was simply deemed nonrecoverable by the data-
restorationexperts.
It took six days for WestHost to get its data center back into operation. Subsequent studies by the
manufacturers of the fire suppression system and of the Inergen gas showed that the disk damage was
causedbytheear-splittingsoundlevelfromthewarningalarms.
