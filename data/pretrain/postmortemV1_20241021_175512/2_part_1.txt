## March 2023

## 6 

[03/06/2023]

Post Incident Review (PIR) - Azure Storage - West Europe

Tracking ID: R_36-P80


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/R_36-P80](https://aka.ms/AIR/R_36-P80)*

**What happened?**

[Between 03:50 UTC and 17:55 UTC on 6 March 2023, a subset of customers
using Azure Storage may have experienced greater than expected
throttling when performing requests against Storage resources located in
the West Europe region. Azure services dependent on Azure Storage may
also have experienced intermittent failures and degraded performance due
to this issue. These included Azure Automation, Azure Arc enabled
Kubernetes, Azure Bastion, Azure Batch, Azure Container Apps, Azure Data
Factory (ADF), Azure ExpressRoute \\ ExpressRoute Gateways, Azure
HDInsight, Azure Key Vault (AKV), Azure Logic Apps, Azure Monitor, and
Azure Synapse Analytics. ]{style="color: rgb(50, 50, 55)"}

**What went wrong and why?**

[Azure Storage employs a throttling mechanism that ensures storage
account usage remains within the published storage account limits (for
more details, refer
to ]{style="color: rgb(50, 50, 55)"}[[https://learn.microsoft.com/azure/storage/common/scalability-targets-standard-account](https://learn.microsoft.com/azure/storage/common/scalability-targets-standard-account)]{style="color: var(--primary); background-color: transparent"}).
This throttling mechanism is also used to protect a storage service
scale unit from exceeding the overall resources available to the scale
unit. In the event that a scale unit reaches such limits, the scale unit
employs a safety protection mechanism to throttle accounts that are
deemed to contribute to the overload, while also balancing load across
other scale units.

Configuration settings are utilized to control how the throttling
services monitor usage and apply throttling. A new configuration was
rolled out to improve the throttling algorithm. While this configuration
change followed the usual Safe Deployment Practices (for more details,
refer
to: [[https://learn.microsoft.com/en-us/devops/operate/safe-deployment-practices](https://learn.microsoft.com/en-us/devops/operate/safe-deployment-practices)]{style="color: var(--primary); background-color: transparent"}),
this issue was found mid-way through the deployment. The change had
adverse effects due to the specific load and characteristics on a small
set of scale units where the mechanism unexpectedly throttled some
storage accounts in an attempt to bring the scale unit to a healthy
state.

**How did we respond?**

[Automated monitoring alerts were triggered, and engineers were
immediately engaged to assess issues that the service-specific alert
reported. This section will provide a timeline for how we
responded. Batch started investigating these specific alerts at 04:08
UTC. This investigation showed that there were storage request failures
and service availability impact. At 04:40 UTC Storage engineers were
engaged to begin investigating the cause. While storage was engaged and
investigating with Batch, at 05:30 UTC, automated monitoring alerts were
triggered for ADF. The ADF investigation showed there was an issue with
underlying Batch accounts. Batch confirmed with ADF that they were
impacted by storage failures and are working with storage engineers to
mitigate. At this time, storage engineers diagnosed that one scale unit
was operating above normal parameters, this included identifying that
Batch storage accounts were throttled due to tenant limits. And the
action engineers took was to load balance traffic across different scale
units. By 06:34 UTC, storage engineers started to do Batch account
migration in efforts to mitigate the ongoing
issues. ]{style="color: rgb(50, 50, 55)"}

[At 07:15 UTC, automation detected an issue with AKV requests. The AKV
investigation showed there was an issue with the underlying storage
accounts. Around 09:10 UTC, engineers performed a failover that
mitigated the issue for all existing AKV read operations. However,
create, read (for new requests), update and delete operations for AKV
were still impacted. Around 10:00 UTC, storage engineers correlated the
occurrences of the downstream impacted services with the configuration
rollout because the scope of the issue expanded to additional scale
units. By 10:15 UTC, storage engineers began reverting the configuration
change on select impacted scale units. The Batch storage account
migration finished around 11:22 UTC, after that, Batch service became
healthy at 11:35 UTC. ADF began to recover after Batch mitigation was
completed. ADF was fully recovered around 12:51 UTC after accumulated
tasks got consumed.]{style="color: rgb(50, 50, 55)"}

[By 16:34 UTC, impacted resources and services were mitigated. Shortly
thereafter, engineers scheduled a rollback of the configuration change
on all scale units (even ones that were not impacted), declaring
mitigation.  ]{style="color: rgb(50, 50, 55)"}

**How are we making incidents like this less likely or less impactful?**

-   We are tuning our monitoring to anticipate and quickly detect when a
    storage scale unit might engage its scale unit protection algorithm
    and employs throttling to storage accounts. These monitors will
    proactively alert engineers to take necessary actions (Estimated
    completion: March 2023).
-   Moving forward, we will rollout throttling improvements in
    tracking-mode first to assess its impact before getting enabled
    since throttling improvements may react differently to different
    workload types and load on a particular scale unit (Estimated
    completion: March 2023).

The above improvement areas will help to prevent/detect storage-related
issues across first-party services that are reliant on Azure storage
accounts - for example, Batch and Data Factory.

-   Our AKV team is working on improvements to the current distribution
    of storage accounts across multiple scale units and update storage
    implementation to ensure that read and write availability can be
    decoupled when such incidents happen (Estimated completion: May
    2023).
-   We continue to expand our AIOps detection system to provide better
    downstream impact detection and correlation - to notify customers
    more quickly, and to identify/mitigate impact more quickly
    (Ongoing).

**How can customers make incidents like this less impactful?**

-   To get the best performance from Azure Storage, including when
    throttled, consider following the Performance and Scalability
    Checklist for Blob
    Storage: [[https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-performance-checklist)]{style="color: var(--primary); background-color: transparent"}
-   [Consider which are the right Storage redundancy options for your
    critical applications. Geo-redundant storage (GRS) enables account
    level failover in case the primary region endpoint becomes
    unavailable, like in this
    incident. ]{style="color: rgb(50, 50, 55)"}[[https://docs.microsoft.com/azure/storage/common/storage-redundancy](https://docs.microsoft.com/azure/storage/common/storage-redundancy)]{style="color: var(--primary); background-color: transparent"}
-   [More generally, consider evaluating the reliability of your
    applications using guidance from the Azure Well-Architected
    Framework and its interactive Well-Architected
    Review:  ]{style="color: rgb(50, 50, 55)"}[[https://learn.microsoft.com/en-us/azure/architecture/framework/resiliency/](https://learn.microsoft.com/en-us/azure/architecture/framework/resiliency/)]{style="color: var(--primary); background-color: transparent"}
-   [Finally, consider ensuring that the right people in your
    organization will be notified about any future service issues - by
    configuring Azure Service Health alerts. These can trigger emails,
    SMS, push notifications, webhooks, and
    more:  ]{style="color: rgb(50, 50, 55)"}[[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)]{style="color: var(--primary); background-color: transparent"}

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/R_36-P80](https://aka.ms/AzPIR/R_36-P80)

## February 2023

## 7 

[02/07/2023]

Post Incident Review (PIR) - Multi-service outage -- Asia-Pacific Area

Tracking ID: VN11-JD8


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/VN11-JD8](https://aka.ms/AIR/VN11-JD8)*

**What happened?  **

[Between 20:19 UTC on 7 February 2023 and 04:30 UTC on 9 February 2023,
a subset of customers with workloads hosted in the Southeast Asia and
East Asia regions experienced difficulties accessing and managing
resources deployed in these regions. ]{style="color: rgb(51, 51, 51)"}

[One Availability Zone (AZ) in Southeast Asia experienced cooling
failure. Infrastructure in that zone was shutdown to protect data and
infrastructure. This led to failures in accessing resources and services
hosted in that zone. However, this zone failure also resulted in two
further unexpected failures. Firstly, regional degradation for some
services and secondly, services designed to support failover to other
regions or zones did not work
reliably. ]{style="color: rgb(51, 51, 51)"}

[The services which experienced degradation in the region included App
Services, Azure Kubernetes Service, Databricks, API Management,
Application Insights, Backup, Cognitive Services, Container Apps,
Container Registry, Container Service, Cosmos DB, NetApp Files, Network
Watcher, Notification Hubs, Purview, Redis Cache, Search, Service Bus,
SignalR Service, Site Recovery, SQL Data Warehouse, SQL Database,
Storage, Synapse Analytics, Universal Print, Update Management Center,
Virtual Machines, Virtual WAN, Web PubSub -- as well as the Azure portal
itself, and subsets of the Azure Resource Manager (ARM) control plane
services.]{style="color: rgb(51, 51, 51)"}

[BCDR Services that were designed to support regional or zonal failover
that didn't work as expected included Azure Site Recovery, Azure Backup
and Storage accounts leveraging Geo Redundant Storage (GRS).
]{style="color: rgb(51, 51, 51)"}

[ ]{style="color: rgb(51, 51, 51)"}

**What went wrong and why? **

[In this PIR we will break down what went wrong into 3 parts. The first
part will focus on the failures that led to the loss of an availability
zone. The second part will focus on the unexpected impact to the ARM
control plane, and BCDR services. Lastly, we will focus on the extended
recovery for the SQL Database service. ]{style="color: rgb(51, 51, 51)"}

**Chiller failures in a single Availability Zone**

[At 15:17 UTC on 7 February 2023, a utility voltage dip event occurred
on the power grid, affecting a single Availability Zone in the Southeast
Asia region. Power management systems managed the voltage dip as
designed, however a subset of chiller units that provide cooling to the
datacenter tripped and shut down. Emergency Operational Procedures (EOP)
were performed as documented for impacted chillers but were not
successful. Cooling capacity was reduced in the facility for a prolonged
time, and despite efforts to stabilize by shutting down non-critical
infrastructure, temperatures continued to rise in the impacted
datacenter. At 20:19 UTC on 7 February 2023, infrastructure thermal
warnings from components in the datacenter directed a shutdown on
critical compute, network and storage infrastructure to protect data
durability and infrastructure health. This resulted in loss of resource
and service availability in the impacted zone in Southeast Asia.
]{style="color: rgb(51, 51, 51)"}

[Maximum cooling capacity for this facility incorporates 8 chillers from
2 different suppliers. Chillers 1 to 5 are from supplier A providing 47%
of the cooling capacity, chillers 6, 7 and 8 from supplier B make up the
remaining 53%. Chiller 5 was offline for planned maintenance. After the
voltage dip, chiller 4 continued to run and 1, 2 and 3 were started, but
could not provide the cooling necessary to stabilize the temperatures
which had already increased during the restart period. These 4 units
were operating as expected, but 6, 7, and 8 were unable to be
started. This was not the expected behavior of these chillers, which
should automatically restart when tripped by a power dip. The chillers
also failed a manual restart as part of the EOP (emergency operating
procedure) execution. ]{style="color: rgb(51, 51, 51)"}

[This fault condition is currently being investigated by the OEM chiller
vendor. Preliminary investigations point to the compressor control card
which ceased to respond after the power dip, inhibiting the chillers
from successfully restarting. From the chiller HMI alarm log, a "phase
sequence alarm" was detected. To normalize this alarm, the compressor
control card had to be hard reset requiring a shut down, leaving it
powered off for at least 5 minutes to drain the internal capacitors
before powering the unit on. The current EOP did not list out these
steps. This is addressed later in the PIR.
]{style="color: rgb(51, 51, 51)"}

[Technicians from Supplier B were dispatched onsite to assist. By the
time the chiller compressor control card was reset with their
assistance, the chilled water loop temperatures exceeded the threshold
of 28 degrees Celsius, causing a lock out of the restart function to
protect the chiller from damage. This condition is referred to as a
thermal lockout. To reduce the chilled water loop temperature, augmented
cooling was required to be brought online and thermal loads had to be
reduced by shutting down infrastructure, as previously stated. This
successfully reduced the chilled water loop temperature below the
thermal lockout threshold and enabled the restart function for chillers
6, 7, and 8. Once the chillers were restarted, temperatures progressed
toward expected levels and, with all units recovered, by 14:00 UTC on 8
February 2023, temperatures were back within our standard operational
thresholds. ]{style="color: rgb(51, 51, 51)"}

[With confidence that temperatures were stable, we then began to restore
power to the affected infrastructure, starting a phased process to first
bring storage scale units back online. Once storage infrastructure was
verified healthy and online, compute scale units were then powered up.
As the compute scale units became healthy, virtual machines and other
dependent Azure services recovered. Most customers should have seen
platform service recovery by 16:30 UTC on 08 February, however a small
number of services took an extended time to fully recover, completing by
04:30 UTC on 9 February 2023. ]{style="color: rgb(51, 51, 51)"}

**Unexpected impact ARM and BCDR services.**

[While infrastructure from the impacted AZ was powered down, services
deployed and running in a zone resilient configuration were largely
unimpacted by the shutdown of power in the zone. However multiple
services may have experienced some form of degradation, these services
are listed above. This was due to dependencies on ARM, which did
experience unexpected impact from the loss of a single AZ.
]{style="color: rgb(51, 51, 51)"}

[In addition, three services that are purpose-built for business
continuity and disaster recovery (BCDR) planning and execution, in both
zonal and regional incidents, also experienced issues, and will be
discussed in detail: ]{style="color: rgb(51, 51, 51)"}

**Azure Site Recovery (ASR)** [- H]{style="color: rgb(51, 51, 51)"}[elps
ensure business continuity by keeping business apps and workloads
running during outages. Site Recovery replicates workloads running on
physical and virtual machines (VMs) from a primary site to a secondary
location. When an incident occurs at a customer's primary site, a
customer can fail over to a secondary location. After the primary
location is running again, customers can fail
back.]{style="color: rgb(22, 22, 22)"}

**Azure Backup** [-- Allows customers to make backups that
k]{style="color: rgb(51, 51, 51)"}[eeps data safe and
recoverable.]{style="color: rgb(22, 22, 22)"}

**Azure Storage** [-- Storage accounts configured for Geo Redundant
Storage (GRS) to provide resiliency for data
replication.]{style="color: rgb(51, 51, 51)"}

**Impact to the ARM service:**

[As the infrastructure of the impacted zone was shutdown, the ARM
control plane service (which is responsible for processing all service
management operations) in Southeast Asia lost access to a portion of its
metadata store hosted in a Cosmos DB instance that was in the impacted
zone. It was expected that this instance was zone resilient, but due to
a configuration error, local to the Southeast Asia region, it was not.
Inability to access this metadata resulted in failures and contention
across the ARM control plane in that region. To mitigate impact, the ARM
endpoints in Southeast Asia were
]{style="color: rgb(51, 51, 51)"}[manually
]{style="color: black"}[removed from their global service Traffic
Manager profile. This was completed at 04:10 UTC on 8 February 2023.
Shifting the ARM traffic away from Southeast Asia increased traffic to
nearby APAC regions, in particular East Asia, which quickly exceeded
provisioned capacity. This exacerbated the impact to include East Asia
where customers would have experienced issues managing their resources
via ARM. The ARM service in East Asia was scaled-up and by 09:30 UTC on
the 8 of February the service was fully
recovered. ]{style="color: rgb(51, 51, 51)"}

[ ]{style="color: rgb(51, 51, 51)"}**Impact to BCDR services**

[ A subset of Azure Site Recovery customers started experiencing
failures with respect to zonal failovers from the Southeast Asia region
and regional failovers to the East Asia region.
]{style="color: rgb(51, 51, 51)"}

