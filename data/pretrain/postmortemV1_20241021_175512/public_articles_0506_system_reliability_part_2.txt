mean time to recover and average amount of data lost following a failure does not tell the whole
story.
SiteDisasters
The literature is rife with systems being taken down for hours or days due to a data-center
disaster. Does this impact a user’s experience? You better believe that it does. Therefore, the
abilityofasystemtosurviveasitedisasterisanotherkeyparameter.
Reliable systems are always redundant. The question is,
howfarapartcantheredundantsystemsbeseparatedso
that a common disastrous event does not take down both
systems? We therefore include a distance parameter in
ourspecificationasameasureofdisastertolerance.
node1 node2
PlannedDowntime
Systems must be taken offline occasionally to upgrade
hardware, operating systems, database management
systems, applications, and so forth. Does this impact the
userexperience?
It depends. If the system is not a 24x7 system, and if
there is a sufficiently long window during which the
HowfastcanIrecover?
system can be taken down to be upgraded, then planned HowmuchdatawillIlose?
downtime is nonintrusive (assuming that the system CanIsurviveadisaster?
comes back online properly following the upgrade). CanIeliminateplanneddowntime?
However, if the system must be in continuous operation,
thenplanneddowntimemustbeeliminated.
The typical way to eliminate planned downtime is to roll the upgrade through the system one
nodeatatime.4Somesystemsallowthis,andothersdonot.
We therefore add another parameter that indicates whether upgrades can or cannot be rolled
throughthesystem.
FailureRates
The availability of a system is a function not only of its recovery time but also of its failure rate.
Given the same recovery time, a system that fails twice per year will have half the reliability of a
systemthatfailsonceper year.
4 Achieving Century Uptimes: Parts 15, 16 – Zero Downtime Migrations for Active/Active Systems, The Connection;
March/April,May/June2009.
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

How do we account for failure rate in our comparisons?Wedon’t. Failure rate is a function of the
components we choose to implement our system. It is not a fundamental characteristic of the
architectureofthesystemorofitsunderlyingtechnology.
ApplicationSupport
Some architectures support certain applications better than others. For instance, some
architectures work bestfor database applications, whereas others are applicabletoa broader set
of applications. In addition, each architecture usually supports only certain vendor hardware,
certainoperatingsystems,andcertaindatabases.
Weleavethistothemarketingpeople.
Nines
Notethatwehaveleftout‘nines’asanavailabilitymeasure.Ifwearetalkingabouthighly-reliable
systems, a measure of availability is, in our opinion, far less meaningful to the user experience
than is recovery. Besides, a measure of ‘nines’ is directly dependent upon the reliability of the
components of a system, a factor that we are ignoring on the basis that failure rates are less
important if recovery is fast enough. If ‘nines’ are used, one must clearly state which downtime
scenariosareincludedinthemeasureandwhicharenot.
This is not to say that ‘nines’ are not an important measure of a system’s reliability. We are only
sayingthatwecanjudgetherelativereliabilityofasystembymeasuresotherthan‘nines.’
AllThoseOtherFaults
Are we only comparing downtimes caused by hardware and software failures? What about all of
thoseothercausesofdowntime–operatorerrors,applicationbugs,networkfaults,environmental
faults? After all, with the reliability of system hardware and software today, they are often the
predominantcausesofsystemfailure.
Some architectures inherently allow an independent recovery attempt from any of them. Others
donot.Thisisafactorthatmustbeconsidered,butisoutsidethescopeofourqualification.
Cost
To soothe the savage marketing community, we leave cost comparisons to them. Beware of
salesbyunder-configuration.
System Reliability Comparison – a Straw Man
The following table is a first pass at quantifying system reliability using the above concepts. By-
and-large,theparameters (if accurate) cannot beeasilymanipulatedformarketingpurposes.Itis
fairly easy to add new systems to the list without extensive benchmarking – all that is needed is
anunderstandingofthearchitecture.
There is no “best” system. It all depends upon the needs of the application. Hopefully, this sort of
comparison can be a valuable tool in matching a highly-reliable technology to the needs of an
applicationwithoutbeingmisleadandconfusedbyloosely-definedmarketingterms.
5
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

System RTO RPO Disaster Rolling
Tolerance Upgrades
StratusftServer5 0 0 0 yes*
StratusAvance6 seconds 0 0.5km yes
MarathonEverRun seconds 0 meters yes
HPNonStopDMR msec. 0 0 no
HPNonStopTMR 0 0 0 no
HPNonStopBlades7 msec. 0 0 no
HPNonStopZLT minutes 0 tensofkm yes
OpenVMScluster8 msec. 0 600miles** yes
Clusters9 minutes 0 0 maybe***
Active/ActiveAsync10 seconds seconds unlimited yes
Active/ActiveSync11 seconds 0 tensofkm yes
Active/ActiveCoordinatedCommits seconds 0 unlimited yes
IBMParallelSysplex–MetroMirror12 minutes 0 100km yes
IBMParallelSysplex–GlobalMirror12 minutes minutes unlimited yes
UnidirectionalReplication minutes minutes unlimited yes
VirtualTape hours hours unlimited yes
MagneticTape days days unlimited yes
* WithActiveUpgradefeature.
**Significantperformancedegradationsincesynchronousreplicationisused.
***Ifoperatingsystemisonprivatedisks,notpublicdisks.
A Comparison of System Reliability
5Fault-TolerantWindowsandLinuxfromStratus,AvailabilityDigest;September2007
(http://www.availabilitydigest.com/public_articles/0209/stratus.pdf).
6StratusAvanceBringsAvailabilitytotheEdge,AvailabilityDigest;February2009
(http://www.availabilitydigest.com/public_articles/0402/avance.pdf).
7HP’sNonStopBlades,AvailabilityDigest;August2008
(http://www.availabilitydigest.com/public_articles/0308/ns_blades.pdf).
8OpenVMSActive/ActiveSplit-SiteClusters,AvailabilityDigest;June2008
(http://www.availabilitydigest.com/public_articles/0306/openvms.pdf).
9Active/ActiveVersusClusters,AvailabilityDigest;May2007
(http://www.availabilitydigest.com/private/0205/clusters.pdf).
10AsynchronousReplicationEngines,AvailabilityDigest;November2006
(http://www.availabilitydigest.com/private/0102/asynchronous_replication.pdf).
11SynchronousReplication,AvailabilityDigest;December2006
(http://www.availabilitydigest.com/private/0103/synchronous_replication.pdf).
12ParallelSysplex–FaultTolerancefromIBM,AvailabilityDigest;April2008
(http://www.availabilitydigest.com/public_articles/0304/ibm_sysplex.pdf).
6
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Summary
If one accepts the premise that reliability is in the perception of users, and if that user perception
isrecoverytime,thensystemreliabilitiescanbecomparedquantitatively.
The first step in any such evaluation is to define the reliability requirements of the applications
(different applications will certainly have different reliability requirements). Various products and
technologies can then be evaluated to narrow the scope of the search. Once a subset of
applicable systems is determined, focus must then be placed on choosing those that support the
systemsandapplicationsofinterest.Lastbutnotleast,thedecidingfactorwillbecost.
It must be noted that the cost of a system is not just a matter of acquisition cost or of operating
cost. It is also a matter of downtime cost.With downtime costs varying from thousands of dollars
per hour tohundreds of thousands of dollars per hour andmore,thedifferencebetweenrecovery
timesmeasuredinmilliseconds,seconds,orminutesmaywelljustifyamoreexpensivesystem.
Let us have your feedback to this attempt at defining reliability on the Continuous Availability
Forum.
7
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
