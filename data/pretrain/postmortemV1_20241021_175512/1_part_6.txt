[https://aka.ms/AIR/QNPD-NC8](https://aka.ms/AIR/QNPD-NC8)*

**What happened?**

Between 15:10 and 17:00 UTC on 9 June 2023, customers may have
experienced error notifications when trying to access the Azure Portal
(portal.azure.com), and actions to load new HTTP content in the Azure
Portal may have resulted in error notifications. Customers may also have
experienced issues accessing other services built on the Azure Portal,
like the Microsoft Entra Admin center (entra.microsoft.com) and
Microsoft Intune (intune.microsoft.com).

**What went wrong and why?**

Our internal telemetry reported an anomaly with increased request rates
as the Azure Portal was displaying a "service unavailable" message in
multiple geographies. Traffic analysis showed an anomalous spike in HTTP
requests being issued against Azure Portal origin servers, bypassing
existing automatic preventive recovery measures and triggering the
service unavailable response.  

We identified that the traffic spikes were issuing floods of incoming
requests at a magnitude significantly larger than the normal traffic
patterns, which overwhelmed the origin servers. Upon further
investigation, we have determined the anomalous traffic spikes to be
Layer 7 distributed denial-of-service (DDoS) attacks. Multiple types of
methods were utilized which included HTTP request floods distributed
from across the globe, and cache bypass techniques to overload the Azure
Portal origin servers.

For more details, including a characterization of the attack types used
and recommendations for customers to protect against these, refer to
this Microsoft Security Response Center (MSRC) blog post
([https://msrc.microsoft.com/blog/2023/06/microsoft-response-to-layer-7-distributed-denial-of-service-ddos-attacks/](https://msrc.microsoft.com/blog/2023/06/microsoft-response-to-layer-7-distributed-denial-of-service-ddos-attacks/)). 

Our automatic preventive recovery measures help us mitigate an average
of about 1,435 DDoS attacks per day. (More information can be found
here:
[https://www.microsoft.com/security/blog/2023/02/21/2022-in-review-ddos-attack-trends-and-insights/](https://www.microsoft.com/security/blog/2023/02/21/2022-in-review-ddos-attack-trends-and-insights/)).
However, this attack resulted in the issues mentioned above, and we have
hardened our protections from the lessons we've learned from it.

**How did we respond?**

We were alerted by our internal monitoring of the traffic spikes
impacting the availability of the Azure Portal. Within 15 minutes,
engineers from both our Portal and networking teams were engaged and
began investigating the issue. DDoS protection rules were adjusted to
block the traffic based on the attack pattern. Traffic throttling rules
were adjusted to throttle the requests. Additional Azure Portal server
instances were added, to handle increased load. Any unhealthy Azure
Portal instances were rebooted, to ensure they could properly take on
external traffic. Applying these mitigation steps improved Azure Portal
availability. By 16:20 UTC, we were able to block 80% of the attack
traffic which significantly helped limit global impact. At that point,
we were at 50% Portal availability, mitigating impact for some
customers.

A spike was observed at 16:50 UTC, which was absorbed by the platform
due to our in-built DDoS protection mechanisms and additional actions
that were being taken. Our internal monitoring started reporting a
healthy state back to baseline at 17:00 UTC for all Azure Portal
endpoints. Our engineers continued to closely monitor the situation and
carried out proactive measures to harden the system against the
potential for a recurrence of the issue.

**How are we making incidents like this less likely or less impactful?**

-   Making the Azure Portal more efficient so scale up runs more
    quickly. (Completed)
-   Using proactive defense logic in adjusting traffic blocking and
    throttling rules. (Completed)
-   Blocking invalid requests and server responses more aggressively.
    (Completed)
-   Increasing Azure Portal scale to cope with high demand leads more
    efficiently. (Completed) 
-   Improving our configuration with additional caching mechanisms for
    the Azure Portal. (Estimated completion: July 2023)
-   Making the Azure Portal startup process faster. (Estimated
    completion: July 2023)
-   Improving our internal Azure incident management monitoring to
    detect such indicators of broader impact more quickly and
    efficiently and help to notify customers sooner. (Estimated
    completion: August 2023)

**How can customers make incidents like this less impactful?**

When the Azure Portal is unavailable:

-   To manage your Azure environment, consider using Azure Command-Line
    Interface
    ([https://learn.microsoft.com/cli/azure/](https://learn.microsoft.com/cli/azure/))
    and PowerShell
    ([https://learn.microsoft.com/powershell/azure/get-started-azureps?view](https://learn.microsoft.com/powershell/azure/get-started-azureps?view)=azps-10.0.0).
    These were not impacted during this incident.
-   To manage resources, consider using Azure Resource Manager (ARM)
    REST API as well as tools that are built using API endpoints and
    scripting
    ([https://learn.microsoft.com/rest/api/resources/](https://learn.microsoft.com/rest/api/resources/)).
-   To submit support requests, consider using Azure Support REST API
    ([https://learn.microsoft.com/rest/api/support/](https://learn.microsoft.com/rest/api/support/)),
    Command-Line Interface
    ([https://learn.microsoft.com/cli/azure/](https://learn.microsoft.com/cli/azure/)azure-cli-support-request)
    or Customer Service Phone Numbers
    ([https://go.microsoft.com/fwlink/p/?linkid](https://go.microsoft.com/fwlink/p/?linkid)=2201739).

For your own Application DDoS protection:

-   We recommend reviewing Layer 7 protection services, such as Azure
    Web Application Firewall (WAF) (available with Azure Front Door,
    Azure Application Gateway) to help reduce the impact of Layer 7 DDoS
    attacks to your applications
    ([https://learn.microsoft.com/azure/web-application-firewall/shared/application-ddos-protection](https://learn.microsoft.com/azure/web-application-firewall/shared/application-ddos-protection)).

For all Azure service issues:

-   Finally, consider ensuring that the right people in your
    organization will be notified about any future service issues by
    configuring Azure Service Health alerts:
    ([https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)).
    These can trigger emails, SMS, webhooks, push notifications (via the
    Azure Mobile app
    [https://aka.ms/AzureMobileApp](https://aka.ms/AzureMobileApp))
    and more.

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/QNPD-NC8](https://aka.ms/AzPIR/QNPD-NC8)

## March 2023

## 23 

[03/23/2023]

Post Incident Review (PIR) -- Azure Resource Manager -- West Europe

Tracking ID: RNQ2-NC8


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/RNQ2-NC8](https://aka.ms/AIR/RNQ2-NC8)*

**What happened?**

Between 02:20 UTC and 07:30 UTC on 23 March 2023 you may have
experienced issues using Azure Resource Manager (ARM) when performing
resource management operations in the West Europe region. This impacted
users of Azure CLI, Azure PowerShell, the Azure portal, as well as Azure
services which depend upon ARM for their internal resource management
operations.

The primary source of impact was limited to ARM API calls being
processed in our West Europe region. This caused up to 50% of customer
requests to this region to fail (approximately 3% of global requests at
the time). This principally affected customers and workloads in
geographic proximity to our West Europe region, while customers
geographically located elsewhere would not have been impacted -- with
limited exceptions for VPN users and those on managed corporate
networks. Additionally, Azure services that leverage the ARM API as part
of their own internal workflows, and customers of these services, may
have experienced issues managing Azure resources located in West Europe
as a result. 

**What went wrong and why?**

This incident was the result of a positive feedback loop leading to
saturation on the ARM web API tier. This was caused by high-volume,
short-held lock contention on the request serving path, which triggered
a significant increase in spin-waits against these locks, driving up CPU
load and preventing threads from picking up asynchronous background
work. As a result of this, latency for long running asynchronous
operations (such as outgoing database and web requests) increased,
leading to timeouts. These timeouts caused both internal and external
clients to retry requests, further increasing load and contention on
these locks, eventually causing our Web API tier to saturate its
available CPU capacity.

There are several factors which contribute to increasing the feedback on
this loop, however the ultimate trigger was the recent introduction of a
cache used to reduce the time spent parsing complex feature flag
definitions in hot loops. This change was intended to reduce the
performance impact of using feature flags on the request serving path,
and had been previously load tested and validated in our internal
testing and canary environments - demonstrating a significant reduction
in performance impact in these scenarios. 

This change was rolled out following our standard safe deployment
practices, progressively deployed to increasingly larger regions over
the course of four days prior to being deployed to West Europe. Over
this period, it was not exposed to the problematic call pattern, and
none of these regions exhibited anomalous performance characteristics.
When this change was deployed to our West Europe region, it was
subjected to a call pattern unique to a specific internal service which
exercised this cache path more heavily than the broad-spectrum workloads
we had tested in our internal and canary environments.

Approximately 24 hours after it was deployed to West Europe, a spike in
traffic from this internal service that executed a daily cache refresh
was able to induce enough lock contention to start this positive
feedback loop across a significant portion of the ARM web API instances
in the region. This cascaded as the service in question retried failed
requests and, over the course of 20 minutes, the region progressed from
a healthy to heavily saturated state. 

These recent contributors combined with several other factors to trigger
and exacerbate the impact, including:

-   A legacy API implementation -- whose responses varied infrequently,
    made heavy use of costly data transforms on each request without
    caching.
-   The introduction of a new feature flag -- which influenced the data
    transforms applied to this legacy API, as well as several others, in
    support of ongoing improvements to regional expansion workflows.
-   Internal retry logic -- which has the potential for increasing load
    during a performance degradation scenario (but this also
    significantly improves customer experienced reliability in other
    scenarios). 
-   External clients which implement retry logic that can increase load
    during a saturation scenario.

**How did we respond? **

At 02:41 UTC we were alerted to a drop in regional availability for
Azure Resource Manager in West Europe. This was 21 minutes after the
first measurable deviation from nominal performance, and 6 minutes after
the first significant drop in service availability for the region.
Diagnosing the cause took an extended amount of time as the issue was
obfuscated by several factors. First, we identified an increase in
failures for calls to downstream storage dependencies, and an increase
in both CPU usage and number of errors served from the reverse proxy
layer in front of our Web API services. The impact appeared to
correspond to a significant increase in CPU load without any visible
increase in traffic to the region. This was not typical, and all
dependencies in the region appeared to be healthy. 

We were aware of the most recent deployment to this region happening
about 24 hours earlier, however this region (and up to 20 others) had
been operating nominally with this new code. Following our standard
response guidance for saturation incidents, we scaled up our Web API
tier in West Europe to reduce the impact to customers, however, these
new instances were immediately saturated with incoming requests, having
little impact on the region\'s degraded availability. We also determined
that shifting traffic away from this region was unlikely to improve the
situation and had the potential to cause a multi-region incident, since
the problem appeared to be contained within the ARM service and tied to
a workload in West Europe.  

As a result of the significant increase in CPU load, our internal
performance profiling systems automatically began sampling call paths in
the service. At 05:15 UTC, this profiling information is how were able
to attribute the cause to a spin lock in our feature flagging system\'s
cache which was being triggered by a specific API. Using this
information, we were able to identify the internal service responsible
for the majority of calls to this API and confirmed that a spike in
requests from this service was the trigger for the load increase. We
engaged team members from this internal service and disabled the
workload, while simultaneously blocking the client to reduce the
generated load. By 06:30 UTC these changes succeeded, which in-turn
reduced the incoming load and we started to observe improvements to
regional availability in West Europe. 

This availability continued to improve as our phased configuration
rollout within the region progressed, improving the health of the
platform . By 07:30 UTC, it had returned to nominal availability at
\>99.999%. At 07:54 UTC we confirmed mitigation and transitioned into
the investigation and repair phase. Once mitigation was confirmed, we
set about hardening the system against any potential for a recurrence of
the previous issue, starting first with a rollback of the latest code
release in West Europe to the previous release. Simultaneously, we set
to work reverting the code changes primarily responsible for triggering
this failure mode and developing patches for the other contributing
factors -- including adjusting the caching system used for feature flags
to remove the potential for lock contention, and adding caching to the
legacy API responsible for generating this load to significantly reduce
the potential for positive feedback loops on these request paths and
others like them.  

**How are we making incidents like this less likely or less
impactful?  **

-   We have rolled back the ARM release globally which contain code
    relating to this performance regression. (Completed)
-   We have removed the recently introduced feature flag performance
    cache and hot path feature flags and features which depend upon it.
    (Completed)
-   We have implemented caching on the legacy API responsible for
    triggering this lock contention. (Completed)
-   We are working to implement and load test a feature flag performance
    cache which operates without the use of locking. (Estimated
    completion: April 2023)
-   We are exploring ways to improve ARM's rate limiting behavior to
    better avoid positive feedback loops. (Estimated completion:
    December 2023)
-   We are implementing additional automated Service Health messaging,
    to communicate more quickly with customers when an issue on
    front-end nodes occurs. (Estimated completion: December 2023)

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://www.aka.ms/AzPIR/RNQ2-NC8](https://www.aka.ms/AzPIR/RNQ2-NC8)

