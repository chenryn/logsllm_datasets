## [Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region] 

**April 29, 2011**

Now that we have fully restored functionality to all affected services,
we would like to share more details with our customers about the events
that occurred with the Amazon Elastic Compute Cloud ("EC2") last week,
our efforts to restore the services, and what we are doing to prevent
this sort of issue from happening again. We are very aware that many of
our customers were significantly impacted by this event, and as with any
significant service issue, our intention is to share the details of what
happened and how we will improve the service for our customers.

The issues affecting EC2 customers last week primarily involved a subset
of the Amazon Elastic Block Store ("EBS") volumes in a single
Availability Zone within the US East Region that became unable to
service read and write operations. In this document, we will refer to
these as "stuck" volumes. This caused instances trying to use these
affected volumes to also get "stuck" when they attempted to read or
write to them. In order to restore these volumes and stabilize the EBS
cluster in that Availability Zone, we disabled all control APIs (e.g.
Create Volume, Attach Volume, Detach Volume, and Create Snapshot) for
EBS in the affected Availability Zone for much of the duration of the
event. For two periods during the first day of the issue, the degraded
EBS cluster affected the EBS APIs and caused high error rates and
latencies for EBS calls to these APIs across the entire US East Region.
As with any complicated operational issue, this one was caused by
several root causes interacting with one another and therefore gives us
many opportunities to protect the service against any similar event
reoccurring.

**Overview of EBS System**

It is helpful to understand the EBS architecture so that we can better
explain the event. EBS is a distributed, replicated block data store
that is optimized for consistency and low latency read and write access
from EC2 instances. There are two main components of the EBS service:
(i) a set of EBS clusters (each of which runs entirely inside of an
Availability Zone) that store user data and serve requests to EC2
instances; and (ii) a set of control plane services that are used to
coordinate user requests and propagate them to the EBS clusters running
in each of the Availability Zones in the Region.

An EBS cluster is comprised of a set of EBS nodes. These nodes store
replicas of EBS volume data and serve read and write requests to EC2
instances. EBS volume data is replicated to multiple EBS nodes for
durability and availability. Each EBS node employs a peer-to-peer based,
fast failover strategy that aggressively provisions new replicas if one
of the copies ever gets out of sync or becomes unavailable. The nodes in
an EBS cluster are connected to each other via two networks. The primary
network is a high bandwidth network used in normal operation for all
necessary communication with other EBS nodes, with EC2 instances, and
with the EBS control plane services. The secondary network, the
replication network, is a lower capacity network used as a back-up
network to allow EBS nodes to reliably communicate with other nodes in
the EBS cluster and provide overflow capacity for data replication. This
network is not designed to handle all traffic from the primary network
but rather provide highly-reliable connectivity between EBS nodes inside
of an EBS cluster.

When a node loses connectivity to a node to which it is replicating data
to, it assumes the other node failed. To preserve durability, it must
find a new node to which it can replicate its data (this is called
re-mirroring). As part of the re-mirroring process, the EBS node
searches its EBS cluster for another node with enough available server
space, establishes connectivity with the server, and propagates the
volume data. In a normally functioning cluster, finding a location for
the new replica occurs in milliseconds. While data is being re-mirrored,
all nodes that have copies of the data hold onto the data until they can
confirm that another node has taken ownership of their portion. This
provides an additional level of protection against customer data loss.
Also, when data on a customer's volume is being re-mirrored, access to
that data is blocked until the system has identified a new primary (or
writable) replica. This is required for consistency of EBS volume data
under all potential failure modes. From the perspective of an EC2
instance trying to do I/O on a volume while this is happening, the
volume will appear "stuck".

In addition to the EBS clusters, there is a set of control plane
services that accepts user requests and propagates them to the
appropriate EBS cluster. There is one set of EBS control plane services
per EC2 Region, but the control plane itself is highly distributed
across the Availability Zones to provide availability and fault
tolerance. These control plane services also act as the authority to the
EBS clusters when they elect primary replicas for each volume in the
cluster (for consistency, there must only be a single primary replica
for each volume at any time). While there are a few different services
that comprise the control plane, we will refer to them collectively as
the "EBS control plane" in this document.

**Primary Outage**

At 12:47 AM PDT on April 21st, a network change was performed as part of
our normal AWS scaling activities in a single Availability Zone in the
US East Region. The configuration change was to upgrade the capacity of
the primary network. During the change, one of the standard steps is to
shift traffic off of one of the redundant routers in the primary EBS
network to allow the upgrade to happen. The traffic shift was executed
incorrectly and rather than routing the traffic to the other router on
the primary network, the traffic was routed onto the lower capacity
redundant EBS network. For a portion of the EBS cluster in the affected
Availability Zone, this meant that they did not have a functioning
primary or secondary network because traffic was purposely shifted away
from the primary network and the secondary network couldn't handle the
traffic level it was receiving. As a result, many EBS nodes in the
affected Availability Zone were completely isolated from other EBS nodes
in its cluster. Unlike a normal network interruption, this change
disconnected both the primary and secondary network simultaneously,
leaving the affected nodes completely isolated from one another.

When this network connectivity issue occurred, a large number of EBS
nodes in a single EBS cluster lost connection to their replicas. When
the incorrect traffic shift was rolled back and network connectivity was
restored, these nodes rapidly began searching the EBS cluster for
available server space where they could re-mirror data. Once again, in a
normally functioning cluster, this occurs in milliseconds. In this case,
because the issue affected such a large number of volumes concurrently,
the free capacity of the EBS cluster was quickly exhausted, leaving many
of the nodes "stuck" in a loop, continuously searching the cluster for
free space. This quickly led to a "re-mirroring storm," where a large
number of volumes were effectively "stuck" while the nodes searched the
cluster for the storage space it needed for its new replica. At this
point, about 13% of the volumes in the affected Availability Zone were
in this "stuck" state.

After the initial sequence of events described above, the degraded EBS
cluster had an immediate impact on the EBS control plane. When the EBS
cluster in the affected Availability Zone entered the re-mirroring storm
and exhausted its available capacity, the cluster became unable to
service "create volume" API requests. Because the EBS control plane (and
the create volume API in particular) was configured with a long time-out
period, these slow API calls began to back up and resulted in thread
starvation in the EBS control plane. The EBS control plane has a
regional pool of available threads it can use to service requests. When
these threads were completely filled up by the large number of queued
requests, the EBS control plane had no ability to service API requests
and began to fail API requests for other Availability Zones in that
Region as well. At 2:40 AM PDT on April 21st, the team deployed a change
that disabled all new Create Volume requests in the affected
Availability Zone, and by 2:50 AM PDT, latencies and error rates for all
other EBS related APIs recovered.

Two factors caused the situation in this EBS cluster to degrade further
during the early part of the event. First, the nodes failing to find new
nodes did not back off aggressively enough when they could not find
space, but instead, continued to search repeatedly. There was also a
race condition in the code on the EBS nodes that, with a very low
probability, caused them to fail when they were concurrently closing a
large number of requests for replication. In a normally operating EBS
cluster, this issue would result in very few, if any, node crashes;
however, during this re-mirroring storm, the volume of connection
attempts was extremely high, so it began triggering this issue more
frequently. Nodes began to fail as a result of the bug, resulting in
more volumes left needing to re-mirror. This created more "stuck"
volumes and added more requests to the re-mirroring storm.

By 5:30 AM PDT, error rates and latencies again increased for EBS API
calls across the Region. When data for a volume needs to be re-mirrored,
a negotiation must take place between the EC2 instance, the EBS nodes
with the volume data, and the EBS control plane (which acts as an
authority in this process) so that only one copy of the data is
designated as the primary replica and recognized by the EC2 instance as
the place where all accesses should be sent. This provides strong
consistency of EBS volumes. As more EBS nodes continued to fail because
of the race condition described above, the volume of such negotiations
with the EBS control plane increased. Because data was not being
successfully re-mirrored, the number of these calls increased as the
system retried and new requests came in. The load caused a brown out of
the EBS control plane and again affected EBS APIs across the Region. At
8:20 AM PDT, the team began disabling all communication between the
degraded EBS cluster in the affected Availability Zone and the EBS
control plane. While this prevented all EBS API access in the affected
Availability Zone (we will discuss recovery of this in the next
section), other latencies and error rates returned to normal for EBS
APIs for the rest of the Region.

A large majority of the volumes in the degraded EBS cluster were still
functioning properly and the focus was to recover the cluster without
affecting more volumes. At 11:30AM PDT, the team developed a way to
prevent EBS servers in the degraded EBS cluster from futilely contacting
other servers (who didn't have free space at this point anyway) without
affecting the other essential communication between nodes in the
cluster. After this change was made, the cluster stopped degrading
further and additional volumes were no longer at risk of becoming
"stuck". Before this change was deployed, the failed servers resulting
from the race condition resulted in an additional 5% of the volumes in
the affected Availability Zone becoming "stuck". However, volumes were
also slowly re-mirroring as some capacity was made available which
allowed existing "stuck" volumes to become "unstuck". The net result was
that when this change was deployed, the total "stuck" volumes in the
affected Availability Zone was 13%.

Customers also experienced elevated error rates until Noon PDT on April
21st when attempting to launch new EBS-backed EC2 instances in
Availability Zones other than the affected zone. This occurred for
approximately 11 hours, from the onset of the outage until Noon PM PDT
on April 21st. Except for the periods of broader API issues describe
above, customers were able to create EBS-backed EC2 instances but were
experiencing significantly-elevated error rates and latencies. New
EBS-backed EC2 launches were being affected by a specific API in the EBS
control plane that is only needed for attaching new instances to
volumes. Initially, our alarming was not fine-grained enough for this
EBS control plane API and the launch errors were overshadowed by the
general error from the degraded EBS cluster. At 11:30 AM PDT, a change
to the EBS control plane fixed this issue and latencies and error rates
for new EBS-backed EC2 instances declined rapidly and returned to
near-normal at Noon PDT.

**Recovering EBS in the Affected Availability Zone**

By 12:04 PM PDT on April 21st, the outage was contained to the one
affected Availability Zone and the degraded EBS cluster was stabilized.
APIs were working well for all other Availability Zones and additional
volumes were no longer becoming "stuck". Our focus shifted to completing
the recovery. Approximately 13% of the volumes in the Availability Zone
remained "stuck" and the EBS APIs were disabled in that one affected
Availability Zone. The key priority became bringing additional storage
capacity online to allow the "stuck" volumes to find enough space to
create new replicas.

The team faced two challenges which delayed getting capacity online.
First, when a node fails, the EBS cluster does not reuse the failed node
until every data replica is successfully re-mirrored. This is a
conscious decision so that we can recover data if a cluster fails to
behave as designed. Because we did not want to re-purpose this failed
capacity until we were sure we could recover affected user volumes on
the failed nodes, the team had to install a large amount of additional
new capacity to replace that capacity in the cluster. This required the
time-consuming process of physically relocating excess server capacity
from across the US East Region and installing that capacity into the
degraded EBS cluster. Second, because of the changes made to reduce the
node-to-node communication used by peers to find new capacity (which is
what stabilized the cluster in the step described above), the team had
difficulty incorporating the new capacity into the cluster. The team had
to carefully make changes to their negotiation throttles to allow
negotiation to occur with the newly-built servers without again
inundating the old servers with requests that they could not service.
This process took longer than we expected as the team had to navigate a
number of issues as they worked around the disabled communication. At
about 02:00AM PDT on April 22nd, the team successfully started adding
significant amounts of new capacity and working through the replication
backlog. Volumes were restored consistently over the next nine hours and
all but about 2.2% of the volumes in the affected Availability Zone were
restored by 12:30PM PDT on April 22nd. While the restored volumes were
fully replicated, not all of them immediately became "unstuck" from the
perspective of the attached EC2 instances because some were blocked
waiting for the EBS control plane to be contactable, so they could
safely re-establish a connection with the EC2 instance and elect a new
writable copy.

Once there was sufficient capacity added to the cluster, the team worked
on re-establishing EBS control plane API access to the affected
Availability Zone and restoring access to the remaining "stuck" volumes.
There was a large backlog of state changes that had to be propagated
both from the degraded EBS nodes to the EBS control plane and vice
versa. This effort was done gradually to avoid impact to the restored
volumes and the EBS control plane. Our initial attempts to bring API
access online to the impacted Availability Zone centered on throttling
the state propagation to avoid overwhelming the EBS control plane. We
also began building out a separate instance of the EBS control plane,
one we could keep partitioned to the affected Availability Zone to avoid
impacting other Availability Zones in the Region, while we processed the
backlog. We rapidly developed throttles that turned out to be too
coarse-grained to permit the right requests to pass through and
stabilize the system. Through the evening of April 22nd into the morning
of April 23rd, we worked on developing finer-grain throttles. By
Saturday morning, we had finished work on the dedicated EBS control
plane and the finer-grain throttles. Initial tests of traffic against
the EBS control plane demonstrated progress and shortly after 11:30 AM
PDT on April 23rd we began steadily processing the backlog. By 3:35PM
PDT, we finished enabling access to the EBS control plane to the
degraded Availability Zone. This allowed most of the remaining volumes,
which were waiting on the EBS control plane to help negotiate which
replica would be writable, to once again be usable from their attached
instances. At 6:15 PM PDT on April 23rd, API access to EBS resources was
restored in the affected Availability Zone.

With the opening up of API access in the affected Availability Zone,
APIs were now operating across all Availability Zones in the Region. The
recovery of the remaining 2.2% of affected volumes required a more
manual process to restore. The team had snapshotted these volumes to S3
backups early in the event as an extra precaution against data loss
while the event was unfolding. At this point, the team finished
developing and testing code to restore volumes from these snapshots and
began processing batches through the night. At 12:30 PM PDT on April 24,
we had finished the volumes that we could recover in this way and had
recovered all but 1.04% of the affected volumes. At this point, the team
began forensics on the remaining volumes which had suffered machine
