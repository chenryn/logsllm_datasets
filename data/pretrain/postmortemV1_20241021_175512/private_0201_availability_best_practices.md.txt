Availability Best Practices
January2007
Every year, ITUG, the Global HP User Community (formerly the International Tandem Users
Group), recognizes with the ITUG NonStop Availability Award the company who has
demonstrated the best availability practices during the prior year. Wendy Bartlett, an HP
Distinguished Technologist, has been documenting the practices of those companies who have
submittedentriesforthisawardandhasgivenmanypresentationsonthesepractices.Thisarticle
summarizesherfindings1withsomeadditionalsuggestionsofourown.
Availability best practices cover the entire gamut of a data processing operation. They start with
the development and use of robust software and proceed with good operator training and
documentedprocedures. Planneddowntimeisminimizedor eliminated,andrepair strategies that
leadtominimalrepairtimeareadoptedtominimizeunplanneddowntime.Weexamineavailability
bestpracticesinsomedetailbelow.
Fault-Tolerant Hardware
The system hardware should be configured to be fault-tolerant. It should be able to survive at
least any single failure. Failure points such as processors, disks, power supplies, fans and
communication controllers should be monitored; and the system should allow component repair
orreplacementwithoutthesystemhavingtobetakendown.
Redundant disk subsystems should be used for all critical data. These could be mirrored disk
pairs or RAID arrays. Network attached storage (NAS) or storage area networks (SAN) often can
offer more reliability and effective administration than can be afforded in a system using its own
direct-attachedstorage.
System and application resources such as swap space and file extents should be configured for
growthandthereaftermonitoredsothatactioncanbetakenifaresourceinnearingexhaustion.
Robust Software
Applications and data should be isolated so that a fault in one application will not corrupt other
applications or their data sets. No application should be able to reach in and modify another
applicationoritsdataset.
1
WendyBartlett,AvailabilityBestPractices,paperMA-03-HP;ITUGEurope2006.
1
Thisdocumentmaybeprintedbutmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

This is commonly accomplished by having applications interact through a messaging system. In
this way, an application can be defensive and can determine whether or not to honor a request
from another application. It has total control over its private data set. Object-oriented languages
suchasJavaaredesignedtodojustthis.
Applications should be designed to have as high a degree of fault tolerance as possible. They
should be able to determine that a problem has occurred and to report that problem. If possible,
an application process that has failed should be able to recover from a problem as quickly as
possible.
Fast recovery from a problem is best accomplished by having a backup process immediately
availabletoreplaceafailedprocess.Thiscanbedoneinseveralways,suchas
 having a pool of like processes so that if one fails, it can be removed from the pool, with
furthertransactionsprocessedbythesurvivingmembersofthepool.
 using an application monitor that can detect a failed process and then restart it
automatically.
 providing a hot backup process whose state is kept in synchronism with its active
companionandwhichisreadytotakeoverprocessingshouldtheactiveprocessfail.
Applicationsshouldbedesignedtohandleanyunanticipatedeventwithoutcrashing.Theyshould
also be designed to minimize or eliminate any disruption due to making normal configuration
changes,suchasaddingusers,addingterminals,ormodifyingglobalparameters.
Redundant Networks
Thesystemmustbeabletosurvivenetworkfaults.Thiscanbedoneby
 usingredundantprivatenetworks(WANsorLANs).
 usingself-repairingnetworks(suchasthepublic telephonesystem,thepublicInternet,or
aproperlyconfiguredprivateIntranet).
 providingabackupcommunicationfacilitysuchasdialedlines.
If redundant long-haul networks are used, theyshould be obtained from separate carriers that do
notuseacommoncarrierintheirbackbone,asthiscouldcauseasinglepointoffailure.
All connections to the network should be redundant. Dual communication controllers should be
used.
If a disaster recovery system is used, the communication facilities serving it should be
independentofthoseservingtheprimarysite.
The Environment
Bothpowerandcoolingshouldberedundant.Twoindependentpowersourcesshouldbeusedto
provide power for the computer systems. Each power source should come from a different
providerifpossible,oratleasttheyshouldberoutedthroughdifferentconduits.
Backuppowerintheformofanuninterruptiblepowersupply(UPS)shouldbeavailable.Sufficient
short-term batterypower should be provided to maintain system operation until the UPS power is
stable.
2
Thisdocumentmaybeprintedbutmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

The computer room cooling facilities should be redundant and should be powered by separate
powersources.Temperatureandhumiditymonitoringshouldbeprovided.
Problem Monitoring and Management
More important than environmental monitoring is system monitoring. There are an abundance of
monitoring tools in the marketplace for most systems. Not only should system resources be
monitored,butapplicationsshouldbemonitoredaswell.
Scripts should be created to perform tests to detect problems that the system monitoring tools
may not detect. For instance, routers should be pinged, the spooler status should be checked,
status commands should be issued on critical processes, and communication lines should be
monitored.
AsinglemonitoringenvironmentsuchasHP’sOpenVieworUnicenterfrom ComputerAssociates
shouldbeprovidedforallsystems.Responsestoproblemsfrom themonitoringsystemshouldbe
automatedtoavoidoperatorerror.Problemscanrange incomplexityfrom addingextentstoafile
thatisfillinguptoswitchingovertoabackupsystem.
The appropriate staff should be notified by email or paging of any problem, even if it is
automaticallycorrected(automaticcorrectionsdon’talwayswork).
A problem management system should be used to track problems, to ensure that they are
corrected,torecordthecorrection,andtomaintainahistorytodetectfailuretrends.
People
Operations and application support teams should be available on a 24x7 basis. Critical staff who
are not currentlyon site should be contactable byphone or pager in the event of a failure.Within
this group of people should be a 24x7 crisis team that can be rapidly assembled to handle any
criticalfailure.
There should be a full-time person assigned to monitor the performance of the system. That
person should be responsible for tuning performance parameters, for balancing the load on the
system, and for predicting future processing requirements so that there will be no overload
failures.
Thereshouldbeadailyhand-off betweenshifts toreviewopenproblem tickets.Weeklymeetings
should be scheduled to review all of the system problems which have been encountered since
thelastmeeting.
Most importantly, a root-cause analysis of each fault should be performed. The root cause of a
failure is that event which, if it had been corrected, would have precluded the failure. Root-cause
analyses are a powerful tool to tune up operational procedures as well as the automated
proceduresforfailurerecovery.
3
Thisdocumentmaybeprintedbutmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

Change Management
Changesarethelifebloodofasystem.Withoutchanges,thesystemwillbecomefunctionallyand
technologicallyobsolete.However,apreponderanceofsystemfailuresarecausedbychanges.2
Proper change management involves change tracking, documentation, review, testing and
monitoring.
Change tracking can best be done by using an existing change management system, such as
IBM’s Tivoli or Scopus. Ensure that all changes are entered into the tracking system. This
includes not only hardware and software changes but also environmental, network, facilities, and
procedurechanges.
Change documentation evaluates a change and describes the procedure for implementing it. It
shouldinclude
 arisk assessmentdescribingthebenefitsofthechangeas wellas thepotentialproblems
whichitmaycause.
 aproductionimpactassessment.
 adetailedplanfortheinstallation.
 proceduresformonitoringtheinstallation.
 adetailedchangetestplan.
 thechangesuccesscriteria.
 achangeback-outplanincasethechangecausesunacceptableproblems.
 contactinformationincasehelpisneeded.
There should be daily change review meetings to keep all technical staff updated on what
changes are being made. There should be a sign-off process for the various stages of the
change. The sign-off process can be aided bychange-checking tools, especiallyfor configuration
changes. Regular change reports should be generated for review by the technical groups and by
management.
A detailed test plan must be generated to ensure that the change is proper and has no
undesirablesideeffectsbeforeputtingitintoservice.Thistestplanshoulditselfbetestedearlyin
the process by having a non-expert run through the documented test procedures. The change
should first be tested on a development system, then on a QA system prior to moving it to the
production system and the backup system, if any. Performance tests should be run to compare
the system operation before and after the change. Finally, end users should be involved in the
finaltestingofthechange.
Theeffectofchangesshouldthenbemonitoreduntilthesuccesscriteriahavebeenmet.
Software Upgrades
A major class of changes is operating system and application upgrades. Operating system
upgrade information should be tracked to understand what corrections, enhancements, and new
features are included in each new version. Using this information, the proper time to install an
upgradecanbeintelligentlydetermined.
2
SeeRule23inBreakingtheAvailabilityBarrier:SurvivableSystemsforEnterpriseComputing,byDr.BillHighleyman,
PaulJ.Holenstein,andDr.BruceHolensten.
4
Thisdocumentmaybeprintedbutmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

Try not to upgrade to a new operating system version until it has been in service for a while at
othersites.Thiswillminimizethechanceofrunningintobugsinthenewversion.
A multiple-system architecture should be used to avoid planned downtime. The new operating
system or application version should first be installed on a non-production system and should be
thoroughly tested before putting it into production. Switchover from the old production system to
the new version must be carefully planned to minimize any application downtime. The best case
is the use of an active/active system which will allow nodes to be taken out of service, upgraded,
andputbackintoservicewithlittleifanyimpactontheusers.
Minimize Planned Downtime
The requirement for planned downtime should be minimized. For instance, batch runs should be
able to run concurrently with normal operations. This can be done by either designing batch
processing to be compatible with operations, or by providing an online copyfacility that can copy
a consistent version of the database to an offline system for batch processing. The results of the
batchprocessingrunmaythenhavetobemovedbacktotheonlinesystem.
Other sources of planned outages include database backups, database reorganizations, and
databasereplicationforshareddatapurposes.
To support major upgrades to hardware, software, and the database, switchover times between
the active and backup systems must be minimized. Switchover times are preferablymeasured in
minutes,nothoursordays.3
Repair Strategies
Just because there is a spare component to take over the functions of a failed component does
not mean that repair time can be casual. The longer the system is running with a single point of
failure,thelongeritisexposedtoacatastrophicsystemfailureshouldthesparecomponentfail.
System availability is directly proportional to the repair time of any failed component. If average
repair time can be improved bya factor of ten (sayfrom one dayto one hour), an additional nine
isaddedtothesystemavailability.
Repair time can be minimized by having onsite spares, onsite service personnel, rapid response
servicecontracts,andmanyothertechniques.
Repair time can be more casual if triple redundancy (or greater) is provided. Multinode
active/activesystemsarejustsuchanexample.4Thiscanhelpgreatlyinlights-outoperations.
Disaster Recovery Site
Any data processing site is subject to a variety of disasters that could take it out of service.
Among these causes are fire, flood, earthquake, social unrest (riots), malicious acts, and
radioactiveordiseasequarantines.
3
SeeTacklingSwitchoverTimes,theAvailabilityDigest;October,2006.
4
AnotherexampleistheHPNonStopserver’striplemodularredundancy(TMR)configuration.
5
Thisdocumentmaybeprintedbutmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

Therefore,ifcontinuousavailabilityistobeachieved,theremustbeabackupsiteseparatedfrom
the primary site by an appropriate distance. This site could contain another corporate-owned
system, or it could be a system provided as a backup service to several disparate customers. In
the latter case, it should be ensured that access to the backup system is reasonablyavailable as
thesesystemsareoftenprovidedonafirst-come,first-servedbasis.
Recovery time of the backup system is a critical parameter. There are several types of backup
configurations,includingacoldstandby,ahotstandbywithadatabasesynchronizedtotheactive
system, and an active/active system. Recovery time for a cold standby could be hours or days.
Recovery time for a hot standbycould be minutes to hours. Recoverytime to another node in an
active/activesystemis,ineffect,instantaneous.
Periodic testing of the switchover plan is paramount. Otherwise, should the backup be needed, it
may be found that the recovery procedures don’t work. An important exception to this rule is
backup provided in an active/active system. In these systems, all processing nodes are actively
processing transactions. Should a node fail, all transactions are routed to surviving nodes.
Therefore,itisalwaysknownthatthe“backup”systemisoperational.
Availability Management
A comprehensive application monitoring and failure management system should be provided.
Transaction volumes and response times should be measured to anticipate growing problems.
Audittrailsshouldbecreatedtoimprovetherecoveryofdata.
Everyoutage, no matter how minor, should be recorded. Information recorded should include the
nature of the failure, the root cause of the failure, the duration of the failure, the manner in which
the failure was corrected (hardware repair, software recovery, switchover to the backup system),
the number of users affected, and any other information that might be useful in later failure
analysesandproceduralreviews.
Summary
Wecanattempttominimizefailuresbyproperoperatortraining,consideredselectionofhardware
andsoftwarecomponents,redundantnetworks,andsoon.Butfailureswilloccur.
The secret to high availability is not to eliminate failures but to recover from them quickly. Only if
recoveryissorapidthattheuserdoesnotviewitasadenialofserviceisavailabilitymaintained.
There are many ways to recover from hardware, software, network, and environmental failures;
and they all require redundancy. There must be a backup component to immediately take over
thefunctionsofthefailedcomponentinatimesoshortthatitisnotvisibletotheusers.
The ultimate in fast failover are active/active systems. In this architecture, multiple nodes in an
active/activenetwork cooperateinacommonapplication,eachnodeprovidingall of thefunctions
of theapplication.Shouldanyonenodefail,allfurther work canberoutedtothesurvivingnodes.
Recoveryfromanykindoffaultcanbeachievedinseconds.
Itis for this reason that active/activesystems canachieve availabilityat the user levelof six 9s or
more,withfailureintervalsmeasuredincenturies.
6
Thisdocumentmaybeprintedbutmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman
