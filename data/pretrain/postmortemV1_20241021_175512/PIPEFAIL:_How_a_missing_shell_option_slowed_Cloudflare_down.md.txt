# PIPEFAIL: How a missing shell option slowed Cloudflare down

At Cloudflare, weâre used to being the fastest in the world. However, for approximately 30 minutes last December,Cloudflare was slow. Between 20:10 and 20:40 UTC on December 16, 2021, web requests served by Cloudflare were artificially delayed by up to five seconds before being processed. This post tells the story of how a missing shell option called âpipefailâ slowed Cloudflare down.BackgroundBefore we can tell this story, we need to introduce you to some of its characters.CloudflareâsFront Lineprotects millions of users from some of thelargest attacksever recorded. This protection is orchestrated by a sidecar service calleddosd, which analyzes traffic and looks for attacks. Whendosddetects an attack, it provides Front Line with a list of attack fingerprints that describe how Front Line can match and block the attack traffic.Instances ofdosdrun on every Cloudflare server, and they communicate with each other using a peer-to-peer mesh to identify malicious traffic patterns. This decentralized design allowsdosdto perform analysis with much higher fidelity than is possible with a centralized system, but its scale also imposes some strict performance requirements. To meet these requirements, we need to providedosdwith very fast access to large amounts of configuration data, which naturally means thatdosddepends onQuicksilver. Cloudflare developedQuicksilverto manage configuration data and replicate it around the world in milliseconds, allowing it to be accessed by services likedosdin microseconds.One piece of configuration data thatdosdneeds comes from theAddressing API, which is our authoritative IP address management service. The addressing data it provides is important becausedosduses it to understand what kind of traffic is expected on particular IPs. Since addressing data doesnât change very frequently, we use a simpleKubernetes cron jobto query it at 10 minutes past each hour and write it into Quicksilver, allowing it to be efficiently accessed bydosd.With this context, letâs walk through the change we made on December 16 that ultimately led to the slowdown.The ChangeApproximately once a week, all of our Bug Fixes and Performance Improvements to the Front Line codebase are released to the network. On December 16, the Front Line team released a fix for a subtle bug in how the code handled compression in the presence of aCache-Control: no-transformheader. Unfortunately, the team realized pretty quickly that this fix actually broke some customers who had starteddependingon that buggy behavior, so the team decided to roll back the release and work with those customers to correct the issue.Hereâs a graph showing the progression of the rollback. While most releases and rollbacks are fully automated, this particular rollback needed to be performed manually due to its urgency. Since this was a manual rollback, SREs decided to perform it in two batches as a safety measure. The first batch went to our smaller tier 2 and 3 data centers, and the second batch went to our larger tier 1 data centers.SREs started the first batch at 19:25 UTC, and it completed in about 30 minutes. Then, after verifying that there were no issues, they started the second batch at 20:10. Thatâs when the slowdown started.The SlowdownWithin minutes of starting the second batch of rollbacks, alerts started firing. âTraffic levels are dropping.â âCPU utilization is dropping.â âA P0 incident has been automatically declared.â The timing could not be a coincidence. Somehow, a deployment of known-good code, which had been limited to a subset of the network and which had just been successfully performed 40 minutes earlier, appeared to be causing a global problem.A P0 incident is an âall hands on deckâ emergency, so dozens of Cloudflare engineers quickly began to assess impact to their services and test their theories about the root cause. The rollback was paused, but that did not fix the problem. Then, approximately 10 minutes after the start of the incident, my team â the DOS team â received a concerning alert: âdosdis not running on numerous servers.â Before that alert fired we had been investigating whether the slowdown was caused by an unmitigated attack, but this required our immediate attention.Based on service logs, we were able to see thatdosdwaspanickingbecause the customer addressing data in Quicksilver was corrupted in some way. Remember: the data in this Quicksilver key is important. Without it,dosdcould not make correct choices anymore, so it refused to continue.Once we realized that the addressing data was corrupted, we had to figure out how it was corrupted so that we could fix it. The answer turned out to be pretty obvious: the Quicksilver key was completely empty.Following the old adage â âdid you try restarting it?â â we decided to manually re-run the Kubernetes cron job that populates this key and see what happened. At 20:40 UTC, the cron job was manually triggered. Seconds after it completed,dosdstarted running again, and traffic levels began returning to normal. We confirmed that the Quicksilver key was no longer empty, and the incident was over.The AftermathDespite fixing the problem, we still didnât really understand what had just happened.Why was the Quicksilver key empty?It was urgent that we quickly figure out how an empty value was written into that Quicksilver key, because for all we knew, it could happen again at any moment.We started by looking at the Kubernetes cron job, which turned out to have a bug:This cron job is implemented using a small Bash script. If youâre unfamiliar with Bash (particularlyshell pipelining), hereâs what it does:First, thedos-make-addr-confexecutable runs. Its job is to query the Addressing API for various bits of JSON data and serialize it into aTomldocument, which gets written Intoconfig.toml. Afterward, that Toml is passed as input into thedosctlexecutable, whose job is to simply write it into a Quicksilver key calledtemplate_vars.Can you spot the bug? Hereâs a hint: what happens ifdos-make-addr-conffails for some reason and exits with a non-zero error code? It turns out that, by default, the shell pipeline ignores the error code and continues executing! This means that the output ofdos-make-addr-conf(which could be empty) gets unconditionally written intodosctland used as the value of thetemplate_varskey, regardless of whetherdos-make-addr-confsucceeded or failed.30 years ago, when the first users of Bourne shell were burned by this problem, a shell option called âpipefailâ was introduced. Enabling this option changes the shellâs behavior so that, when any command in a pipeline series fails, the entire pipeline fails. However, this option is not enabled by default, so itâs widely recommended as best practice that all scripts should start by enablingthis (and a few other) options.Hereâs the fixed version of that cron job:This bug was particularly insidious becausedosdactually did attempt to gracefully handle the case where this Quicksilver key contained invalid Toml. However, an empty string is a perfectly valid Toml document. If an error message had been accidentally written into this Quicksilver key instead of an empty string, thendosdwould have rejected the update and continued to use the previous value.Why did that cause the Front Line to slow down?We had figured out how an empty key could be written into Quicksilver, and we were confident that it wouldnât happen again. However, we still needed to untangle how that empty key caused such a severe incident.As I mentioned earlier, the Front Line relies ondosdto tell it how to mitigate attacks, but it doesnât depend ondosddirectly to serve requests. Instead, once every few seconds, the Front Line asynchronously asksdosdfor new attack fingerprints and stores them in an in-memory cache. This cache is consulted while serving each request, and ifdosdever fails to provide fresh attack fingerprints, then the stale fingerprints will continue to be used instead. So how could this have caused the impact that we saw?As part of the rollback process, the Front Lineâs code needed to be reloaded. Reloading this code implicitly flushed the in-memory caches, including the attack fingerprint data fromdosd. The next time that a request tried to consult with the cache, the caching layer realized that it had no attack fingerprints to return and a âcache missâ happened.To handle a cache miss, the caching layer tried to reach out todosd, and this is when the slowdown happened. While the caching layer was waiting fordosdto reply, it blocked all pending requests from progressing. Sincedosdwasnât running, the attempt eventually timed out after five seconds when the caching layer gave up. But in the meantime, each pending request was stuck waiting for the timeout to happen. Once it did, all the pending requests that were queued up over the five-second timeout period became unblocked and were finally allowed to progress. This cycle repeated over and over again every five seconds on every server until thedosdfailure was resolved.To trigger this slowdown, not only diddosdhave to fail, but the Front Lineâs in-memory cache had to also be flushed at the same time. Ifdosdhad failed, but the Front Lineâs cache had not been flushed, then the stale attack fingerprints would have remained in the cache and request processing would not have been impacted.Why didnât the first rollback cause this problem?These two batches of rollbacks were performed by forcing servers to run aSalt highstate. When each batch was executed, thousands of servers began running highstates at the same time. The highstate process involves, among other things, contacting the Addressing API in order to retrieve various bits of customer addressing information.The first rollback started at 19:25 UTC, and the second rollback started 45 minutes later at 20:10. Remember how I mentioned that our Kubernetes cron job only runs on the 10th minute of every hour? At 21:10 â exactly the time that our cron job started executing â thousands of servers also began to highstate, flooding the Addressing API with requests. All of these requests were queued up andeventuallyserved, but it took the Addressing API a few minutes to work through the backlog. This delay was long enough to cause our cron job to time out, and, due to the âpipefailâ Â bug, inadvertently clobber the Quicksilver key that it was responsible for updating.To trigger the âpipefailâ bug, not only did we have to flood the Addressing API with requests, we also had to do it at exactly 10 minutes after the hour. If SREs had started the second batch of rollbacks a few minutes earlier or later, this bug would have continued to lay dormant.Lessons LearnedThis was a unique incident where a chain of small or unlikely failures cascaded into a severe and painful outage that we deeply regret. In response, we have hardened each link in the chain:A manual rollback inadvertently triggered the thundering herd problem, which overwhelmed the Addressing API. We have since significantly scaled out the Addressing API, so that it can handle high request rates if it ever again has to.An error in a Kubernetes cron job caused invalid data to be written to Quicksilver. We have since made sure that, when this cron job fails, it is no longer possible for that failure to clobber the Quicksilver key.dosddid not correctly handle all possible error conditions when loading configuration data from Quicksilver, causing it to fail. We have since taken these additional conditions into account where necessary, so thatdosdwill gracefully degrade in the face of corrupt Quicksilver data.The Front Line had an unexpected dependency ondosd, which caused it to fail whendosdfailed. We have since removed all such dependencies, and the Front Line will now gracefully survivedosdfailures.More broadly, this incident has served as an example to us of why code and systems must always be resilient to failure, no matter how unlikely that failure may seem.