
addition, a new displaysubsystem is being installed and features consoles that can displaymore
informationwithmuchhigherresolution.
Thefirst of the new Hocsr systems was installed at the Long Island ARTCC and began operation
onFebruary24,2007.
On Sunday, May 6, at 6:25 AM, as technicians were attempting to attach the new display
subsystem, the Hoscr system crashed. However, the old Host system was still configured as a
standby; and it immediately took over. Unfortunately, it, too, malfunctioned. The IT staff was able
to bring a third system into service, but it was much less capable. Controllers had to double the
spacing betweenplanes from 10miles to20miles.As aresult,274flights atLaGuardia,Newark,
and Philadelphia were delayed up to an hour or more, inconveniencing tens of thousands of
passengers.
Henry Brown, an official with the union that represents FAA electronics technicians, claimed that
the new system had not been thoroughly tested and that the FAA had rushed it into service to
demonstratethesuccessofitsmodernizationprogram.
FAA – Yet Again
Almost exactly one month later, the FAA got hit again. This time it was their flight planning
computerinAtlanta.
Before a commercial flight can take off, the pilot must file a flight plan indicating his route,
departure time, time enroute, altitude requested, and several other details. Flight plans are filed
electronically with the Atlanta system, which then routes them to the appropriate air traffic
controllers. Each day, approximately 50,000 domestic flight plans are filed in addition to those
requiredforinternationalflights.
About 9 AM on Friday, June 8, 2007, the Atlanta flight-plan system went down. No worries – the
mirrored backup system in Salt Lake City immediately took over flight-plan processing as
planned, and all flight plans were rerouted to it. However, this system was underconfigured and
was quickly overwhelmed. Air traffic controllers had to input tens of thousands of flight plans
manually, a very time consuming process. Since a plane could not take off until it had a valid
flightplanfiled,delaysacrossthecountryaffectedtensofthousandsofpassengers.
TheAtlanta system was recovered twohours later, at 10:52 AM; but it took another two hours for
the New York systems to reconnect to the Atlanta system. The New York airports, which are in
themostheavilycongestedairspaceintheUnitedStates,experienceddelaysofuptofourhours.
This already terrible situation was aggravated by a severe line of thunderstorms stretching from
CanadatoTexas.
Oh,well.Ifyouhavetimetospare,gobyair.
M&T Bank Snafu Delays Deposits by 24 Hours
M&T Bank isahighlyregardedregionalbank with650branchesinNewYork,Pennsylvania,New
Jersey,Delaware,Maryland,Virginia,WestVirginia,andWashington,D.C.
But even the best aren’t perfect. On Thursday, June 6, 2007, the bank’s automated system that
updates customer accounts at the close of the business day failed. Once the problem was
corrected,ittookuntilFridayafternoonat5:30PMtocompletethepostings.
4
©2007SombersAssociates,Inc.,andW.H.Highleyman

Theproblem especiallyaffectedcustomerswhocountedonwithdrawingmoneyimmediatelyafter
regularlyscheduleddirectdepositofpaychecksintotheiraccounts.
The bank did all they could to notify customers of the problem via their Web site, through their
telephone banking services, and at their local branches. They even sent area managers to the
localbranchestohelpcustomersaccesstheirfunds.
As a result, said M&T Bank spokesman Mike Zabel, “People are generally very understanding
when youmakeamistakeor when you haveaproblem.Everybodyknows whatitmeans tohave
acomputerglitch.It’shappenedtoeverybody.”
All Nippon Airways Glitch Delays or Cancels Hundreds of Flights
All Nippon Airways (ANA) is Japan’s largest domestic carrier. Its computer systems for
reservations and departure control are provided by Unisys, whose equipment ANA has been
usingforthelast25years.
Its system includes a host system (currently being upgraded by Unisys) that, among other tasks,
connects to intermediate computers that serve check-in terminals at the airports served by ANA.
In mid-May of 2007, three of the six intermediate computers were replaced with new systems.
However, after this upgrade, it was suspected that there was a problem with the new computers;
andtheoldsystemsweremovedbackintoplace.
It was only after this move that it was determined that the problem was not a computer problem
after all but rather a problem with the communication channels between the host computer and
theintermediatecomputers.
Itlooks likethis problem caughtup withANAon Sunday,May27,whendataflowingbetweenthe
host computer and the intermediate computers slowed to a crawl. This lead to a backlog of data
that soon overwhelmed the system. The impact of the resultant delay in data delivery led to the
cancellation of 130 flights and the delay by more than one hour of 306 flights. Almost 70,000
passengerswereinconvenienced.
Bythenextday,ANAwasabletoreturnoperationstonormal.
And Then There Was BlackBerry
Perhaps the failure that was most obvious to the most people occurred when the BlackBerry
system went down on Tuesday, April 17, 2007. We reported on this failure in detail in our May,
2007, Never Again article, Blackberry Gets Juiced. Millions of BlackBerry subscribers in North
America were without service until Wednesday morning, when queued-up emails from the huge
backlog of messages started to trickle through. It
wasn’t until Thursday that service was returned to
normal.
BlackBerrys are handheld devices that are used to
sendandreceiveemailsalmostanywhereintheworld.
The BlackBerry service has been highly reliable and
has attracted bankers, lawyers, journalists, law
makers,andbusinesspeople.
The BlackBerry devices and their associated services
are provided by a Canadian company, Research in
MotionLimited(RIM),ofWaterloo,Ontario.
5
©2007SombersAssociates,Inc.,andW.H.Highleyman

On Tuesday, April 17, at 5 pm PDT, BlackBerry messages stopped flowing to subscribers in
North America. The problem was in BlackBerry’s Canadian Network Operations Center (NOC),
whichservesNorthAmerica.
However, RIM management refused to acknowledge that they had a problem (though I suspect
thatthecomputerroomwasinmayhem).
It wasn’t until twelve hours later, on Wednesday morning, that RIM management acknowledged
that they had, in fact, had an outage. But they gave no reason for it and no prognosis of what to
expect.
ByWednesday morning, the system had been returned to service; but it had to deal with a huge
email backlog that had accumulated during the outage. These emails started to trickle out
Wednesday morning, but new emails that came in entered the end of the queue and were
delayedforhours.ItwasnotuntilThursdaythatoperationsreturnedtonormal.
Once operations normalized, RIM management began to release details of the outage. They
reported that the outage was triggered by the “introduction of a new noncritical system routine”
designedtooptimizecacheperformance.Theyhadnotexpectedthischangetoaffecttheregular
operationsofBlackBerry.
However, despite previous testing, the new system routine set off an unexpected chain reaction.
It triggered a series of interaction errors between the system’s operational database and cache
andresultedintheoutageoftheNOC.
After the RIM technicians isolated the database problem and tried unsuccessfully to fix it, they
beganthefailoverprocess tothebackupsystem.Buthorrorofhorrors,thefailoverattemptfailed,
despite failover having been previously tested. The consequence was the extended outage that
affectedmillionsofsubscribers.
The system failure was just one of the failures during this outage. The other was a near lack of
communication from RIM management to its subscribers. RIM’s tight-lipped response angered
many.GrumblescouldbeheardfromtheWhiteHousetotheCanadianParliament.
Lessons Learned
Half of these failures (Dow Jones, BlackBerry, and the two FAA failures) involved backup
systems that didn’t perform properly, either because they were underconfigured or because the
failover attempt failed. Failover is a complex and risky business. It is imperative that full failover
be frequently tested, as expensive as it may be, to ensure that the backup system is configured
properly, that procedures are up-to-date, and that personnel are properly trained. A failover fault
cancostacompanyawholelotmorethanperiodicfailovertesting.
Five of these failures occurred just after a reconfiguration (US Airways, Canadian Revenue
Agency, FAA, All Nippon Airways, and BlackBerry). Reconfigurations are an especially risky
process.Anewconfigurationmustbethoroughlytestedbeforeputtingitintoservice.
These two failure modes accounted for seven of our eight stories. The lesson to be learned may
seem trite when stated, but it is simply that the first three fundamental rules for continuous
availabilityaretest,test,andtest.
6
©2007SombersAssociates,Inc.,andW.H.Highleyman
|---|--|
| 0 |  |
