Amazon’s Cloud Downed by Fat Finger
May2011
Amazon’s Elastic Compute Cloud (EC2) is arguably today the leading service for deploying
custom applications in a cloud environment. Amazon has gone to great lengths to ensure the
availability of its cloud services. It has broken its cloud infrastructure into multiple regional,
geographically-separateddatacenters;andwithineachregionitprovidesindependentAvailability
Zones. A customer can run a critical application in multiple Availability Zones within a region to
ensureavailability.Ifdesired,anapplicationcanalsohavearedundantinstanceinotherregions.
Yet just after midnight Pacific Daylight Savings Time on April 21, 2011, a maintenance error took
down an entire Availability Zone in Amazon’s U.S.-East Region, located in northern Virginia. The
other three AvailabilityZones intheregion werealso affected,eventhoughthe AvailabilityZones
weresupposedtobeindependent.
Evenworse,theresultofthismaintenanceerrorwasnottotaketheclouddownforafewminutes
or even a few hours.It was not until late April24th, four days later, that theU.S.-EastRegion was
fullyreturnedtoservice.
The EC2 Architecture
TheEC2regional datacenters aretrulyindependentof each other.Thearchitectureof interest is
that of the AvailabilityZones within a region. Each regional data center is configured with several
Availability Zones. The U.S.-East Region data center, for instance, has four Availability Zones.
The Availability Zones are intended to be completely independent of each other. Should one fail,
the others continue uninterrupted. (As we shall see in the following description of Amazon’s
outage,thisobservationisflawed.)
As an option, Amazon supports customers who have critical applications running in one
Availability Zone to be backed up by other instances of those applications running in different
AvailabilityZones(albeitinthesameregion).
Tounderstandhowthisfailureoccurred,itis necessarytounderstandthearchitectureofaregion
anditsAvailabilityZones.
ElasticBlockStore
The heart of each Availability Zone (AZ) is a clustered, highly redundant database called the
Elastic Block Store (EBS). EBS is a distributed, replicated block store optimized for consistency,
availability,andlow-latencyread/writeaccess.
1
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

EBS provides the data storage for the applications running in the AZ – the EC2 instances. An
EBS cluster comprises a set of EBS nodes that store replicas of EBS volume data and that
execute read/write requests issued by the EC2 instances. The contents of each volume are
replicated to multiple backup EBS nodes. Fast failover provides recovery from a primary node
failureinmilliseconds.
Acontrolplaneprovides services thatcoordinateuser requests andthatpropagateuser requests
betweenEBSclustersineachoftheAZsintheregion.
The nodes within an EBS cluster are interconnected via two networks. The primary network is a
high-bandwidth network that is used for normal communication between the EBS nodes within
the cluster, with EC2 instances serviced by the cluster, and with the EBS control-plane services.
The secondary network is a lower-capacity network used as a backup for the primary network
shoulditfailandtoprovideoverflowreplicationcapacityifneeded.
controlplane
elasticblock elasticblock elasticblock
store(EBS) store(EBS) store(EBS)
volumes volumes volumes
EC2 EC2 EC2
instance instance instance
AvailabilityZone AvailabilityZone AvailabilityZone
AnAmazonEC2Region
Remirroring
Whenanodeloses connectivityto oneof its replica nodes,itsearches for anew node to which it
can replicate its data. This is called remirroring. To do this, the primary node searches its EBS
cluster for another node that has enough storage capacity to act as a replica. It typically takes
only milliseconds to locate a node that can act as a new replica, and the transfer of volume data
istheninitiatedtothenewreplica.
To ensure consistency, access to volumes that are being remirrored are blocked until the
remirroring has been completed and a primary replica is identified. While this is happening, EC2
instancesattemptingtoaccessthisdataareblocked–inEC2terms,theyarestuck.
TheEBSControlPlane
A control plane provides common services to all EBS clusters in a region. It routes user requests
to the appropriate EBS cluster even if that cluster is in a different AZ than the requesting EC2
instance. It also acts as the authority to ensure that there is one and only one primary replica for
each volume at anyone time.Byinterconnecting the AZs in a region, the control plane is the key
toprovidinghighavailabilityandfaulttolerancefortheEC2instances.
HighAvailability
An EC2 instance onlyneeds to run in one AZ. However, it is then subject to the failure of that AZ
(forinstance,shouldtheAZ’sEBSclusterfail).
2
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

To guarantee availability even in the event of an AZ failure, a customer can elect to run a critical
application in multiple AZs. Through control-plane services, the contents of an application’s
database running in one AZ is replicated to the EBS clusters in the backup AZs. Therefore,
shouldanAZfail,theEC2instancecanberapidlyfailedovertooneofitsbackupAZs.
A customer can run multiple instances of an application in multiple regions if it is desired to
protect the application services from an entire regional data-center failure. However, Amazon
doesnotprovidedatareplicationbetweenregions.Thisistheresponsibilityoftheapplication.
The Outage
This is a tale of multiple faults, one following the other to cascade an outage that should have
been minutes into days. Following the failure, Amazon published a detailed account1 of what
happened (an unusual act of transparencyfor data centers of anykind). Its report is summarized
below.
HumanError–TheFatFinger
The outage began with a normal maintenance activity. Shortly after midnight on April 21st,
maintenance to upgrade the capacity of the primary network of one of the AZs in the eastern
region was begun. The primary network is a redundant network. The first step was to shift all
trafficoffofoneoftheredundantrouterssothatitslinkcouldbeupgraded.
Unfortunately,thetrafficshiftwasexecutedincorrectly.Ratherthanshiftingalltraffictooneofthe
high-speed primary links, all traffic was instead rerouted to the much slower secondary network.
The secondary network could not handle the traffic; consequently, the nodes in the cluster were
effectively isolated from one another. So far as the nodes were concerned, their replicas were
down.
TheRemirroringStorm
The network was quickly restored by the maintenance personnel. At this point, the nodes started
searching the cluster for other nodes that they could use to remirror their data. However,
whatever freespace was availableinthe cluster was quicklyexhausted,leaving manynodes ina
stuckstatecontinuouslysearchingtheclusterforfreespace.
This remirroringstorm exposedan unknown bug in theEBS software and causeda nodetofail if
it was trying to close a large number of replication requests. This caused more nodes to fail and
increasedtheintensityoftheremirroringstorm.
The heavy control-plane traffic caused it to run out of threads. As a result, it could not service
requestsfromotherAZsandbegantofailthoserequests.
The Recovery That Took Days
TherecoveryeffortwasintensefortheAmazonstaff:
April21st
2:40 AM: Create Volume requests in the affected AZ were disabled to decrease the load on the
controlplane.
1SummaryoftheAmazonEC2andAmazonRDSServiceDisruptionintheUS-EastRegion
http://aws.amazon.com/message/65648/
3
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

2:50AM:LatenciesanderrorratesforotherEBSfunctionsreturnedtonormal.
5:30 AM: The latencies and error rates again increased for requested EBS functions due to
control-plane overload. The control plane acts as the authority to negotiate the remirroring of a
volume and designate a new primary volume. As more nodes failed due to the EBS bug
describedpreviously,thevolumeofcontrol-planenegotiationsincreased.Thiscausedabrownout
ofthecontrolplaneandaffectedEBSclustersacrosstheregion.
8:20 AM: The team began disabling all communication between the degraded EBS cluster and
thecontrol plane. Error rates and latencies returned to normalfor all other AZs, butaccess tothe
affectedAZwasnowdisabled.
11:30 AM: The affected cluster was still degrading as the remirroring storm continued. The team
devised a way to prevent EBS servers in the degraded cluster from futilely contacting other
serversintheclusterthatdidnothaveanyspacetoshare.Theclusterstoppeddegradingfurther.
Volumesstartedtoremirrorslowly,allowingstuckvolumestobecomeunstuck.
At this time, another problem was resolved. EC2 instances in AZs other than the affected AZs
were experiencing elevated error rates and latencies due to the control-plane overload. Making
alarms morefine-grained allowed these conditions to be exposed and corrected more readily. By
noontime,theotherAZshadreturnedtonormaloperation.
Atthispoint,thedegradedAZhadstabilized;andtheotherAZswereworkingnormally.
April22nd
When a node fails, it cannot be reused until all replicas in which it was involved have been
remirrored. But there was no space available for remirroring. Therefore, significant new storage
hadtobeadded.This requiredphysicallyrelocatingexcess server capacityfrom across theU.S.-
EastRegiontothedegradedAZ.
However, due to changes made to throttle control-plane negotiations to help calm the remirroring
storm, it was difficult to install the new capacity in the cluster. The team had to carefully modify
the negotiation throttles to allow negotiation with the new nodes without inundating the old nodes
withrequeststhattheycouldnothandle.
12:30PM:Mostremirroringwascompleted.However,EC2instancescouldnotreachmanyofthe
newlyfunctionalnodesbecausetheircommunicationwiththecontrolplanewasstillblocked.
April23rd
AseparateinstanceofthecontrolplanewasbuilttoservicejustthedegradedAZ.Inthisway,the
heavytrafficrequiredtobringthenodesbackonlinewouldnotaffecttheotherAZs.
11:30 AM: Processing began of the backlog of requests required to return the EBS nodes to
service.
3:35PM:AccesstothecontrolplanebythedegradedEBSwasenabled.
6:15PM:Accesstoallbut2.2%oftheEBSwasrestored.
4
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

April24th
Therecoveryoftheremaining2.2%ofvolumesrequiredrestoringsnapshotsthathadbeentaken
earlyintheeventasaprecaution.
12:30 PM: All but 1.04% of the volumes had been restored. The team began forensics on the
volumesthathadsufferedmachinefailureandthatcouldnotbebackedup.
3:00PM:Theteam beganrestoringthevolumesthathadnobackups.Ultimately,allbut0.07%of
thedatawasrestored.Theremainingdatawasirretrievablylost.
Relational Data Services Also Taken Down
Amazon’s Relational Data Services (RDS) facility allows customers to operate RDS instances
either in a single AZ (single-AZ) or across multiple AZs (multi-AZ). Since RDS instances access
multiple volumes, a single-AZ instance will be stuck if any one of its volumes becomes stuck.
Thus,almosthalfofthesingle-AZRDSinstancesrunninginthedegradedAZfailed.
Multi-AZ synchronously replicates data in the primary volume to a volume in another AZ in the
same region. If the primary volume fails, RDS automatically fails over to the replicated backup.
Therefore, multi-AZ will survive an AZ failure. In this case, all but 2.5% of the affected multi-AZ
RDS instances failed over. The unsuccessful failovers were due to a previously unknown bug in
thefailovermechanism.
Lessons Learned and Future Enhancements
This outage event was caused by a long string of root causes interacting with each other.
Amazon, in its outage report, listed several actions that it will take to prevent such an occurrence
inthefuture:
1. The trigger for this outage was a network configuration change. Maintenance procedures
will be further automated. However, the system should have been able to tolerate this
error.
2. Amazonwillmakeanumberofchangestopreventaremirroringstorm:
a. ExcessstoragecapacitywillbeaddedtotheEBSclusters.
b. Retry logic will back off more aggressively when a large interruption occurs and
will focus on reconnecting with previous replicas rather than futilely trying to find
storagespaceonothernodes.
3. Thecontrolplanewillbeimproved:
a. Timeout logic will be improved to prevent thread exhaustion when an AZ cluster
istakingtoolongtoprocessrequests.
b. The capability to be more AZ-aware and to shed load intelligently when the
controlplaneisoverloadedwillbeadded.
c. EBS-relatedserviceswillbemovedoutofthecontrolplaneintotheEBS.
4. Multi-AZdeploymentswillbemadeeasiertodesignandoperate.
5. Therecoverymodelsforthevarioustypesofvolumerecoverywillbeautomated.
6. Snapshots of stuck volumes will be allowed to more easily recover applications in other
AZs.
7. Customercrisiscommunicationswillbeimproved.
5
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Amazon will give a ten-day credit to all customers for their usage of EBS volumes, EC2
instances,andRDSinstancesintheaffectedAZ.
Post Script
Amazon’soutagesupportsthecautionthat“thecloudcanbedangeroustoyourhealth.”True,the
cloud brings with it numerous advantages, including being able to provision new applications
quickly and reliably while paying only for what you use. Even with the spectacular cloud failures
that are periodically reported in the press, it is likely that the cloud providers provide better
availability,performance,andsecuritythanadatacenterthatyoumightoperate.
Still, if you have an application that is so critical that you can tolerate no downtime, you have to
take precautions that may extend beyond the cloud. Using Amazon’s Availability Zones for
redundancy, especially across regions, is a good start. But you may want to have further control
over yourdestiny.
A good case in point is Mashery, an Amazon EC2 user. Masheryhelps engineer and monitor the
APIs that tie a company’s service to its customers. Mashery’s customers include the New York
Times,Netflix,andBestBuy.
Mashery made the assumption that everything in the cloud will fail, so it set up a failover site via
an outside data-center service supplier, InterNAP.2 Switchover from EC2 to the InterNAP
instance of its application is accomplished simply by changing the URL routing in its Domain
NameSystem(DNS)server.
When the EC2 outage occurred, Mashery’s failover to its backup site functioned as expected. Its
monitoring and reporting traffic was rerouted from Amazon’s U.S.-East Region to its backup
systems at InterNAP. Its services were down for about two minutes while failover was taking
place.
Another example is that of Bizo, which provides web-based business-marketing services. Bizo
uses two Availability Zones in the U.S.-East Region backed up by two Availability Zones in
Amazon’s second North American region in northern California. When the outage occurred, Bizo
was able to restore service by DNS rerouting just as Mashery did. Failover took about eight
minutes.
These are examples of backing up truly critical cloud applications either in another cloud as in
Bizo’s case or as in Mashery’s case to a data center outside of the cloud. A technique similar to
these should be considered mandatory if you decide to commit a highly critical application to the
cloud.
2AmazonCloudOutageProvesImportanceofFailoverPlanning,InformationWeek;April28,2011.
6
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com