Singapore Bank Downed by IBM Error
August2010
Déjà vu. Just four months ago, in our April issue, we related Google’s experience1 with
incomplete documentation that took down the entire Google Apps data center for two and a half
hours. A major Singapore bank, DBS Bank, has just had a repeat of that experience. Only in this
case, its systems were down for up to nine hours during a busy banking day. Gone were its
onlinebanking,itsATMsandcredit-cardservices,anditsbackofficesystems.
The problem was caused by an IBM employee who directed operations staff to use an outdated
proceduretoperformmaintenanceonadisk-storagesystem.Thecorrectprocedurehadyettobe
documented.
Thebank compoundedthe problem bywaitingtoolong todustoffits business continuityplan.By
thetimethebankconveneditsdisasterrecoveryteam,thecrisiswasalmostover.
DBS Bank
DBSBank isthelargestbank inSoutheastAsiaandaleadingbank in
Singapore and Hong Kong. With four million customers, 1.4 million of
which do online banking, DBS manages almost 1,000 ATMs and 100
branches in Singapore and Hong Kong and has operations in 15
markets, including China, Taiwan, India, Thailand, Malaysia, the
Philippines,andIndonesia.
The bank began operations in 1968 as a development financing institution led by the Singapore
government. It was known as the Development Bank of Singapore until it changed its name to
DBSBankin2003toreflectitsgrowingroleasamajorregionalbankinSoutheastAsia.
The Outsourcing Decision
In 2002, DBS contracted with IBM to run much of its data-center services. The outsourcing
contract was set for ten years at a cost of S$1.2 billion (a Singapore dollar was worth about 0.73
U.S. dollars at the time). As part of the outsourcing arrangement, 500 DBS Bank staff were
transferred to IBM. IBM built new data-center facilities in Singapore and Hong Kong to house the
DBSITsystems.
1PoorDocumentationSnagsGoogle,AvailabilityDigest;April2010.
http://www.availabilitydigest.com/public_articles/0504/google_power_out.pdf
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

An important element of IBM support is the IBM Asia-Pacific team. Among other capabilities, this
team is the central support for all IBM storage systems in the Asia-Pacific region, whether they
areinhouseoroutsourced.ThisteamwastoplayamajorroleleadingtotheDBSoutage.
The Outage
EarlyWarning
The timeline for this outage began about forty hours before the actual outage occurred. On
Saturday morning, July 3rd, IBM operations staff in the Singapore data center began to receive
alert messages. The messages indicated instability in a communications link within a major
storagesystem usedbymostofthebank’smainframeapplications.However,thebank’ssystems
arehighlyredundantandaredesignedfor highresiliency.Consequently, thestoragesystem was
stillfullyfunctional.
Therefore, the problem was classified as “low severity;” and corrective maintenance was
scheduledfor3AM,Monday,July5th,aquiettimeforthebank.
TheMaintenanceFiasco
A summary timeline compiled by BusinessTimes.com.sg, which cited DBS and IBM as its
sources,showedthefollowingsequenceofevents:2
"July3,11:06a.m.:IBMsoftware-monitoringtoolssentanalertmessagetoIBM'sAsia-Pacific
supportcentreoutsideSingapore,signalinganinstabilityinacommunicationslinkinthestorage
systemconnectedtoDBS'smainframecomputer.AnIBMfieldengineerwasdispatchedtothe
DBSdatacentre.
"July3,7:50p.m.:Theengineerreplacedacable,notusingthemaintenanceinstructionsonthe
machinebutthosegiventohimbythesupport-centrestaff.Althoughthiswasdoneusingan
incorrectstep,theerrormessageceased.
"July4,2:55p.m.:Theerrormessagereappeared,thistimeindicatinginstabilityinthecableand
associatedelectroniccards.TheIBMengineerwasdispatchedagaintothedatacentre.He
askedtheregionalIBMsupportcentreforadvice.
"July4,5:16p.m.:Followinginstructionsfromthesupport-centrestaff,theengineerremovedthe
cableforinspectionandputitbackusingthesameincorrectstep.Theerrormessageceased.
"July4,6:14p.m.:Theerrormessagereappeared.Overthenext5hoursand22minutes,the
regionalIBMsupportcentreanalysedthelogfromthemachineandrecommendedtothe
engineerthatheunplugthecableandlookforabentpin.Throughoutallthis,thestoragesystem
wasstillfunctioning.
"July4,11:38p.m.:Theengineerdidnotfindabentpinandputthecableback.Theerror
messagepersisted.Theregionalsupportcentreandtheengineercontinuedtryingtouncoverthe
problem,includingunpluggingthecableandputtingitbackagain.DBSwascontactedand
authorisedacablechangeat2:50a.m.,aquietperiod.Whilewaitingtoreplacethecable,the
IBMengineerdecidedtoinspectthecableagainfordefectsandtocheckthatitwasinstalled
properly.Heunpluggedthecable,againusingtheincorrectprocedureadvisedbytheregional
support-centrestaff.
2BobEvans,GlobalCIO:IBM’sBankOutage:AnatomyOfADisaster,InformationWeek;August5,2010.
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

"July5,2:58a.m.:Hereplacedthecableusingthesameproceduresasbefore.Thiscaused
errorsthatthreateneddataintegrity.Asaresult,thestoragesystemautomaticallystopped
communicatingwiththemainframecomputerinordertoprotectthedata.Atthispoint,DBS
bankingservicesweredisrupted."
As reported, though this was a routine repair, the IBM repair crew in Singapore sought advice
from the regional Asia-Pacific team on how to fix what they thought to be a minor fault in the
storage system. A member of that team suggested a procedure that had been regularly used
before.Unfortunately, the procedurehad beenmodifiedandevidentlyhadnotbeendocumented.
Theprocedurecausedthestoragesystemtocrash.
For some reason, the procedure also caused the
backup systems to fail. There has been no comment
from either the bank or IBM as to why this happened.
Suffice it to say that most of the bank’s services were
now down. Online banking was dead. ATMs were
silent. Credit cards could not be used. Commercial
bankingwashalted.
At 3:40 a.m. on July 5th, IBM mobilized the “technical
command function,” a crisis support team. At 5:20 AM,
a restart of the system was attempted; but it didn’t
work. The crisis was escalated, and the bank-wide “disaster recovery command center” was
activated.However,by8:30AM,itappearedthatthesystem wasclosetorestorationandthatthe
commandcenterwasnotneeded.
The ATM machines and credit-card operations were restored by 10 a.m., seven hours after the
outage.Itwasanothertwohoursbeforeonlinebankingserviceswererestoredatnoontime.
Branches opened as usual at 8:30 a.m., but their consoles were dead. The bank authorized the
cashing of checks for up to S$500, and customers could make cash withdrawals over the
counter.Thebranchesstayedopenanextratwohoursthateveningtoclearupbacklogs.
The bank reported that, fortunately, no data was lost and that all transactions that day would
completebyendofday.
Management’sResponse
IBMtook fullresponsibilityfor theoutage.Itsaidthat“thefailuretoapplythecorrectprocedureto
fix a simple problem in the data-storage system it maintains for DBS had crashed most of the
bank’ssystems.”Itcommittedtoimprovethetrainingofitssupportpersonnel.
It is unusual in Singapore for a CEO to make a public apology. However, Plyush Gupta, DBS’
CEO, issued a three-page detailed apology to the bank’s
customers. He stated that “a procedural error triggered a
malfunction in the multiple layers of systems redundancies,
whichledtotheoutage.”
Gupta also acknowledged that the bank should have escalated
the problem earlier and that it could have done more to
mobilize broadcast channels to inform customers of the
interruptioninservicesthefirstthinginthemorning.
He did confirm, however, that no data was lost as the result of
PlyushGupta,DBSCEO
the outage and that all payments and transactions of the day Photo:ArthurLee/BT
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

hadbeenproperlyprocessed.
The Regulatory Response
The Monetary Authority of Singapore (MAS) is Singapore’s central bank and financial regulator.
Among its many responsibilities is to write regulatory requirements for banking-system outages.
In its Internet Banking and Technology Risk Management Guidelines, MAS states that users
expect online banking services to be accessible “24 hours every day of the year.” This is
“tantamount to near-zero downtime.” MAS guidelines further state that “a bank’s responsibilities
andaccountabilities arenot diminishedor relievedbyoutsourcingits operations to thirdparties or
joint-venturepartners.”
DBS Bank certainlyfell short of the reliabilitybenchmarks laid out byMAS. In a statement issued
after the outage, MAS said that it would “assess the extent to which the bank has failed to meet
the recommended standards set out in the Internet Banking and Technology Risk Management
Guidelinesbeforedeterminingtheappropriateregulatoryactiontotake.”
Subsequently, on August 4th, MAS censured DBS for “shortcomings and inadequate
management oversight by the bank of its outsourced IT systems.” It ordered DBS to redesign its
online and branch-banking platforms in order to reduce concentration risk and to allow greater
flexibility and resiliency in its operation and recovery capabilities. It also required the bank to
improveitscustomer-communicationsprocedures.
MASalsodirectedthebanktosetasideS$230millionadditionalregulatorycapitalforoperational
risk.
On IBM’s part, it said that it has disciplined the personnel that were directly involved with the
outage and removed them from direct customer-support activity. IBM also has appointed
technicaladvisorstoprovidedeepertechnicalexpertisetoDBS.
Lessons Learned
As we learned from the Google power-outage disaster that we covered this last April, all
procedure changes must be well-documented and tested and the appropriate personnel trained.
These activities should all be part of change management. In response to this incident, IBM has
statedthatithastakenstepstoenhancethetrainingofitspersonnel.
Itisalsoimportanttoknowwhentoescalateaproblemtomoreseniortechnicalstaffandtothose
responsible for public communications and relations. This is an important part of the Business
Continuity Plan and should be memorized by all affected personnel. The middle of a crisis is not
thetimetobeconfusedastowhichstepstotake.
As is the case in so many outages, communication with DBS users concerning the nature of the
outage and the outlook for a return to service was nonexistent. Users can be patient when they
knowwhatisgoingonbutbrutalifleftinthedark.
Andyes,outagescanhavearegulatoryimpactontheenterprise.
As with most enterprises, this is not the first outage experienced by DBS, though it is the most
significant. In September, 2000, all branch services and its ATM network were down for an hour.
In September, 2009, a branch problem limited customers to withdrawals under S$2,000 and
prevented them from updating their passbooks. In October, 2009, online banking was down for
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

three hours. It is important that we learn from each outage and spread the lessons learned
amongourpeersthroughouttheITindustry.3
Acknowledgements
Our thanks to our Availability Digest subscribers Corinna Tseng and Keith Parris for bringing this
incidenttoourattention.Informationforthisreviewwastakenfromthefollowingarticles:
IBMemployeesparksmassivebankoutage,TheRegister;July13,2010.
SingaporeBankSuffersMassiveITOutage,PCWorld;July6,2010.
DowntimenightmarecouldcostDBSdearly,BusinessTimes;July7,2010.
OutsourcingprematurelyblamedforDBSoutage?,BusinessTimes;July8,2010.
IBMstorageglitchcausesmassivebanksystemfailure,Techworld;July1,2010.
BadadvicefromsupportunitcausedDBSsystemscrash,BusinessTimes;July15,2010.
DBSpointsoutagefingeratIBM,ZDNetAsia;July1,2010.
DBScouldhavecopedbetter,saysCEO,BusinessAsiaOne;July1,2010.
Botchedrepaircausedcrash,StraitsTimes;July14,2010.
OutdatedrepairprocedurecausedDBSsystembreakdown,BusinessAsiaOne;July14,2010.
DBSGroupCEOapologises,TodayOnline;July14,2010.
DBSblamesIBM,bracesforbacklashfromMAS,BusinessTimes;July14,2010.
‘Givepublicfullaccount’,StraitsTimes;July13,2010.
‘Wehavefailedyou,”StraitsTimes;July13,2010.
MASassessingtheextentofDBS’failure,BusinessTimes;July13,2010.
SingaporeCensuresDBS, TheWallStreetJournal;August4,2010.
DBS CEO acknowledges censure by MAS, apologises for outage, Singapore News; August 4,
2010.
GlobalCIO:IBM’sBankOutage:AnatomyOfADisaster,InformationWeek;August5,2010.
SingaporecentralbankslamsDBSandIBMoversystemsoutage,Finextra;August5,2010.
3Moretothepoint,ifyouhaveastorytosharethatwillhelpothers,evenifitmustbepublishedanonymously,letusknow
sothatwecanpublishitasaNeverAgainarticleforthebenefitofall.
5
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com