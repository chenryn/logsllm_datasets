Help! My Data Center is Down!
Part 4: Intranet Outages
January2012
Today’s data centers are incredibly complex. No matter how much redundancy data-center designers
build into their infrastructures, things fail. Data centers fail, often with disastrous consequences.
Sometimes,afaultwilltakedownadatacenterforhours.Sometimes,afaultwilltakedownadatacenter
fordays.
In our previous articles, we related several major data-center disasters caused by power failures, storage
subsystemfaults,andinourlastarticle,Internetoutages.
A data center is no good to anyone if it cannot be accessed by its users. The Internet outages we
described in our last article were faults external to the data centers and that seriously impacted their
operations. But data centers also rely heavily on internal networks to interconnect their servers, to
connect internal users, and to provide connectivity to the external Internet. These internal networks are
calledIntranets.
In this article, we explore some notable Intranet failures that rendered data centers useless even though
they were otherwise fully operational. The stories are taken from the Never Again archives of the
AvailabilityDigest.
Intranets
A large data center can comprise hundreds or thousands of servers. These servers must not only be
interconnected, but they must also connect to local users and to the Internet. Today’s internal networks
useInternettechnologyandarecalledIntranets.LargeIntranets arebuilt witha greatdeal of redundancy
to ensure that they provide reliable service. Unfortunately, Intranets can fail. When they do, company
operationscanbeseverelyimpacted.
NetworkProblemTakesDownJohannesburgStockExchange
An undisclosed network problem shut down the Johannesburg Stock Exchange in South Africa for most
of the day on Monday, July 14, 2009. The problem prevented the Exchange from disseminating trade
data from its morning opening until late in the afternoon. Affected were the Stock Exchange News
Service, equity trading, and equity derivatives. It was estimated that billions of rands in trades were lost.
The Exchange reports that its network had an availability of 99.6% over the last six years. Is that an
availabilityofwhichtobeproud?
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

NICClosesDublinAirport
Aircraft position and identification information started to intermittently disappear from controllers’ screens
in early June, 2008, as aircraft departed and approached Dublin Airport, Ireland’s busiest air-traffic hub.
The problem continued through early July, when it finally became so bad that controllers shut down the
air traffic control system on Wednesday afternoon, July 16th, and rerouted planes to other airports. The
problem was ultimatelytraced to a faultyNetwork Interface Card (NIC) providing radar data to the control
systems.There was no backup link. Tens of thousands of passengers were left stranded or delayed over
aseveraldayperiod.
RouterFailureDelaysAiringofALCSGame
With U.S. sports bars crowded for baseball’s American League Championship Series (ALCS) Game 6
betweentheTampaBayRays andthe BostonRedSox onOctober 18,2008, howls of anger wereraised
when TV sets showed instead the Steve Harvey show. It turned out that circuit breakers for Turner
Broadcasting System’s master router and its backup in Turner’s Network Operations Center in Atlanta
both independently tripped, causing routing of the live feed to fail. By the time the game went live, the
Rays were ahead 1 to 0. The Tampa Bay Rays went on to win Game 7 and the American League slot in
theWorldSeries,butlosttothePhiladelphiaPhilliesintheSeries.
NetworkProblemCostsUKPunter£1Million
A network fault took down the U.K.’s National Lottery network just two days after Christmas, 2008, and
prevented ticket sales for that day’s lottery drawing. In addition, the network fault crashed the online
games’ web site. Sales in excess of £1 million pounds were lost, the margin on which would have been
appliedto goodcauses throughouttheU.K. AsingleticketwontheLottojackpot,nettingthe luckywinner
£3.4 million pounds. However, he would have been £800,000 pounds richer if the other tickets had sold,
whichwouldhaveresultedinalargerpool.
USAirwaysSuffersFiberCut
About100USAirwaysflightsweredelayednationwideonJanuary29,2009,whenafiberopticcablewas
cut near one of its data centers in Phoenix, Arizona. US Airways operates about 3,100 flights per day.
The cable cut affected the airline’s flight dispatch systems and some of its airport computer systems.
Data-processingserviceswererestoredinabouttwohours.
TokyoCommodityExchangeTakenDownbyRouter
The Tokyo Commodity Exchange (TOCOM), Japan’s largest commodity market, had to suspend trading
foroverthreehoursonMay16,2009,whenconnectivitybetweenitsmemberfirmsandthefloor waslost.
The problem occurred just days after the exchange upgraded to a new technology platform from Nasdaq
OMX Group. The fault was traced to a router that was showing a 99% load during a time that a 5% load
was expected.Therouter was replaced; andconnectivitywas restored, allowing the Exchangetoresume
tradingahalf-hourbeforeitsdailyclose.Thenightsessionbeginningtwohourslaterwasuneventful.
NYSETradingHaltedDuetoRoutingProblems
Tradingonthefloor of theNew York Stock Exchange for 242stocks,including AmericanExpress,Merck,
GeneralElectric,andExxon,washaltedat10:43AM onJune12,2009,whenorderscouldnotberouted
to brokers on the floor. Eight of the 27 NYSE-traded stocks that make up the Dow Jones Industrial
Average were affected. Though floor trading in these stocks was halted for several hours, electronic
trading continued. The Dow was calculated from share prices that weren’t being updated but that were
correctedoncefloortradingresumed.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

RouterFailureTakesDownWestJetAirlines
OnAugust7,2009,arouterinWestJet’sCalgarynetworkcentertook downtheairline’scomputersystem
that it uses to check in passengers. The router was redundant, and its backup should have taken over
immediately.Theproblem was thattherouter didn’tquite die.Though itfailedto forwardtraffic,it was not
sick enough for its backup to take over. About 1,000 passengers were affected for over an hour, and
severalflightsoutofTorontoandMontrealwereseriouslydelayed.
MaintenanceDisruptsCaliforniaAirspace
A maintenance subcontractor’s mistake shut down the FAA Telecommunications Infrastructure (FTI) in
the Oakland Air Traffic Control center on August 28, 2009, affecting flights over Northern California,
Nevada, and the Pacific Ocean. Controllers had to rely on cell phones to coordinate flights with
neighboring FAA facilities. It seems that a maintenance error created problems with the communication
system. The backup system was put into service but failed the next day, causing the outage. The
controllers were never notified that they were running with a single point of failure. Normally, such a
notificationwouldhaveputthemonalertstatus.
BBC’sWebSiteOfftheAirforHalfaDay
The BBC web site is the site that everyone in the U.K. turns to when they think the Internet is down
because its web site never fails. But the U.K. ‘go to’ web site did fail on November 5, 2009. In a
statement, BBC acknowledged that the failure was due to a network problem but elaborated no further.
BBC apologized for the outage and indicated that BBC engineers were monitoring the network to ensure
thatnofurtherproblemsoccurred.
WordPressBloggingSiteDownforTwoHoursDuetoRoutingError
WordPress hosts over ten million blogs, including TechCrunch. On February 18, 2010, WordPress
suddenly went offline for almost two hours. It is estimated that over five million page accesses were lost,
but no data was compromised. It turned out that a latent cabling error in one of its data-center providers
causedanalternateroutetobeimproperlyconfigured. Theerroneous routecould handleonly10% of the
normalWordPress traffic.Therouting error also brokethefailover mechanisms betweenWordPress’San
Antonio,Texas,andChicagodatacenters.
FailedEdgeRouterIsolatesColgateUniversityforaDay
AllofColgateUniversity’s accesstotheInternetdiedforadaywhenitsedgeroutercrashedonApril22 ,
2010. During this time, the Colgate network was inaccessible to all off-campus users. All Internet traffic
flows through this one router, but Colgate has only one because of the router’s six-figure cost. According
to its service contract, Cisco delivered a replacement router within four hours; but this router also failed.
By the time a good router was received and installed, the University had suffered almost a day of
isolation.
NetworkOutageIsolatesDallasDataCenterofThePlanet
OnMay3,2010,oneoffourborderroutersfailedathostingproviderThePlanetandaffectedconnectivity
with the company’s core network in its Houston data center. The outage cut off access between some
hosted servers and the Internet for almost two hours. The failure also dropped connections to several
Internet transit providers directly connected to the router. Shortly after the network was restored, The
Planet suffered a link failure between its Dallas and Houston data centers. This network outage isolated
somecustomersfromtheirservers.
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

CiscoSoftwareBugTakesDownaPieceoftheCloud
Cloud-hostinginfrastructureproviderHosting.com lostconnectivitytoitsNewark,NewJersey,datacenter
foralmosttwohoursduringabusyafternoononJune2,2010.Thecompanyreportedthatasoftwarebug
in a Cisco Catalyst 6509 switch not only caused the problem but also disabled both the primary and the
backupswitches.Manymajorcloudproviderswereaffected,includingHostway,Rackspace,andAmazon
WebServices.
BellCanadaTakesDownAirCanada
Air Canada, Canada’s major airline, saw its computer operations come to a halt for several hours on the
morning of October 20, 2008. The culprit was a nationwide glitch caused by a routing problem in Bell
Canada’s trans-Canadian backbone communications network. The outage affected all of Air Canada’s
online IT operations, from self-service kiosks to sign-in desks, gate operations, ticketing, and
reservations. There were massive delays in boarding passengers. Bell Canada finally corrected the
problem around noon by routing around the fault.What? No automatic rerouting? Backbone networks do
fail!
Summary
Complex Intranets are the glues that hold together data centers. When an internal network fails, data
centers can lose much if not all of their usefulness. Even worse, a data center may depend upon other
data centers to properly function. If those data centers suffer an internal network failure, the data centers
towhichtheyaresupplyingservicesmayalsocometogrief.
Most internal corporate networks todayare based on Internet technology. Typically, however, a company
does not invest in the redundant paths and the dedicated administration of its internal Intranet that a
public Internet service provider does. When a network path fails, there is often no fast way to recover
from the fault. Identifying the source of the fault and correcting it can take hours, during which the data
centermayeffectivelybedown.
Sofar inthis series, we havefocusedontechnicalfailures.But adisturbinglylargenumber offailures are
caused by human actions, whether accidental or malicious. In our next article, we will examine upgrades
thathavegonewrong–allduetopoorplanningbypeople.
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com