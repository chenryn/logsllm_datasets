# Summary of Windows Azure Service Disruption on Feb 29th, 2012 

By [Microsoft
Azure](https://azure.microsoft.com/en-us/blog/author/microsoft-azure/ "Posts by Microsoft Azure")

Posted on [ March 9, 2012 ]
13 min read

### Introduction

As a follow-up to my March 1 posting, I want to share the findings of
our root cause analysis of the service disruption of February 29th.  We
know that many of our customers were impacted by this event and we want
to be transparent about what happened, what issues we found, how we plan
to address these issues, and how we are learning from the incident to
prevent a similar occurrence in the future.   

Again, we sincerely apologize for the disruption, downtime and
inconvenience this incident has caused.  We will be proactively issuing
a service credit to our impacted customers as explained below.  Rest
assured that we are already hard at work using our learnings to improve
Windows Azure. 

### Overview of Windows Azure and the Service Disruption

Windows Azure comprises many different services, including Compute,
Storage, Networking and higher-level services like Service Bus and SQL
Azure.  This partial service outage impacted Windows Azure Compute and
dependent services: Access Control Service (ACS), Windows Azure Service
Bus, SQL Azure Portal, and Data Sync Services.  It did not impact
Windows Azure Storage or SQL Azure.

While the trigger for this incident was a specific software bug, Windows
Azure consists of many components and there were other interactions with
normal operations that complicated this disruption. There were two
phases to this incident. The first phase was focused on the detection,
response and fix of the initial software bug.  The second phase was
focused on the handful of clusters that were impacted due to
unanticipated interactions with our normal servicing operations that
were underway.  Understanding the technical details of the issue
requires some background on the functioning of some of the low-level
Windows Azure components.

### Fabric Controllers, Agents and Certificates

In Windows Azure, cloud applications consist of virtual machines running
on physical servers in Microsoft datacenters. Servers are grouped into
"clusters" of about 1000 that are each independently managed by a
scaled-out and redundant platform software component called the Fabric
Controller (FC), as depicted in Figure 1. Each FC manages the lifecycle
of applications running in its cluster, provisions and monitors the
health of the hardware under its control. It executes both autonomic
operations, like reincarnating virtual machine instances on healthy
servers when it determines that a server has failed, as well as
application-management operations like deploying, updating and scaling
out applications. Dividing the datacenter into clusters isolates faults
at the FC level, preventing certain classes of errors from affecting
servers beyond the cluster in which they occur.

![](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2012/03/2045.Outage1.webp)

*Figure 1. Clusters and Fabric Controllers*

Part of Windows Azure's Platform as a Service (PaaS) functionality
requires its tight integration with applications that run in VMs through
the use of a "guest agent" (GA) that it deploys into the OS image used
by the VMs, shown in Figure 2. Each server has a "host agent" (HA) that
the FC leverages to deploy application secrets, like SSL certificates
that an application includes in its package for securing HTTPS
endpoints, as well as to "heart beat" with the GA to determine whether
the VM is healthy or if the FC should take recovery actions.

![](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2012/03/7824.Outage2.webp)

*Figure 2. Host Agent and Guest Agent Initialization*

So that the application secrets, like certificates, are always encrypted
when transmitted over the physical or logical networks, the GA creates a
"transfer certificate" when it initializes. The first step the GA takes
during the setup of its connection with the HA is to pass the HA the
public key version of the transfer certificate. The HA can then encrypt
secrets and because only the GA has the private key, only the GA in the
target VM can decrypt those secrets.

There are several cases that require generation of a new transfer
certificate. Most of the time that's only when a new VM is created,
which occurs when a user launches a new deployment, when a deployment
scales out, or when a deployment updates its VM operating system. The
fourth case is when the FC reincarnates a VM that was running on a
server it has deemed unhealthy to a different server, a process the
platform calls "service healing."

### The Leap Day Bug

When the GA creates the transfer certificate, it gives it a one year
validity range. It uses midnight UST of the current day as the
*valid-from* date and one year from that date as the *valid-to* date.
The leap day bug is that the GA calculated the *valid-to* date by simply
taking the current date and adding one to its year. That meant that any
GA that tried to create a transfer certificate on leap day set a
*valid-to* date of February 29, 2013, an invalid date that caused the
certificate creation to fail.

As mentioned, transfer certificate creation is the first step of the GA
initialization and is required before it will connect to the HA. When a
GA fails to create its certificates, it terminates. The HA has a
25-minute timeout for hearing from the GA. When a GA doesn't connect
within that timeout, the HA reinitializes the VM's OS and restarts it.

If a clean VM (one in which no customer code has executed) times out its
GA connection three times in a row, the HA decides that a hardware
problem must be the cause since the GA would otherwise have reported an
error. The HA then reports to the FC that the server is faulty and the
FC moves it to a state called Human Investigate (HI). As part of its
standard autonomic failure recovery operations for a server in the HI
state, the FC will service heal any VMs that were assigned to the failed
server by reincarnating them to other servers. In a case like this, when
the VMs are moved to available servers the leap day bug will reproduce
during GA initialization, resulting in a cascade of servers that move to
HI.

To prevent a cascading software bug from causing the outage of an entire
cluster, the FC has an HI threshold, that when hit, essentially moves
the whole cluster to a similar HI state. At that point the FC stops all
internally initiated software updates and automatic service healing is
disabled. This state, while degraded, gives operators the opportunity to
take control and repair the problem before it progresses further.

### The Leap Day Bug in Action

The leap day bug immediately triggered at 4:00PM PST, February 28^th^
(00:00 UST February 29^th^) when GAs in new VMs tried to generate
certificates. Storage clusters were not affected because they don't run
with a GA, but normal application deployment, scale-out and service
healing would have resulted in new VM creation. At the same time many
clusters were also in the midst of the rollout of a new version of the
FC, HA and GA. That ensured that the bug would be hit immediately in
those clusters and the server HI threshold hit precisely 75 minutes (3
times 25 minute timeout) later at 5:15PM PST. The bug worked its way
more slowly through clusters that were not being updated, but the
critical alarms on the updating clusters automatically stopped the
updates and alerted operations staff to the problem. They in turn
notified on-call FC developers, who researched the cause and at 6:38PM
PST our developers identified the bug.

By this time some applications had single VMs offline and some also had
multiple VMs offline, but most applications with multiple VMs maintained
availability, albeit with some reduced capacity. To prevent customers
from inadvertently causing further impact to their running applications,
unsuccessfully scaling-out their applications, and fruitlessly trying to
deploy new applications, we disabled service management functionality in
all clusters worldwide at 6:55PM PST.  This is the first time we've ever
taken this step.  Service management allows customers to deploy, update,
stop and scale their applications but isn't necessary for the continued
operation of already deployed applications. However stopping service
management prevents customers from modifying or updating their currently
deployed applications. 

We created a test and rollout plan for the updated GA by approximately
10:00PM PST, had the updated GA code ready at 11:20PM PST, and finished
testing it in a test cluster at 1:50AM PST, February 29^th^. In
parallel, we successfully tested the fix in production clusters on the
VMs of several of our own applications. We next initiated rollout of the
GA to one production cluster and that completed successfully at 2:11AM
PST, at which time we pushed the fix to all clusters. As clusters were
updated we restored service management functionality for them and at
5:23AM PST we announced service management had been restored to the
majority of our clusters.

### Secondary Outage

When service management was disabled, most of the clusters either were
already running the latest FC, GA and HA versions or almost done with
their rollouts. Those clusters were completely repaired. Seven clusters,
however, had just started their rollouts when the bug affected them.
Most servers had the old HA/GA combination and some had the new
combination, both of which contained the GA leap day bug, as shown
below:

![](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2012/03/7416.Outage3.webp)

*Figure 3. Servers running different versions of the HA and GA*

We took a different approach to repair these seven clusters, which were
in a partially updated state.  We restored to previous versions of the
FC, HA, but with a fixed GA, instead of updating them to the new HA with
a fixed new GA. The first step we took was to test the solution by
putting the older HA on a server that had previously been updated to the
new HA to keep version compatibility with the older GA. The VMs on the
server started successfully and appeared to be healthy.
