## December 2019

## 27 

[12/27/2019]

RCA - Unable to create new resources in Azure Portal

Tracking ID: 4_6S-NC8


**Summary of Impact: **Between 15:02 UTC and 16:22 UTC on 27 Dec 2019,
customers using the Azure Portal may have received failure notifications
or 404 errors when attempting to create new resources through the Azure
Portal due to the Marketplace Blade not loading. Programmatic
deployments, such as CLI, PowerShell, and template deployments would
have been successful.

**Root Cause:** Customers were unable to create new resources in the
Azure Portal due to a code change that was introduced which caused a key
extension to fail, causing a null-reference within the Marketplace
blade. The failure surfaced when a set of dependent static data became
stale. Approximately 80% of the tenants issuing create calls through the
portal were impacted during the incident. Post mitigation, less than 1%
of tenants may have seen errors if the faulty extension was cached in
their browser.

**Mitigation:** The incident was mitigated by rolling back the
Marketplace extension to a previous build.

**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure Platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Code was checked to remove this specific error, and related code was
    vetted for similar errors.

```{=html}
<!-- -->
```
-   Extend tests to cover all extension flows.

```{=html}
<!-- -->
```
-   Refactor code to remove null return-values and references to static
    data.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey
[https://aka.ms/4_6S-NC8](https://aka.ms/4_6S-NC8)\

## 12 

[12/12/2019]

RCA - Connectivity issue for Azure Resources in North America

Tracking ID: HKZT-N88


**Summary of Impact:** Between 09:45 and 16:26 UTC on 12 Dec 2019, a
subset of customers in North America may have experienced degraded
performance, network drops, or timeouts when accessing Azure resources.
Customers may also have experienced downstream impact to dependent Azure
services.

**Root cause:** Engineers identified a routing protocol metric change
within an ISP backbone network, which resulted in network connectivity
degradation for a limited subset of Azure customers. Instead of sending
traffic to Microsoft to the closest interconnection point, the ISP was
sending traffic from across US regions to an interconnection point in
California, saturating some of the links in California.\
\
**Mitigation:** Engineers brought down the affected peerings between
Azure and the ISP and failed over network traffic in order to mitigate
the issue.\
\
**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure Platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Work with the ISP to streamline the engagement process for service
    incidents to help reduce the time to repair issues in the future.
-   Fine tune Azure monitoring and telemetry to more quickly detect and
    mitigate events of this nature.
-   Create automated remediation of interconnection points suffering
    from network congestion

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey
[https://aka.ms/HKZT-N88](https://aka.ms/HKZT-N88)

## 11 

[12/11/2019]

RCA - Azure CDN and Azure Kubernetes Service - Service Availability
Issue

Tracking ID: LTPP-R98


**Summary of Impact:** Between 17:24 and 18:48 UTC on 11 Dec
2019, clients of customers attempting to reach Azure CDN from
Verizon endpoints would intermittently receive HTTP (5XX) errors or
connection failures instead of expected content.  \
\
**Root Cause:** Azure CDN providers use staged deployment processes to
deploy configuration changes across their global infrastructure Points
of Presence (PoPs). A recent change to Verizon\'s deployment pipeline
introduced a latent bug that caused some deployment service health
notifications to provide incorrect health
status.   While Verizon was performing maintenance to resolve a delay in
a separate configuration deployment, an improperly encoded configuration
file was deployed to production. Due to the aforementioned bug which
caused latency in their service health notifications, the regular safety
features in their deployment process did not trigger, and allowed the
improper configuration to reach global PoPs.  This configuration caused
service instability across the their global PoPs and resulted in
customers receiving HTTP errors (5XX) or connection errors when
attempting to reach Azure CDN from Verizon endpoints.
Verizon\'s monitoring caught this issue immediately however, and teams
were engaged to resolve the issue. Upon Verizon mitigating the issue,
Microsoft services were restored to a healthy state.\
\
**Mitigation:** After determining the root cause to be the improperly
encoded configuration file, a new hotfix was developed and
deployed globally. After which, the Verizon\'s global
infrastructure began recovering.  \
\
**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps with our partners to improve
the Microsoft Azure Platform and our processes to help ensure such
incidents do not occur in the future. In this case, this includes (but
is not limited to):  

-   Review all service change management processes and practices to
    ensure that all health check mechanisms are monitored and interact
    correctly with deployment staging. 
-   Add additional validation to maintenance and deployment practices to
    ensure all configuration deployment paths result in valid
    configuration.  
-   Review all CDN monitoring and alerting services to ensure that all
    CDN infrastructure availability alerting escalates quickly to
    Microsoft engineering teams.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our
survey [https://aka.ms/LTPP-R98](https://aka.ms/LTPP-R98)

## November 2019

## 20 

[11/20/2019]

RCA - Multiple Services - Downstream impact from Azure Front Door

Tracking ID: HLMF-R88


**Summary of Impact:** Between 00:56 and 03:40 UTC on 20 Nov 2019,
multiple services across Microsoft including Azure, Microsoft 365 and
Microsoft Power Platform leveraging the Azure Front Door (AFD) service
experienced availability issues resulting from high request failure
rates. During this event, some impacted services were able to divert
traffic away from the AFD service to mitigate impact for them.

One of the impacted services was the Azure Status Page at
[https://status.azure.com](https://status.azure.com).
Engineering executed the failover plan to the secondary hosting
location, but this resulted in a delay in status communication changes.
Communications were successfully delivered via Azure Service Health,
available within the Azure management portal.

**Root Cause:** Azure Front Door services provide network edge caching
and web acceleration services to many of Microsoft's SaaS services, in
addition to the optimization offering direct to Azure customers. A
routine, periodic deployment was released through our validation
pipeline that, when combined with specific traffic patterns, caused
service-wide, intermittent HTTP request failures for all services
utilizing the AFD service.

Investigation into the faulting behavior revealed that the combination
of a sequenced code deployment, a configuration deployment and specific
traffic patterns triggered a dormant code bug that instigated the
platform to crash. These deployed changes were tested before being
shipped to the broader cloud; however, the specific traffic pattern was
not observed during test and pilot phases.

Azure Front Door deploys to over one hundred points of presence (PoPs)
around the globe and deploys customer configuration globally to each of
these PoPs, enabling customers to quickly make changes to their service.
This is done to ensure customers are able to promptly remove regional
components out of specification and update configuration for network
security services to mitigate attacks. Through a staged deployment,
these changes passed validation and service health-checks. Having passed
these validations, propagation to global PoPs was quick, by design, to
meet the aforementioned service objectives. After propagation, the fault
triggering behavior was instigated only by specific traffic patterns,
that occurred after the deployment had completed.

This resulted in impacted customers experiencing a high, but
intermittent, rate of web request failures globally while accessing
shared services across the Azure and Office platforms.

**Mitigation:** Global monitoring detected the issue and engaged
engineers at 01:04 UTC. Engineers confirmed the multiple sources of the
issue to be primarily triggered by the configuration deployment and
identified a fix for the issue by 01:27 UTC. Engineers immediately
initiated deployment rollback procedures to return the service to a
healthy state; this rolled out quickly, progressively and completely to
all global platforms by 02:40 UTC. Many of the Microsoft SaaS impacted
services were able to initiate failover away from the AFD service,
providing mitigation to customers while the underlying AFD mitigation
was deployed.

**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure Platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Verify that the fix deployed globally to AFD, during mitigation, is
    a stable release and will remain in place until all internal reviews
    of this issue have been completed.
-   Review all service change management processes and practices to help
    ensure appropriate deployment methods are used.
-   Review the change validation process to identify components and
    implement changes, required to increase test traffic diversity,
    improving the scope of trigger and test code paths.
-   Prioritize deployment of a component independent automated recovery
    process so impacted deployments, like that experienced during this
    incident, are automatically returned to the last-known-good (LKG)
    state at a component layer, quickly and without manual intervention,
    to help reduce time to mitigate and scope of impact.
-   Investigate and remediate the delay experienced with publishing
    communications to the Azure Status Page during the impact window.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey
[https://aka.ms/HLMF-R88](https://aka.ms/HLMF-R88)
