More Never Agains V
July2010
It seems that no matter how hard we try, ensured availability of our IT services continues to be
evasive. In this article, we review a sampling of the many outages that have struck major
companies inthepastsix months.Networkingcontinues tobeatopissue,causingover athirdof
all outages. Likewise, during this period, power and cooling failures accounted for another third.
Interestingly, half of the power outages were caused by automatic transfer switches that did not
cut the data center over to backup power. The rest of the outages were a mix of hardware faults,
softwarebugs,andadenial-of-serviceattack.
A disturbing statistic is that half of these outages were experienced by large hosting, cloud, and
SaaS providers such as Amazon, Hostway, Salesforce, Rackspace, and The Planet. This
observation emphasizes the fact that shared services are not yet suitable for critical applications
unlessyouhaveagood,testedfailoverplan.
Amazon’sEC2ServicesDownforHours
Data Center Knowledge, December 10, 2009 – Amazon Web Services suffered an outage in its
Virginia data center when a power distribution unit (PDU) failed at 4 AM. The problem started
when asinglecomponent inthe redundant PDU failed.Beforethatcomponentcould berepaired,
its redundant partner failed; and the data center lost power for 45 minutes. It took hours to get
customer instances back online, though most were up and running within five hours. The failure
affectedonlyoneofAmazon’sAvailabilityZonesontheU.S.EastCoast.
AustralianCardHoldersGetHitwiththeY2010Bug
Topnews.net – January 3, 2010 – Shoppers at over 8,000 Bank of Queensland EFTPOS
(electronic funds transfer/point of sale) terminals found that their credit and debit cards had
“expired.” It seems that at the stroke of midnight on New Year’s Eve, the terminals rolled their
dates over from 2009 to 2016; and anycard with an expiration dateprior to2016 (which included
all cards) was no longer valid. The bank quicklycame up with a code that merchants could enter
tomaketheterminalsignorethedate.
SalesforceTakenDownByDualSANFailure
Tech Target, January 5, 2010 - Software-as-a-service provider Salesforce.com suffered a wide-
spread outage on the first working day of the year. The outage took down most of Salesforce’s
68,000 customers for more than an hour. Though unconfirmed by the company, it appears that
the problem was a dual failure in a major redundant SAN that took out both the primary and
backupsystems.Salesforce.com system operators hadtorebootsystems torestoreconnectivity.
Thecompanyrunsits entireoperationoutofadatacenterinSiliconValleyandreplicatesitsdata
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

toanotherdatacenterontheU.S.EastCoast.Ithasannouncedplanstoopenathirddatacenter
inSingapore.
Y2KBugHitsGermanShoppersaDecadeLate
Financial Times, January 6, 2010 – A Y2K-like bug triggered by the change of the decade left
thirty-million German debit- and credit-card holders unable to make purchases or make ATM
withdrawals. The bug was in the microchips embedded in the “chip and pin” cards that did not
recognize the year 2010 as a valid year. The French card maker Gemalto admitted it issued the
defective cards. Rather than replace millions of cards, banks are reconfiguring their ATMs and
POSterminalstoacceptthefaultycards.
Don’tPut AllYourEggsintheSameCloud
SearchCloudComputing,January8,2010–Heroku,awebhostingservice,uses Amazon’s EC2
cloud services to host its own cloud services. On January 2nd, all 22 instances of its hosted
services running 44,000 applications suddenly vanished; and Heroku was down for an hour.
Amazon blamed the fault on a router failure in its Virginia data center. All 22 Heroku instances
were in a single Amazon Availability Zone. Though failover was built in, it was to the same
Availability Zone. Configuring backups in other Availability Zones may well have prevented the
problem.
VoIPProvider8x8TakenDownbyISP
TMCnet,January15,2010–8x8’sVoIPinternettelephoneservicewastakendownforfourhours
by an “unaffiliated ISP.” According to an 8x8 spokesperson, a Tier-1 ISP began misrouting 8x8’s
IP messages, which use non-contiguous address blocks. 8x8 responded by broadcasting correct
routing information from its backup data providers. During the outage, not only was Twitter alive
with 8x8 customer postings of complaints and suggestions, but also with messages from 8x8
competitorsadvertisingtheirservices.
RackspaceTakenDownbyaUPSFailure
Data Center Knowledge, January 18, 2010 – A UPS failure in a Rackspace London data center
caused a power outage in that facility. It took hours to recover all of the servers. The outage
occurred when a module failed in the UPS unit, and the unit failed to transfer the load properly.
System personnel had to manually intervene to bring up 220 servers. In many cases, the staff
had to replace power supplies, replace firewalls, reconfigure switches, and log on to the failed
servers. Just a month earlier, Rackspace suffered a major outage by a routing error introduced
duringatestofthenetworklinkingitsDallasdatacentertoitsnewChicagodatacenter.
Off-TrackBettingin AustraliaandNewZealandTakenDownbyPowerFailure
Voxy, January18, 2010 - The Totalizator AgencyBoard (TAB) of New Zealand provides off-track
betting for member race tracks throughout New Zealand. About 2:25 in the afternoon of Sunday,
January 17th, a power-supply failure in TAB’s Petone data center took down both of its servers,
forcingTABtocancelracing.Thefailureallowedsomebetstobeplacedontheracesinprogress
after the starting bell, but no bets could be placed on later races.TAB contacted customers who
bet after the race started in order to payfor those bets on winners (horses and greyhounds) and
torefundbetsonlosers.Anewsystemisscheduledtobeinstalledattheendoftheyear.
TwitterGoesDownDuetoaFailoverFault
Cnn.com,January20,2010–Twitterwentdownforabout90minuteswhenitsufferedafaultand
was unable to fail over to its backup system. During the failure, Twitter users saw nothing but the
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

“fail whale,” the iconic symbol of a Twitter failure. Though there was no word on the cause of the
failure, it occurred right after the Haitian earthquake, leading many to believe that Twitter’s
system suddenly became overloaded with tweets. Twitter said that though it was down for an
hourandahalf,notweetswerelost.
SouthAfricaIsolatedforaDaybyaCableFault
TheDailyMaverick,January21,2010–TheSAT-3underseacablethatcarriesmostofthetraffic
between South Africa and Europe broke down for about 24 hours, effectively isolating South
Africa from Europe and the rest of the world. The incident started when Telkom, the cable
operator, began maintenance on the cable after informing customers that they might experience
increased latency on the channel for four to six hours. However, an error by maintenance
personnelworkingonthepowerunitscausedamassivefailureofthecable.
IowaInternetRoutingErrorAffects22States
ColumbiaMissourian,January22,2010–CustomersofMediacom,amajorISPserving22states
in the middle U.S., started having problems with Internet connectivity Tuesday evening. At first,
only customers in Columbia, Iowa, were impacted. But by the next evening and through the
following morning, the problem had spread to customers in 22 states. The problem was finally
traced to a routing error at Mediacom’s Internet Network Operating Center in Iowa. Mediacom
has installed additional monitoring facilities to address similar problems more efficiently in the
future.
PowerSurgeTakesOutCaliforniaDataCenter
DataCenterKnowledge,January28,2010–DuringseverestormsonJanuary19th,alocalpower
outagecausedapower surgethat was nothandledproperlybythesurgesuppressors intheSan
Jose, California, data center of NaviSite, a managed hosting and cloud service provider. The
surge blew out relay fuses and prevented the automatic transfer switch from starting the data
center’s diesel generators. The battery UPS did not last long enough, and the entire data center
wasofflineforalmostanhouruntilthedieselgeneratorscouldbestartedmanually.
Minnesota’sNorthShoreCutOffFromWorldbyaSteamPipe
Minnesota Public Radio, February4, 2010 – During the midmorning of Tuesday, January 26th, all
counties in Minnesota’s North Shore along Lake Superior were cut off from the rest of the world
for about twelve hours bya fiber cable break. The North Shore is connected to Duluth, MN, via a
single cable – no redundancy. Conjecture is that the cable was laid alongside a steam pipe, and
the heat destroyed the cable. Affected were 911 services (which are routed to Duluth), senior
FirstCall emergency alert buttons, customs agents at the Canadian border, ATM and credit/debit
cardtransactions,banks,andonlinebusinesses.
RAIDFailureTakesDownHostingServiceforFiveDays
The Register, February 10, 2010 – HostV is a hosting service with U.S. data centers in New
Jersey and Chicago. HostV provides dedicated and virtual servers to its customers. In early
February, it suffered a massive RAID failure that severely corrupted its database, taking down
many of its servers. After trying to recover the data from the RAID disks, HostV finally realized
that much of the data had to be restored from offline storage. However, the restoration of the
encrypteddatatookmuchlongerthananticipated.Fivedayslater,atleastoneserveranditsdata
werestillinaccessible.
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

