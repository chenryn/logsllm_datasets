Fault Tolerance for Virtual Environments – Part 3
June2008
Virtualization significantly increases the utilization of a server by creating several independent
virtual machines (VMs) on a single physical server. To its copy of the operating system (a guest
operatingsystem),eachvirtualmachineappearsasifitwereadedicatedphysicalserver.
In Parts 1 and 2 of this series,1 we described the reasons for the burgeoning interest in
virtualization and how virtualization is implemented. Though virtualization can significantlyreduce
data-centercosts,theloss ofavirtualizedservercanmeanthelossofmanyapplications. Herein
Part 3, we address the very important problem of achieving continuous availability in virtualized
environments.
Virtualization products provide a broad range of failover capabilities, but all can result in long
failover times as applications are migrated and as corrupt databases are repaired. This problem
can be alleviated by using fault-tolerant servers to host virtualized environments. Fault-tolerant
servers can withstand any single fault and many multiple faults with no impact on the user, and
theycanreducetheincidenceofcostlyfailoversbyoneortwoordersofmagnitude.
We conclude with some brief reviews of virtualization products and fault-tolerant servers that
providethefeaturesneededtoachievethehighavailabilityrequiredinvirtualenvironments.
Butfirst,webrieflyreviewParts1and2.
Virtualization Review
TheDriversforVirtualization
Over the years, data centers have grown to be massive. Some data centers host thousands of
servers. Historically, each application often was hosted by its own dedicated server. This was
necessarynotonlyfrom acapacityviewpointbutbyadesireofmanagementtohavecontrolover
itsownITresources.
However, Moore’s law has prevailed over the years. In 1965, Gordon Moore, one of the founders
of Intel, stated: “The complexityfor minimum component costs has increased at a rate of roughly
a factor of two per year.”2 In effect, he was saying that the density of transistors on a chip would
double every two years (this is now often quoted more conservatively as a doubling every
eighteenmonths).Moore’slawnotonlycontinuestobevalidtoday,morethanfourdecadeslater,
italsoappliesacrosstheboardtoprocessorpowerandstoragecapacityalike.
1FaultToleranceforVirtualEnvironments–Part1,AvailabilityDigest;March2008.
FaultToleranceforVirtualEnvironments–Part2,AvailabilityDigest;April2008.
2GordonE.Moore,Cramming morecomponentsontointegratedcircuits,Electronics,Volume38,Number8;April,1965.
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

Consequently, data-center servers today are much more powerful than their forbearers; but they
continue to run the same old applications. As a result, the average utilization of servers in a data
centerisofteninthe10%to15%range.
Virtualization allows the consolidation of these servers, with each server running as a virtual
machine within a common physical server. Thus, data-center physical-server utilization can be
increased from 15% or less to 70% or more, reducing the physical server count by a significant
factor.
Fewer servers mean less capital cost, less maintenance, less administration, less space, less
cooling and lighting, lower UPS requirements, and less energy consumed. In short, the capital
costs and operating costs for a data center can be significantly reduced, often by a factor of four
orgreater.
HowisVirtualizationImplemented?
Virtualization is an architecture in which access to a single underlying piece of hardware, like a
server, is coordinated so that multiple guest operating systems (virtual machines) can share that
singlepieceofhardwarewithnoguestoperatingsystembeingawarethatitissharinganythingat
all.3 Simply put, virtualization allows a single physical server to be partitioned into multiple virtual
machines(VMs)thatcanindependentlybeusedbyguestoperatingsystems.
An important characteristic of a virtual machine is that it is independent. It is totally isolated from
the other virtual machines just as if it were running in its own separate physical processor. Any
fault in an application or guest operating system in one virtual machine is completely transparent
totheothervirtualmachinesrunningonthatphysicalprocessorandcanhavenoimpactonthem.
This implies that there must be some kind of adjudicator that controls the access by the various
virtual machines to the resources of the physical server - the processor, its memory, its data-
storage devices, and its I/O channels. This adjudicator is known as the hypervisor. The
hypervisor traps guest operating system calls to the processor, memory, data-storage devices,
and network connections and allows only one virtual machine at a time to execute these calls. In
effect, it is multiplexing the access of the various virtual machines to the underlying physical
processor,therebyensuringthateachgetstheresourcesitneeds.
Apps Apps Apps Apps
guestoperating
systemsrunningin
Linux Windows Unix Solaris
theirown
virtualmachines
Hypervisor
PhysicalServer
AVirtualizedServer
Whyisthistechnologyjustnowbecomingwidelyavailable?Itisbecauseofthestandardizationof
industry-standardserversonarelativelyinexpensivecommonchiparchitecture–thex86classof
microprocessors. This has allowed the development of hypervisors to virtualize a common
hardwarearchitectureratherthanhavingtosupportmultiplesucharchitectures.
3BernardGolden,VirtualizationforDummies,WileyPublishingInc.;2007.
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

Therearetwowaysinwhichvirtualizationisimplementedwithtoday’sproducts:
 Operating System Virtualization, in which the virtualization layer sits on top of a host
operating system that is installed on the physical server. The host operating system
provides the interfaces between the virtual environments and the physical processor and
itsI/Odevices.
 Bare-Metal Virtualization, in which the virtualization layer (the hypervisor) sits directly on
top of the hardware with no intervening host operating system. The hypervisor in this
caseprovidesthecommondevicedrivers.
These architectures are described in some detail in Part 2 of this series, and current products
providingthesefeaturesarenoted.
Virtualization and Availability
Butvirtualizationcomes withaprice,andthatpriceis availability.If a classic physicalserver fails,
it takes down only the application that is running on it. If the application is not mission-critical to
the enterprise, this may be acceptable. However, if a virtualized server fails, it takes down the
equivalent of severalservers sinceeach virtualmachinehosted on the virtualizedphysicalserver
fails. Thus, the failure of a virtualized physical server will take down many applications and is far
morepainfultotheenterprise,especiallyifsomeoftheseapplicationsaremission-critical.
Therefore, it is imperative that there be some sort of failover mechanism so that virtual machines
can continue to function in the event of the failure of a physical server. Furthermore, the failure
consequences of a virtualized physical server argue strongly for physical servers that simply will
not fail – at least not very often. This is the realm of fault-tolerant servers, which can survive any
singlefaultaswellasmanycasesofmultiplefaults.
Wenowlookatthevariousmechanismsavailableinvirtualizationproductstoensureavailability.
VirtualMachineFailover
Typical virtualization products come with some availability features. Almost all hypervisors
monitortheoperationalstateofthevirtualmachinesrunningontheirphysicalservers.
At the basic level is failover. Should a virtual machine fail, the hypervisor will detect that and will
restart the virtual machine on the same physical server. Failover can often be accomplished in
seconds because the image of the entire virtual machine can be stored in a file and restored by
thehypervisorveryquickly.
All work-in-progress is lost, a common result of server crashes. This usuallymeans that requests
inprogressmustberesubmittedtotherestoredserver.
This level of failover protects against a virtual machine or a guest operating system crash, but it
does not protect against a crash of the underlying physical server. Should the server crash, all
virtualmachinesthatwererunningontheserverare,ofcourse,lost.
Clustering
Physical server crashes can be handled by pairing virtualized servers in much the same manner
as contemporary clusters. This solution requires that the application databases be on network
attached storage (NAS) or on a storage area network (SAN) so that they can be generally
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

accessible from multiple physical servers. Failover is accomplished via interhypervisor
coordinationonthevariousphysicalserversinvolved.
Thus,if aphysicalserver fails,its virtualmachines can bemigratedtoother physicalservers.Itis
not necessarythat these other servers be idle standbys.Theymaybe managing their own active
virtual machines. The only requirement is that theyhave capacityavailable to pick up some or all
oftheloadofafailedserver.
Aspeciallayer of coordinatingsoftwaremonitors allof thehypervisors andtheir virtualmachines.
If it sees that a hypervisor on one physical server is not responding, it restarts any virtual
machines that were running on the failed hardware on one or more other physical servers. The
restarted virtual machines may be distributed among surviving servers to balance the new load
profile.
The servers to which the failed virtual machines have migrated have access to the networked
application databases, and the migrated applications can continue to function. As with clusters,
failovercantakeseveralminutestohoursasapplicationsarestartedandascorrupteddatabases
arerepaired.Allwork-in-progressislost.
In addition, operating VMs can be migrated to other physical servers without interruption to
supportloadbalancingand toallowmaintenanceandupgrades onaphysicalserver withnouser
downtime.
ServerPooling
Another option is server pooling. In this configuration, several virtualized physical servers are
organized in a pool that itself is virtualized. To outside users, the server pool appears as a single
virtualizedserver.
A specific virtual machine can be resident on any of the physical servers. Moreover, it can be
moved from server to server under control of the pooling management facility without user
interruption.Thisisusefulforloadbalancing.Iftheloadononephysicalservershouldclimbtoan
uncomfortablelevel,thepoolingmanagercanautomaticallymoveittoanotherserver.Duringthis
process,applicationstateismaintainedsothatnowork-in-progressislostduetothemove.
Pooling configurations bring another availabilitybenefit, and that is eliminating planned downtime
for software or hardware upgrades. If the hypervisor is to be upgraded, the virtual machines are
moved to another server, the upgrade is performed, and the virtual machines are then moved
back, a process that is transparent to the users. If a guest operating system is to be upgraded, a
new virtual machine is created, the upgraded operating system is installed, and the applications
aremovedfrom their old virtualmachine and guest operating system tothe new configuration, all
withoutuserinterruptionorlostwork.
Server pooling is the first step in utility computing, wherein applications are run by reservation
whenandonlywhentheyareneeded.
Insummary,virtualizedpoolingcaneliminateplanneddowntimebecauseapplicationstatecanbe
maintained as virtual machines are moved from one operating environment to another. However,
virtualized failover cannot prevent unplanned downtime due to a physical server failure. Though
the failed virtual machines can be restarted on surviving servers, work-in-progress is lost; and it
