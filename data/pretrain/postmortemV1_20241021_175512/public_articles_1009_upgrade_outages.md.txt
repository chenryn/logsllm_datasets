Upgrades Can Take You Down
September2015
Inourrecentarticle,“HumanTripleWhammy–NYSE,UA,WSJ,”1wenotedthathumans
cause about 40% of all outages. Another major cause of outages is software upgrades
gonewrong.
The major problem with upgrades is that the systems being upgraded are becoming ever more complex.
Each component of the system has manypoints of integration with other components. It’s hard to predict
how a little change to one component might affect the overall system or how that system interacts with
othersystems.
It is becoming more difficult to thoroughly test upgrades – there are just too many processing paths
through the system. Consequently, thorough testing is being replaced with risk-based testing. Just the
paths that are most likely to be executed are tested. Edge conditions such as error processing are given
short shrift in the testing process. There just isn’t enough time or budget to run regression testing or
stress testing for all of the code in the system. Instead, the team of programmers stands by with their
fingerscrossedduringthefirstseveraldaysofsystemoperationfollowinganupgrade.
Severalrecentoutageshaveillustratedthisproblem:
New York Stock Exchange
In the early hours of Wednesday, July 8, 2015, technicians at the New York Stock Exchange (NYSE)
were busily upgrading the software to its gateways to support an upcoming change to the Session
Initiation Protocol (SIP). SIP is a communication protocol for controlling multimedia communication
sessions,typicallyInternetvoiceandvideoandinstantmessagingoverIP.
During final testing, around 7:30 AM local time, problems were discovered with the upgrade. The
gateways were misconfigured for the new software release, causing communication issues between the
customer gateways providing access between the member brokers and the NYSE trading systems. The
gatewayswerereloadedwiththepropersoftwareversion,andthemarketsopenedontimeat9:30AM.
However, as the morning progressed, additional communication issues between the gateways and the
trading systems emerged. They became so severe that trading was shut down at 11:32 AM. In a
statement, an NYSE spokesperson explained, “It was determined that the NYSE and NYSE MKT
gateways were not loaded with the proper configuration compatible with the new release.” It was not
feasible to simply back out the upgrade and continue on with the original known configuration since this
mighthavecausedmanytradestobecanceled.
1HumanTripleWhammy–NYSE,UA,WSJ,AvailabilityDigest;July2015.
http://www.availabilitydigest.com/public_articles/1007/human_whammy.pdf
1
©2015SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

TheNYSEfacedaparticularlydifficultdeadlineinthatmutualfundsandotherinvestmentvehiclesneeda
closing price in order to evaluate their portfolios, a regulatory requirement. As the 4 PM closing bell
approached,thingsgrewmoretense.
Finally, at 3:10 PM, the NYSE was able to get its trading systems operational; and trading continued at a
frenziedpaceuntilthe4PMclose.Thestockexchangehadbeendownfornearlyfourhours.
FAA Air Traffic Control System
The FAA is nearing the completion of a multi-year, multi-billion dollar replacement of its air traffic control
system. The new system replaces radar tracking of airliners with GPS tracking. GPS tracking is far more
accurate and allows the FAA to route planes directly to their destinations rather than over a series of
navigationaids,thusreducingtraveltimeandsavingfuelcosts.ThenewsystemisknownasNextGen.
A major component of NextGen is the EnRoute Automation Modernization system (ERAM). ERAM is
installed at the FAA’s 20 high-altitude air-traffic control centers. It generates the display data for the
controllersandalertsthemwhenaircraftareflyingtooclosetogetherforsafety.
An upgrade was being made to the ERAM system at the FAA’s Leesburg, Virginia, air traffic control
center on Saturday, August 15, 2015. This particular ERAM system provides aircraft control for the
Washington, D.C., metro area. The upgrade went awry and caused the ERAM system to fail. Without
ERAM,airtrafficcontrollershadtoreverttoearlierradarprocedures,whichprecludedthemfromhandling
the number of flights that were scheduled into and out of the Washington area. The outage stranded
thousandsofpassengersandcausedthecancellationofhundredsofflights.
EventhoughERAMincludesafullyfunctionalbackupsystem precludingtheneedtorestrictoperations in
theeventofaprimarysystemfailure,thenatureofthefailedupgradepreventedtheFAAfromfailingover
toitsbackupsystem.
SunGard and BNY Mellon
SunGard provides software and services to financial, education, and public sector organizations. As part
of its clientele, it hosts an accounting platform that helps the Bank of New York Mellon (BNY Mellon)
calculateassetvaluesforitsfundsclients.
Over the weekend of August 22, 2015, BNY Mellon found that it could not calculate the prices of mutual
funds and exchange-traded funds (ETFs). The problem persisted for several days, prolonging confusion
over the price of recent trades and anypotential compensation owed. After several days of scrambling to
fixtheproblem,thecauseoftheproblemwasnotfullyknown.
Finally, on Thursday afternoon, August 27, SunGard broke several days of silence and admitted the
problem was its fault. It had been attempting a systems upgrade to its fund accounting software the
previous weekend,andthe upgradecausedthesystem tofail.SunGardsaidthe glitch was causedbyan
unforeseen complication resulting from an operating system change. The production environment
became corrupted during the change, which unfortunately also corrupted the backup system. It
apologizedtoBNYMellonfortheproblemandforitslongsilence.
The breakdown affected twenty mutual fund companies and 26 ETF providers. The SunGard system
resumed with limited capacity on Tuesday, August 25, leaving BNY Mellon with a backlog of funds to
price.BNYMellonsaidthatitwouldtakemorethanadayandperhapsintothe weekendtofinalizeprices
ifnofurtherproblemsarose.
2
©2015SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Lessons Learned
Therearetwolessons tobelearnedfrom theseoutages.Oneis thatupgrades mustbethoroughlytested
before they are put into production. Obviously, this is becoming ever more difficult as systems get larger
andmorecomplex.
The other is that no matter how much testing is done on an upgrade, upgrades will fail. Therefore, there
must be a fallback plan that is known to work. If the upgrade fails, the system must be immediately
returned to the previously known working configuration so that there is no interruption to critical
processing.Successfulfallbackplansrequireagreatdealofplanningtoensurethattheyarereliable.
Acknowledgements
Informationforthisarticlewasobtainedfromthefollowingsources:
NYSEoutagehighlightsneedforITautomation,SearchDataCenter;July9,2015.
FlightdelayspersistasFAAresumesoperations,USAToday;August15,2015.
Softwarelimitsexposedinairtrafficoutage,TheHill;August17,2015.
Wayne-basedSunGardapologizesforfundpricingglitchatBNYMellon,PhillyVoice;August27,2015.
3
©2015SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com