So what did we know by that point?

-   Starting up with no campaign data in the DB worked OK on TQ and on
    all test servers.
-   Starting up around 300 campaigns in the DB failed on TQ, but worked
    OK on all test servers.
-   Eliminating cross-node calls related to alliance capitals did not
    seem to make any difference.
-   We needed to do some experiments around loading campaigns on TQ to
    isolate what point in particular we experienced the problem.

We decided to completely remove campaign loading at start-up. The
consequence of this was that the background spawning of command nodes
wouldn\'t happen, but otherwise the sovereignty structures should load
up as normal. Note that this is not intended as an actual fix, instead
it is to give us a new data point about what conditions cause a
successful/failed start-up.  This experimental change (Hotfix#2) did
indeed get us started up at 19.15, and led us to experiment #3 - what
would happen if we now loaded the campaigns, but only after a brief
delay, to allow everything else to finish starting up first?

We also tried re-enabling the campaign loading (that we had previously
disabled in Hotfix#2), but delayed it and let it run asynchronously to
the rest of the start-up.  At this point we were hit with some slight
snags, with Perforce sync hangs causing delays in our build system, and
some unrelated data storage issues, but with every hotfix we were
getting closer and closer...

![](//content.eveonline.com/www/newssystem/media/67449/1/CJ-vErKWsAARyU8.jpg?w=900&fm=jpg&fl=progressive_large.jpg?w=900&fm=jpg&fl=progressive)

to pizza, which mercifully arrived at 19.48 while our 3^rd^ hotfix was
deploying.  TQ failed to start up again with this change, which was
interesting because in Hotfix#3 we were loading the campaigns
independently of the rest of the start-up tasks, and yet loading them
was still able to break some nodes. 

Reinvigorated by a ping-pong table full of pizza we redeployed Hotfix #2
for more experimentation and LIVE CONSOLE EXPERIMENTS.

![](//content.eveonline.com/www/newssystem/media/67449/1/pics_026.jpg?w=900&fm=jpg&fl=progressive)

Once all nodes were up and we had verified that solar-systems had loaded
ok, we opened up a python console on the server, allowing us to issue
commands and observe the results in real time.  We performed a few
checks to ensure that no campaigns were loaded on any nodes. As
expected, they all reported that their campaigns were empty. We then
requested all nodes to load up the campaigns that they were responsible
for. Immediately the cluster started to become slow and unresponsive.
After a few minutes, we started to see several nodes die and drop out of
the cluster. As this happens, any solar systems on those dead nodes get
remapped to remaining live nodes. We were now down to around 200 nodes
out of 250.  We then instructed the cluster to run the campaign loading
sequence again. What we expected to see was each node log out the
campaigns it knows about, and then report that no further campaigns
required loading (as they were all in memory). What actually happened is
that again the cluster became unresponsive for a few minutes, and when
everything settled down, a bunch more nodes had also died. This was most
strange, as all that should have happened was the nodes logging out some
info, as they had nothing new that required loading. A thought occurred:
maybe it wasn't actually the campaigns themselves that were causing the
issues - what if it related to the logging that happens around the
campaigns?  As the cluster was now in a bad state due all the dead
nodes, we requested a reboot (staying on Hotfix#2) for some more tests.

Via the console we modified the campaign loading functions so that they
performed the load-from-DB operations normally, but all their logging
operations were disabled.  We then repeated the same test from above -
instruct all nodes to load their campaigns. This command completed
almost instantly, and the cluster remained perfectly healthy. WTF!
Further checks indicated that all campaigns had indeed successfully
loaded.  We then repeated the second-time load test, again using the
loading functions with the disabled logging. This also completed
instantly, reporting that no new campaigns needed loading.

As a final test, we then switched the disabled logging back on, and
issued another load-campaigns instruction. The cluster then regressed
back to the earlier behaviour, showing excessive delays as nodes began
to die off.  We seemed to have found the culprit (even if we didn't know
exactly how/why!).  For some reason, the logging channel used by the
campaign system on TQ appeared to degrade node performance to the point
that some of those nodes would actually drop out of the cluster.

*Note: These channels refer to the logs used by developers for testing
feature operation and investigating defects. They are independent of the
activity logs that you might see in your character wallets, for example,
or that GMs use in their customer service duties.*

![](//content.eveonline.com/www/newssystem/media/67449/1/53e7f9e6-fc03-4407-bd68-d7c9ad5791b2-A26675.jpg?w=900&fm=jpg&fl=progressive)

Hotfix #5 was requested at 21.48, containing what we believed would
mitigate the issue. All logging within the campaign system was entirely
removed, but otherwise the code was mostly unchanged. If this change led
to a successful startup, we would be close to being able to re-open TQ. 
It was deployed to TQ at 22.07.  We had a good startup by 22.22 and able
to begin VIP checks to ensure that no sovereignty data had been damaged
by our experiments.  One problem was found with a duplicate campaign,
but that was easy enough to clean away.  We brought CREST back online at
22.38 and lifted VIP to open TQ to all players at 22.41. 

**Aftermath -- Monday 20^th^ July**

Following our previous adventures in clusters failing to start, we
needed to get to the bottom of why one particular log channel (the one
used by sovereignty campaigns) on one particular server (Tranquillity)
could cause extreme server stability issues. Therefore we scheduled some
VIP time on TQ where we could perform a few experiments. Our goal was to
find the simplest possible code that we could execute that would
reproduce the symptoms in order to aid further investigations. We
connected to a python console on a TQ node - from the console we could
interactively \'poke at things\' to see what we could break. (Prior to
this, we had gone through the same steps on the Singularity and Duality
test servers, and all tests had completed without triggering the error
condition).

We had prepared a series of escalating steps that we intended to follow.
Each one would progressively add more factors in to the mix, until the
issue could be hopefully be reproduced. As it turned out, we didn\'t
need to go very far at all! Here is what happened:

Firstly we had two logging channels - a generic channel, and then the
specific channel used within the sovereignty system for logging campaign
activity. We then validated that both channels were working, by
executing the following code on a single node.

+-----------------------------------+-----------------------------------+
| 1                                 |  # generic_logger is a reference  |
|                                   | to a generic log channel          |
| 2                                 |                                   |
|                                   |  # campaign_logger is a reference |
| 3                                 | to the log channel used by        |
|                                   | sovereignty campaigns             |
| 4                                 |                                   |
|                                   |  # Step 1a: Assert that output to |
| 5                                 | the generic logger works          |
|                                   | correctly                         |
| 6                                 |                                   |
|                                   |  generic_logger.warn(\'The quick  |
| 7                                 | brown fox jumps over the lazy     |
|                                   | dog\')                            |
| 8                                 |                                   |
|                                   |  # Step 1b: Assert that output to |
|                                   | the campaign logger works         |
|                                   | correctly                         |
|                                   |                                   |
|                                   |  campaign_logger.warn(\'The quick |
|                                   | brown fox jumps over the lazy     |
|                                   | dog\')                            |
+-----------------------------------+-----------------------------------+

As we run this we were also observing the log output in real time via
Splunk. We did indeed see each line appear once.

Next we run step 1a on all 250 nodes in parallel at the same time.
Immediately we see 250 entries pop up on the Splunk display. Then we do
the same for step 1b, and see the same result - each node logs one
instance of the line, and each line shows it came via the campaign
channel.

So far so good. Now let\'s log a bit more data, but nothing that should
cause any problems, right? Right? Hmm\...

+-----------------------------------+-----------------------------------+
| 1                                 |  # Step 2a: Log the same line 500 |
|                                   | times per node, on the generic    |
| 2                                 | log channel                       |
|                                   |                                   |
| 3                                 |  \[generic_logger.warn(\'The      |
|                                   | quick brown fox jumps over the    |
| 4                                 | lazy dog\') for i in range(500)\] |
|                                   |                                   |
| 5                                 |  # Step 2b: Log the same line 500 |
|                                   | times per node, on the campaign   |
|                                   | log channel                       |
|                                   |                                   |
|                                   |  \[campaign_logger.warn(\'The     |
|                                   | quick brown fox jumps over the    |
|                                   | lazy dog\') for i in range(500)\] |
+-----------------------------------+-----------------------------------+

For those not familiar with Python, line 2 will loop 500 times,
outputting the warning message once per loop via the generic log
channel. Line 5 does the same thing, but to the campaign log channel.

First we ran step 2a on all nodes in parallel. The command completed
instantly, and we saw a spike of 125,000 lines (250\*500) on the Splunk
graph. That might seem like a lot of logging, but it isn\'t anything the
system can\'t handle, especially in small bursts like this.  Next we ran
step 2b in the same way. This was where something curious happened. The
correct number of log lines did show up in Splunk (The logs **do** show
something!), but the command did not appear to return as immediately as
it did for step 2a. In fact it took a few minutes before the console
became responsive again, and the returned data indicated that several
nodes did not respond in time.  Looking over the status of the cluster,
those nodes were now showing as dead. Somehow this innocent log line had
managed to cause these nodes to time-out and drop out of the cluster.

Well that was easier than we thought! We didn\'t even need to start
doing anything with campaigns or the database to reproduce the dead
cluster.  When a developer cannot rely on simple logging actions to not
kill his production server, he is going to have a bad day. Much like we
did on Wednesday 15th.  Having isolated the problem down even more, we
restarted TQ and then gave the go ahead to drop VIP and opened up as
normal.

As mentioned on a few occasions in the time-line above, we had never
seen this happen on any other test server before or since. There is
something unique to the way that TQ operates (or is
configured). Investigations are continuing to try to track down why this
logging behaviour happens only on TQ, and only on specific log channels.
We know that TQ certainly operates at different scale in terms of the
amount of hardware, but so far other experiments have suggested that
this is not simply a matter of number of nodes.  The investigation
continues...


