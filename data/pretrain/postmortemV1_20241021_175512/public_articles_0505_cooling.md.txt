Data Center Cooling Nature’s Way
May2010
How would you like an annual electric bill of $7,000,000? That is about what a typical large data
centerdrawingtenmegawattsofpowerpays,evenatanegotiatedrateofeightcentsperkilowatt
hour(abouthalftheresidentialrateinmanyareasofthecountry).
Heat is a data center’s worst enemy. In manydata centers, half or more of the consumed energy
isusedsimplytocooltheIT equipment–servers,network devices,storagearrays,consoles,and
so forth. Cooling all this equipment not only costs a lot of money, but it also has a significant
impactonourenvironment.
No wonder so many companies are aggressively looking at ways to reduce cooling costs. One
successfulapproachthathasbeentakenis tolocateadatacenterinacoldclimateandtosimply
use the outside air, unconditioned, to cool the data center. Intel has taken this approach to an
extremebylocatinganexperimentaldatacenterinthedesert–withamazingresults.
Inthisarticle,welook attheIntelexperimentandatseveralproductiondatacentersusingMother
Nature’sowncooling.1
PUE
PUE, the Power Usage Effectiveness factor, is a measure of the energy efficiency of a data
center. It is the ratio of the amount of energy consumed by the data center to the amount of
energyconsumedjustbytheIT equipment.For instance,ifadatacenter draws tenmegawattsof
power, and if the IT equipment uses five megawatts of that power, the data center’s PUE is 2.0.
Thisistypicalfortoday’sdatacenters.
Air Economizers
The bulk of the energy required for cooling is consumed by compressors. A typical air-
conditioning system moves cold air in and about the IT equipment to carry off heat. The resulting
hot air passes over coils containing a compressed liquid coolant, evaporating the coolant and
cooling the air. The cooled air is then recirculated over the equipment; and the evaporated
coolant is recompressed to a liquid by a compressor, which squeezes out the heat to repeat the
cycle.
Ifafreesourceofcoolaircanbetapped,thecompressorcanbeeliminated;andonlyfans,ifany,
need to be powered. This requires a fraction of the power consumed by the compressor. This
1WebeganthisdiscussioninourNovember,2009,articleentitledChillerlessDataCenters,availableat
http://www.availabilitydigest.com/public_articles/0411/chillerless.pdf.
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

technique is often referred to as an air economizer that provides “free cooling.” It has been used
tocoolbuildingsfordecadesandisnowbeingexperimentedwithfordatacenters.
Intel’s Experimental Air Economizer
In 2008, Intel reported on a very important experiment to determine the effectiveness of an air
economizer.2Itsetupa pair of smalldatacenters in a desertenvironment.Onedatacenter used
standardairconditioning;andtheotherusedonlyunconditionedoutsideairunlesstheoutsideair
temperature exceeded 90ºF, in which case it reverted to air conditioning. The result – after ten
months of operation, including through the heat of the summer, there was little difference in the
failurerateofITequipment.
Thetestdatacenter was dividedintotwocompartments.Eachcompartmentwas 500squarefeet
in area; and each contained eight racks with 56 blade servers per rack, for a total of 448 blades
per compartment. To simulate real conditions, the blades executed applications that created a
90% server utilization rate. The resulting electric load created by the servers was about 100
kilowatts per compartment. The data center was active over a ten-month period, from October to
August.
The servers in the economizer compartment were subjected to considerable variation in
temperature and humidity. The outside air temperature varied from 65ºF to 90ºF (many
manufacturers today specify that their servers can operate up to 98ºF). If the outside air
temperature exceeded 90ºF, the compartment’s air conditioning was turned on (the outside air
temperature was below 90ºF 91% of the time). If the outside air temperature fell below 65ºF,
intakeairwasheatedwithexcessheatfromtheITequipment.
There was no control over humidity. Relative humidity varied from 4% to 90% and changed
rapidly at times. Furthermore, the economizer air was only marginally filtered. Standard
householdfilterswereusedandeliminatedonlylargeparticles.Smallerdesertdustparticleswere
allowedintothecompartment.
Theservers in theair-conditioned environment werecooled with a well-filtered air supplyof 68ºF.
The exit temperature of the air was 126ºF. Thus, the air conditioning unit had to cool the
circulatingairby58ºF.
Cooling for the air-conditioned compartment required 112 kilowatts of power, giving a PUE of
(100+112/100 = 2.12. During economizer operation, power requirement for cooling (just fans) for
2ReducingDataCenterCostwithanAirEconomizer,IT@IntelBrief;April2008.
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

the economizer compartment required 29 kilowatts, a 76% reduction in power consumption for
cooling.ThisresultedinaPUEof(100+29)/100=1.29fortheeconomizercompartment.
Though the IT equipment in the economizer compartment was covered with a layer of dust at the
end of the test, the surprising result was that there was no significant increase in equipment
failures. The economizer compartment suffered a 4.46% server failure rate, while the air-
conditionedcompartmentsuffereda3.83%failurerate.
Intel estimates that if this were a ten megawatt data center, almost $3 million in annual electric
costswouldbesavedateightcentsperkilowatthour(asavingsofabout3,500kilowatthoursper
kilowatt consumed). In addition, significant capital expenditures in the air-conditioning equipment
could be saved since in high external ambient temperatures, the air only needs to be cooled to
90ºFratherthanto68ºF.
Production Economizers
Several variations of air economizers are now in successful operation to provide “free cooling” in
anincreasingnumberofdatacenters.
HP,Wynyard,England
HPhasrecentlyopenedthefirstphaseofa360,000square-footdatacenterinWynyard,whichis
near Stockton on Tees in Northern England.3 Wynyard is on the eastern sea coast of England,
eightmilesnorthoftheNorthSea.
HP’s new data center has four data halls. It is designed to be a secure, hardware-agnostic data
centertocompetewithcompaniessuchasIBMforITservicesandmanagementcontracts.
Eight large 2.1 meter (about seven feet) plastic and stainless steel fans draw glacial-cooled
coastal air thorough filters and pass the air to a twelve-foot high plenum area beneath its data
center halls. From there, the cool air rises through the server racks and is exhausted back into
theopenair.
The data center also collects and filters rain water. If the air is too dry, this water is sprayed in a
finemisttoincreasethehumidityoftheintakeair.
As an added energy-conserving innovation, all of the server racks are light-colored. This reduces
thelightingrequirementsforthedatacenterby40%
HP expects that it will have to run its air conditioners onlyabout three days per year. It estimates
a PUE of 1.2 for the data center, saving about $1.4 million dollars per year per hall for an
anticipatedenergysavingsof40%comparedtootherdatacenters.
Google,Saint-Ghislain,Belgium
In late 2008, Google opened a prototype air economizer data center in Saint-Ghislain, Belgium.4
Saint-Ghislain is 30 miles southwest of Brussels, which puts it about 70 miles southeast of the
EnglishChannel.
The average summer temperature in Saint-Ghislain is between 66 to 71 degrees Fahrenheit.
Google uses this outside air to keep its data center at temperatures below 80ºF. Google
estimates thatit will have topower up its air conditioningsystem onlyabout sevendays per year.
3HPOpensNewWind-CooledDataCenterinNorthernUK,TheWhir;February10,2010.
4Google’sChiller-lessDataCenter,DataCenterKnowledge;July15,2009.
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

During these periods, Google will turn off IT equipment as needed and shift processing load to
otherdatacenterstomaintainequipmenttemperatureswithinallowableranges.
Free cooling makes local weather forecasting a large factor in data-center management. Google
has developed automated tools to provide advance weather forecasts to help it decide when to
distribute workloads. These tools can also rapidly redistribute computing workloads during an
unanticipatedthermalevent.
GooglereportsaPUEforthisdatacenteratslightlylessthan1.1.
Yahoo!,Lockport,NewYork,U.S.
Yahoo’s planned chillerless data center5 is to be located in Lockport, New York, northeast of
BuffaloandtenmilessouthofLakeOntario.Notonlywillthisdatacenterusehydroelectricpower
generatedbyNiagaraFallstotheeast,butitwillusethewindsoffLakeOntarioforfreecoolingof
itsITequipment.
Thedata center will compriseaset of independent modules.Eachmodule willbe aprefabricated
metal structure 120 feet by 60 feet. Louvers built into the sides of each module will allow cold air
to enter the computing area. The modules are angled to take advantage of the prevailing winds
offLakeOntariosothatthewindswillblowdirectlyintothelouversystem.
Thermal convection normally moves the air through the modules, even eliminating the power
requirements for fans. Each module has a peaked roof with a “penthouse” on top that manages
thereleaseofwasteheatfromthehotisleinthemoduleintotheoutsideair.
A module will house five megawatts of equipment. Initially, five modules will be built, though the
sitehasroomformore.
On days that are warmer than 80º F, the module cooling system will be augmented with
evaporative cooling. It is expected that this maybe required about nine days per year. The result
is an estimated annualized PUE of 1.1, meaning that 90% of the energy consumed by the data
centergoestopoweritsITequipment.
Yahoo! alreadyoperates greendatacenters inWashingtonState.Thedatacenters usewindand
hydroelectricpowerwithfreecoolingformostoftheyear.
AssociatedBank,GreenBay,Wisconsin,U.S.
InGreenBay,Wisconsin,itiscold.However,theAssociatedBank wasrunningits data-centerair
conditionerseveninthedeadofwinter.
When faced with the need to expand its data center, the bank decided to take advantage of the
cold outdoors.6 Aided by a grant from Focus on Energy, a Wisconsin agency that runs
Wisconsin’s energy efficiency and renewable energy program, the bank installed a glycol-based
“freecooling”systematitstwonewdatacenters.
Glycol is the cooling agent. Hot air from the servers is circulated over coils containing glycol
liquid.Theglycolabsorbstheheat,andthecoolairisrecirculatedbackovertheITequipment.
5Yahoo’s‘ChickenCoop’DataCenterDesign,DataCenterKnowledge;June30,2009.
6DataCenterCoolingFromtheFrozenTundra,InformationManagement;March30,2010.
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

However,unlikeastandardairconditioner,duringthewintermonthstheglycoliscirculatedtothe
outside air to be cooled. No compressors are necessary. Compressors are used to liquefy the
glycolonlyiftheoutsidetemperaturerisesabove40ºF.
The Associated Bank estimates that it saves about 1.4 million kilowatt hours of electricity
annually,enoughtoprovideapaybackfromthenewsysteminthreeyears.
MauritiusEco-Park,Mauritius
Eco-Park7 is a project of Mauritius, a small island off the coast of Madagascar. Mauritius lies in
the path of a deep-sea current. The water temperature at about 5,000 meters from shore at a
depthof1,000metersisafairlyconstant5ºC(41ºF).
Seawater willbepumpedfrom thisdepthtocoolintakeairforalargedatacenterthatMauritius is
building.MauritiusispositionedatthecrossroadsofAfrica,Asia,andAustraliaandhasbecomea
major financial and telecommunications hub for the area. It plans for its Sea Water Air
Conditioning system (SWAC) to be a competitive offering to attract outsourcing contracts and to
beusedasadisaster-recoverycenter.
SWAChasalsobeenusedinotherlocationssuchashotelsinHawaiiandCuraçao.
Submersion
Yes. Submersion. If you don’t have cool outside air, but you want to avoid the energy
consumption of a compressor, why not simply dunk the entire rack into a cooling fluid and
dissipate the heat from the bath? That is an approach that Green Revolution Cooling (GRC) of
Austin, Texas, U.S., is taking. Its system looks like a 42U rack tipped over into a bath of mineral
oil.8
The GRC system allows servers to be
operated without a raised floor or an air vertically-mounted
standard1Userver
conditioner. The cooling fluid is similar to
mineral oil in that it is non-hazardous, Ethernetcable
transfers heat almost as well as water, and powercable guides
does not conduct an electrical charge. It guides
can support heat loads up to 100 kilowatts
for a 42U rack. Cooling a 5,000 watt rack PDUmount
requires about40watts ofcoolingpower (a
PUElessthan1.01).
The system can be used with standard
blades from several manufacturers.
Despite being immersed in fluid, the
system is said to be easy to maintain. A
GRCvideoshowsabladebeingdrainedandreadyforservicingwithinsixtyseconds.
GRCplanstoshortlyinstallitsfirstunitsattheTexasAdvancedComputingCenter.
Spraying
AnothersimilarpathisbeingpursuedbySprayCoolofLibertyLake,Washington,intheU.S.Inits
system, a module is attached directly to a microprocessor chip and sprays a fine mist of coolant
7 Sea Water Data Center Cooling promoted at Mauritius data center conference, Data Center Journal; September 14,
2009.
8SubmergedServers:GreenRevolutionCooling,DataCenterKnowledge;March17,2010.
5
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

onto a cold plate surrounding the chip.9 SprayCool claims that with its technology, it only takes
175wattstocool11kilowatts(thiswouldleadtoaPUEof1.02).
HPisreportedlypursuingasimilarstrategyusingink-jetsprayheadsfromitsprinterdivision.
Summary
Data-center power consumptionaccountedfor 1.5% of all worldwide power consumptionin2009,
and it is rapidlygrowing. Half of this energyis used simplyto cool the data centers heated bythe
millionsofmegawattsconsumedbytheIT equipment.Data-centercoolingisnotonlycostly,butit
isalsoamajorfactorinCO emissions.
2
To cut the amount of energy required to cool data centers, efforts are being made by the server
vendors to reduce the power consumption of their servers. On another front, many data centers
are experimenting with “free cooling” – using nature’s own environment to cool IT equipment
withoutmassiveusesofpower.
Whatever the solution, the U.S. Environmental Protection Agency (EPA) has set a 2011 goal for
new data centers to achieve a PUE (Power Utilization Effectiveness) of 1.12, down from today’s
typical PUE of 2.0. This means that only about 12% of data-center power utilization will be used
forotherthantheoperationoftheITequipmentitself.
9NewCoolingTechnologiesTackleDataCenterHeat,InformationWeek;September26,2006.
SprayCool–ReliableGreenComputing,SprayCoolDataSheet.
6
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com