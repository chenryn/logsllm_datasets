# October 21 post-incident analysis 

In-depth analysis of the incident that impacted GitHub services on
October 21 and 22.

Last week, GitHub experienced [an
incident](https://blog.github.com/2018-10-21-october21-incident-report/)
that resulted in degraded service for 24 hours and 11 minutes. While
portions of our platform were not affected by this incident, multiple
internal systems were affected which resulted in our displaying of
information that was out of date and inconsistent. Ultimately, no user
data was lost; however manual reconciliation for a few seconds of
database writes is still in progress. For the majority of the incident,
GitHub was also unable to serve webhook events or build and publish
GitHub Pages sites.

All of us at GitHub would like to sincerely apologize for the impact
this caused to each and every one of you. We're aware of the trust you
place in GitHub and take pride in building resilient systems that enable
our platform to remain highly available. With this incident, we failed
you, and we are deeply sorry. While we cannot undo the problems that
were created by GitHub's platform being unusable for an extended period
of time, we can explain the events that led to this incident, the
lessons we've learned, and the steps we're taking as a company to better
ensure this doesn't happen again.

## Background[](#background)

The majority of user-facing GitHub services are run within our own [data
center
facilities](https://githubengineering.com/evolution-of-our-data-centers/).
The data center topology is designed to provide a robust and expandable
edge network that operates in front of several regional data centers
that power our compute and storage workloads. Despite the layers of
redundancy built into the physical and logical components in this
design, it is still possible that sites will be unable to communicate
with each other for some amount of time.

At 22:52 UTC on October 21, routine maintenance work to replace failing
100G optical equipment resulted in the loss of connectivity between our
US East Coast network hub and our primary US East Coast data center.
Connectivity between these locations was restored in 43 seconds, but
this brief outage triggered a chain of events that led to 24 hours and
11 minutes of service degradation.

![A high-level depiction of GitHub\'s network architecture, including
two physical datacenters, 3 POPS, and cloud capacity in multiple regions
connected via
peering.](https://github.blog/wp-content/uploads/2018/10/network-architecture.png?resize=1700%2C1350){.attachment-full
.size-full decoding="async" width="1700" height="1350" loading="lazy"
recalc-dims="1"}

In the past, we've discussed how we use [MySQL to store GitHub
metadata](https://githubengineering.com/orchestrator-github) as well as
our approach to [MySQL High
Availability](https://githubengineering.com/mysql-high-availability-at-github).
GitHub operates multiple MySQL clusters varying in size from hundreds of
gigabytes to nearly five terabytes, each with up to dozens of read
replicas per cluster to store non-Git metadata, so our applications can
provide pull requests and issues, manage authentication, coordinate
background processing, and serve additional functionality beyond raw Git
object storage. Different data across different parts of the application
is stored on various clusters through functional sharding.

To improve performance at scale, our applications will direct writes to
the relevant primary for each cluster, but delegate read requests to a
subset of replica servers in the vast majority of cases. We use
[Orchestrator](https://github.com/github/orchestrator) to manage our
MySQL cluster topologies and handle automated failover. Orchestrator
considers a number of variables during this process and is built on top
of [Raft](https://raft.github.io/) for consensus. It's possible for
Orchestrator to implement topologies that applications are unable to
support, therefore care must be taken to align Orchestrator's
configuration with application-level expectations.

![In the normal topology, all apps perform reads locally with low
latency.](https://github.blog/wp-content/uploads/2018/10/normal-topology.png?resize=1700%2C1350){.attachment-full
.size-full decoding="async" width="1700" height="1350" loading="lazy"
recalc-dims="1"}

## Incident timeline[](#incident-timeline)

### 2018 October 21 22:52 UTC[](#2018-october-21-2252-utc) 

During the network partition described above, Orchestrator, which had
been active in our primary data center, began a process of leadership
deselection, according to Raft consensus. The US West Coast data center
and US East Coast public cloud Orchestrator nodes were able to establish
a quorum and start failing over clusters to direct writes to the US West
Coast data center. Orchestrator proceeded to organize the US West Coast
database cluster topologies. When connectivity was restored, our
application tier immediately began directing write traffic to the new
primaries in the West Coast site.

The database servers in the US East Coast data center contained a brief
period of writes that had not been replicated to the US West Coast
facility. Because the database clusters in both data centers now
contained writes that were not present in the other data center, we were
unable to fail the primary back over to the US East Coast data center
safely.

### 2018 October 21 22:54 UTC[](#2018-october-21-2254-utc) 

Our internal monitoring systems began generating alerts indicating that
our systems were experiencing numerous faults. At this time there were
several engineers responding and working to triage the incoming
notifications. By 23:02 UTC, engineers in our first responder team had
determined that topologies for numerous database clusters were in an
unexpected state. Querying the Orchestrator API displayed a database
replication topology that only included servers from our US West Coast
data center.

### 2018 October 21 23:07 UTC[](#2018-october-21-2307-utc) 

By this point the responding team decided to manually lock our internal
deployment tooling to prevent any additional changes from being
introduced. At 23:09 UTC, the responding team placed the site into
[yellow
status](https://twitter.com/githubstatus/status/1054147648930897920).
This action automatically escalated the situation into an active
incident and sent an alert to the incident coordinator. At 23:11 UTC the
incident coordinator joined and two minutes later made the decision
change to [status
red](https://twitter.com/githubstatus/status/1054148705450946560).

### 2018 October 21 23:13 UTC[](#2018-october-21-2313-utc) 

It was understood at this time that the problem affected multiple
database clusters. Additional engineers from GitHub's database
engineering team were paged. They began investigating the current state
in order to determine what actions needed to be taken to manually
configure a US East Coast database as the primary for each cluster and
rebuild the replication topology. This effort was challenging because by
this point the West Coast database cluster had ingested writes from our
application tier for nearly 40 minutes. Additionally, there were the
several seconds of writes that existed in the East Coast cluster that
had not been replicated to the West Coast and prevented replication of
new writes back to the East Coast.

Guarding the confidentiality and integrity of user data is GitHub's
highest priority. In an effort to preserve this data, we decided that
the 30+ minutes of data written to the US West Coast data center
prevented us from considering options other than failing-forward in
order to keep user data safe. However, applications running in the East
Coast that depend on writing information to a West Coast MySQL cluster
are currently unable to cope with the additional latency introduced by a
cross-country round trip for the majority of their database calls. This
decision would result in our service being unusable for many users. We
believe that the extended degradation of service was worth ensuring the
consistency of our users' data.

![In the invalid topology, replication from US West to US East is broken
and apps are unable to read from current replicas as they depend on low
latency to maintain transaction
performance.](https://github.blog/wp-content/uploads/2018/10/invalid-topology.png?resize=1700%2C1350){.attachment-full
.size-full decoding="async" width="1700" height="1350" loading="lazy"
recalc-dims="1"}

### 2018 October 21 23:19 UTC[](#2018-october-21-2319-utc) 

It was clear through querying the state of the database clusters that we
needed to stop running jobs that write metadata about things like
pushes. We made an explicit choice to partially degrade site usability
by pausing webhook delivery and GitHub Pages builds instead of
jeopardizing data we had already received from users. In other words,
our strategy was to prioritize data integrity over site usability and
time to recovery.

### 2018 October 22 00:05 UTC[](#2018-october-22-0005-utc) 

Engineers involved in the incident response team began developing a plan
to resolve data inconsistencies and implement our failover procedures
for MySQL. Our plan was to restore from backups, synchronize the
replicas in both sites, fall back to a stable serving topology, and then
resume processing queued jobs. We [updated our
status](https://twitter.com/githubstatus/status/1054161818652946433) to
inform users that we were going to be executing a controlled failover of
an internal data storage system.

![Overview of recovery plan was to fail forward, synchronize, fall back,
then churn through backlogs before returning to
green.](https://github.blog/wp-content/uploads/2018/10/recovery-flow.png?resize=1700%2C1500){.attachment-full
.size-full decoding="async" width="1700" height="1500" loading="lazy"
recalc-dims="1"}

While MySQL data backups occur every four hours and are retained for
many years, the backups are stored remotely in a public cloud blob
storage service. The time required to restore multiple terabytes of
backup data caused the process to take hours. A significant portion of
the time was consumed transferring the data from the remote backup
service. The process to decompress, checksum, prepare, and load large
backup files onto newly provisioned MySQL servers took the majority of
