This technique is used by OpenVMS Active/Active Split-Site Clusters,4 and we will use that
implementationasamodelforthefollowingdescription.
FullCopy
If a full copy of the database is required, it proceeds table by table. Tables may be copied in
parallel if desired. We assume that the node being recovered has an old database copy that is
current as of the time of failure (if not, a bulk load of the database may be made from one of the
operationalcopies).
For a given table, the copy utility reads the first row from a designated active database copy
(which we will call the “master”) and compares that row with the corresponding row on the table
beingrecovered.Iftheyarethesame,thecopyutilityadvancestothenextrow.If bothrowsexist
but are different, the master row replaces the target row. If the master row does not exist in the
target table, it is inserted into the target table. If the target row does not exist in the master table,
itisdeletedfromthetargettable.
One concern is that the source database is being actively updated during this process, and the
copy utility has not locked the master row for performance reasons. Therefore, if a modification
has occurred, the rows are reread and compared again. If they are different, that means that the
master row changed during the comparison; and the above process is repeated. If several
4OpenVMSActive/ActiveSplit-SiteClusters,AvailabilityDigest;June2008.
http://www.availabilitydigest.com/public_articles/0306/openvms.pdf.
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

attempts fail to get a match(due to a highlyactive master row), the master row is locked and the
procedureisonceagainrepeated,thistimewithaguaranteedsuccessfuloutcome.
A “fence” on the recovering table separates the part that has been copied from the part that has
not been copied. Target applications can use the part of the recovering table before the fence for
read-onlyoperations.Thepartfollowingthefenceisoff-limits.
Replicated changes are applied synchronously to the table rows that are before the fence. The
target system will respond to a commit request with a “yes” vote if it is capable of applying all
changestoitsrowsbeforethefence.Itmayormaynotapplyreplicatedchangestorowsafterthe
fencebecausethesewillbecorrectedbythecopyutilitywhenitgetstothoserows.
Whenalltableshavebeensuccessfullycopied,therecovereddatabaseisinsynchronizationwith
the operational database. The surviving processing nodes can now be notified to include the
recoverednodeinthescopeoftheirtransactions.
Thedownsideof this techniqueis performance.Thoughtheapplications arenever pausedduring
the recovery process, each row in the database must be read at least once. This imposes an
additional load on the master database and may affect performance during the copy. The copy
facilitycanbethrottledtocontrolitscopyratesoastominimizetheperformanceimpact.
PartialCopy
If a processing node is to be removed from service for only a short time, a partial copy may be
used. In this case, the designated master node keeps a list of the primary keys and tables of all
rows that have been modified. When the node is returned to service, only those rows need to
undergothecopyprocessdescribedabove.
AsynchronousOnlineCopy
Anothertechniqueforsynchronousdatabaserecoveryistouseasynchronousreplicationtocatch
up, and then switch over to synchronous replication. Using this technique, the surviving nodes
queuechangestothefailednode whileitisdown.Ifuponrecoverythefailednodedoesnothave
its copy of the database at the time of failure, an online load of the operational database is first
madetoitfromasurvivingnode’sdatabasecopy.5
Thechangesqueuedbythesurvivingnodesarethenreplicatedasynchronouslytotherecovering
database. As new changes come in, they are added to the surviving nodes’ change queues and
arereplicated.
This process continues until the change queues have been drained to some specified minimum
size.6 At this time, transaction processing is paused to allow the change queues to completely
drain.In-processtransactionsareallowedtocomplete,butnonewtransactionsarestarted.
When the change queues have been completely drained, the recovering database is in complete
synchronization with the operational database; and transaction processing can be resumed with
therecoverednodeincludedinthescopeoffurthertransactions.
5Thisloadmustbeperformedwithoutaffectingtheoperationofthesurvivingnodes.AloadfacilitysuchasSOLVfrom
Gravic, Inc. will not only load the recovering database while the operational database is active but will keep it current
duringtheloadprocess,Seewww.gravic.com/solv.
6Thataminimumsizeexistsisshownbyqueuingtheory.IftheloadonthereplicationchannelisL,theprobabilitythatthe
queue length will be zero is (1 – L). Of course, this is complicated by change-queue polling and by communication
blocking; but the result is the same. See W. H. Highleyman, pg. 121, Equation (4-79), Performance Analysis of
TransactionProcessingSystems,Prentice-Hall;1989.
5
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Using the online asynchronous copy method for database recovery eliminates the performance
impact imposed by the online synchronous method. The performance impact is no greater than
that imposed by asynchronous replication. In fact, in a two-node system, performance will
probablybebetterwhenonenodeisdownsincetherewillbenoapplication-latencydelay.
However, with this method, the application must be paused at the end of the copy to let the
recoveringdatabasecatchupwiththeoperationaldatabase.Inmanyapplications,thispausecan
be configured to be less than a second (about the time of a transaction). However, if the
application executes long transactions (such as a batch process), application pause time maybe
aslongasthelongesttransaction;andthismethodmaynotbeappropriate.
MixedOnlineCopy
Thesynchronousandasynchronousonlinecopiesdescribedaboveeachcarryapenalty:
 Thesynchronousonlinecopyimposesanadditionalloadononeoftheoperationalnodes
during the copy. This is the node that is designated as the master node from which the
copyismade.
 The asynchronous online copy requires that the application be paused for a short time
while the change queue is finally drained. The duration of this pause depends upon the
transaction-processingtimesrequiredbytheapplication.
Both of these problems can be solved by using a synchronous-replication method known as
coordinatedcommits.7Withcoordinatedcommits,allchanges arereplicatedviaanasynchronous
replication engine. As with normal asynchronous replication, changes are entered into a change
queue. Changes are read from the change queue with no impact on the application and are sent
tothetargetsystem,wheretheyareappliedtothetargetdatabase.
However, in this method, there is an extra step. The asynchronous replication engine joins the
source transaction and is therefore a voting member for that transaction. When the source
transaction is ready to commit, the replication engine is asked to vote. If it has acquired locks on
all data objects to be modified by the transaction, it votes “yes.” If it cannot apply the transaction
updates, it votes “no;” and the transaction is aborted. Application latency is minimized with
coordinated commits since the application must only wait for the commit to complete across the
networkratherthanhavingtowaitforthenetworkcompletionofeachdatabaseoperationplusthe
commit.
Database recovery is seamless using coordinated commits.8 As with the asynchronous online-
copymethod,whenanodefails,thesurvivingnodequeueschangestothefailednode.Whenthe
node is to be recovered, the queue-draining process begins. Changes in the queue are sent to
the target system, where they are applied to the target database. Synchronous changes are
entered into the queue behind the asynchronous changes just as they would be entered during
normaloperation.
When all asynchronous changes have been applied, and when the remaining queue of
synchronous changes has reached an acceptably short length, the replication engine can once
again become a partyto further transactions. However, there are still some queued changes that
were part of earlier transactions to which the failed node was not a voting party. When these
7AchievingCenturyUptimesPart17:HPUnveilsItsSynchronous-ReplicationAPIforTMF,TheConnection;July/August
2009.
8
AchievingCenturyUptimesPart18:RecoveringfromSynchronous-ReplicationFailures,TheConnection;
September/October2009.
6
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

changes havebeenappliedtothetargetdatabase,therecoveringnodeis nowfullysynchronized
andcanbereturnedtoservice.
With coordinated commits, there is no additional load placed on the surviving node during
recovery. It continues to operate in its normal asynchronous-replication mode. Also, there is no
need to pause the application to complete resynchronization. If a very long transaction was in
process at the time that the replication engine began to rejoin transactions, all that happens is
thattherecoveryofthefailednodeisdelayeduntilthattransactioncompletes.
Summary
Recovering the database of a failed node in an active/active system using synchronous
replication is somewhat more involved than in such a system using asynchronous replication.
Recoverymustbeaccomplishedinbothcaseswithouttakingtheapplicationsdown.
Recoveryfirstrequiresthatthenodetoberecoveredhaveacopyoftheapplicationdatabasethat
is current as of the outage or at some time later. Changes that have accumulated during the
outage must then be applied while at the same time keeping the recovering database up-to-date
with new changes coming in. Only when the recovering database is fully synchronized with the
operationaldatabasecanitbereturnedtoserviceandbeginprocessingtransactions.
Recovery methods include synchronous online copying that imposes a load on a surviving
system during recovery, asynchronous online copying that requires the application to be briefly
paused before returning the recovered node to service, and a mixed online copy that avoids
theseproblemsbutthatdependsuponaspecificsynchronous-replicationarchitecture.
In any event, the impact of a node failure on a synchronously-replicated active/active system is
no different than a node failure on an asynchronously-replicated active/active system. The failed
nodeisremovedfromtheactive/activenetwork,whichcontinuestocarrytheapplicationloaduntil
thenodeisreturnedtoservice.
7
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
