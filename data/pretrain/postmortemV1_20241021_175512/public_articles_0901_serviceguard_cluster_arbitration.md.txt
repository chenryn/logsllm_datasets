HP Serviceguard Cluster Arbitration and Fencing Mechanisms
RaviKrishnamurthy,HPMasterArchitect
January2014
High availability clusters protect services and applications from unplanned downtimes and
allow for shorter planned downtimes by facilitating faster and easier maintenance. They
monitorhardwareandsoftwareforfaultsandfailoverapplicationstohealthynodesinthecluster.
Amongst the many faults that clusters handle, network faults need more care and involve more
complexity in handling, since they can affect heartbeat and hence can affect cluster membership as well.
Arbitration mechanisms are needed to handle network faults. In addition, fencing mechanisms are
neededtoavoiddatacorruption,apossibleside-effectofnetworkfaults.
The Impact of Network Partitioning
High availability clusters can experience network partitions in spite of using designs with redundant
network paths and switches. When network failures happen in a specific sequence or only in specific
segments in clusters using these networks for heartbeats, they result in the cluster being partitioned in
various ways. When such partitions happen, resulting in nodes of each partition unable to talk to each
other, clusters typically use arbitration mechanisms to determine which partition survives and which one
will be evicted from its membership. After the arbitrator determines the partition to be evicted, I/O fencing
mechanismslikeSCSI-3Persistentreservationincombinationwithnodefencingmechanismsareusedto
ensure that the evicted members of the cluster reboot and that no I/O is generated from them until they
rejointhecluster.
Various arbitration mechanisms such as a quorum server process, a dedicated shared LUN, or a volume
group per cluster are used for the purpose of arbitration by different clusters. HP Serviceguard cluster for
HP-UX,as well as Linux,supports manysuchmechanisms as arbitrators.Therearedifferentadvantages
anddisadvantagesforeachtypeofarbitrationmechanism.ThesearediscussedhereinthecontextofHP
Serviceguard.
Various arbitration mechanisms
QuorumServer
A quorum server is a process that runs on a separate node outside of the cluster and arbitrates in the
eventofanetwork partition.Itsadvantageisthatitisverysimpletosetupandcanbesharedbyupto50
clusters for the purpose of arbitration. A disadvantage, though, is that during a network partition, the
clusternodesmaynotbeabletocommunicatewiththequorumserverduetopartitionsinthenetwork.To
overcome this, HP Serviceguard supports multiple paths from each cluster node to the quorum server to
provideredundancy.Ifonepathfails,theotherpathis usedbythenodetocommunicatewiththequorum
server.
1
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

LockLUN
Thelock LUN arbitrationmechanism is asmall,dedicated,sharedLUN on which afastmutexstructureis
laid out. The coordinator servers of each partition try to acquire the mutex on the disk, and the partition
thatobtainsthelocksurviveswhiletheotheroneisevictedfromtheclustermembership.
VolumeGroupLock
AnotherarbitratoronHP-UXisthevolumegrouplock,orvglock,wherethesamefastmutexislaidouton
the metadata of the volume group so that coordinator members of each partition tryto acquire the lock in
ordertosurvivethepartition.
Disk basedarbitrationmechanisms likelock LUN andvglock areconvenientand useful whenthe number
of nodes in a cluster is four or less and all the nodes are co-located. A quorum server is preferable either
when the number of cluster nodes exceed four or when they are located in more than one location, i.e.,
clustersstretchedacrosscitiesorlocatedacrosstwoMetropolis.
Handling network partitions and fencing
When a Serviceguard cluster experiences a 50 - 50 network partition, i.e., the cluster is partitioned into
exactly two sets of nodes that can communicate within themselves but not to each other, the cluster
requests the configured arbitration mechanism to select one of these partitions to continue in the
membershipof that cluster. Oncethe arbitrator selects apartition, the nodes of theother partitiontimeout
and evict themselves from the cluster. When the two sides of the network partition have an unequal
number of nodes, the cluster chooses the partition that has a majority number of nodes, without
employing an arbitrator. Clusters are recommended to be configured with redundant network paths for
heartbeatanddatasothatasinglefailuredoesnotcauseclusterreformationsorworkloadfailovers.
Clusters typically use various fencing mechanisms. HP Serviceguard uses a deadman kernel module to
ensure that nodes that are evicted from the membership of a cluster reboot within a guaranteed duration
during a network partition. This is thefirst of two guarantees that are essential to prevent data corruption.
The second one is the prevention of ghost I/O. HP Serviceguard ensures this by using SCSI-3 Persistent
reservationtechnologyon all the datastoragethatis configured as partof thecluster.Theregistrationfor
the node that is evicted from the membership is revoked from the shared cluster storage so that delayed
I/Ofromthatnodeisnothonoredbythestorageaftertheclusterreconfigurationphaseiscompleted.
Arbitration and fencing mechanisms ensure clusters handle network partitions and the associated
requirements to prevent data corruption and ghost I/O. The mechanisms associated with these actions
arecriticaltothestablefunctioningofacluster.
RaviKrishnamurthyisaMasterArchitectfromHPwhohasexperienceinthedomainsofHighAvailability,
EmbeddedOperatingSystems,andDiagnostics.
2
©2013SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com