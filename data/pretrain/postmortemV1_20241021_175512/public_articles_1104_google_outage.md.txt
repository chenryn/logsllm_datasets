Cascading Software Bugs Take Down Google Compute Engine
April2016
On April 11, 2016, Google’s Compute Engine (GCE) had a massive outage that affected
all of Google’s regions worldwide. The outage was caused by a series of software bugs
that fed on each other while Google engineers were busy upgrading their network. The
first software bug caused the network upgrade to become corrupted. The second software bug sent the
corrupted upgrade to the network rather than cancelling it. The third software bug failed to inform the
networkmanagementsoftwarethatacorruptednetworkupgradewasbeingpropagated.
TheresultwasthatinboundInternettraffictoGooglewasnotroutedcorrectly.Connections weredropped
and users could not reconnect. Services dependent upon the network such as VPNs and Level 3 load
balancers began to fail. Google users worldwide were unable to connect to the Google Compute Engine.
OutboundInternettrafficwasnotaffected.
Theasia-east1 region was unreachablefor over anhour.TheentireGoogle worldwide GCE network was
downforeighteenminutes.
Googleannouncedservicerefundstoitsclients.TheyexceededtherequirementsofGoogle’sSLAs.
The Google Compute Engine
The Google Compute Engine is the Infrastructure-as-a-Service (IaaS) component of the
Google Cloud Platform. It is fundamental to the global infrastructure that runs Google’s
searchengine,Gmail,YouTube,andotherGoogleservices.
The GCE enables users to launch virtual machines (VMs) on demand. VMs can be launched from
standard Google images or from custom images created by users. VMs run under the KVM hypervisor
andsupportWindowsorLinuxguestoperatingsystems.
Google IP Blocks
Google uses contiguous blocks of internet addresses for users to connect to Google services. It calls
these blocks IP blocks. IP blocks are announced to the rest of the Internet via the industry-standard
Border Gateway Protocol (BGP).1 This announcement allows systems outside of Google’s network to
‘find’GCPservicesregardlessofthenetworktowhichthesystemsareconnected.
To maximize service performance, Google’s networking systems announce the same IP blocks from
severaldifferentlocations inits network aroundthe world.This allows users totaketheshortestavailable
paththroughtheInternettoreachtheirdesiredGoogleservice.
1EavesdroppingontheInternet,AvailabilityDigest;March2009.
http://www.availabilitydigest.com/public_articles/0403/bgp.pdf
1
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

This approach also enhances reliability. If a user is unable to reach one location announcing an IP block
due to an Internet failure between the user and Google, the user will be sent to the next closest point of
announcement.
The Corrupted Network Upgrade
On Monday, April 11, 2016, Google engineers decided to remove an unused GCE IP block from the
network configuration.TheyinstructedtheGooglesystemstopropagatethenew configurationacross the
network.Thissortofchangehadbeenperformedmanytimespreviouslywithoutincident.
SoftwareBug1
However, on this occasion, the network configuration management software detected an inconsistency in
thenewlysuppliedconfiguration.Thedetection of theinconsistencywas triggered byatimingquirk inthe
IP block removal procedure. The IP block had been removed from one configuration file, but the removal
had not yet propagated to a second configuration file. The network configuration management software
deemedthistobeafailureintheupgrade.
The network configuration management software is designed to be ‘fail safe’ and to revert to its current
configurationifitdetectsaproblemratherthanproceedingwiththenewconfiguration.
SoftwareBug2
However, a second software bug reared its ugly head. Instead of retaining the previously known good
configuration, the network configuration management software instead removed all GCE IP blocks from
the network configuration. It then began to propagate this new (now empty) configuration across the
network.
The Sick Canary
Google’s networking systems have a number of safeguards to prevent the propagation of incorrect
configurations.Onesuchsafeguardisthe‘canarystep.’
Rather than immediately deploying the new network configuration across the entire network, it is first
deployed to a single site. If it works properly at that site, it is deployed to a few more sites. In this way, a
failurehopefullycanbedetectedbeforeitbecomeswidespread.
SoftwareBug3
The canarystep indeed identified the new configuration as being unsafe. However, another software bug
did not send the canary step’s conclusion back to the network management software. Therefore, the
network management software concluded that the new configuration was safe and began it progressive
rollout.
The Google GCE Network Fails
As the rollout progressed, those sites that had been announcing GCE IP blocks ceased to do so when
theyreceived the new configuration with no IP blocks.As moreand moresites stopped announcing GCE
IP blocks, the network continued to send GCE traffic to the remaining sites that were still announcing
GCE IP blocks. However, user communication latency was rising as more and more users were sent to
sitesthatwerenotclosetothemandasthosesitesgothitwitheverincreasingtraffic.
The first area to be noticeably affected was the asia-east1 region. It finally lost connectivity at 18:14 PM
U.S. Pacific time. Google engineers had been trying to determine the cause of the asia-east1 problems.
2
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Fifty-three minutes after asia-east1 lost connectivity, at 19:07, the last site announcing GCE IP blocks
received the configuration. Now, with no sites announcing IP blocks, internet traffic to the GCE dropped
quickly.Twominuteslater,at19:09,ithaddroppedby95%.
Because the outage took down all regions, it made it difficult if not impossible for clients to mitigate the
impact of the outage. However, the outage did not affect the Google App Engine, Google Cloud Storage,
orotherGoogleCloudPlatformproducts.
The Aftermath
The Google engineers now knew they had a widespread problem. Without knowing the cause, they
immediately backed out the configuration changes and propagated the original configuration throughout
the network. This action was successful at ending the outage at 19:27. In total, the entire GCE network
had been down for eighteen minutes. The asia-east 1 region had been down for one hour and thirteen
minutes.
Having frozen all configuration changes, the Google engineers worked through the night to ensure the
systems were stable and to determine the root cause of the problem. By 7 AM the next morning, they
were confident that they had established the cause as software bugs in the network configuration
managementsoftware.TheydeterminedthattheGCEwasnotatriskofareoccurrenceoftheproblem.
The Google engineering teams then turned to the task of identifying a broad array of prevention,
detection, and mitigation systems intended to add additional defenses against similar problems in the
future. After just the first day, they already had planned fourteen distinct engineering changes spanning
prevention,detection,andmitigation.
Google’s Reimbursement
Google is reimbursing all affected Google Compute Engine users with service credits of 10% of the
monthlychargesforGCEclientsand25% ofthemonthlychargesforVPNclients.Thesereimbursements
exceedGoogle’sSLArequirements.2
Lessons Learned
Googlelearnedseverallessonsfromthisoutage:
 There must be safeguards to prevent a failure in a progressive rollout from being masked by a
failureofthemonitoringsystem.
 They should monitor for a decrease in capacity or redundancy even when the system is
functioningproperly.
 IP block announcements should be compared before and after a configuration change to ensure
thattheyarestillcorrect.
 NetworkconfigurationsshouldbecheckedtoensurethattheycontainspecificIPblocks.
2https://cloud.google.com/compute/sla
https://cloud.google.com/vpn/sla
3
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Acknowledgements
Thankstooursubscriber,GaryDick,forpointingustothistopic.
Informationforthisarticlewastakenfromthefollowingresources:
GooglePostMortem,https://status.cloud.google.com/incident/compute/16007?post-mortem;April13,
2016.
GoogleReimbursesCloudClientsAfterMassiveGoogleComputeEngineOutage,Talkin’Cloud;April14,
2016.
GoogleComputeEngine,Wikipedia.
4
©2016SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com