Therx6600testandtrainingnodeatoneofthesitesprovidestwofunctions.Itmatchesthepower
of the production database server nodes and can therefore be used for scalability and
performance testing. It also serves as a cold standby backup for a production node should one
nodesufferanextendedoutage.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman

Each site also has an application server to complete the test and training configuration. These
nodesalsoserveascold-standbybackupsfortheequivalentproductionnodes.
ManagementInformation
BusinessObjectsServersateachsiteoperateinaload-balancedconfigurationtodriveanOracle
Operational Database System (ODS) for management information. Web access is provided to
thissystemformanagementreports.
In addition, two of the EVA 4100 storage arrays (one per site) are configured with additional
capacitytosupportarchiving.TheArchiveServerisanidenticalrx2660nodethatisconfiguredas
asinglenodecluster.
DataandStorageNetworks
In order to minimize risk due to contention for
NBS
network resources, there are private links for
center
cluster communication and for data replication
as well as a WAN network to interconnect the
users in the field to the presentation servers.
2mb 2mb
All network links are redundant to ensure the
uninterrupted provision of services to PULSE
users in the event of any network path or
switchfailures. WAN 512kbDSL small
network site
Cluster interconnect traffic is sent over two
physically separate Gigabit Ethernet circuits
carried on the DWDM2 intersite links. The 34mb 34mb
private cluster-interconnect data network is
implemented using HP ProCurve switches
with ProCurve meshing. The cluster nodes
site1 site2
and storage arrays are interconnected via
dual SAN fabrics, with each cluster and each
storage arrayconnected to both fabrics. SAN- NDCC
fabric traffic is carried over two physically
separate pairs of 2 gigabit/second fibre WANConnections
channel circuits (a total of 4 gbs per SAN
fabric)transmittedonphysicallyseparateDWDMintersitelinks.
The user data network is Cisco-based. All cluster nodes are connected to both the HP ProCurve
private cluster interconnect network and to the Cisco user data network. This completely isolates
thetwotypesoftrafficwithnoriskoftrafficcontention.
ClientconnectionstothefifteenmajorNHSBT distributionsitesareviaaredundantWANnetwork
that interfaces with the Cisco user data network. Each remote distribution site is connected to
both WANs, and each NDCC data center has connections to both WANs. Each data center and
each distribution site connects to the redundant WAN network via separate POPs (points of
presence) to mitigate against exchange failures. In the event that both WAN connections should
fail,DSLbackupisprovidedbetweentheNDCCandthedistributionsite.
The WAN speed is 34 megabits/sec. at the data centers and two megabits/sec. at the sites.
Traffic is load-balanced across the dual WANs. Should a WAN fail, the other WAN carries all of
thetraffic.
2
Dense wavelength division multiplexing. DWDM is an optical technology that increases the bandwidth capability of a
strandoffibreoptic
6
©2008SombersAssociates,Inc.,andW.H.Highleyman

MinorNHSBTsitesareconnectedtothedatacentersvia512kilobit/sec.DSLconnections.
System Management
TheNDCCOpenVMSclustersforthePULSEapplicationsaremonitoredandmanagedwithHP’s
DTCS (Disaster Tolerant Computer Services) products. They provide detailed alerting of any
issues or deviations from the expected operational state of any of the nodes or storage
subsystems.
CutoverFromOldToNew
Migration of the application clearly had to be done as speedily as possible. However, some
outagetimewas essentialfor this tohappen.Thetime-consumingaspectwas the datamigration,
where data had to be unloaded from the old systems, moved to the new systems, and then
merged into a single database. Because the data was previously held in three almost identical
databases,theplanwastomigrateinthreestages,oneforeachdatabase.
Stage1involvedprovingthatthesoftwarefunctionedproperlyontheHPIntegrityserverplatform.
Datafromoneofthethreeregionswasmovedtothenewsystem,whichprovidedconfidencethat
the hardware and software was correctly configured and which demonstrated compatibility
betweentheoldandnewsystems.
Stage 2 involved unloading the data from the second regional database, copying it to the new
platform, and then merging it into the new database. This provided confidence in the database
mergeprocesspriortothefinalregionaldatabasebeingmovedacross.
Stage 3 completed the data unload/merge process, at the end of which the single national
databasewasoperational.
Severaltrialmigrationswerecompletedpriortothefinallivemigration.
The phased cutover also provided confidence in the performance behavior and capacity of the
new system platform. The platform provides excellent response-time performance with minimal
latency and sufficient capacity to absorb intermittent spikes in workload with little impact on
responsetimes.
Disaster Tolerance
RedundantDataCenters
The NDCC’s disaster-tolerance capability starts with a pair of fully configured and geographically
separated data centers sharing the load. Each data center is equipped with dual uninterruptable
power supplies (UPS), automatic fire-suppressant devices, and full environmental monitoring that
islinkedtoautomaticnotificationdevices.
RedundantProcessors
Allprocessingcapacityisredundantandisdistributedbetweenthetwodatacenters.
During normal operation, the primary rx6600 production cluster node runs the Mimer SQL
production database server. Consistent data replication across the three storage arrays is
provided by OpenVMS host-based volume shadowing. Other nodes in the cluster provide
ancillary services such as running historical query requests against a copy of the database, tape
backupprocessing,etc.
7
©2008SombersAssociates,Inc.,andW.H.Highleyman

Should the primary node fail, its workload is swiftly passed to the rx6600 cluster node in the
opposite datacenter.In the event of aserious failuresuchas the long-term loss of adatacenter,
the test and training rx6600 can be brought into the production cluster. In reality, production can
survive on a single node in a single data center. It would take the simultaneous failure of three
rx6600 cluster nodes or all of the EVA storage controllers to take down the NDCC, a highly
unlikelyscenario.
Should an rx2660 node in the production cluster fail, its processing load is taken over by the
survivingnodes inthecluster. Theloss of anrx2660nodeis simplyaninconvenienceasmuchof
itsfunctionalitycanbeabsorbedbytheremainingnodes.
Presentation servers and application servers are not clustered. However, the load is balanced
across them by utilizing Citrix load-balancing techniques for the presentation servers and by
explicit mapping of COM+ application servers to specific “failsafe IP” addresses to spread the
load across all available NICs. Should a presentation server fail, the users connected to that
server canreconnectto asurvivingserver andcontinuereceivingservices.Shouldanapplication
server fail, a surviving server can take over its load. The GUI applications are written in such a
way that each interaction between the COM+ servers and the database and presentation
programsarestateless.
Sinceallnodesareactivelyparticipatingintheapplication,failovertoasurvivingnodefollowinga
nodefailureisvirtuallyinstantaneous.
RedundantStorage
The NHSBT database is three-way, host-based volume shadowed (HBVS) and is distributed
between the two data centers. Since replication is synchronous, no file system data is lost
following a processor or storage-array failure. Shadow-set reconstruction following restoration of
equipment to service is invoked by the DTCS products during the node boot process and can
alsobeinvokedmanuallyifnecessary. Rapidshadow-setrebuildis providedusingtheOpenVMS
HBVSfeaturesknownas“mini-copy”and“mini-merge.”
At the request of NHSBT, Mimer implemented a significant improvement known as “fast restart”
to the Mimer SQL database manager. If the database server terminates abnormally (e.g., a
system failure), the database can be left in an inconsistent state. Thus, the databases must be
scanned to check their integrity during the database server startup process. Previously, users
could not access PULSE until this process was completed, which could take on the order of an
hour.
The“fastrestart” improvement was madetotheMimerSQLdatabasemanager sothatusers can
connect to PULSE immediately upon the restart of the database server. Database integrity and
consistency checking proceed in parallel while users start to use the database. This reduces
application failover time experienced by the users following the failure of the primary node from
about one hour to a few seconds. System availability is therefore increased by greatly reducing
thetimerequiredtorestoreservice.
RedundantNetworks
EveryfibrechannelandGigabitEthernetconnectionisredundant.
Eachrx6600has four 4-gbs fibrechannelpaths (i.e., atotalof 16gbs of bandwidth) toeachlocal
EVA4100storagearrayviatwoentirelyseparateFCfabrics.Theintersitefibrechannellinkstake
diverse paths so that any intersite link disruption is confined to path switching between the fibre
channel switches and does not impact the production system. HBVS balances the read load to
the fastest responding disks, which results in the local EVA storage arrays being used for the
majorityofreadrequests.
8
©2008SombersAssociates,Inc.,andW.H.Highleyman

TheProCurveprivatecluster connections are“dualrail” for cluster traffic(SCSprotocol3) anduse
“LAN failover” (equivalent to NIC teaming on Proliant Servers) for DTCS monitoring, DECnet
interconnects, and EVA scripting functions. This is implemented using ProCurve “meshing” and
VLANs for the different types of traffic, where meshing provides a shortest-path connection
betweenProCurveswitches.Anyintersitelinkdisruptionisconfinedtopathselectionbetweenthe
ProCurveswitchesanddoesnotimpacttheproductionsystem.
This design has already proven its worth when one of the intersite DWDM pipes was severed by
road construction work shortly after the national system was fully operational. There was no
disruption to normal operation, and the DTCS monitoring subsystem alerted the operations staff
totheproblem.
TheCiscouser data network IPconnections use “FailsafeIP” that allows the active IP addresses
tobeautomaticallymovedtootherNICsinthesamefailsafeIPgroupintheeventofaNIC,cable
orswitchfailure.
EveryWAN connection is redundantor has an alternatebackup.Therearetwo separatelyrouted
connectionstotheWANfrom eachdatacenter.Shouldbothfail,aDSLconnectionisestablished
tocontinuecommunications.
Each major NHSBT center is served by two independent WAN connections. Small sites are
servedbyDSLlinksthatareinherentlyredundantinthecarriers’networks.
Summary
NHS Blood & Transplant provides a life-saving service. It cannot fail, especially in the face of
major incidents. NHSBT has achieved an extremely high level of disaster tolerance through the
useofdualdatacentersandsplit-siteOpenVMSclusters.
Every component within the National Data Center Complex is redundant, from data centers to
processors, storage arrays, and networks. There are no standby backup components. Every
component is active and is used in a load-sharing configuration. Therefore, should there be a
component failure, its load can be shifted to a surviving component very quickly. In addition,
NHSBT stocks on-site spares for high probability of failure components such as disks and power
supplies.
Though NHSBT’s contractual requirement is to achieve an availability of three 9s, the NDCC
configuration should far exceed this requirement for unplanned downtime. It is only planned
downtime that will have an impact on the availability of blood services to those serviced by
NHSBT.
In summary, this system demonstrates considerable in-depth strength to deliver extremely high-
availability blood-product services to NHSBT by using the PULSE software, the Mimer SQL
database, and the OpenVMS clusters running on Integrity Server systems and EVA storage
subsystems.
3OpenVMSclusterSystemCommunicationServices.
9
©2008SombersAssociates,Inc.,andW.H.Highleyman

Acknowledgements
WeattheAvailabilityDigestwouldliketothankthefollowingfirmsfortheireffortsinimplementing
thissystemandinthepreparationofthispaper:
-NHSBlood&Transplant(www.blood.co.uk)
-Savant(applicationproviderandoperationalsupport)www.savant.co.uk
-Mimer(databasesupplier)www.mimer.com
-OCSL(hardwareresellerandinstaller)www.ocsl.co.uk
-HPC&I(projectmanagementanddelivery)
-HPDTCS(systemandinfrastructuremonitoring)
-XDelta(platformdesignandimplementationonbehalfofHPC&I)www.xdelta.co.uk
10
©2008SombersAssociates,Inc.,andW.H.Highleyman

