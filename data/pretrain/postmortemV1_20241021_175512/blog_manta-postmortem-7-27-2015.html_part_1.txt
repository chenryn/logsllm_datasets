## Postmortem for July 27 outage of the Manta service 

We would like to share the details of what happened during the July 27
Manta service outage, including what we have learned and what actions we
are taking to prevent this from happening again. We know that the Manta
service is critical for many of our customers (and indeed, it plays a
critical role in our own infrastructure), and we apologize for the
outage and the inconvenience it caused.

## Scope of the event

Around 10:30 UTC (3:30AM US Pacific), the Manta service started serving
a high percentage of error responses. Clients experienced very high
latency, and ultimately received 500-level errors in response to about
22% of all types of requests, including PUT, GET, and DELETE for both
objects and directories. At peak, the error rate approached 27% of all
requests, and for most of the window the error rate varied between 19
and 23%. This lasted until about 20:45 UTC (1:45PM US Pacific). By 20:45
UTC, the error rate returned back to zero.

## Root cause analysis

For transparency, we\'ve included a lot of detail here about how the
incident proceeded. If you\'re interested in more of a summary, skip to
the Summary of Root Cause section below.

### Background

The Manta service employs a metadata tier consisting of sharded
PostgreSQL databases. These databases map the user-visible namespace of
directories and objects (e.g., \"/\$MANTA_USER/stor/your/object\") to a
list of backend servers that store the object.

The metadata tier is queried for all Manta requests relating to
directories and objects, including creating, listing, and removing
directories, as well as fetching and saving objects. Job data is also
stored in the metadata tier, so job-related requests also access the
metadata tier.

Each shard of the Metadata tier is a three-node PostgreSQL cluster using
synchronous replication and using our
[Manatee](https://github.com/joyent/manatee-state-machine) component to
manage automatic failover.

### The problem

We quickly identified that API requests were hanging and ultimately
failing because all queries to one of the PostgreSQL databases were
themselves hanging. API servers timed out these PostgreSQL queries and
returned 500-level errors to clients. Only one metadata shard was
affected, which is why a large percentage of requests continued to
succeed.

The affected database was responding to simple queries, but was not
responding to any queries on the table used for Manta metadata. We saw
that nearly all queries were \"waiting\", which typically means they\'re
blocked on a lock. The only unblocked query was an \"autovacuum\"
operation that had been kicked off at 10:30:09, right around the start
of the outage.

In the past, we have experienced similar, much less severe situations
where queries appeared to be blocked and autovacuum was the only
unblocked operation. In those cases, we\'ve successfully restored
service by cancelling the autovacuum operation. In attempt to restore
service as quickly as possible, we decided to restart PostgreSQL around
14:11. To help root-cause the problem after restoring service, we saved
the data that PostgreSQL makes available around locking status for locks
both held and wanted, as well as the list of queries running. (More on
this data later.) Then we restarted PostgreSQL. This did restore service
for about 5-10 minutes, but another autovacuum kicked off that shortly
resulted in the same situation: all queries blocked on locks, with only
the autovacuum operation actually running. At this point we observed
that the autovacuum had been kicked off and that it was (according to
PostgreSQL) \"to prevent wraparound\".

While this particular reason for autovacuuming was new to us, and while
we did not understand why the autovacuum operation was causing all other
queries to block, we also believed that it was out of our control (e.g.,
a PostgreSQL issue). We also believed that this operation was important
to the continued functioning of PostgreSQL. As we were quickly learning,
transaction wraparound is a serious concern, and it\'s important to
allow PostgreSQL to perform the maintenance required to avoid the
problem. More on both of these issues later.

So engineering spent the next several hours attempting to speed up the
autovacuum process, principally in two ways:

-   Using process microstate accounting (in particular, [prstat
    -Lm](https://www.illumos.org/man/1m/prstat)), we determined that the
    autovacuum process was spending a large percentage of its time
    sleeping. Using DTrace, we determined that the vast majority of time
    it was coming off-CPU was in order to read filesystem data. Also
    using DTrace to see which files were being read, we found that
    PostgreSQL was reading through a large number of files in order.
    While ZFS identifies sequential read behavior within a file and
    prefetches file data, it does not prefetch files that that have not
    been read yet. We manually prefetched this data, which improved
    autovacuum speed considerably. (This particular PostgreSQL operation
    is essentially a full scan of a table and all of its indexes \--
    which exceeded the memory of the system, inducing substantial read
    I/O.)
-   Once we started prefetching data, we found that PostgreSQL was still
    sleeping a lot, and was coming off-CPU as a result of a PostgreSQL
    function that was explicitly sleeping as part of the [cost-based
    vacuum
    delay](http://www.postgresql.org/docs/9.2/static/runtime-config-resource.html#RUNTIME-CONFIG-RESOURCE-VACUUM-COST)
    mechanism, which attempts to limit resources used by vacuum
    operations. In this case, with all database activity effectively
    blocked on this operation, this delay was only hurting us. Since
    these tunables are not dynamically adjustable, we manually patched
    the running process to avoid sleeping in this case. This improved
    performance considerably.

We observed the autovacuum scan the full table and all of its indexes,
and then do the same thing again. After several hours, it successfully
completed and updated the \"last_autovacuum_time\" metadata for the
table. The blocked queries completed and service was restored. However,
a few minutes later, another autovacuum kicked off for the same table,
also \"to prevent wraparound\". We still do not understand why the first
autovacuum did not work to clear this problem, but it was clear that we
could not wait several more hours for another operation to complete that
may still not clear the issue.

By this point, we\'d learned that we could tune up the threshold at
which PostgreSQL kicks off this operation, and that doing so was very
safe as a short-term measure. By doing so, we planned to restore service
and give ourselves time to better understand the issue and resolve it
less disruptively. We made the change around 20:40, service was quickly
restored, and the incident ended. We took action items to better
understand the underlying issues so that we could deal with the
immediate threat of wraparound threat in a less disruptive way, and also
to make sure that we continue dealing with it more proactively instead
of having these disruptive operations kick in.

### Subsequent analysis

#### More background

To fully understand what happened, a little more background on
PostgreSQL is required. PostgreSQL\'s implementation of [Multi-Version
Concurrency
Control](http://www.postgresql.org/docs/7.1/static/mvcc.html) means that
when a tuple (basically, a row) is changed, a new copy of it is created.
The old copy is kept around in case concurrent transactions reference
that version. These old copies eventually have to be removed by a
routine maintenance operation called *vacuuming*.

The vacuum process is also used to address the separate problem of
transaction id wraparound. In order to manage both consistency and
concurrency, rows are assigned a transaction id relating to when they
were created, and transactions can only see rows whose transaction id is
before their own transaction id. As with any fixed-size integer,
transaction ids can wrap around. To deal with this, the 32-bit
transaction id space behaves as a circular space, where the previous 2
billion transactions are in the \"past\" and the next 2 billion
transactions are in the \"future\". But in order for a table to contain
rows that span more than 2 billion transactions, an additional step is
necessary: very old rows are assigned a special value that indicates
that they\'re in the past for all current and future transactions. This
is called \"freezing\" a tuple. When deemed necessary, routine vacuuming
will perform an extra full table scan just to make sure old tuples are
frozen. Administrators can do this themselves using the \"VACUUM
FREEZE\" operation. (This is all a gross simplification of the
underlying concerns. The [official
documentation](http://www.postgresql.org/docs/9.2/static/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND)
and [Josh Berkus\'s blog
series](http://www.databasesoup.com/2012/09/freezing-your-tuples-off-part-1.html)
explain it in much more detail.)

PostgreSQL has an autovacuum mechanism that\'s intended to kick off
vacuum operations as needed to deal with both of the above problems. It
cleans up old tuples and, if necessary, scans the entire table to freeze
very old tuples.

The normal autovacuum mechanism is skipped when locks are held in order
to minimize service disruption. However, because transaction wraparound
