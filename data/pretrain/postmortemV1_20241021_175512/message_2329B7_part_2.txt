snapshot is taken of an EBS volume, only the data that has been modified
since the last snapshot is pushed to S3. When a snapshot is deleted,
only the blocks not referenced by later snapshots should be deleted. A
cleanup process runs periodically to identify all blocks that are no
longer included in any snapshots. This snapshot cleanup identification
process builds a list of the blocks included in the deleted customer
snapshots, a list of blocks referenced by active EBS volumes, and a list
of blocks referenced by other snapshots. Blocks that are referenced by
active volumes or snapshots are removed from the list of blocks to
cleanup.

The resulting cleanup list is saved, but not acted upon. At least one
week passes from the time the snapshot cleanup identification process
runs before any blocks it has flagged for deletion are allowed to be
removed. Each day, it updates the lists of blocks to delete, blocks
referenced by active volumes, and blocks referenced by other snapshots.
It also compares its updated lists to the prior day's and if any block
eligible for deletion the day before now shows up in the most recent
list of blocks referenced by active EBS volumes or snapshots, the
process flags those blocks for analysis. Typically, there are very few,
if any, items that get flagged for analysis. But, this part of the
process was introduced to protect against system or software errors that
could result in blocks falsely flagged for deletion. Actual deletion is
executed by an engineer who first, before running the actual deletion
process, evaluates the blocks flagged for analysis and verifies that
there are no blocks in the list scheduled to be deleted that have been
flagged for analysis. The engineer must present their verification step
to another engineer who approves the deletion.

In one of the days leading up to the Friday, August 5th deletion run,
there was a hardware failure that the snapshot cleanup identification
software did not correctly detect and handle. The result was that the
list of snapshot references used as input to the cleanup process was
incomplete. Because the list of snapshot references was incomplete, the
snapshot cleanup identification process incorrectly believed a number of
blocks were no longer referenced and had flagged those blocks for
deletion even though they were still referenced by customer snapshots. A
subsequent run of the snapshot cleanup identification process detected
the error and flagged blocks for further analysis that had been
incorrectly scheduled for deletion. On August 5th, the engineer running
the snapshot deletion process checked the blocks flagged for analysis
before running the actual deletion process in the EU West Region. The
human checks in this process failed to detect the error and the deletion
process was executed. On Friday evening, an error accessing one of the
affected snapshots triggered us to investigate.

By Sunday morning, August 7th, we had completed the work to fully
understand root cause, prevent the problem from recurring, and build a
tool that could create recovery snapshots for affected snapshots. We
then started to do the work necessary to map these affected snapshots to
customers and build the recovery snapshots, with the aim to communicate
this information to customers by Sunday night. However, before we got
very far in this endeavor, the power event began. We had to temporarily
stop work on the snapshot issue to respond to the power event. Once we'd
been able to restore the majority of the EBS volumes affected by the
power event, we returned to working on the snapshot issue in parallel
with restoring the remainder of the EBS volumes that were recovering
from the power event. By 4:19 PM PDT on August 8th, we'd completed
creating recovery snapshots for all affected snapshots, delivered them
to customers' accounts, and communicated about the issue on the Service
Health Dashboard.

**Actions to Prevent Recurrence**

There are several actions we intend to take to protect against a similar
occurrence. The following are some of the key ones.

To further prevent the loss of power, we will add redundancy and more
isolation for our PLCs so they are insulated from other failures.
Specifically, in addition to correcting the isolation of the primary
PLC, a cold, environmentally isolated backup PLC is being worked with
our vendors. We will deploy this as rapidly as possible.

For EC2, we are going to address the resource saturation that affected
API calls at the beginning of the disruption. We will implement better
load balancing to quickly take failed API management service hosts out
of production. Over the last few months, we have been developing further
isolation of EC2 control plane components (i.e. the APIs) to eliminate
possible latency or failure in one Availability Zone from impacting our
ability to process calls to other Availability Zones. While some of
those mitigations significantly reduced the impact of this disruption
and helped us recover the APIs quickly, we realize how important those
APIs are to customers, especially during an event. It will take us
several more months to complete some of the changes we're making, and we
will test and roll out these changes carefully. At the time of the
disruption, customers who had EC2 instances and EBS volumes
independently operating in multiple EU West Region Availability Zones
did not experience service interruption. We will continue to create
additional capabilities that make it easier to develop and deploy
applications in multiple Availability Zones.

For EBS, our primary action will be to drastically reduce the long
recovery time required to recover stuck or inconsistent EBS volumes when
there is a substantial infrastructure disruption. While some volumes
were recoverable immediately once we had power back, there was an
extended period of time for many volumes to recover due to the need to
create EBS snapshots within S3. As we described above, this long period
of delay was caused by the time required to move a very large amount of
data into S3 and then transfer that data to EBS recovery snapshots. To
significantly reduce the time required to restore these volumes, we will
create the capability to recover volumes directly on the EBS servers
upon restoration of power, without having to move the data off of those
servers. This will require providing a way for customers to know that a
volume has been shut down and restored, but will avoid the need for
restoration via snapshot. This will also substantially diminish any risk
associated with lack of capacity, regardless of how many volumes fail.

We've made changes to our deletion process to prevent recurrence of the
EBS software bug impacting snapshots. We are instrumenting an alarm that
will alert us if there are any unusual situations discovered by the
snapshot cleanup identification process, including blocks falsely
flagged as being unreferenced. We're also adding another holding state
for blocks flagged for deletion where they are logically unavailable but
retrievable for an additional, longer period of time. This will provide
additional time to discover and correct any problem without loss of
data.

We learned a number of lessons from this event that we will use to
continually improve the reliability of RDS Multi-AZ deployments. First,
we will implement changes to our health checks to avoid customer impact
in the event of a unique DNS connectivity issue like we experienced
here. Second, we will promptly fix the software bug that extended
failover times for a portion of Multi-AZ customers with primaries in the
affected Availability Zone. Third, we will implement an improved
handling of the edge case where either primary or secondary is down and
the health check cannot complete. In such a case, the successfully
running member of the Multi-AZ pair will initiate connection retries to
confirm it is no longer in a "split brain" mode, such that involving an
engineer might not be necessary.

**Improving Communication**

Communication in situations like this is difficult. Customers are
understandably anxious about the timing for recovery and what they
should do in the interim. We always prioritize getting affected
customers back to health as soon as possible, and that was our top
priority in this event, too. But, we know how important it is to
communicate on the Service Health Dashboard and AWS Support mechanisms.
Based on prior customer feedback, we communicated more frequently during
this event on our Service Health Dashboard than we had in other prior
events, we had evangelists tweet links to key early dashboard updates,
we staffed up our AWS Support team to handle much higher forum and
Premium Support contacts, and we tried to give an approximate time-frame
early on for when the people with extra-long delays could expect to
start seeing recovery. Still, we know what matters most to customers in
circumstances like this is knowing the status of their resources, when
the impacted ones will be healthy, and what they should do now. While we
provided best estimates for the long-lead recovery snapshots, we truly
didn't know how long that process was going to take or we would have
shared it. For those waiting for recovery snapshots, we tried to
communicate what was possible. If customers were architected to operate
across multiple Availability Zones, they could flip over to and/or
deploy resources in other Availability Zones. If customers were
architected such that spinning up new instances or volumes in the same
Availability Zone worked, they could do that. But, for those single
Availability Zone customers who needed a specific EBS volume to recover,
and whose EBS volume was in the group waiting for recovery snapshots,
there were really no short term actions possible.

There are several places we can improve on the communication front.
First, we can accelerate the pace with which we staff up our Support
team to be even more responsive in the early hours of an event. Second,
we will do a better job of making it easier for customers (and AWS) to
tell if their resources have been impacted. This will give customers
(and AWS) important shared telemetry on what's happening to specific
resources in the heat of the moment. We've been hard at work on
developing tools to allow you to see via the APIs if your
instances/volumes are impaired, and hope to have this to customers in
the next few months. Third, as we were sending customers recovery
snapshots, we could have been clearer and more instructive on how to run
the recovery tools, and provided better detail on the recovery actions
customers could have taken. We sometimes assume a certain familiarity
with these tools that we should not.

**Service Credit for Affected Customers**

For customers with an attached EBS volume or a running RDS database
instance in the affected Availability Zone in the EU West Region at the
time of the disruption, regardless of whether their resources and
application were impacted or not, we are going to provide a 10 day
credit equal to 100% of their usage of EBS Volumes, EC2 Instances and
RDS database instances that were running in the affected Availability
Zone in the EU West region. Additionally, any customers impacted by the
EBS software bug that accidentally deleted blocks in their snapshots
will receive a 30 day credit for 100% of their EBS usage in the entire
EU West Region (inclusive of snapshot storage and requests as well as
volume storage and I/O). These customers will also have access to our
Premium Support Engineers (via the AWS Support Center) if these
customers need any additional technical assistance in recovering from
this issue.

These customers will not have to do anything in order to receive the
credits, as they will be automatically applied to customers' next AWS
bill. The credits can also be viewed as they become available over the
next few weeks by logging into the AWS Account Activity page.

**Summary**

Last, but certainly not least, we want to apologize. We know how
critical our services are to our customers' businesses. We will do
everything we can to learn from this event and use it to drive
improvement across our services. As with any significant operational
issue, we will spend many hours over the coming days and weeks improving
our understanding of the details of the various parts of this event and
determining how to make changes to improve our services and processes.

Sincerely,\
The AWS Team\


