-   30 August 2023 @ 11:36 UTC -- All subscriptions in Australia East
    sent portal communications
-   30 August 2023 @ 12:07 UTC -- Failover initiated for eligible Cosmos
    DB accounts 
-   30 August 2023 @ 12:12 UTC -- Five chillers manually restarted
-   30 August 2023 @ 13:30 UTC -- Data hall temperature normalized
-   30 August 2023 @ 14:10 UTC -- Safety walkthrough completed for both
    data halls
-   30 August 2023 @ 14:25 UTC -- Decision made to start powering up
    hardware in the two affected data halls 
-   30 August 2023 @ 15:10 UTC -- Power restored to all hardware 
-   30 August 2023 @ 15:25 UTC -- Storage, networking, and compute
    infrastructure started coming back online after power restoration
-   30 August 2023 @ 15:30 UTC -- Identified three specific storage
    scale units still experiencing fault codes
-   30 August 2023 @ 16:00 UTC -- 35% of VMs recovered / Began manual
    recovery efforts for three remaining storage scale units 
-   30 August 2023 @ 16:13 UTC -- Account failover completed for all
    Cosmos DB accounts
-   30 August 2023 @ 17:00 UTC -- All but one SQL Database tenant ring
    had recovered
-   30 August 2023 @ 19:20 UTC -- 90% of VMs recovered
-   30 August 2023 @ 19:29 UTC -- Successfully recovered all premium
    storage scale units
-   30 August 2023 @ 20:00 UTC -- 99% of storage accounts were back
    online
-   30 August 2023 @ 22:35 UTC -- Standard storage scale units were
    recovered, except for one scale unit
-   30 August 2023 @ 22:40 UTC -- 99% of VMs recovered
-   31 August 2023 @ 04:04 UTC -- Restoration of Cosmos DB accounts to
    Australia East initiated
-   31 August 2023 @ 04:43 UTC -- Final Cosmos DB cluster recovered,
    restoring all traffic for accounts that were not failed over to
    alternate regions
-   31 August 2023 @ 05:00 UTC -- 100% of VMs recovered
-   1 September 2023 @ 06:40 UTC -- Successfully recovered all standard
    storage scale units 
-   3 September 2023 @ 20:00 UTC -- Final SQL Database tenant ring
    evacuated and all customer databases online

**How are we making incidents like this less likely or less impactful?**

-   Incremental load increases over time in the Availability Zone
    resulted in a chiller operating configuration that was susceptible
    to a timing defect in the Chiller Management System. We have
    de-risked this failure to restart due to voltage fluctuations, by
    implementing a change to the control timing logic on the Chiller
    Management System (Completed).
-   Emergency Operation Procedure for manual restarts of simultaneous
    chiller failures has changed from an 'adjacent' sequence to a data
    hall 'load based' sequence to ensure all impacted data halls have
    partial cooling, to slow thermal runaway while full cooling is being
    restored (Completed).
-   Following this incident, as a temporary mitigation, we increased
    technician staffing levels at the datacenter to be prepared to
    execute manual restart procedures of our chillers prior to the
    change to the Chiller Management System to prevent restart failures.
    Based on our incident analysis the staffing levels at the time would
    have been sufficient to prevent impact if a 'load based\' chiller
    restart sequence had been followed, which we have since implemented
    (Completed). 
-   Datacenter staffing levels published in the Preliminary PIR only
    accounted for "critical environment" staff onsite. This did not
    characterize our total datacenter staffing levels accurately. To
    alleviate this misconception, we made a change to the preliminary
    public PIR posted on the Status History page. 
-   Our Storage team has identified several optimizations in our large
    scale recovery process which will help to reduce time to
    mitigate. This includes augmenting data provided in our incidents to
    enable quicker decision making, and updates to our troubleshooting
    guides (TSGs) that enable faster execution (Estimated completion:
    December 2023).
-   Our Azure Service Fabric team is working to improve reliability of
    SQL Database tenant ring recovery. (Estimated completion: December
    2023).
-   Our SQL Database team is reviewing our 'auto-failover group' trigger
    criteria, to ensure that failovers can happen within the expected
    timeframe. (Estimated completion: October 2023).
-   Our SQL Database team is upgrading internal tooling to enable mass
    migration of databases. (Estimated completion: December 2023).
-   Our Cosmos DB team is working to optimize Service Managed Failover
    for single region write accounts to reduce time to mitigate
    (Estimated completion: November 2023).
-   Our AKS team is immediately converting all operation queue SQL
    Database databases to be zone redundant (Estimated completion:
    September 2023).
-   Our AKS team is also replacing all cross-region SQL Database queue
    usage with Service Bus queues that are zone redundant (Estimated
    completion: September 2023).
-   Our ARM team will complete its storage layer migration to the next
    generation, zonally redundant architecture (Estimated completion:
    December 2023).
-   Our incident management team is exploring ways to harden our
    readiness, process, and playbook surrounding power down scenarios
    (Estimated completion: October 2023)

**How can customers make incidents like this less impactful?**

-   Consider using Availability Zones (AZs) to run your services across
    physically separate locations within an Azure region. To help
    services be more resilient to datacenter-level failures like this
    one, each AZ provides independent power, networking, and cooling.
    Many Azure services support zonal, zone-redundant, and/or
    always-available configurations:
    [https://docs.microsoft.com/azure/availability-zones/az-overview](https://docs.microsoft.com/azure/availability-zones/az-overview)
-   For mission-critical workloads, customers should consider a
    multi-region geodiversity strategy to avoid impact from incidents
    like this one that impacted a single region:
    [https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application/](https://learn.microsoft.com/training/modules/design-a-geographically-distributed-application/)
    and
    [https://learn.microsoft.com/azure/architecture/patterns/geodes](https://learn.microsoft.com/azure/architecture/patterns/geodes)
-   Consider which are the right storage redundancy options for your
    critical applications. Zone redundant storage (ZRS/GZRS) remains
    available throughout a zone localized failure, like in this
    incident. Geo-redundant storage (GRS) enables account level failover
    in case the primary region endpoint becomes unavailable:
    [https://learn.microsoft.com/azure/storage/common/storage-redundancy](https://learn.microsoft.com/azure/storage/common/storage-redundancy)
-   Consider the relevant guidance for recovering your SQL Database
    databases during disaster recovery scenarios:
    [https://learn.microsoft.com/azure/azure-sql/database/disaster-recovery-guidance](https://learn.microsoft.com/azure/azure-sql/database/disaster-recovery-guidance)
-   Consider the relevant guidance for achieving high availability with
    Azure Cosmos DB
    [https://learn.microsoft.com/azure/cosmos-db/high-availability](https://learn.microsoft.com/azure/cosmos-db/high-availability)
-   More generally, consider evaluating the reliability of your
    applications using guidance from the Azure Well-Architected
    Framework and its interactive Well-Architected Review:
    [https://docs.microsoft.com/azure/architecture/framework/resiliency](https://docs.microsoft.com/azure/architecture/framework/resiliency)
-   Finally, consider ensuring that the right people in your
    organization will be notified about any future service issues -- by
    configuring Azure Service Health alerts. These can trigger emails,
    SMS, push notifications, webhooks, and more:
    [https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/VVTQ-J98](https://aka.ms/AzPIR/VVTQ-J98)

## July 2023

## 6 

[07/06/2023]

Post Incident Review (PIR) - Azure Monitor - Logs data access issues

Tracking ID: XMGF-5Z0


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/XMGF-5Z0](https://aka.ms/AIR/XMGF-5Z0)*

**What happened? **

Between 23:15 UTC on 6 July 2023 and 09:00 UTC on 7 July 2023, a subset
of data for Azure Monitor Log Analytics and Microsoft Sentinel failed to
ingest. Additionally, platform logs gathered via Diagnostic Settings
failed to route some data to customer destinations such as Log
Analytics, Storage, Event Hub and Marketplace. These failures were
caused by a deployment of a service within Microsoft, with a bug that
caused a much higher than expected call volume that overwhelmed the
telemetry management control plane. Customers in all regions experienced
impact.

Security Operations Center (SOC) functionality in Sentinel may have been
impacted. Queries against impacted tables with date range listed above,
inclusive of the logs data that we failed to ingest, might have returned
partial or empty results. This includes analytics (detections), hunting
queries, workbooks with custom queries, and notebooks. In cases where
Event or Security Event tables were impacted, incident investigations of
a correlated incident may have showed partial or empty results. 

**What went wrong and why? **

A code deployment for the Azure Container Apps service was started on 3
July 2023 via the normal Safe Deployment Practices (SDP), first rolling
out to Azure canary and staging regions. This version contained a
misconfiguration that blocked the service from starting normally. Due to
the misconfiguration, the service bootstrap code threw an exception, and
was automatically restarted. This caused the bootstrap service to be
stuck in a loop where it was being restarted every 5 to 10 seconds. Each
time the bootstrap service was restarted, it provided configuration
information to the telemetry agents also installed on the service hosts.
Each time the configuration information was sent to the telemetry hosts,
they interpreted this as a configuration change, and therefore they also
automatically exited their current process and restarted as well. Three
separate instances of the agent telemetry host, per application host,
were now also restarting every 5 to 10 seconds.

Upon each startup of the telemetry agent, the agent immediately
contacted the telemetry control plane to download the latest version of
the telemetry configuration. Normally this is an action that would take
place one time every several days, as this configuration would be cached
on the agent. However, as the deployment of the Container Apps service
progressed, several hundred hosts now had their telemetry agents
requesting startup configuration information from the telemetry control
plane every 5-10 seconds. The Container Apps team detected the fault in
their deployment on 6 July 2023, stopped the original deployment before
it was released to any production regions, and started a new deployment
of their service in the canary and staging regions to correct the
misconfiguration.

However, the aggregate rate of requests from the services that received
the build with the misconfiguration exhausted capacity on the telemetry
control plane. The telemetry control plane is a global service, used by
services running in all public regions of Azure. As capacity on the
control plane was saturated, other services involved in ingestion of
telemetry, such as the ingestion front doors and the pipeline services
that route data between services internally, began to fail as their
operations against the telemetry control plane were either rejected or
timed out. The design of the telemetry control plane as a single point
of failure is a known risk, and investment to eliminate this risk has
been underway in Azure Monitor to design this risk out of the system.

**How did we respond?**

The impact on the telemetry control plane grew slowly and did not create
problems that were detected until 12:30 UTC on 6 July 2023. When the
issues were detected, the source of the additional load against the
telemetry control plane was not known, but the team suspected additional
load had been created against the control plane and took these actions:

-   6 July 2023 @ 14:53 UTC -- Internal incident bridge created.
-   6 July 2023 @ 15:56 UTC -- \~500 instances of garbage collector
    service were removed, to reduce load on telemetry control plane
-   6 July 2023 @ 16:09 UTC -- First batch of Node Diagnostics servers
    were removed, to reduce load on telemetry control plane. This
    process of removing this type of server continued over the next 10
    hours.
-   6 July 2023 @ 20:20 UTC -- Source of anomalously high traffic was
    identified, and the responsible team was paged to assist.
-   6 July 2023 @ 23:00 UTC -- IP address blocks deployed, to prevent
    anomalous traffic from hitting telemetry control plane.
-   6 July 2023 @ 23:15 UTC -- External customer impact started, as
    cached data started to expire.
-   7 July 2023 @ 01:30 UTC -- Three additional clusters were added to
    telemetry control plane to handle additional load, and we began
    restarting existing clusters to clear backlogged connections.
-   7 July 2023 @ 02:19 UTC -- Initial customer notification was posted
    to the Azure Status page, to acknowledge the incident was being
    investigated while we worked to identify which specific
    subscriptions were impacted.
-   7 July 2023 @ 02:45 UTC -- An additional three clusters were added
    to telemetry control plane.
-   7 July 2023 @ 06:15 UTC -- Targeted customer notifications sent via
    Azure Service Health to customers with impacted subscriptions (sent
    on Tracking ID XMGF-5Z0).
-   7 July 2023 @ 09:00 UTC -- Incident declared mitigated, as call
    error rate and call latency against control plane APIs stabilized at
    typical levels.

**How are we making incidents like this less likely or less
impactful? **

We know customer trust is earned and must be maintained, not just by
saying the right thing but by doing the right thing. Data retention is a
fundamental responsibility of the Microsoft cloud, including every
engineer working on every cloud service. We have learned from this
incident and are committed to the following improvements:

-   We have ensured that our telemetry control plane services are now
    running with additional capacity (Completed)
-   We have created additional alerting on certain metrics that indicate
    critical, unusual failure patterns in API calls (Completed)
-   We will be adding new positive caching and negative caching to the
    control plane, to reduce load on backing store (Estimated
    completion: September 2023)
-   We are putting in place additional throttling and circuit breaker
    patterns to our core telemetry control plane APIs (Estimated
    completion: September 2023)
-   In the longer term, we are creating isolation between
    internal-facing and external-facing services using the telemetry
    control plane (Estimated completion: December 2023) 

**How can customers make incidents like this less impactful?**

Note that the Azure Monitoring Agent (AMA) provides more advanced
collection and ingestion resilience capabilities (such as caching,
buffering and retries) than the legacy Microsoft Monitoring Agent
(MMA). Customers who have not yet completed their migration from MMA to
AMA, would benefit from accelerating and completing the migration - for
use cases required by them and supported in AMA. For more details:
[https://learn.microsoft.com/azure/azure-monitor/agents/azure-monitor-agent-migration](https://learn.microsoft.com/azure/azure-monitor/agents/azure-monitor-agent-migration)

Customers using Microsoft Sentinel can consider the following
compensating steps:

-   Identify high priority assets, log sources that cover those assets,
    and detection or hunting logic normally applied to those assets and
    logs.
-   If possible, run one-time queries at the source, for the indicated
    date range where ingestion was impacted - based on the prioritized
    assets, logs, and logic. It may also be useful to run the same
    queries for week prior to the incident, compare, and look for
    differences.
-   Alternatively, to the extent the collection architecture allows for
    it, re-ingest data from the source into Sentinel, and run those
    one-time queries in Sentinel.

Finally, consider ensuring that the right people in your organization
will be notified about any future service issues - by configuring Azure
Service Health alerts. These can trigger emails, SMS, push
