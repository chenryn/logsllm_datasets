What Is the Availability Barrier?
March2010
Since the dawn of commercial computing in the 1950s, the technological quest for computer
engineers has been the drive to reduce the price/performance ratio of computing systems. We
needed to improve their performance and at the same time reduce their cost. Through intense
competition and massive investment in computing research and development, we have
surpassed early expectations manyfold. Today’s $400 desktop has thousands of times the
processingpowerofthemultimilliondollarmainframesof1960.
With today’s demands for 24x7 global services, we must now shift our attention to another goal –
reliability. No longer is it acceptable to be down for half a day due to a processor failure or to be
offlineforaweekendupgrade.Oursystemsareexpectedtobeupatalltimes.Anydowntimecan
have a significant corporate impact that might be measured in dollars, lost customers, regulatory
violations,orbadpublicity.
However,thepressisfullofstoriesaboutcatastrophic systemfailures.Whatisthebarrierthatwe
mustovercometoeliminatethesefailuresandtoachievehighreliability?
First we have to understand what we mean by reliability? That depends upon what we are trying
toachieve.
What is Reliability?
Therearethreemeasuresofsystemreliability:
 Availability, or the proportion of time that a system is operational. Availability is
representedbythesymbol“A”for“availability.”
 Failure Interval, or the average time between system failures. Failure interval is
representedbythesymbol“MTBF”for“meantimebetweenfailures.”
 Recovery time, or the average time that it takes to return the system to service following
afailure.Recoverytimeisrepresentedbythesymbol“MTR”for“meantimetorecover.”
Thesethreeparametersarerelatedbythewell-knownrelationship:
MTBF
A  (1)
(MTBF +MTR)
1
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

100%
TheAvailabilityBarrier
ytilibaliava
increasingMTBF
decreasingMTR
Over the last decades, we have attempted to improve reliability by increasing MTBF, the time
between system failures. We have become quite successful at this. In the early days of vacuum-
tube computing, the Whirlwind computer at MIT had an MTBF of about eight hours. Today,
computersmightfailonceortwiceperyear.
But try as we might to reduce our failure rates, we will never achieve 100% availability. From
Equation 1, continuous availability requires either that we have an infinite MTBF or a zero MTR.
Neitherispracticallyachievable.
Sowhichoftheseparameters,A,MTBFor MTR,should weconcentrateontoimprovereliability?
Asitturnsout,reliabilityisintheeyeofthebeholder.Forinstance:
 In a satellite, the failure interval, MTBF, is of paramount importance. Systems in a
satellite are typically not repairable. Therefore, when a system fails and cannot be
recovered by commands from the ground, the satellite fails. There is no recovery time,
andavailabilityismeaningless.Thesatelliteiseitheroperationalorithasdied.
 In a system measuring oil flow in a pipeline, availability, A, is the key parameter. To the
extent that the system is down, no oil flow is recorded; and revenue for oil delivered is
lost.
 We submit that in commercial data-processing systems, the recovery time, MTR, is the
importantparameter.Weexpandonourreasoningnext.
Recovery Time – The Availability Barrier for Commercial Data Processing
Bycommercial data-processing systems, we mean those that run applications in which users are
people or other computers that submit requests and that expect a timely response. These
systems are typically running OLTP (online transaction processing) applications that are often
mission-criticaltothecompany.Wedonotincludebatchsystems inthis discussionas availability
(A)maybeamoreimportantparametertobatchsystemsthanrecoverytime(MTR).
The importance of recovery time in critical applications can be illustrated by some examples.
Assume that you have an application with three nines of availability. Which set of reliability
parametersdoyouthinkuserswouldprefer?
 Case 1: The application is down one second every sixteen minutes. The users will
probably not even notice these outages if they are within the expected response time of
theapplication.
2
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

 Case 2: The application is down one hour every six weeks. Users are likely to complain
butgrudginglyacceptit.Afterall,“computersdofail,”theymaysay.
 Case 3: The application is down one day every three years. If the application is mission-
critical, this could have a serious impact on the company, perhaps causing it to halt
operationsortoreverttomanualproceduresatsignificantcostorperiltothecompany.
We submit that Case 1 is acceptable, Case 2 is painful, and Case 3 in unacceptable. The
interesting thing about these cases is that they all have the same availability of three 9s; so
availability, A, is not a factor in acceptable reliability for commercial systems. Even more to the
point, the failure interval, MTBF, is shorter (worse) for the better cases; so MTBF is certainly not
thedrivingfactor.
The driving factor is recovery time, MTR. In other words, let it fail, but fix it fast. This is the
reliabilitymantraforcriticalcommercialdata-processingsystems.Infact,ifrecoveryissofastthat
no one notices that there has been an outage, continuous availability has effectively been
achieved.
The Progression of System Availability
Up until recently, availability has been a second cousin to performance. Over the last sixty years,
performance has increased by a factor of several thousand, from one million instructions per
second to several billion instructions per second. But the availability of nonredundant systems
hasonlyincreasedbyahundredfold,fromone9tothree9s.
Toillustratethispoint,letuslookatthehistoryofavailability:
TheVacuum-TubeDays–One9
Computing in the 1950s was done by vacuum-tube monsters. These systems had to be taken
down every week or so for preventive maintenance. For instance, operating voltages were
reduced to see which vacuum tubes would fail; and they were replaced to try to minimize
unplannedfailuresduringnormaloperations.
TheSolid-StateRenaissance–Two9s
The advent of the transistor, which led to integrated circuits and the powerful processor chips of
today, was probablythemostimportantcontributor to availability(and performance) inthehistory
of computing. Solid-state systems in the 1960s showed an order of magnitude more availability
thantheirvacuum-tubepredecessors.
HardenedServers–Three9s
As we entered the 1970s, two 9s of availability (80 hours of downtime per year) became
intolerable for many critical applications. To reduce the frequency of failures, system vendors
began to include redundancy in critical components that were subject to the most failures. This
included power supplies, fans, RAID disk arrays and SMP (symmetric multiprocessing)
processors. On the software side, operating-system facilities were improved to ease system
managementsoastoreduceoperatorerrors.
Fault-TolerantServers–Four9s
By the late 1970s, it was becoming clear that further efforts to increase the MTBF of a single
nonredundant system were having less and less impact. Components were going to fail, and
nothing could be done about that. This realization led to the emergence of fault-tolerant systems
3
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

suchasthosefromTandemandStratus(andmanyothersthatdidnotsurvive).Inthesesystems,
faulty components were automatically detected. They were removed from service and replaced
virtuallyinstantlywithanequivalentcomponent.Theoperatingsystems werehardenedto reduce
software failures. In Tandem systems, a failure caused by a software bug could often be
recovered by switching to another copy of the application that had been kept synchronized with
theactivecopy.
IncreasingMTBF–TheEndoftheLine
Fault-tolerant systems in and of themselves achieved availabilities of five nines. However, other
failure reasons, such as manual errors and software bugs, reduced this availability to four nines.
What was needed was a way to quicklyswitch to another system if the active system failed. The
concept of “let it fail, but fix it fast” took a major step forward. No longer was it acceptable to
simply replace failed components. Many failure modes required that the entire system be
replaced – and fast. However, the existing technology for recovering from a massive system
failure – magnetic tape backup to a standby system – was woefully inadequate for high
availabilitysincefailovertookhoursordays.
Clusters–Five9s
Clusters solved the failover time problem to a great extent. A cluster comprises two or more
processors with access to a common database. Only one processor can be updating the
database, but the others are standing by in case the active processor fails.With clusters, failover
can be in minutes, leading to availabilities in the order of five 9s (five minutes of downtime per
year). This was the first step in achieving very high availability by rapidly failing over to a backup
system.
Active/ActiveSystems–Six9sandBeyond
For many critical applications, several minutes of downtime are simply unacceptable. Besides,
clusters cannotbegeographicallyseparated;andasitedisaster cantook downtheentirecluster.
Recovery has to be made to a remote backup system, which even with modern-day recovery
procedures such as virtual tape and database replication can take hours. To go beyond five 9s,
systems must be protected against site disasters as well as against system faults; and recovery
timesmustbereducedfromminutestoseconds.
Active/active systems solve this problem.1 The technology for active/active systems appeared in
the 1990s when bidirectional data-replication engines were introduced.2 An active/active system
is a network of geographically-distributed processing nodes, each having access to a distributed
copy of the application database. The database copies are kept in synchronism via data
replication. All processing nodes are actively engaged in the application, and a transaction may
be sent to any node in the application network. Should a processing node fail, all that is required
istoreroutetransactionstosurvivingnodes.Failoverismeasuredinsubsecondstoseconds.
Furthermore, active/active systems eliminate planned downtime. Upgrades can be rolled through
thenodes oneatatimebytakingeachnodedown,reroutingtraffic totheother nodes,upgrading
thedownednode,andreturningittoservice.
Breaking the Availability Barrier
1WhatisActive/Active?,AvailabilityDigest;October2006.
2Alimitedformofactive/activesystemswasintroducedbyDigital(nowHPOpenVMS)in1984.Itssplit-siteclusterswere
trulyactive/activesystems,butthesynchronousreplicationtechnologythatthesesystemsusedlimitedthegeographical
separationofthedatacenters.SeeOpenVMSActive/ActiveSplit-SiteClusters,AvailabilityDigest;June2008.
4
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

It took decades for computer technology to progress to the point that system reliability could no
longer be significantlyimproved byincreasing failure intervals. To achieve six 9s and beyond, we
must now look to recovering rapidly from the failures that we know are bound to occur. The
technology is now here to provide failover times measured in seconds or even subseconds.3We
break theavailabilitybarrierbyfocusingonMTR,thesystem recoverytime,ratherthanonMTBF,
thefailureinterval.Letitfail,butfixitfast.
The availability barrier is recovery time. But even if we can reduce recovery time in an
active/active system to milliseconds, have we really broken the availability barrier? Have we
achievedabsolutecontinuousavailability?Notquite.Wehavemovedtheavailabilitybarrierback,
but we haven’t eliminated it. There is always the remote possibility that all nodes in the system
will fail. In this case, we may be down for hours before we can restore enough nodes to service.
For instance, if a pair of fault-tolerant nodes is used in an active system, each with an availability
offour9s,thesystem willhaveanavailabilityofeight9s.Ifrecoverytimefrom atotalnodefailure
isfourhours,thistranslatestoonefour-hourfailureevery400centuries.
Why worry about that? Won’t we be long gone by then? Maybe and maybe not. That one failure
in 400 centuries might just happen tomorrow. Let’s make sure that if this unlikely failure should
occur,westillhaveanoperationalbusiness-continuityplan.
3AchievingFastFailoverinActive/ActiveSystems–Parts1and2;AvailabilityDigest;August,September2009.
5
©2010SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com