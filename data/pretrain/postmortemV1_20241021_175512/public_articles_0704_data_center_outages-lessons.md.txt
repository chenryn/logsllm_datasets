Help! My Data Center is Down!
Part 7: Lessons Learned
April2012
Inthefirstsix parts of this series, we described severalspectacular data-center failures that were caused
by a variety of factors – power outages, storage crashes, Internet and intranet failures, upgrades gone
wrong, and the actions of IT staff.These stories were taken from the Never Again series published in the
AvailabilityDigest(www.availabilitydigest.com).
Interestingly, most of these failures could not have been prevented by a better hardware/software
infrastructure. In none of the outages was a server failure the root cause. We seem to have learned how
tomanageredundancyinourserverfarmsquitewell.Afewoutageswerecausedbydualstorage-system
crashes. The recovery from intranet failures could have been mitigated with better internal network
monitoring.
The predominant cause of the failures was the direct action – or lack of action – by IT staff. In some
cases, the failure was directly caused by the actions of an IT staff member. In others, a failure due to
another cause was aggravated by human actions. In still others, the failure could have been averted if
earlier actions had taken place, such as better testing or documentation. In fact, studies have shown that
about70%ofalldata-centeroutagesinvolvedatleasttosomeextenttheactionsofhumanbeings.
When a data center fails, two questions must be answered – how long will it take to recover IT services,
and what can be done in the future to prevent a repeat of the failure? In the final part of this series, we
review some of the lessons that address these two questions based on what we have learned from the
data-centerfailuresthatwehavediscussed.1
People Need Redundancy, Too
We spend a lot of money on making our data centers redundant so that they will survive one or more
failures. However, we seem to never provide redundancy for the most fault-prone component – the
humanbeing.
It is probably obvious to most that there should be multiple staff members skilled in each critical task.
Someone must always be available when others are unavailable due to vacation or sickness. We
reviewed only one case (the Google Apps Engine outage) in which the outage was extended because of
thedifficultyinreachingknowledgeablestaff.
1OurthankstoTheConnectionforgivinguspermissiontoreprintthisseriesofarticles.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

However, weseem topay noattentionto an equallyimportanthumancomponentthat needs redundancy
– the fat finger. There were an abundance of failures that we described that were caused by a staff
member’sinappropriateaction:
 A State of Virginia technician pulled the wrong controller and crashed a redundant SAN that
alreadyhadsufferedacontrollerfailure.
 A technician with DBS Bank made an unauthorized repair on a redundant SAN and took down
bothsides.
 A system operator mistakenlydeleted the $38 billion Alaska Permanent Fund database and then
deleteditsbackup.
 Amaintenancecontractor’smistakeshutdowntheOaklandAirTrafficControlCenter.
 Thirteen million German web sites went dark when an operator mistakenly uploaded an empty
zonefile.
 Atesttechnicianfailedtodisableafirealarm actuatorpriortotestingthefiresuppressionsystem.
Theresultingsirennoisedamagedseveraldisks,includingthevirtualbackupdisks.
 A system administrator closed all applications on one server in an active/active pair to upgrade it
andthenshutdowntheoperatingserver.
All of these outages could have been prevented if a second pair of eyes were checking the actions of the
staff member. The lesson is that any critical operation that could have a negative impact on operations
should be checked by a second person prior to initiation. Let one person create the command or point to
the board to be pulled, and have a second person confirm the action prior to executing it. This is
especiallyimportantfollowingacomponentfailurethatleavesthesystemwithasinglepointoffailure.
Test, Test, and Test
Failover
Testing failover procedures is a time-consuming and riskytask that is often avoided by companies. They
wouldratherrelyonfaithandhopethatafailoverwillbesuccessfulfollowingaproduction-systemoutage.
Too often, faith and hope don’t come through. Failover faults are all too common, as experienced by
Google, Amazon, and BlackBerry. American Eagle found that not only could it not fail over to its backup
system,itcouldnotevenfailovertoitsbackupdatacenter.
Backup/Restore
Doyouknowhowlongitwilltaketorebuildyourdatabasefrombackuptapes?Youprobablydon’tknowif
you have never tried it. After multiple failover faults, American Eagle tried to restore its database from
magnetic tape and found that it would take an unacceptable amount of time. Clearly, it had never tested
taperecovery.Theresult–itsonlinestorewasofflineforeightdays.
Fallback Planning for Failed Upgrades
An upgrade is notorious for causing problems. An upgrade problem is survivable if fallback can be made
to the original system so that the upgrade can be corrected. However, much too frequently, there was no
fallback plan. When an upgrade caused problems, the systems were down, as experienced by PayPal,
Google,andBlackBerry.
Perhaps the most striking example of a failed upgrade with no fallback plan was the experience of the
IRS,theU.S.InternalRevenue Service.Itconfidentlydecommissioned its oldfraud-detectionsystem and
disposed of the hardware before it tested a new, upgraded system. The new system failed to work. With
no system to which to fall back, fraudulent tax returns went undetected for the next year, costing U.S.
taxpayersanestimated$300million.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Arelatedproblem isupgradesuploadedbyonlineservices.McAfeesentoutananti-virusupdatethathad
notbeencompletelytestedandtook downmillionsofPCsworldwide.Thelesson–burninupgradesona
handfuloftestsystemsbeforedistributingthemwidelythroughouttheorganization.
Document Failover and Recovery Procedures
When things go wrong, people get stupider. You cannot rely on memory or good judgment when your
staff is under great pressure to correct an outage. They must have good documentation to guide them.
The documentation should be simple and brief – there is no time to read a fifty-page document. Most
importantly, the documentation must be tested to ensure that it is complete and accurate; and your
peoplemustbetrainedinitsuse.
Googlelostits AppEngine services whenitcouldnotfailover toitsbackupdatacenterfollowingdamage
due to a power outage. It turned out that the failover procedures had changed, and the documentation
wasnotyetup-to-date.
Provide Independent Checking of Installations
Therewerea few incidents in whichan installation was done improperlyor was putoff andforgotten. It is
agoodidea tohaveaseparateteam thoroughlycheck aninstallationfollowingits completion. Keepa log
ofinstallationchangesthatarepending,andhavesomeoneresponsibleformonitoringthelog.
Review Security Logs
Speaking of logs, a recent report by Verizon and the U.S. Secret Service reported that the majority of
system intrusions by hackers were easily identifiable by information that had been logged, but no one
noticed. Many intrusions went on for weeks until some third party notified the company about the
fraudulentactivity.
Store all security-related information in a SIEM (Security Information and Event Management) system,
andusealog-analysistooltocontinuallymonitortheloggeddataforindicationsofsecuritybreaches.
The Internet Is a Best-Efforts Network
No one says that the Internet is reliable. The Internet goes down all the time, sometimes taking out
massiveareasforweeks.IfInternetcommunicationisthelife-bloodof yourcompany’sservices,youmust
have a backup plan. One viable option is to subscribe to one of the many satellite Internet backup
services.
Intranets Are Not Much Better
Many of the outages that we reported were caused by failures in a company’s intranet that tied its
systemstogether.Someofthesewereequipmentfailures,andmanywereroutingproblems.
Intranets can be very complex and expensive. Companies typically do not put the time and effort into
making their intranets as redundant as the Internet. Even worse, when an intranet problem does occur,
the network is so complex that it can take an unacceptable amount of time to determine where the
problemissothatitcanbecorrected.
This argues for a significant investment in network monitoring so that problems can be detected before
theybecomecatastrophic,andoutagescanberapidlyidentifiedandcorrected.
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Don’t Trust the Cloud
If youareusingthecloudforcriticalapplications,youmustconcernyourselfwiththepotentiallossof your
data. You can perhaps survive a few hours of application downtime, but you may not be able to survive
thepermanentlossofyourdatabases.
It is therefore prudent to maintain a local snapshot of your critical databases on systems under your
control. Amazon, WestHost, and T-Mobile’s Sidekick service have all experienced permanent database
losses.
A Strange One
When WestHost ran its yearly test of its Inergen fire-suppression system, a technician failed to properly
disable the actuators. The result was that the fire suppression system was triggered. Unexpectedly,
several disks in the data center were damaged or destroyed. It took six days for WestHost to get its data
centerbackintooperation,andmuchdatawaslost.
Theinitialsuspicion was thattheexplosiveforceof thegas release was theculprit.However,subsequent
studies bythe manufacturers of the fire-suppression system and of the Inergen gas showed that the disk
damagewascausedbytheear-splittingsoundlevelfromthewarningalarms.
Resulting recommendations are to ensure that sirens are not directed at cabinet enclosures and that disk
enclosuresbesound-proofed.It’snohelpifthefireisputoutbutthedatacenterisdestroyedbynoise.
And Finally …
Many of the failures could not have been anticipated or avoided. Who would expect that their battery
room would explode or that the entire Northeast United States and large areas of Canada would be
blacked out? No matter how much redundancy you have built into your system, no matter how well you
have your failover recovery procedures documented and your people trained, something is going to
happentotakedownyourdatacenter.
Consequently, your last line of defense must be a good Business Continuity Plan that guides your
companyalonga path of survival when it has lostcriticalIT services,perhaps for days. And remember,a
goodBCPmustbewell-documentedandwell-tested;andyourpeoplemustbetrainedinitsuse.
Based on the failures that we have seen, the IT portion of the BCP should not focus on the cause of an
outage but rather on its impact. It doesn’t make much difference whether your systems have been
destroyed byflood or fire. In either case, your systems are down; and what are you going to do about it?
After all, would you ever worry about an old lady severing your Internet service because she thought a
fibercablewasvaluablecopperthatshecouldsell?Yes,thathappened.
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com