# Incident report on memory leak caused by Cloudflare parser bug 

02/23/2017

![John
    Graham-Cumming](https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=64,height=64/https://blog.cloudflare.com/content/images/2017/03/url-2.jpg)

12 min read

Last Friday, [Tavis Ormandy](https://twitter.com/taviso) from Google's
[Project Zero](https://googleprojectzero.blogspot.co.uk/)
[contacted](https://twitter.com/taviso/status/832744397800214528)
Cloudflare to report a security problem with our edge servers. He was
seeing corrupted web pages being returned by some HTTP requests run
through Cloudflare.

It turned out that in some unusual circumstances, which I'll detail
below, our edge servers were running past the end of a buffer and
returning memory that contained private information such as HTTP
cookies, authentication tokens, HTTP POST bodies, and other sensitive
data. And some of that data had been cached by search engines.

For the avoidance of doubt, Cloudflare customer SSL private keys were
not leaked. Cloudflare has always terminated SSL connections through an
isolated instance of NGINX that was not affected by this bug.

We quickly identified the problem and turned off three minor Cloudflare
features ([email
obfuscation](https://support.cloudflare.com/hc/en-us/articles/200170016-What-is-Email-Address-Obfuscation-),
[Server-side
Excludes](https://support.cloudflare.com/hc/en-us/articles/200170036-What-does-Server-Side-Excludes-SSE-do-)
and [Automatic HTTPS
Rewrites](https://support.cloudflare.com/hc/en-us/articles/227227647-How-do-I-use-Automatic-HTTPS-Rewrites-))
that were all using the same HTML parser chain that was causing the
leakage. At that point it was no longer possible for memory to be
returned in an HTTP response.

Because of the seriousness of such a bug, a cross-functional team from
software engineering, infosec and operations formed in San Francisco and
London to fully understand the underlying cause, to understand the
effect of the memory leakage, and to work with Google and other search
engines to remove any cached HTTP responses.

Having a global team meant that, at 12 hour intervals, work was handed
over between offices enabling staff to work on the problem 24 hours a
day. The team has worked continuously to ensure that this bug and its
consequences are fully dealt with. One of the advantages of being a
service is that bugs can go from reported to fixed in minutes to hours
instead of months. The industry standard time allowed to deploy a fix
for a bug like this is usually three months; we were completely finished
globally in under 7 hours with an initial mitigation in 47 minutes.

The bug was serious because the leaked memory could contain private
information and because it had been cached by search engines. We have
also not discovered any evidence of malicious exploits of the bug or
other reports of its existence.

The greatest period of impact was from February 13 and February 18 with
around 1 in every 3,300,000 HTTP requests through Cloudflare potentially
resulting in memory leakage (that's about 0.00003% of requests).

We are grateful that it was found by one of the world's top security
research teams and reported to us.

This blog post is rather long but, as is our tradition, we prefer to be
open and technically detailed about problems that occur with our
service.

## Parsing and modifying HTML on the fly 

Many of Cloudflare's services rely on parsing and modifying HTML pages
as they pass through our edge servers. For example, we can
[insert](https://www.cloudflare.com/apps/google-analytics/) the Google
Analytics tag, safely rewrite http:// links to https://, exclude parts
of a page from bad bots, obfuscate email addresses, enable
[AMP](https://blog.cloudflare.com/accelerated-mobile/), and more by
modifying the HTML of a page.

To modify the page, we need to read and parse the HTML to find elements
that need changing. Since the very early days of Cloudflare, we've used
a parser written using [Ragel](https://www.colm.net/open-source/ragel/).
A single .rl file contains an HTML parser used for all the on-the-fly
HTML modifications that Cloudflare performs.

About a year ago we decided that the Ragel-based parser had become too
complex to maintain and we started to write a new parser, named cf-html,
to replace it. This streaming parser works correctly with HTML5 and is
much, much faster and easier to maintain.

We first used this new parser for the [Automatic HTTP
Rewrites](https://blog.cloudflare.com/how-we-brought-https-everywhere-to-the-cloud-part-1/)
feature and have been slowly migrating functionality that uses the old
Ragel parser to cf-html.

Both cf-html and the old Ragel parser are implemented as NGINX modules
compiled into our NGINX builds. These NGINX filter modules parse buffers
(blocks of memory) containing HTML responses, make modifications as
necessary, and pass the buffers onto the next filter.

For the avoidance of doubt: the bug is *not* in Ragel itself. It is in
Cloudflare\'s use of Ragel. This is our bug and not the fault of Ragel.

It turned out that the underlying bug that caused the memory leak had
been present in our Ragel-based parser for many years but no memory was
leaked because of the way the internal NGINX buffers were used.
Introducing cf-html subtly changed the buffering which enabled the
leakage even though there were no problems in cf-html itself.

Once we knew that the bug was being caused by the activation of cf-html
(but before we knew why) we disabled the three features that caused it
to be used. Every feature Cloudflare ships has a corresponding [feature
flag](https://en.wikipedia.org/wiki/Feature_toggle), which we call a
'global kill'. We activated the Email Obfuscation global kill 47 minutes
after receiving details of the problem and the Automatic HTTPS Rewrites
global kill 3h05m later. The Email Obfuscation feature had been changed
on February 13 and was the primary cause of the leaked memory, thus
disabling it quickly stopped almost all memory leaks.

Within a few seconds, those features were disabled worldwide. We
confirmed we were not seeing memory leakage via test URIs and had Google
double check that they saw the same thing.

We then discovered that a third feature, Server-Side Excludes, was also
vulnerable and did not have a global kill switch (it was so old it
preceded the implementation of global kills). We implemented a global
kill for Server-Side Excludes and deployed a patch to our fleet
worldwide. From realizing Server-Side Excludes were a problem to
deploying a patch took roughly three hours. However, Server-Side
Excludes are rarely used and only activated for malicious IP addresses.

## Root cause of the bug 

The Ragel code is converted into generated C code which is then
compiled. The C code uses, in the classic C manner, pointers to the HTML
document being parsed, and Ragel itself gives the user a lot of control
of the movement of those pointers. The underlying bug occurs because of
a pointer error.

    /* generated code */
    if ( ++p == pe )
        goto _test_eof;

The root cause of the bug was that reaching the end of a buffer was
checked using the equality operator and a pointer was able to step past
the end of the buffer. This is known as a buffer overrun. Had the check
been done using \>= instead of == jumping over the buffer end would have
been caught. The equality check is generated automatically by Ragel and
was not part of the code that we wrote. This indicated that we were not
using Ragel correctly.

The Ragel code we wrote contained a bug that caused the pointer to jump
over the end of the buffer and past the ability of an equality check to
spot the buffer overrun.

Here's a piece of Ragel code used to consume an attribute in an HTML
`<script>` tag. The first line says that it should attempt to find zero
or more `unquoted_attr_char` followed by (that's the :\>\> concatenation
operator) whitespace, forward slash or then \> signifying the end of the
tag.

    script_consume_attr := ((unquoted_attr_char)* :>> (space|'/'|'>'))
    >{ ddctx("script consume_attr"); }
    @{ fhold; fgoto script_tag_parse; }
    $lerr{ dd("script consume_attr failed");
           fgoto script_consume_attr; };

If an attribute is well-formed, then the Ragel parser moves to the code
inside the `@{ }` block. If the attribute fails to parse (which is the
start of the bug we are discussing today) then the `$lerr{ }` block is
used.

For example, in certain circumstances (detailed below) if the web page
*ended* with a broken HTML tag like this:

    <script type=

the `$lerr{ }` block would get used and the buffer would be overrun. In
this case the `$lerr` does `dd(“script consume_attr failed”);` (that's a
debug logging statement that is a nop in production) and then does
`fgoto script_consume_attr;` (the state transitions to
`script_consume_attr` to parse the next attribute).\
From our statistics it appears that such broken tags at the end of the
HTML occur on about 0.06% of websites.

If you have a keen eye you may have noticed that the `@{ }` transition
also did a `fgoto` but right before it did `fhold` and the `$lerr{ }`
block did not. It's the missing `fhold` that resulted in the memory
leakage.

Internally, the generated C code has a pointer named `p` that is
pointing to the character being examined in the HTML document. `fhold`
is equivalent to `p--` and is essential because when the error condition
occurs `p` will be pointing to the character that caused the
`script_consume_attr` to fail.

And it's doubly important because if this error condition occurs at the
end of the buffer containing the HTML document then `p` will be after
the end of the document (`p` will be `pe + 1` internally) and a
subsequent check that the end of the buffer has been reached will fail
and `p` will run outside the buffer.

Adding an `fhold` to the error handler fixes the problem.

## Why now 

That explains how the pointer could run past the end of the buffer, but
not why the problem suddenly manifested itself. After all, this code had
been in production and stable for years.

Returning to the `script_consume_attr` definition above:

    script_consume_attr := ((unquoted_attr_char)* :>> (space|'/'|'>'))
    >{ ddctx("script consume_attr"); }
    @{ fhold; fgoto script_tag_parse; }
    $lerr{ dd("script consume_attr failed");
           fgoto script_consume_attr; };

What happens when the parser runs out of characters to parse while
consuming an attribute differs whether the buffer currently being parsed
is the last buffer or not. If it's not the last buffer, then there's no
need to use `$lerr` as the parser doesn't know whether an error has
occurred or not as the rest of the attribute may be in the next buffer.

But if this is the last buffer, then the `$lerr` is executed. Here's how
the code ends up skipping over the end-of-file and running through
memory.

The entry point to the parsing function is `ngx_http_email_parse_email`
(the name is historical, it does much more than email parsing).

    ngx_int_t ngx_http_email_parse_email(ngx_http_request_t *r, ngx_http_email_ctx_t *ctx) {
        u_char  *p = ctx->pos;
        u_char  *pe = ctx->buf->last;
        u_char  *eof = ctx->buf->last_buf ? pe : NULL;

You can see that `p` points to the first character in the buffer, `pe`
to the character after the end of the buffer and `eof` is set to `pe` if
this is the last buffer in the chain (indicated by the `last_buf`
boolean), otherwise it is NULL.

