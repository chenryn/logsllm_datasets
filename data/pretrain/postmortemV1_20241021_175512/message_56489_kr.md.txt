**[]{#ko_KR}2019년 8월 28일 업데이트:**

초기 요약 글에서 설명된 바와 같이, 이번 이벤트로 도쿄 리전 내 단일 가용
영역(AZ: Availability Zone)의 작은 부분이 영향을 받았습니다. 이는 해당
가용 영역의 Amazon EC2와 Amazon EBS 리소스에 영향을 미쳤습니다. 다만,
기저 EC2 인스턴스가 영향을 받은 경우, 해당 가용 영역의 다른 서비스(예:
RDS, Redshift, ElastiCache, Workspaces)에 다소 영향이 있었을 것입니다.
고객과 함께 이번 이벤트를 자세히 조사한 결과, 여러 가용 영역에서
실행되는 고객의 애플리케이션이 예상치 못한 영향을 받은 몇 가지 드문
사례(예: AWS 웹 애플리케이션 방화벽 또는 고정 세션과 함께 애플리케이션
로드 밸런서를 사용하는 일부 고객에서 예상보다 높은 비율로 내부 서버
에러가 발생)를 발견했습니다. AWS는 이러한 예외적인 문제에 대한 추가 세부
사항을 영향을 받은 고객과 직접 공유하고 있습니다.

## 도쿄(AP-NORTHEAST-1) 리전에서 발생한 아마존 EC2 및 아마존 EBS 서비스 이벤트 요약

2019년 8월 23일 도쿄(AP-NORTHEAST-1) 리전에서 발생한 서비스 장애와
관련해 추가 정보를 전달드립니다. 일본표준시간 오후 12시 36분부터
도쿄(AP-NORTHEAST-1) 리전 내 한 가용영역(AZ: Availability Zone)의 EC2
서버 중 일부가 과열로 인해 가동중단 되었습니다. 그 결과 EC2 인스턴스가
손상되고, 해당 가용영역의 영향을 받는 일부 리소스의 EBS 볼륨 성능이
저하되었습니다. 이번 과열은 제어 시스템 장애로 발생한 것으로 이로 인해
문제가 발생한 가용영역 일부에서 다수의 중복 냉각 시스템이 오작동을
일으키게 되었습니다. 해당 냉각 시스템은 일본표준시간 기준 오후 3시
21분에 복구되었으며, 해당 영역의 온도도 정상을 회복하기 시작했습니다.
온도가 정상화되면서, 영향을 받은 인스턴스에 전력 공급도 복구되었습니다.
일본표준시간 기준 오후 6시 30분까지 영향을 받은 인스턴스와 볼륨 대다수가
복구되었습니다. 전력 중단과 과열로 인해 영향을 받은 하드웨어에
호스팅되어 있던 인스턴스와 볼륨은 소수였습니다. 해당 인스턴스와 볼륨을
복구하는데 보다 오랜 시간이 걸렸으며, 일부는 기저 하드웨어의 장애로 인해
사용을 중단해야만 했습니다.

해당 인스턴스와 EBS 볼륨에 영향을 미친 것 외에도, EC2 RunInstances
API에도 일부 영향이 있었습니다. 일본표준시 오후 1시 21분, 영향을 받은
가용영역 내 새로운 EC2 인스턴스 개시 시도와 리전의 RunInstances API로
"idempotency token"(고객이 여러 개의 인스턴스가 시작되는 위험 부담 없이
run instance 명령어를 재시도하게 해주는 기능)을 사용하려는 시도가
오류율을 보이기 시작했습니다. "idempotency token"을 포함하지 않은 다른
EC2 API와 개시는 정상적으로 작동했습니다. 이 이슈는 "idempotency
token"에 의존하는 오토 스케일링(Auto Scaling)의 새로운 시작을
방해했습니다. 일본표준시 기준 오후 2시 51분 엔지니어들은 "idempotency
token"과 오토 스케일링를 해결했습니다. 영향을 받은 가용영역 내 새로운
EC2 인스턴스 시작은 오후 4시 5분 해당 가용영역에서 EC2 컨트롤 플레인
서브시스템을 복구할 때까지 장애를 일으켰습니다. 영향을 받은 EBS 볼륨에
새로운 스냅샷을 생성하고자 한 시도 역시 해당 이벤트 동안 오류율이
증가했습니다.

이번 이벤트는 데이터센터에서 사용하는 다양한 냉각 시스템을 제어하고
최적화에 사용하는 데이터센터 컨트롤 시스템의 장애로 발생하게 되었습니다.
해당 컨트롤 시스템은 고가용성 확보를 위해 다수의 호스트 상에서
운영됩니다. 그리고, 컨트롤 시스템은 팬, 냉각장치, 온도 센서 등 제3자
장치와 커뮤니케이션을 할 수 있도록 제3자 코드를 가지고 있습니다. 컨트롤
시스템은 직접적으로 또는 실제 장치와 커뮤니케이션을 하는 임베디드
PLC(Programmable Logic Controller)를 통해 커뮤니케이션 합니다. 이번
이벤트가 발생하기 바로 전, 해당 데이터센터 컨트롤 시스템은 컨트롤 호스트
중 하나에서 장애를 일으키고 있었습니다. 이러한 종류의
페일오버(failover)가 발생하게 되면, 해당 컨트롤 시스템은 새로운 컨트롤
호스트가 데이터센터 상태에 대한 가장 최신 정보를 갖도록 다른 컨트롤
시스템 및 컨트롤하는 데이터센터 장비(예: 데이터센터 전반에 설치된 냉각
장비와 온도 센서)와 정보를 교환해야만 합니다. 제3자 컨트롤 시스템 로직
내 버그로 인해 이러한 정보 교환은 데이터센터의 컨트롤 시스템과 장치 간
과도한 인터랙션이 발생하게 되었고, 이로 인해 컨트롤 시스템이 무응답
상태가 되었습니다. AWS의 데이터센터는 데이터센터 컨트롤 시스템이 장애를
일으키게 되면, 컨트롤 시스템 기능이 복구될 때까지 냉각 시스템이 최대
냉각 모드로 작동하도록 설계되어 있습니다. 이러한 설계가 대부분의
데이터센터에서는 제대로 작동했지만, 일부에서 냉각 시스템이 안전한 냉각
설정으로 전환하지 못하고 가동이 중단되었습니다. 추가 안전장치로,
데이터센터 운영자들은 오작동 상황에서 데이터센터 컨트롤 시스템을
우회해서 냉각 시스템을 "퍼지(purge)" 모드에 두고 재빨리 뜨거운 공기를
빼낼 수 있습니다. 운영팀에서는 문제가 발생한 데이터센터 구역에서 퍼지
기능 활성화를 시도했지만 실패했습니다. 이 때 해당 부분의 데이터센터
온도가 오르기 시작했고, 지나치게 과열되면서 서버 전원 공급이
차단되었습니다. 데이터센터 컨트롤 시스템을 사용할 수 없었기 때문에,
운영팀에서는 데이터센터 냉각 시스템 상태에 대해 최소한의 가시성만 확보할
수 있었습니다. 복구를 위해 운영팀에서는 수작업으로 문제가 발생한 장비
전체를 조사하고 재설정해 최대 냉각 설정 상태로 놓아야 했습니다. 이
과정에서 일부 공기조화장치를 제어하는 PLC 역시 무응답 상태라는 것을 알게
되었습니다. 그래서 PLC 역시 재설정이 필요했습니다. 이 PLC 컨트롤러가
장애를 일으켜 디폴트 냉각 및 "퍼지" 모드가 작동을 하지 못했습니다. 해당
컨트롤러를 재설정한 뒤에는 데이터센터 이벤트 발생 구역의 냉각이
복구되었으며, 온도가 떨어지기 시작했습니다.

AWS는 아직 제3자 벤더들과 함께 컨트롤 시스템과 해당 PLC의 무응답을
일으킨 버그와 추가 인터랙션을 파악하는 중에 있습니다. 임시 방편으로 해당
이벤트가 재발하는 것을 방지하고자 컨트롤 시스템의 버그를 일으킨 페일오버
모드를 비활성화 상태로 만들었습니다. AWS는 만약 재발하게 될 경우 상황을
재빨리 파악하고, 해결할 수 있도록 현지 운영팀도 교육했습니다. 어떤
사유로 비슷한 상황이 발생하게 되면 고객 영향이 발생하기 전에 시스템을
재설정할 수 있을 것으로 확신하고 있습니다. 마지막으로, "퍼지 모드"가 PLC
컨트롤러를 완전히 우회할 수 있도록 문제를 일으킨 공기조화장치 제어
방식을 변경 작업을 하고 있습니다. 이 방식은 최신 데이터센터 설계에
적용하기 시작한 것으로, PLC 무응답 상태에도 "퍼지 모드"가 보다 확실히
작동하도록 할 것입니다.

해당 이벤트 동안 리전의 다른 가용영역에 있는 EC2 인스턴스와 EBS 볼륨은
영향을 받지 않았습니다. 여러 가용영역에서 애플리케이션을 실행한 고객은
이벤트 기간 동안 가용성을 유지할 수 있었습니다. 애플리케이션에 가장 높은
가용성이 필요한 고객에게는 다중 가용영역 아키텍처로 애플리케이션을
실행할 것을 권합니다. 고객에게 가용성 문제를 일으킬 수 있는 모든
애플리케이션 구성 요소는 이러한 내구성이 높은(fault tolerant) 방식으로
실행되어야 합니다.

 

