Rules of Availability – Part 1
March2008
There are many ways in use today to achieve high availabilities. Predominant among these
techniques are lockstepped processors, checkpointed or persistent processes, clusters, and
active/activesystems.All usesomeform of redundancytorecover quicklyfrom faults,andall are
subjecttoacommonsetofprinciples.
Manyof theseprinciples aresetforthinthebook series entitledBreakingthe Availability Barrier,1
which I coauthored. The principles are presented as sixty-four “Rules of Availability.” The rules
focus on active/active architectures, but many of the rules are applicable in a broader sense. In
this article, which is the first part in a series on availability principles, we review some of these
rules.
General Availability
Rule2:Providingabackupdoublesthe9s.
That’s right. If you have a system with three 9s availability(sayan industry-standard server), and
if you add a backup system to it, the resulting two-node configuration will have six 9s availability.
This, of course, assumes that the failover time to the backup is instantaneous. Otherwise, the
failovertimemustbetakenintoaccountandcansignificantlyimpactsystemavailability.
Instantaneousfailovertimes(i.e.,unavailabilitytimesthataretransparenttotheuser)arevirtually
achieved with lockstep processors (as used by NonStop and Stratus) and by checkpointed
processes (as used for critical processes by NonStop). Very short failover times, measured in
subseconds, can be achieved with persistent processes (i.e., a backup process started by a
checkpointedmonitorprocess)orwithactive/activesystems.Clustersrequireminutestofailover,
reducingtheiravailabilitytypicallytolessthanfive9s.
Active/standby system configurations can take hours to fail over. For instance, a typical NonStop
system backed up by another system will have an availability of a little more than four 9s if
failuresoccuronceeveryfiveyearsandiffailovertakesfourhours.
Rule3:Systemreliabilityisinverselyproportionaltothenumberoffailuremodes.
This rule simply states the obvious - the more ways that a system can fail, the less reliable it will
be.Butitleadstoanotsoobviousconclusion.
1BreakingtheAvailabilityBarrier:SurvivableSystemsforEnterpriseComputing,AuthorHouse;
2004.
BreakingtheAvailabilityBarrierII:AchievingCenturyUptimeswithActive/ActiveSystems,
AuthorHouse;2007.
BreakingtheAvailabilityBarrierIII:Active/ActiveSystemsinPractice,AuthorHouse;2007.
1
©2008SombersAssociates,Inc.,andW.H.Highleyman

Considerasystem withnprocessorsrunningcheckpointedprocesspairs.Eachprocessrunsina
processor and is backed up by a backup process running in another processor. If a process or
the processor in which it is running fails, the backup process takes over. This is the NonStop
model.
If the pair of processors running the active process and its backup fail, the process fails and the
system is down (assuming that the process is a critical process, such as a disk process). This
particularfailureiscalledafailuremodeofthesystem.
In the general case, critical processes will be distributed across all processors so that the failure
of any pair of processors will bring down the system. If there are only two processors in the
system, the number of failure modes is one (i.e., there is only one way that two processors can
fail). If there are three processors, there are three failure modes (processors 0 and 1, processors
0 and 2, and processors 1 and 2). In general, if there are n processors in the system, there are
n(n-1)/2 failure modes. If a pair of processors will fail once every four years, a two-processor
system will fail once every four years. A three-processor system will fail three times every four
years.Afour-processorsystemwillfailsixtimeseveryfouryears.
Thisleadstoanotsoobviousbutveryimportantcorollary.Believeitornot:
CorollarytoRule3:Addingprocessorstoasystemmakesitlessreliable.
This corollary assumes that the level of sparing remains the same. In the above examples, we
have assumed a single spare; that is, the system can withstand any single failure. If the addition
of new processors adds to the sparing (i.e., now the system can withstand two processor
failures),thecorollaryisnottrue.SeeRule6below.
Rule4:Organizeprocessorsintopairs,andallocateeachprocesspaironlytoaprocessorpair.
This rule is a result of Rule 3. Consider a sixteen-processor NonStop system that is singly-
spared. It will have 120 failure modes (16x15/2) if critical process pairs are distributed randomly
among all processors (this is actually quite common as systems managers often allocate critical
processes based on load and not availability). However, if the system is divided into eight two-
processorpairs,andifeachcriticalprocess isrestrictedtorunonlyinoneofthesepairs,thenthe
system has onlyeightfailuremodes (i.e.,oneof thepairs).Wehavejustmadethesystem fifteen
timesmorereliablewiththissimplereorganization.
Rule 6: System availability increases dramatically with increased sparing. Each level of sparing
addsasubsystem’sworthof9stotheoverallsystemavailability.
This is an extension to Rule 2. Consider again an industry-standard server with three 9s
availability. Adding one spare to it (say in an active/backup configuration) increases system
availability to six 9s. Adding a second spare (i.e., an active system with two backups) results in a
system with nine 9s of availability since now all three systems must simultaneously fail to bring
downtheapplication.Athirdbackupwillincreasesystem availabilitytotwelve9s.This,ofcourse,
assumesinstantaneousfailover.
Rule 6 represents the exception condition to the corollary to Rule 3, given above. If adding
processors to a system is done in such a waythat the extra processors are used in part for extra
spares,thenaddingprocessorstoasystemwilldramaticallyimproveitsavailability.
2
©2008SombersAssociates,Inc.,andW.H.Highleyman

Rule7:Forasinglesparesystem,thesystemMTRisone-halfthesubsystemmtr.
This rule assumes parallel repair. That is, if a system goes down because two subsystems have
failed, then each subsystem is being repaired independently by different repair teams. If the
subsystem meantimetorepair is mtr, arepair team canrepair subsystems atarateof 1/mtr.For
instance, if the average repair time is half of an eight-hour working day, a repair team can repair
two subsystems per working day, or one every four hours. Two repair teams can repair four
systems per working day, or one every two hours. Thus, in a singly spared system with a dual
subsystem failure, it will take two hours for the first subsystem to be put back on line and user
service restored. The mean time to repair the system, MTR, is only two hours, or half the
subsystemmtr.
This rule is easily extendable to systems with higher levels of sparing. For instance, if a system
hastwospares,itwillgodownonlyifthreesubsystemsarelost.Inthiscase,thesystemMTRwill
beone-thirdof thesubsystem mtr sincethere willbe threerepair teams workingindependentlyto
getthesystemoperational.
Rule 8: For the case of a single spare, cutting subsystem mtr by a factor of k will reduce system
MTRbyafactorofkandincreasesystemMTBFbyafactorofk,thusincreasingsystemreliability
byafactorofk2.
This is an extremely important rule. It expresses the importance of reducing repair time to
increase system availability. For instance, if a singly-spared system has an overall mean time to
repair, MTR, of four hours and a mean time between failures, MTBF, of 4,000 hours, it will be
down 4/4,000 = 0.1% of the time (an availability of three 9s). This is based on some particular
subsystem repair time. If the subsystem repair time can be cut by a factor of two, the system
MTR willbereducedtotwo hours and its MTBF increasedto8,000hours,resultinginthesystem
being down 2/8,000 = .025% of the time. The system down time has been reduced bya factor of
four.
Subsystem repair time can be reduced by several means. For instance, critical spare parts can
be located on site. Diagnostic facilities can be improved. Service contracts can be upgraded to
reduceresponsetime.Systemswithcustomerreplaceableunits(CRUs)canbeused.
Rule 9: If a system is split into k parts, the resulting system network will be more than k times as
reliableastheoriginalsystemandstillwilldeliver(k-1)/kofthesystemcapacityintheeventofan
outage.
This rule is a direct result of Rule 3, which says that failure modes should be reduced, and its
corollary,whichstatesthatthebiggerthesystem,thelessreliableitis.
For instance, consider a sixteen-processor NonStop system with randomly allocated critical
processes. Such a system has 120 failure modes. Now break that system into four four-node
systems in which each node is actively processing transactions. Each node has only six failure
modes,andthefailureofanyonenodeisassumedtotakedownthesystem.Therefore,thereare
a total of 4x6 = 24 failure modes in this split system, or one-fifth of the failure modes of the
monolithic system. The reliability of the system has been increased by a factor of five by splitting
itintofourparts.
Furthermore,if themonolithic system fails,allcapacityis lost;and allusers are affected. If oneof
thefournodesfails,only25%ofthecapacityislost;andonly25%oftheusersareaffected.
Thisisoneofthecornerstonesofactive/activesystems.Forinstance,assumethatfivenodesare
provided and give a total capacity of 125%. Then, even in the event of a node failure, there will
still be 100% of the needed capacity available. It will take the failure of two nodes – an unlikely
event – to reduce the capacity below 100%, but 75% of capacity will still be available. This is in
3
©2008SombersAssociates,Inc.,andW.H.Highleyman

stark contrast to amonolithic system in which the failure of one node (the system itself) results in
thelossofallcapacity.ThisleadsdirectlytoRule10.
Rule 10: If a system is split into k parts, the chance of losing more than 1/k of its capacity is
many,manytimeslessthanthechancethatthesinglesystemwillloseallofitscapacity.
Replication
Rule11:Minimizedatareplicationlatencytominimizedatalossfollowinganodefailure.
Themostcommonwaytodaytomaintaindatabasecopiesinsynchronismacrossanactive/active
network is to use asynchronous replication. With asynchronous replication, there is a delay from
the time that an update is made to the source database to the time that it is applied to the target
database.Thistimedelayisknownasreplicationlatency.
At the time of a failure of an active/active node, any of that node’s updates that are still in the
replication pipeline may not make it to the target database. They will be lost. This data loss can
be minimized bychoosing a data replication engine that has minimum replication latency(that is,
itisveryfast).
Rule 12: Database changes generally must be applied to the target database in natural flow
ordertopreventdatabasecorruption.
This rulestates theobvious - theupdates toatargetdatabasemustbeappliedin thesameorder
as they were to the source database. Otherwise, the target database could end up in a different
statethanthesourcedatabaseandresultinthecorruptionofdatabasecopies.Thesimplestcase
is thatof twotransactions updating thesamedata item.If transaction 1occurs beforetransaction
2, the source database will be left with the results of transaction 2. However, if transaction 1
arrivesatthetargetdatabaseaftertransaction2,thetargetdatabasewillbeleftwiththeresultsof
transaction1;andthedatabaseshavediverged.
This is a particular problem in high-performance data replication engines that use multiple
threads. There is no guarantee that the combined output of the threads at the target system will
beinthesameorderasatthesourcesystem.Therefore,thedatareplicationenginemustprovide
a mechanism for proper reordering of updates or transactions at the target system in a
multithreadedenvironment.
Rule15:Minimizereplicationlatencytominimizedatacollisions.
Rule 15 adds importance to the dictum of Rule 11 to minimize replication latency. If two users at
two different active/active nodes update the same data item nearly simultaneously (within the
replication latency), both updates will be made at the local nodes and will be replicated to the
other node. There, each update will be overwritten by the update from its remote node. The
databasesarenowdifferent,andeachiswrong.Thisiscalledadatacollision.
Data collisions are perhaps the most vexing problem in active/active systems. They must either
be avoided, or they must be detected and resolved. The shorter the replication latency time
interval,thelesslikelythattherewillbedatacollisions.
Rule 16: (Gray’s Law) – Waits under synchronous replication become data collisions under
asynchronousreplication.
If synchronous replication is used, locks on all copies of a data item to be modified are acquired
across the network before any are modified. Thus, either all data items will be modified at the
same time, or none are. Synchronous replication solves the problems of data loss and data
collisionsthatareinherentwithasynchronousreplication.
4
©2008SombersAssociates,Inc.,andW.H.Highleyman

With synchronous replication, if two users at two different nodes want to update a data item at
nearly the same time, one user will get the lock on that data item; and the other user will have to
wait until the first update has been made before he can obtain the lock andmodifythe data item.
Thus,withsynchronousreplication,datacollisionsbecomedatawaitsinstead.
Rule 17: For synchronous replication, coordinated commits using data replication become more
efficient relative to network transactions and replicated lock management under a transaction
manager as transactions become larger, as communication channel propagation time increases,
orasthetransactionloadincreases.
There are several ways in which synchronous replication can be implemented. One way is to
extend the scope of the transaction to include all of the copies of the data items affected by the
transaction. Another is to have a distributed lock-management facility that will obtain locks on all
of the copies of a data item across the network. With these techniques, a round-trip
communication time (communication latency) is required to make each update. Communication
latencies can be tens of milliseconds, and large transactions over long distances can be
substantially slowed by the use of these techniques. In addition, a high rate of very short
messagesisimposedonthenetwork.Thiscouldrepresentamajornetworkload.
Coordinated commits, on the other hand, replicate updates asynchronously, transparently to the
application. Updates are sent in large blocks and use the network efficiently. Only at the end of a
transaction is the application paused while the synchronous replication engine coordinates with
the other nodes to decide whether or not to commit the transaction. This commit time requires a
singlereplicationlatency.
For collocated active/active nodes and small transactions, the delay due to multiple round-trip
communication hops may be less than a replication latency time interval; and network
transactions or distributed lock managers may be more efficient. However, for nodes that are
geographically distributed, or for transactions that are large, coordinated commits can be
significantly more efficient. If transaction rates are high, the multiplicity of the many small
messages required by network transactions or distributed lock managers may preclude their use
evenifsystemsarecloselylocated.
What’s Next
Inournextarticle,wewillcontinuewithmoreofRulesofAvailability.
5
©2008SombersAssociates,Inc.,andW.H.Highleyman
