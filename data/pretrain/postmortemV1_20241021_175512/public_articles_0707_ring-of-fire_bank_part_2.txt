Fordiscussion,contacteditorest.com

hours or days of downtime. Disaster tolerance means that recovery is so fast that no one notices the
outageoratleastisnotinconveniencedbyit.Inessence,theapplicationisavailableduringthefailure.
Thebank was notanxious toimmediatelyandtotallyreconfigureits systems intoacomplex,continuously
available architecture. Rather, it wanted to take small, controlled steps towards improved availability with
theeventualgoalofcontinuousavailability.
Implementing a disaster-tolerant architecture can be a daunting task, as there are many considerations
beyond those needed for a disaster-recovery active/passive architecture. This task can be accomplished
via a controlled process that can achieve incremental improvements. At the same time, experience and
trust in the process and products used can build as the application’s overall availability profile is
improved. The key, then, is to understand the end-state goals and to define an incremental process to
achievethosegoals.
The Availability Improvement Process
Thebank’savailabilityimprovementprocessproceededasfollows:
Step1:DefineRequirements
The bank began byreviewing its options for a new replication product. It decided that the only way it was
going to eliminate the failover problem was to have a backup system that was ready to take over
instantly.Therefore,thereplicationenginethatitneededhadtopossessthefollowingcharacteristics:
 Backup applications had to be up and running with the databases mounted, ready to take
over processing on an instant’s notice, meaning that the replication engine had to allow
applicationstoopenthedatabaseinread/writemodeevenasreplicationwasinprogress.
 The target database always had to be in a consistent state during replication so that it could
beusedimmediatelyfollowingafailover.
 The systems had to be decoupled so that they did not have to be configured identically, thus
eliminatingfailoverfaultsduetoconfigurationerrors.
 The delay in replicating database changes (the replication latency) had to be small to
minimizedatalossfollowingasource-systemfailure.
 The replication product had to support bidirectional replication so that reverse replication
could be configured for the backup system. In this way, after a failover, the backup system
would queue changes that it made to the database while the production system was down.
The change queue would be used to resynchronize the failed system upon its return to
service. In addition, upon system restoration, a ‘new’ backup system would immediately be
availablewithoutreconfiguration.
Step2:ChooseaDataReplicationEngine
With its specifications in hand, the bank next evaluated the various replication alternatives that were
available for NonStop systems. It chose the Shadowbase replication engine from Gravic, Inc.
(www.gravic.com/shadowbase)astheonethatbestsatisfieditsrequirements.
Shadowbase software supports bidirectional replication. Applications can be actively running on both
systems and can be simultaneously updating the application database. Replication is process-to-process
with no intermediate disk storage, leading to small replication-latency times. Shadowbase solutions can
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

replicate between heterogeneous systems, so maintaining identical system configurations is not a
requirement.
Step3:SwitchReplicationEngines
Before taking any further steps, the bank wanted to make sure that it was comfortable with its new data
replication engine. To start the process, it replaced its original replication engine with the new engine
doing the same job. The replication engine was installed in unidirectional mode to keep the backup
databasesynchronizedwiththeproductiondatabaseinanactive/passiveconfiguration.
Asofthis writing,thebank issuccessfullyrunninginactive/passivemodewiththenewreplicationengine.
It is gaining experience with the replication engine, is confident in the engine’s handling of fault and error
conditions,andhaslearnedtotuneitproperly.
Step4:TestBidirectionalReplication
Thebank isnowpreparingforitsnextstep,andthatis extendingtobidirectionalreplication.Initially,there
will be no applications running on the backup system. However, failover testing will now be simpler: start
thetargetapplications,switchthenetwork,andtestthebackupsystem.
Equally important, since replication will be bidirectional, any changes made by the backup system during
the test will optionally be replicated back to the production system. This change will make the fallback
process much easier since the paused production system will now have an up-to-date database. There
willbenoneedforaspecialsteptobringtheproductionsystem intosynchronizationbeforereturningitto
service.
In addition, the applications on the production system will not have to be shut down during the test since
these applications will continue to have the database mounted. When the backup test is complete, the
network will be switched back to the production system so that it will receive further transactions to
process.
Step5:ConfiguretheFastFailoverSystem
Onceit has becomecomfortable with bidirectional replication, the bank will be in aposition toreconfigure
its existingsystems toprovidefastfailover.Withbidirectionalreplicationfunctionalonbothsystems,itwill
be able to put both systems into operation with all applications up and running. Both systems will be fully
functional, with transactions sent to either system for proper processing. However, the bank will direct all
transactionactivitytoonlyonesystem;theothersystemwillserveasasizzling-hotstandby.
Since the application is already running on the standby node, the standby system can be frequently
tested by simply sending it verification (or test) transactions. This testing is risk-free and will be totally
transparent to the users. It can be performed at any time of the day or night with no source-application
outage needed. Therefore, in the event that the standby system should be needed, it will be known to be
operating properly and will take over with no failover faults. In fact, should the active system fail, all that
will be necessary will be to reroute further transactions to the standby system, which will immediately
continue transaction processing. This failover will require only network rerouting, which can be
accomplishedinseconds.
With only the acquisition of a proper replication engine and some system reconfiguration, the bank will
have moved from multi-hour unreliable failover to multi-second reliable failover. It will have achieved its
goalofcontinuousavailabilitywithnochangeinitshardwareconfiguration.
5
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Step6:MovetoActive/Active(afutureoption)
At this point, the bank will have achieved its availability goal by following a very structured and
straightforward process; and it will be in a position to take this process one step further if it wishes. It can
put both of its systems into active production, called an active/active system. Since both systems can
process transactions,the transaction workload canbe split between the twosystems.Shouldonesystem
fail,itwillonlybenecessarytorerouteallfurthertransactionstothesurvivingsystem.
When using an active/active configuration, the capacity of both systems is utilized. Though each must be
configured to handle the entire transaction load in case one system fails, performance is improved during
normal operation since each system is less heavily loaded. Production load spikes are much more easily
toleratedacrosstwonodesratherthanjustone.
The bank realizes that going active/active is not as simple as moving to a sizzling-hot standby. Several
problems must be faced when running an application in a distributed environment. For instance, the
applications may need to be modified to correctly handle memory-resident data structures that are not
replicated. Applications cannot open files for exclusive access since the replication engine cannot then
alsoopenthemtoapplyreplicatedchanges.
Another problem is data collisions. A data collision occurs if applications at two nodes modify the same
data object within the replication latency of the replication engine. Neither will know of the other’s update
until that update is replicated, thus overwriting the original update. Shadowbase replication provides
facilitiesfordetectingandresolvingdatacollisions.
Fortunately for this bank, its third party banking application software provider is already running
Shadowbase active/active implementations at other sites. The eventual move to anactive/active
architectureinordertoattaincontinuousavailabilityisthereforewellwithinthebank’sreach.
By following a controlled process, the bank can take its time to study active/active configurations and to
decidewhetherthecomplexityofthismoveis worthit.Itcanmigrateapplications toactive/activeoneata
time if it desires. This approach was implemented by many other banks, including such North American
banks as Wells Fargo, U.S. Bank, Royal Bank of Canada, and Fifth Third Bank, as well as banks
worldwide,includingLloyd’s,Bank-Verlag,Swedbank,andBankofChile.
Summary
The success that this bank has achieved in moving in a controlled fashion towards continuous availability
teaches an important lesson – do not give up if you think moving to higher availabilityarchitectures is too
hard. Each architectural step, from magnetic-tape backup to virtual-tape backup to active/passive to
sizzling-hot standby to active/active, moves you closer to continuous availability. The migration is a
processthatcanbemanagedandcontrolledtoensuresuccessonyourschedule.
After all, if you already are running a backup site, you already have accepted the cost of redundancy,
which is thefirstrequirementfor improvedavailability. Why,then, acceptoutages measuredinhours with
the possibility of catastrophic failover faults when you can have continuous availability for the cost of a
datareplicationengineandsomereconfiguration?Thisiswhatactive/activearchitecturesprovide.
6
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
