
## 12 

[02/12/2022]

Virtual Machines - West US - Resolved

Tracking ID: ZS1T-LCG


**Impact Statement**: Between 04:38 UTC and 6:30 UTC on 12 Feb 2022, you
were identified as a customer using Virtual Machines, Azure SQL and
Storage in West US who may have experienced connection failures when
trying to access some resources hosted in the region. Additional
downstream services may have also been impacted.

**Preliminary Root Cause**: We determined that a subset of storage
resources experienced a drop in network connectivity.

**Mitigation**: We restored the network connectivity to storage
resources to mitigate the issue.

**Next steps: **[We will continue to investigate to establish the full
root cause and prevent future
occurrences.]{style="color: rgb(50, 50, 55)"}

## 2 

[02/02/2022]

RCA - Azure AD - Service Management Failures

Tracking ID: SMWW-BDZ


**Summary of Impact**: Between 19:50 UTC and 22:06 UTC on Feb 2, 2022,
customers using Azure Active Directory (Azure AD) may have experienced
failures when performing any service management operations.

During this incident, the Azure AD REST API service experienced some
availability impact globally. However, the overall availability was at
99.996% through the duration of the incident. This means that retries
had a high probability of success and customer impact would have been
minimal or unnoticed.

After investigation, it was determined that there was no impact to Azure
AD B2C, as previously reported.

Even though customer impact may have been minimal or unnoticed, the
global nature of the incident led us to attend to this issue at a high
severity and overcommunicate to ensure the potentially broad impact was
notified.

**Root Cause**: As part of planned maintenance, a change to a dependency
was rolled out which affected the availability of the Azure AD REST API
service. Resiliency measures initially delayed the impact by relying on
caches. When said measures were exhausted, the first request every 10
seconds resulted in a call to the dependency, causing a timeout and the
request to fail.

**Mitigation**: Engineers rolled back the change to the dependency,
mitigating the incident.

**Next Steps**: We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Improve monitoring of the dependency to help ensure that its
    availability is more accurately observed regardless of underlying
    resiliency measures, which would help prevent customer impact much
    earlier.
-   The dependency on the service that caused the impact is being
    re-evaluated to determine if it can be removed from the Azure AD
    REST API.

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## January 2022

## 13 

[01/13/2022]

RCA - Azure Resource Manager - Issues with management and resource
operations

Tracking ID: 8V39-P9Z


**Summary of Impact:** Between 09:00 UTC on 13 Jan 2022 and 20:00 UTC on
14 Jan 2022, a subset of customers using Azure Resource Manager (ARM) to
deploy, modify, or remove Azure resources experienced delays, timeouts,
and failures which were visible for long running operations executed on
the platform. Impact was most severe for a period of 5 hours starting at
15:30 UTC on Jan 13 and another period of 8 hours starting at 00:00 UTC
on Jan 14, and in regions including but not limited to West US, West US
2, South Central US, North Europe, West Europe, East Asia and Southeast
Asia.

Impact to customers will have been broad, as numerous Azure services
rely on service management operations orchestrated by the ARM platform.
Most customers will have experienced delays and timeouts, but many
customers will have seen deployment or resource management failures.

**Root Cause:** A code modification which started rolling out on 6 Jan
2022 exposed a latent defect in the infrastructure used to process long
running operations (informally, \"jobs\"). The code modification
resulted in an exception for a tiny fraction of job executions, each one
of them disabled a small part of the job execution infrastructure. Over
the course of hours, the job executions shifted entirely away from the
regions that had received the new code to their backup paired regions.
For a period of 16 hours, there was no customer impact as the backup
paired regions executed the jobs as intended. The impact spread to
backup paired regions as the new code was deployed, resulting in job
queue up, latency delays, and timeouts. In some cases, the jobs executed
with such prolonged delays that they were unable to succeed, and
customers will have seen failures in these cases.

As a result of the way that the job execution infrastructure was
implemented, the compounding failures were not visible in our
telemetry - leading to engineer's mis-identifying the cause initially
and attempting mitigations which did not improve the underlying health
of the service. The consequence of this was a second period of impact
starting at 00:00 UTC on 14 Jan 2022 and extending for approximately 8
hours.

**Mitigation:** Identifying the source of the problems in this case took
time, as some parts of the job infrastructure remained healthy and
processing jobs, while other key parts were being disabled. At the time
we were unable to clearly identify the newly released code as
correlating with the impact we were seeing. When the nature of the
problem became clear we immediately started to roll back to a previous
build. This change rolled out progressively completed at 20:00 UTC on 14
Jan 2022.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Reviewing and improving our monitoring and alerting strategy for our
    job execution infrastructure to improve our ability to detect
    problems like this one before they become customer-impacting.
-   Fixing the underlying problem which allows a single rare exception
    to disable parts of the job execution infrastructure.
-   Providing better visibility for operators when a paired region has
    assumed responsibility for job execution, in order to indicate a
    reduced-redundancy state and signal the need to pause or roll back a
    deployment.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 13 

[01/13/2022]

Azure Data Factory V2 - West Europe - Mitigated

Tracking ID: PKJ8-TTZ


**Summary of Impact:** Between 10:10 UTC on 13 Jan 2022 and 20:00 UTC on
14 Jan 2022, a subset of customers may have experienced issues,
timeouts, or failures for some service management operations for
services leveraging Azure Resource Manager (ARM). This could have also
included issues with operations attempted to manage resources or
resource groups. This could have resulted in a downstream impact on
other Azure services that rely on Azure Resource Manager.

\

**Preliminary Root Cause:** We have identified a change to backend role
instances leveraged by Azure Resource Manager causing the timeouts and
is root cause of the failure. 

\

**Mitigation:** We mitigated background job execution systems causing
failures and performed a roll back of recent change following our safe
deployment practices (SDP) to return ARM to a previous healthy state,
mitigating the issue. The roll back took several hours to complete
globally following our SDP process. 

\

**Next Steps:** We will also continue to investigate to establish the
full root cause and prevent future occurrences. You can stay informed
about Azure service issues, maintenance events, or advisories by
creating custom service health alerts
([https://aka.ms/ash-videos](%3Ca%20href=){.wa-link-status <a=""}\"
class=\"wa-link-status\"\>[https://aka.ms/ash-videos](https://aka.ms/ash-videos)\"
rel=\"noopener noreferrer\"
target=\"\_blank\"\>[https://aka.ms/ash-videos](%3Ca%20href=){.wa-link-status
https:="" aka.ms="" ash-videos\"=""}\"
class=\"wa-link-status\"\>[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](%3Ca%20href=){.wa-link-status <a=""}\"
class=\"wa-link-status\"\>[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)\"
rel=\"noopener noreferrer\"
target=\"\_blank\"\>[https://aka.ms/ash-alerts](%3Ca%20href=){.wa-link-status
https:="" aka.ms="" ash-alerts\"=""}\"
class=\"wa-link-status\"\>[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation) and you will be notified via your preferred
communication channel(s).

\

## 4 

[01/04/2022]

Azure Cosmos DB - East US

Tracking ID: 9VT8-HPG


**Summary of Impact: ** Between 12:30 UTC on 04 Jan 2022 and 7:41 UTC on
5 Jan 2022, customers with Azure Cosmos DB accounts in East US may have
experienced connectivity and service availability errors while accessing
their Cosmos DB databases. One Cosmos DB cluster in the East US region
was unavailable during this time, so both new and existing connections
to databases in this subscription in this region may have resulted in
errors or timeouts.

**Root Cause:** Cosmos DB uses Azure Service Fabric as the underlying
platform for providing fault tolerance in the cluster. Service Fabric
uses the ring topology, and each node establishes a lease relationship
with nodes in its proximity (i.e. neighborhood) to detect failure. It
has a set of nodes that are responsible for determining cluster
memberships of other nodes, known as Arbitrators. A node that fails to
refresh lease within a timeout period will be reported by its neighbors,
and the arbitrators need to determine whether the node should leave the
cluster. This check is done in a timer callback.

During this incident, the timer callback on one of the nodes was fired
multiple times at a frequency higher than intended. This resulted in the
node\'s neighbors getting incorrectly reported as unavailable. By
design, the Arbitrators trusted this information as they did not receive
any healthy uptime notification within the stipulated time frame. This
continued until the quorum of nodes was lost, and the cluster went down
eventually. The cluster came back up once the culprit node was manually
rebooted as part of the mitigation efforts.

**Mitigation:** After the initial investigation, the cluster was marked
as offline at 14:08 UTC on 04 Jan 2022 which triggered regional failover
for accounts that had multiple regions and automatic failover enabled.
Customers that did not have automatic failover enabled continued to be
impacted until the cluster was recovered.

The cluster was recovered by rebooting the Service Fabric Infrastructure
nodes after removing the culprit node. However, recovery of the cluster
was delayed due to overload of the configuration store as the service
was restarting. Cosmos DB Engineers initially tried to reduce the load
on the configuration store by delaying the startup of about 20% of the
nodes. This approach did not fully resolve the problem. Engineers then
manually applied configuration changes to increase the timeout on the
requests used to fetch data from the configuration store. This change
allowed the recovery to continually make progress. Availability to the
cluster was incrementally restored as service back end processes started
running. Recovery was completed at 07:41 UTC on 05 Dec 2022.

\

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

Service Fabric team to develop a fix to improve resilience in case of
misfired timer(s) reporting incorrect node health status within Azure
Service Fabric.

Azure Cosmos DB to improve monitoring to better identify culprit nodes
early on if this failure pattern reoccurs.

Provide Feedback: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

