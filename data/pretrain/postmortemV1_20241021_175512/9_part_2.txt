    updates.
-   Improving rollout monitoring and correlation capabilities to detect
    such faults and halt the rollout sooner.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 4 

[07/04/2020]

Azure DevOps - Service Outage - Mitigated

Tracking ID: SNDL-NS8


**Summary of Impact**: Between 02:26 am and 03:40 am UTC on 04 Jul 2020,
customers using Azure DevOps in multiple regions may have observed
connectivity errors to DevOps services. 

**Preliminary Root Cause**: We identified an inadvertent error with a
configuration change in the back-end service which caused the outage. 

**Mitigation**: We applied a configuration update which has fully
mitigated the issue. 

**Next Steps**: We will continue investigations to establish the full
root cause and prevent future occurrences. Stay informed about Azure
service issues by creating custom service health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation.

## 1 

[07/01/2020]

RCA - Azure SQL Database - Japan East

Tracking ID: CLCK-LD0


**Summary of Impact:** Between 09:24 and 11:15 UTC on 01 Jul 2020, a
subset of customers using Azure SQL Database, Azure SQL Data
Warehouse/Synapse Analytics, Azure Database for MySQL, Azure Database
for PostgreSQL, and Azure Database for MariaDB in Japan East may have
experienced service connection failures or possible timeouts. Services
utilizing SQL Databases may have also been impacted.

**Root Cause:** Connections to Azure SQL Database and related data
services go through a load balanced set of front-end nodes (Gateways)
that provide directory lookup services and reroute the incoming
connections to the intended backend nodes hosting the database. For
scalability and zone redundancy purposes, there are multiple active SQL
Gateway clusters in a region. During this incident, one of the SQL
Gateway clusters became unhealthy, having an intermittent impact on
login availability. A specific network traffic pattern combined with a
networking stack configuration on the SQL Gateway instances triggered an
imbalance on the CPU processing of new connection requests. The
persistence of such CPU imbalance over a long period of time caused high
response latency and increased timeouts on connection requests. The
error condition propagated across multiple instances of the SQL Gateway
cluster in this region, sometimes causing a service restart.

**Mitigation:** Multiple SQL Gateway instances became healthy upon the
triggered service restart. On further investigation, we were able to
isolate the specific network pattern and the configuration setting that
caused this incident and were able to reconfigure the traffic to prevent
a recurrence. 

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Fix the underlying issue that causes service restart when such a
    condition occurs.
-   Improve the alerting logic and add identified telemetry to diagnose
    this kind of issues faster.
-   Activate a newer SQL Gateway cluster in this region with a more
    efficient networking stack configuration that reduces the chances of
    hitting a processing imbalance.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)\

## June 2020

## 14 

[06/14/2020]

RCA - Azure Active Directory - Authentication Errors

Tracking ID: PMHH-NS0




**Summary of Impact:** Between 23:00 UTC on 14 Jun 2020 and 01:40 UTC on
15 Jun 2020, a subset of customers using Azure Active Directory may have
experienced authentication issues when accessing resources. Customers
may have received the following error message "AADSTS90033: A Transient
error has occurred. Please try again."

**Root Cause:** An unexpected increase in traffic volume and resource
utilization of infrastructure in the region responsible for acquiring
authentication tokens resulted in regional contention which exceeded
operational thresholds; resulting in authentication issues for a subset
of customers.

**Mitigation:** The backend infrastructure was scaled out to increase
resources and traffic was redistributed.

**Next Steps:** We are continuously taking steps to improve the
Microsoft Azure Platform and our processes to help ensure such incidents
do not occur in the future. This includes, and is not limited to:

-   Improvements to our systems that dynamically scale resources in this
    scenario accordingly with corresponding monitoring
-   Improvements to monitoring to better detect increases in traffic for
    similar scenarios
-   Review and enhance procedures to improve customer communication
    experience

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 11 

[06/11/2020]

RCA - Storage - East US

Tracking ID: 9VHK-J80


**Summary of impact:** Between 11:57 and 14:20 UTC on 11 Jun 2020, a
subset of Storage customers in East US may have experienced connection
failures when trying to access some of their resources hosted in this
region. Services with dependencies on the impacted storage resources,
such as Virtual Machines, may also have experienced downstream impact
during this time.

**Root Cause:** Engineers determined that an incident during a planned
power maintenance activity at the datacenter caused an impact to a
single storage scale unit, which then became unhealthy. The incident
caused power to be lost to to a subset of racks comprising 60% of this
single storage scale unit.

The maintenance activity itself did not impact the storage scale unit,
but it caused the scale unit to have reduced redundant power options at
the time of the incident. All racks and network devices have two sources
of power for redundancy, but it is standard procedure in some types of
maintenance to isolate some resources to a single source of power for a
short period. After the isolation had been completed on this scale unit,
but before maintenance could begin, a distribution breaker in the
redundant power source tripped open unexpectedly and the power was lost
to the subset of racks.

**Mitigation:** The site engineers paused all maintenance work and
inspected the electrical distribution system to ensure there were no
apparent equipment fault issues. They found the tripped breaker and
determined it had failed. Power was restored by closing the other
breaker that had previously been opened to commence the isolation for
the scale unit, and this restored a single power source to the impacted
racks. A new breaker was located and fully-tested before installation.
The bad breaker on the redundant power supply was replaced with the new
breaker and redundant power was then also restored to the affected
racks.

Once power was restored to the impacted storage racks, the automated
restart process for storage resources began, and restored the scale unit
to full operation. The restart process for storage clusters follows a
series of structured steps to ensure full integrity of customers\' data
is preserved, and access to storage resources on this scale unit would
have become available over a short period of time. Final mitigation was
declared at 14:20, but most customers would have seen recovery prior to
this time.

Subsequent testing showed that the breaker had an internal failure on
one phase and it has been sent to the manufacturer for full forensic
analysis.

**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure Platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Forensic investigation into the breaker that failed.
-   Review of breaker maintenance and testing requirements
-   Improving monitoring and alerting visibility when in maintenance
    modes.
-   Ensuring that electrical maintenance activities associated power
    switching are monitored closely with automated processes to manage
    unintended impact.

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 4 

[06/04/2020]

RCA - Azure Resource Manager - Failures creating or deleting resources

Tracking ID: DLZG-7C0


**Summary of Impact:** Between 07:45 and 16:57 UTC on 04 Jun 2020, a
subset of customers across all Public Azure regions may have experienced
deployment failures when attempting to create or delete certain service
based resources via Azure Resource Manager (ARM) deployment and
management service due to an underlying networking issue. While the
related networking resources for the impacted services were actually
being created or deleted during this time, ARM was not notified of the
deployment status and hence was failing the service creation or
deletion. This issue may have impacted some GET or READ action on the
resources. Less than .01% of users would have experienced this issue.

This issue was initially detected an hour after the impact start time
and was identified and escalated by an underlying service experiencing
end user impact. Once detected, multiple engineering teams were engaged
to investigate the cause of the issue to understand what needed to be
fixed. By 11:00 UTC, the appropriate networking team was engaged and
began investigating. The underlying cause was identified by 13:00 UTC.
We identified the appropriate fix and rolled it out to a single region
to validate success. We confirmed success of the roll out and began
deploying to other regions in 3 batches. At the end of each batch we
validated the success of the fix. By 16:57 UTC, the fix was rolled out
to all regions and mitigation was confirmed.

**Root Cause:** A recent ARM deployment contained a configuration file
that stores the URL endpoint that ARM connects to for operation status
query calls. The configuration file had an incorrect endpoint for
networking resources. Due to this wrong setting, the ARM status query
for networking service management operations failed, which customers saw
as failures when attempting to create or delete networking resources.
The faulty configuration file was not caught prior to production because
the update that caused the network resource failures was applied after
testing was performed on a then healthy configuration file. When picking
up the latest configuration file for deployment, the faulty file was
assessed for production and not testing. The faulty configuration file
was then manually rolled out without testing being performed with the
newest configuration, breaking change.

**Mitigation:** We corrected the incorrect URL endpoint within the
configuration file and safely re-deployed to mitigate the issue.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Networking service will onboard to SDP (Safe Deployment Practice)
    endpoint configuration rollout process immediately, to ensure enough
    testing is done and enough time occurs between deployment batches to
    catch any misconfigurations or changes prior to deployment.
-   Networking service will immediately plug-in testing and monitoring
    holes to make sure we immediately identify an issue like this on the
    networking end as failures were only seen on the ARM end.
-   Networking service will work with ARM team to streamline
    configuration rollout process, to guard against errors that may
    occur with the current manual deployment process.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

