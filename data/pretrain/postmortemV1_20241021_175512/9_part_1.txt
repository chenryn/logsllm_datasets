## September 2020

## 3 

[09/03/2020]

RCA - Network Latency Issue -- West Europe

Tracking ID: 8KLC-1T8


**Summary of Impact:** Between 09:21 and 17:32 UTC on 03 Sep 2020, a
subset of customers may have experienced intermittent latency or issues
connecting to resources hosted in West Europe. Retries may have worked
during this timeframe.

**Root Cause:** Two separate events occurred in close succession prior
to the start of impact from this incident:

-   Approximately 4 hours before the impact start, some local activity
    (likely construction) in the vicinity of the data centre cause an
    increase in the number of packets corrupted during transmission over
    fiber optic cables between data centers in the West Europe region.
    These errored packets were detected and dropped, and our networking
    automation systems took the links out of service and opened tickets
    with the local site to have them repaired. This is a standard
    process, and our automated safety checks validated that there was no
    impact related to this.
-   Separately, between 09:21 and 09:26 UTC a significant fiber-cut
    occurred approximately 5 kilometres from the data centre on one one
    of the other paths between the data centers. This cut impacted 50%
    of the capacity for that route, but again, this event on its own
    would have no impact on traffic overall in the West Europe region.

Each of the events in isolation would have had no perceptible impact on
the networking operations for West Europe, but when combined, they
resulted in 9 links between data centres receiving an unequal share of
traffic, becoming congested, and dropping packets (the impact was to
less than 2% of the total capacity on the impacted links). Connections
that travelled over these congested links would have experienced
increased packet loss and latency. As connections are spread over the
available links, services that retried requests by opening new
connections were likely to have been unaffected and successful.

The time to mitigate was extended by the need for on-call engineers to
identify that there were multiple causes for down links and identify the
best way to reduce congestion and rebalance traffic. During the initial
response, the large number of concurrent alerts resulted in on-call
engineers taking actions that moved the congestion from one link to
another, but did not resolve it.

**Mitigation:** Mitigation was achieved by engineers manually
determining which of the links that had experienced errors could be put
back into service and rebalancing traffic across the links in service.
Full mitigation was declared at 17:32 UTC, but most customers would have
see improvement in advance of this time. Full restoration was achieved
by September 4 02:00 UTC when the significant fiber cut was repaired.

**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure Platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Accelerate the readiness of additional fiber paths between these
    data centres to reduce the impact of future fiber cuts.
-   Improve the tooling used by on-call engineers when responding to
    complex incidents with multiple causes of downed links, so that they
    can reduce congestion faster and achieve mitigation more quickly.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## August 2020

## 21 

[08/21/2020]

Content Delivery Network (CDN) - Service Degradation - Mitigated

Tracking ID: DLYY-ND8


**Summary of Impact:** Between 18:05 and 19:55 UTC on 21 Aug 2020,  a
subset of customers using Azure CDN from Verizon may have experienced
service degradation.

**Preliminary Root Cause:** We determined that a recent deployment task
impacted connectivity to origins, causing dynamic or cache miss requests
to fail.

**Mitigation:** The CDN provider rolled out an update that fixed the
issue.

**Next Steps:** We will continue to investigate to establish the full
root cause and prevent future occurrences. Stay informed about Azure
service issues by creating custom service health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation.

## 14 

[08/14/2020]

RCA - Degraded connectivity to Microsoft Services within the Southeast
region of the United States

Tracking ID: 9MDM-TT8


**Summary of Impact:** Between approximately 02:20 UTC to 03:30 UTC and
04:07 UTC to 04:52 UTC on 14 Aug 2020, a subset of customers connecting
through one of Microsoft\'s edge-nodes
([https://aka.ms/MSGlobalNetwork](https://aka.ms/MSGlobalNetwork)) in
the Southeast United States (US) may have experienced intermittent
periods of degraded connectivity when attempting to connect to Azure,
Microsoft 365, and Xbox resources.

**Root Cause:** Microsoft\'s Global Network consists of edge-nodes that
connect to the Internet externally and two or more Backbone sites
internally via diverse optical fiber paths for redundancy during failure
scenarios.

On 14 Aug 2020 at 02:20 UTC, we experienced a dual fiber path failure
isolating one of our edge-nodes in the Southeastern US. The initial
fiber path incident occurred on 13 Aug 2020 at 18:34 UTC due to a fiber
cut causing the path to be removed from Microsoft\'s Global Network.
Traffic was then routed to our secondary fiber path per design.
Meanwhile, our fiber provider had dispatched a technician to work on
resolving the initial fiber incident. While working on that incident,
the technician inadvertently disconnected our secondary fiber path at
02:20 UTC, which resulted in the secondary path to be removed from
Microsoft\'s Global Network isolating this edge-node site.

Our network is designed to withstand site isolation and all traffic
should have rerouted to the next closest edge-node in the region.
However, we identified a router in this edge-node site that continued to
advertise a few local prefixes to the Internet, which resulted in the
blackholing of all Internet traffic destined to those prefixes in the
edge-node site. The route advertisement of the local prefixes should
have been withdrawn by the router when the site was isolated from
Microsoft Global Network during the secondary fiber path incident but
that did not occur due to a missing configuration at this site to detect
site isolation and resulted in an outage. In addition, customer
notification of the event was delayed due to correlation of the event
and the impact.

**Mitigation:** The outage was mitigated when the fiber provider
technician completely restored the fiber connectivity at 04:52 UTC on 14
Aug 2020.

**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Taking steps to prevent dual failures from occurring, reduce the
    degree of impact, and shorten time-to-mitigate by implementing
    improved failover operations to backup sites.
-   Modifying our router configurations globally, to implement
    conditional prefix advertisement and withdrawal to ensure routers
    disconnect as expected during isolation events.
-   Improving our alert correlation to notify fiber technicians in a
    timely manner, and to improve the overall notification experience.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our
survey: [https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## July 2020

## 18 

[07/18/2020]

RCA - Azure DNS - Connectivity issues

Tracking ID: TTPY-3P0


**Summary of Impact:** Between 07:50 and 08:45 UTC (approx.) on 18 Jul
2020, Azure DNS experienced a transient resolution issue which in-turn
impacted connectivity for some other Azure services. Authoritative and
other DNS services were not impacted by this issue.

**Root Cause:** The decommissioning of a legacy (preview) DNS solution
inadvertently caused some data streams for Azure DNS recursive resolver
service to become out of sync with the resolver state. This was detected
by a sync pipeline, which triggered a reset of the resolver instances to
recover from the stale state. Unfortunately, this reset was not done in
a staggered fashion and led to multiple resolver instances rebooting at
the same time. This in turn led to degradation of the service and caused
DNS resolution failures for the queries originating from virtual
networks. Azure services dependent on the Azure DNS resolver service
also saw degradation of service during this time.

The impact of the incident was observed across multiple Azure regions to
varying degrees. While some instances of the service saw no impact, most
impacted instances auto-recovered within 10 minutes, though some
instances took up to 30 minutes to recover. The DNS resolution issues
were fully auto-mitigated across all regions within 54 minutes. During
this time, authoritative Azure DNS service was not impacted and DNS
queries originating from the internet for zones hosted on Azure DNS were
answered successfully.

**Mitigation:** The issue was self-healed as the restarts completed, and
all services with dependencies on the recursive DNS service would have
seen a restoration of functionality also. The DNS service was fully
mitigated at 08:45 UTC, but some services with multiple dependencies may
have taken longer for all customers to see full service restoration.

**Next Steps:** We sincerely apologize for the impact to affected
customers. We are continuously taking steps to improve the Microsoft
Azure platform and our processes to help ensure such incidents do not
occur in the future. In this case, this includes (but is not limited
to):

-   Fixing the orchestration logic in the sync pipeline to help ensure
    that resolver instances are reset in a staggered, partitioned
    fashion
-   Improving the resolver startup sequence to help ensure that a
    resolver instance can be up and running with 10 minutes after a
    reset

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/azurePIRsurvey](https://aka.ms/azurePIRsurvey)

## 7 

[07/07/2020]

RCA - Virtual Machines - Virtual machine unexpected restarts

Tracking ID: 8S8J-9T8


**Summary of impact:** Between 07:24 UTC on 07 Jul 2020 and 21:16 UTC on
17 Jul 2020, a subset of customers using Virtual Machines (VMs) may have
experienced intermittent connection failures when trying to access some
virtual machines. These virtual machines may have also restarted
unexpectedly.

**Root cause:** We determined that an ongoing OS update deployment task
inadvertently contained a code configuration error that resulted in a
number of previously addressed bug fixes being reverted on a subset of
clusters. This manifested as system deadlock on a subset of host nodes
which were running VM workloads with heavy disk I/O. As a result, VMs on
those nodes rebooted.

**Mitigation:** We stopped the ongoing deployment and subsequently
developed and rolled out a new deployment task which contained a code
fix to detect that a new patch needed to be applied. This fix was
deployed to all impacted clusters, thereby mitigating the VM reboots and
customer impact.

In parallel to deploying the permanent fix across all regions, we
expedited mitigation for some customers by identifying affected nodes
that were hosting the customers\' VM workloads and reattaching patches
to those nodes.

**Next Steps:** We understand that the time to mitigate for this
incident was longer than desired, and we sincerely apologize for the
impact to affected customers. We are continuously taking steps to
improve the Microsoft Azure Platform and our processes to help reduce
the duration of such incidents. This includes (but is not limited to):

-   Incorporating the missed combination of software versions and system
    configurations in our validation matrix before deploying similar
