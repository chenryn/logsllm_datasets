Door, Key Vault, Log Analytics, Migrate, Relay, Service Bus, SQL
Database, Storage, Synapse Analytics, and Virtual Machines.

In several cases, data plane impact on downstream Azure services was the
result of dependencies on ARM for retrieval of Role Based Access Control
(RBAC) data (see:
[https://learn.microsoft.com/azure/role-based-access-control/overview](https://learn.microsoft.com/azure/role-based-access-control/overview)).
For example, services including Storage, Key Vault, Event Hub, and
Service Bus rely on ARM to download RBAC authorization policies. During
this incident, these services were unable to retrieve updated RBAC
information and once the cached data expired these services failed,
rejecting incoming requests in the absence of up-to-date access
policies. In addition, several internal offerings depend on ARM to
support on-demand capacity and configuration changes, leading to
degradation and failure when ARM was unable to process their requests.

**What went wrong and why?**

In June 2020, ARM deployed a private preview integration with Entra
Continuous Access Evaluation (see:
[https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation](https://learn.microsoft.com/entra/identity/conditional-access/concept-continuous-access-evaluation)).
This feature is to support continuous access evaluation for ARM, and was
only enabled for a small set of tenants and private preview customers.
Unbeknownst to us, this preview feature of the ARM CAE implementation
contained a latent code defect that caused issues when authentication to
Entra failed. The defect would cause ARM nodes to fail on startup
whenever ARM could not authenticate to an Entra tenant enrolled in the
preview.

On 21 January 2024, an internal maintenance process made a configuration
change to an internal tenant which was enrolled in this preview. This
triggered the latent code defect and caused ARM nodes, which are
designed to restart periodically, to fail repeatedly upon startup. ARM
nodes restart periodically by design, to account for automated recovery
from transient changes in the underlying platform, and to protect
against accidental resource exhaustion such as memory leaks.

Due to these ongoing node restarts and failed startups, ARM began
experiencing a gradual loss in capacity to serve requests. Eventually
this led to an overwhelming of the remaining ARM nodes, which created a
negative feedback loop (increased load resulted in increased timeouts,
leading to increased retries and a corresponding further increase in
load) and led to a rapid drop in availability. Over time, this impact
was experienced in additional regions -- predominantly affecting East
US, South Central US, Central US, West Central US, and West Europe. 

**How did we respond?**

At 01:59 UTC, our monitoring detected a decrease in availability, and we
began an investigation. Automated communications to a subset of impacted
customers began shortly thereafter and, as impact to additional regions
became better understood, we decided to communicate publicly via the
Azure Status page. By 04:25 UTC we had correlated the preview feature to
the ongoing impact. We mitigated by making a configuration change to
disable the feature. The mitigation began to rollout at 04:51 UTC, and
ARM recovered in all regions except West Europe by 05:30 UTC. 

The recovery in West Europe was slowed because of a retry storm from
failed ARM calls, which increased traffic in West Europe by over 20x,
causing CPU spikes on our ARM instances. Because most of this traffic
originated from trusted internal systems, by default we allowed it to
bypass throughput restrictions which would have normally throttled such
traffic. We increased throttling of these requests in West Europe which
eventually alleviated our CPUs and enabled ARM to recover in the region
by 08:58 UTC, at which point the underlying ARM incident was fully
mitigated. 

The vast majority of downstream Azure services recovered shortly
thereafter. Specific to Key Vault, we identified a latent bug which
resulted in application crashes when latency to ARM from the Key Vault
data plane was persistently high. This extended the impact for Vaults in
East US and West Europe, beyond the vaults that opted into Azure RBAC.

-   20 January 2024 @ 21:00 UTC -- An internal maintenance process made
    a configuration change to an internal tenant enrolled in the CAE
    private preview.
-   20 January 2024 @ 21:16 UTC -- First ARM roles start experiencing
    startup failures, but no customer impact as ARM still has sufficient
    capacity to serve requests.
-   21 January 2024 @ 01:30 UTC -- Initial customer impact due to
    continued capacity loss in several large ARM regions.
-   21 January 2024 @ 01:59 UTC -- Monitoring detected additional
    failures in the ARM service, and on-call engineers began immediate
    investigation.
-   21 January 2024 @ 02:23 UTC -- Automated communication sent to
    impacted customers started.
-   21 January 2024 @ 03:04 UTC -- Additional ARM impact was detected in
    East US and West Europe.
-   21 January 2024 @ 03:24 UTC -- Due to additional impact identified
    in other regions, we raised the severity of the incident, and
    engaged additional teams to assist in troubleshooting.
-   21 January 2024 @ 03:30 UTC -- Additional ARM impact was detected in
    South Central US.
-   21 January 2024 @ 03:57 UTC -- We posted broad communications via
    the Azure Status page.
-   21 January 2024 @ 04:25 UTC -- The causes of impact were understood,
    and a mitigation strategy was developed.
-   21 January 2024 @ 04:51 UTC -- We began the rollout of this
    configuration change to disable the preview feature. 
-   21 January 2024 @ 05:30 UTC -- ARM recovered in all regions except
    West Europe.
-   21 January 2024 @ 08:58 UTC -- ARM recovered in West Europe,
    mitigating vast majority of customer impact beyond specific services
    who took more time to recover.
-   21 January 2024 @ 09:28 UTC -- Key Vault recovered instances in West
    Europe by adding new scale sets to replace the VMs that had crashed
    due to the code bug.

**How are we making incidents like this less likely or less impactful?**

-   Our ARM team have already disabled the preview feature through a
    configuration update. (Completed)
-   We have offboarded all tenants from the CAE private preview, as a
    precaution. (Completed)
-   Our Entra team improved the rollout of that type of per-tenant
    configuration change to wait for multiple input signals, including
    from canary regions. (Completed)
-   Our Key Vault team has fixed the code that resulted in applications
    crashing when they were unable to refresh their RBAC caches.
    (Completed)
-   We are gradually rolling out a change to proceed with node restart
    when a tenant-specific call fails. (Estimated completion: February
    2024)
-   Our ARM team will audit dependencies in role startup logic to
    de-risk scenarios like this one. (Estimated completion: February
    2024)
-   Our ARM team will leverage Azure Front Door to dynamically
    distribute traffic for protection against retry storm or similar
    events. (Estimated completion: February 2024)
-   We are improving monitoring signals on role crashes for reduced time
    spent on identifying the cause(s), and for earlier detection of
    availability impact. (Estimated completion: February 2024)
-   Our Key Vault, Service Bus and Event Hub teams will migrate to a
    more robust implementation of the Azure RBAC system that no longer
    relies on ARM and is regionally isolated with standardized
    implementation. (Estimated completion: February 2024)
-   Our Container Registry team are building a solution to detect and
    auto-fix stale network connections, to recover more quickly from
    incidents like this one. (Estimated completion: February 2024)
-   Finally, our Key Vault team are adding better fault injection tests
    and detection logic for RBAC downstream dependencies. (Estimated
    completion: March 2024).

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question
survey: [https://aka.ms/AzPIR/NKRF-1TG](https://aka.ms/AzPIR/NKRF-1TG)

## September 2023

## 16 

[09/16/2023]

Post Incident Review (PIR) -- Services impacted by power, BIOS, and
Virtual Machine issues -- East US

Tracking ID: 2LZ0-3DG


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/2LZ0-3DG](https://aka.ms/AIR/2LZ0-3DG)*

**What happened?**

Between 07:24 and 19:00 UTC on 16 September 2023, a subset of customers
using Virtual Machines (VMs) in the East US region experienced
connectivity issues. This incident was triggered when a number of scale
units within one of the datacenters in one of the Availability Zones
lost power and, as a result, the nodes in these scale units rebooted.
While the majority rebooted successfully, a subset of these nodes failed
to come back online automatically. This issue caused downstream impact
to services that were dependent on these VMs - including SQL Databases,
Service Bus and Event Hubs. Impact varied by service and configuration:

-    Virtual Machines were offline during this time. while recovery
    began at approximately 16:30 UTC, full mitigation was declared at
    19:00 UTC. 
-   While the vast majority of zone-redundant Azure SQL Databases
    leveraging were not impacted, some customers using proxy mode
    connection may have experienced impact, due to one connectivity
    gateway not being configured with zone-resilience.
-   SQL Databases with 'auto-failover groups' enabled were failed out of
    the region, incurring approximately eight hours of downtime prior to
    the failover completing.
-   SQL Databases with 'active geo-replication' were able to
    self-initiate a failover to an alternative region manually to
    restore availability.
-   The majority of SQL Databases were recovered no later than 19:00
    UTC. Customers would have seen gradual recovery over time during
    mitigation efforts.
-   Finally, non-zonal deployments of Service Bus and Event Hubs would
    have experienced a degradation. Zonal deployments of Service Bus and
    Event Hubs were unaffected.

** What went wrong and why?**

 It is not uncommon for datacenters to experience an intermittent loss
of power, and one of the ways we protect against this is by leveraging
Uninterruptible Power Supplies (UPS). The role of the UPS is to provide
stable power to infrastructure during short periods of power
fluctuations, so that infrastructure does not fault or go offline.
Although we have redundant UPS systems in place for added resilience,
this incident was initially triggered by a UPS rectifier failure on a
Primary UPS.

The UPS was connected to three Static Transfer Switches (STS) -- which
are designed to transfer power loads between independent and redundant
power sources, without interruption. The STS is designed to remain on
the primary source whenever possible, and to transfer back to it when
stable power is available again. When the UPS rectifier failed, the STS
successfully transferred to the redundant UPS -- but then the primary
UPS recovered temporarily, albeit in a degraded state. In this degraded
state, the primary UPS is unable to provide stable power for the full
load. So, after a 5-second retransfer delay, when the STS transferred
from the redundant UPS back to the primary UPS, the primary UPS failed
completely.

While the STS should then have transferred power back to the redundant
UPS, the STS has logic designed to stagger these power transfers when
there are multiple transmissions (to and from primary and redundant UPS)
happening in a short period of time. This logic prevented the STS from
transferring back to the redundant power, after the primary UPS failed
completely, which ultimately caused a power loss to a subset of the
scale units within the datacenter -- at 07:24 UTC, for 1.9 seconds. This
scenario of load transfers, to and from degraded UPS, over a short
period of time, was not accounted for in the design. After 1.9 seconds,
the load moved to the redundant source automatically for a final time.
Our onsite datacenter team validated that stable power was feeding all
racks immediately after the event, and verified that all devices were
powered on.

Following the restoration of power, our SQL monitoring immediately
observed customer impact, and automatic communications were sent to
customers within 12 minutes. SQL telemetry also provided our first
indication that some nodes were stuck during the boot up process. When
compute nodes come online, they first check the network connectivity,
then make multiple attempts to communicate with the preboot execution
environment (PXE) server, to ensure that the correct network routing
protocols can be applied. If the host cannot find a PXE server, it is
designed to retry indefinitely until one becomes available so it can
complete the boot process.

 A previously discovered bug that applied to some of our BIOS software
led to several hosts not retrying to connect to a PXE server, and
remaining in a stuck state. Although this was a known issue, the initial
symptoms led us to believe that there was a potential issue with the
network and/or our PXE servers -- troubleshooting these symptoms led to
significant delays in correlating to the known BIOS issue. While
multiple teams were engaged to help troubleshoot these issues, our
attempts at force rebooting multiple nodes were not successful. As such,
a significant amount of time was spent exploring additional mitigation
options. Unbeknownst to our on call engineering team, these bulk reboot
attempts were blocked by an internal approval process, which has been
implemented as a safety measure to restrict the number of nodes that are
allowed to be forced rebooted at one time. Once we understood all of the
factors inhibiting mitigation, at around 16:30 UTC we proceeded to
reboot the relevant nodes within the safety thresholds, which mitigated
the BIOS issue successfully.

 One of the mechanisms our platform deploys when VMs enter an unhealthy
state is 'service healing' in which our platform automatically redeploys
or migrates it to a healthy node. One of the prerequisites to initiate
service healing requires a high percentage of nodes to be healthy -- to
ensure that, during a major incident, our self-healing systems do not
exacerbate the situation. Once we had recovered past the safe threshold,
the service healing mechanism initiated for the remainder of the nodes.

 Throughout this incident, we did not have adequate alerting in place,
and could not determine which specific VMs were impacted, because our
assessment tooling relies on a heartbeat emitted from the compute nodes,
which were stuck during the boot up process. Unfortunately, the time
taken to understand the nature of this incident meant that
communications were delayed. For customers using Service Bus and Event
Hubs, this was multiple hours. For customers using Virtual Machines,
this was multiple days. As such, we are investigating several
communications related repairs, including why automated communications
were not able to inform customers with impacted VMs in near real time,
as expected.

**How did we respond? **

-   16 September 2023 @ 07:23 UTC - Loss of power to the three STSs.
-   16 September 2023 @ 07:24 UTC - All three downstream STSs fully
    re-energized.
-   16 September 2023 @ 07:33 UTC - Initial customer impact to SQL DB
    detected via monitoring.
-   16 September 2023 @ 07:34 UTC - Communications sent to Azure Service
    Health for SQL DB customers.
-   16 September 2023 @ 11:40 UTC - The relevant compute deployment team
    engaged to assist in rebooting nodes.
-   16 September 2023 @ 12:13 UTC - The infrastructure firmware team was
    engaged to troubleshoot the BIOS issues.
-   16 September 2023 @ 13:38 UTC - Multiple compute nodes attempted to
    be forcefully rebooted with no success.
-   16 September 2023 @ 15:30 UTC - SQL Databases with 'auto-failover
    groups' were successfully failed over.
-   16 September 2023 @ 15:37 UTC - Communications sent to Azure Service
    Health for Service Bus and Event Hub customers.
-   16 September 2023 @ 16:30 UTC - Safety thresholds blocking reboot
    attempts understood, successful batch rebooting begins.
-   16 September 2023 @ 16:37 UTC - Communications published to Azure
