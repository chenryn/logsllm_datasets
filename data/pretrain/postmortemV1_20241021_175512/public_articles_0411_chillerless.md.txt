Chillerless Data Centers
November2009
Data centers are energy gobblers! The energy consumption of the world’s data centers doubled
from 2000to2005,growingfrom 0.5%to1.0% ofthetotalelectricalenergygenerated worldwide.
Today, in 2009, these data centers consume 1.5% of worldwide electrical energy; and this
numberisrapidlyrising.
The cooperative technologies of cloud computing and virtualization are working together to make
data processing more efficient in terms of energy usage. But these technologies also bring ever-
reducing costs to computing services, thus generating even more demand on data centers. The
result–morestressonourenergysupplies.
“Green” is the new paradigm for data centers. Major operators of data centers are striving to
make their data processing services ever more energy efficient. The energy efficiency of a data
center is measured by its PUE, the Power Usage Effectiveness. PUE is the ratio of the power
delivered to a data center’s IT equipment as compared to the total power consumed by the data
center.
A typical data center today has a PUE of 2.0. This means that only half of the energy needed to
support the data center is consumed by the IT equipment – servers, network devices, storage
units, and consoles. The rest is needed to support lighting, ventilation, telephones, and, most
significantly,equipmentcooling.
Equipment cooling is by far the largest consumer of electrical power next to the IT equipment
itself. Themostcommon cooling technique is to use water chillers to cool hot air flowing from the
equipment bays. This cold air is then recirculated back through the bays to keep the equipment
cool.
“Free Cooling”
Some major operators of data centers are taking significant steps to eliminate the energy-
inefficient chillers in their new data centers. They are accomplishing this by using “free cooling.”
Free cooling is the use of outside air to cool data centers. When the outside air is cool, it is
circulated through the equipment to control its ambient temperature. Should the day warm up, a
fall-back strategyis invokedtoeither increasethecoolingcapacityofthedatacenteror toreduce
itsheatload.
Both Google and Yahoo! are taking advantage of free cooling by locating new data centers in
areas of the world where outside temperatures are naturally low so that they can cool their
equipmentbynaturalairflowratherthanbywaterchillers.GooglehasalreadyachievedaPUEof
1
©2009SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

less that 1.1 – that is, less than 10% of data center power is being used for purposes other than
computing.Yahoo!expectstofollowsuit.
Google
Google chose Saint-Ghislain, Belgium, for its prototype chillerless data center, which began
operations in late 2008. St.-Ghislain is 30 miles southwest of Brussels, which puts it about 70
milessoutheastoftheEnglishChannel.
The average temperature in summer in this
area is 66 to 71 degrees Fahrenheit. Google
maintains its data center equipment at
temperatures above 80º F. Google estimates
thattheambienttemperaturewillsupportfree
cooling year round except for about seven
days per year. During these times, Google
will turn off equipment as needed and will
shift some or all of the data center’s
processing loads to other data centers to
maintain equipment temperatures within
allowableranges.
Freecoolingmakes local weather forecasting
a large factor in data center management.
Google has developed automated tools to
manage data center heat loads. These tools
use advance weather forecasts to decide
TheequipmentyardatGoogle’s
when to distribute workloads. These tools
chillerlessdatacenterinBelgium
can also rapidly redistribute computing
workloadsduringanunanticipatedthermalevent.
GooglereportsaPUEforitschillerlessdatacenterinBelgiumthatisslightlylessthan1.1.
Paradoxically, Google has had problems with workload redistributions in the past. On Tuesday,
February 24, 2009, Google’s Gmail was down for two and a half hours. Google later explained
that, in preparation for a routine maintenance event at one of its European data centers, users
were routed to another nearby data center. This inadvertently overloaded that data center, which
caused a cascading effect from one data center to another, ultimately taking down the entire
Gmailnetwork.1
Yahoo!’sChickenCoops
Yahoo’s planned chillerless data center is to be located in Lockport, New York, northeast of
Buffalo and ten miles south of Lake Ontario. This data center will be one of the greenest data
centers in the world. Not only will it use hydroelectric power generated by Niagara Falls to the
east,butitwillusethewindsoffofLakeOntarioforfreecoolingofitsITequipment.
The data center will comprise a set of independent modules called “coops” because of their
resemblance to chicken coops. According to plans filed with the city of Lockport, each coop will
be a prefabricated metal structure 120 feet by 60 feet. Louvers built into the sides of each coop
will allow cold air to enter the computing area. The coops are angled to take advantage of the
prevailingwindsofLakeOntariosothatthewindswillblowdirectlyintothelouversystem.
1GoogleTroubles:ACaseStudyinCloudComputing,AvailabilityDigest;October2009.
http://www.availabilitydigest.com/public_articles/0410/google_troubles.pdf
2
©2009SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Each coop has a peaked roof with a “penthouse” on top that manages the release of waste heat
fromthehotisleinthecoopintotheoutsideair.
A coop will house five megawatts of
equipment. Initially, five coops have been
approved by the town, though the Yahoo! site
planshowsroomformanymore.
On days that are warmer than 27 degrees
Celsius(about80ºF),thecoopcoolingsystem
is augmented with evaporative cooling. It is
expected that this may be required about 212
hours (about 9days) per year.Theresult is an
estimated annualized PUE of 1.1, meaning
that 90% of the energy consumed by the data
centergoestopoweritsITequipment.
Yahoo! alreadyoperates green datacenters in
Washington State that use wind and Yahoo!’sproposedLockport
hydroelectric power with free cooling for most chillerlessdatacenter
oftheyear.
Summary
The day of the large campuses housing major data centers may be coming to an end. Both
Google and Yahoo! are moving to compact and efficient data center modules that can be scaled
by simply adding additional modules. This is similar in some respects to the move towards “data
centers in a box” in which entire data centers are being built within a portable shipping container
by,amongothers,HP,Google,Sun,Dell,Microsoft,andRackspace.2
TheU.S.EnvironmentalProtectionAgency(EPA) has setagoalforadatacenter PUEof1.12by
2011.ThenewchillerlessdatacentersofGoogleandYahoo!meetthisrequirement.Tomeetthis
goal across all data centers, we may see major enterprises like Google and Yahoo! adopting a
“follow the moon” strategy in which workloads are shifted seamlessly between data centers to
take advantage of better cooling during overnight hours (not to mention cheaper off-peak energy
costs).
2DataCenterinaBox,AvailabilityDigest;July2009.
http://www.availabilitydigest.com/public_articles/0407/pods.pdf
3
©2009SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com