# 从头到尾复盘腾讯云故障

**非法加冯**

作为腾讯云的深度上云用户，我完整的经历了腾讯云故障的发现到恢复过程，并从中自己总结了下此次故障的根因。

本来打算等待腾讯云官方的公告，但 1
天过去了，貌似除了一个三无的公告和几个调皮的微博帖子之外，我大概拿不到任何的故障复盘了。

既然这样，我就把自己的故障复盘发出来，并讲一些自己在经历过此次故障之后的一些思考。

## 我的故障经历视角 

**4 月 8 日
15:15**分，我的企业微信开始收到服务器报警，报警内容大概表述的信息为：

> SCF（云函数）、COS（对象存储）、数据万象出现 API
> 请求问题，原因均为：\[TencentCloudSDKException\]message:An internal
> error has occurred. Retry your request, but if the problem persists,
> contact us.

一直到**15:21**分左右，所有依赖腾讯云 API
的服务正确率开始急速下降，另外服务器的状态拨测 API
也报同样的`TencentCloudSDKException`错误。

简单测试了一下，服务器、数据库、网络正常访问。我意识到这不是我自己的服务问题，大概率腾讯云
API 服务挂了，很自然的想到了去年阿里云双 11 故障。

那时候我还在入海关，过了海关通道后找个椅子开电脑检查起来。**15:30:55**秒登录进腾讯云控制台。

![图片](./从头到尾复盘腾讯云故障_files/640)

登进去的当时还在想既然能登录进去，那就证明 API
服务还正常。紧接着迅速打脸，控制台全部爆红，红色的弹窗如潮水般涌来。

重试了一下登录，无法成功。自己已经确认是腾讯云故障了，并且当时并没有做
API
这么底层的灾备方案，于是开启祈祷模式。16:32，我向群友发了**腾讯云挂了**的情报。

由于数据库、网络、服务器正常运行，所有控制台管控面失效，开发者自身服务使用的
SDK、API 出现相同的`internal error`

API 底层整个挂掉了，从 15 分到 31 分的这 15 分钟时间里，API
底层故障是蔓延的趋势，从一开始的小范围单地域影响，到全部地域整个 API
管控面全部完蛋。（原因我文章后面再大胆分析）

这和去年阿里云双 11 故障几乎是翻版，于是在群里给冯老板调侃说：可以直接把去年的[文章](https://mp.weixin.qq.com/s?__biz=MzU5ODAyNTM5Ng==&mid=2247486468&idx=1&sn=7fead2b49f12bc2a2a94aae942403c22&scene=21#wechat_redirect)改个名字再发一遍。

**15:45**，腾讯云主站挂掉，开始报 502 和 500
错误，猜测是用户登录控制台大量重试造成 API
访问量激增，对恢复工作有影响，于是主站切断了访问。**估计这个 35-45
时间段相关同学已经开始介入准备恢复了。**

**15:55 分左右**的 10
分钟时间段，客户经理得到确切的回复口径，开始在客户群里回复。

**16:05 分开始**，故障开始逐步恢复；16:10
分控制台外壳管控面基本可用，但没有 100%恢复，仍然有报错。

**16:20 分左右**，API
在部分地域恢复见效，开始逐级消化响应，此时独立使用服务器、存储、网络底层的服务（比如服务器、NAT
网关控制面、数据库控制面）已经可用，成功率未达到 100%。

**16:21
分**，腾讯云官网发出恢复公告，告知控制台已经完全恢复，我觉得这里**控制台**三个字用的好，按照我的观测，确实是控制台外壳管控面已恢复，但这对于生产用户来说毫无意义。

**16:40 分左右**，先期恢复的地域，管控面基本 100%恢复。重度依赖 API
的产品服务（比如数据万象、云函数）恢复见效，但成功率未达到 100%。

**16:45 分左右**，官方宣布部分地域已恢复，其他地域仍在恢复中。

**16:55 分左右**，观测到先期恢复的地域，重度依赖 API 的产品服务基本
100%恢复。

**17:05 分左右**，生产环境已经不出现报错，绝大部分地域正常，偶现 CDN
缓存失效问题，应该是故障时的脏请求留存。

**17:16 分**，官方微博宣布整体恢复，除了上海 API
服务之外其他的均以恢复。目测只是个别服务的 API 不可用。

以上就是我经历的整个故障过程，故障结束后我查看了日志和外部观测平台，确认了各个服务的故障时间段，为后面的复盘打个基础。

我是深度用云用户，除了服务器、数据库 RDS、NAT 网关、CLB
负载均衡之外，还用了对象存储 COS、内容分发网络 CDN、数据万象 CI、云函数
SCF、云开发 TCB、日志服务 CLS、凭据管理 SSM
等等，涉及地域有上海、南京、北京、广州、重庆。

在整个故障期间我的体感非常丰富，也让我掌握了比大部分用户更准确的故障时间节点。

接下来就开始一望而知的复盘了。

## 故障复盘 

首先基本的服务器、网络、数据库 RDS
这些基础服务（只使用网络、CPU/内存、硬盘的服务）都能正常使用；而控制台和开发者生产环境
SDK 同时出现一致的内部错误，可以判定是 API
出现问题，除了它几乎不会有什么能导致全部地域全部接入都挂掉。

从**15:15 分**开始，到**15:31
分**完全挂掉，故障是以蔓延的趋势发生的，从单个地域到多个地域，从重度 API
服务到管控面，这种趋势基本可以判断这是人为因素导致的。

人为因素有两个：

1.  发布新版本，在灰度过程中出现不可知的问题，导致故障的迅速蔓延。

2.  现行版本有隐藏
    bug，在一个时间节点或者一定压力下会出现，进而因为调度导致影响面开始扩大，造成全部挂掉。

而从官方发布的公告来看，发布新版本的可能性最大。

如果有隐藏
