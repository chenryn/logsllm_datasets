## April 2022

## 8 

[04/08/2022]

RCA - Service Management Operation Errors Across Azure Services in East
US 2

Tracking ID: Y\_\_5-9C0


**Summary of Impact:** Between 12:25 UTC on 08 Apr 2022 and 14:40 UTC on
09 Apr 2022, customers running services in the East US 2 region may have
experienced service management errors, delays, and/or timeouts.
Customers may have experienced issues that caused GET and PUT errors
impacting the Azure portal itself, as well as services including Azure
Virtual Machines (VMs), Virtual Machine Scale Sets (VMSS), Azure Data
Factory (ADF), Azure Databricks, Azure Synapse, Azure Backup, Azure Site
Recovery (ASR), and Azure Virtual Desktop (AVD) Customers may have seen
errors including "The network connectivity issue encountered for
Microsoft.Compute cannot fulfill the request. For some downstream
services that have auto-scale enabled, this service management issue may
have caused data plane impact.

**Root Cause:** We determined that the Compute Resource Provider (CRP)
Gateway service experienced an issue which severely reduced its
throughput. The underlying issue was a retry storm triggered by the
zonal failure of a related Allocator service. While we were able to
recover the Allocator service by restarting the instances in the failed
zone, the backlog of work exposed a potential issue with .Net CLR and
Garbage Collector. This resulted in a large percentage of incoming calls
to the CRP Gateway to fail. The retries triggered by the upstream
services only made the load situation worse. Under normal circumstances,
the Gateway instances are overprovisioned for such retry storms but the
combination of the reduced throughput across all instances and
continuous retries (some services which normally make 25K calls in 1
minutes were making 150K calls in the same period due to retries)
resulted in a prolonged impact.

Deeper investigation into process profile data exposed that the process
was experience a high rate of timeout exceptions for ongoing operations
and .Net Garbage Collector was overworked due to high heap churn under
above mentioned spike in load. A high rate of exceptions and the
simultaneous pressure on the .Net GC exposed an unfavorable interaction
in the .Net runtime\'s process wide lock.

**Mitigation:** To mitigate the situation, the below steps were taken:

-   Two large services were temporarily throttled more aggressively to
    ensure they do not continue to overload gateway.
-   Once the underlying issue of the throughput reduction was partially
    understood, the gateway services were restarted multiple times until
    they got out of the wedged state.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   As a long-term fix, we initiated a CRP gateway hotfix that will
    prevent the gateway from entering into the wedged state. The hotfix
    roll-out is progressing as per our Safe Deployment Practices.
-   We are flighting a configuration change to make the .Net GC work
    less hard and avoid interaction with the process wide lock which is
    surfaced with exception handling.
-   Repair items have been identified to optimize areas of code which
    were causing heap churn

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## March 2022

## 16 

[03/16/2022]

RCA - Azure AD B2C -- Authentication Failures and Error Notifications

Tracking ID: TTCR-NTZ


**Summary of Impact:** Between 09:13 and 10:22 UTC on March 16, 2022,
end-users of customers using Azure Active Directory B2C may have
experienced errors and timeouts when attempting to sign in or sign up.
Retry attempts were likely to succeed during this incident

**Root Cause:** The service experienced a significant increase in
workload in the affected region during a planned maintenance operation.
As a result, a subset of sign-in requests was queued up by the system,
which increased processing time, and in some cases sign-in attempts by
end-users may have timed out.

**Mitigation:** The service automatically scaled up compute resources in
response to the increase in workload, which provided partial relief. In
addition, we rerouted subsets of the workload to alternate capacity to
achieve complete mitigation.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Improve planned maintenance Standard Operating Procedures (SOP) by
    pre-provisioning of additional capacity to affected regions to help
    handle unanticipated workload.
-   Improve planned maintenance SOP to include pro-active assessment of
    similar pre-provisioning in other regions beyond the affected region
    for the service.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 1 

[03/01/2022]

RCA - Azure Resource Manager - Service Management Operation Failures

Tracking ID: ZNRZ-HDG


**Summary of Impact**: Between 11:49 EST on 01 March 2022 and 03:08 EST
on 03 Mar 2022, a subset of customers experienced errors when using
Azure Resource Manager to perform service management operations in the
Azure Government cloud.

**Root Cause**: A synchronization issue occurred between backend
components used to permit certain ARM requests. A configuration change
was applied to these backend components, which resulted in some
instances of the ARM service becoming unreachable, causing errors for a
subset of operation requests.

**Mitigation**: We rolled out a hotfix to affected components, restoring
the ARM service, which allowed operation requests to complete as
expected.

**Next Steps**: We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Update ARM component configuration methods to help prevent
    synchronization issues when similar updates are required.

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

\

## February 2022

## 16 

[02/16/2022]

RCA - SQL Database and App Service - West Europe

Tracking ID: 9TDP-N8G


**Summary of Impact**: Between 07:31 UTC and 15:31 UTC on 16 Feb 2022, a
subset of customers using SQL Database instances in West Europe may have
experienced database connectivity errors in this region including when
attempting to create new connections. Retries may have been successful.

Additionally, customers utilizing Azure App Service in West Europe may
have experienced issues while performing service management operations
such as site create, delete, and move resources on App Service (Web,
Mobile and API Apps) applications. Autoscaling and loading site metrics
may have also been impacted.

**Root Cause**: Due to a memory hardware failure in a network router, a
control plane/data plane synchronization process in that router failed
during a regular automated maintenance operation. This router was one of
8 redundant routers in that tier of the network, and the failure of the
synchronization process led to the router dropping packets to a subset
of the IP addresses it handled. The result was the failure of up to 12%
of network flows to endpoints below the router. In particular, up to 12%
of new connections to a subset of Virtual IP addresses (VIPs) would have
failed. Retries would have likely succeeded and connections worked
properly once established. As a downstream effect of the availability
impact to some SQL services, App Service resources in the region with a
dependency on SQL may have experienced issues.

The mitigation took longer than expected as the impacted device was
going through automated maintenance, during which alerts were
suppressed. Alerts were triggered as expected by the automated alerting
system once the device was brought back into rotation.

This network device was being upgraded to a new firmware version which
has, among other capabilities, the ability to automatically recover the
control plane/data plane synchronization process in case of failure.
This update helps prevent such failure scenarios in the future.

**Mitigation**:

-   Engineers isolated the impacted network switch to mitigate the
    incident.
-   Engineers verified that no other network switches were impacted due
    to the same issue.

**Next Steps**: We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Expedite the upgrade of network switches to the firmware that
    contains auto recovery of the control plane/data plane
    synchronization process.
-   Enhancements to automated maintenance and alerting platforms to
    improve alerting for devices undergoing maintenance.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 12 

[02/12/2022]

RCA - Azure SQL DB and Cosmos DB Unavailable

Tracking ID: SL1P-TSZ


**Summary of Impact:** Between 11:45 UTC on 12 Feb 2022 and 11:43 UTC on
15 Feb 2022, a limited subset of customers using SQL Databases or Cosmos
DB experienced database unavailability and may have seen errors when
connecting to their database instances. This issue affected a specific
generation of hardware hosting SQL and Cosmos DB resources in six
regions.

**Root Cause:** Our telemetry has shown that a subset of nodes running
our newest hardware generation hosting SQL DB and Cosmos DB experienced
a loss of connectivity to the network control plane starting on 12 Feb
2022. Our newest hardware generation uses a new and optimized network
control plane and data plane designed to improve performance and reduce
irregularities and latency, with a new control plane secured by TLS
authentication. The TLS certificates that secure this channel are
rotated regularly and expire within a short timespan for security
purposes. A race condition in the underlying Remote Procedure Call (RPC)
mechanism caused the network control plane channel in some cases to not
pick up the rotated certificate, leading to connectivity failures once
the certificate expired.Â 

Routine maintenance in the general Azure compute fleet had fixed this
issue with a code update in January. However, based on telemetry, we did
not believe this was a significant risk to SQL and Cosmos DB
environments, which were scheduled to receive the code update later in
February. On 12 Feb 2022, a significant number of previously rotated
certificates expired simultaneously, causing impact. We first mitigated
the impacted nodes and DBs, and then pushed the code update using an
emergency process through the SQL and Cosmos DB fleets to make sure the
impact would not recur.

**Mitigation:** Once impact was identified, we recovered nodes with
network control plane connectivity loss and brought DBs back to a
healthy state over the course of 12 Feb - 15 Feb. While most DBs
recovered earlier in the incident, running a full update to get the code
fix to all nodes had to proceed slowly to ensure uptime and data
integrity, so it took until 14 Feb for Cosmos DB and 15 Feb for SQL DB
to complete the rollout.

**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Improved procedures to help determine if code updates for Azure
    servers are critical, so we can patch critical data services like
    SQL and Cosmos DB sooner, instead of taking them at a normally
    scheduled pace.
-   Faster automated recovery and deployment for critical fixes to SQL
    and Cosmos DB, to help reduce impact time for critical node fixes.
-   Improved telemetry to help detect certificate expiration risk.

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)
