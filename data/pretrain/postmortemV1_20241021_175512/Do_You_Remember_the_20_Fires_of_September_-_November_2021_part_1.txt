Do You Remember, the
Twenty Fires of September
—
Over September and early October, Honeycomb declared five distinct public incidents, for
various reasons and of various severities. As far as we are concerned, the whole month was
part of a broader operational burden, where over 20 different issues came up to interrupt normal
work. A fraction of them bubbled up to having a public impact that was noticeable and declared,
but most of the significant work would have been invisible. A retrospective would be incomplete
if we considered the incidents as distinct entities rather than part of a longer connected
sequence.
This series of incidents occurred in a context of continuous change. From August to September:
● The amount of data ingested by Honeycomb grew by roughly 40% (due to both individual
customer growth and overall customer count),
● The platform team was working on a migration of our infrastructure from Chef-managed
instances to containerized deployments
● The development of new features and integrations kept at a steady pace.
Most of the challenges described here come from a pattern of accelerated growth and scale,
highlighting performance degradation and brittleness in our stack in non-obvious
ways—particularly when multiple components are hitting inflection points at the same time on
multiple dimensions, and there’s no clear way to single out any particular slow part. We’ll
describe the various events that composed into that work—and omit some less relevant ones
for brevity even though they were part of a challenging workload—before doing a review of the
lessons that can be learned from our experience.
One additional thing to keep in mind as you read these is how many of these individual incidents
or near-misses feed into each other to progressively paint a more complete picture, where we
finally have enough data to explain everything that happened.

Incidents and near misses
Kafka Rebalancer wedges
Every Tuesday, an automated task we have shuts down one of our Kafka brokers in each
environment. This has become standard practice with multiple services that have a fixed cluster
size to ensure we are able to replace them. On August 31, the Kafka auto-balancer, which takes
care of moving partitions to keep load even, got stuck. A replacement broker came up, but no
partitions were assigned to it.
We got a non-paging alert from a trigger telling us that some partitions were under-replicated.
That tends to happen right after scheduled replacements, and along with other operations going
on, we believed it to be normal. It was only later, on September 1, that we noticed the Kafka
replication trigger was stuck in alerting mode.
We finally detected the replacement broker receiving no traffic. Thinking there was something
wrong with it, and knowing it had held little data and was leading no partition, we took it out.
Then the replacement’s replacement got stuck as well, and we knew something was odd.
Fortunately, none of this affected any customer since all our Kafka partitions are replicated on 3
brokers in different availability zones and could still tolerate more failures. After help from
Confluent’s support, on September 3, we found a procedure that unwedged everything.
A bad SSD blamed on a big customer
On Saturday, September 11, a single node of our distributed columnar data storage engine,
retriever, started seeing an elevated rate of file system errors that suggested a failure of its
solid-state drive. Retriever nodes operate in redundant pairs, so data storage was not impacted
by this failure. Both nodes in a retriever pair, however, participate in answering queries for a
particular subset of events. During this degradation, the disk errors caused queries handled by
this retriever node to incur a performance penalty of a couple seconds. T he issue was noticed
on Monday morning, and after investigation, the offending node was replaced, restoring service
to normal.
The investigation was made more complex by recent discussions where we had wanted to try
going to bigger instances, which anchored responders into thinking this could be a capacity
issue. Since a large customer was the most impacted, we assumed they were to blame for the
overload.
1

Eventually, we would find that this fault was part of a broader series of failures that were noisy
throughout the month and related to a kernel bug around file systems. We have, however,
needed to go through many of these failures to see a pattern emerge and to investigate.
Some lessons from this investigations that may be of general interest:
1. Large customers can be red herrings. The incident happened early on the East Coast,
where a specific customer starts business and ramps up quickly. They were hit hardest,
to the point where it looked like they were causing the damage. The correlation was
thought to be causation when it was not. “Normal” has different meanings at different
times.
2. Hardware problems can be hard to detect with the way we instrumented our systems
because we seek problems in the data we have first. Things hidden in dmesg are
surfaced only after we exhaust more accessible tools.
3. While we thought only a major customer was impacted, all communications surrounding
the incident were kept internal, with remediation focused on managing their load. Once
the incident was re-framed as a general disk fault, we shared it publicly and reoriented
our response.
Overloading our dogfood environment
Through the summer and until mid-August, various optimizations were added and limits raised
in our stack, which drastically increased our burst capacity for read queries.
Since then, we had seen a few alerts where multiple components would alert across
environments, but without a clear ability to explain why that was.
On September 16, it happened far more violently than we had experienced before. We later
found out it was caused by another large customer issuing multiple costly queries, which
2

collectively read about 3.3 billion column files (1.55 petabytes) in a short time, over S3. Each of
these accesses to S3 in turn generate access logs.
What happened then is our dogfood ingestion pipeline, generally seeing far less traffic, was
under-provisioned for this spike. This is effectively equivalent to a Denial of Service (DoS)
attack. When this happens, other clients trying to write to the dogfood API endpoint (all
production components) get delays and possibly errors until auto-scaling catches up.
Amid the confusion, the volume and back-pressure caused ingestion delays and even crashed
production Kafka reporters to Honeycomb, and reports going to third-party tools were brought
down with them, making it look like full production outage to our redundant alerting systems.
Beagle processing delays
Beagle analyzes input streams for service level objectives (SLOs) data. Its auto-scaling works by
being CPU bound. Partition imbalance, SLO definition imbalance, and network throughput are all
different things that can contribute to CPU being a poor proxy metric for its load. We knew
about this, but felt it wasn’t worth the cost of implementing a custom CloudWatch metric source
when CPU worked tolerably, only to get rid of it with our container migration that would soon use
a new scheduler.
Previously when beagle would warn of being behind on one or two Kafka partitions, we’d
manually scale up its auto-scaling group, which fixed the problem with minimal effort. On
September 21, the problem looked very different:
The big bump at 13:50 matches an increase in capacity where some partitions got better, but
some still got worse. This is a sign to us that this isn’t related to the scale of the consumers.
3

All of a sudden, a lot of unrelated partitions were lagging behind and struggling, and scaling up
gave no immediate results. Scaling yet again seemed to improve things a bit, but the catch-up
rate was below our expectations. Eventually, the cluster caught up and stabilized again, without
any specific intervention. It did, however, divide our attention.
Migrating dogfood retrievers
On September 21, after a few weeks of running retrievers in hybrid mode between EC2 and EKS
in our dogfood environment, we completed the rollout of EKS retrievers. EC2 retrievers were
scaled down. Everything seemed fine—until the next day, when we woke up to alerts stating that
records, segment data, and columns couldn’t be written to disk.
This meant that the entirety of dogfood retrievers was out of disk space and couldn’t even write
down metadata, and cascaded into other alerts throughout the platform.
What happened (but we didn’t know at the time) was that in containerizing retrievers, we
accidentally omitted to transfer cronjobs that ran only on EC2 retrievers. One of these is a task
that orchestrates our data’s life cycle. In a nutshell, all data on retrievers goes through a sort of
long-lasting garbage collection for database files that range from their creation, to S3 upload, to
deletion through aging out. By not having it running, retriever instances kept accumulating data
until they were entirely out of room.
We usually get warnings about disks filling, but got none in this case because we believe
retrievers on EKS (which use h ostPath mounted volumes from the parent host to store their
data) don’t see that disk usage reported in K ubernetes metrics. So any early warning that could
have let us know things were getting dire was not there.
We guessed that something might have gone wrong due to the retriever migration. Not knowing
what it was, we decided to boot EC2 instances again to run whatever was missing. We
eventually spotted the cronjob issue and started manually clearing disk space to salvage
instances. This failed because as soon as we’d free space, most Retrievers would write back to
it before we could clear enough to let the garbage collection run.
This, in turn, generated a lot of noise on kibble, our environment that monitors dogfood (which
monitors production), which also ran out of disk space. Unlike dogfood, it was due to generating
so much traffic in a short time for its tiny cluster size that it ran out of space before we could
even run a GC lifecycle on it.
After failing to free up space, we saw that our new EC2 instances were healthy and had run their
own life cycle tasks to completion. This meant that we could now swap the dogfood EKS
instances to let new ones take over by fetching correct state (written by EC2 instances) off of
S3.
4

Kibble was salvaged by manually deleting the data from the dataset that was spammed by logs.
We had no better solution, and we knew that for the last hours, all the logs were garbage, so we
took the loss.
While this was going on, a production host ran out of disk space as well, but it was due to a bad
disk (again!) and was easy to fix. Still, for a brief period of time, all three environments (prod,
dogfood, kibble) were reporting retrievers with filled disks at the same time, for different
reasons. No customer data or performance was impacted at any point, but this was still an
all-hands-on-deck situation.
We scheduled an incident review because a lot of interesting stuff happened there despite
having no production impact. Unfortunately, we did not even have time to finish the incident
review before we got interrupted by yet another incident.
Further beagle processing delays
On September 23, beagle alarms tripped once more. We initially blamed high CPU variance, but
after looking further into Kafka, found out that the rebalancer got wedged once again:
broker: 1262 leading: 7 non-leading: 12 total: 19
broker: 1263 leading: 7 non-leading: 14 total: 21
broker: 1264 leading: 7 non-leading: 13 total: 20
broker: 1266 leading: 6 non-leading: 15 total: 21
broker: 1267 leading: 6 non-leading: 16 total: 22
broker: 1268 leading: 8 non-leading: 11 total: 19
broker: 1269 leading: 0 non-leading: 1 total: 1
This had caused things to go out of balance and thought this could have overloaded some
leaders. We reused the procedure we had developed earlier that month to fix it. The rebalancing
nearly caused some Kafka partitions to run out of disk space, so we dropped our non-tiered
retention from 3 hours to 2 hours.
Without us knowing about it, the previous day’s dogfood migration issues repeated the DoS
incident effect that shut down Kafka’s production metrics pipeline, which silenced all the data
that would usually warn us of under-replicated partitions. The radio silence meant we only found
out through indirect signals related to performance.
The dropped retention brought enough room to rebalance the cluster and eventually fixed
beagle’s lag. We decided to add alerting and make a complete runbook to detect and manage
future rebalancer wedges. This alarm has proven useful a few times already.
5

Lambda deleted by Terraform
On September 23 while running the Dogfood Disk Exhaustion retrospective, we got interrupted
by another odd issue, where a seemingly routine Terraform cloud deployment deleted the
reference to our production lambda worker for all retriever reads.
We still don’t know why the package type changed and why that forced a replacement (which
puts in a placeholder). The actual lambda is written by our regular deploy mechanism, which
was re-run manually to force a resource to be put in place.
It took a short while for the system to stabilize again, and we added a protection in the terraform
file to prevent it from accidentally being deleted again. This is an interesting event because it
interrupted corrective work for other issues, and is part of a pattern of ongoing pressures that
made it difficult to keep up with and improve our overload situation.
Kafka scale-up
On September 24, the beagle processing delays kept happening, but this time we knew the
Kafka cluster to be balanced. However, we detected that some Kafka partitions seemed to be
lagging behind others.
After plenty of debugging in a Zoom call (we were getting fed up with these issues), we
discovered that our Kafka cluster’s brokers were silently being throttled over network
allowances by AWS:
6

We made an agent to extract the values and to start accumulating data, and comparing it to
other services showed that the Kafka instances were being impacted at a far higher rate than
others.
We manually checked other network values and decided that our Kafka cluster needed to be
scaled up vertically to get onto instances with more network capacity.
As we were planning to grow the cluster, beagle started lagging again and had a hard time
recovering, so we decided to fast-track the migration of the most impacted partitions by shifting
them from an older smaller instance to one of the new ones.
We then decided we’d transfer data slowly over days with rebalancing off since we wanted to
leave old instances nearly empty and the new ones full.
We left things stable on Friday, and completed the migration on Monday and Tuesday
(September 27-28). At some point on the 28th, beagle kept being delayed some more, and our
end-to-end tests started firing one again.
We found out that the latter was caused by many retriever partitions “double-consuming,” which
means that both retrievers in each pair for a partition is multiple seconds behind in reading from
Kafka. This is tolerated by readers, but it means the data Honeycomb users see is either
temporarily incomplete (because their data may be on partitions at different levels of progress)
or missing (because it’s late on all partitions). W e posted a public status for this since it was
customer-impacting.
We quickly found out that the issue was partially caused by having turned on the autobalancer
back on for the Kafka migration (so it would move partitions from a removed older instance
onto newer ones), and having it move partitions back onto the smaller instances we were still
7

planning to cordon off. We canceled the migrations and turned the balancer back off except for
instance replacement. This let all consumers catch back up.
Understanding beagle processing delays
We hoped that completing our Kafka
migration would solve the beagle
processing delays once for all, but on
Wednesday (September 29), they happened
once again. It now became clear that this
issue shouldn’t be caused only by Kafka
being overloaded since we had added over
50% extra capacity.
We once again found out that leaders were left unbalanced:
broker: 1271 leading: 8 non-leading: 13 total: 21
broker: 1272 leading: 7 non-leading: 14 total: 21
broker: 1273 leading: 6 non-leading: 14 total: 20
broker: 1274 leading: 7 non-leading: 13 total: 20
broker: 1275 leading: 8 non-leading: 23 total: 31
broker: 1276 leading: 5 non-leading: 5 total: 10
The rebalancer had died once again, and we had no metrics to fuel early alerting because again,
the reporters from Kafka had died as well. We kicked them back up, planned an upgrade that
would solve the crash issues, and quickly juggled leadership on partitions and migrated some to
once again get things in balance.
Everything was catching up as we came closer and closer to being fully balanced, but once the
rebalancing was complete, beagle latency worsened again. Therefore, the balance alone
couldn’t explain the performance issue.
8

At one point, we got an alternative version of the graph where instead of grouping by partition,
we grouped both by partition and by beagle consumer. And now things looked fun:
