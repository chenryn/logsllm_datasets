Help! My Data Center is Down!
Part 5 - Upgrades
February2012
Data centers are extraordinarily complex. They include hundreds or thousands of servers and storage
subsystems with their applications and operating systems, all interconnected by vast internal networks. A
failureinanyoneofthesecomponentscanbringsomeifnotalldata-centerfunctionstotheirknees.
However,major failures arenot always causedbyhardwareor software. Adisturbingnumber arecaused
by upgrades that go wrong. If a fallback plan has been put in place, then such a failure is typically not a
problem. However, in too many cases, data centers have undertaken an upgrade with no backout
procedure in place. If the upgrade fails, major applications will be down – for hours and sometimes for
days.
Inourpreviousarticles on data-centerfailures,wefocusedonfailures duetopower,storagesubsystems,
andnetwork faults.Inthis article, we look atsomemajor data-center outages due tofaultyupgrades.The
storiesarealltrueandaretakenfromtheNeverAgainarchivesoftheAvailabilityDigest.
IRS Goof Costs U.S. Taxpayers $300M
The IRS (the Internal Revenue Service) is responsible for collecting taxes in the
U.S. As part of this responsibility, the IRS uses a complex fraud-detection system.
However, tax laws change; and fraud perpetrators get smarter. After several years
of successfuloperationof itsfraud-detectionsystem,theIRSrealizedthatithadto
upgrade the system if it were to continue to be effective at catching fraud. To
implement this upgrade, the IRS asked for competitive quotes for a new web-based system. The winner
began work on the new system in 2001 with an expected completion date in late 2005, just in time for
processingthe2006taxreturns.
However, as often happens, the project experienced delays. As a result, the system cutover was
rescheduled to January, 2006, and then to February. Though there were numerous warning flags, the
contractor repeatedly voiced confidence that a February, 2006, cutover would be achieved. Based on
these assurances,the IRS decided toshut down its currentsystem in late 2005 in anticipation of the new
system becoming available shortly in February. So confident were they in the new system that a
contingencyplantorecoverfromafailedcutoverwasneverevencreated.
The ax fell when a March, 2006, test showed that the new system could not even process a day’s worth
ofdatainaday.Ineffect,itcouldnotkeepupwiththeworkloadandwouldnever work!Andtherewas no
fallback plan – the old system was gone. The result was that the IRS paid an estimated $300,000,000 or
moreinfraudulentorimproperincometaxrefundsfortaxreturnsfiledin2005.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

PayPal Upgrades with No Fallback Plan
PayPal provides payment processing services for online merchants, auction sites,
and others. Now owned by auction giant eBay, PayPal processes over $50 billion
USD per year and services 200 million accounts. It is used in 190 countries and
supports19currencies.
Clearly, agreatdeal of today’s ecommerceflows through PayPal.Its services mustbeextremelyreliable,
as billions of dollars of revenue for millions of small online merchants depend upon it. An extended
outage could put many small merchants out of business. That is what happened when critical PayPal
serviceswentdown–notforhoursbutforweeks.
The problem occurred when PayPal attempted to upgrade its Instant Payment Notification system with
no rollback plans in the event of an upgrade problem. Suddenly, many ecommerce customers could not
process orders. PayPal accepted orders from buyers with no problem and extracted its fees. Upon
completion of each transaction, sellers expected from PayPal an Instant Payment Notification message,
which would allow them to process the order. Instead, they received an “invalid order” message.
Consequently,merchantscouldnotprocesstheirorders.
This left a situation in which buyers were told that their order was accepted and that their money was
removed from their account. However, they never received the goods they ordered. Not only were
merchants denied the revenues upon which they depended, but they also were swamped with negative
complaintsbybuyers,complaintsthatsanktheirratingsoneBayandothersites.
Clearly, the upgrade had gone wrong; but PayPal had not prepared a fallback plan. It took them two
weekstogetmerchantservicesonceagainoperational.
BlackBerry – OMG, It’s Déjà Vu!
From April, 2007, to December, 2009, the Blackberry service (provided by
Research in Motion, RIM) suffered four multi-hour and in some cases multiday
outages,allcausedbyfailedupgrades.
April,2007– Blackberryservicesufferedatwo-dayoutage, and ittook another day
to clear up the backlog of messages before service returned to normal. Blackberry
reported thattheoutage was causedbythe“introduction of anew noncriticalsystem routine” designedto
optimizecacheperformance.Sincewhenismessingaroundwithcachenoncritical?
February, 2008 – Half of all North American subscribers suddenly found their email screens empty. This
outage was caused byan upgrade to RIM’s routing system. For redundancy purposes, RIM provides two
IP networks for its North American service. RIM clients are split between these paths. The upgrade took
down one path, taking out half of the North American subscribers. It seems that there was no way to
switchthesesubscriberstothe“redundant”path.
December 2009 – RIM issued an upgrade to its BlackBerry Messenger instant messaging service and
encouraged all subscribers to download it. A few days later, the upgrade caused BlackBerry to suffer a
major outage that took down email, Internet browsing, and instant messaging across North and South
America. It was hours before service was restored. RIM released a new upgrade a few days later and
directeditssubscriberstodownloadthisupgrade.
December, 2009 – Just a week later, it became apparent that the new upgrade was also faulty. Users in
the Americas and Asia Pacific reported problems. RIM immediately issued a third upgrade, which
fortunatelycorrectedtheproblem.However,ittookfromTuesdaytolateThursday(ChristmasEve)before
email was freely flowing again. RIM explained that the problem stemmed from the two updates having a
flawthatcausedanunanticipateddatabaseissue.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Even Mighty Google is Not Immune
SimilartotheBlackberryservice,Googlehassufferedmanymajoroutagesduetoupgradefailures.
October, 2008 – Following a major upgrade to iGoogle, Google decided
unilaterally and without prior warning to update its Google Apps portal pages to
look more like its iGoogle personalized home pages. Suddenly, links were
broken, buttons were misconfigured, and strange “gadgets” caused confusion,
preventing access to many Google Apps services. It took days for Google to
correcttheproblems.
February,2009–Gmailwasdownaroundthe worldfortwo-and-a-halfhours,earningGmailtheinfamous
nickname of “Gfail.” The cause was a new feature that it had installed to keep email geographicallyclose
to its owners. In preparation for the update at one of its European data centers, Google routed users to
another nearby data center. This inadvertently overloaded that data center, which caused a cascading
effectfromonedatacentertoanother,ultimatelytakingdownallofGmailservices.
September,2009–Toperform arouter upgrade,Googlestaff took downseveral Gmailrouters.Whatthe
staff had underestimated was the additional load that this would put on the remaining routers. The
overloaded routers rejected traffic that they could not carry, and this traffic was rerouted through other
routers that then became overloaded. Within minutes, all of the routers in the Gmail network were
overloaded;andGmailcrashed.
Summary
An upgrade to any data-center component is a complex operation. It should be properly planned, and all
cognizant personnel should be available during the upgrade. There is too much of a chance that
regardlessoftheeffortputintotheupgradeplan,somethingwillgowrong.
If something does go wrong, there had better be a fallback plan of some sort that will allow operations to
continue whiletheupgrade is corrected.Fallback plans comeforfreeinactive/activenetworks.Theother
nodes in the application network are currently operational and are handling the transaction load. No
action has to be taken to ensure continued operations. In an active/backup configuration, the alternate
system perhaps can be put into service. If there is no redundancy, the fallback plan typically involves
backingouttheupgradeandreturningtotheoriginalsystem.
Sofar inthis series, we havefocusedontechnicalfailures.But adisturbinglylargenumber offailures are
caused by human actions, whether accidental or malicious. In fact, human error played a role in many of
the failures that we have described in these articles. In our next article, we will look at some spectacular
data-centerfailuresthatwerecausedbypeople.
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com