during the high-impact commands evaluation for this router model and,
therefore, had not been added to the block list.

Azure Networking implements a defense-in-depth approach to maintenance
operations which allows access to only one device at a time to ensure
that any change has limited impact. In this instance, even though the
engineer only had access to a single router, it was still connected to
the rest of the Microsoft WAN via the IGP protocol. Therefore, the
change resulted in two cascading events. First, routers within the
Microsoft global network started recomputing IP connectivity throughout
the entire internal network. Second, because of the first event, BGP
routers started to readvertise and validate prefixes that we receive
from the Internet. Due to the scale of the network, it took
approximately 1 hour and 40 minutes for the network to restore
connectivity to every prefix.

Issues in the WAN were detected by monitoring and alerts to the on-call
engineers were generated within 5 minutes of the command being
run. However, the engineer making changes was not informed due to the
unqualified changes to the SOP. Due to this, the same operation was
performed again on the second Madrid router 33 mins after the first
change, thus creating two waves of connectivity issues throughout the
network impacting Microsoft customers.

This event caused widespread routing instability affecting Microsoft
customers and their traffic flows: to/from the Internet, Inter-Region
traffic, Cross-premises traffic via ExpressRoute or VPN/vWAN and US Gov
Cloud services using commercial/public cloud services. During the time
it took for routing to automatically converge, customer impact
dynamically changed as the network completed its convergence. Some
customers experienced intermittent connectivity, some saw connections
timeout, and others experienced long latency or in some cases even a
complete loss of connectivity.

**How did we respond?**

Our monitoring detected DNS and WAN issues starting at 07:11 UTC. We
began investigating by reviewing all recent changes. By 08:20 UTC, as
the automatic recovery was happening, we identified the problematic
command that triggered the issue. Networking telemetry shows that nearly
all network devices had recovered by 09:05 UTC, by which point most
regions and services had recovered. Final networking equipment recovered
by 09:25 UTC.

After routing in the WAN fully converged and recovered, there was still
above normal packet loss in localized parts of the network. During this
event, our automated systems for maintaining the health of the WAN were
paused, including the systems for identifying and removing unhealthy
devices and the traffic engineering system for optimizing the flow of
data across the network. Due to the pause in these systems, some paths
in the network were not fully optimized and, therefore, experienced
increased packet loss from 09:25 UTC until those systems were manually
restarted, restoring the WAN to optimal operating conditions. The
recovery was ultimately completed at 12:43 UTC and explains why
customers in different geographies experienced different recovery
times. The long poles were traffic traversing our regions in India and
parts of North America.

**How are we making incidents like this less likely and less
impactful?**

Two main factors contributed to the incident:

1.  A change was made to a standard operating procedure that was not
    properly revalidated and left the procedure containing an error and
    without proper pre- and post- checks.
2.  A standard command that has different behaviors on different router
    models was issued outside of standard procedure that caused all WAN
    routers in the IGP domain to recompute reachability.

As such, our repair items include the following:

-   Audit and block similar commands that can have widespread impact
    across all three vendors for all WAN router roles (Estimated
    completion: February 2023).
-   Publish real-time visibility of approved-automated and
    approved-break glass, as well as unqualified device activity, to
    enable on-call engineers to see who is making what changes on
    network devices. (Estimated completion: February 2023).
-   Continued process improvement by implementing regular, ongoing
    mandatory operational training and attestation of following all
    SOPs. (Estimated completion: February 2023).
-   Audit of all SOPs still pending qualification will immediately be
    prioritized for a Change Advisory Board (CAB) review within 30 days,
    including engineer feedback to the viability and usability of the
    SOP. (Estimated completion: April 2023).

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://aka.ms/AzPIR/VSG1-B90](https://aka.ms/AzPIR/VSG1-B90)

## 23 

[01/23/2023]

Post Incident Review (PIR) - Intermittent networking issues - South
Central US

Tracking ID: 7NBR-T98


**What happened?**

Between 15:39 UTC and 19:38 UTC on 23 January 2023, a subset of
customers in the South Central US region may have experienced increased
latency and/or intermittent connectivity issues while accessing services
hosted in the region. Downstream services that were impacted by this
intermittent networking issue included Azure App Services, Azure Cosmos
DB, Azure IoT Hub, and Azure SQL DB.

**What went wrong and why?**

The Regional Network Gateway (RNG) in the South Central US region serves
network traffic between Availability Zones, which includes datacenters
in South Central US and network traffic in and out of the region. During
this incident, a single router in the RNG experienced a hardware fault,
causing a fraction of network packets to be dropped. Customers may have
experienced intermittent connectivity errors and/or error notifications
when accessing resources hosted in this region. 

**How did we respond?**

The Azure network design includes extensive redundancy such that, when a
router fails, only a small fraction of the traffic through the network
is impacted - and automated mitigation systems can restore full
functionality by removing failed any routers from service. In this
incident, there was a delay with our automated systems in detecting the
hardware failure on the router, as there was no previously known
signature for this specific hardware fault. Our synthetic probe-based
monitoring fired an alert at 17:58 UTC, which helped to narrow down the
potential causal location to the RNG. After further investigation we
were able to pinpoint the offending router, but it took additional time
to isolate because the failure made the router inaccessible via its
management interfaces. The unhealthy router was isolated and removed
from service at 19:38 UTC, which mitigated the incident.** **

**How are we making incidents like this less likely or less impactful?**

-   We are implementing improvements to our standard operating
    procedures for this class of incident to help mitigate similar
    issues more quickly (Estimated completion: February 2023).
-   We are implementing additional automated mitigation mechanisms to
    help identify and isolate such unhealthy routers more quickly in the
    future (Estimated completion: May 2023).
-   We are still investigating the cause of the hardware fault and are
    collaborating weekly with the hardware vendor to diagnose this
    unknown OS/hardware failure and obtain deterministic repair actions.

**How can we make our incident communications more
useful?**[ ]{style="color: rgb(0, 0, 0)"}

You can rate this PIR and provide any feedback using our quick
3-question
survey: [[https://aka.ms/AzPIR/7NBR-T98](https://aka.ms/AzPIR/7NBR-T98)]{style="background-color: transparent; color: rgb(5, 99, 193)"}

## 18 

[01/18/2023]

Post Incident Review (PIR) -- Single zone power event -- West Europe

Tracking ID: 6S_Q-JT8


**What happened?**

Between 09:44 and 13:10 UTC on 18 January 2023, a subset of customers
using Storage services in West Europe may have experienced higher than
expected latency, timeouts or HTTP 500 errors when accessing data stored
on Storage accounts hosted in this region. Other Azure services with
dependencies on this specific storage infrastructure may also have
experienced impact -- including Azure Application Insights, Azure
Automation, Azure Container Registry, Azure Database for MySQL, Azure
Database for PostgreSQL, Azure Red Hat OpenShift, Azure Search, Azure
SQL Database, and Azure Virtual Machines (VMs).

**What went wrong and why?**

We determined that an issue occurred during a planned power maintenance.
While all server racks have redundant dual feeds, one feed was powered
down for maintenance, and a failure in the redundant feed caused a
shutdown of the affected racks. This unexpected event was caused by a
failure in the electrical systems feeding the affected racks. Two Power
Distribution Unit (PDU) breakers tripped, which were feeding the
impacted racks. The breakers had lower than designed trip values of
160A, versus the 380A to which they should have been set. Our
investigation determined that this lower value was a remnant from
previous heat load tests, which should have been aligned back to the
site design after testing had completed. This led to an overload event
on the breakers once power had been removed from the secondary feeds for
maintenance. This caused an incident for a subset of storage and
networking infrastructure in one datacenter of one Availability Zone in
West Europe. This impacted storage tenants, and network devices which
may have rebooted.

**How did we respond?**

The issue was detected by the datacenter operation team performing the
maintenance at the time. We immediately initiated the maintenance
rollback procedure, and restored power to the affected racks.
Concurrently, we escalated the incident and engaged other Azure service
stakeholders to initiate/validate service recovery. Most impacted
resources automatically recovered following the power event, through
automated recovery processes.

The storage team identified two storage scale units that did not come
back online automatically -- nodes were not booting properly, as network
connectivity was still unavailable. Networking teams were engaged to
investigate, and identified a Border Gateway Protocol (BGP) issue. BGP
is the standard routing protocol used to exchange routing and
reachability information between networks. Since BGP functionality did
not recover automatically, 3 of the 20 impacted top-of-rack (ToR)
networking switches stayed unavailable. Networking engineers restored
the BGP session manually. One storage scale unit was fully recovered by
10:00 UTC, the other storage scale unit was fully recovered by 13:10
UTC.

**How are we making incidents like this less likely or less impactful?**

-   We have reviewed (and corrected where necessary) all PDU breakers
    within the facility to align to site design (Completed).
-   We are ensuring that the Operating Procedures at all datacenter
    sites are updated to pre-check breaker trip values prior to all
    maintenance in the future (Estimated completion: February 2023).
-   We are conducting a full review of our commissioning procedures
    around heat load testing, to ensure that systems are aligned to site
    design, after any heat load tests (Estimated completion: February
    2023).
-   In the longer term, we are exploring ways to improve our networking
    hardware automation, to differentiate between hardware failure and
    power failure scenarios, to ensure a more seamless recovery during
    this class of incident (Estimated completion: September 2023).

**How can customers make incidents like this less impactful?**

-   Consider using Availability Zones (AZs) to run your services across
    physically separate locations within an Azure region. To help
    services be more resilient to datacenter-level failures like this
    one, each AZ provides independent power, networking, and cooling.
    Many Azure services support zonal, zone-redundant, and/or
    always-available configurations:
    [https://docs.microsoft.com/azure/availability-zones/az-overview](https://docs.microsoft.com/azure/availability-zones/az-overview)
-   Consider which are the right Storage redundancy options for your
    critical applications. Zone redundant storage (ZRS) remains
    available throughout a zone localized failure, like in this
    incident. Geo-redundant storage (GRS) enables account level failover
    in case the primary region endpoint becomes unavailable:
    [https://docs.microsoft.com/azure/storage/common/storage-redundancy](https://docs.microsoft.com/azure/storage/common/storage-redundancy)
-   Consider using Azure Chaos Studio to recreate the symptoms of this
    incident as part of a chaos experiment, to validate the resilience
    of your Azure applications. Our library of faults includes VM
    shutdown, network block, and AKS faults that can help to recreate
    some of the connection difficulties experienced during this incident
    -- for example, by targeting all resources within a single
    Availability Zone:
    [https://docs.microsoft.com/azure/chaos-studio](https://docs.microsoft.com/azure/chaos-studio)
-   More generally, consider evaluating the reliability of your
    applications using guidance from the Azure Well-Architected
    Framework and its interactive Well-Architected Review:
    [https://docs.microsoft.com/azure/architecture/framework/resiliency](https://docs.microsoft.com/azure/architecture/framework/resiliency)
-   Finally, consider ensuring that the right people in your
    organization will be notified about any future service issues - by
    configuring Azure Service Health alerts. These can trigger emails,
    SMS, push notifications, webhooks, and more:
    [https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question survey:
[https://www.aka.ms/AzPIR/6S_Q-JT8](https://www.aka.ms/AzPIR/6S_Q-JT8)

## November 2022

## 2 

[11/02/2022]

Post Incident Review (PIR) - Enrolling new certificates / Provisioning
new resources - Azure Public / Government / China

Tracking ID: YTGZ-1Z8


**What happened?**

Between 00:42 UTC on 2 November and 05:55 UTC on 3 November 2022,
customers experienced failures when attempting to provision new
resources in a subset of Azure services including Application
Gateway, Bastion, Container Apps, Database Services (MySQL - Flexible
Server, Postgres - Flexible Server, and others), ExpressRoute,
HDInsight, Open AI, SQL Managed Instance, Stream Analytics, VMware
Solution, and VPN Gateway. Provisioning new resources in these services
requires creating new certificates. The certificate Registration
Authority (RA) that processes new certificate requests experienced a
service degradation, which led to an incident preventing provisioning
new resources in this subset of Azure services. We're providing you with
this Post Incident Review (PIR) to summarize what went wrong, how we
responded, and the steps Microsoft is taking to learn and improve.
Communications for this incident were provided under Tracking IDs
YTGZ-1Z8 (Azure Public), 7LHZ-1S0 (Azure Government) and YTHP-180 (Azure
China).

**What went wrong, and why?**

At 23:56 UTC on 1 November through 00:52 UTC on 2 November, an internal
Certificate Authority (CA) experienced a brief service degradation. At
the same time, the Registration Authority (RA) that sends requests to
the CA received a burst of certificate renewal requests, resulting in
requests queueing at the RA. Once the CA recovered, the request queue
started processing. Due to a latent performance bug in the RA,
inadvertently introduced in the past month as part of a feature
enhancement, the rate of new incoming requests was greater than the rate
at which requests could be processed from the queue. This triggered
automatic throttling of incoming requests, but the RA was unable to
recover fully due to throttled requests causing additional retry
traffic. The latent performance bug was not caught during deployment or
through health monitoring, due to a test gap where this specific set of
conditions was not exercised. 

**How did we respond?** 

The issue was detected via our internal monitoring and the relevant
engineering team was engaged within one minute of the alert firing. The
service was unable to self-heal, so the following steps were taken to
mitigate the incident. Firstly, we rolled back the change containing the
performance bug. Secondly, we blocked requests to the RA, to enable the
queue to drain. Finally, once the queue was at a manageable length, we
re-enabled traffic slowly - and monitored service recovery until all
