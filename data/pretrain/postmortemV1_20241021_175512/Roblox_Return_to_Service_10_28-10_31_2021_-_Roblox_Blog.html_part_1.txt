# Roblox Return to Service 10/28-10/31 2021

January 20, 2022

by Daniel Sturman in collaboration with Scott Delap, Max Ross, & many
others from Roblox and HashiCorp.

Product &
Tech](https://blog.roblox.com/category/product-tech/){.btn-w-border}

Starting October 28th and fully resolving on October 31st, Roblox
experienced a 73-hour outage.¹ Fifty
million players regularly use Roblox every day and, to create the
experience our players expect, our scale involves hundreds of internal
online services. As with any large-scale service, we have service
interruptions from time to time, but the extended length of this outage
makes it particularly noteworthy. We sincerely apologize to our
community for the downtime.

We're sharing these technical details to give our community an
understanding of the root cause of the problem, how we addressed it, and
what we are doing to prevent similar issues from happening in the
future. We would like to reiterate there was no user data loss or access
by unauthorized parties of any information during the
incident.

Roblox Engineering and technical staff from HashiCorp combined efforts
to return Roblox to service. We want to acknowledge the HashiCorp team,
who brought on board incredible resources and worked with us tirelessly
until the issues were resolved.

## Outage Summary

The outage was unique in both duration and complexity. The team had to
address a number of challenges in sequence to understand the root cause
and  bring the service back up.

-   The outage lasted 73 hours.
-   The root cause was due to two issues. Enabling a relatively new
    streaming feature on Consul under unusually high read and write load
    led to excessive contention and poor performance. In addition, our
    particular load conditions triggered a pathological performance
    issue in BoltDB. The open source BoltDB system is used within Consul
    to manage write-ahead-logs for leader election and data
    replication. 
-   A single Consul cluster supporting multiple workloads exacerbated
    the impact of these issues.
-   Challenges in diagnosing these two primarily unrelated issues
    buried deep in the Consul implementation were largely responsible
    for the extended downtime. 
-   Critical monitoring systems that would have provided better
    visibility into the cause of the outage relied on affected systems,
    such as Consul. This combination severely hampered the triage
    process.
-   We were thoughtful and careful in our approach to bringing Roblox
    up from an extended fully-down state, which also took notable
    time.
-   We have accelerated engineering efforts to improve our monitoring,
    remove circular dependencies in our observability stack, as well as
    accelerating our bootstrapping process. 
-   We are working to move to multiple availability zones and data
    centers.
-   We are remediating the issues in Consul that were the root cause of
    this event.

## Preamble: Our Cluster Environment and HashiStack

Roblox's core infrastructure runs in Roblox data centers. We deploy and
manage our own hardware, as well as our own compute, storage, and
networking systems on top of that hardware. The scale of our deployment
is significant, with over 18,000 servers and 170,000
containers.

In order to run thousands of servers across multiple sites, we leverage
a technology suite commonly known as the
"[HashiStack](https://www.hashicorp.com/resources/how-we-used-the-hashistack-to-transform-the-world-of-roblox)." **Nomad**,
**Consul** and **Vault** are the technologies that we use
to manage servers and services around the world, and that allow us to
orchestrate containers that support Roblox services.

Nomad [is used for scheduling work. It
decides which containers are going to run on which nodes and on which
ports they're accessible. It also validates container health. All of
this data is relayed to a Service Registry, which is a database of
IP:Port combinations. Roblox services use the Service Registry to find
one another so they can communicate. This process is called "service
discovery." We use **Consul** for service
discovery, health checks, session locking
(for HA systems built on-top), and as a KV
store.

Consul is deployed as a cluster of machines in two roles. "Voters" (5
machines) authoritatively hold the state of the cluster; "Non-voters" (5
additional machines) are read-only replicas that assist with scaling
read requests. At any given time, one of the voters is elected by the
cluster as leader. The leader is responsible for replicating data to the
other voters and determining if written data has been fully committed. 
Consul uses an algorithm called [Raft](https://raft.github.io/) for leader election and [to
distribute state across the cluster in a way that ensures each node in
the cluster agrees upon the updates.  It is not uncommon for the leader
to change via leader election several times throughout a given
day.

The following is a recent screenshot of a Consul dashboard at Roblox
after the incident.  Many of the key operational metrics referenced in
this blog post are shown at normal levels.  KV Apply time for instance
is considered normal at less than 300ms and is 30.6ms in this moment.
The Consul leader has had contact with other servers in the cluster in
the last 32ms, which is very recent.

![](./Roblox%20Return%20to%20Service%2010_28-10_31%202021%20-%20Roblox%20Blog_files/1-normal-consul-a.jpg)

1\. Normal Operations of the Consul at Roblox

In the months leading up to the October incident, Roblox upgraded from
Consul 1.9 to [Consul
1.10](https://learn.hashicorp.com/tutorials/consul/1-10?in=consul/new-release) to take advantage of [a
new streaming
feature](https://medium.com/criteo-engineering/consul-streaming-whats-behind-it-6f44f77a5175). This streaming feature is designed to significantly
reduce the CPU and network bandwidth needed to distribute updates across
large-scale clusters like the one at Roblox.

## Initial Detection (10/28 13:37)

On the afternoon of October 28th, V[ault
performance was degraded and a single Consul server had high CPU
loa[d. Roblox engineers began to
investigate. At this point players were not
impacted.

## Early Triage (10/28 13:37 -- 10/29 02:00)

The initial investigation suggested that the Consul cluster that Vault
and many other services depend on was unhealthy.  Specifically, the
Consul cluster metrics showed elevated write latency for the underlying
KV store in which Consul stores data. The 50th percentile latency on
these operations was typically under 300ms but was now 2 seconds.
Hardware issues are not  unusual at Roblox's scale, and Consul can
survive hardware failure. However, if hardware is merely slow rather
than failing, it can impact overall Consul performance. In this case,
the team suspected degraded hardware performance as the root cause and
began the process of replacing one of the Consul cluster nodes. This was
our first attempt at diagnosing the
incident**.** Around this time, staff from
HashiCorp joined Roblox engineers to help with diagnosis and
remediation. All references to "the team" and "the engineering team"
from this point forward refer to both Roblox and HashiCorp
staff.

Even with new hardware, Consul cluster performance continued to suffer.
At 16:35, the number of online players dropped to 50% of
normal.

![](./Roblox%20Return%20to%20Service%2010_28-10_31%202021%20-%20Roblox%20Blog_files/2-player-drop.png)

2\. CCU during the 16:35 PST Player Drop

This drop coincided with a significant degradation in system health,
which ultimately resulted in a complete system outage. Why? When a
Roblox service wants to talk to another service, it relies on Consul to
have up-to-date knowledge of the location of the service it wants to
talk to. However, if Consul is unhealthy, servers struggle to connect.
Furthermore, Nomad and Vault rely on Consul, so when Consul is
unhealthy, the system cannot schedule new containers or retrieve
production secrets used for authentication. In short, the system failed
because Consul was a single point of failure, and Consul was not
healthy.

At this point, the team developed a new theory about what was going
wrong: increased traffic. Perhaps Consul was slow because our system
reached a tipping point, and the servers on which Consul was running
could no longer handle the load? This was our second attempt at
diagnosing the root cause of the incident.

Given the severity of the incident, the team decided to replace all the
nodes in the Consul cluster with new, more powerful machines. These new
machines had 128 cores (a 2x increase) and newer, faster NVME SSD disks.
By 19:00, the team migrated most of the cluster to the new machines but
the cluster was still not healthy. The cluster was reporting that a
majority of nodes were not able to keep up with writes, and the 50th
percentile latency on KV writes was still around 2 seconds rather than
the typical 300ms or less. 

## Return to Service Attempt #1 (10/29 02:00 -- 04:00)

The first two attempts to return the Consul cluster to a healthy state
were unsuccessful. We could still see elevated KV write latency as well
as a new inexplicable symptom that we could not explain: the Consul
leader was regularly out of sync with the other
voters. 

The team decided to shut down the entire Consul cluster and reset its
state using a snapshot from a few hours before -- the beginning of the
outage. We understood that this would potentially incur a small amount
of system config data loss (**not** user
data loss). Given the severity of the outage and our confidence that we
could restore this system config data by hand if needed, we felt this
was acceptable. 

We expected that restoring from a snapshot taken when the system was
healthy would bring the cluster into a healthy state, but we had one
additional concern. Even though Roblox did not have any user-generated
traffic flowing through the system at this point, internal Roblox
services were still live and dutifully reaching out to Consul
to learn the location of their dependencies
and to update their health information.
These reads and writes were generating a significant load on the
cluster. We were worried that this load might immediately push the
cluster back into an unhealthy state even if the cluster reset was
successful. To address this concern, we configured iptables on
the cluster to block access. This would allow us to bring the cluster
back up in a controlled way and help us understand if the load we were
putting on Consul independent of user traffic was part of the
problem.

The reset went smoothly, and initially, the metrics looked good. When
we removed the iptables
block, the service discovery and health check load from the internal
services returned as expected. However, Consul performance began to
degrade again, and eventually we were back to where we started: 50th
percentile on KV write operations was back at 2 seconds. Services that
depended on Consul were starting to mark themselves "unhealthy," and
eventually, the system fell back into the now-familiar problematic
state. It was now 04:00. There was clearly something about our load on
Consul that was causing problems, and over 14 hours into the incident,
we still didn't know what it was.

## Return to Service Attempt #2  (10/29 04:00 -- 10/30 02:00)

We had ruled out hardware failure. Faster hardware hadn't helped and,
as we learned later, potentially hurt stability. Resetting Consul's
internal state hadn't helped either. There was no user traffic coming
in, yet Consul was still slow. We had leveraged iptables to let traffic back into the
cluster slowly. Was the cluster simply getting pushed back into an
unhealthy state by the sheer volume of thousands of containers trying to
reconnect? This was our third attempt at diagnosing the root cause of
the incident**.**

The engineering team decided to reduce Consul usage and then carefully
and systematically reintroduce it. To ensure we had a clean starting
point, we also blocked remaining external traffic. We assembled an
exhaustive list of services that use Consul and rolled out config
changes to disable all non-essential usage. This process took several
hours due to the wide variety of systems and config change types
targeted. Roblox services that typically had hundreds of instances
running were scaled down to single digits. Health check frequency was
decreased from 60 seconds to 10 minutes to give the cluster additional
breathing room. At 16:00 on Oct 29th, over 24 hours after the start of
the outage, the team began its second attempt to bring Roblox back
online. Once again, the initial phase of this restart attempt looked
good, but by 02:00 on Oct 30th, Consul was again in an unhealthy state,
this time with significantly less load from the Roblox services that
depend on it.

At this point, it was clear that overall Consul usage was not the only
contributing factor to the performance degradation that we first noticed
on the 28th. Given this realization, the team again pivoted. Instead of
looking at Consul from the perspective of the Roblox services that
depend on it, the team started looking at Consul internals for
clues.

## Research Into Contention (10/30 02:00 -- 10/30 12:00)

Over the next 10 hours, the engineering team dug deeper into debug logs
and operating system-level metrics.  This data showed Consul KV writes
getting blocked for long periods of time. In other words,
"contention."The cause of the contention was not immediately obvious,
but one theory was that the shift from 64 to 128 CPU Core servers early
in the outage might have made the problem worse. After looking at the
htop data and performance debugging data shown in the screenshots below,
the team concluded that it was worth going back to 64 Core servers
similar to those used before the outage. The team began to prep the
hardware: Consul was installed, operating system configurations triple
checked, and the machines readied for service in as detailed a manner as
possible. The team then transitioned the Consul cluster back to 64 CPU
Core servers, but this change did not help. This was our fourth attempt
at diagnosing the root cause of the
incident.

![](./Roblox%20Return%20to%20Service%2010_28-10_31%202021%20-%20Roblox%20Blog_files/3-perf-report.png)

3\. We then displayed this with a perf report as shown above. The
majority of time was spent in kernel spin locks via the Streaming
subscription code path.

![](./Roblox%20Return%20to%20Service%2010_28-10_31%202021%20-%20Roblox%20Blog_files/4-htop.png)

4\. HTOP showing CPU Usage across 128 cores.

## Root Causes Found (10/30 12:00 -- 10/30 20:00)

Several months ago, we enabled a new Consul streaming feature on a
subset of our services.  This feature, designed to lower the CPU usage
and network bandwidth of the Consul cluster, worked as expected, so over
the next few months we incrementally enabled the feature on more of our
backend services. On October 27th at 14:00, one day before the outage,
we enabled this feature on a backend service that is responsible for
traffic routing. As part of this rollout, in order to prepare for the
increased traffic we typically see at the end of the year, we also
increased the number of nodes supporting traffic routing by 50%.  The
system had worked well with streaming at this level for a day before the
incident started, so it wasn't initially clear why it's performance had
changed. However through analysis of perf reports and flame graphs from
Consul servers, we saw evidence of streaming code paths being
responsible for the contention causing high CPU usage. We disabled the
streaming feature for all Consul systems, including the traffic routing
nodes. The config change finished propagating at 15:51, at which time
the 50th percentile for Consul KV writes lowered to 300ms. We finally
had a breakthrough.

Why was streaming an issue?  HashiCorp explained that, while streaming
was overall more efficient, it used fewer concurrency control elements
(Go channels) in its implementation than long polling.  Under very high
load -- specifically, both a very high read load and a very high write
load -- the design of streaming exacerbates the amount of contention on
a single Go channel,  which causes blocking during writes, making it
significantly less efficient. This behavior also explained the effect of
higher core-count servers: those servers were dual socket architectures
with a NUMA memory model.  The additional contention on shared resources
thus got worse under this architecture.  By turning off streaming, we
dramatically improved the health of the Consul
cluster.

Despite the breakthrough, we were not yet out of the woods. We saw
Consul intermittently electing new cluster leaders, which was normal,
but we also saw some leaders exhibiting the same latency problems we saw
before we disabled streaming, which was not normal. Without any obvious
clues pointing to the root cause of the slow leader problem, and with
evidence that the cluster was healthy as long as certain servers were
not elected as the leaders, the team made the pragmatic decision to work
around the problem by preventing the problematic leaders from staying
elected. This enabled the team to focus on returning the Roblox services
that rely on Consul to a healthy state.

