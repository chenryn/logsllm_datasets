[ Initial failures were caused by a recent update to a workflow that
leveraged an instance of Service Bus as the communication backbone,
which was non-zonally resilient and was in the impacted zone. Failures
related to workflow dependencies on Service Bus were completely
mitigated by 04:45AM UTC on 8 February 2023 by disabling the newly added
update in the Southeast Asia region. ]{style="color: rgb(51, 51, 51)"}

[ Unfortunately, further failures continued for a few customers due to
the above ARM control plane issues that were ongoing. At 09:15 UTC on 8
February 2023, once the ARM control plane issues were mitigated, all
subsequent ASR failovers worked
reliably.]{style="color: rgb(51, 51, 51)"}

[ The ARM impact caused failures and delays in restoring applications
and data using Azure Backup cross region restore to East Asia. A portion
of Azure Backup customers in the Southeast Asia region had enabled the
cross region restore (CRR) capability which allows them restore their
application and data in the East Asia region during regional outages. At
09:00 UTC on the 08 Feb, we started proactively enabling CRR capability
for all Azure Backup customers with GRS Recovery Services Vaults (RSV).
This was a proactive mitigation action to enable customers had not
enabled this capability to recover their application and data in the
East Asia region. Observing customer activity, we detected a
configuration issue in one of the microservices specific to the
Southeast Asia region which caused CRR calls to fail for a subset of
customers. This issue was mitigated for this subset, through a
configuration update and the cross region application and data restores
were fully functioning by 15:10 UTC on 8 February
2023.]{style="color: rgb(51, 51, 51)"}

[ Most customers using Azure Storage with GRS were able to execute
customer controlled failovers without any issues. A subset of customers
encountered unexpected delays or failures. There were two underlying
causes for this. The first was an integrity check that the service
performs prior to a failover. In some instances, this check was too
aggressive and blocked the failover from completing. The second cause
was that storage accounts using hierarchical namespaces could not
complete a failover. ]{style="color: rgb(51, 51, 51)"}

**Extended Recovery for SQL Database**

[The impact to Azure SQL Database customers throughout this incident
would have fallen into the following categories.
]{style="color: rgb(36, 36, 36)"}

[Customers using Multi-AZ Azure SQL were not impacted, except those
using proxy mode connection due to one connectivity capacity unit not
being configured with zone-resilience.]{style="color: black"}

[Customer using active geo-replication with automatic failover were
failed out of region and failed back when recovery was complete. These
customers had \~1.5 hours of downtime prior to automatic failover
completing. ]{style="color: black"}

[Self-help geo-failover or geo-restore: Some customers who had active
geo-replication self-initiated failover, and some performed geo-restore
to other regions.  ]{style="color: black"}

[The rest of the impacted SQL Database customers were impacted from the
point of power down and recovered as compute infrastructure was brought
back online and Tenant Rings (TR) reformed automatically. One Tenant
Ring of capacity for SQL Database required manual intervention to
properly reform. ]{style="color: black"}[Azure SQL Database capacity in
a region is divided into Tenant rings. In Southeast Asia there are
hundreds of rings. Each ring consists of a group of VMs (10-200) hosting
a set of databases. Rings are managed by Azure Service Fabric (SF) to
provide availability when we have VM, network or storage failures. For
one of the TRs, when power was restored to the compute infrastructure
hosting the supporting group of VMs, the ring did not reform and thus
was unable to make any of the databases on that ring available.
Infrastructure hosting these VMs did not recover automatically due to a
BIOS bug. After the compute infrastructure was successfully started, a
second issue in heap memory management in SF also required manual
intervention to reform the TR and make customer databases available. The
final TR became available at 04:30 UTC on the 09 February
2023.]{style="color: rgb(36, 36, 36)"}

**How are we making incidents like this less likely or less impactful?**

[Firstly, we wanted to provide a status update of the review of
facilities readiness in response to the chiller
issues.]{style="color: rgb(51, 51, 51)"}

[Our datacenter team engaged with the OEM vendor to fully understand the
results and any additional follow-ups to prevent, shorten the duration,
and reliably recover after these types of events.
(Completed)]{style="color: rgb(51, 51, 51)"}

[We\'re also implementing updates to existing procedures to include
manual (cold) restarting of the chillers as recommended by the OEM
vendor. (Completed)]{style="color: rgb(51, 51, 51)"}

[We are also conducting additional training from the OEM to our
Datacenter operations personnel to be familiar with the updated
procedures. (Completed)]{style="color: rgb(51, 51, 51)"}

[When it comes to executing BCDR plans in the event of single AZ impact,
we discussed in the earlier PIR that the impact to ARM in multiple
regions inhibited customers\' ability to reliably self-mitigate. We will
complete a validation of ARM service configuration in the Southeast Asia
region to prevent this class of impact from repeating.
(Completed)]{style="color: rgb(51, 51, 51)"}

[We will also validate that all instances of Azure Resource Manager
service are automatically region and/or zone redundant.
(Completed)]{style="color: rgb(51, 51, 51)"}

[We will ensure that there is sufficient capacity for ARM in all regions
to manage additional traffic caused by zonal and/or regional failures.
(Completed)]{style="color: rgb(51, 51, 51)"}

[We will be conducting a thorough review of all Foundational services
for gaps in resiliency models. This work will be ongoing, however the
first round will be completed for ARM, Software Load Balancer, SQL DB,
Cosmos DB, and Redis Cache by April 2023. You can learn more about our
Foundational services here
(]{style="color: rgb(51, 51, 51)"}[https://learn.microsoft.com/en-us/azure/reliability/availability-service-by-category](https://learn.microsoft.com/en-us/azure/reliability/availability-service-by-category)[).]{style="color: rgb(51, 51, 51)"}

[We will be conducting a dependency assessment for all zonal services
with an automated daily walk of the dependency graph, documenting points
of failures and closing any gaps.
\[Ongoing\]]{style="color: rgb(51, 51, 51)"}

[BCDR critical services will take the following
remediations:]{style="color: rgb(51, 51, 51)"}

[We will verify that the configuration issue with Azure Backup cross
region restores will be corrected in the impacted region and across all
other regions. (Completed)]{style="color: rgb(51, 51, 51)"}

[We will verify that Azure Site Recovery will revert to a zonally
resilient workflow model. (Completed)]{style="color: rgb(51, 51, 51)"}

[We will be adjusting safety check thresholds for customer-initiated
Storage failover for GRS accounts to ensure failovers aren't
unnecessarily blocked. (Completed)]{style="color: rgb(51, 51, 51)"}

[Beyond VMSS, VMs, Service Fabric and Azure Kubernetes Services, Chaos
Studio will be expanding its scenarios so that customers and Azure
services can run further service simulations where it is possible to
inject faults that emulate zonal failures. (Dec
2023)]{style="color: rgb(51, 51, 51)"}

[We will ensure that our documentation represents the SQL behavior that
honors setting changes made by customers when it comes to the SQL
services failing back. More information can be found here
(]{style="color: rgb(51, 51, 51)"}[https://learn.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-sql-db](https://learn.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-sql-db)[).]{style="color: rgb(51, 51, 51)"}
[(Completed)]{style="color: rgb(51, 51, 51)"}

[Our communications aim to provide insights into mitigation workstreams
for incidents. We understand that for some scenarios this information is
critical for customers to make decisions about invoking BCDR plans. We
will strive to provide a recoverability expectation and timeline to
assist customers in making these decisions for all incidents moving
forward. (Completed)]{style="color: rgb(51, 51, 51)"}

We acknowledge that rapid correlation to impacted or potentially
impacted resources in customers\' subscriptions was difficult to
translate into customer impact and action plans. Work is planned to be
completed by December 2023 that addresses improvements in this ability.
The below article provides current steps for customers to understand AZ
logical mappings across multiple
subscriptions[ (]{style="color: rgb(51, 51, 51)"}[https://learn.microsoft.com/rest/api/resources/subscriptions/check-zone-peers](https://learn.microsoft.com/rest/api/resources/subscriptions/check-zone-peers)).

[If customers have questions about Compliance, Regulations & Standards
visit the Service Trust Portal
(]{style="color: rgb(51, 51, 51)"}[https://servicetrust.microsoft.com](https://servicetrust.microsoft.com)).

**How can we make our incident communications more useful?**

[You can rate this Post Incident Review (PIR) and provide any feedback
using our quick 3-question
survey ]{style="color: rgb(51, 51, 51)"}[https://aka.ms/AzPIR/VN11-JD8](https://aka.ms/AzPIR/VN11-JD8)

## January 2023

## 31 

[01/31/2023]

Post Incident Review (PIR) - Service management issues - East US 2

Tracking ID: BS81-390


**What happened?**

Between 05:55 UTC on 31 January 2023 and 00:58 UTC on 1 February 2023, a
subset of customers using Virtual Machines (VMs) in the East US 2 region
may have received error notifications when performing service management
operations -- such as create, delete, update, scale, start, stop -- for
resource hosted in this region.

The impact was limited to a subset of resources in one of the region's
three Availability Zones, (physical AZ-02) and there was no impact to
Virtual Machines in the other two zones. This incident did not cause
availability issues for any VMs already provisioned and running, across
any of the three Availability Zones (AZs) -- impact was limited to
service management operations described above.

Downstream Azure services with dependencies in this AZ were also
impacted, including Azure Backup, Azure Batch, Azure Data Factory, Azure
Database for MySQL, Azure Database for PostgreSQL, Azure Databricks, and
Azure Kubernetes Service.

**What went wrong and why?**

The East US 2 region is designed and implemented with three Availability
Zones. Each of these AZs are further internally sliced into partitions,
to ensure that a failure in one partition does not affect the processing
in other partitions. Every VM deployment request is processed by one of
the partitions in the AZ, and a gateway service is responsible for
routing traffic to the right partition. During this incident, one of the
partitions associated with physical AZ-02 experienced data access issues
at 05:55 UTC on 31 January 2023, because the underlying data store had
exhausted an internal resource limit.

While the investigation on the first partition was in progress, at
around 11:43 UTC a second partition within the same AZ experienced a
similar issue and became unavailable, leading to further impact. Even
though this partition became unavailable, the cache service was keeping
the partition data and so new deployments in the cluster were
succeeding. At 12:04 UTC, the cache service restarted and couldn\'t
retrieve the data, as the target partition was down. Due to a resource
creation policy configuration issue, all partitions were required to
create a new resource in the gateway service. When the cache service
didn't have the data, this policy resulted in blocking all new VM
creations in the AZ. This resulted in additional failures and slowness
during the impact timeline, causing failures to downstream services.

**How did we respond?**

Automated monitoring alerts were triggered, and engineers were
immediately engaged to assess the issue. As the issue was being
investigated, attempts to auto-recover the partition failed to succeed.
By 16:45 UTC, engineers had determined and started implementing
mitigation steps on the first partition.

At 17:49 UTC, the first partition was successively progressing with
recovery, and engineers decided to implement the recovery steps on the
second partition. During this time, the number of incoming requests for
VM operations continued to grow. To avoid further impact and failures to
additional partitions, at 19:15 UTC Availability Zone AZ-02 was taken
out of service for new VM creation requests. This ensured that all new
VM creation requests would succeed, as they were automatically
redirected to the other two Availability Zones.

By 23:45 UTC on 31 January 2023, both partitions had completed a
successful recovery. Engineers continued to monitor the system and, when
no further failures were recorded beyond 00:48 UTC on 1 February 2023,
the incident was declared mitigated and Availability Zone AZ-02 was
fully opened for new VM creations.

**How are we making incidents like this less likely or less impactful?**

-   As an immediate measure, we scanned all partitions across the three
    Availability Zones to successfully confirm that no other partitions
    were at risk of the same issue (Completed).
-   We have improved our telemetry to increase incident severity
    automatically if the failure rate increases beyond a set threshold
    (Completed).
-   We have also added automated preventive measures to avoid data store
    resource exhaustion issues from happening again (Completed).
-   We are working towards removing the cross-partition dependencies for
    VM creation (Estimated completion: April 2023).
-   We are adding a new capability to redirect deployments to other AZs
    based on known faults from a specific AZ (Estimated completion: May
    2023).

**How can we make our incident communications more useful?**

You can rate this PIR and provide any feedback using our quick
3-question
survey: [https://aka.ms/AzPIR/BS81-390](https://aka.ms/AzPIR/BS81-390)

## 25 

[01/25/2023]

Post Incident Review (PIR) -- Azure Networking -- Global WAN issues

Tracking ID: VSG1-B90


*Watch our \'Azure Incident Retrospective\' video about this incident:
[https://aka.ms/AIR/VSG1-B90](https://aka.ms/AIR/VSG1-B90)*

**What happened?**

Between 07:08 UTC and 12:43 UTC on 25 January 2023, customers
experienced issues with network connectivity, manifesting as long
network latency and/or timeouts when attempting to connect to resources
hosted in Azure regions, as well as other Microsoft services including
Microsoft 365 and Power Platform. This incident also impacted Azure
Government cloud services that were dependent on Azure public cloud.
While most regions and services had recovered by 09:05 UTC, intermittent
packet loss issues caused some customers to continue seeing connectivity
issues due to two routers not being able to recover automatically. All
issues were fully mitigated by 12:43 UTC.

**What went wrong and why?**

At 07:08 UTC a network engineer was performing an operational task to
add network capacity to the global Wide Area Network (WAN) in Madrid.
The task included steps to modify the IP address for each new router,
and integration into the IGP (Interior Gateway Protocol, a protocol used
for connecting all the routers within Microsoft's WAN) and BGP (Border
Gateway Protocol, a protocol used for distributing Internet routing
information into Microsoft's WAN) routing domains.

Microsoft's standard operating procedure (SOP) for this type of
operation follows a 4-step process that involves: \[1\] testing in our
Open Network Emulator (ONE) environment for change validation; \[2\]
testing in the lab environment; \[3\] a Safe-Fly Review documenting
steps 1 and 2, as well as a roll-out and roll-back plans; and \[4\]
Safe-Deployment which allows access to only one device at a time, to
limit impact. In this instance, the SOP was changed prior to the
scheduled event, to address issues experienced in previous executions of
the SOP. Critically, our process was not followed as the change was not
re-tested and did not include proper post-checks per steps 1-4 above.
This unqualified change led to a chain of events which culminated in the
widespread impact of this incident. This change added a command to purge
the IGP database -- however, the command operates differently based on
router manufacturer. Routers from two of our manufacturers limit
execution to the local router, while those from a third manufacturer
execute across all IGP joined routers, ordering them all to recompute
their IGP topology databases. While Microsoft has a real-time
Authentication, Authorization, and Accounting (AAA) system that must
approve each command run on each router, including a list of blocked
commands that have global impact, the command's different, global,
default action on the router platform being changed was not discovered
