activities of initial responders create a new situation that has its own
history. The incoming expert usually needs to review that history, e.g.:

-   What was happening during the time just before the anomaly appeared?
-   How did it present itself?
-   What investigations have been undertaken?
-   What were the results of attempts to correct the anomaly?

For work where immediate anomaly response is essential it can be
impossible for the expert to gain enough context so that her expertise
can be usefully employed. For these situations experts often stay
engaged with the work in order to keep their contextual appreciation
fresh ([Johannesen, 1994](http://snafucatchers.github.io/#ref_14)). This
requires effort and limits the scope of that expert\'s work on other
tasks but provides some assurance that the individual\'s expertise can
actually be brought to bear when a difficult anomaly occurs.

High expertise is not simply encyclopedic knowledge of technical
artifacts. The communications between problem solvers reveals that they
know a lot about the underlying technical processes where the anomaly
lies and also a lot about each other\'s expertise and capabilities. They
need less explicit coordination because they have formed expectations
about how the others will behave. Their verbal and written
communications are frequently terse, telegraphic, and pointed. Often the
exchanges are mainly focused on synchronization across workers for tasks
that need to interact or happen sequentially. The relative absence of
explicit talk about the tasks themselves is an indication of the high
level of sophistication of the workers. Analysis of these exchanges can
be revealing about the structure of coordinated work and indicates that
shared experience working in teams is particularly valuable ([Nemeth et
al. 2008](http://snafucatchers.github.io/#ref_26)).

### 4.3.3 Supporting communication and coordination with tools 

There are a host of tools that can be used to lower the costs of
coordination during an anomaly response. NASA\'s mission control uses
\"voice loops\" to share and separate communications during space
missions. These loops are essentially telephonic \"party lines\", each
dedicated to a particular function. Workers can listen simultaneously on
multiple \'lines\' and use what they hear to alert them to changes in
the situation, receive directions, and contribute information. \[Note:
voice loops are effective because they are carefully structured, because
the people participating on each loop are highly skilled at using them,
and for other reasons. See [Patterson et al., 1999]{herf="#ref_27"}, for
a description.\]

There is a tradition of research into coordination using advanced IT,
originally described as computer supported cooperative work (CSCW). As
with other areas, the pace of technological innovation has outpaced the
research results.^[9](http://snafucatchers.github.io/#foot_9)^ The use
of \"chat ops\" to manage anomaly response is well established. All of
the SNAFUcatchers partners use one or another type of online messaging
and all of them have channels dedicated to specific purposes, often
including a \"war room\" channel for coordination of important anomaly
response. Controlling the costs of coordination remains challenging,
especially when that coordination reaches across organizational
boundaries, when the anomaly persists over longer periods, or when
anomalies escalate and the resource commitments balloon. The trend
towards increasing dependencies on SaaS services and infrastructure
means that coordination across organization (and company) boundaries is
more frequently necessary.

A good deal of attention has been paid recently to the potential of
automation to augment human performance in anomaly response. All the
SNAFUcatchers partners have invested in automation that interacts with
their online messaging systems. The results of these experiments have
been mixed. It is far easier to imagine how automation could be useful
than it is to produce working automation that functions as a genuine
\"team player\" in anomaly response. More generally, during high tempo
times the usually trivial costs of coordination become significant.

Although tools are important, the issue of controlling the costs of
coordination is not fundamentally about tools. Indeed, the quick growth
of tooling in this area is symptomatic of a larger problem. We see lots
of channel proliferation and, simultaneously, attempts at control and
rationalization of channel assignments, memberships, and permissions.
This pattern recapitulates the structural tensions and diverse needs of
the organization. This is not a surprise; [Conway
(1968)](http://snafucatchers.github.io/#ref_3) points out that

> organizations which design systems\... are constrained to produce
> designs which are copies of the communication structures of these
> organizations.

This suggests that there is an opportunity for inquiry and experiment
directed to better understanding the nature of coordination and its
costs via experience with the tool. The payoff would be to make the
creation of a meaningful computer supported cooperative work (CSCW)
environment that bridges the gap that exists between technical tools and
organizational functions. Of particular interest, here is the way in
which communication tools recapitulate Conway\'s law through the
proliferation of channels, bots, and other paraphernalia. Like the
organizations that they represent, instances of tool application do not
appear to scale well and the management of the tools themselves presents
new challenges and provides new paths to failure.

During the STELLA meeting, several contributors noted that face-to-face
meetings have high coordination costs but that they are often critically
important when significant decisions are in the offing. Deciding on a
risky or expensive course of action, coping with the emotional nature of
severe anomalies, and gauging fatigue may be more reliable, efficient,
or nuanced with such meetings.

The use of tools to enhance anomaly response is an important area of
growth and development and ripe for research. Machine-generated
transcripts and logs of anomaly response can support timeline
construction and segmenting the cognitive process tracing. Several of
the events examined during the meeting included detailed, high
resolution data from \"chat ops\" sources. Adding instrumentation to
collect more detailed activity records and to make post-processing
easier may be useful.

## 4.4 Supporting anomaly response through improved visualizations 

There are important opportunities to improve anomaly response through
improving the representations of network behavior available to
responders. Understanding the ways in which anomaly responders cope with
complexity can lead to cognitive engineering of useful representations
for anomaly management. Much of the anomaly response revolves around
sensemaking, that is, examining and contrasting the many pieces of data
to extract meaningful patterns (see [Albolino et al.
2006](http://snafucatchers.github.io/#ref_1)). A great deal is known
about how to design visual representations that reveal patterns and help
users see the unexpected with a variety of successes in other industries
([Woods, 1984](http://snafucatchers.github.io/#ref_36);
[1995](http://snafucatchers.github.io/#ref_40)).

Business critical software presents a unique opportunity for innovative
visualizations that improve resilient performance. There are many signal
sources, finding what is relevant is difficult, and displays already
present and more are being introduced constantly. All of the data is
collected which means (a) data overload always is a threat to
sensemaking and (b) all of the 'data stuff' is available from which one
could to construct smart dynamic representations that highlight
unexpected behavior and the potential for cascading effects.

### 4.4.1 Understanding cognitive work in context is the starting point 

We have some idea of the cognitive work that responders perform when
anomalies arise. Research in multiple domains and spanning 30 years
informs the current inquiry into coping with complexity in information
technology intensive settings. Broadly, what we now understand is the
result of tracing responder work processes using a variety of
techniques. The STELLA meeting cases and our work with each of the
consortium partners has been guided by work in other domains. The
SNAFUcatchers project work confirms and expands those results. This work
provides the basis to begin the innovation and design process.

In this domain, workers are alerted to anomalies by monitoring or
reports of problems. The alerts draw attention but they are usually not,
in themselves, diagnostic. Instead, alerts trigger a complex process of
exploration and investigation that allows the responders to build a
provisional understanding of the source(s) of the anomalous behavior
that generated the alert. This provisional understanding then guides
further exploration and investigation that solidifies and refines the
understanding. Depending on the nature of the anomaly this cycle may
repeat several times before the sources of the anomaly are identified
([Woods and Hollnagel, 2006](http://snafucatchers.github.io/#ref_43)).

After the putative sources are identified the responders must devise and
consider the implications of and test one or more countermeasures. The
results of these considerations often add to the understanding of the
underlying problem; they lead the responders to test their mental models
in-cognate by running mental simulations (See [Klein
1999](http://snafucatchers.github.io/#ref_17), chapter 7).

Although some anomalies resolve themselves, many do not. The
interventions that responders make are experiments that test their
mental models of the anomaly sources and the surrounding system. The
responders typically study the results of their interventions to assure
themselves that their models are correct and that the interventions will
be successful. In many cases the early interventions are intended to
recover function quickly, leaving some corrective work for later. Often
the early interventions involve some sacrifice in function or temporary
change that, while undesirable in itself, is considered necessary to
achieve a more general
goal.^[10](http://snafucatchers.github.io/#foot_10)^

The type of challenge in anomaly management varies. Sometimes, as in the
[Logstash
SNAFU](http://snafucatchers.github.io/#3_3_Catching_the_Logstash_SNAFU),
the anomaly sources are exceedingly difficult to uncover. Other times,
as in the [Apache
SNAFU](http://snafucatchers.github.io/#3_1_Catching_the_Apache_SNAFU),
the anomaly source is immediately known but the countermeasures are
difficult to identify or hazardous to apply.

Representations that support the cognitive tasks in anomaly response are
likely to be quite different from those now used in \"monitoring\".
Current automation and monitoring tools are usually configured to gather
and represent data about anticipated problem areas. It is unanticipated
problems that tend to be the most vexing and difficult to manage.
Although representation design for cognitive aiding is challenging, it
is an area that is likely to be fruitful for this community. The
environment is ripe for innovation.

## 4.5 Strange loops dependencies 

As systems become more complex, strange loops emerge, where some part
that provides a function, also depends on the function it provides
([Hofstadter 2007](http://snafucatchers.github.io/#ref_12), p. 101).
This can remain unproblematic when systems function normally. Strange
loops produce difficulties when surprises occur and anomalies arise.
Managing, monitoring, modifying digital services depend on digital
services in the same network (cardiovascular system and nuclear power
plants also contain strange loops). All three cases were complicated by
strange loop dependencies. For example, assembling an understanding the
[Logstash SNAFU
(3.3)](http://snafucatchers.github.io/#3_3_Catching_the_Logstash_SNAFU)
depended on examining logs created by rsyslog but rsyslog was unable to
stream entries into the remote log because Logstash was processing new
messages so slowly that the kernel TCP/IP queue was nearly always full.
Issuing new console commands that would normally have produced
meaningful log entries instead slowed the system even further as those
commands generate traffic via snoopy directed towards rsyslog. There are
strange loop dependencies that contributed to the Travis CI SNAFU and
the Apache SNAFU as well.

Strange loop phenomena are common in modern computing with its elaborate
tool chains and complex dependencies. Sometimes a strange loop
complication can be anticipated. An example from another source:

> A site was constantly being revised and corrected according to the
> continuous deployment paradigm and using an automated process that had
> become reliable over several years of improvement. The site was
> changed as often as twenty times per day using this automation.
> Routine maintenance on the automation created a fault pathway which,
> if activated, would keep the deploy automation from upgrading the
> site. Because the viability of the site was assured by the constant
> attention it received and the capacity to immediately deploy a
> (corrective) change, this was regarded as a higher order emergency
> than a site outage.

The realization that the organization was so dependent on its deployment
automation came as a shock. Because the firm had invested so heavily in
deployment automation, monitoring, and the ability to quickly correct
faults, site failures had become \'ordinary\' events, in contrast to a
failure of the deployment automation which now took on an existential
character.

Discussion around the topic of strange loops was lively. All three cases
presented had complications from or central effects of strange loops
dependencies. There is also a troubling association between the strange
loop quality of the anomalies and failures of automation. What is clear
is (a) the complexity of business-critical software means strange loops
are present; and (b) strange loop dependencies make anomalies difficult
to resolve. What is not clear is how to manage the risks posed by
strange loop dependencies in business-critical software.

## 4.6 Dark Debt 

There was a wide-ranging discussion regarding decisions during
development and the liabilities they introduce. In addition to
\'technical debt\' another sort of liability, *dark debt*, was
suggested. This section reviews technical debt and proposes the notion
of dark debt.

## 4.6.1 Technical debt 

### Origins of the debt metaphor: 

In a 1992 \"Experience Report\", Cunningham suggested that software
development may incur future liability in order to achieve short-term
goals in this oft-quoted portion of an object-oriented programming
conference proceedings paper:

> \"Shipping first-time code is like going into debt. A little debt
> speeds development so long as it is paid back promptly with a rewrite.
> Objects make the cost of this transaction tolerable. The danger occurs
> when the debt is not repaid. Every minute spent on not-quite-right
> code counts as interest on that debt. Entire engineering organizations
> can be brought to a stand-still under the debt load of an
> unconsolidated implementation, object-oriented or otherwise.\"
> [Cunningham, 1992](http://snafucatchers.github.io/#ref_7)

The choice of \'debt\' as the metaphorical foundation was, according to
Cunningham, prompted partly because the system being developed, WyCash,
was for use by institutional investors who understood debt as a
technical management tool \-- one part of their portfolio of regular
methods of work.^[11](http://snafucatchers.github.io/#foot_11)^

The paper in which this suggestion appears was not about debt per se but
about the way that object-oriented programming was changing the way in
which big systems were developed:

> \"\...changing market demands often require massive revisions which we
> have been able to accommodate because of the modularity intrinsic in a
> totally object-oriented implementation. Our customers value our
> responsiveness as much as, if not more than, our product's fit to
> their current needs\... Mature sections of the program have been
> revised or rewritten many times\...\"

> \"\...key implementation ideas were slow to emerge \[during\]
> development\... \[a\] category of objects only surfaced through a
> process we could call Incremental Design Repair. We found these highly
> leveraged abstractions only because [we were willing to reconsider
> architectural decisions in the light of recent
> experience]{.underline}. \...\[P\]ure object oriented programming\...
> allowed us to include architectural revisions in the production
> program that would be judged too dangerous for inclusion under any
> other circumstance.\" \[emphasis added\]

