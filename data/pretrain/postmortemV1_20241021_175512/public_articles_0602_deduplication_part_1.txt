Data Deduplication
February2011
What is Data Deduplication?
Data deduplication is a technology that can reduce disk storage-capacity requirements and
replicationbandwidthrequirementsbyafactorof20:1.
This is certainlya very powerfulmarketing statement, and it is generallyaccurate. However, data
deduplication comes with a lot of ifs, ands, and buts. In this article, we explore what data
deduplicationisanditsmanystrengths,alongwithsomecaveats.
In simple terms, data deduplication is a method in which a specific block of data in a large
databaseisstoredonlyonce.Ifitappearsagain,onlyapointertothefirstoccurrenceoftheblock
is stored. Since pointers are verysmall compared to the data blocks, significant reductions in the
amountofdatastoredcanbeachieved.
The amount of storage savings is very dependent upon the characteristics of the data. For
instance, full database backups contain mostly data that is present in earlier backups. Email
systems often contain many copies of the same email message and/or attachments that have
been sent to several people. These examples can benefit significantly from data deduplication.
However,incrementalbackupscontainmostlyuniquedataandwillnotbenefitverymuch.
Vendors claim storage-reduction ratios from 10:1 to 50:1, depending upon the particular data
characteristics.Aratioof20:1seemstobetheaveragequotedreductionratio.
The reduction of storage capacity, along with the associated costs for hardware, power, cooling,
and space, is only one of the advantages of deduplication. Another is bandwidth reduction
requirements for replicating data to a disaster-recovery (DR) site. Replicating a deduplicated file
thathasbeenreducedto1/20thofitsoriginalsizewilllikewiseonlyrequire1/20thofthebandwidth
toreplicateittoaremotesite.
Driving the need for data deduplication is the information explosion that is leading to the
generation of a massive amount of data. All of this data must not only be stored online, but
restoration snapshots and archives of the data must also be kept. It is estimated that the
worldwide amount of digital data in 2009 reached 800 billion gigabytes (8x1020 bytes) and that it
will grow another 44% by 2020 – an exponential growth.1 Currently, 23% of all companies are
usingdeduplication;andanother61%areeitherdeployingitorareevaluatingit.2
1HPStoreOnce:reinventingdatadeduplication,HPwhitepaper;July2010.
22010StoragePrioritiesSurvey,searchstorage.com;March2010.
1
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Uses for Deduplication
Perhaps theoriginalmotivator for datadeduplication was theadventof virtual-tapesystems.With
virtualtape,periodicbackupsofadatabasearemadetodiskratherthantomagnetictape.Thisis
typically transparent to the applications. Applications think they are writing to magnetic tape.
However,thevirtualtapefacilitywritesthemagnetic-tapeimagestodiskratherthantotape.
Virtual tape backups have many advantages over magnetic-tape backups. Backups to disk can
be much faster, and they eliminate the need for physical media handling. Backup tapes do not
have to moved offsite to a secure storage vault; rather, tape images are transmitted to a remote
data center. Most importantly, the recovery of lost or corrupted files or of an entire database is
much faster and eliminates the need to recover tapes from a vault and to deal with tapes that
cannot be read. Recovery of a large database that used to take days from magnetic tape can
oftennowbedoneinhoursfromdisk.
Theproblem withvirtualtapeisthattheamountofdiskstoragerequiredtostorearchivalbackups
soars over a short period of time. To facilitate recoveries to a point-in-time and to satisfy
regulatory requirements, each daily or weekly backup has to be stored and retained. A petabyte
database can soon consume hundreds of petabytes of disk storage. Therefore, data has to be
rolledofftomagnetictapeafteronlyafewbackupiterations.
Enter data deduplication. It is based on the recognition that most of the data in a backup is an
exactcopyofdataintheprecedingbackup.Whynotsimplystorethechangeddataandprovidea
waytoreconstituteafileordatabasewhenneeded?Thistechniquehasbeenfoundtoreducethe
amount of additional storage required after the first backup by a factor of 20 or more. Backups
cannowbeeconomicallyretainedondisk foryearsratherthanformonthsbeforetheyhavetobe
rolledofftotapeforlong-termarchivalstorage.
Data deduplication is also of value to disaster-recovery sites. A disaster-recovery site must
contain a current copy of the database in order to ensure fast recovery and to minimize the
amount of lost data should the primary data center fail. Data replication techniques work fine for
relationaldatabases,inwhichonlychangedrecordsarereplicated.However,agoodbitofdatais
kept in files that are a single logical entity. If a few bytes in a file change, the entire file must be
replicated.
Data deduplication can reduce the amount of file data that has to be replicated to the DR site,
thus effecting significant savings in the communication bandwidth required. To achieve this, data
tobereplicatedisdeduplicatedattheprimarysite.Thededuplicateddataisthenreplicatedtothe
remotesite.
Companiesarenowlookingatdeduplicatingcertainprimarydatabases.Relationaldatabases are
effectively deduplicated since the third normal form requires that no data be repeated. However,
large files may contain repeated data and are candidates for storing in deduplicated form. The
restoration of a deduplicated file, as we shall see, requires very little overhead and supports
onlineuseinmanyapplications.
Datadeduplicationiscertainlynotacure-allforalldatastorageneeds:
 Deduplication requires significant processing overhead. Therefore, it is not suitable for
data that is changing frequently and to which frequent access is required. This precludes
itsuseformostonlinedatainprimarydatabases.
 Deduplication is not suitable for storing data on magnetic tape because of the way it
fragmentsdata.
2
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Therefore,deduplication’sroleisacomplementaryonewithotherclassicbackuptechnologies:
 Ifdatachangesrapidlyandisneededforonlyafewdays,fullstorageondiskisbest.
 If data changes moderately and is needed only for weeks or months, deduplicated data
ondiskispreferred.
 Ifdatamustbearchivedaftermonthsandsavedfor years,tapeisthechoice.
How Data Deduplication Works
Deduplicationrequiresthataninitialfullcopyofthedatabepresent.Thereafter,iffurtherupdated
copiesofthatdatamustbemade,onlythechangesarestored.
Deduplicationproceedsasfollows.Thefiletobecopiedisbrokenupintochunks.Ahashvalueis
then calculated for each chunk. The hash values for previously stored chunks (including those in
theoriginalcopy)arestoredinahashtablewithpointerstothedatachunkstheyrepresent.
If the hash value for an incoming chunk is found in the hash table, the incoming chunk of data is
replaced withapointer tothepreviouslystoredchunk.If thehashvaluefor the incomingchunk is
notfoundinthehashtable,thechunk ispreservedinthefilecopy;anditshashvaluewithpointer
is stored in the hash table. The result is that a copied file is a series of pointers to pre-existing
chunksinterspersedwithnewuniquechunks.
As deduplicated files are deleted, chunks that are pointed to by hash keys cannot be deleted.
However, when there are no more pointers to a chunk, it may be deleted and its hash value
removedfromthehashtable.
The calculation of hash values is processing-intensive. Therefore, the deduplication of a file
requires significant processing resources. Depending upon the implementation, deduplication
may be done either as inline processing or as post processing. An inline-processing
implementation deduplicates as the file is being received. Post processing stores the file first and
then deduplicates it. Inline processing may slow down the file transfer. Post-processing is faster
butrequiresadditionalstoragetoholdthefullfilebeforeitdeduplicatesit.
When a file is to be restored, it is read a chunk at a time and is delivered to the requesting
application. Whenever a pointer is encountered, the chunk to which it is pointing is delivered
instead.Unlikededuplication,filerestorationimposes verylittleoverheadandis almostasfastas
full-file reading. Both deduplication and restoration are transparent to the end users and
applications.
Note that deduplication is not performed just within a file. The hash table can point to chunks
anywhere in the entire database. Therefore, if one file contains data that is identical to that
contained in other files, the deduplicated file contains pointers to the common data no matter
wherethatdataisfound.
ChunksandHashes
ChunkSelection
Theselectionofachunksizeisthemostimportantcharacteristic withrespecttotheeffectiveness
of deduplication. As the chunk size grows, the probability of finding matching chunks decreases.
Asthechunksizegetssmaller,processingoverheadincreases.
3
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Experiencehas shown that thechunk sizeshould be less thantenkilobytes.A 4KBchunk size is
common.
Therearetwofundamentalmethodsforbreakingupa datastream intochunks–fixedchunk size
andvariablechunksize.
FixedChunks
A fixed chunk size is usually based on a physical characteristic of the disk subsystem. For
instance, if the disks use a4 KB block size, the chunk size might be 4 KB. Data stored in a block
generally remains in that block. Room is left in the block so that if data is to be inserted, there is
room for it. If the block overflows, it is split into two blocks; and each block now has room for
additionaldata.
Therefore,if datais insertedintoor deletedfrom thefile,onlythe disk blocks containing thatdata
are affected. The rest of the blocks remain unchanged and are replaced with pointers to the
originaldata.
Arbitrarily breaking up a file into fixed-size blocks will not work. If data is inserted, all following
blocksarenowchangedandmaynotbededuplicated.
VariableChunks
Withvariablechunks,achunk isdefinedbysomerepetitivesequencefoundinthedata.Asimple
example is a text file. A chunk may be a paragraph. Whenever a new paragraph is found, the
currentchunkisterminatedanddeduplicated.Anewchunkisthenbegun.
Variable chunks are sensitive only to the characteristics of the data and not to the underlying
characteristicsofthedatastore.
HashingAlgorithms
Another choice important to the effectiveness of deduplication is the selection of an appropriate
hashingalgorithm.Thealgorithmshouldhavethefollowingcharacteristics:
 It should generate a hash key that is sufficiently large to guarantee a very sparse hash
space (i.e., only a small proportion of hash keys are in use by chunks). This is important
tominimizedatacollisions,inwhichtwodifferentchunksgeneratethesamehashkey.
 It should not be sensitive to similar data patterns in such a way that it generates similar
hash keys. For instance, if one character is changed in a paragraph, the resulting hash
keyshouldbeagreatdistancefrom theoriginalhashkeyinhashspace.This willprevent
data with similar characteristics from bunching up in hash space, increasing the
probabilityofdatacollisions.
 Itshouldbereasonablyefficientinitsrequirementsforprocessingresources.
Most deduplication products today use the MD4/MD5 (Message Digest algorithm 4 or 5)3 or the
SHA-1 (Secure Hash Algorithm 1)4 hashing algorithms. The MD algorithms generate a 128-bit
hashkey.TheSHA-1algorithmgeneratesa160-bitkey.
3MD4,Wikipedia.
4SHA-1,Wikipedia.
4
