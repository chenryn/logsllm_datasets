Synchronous Replication
December2006
Review of Asynchronous Replication
In the October, 2006, issue of the Availability Digest, we described asynchronous replication in
the article, Asynchronous Replication Engines. Asynchronous replication is currently the favored
technique for keeping database copies in an active/active network in synchronization because it
is noninvasive (no code changes required), it is transparent to the application, and there is good
productsupportforit.
However, asynchronous replication comes with one issue, and that is replication latency.
Replication latency is the time delay between a change being made to the source database and
that change being applied to the target database. Depending upon the efficiency of the
asynchronous replication engine and the distance between the active/active nodes, replication
latencycanrangefromhundredsofmillisecondstoseconds.
In many applications, replication latency is not a serious problem. However, it creates certain
challenges which may be unacceptable in other applications. These problems include data
collisions,datalossfollowinganodefailure,andthecompromiseoffairness.
DataCollisions
Because the source database is updated before the target database, it is quite possible that a
user at one node canmakeachangeto a particular dataitem atabout the sametimeas another
user making a change to the same data item at a
different node (that is, the changes are made within a
time that is less than the replication latency). This will
data data
lead to database divergence, and both databases will base X base
bewrong.
There are many ways to structure a system to avoid DataCollisions
collisions, such as properly partitioning the database
and having all updates to a partition be made at only one node and replicated to the other
databasecopies.
However, if data collisions cannot be avoided, they must be handled either automatically by
business rules or, in the worst case, manually. Collision rates can be quite significant. In an
active/activesystemwithtwodatabasecopiesandrowlocking,theworst-casecollisionrateis1
1
SeeChapter9,DataConflictRates,inBreakingtheAvailabilityBarrier:SurvivableSystemsforEnterpriseComputing,
AuthorHouse;2004.
1
©2006SombersAssociates,Inc.,andW.H.Highleyman

2
1 (updaterate)
datacollisionrate= (replication latency)
2 databasesize(rows)
As an example of data collision rates, consider an active/active system with two copies of the
database.Letusassumethatthesystem-widetransactionrateis20transactions persecondand
that an average transaction has four updates (that is, an update rate of 80 updates per second).
Let us further assume that the replication latency is 500 milliseconds and that there are
10,000,000rowsinthedatabase.Inthiscase,wecanexpectaboutonecollisioneverytwohours
–aheadache,perhaps,butmanageable.
Should the transaction rate increase to 200 transactions per second, the data collision rate could
be more than 50 collisions per hour. This would certainly keep a team of people busy. At 1,000
transactions per second, there may be almost 1,500 collisions per hour. This may well be
unacceptable.
DataLossFollowingaNodeFailure
Should a node containing a database copy fail, any of that node’s updates that are still in the
replication pipeline will not make it to the target system and will be lost. For instance, if the nodal
transaction rate is 100 transactions per second, and if the replication latency is 500 milliseconds,
ontheaverage50transactionswillbelostfollowinganodefailure.
These transactions perhaps can be recovered if the
node is returned to service with its database intact.
data data
However, should the failed node lose its database X
base base
contents and have to be recovered by copying the
database from a surviving node, these transactions
arelostpermanently. LostData
TheFairnessPrinciple
In some applications, such as those for
equities trading systems, it is important
(and sometimes required by regulation)
update
data data that all users have the same speed of
base base update accesstoalldata.However,becauseof
replication latency, data posted to one
node will be accessible to users at that
Fairness node before it is available to users at
othernodes.
Synchronous Replication
By using synchronous replication, the above problems are avoided. Synchronous replication
acquires locks across thenetwork onalldata items tobeupdatedbyatransaction.Onlywhenall
locks have been granted in all database copies in the application network is the transaction
committed.
Since locks are held on all data items in all database copies before any data items are updated,
there can be no data collisions. Since either all database copies are updated or none are (the
atomic characteristic of a transaction), there will be no data loss due to a node failure. Finally, for
the same reason, all users will see the database updates at their local database copy at
substantiallythesametime,enforcingfairness.
2
©2006SombersAssociates,Inc.,andW.H.Highleyman

Though synchronous replication solves all of the problems associated with asynchronous
replication, as described above, it generates its own set of problems. For example, it delays the
completion of transactions until all database copies across the network have committed the
transaction. It is often invasive (that is, changes to the application may be required). Network
deadlocks can occur. Should a node fail, provisions must be made to exclude that node from
futuretransactions.
There are several ways to implement synchronous replication, each with its own strengths and
weaknesses. We discuss three different techniques below – network transactions, coordinated
commits,anddistributedlockmanagement.
NetworkTransactions
Network transactions are the most straightforward to understand. The scope of the transaction is
simplyextended to include all data items in all database copies in the application network. In this
way, the transaction holds all locks across the network for the data items to be updated, and
eitheralldataitemsareupdatedornoneare.
The Achilles heel of network transactions is
application communication latency. Each action must flow
independently across the communication network. This
includes the begin transaction, alllock requests,all update
1.begin requests, and the two-phase transaction commit. Each of
2.update these requires a communication round trip for the
3.commit request/response message pair. This may not be a
problem for campus environments but can be a significant
problem if nodes are widely geographically dispersed. The
data data round-trip time for a signal over copper or wire
base base transmission facilities from the U.S. East Coast to its West
Coastisabout50milliseconds.
Network Transactions As an example, consider a transaction that contains four
updates. This transaction could require up to thirteen
round trips (one for the begin transaction, two each for the updates, and two each for the two
phases of thetransactioncommit).If this transactionspans nodes ontheeastandwestcoasts of
the U.S., 650 milliseconds may be added to the transaction completion time. The application is
held up by this time as it must wait for the transaction to complete across all nodes before it is
notifiedthatthetransactionhasbeencommitted.
This delay is called application latency. If subsecond response times are to be achieved in a
widely distributed active/active network, the application latency imposed by network transactions
mayprecludethis.
Furthermore, extending an existing application to run in an active/active environment using
network transactions will usually require that the application be modified to extend the scope of
transactionstoincludealldatabasecopies.Alternatively, aninterceptlibraryorperhapsdatabase
triggerscouldbeprovidedtoperformthisfunction.
CoordinatedCommits
Coordinated commits minimize the network latency problems of network transactions. Basically,
asynchronous replication is used to start a transaction on each database copy and to propagate
all of the transaction updates to all database copies. As each update is received at the target
3
©2006SombersAssociates,Inc.,andW.H.Highleyman

system,theasynchronous replicationengine will acquirealock onthat data item (if itcan) before
applying the update. Since this process is decoupled from the application, the application is
unawareoftheupdateactivityacrossthenetwork.
However, when the application directs that the transaction be committed, that commit cannot be
executeduntilitisknownthatallreplicationenginesweresuccessfulinacquiringlocksonalldata
items to be updated. Therefore, a “ready to commit?” query is first sent to all replication engines
containing a database copy. This query must be sent through the replication channels to ensure
that each replication engine receives the query after it has received all database updates. If an
engine is holding locks on all data items to be updated, it responds positively to the query. If not,
itreturnsanegativeacknowledge.
If the source system receives a “ready to commit” message from each replication engine, it can
releasethecommit.Otherwise,itmustabortthetransaction.
Coordinated commits impose their own form of application latency. In this case, the application
must wait a replication latency interval for the “request to commit?” queries to propagate through
thereplicationchannelsplusacommunicationlatencytimetoreceivetheresponses.
For instance, consider again the case of two nodes on either coast of the U.S. Assume that the
replication latency of the asynchronous replication engine is 200 milliseconds (including the
communication latency required to get the updates to the target system) and that the one-way
communication latency is 25 milliseconds. In this case, the application latency imposed by
coordinated commits is 225 milliseconds. This compares favorably with the 650 millisecond
