Calculating Availability – Failover Faults
March,2007
In our last article, we analyzed the effect of failover time on system availability. We found that
failover times measured in minutes or more could have a profound effect on the availability of a
system that was otherwise designed to be highlyavailable. Onlyif failover times can be kept to a
few seconds are extremely available systems such as active/active configurations relatively
immunetofailovertime.
However, the effect of failover time on availability is only half the story. The problem is that
failover doesn’t always work. When it doesn’t, the system often goes down and has to be
recovered. Instead of a few seconds or minutes of failover time, there now may be hours of
systemrecoverytime.
Inthisarticle,welookattheeffectofthesefailoverfaultsonsystemavailability.
Where We Left Off
In our analysis of failover faults, we evaluated redundant systems in which every subsystem is
backed up by one or more subsystems. The system is provisioned with one or more spare
subsystems. If all spare subsystems fail, the system is left open to failure should one more
subsystemfail.
When a subsystem fails, a failover procedure is executed. The failover takes a time of MTFO
(mean time for failover). During this interval, some or all of the system’s services are unavailable
tosomeoralloftheusers.
We assumed that a failed node has to be repaired and recovered before it can be returned to
service. Following a total system failure, the system can be returned to service as soon as the
first repaired node is available. Returning the system to service requires some additional
restorationtasks.
Becauseoffailovertime,therearetwocomponentstotheavailabilityequation:
 Theprobabilitythatthesystemwillbedownduetoamultiplesubsystemfailure.
 Theprobabilitythatthesystemwillbedownduetofailoverprocessing.
Theresultingfailureprobability,F,is(forthecaseofasingly-sparedsystemwithparallelrepair)
r/2Rn(n1) MTFO
F (1a)2  n(1a) (1)
r/2 2 r
1
©2007SombersAssociates,Inc.,andW.H.Highleyman

where
F istheprobabilityoffailureofthesystem.
a istheavailabilityofasubsystem.
n isthenumberofsubsystemsinthesystem.
r is the repair and recovery time for a subsystem. That is, it is the time
required to return the subsystem to service. This term is often referred to
asthemeantimetorepair(mtr)forthesubsystem.
R is the restore time of the system. It is the time that it takes to perform
system-wide functions required to return the system to service once it has
a full complement of subsystems. For instance, restoration functions may
include database resynchronization and the reentry of transactions
submittedduringthesystem’sdowntime.
MTFO isthetimerequiredtofailover(themeantimeforfailover).
The first term in Equation (1) is the probability that the system will be down due to a multiple
system failure. The second term is the probability that the system will be down during a failover
process.
Thesystemavailability,A,is,ofcourse,1-F.
We showed by example that a typical cluster with a three-minute failover time might have its
probability of failure increased by an order of magnitude but that a typical active/active system
retainsitsextremeavailabilityiffailovertimescanbekepttoseconds.
On the other end of the scale, failover time for a cold standby with a failover time of two hours
(which is reasonably fast for a cold standby) completely dominates the availability relationship.
The probability of a dual failure of both the active and standby systems is not an issue, though
suchafailurecouldleadtoalongrecoverytime.
What is a Failover Fault?
The purpose of a failover is to direct an
operating system component to take over
the functions of a component that has just
failed. This can be a very complex task
and can take some time (seconds to
X
hours).
failover
Not only are failover processes complex, FailoverFault
but they must cover a myriad of failure
scenarios, some of which cannot even be visualized during the design of the failover process.
Furthermore, the failover process can be verydifficult to test. Ameans must be provided to inject
faults into the system, and even the best fault-injection facilities cannot reasonably inject all
known possible faults, much less those that are unknown. Consequently, because of their
complexityandthedifficultyintestingthem,failoversaresubjecttofailurethemselves.
In practice, failover procedures are substantially debugged in the field. This is a neverending
process,leadingtocontinualimprovementbutneverperfectioninthefailoverprocess.
When a failover attempt fails and takes down the system, this is known as a failover fault. It has
been estimated that about 1% of failovers are unsuccessful.1 In a three 9s system, this may not
1
Estimatesfromavendorofhighlyavailablesystems.
2
©2007SombersAssociates,Inc.,andW.H.Highleyman

be much of a factor. But if we are looking for availabilities measured in centuries, failover faults
canindeedbeafactorinsystemavailability.
Failover faults are a consideration for fault-tolerant systems, for clusters, and for active/active
systems. However, they do not generally apply to active/backup configurations because the
failover process is reallythe bringing up of the backup system. If this fails, it is because of a dual
systemfailure,notafailoverfault.
FailureProbabilitywithFailover Faults
By introducing the possibility of failover faults, the availability relationship given by Equation (1)
mustbeextended.Therearenowthreeclassesofsystemdowntimethatwemustconsider:
 Thesystemmaybedownduetoamultiplesubsystemfailure.
 Thesystemmaybedownduetofailoverprocessing.
 Thesystemmaybedownduetoafailoverfault.
The first two classes are covered by Equation (1). Let us determine the third term that will define
theimpactoffailoverfaultsonsystemavailability.2
Letpbetheprobabilitythatafailoverattemptwillendinfailure.Thatis,givenasingle-subsystem
failure,pistheprobabilityofafailoverfault.
The probability that a failover attempt will be made is the probability that there is the failure of a
single subsystem. If a subsystem has an availability of a, the probability of a single-subsystem
failure is (1-a). If there are n subsystems in the system, the probability that there will be a single-
systemfailureandthusafailoverattemptisn(1-a).
Ifpofthesefailoverattemptsfail,theprobabilityofafailoverfaultis
failoverfaultprobability pn(1a) (2)
The measured availability of the subsystem, a, is based on it being returned to service after it is
repaired and recovered. This is our term r defined above. However, recoveryfrom a failover fault
does not require a subsystem repair and recovery. Rather, it requires a system restoration of R,
as defined above. Therefore, to convert the above failover fault probability to the probability that
thesystemisdown,3itmustbeadjustedbythefactorR/r:
R
probabilitythatsystem willbedownduetoafailoverfault  pn(1a) (3)
r
If pofallsystem faults arecausedbyfailover faults,then(1-p) of allfaults arecausedbymultiple
system faults or failover times. Applying these observations, we have an expanded availability
relationthatcoversallofthethreecasesinwhichweareinterested:
r/2Rn(n1) MTFO  R
F(1p) (1a)2  n(1a) p n(1a) (4)
 
 r/2 2 r  r
2
FailoverfaultsarediscussedinsomedetailinChapter5,TheFactsofLife,BreakingtheAvailabilityBarrier:Survivable
SystemsforEnterpriseComputing,byDr.BillHighleyman,PaulJ.Holenstein,andDr.BruceHolenstein.
3
ThisrelationshipisderivedformallywithMarkovmodelsinAppendix3,FailoverFaultModels,BreakingtheAvailability
Barrier:SurvivableSystemsforEnterpriseComputing,referencedabove.
3
©2007SombersAssociates,Inc.,andW.H.Highleyman

Since a node failure in an active/active system affects only 1/n of the users, Equation (4)
becomes,foractive/activesystems,
r/2Rn(n1) MTFO  R
F(1p) (1a)2  (1a) p (1a) (active/active) (5)
 
 r/2 2 r  r
The first term of Equations (4) and (5) is the probability that the system will be down due to a
dual-system failure.Thesecondterm is theprobabilitythatit willbedown while it is undergoinga
failover procedure. The third term is the probabilitythat it will be down because of a failover fault.
Thesumoftheseprobabilitiesistheprobabilitythatthesystemwillbedown.
Examples
To get a feel for the effect of failover faults, let us repeat the clustering example and the
active/active example from our previous article, which evaluated the effects of failover time, and
addafailoverfaultof1%totheseexamples.
AClusteredSystem
We consider the case of a single-spared, four-node cluster, which is made up of nodes with
availabilities of .999. The recovery time for a node is two hours, as is the system restore time.
Thefailovertimeisthreeminutes,andfailovershavea1%chanceoffailing.Thus,
r =2hours
R =2hours
n =4
a =.999
MTFO =.05hours
p =.01
FromEquation(4),theprobabilitiesoffailureforthiscaseare:
Probabilitythatthesystemisdownduetoamultiplenodefailure=1.8x10-5
Probabilitythatthesystemisdownduringfailover=9.9x10-5
Probabilitythatthesystemisdownduetoafailoverfault=4x10-5
Probabilitythatthesystemisdown=1.6x10-4
A1%chanceofafailoverfaulthasadded34%tothesystemavailabilitythatwouldbeachievedif
therehadbeennofailoverfaults.
AnActive/ActiveSystem
Inthisexample,weconsideratwo-nodeactive/activesystem.Thenoderepairandrecoverytime,
r, and the system restore time, R, are the same as the clustered example above. The nodal
availabilityisfour9s,andthefailovertimeis1second.Thefailoverfaultprobabilityisagain1%.
r =2hours
R =2hours
n =2
a =.9999
MTFO =1seconds=.00028hours
p =.01
4
©2007SombersAssociates,Inc.,andW.H.Highleyman

FromEquation(5),theresultsforthiscaseare
Probabilitythatthesystemisdownduetoamultiplenodefailure=3x10-8
Probabilitythatthesystemisdownduringfailover=1.4x10-8
Probabilitythatthesystemisdownduetoafailoverfault=1x10-6
Probabilitythatthesystemisdown= 1x10-6
In this case, failover faults dominate the system availability, reducing it byalmosttwo 9s. Clearly,
as the availability of the nodes increases and as the failover time decreases, failover faults play
anevermoredominantroleinsystemavailability.
A Limiting Case
Wecangetaninsightintothisphenomenonbyconsideringalimitingcase.Letusconsideratwo-
nodesystem witharestore time,R,thatis half of thenodalrepair andrecoverytime, r.Thatis,R
= r/2. Assume that MTFO is small enough so that it can be ignored and that the probability of
failoverissmall(p<<1).Thenequation(4)reducesto
F2(1a)2 p(1a)[2(1a)][(1a)p/2] (6)
The limiting case that we want to consider is the case in which the probabilityof a failover fault is
verymuchgreater thanthe probabilityof anodefailureas itis intheactive/active exampleabove
(.01versus.0001).Inthiscase,p/2>>(1–a);andEquation(6)furtherreducesto
p
F2(1a) (1a)p (7)
2
This equation says that if a failover fault is much more likely than a node fault, then following a
nodefailurethesurvivingsystemactsasifithasaprobabilityoffailureofpratherthanof(1-a).
Thus, in high-availability systems, failover faults may cause a disproportionate decrease in
systemavailability.
Summary
Failover faults can have a serious impact on system availability. For systems with modest
availability,failoverfaultsarenotterriblyserioussofarasoverallsystem availabilityisconcerned.
However, as the inherent reliability of a system improves, that is, as the nodes become more
reliableandasfailovertimedecreases,theimpactoffailoverfaultscanincreasedramatically.
In the limit, once one node in a single-spared system has failed, the system availability is
determined by the probability of a failover fault rather than by the probability that a second node
willfail.
5
©2007SombersAssociates,Inc.,andW.H.Highleyman
