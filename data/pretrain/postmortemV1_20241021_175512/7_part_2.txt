do not occur in the future. In this case, this includes (but is not
limited to):

-   The Azure Key Vault team has immediately fixed the storage access
    patterns for the resource type that triggered the incident.
-   Capacity has been reevaluated globally to ensure that the service is
    resilient to increased usage.
-   New monitors have been added to watch for overly high resource usage
    and these monitors will trigger autoscaling.
-   We are modifying the failover pattern for the service so that the
    paired region is not affected by failover traffic from another
    region.
-   Constraints on resource usage and circuit breakers are being added
    for all down level dependencies so that the service can gracefully
    react to spikes in traffic and avoid extended incidents.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 15 

[03/15/2021]

RCA - Authentication errors across multiple Microsoft services

Tracking ID: LN01-P8Z


**Summary of Impact:** Between 19:00 UTC on March 15, 2021 and 09:37 UTC
on March 16, 2021, customers may have encountered errors performing
authentication operations for any Microsoft services and third-party
applications that depend on Azure Active Directory (Azure AD) for
authentication. Mitigation for the Azure AD service was finalized at
21:05 UTC on 15 March 2021. A growing percentage of traffic for services
then recovered. Below is a list of the major services with their
extended recovery times:

22:39 UTC 15 March 2021 Azure Resource Manager.\
01:00 UTC 16 March 2021 Azure Key Vault (for most regions).\
01:18 UTC 16 March 2021 Azure Storage configuration update was applied
to first production tenant as part of safe deployment process.\
01:50 UTC 16 March 2021 Azure Portal functionality was fully restored.\
04:04 UTC 16 March 2021 Azure Storage configuration change applied to
most regions.\
04:30 UTC 16 March 2021 the remaining Azure Key Vault regions (West US,
Central US, and East US 2).\
09:25 UTC 16 March 2021 Azure Storage completed their recovery and we
declared the incident fully mitigated.

**Root Cause and Mitigation:** Azure AD utilizes keys to support the use
of OpenID and other Identity standard protocols for cryptographic
signing operations. As part of standard security hygiene, an automated
system, on a time-based schedule, removes keys that are no longer in
use. Over the last few weeks, a particular key was marked as "retain"
for longer than normal to support a complex cross-cloud migration. This
exposed a bug where the automation incorrectly ignored that "retain"
state, leading it to remove that particular key.

Metadata about the signing keys is published by Azure AD to a global
location in line with Internet Identity standard protocols. Once the
public metadata was changed at 19:00 UTC on 15 March 2021, applications
using these protocols with Azure AD began to pick up the new metadata
and stopped trusting tokens/assertions signed with the key that was
removed. At that point, end users were no longer able to access those
applications.\
\
Service telemetry identified the problem, and the engineering team was
automatically engaged. At 19:35 UTC on 15 March 2021, we reverted
deployment of the last backend infrastructure change that was in
progress. Once the key removal operation was identified as the root
cause, the key metadata was rolled back to its prior state at 21:05 UTC.

Applications then needed to pick up the rolled back metadata and refresh
their caches with the correct metadata. The time to mitigate for
individual applications varies due to a variety of server
implementations that handle caching differently. A subset of Storage
resources experienced residual impact due to cached metadata. We
deployed an update to invalidate these entries and force a refresh. This
process completed and mitigation for the residually impacted customers
was declared at 09:37 UTC on 16 March 2021.

Azure AD is in a multi-phase effort to apply additional protections to
the backend Safe Deployment Process (SDP) system to prevent a class of
risks including this problem. The first phase does provide protections
for adding a new key, but the remove key component is in the second
phase which is scheduled to be finished by mid-year. A previous Azure AD
incident occurred on September 28th, 2020 and both incidents are in the
class of risks that will be prevented once the multi-phase SDP effort is
completed.

**Next Steps:** We understand how incredibly impactful and unacceptable
this incident is and apologize deeply. We are continuously taking steps
to improve the Microsoft Azure platform and our processes to help ensure
such incidents do not occur in the future. In the September incident, we
indicated our plans to "apply additional protections to the Azure AD
service backend SDP system to prevent the class of issues identified
here.\"

-   The first phase of those SDP changes is finished, and the second
    phase is in a very carefully staged deployment that will finish
    mid-year. The initial analysis does indicate that once that is fully
    deployed, it will prevent the type of outage that happened today, as
    well as the related incident in September 2020. In the meantime,
    additional safeguards have been added to our key removal process
    which will remain until the second phase of the SDP deployment is
    completed.
-   In that September incident we also referred to our rollout of Azure
    AD backup authentication. That effort is progressing well.
    Unfortunately, it did not help in this case as it provided coverage
    for token issuance but did not provide coverage for token validation
    as that was dependent on the impacted metadata endpoint.
-   During the recent outage we did communicate via Service Health for
    customers using Azure Active Directory, but we did not successfully
    communicate for all the impacted downstream services. We have
    assessed that we have tooling deficiencies that will be addressed to
    enable us to do this in the future.
-   We should have kept customers more up to date with our
    investigations and progress. We identified some differences in
    detail and timing across Azure, Microsoft 365 and Dynamics 365 which
    caused confusion for customers using multiple Microsoft services. We
    have a repair item to provide greater consistency and transparency
    across our services.

**Provide Feedback**: Please help us improve the Azure customer
communications experience by taking our survey at
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)
.

## 9 

[03/09/2021]

Argentina and Uruguay - Issue Accessing Azure Resources

Tracking ID: 8NVQ-HD8


**Summary of Impact:** Between 17:21 and 17:37 UTC on 09 Mar 2021, a
network infrastructure issue occurred impacting traffic into and out of
Argentina and Uruguay. During this time, customers in these areas may
have experienced intermittent issues connecting to Azure resources.\
\
**Root Cause:** A regional networking fiber cut resulted in a brief loss
of connectivity to Microsoft resources.\
\
**Mitigation:** An automated failover of network traffic to an
alternative fiber route mitigated the issue.\
\
Stay informed about Azure service issues by creating custom service
health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation. 

## February 2021

## 26 

[02/26/2021]

RCA - Azure Storage and dependent services - Japan East

Tracking ID: PLWV-BT0


**Summary of Impact:** Between 03:26 UTC and 10:02 UTC on 26 Feb 2021, a
subset of customers in Japan East may have experienced service
degradation and increased latency for resources utilizing Azure Storage,
including failure of virtual machine disks. Some Azure services
utilizing Storage may have also experienced downstream impact.\
\

**Summary Root Cause**: During this incident, the impacted storage scale
unit was under heavier than normal utilization. This was due to:

-   Incorrect limits set on the scale unit which allowed more load than
    desirable to be placed on it. This reduced the headroom that
    is usually available for unexpected events such as sudden
    spikes in growth which allows time to take load-balancing actions.
-   Additionally, the load balancing
    automation was not sufficiently spreading the load to other scale
    units within the region.

This high utilization triggered heavy throttling of storage
operations to protect the scale unit from catastrophic failures. This
throttling resulted in failures or increased latencies for storage
operations on the scale unit.

Note: The original RCA mistakenly identified a deployment as a
triggering event for the increased load. This is because during an
upgrade, the nodes to be upgraded are removed from rotation, temporarily
increasing load on remaining nodes. An upgrade was in queue on the scale
unit but had not yet started. Our apologies for the initial mistake.

**Background:** An internal automated load balancing system actively
monitors resource utilization of storage scale units to optimize load
across scale units within an Azure region. For example, resources such
as disk space, CPU, memory and network bandwidth are targeted for
balancing. During this load balancing, storage data is migrated to a new
scale unit, validated for data integrity at the destination and
finally the data is cleaned up on the
source to return free resources. This automated load-balancing
happens continuously and in real-time to ensure workloads are properly
optimized across available resources.

**Detailed Root Cause:** Prior to the start of impact, our automated
load-balancing system had detected high utilization on the scale-unit
and was performing data migrations to balance the load. Some of these
load-balancing migrations did not make sufficient progress, creating a
situation where the resource utilization on the scale unit reached
levels that were above the safe thresholds that we try to maintain for
sustained production operation. This kick-started automated throttling
on incoming storage write requests to protect the scale unit from
catastrophic failures. When our engineers were engaged, they also
detected that the utilization limits that were set on the scale unit to
control how much data and traffic should be directed to the scale unit
was higher than expected. This did not give us sufficient headroom to
complete load-balancing actions to prevent customer facing impact.\
\
**Mitigation:** To mitigate customer impact as fast as possible, we
took the following actions:

-   Engineers took steps to aggressively balance resource load out of
    the storage scale unit. The load-balancing migrations that were
    previously unable to finish were manually unblocked and completed,
    allowing a sizeable quantity of resources to be freed up for use.
    Additionally, load-balancing operations were tuned to improve its
    throughput to more effectively distribute load.
-   We prioritized recovery of nodes with hardware failures that had
    been taken out of rotation to bring additional resources online.

These actions brought the resource utilization on the scale unit to a
safe level which was well below throttling thresholds. Once Storage
services were recovered around 06:56 UTC, dependent services started
recovering. We declared full mitigation at 10:02 UTC.\
\
**Next steps:** We sincerely apologize for the impact this event had on
our customers. Next steps include but are not limited to:

-   Optimize the maximum allowed resource utilization levels on this
    scale unit to provide increased headroom in the face of multiple
    unexpected events.
-   Improve existing detection and alerting for cases when
    load-balancing is not keeping up, so corrective action can be
    triggered early to help avoid customer impact.
-   Improve load-balancing automation to handle certain edge-cases under
    resource pressure where manual intervention is currently required to
    help prevent impactful events.
-   Improve emergency-levers to allow for faster mitigation of impactful
    resource utilization related events.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 16 

[02/16/2021]

Azure Frontdoor - Europe - Timeouts connecting to resources

Tracking ID: ZN8\_-VT8


**Summary of Impact**: Between approximately 12:00 UTC and 13:30 UTC a
subset of customers using Azure Frontdoor in Europe may have experience
timeouts and/or issues connecting to resources. \
\
**Root Cause**: Engineers determined that a backend network device
became unhealthy, and traffic was not automatically rerouted. This
resulted in Azure Front Door requests to fail. \
\
**Mitigation**: We manually removed the faulty backend network device
and rerouted network traffic. This mitigated the issue.\
\
Stay informed about Azure service issues by creating custom service
health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation.

<div>

</div>

