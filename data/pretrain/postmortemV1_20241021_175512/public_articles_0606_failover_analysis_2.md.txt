Simplifying Failover Analysis – Part 2
June2011
In our article, Simplifying Failover Analysis – Part 1,1 we discussed the impact of failover time and
failoverfaultsonredundantsystems.Inatwo-noderedundantsystem,usersaredownif:
 bothnodesfail,or
 onenodefails,andtheusersareintheprocessofbeingfailedover,or
 onenodefails,andafailoverfaultoccurs.
Weshowed that failover can be modeled as a two-node redundant system with the availability of
onenodebeingreducedbytheeffectsoffailovertimeandfailoverfaults.
In this article, we extend the results of Part 1 to accommodate two additional complexities
when modeling failover:
 What if the redundant nodes are different with different availability characteristics?
 How do we handle a redundant production node that is in the process of failing over
internally? Even though it is technically down, it will not fail over to its backup node.
A Review of Failover Analysis
TheImpactofFailoverTimeandFailoverFaults
InPart1,wedefinedthefollowingparameters:
a availabilityofanode
mtbf nodalmeantimebetweenfailure(theaveragetimebetweenfailuresforanode)
mtr nodalmeantimetorecover(theaveragetimetorestoreanodetoservice)
mtfo timetofailover(theaveragefailovertime)
1http://www.availabilitydigest.com/public_articles/0510/failover_analysis.pdf
1
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

d probabilityofafailoverfault
A availabilityofasystem(probabilitythatsystemisup)
F probabilitythatasystemisdown
Weshowedthattheprobabilityofdowntimewhenfailoverisconsideredis
mtfo
p(downtime)F(1a)2  (1a)d (1)
mtbf
where the first term is the probability that both nodes will fail, the second term is the probability
thatthesystem willbeintheprocessoffailingover,andthethirdtermistheprobabilitythatthere
willbeafailoverfault.
Usingtherelationshipa=1–mtr/mtbf,ormtbf=mtr/(1-a),Equation(1)canbewrittenas
mtfo
p(downtime)F(1a)2 (1a) (1a)d (2)
mtr
Rearrangingterms,wehave
 mtfo    mtfo 
F(1a) (1a) d (1a) 1a d (3)

 mtr    mtr 
Letusdefineamodifiednodeavailability,a’:
mtfo
a'a d (4)
mtr
Equation(3)canthenbewrittenas
F(1a)(1a') (5)
and
system availability A (1F)1(1a)(1a') (6)
Thus, the system behaves as a two-node system, a first node with an availability of a and a
secondnodewithanavailabilityofareducedbytheeffectsoffailover:
2
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Thisobservationprovidesasimplewaytocalculatetheavailabilityofaredundantsystem whenit
isimpactedbyfailovertimesandfailoverfaults.
TheImpactofFailover
The impact of failover on system availability can be shown through a simple example. Consider a
redundant system comprising two servers, each with three 9s availability (a) and a mean time to
restore (mtr) of four hours. From Equation (6), if it were not for failover, this system would have an
availability,A,of six 9s.Inother words,theonlyfailuremodeis if bothnodesshouldfail.Sincewe
have no control over nodal availability (that is up to the manufacturer), we call this the inherent
availability ofthesystem.Wecandonobetterthanthis.
Let us consider the impact of failover on this system. Assume that thefailover time is one minute,
and the probability of a failover fault is 0.5%. From our above analysis resulting in Equation (4),
theeffectiveavailability, a’,ofthesecondnodeis
mtfo 1
a'a d0.999 0.0050.990
mtr 4x60
Thesystemavailabilityistherefore
A 110.999)(10.9900.99999
0.999 0.999
0.99999
0.999 0.99
mtfo=0 mtr=4hours
d=0 mtfo=1minute
F=10-3x10-3 10-6 d=0.005
A=six9s F=10-3x10-2 10-5
A=five9s
3
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Thesystem availabilityhas been reducedfrom six 9s tofive 9s. Afailover time ofonlyoneminute
and a failover fault rate of one in 200 has increased the amount of downtime by an order of
magnitude!Failoversdocount.
An Example System
The above analysis was then applied in Part 1 to a complex configuration of two data centers,
eachwitharedundantsetofsystems.
Though both datacenters are activelyengaged in their own processing activities, Data Center A
is running a particularly critical application that is backed up by less expensive systems in Data
CenterB.
In Data Center A, the application is running in an active/active system2 comprising two fault-
tolerantnodes.Eachofthefault-tolerantnodes has anavailabilityoffour 9s3(eachis up99.99%
of the time). Being active/active, users on afailed node can befailed over to thesurviving node in
three seconds. There are no failover faults since it is known that the surviving node is properly
operating–afterall,itiscurrentlyprocessingtransactions.
DataCenterA DataCenterB
0.9999 datacenter 0.999
failover
mtfo =20minutes
0.9999 0.999
d=0.1
mtfo=3seconds mtfo=5 minutes
Legend:
d=0 d=0.01
mtfo=meantimetofailover
d=probabilityofafailoverfault
mtr=meantimetorecover=4hours
The active/active system is backed up by a more economical cluster in Data Center B. Industry-
standard servers are used with a nodal availability of three 9s (each node is up 99.9% of the
time). Failover time is five minutes, and the probably of a failover fault is 1% (that is, 99 out of 100
failoverswillbesuccessful).
Should the active/active system in Data Center A fail, it takes twenty minutes on the average to
failover toDataCenter B. Failover testing is limitedduetothecomplexityandrisk offailover,but
what testing has been done indicates that one out of 10 failovers will be unsuccessful (the
probabilityofafailoverfaultis10%).
2WhatisActive/Active?,AvailabilityDigest;October2006.
3 W. H. Highleyman, P. J. Holenstein, B. D. Holenstein, Chapter 1, The9s Game, Breaking the Availability Barrier:
SurvivableSystemsforEnterpriseComputing,AuthorHouse;2004.
4
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Regardless of the node type – fault-tolerant or standard servers – the time to repair a node – the
nodalmtr-averagesfourhours.
What is the system availability from the user’s viewpoint?
Step1–CalculatetheAvailabilityofeachDataCenters
The first step of this analysis was carried out in Part 1. In that step, the data-center nodes were
replaced with nodes with availabilities modified by the failover parameters. The following
configurationresulted:
The availability of each data center now can be easily determined, resulting in the following
configuration:
Step2–CalculatetheAvailabilityoftheDataCenterComplex
We left Part 1 at this point with the observation that we now have two problems that require some
modificationstoourearlieranalysis:
•
A failover from Data Center A to Data Center B occurs only if Data Center A “fails hard”
(i.e., both of its nodes fail, or it suffers an internal failover fault). There will not be a
5
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

failover from Data Center A to Data Center B if Data Center A is down because it is in
theprocessoffailingoverinternally.
 Thetwonodeshavedifferentavailabilities.
FailovertoDataCenterBDoesNotHappenIfDataCenterAisFailingOverInternally
The first problem is solved by separating out the Data Center A failover time. If failover time is
ignored, the only impact of failover on Data Center A is a failover fault. However, since Data
Center A uses an active/active system, the probability of a failover fault is 0 (d = 0). The
availabilityofDataCenterAwhenconsideringonlyhardfailuresistherefore[seeEquation(3)]:
1-(1-a)[1-(a-d)]=1-(1-0.9999)(1-0.9999) =0.99999999(eight9s)
Let us call this value the “hard” availability of Data Center A, A . A is the probability that Data
a a
Center A will be up unless because both of its nodes are down or because it suffered an internal
failoverfault:
A =0.99999999
a
TheprobabilitythatDataCenterAwillbedownbecauseitisfailingoveris,fromEquation(1):
mtfo mtfo 3
p(A isfailingover)= (1a) (10.9999) 2x108
mtbf mtr 4x3600
Note that subtracting the probability of failover of Data Center A from its “hard” availability gives
theDataCenterAavailabilitycalculatedearlier:
0.99999999–2x10-8=0.99999997
NodalAvailabilitiesareDifferent
So far as the second issue is concerned (the two data centers have different availabilities), we
can refine Equation (3) for this purpose by noting that it is only the failure of the production node
that will cause a failover. Using the notation A for the productionsite hard availabilityand Abfor
a
thebackupsiteavailability(includingfailovertime),Equation(3)canberecastas
  mtfo 
F(1A a)  1 A b  mtr d   (7)
Inthisexample,ascalculatedabove,
A =1–(1–0.9999)(1–0.9999)=0.99999999
a
Ab=1–(1-0.999)(1–0.9682) =0.9999682
6
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Thus, the availability of the system is that of the production node backed up with a node whose
availability is its inherent availability reduced by the effects of failover. This yields the following
configuration:
whereAb’=Ab–mtfo/mtr–d=9999682–20/(4x60)-0.1=0.816635.
The overall availability for the dual data centers is therefore (ignoring Data Center A’s failover
time):
Dual data center availability = 1 – (1 - .99999999)(1 - .816635) = 0.9999999982
(almostnine9s)
From this, we have to subtract the probability that the system will be down due to a Data Center A
internalfailover:
Dualdatacenteravailability=0.9999999982-0.00000002=0.9999999782(almosteight9s)
Notethatthethree-second failover timeof DataCenter Ahas reduced totalsystem availabilityby
one 9. In fact, the probability of total system failure is primarily due to the three-second failover
timeofDataCenterA!
The five-9s cluster backup in Data Center B has increased the overall availability by only 40%
becausethepredominantfactor inthesystem’savailabilityisthefailover timeinDataCenter A–
no amount of data-center redundancy can make that any better. More precisely, the probability of
downtime has been decreased from 3x10-8 to 2.18x10-8, a reduction of downtime of about 1.4.
2x10-8ofthisprobabilityisduetoDataCenterAfailovertime.
Is the cost of this remote cluster worth an increase of 40% in the reliability of the application?
That, of course, depends upon the application. But this technique provides a simple way to
calculatetheavailabilityofcomplexsystemssothatthisjudgmentcanbemade.
It is worth noting that if there were a modest probabilityof a failover fault in Data Center A – say
in the order of 1%, a similar computation would show that the backup data center does, in fact,
significantlyimprovereliability.
Of course, beyond the need for high availability, the remote cluster may be justified in order to
guarantee recovery from a disaster that might destroy the primary site. That is a different
consideration.
7
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

MTRsareDifferent
One case we have not considered is what if the mean time to repair is different for both data
centers.InEquation(7),whichmtrdoweuse?
Note that themtr term in Equation (7) comes about through rearrangementof the term mtfo/mtbf.
In this term, mtfo is the failover time from Data Center A to Data Center B; and mtbf is the time
interval between failovers. Therefore, we are dealing with the mtbf of Data Center A (it is its
failurethattriggersafailover).Consequently,mtrinEquation(7)isthatofDataCenterA.
Toaccountforthis,Equation(7)shouldbewrittenas
  mtfo 
F(1A )1A  d (8)
a   b mtr 
a
where mtr is the mean time to repair Data Center A. This is the mtr of its nodes (a node must be
a
broughtbackintoserviceinordertobringDataCenterAbackintoservice).4
Summary
Failover in redundant systems is a fact of life with which we have to deal. Even fast and
reasonably reliable failovers can have a dramatic effect on downtime and availability. This
analysis has shown how the failover characteristics of a complex redundant system can be
replacedwithanequivalenttwo-nodesystemtofacilitatesimpleandquickanalysis.
4Thisisconservativeiftherearemultiplemaintenancetechnicians workingonbothfailednodessimultaneouslyrather
thanasingletechnicianworkingonlyononenode.Inthiscase,theaveragerepairtimeforthedatacenterwilllikelybe
shorterthantheaveragerepairtimeofasinglenode.
8
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com