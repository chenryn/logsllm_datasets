## February 2021

## 12 

[02/12/2021]

RCA - Azure Cosmos DB connectivity issues affecting downstream services
in West US region

Tracking ID: CVTV-R80


**Summary of Impact:** Between February 11, 23:23 UTC and February 12,
04:30 UTC, a subset of customers using Azure Cosmos DB in West US may
have experienced issues connecting to resources. Additionally, other
Azure services that leverage Azure Cosmos DB may have also seen
downstream impact during this time. The Cosmos DB outage affected user
application requests to West US. A small subset of customers using
Cosmos DB in other regions saw an impact on their replication traffic
into West US. Customer impact for Azure Cosmos DB accounts was dependent
on the Geo-Replication configurations in place:

-   Accounts with no Geo-Replication: Read and write requests failed for
    West US
-   Accounts with Geo-Replicated Single-Write + Multiple-Read regions:
    Read and write requests failed for West US. The Cosmos DB client SDK
    automatically redirected read requests to a healthy region -- an
    increased latency may have been observed due to longer geographic
    distances
-   Accounts with Geo-Replicated Multiple Write + Read regions: Read and
    write requests may have failed in West US. The Cosmos DB client SDK
    automatically redirected read and write requests to a healthy region
    -- an increased latency may have been observed due to longer
    geographic distances

**Root Cause:** On February 11, 10:04 UTC (approximately thirteen hours
before the incident impact), a Cosmos DB deployment was completed in
West US using safe deployment practices; unfortunately, it introduced a
code regression that triggered at 23:11 UTC, resulting in the customer
impact described above.

A rare failure condition in the configuration store for one of the West
US clusters was encountered. The front-end service (which is responsible
for request routing of customer traffic) should handle this. Due to the
code regression, the cluster\'s front-end service failed to address the
condition and crashed.

Front-end services for other clusters in the region also make calls to
the impacted cluster\'s front-end service to obtain configuration. These
calls were timed out because of unavailability, triggering the same
unhandled failure condition and resulting crash. This cascading effect
impacted most West US Cosmos DB front-end services. Cosmos DB customers
in the region would have observed this front-end service outage as a
loss of availability.

**Mitigation:** Cosmos DB internal monitoring detected the failures and
triggered high severity alerts. The appropriate teams responded to these
alerts immediately and began investigating. During the triage process,
Engineers noted that the configuration store\'s failure condition (which
led to the unhandled error) was uncommon and not triggered in any other
clusters worldwide.

The team applied a configuration change to disable the offending code
causing the process crashes. Automated service recovery then restored
all cluster operations.

**Next Steps:** We apologize for the impact on affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Expediting roll out of a hotfix for the Cosmos DB Gateway
    application to isolate failures for internal metadata requests to
    reduce the regional and inter-regional impact
-   Improving Cosmos DB monitoring to detect unhandled failures
-   Improving the Cosmos DB front-end service to remove dependencies on
    current configuration store in steady-state
-   Improving publicly available documentation, with the intent of
    providing more straightforward guidance on the actions customers can
    take with each account configuration type in the event of partial,
    regional, or availability zone outages
-   Improving Cosmos DB automated failover logic to accelerate failover
    progress due to partial regional outages

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## January 2021

## 15 

[01/15/2021]

Azure Network Infrastructure service availability issues for customers
located in Argentina - Mitigated

Tracking ID: DM7S-VC8


**Summary of Impact**: Between 17:30 and 20:15 UTC on 15 Jan 2021,
customers located in Argentina attempting to access the Azure Portal
and/or Azure Resources may have experienced degraded performance,
network drops, or timeouts. Customers may also have experienced
downstream impact to dependent Azure services due to an underlying
networking event.

**Preliminary Root Cause**: We determined that a network device,
affecting network traffic in Argentina, experienced a hardware fault and
that network traffic was not automatically rerouted.

**Mitigation**: We took the faulty network device out of rotation and
rerouted network traffic to mitigate the issue.

**Next Steps**: We will continue to investigate to establish the full
root cause and prevent future occurrences. Stay informed about Azure
service issues by creating custom service health alerts:
[https://aka.ms/ash-videos](https://aka.ms/ash-videos)
for video tutorials and
[https://aka.ms/ash-alerts](https://aka.ms/ash-alerts)
for how-to documentation.

## December 2020

## 14 

[12/14/2020]

RCA - Azure Active Directory - Authentication errors

Tracking ID: PS0T-790


**Summary of impact:** Between 08:01 and 09:20 UTC on 14 Dec 2020, a
subset of users in Europe might have encountered errors while
authenticating to Microsoft services and third-party applications.
Impacted users would have seen the error message: "AADSTS90033: A
transient error had occurred. Please try again". The impact was isolated
to users who were served through one specific back end scale unit in
Europe. Availability for Azure Active Directory (AD) authentication in
Europe dropped to a 95.85% success rate during the incident.
Availability in regions outside of Europe region remained within Service
Level Agreement (SLA).\
\
**Root Cause:** The Azure AD back end is a geo-distributed and
partitioned cloud directory store. The back end is partitioned into many
scale units with each scale unit having multiple storage units
distributed across multiple regions. Request processing for one of the
back end scale units experienced high latency and timeouts due to high
thread contention. The thread contention happened on the scale unit due
to a particular combination of requests and a recent change in service
topology for the scale unit rolled out previously.\
\
**Mitigation:** To mitigate the problem, engineers updated the backend
request routing to spread the requests to additional storage units.
Engineers also rolled back the service topology change that triggered
high thread contention.\
\
**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Augment existing load testing to validate the combination of call
    patterns that caused the problem.
-   Further root cause the reason for thread contention and make
    necessary fixes before re-enabling the service topology change.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our
survey: [https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## October 2020

## 27 

[10/27/2020]

RCA - Azure Active Directory B2C - North Europe / West Europe

Tracking ID: 8SHB-PD0



**Summary of Impact: **Between 08:40 UTC and 11:10 UTC on 27 Oct 2020, a
subset of customers using Azure Active Directory B2C (AAD B2C) in North
Europe/West Europe may have experienced errors when connecting to the
service. Customers may have received an HTTP status code 502 (Bad
Gateway) or HTTP status code 504 (Gateway Timeout).\
[]{style="text-decoration: underline"}\
**Root Cause:** In the North Europe/West Europe regions a configuration
change was compounded by a surge in traffic which exceeded the regions\'
operational thresholds and required the Azure AD B2C Service to be
augmented.\
\
**Mitigation: **We performed a change to the service configuration,
routing all traffic for the affected regions to an alternate production
environment. This production environment, which was located in the same
regions, had the necessary operational thresholds and measures in
place.\
\
**Next Steps: **We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Ensuring that the affected regions\' operational thresholds are set
    appropriately for the service.
-   Thorough testing of the new environment to ensure that it operates
    and scales as expected.
-   Reviewing our monitoring/alerts and making adjustments to ensure
    that proximity to operational thresholds is detected much earlier,
    enabling us to take proactive action to prevent such issues.
-   Ensuring that failover systems are in place to allow for more rapid
    routing of traffic between environments.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)

## 19 

[10/19/2020]

RCA - Azure Resource Manager - Issues accessing Azure resources via ARM

Tracking ID: ZLXD-HT8


**Summary of Impact:** Between 19:07 UTC and 22:20 UTC on 19 Oct 2020, a
subset of customers using resources that leverage Azure Resource Manager
(ARM) may have received intermittent errors while accessing or
performing service management operations - such as create, update,
delete - for multiple resources from the Azure portal or when using
CLI.\
\
**Root Cause:** The issue was caused by a misconfiguration in the broad
phase of a deployment for ARM services, which resulted in unanticipated
utilization of a single partition of Cosmos DB. The impact period was
due to the normal organic increase in requests exceeding limits for that
single Cosmos DB partition, which triggered throttling on those
requests, and as a result, the failures or errors were received for
those ARM requests.  We were alerted to impact based on internal
telemetry at 19:07 UTC and commenced investigation. By 20:30 UTC the
impact had become more widespread.

During integration testing and in the early phases of the rollout,
in-line with safe deployment practices, the deployment did not show any
problems or regression.\
\
**Mitigation:** A recent deployment was identified as the likely root
cause. In parallel, teams worked to disable the calls to Cosmos DB,
which were introduced by the deployment while also scaling up the Cosmos
DB instance, which collectively mitigated the impact. By 21:15 UTC
telemetry showed the expected decrease in errors and by 22:20 UTC impact
had subsided.\
\
**Next Steps:** We apologize for the impact to affected customers. We
are continuously taking steps to improve the Microsoft Azure Platform
and our processes to help ensure such incidents do not occur in the
future. In this case, this includes (but is not limited to):

-   Investigate auto-scaling and other resiliency techniques for Cosmos
    DB and other dependencies.
-   Review and ensure proactive monitoring procedures include expected
    thresholds for Cosmos DB and dependent services in test, Pilot and
    Early phases of deployment.
-   Review procedures, and create additional automated rules to catch
    this class of misconfiguration in the code during testing phase.

**Provide Feedback:** Please help us improve the Azure customer
communications experience by taking our survey:
[https://aka.ms/AzurePIRSurvey](https://aka.ms/AzurePIRSurvey)\

## 7 

[10/07/2020]

RCA - Issues accessing Microsoft and Azure services

Tracking ID: 8TY8-HT0


**Summary of Impact:** Between 18:20 UTC and 18:42 UTC on 07 Oct 2020, a
subset of customers may have encountered increased latency, packet loss,
failed connections and authentication failures across multiple Azure
services. Retries may have succeeded during this time and users who had
authenticated prior to the impact start time were less likely to
experience authentication issues.

Network resources were restored at 18:42 UTC; Azure services began
auto-mitigation. While other services had to undergo manual intervention
to recover this could have led to varying times of recovery for Azure
services. By 21:30 UTC it was confirmed that all Azure services had
recovered.

**Root Cause:** The incident was caused by a code defect in a version
update of a component that controls network traffic routing between
Azure regions. Because the main parameters of the new code were invoked
only at production scale and scope levels, the pre-production validation
process did not flag an issue. Following the deployment into production,
the code defect prevented anomaly detection from occurring, which
normally would catch an abnormal, sudden increase in the number of
unhealthy devices and force a health validation of those devices before
removing routes from the network. In this instance, due to the
prevention of the anomaly detection process, the Wide Area Network
Software Defined Network (WAN SDN) controller removed the corresponding
routes to these devices from the network. This code defect was triggered
1 hour after rollout of the service update at 18:20 UTC and caused
traffic to use sub-optimal routes, in-turn causing network congestion
and packet loss.
