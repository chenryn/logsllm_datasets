## [Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region] 

We would like to share more details with our customers about the events
that occurred with Amazon Elastic Compute Cloud ("EC2"), Amazon Elastic
Block Store ("EBS"), and Amazon Relational Database Service ("RDS")
earlier this week, and what we are doing to prevent these sorts of
issues from happening again. The service disruption primarily affected
EC2 instances, RDS instances, and a subset of EBS volumes in a single
Availability Zone in the EU West Region.

**Primary Outage**

The service disruption began at 10:41 AM PDT on August 7th when our
utility provider suffered a failure of a 110kV 10 megawatt transformer.
This failure resulted in a total loss of electricity supply to all of
their customers connected to this transformer, including a significant
portion of the affected AWS Availability Zone. The initial fault
diagnosis from our utility provider indicated that a lightning strike
caused the transformer to fail. The utility provider now believes it was
not a lightning strike, and is continuing to investigate root cause.

Normally, when utility power fails, electrical load is seamlessly picked
up by backup generators. Programmable Logic Controllers (PLCs) assure
that the electrical phase is synchronized between generators before
their power is brought online. In this case, one of the PLCs did not
complete the connection of a portion of the generators to bring them
online. We currently believe (supported by all observations of the state
and behavior of this PLC) that a large ground fault detected by the PLC
caused it to fail to complete its task. We are working with our supplier
and performing further analysis of the device involved to confirm. With
no utility power, and backup generators for a large portion of this
Availability Zone disabled, there was insufficient power for all of the
servers in the Availability Zone to continue operating. Uninterruptable
Power Supplies (UPSs) that provide a short period of battery power
quickly drained and we lost power to almost all of the EC2 instances and
58% of the EBS volumes in that Availability Zone. We also lost power to
the EC2 networking gear that connects this Availability Zone to the
Internet and connects this Availability Zone to the other Availability
Zones in the Region. This caused connectivity issues to the affected
Availability Zone and resulted in API errors when customers targeted API
requests (RunInstance, CreateVolume, etc.) to the impacted Availability
Zone.

At 11:05 AM PDT, we were seeing launch delays and API errors in all EU
West Availability Zones. There were two primary factors that contributed
to this. First, our EC2 management service (which handles API requests
to RunInstance, CreateVolume, etc.), has servers in each Availability
Zone. The management servers which receive requests continued to route
requests to management servers in the affected Availability Zone.
Because the management servers in the affected Availability Zone were
inaccessible, requests routed to those servers failed. Second, the EC2
management servers receiving requests were continuing to accept
RunInstances requests targeted at the impacted Availability Zone. Rather
than failing these requests immediately, they were queued and our
management servers attempted to process them. Fairly quickly, a large
number of these requests began to queue up and we overloaded the
management servers receiving requests, which were waiting for these
queued requests to complete. The combination of these two factors caused
long delays in launching instances and higher error rates for the EU
West EC2 APIs. At 12:00 PM PDT, when we disabled EC2 launches in the
affected Availability Zone and removed the failed management servers
from service, EC2 API launch times for other Availability Zones
recovered.

At 11:54 AM PDT, we had been able to bring some of the backup generators
online by manually phase-synchronizing the power sources. This restored
power to many of the EC2 instances and EBS volumes, but a majority of
the networking gear was still without power, so these restored instances
were still inaccessible. By 1:49 PM PDT, power had been restored to
enough of our network devices that we were able to re-establish
connectivity to the Availability Zone. Many of the instances and volumes
in the Availability Zone became accessible at this time.

**Recovering EBS in the Affected Availability Zone**

To understand why restoration of EBS took longer, it's helpful to
understand a little about the EBS architecture. EBS volume data is
replicated across a set of EBS nodes for durability and availability.
These nodes serve read and write requests to EC2 instances. If one node
loses connectivity to another node that it is replicating data to, it
must find and replicate its data to a new node (this is called
re-mirroring)\-- and it will block writes until it has found that new
node. From the perspective of an EC2 instance trying to do I/O on an EBS
volume that is blocking writes, the volume will appear "stuck."

On Sunday, when a large portion of the EBS servers lost power and shut
down, EBS volumes in the affected Availability Zone entered one of three
states: (1) online -- none of the nodes holding a volume's data lost
power, (2) re-mirroring -- a subset of the nodes storing the volume were
offline due to power loss and the remaining nodes were re-replicating
their data, and (3) offline -- all nodes lost power.

In the first case, EBS volumes continued to function normally.

In the second case, the majority of nodes were able to leverage the
significant amount of spare capacity in the affected Availability Zone,
successfully re-mirror, and enable the volume to recover. But, because
we had such an unusually large number of EBS volumes lose power, the
spare capacity we had on hand to support re-mirroring wasn't enough. We
ran out of spare capacity before all of the volumes were able to
successfully re-mirror. As a result, a number of customers' volumes
became "stuck" as they attempted to write to their volume, but their
volume had not yet found a new node to receive a replica. In order to
get the "stuck" volumes back online, we had to add more capacity. We
brought in additional labor to get more onsite capacity online and
trucked in servers from another Availability Zone in the Region. There
were delays as this was nighttime in Dublin and the logistics of
trucking required mobilizing transportation some distance from the
datacenter. Once the additional capacity was available, we were able to
recover the remaining volumes waiting for space to complete a successful
re-mirror.

In the third case, when an EC2 instance and all nodes containing EBS
volume replicas concurrently lose power, we cannot verify that all of
the writes to all of the nodes are completely consistent. If we cannot
confirm that all writes have been persisted to disk, then we cautiously
assume that the volume is in an inconsistent state (even though in many
cases the volume is actually consistent). Bringing a volume back in an
inconsistent state without the customer being aware could cause
undetectable, latent data corruption issues which could trigger a
serious impact later. For the volumes we assumed were inconsistent, we
produced a recovery snapshot to enable customers to create a new volume
and check its consistency before trying to use it. The process of
producing recovery snapshots was time-consuming because we had to first
copy all of the data from each node to Amazon Simple Storage Service
(Amazon S3), process that data to turn it into the snapshot storage
format, and re-copy the data to make it accessible from a customer's
account. Many of the volumes contained a lot of data (EBS volumes can
hold as much as 1 TB per volume). By 6:04 AM PDT on August 9th, we had
delivered approximately 38% of the recovery snapshots for these
potentially inconsistent volumes to customers. By 2:37 AM PDT on August
10th, 85% of the recovery snapshots had been delivered. By 8:25 PM PDT
on August 10th, we were 98% complete, with the remaining few snapshots
requiring manual attention.

**Impact on Amazon RDS**

RDS Instances were also affected by the disruption. RDS database
instances utilize EBS volumes for database and log storage. As a result,
the power outage in the affected Availability Zone had significant
impact on RDS as well. Single Availability Zone ("Single-AZ") database
instances in the affected Availability Zone were almost all initially
unavailable. They recovered when their corresponding EBS volumes were
restored or their databases were restored to new volumes. All Amazon RDS
Single-AZ database instances have automated backups turned on by
default. The majority of customers whose databases did not recover when
the first tranche of EBS volumes came back online, or could not be
recovered due to inconsistency of their volumes, used this backup
functionality to initiate Point-in-Time-Restore operations, per our
Service Health Dashboard instructions. Customers with automated backups
turned off, could not initiate Point-in-Time-Restores.

In addition to impacting Single-AZ database instances, the severity of
the event and nature of failure also caused a portion of Multiple
Availability Zone ("Multi-AZ") database instances to be impacted. Rapid
failover occurred for the vast majority of Multi-AZ databases, and all
affected Multi-AZ databases in the EU-West Region failed over without
data loss. However, a portion of Multi-AZ database instances experienced
prolonged failover times.

To understand why some Multi-AZ database instances did not promptly
failover, it is useful to understand how Multi-AZ databases work. RDS
Multi-AZ database instances consist of a "primary" database instance and
a synchronously replicated "secondary" database instance in another
Availability Zone. When the system detects that a primary database
instance might be failing, upon verification via a health check that the
primary is no longer accepting traffic, the secondary is promoted to
primary. This verification is important to avoid a "split brain"
situation, one where both the primary and the secondary database
instances are accepting writes and some writes exist on one database
while some exist on another. Similarly, when the system detects that a
secondary database instance is failing, upon performing the health check
and verifying that the secondary hasn't assumed the role of primary, the
primary will allow itself to continue as a Single-AZ database instance
until a new secondary is established and connected to the primary,
bringing the pair back into Multi-AZ status.

During the event, there were failures of Multi-AZ primary database
instances in the affected Availability Zone. However, for a portion of
these Multi-AZ primary-secondary pairs, a DNS connectivity issue related
to the power loss prevented the health check from finding the IP address
it needed to contact and kept the secondary from immediately assuming
the role of the primary. DNS connectivity was restored within 4 minutes,
and the majority of Multi-AZ deployments then completed failover within
an additional 10 minutes. However, the DNS connectivity issues triggered
a software bug that caused failover times to the secondary database
instance to extend significantly for a small subset of Multi-AZ
deployments.

This DNS connectivity issue also triggered extended failover times for a
small portion of Multi-AZ deployments with secondary replicas in the
affected Availability Zone. For these deployments, DNS connectivity
prevented the primary replicas from confirming their secondary replica's
status. In the rare case where the status of the secondary cannot be
determined, the primary does not make itself a Single AZ-mode database
instance and instead immediately involves the RDS team. This cautious
approach is taken to help prevent the "split brain" scenario described
above. Instead, an RDS engineer makes the decision to either promote the
secondary to primary (if the old primary is not functioning), or to move
the primary to Single-AZ mode (if the secondary is not functioning).
This approach minimizes the risk of data loss in edge cases, but extends
the period of time the Multi-AZ instance is unavailable.

**EBS Software Bug Impacting Snapshots**

Separately, and independent from issues emanating from the power
disruption, we discovered an error in the EBS software that cleans up
unused storage for snapshots after customers have deleted an EBS
snapshot. An EBS snapshot contains a set of pointers to blocks of data,
including the blocks shared between multiple snapshots. Each time a new
