SchoonerSQL Brings Five 9s to MySQL
January2012
MySQL is one of the most popular relational databases in use today. It is now owned by Oracle
Corporationandisavailableasopensourceaswellasseveralproprietaryofferingswithfullsupport.
Schooner Information Technology, Inc. (http://www.schoonerinfotech.com), has made significant
extensions to MySQL to improve its availability and replication performance. The goal of the resulting
product, SchoonerSQLTM, is to achieve five 9s availability, or about five minutes of downtime per year on
theaverage.
SchoonerSQL
SchoonerSQL is a highly available, high performance, transactional, crash-safe MySQL database. The
SchoonerSQL enhancements are implemented as extensions to MySQL’s underlying storage engine,
InnoDB. Schooner has an agreement with Oracle that entitles it to use MySQL and InnoDB source code
with distribution rights for the full build of MySQL. SchoonerSQL is certified by Oracle as being fully
compatiblewithMySQLEnterpriseandInnoDB.Noschemaorapplicationchangesarerequired.
ThepredominantelementsofSchoonerSQL’savailabilityandperformanceenhancementsinclude:
 MySQL clusters comprising a master node and up to seven slave nodes that are kept in exact
synchronismwiththemasternodeviasynchronousreplication.
 Scaling by adding additional clusters synchronized with the primary cluster via asynchronous
replication.
 HighlyparallelizedsynchronousreplicationwithinaclusteracrossLANsandMANs.
 HighlyparallelizedasynchronousreplicationbetweenclustersacrossLANs,MANs,andWANs.
 AutomatedfailoverwithinsecondsoverLANs,MANs,andWANs.
Transaction-based synchronous replication within a cluster guarantees that all nodes in the cluster have
the identical consistent view of the contents of the database. Querying any one of the cluster nodes will
havethesameresult.Thereisnostaledatadelivered.Ifthemasternodeshouldfail,thereisnodataloss
followingrecoverytoaslavenode.
SchoonerSQL provides a centralized GUI-based cluster administration manager that provides point-and-
click capabilities for cluster management, monitoring, tuning, and trouble shooting. Email alerts can be
configuredforcriticaleventssuchasadownednodeorafailover.
SchoonerSQL can use hard drives, SAN, or flashmemoryas its storage medium.It will run on anyof the
popularx86serversrunningLinuxorCentOS.
1
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

SchoonerSQL’s highavailabilitygoalis achieved viafastfailover toone ofmultiplesurviving nodes within
a cluster. SchoonerSQL’s performance advantages are achieved via its implementation of highly
parallelizedreplicationthreads supportingmulticoreprocessor environments.Usingflashmemoryinstead
ofharddrivescanincreasedatabaseperformancebyanorderofmagnitude.
SchoonerSQL Clusters
ClusterArchitecture
A SchoonerSQL cluster is the fundamental structure in a SchoonerSQL environment. A SchoonerSQL
cluster can contain up to eight nodes, as shown in Figure 1. Each node contains one or more instances
ofaMySQLdatabase.
One node in the cluster is the Master node. All
updates to the MySQL database are routed to
the Master node. The Master node is assigned
a virtual IP address (VIP), and applications
route all write/update/delete commands to that
VIP(vip0inFigure1).
The other nodes in the cluster (up to seven)
are slave nodes. The slave node databases
are kept in synchronism with the Master
database via synchronous replication, as
described later. Schooner calls these Read
Master nodes since their database contents
are always an exact copy of the Master
database.
Each Read Master node is assigned one or
more VIP’s to which applications can connect
in order to read the contents of the database (vip2 through vip7 in Figure 1). The Master node can also
support one or more read VIPs (vip1 in Figure 1). Multiple VIPs per node provide the basis for easy load
balancingbysimplymovingtheownershipofaVIPfromonenodetoanother.
The nodes can be interconnected either via a LAN or a MAN (metropolitan area network using fiber optic
connections). To ensure performance, the replication network should be separate from the client network
soas notto load down replicationcapacitywithclienttraffic.Toensure availability,normalredundancyof
networklinks,networkswitches,powerfeeds,andothersinglepointsoffailureshouldbeincorporated.
ZeroDowntimeUpgrades
Nodes in a SchoonerSQL cluster can be upgraded without taking the cluster down. To upgrade a node,
its load is moved to another node by reassigning its VIPs. The node upgrade is made, and the node is
thenreturnedtoserviceafterresynchronizingitsdatabase.
OnlineBackupsandRestores
SchoonerSQL provides online backups and restores. A full or incremental backup can be taken of the
databasewithoutimpactingcurrentupdateactivity.Thebackupisaconsistentcopyofthedatabasesince
itonlyincludescommittedtransactions.
A nodal database can be restored following a recovery, an upgrade, or the addition of a new node to the
clusterwithoutaffectingnormaloperation.
2
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

SynchronousReplication
The power of a SchoonerSQL cluster lies in the updates
synchronous replication that it uses to keep its
nodes synchronized. As shown in Figure 2, 1.updates
2.commit
replication is on a transaction basis. However, a Master ReadMaster
two-phasecommitprotocolisnotnecessary. Node Node
3.acknowledge
The Master node processes a transaction 4.commit 4.commit
normally until commit time. As it makes each
update in the transaction, it sends that update to MySQL InnoDB InnoDB MySQL
data data
the Read Master over the replication channel (1). base txlog txlog base
However, prior to committing the transaction, the
Master sends a commit directive to the Read SchoonerSQLSynchronousReplication
Master (2). The Read Master responds with an Figure2
acknowledgement to the Master that it has safe-stored the transaction (3). At this point, the transaction
canbecommittedbyboththeMasterandtheReadMaster(4).
Checksumsonreplicationmessagesareusedtoguardagainstdatacorruption.
If there are multiple Read Master nodes in a cluster, they are all kept in synchronism with the Master via
synchronous replication. SchoonerSQL’s synchronous replication engine is multithreaded to take
advantage of multicore processors so that multiple Read Masters can be synchronized simultaneously,
thusminimizingtheamountoftimethattheMasterhastowaitfortheReadMasteracknowledgements.
If a Read Master cannot positively acknowledge the commit directive from its Master, the Read Master is
removed from the cluster; and an attempt is made to resynchronize its database. If resynchronization is
successful,theReadMasterrejoinsthecluster.
The result of the synchronous replication sequence is that the Read Master databases are always in
exactsynchronismwiththeMaster’sdatabase.Thisprocesshasseveralbenefits:
 There is read consistency across all nodes since every node is updated simultaneously and all
nodeshavethesamecopyofthedatabase.
 No stale data is ever delivered to an application since there is no lag in a Read Master getting
dataupdates.
 NodataislostfollowingaMasternodefailuresinceeachReadMasterhasacompleteup-to-date
copyofthedatabase.
 The Read Masters can keep up with the Master node so that the Master node does not have to
bethrottledtoallowslowerslavestokeepup.
Failover and Recovery
Should a node fail in a cluster, SchoonerSQL provides rapid and automated recovery. Recovery typically
canbeeffectedinjustafewseconds.KeytofastrecoveryistheuseofvirtualIPaddresses,asdescribed
below.
RecoveringfromaMasterNodeFailure
Should the Master node fail, any Read Master can be promoted to Master since it has a complete and
consistent copy of the database. All that is required is that one Read Master be chosen to be the new
Master and the Master update VIP assigned to it. Thereafter, all new database modification commands
willbesenttothenewMaster.
3
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

An additional task is to rebalance the cluster so that read activity is evenly spread among the surviving
nodes. This is done by reassigning the read VIPs to Read Masters so that each handles its fair share of
thereadload.
This process is shown in Figure 3, using the cluster of Figure 1 as an example. Should the Master node
fail,theReadMaster2ispromotedtoMasterbyassigningtheupdateVIP,vip0,toit.Tobalanceits load,
its vip3 address is moved to Node 2, which is
also assigned the read VIP vip1 originally
serviced by the Master node. All this is done
within a few seconds. The cluster resumes
operation with Read Master 1 playing the role of
the Master node and the read activity
redistributedamongthesurvivingnodes.
When the original Master node is recovered, its
database is resynchronized with one of the Read
Master databases; and the VIPs are reassigned
to restore the cluster to its original configuration.
If the Master database is still intact, only the
transactions that were committed following its
failure need be applied to resynchronize it,
thereforespeedingrecovery.
RecoveringfromaReadMasterNodeFailure
The recovery from a Read Master failure is similar, except that a new Master does not have to be
configured. The read VIPs that the failed Read Master had been servicing are simply reassigned to
survivingnodes.
Uponrecoveryof thefailed ReadMaster,its database is incrementallyreconstructed,anditis returnedto
servicebyreassigningitsreadVIPstoit.
Scaling with Asynchronous Clusters
SchoonerSQL can be scaled beyond the capacity of a single cluster by configuring additional slave
clusters that are kept synchronized with the master cluster via asynchronous replication, as shown in
Figure 4. These additional slave clusters
can be contained within the same data
centerasthemasterclusterandconnected
to it via a LAN or MAN, or they may be
contained in distant data centers and
connectedviaaWAN.
By configuring multiple clusters, unlimited
read and write scaling is possible. Read
scaling is achieved by the additional Read
Mastersinthemultipleslaveclusters.Each
cluster may also act as a master cluster
processing its own database updates and
replicating them to the other clusters so
long as each cluster manages a different
database or database partition (data
collisions are not detected or resolved). In
thisway,writescalingmaybeachieved.
4
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

AsynchronousReplication
Synchronous replication is not practical between clusters that are separated by large distances because
the communication latency, which can range from tens to hundreds of milliseconds per message, would
slow down the application seriously as it awaited responses from distant clusters. Therefore,
asynchronousreplicationisusedforinter-clustersynchronization.
With asynchronous replication, the master cluster is unaware that its data is being replicated. Rather, the
remote slave cluster reads changes from the master cluster’s transaction log after the fact and applies
them to the nodes within the slave cluster. As shown in Figure 4, the Master node in the slave cluster is
alsoaslavenodetotheMastercluster.Therefore,itiscalledanAsynchronous Slave/Master.Itreadsthe
changes from themaster cluster’s transaction logasynchronouslyas aslave and applies them toits local
ReadMasternodessynchronouslyasaMasternode.
As with its synchronous replication engine, SchoonerSQL’s asynchronous replication is multithreaded so
that it can take advantage of multicore processors to replicate to multiple remote clusters simultaneously.
In addition, the Asynchronous Slave/Master node incorporates multithreaded appliers to apply data
changesrapidlytoitsdatabasewhileensuringdatabaseconsistency.
SchoonerSQL’s asynchronous replication trades some of the benefits of synchronous replication for
scalability. There is a lag, known as replication latency, between the time that an update is made to the
master cluster’s database and the time that the update appears in the database of the slave cluster.
Thus, reads on Read Masters in the slave cluster may on occasion be somewhat stale. However, the
parallelized architecture of the SchoonerSQL replication engine limits this delayto typicallya fraction of a
second.
Likewise, some data may be lost should the master cluster fail (an unlikely event since there may be
several slave nodes to which to fail over). Whatever data is in the replication pipeline that has not yet
madeittotheslavecluster (again,typicallylessthanasecond’s worthofdata) willbelost.Thisdatamay
beretrievablewhenthemasterclusterisreturnedtoservice.
SchoonerSQL’s asynchronous replication engine can interoperate in a mixed environment with traditional
MySQLasynchronousandsemisynchronousmastersandslaves.
RecoveringfromaMasterNodeFailure
Should the Master node in the master cluster fail, asynchronous replication will be lost. When a new
Master is configured in the master cluster, the slave cluster’s Asynchronous Slave/Master node will
reconnect with the transaction log of the new Master in the master cluster, and asynchronous replication
isrestoredwithnodataloss.
RecoveringfromanAsynchronousMaster/SlaveNodeFailure
Should the Asynchronous Master/Slave node in a slave cluster fail, one of the Read Masters in the slave
cluster will be promoted just as in the master cluster. The new Asynchronous Master/Slave will connect
with the Master transaction log in the Master cluster and asynchronous replication proceeds without data
loss.
RecoveringfromanAsynchronousReplicationChannelFailure
The reliability of the asynchronous replication channel is paramount to prevent split-brain operation. If
asynchronous replication is lost, the slavecluster cannotreceive updates andits database willfallbehind
that of the master cluster. Therefore, it is recommended that the channels used for asynchronous
replicationberedundant.
5
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

Should the asynchronous replication channel fail, the slave cluster continues to provide read services
even though some of the data may be stale. When the channel is returned to service, the slave cluster
databasewillberesynchronizedwiththemasterclusterdatabase;andnormaloperationisresumed.
Administration
SchoonerSQL provides an Administration Console with a simple point-and-click GUI interface for cluster,
node,anddatabasemonitoringandmanagement.Itsfunctionsinclude:
 OnlineprovisioningofserversandMySQLinstances.
 Createsynchronousandasynchronousclusters.
 AssignandremoveMySQLinstancesfromnodes.
 AssignVIPstoMasters,ReadMasters,andasyncmastersandslaves.
 Databasemigrationfromonenodetoanotherwithinacluster.
 Onlineupgrades.
 Automatedfailoverandfallback.
 Onlinefullandincrementalbackupandrestore.
 Monitoringandoptimizationstatisticsforphysicalandlogicalcomponents.
SchoonerSQLalsosupportsconfigurableemailalertsforcriticaleventssuchas:
 Instancecreationanddeletion.
 Instanceupordown.
 Instanceattachedordetached.
 Clustercreatedorremoved.
 ChangeinVIPassignment.
 Synchronousfailover.
 Asynchronousfailover.
 Split-brainoperation.
Performance
Benchmarks run by Schooner1 show significant performance improvement over distributed configurations
usingMySQL.Thebenchmarksarebasedupona1,000warehouseconfigurationwith32connections.
These benchmarks show a 3x performance advantage over MySQL asynchronous replication and a 2x
performance advantage over MySQL semisynchronous replication when using hard disk drives. For flash
memory, SchoonerSQLshows a5x performanceadvantageover MySQLasynchronous replication and a
4xperformanceadvantageoverMySQLsemisynchronousreplication.
Within SchoonerSQL, an order of magnitude improvement in performance is achievable by moving from
harddiskdrivestoflashmemory.
Supported Platforms
SchoonerSQL can run on Dell, HP, and IBM Intel (not AMD) two-, four- and eight-core x86 servers
runningRedHatLinuxorCentOS.Serversshouldhaveatleast64GBofmemory.
SchoonerSQL can use hard disk drives, SAN, or flash memory for data storage. It supports the use of
multipleflashdrivesinparallel.
1TheShortGuidetoMySQLHigh-AvailabilityOptions,SchoonerWhitePaper.
http://www.schoonerinfotech.com/whitepapers/Short_Guide_to_MySQL_HA_Options.pdf
6
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

SchoonerSQLisnotcompatiblewithvirtualmachineenvironments.
Licensing
SchoonerSQL is licensed as of this writing at $9,500 per year per server in the U.S. Multiyear discounts
areavailable,asaresite,project,andenterpriselicenses.
Schooner Membrain
Schooner Membrain is a flash-optimized implementation of the widely used memcached software cache
facility. Memcached is a general purpose distributed caching system used to speed up database-driven
websitesandotherapplications.
Schooner Membrain is used as a transient cache and a persistent data store for NoSQL (Not Only SQL).
Itshighly-concurrentmulticoreimplementationofmemcachedisextendedtoprovidetruepersistence.
Membraincanmanageupto512GBofflashmemory.
Summary
SchoonerSQL provides significant availability and performance advantages over standard MySQL
implementations. Certified byOracle as being fullycompatible with MySQL Enterprise and InnoDB, it can
beusedwithoutmodificationbyanyMySQLEnterpriseapplication.
Its performance improvement is achieved by highly parallelized multithreaded replication threads for use
with multicore processors. Benchmarks have shown a performance improvement over MySQL
configurationsoftwotofivetimes.
Its high availability is achieved through the use of synchronously replicated multinode clusters that
provide fast failover (within seconds) of a node failure. A SchoonerSQL cluster is scalable by configuring
it with up to eight nodes. Additional read and write scalability is provided by adding additional clusters
synchronized via asynchronous replication. A multicluster environment can be distributed over unlimited
geographicaldistancestoprovidefulldisastertolerance.
7
©2012SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com