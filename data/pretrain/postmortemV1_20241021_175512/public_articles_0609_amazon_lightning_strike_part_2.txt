3The25-minutedifferencebetweenthepowercompaniesreportofpowerlossandAmazon’sreportofpowerlosswasperhapsthe
timeittookforthebackupbatteriestodrain.
3
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

requests targeted at the impacted Availability Zone. Rather than failing these requests immediately,
they were queued and our management servers attempted to process them. Fairly quickly, a large
number of these requests began to queue up and we overloaded the management servers receiving
requests…”
11:54 AM PDT: “We had been able to bring some of the backup generators online by manually
phase-synchronizing the power sources. This restored power to manyof the EC2 instances and EBS
volumes, but a majority of the networking gear was still without power, so these restored instances
werestillinaccessible.
1:49 PM PDT: “Power had been restored to enough of our network devices that we were able to re-
establish connectivity to the Availability Zone. Many of the instances and volumes in the Availability
Zonebecameaccessibleatthistime.”
EBS Node Recovery: “EBS volumes in the affected Availability Zone entered one of three states: (1)
online – none of the nodes holding a volume’s data lost power, (2) re-mirroring – a subset of the
nodes storing the volume were offline due to power loss and the remaining nodes were re-replicating
theirdata,and(3)offline–allnodeslostpower.
“Inthefirstcase,EBSvolumescontinuedtofunctionnormally.
“In the second case, the majority of nodes were able to leverage the significant amount of spare
capacity in the affected Availability Zone to successfully re-mirror, and enable the volume to recover.
But,becausewehadsuchanunusuallylargenumberofEBSvolumeslosepower,thesparecapacity
we had on hand to support re-mirroring wasn’t enough.Weran out of spare capacitybefore all of the
volumes were able to successfully re-mirror. As a result, a number of customers’ volumes became
“stuck” as they attempted to write to their volume, but their volume had not yet found a new node to
receive a replica. In order to get the “stuck” volumes back online, we had to add more capacity. We
brought in additional labor to get more onsite capacity online and trucked in servers from another
AvailabilityZone inthe Region. There were delays as this was nighttimein Dublin and the logistics of
trucking required mobilizing transportation some distance from the datacenter. Once the additional
capacitywas available, we wereabletorecover theremainingvolumes waitingfor spacetocomplete
asuccessfulre-mirror.
“In the third case, when an EC2 instance and all nodes containing EBS volume replicas concurrently
lose power, we cannot verify that all of the writes to all of the nodes are completely consistent. …
Bringing a volume back in an inconsistent state without the customer being aware could cause
undetectable,latentdatacorruptionissues whichcouldtriggeraseriousimpactlater.Forthevolumes
we assumed were inconsistent, we produced a recovery snapshot to enable customers to create a
new volume and check its consistency before trying to use it. The process of producing recovery
snapshots was time-consuming because we had to first copy all of the data from each node to
Amazon Simple Storage Service (Amazon S3), process that data to turn it into the snapshot storage
format, and re-copy the data to make it accessible from a customer’s account. Many of the volumes
contained alotof data(EBSvolumes canholdas muchas 1TBper volume).… By8:25PMPDT on
August10th,wewere98% complete,withtheremainingfewsnapshotsrequiringmanualattention.”
Multi-AZFailoverFaults:“… aportionofMulti-AZdatabaseinstancesexperiencedprolongedfailover
times. … Multi-AZ database instances consist of a “primary” database instance and a synchronously
replicated “secondary” database instance in another Availability Zone. When the system detects that
a primary database instance might be failing, upon verification via a health check that the primary is
no longer accepting traffic, the secondary is promoted to primary. This verification is important to
avoid a “split brain” situation, one where both the primary and the secondary database instances are
accepting writes and some writes exist on one database while some exist on another. During the
event,therewerefailuresofMulti-AZprimarydatabaseinstancesintheaffectedAvailabilityZone.
4
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

“For a portion of these Multi-AZ primary-secondary pairs, a DNS connectivity issue related to the
power loss prevented the health check from finding the IP address it needed to contact and kept the
secondary from immediately assuming the role of the primary. … The DNS connectivity issues
triggered a software bug that caused failover times to the secondary database instance to extend
significantlyforasmallsubsetofMulti-AZdeployments.”
Snapshot Failures: “Separately, and independent from issues emanating from the power disruption,
we discovered an error in the EBS software that cleans up unused storage for snapshots after
customers have deleted an EBS snapshot. … At least one week passes from the time the snapshot
cleanup identification process runs before any blocks it has flagged for deletion are allowed to be
removed.Each day,itupdates the lists of blocks todelete,… and if anyblock eligiblefor deletionthe
day before now shows up in the most recent list of blocks referenced by active EBS volumes or
snapshots, the process flags those blocks for [manual] analysis. Actual deletion is executed by an
engineer who first, before running the actual deletion process, evaluates the blocks flagged for
analysis and verifies that there are no blocks in the list scheduled to be deleted that have been
flagged for analysis. The engineer must present his verification step to another engineer who
approvesthedeletion.
“InoneofthedaysleadinguptotheFriday,August5thdeletionrun,therewasahardwarefailurethat
the snapshot cleanup identification software did not correctly detect and handle. The result was that
thelistof snapshotreferences usedas inputtothecleanup process was incomplete.Becausethelist
of snapshot references was incomplete, the snapshot cleanup identification process incorrectly
believed a number of blocks were no longer referenced and had flagged those blocks for deletion …
On August 5th, the engineer running the snapshot deletion process checked the blocks flagged for
analysis beforerunningtheactualdeletionprocessintheEUWestRegion.Thehumanchecksinthis
process failedtodetectthe error andthedeletionprocess was executed.OnFridayevening,anerror
accessingoneoftheaffectedsnapshotstriggeredustoinvestigate.
“By Sundaymorning, August 7th, we had completed the work to fullyunderstand root cause, prevent
the problem from recurring, and build a tool that could create recovery snapshots for affected
snapshots. We then started to do the work necessary to map these affected snapshots to customers
and build the recovery snapshots, with the aim to communicate this information to customers by
Sunday night. However, before we got very far in this endeavor, the power event began. We had to
temporarily stop work on the snapshot issue to respond to the power event. Once we’d been able to
restore the majority of the EBS volumes affected by the power event, we returned to working on the
snapshot issue in parallel with restoring the remainder of the EBS volumes thatwere recovering from
thepowerevent.…”
Lessons Learned
Asaresultofthischainoffaults,Amazonistakingseveralcorrectiveactions:
 It will add redundancy and more isolation for its backup generator control PLCs so they are
insulatedfromotherfailures.
 It will address the resource saturation that affected API calls at the beginning of the disruption. It
will implement better load balancing to quickly take failed API management service hosts out of
production.
 It will continue to create additional capabilities that make it easier to develop and deploy
applicationsinmultipleAvailabilityZones.
 It drastically reduced the long recovery time required to recover stuck or inconsistent EBS
volumes. It will create the capability to recover volumes directly on the EBS servers upon
restorationofpowerwithouthavingtomovethedataoffofthoseserverstoS3.
5
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

 Ithascorrectedthesoftwarebugthatinappropriatelydeletedsnapshotdata.
 Itwillimprovethehandlingofhealthcheckfailures.
 Thoughitscommunicationofthestatusofthesituationwasgreatlyimprovedover pastinstances,
itwillstrivetoimprovecrisiscommunicationsevenfurther.
Amazon provided a ten-day usage credit for all customers who had an EBS volume in the compromised
AZ, whether or not they were affected by the outage. They provided a thirty-day credit for all customers
whosesnapshotblockswereinadvertentlydeleted.
Acknowledgements
In addition to the references previously noted, the following resources were used to provide material for
thisarticle:
LightningStrikeinDublinDownsAmazon,MicrosoftClouds,CIO.com;August8,2011.
AmazonCloudOutage:WhatCanBeLearned?,InformationWeek;August9,2011.
AmazonCloudOutageCleanupHitsSoftwareError,InformationWeek;August10,2011.
AmazonOutageAndCloudCommonSense,InformationWeek;August10,2011.
Questions raised around Amazon’s “lightning claims” at Dublin data center, TNW Insider; August 10,
2011.
AmazonAdmitsMultipleProblemsatDublinDatacenter,WallStreetJournal;August15,2011.
6
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com
