When the old and new parsers are both present during request handling a
buffer such as this will be passed to the function above:

    (gdb) p *in->buf
    $8 = {
      pos = 0x558a2f58be30 "<script type=\"",
      last = 0x558a2f58be3e "",

      [...]

      last_buf = 1,

      [...]
    }

Here there is data and `last_buf` is 1. When the new parser is not
present the final buffer that *contains data* looks like this:

    (gdb) p *in->buf
    $6 = {
      pos = 0x558a238e94f7 "<script type=\"",
      last = 0x558a238e9504 "",

      [...]

      last_buf = 0,

      [...]
    }

A final empty buffer (`pos` and `last` both NULL and `last_buf = 1`)
will follow that buffer but `ngx_http_email_parse_email` is not invoked
if the buffer is empty.

So, in the case where only the old parser is present, the final buffer
that contains data has `last_buf` set to 0. That means that `eof` will
be NULL. Now when trying to handle `script_consume_attr` with an
unfinished tag at the end of the buffer the `$lerr` will not be executed
because the parser believes (because of `last_buf`) that there may be
more data coming.

The situation is different when both parsers are present. `last_buf` is
1, `eof` is set to `pe` and the `$lerr` code runs. Here's the generated
code for it:

    /* #line 877 "ngx_http_email_filter_parser.rl" */
    { dd("script consume_attr failed");
                  {goto st1266;} }
         goto st0;

    [...]

    st1266:
        if ( ++p == pe )
            goto _test_eof1266;

The parser runs out of characters while trying to perform
`script_consume_attr` and `p` will be `pe` when that happens. Because
there's no `fhold` (that would have done `p--`) when the code jumps to
`st1266` `p` is incremented and is now past `pe`.

It then won't jump to `_test_eof1266` (where EOF checking would have
been performed) and will carry on past the end of the buffer trying to
parse the HTML document.

So, the bug had been dormant for years until the internal feng shui of
the buffers passed between NGINX filter modules changed with the
introduction of cf-html.

## Going bug hunting 

Research by IBM in the 1960s and 1970s showed that bugs tend to cluster
in what became known as "error-prone modules". Since we'd identified a
nasty pointer overrun in the code generated by Ragel it was prudent to
go hunting for other bugs.

Part of the infosec team started
[fuzzing](https://en.wikipedia.org/wiki/Fuzzing) the generated code to
look for other possible pointer overruns. Another team built test cases
from malformed web pages found in the wild. A software engineering team
began a manual inspection of the generated code looking for problems.

At that point it was decided to add explicit pointer checks to every
pointer access in the generated code to prevent any future problem and
to log any errors seen in the wild. The errors generated were fed to our
global error logging infrastructure for analysis and trending.

    #define SAFE_CHAR ({\
        if (!__builtin_expect(p < pe, 1)) {\
            ngx_log_error(NGX_LOG_CRIT, r->connection->log, 0, "email filter tried to access char past EOF");\
            RESET();\
            output_flat_saved(r, ctx);\
            BUF_STATE(output);\
            return NGX_ERROR;\
        }\
        *p;\
    })

And we began seeing log lines like this:

    2017/02/19 13:47:34 [crit] 27558#0: *2 email filter tried to access char past EOF while sending response to client, client: 127.0.0.1, server: localhost, request: "GET /malformed-test.html HTTP/1.1‚Äù

Every log line indicates an HTTP request that could have leaked private
memory. By logging how often the problem was occurring we hoped to get
an estimate of the number of times HTTP request had leaked memory while
the bug was present.

In order for the memory to leak the following had to be true:

The final buffer containing data had to finish with a malformed script
or img tag\
The buffer had to be less than 4k in length (otherwise NGINX would
crash)\
The customer had to either have Email Obfuscation enabled (because it
uses both the old and new parsers as we transition),\
... or Automatic HTTPS Rewrites/Server Side Excludes (which use the new
parser) in combination with another Cloudflare feature that uses the old
parser.\
... and Server-Side Excludes only execute if the client IP has a poor
reputation (i.e. it does not work for most visitors).

That explains why the buffer overrun resulting in a leak of memory
occurred so infrequently.

Additionally, the Email Obfuscation feature (which uses both parsers and
would have enabled the bug to happen on the most Cloudflare sites) was
only enabled on February 13 (four days before Tavis' report).

The three features implicated were rolled out as follows. The earliest
date memory could have leaked is 2016-09-22.

2016-09-22 Automatic HTTP Rewrites enabled\
2017-01-30 Server-Side Excludes migrated to new parser\
2017-02-13 Email Obfuscation partially migrated to new parser\
2017-02-18 Google reports problem to Cloudflare and leak is stopped

The greatest potential impact occurred for four days starting on
February 13 because Automatic HTTP Rewrites wasn't widely used and
Server-Side Excludes only activate for malicious IP addresses.

## Internal impact of the bug 

Cloudflare runs multiple separate processes on the edge machines and
these provide process and memory isolation. The memory being leaked was
from a process based on NGINX that does HTTP handling. It has a separate
heap from processes doing SSL, image re-compression, and caching, which
meant that we were quickly able to determine that SSL private keys
belonging to our customers could not have been leaked.

However, the memory space being leaked did still contain sensitive
information. One obvious piece of information that had leaked was a
private key used to secure connections between Cloudflare machines.

When processing HTTP requests for customers' web sites our edge machines
talk to each other within a rack, within a data center, and between data
centers for logging, caching, and to retrieve web pages from origin web
servers.

In response to heightened concerns about surveillance activities against
Internet companies, we decided in 2013 to encrypt all connections
between Cloudflare machines to prevent such an attack even if the
machines were sitting in the same rack.

The private key leaked was the one used for this machine to machine
encryption. There were also a small number of secrets used internally at
Cloudflare for authentication present.

## External impact and cache clearing 

More concerning was that fact that chunks of in-flight HTTP requests for
Cloudflare customers were present in the dumped memory. That meant that
information that should have been private could be disclosed.

This included HTTP headers, chunks of POST data (perhaps containing
passwords), JSON for API calls, URI parameters, cookies and other
sensitive information used for authentication (such as API keys and
OAuth tokens).

Because Cloudflare operates a large, shared infrastructure an HTTP
request to a Cloudflare web site that was vulnerable to this problem
could reveal information about an unrelated other Cloudflare site.

An additional problem was that Google (and other search engines) had
cached some of the leaked memory through their normal crawling and
caching processes. We wanted to ensure that this memory was scrubbed
from search engine caches before the public disclosure of the problem so
that third-parties would not be able to go hunting for sensitive
information.

Our natural inclination was to get news of the bug out as quickly as
possible, but we felt we had a duty of care to ensure that search engine
caches were scrubbed before a public announcement.

The infosec team worked to identify URIs in search engine caches that
had leaked memory and get them purged. With the help of Google, Yahoo,
Bing and others, we found 770 unique URIs that had been cached and which
contained leaked memory. Those 770 unique URIs covered 161 unique
domains. The leaked memory has been purged with the help of the search
engines.

We also undertook other search expeditions looking for potentially
leaked information on sites like Pastebin and did not find anything.

## Some lessons 

The engineers working on the new HTML parser had been so worried about
bugs affecting our service that they had spent hours verifying that it
did not contain security problems.

Unfortunately, it was the ancient piece of software that contained a
latent security problem and that problem only showed up as we were in
the process of migrating away from it. Our internal infosec team is now
undertaking a project to fuzz older software looking for potential other
security problems.

## Detailed Timeline 

We are very grateful to our colleagues at Google for contacting us about
the problem and working closely with us through its resolution. All of
which occurred without any reports that outside parties had identified
the issue or exploited it.

All times are UTC.

2017-02-18 0011 Tweet from Tavis Ormandy asking for Cloudflare contact
information\
2017-02-18 0032 Cloudflare receives details of bug from Google\
2017-02-18 0040 Cross functional team assembles in San Francisco\
2017-02-18 0119 Email Obfuscation disabled worldwide\
2017-02-18 0122 London team joins\
2017-02-18 0424 Automatic HTTPS Rewrites disabled worldwide\
2017-02-18 0722 Patch implementing kill switch for cf-html parser
deployed worldwide

2017-02-20 2159 SAFE_CHAR fix deployed globally

2017-02-21 1803 Automatic HTTPS Rewrites, Server-Side Excludes and Email
Obfuscation re-enabled worldwide

*NOTE: This post was updated to reflect updated information.*

