# The Consul outage that never happened 

Sometimes a good plan is the best tool for the job.

When things go wrong on a large website, it can be fun to read the
dramatic stories of high pressure incidents where nothing goes as
planned. It makes for good reading. Every once in a while though, we get
a success story. Every once in a while, things go exactly as planned.

[GitLab.com](http://GitLab.com) is a large, high availability instance
of GitLab. It is maintained by the [Infrastructure
group](/company/team/?department=infrastructure-department), which
currently consists of 20 to 24 engineers (depending on how you count),
four managers, and a director, distributed all around the world.
Distributed, in this case, does not mean across a few different offices.
There are three or four major cities which have more than one engineer
but with the exception of coworking days nobody is working from the same
building.

In order to handle the load generated by about four million users
working on around 12 million projects, GitLab.com breaks out the
individual components of the GitLab product and currently spreads them
out over 271 production servers.

The site is slowly migrating to using Hashicorp\'s
[Consul](https://www.consul.io) for service location. Consul can be
thought of like DNS, in that it associates a well-known name with the
actual physical location of that service. It also provides other useful
functions such as storing dynamic configuration for services, as well as
locking for clusters. All of the Consul client and server components
talk to each other over encrypted connections. These connections require
a certificate at each end to validate the identity of the client and
server and to provide the encryption key. The main component of
GitLab.com which currently relies on this service is the database and
its high availability system
[Patroni](https://patroni.readthedocs.io/en/latest/). Like any website
that provides functionality and not just information, the database is
the central service that everything else depends on. Without the
database, the website, API, CI pipelines, and git services will all deny
requests and return errors.


The
[issue](https://gitlab.com/gitlab-com/gl-infra/production/issues/1037)
came to our attention when a database engineer noticed that one of our
database servers in the staging environment could not reconnect to the
staging Consul server after the database node was restarted.

It turns out that the TLS certificate was expired. This is normally a
simple fix. Someone would go to the Certificate Authority (CA) and
request a renewal -- or if that fails, generate a new certificate to be
signed by the same CA. That certificate would replace the expired copy
and the service would be restarted. All of the connections should
reestablish using the new certificate and just like with any other
rolling configuration change, it should be transparent to all users.

After looking everywhere, and asking everyone on the team, we got the
definitive answer that the CA key we created a year ago for this
self-signed certificate had been lost.

These test certificates were generated for the original proof-of-concept
installation for this service and were never intended to be transitioned
into production. However, since everything was working perfectly, the
expired test certificate had not been calling attention to itself. A few
things should have been done, including: Rebuilding the service with
production in mind; conducting a production readiness review; and
monitoring. But a year ago, our production team was in a very different
place. We were small with just four engineers, and three new team
members: A manager, director, and engineer, all of whom were still
onboarding. We were less focused on the gaps that led to this oversight
a year ago and more focused on fixing the urgent problem today.


First, we needed to validate the problem using the information we\'d
gathered. Since we couldn\'t update the existing certificates, we turned
validation off on the client that couldn\'t connect. Turning validation
off didn\'t change anything since the encrypted connections validate
both the cluster side and client side. Next, we changed the setting on
one server node in the cluster and so the restarted client could then
connect to the server node. The problem now was that the server could no
longer connect to any other cluster node and could not rejoin the
cluster. The server we changed was not validating connections, meaning
it was ignoring the expired certificate of its peers in the cluster but
the peers were not returning the favor. They were shunning it, putting
the whole cluster in a degraded state.

We realized that no matter what we did, some servers and some clients
would not be able to connect to each other until after the change had
been made everywhere and after every service was restarted.
Unfortunately, we were talking about 255 of our 271 servers. Our tool
set is designed for gradual rollouts, not simultaneous actions.

We were unsure why the site was even still online because if the clients
and services could not connect it was unclear why anything was still
working. We ran a small test, confirming the site was only working
because the connections were already established when the certificates
expired. Any interruption of these long-running connections would cause
them to revalidate the new connections, resulting in them rejecting all
new connections across the fleet.

> Effectively, we were in the middle of an outage that had already
> started, but hadn\'t yet gotten to the point of taking down the site.


We declared an incident and began testing every angle we could think of
in the staging environment, including:

-   Reloading the configuration of the running service, which worked
    fine and did not drop connections, but the [certificate
    settings](https://github.com/hashicorp/consul/pull/4204) are [not
    included in the reloadable
    settings](https://www.consul.io/docs/agent/options.html#reloadable-configuration)
    for our version of Consul.
-   Simultaneous restarts of various services, which worked, but our
    tools wouldn\'t allow us to do that with ALL of the nodes at once.

Everything we tried indicated that we had to break those existing
connections in order to activate any change, and that we could only
avoid downtime if that happened on **ALL nodes at precisely the same
time**.

Every problem uncovered other problems and as we were troubleshooting
one of our production Consul servers became unresponsive, disconnected
all SSH sessions, and would not allow anyone to reconnect. The server
did not log any errors. It was still sending monitoring data and was
still participating in the Consul cluster. If we restarted the server,
then it would not have been able to reconnect to its peers and we would
have an even number of nodes. Not having quorum in the cluster would
have been dangerous when we went to restart all of the nodes, so we left
it in that state for the moment.


Once the troubleshooting was finished [it was time to start
planning](https://gitlab.com/gitlab-com/gl-infra/production/issues/1042).

There were a few ways to solve the problem. We could:

-   Replace the CA and the certificates with new self-signed ones.
-   Change the CA setting to point to the system store, allowing us to
    use certificates signed by our standard certificate provider and
    then replace the certificates.
-   Disable the validation of the dates so that the expired certificate
    would not cause connections to fail.

All of these options would incur the same risks and involve the same
risky restart of all services at once.

We picked the last option. Our reasoning was that disabling the
validation would eliminate the immediate risk and give us time to slowly
roll out a properly robust solution in the near future, without having
to worry about disrupting the whole system. It was also the [smallest
and most incremental change](/handbook/values/#iteration).


While there was some time pressure due to the [risk of network
connections being
interrupted](https://gitlab.com/gitlab-com/gl-infra/production/issues/1037#note_201745119),
we had to consider the reality of working across timezones as we planned
our solution.

> We decided not to hand it off to the European shift, who were coming
> online soon. Being a [globally
> distributed](/company/culture/all-remote/) team, we had already handed
> things off from the end of the day in Mongolia, through Eastern and
> Western Europe and across the Americas, and were approaching the end
> of the day in Hawaii and New Zealand.

Australia still had a few more hours and Mongolia had started the day
again, but the folks who had been troubleshooting it throughout the day
had a pretty good handle on what needed to happen and what could go
wrong. It made sense for them to be the ones to do the work. We decided
to make a \"Break Glass\" plan instead. This was a merge request with
all of the changes and information necessary for the European shift to
get us back into a good state in case a full outage happened before
anyone who had been working on it woke up. Everyone slept better knowing
that we had a plan that would work even if it could not be executed
without causing down time. If we were already experiencing down time,
there would be no problem.


In the morning (HST) everything was how we left it so we started
planning how to change the settings and restart all of the services
without downtime. Our normal management tools were out because of the
time it takes to roll out changes. Even sequential tools such as
`knife ssh`, `mussh`, or `ansible` wouldn\'t work because the change had
to be **precisely simultaneous**. Someone joked about setting it up in
`cron` which led us to the standard linux `at` command (a relative of
the more widely used `batch`). `cron` would require cleanup afterward
but an `at` command can be pushed out ahead of time with a sequential
tool and will run a command at a precise time on all machines. Back in
the days of hands-on, bare metal system administration, it was a useful
trick for running one-time maintenance in the middle of the night or
making it look like you were working when you weren\'t. Now `at` has
become more obscure with the trend toward managing fleets of servers
rather than big monolithic central machines. We chose to run the command
`sudo systemctl restart consul.service`. We tested this in staging to
verify that our Ubuntu distribution made environment variables like
`$PATH` available, and that `sudo` did not ask for a password. On some
distributions (older CentOS especially) this is not always the case.

With those successful tests, we still needed to change the config files.
Luckily, there is nothing that prevents changing these ahead of time
since the changes aren\'t picked up until the service restarts. We
didn\'t want to do this step at the same time as the service restart so
we could validate the changes and keep the `at` command as small as
possible. We decided not to use Chef to push out the change because we
needed complete and immediate transparency. Any nodes that did not get
the change would fail after the restart. `mussh` was the tool that
offered the most control and visibility while still being able to change
all hosts with one command.

We also had to disable the Chef client so that it didn\'t overwrite the
changes between when they were written and when the service restarted.

Before running anything we also needed to address the one Consul server
that we couldn\'t access. It likely just needed to be rebooted and would
come up and be unable to reconnect to the cluster. The best option was
to do this manually just before starting the rest of the procedure.

Once we had mapped out the plan we practiced it in the disaster recovery
environment. We used the disaster recovery environment instead of the
staging environment because all of the nodes in the staging environment
had already been restarted, so there were no long-running connections to
test. Making the disaster recovery environment was the next best option.
It did not go perfectly since the database in this environment was
already in an unhealthy state but it gave us valuable information to
adjust the plan.
