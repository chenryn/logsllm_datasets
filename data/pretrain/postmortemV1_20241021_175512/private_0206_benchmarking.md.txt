Let’s Get an Availability Benchmark
June2007
Benchmarks have been around almost as long as computers, Benchmarks are immensely
valuable to buyers of data processing systems for comparing cost and performance of the
systems which they are considering. Theyare of equal value to system vendors to compare
thecompetivenessoftheirproductswithothers.
In the early days of computing, there were no formal benchmarking specifications; and
vendors werefreetodefinetheirownbenchmarks.This,ofcourse,ledtoagreatdisparityin
results and benchmark claims. In response to this problem, several formal and audited
benchmarks have evolved, such as those from the Standards and Performance Evaluation
Corporation(SPEC)1andtheTransactionProcessingPerformanceCouncil(TPC).2
Virtually all of today’s accepted benchmarks focus on performance and cost. But what good
is a super fast system if it is down and not available to its users? It seems that system
availability is just as important todayto system purchasers as is raw performance. Shouldn’t
availabilitybepartofthesebenchmarks?
PerformanceBenchmarks
As mentioned above, primary examples of today’s benchmark specifications are those from
SPEC and TPC. The SPEC benchmarks focus on raw processing power and are useful for
comparing system performance for applications such as graphics, mail servers, network file
systems,Webservers,andhighperformancecomputing(HPC)applications.
The TPC benchmarks, however, deal with transaction processing systems. As such, they
relatemorecloselytothetopicsofinteresttoushereattheAvailabilityDigest.
TransactionProcessingBenchmarketing
The Transaction Processing Performance Council was founded in 1988 by Omni Serlin and
Tom Sawyer in response to the growing problem of “benchmarketing,” the inappropriate use
ofquestionablebenchmarkresultsinmarketingpromotionsandcompetitivecomparisons.At
the time, the accepted benchmarks were the TP1 benchmark and the Debit/Credit
benchmark(newlyproposedbyJimGrayofTandemComputers).
TPC’s first benchmark was TPC-A, which was a formalization of the TP1 and Debit/Credit
benchmarks. However, even though a formal and accepted benchmark for system
performancenowexisted,therecontinuedtobemanycomplaintsofbenchmarketing.
1www.spec.org
2
www.tpc.org
1
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

In response, TPC initiated a review process wherein each benchmark test had to be
extensively documented and then audited by TPC before it could be published as a formal
TPCbenchmarkresult.
Today,allpublishedresultsofTPCbenchmarkshavebeenauditedandverifiedbyTPC.
CurrentTPCBenchmarks
TheTPCbenchmarkshaveevolvedsignificantlyovertheyearsandnowinclude:
 TPC-C, which is an extended version of TPC-A. It deals with the performance of
transaction processing systems, including networking, processing, and database
access. It specifies a fairly extensive mix of transactions in an order management
environment.
 TPC-H,whichmeasuresdecisionsupportforad-hocqueries.
 TPC-App,whichfocusesonWebApplicationServers.
 TPC-E, which is a TPC-C-like benchmark but which includes many requirements to
makeitmorecloselyresembletheenterprisecomputinginfrastructure.TPC-Eisstill
intheTPCapprovalprocess.
TheTPC-Cbenchmarkistheonemostpertinenttothisdiscussion.
TPCPerformanceMetrics
The results of the TPC-C transaction benchmark are provided as two numbers, along with a
detailed description of the configuration of the system that achieved the results. Thesemeasures
are:
 tpmC, the number of TPC-C transactions per minute that the system was able to
consistentlyprocess.
 Price/tpmC - the cost of the system expressed as its cost for one transaction per minute
(tpm)ofcapacity.
For instance, if a system is able to process 100 transactions per minute and costs $100,000, its
costmetricis$1,000pertpm.
System performance has come a long way. The first TPC benchmarks showed a capacity in the
order of 2,000 transactions per minute (using a TPC-A transaction, simpler than today’s TPC-C
transaction) and a cost of about $400 per tpm. Today, commercially available systems are
achieving 4,000,000 TPC-C transactions per minute and cost less than $3 per tpm. This is
roughly a 2,000-fold increase in performance and a 130-fold reduction in price in less than 20
years,equatingtoovera250,000-foldincreaseinprice-performance!
AvailabilityBenchmarks
DowntimeCosts
Inthe earlydays of benchmarking,performancewas king.Therewas notmuchattentiongivento
system reliability because (a) high-availability technology was not there yet and (b) systems did
nothavetorun24x7.
2
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

Buttoday,asmarketsbecomeglobalandastheWebbecomesubiquitous,systemsareexpected
to operate continually. Not only is downtime unacceptable, but its cost can be enormous. A USA
Todaysurveyof 200data center managers foundthat over 80% of these managers reported that
their downtime costs exceeded $50,000 per hour. For over 25%, downtime cost exceeded
$500,000perhour.3
Asaresult,today’scostofsystem ownershipcanbegreatlyinfluencedbydowntime.Think about
it. Consider a critical application in which the cost of downtime is $100,000 per hour. The
company running the application chooses a system which costs $1 million dollars and which has
an availability of three 9s (considered today to be a “high availability” system). An availability of
three 9s means that the system will be expected to be down an average of only eight hours per
year. However, over the five-year life of this million dollar system, it will be down about 40 hours
atacostof$4milliondollars.
$1 million dollars for the system and $4 million dollars for downtime. There has to be a better
way.
One solution to this problem is to provide purchasers of systems not only performance/cost
metrics but also availability measures. A company could then make a much more informed
decisionastothesystemwhichwouldbebestforitspurposes.
For instance, consider two systems, both of which meet a company’s performance requirements.
System A costs $200,000 and has an availability of three 9s (eight hours of downtime per year).
System B costs $1 million dollars and has an availability of four 9s (0.8 hours of downtime per
year).Overafive-yearsystem life,System AcanexpecttobedownforfortyhoursandSystem B
canbeexpectedtobedownforfourhours.
If downtime costs the company $10,000 per hour, the initial cost plus the cost of downtime for
System A is $600,000. That same cost for System B is $1,040,000. System A is clearly more
attractive.
However, if the company’s cost of downtime is $100,000 per hour, the same costs for System A
are $4,200,000 and those costs for System B are $1,400,000. System B is clearly the winner in
thiscase.
One interesting observation casts some doubt on this analysis. Too many times, a manager is
judged byhis short-term performance.Consequently, hemaychoose System Awithoutregardto
its longterm costonlybecauseit has asmaller initialcost.Heis now ahero and leaves the long-
termdamagetohissuccessor.
AvailabilityMetrics
How do we characterize availability? The classic wayis to provide the probability that the system
willbeup,oroperational(oralternativelythatitwillbedown,whichis1–theprobabilitythatit will
beup).
For highly available systems, this probability can be awkward to express. For instance, it is
difficult to say or read .999999. Does this number have five 9s in it? Six 9s? Seven 9s?
Therefore, it is conventional to express availability as a number of nines. The above availability
would be expressed as six 9s. A system with three 9s availability would be up .999 of the time
(99.9%)andwouldbedown1-.999=.001ofthetime(0.1%).
3
USAToday,page1B;July3,2006.
3
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

Therearemanyothermetricsthatcanbeused.Commonmetricsincludethemeantimebetween
failures(MTBF),whichistheaveragetimethatasystemcanbeexpectedtobeup,andthemean
time to repair (MTR), which is the average time that it will take to return the system to service
onceithasfailed.
AvailabilityandfailureprobabilityarerelatedtoMTBFandMTRasfollows:
MTBF
availability A 
MTBFMTR
MTR
probabilityoffailureF1A 
MTBF
MeasuringAvailability
Before measuring availability, it must be more carefully defined. Most data processing
professionalstodayacceptthatavailabilityisnotsimplytheproportionoftimethatasystem isup.
Availability has to be measured from the user’s perspective. If a network faults denies access to
10% of the users, the system is up but not so far as those users are concerned. If the system is
expected to provide subsecond response times but is currently responding in 10 seconds, it may
wellbeconsideredtobeuselessandthereforebedowntotheusers.
One form for the definition of availability might be that the system is up if it is serving 60% of the
users and is responding to those users within the response time requirement 90% of the time.
Another, perhaps more useful, method would be to use partial availabilities in the availability
calculation. For instance, following the above example, it the system were satisfactorily serving
60%ofitsusers,thenduringthattimeithasanavailabilityof60%.
One problem with considering an availability benchmark is that availability can be very hard to
measureandverify.For instance,considerasystem withanavailabilityoffour9s andthus giving
anexpecteddowntimeof0.8hoursper year.Thisdoes notmeanthatthesystemwillbedown48
minutes every year. It is more likely that it will fail occasionally and will take longer to return to
service. For instance, experience may show that this system in the field fails about once every
fiveyearsandrequiresanaverageoffourhourstoreturntoservice.
We cannot reasonably measure the availability of these systems for benchmarking purposes.
Even if there were many of these systems in the field, by the time we accumulated reliable data,
the system would probably be obsolete and no longer sold. Looking over our shoulders at
availabilityprovidesnousefulservicetopotentialbuyers.
However,todaythisisallwehavebutinamoregenericsense.Therehavebeenmanystudiesof
the availabilities of various systems based on field experience. For instance, in 2002, Gartner
Grouppublishedbasedonfieldstatisticsthefollowingavailabilitiesforvarioussystems:
HPNonStop .9999
Mainframe .999
OpenVMS .998
AS400 .998
HPUX .996
Tru64 .996
Solaris .995
NTCluster .992-.995
These availabilities took into account all sources of failure, such as those caused by hardware,
vendor software, application software, operator error, networks, and environmental faults (power,
4
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

air conditioning, etc.). Though these results are quite dated, they have been roughly verified by
severallaterstudies.
This is the best we have today for availability benchmarks. They are very broad-brush, they are
outdated,andtheylackformalverification.
SoWhatDoWeDo
Availability benchmarking is too important today to simply give up and say that it isn’t possible.
Dean Brock of Data General has submitted an excellent TPC white paper that deals with this
subject.4 An approach that he suggests evaluates the system architecture and gives a score for
variousavailabilityattributes.Theseattributescouldinclude:
 System Architecture
Towhatextentcanthesystemrecoverfromaprocessingnodefailure:
Singlenode
Cluster
Active/active
Remotedatamirroring
Demonstratedfailoverorrecoverytime
 NoSinglePointofFailurewithinaSingleNode
Every critical component should have a backup which is put into service automatically
uponthefailureofitsmate.Thesecomponentsinclude:
Processors
Memory
Controllers
Controllerpaths
Disks(mirroredgetsahigherscorethanRAID5)
Networks
Powersupplies
Fans
 HotRepairofSystemComponents
Failed components can be removed and replaced with operational components without
takingdownthesystem:
Allofthecomponentslistedabove
 DisasterBackupandRecovery
Availabilityofthebackupsite
Currencyofbackupdataatthebackupsite:
o Backuptapes
o Virtualtape
o Asynchronousreplication
o Synchronousreplication
Demonstratedtimetoputthebackupsiteintoservice
 System AdministrationFacilities
Monitoring
Faultreporting
Automaticrecovery
4http://www.tpc.org/information/other/articles/ha.asp
5
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman

 EnvironmentalNeeds
SizeofbackupUPSpowersourceincludedintheconfiguration
Sizeofbackupairconditioningincludedintheconfiguration
 VendorQAProcesses
SoftwareQA
HardwareQA
This is undoubtedly only a partial list and will certainly grow if this approach becomes accepted.
Rather than simply a checklist of attributes, a careful availability analysis could assign weights to
these attributes. Based on these weights, an availability score could then be determined; and
systems could be compared relative to their expected availability (one will never know what the
realavailabilityofasystemisuntillongafteritispurchased).
A complexity in the weighting of attributes is that many weights will be conditional. For instance,
the impact of nodal single points of failure is much more important in single-node configurations
thanitisinclustersoractive/activesystems.
One problem not addressed in the above discussion is how to relate an availability score to
system downtime so that the costs of downtime can be estimated. It may be that some years of
fieldexperiencecanyieldarelationtotheavailabilityscoreandtheprobabilityofdowntime.
Summary
We have spent a decade perfecting performance measuring via formal benchmarks. It is now
timetostarttoconsiderhowwecanaddavailabilitymeasurestothoseresults.
There are many problems to solve in order to achieve meaningful availability benchmarks,
primarily due to the infrequency of failures in the field. However, a careful analysis of availability
attributescanleadtoagoodstarttowardobtainingusefulavailabilitymetrics.
6
Thisdocumentmaynotbedistributedinanywaybyelectronicmeans.
©2007SombersAssociates,Inc.,andW.H.Highleyman
