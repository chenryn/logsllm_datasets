Help! My Data Center is Down!
Part 1: Power Outages
October2011
Nothing strikes fear in the hearts of IT management (and corporate management for that matter) like that
of losing the entire corporate IT infrastructure. To make sure that this never happens, companies invest
heavilyintheir data centers usingtechnologies ranging from fault-tolerant NonStopsystems toredundant
datacenters.
The Uptime Institute defines a four-tier certification for data centers.1 The most reliable, a Tier IV data
center, has redundant everything and can achieve an availability of 0.99995. This represents an average
downtimeoftenminutesperyear.
Many companies consider this unacceptable since a data center mayvirtually never fail; but should such
a failure occur, it might be days before IT services can resume. The company might well be out of
business bythen.Therefore,it is commontoprovide oneor morebackupdatacenters sothatoperations
cancontinuewithinafewhoursofadata-centerfailure.
Nevertheless,thereisadisturbingincidenceofsignificantdata-centeroutages.Theseincidentsshowthat
itis notenoughtotrytoprotectagainstanyevent–fire,flood,poweroutage,networkfailure,andsoon–
that might take down a data center. Some event that was not even envisioned is going to happen
sometime, and it will take down a data center somewhere. If this is your data center, your Business
ContinuityPlanhadbetterspecifyhowtheenterpriseisgoingtocontinueintheabsenceofITservicesfor
hoursorevendays.
In this series, we review from the archives of the Availability Digest some Never Again horror stories that
show some of the unlikely events that have taken down entire data centers, including in some cases the
disasterrecoverysiteaswell.Inthisfirstpart,welookatsomeunusualpoweroutagestories.2
Google’s App Engine
The Google App Engine is a compute cloud service for users to develop and host their web applications
in Google’s data centers. The App Engine virtualizes applications across not only multiple servers but
also across multiple data centers to ensure that no fault will take down the applications. Should a server
fail due to equipment problems or even due to an entire data-center outage, the applications on that
server will be rapidly migrated to a surviving server, where they will continue to run. Or so Google
thought.
1DataCenterTiers,Webopedia.
2OurthankstoTheConnectionforgivinguspermissiontoreprintthisseriesofarticles.
1
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

On Wednesday, February 24, 2010, a Google data center hosting the Google App Engine suffered a
major power outage.3 The power was out for about thirty minutes, but Google’s backup power kicked in
andcontinuedtopowerthedatacenter.
The only problem was that for some reason (never disclosed), about 25% of the servers did not receive
backup power and subsequently went down. This caused their applications to fail over to the surviving
servers in the data center, which were not configured to handle that much additional load. The server
failurescascadeduntiltheentiredatacenterwentdown.
The simultanous failure of all the servers was an event that had never been considered. There was a
greatdeal of confusionabouthowtohandlethemultipleserver failures.Thefirstdecision was tofailover
thedatacenter.Thedocumentedprocedureswerefollowedbutledtoanunsuccessfulfailover.
It then looked like the primary data center had returned to operation, so processing was returned to it.
This turned out to be an erroneous observation since the primarydata center became operational only in
the presence of reduced load. It could still not handle the full load, and it once again failed. Finally,
knowledgeable technical personnel were reached; and the backup data center was brought successfully
online. Two-and-a-half hours had passed since the initial failure. If things had gone properly, it would
havetakenonlyabouttenminutestofailovertothebackupdatacenter.
A post-mortem showed that the failover procedures had just been improved to make them more
automatic. Unfortunately, the failover documentation was still in the process of being updated. Parts of
thedocumentationfor thenewfailover procedures incorrectlyreferredtotheolddata-center configuration
rather than to the upgraded configuration. Clearly, the newly documented failover procedures had not
beentested;norhadthestaffbeentrained.Otherwise,theerrorswouldhavebeenfound.
The Coffee Pot Fiasco
It was time to upgrade the nodes in another company’s active/active system. This was a major upgrade
involving new hardware and a new operating system. The company had successfully applied rolling
upgradestoitsnodesinthepastbytakingdownonenodeatatime,upgradingit,andreintroducingitinto
thesystem.4
As best practices dictate, the system at each node was powered by a separate circuit protected by an
uninterruptiblepowersupply(UPS).Whenthenewsystemwasrolledinatoneofthenodes,theinstallers
found that all of the UPS power connectors were being used. There was not one available for the new
system.Asaconsequence,thenewsystemcouldnotbepoweredup.
So as not to delay the upgrade, the new node was temporarily connected to the facility’s unprotected
power. Though this power source was not protected by a UPS, the plan was to correct this problem in
shortorderbyaddinganadditionalconnectortotheUPSoutput.
Evidently, no record was made of this issue on any task list. The required power connector change was
forgotten; and the upgraded system continued to run successfully for quite a while on the unprotected
powersource.
Astimewenton,theloadontheunprotectedcircuitsgraduallyincreasedasthecompanygrew.Moreand
morepeoplemeantmorelighting,moreheating,moreairconditioning,andmoreworkstations.
One fateful day, an employee performed a normal, everyday task. He or she plugged in the coffee pot to
make fresh coffee. This was the straw that broke the camel’s back. The coffee pot blew the circuit
3PoorDocumentationSnagsGoogle,AvailabilityDigest;April2010.
http://www.availabilitydigest.com/public_articles/0504/google_power_out.pdf
4Active/ActiveSave#1:CoffeePotTakesDownNode,AvailabilityDigest;November2006.
http://www.availabilitydigest.com/private/0102/a-a_save_1_coffee_pot_takes_node_down.pdf
2
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

breaker, taking down everything that was on that circuit. This included dropping primary power to the
upgradedsystem,whichhadneverbeenmovedtotheUPScircuit.
Thenode,however,keptonrunningforawhile.ItwassupportedbyaninternalUPSthatkeptitoperating
longenoughtosaveitsstateandtoshutitdowngracefullyfollowingaprimarypowerfailure.
Fast action on the part of the staff at the site restored the primary power in just 35 seconds – an
admirable feat. Unfortunately, the system’s internal UPS onlylasted for 30 seconds.The node shut down
andsuffereda30-minuteoutageuntilitwasbroughtbackonline.
If the system had not been an active/active system, its users would have been denied service for a half
hour. However, as it turned out, the users assigned to the failed node were quickly switched over to
survivingnodesandsufferednoapparentoutage.
The Planet Blows Up
The Planet provides dedicated servers for a variety of companies, many of which are web-hosting
companies.OperatingsixdatacentersinTexas,ThePlanetisthelargest,privately-helddedicated-server
hostingcompanyintheworld.
On Saturday, May 31, 2008, at 5:55 PM, an explosion took down The Planet’s Houston data center.5 A
short circuit in a high-volume wire conduit set a transformer on fire, which then caused an explosion of
battery-acidfumesfromtheUPSbattery-backupsystem.
Theexplosion was strongenoughto blow downthree walls surroundingthe electricalequipmentroom on
the first floor of the data center. It blew apart the power-transfer switch that transferred the data center
from utility power to backup diesel generator power, thus knocking out power to the entire data center.
Fortunately,noonewasinjured.
For safety reasons, the fire department evacuated the building and directed that the backup generators
could not be turned on. It wasn’t until after 10 pm that staff was allowed back into the building to assess
thedamage.
Thestaffwas abletomovesomecustomerstonewserversinother datacenters,butlimitedcooling
capacity in the data centers limited this to only a few customers. Shortly after the explosion, The
Planethadtodenyfurtherrequestsforreprovisioning.
In total, 9,000 servers went down. Three thousand of the affected servers were on the first floor of
the data center, and 6,000 servers were on the second floor. Since The Planet provided dedicated
servers for thousands of web-hosting companies, the web sites that were taken down measured in
themillions.
The Planet’s staff was able to restore power to the second-floor servers; and around 5 pm Monday
evening-twodaysaftertheexplosion,thesecond-floorserverswereonceagainoperational.
Restoring power to the first-floor servers was a much more difficult challenge due to the extensive
damage.Eachoftheseservers wasbroughtonlineassoonaspossible;butfourdaysaftertheexplosion,
fullservicetoallcustomershadyettoberestored.
5ThePlanetBlowsUp,AvailabilityDigest;September2008.
http://www.availabilitydigest.com/public_articles/0309/planet_explosion.pdf
3
©2011SombersAssociates,Inc.,andW.H.Highleyman
Fordiscussion,contacteditorest.com

A Truck Downs Rackspace
Rackspace, headquartered in San Antonio, Texas, provides web-hosting services for thousands of web
sites around the world. It operates eight data centers – four in the U.S. and four in the U.K. Founded in
