        reverberating effects on the system owners. The result is a
        \'pile on\' effect in which the gravity of the anomaly increases
        and the difficulty of unwinding its consequences becomes
        greater.
    -   Some buffering of the anomaly consequences was provided by the
        technical components (e.g. queues, recruitable resources, and
        failover mechanisms).
    -   These buffering mechanisms are normally capable of handling the
        distributed system\'s functional variation.
    -   The anomalies exhausted those mechanisms.
    -   The people involved also took action to buffer the anomaly
        consequences.
    -   These actions sometimes preceded the saturation of the technical
        components buffering capacity.

### 3.4.2 Features of the anomaly responses 

Participants often said things like \"oh, we have that too\" or \"that
has happened to us.\" Common features include:

-   [surprise](http://snafucatchers.github.io/#3_4_2_Surprise)
-   [uncertainty](http://snafucatchers.github.io/#3_4_2_Uncertainty)
-   [the role of
    search](http://snafucatchers.github.io/#3_4_2_The_role_of_search)
-   [the role of system
    representations](http://snafucatchers.github.io/#3_4_2_Evolving_system_representations)
-   [generate
    hypotheses](http://snafucatchers.github.io/#3_4_2_Generating_hypotheses)
-   [use of basic
    tools](http://snafucatchers.github.io/#3_4_2_Basic_tools)
-   [coordinating work and
    action](http://snafucatchers.github.io/#3_4_2_Coordination)
-   [communications in joint
    activity](http://snafucatchers.github.io/#3_4_2_Communications_in_joint_activity)
-   [shared
    artifacts](http://snafucatchers.github.io/#3_4_2_Shared_artifacts)
-   [the consequences of escalating consequences and disturbance
    management](http://snafucatchers.github.io/#3_4_2_The_consequences_of_escalating_consequences)
-   [managing risk](http://snafucatchers.github.io/#3_4_2_Managing_risk)
-   [goal
    sacrifice](http://snafucatchers.github.io/#3_4_2_Goal_sacrifice)

These features are discussed in more detail below.

#### Surprise 

In all cases, the participants experienced surprise. Although anomalies
are relatively common for all groups, each case had specific surprising
features. These were mainly discoveries of previously unappreciated
dependencies that generated the anomaly or obstructed its resolution or
both. The fact that experts can be surprised in this way is evidence of
systemic complexity and also of operational variety.

A common experience was \"I didn\'t know that it worked this way.\"
People are surprised when they find out that their own mental model of
The System (in the Figure 1 or Figure 2 sense) doesn\'t match the
behavior of the system.

More rarely a surprise produces astonishment, a sense that the world has
changed or is unrecognizable in an important way. This is sometimes
called fundamental surprise ([Lanir,
1983](https://www.yumpu.com/en/document/view/35528654/fundamental-surprises-zvi-lanir-decision-research-1201-oak-);
[Woods et al.,
2010](https://www.amazon.com/Behind-Human-Error-David-Woods/dp/0754678342),
pp 215-219). Bob Wears four characteristics of fundamental surprise that
make it different from situational surprise ([Wears, R. L., & Webb, L.
K., 2011](http://snafucatchers.github.io/#ref_35)):

1.  situational surprise is compatible with previous beliefs about 'how
    things work'; fundamental surprise refutes basic beliefs;
2.  it is possible to anticipate situational surprise; fundamental
    surprise cannot be anticipated;
3.  situational surprise can be averted by tuning warning systems;
    fundamental surprise challenges models that produced success in the
    past;
4.  learning from situational surprise closes quickly; learning from
    fundamental surprise requires model revision and changes that
    reverberate.

Information technology anomalies are frequently fundamental surprises.
This is due to the difficulty in maintaining adequate mental models of
what is below the line, understanding how this connects to what is above
the line \-- crossing the line, as software systems grow in complexity
and continuously change.

This adjustment of the understanding of what the system was and how it
worked was important to both immediate anomaly management and how
post-anomaly system repairs add to the ongoing processes of change.

#### Uncertainty 

Uncertainty is closely linked to surprise. Sorting out the uncertainty
that attends a SNAFU is an important cognitive task for people
responsible for the technological artifacts.

At the earliest stage the details of a SNAFU are unknown and the range
of possibilities is large.

First indications of a SNAFU are often uninformative about its
significance. A SNAFU may be a minor event or herald a devastating loss
or something in between ([Klein et al.
2005](http://snafucatchers.github.io/#ref_18)).

It is commonly not immediately clear what response is required. Some
SNAFUs will resolve themselves (e.g. brief network traffic bursts), some
will require immediate intervention (e.g. restart of a process), while
others will need to be addressed in complicated ways (e.g. deploying new
code).

It can be hard to tell if there is actually a SNAFU occurring. Large IT
systems have complex behaviors and substantial moment-to-moment
variability. Instantaneous performance changes may or may not indicate
that some problem is occurring but almost always worth examining,
recording, or discussing.^[1](http://snafucatchers.github.io/#foot_1)^

Failure to distinguish between a SNAFU-in-progress and ordinary
variability and discrepancies can lead to missing opportunities to
intervene in the evolution of a critical event or, alternately, lead to
wasting of valuable time and attention on what is essentially noise.

#### The role of search 

Developing an understanding of the anomaly required participants to
search for information about the system and its function. Evidence for
these searches is found in the command sequences and dialog (including
some detailed chat records) that occurred during the response. The
participants were engaged in a particularly complicated form of search:
exploring the external world based on their internal representations of
that world, available affordances, and multiple, interacting goals. In
every case the search was effortful and iterative. Neither the sources
of the anomaly nor the route to correction were immediately apprehended.

The control and modulation of such complex searches is presently not
understood but it is clear from other studies that there is intense,
meaningful interaction between the progression of the anomaly, available
(and potentially available) information, and the quality of
collaboration across multiple cognitive agents ([Watts-Perotti, & Woods,
2009](http://snafucatchers.github.io/#ref_34%22)).

#### Evolving system representations 

In each case we can infer that the practitioners used internal models of
the technical system to direct their search, interpret results, and plan
further investigations and corrections. Mental models are
representations of some part of the external world \-- in this case a
complicated computer system and the factors acting on it. The experts\'
ability to manage the anomaly was heavily dependent on the quality of
their system representation and its use to derive possible sources and
routes for the anomaly as observed.

The complexity of the technical artifacts precludes a comprehensive
understanding of the system \[see [Woods\'
theorem](http://snafucatchers.github.io/#woods_theorem)\]. Instead,
experts demonstrated their ability to use their incomplete, fragmented
models of the system as starting points for exploration and to quickly
revise and expand their models during the anomaly response in order to
understand the anomaly and develop and assess possible solutions.

#### Generating hypotheses 

Process tracing ([Woods, 1993](http://snafucatchers.github.io/#ref_39))
of parts of the anomaly responses yielded protocols indicating that
participants formed, tested, and abandoned multiple hypotheses during
their exploration of the anomaly and search for its sources. This work
was quite fast and efficient; participants were quick to seek and use
information, especially in the early stages of the response when the
nature, extent, and severity of the anomaly was unknown. The earliest
activities, however, did not appear to be hypothesis-driven but instead
focused on hypothesis generation (Woods and Hollnagel 2006). These
efforts were sweeping looks across the environment looking for cues.
This behavior is consistent with recognition primed decision making
(RPD; [Klein, 1993](http://snafucatchers.github.io/#ref_16)) and
explicit in [Allspaw (2015)](http://snafucatchers.github.io/#ref_2).

#### Basic tools 

Although automation (e.g. Chef, Travis CI) and sophisticated monitoring
(e.g. Nagios, ELK) are integral in their technical systems,
practitioners use basic tools for assessment and modification during
anomalies.

Command line tools entered from the terminal prompt are heavily used.
These commands are a *lingua franca* among practitioners. Although
automation and monitoring provide convenient and efficient ways of doing
things and keeping track of nominal performance, when things are broken
or confusing or when decisive actions are taken, tools that provide
tight interaction with the operating system are commonly used. The
command line tools allow \-- for want of a better word \-- *primal*
interaction with the platform. The automation and monitoring
applications are treated, by comparison, as being indirect or even
opaque.

Self-generated records (\"logs\") were used extensively both for
post-anomaly reconstruction *and in real time for anomaly response*.

By their nature, logs are sequential, documentary accounts of processes
and conditions within the system. The sequential character is crucial to
the use of logs; much of the reasoning about the system is causal
inference about influences and pre-conditions.

Logs can be so voluminous that human processing of them is difficult;
much effort has been expended on developing programs to analyze logs.
\[Writing programs to analyze the output of other programs is a way of
managing this data overload.\] But in virtually all cases, those
struggling to cope with complex failures searched through the logs and
analyzed prior system behaviors using them directly via a terminal
window.

#### Coordination 

Each anomaly response involved joint activity coordinating their efforts
to understand the events underway and synchronizing their activities to
mitigate and resolve the anomalies ([Klein, Feltovich, et al.,
2005](http://snafucatchers.github.io/#ref_19)); ([Woods and Hollnagel,
2006](http://snafucatchers.github.io/#ref_43)).

The detailed traces show that investigating and repairing sometimes
proceeded in parallel along different avenues, sometimes was distinctly
cooperative and in tandem, and sometimes diverged widely in purpose and
direction.

Coordinating these different threads of work demanded attention and
effort beyond that directed towards the anomaly per se.

The number of people involved in the anomaly response started out small,
with one or two people, but quickly rose as the existence and
significance of the anomaly was appreciated. In some cases, the initial
responders recruited other experts to help in assessing the situation.
In others, experts noticed the anomaly or observed the response
activities and joined the response process on their own initiative.

We identified little explicit coordination of experts responding to the
anomaly. Instead, coordination was largely implicit with individuals
taking on roles, performing actions, and contributing questions,
information, or observations to the ongoing process. The technical
leaders emerged quickly.

The participants noted that those not directly involved in the response
we careful to avoid interrupting those engaged in the anomaly response.
For example, although the physical workspace and IRC channels were open
and accessible, co-workers stayed away from the responders' workstations
and did not interact with them via IRC during an escalating outage. The
participants also noted that demands for status updates, extraneous
requests, and work required to bring newly joined persons \"up to
speed\" have made other anomaly responses difficult.

This coordination effort is among the most interesting and potentially
important aspects of the anomaly response.

Participants in an incident were sometimes outside the immediate sphere
of those taking action. They may be data gathering for the purposes of
disseminating information that direct participants are unable to share
as a means of reducing context switching while attempting to resolve the
issue.

In some cases, spectators made themselves available to handle potential
future incidents since normal staffing was absorbed in the current
anomaly.

#### Communications in joint activity 

Communication among the multiple people and roles engaged in the event
was prominent in all the cases.

-   [Chat]{style="font-weight:bold;"}: all groups used some form of chat
    application (IRC, Slack, Pidgin, Hipchat, etc.) during the anomaly
    response.
    -   In some cases, chat allowed communication with remote locations,
        but it was also used \-- to varying degrees \-- by people in the
        same location.
    -   Messages appeared on more than one chat channel during the
        response. Initial exchanges appeared on technical or operational
        channels. Later, other channels (management, organizational
        coordination, and internal broadcast) were used for internal
        notification and updating.
    -   In one case the chat system itself failed during the anomaly
        response. The participants then used telephone (both conference
        line and person-to-person) contact to replace some of the chat
        functionality.
    -   Although all groups have one or more chat \'bots\', no one
        reported that bots contributed to or interfered with chat
        communications.
    -   Because chat exchanges are preserved and recoverable, records of
        chat were especially useful in reconstructing the anomaly
        response for subsequent investigation and for postmortems.
-   [Face-to-face verbal communication]{style="font-weight:bold;"}: all
    groups reported that face-to-face verbal exchanges were part of the
    anomaly response. Most groups have shared workspaces where workers
    can speak to each other. In contrast with chat, these exchanges are
    not routinely recorded.
-   [Non-verbal communication]{style="font-weight:bold;"}: shared
    workspaces promote observation of activities by others including
    managers and those affected by the anomaly. In one case this
    contributed to the anomaly response by reducing the need for
    explicit communications about the anomaly. One participant commented
    that, during the event, \"they could see we were busy and they kind
    of stood back and let us have room to work\".
-   Information about the event was communicated internally and
    externally by people assigned to that task. This alerting or
    updating activity is considered part of anomaly response by all the
    groups. The format, frequency, and detail in this updating varied
    but the activity had a common goal: Letting users of the system know
