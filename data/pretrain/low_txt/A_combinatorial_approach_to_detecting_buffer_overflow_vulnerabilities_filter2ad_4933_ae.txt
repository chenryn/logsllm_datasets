### Testing-Based Approaches for Detecting Buffer Overflow Vulnerabilities

In this section, we focus on testing-based approaches, which involve actual program executions. We will not discuss methods based purely on static analysis [18][22][30][40], as these employ fundamentally different techniques. As mentioned in Section I, static analysis is often plagued by false positives and negatives.

#### Black-Box Testing Approaches

We begin with black-box testing approaches, which are most closely related to our work. Fuzzing [35] is one of the most widely used black-box testing methods in security testing. Fuzzing typically starts with one or more valid inputs and then randomly mutates these inputs to generate new ones. Advanced fuzzing techniques [14][15] can also incorporate domain knowledge and use heuristics, such as assigning different weights to different components.

As noted in Section I, a major limitation of fuzzing is its poor code coverage [35]. In contrast, our approach systematically samples the input space to achieve combinatorial coverage. Empirical studies suggest a strong correlation between combinatorial coverage and code coverage [3][11]. However, our approach is not fully automated. Specifically, attack-payload and attack-control parameters, along with their values, are identified manually. Our empirical studies indicate that this identification can be performed with reasonable effort. Additionally, this manual process allows us to leverage domain knowledge that may be readily available in practice. Many black-box testing techniques, including combinatorial testing in its original form, also require manual identification of individual parameters and values. Further discussion on this aspect of our approach is provided in the next section.

#### Combining Symbolic Execution and Testing

Recently, there has been growing interest in approaches that combine symbolic execution and testing [5][13][41]. In these methods, symbolic execution is used to collect path conditions, which consist of a sequence of branching decisions. These branching decisions are then systematically negated to derive test inputs that, when executed, explore different paths. To detect buffer overflow vulnerabilities, memory safety constraints are formulated and solved using these path conditions. A potential issue with these approaches is path explosion. Techniques such as functional summaries, generational search, and length abstraction have been developed to mitigate this problem. These approaches generate tests in a fully automatic manner. However, symbolic execution often requires extensive instrumentation, either at the source or binary level, making the solutions specific to particular languages, build environments, or platforms. Symbolic execution can also be much slower than actual program executions. More importantly, for large and complex programs, the number of constraints that need to be solved presents significant challenges to the capacity of existing constraint solvers.

### Conclusions and Future Work

In this paper, we presented a black-box testing approach for detecting buffer overflow vulnerabilities. Our approach simulates the process an attacker typically follows to exploit a buffer overflow vulnerability. A novel aspect of our method is the adaptation of combinatorial testing, a general software testing technique, to the domain of security testing. Combinatorial testing often achieves a high level of code coverage, which our approach leverages. We implemented our approach in a prototype tool called Tance. Empirical results from applying Tance to five open-source programs demonstrate that our approach is effective in detecting buffer overflow vulnerabilities.

In our approach, attack-payload and attack-control parameters and their values are identified manually based on specifications or domain knowledge, or both. We provide guidelines for performing this identification, and our empirical studies show that these guidelines are very effective and can be followed with reasonable effort. Security testing is often performed after functional testing, so knowledge and experience gained from functional testing can be utilized to effectively identify these parameters and values. While proper identification of these parameters and values enhances the effectiveness of our approach, it does not need to be perfect. In practice, we can adjust the test effort based on available resources, identifying more parameters and values for greater confidence at the cost of more tests, or fewer parameters and values to reduce test effort, potentially missing some vulnerabilities.

While fully automated solutions are often desirable, semi-automated solutions like ours have their merits. Our approach allows us to leverage domain knowledge, which can make the testing process more efficient and may uncover bugs that would otherwise go undetected. Fully automated and semi-automated solutions can complement each other. For example, we can first apply fuzzing and then our approach to achieve higher fault coverage. The flexibility of our approach in scaling test effort further facilitates its use in combination with other methods.

In future work, we plan to develop lightweight static analysis techniques to automatically identify attack-payload and attack-control parameters and their values. These techniques can be applied when source code is available, enabling full automation of our test generation process. This will allow for a direct comparison between our approach and existing methods that combine symbolic execution and testing, further evaluating the effectiveness of our approach.

### Acknowledgment

This work is partly supported by a grant (Award No. 70NANB10H168) from the Information Technology Lab (ITL) of the National Institute of Standards and Technology (NIST). Authorized licensed use limited to: Tsinghua University. Downloaded on March 18, 2021, at 14:29:42 UTC from IEEE Xplore. Restrictions apply.

### References

[1] D. Aitel, “The Advantages of Block-based Protocol Analysis for Security Testing”, Immunity Inc, 2002. DOI= http://www.net-security.org/article.php?id=378.
[2] J. H. Andrews, L.C. Briand, Y. Labiche, and A.S. Namin, “Using Mutation Analysis for Assessing and Comparing Testing Coverage Criteria”, IEEE Transactions on Software Engineering, 32(8): 608-624, 2006.
[3] K. Burr, and W. Young, “Combinatorial Test Techniques: Table-based Automation, Test Generation and Code Coverage”, Proceedings of the International Conference on Software Testing Analysis and Review, pp. 503-513, 1998.
[4] R. Bryce, C. J. Colbourn, M.B. Cohen, “A framework of greedy methods for constructing interaction tests,” Proceedngs of the 27th International Conference on Software Engineering (ICSE), pp. 146-155, 2005.
[5] C. Cadar, V. Ganesh, P.M. Pawlowski, D.L. Dill, and D.R. Engler, “EXE: Automatically Generating Inputs of Death”, Proceedings of the 13th ACM Conference on Computer and Communications Security (CCS), pp. 322-335, 2006.
[6] B. Chess, and G. McGraw, “Static Analysis for Security”, IEEE Security and Privacy, 2(6):76-79, 2004.
[7] M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton, “The AETG System: An Approach to Testing Based on Combinatorial Design”, IEEE Transactions on Software Engineering, 23(7): 437-444, 1997.
[8] M. B. Cohen, M. B. Dwyer, and J. Shi, “Constructing interaction test suites for highly-configurable systems in the presence of constraints: a greedy approach”, IEEE Transactions on Software Engineering, 34(5), pp. 633-650, 2008.
[9] C. Cowan, C. Pu, D. Maier, H. Hintony, J. Walpole, P. Bakke, S. Beattie, A. Grier, and Q. Zhang, “StackGuard: automatic adaptive detection and prevention of buffer-overflow attacks”, Proceedings of the 7th conference on USENIX Security Symposium, pp. 5-5, 1998.
[10] R. Dhurjati and V. Adve, “Backwards-compatible Array Bounds Checking for C with Very Low Overhead”, Proceedings of the 28th IEEE International Conference on Software Engineering, pp. 162-171, 2006.
[11] S. Dunietz, W. K. Ehrlich, B. D. Szablak, C. L. Mallows, and A. Iannino, “Applying design of experiments to software testing”, Proceedings of the International Conference on Software Engineering, pp. 205–215, 1997.
[12] Ghttpd-1.4.4. DOI= http://gaztek.sourceforge.net/ghttpd/.
[13] P. Godefroid, N. Klarlund, and K. Sen, “DART: Directed Automated Random Testing”, Proceedings of the 2005 ACM SIGPAN Conference on Programming Language Design and Implementation (PLDI), pp. 213-233, 2005.
[14] P. Godefroid, A. Kiezun, and M.Y. Levin, “Grammar-based Whitebox Fuzzing”, Proceedings of the ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation (PLDI), pp. 206-215, 2008.
[15] P. Godefroid, M. Levin, and D. Monlnar, “Automated Whitebox Fuzz Testing”, Proceedings of the Network and Distributed Security Symposium, 2008.
[16] M. Grindal, J. Offutt, and S.F. Andler, “Combination Testing Strategies: A Survey”, Software Testing, Verification and Reliability, 15(3): 167-199, 2005.
[17] Gzip-1.2.4. DOI= http://www.gzip.org/.
[18] B. Hackett, M. Das, D. Wang, Z. Yang, “Modular Checking for Buffer Overflows in the Large”, Proceedings of the 28th International Conference on Software Engineering, pp. 232-241, 2006.
[19] Hypermail-2.1.3. DOI= http://www.hypermail.org/.
[20] R. Kuhn, D.R. Wallace, and A.M. Gallo Jr, “Software Fault Interactions and Implications for Software Testing”, IEEE Transactions on Software Engineering, 30(6):418-421, 2004.
[21] R. Kunh and C. Johnson, “Vulnerability Trends: Measuring progress”, IEEE IT Professional, 12(4):51-53, 2010.
[22] W. Le, M. L. Soffa, “Marple: a Demand-Driven Path-Sensitive Buffer Overflow Detector”, Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering, pp. 272-283, 2008.
[23] Y. Lei, R. Carver, R. Kacker, D. Kung, “A Combinatorial Strategy for Testing Concurrent Programs”, Journal of Software Testing, Verification, and Reliability, 17(4):207-225, 2007.
[24] Y. Lei, R. Kacker, R.D. Kuhn, V. Okun, and J. Lawrence, “IPOG/IPOD: Efficient Test Generation for Multi-way Combinatorial Testing”, Software Testing, Verification and Reliability, 18(3):287-297, 2007.
[25] A. Mathur, “Foundations of Software Testing”, Addison-Wesley Professional, 2008.
[26] G. McGraw, “Software Security”, IEEE Security & Privacy, 2(2): 80-83, 2004.
[27] National Vulnerability Database. DOI= http://nvd.nist.gov/.
[28] Nullhttpd-0.5.0. DOI= http://www.nulllogic.ca/httpd/.
[29] Pine-3.96. DOI= http://www.washington.edu/pine/.
[30] M. Pistoia, S. Chandra, S.J. Fink, and E. Yahav, “A Survey of Static Analysis Methods for Identifying Security Vulnerabilities in Software Systems”, IBM Systems Journal, 46(2):265-288, 2007.
[31] J. Roning, M. Laakso, A. Takanen and R. Kaksonen, “PROTOS – Systematic Approach to Eliminate Software Vulnerabilities”. DOI= http://www.ee.oulu.fi/research/ouspg/.
[32] SecurityFocus. DOI= http://www.securityfocus.com/.
[33] SecurityTracker. DOI= http://www.securitytracker.com/.
[34] E. C. Sezer, P. Ning, C. Kil and J. Xu, “Memsherlock: An Automated Debugger for Unknown Memory Corruption Vulnerabilities”, Proceedings of the 14th ACM Conference on Computer and Communications Security (CCS), pp. 562-572, 2007.
[35] M. Sutton, A. Greene, and P. Amini, “Fuzzing: Brute Force Vulnerability Discovery”, Addison-Wesley, 2007.
[36] W. Wang, S. Sampath, Y. Lei, and R. Kacker, "An Interaction-Based Test Sequence Generation Approach for Testing Web Applications", Proceedings of the 11th IEEE High Assurance Systems Engineering Symposium, pp. 209-218, 2008.
[37] W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, J. Lawrence, "A Combinatorial Approach to Building Navigation Graphs for Dynamic Web Applications", Proceedings of the 25th International Conference on Software Maintenance, pp. 211-220, 2009.
[38] W. Wang, and D. Zhang, “External Parameter Identification Report”. University of Texas at Arlington. DOI= https://wiki.uta.edu/pages/viewpageattachments.action?pageId=35291531.
[39] J. Wilander, and M. Kamkar, "A Comparison of Publicly Available Tools for Dynamic Buffer Overflow Prevention", Proceedings of the 10th Network and Distributed System Security Symposium, pp. 149-162, 2003.
[40] Y. Xie, A. Chou, and D. Engler, “ARCHER: Using Symbolic, Path-sensitive Analysis to Detect Memory Access Errors”, Proceedings of the 11th ACM SIGSOFT International Symposium on Foundations of Software Engineering, pp. 327-336, 2003.
[41] R. Xu, P. Godefroid, R. Majumdar, “Testing for Buffer overflows with Length Abstraction”, Proceedings of the 2008 International Symposium on Software Testing and Analysis, pp. 27-38, 2008.

Authorized licensed use limited to: Tsinghua University. Downloaded on March 18, 2021, at 14:29:42 UTC from IEEE Xplore. Restrictions apply.