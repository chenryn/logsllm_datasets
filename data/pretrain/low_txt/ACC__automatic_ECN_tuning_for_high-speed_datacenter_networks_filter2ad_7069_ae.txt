### Transmission Rate for Each Flow

In PDQ and D3, switches allocate bandwidth for each flow based on the available link capacity. pFabric achieves near-optimal Flow Completion Time (FCT) by using infinite priority queues on switches. HPCC directly adopts switch loading information from In-band Network Telemetry (INT). However, the tuning of the Explicit Congestion Notification (ECN) threshold remains an open question. pHost [21] and Homa [39] rely on the receiver to send credit packets to determine the sending rate of each flow. TIMELY [35] and Swift [27] are Round-Trip Time (RTT)-based schemes that adjust the flow rate at the end-host. These approaches achieve remarkable performance but require modifications to the networking stack, which can be challenging for RDMA hardware-based implementations.

### Tuning ECN in Datacenters

Extensive studies have been proposed to optimize delay and throughput performance by determining the appropriate ECN marking threshold. Adaptive Active Queue Management (AQM) mechanisms, which update the virtual queue capacity based on the arrival rate, have been introduced in traditional TCP/IP networks [28, 61]. For modern datacenter networks, ECN* [57] shows that if the instantaneous queue length-based ECN threshold is properly tuned, it is possible for RED-like probability marking to achieve optimal incast performance. Notably, although ECN switches accept two threshold parameters (low and high), prior works often set both thresholds to the same value, effectively considering only one single threshold. TCN [10] uses sojourn time, i.e., the amount of time a packet spends in the queue, to mark packets. ECN# [62] studies RTT variation in datacenter networks and marks packets based on both instantaneous and persistent congestion states. ACC addresses operational challenges and uses dynamic ECN on commodity switches.

### Learning-Based Network Optimization

Learning-based approaches have been applied to handle flow-level traffic optimization [17] and parameter setting for congestion control at the end-host [26, 45, 56]. Remy [56] and Indigo [59] learn to adjust the rate from pre-collected network traffic samples. Vivace [19] utilizes an online algorithm, while Aurora [25] employs Deep Reinforcement Learning (DRL) to update the sending rate. To avoid performance issues with unpredictable traffic, Orca [5] combines conventional TCP Cubic with learning methods. However, most learning-based approaches focus on adjusting sending rates at the end-host based on passive feedback. None of these studies explore in-network optimization of feedback mechanisms like ECN. ACC is compatible with ECN-based solutions and adapts to traffic variations.

### Conclusion

We have shared our operational experience with automatic in-network optimization. ECN is key to achieving low latency and high throughput communications with state-of-the-art congestion control schemes. We propose ACC, a pragmatic approach that allows automatic ECN parameter tuning at each switch. By leveraging deep reinforcement learning, ACC significantly improves FCT for small messages and maintains high throughput for large messages. Without any modification at the end-host, ACC has been rapidly deployed in production datacenters to support stable storage and computing services.

This work does not raise any ethical issues.

### Acknowledgments

The authors thank the anonymous reviewers and our shepherd, Keon Jang, for their valuable feedback. We also thank Yashar Ganjali, Camtu Nguyen, and the teams at Huawei for their contributions to this work.

### References

[1] InfiniBand Architecture Volume 1 and released specification Volume 2. 2015. https://cw.infinibandta.org/document/dl/7859.
[2] Network Simulator 3. 2019. https://wwwnsnam.org.
[3] IEEE. 802.11Qau. 2011. Priority based flow control (PFC).
[4] S. Abbasloo, C. Y. Yen, and H. J. Chao. 2020. Classic Meets Modern: a Pragmatic Learning-Based Congestion Control for the Internet. In SIGCOMM ’20.
[5] Soheil Abbasloo, Chen-Yu Yen, and H Jonathan Chao. 2020. Classic meets modern: A pragmatic learning-based congestion control for the Internet. In ACM SIGCOMM. 632–647.
[6] Mohammad Alizadeh, Javanmard Adel, and Balaji Prabhakar. 2011. Analysis of DCTCP: stability, convergence, and fairness. In ACM SIGMETRICS Performance Evaluation Review. 73–84.
[7] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data Center TCP (DCTCP). In ACM SIGCOMM.
[8] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. 2013. pFabric: minimal near-optimal data-center transport. In ACM SIGCOMM.
[9] InfiniBand Trade Association. 2014. Supplement to InfiniBand architecture specification volume 1 release 1.2.2 annex A17: RoCEv2 (IP routable RoCE).
[10] Wei Bai, Kai Chen, Li Chen, Changhoon Kim, and Haitao Wu. 2016. Enabling ECN over generic packet scheduling. In ACM CoNEXT.
[11] In-band Network Telemetry in Barefoot Tofino. 2019. https://barefootnetworks.com/use-cases/ad-telemetry.
[12] In-band Network Telemetry in Broadcom Tomahawk 3. 2019. https://www.broadcom.com/company/news/product-releases/2372840.
[13] In-band Network Telemetry in Broadcom Trident 3. 2019. https://www.broadcom.com.
[14] Claude Barthels, Simon Loesing, Gustavo Alonso, and Donald Kossmann. 2015. Rack-scale in-memory join processing using RDMA. In ACM SIGMOD.
[15] Flexible Input/Output benchmark. 2017. http://github.com/axboe/fio.
[16] Linpack benchmark. 2018. http://www.netlib.org/benchmark/hpl/.
[17] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. Auto: Scaling deep reinforcement learning for datacenter-scale automatic traffic optimization. In ACM SIGCOMM.
[18] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. Statistics (2015).
[19] M. Dong, Tong Meng, Doron Zarchy, E. Arslan, Y. Gilad, B. Godfrey, and M. Schapira. 2018. PCC Vivace: Online-Learning Congestion Control. In USENIX NSDI.
[20] Sally Floyd and Van Jacobson. 1993. Random early detection gateways for congestion avoidance. IEEE/ACM Transactions on Networking 1, 4 (1993), 397–413.
[21] Peter X Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Ratnasamy, and Scott Shenker. 2015. pHost: Distributed near-optimal datacenter transport over commodity network fabric. In ACM CoNEXT.
[22] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula, Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta Sengupta. 2009. VL2: A Scalable and Flexible Data Center Network. In ACM SIGCOMM.
[23] Chi-Yao Hong, Matthew Caesar, and P. Brighten Godfrey. 2012. Finishing flows quickly with preemptive scheduling. ACM SIGCOMM Computer Communication Review 42, 4 (2012), 127–138.
[24] Horovod. 2018. https://github.com/horovod/horovod.
[25] Nathan Jay, Noga Rotman, Brighten Godfrey, Michael Schapira, and Aviv Tamar. 2019. A deep reinforcement learning perspective on internet congestion control. In International Conference on Machine Learning (ICML). 3050–3059.
[26] Lavanya Jose, Lisa Yan, Mohammad Alizadeh, George Varghese, Nick McKeown, and Sachin Katti. 2015. High speed networks need proactive congestion control. In ACM Workshop on Hot Topics in Networks.
[27] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan M. G. Wassel, Xian Wu, Behnam Montazeri, Yaogong Wang, Kevin Springborn, Christopher Alfeld, Michael Ryan, David Wetherall, and Amin Vahdat. 2020. Swift: Delay is Simple and Effective for Congestion Control in the Datacenter. In ACM SIGCOMM.
[28] Srisankar Kunniyur and Rayadurgam Srikant. 2001. Analysis and design of an adaptive virtual queue (AVQ) algorithm for active queue management. In ACM SIGCOMM.
[29] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang, Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and et al. 2019. HPCC: High Precision Congestion Control. In ACM SIGCOMM.
[30] Xiaoyi Lu, Nusrat S. Islam, Md Wasi-Ur-Rahman, Jithin Jose, Hari Subramoni, Hao Wang, and Dhabaleswar K. Panda. 2013. High-performance design of Hadoop RPC with RDMA over InfiniBand. In International Conference on Parallel Processing (ICPP). IEEE, 641–650.
[31] Xiaoyi Lu, Md Wasi Ur Rahman, Nahina Islam, Dipti Shankar, and Dhabaleswar K. Panda. 2014. Accelerating Spark with RDMA for big data processing: Early experiences. In Annual Symposium on High Performance Interconnects.
[32] Xiaoyi Lu, Dipti Shankar, Shashank Gugnani, and Dhabaleswar K. Panda. 2016. High-performance design of Apache Spark with RDMA and its benefits on various workloads. In International Conference on Big Data (Big Data). IEEE.
[33] Youyou Lu, Jiwu Shu, Youmin Chen, and Tao Li. 2017. Octopus: An RDMA-enabled distributed persistent memory file system. In USENIX ATC.
[34] Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Martín Abadi, Paul Barham, and Xiaoqiang Zheng. 2016. TensorFlow: A system for large-scale machine learning. In USENIX OSDI.
[35] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats. 2015. TIMELY: RTT-based Congestion Control for the Datacenter. In ACM SIGCOMM.
[36] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi, Arvind Krishnamurthy, Sylvia Ratnasamy, and Scott Shenker. 2018. Revisiting Network Support for RDMA. In ACM SIGCOMM.
[37] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning. PMLR, 1928–1937.
[38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529–533.
[39] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018. Homa: A Receiver-driven Low-latency Transport Protocol Using Network Priorities. In ACM SIGCOMM.
[40] Mellanox Perftest Package. 2017. https://community.mellanox.com/docs/DOC-2802.
[41] SparkRDMA Shuffle Manager Plugin. 2018. https://github.com/Mellanox/SparkRDMA.
[42] Haonan Qiu, Xiaoliang Wang, Tianchen Jin, Zhuzhong Qian, Baoliu Ye, Bin Tang, Wenzhong Li, and Sanglu Lu. 2018. Toward Effective and Fair RDMA Resource Sharing. In Asia-Pacific Workshop on Networking (APNet).
[43] K. Ramakrishnan, Sally Floyd, and D. Black. 2001. RFC3168: The addition of explicit congestion notification (ECN) to IP.
[44] Danfeng Shan and Fengyuan Ren. 2018. ECN Marking With Micro-Burst Traffic: Problem, Analysis, and Improvement. IEEE/ACM Transactions on Networking 26, 4 (2018), 1533–1546.
[45] Anirudh Sivaraman, Keith Winstein, Pratiksha Thaker, and Hari Balakrishnan. 2014. An experimental study of the learnability of congestion control. ACM SIGCOMM Computer Communication Review 44, 4 (2014), 479–490.
[46] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. MIT Press.
[47] Iperf: The TCP/UDP bandwidth measurement tool. 2005. dast. nlanr. net/Projects.
[48] Quantum Espresso test suite. 2019. https://www.quantum-espresso.org/.
[49] Barefoot Tofino. 2019. https://www.barefootnetworks.com/.
[50] Broadcom Tomahawk. 2019. https://www.broadcom.com/company/news/product-releases.
[51] Broadcom Trident. 2019. https://www.broadcom.com.
[52] Hasselt, van, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double Q-learning. In AAAI Conference on Artificial Intelligence.
[53] Yandong Wang, Li Zhang, Jian Tan, Min Li, Yuqing Gao, Xavier Guerin, Xiaoqiao Meng, and Shicong Meng. 2015. HydraDB: A resilient RDMA-driven key-value middleware for in-memory cluster computing. In International Conference for High Performance Computing, Networking, Storage and Analysis (SC). IEEE, 1–11.
[54] Xingda Wei, Zhiyuan Dong, Rong Chen, and Haibo Chen. 2018. Deconstructing RDMA-enabled Distributed Transactions: Hybrid is Better!. In USENIX OSDI.
[55] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011. Better never than late: Meeting deadlines in datacenter networks. ACM SIGCOMM Computer Communication Review 41, 4 (2011), 50–61.
[56] Keith Winstein and Hari Balakrishnan. 2013. TCP ex machina: Computer-generated congestion control. ACM SIGCOMM Computer Communication Review 43, 4 (2013), 123–134.
[57] Haitao Wu, Jiabo Ju, Guohan Lu, Chuanxiong Guo, Yongqiang Xiong, and Yongguang Zhang. 2012. Tuning ECN for data center networks. In ACM CoNEXT.
[58] Ming Wu, Fan Yang, Jilong Xue, Wencong Xiao, Youshan Miao, Lan Wei, Haoxiang Lin, Yafei Dai, and Lidong Zhou. 2015. GRAM: Scaling graph computation to the trillions. In ACM Symposium on Cloud Computing (SoCC).
[59] Francis Y. Yan, Jestin Ma, Greg D. Hill, Deepti Raghavan, Riad S. Wahby, Philip Levis, and Keith Winstein. 2018. Pantheon: The training ground for Internet congestion-control research. In USENIX ATC.
[60] Lingbo Tang, Yongqing Xi, Pengcheng Zhang, Wenwen Peng, Bo Li, Yaohui Wu, Shaozong Liu, Lei Yan, Fei Feng, Yan Zhuang, Fan Liu, Pan Liu, Xingkui Liu, Zhongjie Wu, Junping Wu, Yixiao Gao, Qiang Li, Jinbo Wu, Jiaji Zhu, Haiyong Wang, Dennis Cai, Zheng Cao, Chen Tian, and Jiesheng Wu. 2021. When Cloud Storage Meets RDMA. In USENIX NSDI.
[61] Honggang Zhang, Don Towsley, C. V. Hollot, and Vishal Misra. 2003. A Self-Tuning Structure for Adaptation in TCP/AQM Networks. SIGMETRICS Perform. Eval. Rev. 31, 1 (June 2003).
[62] Junxue Zhang, Wei Bai, and Kai Chen. 2019. Enabling ECN for datacenter networks with RTT variations. In ACM CoNEXT.
[63] Qiao Zhang, Vincent Liu, Hongyi Zeng, and Arvind Krishnamurthy. 2017. High-resolution measurement of data center microbursts. In ACM Internet Measurement Conference (IMC). 78–85.
[64] Yiwen Zhang, Juncheng Gu, Youngmoon Lee, Mosharaf Chowdhury, and Kang G. Shin. 2017. Performance Isolation Anomalies in RDMA. In KBNets. ACM.
[65] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn, Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and Ming Zhang. 2015. Congestion control for large-scale RDMA deployments. In ACM SIGCOMM.
[66] Yibo Zhu, Monia Ghobadi, Vishal Misra, and Jitendra Padhye. 2016. ECN or Delay: Lessons Learnt from Analysis of DCQCN and TIMELY. In ACM CoNEXT.

### Appendix

#### Impact of Reward Design

The design of the reward function significantly impacts the performance of the Deep Reinforcement Learning (DRL) agent. To gain a deeper understanding of reward function design, we present two typical designs here. As shown in Figure 17(a), Design-1 applies a linear function, \( D(L) = 1 - \frac{L}{Q_{\text{max}}} \) in Equation (2) (where \( Q_{\text{max}} \) is the allocated buffer size, 10MB in the testbed). Design-2 is our stepwise mapping function of queue length reward, as illustrated in Figure 4. To evaluate the performance of these two designs, we selected ten levels of high ECN thresholds. Under the scenario of incast congestion, the action decisions made by the two designs are shown in Figure 17(b). We observe that ACC with reward Design-2 achieves the expected actions. To understand why, we review the reward functions. For Design-1, if the queue length is directly mapped to the reward using a linear function, the rewards for different actions are similar. To introduce differentiation, we propose a step function that uses fine-grained intervals for shallow queue depths and coarse-grained intervals for large queue sizes. This piecewise mapping design is motivated by the following reasons:
1. Most network congestion occurs at small queue sizes, where small changes in queue length can significantly impact performance.
2. A large queue size (e.g., 1MB) always implies a long queue latency (>300us at 25Gbps link), which is two orders of magnitude larger than the transmit latency, meeting the design requirements.