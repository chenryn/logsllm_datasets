### Impact of M_degr on Cpeak and Crequ

We now consider the impact of `M_degr` on `Cpeak` and subsequently on `Crequ`. When `M_degr` is set to 3%, we allow 3% of the measurement points to have a utilization of allocation between `U_high` and `U_degr`.

For cases where `T_degr` is set to "none," the impact of `M_degr = 3%` on `Cpeak` is identical for both values of θ. Specifically, there is a 24% reduction in `Cpeak` for both θ values.

For all experiments, the resource access QoS commitment has a deadline value `s` that corresponds to 60 minutes (see Section IV).

#### Figure 8: Different Time-Contiguous Requirements
The figure below shows the percentage of allocations with degraded performance for different time-contiguous requirements and application numbers.

**Figure 8: Different Time-Contiguous Requirements**
- **(a) CoS2, θ = 0.95**
  - No contiguous time-limit on `U_degr`
  - No more than 2 hours of `U_degr`
  - No more than 1 hour of `U_degr`
  - No more than 0.5 hour of `U_degr`

- **(b) CoS2, θ = 0.6**
  - No contiguous time-limit on `U_degr`
  - No more than 2 hours of `U_degr`
  - No more than 1 hour of `U_degr`
  - No more than 0.5 hour of `U_degr`

**Application Number vs. % of Measurements with Degraded Performance**

| App's Number | 0% | 5% | 10% | 15% | 20% | 25% | 30% |
|--------------|----|----|-----|-----|-----|-----|-----|
| 0            | 0  | 0  | 0   | 0   | 0   | 0   | 0   |
| 5            | 0  | 0  | 0   | 0   | 0   | 0   | 0   |
| 10           | 0  | 0  | 0   | 0   | 0   | 0   | 0   |
| 15           | 0  | 0  | 0   | 0   | 0   | 0   | 0   |
| 20           | 0  | 0  | 0   | 0   | 0   | 0   | 0   |
| 25           | 0  | 0  | 0   | 0   | 0   | 0   | 0   |
| 30           | 0  | 0  | 0   | 0   | 0   | 0   | 0   |

### Table I: Impact of M_degr, T_degr, and θ on Resource Sharing

| Case | M_degr | θ   | T_degr    | Cpeak (CPU) | Crequ (CPU) |
|------|--------|-----|-----------|-------------|-------------|
| 1    | 0      | 0.6 | none      | 218         | 123         |
| 2    | 3      | 0.6 | 30 min    | 188         | 106         |
| 3    | 3      | 0.6 | none      | 166         | 104         |
| 4    | 0      | 0.95| none      | 218         | 118         |
| 5    | 3      | 0.95| 30 min    | 167         | 103         |
| 6    | 3      | 0.95| none      | 166         | 104         |

For the cases with `T_degr = 30 minutes`, the reduction in `Cpeak` is 14% for θ = 0.6 and 23% for θ = 0.95. This difference is due to the interaction between `T_degr` and θ, as discussed earlier. A higher θ value appears to be advantageous for a resource pool operator.

### Comparison of M_degr and T_degr on Crequ and Cpeak

- For θ = 0.6, the impact of `M_degr = 3%` and `T_degr = 30 minutes` on `Crequ` and `Cpeak` is approximately the same, resulting in a 14% reduction.
- For θ = 0.95, `Crequ` is reduced by 14% and `Cpeak` by 23% compared to the `M_degr = 0%` case. The workload placement service was not able to achieve the same reduction in `Crequ`, likely because lowered per-application peak demands do not necessarily coincide with peaks in aggregate demand.

### Application QoS Constraints and Server Requirements

Cases 1 and 4 from Table I require 8 servers, one more server than the remaining cases. From the perspective of application QoS constraints for normal and failure modes, we can offer case 1 and 4 constraints as normal mode constraints and the remaining cases as possible constraints for failure mode. In normal mode, the system would use 8 servers. In the event of a single server failure, the table shows that the remaining 7 servers could support the system with the other application QoS constraints (cases 2, 3, 5, or 6). However, an appropriate workload migration technology is needed to realize the new configuration without disrupting application processing.

### Summary

- Higher values of θ permit more demand to be associated with CoS2, giving greater freedom to the workload placement service to overbook capacity.
- `M_degr` has a larger impact on `Cpeak` than `Crequ` because not all reductions in peak application demands occur at the same times as peak aggregate allocation requirements.
- Greater values for θ can decrease the maximum required allocations of applications compared to lower values for θ.
- Even minor reductions in application QoS requirements can have a significant impact on system resource requirements. The appropriate use of QoS requirements can help to support workload placement exercises that deal with resource failures.

### Related Work

Historically, enterprise capacity management groups have relied on curve fitting and queueing models to anticipate capacity requirements for shared resources such as mainframes or large servers. Curve fitting and business-level demand forecasting methods are used to extrapolate measurements of application demands on each resource. Queueing models may be used to relate desired mean response times for model-specific workload classes (e.g., batch or interactive, payroll, accounts receivable) to targets for maximum resource utilizations. Unfortunately, such planning exercises are people-intensive and hence expensive. Most organizations only conduct these exercises when the costs can be justified. Even so, capacity management remains a challenge as today’s enterprise data centers can have hundreds of large shared servers and thousands of lightly utilized smaller server resources.

We employ a trace-based approach to model the sharing of resource capacity for resource pools. Many groups have applied trace-based methods for detailed performance evaluation of processor architectures. They can also be used to support capacity management on more coarse data, e.g., resource usage as recorded every five minutes. Our early work on data center efficiency relied on traces of workload demands to predict opportunities for resource sharing in enterprise data centers. We conducted a consolidation analysis that packed existing server workloads onto a smaller number of servers using an Integer Linear Programming based bin-packing method. Unfortunately, the bin-packing method is NP-complete for this problem, making it computationally intensive and impractical for larger consolidation exercises and ongoing capacity management.

### Summary and Conclusions

We have introduced R-Opus, a composite framework for realizing application QoS requirements in shared resource pools. The framework includes a method for dividing application workload demands across two workload manager allocation priorities. We have shown how this can be done to satisfy per-application QoS objectives in shared resource environments. Application owners specify application QoS requirements using a range for acceptable performance along with terms that limit acceptable degradations to this performance. These, along with resource pool resource access QoS, determine how much of each application’s demands must be associated with a guaranteed allocation class of service and how much with a second class of service that offers resources with a given probability defined by a resource pool operator. A workload placement service assigns workloads to resources in a manner expected to satisfy the resource access CoS objectives. The more workload that is associated with the second class of service, the greater the opportunity for the resource pool to overbook resources.

Case study results validate our technique. The results show that relatively small diminishment in application QoS requirements can lead to a significant reduction in per-application maximum allocation, e.g., 25% in our case study. Higher θ values from resource pool operators can lead to greater reductions, particularly when the time-limited degradation is employed. Having a non-guaranteed CoS greatly reduces aggregate capacity requirements when consolidating workloads to servers. The workload placement service was able to realize significant benefits from consolidation, e.g., up to 45% with respect to the sum of peak aggregate application allocation requirements, for these workloads.

Finally, the approach we present aims to ensure that applications have the utilization of allocation values they need. This is necessary to provide application quality of service. However, other system features may also affect responsiveness but are not modeled by our approach. These include the impact of caching, database locks, garbage collection, and software bugs. Furthermore, performance tuning and capacity management exercises often aim to ensure that sufficient memory and input-output capacity are available to make CPU resources the bottleneck to manage. Future work will look at extending our techniques to consider the impact of greater sharing of other capacity attributes such as memory and input-output resources. We believe R-Opus should be part of a larger management system that takes these aspects of system behavior into account as well.

### References

[1] www.aogtech.com  
[2] A. Andrzejak, J. Rolia, and M. Arlitt, Bounding Resource Savings of Utility Computing Models, HP Labs Technical Report HPL-2002-339.  
[3] G. Banga, P. Druschel, J. Mogul. Resource containers: a new facility for resource management in server systems, in Proc. of the 3rd Symposium on Operating System Design and Implementation (OSDI ’99), New Orleans, LA, 1999.  
[4] Daniel Gmach, Stefan Seltzsam, Martin Wimmer, and Alfons Kemper. AutoGlobe: Automatische Administration von dienstbasierten Datenbankanwendungen. GI Conference on Database Systems for Business, Technology, and Web (BTW), Karlsruhe, Germany, February 2005.  
[5] B. Dragovic, K. Fraser, S. Hand, et al. Xen and the Art of Virtualization, in Proc. of 19th ACM Symposium on Operating Systems Principles (SOSP 2003), Bolton Landing, NY, October 2003.  
[6] K. Duda and D. Cheriton. Borrowed-virtual-time (BVT) scheduling: Supporting latency-sensitive threads in a general-purpose scheduler. In Proc. of the 17th ACM Symposium on Operating Systems Principles (SOSP 1999), Kiawah Island Resort, SC, December 1999.  
[7] E. J. Elton and M. J. Gruber, Modern Portfolio Theory and Investment Analysis, Wiley, 1995.  
[8] IBM Enterprise Workload Manager. http://www.ibm.com/developerworks/autonomic/ewlm/  
[9] HP-UX Workload Manager. http://www.hp.com/products1/unix/operating/wlm/  
[10] D. Krishnamurthy, Synthetic Workload Generation for Stress Testing Session-Based Systems. Ph.D. Thesis, Carleton University, Jan. 2004.  
[11] J. J. Pieper, A. Mellan, J. M. Paul, D. E. Thomas, and F. Karim, High level cache simulation for heterogeneous multiprocessors, Proceedings of the 41st annual conference on Design automation, San Diego, USA, ACM Press, pages 287-292, 2004.  
[12] J. Rolia, L. Cherkasova, M. Arlitt, and A. Andrzejak. A Capacity Management Service for Resource Pools, in Proc. of the 5th International Workshop on Software and Performance (WOSP 2005), Palma, Spain, July 2005, pp. 229-237.  
[13] J. Rolia, L. Cherkasova, M. Arlitt, and V. Machiraju. An Automated Approach for Supporting Application QoS in Shared Resource Pools. In Proc. of the 1st International Workshop on Self-Managed Systems & Services (SelfMan 2005), Nice, France, May 2005.  
[14] J. Rolia, X. Zhu, M. Arlitt, and A. Andrzejak, environments. Performance Evaluation Journal 58, pages 319-339, 2004.  
[15] S. Singhal, S. Graupner, A. Sahai et al. Resource Utility System. In Proc. of the 9th International Symposium on Integrated Network Management (IM 2005), Nice, France, May 2005.  
[16] www.teamQuest.com  
[17] B. Urgaonkar, P. Shenoy, and T. Roscoe. Resource overbooking and application profiling in shared hosting platforms. In USENIX OSDI, December 2002.  
[18] A. Whitaker, M. Shaw, and S. Gribble. Scale and Performance in the Denali Isolation Kernel. In Proc. of the 5th Symposium on Operating System Design and Implementation (OSDI 2002), Boston, MA, December 2002.  
[19] VMware VirtualCenter 1.2. http://www.vmware.com/products/vmanage/vc_features.html

Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06)  
0-7695-2607-1/06 $20.00 © 2006 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 12:28:03 UTC from IEEE Xplore. Restrictions apply.