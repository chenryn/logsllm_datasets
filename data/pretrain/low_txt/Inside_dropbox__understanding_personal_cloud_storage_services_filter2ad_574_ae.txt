### 5.6 Implications

The results presented in this section provide valuable insights into the current usage patterns of the Dropbox client. Understanding these patterns is crucial for the effective provisioning of data centers and networks to handle cloud storage traffic. Our analysis reveals that users exhibit diverse behaviors when using the application. For instance, although Dropbox is installed on more than 6% of households (as discussed in Section 3.3), less than 40% of these households fully utilize its functionalities, such as device synchronization and folder sharing. The similarity in usage patterns across both home networks reinforces our conclusions.

The significant amount of traffic generated by a limited percentage of users suggests that cloud storage systems will soon be among the top applications producing Internet traffic. However, to generalize these findings, it is necessary to gather geographically dispersed data and conduct longitudinal studies as more people adopt such solutions.

### 6. WEB STORAGE

In addition to the client application, Dropbox allows users to access shared folders and files through its main Web interface and a direct link download mechanism. In the following sections, we analyze the usage of these interfaces.

**Figure 17: Storage via the Main Web Interface**

[Insert Figure 17 here]

**Figure 18: Size of Direct Link Downloads**

[Insert Figure 18 here]

#### 6.1 Usage of the Main Web Interface

When considering the number of uploaded bytes, it is evident that the main Web interface is rarely used for uploading content. More than 95% of the flows submitted less than 10kB. For downloaded bytes, up to 80% of the flows exchanged less than 10kB. These distributions are strongly influenced by SSL handshake sizes, as the Dropbox interface retrieves thumbnails from storage servers using SSL, and web browsers open several parallel connections to retrieve HTTP objects. The remaining flows, in more than 95% of cases, have less than 10MB, indicating that only small files are typically retrieved from this Web interface.

#### 6.2 Direct Link Downloads

We also analyzed flows related to direct link downloads. Notably, these flows account for 92% of the Dropbox Web storage flows in Home 1, confirming the preference for this mechanism over the main Dropbox Web interface. Figure 18 shows the CDF of the size of direct link downloads. Since these downloads are not always encrypted, the CDF does not have the SSL lower-bound. Only a small percentage of direct link downloads exceed 10MB, suggesting that their usage is not primarily for sharing large files like movies or archives.

### 7. CONCLUSIONS

To the best of our knowledge, this is the first comprehensive analysis of Dropbox usage on the Internet. Our study highlights the growing interest in cloud-based storage systems, with major players like Google, Apple, and Microsoft offering similar services. We found that Dropbox is currently the most popular provider in this landscape.

By analyzing flows captured at four vantage points in Europe over 42 consecutive days, we demonstrated that Dropbox is responsible for a considerable volume of traffic. In one of our datasets, Dropbox traffic was equivalent to one-third of YouTube's traffic.

Our extensive characterization of Dropbox, both in terms of system workload and typical usage, revealed that the service performance is highly impacted by the distance between clients and data centers, which are currently located in the U.S. The use of per-chunk acknowledgments in the client protocol, combined with small chunk sizes, significantly limits the effective throughput of the service. We identified two potential improvements to the protocol: (i) the use of a chunk bundling scheme and (ii) the introduction of delayed acknowledgments. We showed that the recent deployment of a bundling mechanism has dramatically improved system performance. Additionally, we expect that overall performance will be further enhanced by deploying data centers in different locations.

Regarding the typical workload of the Dropbox system, our analysis revealed a variety of user behaviors. A considerable number of users take full advantage of Dropbox's functionalities, actively storing and retrieving files and sharing multiple folders. However, we also noted that around one-third of users rarely use the service, exchanging very little data during the 42-day observation period.

### 8. ACKNOWLEDGMENTS

This work was carried out in the context of the TMA COST Action IC0703, the EU-IP project mPlane, and the IOP GenCom project SeQual. mPlane is funded by the European Commission under grant n-318627. SeQual is funded by the Dutch Ministry of Economic Affairs, Agriculture and Innovation via its agency Agentschap NL.

### 9. REFERENCES

[1] A. Bergen, Y. Coady, and R. McGeer. Client Bandwidth: The Forgotten Metric of Online Storage Providers. In Proceedings of the 2011 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing, PacRim’2011, pages 543–548, 2011.
[2] I. Bermudez, M. Mellia, M. M. Munaf`o, R. Keralapura, and A. Nucci. DNS to the Rescue: Discerning Content and Services in a Tangled Web. In Proceedings of the 12th ACM SIGCOMM Conference on Internet Measurement, IMC’12, 2012.
[3] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and S. Moon. I Tube, You Tube, Everybody Tubes: Analyzing the World’s Largest User Generated Content Video System. In Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement, IMC’07, pages 1–14, 2007.
[4] N. Dukkipati, T. Reﬁce, Y. Cheng, J. Chu, T. Herbert, A. Agarwal, A. Jain, and N. Sutin. An Argument for Increasing TCP’s Initial Congestion Window. SIGCOMM Comput. Commun. Rev., 40(3):26–33, 2010.
[5] A. Finamore, M. Mellia, M. Meo, M. M. Munaf`o, and D. Rossi. Experiences of Internet Traffic Monitoring with Tstat. IEEE Network, 25(3):8–14, 2011.
[6] A. Finamore, M. Mellia, M. M. Munaf`o, R. Torres, and S. G. Rao. YouTube Everywhere: Impact of Device and Infrastructure Synergies on User Experience. In Proceedings of the 11th ACM SIGCOMM Conference on Internet Measurement, IMC’11, pages 345–360, 2011.
[7] M. Gjoka, M. Sirivianos, A. Markopoulou, and X. Yang. Poking Facebook: Characterization of OSN Applications. In Proceedings of the First Workshop on Online Social Networks, WOSN’08, pages 31–36, 2008.
[8] S. Halevi, D. Harnik, B. Pinkas, and A. Shulman-Peleg. Proofs of Ownership in Remote Storage Systems. In Proceedings of the 18th ACM Conference on Computer and Communications Security, CCS’11, pages 491–500, 2011.
[9] D. Harnik, B. Pinkas, and A. Shulman-Peleg. Side Channels in Cloud Services: Deduplication in Cloud Storage. IEEE Security and Privacy, 8(6):40–47, 2010.
[10] S. H¨at¨onen, A. Nyrhinen, L. Eggert, S. Strowes, P. Sarolahti, and M. Kojo. An Experimental Study of Home Gateway Characteristics. In Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement, IMC’10, pages 260–266, 2010.
[11] W. Hu, T. Yang, and J. N. Matthews. The Good, the Bad and the Ugly of Consumer Cloud Storage. ACM SIGOPS Operating Systems Review, 44(3):110–115, 2010.
[12] A. Lenk, M. Klems, J. Nimis, S. Tai, and T. Sandholm. What’s Inside the Cloud? An Architectural Map of the Cloud Landscape. In Proceedings of the 2009 ICSE Workshop on Software Engineering Challenges of Cloud Computing, CLOUD’09, pages 23–31, 2009.
[13] A. Li, X. Yang, S. Kandula, and M. Zhang. CloudCmp: Comparing Public Cloud Providers. In Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement, IMC’10, pages 1–14, 2010.
[14] M. Mellia, M. Meo, L. Muscariello, and D. Rossi. Passive Analysis of TCP Anomalies. Computer Networks, 52(14):2663–2676, 2008.
[15] A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee. Measurement and Analysis of Online Social Networks. In Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement, IMC’07, pages 29–42, 2007.
[16] M. Mulazzani, S. Schrittwieser, M. Leithner, M. Huber, and E. Weippl. Dark Clouds on the Horizon: Using Cloud Storage as Attack Vector and Online Slack Space. In Proceedings of the 20th USENIX Conference on Security, SEC’11, 2011.
[17] G. Wang and T. E. Ng. The Impact of Virtualization on Network Performance of Amazon EC2 Data Center. In Proceedings of the 29th IEEE INFOCOM, pages 1–9, 2010.
[18] Q. Zhang, L. Cheng, and R. Boutaba. Cloud Computing: State-of-the-Art and Research Challenges. Journal of Internet Services and Applications, 1:7–18, 2010.
[19] M. Zhou, R. Zhang, W. Xie, W. Qian, and A. Zhou. Security and Privacy in Cloud Computing: A Survey. In Sixth International Conference on Semantics Knowledge and Grid, SKG’10, pages 105–112, 2010.

### APPENDIX

#### A. STORAGE TRAFFIC IN DETAILS

##### A.1 Typical Flows

Figure 19 shows typical storage flows observed in our testbed. All packets exchanged during initial and final handshakes are depicted. The data transfer phases (in gray) are shortened for the sake of space. Key elements for our methodology, such as TCP segments with PSH flag set and flow durations, are highlighted. To confirm that these models are valid in real clients, Tstat in Campus 1 was set to record statistics about the first 10 messages delimited by TCP segments with PSH flag set. In the following, more details of our methodology and the results of this validation are presented.

**Figure 19: Typical Flows in Storage Operations**

[Insert Figure 19 here]

##### A.2 Tagging Storage Flows

Storage flows are first identified using FQDNs and SSL certificate names, as explained in Section 3.1. After identification, they are classified based on the number of bytes sent by each endpoint of the TCP connection. The method is built on the assumption that a storage flow is used either for storing chunks or for retrieving chunks, but never for both. This assumption is supported by two factors: (i) when both operations happen in parallel, Dropbox uses separate connections to speed up synchronization; (ii) idle storage connections are kept open waiting for new commands only for a short time interval (60 seconds).

Our assumption could be violated during this idle interval, but in practice, this seems to be rare. Figure 20 illustrates this by plotting the number of bytes in storage flows in Campus 1. Flows are concentrated near the axes, as expected under our assumption.

**Figure 20: Bytes Exchanged in Storage Flows in Campus 1**

[Insert Figure 20 here]

Flows in Figure 20 are already divided into groups. The limit between store and retrieve regions is determined by the function \( f(u) = 0.67(u - 294) + 4103 \), where \( u \) is the number of uploaded bytes. This function was empirically defined using the extra information collected in Campus 1, where the following was observed:
- Both store and retrieve operations require at least 309 bytes of overhead from servers.
- Store and retrieve operations require at least 634 and 362 bytes of overhead from clients, respectively.
- Typically, SSL handshakes require 294 bytes from clients and 4103 bytes from servers.

\( f(u) \) is centralized between the regions determined according to these constants. For improving visualization, SSL overheads are subtracted from each point in the figure.

Since client machines in Campus 1 are relatively homogeneous, the gap between the two groups is very clear in this dataset. More variation in message sizes is observed at other vantage points, particularly in SSL handshakes, which are affected by different software configurations. However, this does not change considerably the regions of each storage operation when compared to Campus 1.

Finally, we quantify the possible error caused by violations of our assumption. In all vantage points, flows tagged as store download less than 1% of the total storage volume. Since this includes protocol overheads (e.g., HTTP OK messages in Figure 19), mixed flows marked as store might have only a negligible impact on our results. Similar reasoning is valid for retrieve flows.

##### A.3 Number of Chunks

The number of chunks transported in a storage flow (\( c \)) is estimated by counting TCP segments with PSH flag set (\( s \)) in the reverse direction of the transfer, as indicated in Figure 19. For retrieve flows, \( c = \frac{s - 2}{2} \). For store flows, \( c = s - 3 \) or \( c = s - 2 \), depending on whether the connection is passively closed by the server or not. This can be inferred by the time difference between the last packet with payload from the client and the last one from the server: when the server closes an idle connection, the difference is expected to be around 1 minute (otherwise, only a few seconds). Tstat already records the timestamps of such packets by default.

We validate this relation by dividing the amount of payload (without typical SSL handshakes) in the reverse direction of a transfer by \( c \). This proportion must be equal to the overhead needed per storage operation. Figure 21 shows that for the vast majority of store flows, the proportion is about 309 bytes per chunk, as expected.

**Figure 21: Payload in the Reverse Direction of Storage Operations per Estimated Number of Chunks**

[Insert Figure 21 here]

In Home 2, the apparently misbehaving client described in Section 4.3 biases the distribution: most flows from this client lack acknowledgment messages. Most retrieve flows have a proportion between 362 and 426 bytes per chunk, which are typical sizes of the HTTP request in this command. The exceptions (3% – 8%) might be caused by packet loss in our probes as well as by the flows that both stored and retrieved chunks. Our method underestimates the number of chunks in those cases.

##### A.4 Duration

Figure 19 shows the duration used when computing the throughput of a storage flow (\( \Delta t \)). Since initial TCP/SSL handshakes affect users' perception of throughput, the first SYN packet is taken as the beginning of the transfer. Termination handshakes, on the other hand, are ignored. In store flows, the last packet with payload sent by the client is considered the end of the transfer. In retrieve flows, the last packet with payload is normally a server alert about the SSL termination. We compensate for this by subtracting 60 seconds from the duration of retrieve flows whenever the difference between the last packet with payload from the server and the one from the client is above 60 seconds.

Because of our monitoring topology, \( \Delta t \) does not include the trip time between clients and our probes. Therefore, \( \Delta t \) is slightly underestimated. Figure 19 also shows that 4 or 5 RTTs are needed before the client starts to send or receive data. In some cases, this already accounts for about 500 ms in the flow duration. Note that the initial TCP congestion window in place at servers was forcing a pause of 1 RTT during the SSL handshake. This parameter has been tuned after the release of Dropbox 1.4.0, reducing the overhead.