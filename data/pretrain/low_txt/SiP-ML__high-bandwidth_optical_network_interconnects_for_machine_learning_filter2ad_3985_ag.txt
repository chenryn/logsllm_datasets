以下是优化后的文本，使其更加清晰、连贯和专业：

---

### 参考文献

1. **Transceivers in DARPA PIPES项目：从2 Tbps到100 Tbps的目标**  
   [61] Pipes研究人员展示光学互连以改进数字微电子性能, 2020年3月。https://www.darpa.mil/news-events/2020-03-25。

2. **Ayar Labs在Hot Chips上展示光子芯片**  
   [62] Tiffany Trader. Ayar Labs将在Hot Chips上展示FPGA封装中的光子芯片，2019年8月。https://www.hpcwire.com/2019/08/19/ayar-labs-to-demo-photonics-chiplet-in-fpga-package-at-hot-chips/。

3. **Fleet - 快速通道以实现每秒10太比特的加速执行**  
   [63] F. Douglis, S. Robertson, E. Van den Berg, J. Micallef, M. Pucci, A. Aiken, M. Hattink, M. Seok, and K. Bergman. Fleet—快速通道以实现每秒10太比特的加速执行：项目概述。IEEE Internet Computing, (01):1–1, 2020年4月。

4. **Ayar Labs TeraPHY硅芯片**  
   [64] Ayar Labs TeraPHY硅芯片。https://ayarlabs.com/products/。

5. **Ayar Labs的光学I/O多芯片封装和单芯片封装解决方案演示**  
   [65] Ayar Labs的光学I/O多芯片封装和单芯片封装解决方案演示，2020年8月。https://vimeo.com/449164007。

6. **Rotornet: 一种可扩展、低复杂度的光学数据中心网络**  
   [66] William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich, George Papen, Alex C. Snoeren, and George Porter. Rotornet: 一种可扩展、低复杂度的光学数据中心网络。SIGCOMM '17, pages 267–280, 2017。

7. **大规模宽带数字硅光子开关与垂直绝热耦合器**  
   [67] Tae Joon Seok, Niels Quack, Sangyoon Han, Richard S. Muller, and Ming C. Wu. 大规模宽带数字硅光子开关与垂直绝热耦合器。Optica, 3(1):64–70, 2016年1月。

8. **128×128硅光子MEMS开关**  
   [68] Kyungmok Kwon, Tae Joon Seok, Johannes Henriksson, Jianheng Luo, Lane Ochikubo, John Jacobs, Richard S Muller, and Ming C Wu. 128×128硅光子MEMS开关与可扩展行列寻址。CLEO: Science and Innovations, pages SF1A–4. Optical Society of America, 2018。

9. **跨时间扩展以提供带宽效率和低延迟**  
   [69] William M. Mellette, Rajdeep Das, Yibo Guo, Rob McGuinness, Alex C. Snoeren, and George Porter. 跨时间扩展以提供带宽效率和低延迟。NSDI'20, 2020。

10. **Quartz: 用于低延迟DCN的新设计元素**  
    [70] Yunpeng James Liu, Peter Xiang Gao, Bernard Wong, and Srinivasan Keshav. Quartz: 用于低延迟DCN的新设计元素。SIGCOMM'14, pages 283–294。

11. **将微秒级电路交换集成到数据中心**  
    [71] George Porter, Richard Strong, Nathan Farrington, Alex Forencich, Pang Chen-Sun, Tajana Rosing, Yeshaiahu Fainman, George Papen, and Amin Vahdat. 将微秒级电路交换集成到数据中心。SIGCOMM'13, pages 447–458。

12. **Theia: 用于超密集数据中心的简单且廉价的网络**  
    [72] Meg Walraed Sullivan, Jitu Padhye, and Dave Maltz. Theia: 用于超密集数据中心的简单且廉价的网络。HotNets-XIII Proceedings of the 13th ACM Workshop on Hot Topics in Networks. ACM, 2014年10月。

13. **CamcubeOS: 用于3D环面集群拓扑的关键基于网络堆栈**  
    [73] Paolo Costa, Austin Donnelly, Greg O’Shea, and Antony Rowstron. CamcubeOS: 用于3D环面集群拓扑的关键基于网络堆栈。Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing, HPDC '13, pages 73–84, New York, NY, USA, 2013. Association for Computing Machinery。

14. **未来数据中心的共生路由**  
    [74] Hussam Abu-Libdeh, Paolo Costa, Antony Rowstron, Greg O’Shea, and Austin Donnelly. 未来数据中心的共生路由。Proceedings of the ACM SIGCOMM 2010 Conference, SIGCOMM '10, page 51?62, New York, NY, USA, 2010. Association for Computing Machinery。

15. **扩展超立方体：超立方体的分层互连网络**  
    [75] J. M. Kumar and L. M. Patnaik. 扩展超立方体：超立方体的分层互连网络。IEEE Transactions on Parallel and Distributed Systems, 3(1):45–57, 1992。

16. **技术驱动的高可扩展龙蝇拓扑**  
    [76] Min Yee Teh, Jeremiah J. Wilke, Keren Bergman, and Sébastien Rumley. 龙蝇拓扑的设计空间探索。Julian M. Kunkel, Rio Yokota, Michela Taufer, and John Shalf, editors, High Performance Computing, pages 57–74, Cham, 2017. Springer International Publishing。

17. **Calient Optical Circuit Switch**  
    [77] Calient Optical Circuit Switch. https://www.calient.net/products/edge640-。

18. **一种可扩展的商品数据中心网络架构**  
    [78] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. 一种可扩展的商品数据中心网络架构。SIGCOMM Comput. Commun. Rev., 38(4):63–74, 2008年8月。

19. **Bcube: 适用于模块化数据中心的高性能、服务器中心网络架构**  
    [79] Chuanxiong Guo, Guohan Lu, Dan Li, Haitao Wu, Xuan Zhang, Yunfeng Shi, Chen Tian, Yongguang Zhang, and Songwu Lu. Bcube: 适用于模块化数据中心的高性能、服务器中心网络架构。Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication, SIGCOMM '09, page 63?74, New York, NY, USA, 2009. Association for Computing Machinery。

20. **Slim Fly: 低成本低直径网络拓扑**  
    [80] M. Besta and T. Hoefler. Slim Fly: 低成本低直径网络拓扑。SC '14: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 348–359, 2014年11月。

21. **NVIDIA的NVLink-Switching芯片和Scale-Up GPU-Compute服务器**  
    [81] Alexander Ishii, Denis Foley, Eric Anderson, Bill Dally, Glenn Dearth Larry Dennison, Mark Hummel, and John Schafer. NVIDIA的NVLink-Switching芯片和Scale-Up GPU-Compute服务器。HotChips, 2018. https://www.hotchips.org/hc30/2conf/2.01NvidiaNVswitchHotChips2018DGX2NVSFinal.pdf。

22. **深度残差学习用于图像识别**  
    [82] K. He, X. Zhang, S. Ren, and J. Sun. 深度残差学习用于图像识别。2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016年6月。

23. **通过生成预训练改进语言理解**  
    [83] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 通过生成预训练改进语言理解。https://openai.com/blog/language-unsupervised/。

24. **测量数据并行性对神经网络训练的影响**  
    [84] Christopher J. Shallue, Jaehoon Lee, Joseph M. Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. 测量数据并行性对神经网络训练的影响。CoRR, abs/1811.03600, 2018。

25. **Megatron: 一个大型强大的Transformer**  
    [85] Raul Puri. Megatron: 一个大型强大的Transformer, 2019年8月。https://github.com/NVIDIA/Megatron-LM。

26. **MLPerf: 广泛的机器学习基准套件**  
    [86] MLPerf: 广泛的机器学习基准套件。https://mlperf .org/。

27. **FlexFlow GitHub**  
    [87] FlexFlow GitHub. https://github.com/flexflow/FlexFlow.git。

28. **极小批量SGD: 在15分钟内训练ResNet-50**  
    [88] Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. 极小批量SGD: 在15分钟内训练ResNet-50。arXiv preprint arXiv:1711.04325, 2017。

29. **在几分钟内训练ImageNet**  
    [89] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. 在几分钟内训练ImageNet。Proceedings of the 47th International Conference on Parallel Processing, pages 1–10, 2018。

30. **高度可扩展的混合精度深度学习训练系统: 在四分钟内训练ImageNet**  
    [90] Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et al. 高度可扩展的混合精度深度学习训练系统: 在四分钟内训练ImageNet。arXiv preprint arXiv:1807.11205, 2018。

31. **数据中心TCP (DCTCP)**  
    [91] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 数据中心TCP (DCTCP)。Proceedings of the ACM SIGCOMM 2010 Conference, SIGCOMM '10, pages 63–74, New York, NY, USA, 2010. ACM。

32. **HPCC: 高精度拥塞控制**  
    [92] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang, Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, et al. HPCC: 高精度拥塞控制。Proceedings of the ACM Special Interest Group on Data Communication, pages 44–58, 2019。

33. **Teraphy: 用于多芯片模块中光学I/O的高密度电子-光子芯片**  
    [93] Roy Meade, Shahab Ardalan, Michael Davenport, John Fini, Chen Sun, Mark Wade, Alexandra Wright-Gladstein, and Chong Zhang. Teraphy: 用于多芯片模块中光学I/O的高密度电子-光子芯片。Optical Fiber Communication Conference (OFC) 2019, page M4D.7. Optical Society of America, 2019。

34. **具有SOA和半导体锁模激光器的硅光子WDM收发器**  
    [94] Alvaro Moscoso Martir, Juliana Müller, Johannes Hauck, Nicolas Chimot, Rony Setter, Avner Badihi, Daniel Rasmussen, Alexandre Garreau, Mads Nielsen, Elmira Islamova, Sebastian Romero-García, Bin Shen, Anna Sandomirsky, Sylvie Rockman, Chao Li, Saeed Sharif Azadeh, Guo-Qiang Lo, Elad Mentovich, Florian Merget, and Jeremy Witzens. 具有SOA和半导体锁模激光器的硅光子WDM收发器。Scientific Reports, 7, 2016年5月。

35. **2020年通用Europractice价格表**  
    [95] 2020年通用Europractice价格表, 2020年1月。https://europractice-ic.com/wp-content/uploads/2020/01/General-MPW-EUROPRACTICE-200123-v3.pdf。

36. **用于400 Gbps电子-光子集成电路平台封装的2.5D硅光学插板**  
    [96] D. Kim, K. Y. Au, H. Y. L. X. Luo, Y. L. Ye, S. Bhattacharya, and G. Q. Lo. 用于400 Gbps电子-光子集成电路平台封装的2.5D硅光学插板。2017 IEEE 19th Electronics Packaging Technology Conference (EPTC), pages 1–4, 2017年12月。

37. **直接使用光通信的单芯片微处理器**  
    [97] Chen Sun, Mark T. Wade, Yunsup Lee, Jason S. Orcutt, Luca Alloatti, Michael S. Georgas, Andrew S. Waterman, Jeffrey M. Shainline, Rimas R. Avizienis, Sen Lin, Benjamin R. Moss, Rajesh Kumar, Fabio Pavanello, Amir H. Atabaki, Henry M. Cook, Albert J. Ou, Jonathan C. Leu, Yu-Hsin Chen, Krste Asanović, Rajeev J. Ram, Miloš A. Popović, and Vladimir M. Stojanović. 直接使用光通信的单芯片微处理器。Nature, 528(7583):534–538, 2015。

38. **大规模分布式深度网络**  
    [98] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. 大规模分布式深度网络。Advances in Neural Information Processing Systems, pages 1223–1231, 2012。

39. **使用COTS HPC系统的深度学习**  
    [99] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew. 使用COTS HPC系统的深度学习。International Conference on Machine Learning, pages 1337–1345, 2013。

40. **Project Adam: 构建高效且可扩展的深度学习训练系统**  
    [100] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project Adam: 构建高效且可扩展的深度学习训练系统。OSDI'14, pages 571–582, 2014。

41. **Tiresias: 用于分布式深度学习的GPU集群管理器**  
    [101] Juncheng Gu, Mosharaf Chowdhury, Kang G Shin, Yibo Zhu, Myeongjae Jeon, Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. Tiresias: 用于分布式深度学习的GPU集群管理器。NSDI'19, pages 485–500, 2019。

42. **优化GPU集群上分布式DNN训练的网络性能: 在1.5分钟内训练ImageNet/AlexNet**  
    [102] Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan, and Yonggang Wen. 优化GPU集群上分布式DNN训练的网络性能: 在1.5分钟内训练ImageNet/AlexNet。arXiv preprint arXiv:1902.06855, 2019。

43. **PyTorch-BigGraph: 一个大规模图嵌入系统**  
    [103] Adam Lerer, Ledell Wu, Jiajun Shen, Timothée Lacroix, Luca Wehrstedt, Abhijit Bose, and Alexander Peysakhovich. PyTorch-BigGraph: 一个大规模图嵌入系统。CoRR, abs/1903.12287, 2019。

44. **优化分布式机器学习中的网络性能**  
    [104] Luo Mai, Chuntao Hong, and Paolo Costa. 优化分布式机器学习中的网络性能。7th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 15), Santa Clara, CA, 2015. USENIX Association。

45. **深度梯度压缩: 减少分布式训练的通信带宽**  
    [105] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. 深度梯度压缩: 减少分布式训练的通信带宽。arXiv preprint arXiv:1712.01887, 2017。

46. **QSGD: 通过随机量化和编码实现通信高效的SGD**  
    [106] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: 通过随机量化和编码实现通信高效的SGD。volume 3, pages 1710 – 1721, 2018。

47. **3LC: 分布式机器学习的有效流量压缩**  
    [107] Hyeontaek Lim, David G Andersen, and Michael Kaminsky. 3LC: 分布式机器学习的有效流量压缩。arXiv preprint arXiv:1802.07389, 2018。

48. **分布式深度学习的线性加速分析**  
    [108] Peng Jiang and Gagan Agrawal. 分布式深度学习的线性加速分析。S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 2525–2536. Curran Associates, Inc., 2018。

49. **并行化卷积神经网络的一个奇怪技巧**  
    [109] Alex Krizhevsky. 并行化卷积神经网络的一个奇怪技巧。arXiv preprint arXiv:1404.5997, 2014。

50. **使用弹性平均SGD进行深度学习**  
    [110] Sixin Zhang, Anna E Choromanska, and Yann LeCun. 使用弹性平均SGD进行深度学习。Advances in Neural Information Processing Systems, pages 685–693, 2015。

51. **分布式延迟随机优化**  
    [111] Alekh Agarwal and John C Duchi. 分布式延迟随机优化。Advances in Neural Information Processing Systems, pages 873–881, 2011。

52. **Hogwild!: 无锁的随机梯度下降并行化方法**  
    [112] Feng Niu, Benjamin Recht, Christopher Re, and Stephen J. Wright. Hogwild!: 无锁的随机梯度下降并行化方法。Proceedings of the 24th International Conference on Neural Information Processing Systems, NIPS'11, pages 693–701, 2011。

53. **AKO: 基于部分梯度交换的去中心化深度学习**  
    [113] Pijika Watcharapichat, Victoria Lopez Morales, Raul Castro Fernandez, and Peter Pietzuch. AKO: 基于部分梯度交换的去中心化深度学习。SoCC '16, 2016。

54. **分布式机器学习系统中的通信调度作为一等公民**  
    [114] Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, and Roy H. Campbell. 分布式机器学习系统中的通信调度作为一等公民。CoRR, abs/1803.03288, 2018。

55. **FireCaffe: 深度神经网络训练在计算集群上的近线性加速**  
    [115] Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, and Kurt Keutzer. FireCaffe: 深度神经网络训练在计算集群上的近线性加速。CoRR, abs/1511.00175, 2015。

56. **Horovod: TensorFlow中的快速且简单的分布式深度学习**  
    [116] Alexander Sergeev and Mike Del Balso. Horovod: TensorFlow中的快速且简单的分布式深度学习。CoRR, abs/1802.05799, 2018。

57. **加速数据中心规模的持久神经网络**  
    [117] Eric Chung, Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Adrian Caulfield, Todd Massengil, Ming Liu, Daniel Lo, Shlomi Alkalay, and Michael Haselman. 加速数据中心规模的持久神经网络。Hot Chips, volume 29, 2017。

58. **分布式DNN训练的基于优先级的参数传播**  
    [118] Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, and Gennady Pekhimenko. 分布式DNN训练的基于优先级的参数传播。CoRR, abs/1905.03960, 2019。

59. **通过强化学习进行设备放置优化**  
    [119] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. 通过强化学习进行设备放置优化。Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2430–2439. JMLR. org, 2017。

60. **SuperNeurons: 用于训练深度神经网络的动态GPU内存管理**  
    [120] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. SuperNeurons: 用于训练深度神经网络的动态GPU内存管理。ACM SIGPLAN Notices, volume 53, pages 41–53. ACM, 2018。

61. **GPipe: 使用管道并行性高效训练巨型神经网络**  
    [121] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. GPipe: 使用管道并行性高效训练巨型神经网络。NeurIPS, 2019。

62. **Eyeriss: 一种用于深度卷积神经网络的能量高效可重构加速器**  
    [122] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss: 一种用于深度卷积神经网络的能量高效可重构加速器。IEEE Journal of Solid-State Circuits, 52(1):127–138, 2017。

63. **使用相干纳米光子电路进行深度学习**  
    [123] Yichen Shen, Nicholas C Harris, Scott Skirlo, Mihika Prabhu, Tom Baehr-Jones, Michael Hochberg, Xin Sun, Shijie Zhao, Hugo Larochelle, Dirk Englund, et al. 使用相干纳米光子电路进行深度学习。Nature Photonics, 11(7):441, 2017。

64. **忆阻玻尔兹曼机: 一种用于组合优化和深度学习的硬件加速器**  
    [124] Mahdi Nazm Bojnordi and Engin Ipek. 忆阻玻尔兹曼机: 一种用于组合优化和深度学习的硬件加速器。2016 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 1–13. IEEE, 2016。

65. **DLAU: 一种可扩展的FPGA深度学习加速单元**  
    [125] Chao Wang, Lei Gong, Qi Yu, Xi Li, Yuan Xie, and Xuehai Zhou. DLAU: 一种可扩展的FPGA深度学习加速单元。IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 36(3):513–517, 2017。

66. **张量处理单元的数据中心性能分析**  
    [126] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 张量处理单元的数据中心性能分析。2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), pages 1–12. IEEE, 2017。

67. **GPU和并行计算的未来**  
    [127] Stephen W Keckler, William J Dally, Brucek Khailany, Michael Garland, and David Glasco. GPU和并行计算的未来。IEEE Micro, 31(5):7–17, 2011。

68. **Hedera: 数据中心网络的动态流调度**  
    [128] Mohammad Al-Fares, Sivasankar Radhakrishnan, Barath Raghavan, Nelson Huang, and Amin Vahdat. Hedera: 数据中心网络的动态流调度。Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation, NSDI'10, pages 19–19, Berkeley, CA, USA, 2010. USENIX Association。

69. **Firefly: 使用自由空间光通信的可重新配置无线数据中心结构**  
    [129] Navid Hamedazimi, Zafar Qazi, Himanshu Gupta, Vyas Sekar, Samir R. Das, Jon P. Longtin, Himanshu Shah, and Ashish Tanwer. Firefly: 使用自由空间光通信的可重新配置无线数据中心结构。SIGCOMM'14, pages 319–330。

70. **Projector: 敏捷可重新配置的数据中心互连**  
    [130] M. Ghobadi, R. Mahajan, A. Phanishayee, N. Devanur, J. Kulkarni, G. Ranade, P. Blanche, H. Rastegarfar, M. Glick, and D. Kilper. Projector: 敏捷可重新配置的数据中心互连。SIGCOMM '16, pages 216–229, 2016。

71. **混合电路/数据包网络的调度技术**  
    [131] He Liu, Matthew K. Mukerjee, Conglong Li, Nicolas Feltman, George Papen, Stefan Savage, Srinivasan Seshan, Geoffrey M. Voelker, David G. Andersen, Michael Kaminsky, George Porter, and Alex C. Snoeren. 混合电路/数据包网络的调度技术。CoNEXT, pages 41:1–41:13. ACM, 2015。

72. **OSA: 一种具有前所未有的灵活性的数据中心网络光学交换架构**  
    [132] Ankit Singla, Atul Singh, and Yan Chen. OSA: 一种具有前所未有的灵活性的数据中心网络光学交换架构。Presented as part of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12), pages 239–252, San Jose, CA, 2012. USENIX。

73. **Shoal: 一种用于解聚机架的网络架构**  
    [133] Vishal Shrivastav, Asaf Valadarsky, Hitesh Ballani, Paolo Costa, Ki Suh Lee, Han Wang, Rachit Agarwal, and Hakim Weatherspoon. Shoal: 一种用于解聚机架的网络架构。16th USENIX Symposium on Networked Systems Design and Implementation (NSDI'19). USENIX, February 2019。

74. **使用REACToR在雷达下进行电路切换**  
    [134] He Liu, Feng Lu, Alex Forencich, Rishi Kapoor, Malveeka Tewari, Geoffrey M. Voelker, George Papen, Alex C. Snoeren, and George Porter. 使用REACToR在雷达下进行电路切换。NSDI'14, pages 1–15。

75. **Jellyfish: 随机连接的数据中心网络**  
    [135] Ankit Singla, Chi-Yao Hong, Lucian Popa, and P. Brighten Godfrey. Jellyfish: 随机连接的数据中心网络。Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI'12, pages 17–17, Berkeley, CA, USA, 2012. USENIX Association。

76. **Larry: 数据中心中的实用网络可重新配置性**  
    [136] Andromachi Chatzieleftheriou, Sergey Legtchenko, Hugh Williams, and Antony Rowstron. Larry: 数据中心中的实用网络可重新配置性。15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18), pages 141–156, Renton, WA, April 2018. USENIX Association。

77. **Xfabric: 用于机架规模计算机的可重新配置机架内网络**  
    [137] Sergey Legtchenko, Nicholas Chen, Daniel Cletheroe, Antony Rowstron, Hugh Williams, and Xiaohan Zhao. Xfabric: 用于机架规模计算机的可重新配置机架内网络。13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16), pages 15–29, Santa Clara, CA, March 2016. USENIX Association。

78. **极端规模计算系统的光学互连**  
    [138] Sebastien Rumley, Meisam Bahadori, Robert Polster, Simon D. Hammond, David M. Calhoun, Ke Wen, Arun Rodrigues, and Keren Bergman. 极端规模计算系统的光学互连。Parallel Computing, 64:65 – 80, 2017. High-End Computing for Next-Generation Scientific Discovery。

79. **用于片上光网络（NoC）的4×4无损硅路由器**  
    [139] Nicolas Sherwood-Droz, Howard Wang, Long Chen, Benjamin G. Lee, Aleksandr Biberman, Keren Bergman, and Michal Lipson. 用于片上光网络（NoC）的4×4无损硅路由器。Opt. Express, 16(20):15915–15922, 2008年9月。

---

希望这些优化后的引用条目能够帮助您更好地组织和呈现您的研究内容。