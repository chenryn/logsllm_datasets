### Analysis and Classification of Observations

We analyzed the observations under given levels of the factors and (ii) the observations obtained with the same value of `SIZE` but `SNR=0` (the baseline chunks). A threshold-based classifier was used to measure precision (P) and recall (R). Table V reports the results of the analysis when `SNR=LOW` and `VERB=MEDIUM`, corresponding to the middle row of the box plots in Figure 16. Similar findings were observed for other levels of `VERB` (not reported here due to space limitations).

#### Table V: Precision (P), Recall (R), and Threshold (Thd) for Each Classification Experiment (SNR=LOW, VERB=MEDIUM)

| FP1LNR | SIZE  | MEDIUM | LOW  | R  | Thd  | DICT  | P  | R  | Thd  | (%)  |
|--------|-------|--------|------|----|------|-------|----|----|------|------|
| 0.75   | 0.75  | 3.47   | 0.94 | 0.93 | 4.66 | 0.97  | 0.97 | 5.81 | 0.90 | 0.87 |
| 25     | 0.89  | 0.88   | 3.47 | 0.98 | 0.98 | 4.99  | 0.97 | 0.97 | 5.81 | 0.96 |
| 50     | 0.94  | 0.93   | 3.74 | 0.98 | 0.98 | 4.99  | 0.97 | 0.97 | 5.81 | 0.98 |
| 75     | 100   | 0.96   | 0.95 | 3.85 | 0.98 | 0.98  | 4.99 | 0.97 | 0.97 | 5.81 |

For example, when `DICT=25%` and `SIZE=LOW`, the precision (P) and recall (R) are 0.75 in FP1LNR, and the threshold (Thd) is 3.47. This means that assuming interesting chunks with `log.entropy ≥ 3.47` allows capturing 75% of true interesting chunks. The classification significantly improves when `DICT=50%`, `SNR=LOW`, and `SIZE=LOW` in FP1LNR, where `P=0.89` and `R=0.88`. When `SIZE=MEDIUM`, the interesting chunks are retained with `P≥0.92` and `R≥0.92` in all log sources, even when `DICT=25%`.

Table V provides several insights. For instance, P and R are significantly high regardless of the level of `DICT` when `SIZE≥MEDIUM` (e.g., the worst case is `P=R=0.92` in D02PAN with `SIZE=MEDIUM` and `DICT=25%`). When `SIZE=LOW`, `DICT` should be around 50% to achieve values of P and R close to or greater than 0.9. An increase in `DICT` from 25% to 50% improves P and R by about 0.15 when `SIZE=LOW`. These findings were obtained with `SNR=LOW`, meaning exactly one interesting entry out of an entire chunk when `SIZE=LOW`, which is the worst case. The entropy-based method can accurately retain even those chunks that contain one interesting entry, which is half-similar to a baseline entry. Thus, our method may be a viable solution to supplement the analysis of textual logs within current Security Information and Event Management (SIEM) systems.

### Threats to Validity

As with any empirical analysis, there may be concerns regarding the reproducibility, validity, and generalization of the results. These concerns are further exacerbated when the data sources consist of unstructured textual events, such as the datasets addressed in this work. The findings discussed in this paper might be subject to threats to validity, which are described and mitigated as follows:

- **Validation**: Validation was done using synthetic data generated under controlled conditions rather than real incidents. Obtaining labeled incident data from production settings is challenging due to costs, the dynamic nature of incidents, lack of instances for many incident categories, and the time it takes to collect naturally-occurring incidents. This is particularly true in critical industrial systems.
- **Assumptions**: Our work assumes that interesting activity is much more infrequent than behavioral baseline data, which is common in unsupervised problems.
- **Real-World Datasets**: The analysis is based on real-world datasets, unlike many past studies that used DARPA 98/99. We used logs from a critical information system and faced the inherent challenges of dealing with production unstructured data. The data were generated by emulating real workload conditions, supplied by the industry provider, and involved direct communication with the ATC operations team to understand and gain feedback on system dynamics.
- **Data Analysis Techniques**: We adopted well-documented data analysis techniques, and experiments were repeated several times to obtain statistically significant conclusions.
- **Generalizability**: While the findings are inferred from one system, this is common in many security studies. The results are contextualized to our datasets, but they contribute to new knowledge in an area that remains an open research field. Due to the lack of empirical analysis and validation on production datasets and real-world critical information systems, our results are relevant to practitioners.

### Conclusion

This paper proposes a method to automatically measure the occurrence of interesting activity within textual and heterogeneous runtime log streams. The method leverages the `log.entropy` term weighting scheme, which makes no assumptions about the structure of the logs, allowing it to handle unstructured text without prior knowledge of interesting patterns. The method was implemented using cutting-edge stream data analytics technologies, such as Apache Storm and Cassandra, and applied to a real-world Air Traffic Control information system. Measurements were conducted using the runtime logs generated by the system nodes. Off-line experiments characterized the system's behavioral baseline, and regular operations and misuse conditions were emulated. The results show that `log.entropy` measurements deviate from the system's behavioral baseline upon the occurrence of interesting activity. Misuse affects different logs, leading to deviations in `log.entropy` across the nodes. We also investigated how the characteristics of the interesting activity impact the measurements. Results indicate that even when the interesting activity consists of only one log entry, it can be discriminated with high precision and recall, even if it is half-similar to a baseline entry.

Future work will explore the potential of `log.entropy`-based measurements by evaluating the proposed method on different systems and security datasets to understand its limits and boundaries under various security scenarios. Additionally, efforts will be devoted to integrating the proposal into SIEM technologies, enabling them to handle unstructured data sources.

### Acknowledgment

This work was partially supported by the CINI Cybersecurity National Laboratory within the project FilieraSicura: Securing the Supply Chain of Domestic Critical Infrastructures from Cyber Attacks (www.filiersicura.it), funded by CISCO Systems Inc. and Leonardo SpA, and by the Italian Ministry of Education, University and Research under the NAPOLI FUTURA Start-up Project (PAC02L1_00161). The authors would like to thank Marco Delle Curti for his valuable support with the experimental testbed.

### References

[1] T. Mahmood and U. Afzal. Security analytics: Big data analytics for cybersecurity: A review of trends, techniques and tools. In Information Assurance (NCIA), 2013 2nd National Conference on, pages 129–134, Dec 2013.

[2] A. A. Cardenas, P. K. Manadhata, and S. P. Rajan. Big data analytics for security. IEEE Security & Privacy, 11(6):74–76, 2013.

[3] M. Cinque, D. Cotroneo, R. Della Corte, and A. Pecchia. Characterizing direct monitoring techniques in software systems. IEEE Transactions on Reliability, 65(4):1665–1681, Dec 2016.

[4] K. M. Kavanagh, O. Rochford, and T. Bussa. Magic quadrant for security information and event management. Technical report, Gartner Reasearch, 2016.

[5] A. A. Cardenas, P. K. Manadhata, and S. P. Rajan. Big data analytics for security intelligence. Technical report, Cloud Security Alliance - Big Data Working Group, 2013.

[6] A. Sharma, Z. Kalbarczyk, J. Barlow, and R. Iyer. Analysis of security data from a large computing organization. In The 41nd IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2011), pages 506–517, 2011.

[7] J. Cao, B. Yu, F. Dong, X. Zhu, and S. Xu. Entropy-based denial-of-service attack detection in cloud data center. Concurrency and Computation: Practice and Experience, 27(18):5623–5639, 2015.

[8] S. Yu, W. Zhou, R. Doss, and W. Jia. Traceback of ddos attacks using entropy variations. IEEE Transactions on Parallel and Distributed Systems, 22(3):412–425, March 2011.

[9] K. F. Hong, C. C. Chen, Y. T. Chiu, and K. S. Chou. Scalable command and control detection in log data through uf-icf analysis. In Security Technology (ICCST), 2015 International Carnahan Conference on, pages 293–298, Sept 2015.

[10] Y. Liao and V. R. Vemuri. Using text categorization techniques for intrusion detection. In Proceedings of the 11th USENIX Security Symposium, 2002.

[11] A. D’Amico and K. Whitley. The real work of computer network defense analysts. In VizSEC 2007, pages 19–37. Springer, 2008.

[12] G. P. Spathoulas and S. K. Katsikas. Reducing false positives in intrusion detection systems. Computers & Security, 29(1):35–44, 2010.

[13] N. A. Bakar, B. Belaton, and A. Samsudin. False positives reduction via intrusion alert quality framework. In Networks, 2005. Jointly held with the 2005 IEEE 7th Malaysia International Conference on Communication, volume 1, pages 6–pp. IEEE, 2005.

[14] R. Alshammari, S. Sonamthiang, M. Teimouri, and D. Riordan. Using neuro-fuzzy approach to reduce false positive alerts. In CNSR, pages 345–349. IEEE Computer Society, 2007.

[15] T. Pietraszek. Using adaptive alert classification to reduce false positives in intrusion detection. In Erland Jonsson, Alfonso Valdes, and Magnus Almgren, editors, RAID, volume 3224 of Lecture Notes in Computer Science, pages 102–124. Springer, 2004.

[16] X. Fu, J. Shi, and L. Xie. A novel data mining-based method for alert reduction and analysis. Journal of Networks, 5(1), 2010.

[17] F. Valeur, G. Vigna, C. Kruegel, and R. A. Kemmerer. Comprehensive approach to intrusion detection alert correlation. Dependable and Secure Computing, IEEE Transactions on, 1(3):146–169, 2004.

[18] D. Cotroneo, A. Paudice, and A. Pecchia. Automated root cause identification of security alerts: Evaluation in a saas cloud. Future Generation Computer Systems, 56:375 – 387, 2016.

[19] K. Julisch and M. Dacier. Mining intrusion detection alarms for actionable knowledge. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pages 366–375, New York, NY, USA, 2002. ACM.

[20] P. Giura and W. Wang. Using large scale distributed computing to unveil advanced persistent threats. Science J, 1(3):93–105, 2012.

[21] X. Shu, J. Smiy, D. Daphne Yao, and H. Lin. Massive distributed and parallel log analysis for organizational security. In 2013 IEEE Globecom Workshops (GC Wkshps), pages 194–199, Dec 2013.

[22] D. Gonçalves, J. Bota, and M. Correia. Big data analytics for detecting host misbehavior in large logs. In Trustcom/BigDataSE/ISPA, 2015 IEEE, volume 1, pages 238–245, Aug 2015.

[23] T.-F. Yen, A. Oprea, K. Onarlioglu, T. Leetham, W. Robertson, A. Juels, and E. Kirda. Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks. In Proceedings of the 29th Annual Computer Security Applications Conference, ACSAC ’13, pages 199–208, New York, NY, USA, 2013. ACM.

[24] C. W. Ten, G. Manimaran, and C. C. Liu. Cybersecurity for critical infrastructures: Attack and defense modeling. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans, 40(4):853–865, July 2010.

[25] D. Hadiosmanovic, D. Bolzoni, P. Hartel, and S. Etalle. Melissa: Towards automated detection of undesirable user actions in critical infrastructures. In Computer Network Defense (EC2ND), 2011 Seventh European Conference on, pages 41–48, Sept 2011.

[26] J. Timonen, L. Lääperi, L. Rummukainen, S. Puuska, and J. Vankka. Situational awareness and information collection from critical infrastructure. In Cyber Conflict (CyCon 2014), 2014 6th International Conference On, pages 157–173, June 2014.

[27] A. J. Oliner, A. Aiken, and J. Stearley. Alert detection in system logs. In IEEE International Conference on Data Mining, 2008.

[28] J. Stearley and A. J. Oliner. Bad words: Finding faults in spiritís syslogs. In IEEE International Symposium on Cluster Computing and the Grid, pages 765–770, 2008.

[29] C. Lim, N. Singh, and S. Yajnik. A Log Mining Approach to Failure Analysis of Enterprise Telephony Systems. In Proc. Intl. Conf. on Dependable Systems and Networks, June 2008.

[30] M. F. Porter. An algorithm for suffix stripping. In K. Sparck Jones and P. Willett, editors, Readings in Information Retrieval, pages 313–316, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers Inc.

[31] R. Vaarandi. Mining event logs with slct and loghound. In NOMS 2008 - 2008 IEEE Network Operations and Management Symposium, pages 1071–1074, April 2008.

[32] M. W. Berry, Z. Drmac, and E. R. Jessup. Matrices, vector spaces, and information retrieval. volume 41, pages 335–362. Society for Industrial and Applied Mathematics, June 1999.

[33] A. Pecchia, D. Cotroneo, R. Ganesan, and S. Sarkar. Filtering security alerts for the analysis of a production saas cloud. In Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing, UCC ’14. IEEE Computer Society, 2014.

[34] A. Toshniwal et al. Storm@twitter. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data, SIGMOD '14, pages 147–156, New York, NY, USA, 2014. ACM.

[35] R. Jain. The Art of Computer Systems Performance Analysis. Wiley & Sons New York, 1991.

[36] D. Ruiu. Cautionary tales: stealth coordinated attack how to, 1999.

[37] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Comput. Surv., 41(3):15:1–15:58, July 2009.

[38] X. Qin and W. Lee. Statistical causality analysis of infosec alert data. In Proceedings of The 6th International Symposium on Recent Advances in Intrusion Detection (RAID 2003), pages 73–93, 2003.