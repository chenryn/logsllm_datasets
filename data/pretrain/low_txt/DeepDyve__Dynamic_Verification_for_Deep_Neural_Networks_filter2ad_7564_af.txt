### DeepDyve and Its Impact on Real-Time Tasks

DeepDyve introduces latency and reduces throughput due to recomputation, which must be considered when performing real-time tasks. For instance, the DeepDyve model constructed on the GTSRB dataset experiences approximately a 1% loss in throughput. This overhead can be mitigated as the consistency rate between the checker DNN and the task DNN models increases.

### Evaluation on 8-bit Integer (INT-8) Data Representation

Our current evaluation is limited to the 8-bit integer (INT-8) data representation. This choice is motivated by two primary factors:
1. **Popularity and Support**: INT-8 is a widely used data type, supported by prominent deep learning frameworks and toolchains such as PyTorch, TensorFlow, NVIDIA® TensorRT, and Xilinx® DNNDK.
2. **Toolchain Compatibility**: The version of PyTorch (1.3) and the code from the BFA attack that we use only support INT-8 quantization. Future work will explore the impact of different data types.

### Related Works

Deep Neural Networks (DNNs) have become a critical technology for many safety-critical applications, leading to a growing interest in fault-tolerant DNN designs. 

#### Fault-Tolerance in DNN Systems

1. **RRAM-Based DNNs**:
   - Some studies focus on improving fault tolerance in DNN systems built with unreliable resistive random-access memory (RRAM). However, this technology is still in its early stages and not suitable for safety-critical AI applications.

2. **Fault-Tolerant Solutions for Conventional CMOS Devices**:
   - These solutions can be categorized into passive and active approaches.

   **Passive Fault-Tolerant Solutions**:
   - **Fault-Mitigation Techniques**:
     - **Resilient Training**: Explicitly considers hardware faults during the DNN training phase [7, 16, 43, 46].
     - **Resilient Architecture Design**: Seeks error-resilient network architectures or inserts redundancies for critical elements [5, 6, 8, 37].
     - **Device Hardening**: Protects critical storage elements by selectively hardening them [2, 23].
   - **Fault-Masking Techniques**:
     - **Error-Correcting Codes (ECC)**: Protects memory elements [35].
     - **Triple-Modular Redundancy (TMR)**: Protects computational units [29, 42].
   - **Challenges**: Significant design effort and high hardware overhead.

   **Active Fault-Tolerant Solutions**:
   - Detect and recover from faults by reorganizing the system in real-time. Online fault detection is crucial for these solutions [23, 24, 38].
   - **Techniques**:
     - [23, 38] have low hardware overhead but limited fault detection capabilities, especially for quantized DNNs.
     - [24] offers comparable fault coverage to DeepDyve but with much higher overhead.

### Conclusion

In this paper, we introduced DeepDyve, a novel dynamic verification technique to protect DNN systems against fault injection attacks. By incorporating a simpler and smaller checker DNN, DeepDyve significantly outperforms existing solutions, reducing risks by 90% at around 10% computational overhead.

### Acknowledgments

This work is supported in part by the General Research Fund (GRF) of the Hong Kong Research Grants Council (RGC) under Grant No. 14205018 and No. 14205420, and in part by the National Natural Science Foundation of China (NSFC) under Grant No. 61532017.

### References

[1] Todd M. Austin. 1999. DIVA: A reliable substrate for deep submicron microarchitecture design. In Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture (MICRO). IEEE, 196–207.

[2] Arash Azizimazreah, Yongbin Gu, Xiang Gu, and Lizhong Chen. 2018. Tolerating soft errors in deep learning accelerators with reliable on-chip memory designs. In IEEE International Conference on Networking, Architecture and Storage (NAS). IEEE, 1–10.

[3] Lerong Chen, Jiawen Li, Yiran Chen, Qiuping Deng, Jiyuan Shen, Xiaoyao Liang, and Li Jiang. 2017. Accelerator-friendly neural-network training: Learning variations and defects in RRAM crossbar. In Proceedings of the Conference on Design, Automation & Test in Europe (DATE). European Design and Automation Association, 19–24.

[4] Yu-Hsin Chen, Tushar Krishna, Joel S. Emer, and Vivienne Sze. 2016. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. In IEEE Journal of Solid-State Circuits (JSSC). IEEE, 127–138.

[5] C-T Chiu, Kishan Mehrotra, Chilukuri K. Mohan, and Sanjay Ranka. 1993. Robustness of feedforward neural networks. In IEEE International Conference on Neural Networks (ICNN). IEEE, 783–788.

[6] L-C Chu and Benjamin W. Wah. 1990. Fault tolerant neural networks with hybrid redundancy. In International Joint Conference on Neural Networks (IJCNN). IEEE, 639–649.

[7] Jiacnao Deng, Yuntan Rang, Zidong Du, Ymg Wang, Huawei Li, Olivier Temam, Paolo Ienne, David Novo, Xiaowei Li, Yunji Chen, et al. 2015. Retraining-based timing error mitigation for hardware neural networks. In Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 593–596.

[8] Fernando Morgado Dias, Rui Borralho, Pedro Fontes, and Ana Antunes. 2010. FTSET-a software tool for fault tolerance evaluation and improvement. In Neural Computing and Applications (Neural. Comput. Appl.), Vol. 19. Springer, 701–712.

[9] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In 3rd International Conference on Learning Representations (ICLR). OpenReview.net.

[10] Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. In 4th International Conference on Learning Representations (ICLR). OpenReview.net.

[11] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2014. Distilling the Knowledge in a Neural Network. In NIPS Deep Learning and Representation Learning Workshop (NIPS Workshop). Curran Associates Inc.

[12] Sanghyun Hong, Pietro Frigo, Yiğitcan Kaya, Cristiano Giuffrida, and Tudor Dumitras. 2019. Terminal brain damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks. In 28th USENIX Security Symposium (USENIX Security). USENIX Association, 497–514.

[13] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017).

[14] ISO. 2016. ISO-26262: Road vehicles – Functional safety. ISO, Geneva, Switzerland.

[15] Norman P. Jouppi and et al. 2017. In-datacenter performance analysis of a tensor processing unit. In ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1–12.

[16] Sung Kim, Patrick Howe, Thierry Moreau, Armin Alaghi, Luis Ceze, and Visvesh Sathe. 2018. MATIC: Learning around errors for efficient low-voltage neural network accelerators. In Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 1–6.

[17] Yoongu Kim, Daly Ross, and et. al. 2014. Flipping bits in memory without accessing them: An experimental study of DRAM disturbance errors. In ACM/IEEE 41st International Symposium on Computer Architecture (ISCA). IEEE, 361–372.

[18] Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional networks for efficient inference: A whitepaper. In arXiv preprint arXiv:1806.08342.

[19] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. Technical Report. Citeseer.

[20] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Machine Learning at Scale. In 5th International Conference on Learning Representations (ICLR). OpenReview.net.

[21] Ya Le and Xuan Yang. 2015. Tiny imagenet visual recognition challenge. Stanford CS 231N Course.

[22] Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In Advances in neural information processing systems (NIPS). Curran Associates Inc., 598–605.

[23] Guanpeng Li and et al. 2017. Understanding error propagation in deep learning neural network (DNN) accelerators and applications. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC). ACM, 8:1–8:12.

[24] Yu Li, Yannan Liu, Min Li, Ye Tian, Bo Luo, and Qiang Xu. 2019. D2NN: a fine-grained dual modular redundancy framework for deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference (ACSAC). ACM, 138–147.

[25] Chenchen Liu, Miao Hu, John Paul Strachan, and Hai Li. 2017. Rescuing memristor-based neuromorphic design with high defects. In 54th ACM/EDAC/IEEE Design Automation Conference (DAC). IEEE, 1–6.

[26] Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. 2017. Fault injection attack on deep neural network. In IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 131–138.

[27] Bo Luo, Yannan Liu, Lingxiao Wei, and Qiang Xu. 2018. Towards imperceptible and robust adversarial example attacks against neural networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI). AAAI Press, 1652–1659.

[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In 6th International Conference on Learning Representations (ICLR). OpenReview.net.

[29] Hamid Reza Mahdiani, Sied Mehdi Fakhraie, and Caro Lucas. 2012. Relaxed fault-tolerant hardware implementation of neural networks in the presence of multiple transient errors. IEEE transactions on neural networks and learning systems (TNNLS) 23, 1215–1228.

[30] Masato Matsubayashi, Akashi Satoh, and Jun Ishii. 2016. Clock glitch generator on SAKURA-G for fault injection attack against a cryptographic circuit. In IEEE 5th Global Conference on Consumer Electronics (GCCE). IEEE, 1–4.

[31] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the ACM SIGSAC conference on computer and communications security (CCS). ACM, 135–147.

[32] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine learning. In Proceedings of the ACM on Asia conference on computer and communications security (Asia CCS). ACM, 506–519.

[33] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. 2019. Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search. In IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 1211–1220.

[34] Brandon Reagen, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. Ares: A framework for quantifying the resilience of deep neural networks. In 55th ACM/ESDA/IEEE Design Automation Conference (DAC). IEEE, 1–6.

[35] Brandon Reagen, Paul Whatmough, and et. al. Adolf. 2016. Minerva: Enabling low-power, highly-accurate deep neural network accelerators. In ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA). IEEE, 267–278.

[36] Yolan Romailler and Sylvain Pelissier. 2017. Practical fault attack against the Ed25519 and EdDSA signature schemes. In Workshop on Fault Diagnosis and Tolerance in Cryptography (FDTC). IEEE, 17–24.

[37] Christoph Schorn, Thomas Elsken, Sebastian Vogel, Armin Runge, Andre Guntoro, and Gerd Ascheid. 2020. Automated design of error-resilient and hardware-efficient deep neural networks. In Neural Computing and Applications (Neural. Comput. Appl.). Springer, 1–19.

[38] Christoph Schorn, Andre Guntoro, and Gerd Ascheid. 2018. Efficient On-Line Error Detection and Mitigation for Deep Neural Network Accelerators. In International Conference on Computer Safety, Reliability, and Security (SAFECOMP). Springer, 205–219.

[39] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks 32, 323–332.

[40] Mingxing Tan and Quoc V. Le. 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In Proceedings of the 36th International Conference on Machine Learning (ICML), Vol. 97. PMLR, 6105–6114.

[41] Lixue Xia, Mengyun Liu, Xuefei Ning, Krishnendu Chakrabarty, and Yu Wang. 2017. Fault-tolerant training with on-line fault detection for RRAM-based neural computing systems. In Proceedings of the 54th Annual Design Automation Conference 2017 (DAC). ACM, 33:1–33:6.

[42] Zheyu Yan, Yiyu Shi, Wang Liao, Masanori Hashimoto, Xichuan Zhou, and Cheng Zhuo. 2020. When Single Event Upset Meets Deep Neural Networks: Observations, Explorations, and Remedies. In 25th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 163–168.

[43] Lita Yang and Boris Murmann. 2017. SRAM voltage scaling for energy-efficient convolutional neural networks. In 18th International Symposium on Quality Electronic Design (ISQED). IEEE, 7–12.

[44] Fan Yao, Adnan Siraj Rakin, and Deliang Fan. 2020. DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips. In 29th USENIX Security Symposium (USENIX Security). USENIX Association, 1463–1480.

[45] Pu Zhao, Siyue Wang, Cheng Gongye, Yanzhi Wang, Yunsi Fei, and Xue Lin. 2019. Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks. In Proceedings of the 56th Annual Design Automation Conference 2019 (DAC). ACM, 165.

[46] He Zhezhi, Rakin Adnan, Siraj, Li Jingtao, Chakrabarti Chaitali, and Deliang Fan. 2020. Defending and Harnessing the Bit-Flip based Adversarial Weight Attack. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 14083–14091.