### Ply Checks and Policy Oracle

The ply check verifies if all known users are currently online, i.e., whether \( O_i = M_i \). If this condition is met, it returns \( P_i = M_i \); otherwise, it returns \( P_i = \emptyset \). In effect, the Policy Oracle halts system progress—allowing no one to post to any Nym—unless all users are online. Since messages are only posted when all users are online, the intersection of all non-empty rounds' user sets is \( M_i \), and the system maintains "perfect" anonymity, assuming the Anonymizer functions as required. The trade-off is effective system availability, which would be impractical in most real-world scenarios.

### Key Technical Challenge

A primary contribution of this paper is developing more nuanced methods to control the user filtering step in each round. By managing these filtering choices, we aim to maintain both measurable anonymity levels under intersection attacks and "usable" levels of availability, under realistic conditions.

### Limitations and Realistic Expectations

In some cases, no active control mechanism can help. For example, if all but one user go offline permanently, the Policy Oracle has two options: allow the remaining user to post, thereby sacrificing all anonymity under intersection attack, or filter that user forever, sacrificing availability. Thus, we must set realistic expectations. Section 5 experimentally investigates the feasibility of resisting intersection attacks in IRC communities and tests possible control policies against feasibility metrics.

### Architectural Separation

Buddies' architecture separates the Policy Oracle from the Anonymizer, allowing the Policy Oracle access only to "public information" assumed to be known by everyone, including the adversary. This eliminates the risk that policies might "accidentally" compromise anonymity by leaking Nym ownership. By disallowing the Policy Oracle from accessing private information, we avoid the need to analyze each policy for "side-channel" anonymity leaks and focus on how effectively a policy mitigates intersection attacks while maintaining usable availability.

### Public Information and Adversary Strength

Another issue is whether the information the Policy Oracle needs to simulate the Adversary's intersection attacks—such as the set of users online in each round—should be considered "public information." Although an ideal global adversary would know this information, more realistic adversaries may not be able to monitor all users. If Buddies' design "hands out" information that would otherwise be at least partially private—such as IP addresses of all online users—we risk strengthening a weak adversary into an effectively omniscient one. In the case of users' network identities like IP addresses, our design mitigates this leak by replacing IP addresses with anonymized tags when reporting online user sets to the Policy Oracle, as discussed in Section 4.3. However, whether Buddies' simulation-based architecture may strengthen weak adversaries in other unexpected ways, by making "too much information" public, merits further study.

### Analyzing Intersection Attacks

While we do not attempt full formal analysis, the simplicity of the conceptual model facilitates straightforward informal analysis. Our focus is on what an adversary can learn from users' online status over time (the "switches" in Figure 1). Other known attacks against practical anonymity systems are important but out of this paper's scope. We also claim no particular novelty in our analysis techniques or metrics; our goal is to apply known attacks [16, 31, 42] and anonymity metrics [17, 43] to the Buddies model.

We assume the Anonymizer is trusted to keep secret the linkage between Users and the Nyms they own. We also assume honest Users do not "give away" their identities or the relationships between their Nyms via the messages they post. Under these conditions, the Adversary obtains three potentially important pieces of information in each round \( i \): (a) the set of online users \( O_i \), (b) the set \( P_i \) of online users who passed the Policy Oracle’s filter in step 5, and (c) the \( B_i \) message bits that were posted to the scheduled Nym \( T_i \).

An observation key to Buddies’ design is that only (b) and (c) are relevant to intersection attack analysis: the adversary gains no useful information from knowing which users were online during a given round, beyond what the adversary learns from knowing which users were online and unfiltered.

Since we assume honest users do not "give away" their identities in their message content, we ultimately care only whether the message posted to Nym \( T_i \) was null or non-null. If a non-null message appeared for Nym \( T_i \) in round \( i \), the adversary infers that the owner of \( T_i \) must be a member of the filtered user set \( P_i \) in that round. If a null message appears for \( T_i \), the Anonymizer's design ensures that the adversary cannot distinguish among the following three possibilities: (1) the owner of Nym \( T_i \) was offline, (2) the owner was online but filtered, or (3) the owner was online and unfiltered, but had nothing useful to send and thus intentionally posted a null message.

### Possibilistic Anonymity Analysis

To construct a simple possibilistic anonymity set \( P_N \) for a given Nym \( N \), the adversary intersects the filtered user sets \( P_i \) across all rounds \( i \) for which Nym \( N \) was scheduled and a non-null message appeared: i.e., \( P_N = \bigcap_{i} \{O_i | T_i = N \wedge m_i \neq 0\} \). Thus, \( P_N \) represents the set of users who might conceivably own Nym \( N \), consistent with the observed set of non-null messages that have appeared for Nym \( N \) up to any point in time. Null-message rounds do not eliminate the possibility that users offline during that round may own \( N \), so such rounds leave the possibilistic anonymity set \( P_N \) unaffected.

We define the size of a Nym’s possibilistic anonymity set, \( |P_N| \), as Nym \( N \)'s possibilistic anonymity, which we abbreviate as possinymity. Although possinymity is a simplistic metric, it captures a useful measure of "plausible deniability." If, for example, a user is brought to court, and the judge is shown network traces of a Buddies system in which the accused is one of \( |P_N| \) users who may in principle have posted an offending message, then a large possibilistic anonymity may help sow uncertainty about the user’s guilt. We acknowledge the weaknesses of plausible deniability, especially in environments where "innocent until proven guilty" is not the operative principle.

### Probabilistic Anonymity Analysis

While a simplistic adversary might stop at the above analysis, a smarter adversary can probabilistically learn from rounds in which only a null message appeared. Suppose the adversary correctly surmises that, in each round \( i \), the owner of Nym \( N \) will have no useful message to post with some independent and uniformly random probability \( p \). In this case, the user will "pass" by submitting a null message. With probability \( 1 - p \), the user will have a non-null message and will try to post it—but this post attempt fails, yielding a null message anyway, if the owner is offline or filtered in that round.

For simplicity, assume there are two users \( A \) and \( B \), the adversary observes exactly one round \( i \), this round results in a null message, and \( P_i = \{A\} \): user \( A \) participated but user \( B \) did not. The null output from round \( i \) means one of two events occurred: (a) \( A \) owns \( N \), but chose with probability \( p \) not to post in round \( i \); or (b) \( B \) owns \( N \), and no message appeared independently of \( p \) because \( B \notin P_i \). Because Nyms are assigned to users uniformly at random on creation, the "base" probability that either user owns \( N \) is \( 1/2 \). The probability of the above events (a) and (b) occurring conditioned on the observed history, however, is different. To be precise, \( P[(a) | (a) \cup (b)] = \frac{p/2}{p/2 + 1/2} = \frac{p}{p + 1} \), and \( P[(b) | (a) \cup (b)] = \frac{1/2}{p/2 + 1/2} = \frac{1}{p + 1} \).

From the adversary’s perspective, observing one round in which no message appears for Nym \( N \), and in which \( A \) participated but \( B \) did not, reduces the relative likelihood of \( A \) being the owner by a factor of \( p \). Observing similar events across multiple rounds exponentially increases the adversary’s "certainty" of \( B \) being the owner: after \( k \) such rounds, the likelihood of \( A \) being the owner is only \( \frac{p^k}{p^k + 1} \).

### Indistinguishability Under Probabilistic Attack

The above reasoning generalizes to many users and varying probabilities of posting. Our focus is not on deepening such analysis, a goal addressed in prior work [17, 43]. Instead, we wish to achieve measurable resistance to unknown probabilistic attacks. We do not know the probabilities with which users will attempt to post in particular rounds or how well the unknown attacker may predict when the owner of a given Nym will post. Instead of relying on the relevance of any particular probabilistic analysis, Buddies relies on an indistinguishability principle that applies to all attacks of this class. If two users \( A \) and \( B \) have exhibited identical histories with respect to inclusion in each round’s filtered user set \( P_i \), across all rounds \( i \) in which a Nym \( N \) was scheduled so far, then under any probabilistic analysis, the adversary must assign identical probabilities to \( A \) and \( B \) owning Nym \( N \). That is, if for every round \( i \), it holds that \( (A \in P_i) \Leftrightarrow (B \in P_i) \), then users \( A \) and \( B \) are probabilistically indistinguishable from each other, hence equally likely to own Nym \( N \).

For any user \( A \) and Nym \( N \), we define \( A \)'s buddy set \( B_N(A) \) as the set of users probabilistically indistinguishable from \( A \), including \( A \) itself, with respect to potential ownership of Nym \( N \). If \( n \) users are probabilistically indistinguishable from \( A \), then under the attacker’s analysis, each such user in \( B_N(A) \) has an individual probability no greater than \( \frac{1}{|B_N(A)|} \) of being the owner of \( N \). Intuitively, buddy-sets form equivalence classes of users who "hang together" against probabilistic intersection attacks—so that individual buddies do not "hang separately."

We next define a second anonymity metric, indistinguishability set size, or indinymity for short, as the size of the smallest buddy-set for a given Nym \( N \). Since we do not know how a real attacker will actually assign probabilities to users, indinymity represents the minimum level of anonymity a member of any buddy set can expect to retain, even if the adversary correctly intersects the owner’s anonymity set down to the members of that buddy set. Thus, the attacker cannot (correctly) assign a probability greater than \( \frac{1}{|B_N|} \) to any user, including, but not limited to, the owner of \( N \).

One might argue that we "mainly" care about the buddy set containing the true owner of \( N \), not about other buddy sets not containing the owner. A counter-argument, however, is that a particular observation history might make some other buddy set falsely appear to the adversary as "more likely" to own \( N \). In this case, we may well care how much protection the innocent members of that "unlucky" buddy set have against being "falsely accused" of owning \( N \). Thus, to ensure that all users have the "strength in numbers" of being indistinguishable in a crowd of at least \( n \) users, regardless of the adversary’s probabilistic reasoning, we must ensure that all buddy sets have size at least \( n \).

### Attack Mitigation Policies

Based on the above architecture, we now explore possible intersection attack mitigation policies. We make no claim that these are the "right" policies, merely a starting point for ongoing refinement. Two key benefits of Buddies' architecture are to modularize these policies into replaceable components independent of the rest of the anonymous communication system, allowing for easy evolution, and to ensure by system construction that policies cannot leak sensitive information other than by failing to protect adequately against intersection attacks.

We first explore policies for maintaining possinymity, then policies to enforce a lower bound on indinymity. An important caveat with any anonymity metric is that Buddies cannot guarantee that measured anonymity necessarily represents useful anonymity, if, for example, an attacker can compromise many users or create many Sybil identities [19]. Section 4.4 discusses these issues in more detail.

### Maximizing Possinymity

The possinymity metric defined in Section 2.3.1 considers only rounds in which non-null messages appear for some Nym \( N \), intersecting the filtered user sets across all such rounds to determine \( N \)'s possinymity set \( P_N \). We consider several relevant goals: maintaining a minimum possinymity level, mitigating the rate of possinymity loss, or both.

#### Maintaining a Possinymity Threshold

Suppose a dissident, posting anonymously in a public chat room under a Nym \( N \), wishes to maintain "plausible deniability" by ensuring that \( |P_N| \geq 100 \) throughout the conversation—and would rather be abruptly disconnected from the conversation (or have Nym \( N \) effectively "squelched") than risk \( |P_N| \) going below this threshold. As a straightforward policy for this case, at step 5 of each round \( i \), the Policy Oracle computes the new possinymity that \( N \) would have if \( O_i \) is intersected with \( N \)'s "running" possinymity set from the prior round. The Policy Oracle returns \( P_i = O_i \) if the new possinymity remains above the threshold, or \( P_i = \emptyset \) otherwise.

In practice, the effect is that \( N \)'s possinymity starts at an initial maximum of the total set of users online when the user first posts via \( N \), then decreases down to (but not below) the possinymity threshold as other users go offline.