### Bounding the Expression with Real Numbers

We can bound the above expression by replacing natural numbers with real numbers, resulting in:
\[ SS_{\sigma,b}(\hat{x}_{\lambda p}) \leq 20\kappa(-\ln \delta)^{1.5} \exp(-1) G^{-1} - \left(2p_{\text{max}} \ln p_{\text{max}}\right) \frac{n_s (1 - \beta)}{n} \]

### Influence of Global Parameters on the Improvement Factor
Figure 7 illustrates the influence of global parameters on the improvement factor.

### Private Choice of Parameters

The parameters required as input for Mechanism 1 are \( \epsilon_1 \), \( \epsilon_2 \), \( \delta \), \( m \), and \( r \). For privacy reasons, these parameters cannot be optimized based on a specific input dataset. Therefore, we discuss some heuristic choices for these parameters based on our experiments.

- **Choosing \( \epsilon \) and \( \delta \)**: These parameters can be chosen in the standard way. For instance, \( \epsilon = 1 \) and \( \delta = n^{-2} \).
- **Choosing \( \epsilon_1 \)**: From Tables I and II, a value in the range 0.8 to 0.9 is a reasonable choice for \( \epsilon_1 \). Note that \( \epsilon_2 \) is determined by \( \epsilon \) and \( \epsilon_1 \).
- **Choosing \( r \)**: From the same tables, an \( r \) between 1 and 2 suffices.
- **Choosing \( m \)**: We specify two criteria that should be satisfied by the time lag \( m \). By assigning reasonably conservative values to the free parameters in the two criteria, we obtain a value of \( m \) that is expected to provide good utility in practice. With \( \beta = 0.02 \), \( \delta = 2^{-20} \), \( \epsilon = 1 \), and \( p_{\text{max}} = 0.005 \), we find that \( m \) should be at least 20,000 to satisfy the first criterion. The second criterion imposes \( m > 80,000 \) for good utility. From Tables I and II, we see that with \( m > 80,000 \), we indeed obtain good utility. The higher the time lag \( m \), the better the utility gain, with the trade-off being a longer time lag before outputting the sum.

### Experimental Evaluation

#### Accumulated Error on the Sum

We now show the improvement factor in computing the moving average (sum) through our mechanism. Since the error is maximized at step \( n \), i.e., the last observation of the stream, we compare the value of \( \hat{c}(\sigma, n) \) through our mechanism against its counterpart via the BT algorithm. For both datasets, we run the two mechanisms a total of 20,000 times and display the empirical probability density function (PDF) of the error.

1. **Train Trips Dataset**: We fixed \( \epsilon = 1 \), \( \delta = 2^{-20} \), \( \beta = 0.02 \), \( m = 50,000 \), and \( n = 25,000,000 \), and obtained the values of the local parameters after optimization, shown in Table I. We set \( B = 1440 \) minutes, which is the maximum possible commute time in a 24-hour period. Figure 8 shows the PDF of the resulting error (normalized by the maximum value \( B \)) of our mechanism and the BT algorithm. We see that the error in our case is more tightly concentrated around 0. On average, we obtain an improvement factor of 3.5.

2. **Supermarket Dataset**: For the supermarket dataset, we use the same set of parameters except that we have \( n = 150,000 \) (due to fewer data points) and \( B = 3,000 \) dollars (a conservative guess on the amount spent). Figure 10 shows the PDF of the error from our mechanism and the BT algorithm. Once again, the error through our mechanism is more tightly concentrated around 0. For this dataset, we perform much better than the BT algorithm, with an improvement factor of 9 on average.

### Distribution of Outlier Error Ratio

We are also interested in knowing whether our estimation of the outlier error (i.e., the second summand in Eq. 16) is close to the actual outlier error. This will validate whether our assumption that the distribution of the dataset is light-tailed. To verify this, we re-ran our mechanism 20,000 times on the same dataset and obtained the ratio of the estimated error to the real error. Figure 9 shows that we have erred on the precautionary side, with our estimation of outlier error being well below the real error.

### Does the Distribution Remain Light-Tailed across Time?

Recall that our mechanism promises improved utility based on the premise that the data distribution is light-tailed. Since the input stream is time-dependent, the estimated threshold (using the \( \lambda_p \)-quantile) through \( m \) observations with a given time period may be drastically different from its estimate via a different time period. To ensure this is not the case, we analyzed the distribution of the train trips dataset across different hours and different days of the week. The distributions are shown in Figures 12 and 13, respectively. While the beginning of the distributions shows variation based on the time period, the tails are similar and light-tailed. Thus, our estimated threshold is likely to improve utility independent of the time period in real datasets.

### Related Work

As previously noted, the privacy-preserving algorithms for continual release of statistics from binary streams proposed in [4] and [5] can be generalized to the scenario addressed in this paper, i.e., release of statistics from a stream whose values are from the real interval \([0, B]\). Indeed, we have used the algorithm from [5] as one of the components of our method. However, the focus of the works in [4] and [5] is on improving the error for binary strings, which do not have the added factor of \( B \). The two algorithms are based on event-level privacy. If the aim of privacy is to protect all events from an individual (e.g., all trips made by an individual over the course of the whole year), then the privacy provided by these algorithms is insufficient. The work from [14] attempts to improve this by offering privacy for up to \( w \) successive events. Noting that any \( w \) successive events might not contain multiple events originating from a single individual, the authors from [15] introduce \( l \)-trajectory privacy, where any successive \( l \) events from a user are targeted for privacy. These works essentially propose privacy mechanisms for variants of the definition of differential privacy where neighboring streams are defined differently from the standard definition of Hamming distance. We note that our method can be easily used in conjunction with these algorithms, as we only use the BT algorithm from [5] in a modular way. However, to find a utility-maximizing threshold in a differentially private manner for any variation in the definition of neighboring streams requires tweaking our mechanism. Likewise, these algorithms also target infinite streams as opposed to bounded streams (as is done in our paper). Application of our approach to these settings is an interesting area for future work.

### Discussion

An interesting question to ponder is what kind of data distributions are likely to have a light-tailed distribution. Looking at the two datasets evaluated in this paper, we see that one common characteristic is that they emerge from short-lived, time-constrained events. Thus, more generally, streaming data with short-lived events is likely to exhibit light-tailed distributions. In addition to the two datasets used in this paper, other examples of data exhibiting a light-tailed distribution include smart meter-based energy readings data (e.g., electricity usage), phone call durations data, length of posts/comments on online social networks (e.g., on the website Reddit), average time spent on a given location, or daily average inter-arrivals of check-in times (location-based networks). Consequently, the resulting readings are bounded, even though the bound \( B \) might be unknown in advance or only loosely known. These are in contrast to heavy-tailed distributions where (underlying) events are not short-lived or time-constrained, e.g., income distribution, file sizes in computer systems, and network traffic over a long period of time.

We would like to stress that if the input distribution is not light-tailed, our estimated threshold \( \tau \) would be closer to the global bound \( B \). Thus, in the worst case, we would be able to provide utility similar to the BT algorithm (with the disadvantage that we add a time lag \( m \)). An example of this is a uniform distribution over \([0, B]\), where the threshold \( \tau \) would be close to \( B \) (estimated via the first \( m \) readings). A similar argument applies to other heavy-tailed distributions. Importantly, our privacy definition is not dependent on the light-tailed distribution assumption. Thus, the privacy guarantee remains the same regardless of the nature of the input distribution.

### Conclusion

We have presented a privacy-preserving mechanism to continually display the moving average of a stream of observations where the bound on each observation is either too conservative or not known a priori. We have relied on justified assumptions on real-world datasets to obtain a better bound on observations of the stream. Moreover, we have shown how to obtain this bound in a differentially private manner while optimizing utility. Our mechanism can be applied to many real-world applications where continuous monitoring and reporting of statistics is required, e.g., smart meter data and commute times. Our techniques can be improved in several ways. We have relied on the quantile to estimate the bound on the streaming data based on smooth sensitivity. There may be other ways to display the quantile using other robust statistics. Our mechanism can be adapted to compute functions other than the moving average. Likewise, our method can be used in conjunction with algorithms that provide privacy for multiple events instead of single events as is done in this paper. Overall, we see our work as an instance of applying differential privacy in practice.

### References

[1] A. Molina-Markham, P. Shenoy, K. Fu, E. Cecchet, and D. Irwin, âPrivate memoirs of a smart meter,â in Proceedings of the 2nd ACM workshop on embedded sensing systems for energy-efficiency in building. ACM, 2010, pp. 61â66.

[2] C. Dwork, F. McSherry, K. Nissim, and A. Smith, âCalibrating noise to sensitivity in private data analysis,â in TCC, vol. 3876. Springer, 2006, pp. 265â284.

[3] C. Dwork, A. Roth et al., âThe algorithmic foundations of differential privacy,â Foundations and Trends R in Theoretical Computer Science, vol. 9, no. 3â4, pp. 211â407, 2014.

[4] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum, âDifferential privacy under continual observation,â in Proceedings of the forty-second ACM symposium on Theory of computing. ACM, 2010, pp. 715â724.

[5] T.-H. H. Chan, E. Shi, and D. Song, âPrivate and continual release of statistics,â ACM Transactions on Information and System Security (TISSEC), vol. 14, no. 3, p. 26, 2011.