# Intrusion Detection Systems

Network intrusion detection systems (NIDS) should be relatively straightforward to evaluate. Given a traffic dump collected during real or simulated intrusions, a NIDS should be able to detect a subset of the attacks while producing a minimal number of false positives. This is not as straightforward for other types of intrusion detection systems, such as host-based and application-based systems, because the quantity and quality of information collected about the actions performed by the operating system (OS) and its applications can vary significantly.

Additionally, systems that use an anomaly-based approach to intrusion detection require training data, which must be realistic, complete, and free from attacks. Collecting and generating this type of data is particularly challenging.

## Evaluation Datasets

Despite the challenges in creating a dataset for comparing the performance of intrusion detection systems, a group of researchers from the MIT Lincoln Laboratory undertook this task in 1998 and 1999. They produced a dataset that included both training data and test data (with truth files) in the form of network packets, OS audit records, and file system dumps [6, 7].

These datasets were used to evaluate several intrusion detection systems developed by academic researchers. The evaluation process began with the distribution of attack-free training data to the participants. After a period, the test data containing attacks was distributed without the truth files. Participants then had to identify the attacks and submit their detection alerts, which were evaluated against the truth files.

The results of the evaluation were disclosed partially, without declaring a clear winner, to avoid making any single group look bad. Instead, the authors provided a set of scores that considered various characteristics of the systems, creating a no-winner/no-loser situation. We believe this was a missed opportunity to foster research by creating a competition with a clear winner, as demonstrated by other challenges like the DARPA Grand Challenge for unmanned vehicles. Such competitions motivate competitors, foster innovation and creativity, and provide great publicity for both the participants and the funding agency.

### Determining a Winner

A more rigorous approach to evaluation would involve combining the recall and precision of the intrusion detection systems. Specifically, the effectiveness of a system can be measured by calculating the percentage of hits \( H \) over the total number of attacks \( T \), i.e., \(\frac{H}{T} \times 100\). This measures how many attacks were detected relative to the overall set of attacks (recall).

To characterize the precision of the system, one would compute the percentage of false alarms \( F \) over the total number of detections \( H + F \), i.e., \(\frac{H}{H + F} \times 100\). For example, a system with three detections and no false alarms would have a precision of 100%, but it would not be very effective if the dataset contained hundreds of attack instances. Conversely, a system that flagged every packet as malicious would have an effectiveness of 100% but an abysmal precision. Therefore, the optimal approach is to multiply the two measures to account for both aspects of intrusion detection.

Table 2 shows the values of these metrics for the systems that participated in the 1999 MIT Lincoln Laboratory evaluation. According to the proposed metrics, UCSB’s NetSTAT would be the winner of the 1999 competition, closely followed by SRI’s EMERALD.

### Criticisms and Impact

Although the evaluation did not declare a clear winner and faced some criticisms [10], the dataset produced was immensely popular and remains the most used dataset in the intrusion detection community. Unfortunately, the MIT/LL dataset and corresponding truth files were used in scientific publications where the performance of intrusion detection systems evaluated on the non-blind dataset was compared to those that participated in the blind evaluation, leading to unfair results. Consequently, the dataset has become outdated and is rarely used in recent research publications.

## The Evolution of Intrusion Detection

In the years following the MIT/LL evaluation, there was increased skepticism towards network intrusion detection and its ability to detect attacks, especially zero-day exploits and mutations of existing attacks [17]. Researchers also started developing attacks against stateful intrusion detection systems, exposing the challenge of detecting low-traffic, slow-paced attacks that can last months or even years.

There was a shift from analyzing network data to analyzing host data, under the assumption that monitoring end nodes could better detect sophisticated attacks. During the early 2000s, academia lost interest in network intrusion detection, while the use of commercial NIDS became a best practice in enterprise networks. Sometimes, NIDS were relabeled as "intrusion prevention systems" to describe systems with traffic-blocking responses.

By 2003-2004, research on the "classic" network intrusion detection problem (i.e., detecting attacks by looking at network packets) was declining. However, techniques used to characterize network attacks were applied to the detection of malicious code components, such as worms and bots. Both misuse-based and anomaly-based techniques were leveraged to identify various kinds of malware.

This "reborn" network intrusion detection research heavily uses data-mining and machine-learning techniques to address the main problem associated with misuse-based NIDS: the need for manual specification of attack models. Some seminal work in this field was performed in the late 1990s [5].

## Conclusions

Even though the term "Intrusion Detection" is sometimes looked down upon by the academic community, it will always be a core part of the security field. The focus of intrusion detection may shift towards more semantically-rich domains, such as the OS and the web. For example, web-based intrusion detection systems (often referred to as "Web Application Firewalls") leverage knowledge about the characteristics of web applications and their logic to identify attacks. These systems use concepts that were researched and applied more than two decades ago.

The re-invention of network intrusion detection techniques and approaches highlights the ongoing importance of intrusion detection (be it network-based, web-based, or host-based) as a research problem. As new attacks and new ways of compromising systems are introduced, both researchers and practitioners will develop or re-discover techniques for analyzing events to identify malicious activity.

The next challenge will be to expand the scope of intrusion detection to consider the surrounding context, including abstract and difficult-to-define concepts such as missions, tasks, and stakeholders, when analyzing data to identify malicious intent.

## References

[1] C. Berge. Hypergraphs. North-Holland, 1989.
[2] S. Eckmann, G. Vigna, and R. Kemmerer. STATL: An Attack Language for State-based Intrusion Detection. Journal of Computer Security, 10(1,2):71–104, 2002.
[3] L. Heberlein, G. Dias, K. Levitt, B. Mukherjee, J. Wood, and D. Wolber. A Network Security Monitor. In Proceedings of the IEEE Symposium on Research in Security and Privacy, pages 296 – 304, Oakland, CA, May 1990.
[4] K. Ilgun, R. Kemmerer, and P. Porras. State Transition Analysis: A Rule-Based Intrusion Detection System. IEEE Transactions on Software Engineering, 21(3):181–199, March 1995.
[5] W. Lee and S. Stolfo. Data Mining Approaches for Intrusion Detection. In Proceedings of the USENIX Security Symposium, San Antonio, TX, January 1998.
[6] R. Lippmann, D. Fried, I. Graf, J. Haines, K. Kendall, D. McClung, D. Weber, S. Webster, D. Wyschogrod, R. Cunningham, and M. Zissman. Evaluating Intrustion Detection Systems: The 1998 DARPA Oﬀ-line Intrusion Detection Evaluation. In Proceedings of the DARPA Information Survivability Conference and Exposition, Volume 2, Hilton Head, SC, January 2000.
[7] R. Lippmann and J. Haines. Analysis and Results of the 1999 DARPA Oﬀ-Line Intrusion Detection Evaluation. In Proceedings of the Symposium on the Recent Advances in Intrusion Detection (RAID), pages 162–182, Toulouse, France, 2000.
[8] S. McCanne and V. Jacobson. The BSD Packet Filter: A New Architecture for User-level Packet Capture. In Proceedings of the 1993 Winter USENIX Conference, San Diego, CA, January 1993.
[9] S. McCanne, C. Leres, and V. Jacobson. Tcpdump 3.7. Documentation, 2002.
[10] J. McHugh. Testing Intrusion Detection Systems: A Critique of the 1998 and 1999 DARPA Intrusion Detection System Evaluations as Performed by Lincoln Laboratory. ACM Transaction on Information and System Security, 3(4), November 2000.
[11] V. Paxson. Bro: A System for Detecting Network Intruders in Real-Time. In Proceedings of the 7th USENIX Security Symposium, San Antonio, TX, January 1998.
[12] P. Porras. STAT – A State Transition Analysis Tool for Intrusion Detection. Master’s thesis, Computer Science Department, University of California, Santa Barbara, June 1992.
[13] P. Porras and P. Neumann. EMERALD: Event Monitoring Enabling Responses to Anomalous Live Disturbances. In Proceedings of the 1997 National Information Systems Security Conference, October 1997.
[14] M. Roesch. Snort - Lightweight Intrusion Detection for Networks. In Proceedings of the USENIX LISA ’99 Conference, Seattle, WA, November 1999.
[15] G. Vigna. A Topological Characterization of TCP/IP Security. In Proceedings of the 12th International Symposium of Formal Methods Europe (FME ’03), number 2805 in LNCS, pages 914–940, Pisa, Italy, September 2003. Springer-Verlag.
[16] G. Vigna and R. Kemmerer. NetSTAT: A Network-based Intrusion Detection Approach. In Proceedings of the 14th Annual Computer Security Applications Conference (ACSAC ’98), pages 25–34, Scottsdale, AZ, December 1998. IEEE Press.
[17] G. Vigna, W. Robertson, and D. Balzarotti. Testing Network-based Intrusion Detection Signatures Using Mutant Exploits. In Proceedings of the ACM Conference on Computer and Communication Security (ACM CCS), pages 21–30, Washington, DC, October 2004.