### Threats and Extension Installation

With the possible exception of cookies, threats often require the installation of spoofing extensions. As a rough metric, consider that Adblock Plus, the most popular extension for Mozilla Firefox, is installed by fifteen million users—25 times more than UserAgent Switcher, the most popular extension in Table IV.

### Characterization of the Extension Problem

We characterize the issue with these extensions as iatrogenic. Users who install such extensions in an attempt to blend in with the crowd of popular browsers inadvertently make themselves more visible and distinguishable from other users who do not modify their browsers. Consequently, we advise against using user-agent-spoofing extensions as a means of enhancing privacy. Our findings contradict the advice given by Yen et al. [18], who suggest that user-agent-spoofing extensions can be used to make tracking more difficult. Although their study focuses on common identifiers reported by client-side HTTP headers and the client's IP address, a server capable of viewing these can respond with JavaScript code that will uncover the user-agent-spoofing extension using the techniques mentioned earlier.

### Discussion

#### Reducing the Fingerprintable Surface

**Flash:**
As described in Section II, Adobe Flash was utilized by all three fingerprinting libraries we studied due to its rich API, which allows SWF files to access information not typically available through a browser's API. In all cases, the SWF file responsible for gathering information was hidden from the user, either by setting the width and height of the `<object>` tag to zero or by placing it in an iframe with zero height and width. This observation can serve as a first line of defense. Modern browsers have extensions that prevent Flash and Silverlight from loading until explicitly requested by the user (e.g., through a click on the object itself). These hidden files cannot be clicked on and thus will never execute. While this solution effectively stops the Flash part of the fingerprint, it can be circumvented if the fingerprinting code is wrapped into an object of the first-party site and made necessary for the page's functionality. This, however, requires more integration between the first-party website and the third-party fingerprinting company than the current "one-size-fits-all" JavaScript and Flash model.

In the long run, the best solution against fingerprinting through Flash should come directly from Flash. In the past, researchers discovered that Flash's Local Shared Objects (LSOs), the equivalent of browser cookies, were not deleted when a user exited the browser's private mode or even when they used the "Clear Private Data" option. As a result, in the latest version of Flash, LSOs are not stored to disk but kept in memory when the browser's private mode is enabled. Similarly, when a browser enters private mode, Flash could provide less system information, respect any browser-set HTTP proxies, and report only a standard subset of a system's fonts to protect a user's environment from fingerprinting.

**JavaScript:**
Multiple vendors are involved in developing JavaScript engines, and each major browser uses a different engine. To unify JavaScript behavior across different browsers, all vendors would need to agree on a single set of API calls and internal implementation specifics. For example, hash table implementations can affect the order of objects in the exposed data structures of JavaScript, which can be used to fingerprint the engine's type and version. Achieving such a consensus is challenging, and we have seen divergences in the exposed APIs of JavaScript, even in the names of functions that offer the same functionality (e.g., `execScript` and `eval`). Additionally, vendors may be reluctant to follow specific design choices that might affect performance. However, browsers could agree to sacrifice performance in "private mode" to expose a unified interface.

#### Alternative Uses of Fingerprinting

While we have primarily focused on fingerprinting as a fraud-detection and web-tracking mechanism, there is another aspect that requires attention. Attackers use fingerprinting to determine if the browser they are executing on is vulnerable to one of the multiple available exploits. This way, the server can decide which exploit to reveal to the client, exposing as little of their attack capabilities as possible. There are three different architectures to detect drive-by downloads: low-interaction honeypots, high-interaction honeypots, and honeyclients. In all three cases, the browser is either a specially crafted one for instrumenting visited pages or a browser installation that was never used by a real user. Given the precise, browser-revealing fingerprinting techniques described in this paper, it is possible that attackers will use these mechanisms in the future to detect monitoring environments and circumvent detection.

### Related Work

To the best of our knowledge, this paper is the first to study the problem of web-based fingerprinting from the perspectives of all the players involved: fingerprinting providers and their methods, sites utilizing fingerprinting, users employing privacy-preserving extensions, and the browser's internals and how they relate to its identity.

Eckersley conducted the first large-scale study showing that various properties of a user's browser and plugins can be combined to form a unique fingerprint [12]. Eckersley found that 94.2% of about 500,000 users who visited panopticlick.eff.org and had Flash or Java enabled could be uniquely identified. His study, and the surprisingly accurate identification results, prompted us to investigate commercial fingerprinting companies and their approaches. Yen et al. [18] performed a similar fingerprinting study by analyzing month-long logs of Bing and Hotmail. Interestingly, the authors used a client's IP address as part of their tracking mechanism, which Eckersley explicitly avoided, dismissing it as "not sufficiently stable." The authors advocated the use of user-agent-spoofing extensions, but as discussed in Section V, this is counterproductive since it allows for more fingerprinting rather than less.

Mowery et al. [13] proposed using benchmark execution time as a way of fingerprinting JavaScript implementations, assuming that specific versions of JavaScript engines will perform consistently. Each browser executes a set of predefined JavaScript benchmarks, and the completion time of each benchmark forms part of the browser's performance signature. While their method correctly detects a browser family (e.g., Chrome) 98.2% of the time, it requires over three minutes to fully execute. According to a study by Alenty [38], the average view time of a web page is 33 seconds, meaning the benchmarks will likely not complete, leading to misclassification. Moreover, the reported detection rate for more specific attributes, such as browser version, operating system, and architecture, is significantly less accurate.

Mowery and Shacham later proposed using text rendering and WebGL scenes to a `<canvas>` element as another way of fingerprinting browsers [39]. Different browsers display text and graphics differently, which, though small, can be used to differentiate and track users between page loads. While this method is faster than executing browser benchmarks, these technologies are only available in the latest versions of modern browsers, making them ineffective for tracking users with older versions. In contrast, the fingerprinting techniques introduced in Section IV can differentiate browsers and their versions for any past version.

Olejnik et al. [40] show that web history can also be used for fingerprinting without additional client-side state. They made this observation by analyzing data from when the CSS-visited history bug was still present in browsers. Today, all modern browsers have corrected this issue, making the extraction of a user's history less straightforward, especially without user interaction [41]. Olejnik et al. claim that large script providers like Google can use their near-ubiquitous presence to extract a user's history. While this is true [42], most users have first-party relationships with Google, meaning they can be tracked accurately without resorting to history-based fingerprinting.

### Conclusion

In this paper, we first investigated the real-life implementations of fingerprinting libraries as deployed by three popular commercial companies. We focused on their differences compared to Panopticlick and discovered increased use of Flash, backup solutions for when Flash is absent, broad use of Internet Explorer's special features, and the existence of intrusive system-fingerprinting plugins.

Second, we created our own fingerprinting script using multiple novel features, focusing on the differences between special objects like `navigator` and `screen` as implemented and handled by different browsers. We identified that each browser deviated from the rest in a consistent and measurable way, allowing scripts to almost instantaneously discover the true nature of a browser, regardless of its attempts to hide it. We also analyzed eleven popular user-agent spoofing extensions and showed that, even without our newly proposed fingerprinting techniques, all of them fall short of properly hiding a browser's identity.

The purpose of our research was to demonstrate that, when considering device identification through fingerprinting, user privacy is currently on the losing side. Given the complexity of fully hiding the true nature of a browser, we believe this can be efficiently done only by the browser vendors. Regardless of their complexity and sophistication, browser plugins and extensions will never be able to control everything that a browser vendor can. At the same time, it is unclear whether browser vendors would desire to hide the nature of their browsers, so the discussion of web-based device fingerprinting, its implications, and possible countermeasures must start at a policy-making level, similar to how stateful user tracking is currently discussed.

### Acknowledgments

We want to thank our shepherd and the anonymous reviewers for their valuable comments. For KU Leuven, this research was performed with the financial support of the Prevention against Crime Programme of the European Union (B-CCENTRE), the Research Fund KU Leuven, the EU FP7 projects NESSoS and WebSand, as well as the IWT project SPION. For UCSB, this work was supported by the Office of Naval Research (ONR) under grant N000140911042, and by the National Science Foundation (NSF) under grants CNS-0845559 and CNS-0905537, and in part by Secure Business Austria.

### References

[1] The New York Times - John Schwartz, “Giving the Web a Memory Cost Its Users Privacy,” http://www.nytimes.com/2001/09/04/technology/04COOK.html.
[2] B. Krishnamurthy, “Privacy leakage on the Internet,” presented at IETF 77, March 2010.
[3] B. Krishnamurthy and C. E. Wills, “Generating a privacy footprint on the Internet,” in Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement, ser. IMC ’06, New York, NY, USA, 2006, pp. 65–70.
[4] F. Roesner, T. Kohno, and D. Wetherall, “Detecting and defending against third-party tracking on the web,” in NSDI’12: Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. Berkeley, CA, USA: USENIX Association, 2012, pp. 12–12.
[5] The Wall Street Journal, “What They Know,” http://blogs.wsj.com/wtk/.
[6] J. Turow, J. King, C. J. Hoofnagle, A. Bleakley, and M. Hennessy, “Americans Reject Tailored Advertising and Three Activities that Enable It,” 2009.
[7] B. Ur, P. G. Leon, L. F. Cranor, R. Shay, and Y. Wang, “Smart, useful, scary, creepy: perceptions of online behavioral advertising,” in Proceedings of the Eighth Symposium on Usable Privacy and Security, ser. SOUPS ’12. New York, NY, USA: ACM, 2012, pp. 4:1–4:15.
[8] comScore, “The Impact of Cookie Deletion on Site-Server and Ad-Server Metrics in Australia,” January 2011.
[9] “Ghostery,” http://www.ghostery.com.
[10] “Collusion: Discover who’s tracking you online,” http://www.mozilla.org/en-US/collusion/.
[11] J. R. Mayer, “Any person... a pamphleteer,” Senior Thesis, Stanford University, 2009.
[12] P. Eckersley, “How Unique Is Your Browser?” in Proceedings of the 10th Privacy Enhancing Technologies Symposium (PETS), 2010.
[13] K. Mowery, D. Bogenreif, S. Yilek, and H. Shacham, “Fingerprinting information in JavaScript implementations,” in Proceedings of W2SP 2011, H. Wang, Ed. IEEE Computer Society, May 2011.
[14] C. Kolbitsch, B. Livshits, B. Zorn, and C. Seifert, “Rozzle: De-cloaking internet malware,” in IEEE Symposium on Security and Privacy, May 2012.
[15] E. Mills, “Device identification in online banking is privacy threat, expert says,” CNET News (April 2009).
[16] “Opt out of being tracked,” http://www.bluecava.com/preferences/.
[17] J. R. Mayer, “Tracking the Trackers: Early Results — Center for Internet and Society,” http://cyberlaw.stanford.edu/node/6694.
[18] T.-F. Yen, Y. Xie, F. Yu, R. P. Yu, and M. Abadi, “Host Fingerprinting and Tracking on the Web: Privacy and Security Implications,” in Proceedings of the 19th Annual Network and Distributed System Security Symposium (NDSS), 2012.
[19] J. R. Mayer and J. C. Mitchell, “Third-party web tracking: Policy and technology,” in IEEE Symposium on Security and Privacy, 2012, pp. 413–427.
[20] G. Cluley, “How to turn off Java on your browser - and why you should do it now,” http://nakedsecurity.sophos.com/2012/08/30/how-turn-off-java-browser/.
[21] B. Krebs, “How to Unplug Java from the Browser,” http://krebsonsecurity.com/how-to-unplug-java-from-the-browser.
[22] D. Jang, R. Jhala, S. Lerner, and H. Shacham, “An empirical study of privacy-violating information flows in JavaScript Web applications,” in Proceedings of CCS 2010, Oct. 2010.
[23] “Torbutton: I can’t view videos on YouTube and other flash-based sites. Why?” https://www.torproject.org/torbutton/torbutton-faq.html.en#noflash.
[24] “Anubis: Analyzing Unknown Binaries,” http://anubis.iseclab.org/.
[25] “VirusTotal - Free Online Virus, Malware and URL Scanner,” https://www.virustotal.com/.
[26] G. Pierson and J. DeHaan, “Patent US20080040802 - NETWORK SECURITY AND FRAUD DETECTION SYSTEM AND METHOD.”
[27] M. Cova, C. Kruegel, and G. Vigna, “Detection and analysis of drive-by-download attacks and malicious JavaScript code,” in Proceedings of the 19th International Conference on World Wide Web (WWW), 2010, pp. 281–290.
[28] “ECMAScript Language Specification, Standard ECMA-262, Third edition.”
[29] M. Zalewski, The Tangled Web: A Guide to Securing Modern Web Applications. No Starch Press, 2011.
[30] A. Andersen, “History of the browser user-agent string,” http://webaim.org/blog/user-agent-string-history.
[31] “Web Tracking Protection,” http://www.w3.org/Submission/2011/SUBM-web-tracking-protection-20110224/.
[32] P. Eckersley, “Panopticlick — Self-Defense,” https://panopticlick.eff.org/self-defense.php.
[33] J. Scott, “How many Firefox users have add-ons installed? 85%!” https://blog.mozilla.org/addons/2011/06/21/firefox-4-add-on-users/.
[34] “Adblock plus - for annoyance-free web surfing,” http://adblockplus.org.
[35] A. Klein, “How Fraudsters are Disguising PCs to Fool Device Fingerprinting,” http://www.trusteer.com/blog/how-fraudsters-are-disguising-pcs-fool-device-fingerprinting.
[36] A. Soltani, S. Canty, Q. Mayo, L. Thomas, and C. J. Hoofnagle, “Flash Cookies and Privacy,” in SSRN preprint (August 2009).
[37] J. Xu and T. Nguyen, “Private browsing and Flash Player 10.1,” http://www.adobe.com/devnet/flashplayer/articles/privacy_mode_fp10_1.html.
[38] J.-L. Gassée and F. Filloux, “Measuring Time Spent On A Web Page,” http://www.cbsnews.com/2100-215162-5037448.html.
[39] K. Mowery and H. Shacham, “Pixel perfect: Fingerprinting canvas in HTML5,” in Proceedings of W2SP 2012, M. Fredrikson, Ed. IEEE Computer Society, May 2012.
[40] Ł. Olejnik, C. Castelluccia, and A. Janc, “Why Johnny Can’t Browse in Peace: On the Uniqueness of Web Browsing History Patterns,” in the 5th workshop on Hot Topics in Privacy Enhancing Technologies (HOTPETS 2012).
[41] Z. Weinberg, E. Y. Chen, P. R. Jayaraman, and C. Jackson, “I still know what you visited last summer: Leaking browsing history via user interaction and side channel attacks,” in Proceedings of the 2011 IEEE Symposium on Security and Privacy, ser. SP ’11, 2011, pp. 147–161.
[42] N. Nikiforakis, L. Invernizzi, A. Kapravelos, S. V. Acker, W. Joosen, C. Kruegel, F. Piessens, and G. Vigna, “You Are What You Include: Large-scale Evaluation of Remote JavaScript Inclusions,” in Proceedings of the ACM Conference on Computer and Communications Security (CCS), 2012.