### Supervised Method for Anomaly Detection Across 15 Endpoints

The accuracy of the supervised method for all 15 endpoints across 5 different scenarios is presented in Table II. The average accuracies are as follows: 85.5%, 95.3%, 94.6%, 97.9%, and 97.0%.

#### Accuracy Calculation
Accuracy is computed using the following formula:
\[
\text{Accuracy} = \frac{T_a}{T_i}
\]
where \( T_i \) is the number of injected anomaly events and \( T_a \) is the number of accurately detected anomalies.

#### Scenarios and Anomalies
The number of injected anomalies varies depending on each scenario and endpoint. Missing values in Table II indicate that the anomaly did not affect those endpoints. The model is trained on each of the three endpoints, and the detection accuracy is computed for each pattern injected into the time series.

#### Normal System Scenario
In the normal system scenario, the accuracy exceeds 99% due to the tolerance module. Figures 7a and 7b visually demonstrate the performance in scenarios 5 and 6, showing that the method successfully flags almost all anomalous events.

#### Robustness Testing
To evaluate the robustness of the algorithm, we tested it with various augmentations of the original patterns, including:
- Translation
- Increasing response time (e.g., \( RTi = 0.2 \) means setting the response time of an event to 20% of the maximum value)
- Changing the size of the anomaly (e.g., in a gradual increase, \( size = 10 \) means the amplitude increases over 10 data points)

Table III summarizes the aggregated results, indicating the minimum values for parameters \( size \) and \( RTi \) required for detecting different types of anomalies.

### Results: Production Cloud Data
Due to the low number of production-system errors, we injected several types of anomalies. We defined seven common anomaly types:
- Additive outlier
- Normal mean shift
- Temporary change
- Gradual change
- Mean shift
- Step and decrement
- Incremental change

Figure 8 illustrates examples of successfully detected anomalies, such as gradual and mean shift anomalies, in the {host}/vl/{p_id}/cs/limits series.

### Performance Evaluation
#### Training and Prediction
The performance of the model in training and prediction is crucial, especially in real-world production systems where large amounts of traces and events are generated in a short period. We evaluated the performance and present the results in Tables IV and V.

- **Training Performance**:
  - 60,000 windows: 0.29 ms/window
  - 27,000 windows: 0.28 ms/window
  - 9,000 windows: 0.53 ms/window
  - 1,000 windows: 1.25 ms/window

- **Prediction Performance**:
  - 1,500 windows: 0.22 ms/window
  - 1,000 windows: 0.28 ms/window
  - 500 windows: 0.53 ms/window
  - 100 windows: 1.25 ms/window

#### Faulty Pattern Classification
The module can be evaluated separately from the rest of the solution. The dataset consists of 15 different types of patterns, similar to those shown in Figure 6. Users have the option to define their own patterns. We augmented the dataset with horizontal shifts, small noise additions, and amplitude shifts.

- **Noise and Classification Accuracy**:
  - No additional noise: 100% accuracy
  - Gaussian noise (\( \sigma = 0.05 \)): 80% accuracy
  - Gaussian noise (\( \sigma = 0.1 \)): 48% accuracy

### Conclusion and Future Work
This paper addresses the growing challenge of automating operation and maintenance tasks in large-scale IT infrastructures. We experimentally demonstrate the advantages of combining GRUs (simplified LSTMs) with variational autoencoders (AEVB) for learning complex data distributions underlying time series data generated by distributed tracing systems. Our approach achieves high accuracy (>90%), fast prediction times (<10ms), and robust classification of detected anomalies.

Future work will extend the approach to consider the structure of distributed traces, cross-event and cross-trace relations, and structural trace anomalies using complete trace information. These advancements can support the development of zero-touch AIOps solutions for automated detection, root-cause analysis, and remediation of IT infrastructures.

### References
[1] F. Schmidt, A. Gulenko, M. Wallschlager, A. Acker, V. Hennig, F. Liu, and O. Kao, "TFTM - Unsupervised Anomaly Detection for Virtualized Network Function Services," in 2018 IEEE International Conference on Web Services (ICWS), July 2018, pp. 187-194.
[2] A. Gulenko, F. Schmidt, A. Acker, M. Wallschlager, O. Kao, and F. Liu, "Detecting Anomalous Behavior of Black-Box Services Modeled with Distance-Based Online Clustering," in 2018 IEEE 11th International Conference on Cloud Computing (CLOUD), vol. 00, Jul 2018, pp. 912-915.
[3] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson, M. Plakal, D. Beaver, S. Jaspan, and C. Shanbhag, "Dapper, a Large-Scale Distributed Systems Tracing Infrastructure," Google, Inc., Tech. Rep., 2010.
[...]
[39] OpenZipkin, "openzipkin/zipkin," 2018. [Online]. Available: https://github.com/openzipkin/zipkin