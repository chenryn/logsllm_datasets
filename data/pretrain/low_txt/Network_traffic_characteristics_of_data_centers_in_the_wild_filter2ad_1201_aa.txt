# Network Traffic Characteristics of Data Centers in the Wild

**Authors:**
- Theophilus Benson
- Aditya Akella
- David A. Maltz

**Affiliations:**
- Theophilus Benson and Aditya Akella: University of Wisconsin–Madison
- David A. Maltz: Microsoft Research–Redmond

## Abstract

Despite significant interest in designing improved networks for data centers, there is limited knowledge about the network-level traffic characteristics of current data centers. In this paper, we conduct an empirical study of network traffic in 10 data centers across three types of organizations: university, enterprise, and cloud. Our definition of cloud data centers includes those used by large online service providers for Internet-facing applications and those hosting data-intensive (MapReduce style) applications. We collect and analyze SNMP statistics, topology, and packet-level traces to examine the range of applications deployed, their placement, and their impact on network utilization, link utilization, congestion, and packet drops. We discuss the implications of these traffic patterns for internal traffic engineering and recently proposed architectures for data center networks.

## Categories and Subject Descriptors

C.4 [Performance of Systems]: Design studies; Performance attributes

## General Terms

Design, Measurement, Performance

## Keywords

Data center traffic, characterization

## 1. Introduction

A data center (DC) is a large, dedicated cluster of computers owned and operated by a single organization. Data centers of various sizes are being built and employed for diverse purposes. Universities and private enterprises are consolidating IT services within on-site data centers containing a few hundred to a few thousand servers. Large online service providers like Google, Microsoft, and Amazon are building geographically diverse cloud data centers, often with more than 10,000 servers, to offer cloud-based services such as email, web servers, storage, search, gaming, and instant messaging. These providers also use some data centers for large-scale data-intensive tasks, such as indexing web pages or analyzing large datasets, often using variations of the MapReduce paradigm.

Despite the growing applicability of data centers, there are few systematic measurement studies to guide practical issues in data center operations. Little is known about the key differences between different classes of data centers, specifically university campus data centers, private enterprise data centers, and cloud data centers (both for customer-facing applications and large-scale data-intensive tasks).

While several aspects of data centers still need substantial empirical analysis, our focus is on issues related to the operation of data center networks. We examine the sending/receiving patterns of applications running in data centers and the resulting link-level and network-level performance. A better understanding of these issues can lead to advancements in traffic engineering mechanisms, quality-of-service, and resource management, including energy consumption.

Unfortunately, recent empirical studies of data center networks are limited in scope, making their observations difficult to generalize and apply in practice.

In this paper, we study data from ten data centers to shed light on their network design and usage and to identify properties that can help improve the operation of their networking substrate. The data centers include three university campus data centers, two private enterprise data centers, and five cloud data centers, three of which run a variety of Internet-facing applications while the remaining two predominantly run MapReduce workloads. Some of the data centers have been in operation for over 10 years, while others were commissioned more recently. Our data includes SNMP link statistics for all data centers, fine-grained packet traces from select switches in four of the data centers, and detailed topology for five data centers. By studying different classes of data centers, we aim to understand how similar or different they are in terms of network usage and whether results from one class can be applied to others.

We perform a top-down analysis, starting with the applications run in each data center and then examining their send and receive patterns and network-level impact. Using packet traces, we first examine the types of applications and their relative contribution to network traffic. We then examine the fine-grained sending patterns at the packet and flow levels, both in aggregate and per-application. Finally, we use SNMP traces to examine network-level impact in terms of link utilization, congestion, and packet drops, and the dependence of these properties on the location of links in the network topology and the time of day.

Our key empirical findings are:
- A wide variety of applications across the data centers, ranging from customer-facing applications to data-intensive applications.
- Most flows in the data centers are small (≤ 10KB), with a significant fraction lasting under a few hundred milliseconds.
- Traffic originating from a rack in a data center is ON/OFF in nature, fitting heavy-tailed distributions.
- In cloud data centers, 80% of traffic stays within the rack, while in university and private enterprise data centers, 40-90% of traffic leaves the rack.
- Link utilizations are low in all layers but the core, where a subset of core links experiences high utilization.
- Losses occur at links with low average utilization, implying momentary spikes as the primary cause.
- Link utilizations vary by time of day and day of week, with variations more pronounced in cloud data centers.

To highlight the implications of our observations, we conclude with an analysis of two data center network design issues: network bisection bandwidth and centralized management techniques.

- **Bisection Bandwidth:** Recent proposals argue for high bisection bandwidth to support demanding applications. Our measurements show that only a fraction of existing bisection capacity is utilized, even in the worst case. Load balancing mechanisms can help manage occasional congestion.
- **Centralized Management:** Proposals for centrally managing and scheduling network-wide transmissions must employ parallelism and fast route computation heuristics to scale to the size of data centers today.

The rest of the paper is structured as follows: Section 2 presents related work, Section 3 describes the data centers studied, their high-level design, and typical uses. Section 4 describes the applications running in these data centers. Section 5 examines the microscopic properties of the data centers. Section 6 examines the flow of traffic and link utilization. Section 7 discusses the implications of our empirical insights, and Section 8 summarizes our findings.

## 2. Related Work

There is significant interest in designing improved networks for data centers, but such work and its evaluation are driven by a few studies of data center traffic, primarily of huge (> 10K server) data centers running data mining, MapReduce jobs, or Web services. Table 1 summarizes prior studies, showing that many data architectures are evaluated without empirical data from data centers. For architectures evaluated with empirical data, evaluations are performed with traces from cloud data centers, implying that the actual performance of these techniques in other types of data centers is unknown.

This paper analyzes the network traffic of the broadest set of data centers studied to date, including data centers running Web services and MapReduce applications, as well as common enterprise and campus data centers. Our work provides the information needed to evaluate data center network architecture proposals under a broad range of data center environments.

Previous studies have focused on traffic patterns at coarse time-scales, reporting flow size distributions, number of concurrent connections, duration of congestion periods, and diurnal patterns. We extend these measures by considering additional issues, such as the applications employed, their transmission patterns, and their impact on link and network utilizations.

The closest prior works are [19] and [3]. The former focuses on a single MapReduce data center, while the latter considers cloud data centers. Neither study considers non-cloud data centers, and neither provides as complete a picture of traffic patterns as this study. Our work complements these by examining variations in link utilizations over time, the localization of losses, and the magnitude of losses over time. We also quantify the exact fraction of traffic that stays within a rack and the number of hot-spots, showing that losses are due to underlying burstiness of traffic.

Our work complements prior work on measuring Internet traffic by presenting an equivalent study on the flow characteristics of applications and link utilizations within data centers. We find that data center traffic is statistically different from wide area traffic, with serious implications for the design and implementation of techniques for data center networks.

## 3. Datasets and Overview of Data Centers

In this paper, we analyze datasets from 10 data centers, including 5 commercial cloud data centers, 2 private enterprise data centers, and 3 university campus data centers. For each data center, we examine one or more of the following datasets: network topology, packet traces from select switches, and SNMP polls from the interfaces of network switches. Table 2 summarizes the data collected from each data center and key properties.

Table 2 shows that the data centers vary in size, both in terms of the number of devices and the number of servers. The largest data centers are used for commercial computing needs, with enterprise and university data centers being an order of magnitude smaller.

The data centers also vary in their proximity to users. Enterprise and university data centers are located in the western/mid-western U.S. and serve local users, while commercial data centers are distributed globally to reduce latency, ensure geo-redundancy, and comply with regulatory constraints.

### 3.1 Data Collection

**SNMP Polls:** For all data centers, we polled the switches' SNMP MIBs for bytes-in and bytes-out at granularities ranging from 1 minute to 30 minutes. For the 5 commercial cloud data centers and the 2 private enterprises, we also polled for the number of packet discards. We collected SNMP data for at least 10 days, with some data spanning multiple weeks, allowing us to observe time-of-day and day-of-week dependencies in network traffic.

**Network Topology:** For private enterprises and university data centers, we obtained topology via the Cisco CDP protocol, which provides both the network topology and link capacities. For the 5 cloud data centers, we analyzed device configurations to derive properties of the topology.

**Packet Traces:** We collected packet traces from a few private enterprise and university data centers. Our packet trace collection spans 12 hours over multiple days. Since it is difficult to instrument an entire data center, we selected a handful of locations at random per data center and installed sniffers. Table 3 presents the number of sniffers per data center. In smaller data centers, we installed 1 sniffer, while in larger data centers, we installed up to 4 sniffers.