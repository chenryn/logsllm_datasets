To enhance the clarity, coherence, and professionalism of the provided text, I have restructured and refined it. Here is the optimized version:

---

**Optimizing Command Dependencies for Enhanced Performance**

In order to deliver commands swiftly, systems often share metadata among local threads, which can introduce contention and lead to suboptimal CPU utilization (as evidenced in Figure 4). The overhead associated with maintaining dependency relations becomes more pronounced when commands are transmitted over a network, as dependencies must be included in the messages themselves. This results in larger message sizes, increasing the time required for transmission.

**Evaluating Scalability with Increased CPU Capacity**

We further evaluated the scalability of consensus protocols by holding the number of nodes constant and incrementally increasing the CPU capacity of each node from 4 to 32 cores. This is particularly relevant for Generalized Consensus implementations, such as EPaxos, to assess their ability to leverage parallelism in scenarios with low or no conflicts among commands. For this purpose, we conducted our benchmark on four classes of Amazon EC2 machines, each class representing a doubling of CPU cores and an almost 2× increase in available RAM.

**Experimental Results and Analysis**

Figure 4 presents the results of this experiment across four deployments, each consisting of 11 nodes. M2PAXOS demonstrates excellent scalability up to 16 cores, with throughput continuing to increase, albeit at a slower rate, beyond that point. This is due to other system components, particularly the networking layer, becoming bottlenecks. Single-leader algorithms do not benefit from this scalability. EPaxos also fails to take advantage of additional local resources due to the costs associated with dependency management and graph processing, both of which require synchronization among local threads. In contrast, M2PAXOS does not generate thread contention, allowing it to effectively utilize additional CPUs to perform more parallel tasks per time unit.

**Latency vs. Throughput Plots**

Figure 5 illustrates the latency versus throughput plots for various deployments (5, 11, and 49 nodes) under two extreme workloads: one with perfect locality (100% local commands) and another with no locality (0% local commands). Multi-Paxos and Generalized Paxos are insensitive to command locality, while M2PAXOS handles non-local commands by forwarding them to the node that currently owns the requested object (see Section IV-B). EPaxos, however, can experience delays in delivering transactions due to the collection of conflicting dependencies during its broadcast phase, leading to a breakdown up to 10% earlier in the workload with no locality.

**Impact of Non-Local Commands**

Figure 6 shows the performance impact of varying the probability of proposing non-local commands in configurations with 3 and 11 nodes. The forwarding mechanism in M2PAXOS results in a very small performance degradation (on average 4%), whereas other protocols reach their peak performance, indicating that changes in the probability of issuing local commands do not significantly affect their performance.

**Complex Commands and Their Impact**

The final scenario involves complex commands that access multiple objects, potentially conflicting with commands from multiple nodes. In this experiment, a complex command accesses one object in a local set, where the local node likely has ownership, and one uniformly distributed across all objects. With 49 nodes, we varied the size of the local set. The results in Figure 7 show a drop in throughput as the fraction of complex commands increases, with the drop rate and final throughput dependent on the local set size. Multi-Paxos and Generalized Paxos are unaffected by complex commands, while EPaxos exhibits a small reduction in throughput as the percentage of complex commands approaches 100%. M2PAXOS, however, can sustain throughput even with nearly 50% complex commands, provided the local set size is 1000.

**Ownership and Finalization Mechanisms**

It is important to note that when a node \( p_i \) acquires ownership of an object \( l \) to propose commands accessing \( l \), it first finalizes the decision of all pending and undecided commands accessing \( l \) (see line 10 of Algorithm 4). This mechanism incurs a cost, reflected in the results shown in Figure 7, as undecided commands are often present during ownership changes due to concurrent operations. Specifically, a node \( p_j \) may not have enough time to finalize a command on \( l \) before \( p_i \) acquires ownership, similar to a crash scenario. Thus, the results in Figure 7 also account for the costs incurred in case of crashes.

**TPC-C Benchmark Evaluation**

In addition to the synthetic benchmark, we included a TPC-C benchmark to evaluate the performance under realistic workloads. We configured the benchmark by deploying a total number of warehouses equal to 10 times the number of nodes. TPC-C has five transaction profiles, each with a set of indexes identifying the objects to access. We defined a warehouse as local to a node if its warehouse object and all related objects belong to the local set of that node.

Figure 8 shows the performance by varying the likelihood of a thread broadcasting a command on a local warehouse (Figure 8(a)) versus a uniformly selected warehouse (Figure 8(b)). According to the TPC-C specification, even if the requested warehouse is local, 15% of payment transactions can still access a customer of another warehouse.

M2PAXOS's throughput is lower than in single-object command cases due to the larger command sizes in TPC-C. Performance decreases further (by up to 40%) when 15% of the commands access a non-local warehouse. However, M2PAXOS still achieves a throughput greater than 400k commands per second in the configuration of Figure 8(a) and more than 250k in the configuration of Figure 8(b).

**Comparison with Competitors**

The closest competitor to M2PAXOS is Multi-Paxos, which is 2.4× slower. EPaxos, which is 5.5× slower, struggles with higher contention, leading to additional ordering phases after failed fast delivery attempts. Multi-Paxos's performance is independent of message composition and application contention, as it produces a total order without considering message conflicts, resulting in performance similar to Figure 6(a).

**Conclusion**

In this paper, we presented M2PAXOS, a scalable and high-performance implementation of Generalized Consensus. It decides sequences of commands with the optimal cost of two communication delays for partitionable workloads and with the minimum quorum size achievable for solving consensus in asynchronous systems. Our evaluation study confirms the effectiveness of our approach.

**Acknowledgments**

The authors thank Pierre Sutra and the anonymous reviewers for their valuable comments. This work is supported in part by the US National Science Foundation under grant CNS-1523558 and by the US Air Force Office of Scientific Research under grant FA9550-15-1-0098.

**References**

[1] L. Lamport, “The Part-time Parliament,” ACM Trans. Comput. Syst., vol. 16, no. 2, pp. 133–169, 1998.
[2] B. Charron-Bost and A. Schiper, “Uniform Consensus is Harder Than Consensus,” J. Algorithms, vol. 51, no. 1, pp. 15–37, 2004.
[3] J. C. Corbett et al., “Spanner: Google’s Globally Distributed Database,” ACM Trans. Comput. Syst., vol. 31, no. 3, pp. 8:1–8:22, 2013.
[4] S. Hirve, R. Palmieri, and B. Ravindran, “Archie: A Speculative Replicated Transactional System,” in Middleware, 2014, pp. 265–276.
[5] T. Kraska, G. Pang, M. J. Franklin, S. Madden, and A. Fekete, “MDCC: Multi-data Center Consistency,” in EuroSys, 2013, pp. 113–126.
[6] H. Mahmoud, F. Nawab, A. Pucher, D. Agrawal, and A. El Abbadi, “Low-latency Multi-datacenter Databases Using Replicated Commit,” Proc. VLDB Endow., vol. 6, no. 9, pp. 661–672, 2013.
[7] L. Lamport, “Paxos made simple,” ACM Sigact News, 2001.
[8] I. Moraru, D. G. Andersen, and M. Kaminsky, “There is More Consensus in Egalitarian Parliaments,” in SOSP, 2013, pp. 358–372.
[9] Y. Mao, F. P. Junqueira, and K. Marzullo, “Mencius: Building Efficient Replicated State Machines for WANs,” in OSDI 2008, 2008, pp. 369–384.
[10] A. Turcu, S. Peluso, R. Palmieri, and B. Ravindran, “Be General and Don’t Give Up Consistency in Geo-Replicated Transactional Systems,” in OPODIS, 2014, pp. 33–48.
[11] L. Lamport, “Generalized Consensus and Paxos,” Microsoft Research, Tech. Rep. MSR-TR-2005-33, March 2005.
[12] F. Pedone and A. Schiper, “Generic Broadcast,” in DISC, 1999, pp. 94–108.
[13] L. Lamport, “Fast paxos,” Distributed Computing, vol. 19, no. 2, pp. 79–103, 2006.
[14] ——, “Future directions in distributed computing.” Springer-Verlag, 2003, ch. Lower Bounds for Asynchronous Consensus.
[15] J. Cowling and B. Liskov, “Granola: Low-overhead Distributed Transaction Coordination,” in USENIX ATC, 2012.
[16] D. Sciascia, F. Pedone, and F. Junqueira, “Scalable Deferred Update Replication,” in DSN, 2012, pp. 1–12.
[17] S. Peluso, P. Romano, and F. Quaglia, “SCORe: A Scalable One-Copy Serializable Partial Replication Protocol,” in Middleware, 2012, pp. 456–475.
[18] S. Peluso, P. Ruivo, P. Romano, F. Quaglia, and L. Rodrigues, “When Scalability Meets Consistency: Genuine Multiversion Update-Serializable Partial Data Replication,” in ICDCS, 2012, pp. 455–465.
[19] S. Peluso, A. Turcu, R. Palmieri, and B. Ravindran, “On Exploiting Locality for Generalized Consensus,” in ICDCS, 2015, pp. 766–767.
[20] “Tpc-c benchmark,” http://www.tpc.org/tpcc/.
[21] L. Lamport, Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers. Addison-Wesley Longman Publishing Co., Inc., 2002.
[22] S. Peluso, A. Turcu, R. Palmieri, G. Losa, and B. Ravindran, “Making Fast Consensus Generally Faster,” Virginia Tech, Tech. Rep., 2016. [Online]. Available: http://www.hyflow.org/pubs/peluso-M2PAXOS-TR.pdf
[23] P. Sutra and M. Shapiro, “Fast Genuine Generalized Consensus,” in SRDS, 2011, pp. 255–264.
[24] D. Hendler, A. Naiman, S. Peluso, F. Quaglia, P. Romano, and A. Suissa, “Exploiting Locality in Lease-Based Replicated Transactional Memory via Task Migration,” in DISC, 2013, pp. 121–133.
[25] R. Boichat, P. Dutta, and R. Guerraoui, “Asynchronous Leasing,” in WORDS, 2002, pp. 180–187.
[26] C. Li, D. Porto, A. Clement, J. Gehrke, N. Preguiça, and R. Rodrigues, “Making Geo-replicated Systems Fast As Possible, Consistent when Necessary,” in OSDI, 2012, pp. 265–278.
[27] M. J. Fischer, N. A. Lynch, and M. S. Paterson, “Impossibility of Distributed Consensus with One Faulty Process,” J. ACM, vol. 32, no. 2, pp. 374–382, 1985.
[28] R. Guerraoui and A. Schiper, “Genuine Atomic Multicast in Asynchronous Distributed Systems,” Theor. Comput. Sci., vol. 254, no. 1-2, pp. 297–316, 2001.
[29] R. Guerraoui and L. Rodrigues, Introduction to Reliable Distributed Programming. Springer-Verlag New York, Inc., 2006.
[30] F. Junqueira, Y. Mao, and K. Marzullo, “Classic Paxos vs. Fast Paxos: Caveat Emptor,” in HotDep, 2007.
[31] R. Guerraoui, V. Kuncak, and G. Losa, “Speculative Linearizability,” in PLDI, 2012, pp. 55–66.
[32] “The go programming language.” http://golang.org/.
[33] T. Friedman and R. V. Renesse, “Packing Messages As a Tool for Boosting the Performance of Total Ordering Protocols,” in HPDC, 1997, pp. 233–242.
[34] B. Kemme, F. Pedone, G. Alonso, A. Schiper, and M. Wiesmann, “Using optimistic atomic broadcast in transaction processing systems,” IEEE Trans. Knowl. Data Eng., vol. 15, no. 4, pp. 1018–1032, 2003.

---

This version aims to provide a clear, coherent, and professional presentation of the research and findings.