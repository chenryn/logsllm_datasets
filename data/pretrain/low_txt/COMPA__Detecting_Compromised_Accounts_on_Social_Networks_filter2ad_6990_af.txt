### Detecting Scams and Malicious Campaigns

An example of a scam message might read, "Obama is giving FREE Gas Cards Worth $250! Call now -> 1-888-858-5783 (US Only)@@@." In our evaluation, 0.3% of the generated groups did not include URLs. Existing techniques, such as [13], which focus solely on URLs and their reputation, would fail to detect such campaigns.

### Detecting Worms in Online Social Networks

Online social networks have repeatedly faced XSS worm outbreaks that rapidly infect thousands of accounts. Since the behavior of affected accounts diverges from their usual behavioral profiles, we demonstrate in Appendix A that COMPA successfully detects such outbreaks.

### Limitations

An attacker aware of COMPA has several strategies to prevent compromised accounts from being detected:

1. **Behavioral Alignment**: The attacker can post messages that align with the behavioral profiles of the compromised accounts. As described in Section 3, this would require significant time and computational resources to gather the necessary profile information from victims. Additionally, social networks have mechanisms in place to prevent automated crawling, thereby slowing down data gathering efforts.
   
2. **Evasion of Similarity Measures**: An attacker could send messages that evade our similarity measures. Although these messages might violate the compromised accounts' behavioral profiles, they would not be grouped together. To counter such evasion attempts, COMPA can be extended with additional and more comprehensive similarity measures. For example, a similarity measure using the landing page instead of URLs in messages can be implemented. Other computationally expensive measures, such as text shingling or edit distances for text similarity, can also be used. Additional similarity measures might leverage the way messages propagate along the social graph to evaluate message similarity.

### Related Work

The popularity of social networks has inspired numerous scientific studies in both networking and security. Early detection systems for malicious activity on social networks focused on identifying fake accounts and spam messages [5, 6, 7] by leveraging features geared towards recognizing characteristics of spam accounts (e.g., the presence of URLs in messages or message similarity in user posts). Cai et al. [18] proposed a system that detects fake profiles on social networks by examining densely interconnected groups of profiles. These techniques work reasonably well, and both Twitter and Facebook rely on similar heuristics to detect fake accounts [28, 29].

In response to defense efforts by social network providers, attackers have shifted their focus. A majority of accounts carrying out malicious activities were not created for this purpose but started as legitimate accounts that were compromised [3, 4]. Since these accounts do not show consistent behavior, previous systems will fail to recognize them as malicious. Grier et al. [4] studied the behavior of compromised accounts on Twitter by entering the credentials of an account they controlled on a phishing campaign site. This approach does not scale as it requires identifying and joining each new phishing campaign and is limited to phishing campaigns.

Gao et al. [10] developed a clustering approach to detect spam wall posts on Facebook. They attempted to determine whether an account that sent a spam post was compromised by looking at the wall post history of spam accounts. However, their classification is simple: if an account received a benign wall post from one of their connections, it was automatically considered legitimate but compromised. This technique has limitations, as previous work showed that spam victims occasionally send messages to spam accounts [7], leading to the misclassification of legitimate accounts as compromised. Moreover, the system needs to know whether an account has sent spam before classifying it as fake or compromised. Our system, on the other hand, detects compromised accounts even when they are not involved in spam campaigns.

As an improvement, Gao et al. [10] proposed a system that groups similar messages posted on social networks and makes a decision about the maliciousness of the messages based on features of the message cluster. Although this system can detect compromised and fake accounts, it focuses on detecting accounts that spread URLs through their messages, making it less generic than COMPA.

Thomas et al. [13] built Monarch to detect malicious messages on social networks based on URLs linking to malicious sites. By relying only on URLs, Monarch misses other types of malicious messages, such as scams based on phone numbers detected by COMPA. It also fails to detect XSS worms spreading without a URL and new, emerging spam that includes incomplete links in tweets (e.g., missing http://). These spam messages ask users to copy and paste a fragmented URL in the browser address bar, where the URL is automatically reassembled [30]. Lee et al. [12] proposed WARNINGBIRD, a system that detects spam links posted on Twitter by analyzing the characteristics of HTTP redirection chains leading to a final spam page.

Xu et al. [31] present a system that, by monitoring a small number of nodes, detects worms propagating on social networks. This paper does not directly address the problem of compromised accounts but could detect large-scale infections such as Koobface [2].

Yang et al. [17] studied new Twitter spammers that act stealthily to avoid detection. Their system uses advanced features such as the topology of the network surrounding the spammer but does not distinguish between compromised and spam accounts.

### Conclusions

In this paper, we presented a novel approach to detect compromised accounts in social networks. We developed statistical models to characterize the behavior of social network users and used anomaly detection techniques to identify sudden changes in their behavior. We developed COMPA, a prototype tool that implements this approach, and applied it to a large stream of messages. The results show that our approach reliably detects compromised accounts, even though we do not have full visibility of every message exchanged on Facebook and Twitter.

### Acknowledgements

This work was supported by the Office of Naval Research (ONR) under Grant N000140911042, the Army Research Office (ARO) under grant W911NF0910553, and the National Science Foundation (NSF) under grants CNS-0845559 and CNS-0905537.

### References

[1] Harris Interactive Public Relations Research, “A Study of Social Networks Scams,” 2008.
[2] J. Baltazar, J. Costoya, and R. Flores, “KOOBFACE: The Largest Web 2.0 Botnet Explained,” 2009.
[3] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Zhao, “Detecting and Characterizing Social Spam Campaigns,” in Internet Measurement Conference (IMC), 2010.
[4] C. Grier, K. Thomas, V. Paxson, and M. Zhang, “@spam: the underground on 140 characters or less,” in ACM Conference on Computer and Communications Security (CCS), 2010.
[5] F. Benvenuto, G. Magno, T. Rodrigues, and V. Almeida, “Detecting Spammers on Twitter,” in Conference on Email and Anti-Spam (CEAS), 2010.
[6] K. Lee, J. Caverlee, and S. Webb, “Uncovering social spammers: social honeypots + machine learning,” in International ACM SIGIR Conference on Research and Development in Information Retrieval, 2010.
[7] G. Stringhini, C. Kruegel, and G. Vigna, “Detecting Spammers on Social Networks,” in Annual Computer Security Applications Conference (ACSAC), 2010.
[8] B. Stone-Gross, M. Cova, L. Cavallaro, B. Gilbert, M. Szydlowski, R. Kemmerer, C. Kruegel, and G. Vigna, “Your Botnet is My Botnet: Analysis of a Botnet Takeover,” in ACM Conference on Computer and Communications Security (CCS), 2009.
[9] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda, “All Your Contacts Are Belong to Us: Automated Identity Theft Attacks on Social Networks,” in World Wide Web Conference (WWW), 2009.
[10] H. Gao, Y. Chen, K. Lee, D. Palsetia, and A. Choudhary, “Towards Online Spam Filtering in Social Networks,” in Symposium on Network and Distributed System Security (NDSS), 2012.
[11] “foursquare,” http://foursquare.com.
[12] S. Lee and J. Kim, “WarningBird: Detecting Suspicious URLs in Twitter Stream,” in Symposium on Network and Distributed System Security (NDSS), 2012.
[13] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song, “Design and Evaluation of a Real-Time URL Spam Filtering Service,” in IEEE Symposium on Security and Privacy, 2011.
[14] “OAuth community site,” http://oauth.net.
[15] W. B. Cavnar and J. M. Trenkle, “N-gram-based text categorization,” in Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, 1994, pp. 161–175.
[16] J. C. Platt, “Fast Training of Support Vector Machines Using Sequential Minimal Optimization,” in Advances in Kernel Methods - Support Vector Learning, 1998.
[17] C. Yang, R. Harkreader, and G. Gu, “Die Free or Live Hard? Empirical Evaluation and New Design for Fighting Evolving Twitter Spammers,” in Symposium on Recent Advances in Intrusion Detection (RAID), 2011.
[18] Z. Cai and C. Jermaine, “The Latent Community Model for Detecting Sybils in Social Networks,” in Symposium on Network and Distributed System Security (NDSS), 2012.
[19] J. Song, S. Lee, and J. Kim, “Spam Filtering in Twitter using Sender-Receiver Relationship,” in Symposium on Recent Advances in Intrusion Detection (RAID), 2011.
[20] “SURBL,” http://www.surbl.org.
[21] “Weka - data mining open source program,” http://www.cs.waikato.ac.nz/ml/weka/.
[22] “Spamhaus DBL,” http://www.spamhaus.org.
[23] “Google Safe Browsing,” http://code.google.com/apis/safebrowsing/.
[24] “PhishTank,” http://www.phishtank.com.
[25] “Wepawet,” http://wepawet.iseclab.org.
[26] “Exposure,” http://exposure.iseclab.org/.
[27] “Fox News’s hacked Twitter feed declares Obama dead,” http://www.guardian.co.uk/news/blog/2011/jul/04/fox-news-hacked-twitter-obama-dead, 2011.
[28] C. Ghiossi, “Explaining Facebook’s Spam Prevention Systems,” http://blog.facebook.com/blog.php?post=403200567130, 2010.
[29] Twitter, “The Twitter Rules,” http://support.twitter.com/entries/18311-the-twitter-rules, 2010.
[30] F-Secure, “The increasingly shapeshifting web,” http://www.f-secure.com/weblog/archives/00002143.html.
[31] W. Xu, F. Zhang, and S. Zhu, “Toward worm detection in online social networks,” in Annual Computer Security Applications Conference (ACSAC), 2010.
[32] “Nielsen,” http://blog.nielsen.com.

### Appendix A: Detecting Worms

Twitter has been affected by multiple worm outbreaks. For example, in September 2010, an XSS worm exploited a vulnerability in how Twitter parsed URLs in tweets. Specifically, if a URL contained an "@" symbol, Twitter would interpret everything following that character as JavaScript. Therefore, a user who hovered her mouse over a tweet containing a URL similar to `http://x.xx/@onmouseover="alert(1)"` would execute the JavaScript event handler in her browser. The real worm used JavaScript to self-propagate rather than the alert statement. Posting the tweet that contained the body of the worm happened without the user's consent, so such accounts must be considered compromised. Note that the URL preceding the "@" sign was irrelevant for the attack. Therefore, existing detection approaches that examine the maliciousness of URLs would fail to detect this XSS worm attack, as the attacker could choose any benign domain (e.g., `http://www.google.com`).

To evaluate whether COMPA can detect worm outbreaks, we simulated the worm outbreak on real Twitter data. We chose a random message \( S_0 \) of a random user \( U_0 \) on the Twitter network. We assumed the worm would propagate from user A to user B if user B follows user A and user B was active on Twitter within a time window \( T \) around the point in time when user A posts the offending message. Due to the lack of detailed usage information, we determined the activity of a user by observing when they tweet. Thus, a user is deemed active \( T/2 \) before and after she posted any status updates through the Twitter web interface. This definition of activity (i.e., a user is only deemed active when she is posting) is conservative, as users often browse Twitter or read other people’s tweets, even if they do not post at the same time. Furthermore, the worm only propagates itself if the tweet that user B sent was posted through the Twitter website. Alternative clients are assumed not to contain the same vulnerability in their URL parsing routines. The XSS worm we are simulating is aggressive, spreading as soon as the user hovers the mouse over the tweet. We assume that if a user is active, she will hover her mouse over the tweet and thus get infected. For every propagation step, we record the IDs of users A and B, as well as the ID of the tweet that was used to determine that user B is active (i.e., the tweet user B sent within the time window \( T \)). According to [32], web users spend roughly 10 minutes per day on social networks. Thus, we assumed a value of 10 minutes for \( T \) in our simulation.

Subsequently, we downloaded the timelines of the users infected by the simulated worm. Then, we substituted the tweets responsible for the worm propagation with a copy of the XSS worm. Finally, we ran COMPA on these timelines. Although the way we simulated the worm outbreak means that the timing and source models are drawn from real information (i.e., we only substituted the text of the tweet), COMPA was able to successfully detect the outbreak and the compromised accounts after the worm spread to 2,256 accounts in 20 minutes. This means that the "worm group" contained enough tweets that violated their respective users' behavioral profiles. Our propagation strategy was chosen conservatively, as news reports [2] of previous Twitter worms report 40,000 infected accounts within 10 minutes. Assuming the distribution of profile violations is similar for such aggressive worms, COMPA would detect such a large-scale outbreak even faster.

[2] http://eu.techcrunch.com/2010/09/21/warning-mouseover-tweets-security-flaw-is-wreaking-havoc-on-twitter/