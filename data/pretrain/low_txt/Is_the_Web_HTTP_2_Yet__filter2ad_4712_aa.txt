# Is the Web HTTP/2 Yet?

## Authors
- Matteo Varvello<sup>1</sup>
- Kyle Schomp<sup>2</sup>
- David Naylor<sup>3</sup>
- Jeremy Blackburn<sup>1</sup>
- Alessandro Finamore<sup>1</sup>
- Konstantina Papagiannaki<sup>1</sup>

<sup>1</sup> Telefónica Research, Barcelona, Spain  
<sup>2</sup> Case Western Reserve University, Cleveland, USA  
<sup>3</sup> Carnegie Mellon University, Pittsburgh, USA

[Project Website](http://isthewebhttp2yet.com/)

### Abstract
Version 2 of the Hypertext Transfer Protocol (HTTP/2) was finalized in May 2015 as RFC 7540. It addresses well-known issues with HTTP/1.1, such as head-of-line blocking and redundant headers, and introduces new features like server push and content prioritization. While HTTP/2 is designed to be the future of the web, it remains unclear whether the web will—or should—adopt it. To address this, we developed a measurement platform that monitors HTTP/2 adoption and performance across the Alexa top 1 million websites on a daily basis. Our system is live, and up-to-date results can be viewed at [our website](http://isthewebhttp2yet.com/).

In this paper, we report findings from an 11-month measurement campaign (November 2014 – October 2015). As of October 2015, we found that 68,000 websites reported HTTP/2 support, with about 10,000 actually serving content over HTTP/2. Popular sites are quicker to adopt HTTP/2, with 31% of the Alexa top 100 already supporting it. Most websites do not change their structure when moving from HTTP/1.1 to HTTP/2; current web development practices like inlining and domain sharding are still prevalent. Contrary to previous results, we find that these practices make HTTP/2 more resilient to losses and jitter. Overall, 80% of websites supporting HTTP/2 experience a decrease in page load time compared to HTTP/1.1, with the improvement being more significant in mobile networks.

### 1. Introduction
HTTP/2 (H2) is the new version of HTTP, expected to replace HTTP/1.1 (H1), which was standardized in 1999. H2 aims to make the web faster and more efficient by compressing headers, introducing server push, addressing head-of-line blocking, and loading page elements in parallel over a single TCP connection. Although the standard does not require encrypting H2 connections with Transport Layer Security (TLS), major browser vendors currently only support encrypted H2.

While H2 represents the future of the web on paper, its adoption may face challenges similar to those of IPv6. Modern websites are already designed to mitigate H1's inefficiencies using techniques like spriting, inlining, and domain sharding. While H2 would eliminate the need for such hacks, it is unclear how much it can improve performance over H1, given the widespread use of these practices. Furthermore, it is uncertain how these practices will affect H2 performance, as web developers cannot rebuild their sites overnight nor maintain two versions until H1 disappears.

Motivated by these uncertainties, we built a measurement platform to monitor the adoption and performance of H2. Using machines on PlanetLab and in our labs in Spain and the U.S., we probe the top 1 million Alexa websites daily to check for H2 support. For those that support H2, we measure performance with both H1 and H2. Results are published daily on [our website](http://isthewebhttp2yet.com/).

This paper reports findings from an 11-month measurement campaign, from November 2014 to October 2015. As of October 2015, we found 68,000 websites reporting H2 support, with only 10,000 actually serving content over H2. NGINX powers 71.7% of the working H2 websites, with LiteSpeed following at 13.7%. Our results show that sites that have deployed H2 have not significantly altered their content; classic H1 hacks are still used. In terms of page load time, 80% of the websites we measured experienced an average reduction of 300 to 560 ms when accessed from a wired connection (in Europe and the USA, respectively), 800 ms from a European 4G connection, and 1.6 s from a European 3G connection. The observed H2 benefits for mobile contradict previous studies, suggesting that domain sharding makes H2 more resilient to losses and jitter typical of mobile networks.

### 2. Background and Related Work
#### 2.1 HTTP/1.1 (H1)
H1 is an ASCII protocol that allows clients to request and submit content to a server. It is primarily used to fetch web pages, where clients request objects from a server, and the responses are serialized over a persistent TCP connection. H1 provides pipelining to request multiple objects over the same TCP connection, but the benefits are limited because servers must respond to requests in order. This leads to head-of-line blocking, where an early request for a large object can delay subsequent pipelined requests. Clients mitigate this by opening several concurrent TCP connections to the server, which incurs additional overhead (TCP state on the server, TCP handshake latency, and TLS session setup in the case of HTTPS). Browsers limit the number of simultaneous connections to each domain (e.g., 6 in Chrome and 15 in Firefox). Web developers have responded to this limitation with domain sharding, distributing content across multiple domains to circumvent the per-domain connection limit. H1 also requires the explicit transmission of headers for each request and response, leading to redundant header transmission, especially for pages with many small objects.

#### 2.2 SPDY and HTTP/2 (H2)
SPDY, developed by Google, updates H1 by using a binary format, enabling efficient parsing, a lighter network footprint, and reduced susceptibility to security issues. SPDY opens a single TCP connection to a domain and multiplexes requests and responses, reducing the number of TCP/TLS handshakes and CPU load at the server. SPDY introduces content prioritization, allowing clients to load important objects like CSS and JavaScript earlier, and server push, where the server can push objects before the client requests them. SPDY also includes header compression to reduce redundant header transmission. H2 builds on SPDY, making relatively small changes, such as using HPACK for header compression, which eliminates SPDY's vulnerability to the "crime" attack.

#### 2.3 NPN and ALPN
Since SPDY, H1, and H2 all use TLS over port 443, the port number is no longer sufficient to indicate the application protocol. Next Protocol Negotiation (NPN) is a TLS extension developed by Google as part of SPDY. During the TLS handshake, the server provides a list of supported application protocols, and the client chooses the protocol to use and communicates it to the server via an encrypted message. Application Layer Protocol Negotiation (ALPN) is a revised version of NPN standardized by the IETF. In ALPN, the client sends a list of supported application protocols ordered by priority, and the server selects the protocol based on the client's priority and returns the selected protocol via a plain text message.

#### 2.4 Related Work
Previous work has primarily focused on SPDY performance, with [17] being the only study focusing on H2 prior to ours. These studies have contradictory results but generally report poor SPDY (and H2) performance on mobile networks. Erman et al. [7] measured page load times for the top 20 Alexa websites via SPDY and H1 proxies in 3G, finding that SPDY performs poorly in mobile networks due to TCP interpreting cellular losses and jitter as congestion, causing unnecessary backoffs. Xiao et al. [23] introduced new measurement techniques, showing that SPDY outperforms H1 in the absence of browser dependencies and computation, but the gains are reduced when these factors are included. De Saxcè et al. [17] extended this analysis to H2, confirming the results for SPDY but ignoring the impact of real-world website properties like domain sharding.

Our aim is to further characterize H2 performance. Our measurements improve upon prior work in five ways: (1) targeting more websites (thousands instead of tens or hundreds); (2) measuring real servers from real networks (wired, 3G, and 4G); (3) testing real websites, not clones or synthetic traces; (4) building on Chrome reliability to develop an accurate performance estimation tool; and (5) studying adoption and website structure trends.

### 3. Measurement Platform
This section describes our measurement platform, which consists of a set of tools and a master-worker architecture. We start by summarizing the tools and then explain how they are used together to monitor H2 deployment and performance.

#### 3.1 Tools
- **Prober**: A lightweight bash script that identifies which application protocols a website announces. Prober uses OpenSSL to attempt ALPN and NPN negotiations and checks for H2 cleartext (H2C) support.
- **H2-lite**: A lightweight client that attempts to download only the root object of a website using H2. H2-lite uses the Node.js H2 library and follows HTTP redirects, reporting any protocol errors and identifying sites with certificate problems.
- **Chrome-loader**: A Python tool that loads pages using Chrome and extracts object sizes and timing information using chrome-har-capturer. Chrome-loader can instruct Chrome to use either H1 or SPDY/H2 and reports which protocol was used for each object.

#### 3.2 Measurement Platform
Our platform consists of a single master and multiple workers. The master issues crawl requests to the workers, which are deployed on PlanetLab and in our labs in the U.S. and Spain. We use PlanetLab for large-scale, simple measurements and our lab machines for more complex, smaller-scale measurements where machine reliability is crucial.

The master constantly monitors PlanetLab to identify a pool of candidate machines (at least 500 MB of free memory, CPU load under 30%, and no network connectivity issues). We collect measurements in three phases:

- **Phase I**: Daily discovery of which protocols are supported by the top 1 million Alexa websites. The master launches instances of the prober on each PlanetLab worker, assigning a unique 100-website list to each. When a worker finishes, it reports results to the master and receives a new list if available. This ensures load balancing among heterogeneous workers. The master reassigns uncompleted tasks to new workers after a timeout.
- **Phase II**: Daily verification of whether the sites that reported H2 support in Phase I actually serve content over H2. The master launches instances of h2-lite and assigns each 100 sites that reported H2 support. Because the H2 library requires more up-to-date software, we run h2-lite on four machines under our control, two in Barcelona and two in Cleveland.
- **Phase III**: Fetching both the H1 and H2 versions of websites that serve content via H2 using multiple network locations and access types (e.g., fiber and 4G). The master selects the machines and instructs them which websites to test. We use three strategies: (1) regular, where each network location with fiber access tests all H2 websites weekly; (2) lazy, similar to the regular strategy but testing only under specific conditions; (3) mobile, where only mobile-enabled locations are selected, and a subset of websites are tested based on their Alexa popularity.

To test a website, we fetch it five times with H1 and five times with either SPDY or H2. Fetches are run sequentially to limit the impact of network load. We run a background ping process to collect statistics about network latency and packet loss. For the mobile strategy, we force Chrome to report a mobile user-agent to the server. Before the five trials, an initial "primer" load is performed to ensure DNS entries are cached and to detect content changes. Local content and DNS caches are disabled, and each request carries a cache-control header to prevent network caching.

We currently have Phase III workers in three locations: Barcelona, Cleveland, and Pittsburgh, each with three machines with fiber connectivity. In Spain, machines are connected to Android phones via USB tethering, configured for 3G or 4G. Due to data plan limits, we can test a maximum of about 200 websites before exhausting the plan. We ran Phase III with the regular strategy from November 2014 to September 2015, switching to the lazy strategy due to the widespread deployment of H2. We ran the mobile strategy once in October 2015 and plan to run it monthly.

### 4. Results
This section presents and analyzes the data collected using our measurement platform between November 10, 2014, and October 16, 2015. We invite the reader to access fresh data and analysis at [our website](http://isthewebhttp2yet.com/).

#### 4.1 Adoption
We recorded protocol support announced via ALPN/NPN for 11 months, during which we saw support for 44 protocols (34 of which were versions of SPDY). Table 1 summarizes the evolution over time of the three most popular protocols: H1, SPDY 3.1, and H2. Each percentage is the maximum observed during the period.