### Rate and Recall Analysis

- **Recall ≥ 75%**: Non-damaging
- **Actual (Predicted)**
  - **Accuracy**: 0.713 (0.913)
  - **Precision**: 1.000 (0.909)
  - **Filter Rate**: 0.988 (0.998)
  - **Recall**: 0.040 (0.045)

- **Recall < 68.6%**: Non-damaging
- **Actual (Predicted)**
  - **Accuracy**: 0.574 (0.904)
  - **Precision**: 0.396 (0.226)
  - **Filter Rate**: 0.386 (0.887)
  - **Recall**: 0.814 (0.751)

### Filtering Results
- **High Confidence of Damage**: 10 out of 847 edits were dropped, all of which were confirmed to be damaging by prior hand coding.
- **Moderate Confidence of Damage**: 520 out of 847 edits were routed for human review, with 206 of these being confirmed as damaging.

### Table VIII: Top 5 Topics for Each Dataset

| Group | Topics |
|-------|--------|
| Tor   | Politics, Technology, Locations, Movies & TV, Religion |
| IP    | Music, Movies & TV, Locations, Politics, Sports |
| First-time | Education, Politics, Movies & TV, Sports, Locations |
| Registered | Politics, Music, Movies & TV, Sports, Locations |

### LDA Topic Model Analysis
The results below are from LDA topic models estimated using 20 topics. All other parameters for the LDA algorithm were run with default values in MALLET. After fitting the LDA topics, we manually interpreted each cluster of words and created appropriate topic headers. For reference, the mapping of keyword collections to topic headers is provided in Table IX in the appendix.

As a mixture model, LDA treats every document as belonging to every topic, but to varying degrees. We identified the topic with the highest probability and described each article as being "in" that topic for the purposes of comparisons between groups of edits. A Pearson’s Chi-squared test suggests that the distribution of articles across topics is different between Tor editors and IP editors (χ² = 1655; df = 19; p < 0.01), First-time editors (χ² = 848; df = 19; p < 0.01), and Registered editors (χ² = 1508; df = 19; p < 0.01). These differences are statistically significant after adjusting for multiple comparisons using a Bonferroni correction and suggest that Tor editors, although distinct from other groups, are most similar to First-time editors in their topic selections.

### Similarities and Differences in Editing Interests
Our analysis shows some similarities between Tor editors' interests and those of other groups. Table VIII compares the top 5 topics that each group focused on. Figure 6 visualizes the distribution of topics using a gradient where more prevalent topics are darker and less prevalent topics are lighter. While there are many horizontal bands of a similar shade indicating similar topics edited by different sets of users, we can also see many differences.

For example, like other editors, Tor editors frequently edit topics such as Movies and TV and Locations, which are popular across all groups. However, Tor editors contribute proportionally fewer edits to Sports, Soccer, and American Football. Compared with other kinds of users, Tor editors are more likely to contribute to articles related to Politics, Technology, and Religion—topics that may be considered controversial. Our findings provide evidence to support previous qualitative work suggesting that sensitive or stigmatized topics might attract Wikipedia editors interested in using tools like Tor to conceal their identity [14].

### Limitations
Our work has several important limitations:
1. **Language and Platform**: Our analysis is conducted only on English Wikipedia. We cannot know how this work would extend to users of privacy-enhancing technologies other than Tor or to user-generated content sites beyond English Wikipedia. As a minimal first step, we analyzed editing activity made by Tor users in other language editions of Wikipedia. Although we do not report on them in depth, we have included information in the appendix (see Table X) that displays the number of Tor edits in different language editions of Wikipedia relative to contributions made by the communities as a whole. Although Tor users are active in many language editions of Wikipedia, only a small number of edits by Tor users evaded the ban.
2. **Geographic and Cultural Factors**: The behavior of Tor editors contributing to English Wikipedia might differ from that of editors in other language editions. For example, we identified thousands of edits from Tor exit nodes contributing to the Russian Wikipedia edition, despite the Russian government partially banning access to Tor and Wikipedia. Although a closer inspection of Wikipedia language editions may yield interesting motivational and cultural differences regarding anonymity-seeking practice, our team is not sufficiently versed in these languages to conduct a replication of our analyses across different Wikipedia language editions. We are making our full datasets available and invite other researchers’ interest.
3. **Editor Identification**: Because our study uses IP addresses and account names to identify editors, we cannot know exactly how usernames and IP addresses map onto people. Some users may choose different levels of identifiability depending on the kinds of edits they wish to make. For example, a registered editor may use Tor for certain activities and not for others [14].
4. **Survivorship Bias**: Our samples might reflect survivorship bias. We cannot know if our sample of Tor edits is representative of the edits that would occur if Wikipedia did not block anonymity-seeking users. Many Tor users who are told by Wikipedia that Tor is blocked will not try again. As a result, our dataset might overrepresent casual one-off Wikipedia contributors, including both constructive “wiki gnomes” and drive-by vandals. Our sample might also over-represent individuals with a deep commitment to editing Wikipedia or with technical sophistication (i.e., the knowledge that one could repeatedly request new Tor circuits to find exit nodes that are not banned by Wikipedia). Tor users who manage to evade the ban might include committed activists as well as banned Wikipedia users with deeply held grudges. Although we do not know what else would happen if Wikipedia unblocked Tor, we know that the almost total end of contributions to Wikipedia from Tor in 2013 means that, at a minimum, a large number of high-quality contributions are not occurring. Our analysis describes some part of what is being lost today—both good and bad—due to Wikipedia’s decision to continue blocking users of anonymity-protecting proxies.

### Conclusions and Implications for Design
Wikipedia’s imperfect blocking of Tor provides a unique opportunity to gain insight into what might not be happening when user-generated content sites block participation by anonymity-seeking users. We employed multiple methods to compare Tor contributions to a number of comparison groups. Our findings suggest that privacy seekers’ contributions are more often than not comparable to those of IP editors and First-time editors. Using hand-coded data and a machine-learning classifier, we estimated that edits from Tor users are of similar quality to those by IP editors and First-time editors. We estimated that Tor users make more higher quality contributions than other IP editors, on average, as measured by PTRs. Our analysis also pointed to several important differences. We found that Tor users are significantly more likely than other users to revert someone else’s work and appear more likely to violate Wikipedia’s policy against back-and-forth edit wars, especially on discussion pages. Tor users also edit topics that are systematically different from other groups. We found that Tor editors focused more on topics related to religion, technology, and politics and less on topics related to sports and music.

The Tor network is steadily growing, with approximately two million active users at the time of writing. Many communities around the world face Internet censorship and authoritarian surveillance. In order to be Wikipedia contributors, these communities must rely on anonymity-protecting tools like Tor. In our opinion, our results show that the potential value to be gained by creating a pathway for Tor contributors may exceed the potential harm. Wikipedia’s systemic block of Tor editors remains controversial within the Wikipedia community. We have been in close contact with Wikipedia contributors and staff at the Wikimedia Foundation as we conducted this research to ensure that our use of Wikipedia metrics is appropriate and to give them advance notice of our results. We are hopeful that our work can inform the community and encourage them to explore mechanisms by which Tor users might legitimately contribute to Wikipedia—perhaps with additional safeguards. Given the advances of the privacy research community (including anonymous blacklisting tools such as Nymble [35]), and improvements in automated damage-detecting tools in Wikipedia, alternatives to an outright ban on Tor contributions may be feasible without substantially increasing the burden already borne by the vandal-fighting efforts of the Wikipedia community. We hope our findings will inform progress toward these ends.

### Acknowledgements
We owe a particular debt of gratitude to Nora McDonald and Erica Racine, who both contributed enormously to the content analysis included in the paper. Our methodology was improved via generous feedback from members of the Tor Metrics team, including Karsten Loesing, and the Wikimedia Foundation, including Aaron Halfaker, Morten Warncke-Wang, and Leila Zia. Feedback and support for this work came from members of the Community Data Science Collective, and the manuscript benefited from excellent feedback from several anonymous referees at IEEE S&P. The creation of the dataset was aided by the use of advanced computational, storage, and networking infrastructure provided by the Hyak supercomputer system at the University of Washington. This work was supported by the National Science Foundation (awards CNS-1703736 and CNS-1703049) and included the work of two undergraduates supported through an NSF REU supplement.

### References
[1] Nazanin Andalibi, Oliver L. Haimson, Munmun De Choudhury, and Andrea Forte. Understanding social media disclosures of sexual abuse through the lenses of support seeking and anonymity. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, pages 3906–3918, New York, NY, USA, 2016. ACM.
[2] John A. Bargh, Katelyn Y. A. McKenna, and Grainne M. Fitzsimons. Can you see the real me? Activation and expression of the ‘true self’ on the internet. Journal of Social Issues, 58(1):33–48, 2002.
[3] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan):993–1022, 2003.
[4] L. S. Buriol, C. Castillo, D. Donato, S. Leonardi, and S. Millozzi. Temporal analysis of the wikigraph. In 2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI’06), pages 45–51, Dec 2006.
[5] Abdelberi Chaabane, Pere Manils, and Mohamed Ali Kaafar. Digging into Anonymous Traffic: A Deep Analysis of the Tor Anonymizing Network. In 2010 Fourth International Conference on Network and System Security, pages 167–174, Melbourne, Australia, September 2010. IEEE.
[6] Kaylea Champion, Nora McDonald, Stephanie Bankes, Joseph Zhang, Rachel Greenstadt, Andrea Forte, and Benjamin Mako Hill. A Forensic Qualitative Analysis of Contributions to Wikipedia from Anonymity Seeking Users. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1–26, November 2019.
[7] Andrea Chester and Gillian Gwynne. Online teaching: Encouraging collaboration through anonymity. Journal of Computer-Mediated Communication, 4(2):0–0, 1998.
[8] M.D. Choudhury and S De. Mental health discourse on reddit: Self-disclosure, social support, and anonymity. Proceedings of the 8th International Conference on Weblogs and Social Media, ICWSM 2014, pages 71–80, 01 2014.
[9] William S. Cleveland. Robust Locally Weighted Regression and Smoothing Scatterplots. Journal of the American Statistical Association, 74(368):829–836, December 1979.
[10] Q. Dang and C. Ignat. Measuring quality of collaboratively edited documents: The case of wikipedia. In 2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC), pages 266–275, Nov 2016.
[11] Quang Vinh Dang and Claudia-Lavinia Ignat. Quality assessment of wikipedia articles: A deep learning approach by Quang Vinh Dang and Claudia-Lavinia Ignat with Martin Vesely as coordinator. SIGWEB Newsletter, Autumn:5:1–5:6, November 2016.
[12] Judith S Donath. Identity and deception in the virtual community. Routledge, 2002.