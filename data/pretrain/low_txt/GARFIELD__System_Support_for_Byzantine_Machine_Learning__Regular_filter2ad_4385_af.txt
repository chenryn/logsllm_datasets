### 通过增加工作节点数量进行吞吐量比较

图8展示了在CPU（图8a）和GPU（图8b）上，随着工作节点数量（nw）的增加，基于GARFIELD的应用程序在训练CifarNet和ResNet-50时的可扩展性。图中吞吐量以每秒处理的批次数（batches/sec）来衡量，而不是更新次数（updates/sec），因为使用更多的工作节点可以增加每次迭代中处理的批次数。

#### (a) CPU TF版本
#### (b) GPU PT版本
图8：随着工作节点数量增加的吞吐量比较。

### 工作节点数量的影响

增加工作节点的数量（nw）对于扩大分布式机器学习应用至关重要。这将有效批量大小也相应增加。图8显示了在CPU（图8a）和GPU（图8b）上训练CifarNet和ResNet-50时，基于GARFIELD应用程序的可扩展性。该图中的吞吐量以每秒处理的批次数（batches/sec）而非更新次数（updates/sec）来度量，因为在每个迭代中使用更多工作节点可以增加处理的批次数。

### 拜占庭工人数量的影响

随着拜占庭工人数（fw）的增加，并不意味着需要增加总的工作节点数。因此，在所有情况下，我们固定了nw，从而保持了固定的通信成本。这使得即使fw增加，吞吐量也几乎相同。这一结果在我们的两个框架中都得到了验证，PyTorch的表现略优于TensorFlow。

### 拜占庭服务器数量的影响

增加拜占庭服务器的数量（fps）要求增加服务器副本数量（nps），以满足拜占庭容错条件：nps ≥ 3fps+1。因此，增加fps会引入新的通信链接，导致吞吐量下降，如图10b所示。这种下降在状态机复制（SMR）文献[28]、[7]中得到了证实，其下降幅度（小于50%）与之前文献[16]报告的结果相比是合理的。假设一个故障参数服务器会导致33%的开销以实现拜占庭容错。最后需要注意的是，增加fps不会影响收敛所需的迭代次数。

### 相关工作

据我们所知，AggregaThor [17] 是唯一早于我们工作的拜占庭ML系统实现。AggregaThor依赖于两个组件：聚合层，使用Multi–Krum稳健地聚合工人的梯度；通信层，允许在损失网络中进行实验。尽管AggregaThor遵循共享图设计，但它不允许工人更改该图以对抗任何可能的拜占庭行为。

GARFIELD的设计从根本上不同于AggregaThor：后者仅是TensorFlow的一个附加层，而GARFIELD是一个可以插接到不同框架的独立库。事实上，AggregaThor只支持一种架构，即在同步环境中使用单一可信服务器和多个工人。从这些角度来看，它并不十分健壮。相反，GARFIELD可以灵活适应不同的场景，例如具有多个服务器副本和异步环境的情况。

在理论方面，已经提出了几种拜占庭容错ML算法；它们都试图在数学上限制聚合梯度与正确梯度之间的偏差。Krum [11] 使用了一种类似中位数的聚合规则。Multi-Krum [17] 通过平均更多的梯度来推广这个想法，从而受益于额外的工人。Bulyan [21] 解决了一些可以通过让某些拜占庭容错算法收敛到稳定但错误的状态来欺骗攻击的问题。[51] 和 [58] 考虑了在不同假设和场景下的各种鲁棒均值算法变体。Kardam [18] 使用过滤机制在异步训练设置中实现拜占庭容错。Zeno [52] 和 Zeno++ [55] 分别在同步和异步设置中使用基于性能的排名方法来实现拜占庭容错。Draco [14] 使用编码方案通过冗余计算恢复正确的梯度。Detox [43] 通过结合编码方案和鲁棒聚合来达到最佳的韧性-最优性平衡。ByzSGD [20] 展示了如何结合鲁棒GARs来容忍拜占庭服务器。特别是，它通过在多台机器上复制参数服务器并让它们相互通信来限制模型状态之间的差异。ByRDiE [57] 和 BRIDGE [56] 结合了鲁棒聚合和基于性能的排名，以在去中心化环境中实现拜占庭容错。

虽然这些提议塑造了拜占庭容错ML的文献，但它们仍然停留在理论上（即没有深入探讨这些解决方案的实际成本）。GARFIELD通过提供实用工具来构建这些解决方案填补了这一空白。GARFIELD使用鲁棒聚合，并已实现了许多上述提到的GARs。GARFIELD可以轻松地包含其他GARs。

### 容忍良性（即崩溃）参数向量失败的问题

在文献中还解决了容忍良性（即崩溃）参数向量失败的问题。Qiao等人[42] 利用SGD的自纠正行为来容忍此类故障。其他提案则解决了使参数服务器崩溃容错的问题[34], [15]，使用Paxos [31]。其他人依赖于检查点或实时复制[6] 参数服务器。然而，我们认为将这些工具扩展到拜占庭环境中将是难以承受的。

### 结论

本文介绍了GARFIELD，这是一个在流行框架（如TensorFlow和PyTorch）之上构建拜占庭机器学习（ML）应用程序的库，同时实现了透明性：使用任一框架开发的应用程序无需更改接口即可实现拜占庭容错。GARFIELD支持多种统计稳健的梯度聚合规则（GARs），这些规则可以以各种方式组合以实现不同的韧性属性。在某些情况下，当基础假设（有界方差）不成立时，GARs无法确保拜占庭容错[10]。然而，已经提出了多种方差减少技术，例如[50], [9], [39]，这些技术有助于恢复此类GARs的韧性保证。这些技术可以无缝添加到GARFIELD中，而不会影响其吞吐量性能。同样，我们认为GARFIELD也可以用于实现结合隐私和安全属性以及拜占庭容错的应用程序，如[27], [37]。确实，任何依赖于交换回复并在某种稳健方式下进行聚合的协议，例如[24]，都可以使用GARFIELD实现。我们的代码是开源的，可在[5]获取。我们对GARFIELD的评估（使用三个拜占庭ML应用程序）表明，拜占庭容错与崩溃容错不同，它会导致最终准确性的固有损失，并且拜占庭容错的吞吐量开销比崩溃容错要适度得多。此外，我们还展示了（1）拜占庭容错的开销更多来自通信而不是聚合，（2）容忍拜占庭服务器的开销远大于容忍拜占庭工人的开销。

### 致谢

我们感谢我们的导师Fernando Pedone教授、EPFL DCL实验室的同事们以及匿名审稿人提供的宝贵反馈。这项工作部分得到了瑞士国家科学基金会（FNS资助200021 182542/1）的支持。本论文中呈现的大多数实验是在Grid'5000测试平台上进行的，该平台由Inria主办的一个科学兴趣小组支持，包括CNRS、RENATER以及多所大学和其他组织（参见https://www.grid5000.fr）。

### 参考文献

[1] Aggregathor源代码. https://github.com/LPD-EPFL/AggregaThor.
[2] Cifar数据集. https://www.cs.toronto.edu/∼kriz/cifar.html.
[3] Grid5000. https://www.grid5000.fr/.
[4] Mnist数据集. http://yann.lecun.com/exdb/mnist/.
[5] GARFIELD源代码. https://github.com/LPD-EPFL/Garﬁeld.
[6] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. TensorFlow: A system for large-scale machine learning. In OSDI, 2016.
[7] Michael Abd-El-Malek, Gregory R Ganger, Garth R Goodson, Michael K Reiter, and Jay J Wylie. Fault-scalable Byzantine fault-tolerant services. ACM SIGOPS Operating Systems Review, 39:59–74, 2005.
[8] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Neural Information Processing Systems, to appear, 2018.
[9] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International conference on machine learning, pages 699–707, 2016.
[10] Moran Baruch, Gilad Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. arXiv preprint arXiv:1902.06156, 2019.
[11] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Neural Information Processing Systems, pages 118–128, 2017.
[12] Cara Bloom, Joshua Tan, Javed Ramjohn, and Lujo Bauer. Self-driving cars and data collection: Privacy perceptions of networked autonomous vehicles. In Thirteenth Symposium on Usable Privacy and Security ({SOUPS} 2017), pages 357–375, 2017.
[13] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pages 1175–1191. ACM, 2017.
[14] Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Draco: Byzantine-resilient distributed training via redundant gradients. In International Conference on Machine Learning, pages 902–911, 2018.
[15] Trishul M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project Adam: Building an efficient and scalable deep learning training system. In OSDI, volume 14, pages 571–582, 2014.
[16] James Cowling, Daniel Myers, Barbara Liskov, Rodrigo Rodrigues, and Liuba Shrira. HQ replication: A hybrid quorum protocol for Byzantine fault tolerance. In Proceedings of the 7th symposium on Operating systems design and implementation, pages 177–190. USENIX Association, 2006.
[17] Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Arsany Guirguis, and Sébastien Rouault. Aggregathor: Byzantine machine learning via robust gradient aggregation. In SysML, 2019.
[18] Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Rhicheek Patra, Mahsa Taziki, et al. Asynchronous Byzantine machine learning (the case of SGD). In ICML, pages 1153–1162, 2018.
[19] El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Lê Nguyên Hoang, and Sébastien Rouault. Collaborative learning as an agreement problem. arXiv preprint arXiv:2008.00742, 2020.
[20] El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Lê Nguyên Hoang, and Sébastien Rouault. Genuinely distributed Byzantine machine learning. In Proceedings of the 39th Symposium on Principles of Distributed Computing, pages 355–364, 2020.
[21] El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault. The hidden vulnerability of distributed learning in Byzantium. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3521–3530, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR.
[22] El-Mahdi El-Mhamdi, Rachid Guerraoui, and Sébastien Rouault. Distributed momentum for Byzantine-resilient learning. arXiv preprint arXiv:2003.00010, 2020.
[23] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115, 2017.
[24] Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. The limitations of federated learning in Sybil settings. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses ({RAID} 2020), pages 301–316, 2020.
[25] Amirmasoud Ghiassi, Taraneh Younesian, Zhilong Zhao, Robert Birke, Valerio Schiavoni, and Lydia Y Chen. Robust (deep) learning framework against dirty labels and beyond. In 2019 First IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA), pages 236–244. IEEE, 2019.
[26] Lie He, An Bian, and Martin Jaggi. COLA: Decentralized linear learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 4536–4546. Curran Associates, Inc., 2018.
[27] Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Secure Byzantine-robust machine learning. arXiv preprint arXiv:2006.04747, 2020.
[28] Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed. Zookeeper: Wait-free coordination for internet-scale systems. In USENIX annual technical conference, volume 8. Boston, MA, USA, 2010.
[29] M. Kachelrieß. Branchless vectorized median filtering. In 2009 IEEE Nuclear Science Symposium Conference Record (NSS/MIC), pages 4099–4105, Oct 2009.
[30] Larry Kim. How many ads does Google serve in a day? URL http://goo.gl/oIidXO. http://goo. gl/oIidXO, 1(1), 2012.
[31] Leslie Lamport et al. Paxos made simple. ACM Sigact News, 32(4):18–25, 2001.
[32] Leslie Lamport, Robert Shostak, and Marshall Pease. The Byzantine generals problem. TOPLAS, 4(3):382–401, 1982.
[33] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI, volume 1, page 3, 2014.
[34] Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander Smola. Parameter server for distributed machine learning. In Big Learning NIPS Workshop, volume 6, page 2, 2013.
[35] H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: A view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1222–1230. ACM, 2013.
[36] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al. MLlib: Machine learning in Apache Spark. JMLR, 17(1):1235–1241, 2016.
[37] Luis Muñoz-González, Kenneth T Co, and Emil C Lupu. Byzantine-robust federated machine learning through adaptive model averaging. arXiv preprint arXiv:1909.05125, 2019.
[38] David R Musser. Introspective sorting and selection algorithms. Software: Practice and Experience, 27(8):983–993, 1997.
[39] Jay H Park, Sunghwan Kim, Jinwon Lee, Myeongjae Jeon, and Sam H Noh. Accelerated training for CNN distributed deep learning through automatic resource-aware layer placement. arXiv preprint arXiv:1901.05803, 2019.
[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026–8037, 2019.
[41] Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. Journal of Parallel and Distributed Computing, 69(2):117–124, 2009.
[42] Aurick Qiao, Bryon Aragam, Bingjing Zhang, and Eric Xing. Fault tolerance in iterative-convergent machine learning. In International Conference on Machine Learning, pages 5220–5230, 2019.
[43] Shashank Rajput, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Detox: A redundancy-based framework for faster and more robust gradient aggregation. arXiv preprint arXiv:1907.12205, 2019.
[44] Qing Rao and Jelena Frtunikj. Deep learning for self-driving cars: chances and challenges. In 2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS), pages 35–38. IEEE, 2018.
[45] Peter J Rousseeuw. Multivariate estimation with high breakdown point. Mathematical statistics and applications, 8:283–297, 1985.
[46] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323(6088):533–536, 1986.
[47] Paul Vanhaesebrouck, Aurélien Bellet, and Marc Tommasi. Decentralized collaborative learning of personalized models over networks. In AISTATS, 2017.
[48] Kenton Varda. Protocol buffers. https://github.com/protocolbuffers/protobuf.
[49] Pooja Vyavahare, Lili Su, and Nitin H Vaidya. Distributed learning with adversarial agents under relaxed network condition. arXiv preprint arXiv:1901.01943, 2019.
[50] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. Advances in Neural Information Processing Systems, 26:181–189, 2013.
[51] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized Byzantine-tolerant SGD. arXiv preprint arXiv:1802.10116, 2018.
[52] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Zeno: Byzantine-suspicious stochastic gradient descent. arXiv preprint arXiv:1805.10032, 2018.
[53] Cong Xie, Oluwasanmi O Koyejo, and Indranil Gupta. Faster distributed synchronous SGD with weak synchronization. 2018.
[54] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking Byzantine-tolerant SGD by inner product manipulation. arXiv preprint arXiv:1903.03936, 2019.
[55] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous SGD. arXiv preprint arXiv:1903.07020, 2019.
[56] Zhixiong Yang and Waheed U Bajwa. BRIDGE: Byzantine-resilient decentralized gradient descent. arXiv preprint arXiv:1908.08098, 2019.
[57] Zhixiong Yang and Waheed U Bajwa. ByRDIE: Byzantine-resilient distributed coordinate descent for decentralized learning. IEEE Transactions on Signal and Information Processing over Networks, 5(4):611–627, 2019.
[58] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. arXiv preprint arXiv:1803.01498, 2018.

---

**授权许可使用范围限于：清华大学。下载日期：2021年10月11日，UTC时间09:24:13，来自IEEE Xplore。使用受限。**

**版权所有，未经授权不得复制。**