### Client-Side Honeypot and Web Emulation

A client-side honeypot, as described in [25, 24], involves running a web browser (in this case, Microsoft Internet Explorer) on a Windows operating system within a virtual machine. The browser is scripted to automatically download a list of URLs via a consumer-grade cable modem connection to the internet. For each URL, the honeyclient retrieves the page and records the full contents of each generated packet. After each page retrieval, the VM is reverted to a clean state.

To ensure broad coverage of the web, we start with a list of over ten thousand URLs from categorized directories such as Mozilla’s Open Directory Project [50] or Yahoo!’s directory [51], as well as lists of the most popular URLs from Alexa.com rankings [52]. For increased realism, the honeyclient can be scripted to "crawl" more links, providing greater depth for interactive sites.

### TCP Stream Reassembly and Data Storage

We perform TCP stream reassembly on the captured packets to reconstruct the application-layer conversations for each URL and all associated HTTP sessions. For each HTTP request, we store the full text of the resulting HTTP response, including headers and body, in a database. The data is keyed based on the hostname used in the request, the URL requested, and some meta-information from the headers, such as transport and content encoding parameters.

### Web Emulation on the Testbed Network

On the testbed network, we deploy a simple web server program to emulate the web using the collected data. Upon receiving an HTTP request, the server parses out the hostname, URL, and other parameters, looks up the corresponding HTTP response text in the database, and sends this response back to the client. The content from the database can be distributed across several web servers to handle large traffic loads and provide more realistic network characteristics.

To simulate a much larger network than can be realistically installed on a testbed, we typically deploy the web server on several Linux machines, each configured with hundreds or even thousands of virtual network interfaces for each physical interface. Each machine can respond to HTTP requests sent to any one of thousands of IP addresses. Each instance of the web server application listens on a designated subset of the host's IP addresses and serves content for a designated set of websites. This flexibility allows us to emulate both simple sites hosted at a single IP address and dynamic, worldwide content distribution networks. We store the mapping from hostnames to IP addresses and Linux hosts in the testbed’s central LARIAT database, which also configures the testbed’s DNS servers. Additionally, we provide artificial Root DNS servers and a core BGP routing infrastructure to redistribute all the routing information for these IP addresses.

### Discussion and Limitations

This combination of lightweight application-level replay on the server side with automation of heavyweight GUI applications on the client side allows us to generate high-fidelity network traffic for many use cases. It requires no parsing or understanding of JavaScript, but many JavaScript-heavy sites can be emulated and appear fully functional from the client’s perspective, limited only by the extent of the data collection. Google Maps is a notable example of such a site.

However, our focus on lightweight and efficient server-side replay techniques leads to some important limitations. First, because the server is stateless, it cannot handle HTTP authorization or customize page content based on cookies. Second, since it only looks for exact matches in the URLs, pages that dynamically generate links may fail to find a matching page when run on the testbed. Pages that dynamically choose IP addresses or hostnames for links may need to be fetched multiple times during the data collection step to find all IP addresses or hostnames that should occur in the linked pages’ URLs. Otherwise, any client-side JavaScript code that uses random numbers to control its actions (e.g., client-side load balancing) will fail to function, as previously unrequested URLs will not be found in the new closed environment. Finally, while our approach could be extended to support the concurrent use of multiple browsers or operating systems, it does not currently do so.

Despite these limitations, the current techniques are sufficient for many kinds of experiments involving network traffic. They are also valuable for tests that focus primarily on host behavior, as they enable a wider range of applications to be run on the host, notably the web browser. In the next section, we walk through a simple experiment with a host-based security system where the use of the browser makes up a significant fraction of the client machine’s workload.

### Example Experiment: Measuring Anti-Virus Performance Impact

In this section, we walk through a simple experiment to measure and quantify the performance penalty incurred by running anti-virus protection on desktop computers. Although the underlying LARIAT test range automation tool and AUSM-based workload generators can scale to hundreds or even thousands of nodes, we limit ourselves to a smaller test scenario for ease of exposition.

#### Hypothesis

Our hypothesis is that "anti-virus increases the system’s resource consumption." This is a measurable and falsifiable statement, unlike a subjective claim about system slowness.

#### Testbed Setup

We use a simple experimental testbed comprised of two physical machines:
- **HOST**: A server-class machine (Dell PowerEdge 2650 with dual Intel Xeon 2.60GHz processors and 2GB of RAM) that provides the LARIAT infrastructure and deploys two virtual servers (LARIAT control server and a web services server).
- **SUT (System Under Test)**: A Dell Latitude D610 laptop (1.7GHz Pentium M processor and 1GB of RAM) with a partitioned hard disk. One partition runs Ubuntu Linux 9.10, and the other runs Windows XP with Service Pack 2, Microsoft Office XP, and client-side workload generation tools. Performance Co-Pilot (PCP) software is installed to collect performance measurements.

#### Experimental Methods

1. **Prepare Systems for Test Run**
   - Revert disk images on SUT and INTERNET.
   - Revert system clocks on SUT and INTERNET.
   - Reboot SUT into the Windows environment.
   - Seed PRNGs using a master experiment seed.
   - Start PCP performance logging service.

2. **Execute Test Run**
   - Start AUSM-based client workload generation.
   - Let the workload generation run for 2 hours.
   - Stop AUSM-based client workload generation.

3. **Collect Results**
   - Stop PCP performance logging service.
   - Archive performance logs.
   - Reboot SUT into the Linux environment.

#### Experimental Results

Using the above procedure, we select a master experiment seed and repeatedly execute a 2-hour test under two scenarios:
1. **Baseline**: No anti-virus protection on the SUT.
2. **With Anti-Virus**: An open-source anti-virus product installed.

We measure CPU utilization, memory consumption, and disk I/O volume. The results show consistent spikes in CPU load and disk I/O starting near 0 seconds when the user agent launches Outlook and Word, and again near 600 seconds when Internet Explorer is started. The open-source AV product consistently causes near 100% CPU use for a period of nearly 10 minutes, with a standard deviation near zero, indicating highly repeatable behavior.

Figures 5 and 6 show clear evidence that the system with open-source anti-virus protection consumes more resources than the baseline system. Formal statistical tests confirm our hypothesis with high confidence. Figure 7 shows that the anti-virus system does not cause a statistically significant increase in disk I/O loads relative to the baseline system.

### Conclusions and Future Work

We presented new techniques for driving ubiquitous, commercial-off-the-shelf Windows GUI applications in place of a human user on an isolated testbed network. Using statistical models of user behavior to generate workloads on the testbed hosts, together with dynamic application-level protocol replay techniques to emulate remote services like the World Wide Web, we can generate traffic on the testbed network that resembles real traffic to a very high degree of fidelity.

In the future, we plan to improve our techniques by collecting data from a larger set of users, developing validation techniques, reducing the software footprint on the system under test, and developing more robust techniques for dynamic application-level replay of websites that heavily use JavaScript or other active content generation techniques.

### Acknowledgments

The authors extend their sincere thanks to the members of the MIT-LL Cyber Testing team who implemented much of the software described here and provided valuable feedback on the experiments and the paper.

### References

1. Barford, P., Landweber, L.: Bench-style network research in an Internet Instance Laboratory. ACM SIGCOMM Computer Communication Review 33(3), 21–26 (2003)
2. Peisert, S., Bishop, M.: How to Design Computer Security Experiments. In: Proceedings of the 5th World Conference on Information Security Education (WISE), pp. 141–148 (2007)
3. US Department of Homeland Security: A Roadmap for Cybersecurity Research. Technical report (November 2009), www.cyber.st.dhs.gov/docs/DHS-Cybersecurity-Roadmap.pdf
4. White, B., Lepreau, J., Stoller, L., Ricci, R., Guruprasad, S., Newbold, M., Hibler, M., Barb, C., Joglekar, A.: An integrated experimental environment for distributed systems and networks. In: Proceedings of the 5th Symposium on Operating Systems Design and Implementation (December 2002)
5. Ricci, R., Duerig, J., Sanaga, P., Gebhardt, D., Hibler, M., Atkinson, K., Zhang, J., Kasera, S., Lepreau, J.: The Flexlab approach to realistic evaluation of networked systems. In: Proceedings of the 4th USENIX Symposium on Networked Systems Design & Implementation, pp. 201–214 (April 2007)
6. Vahdat, A., Yocum, K., Walsh, K., Mahadevan, P., Kostic, D., Chase, J., Becker, D.: Scalability and Accuracy in a Large-Scale Network Emulator. In: Proceedings of the 5th Symposium on Operating Systems Design and Implementation (December 2002)
7. Bavier, A., Feamster, N., Huang, M., Peterson, L., Rexford, J.: VINI veritas: Realistic and controlled network experimentation. In: Proceedings of ACM SIGCOMM (September 2006)
8. Rossey, L.M., Cunningham, R.K., Fried, D.J., Rabek, J.C., Lippmann, R.P., Haines, J.W., Zissman, M.A.: LARIAT: Lincoln Adaptable Real-time Information Assurance Testbed. In: Proceedings of the IEEE Aerospace Conference (2002)
9. Provos, N., McNamee, D., Mavrommatis, P., Wang, K., Modadugu, N.: The Ghost in the Browser: Analysis of Web-based Malware. In: Proceedings of the First Workshop on Hot Topics in Understanding Botnets (HotBots 2007) (April 2007)
10. Fossi, M.: Symantec Internet Security Threat Report: Trends for 2008 (April 2009)
11. Deibert, R., Rohozinski, R.: Tracking GhostNet: Investigating a Cyber Espionage Network. Technical Report JR02-2009, Information Warfare Monitor (March 2009)
12. Nagaraja, S., Anderson, R.: The Snooping Dragon: Social-Malware Surveillance of the Tibetan Movement. Technical Report UCAM-CL-TR-746, University of Cambridge Computer Laboratory (March 2009)
13. Provos, N., Mavrommatis, P., Rajab, M., Monrose, F.: All Your iFrames Point to Us. In: Proceedings of the 17th USENIX Security Symposium (July 2008)
14. Pinheiro, E., Weber, W.D., Barroso, L.A.: Failure Trends in a Large Disk Drive Population. In: Proceedings of the 5th USENIX Conference on File and Storage Technologies (February 2007)
15. Lippmann, R.P., Fried, D.J., Graf, I., Haines, J.W., Kendall, K.R., McClung, D., Weber, D., Webster, S.E., Wyschogrod, D., Cunningham, R.K., Zissman, M.A.: Evaluating Intrusion Detection Systems: The 1998 DARPA Off-Line Intrusion Detection Evaluation. In: Proceedings of the 2000 DARPA Information Survivability Conference and Exposition (2000)
16. Lippmann, R., Haines, J.W., Fried, D.J., Korba, J., Das, K.: The 1999 DARPA Off-line Intrusion Detection Evaluation. Computer Networks 34(4), 279–595 (2000)
17. Yu, T., Fuller, B., Bannick, J., Rossey, L., Cunningham, R.: Integrated Environment Management for Information Operations Testbeds. In: Proceedings of the 2007 Workshop on Visualization for Computer Security (October 2007)
18. Benzel, T., Braden, R., Kim, D., Neuman, C., Joseph, A., Sklower, K., Ostrenga, R., Schwab, S.: Experience with DETER: A Testbed for Security Research. In: Proceedings of the 2nd International Conference on Testbeds and Research Infrastructures for the Development of Networks and Communities (TRIDENTCOM) (March 2006)
19. Boothe-Rabek, J.C.: WinNTGen: Creation of a Windows NT 5.0+ network traffic generator. Master’s thesis, Massachusetts Institute of Technology (2003)
20. Garg, A., Vidyaraman, S., Upadhyaya, S., Kwiat, K.: USim: A User Behavior Simulation Framework for Training and Testing IDSes in GUI Based Systems. In: ANSS 2006: Proceedings of the 39th Annual Symposium on Simulation, Washington, DC, USA, pp. 196–203. IEEE Computer Society, Los Alamitos (2006)
21. Cui, W., Paxson, V., Weaver, N.C.: GQ: Realizing a System to Catch Worms in a Quarter Million Places. Technical Report TR-06-004, International Computer Science Institute (September 2006)
22. Cui, W., Paxson, V., Weaver, N.C., Katz, R.H.: Protocol-Independent Adaptive Replay of Application Dialog. In: Proceedings of the 13th Annual Symposium on