### Relevant Inputs and Overfitting

When a targeting detection algorithm includes both heavily targeted inputs and other, less relevant ones, overfitting can occur. This happens if the specific training set correlates an additional input with the output, even though it is not truly relevant. For example, if an ad appears in 10 profiles in the testing set, all of which have the relevant input, and inputs are assigned to profiles with a 20% probability, the p-value should be approximately 1.0e-7 with only the relevant input. Adding a second input increases the p-value because a combination is more likely to cover more profiles. However, the new p-value remains at 3.6e-5 for two inputs, which is still below the 5% accepted error rate after correction.

### Hypothesis Recall

Assessing hypothesis recall through manual inspection is challenging, especially when there are many ads to analyze. Finding a highly likely targeted input among hundreds is difficult, and the numerous possibilities can lead to false connections. Therefore, we did not attempt to quantify hypothesis recall. Instead, we focused on low p-value hypotheses that are rejected after correction. This method provides insight into how many hypotheses are lost due to correction. In our experience, this typically occurs when an ad does not appear frequently enough, making it impossible for the p-value to be low enough to remain below 5% after correction. For instance, if an ad appears in 10 profiles, it will be in about 3 profiles in the testing set, and the p-value cannot be below 0.008 if inputs are assigned with a 20% probability. After correction, this p-value will likely exceed the 5% threshold in large experiments.

### Contextual and Behavioral Targeting

Figure 10(a) shows the cumulative distribution function (CDF) of how often targeted ads appear in their target context for the Gmail and Website-large datasets. The Y-axis represents the fraction of all targeted ads in each experiment.

In Gmail, ads frequently appear out of context (i.e., alongside emails they do not target). Approximately 28% of Gmail ads labeled as targeted appear only in their targeted context, and half of the targeted Gmail ads appear out of context 48% of the time or more. This indicates significant behavioral targeting in Gmail.

On the web, display ads are rarely shown outside their targeted context. 73% of ads are only ever shown on the site targeted by the ad. Of the targeted ads that do appear out of context, most appear on only 1 or 2 other sites, suggesting heavy contextual targeting for display ads. However, we found convincing examples of behaviorally targeted ads that appear entirely outside their targeted context. For instance, an ad for The Economist encouraging viewers to subscribe never appeared on the targeted site. Similar examples were found for The New York Times.

### Targeting Per Category

Figures 10(b) and 10(c) show the number of ads targeting emails and websites, respectively, in specific categories. For emails, we classify them based on their content. For websites, we use Alexa categories. It is common for Sunlight to detect that an ad targets multiple emails, so the cumulative number of guesses may be larger than the total number of ads.

In Gmail, the most targeted category was shopping (e.g., emails containing keywords such as clothes, antiques, furniture, etc.). The second most popular targeted category was General health (e.g., emails with keywords such as vitamins, yoga, etc.). On the web, no single dominant category was observed. The News category, containing sites like The Economist and Market, was targeted by the most ads, but only slightly more than the Home category.

### Related Work

Previous works discussed in §2.2 include web transparency tools and measurements [2,6,8,15,16,18–22,27,29]. These aim to quantify various data uses on the web, including targeting, personalization, price tuning, or discrimination. Sunlight is the first system to detect targeting at fine grain (individual inputs), at scale, and with solid statistical justification.

The works closest in spirit to ours are AdFisher [8] and XRay [18], which also aim to create generic, broadly applicable methodologies for various web transparency goals. AdFisher shares our goal of providing solid statistical justification for its findings. However, AdFisher's design makes it hard to simultaneously track many inputs and does not exhaustively single out the output subject to targeting. XRay, while aiming to detect targeting at scale on many inputs, does not provide statistical validation of its findings, missing the inherent trade-off between scale and confidence in results.

### Conclusions

This paper argues for the need for scalable and statistically rigorous methodologies, plus infrastructures that implement them, to shed light on today’s opaque online data ecosystem. We presented Sunlight, a system designed to detect targeting at fine granularity, at scale, and with statistical justification for all its inferences. Sunlight's methodology consists of a four-stage pipeline, which gradually generates, refines, and validates hypotheses to reveal the likely causes of observed targeting. Our empirical study suggests that favoring high precision hypothesis generation can yield better recall at high confidence, and that scaling the number of outputs of an experiment may require accepting lower statistical semantics. Future work will focus on breaking the scaling barrier by developing a reactive architecture that runs additional experiments to confirm plausible hypotheses.

### Acknowledgements

We thank the anonymous reviewers for their valuable feedback. We also thank Francis Lan for his work on early versions of Sunlight. This work was supported by a Google Research Award, a Microsoft Faculty Fellowship, a Yahoo ACE award, a grant from the Brown Institute for Media Innovation, NSF CNS-1514437, NSF CNS-1351089, NSF 1254035, and DARPA FA8650-11-C-7190.

### References

[1] ADBLOCKPLUS. https://adblockplus.org/, 2015.
[2] BARFORD, P., CANADI, I., KRUSHEVSKAJA, D., MA, Q., AND MUTHUKRISHNAN, S. Adscape: Harvesting and Analyzing Online Display Ads. WWW (Apr. 2014).
[3] BENJAMINI, Y., AND YEKUTIELI, D. The control of the false discovery rate in multiple testing under dependency. Annals of statistics (2001), 1165–1188.
[4] BICKEL, P. J., RITOV, Y., AND TSYBAKOV, A. B. Simultaneous analysis of lasso and dantzig selector. Ann. Statist. 37, 4 (08 2009), 1705–1732.
[5] BODIK, P., GOLDSZMIDT, M., FOX, A., WOODARD, D. B., AND ANDERSEN, H. Fingerprinting the datacenter: Automated classification of performance crises. In European Conference on Computer Systems (2010).
[6] BOOK, T., AND WALLACH, D. S. An Empirical Study of Mobile Ad Targeting. arXiv.org (2015).
[7] BRANDEIS, L. What Publicity Can Do. Harper’s Weekly (Dec. 1913).
[8] DATTA, A., TSCHANTZ, M. C., AND DATTA, A. Automated Experiments on Ad Privacy Settings. In Proceedings of Privacy Enhancing Technologies (2015).
[9] DONOHO, D. L. Compressed sensing. IEEE Transactions on Information Theory 52, 4 (2006), 1289–1306.
[10] DUDOIT, S., AND VAN DER LAAN, M. Multiple testing procedures with applications to genomics. Springer, 2008.
[11] FELDMAN, V. Optimal hardness results for maximizing agreement with monomials. SIAM Journal on Computing 39, 2 (2009), 606–645.
[12] GOOGLE. AdSense policy. https://support.google.com/adsense/answer/3016459?hl=en, 2015.
[13] GOOGLE. AdWords policy. https://support.google.com/adwordspolicy/answer/6008942?hl=en, 2015.
[14] GRETTON, A., BOUSQUET, O., SMOLA, A., , AND SCHÖLKOPF, B. Measuring statistical dependence with Hilbert-Schmidt norms. In Algorithmic Learning Theory (2005).
[15] HANNAK, A., SAPIEZYNSKI, P., KAKHKI, A. M., KRISHNAMURTHY, B., LAZER, D., MISLOVE, A., AND WILSON, C. Measuring personalization of web search. In WWW (May 2013).
[16] HANNAK, A., SOELLER, G., LAZER, D., MISLOVE, A., AND WILSON, C. Measuring Price Discrimination and Steering on E-commerce Web Sites. In IMC (Nov. 2014).
[17] HOLM, S. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6, 2 (1979), 65–70.
[18] LÉCUYER, M., DUCOFFE, G., LAN, F., PAPANCEA, A., PETSIOS, T., SPAHN, R., CHAINTREAU, A., AND GEAMBASU, R. XRay: Enhancing the Web’s Transparency with Differential Correlation. 23rd USENIX Security Symposium (USENIX Security 14) (2014).
[19] LIU, B., SHETH, A., WEINSBERG, U., CHANDRASHEKAR, J., AND GOVINDAN, R. AdReveal: improving transparency into online targeted advertising. In HotNets-XII (Nov. 2013).
[20] MIKIANS, J., GYARMATI, L., ERRAMILLI, V., AND LAOUTARIS, N. Detecting price and search discrimination on the internet. In HotNets-XI: Proceedings of the 11th ACM Workshop on Hot Topics in Networks (Oct. 2012), ACM Request Permissions.
[21] MIKIANS, J., GYARMATI, L., ERRAMILLI, V., AND LAOUTARIS, N. Crowd-assisted Search for Price Discrimination in E-Commerce: First results. arXiv.org (July 2013).
[22] NATH, S. MAdScope: Characterizing Mobile In-App Targeted Ads. Proceedings of ACM Mobisys (2015).
[23] NG, A. Y. Feature selection, l1 vs. l2 regularization, and rotational invariance. In Proceedings of the Twenty-first International Conference on Machine Learning (2004).
[24] RUBIN, D. B. Estimating the causal effects of treatments in randomized and non-randomized studies. Journal of Educational Psychology 66 (1974), 688–701.
[25] SELENIUM. http://www.seleniumhq.org/, 2015.
[26] TIBSHIRANI, R. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58 (1994), 267–288.
[27] VISSERS, T., NIKIFORAKIS, N., BIELOVA, N., AND JOOSEN, W. Crying Wolf? On the Price Discrimination of Online Airline Tickets. Hot Topics in Privacy Enhancing Technologies (June 2014), 1–12.
[28] WU, T. T., CHEN, Y. F., HASTIE, T., SOBEL, E., AND LANGE, K. Genome-wide association analysis by lasso penalized logistic regression. Bioinformatics 25, 6 (2009), 714–721.
[29] XING, X., MENG, W., DOOZAN, D., FEAMSTER, N., LEE, W., AND SNOEREN, A. C. Exposing Inconsistent Web Search Results with Bobble. In PAM ’14: Proceedings of the Passive and Active Measurements Conference (2014).