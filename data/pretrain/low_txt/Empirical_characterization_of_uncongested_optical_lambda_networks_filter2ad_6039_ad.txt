### 3.5 Summary of Results

**Packet Inter-Arrival Time and Batch Processing:**

- **Measurement Context:** The packet inter-arrival time, as a function of the number of packets, was measured with NAPI (New API) disabled. This context is crucial for understanding the behavior of a group of packets arriving during a specific time interval.
- **Interrupt Throttling and Packet Batching:** For Intel NICs, the solution to manage interrupt frequency is to batch packets by parameterizing the maximum number of interrupts the device supports. At low data rates, the kernel operates in an interrupt-driven mode, while at high data rates, it switches to polling mode (NAPI). 
- **Impact on Performance:** Enabling NAPI and setting Interrupt Throttling to a default rate of 8000 interrupts per second can achieve a throughput of approximately 1.9 Gbps. Disabling Interrupt Throttling can increase this to around 3 Gbps. However, this does not imply that packet batching is ideal in all scenarios.

**Key Observations:**

- **UDP Loss and TCP Throughput:** UDP loss is influenced by the size of socket buffers and DMA rings, as well as the specifics of interrupt affinity in end-host network adapters. TCP throughput decreases with increased path length and packet loss, but increases with window size. The congestion control algorithm has a marginal impact on achievable throughput.
- **Parameter Sensitivity:** The performance and reliability of end-hosts are highly sensitive to subtle configuration issues such as socket buffer size, TCP window size, and the status of various packet batching techniques. Even at low data rates, these effects are readily observable.
- **End-to-End Performance:** In our experiments, we observed significant penalties in end-to-end dependability, including packet loss at the receiving end, even when traffic was sent at relatively low data rates.

### 4 Related Work

**Characterization of Network Behavior:**

- **Analytical and Empirical Studies:** There has been extensive work aimed at characterizing the Internet's packet delay, packet loss, and packet dispersion behaviors. These studies have used a variety of methods, including modeling, simulation, and empirical measurements.
- **Tools and Techniques:** Various tools have been developed to measure end-to-end available bandwidth, packet dispersion, and other network metrics. These tools often embody a tradeoff between intrusiveness and accuracy, using small probes or self-induced congestion.
- **Historical Context:** Early networks like ARPANET, NSFNET, and the early Internet were prone to erratic round-trip times and varying characteristics. Modern optical lambda networks, which provide high-bandwidth interconnections, face different challenges, particularly in end-host performance and reliability.

**Specific Studies:**

- **Murray et al. [26]:** Compared end-to-end bandwidth measurement tools on the 10GbE TeraGrid backbone.
- **Bullot et al. [12]:** Evaluated the performance of advanced TCP stacks on fast long-distance networks, focusing on throughput stability and behavior under competing UDP traffic.
- **Other Works:** Various studies have focused on specific aspects such as packet delay, packet loss, and available bandwidth, providing valuable insights into network behavior and performance.

### 5 Conclusion

**Summary and Future Directions:**

- **Empirical Study Insights:** Our empirical study highlights the difficulty of reliably and consistently maximizing the performance of high-data-rate networks. No single configuration is optimal for all scenarios, and end-hosts face significant challenges in handling high data rates.
- **Future Challenges:** As optical networking and clock speeds of commodity hardware continue to advance, more end-to-end applications will face similar issues. Further research is needed to develop robust solutions for reliable and consistent network performance.

**Acknowledgments:**

- **Contributions and Support:** We would like to thank the anonymous reviewers and the engineering staff who helped establish and monitor the Cornell NLR Rings testbed. Special thanks to Dan Eckstrom, Greg Boles, Brent Sweeny, and Ed Kiefer from Cornell, as well as Joe Lappa from National LambdaRail, Eric Cronise, Scott Yoest, and Larry Parmelee. We also acknowledge the support from Intel, Cisco, NSF TRUST, and AFRL.

### References

- [1] Irqbalance. http://www.irqbalance.org/
- [2] Ixia. http://www.ixiacom.com/
- [3] NAPI. http://www.linuxfoundation.org/
- [4] National LambdaRail. http://www.nlr.net/
- [5] NLR PacketNet Atlas. http://atlas.grnoc.iu.edu/atlas.cgi?map_name=NLR%20Layer3
- [6] TeraGrid. http://teragrid.org/
- [7] Think Big with a Gig: Our Experimental Fiber Network. http://googleblog.blogspot.com/2010/02/think-big-with-gig-our-experimental.html
- [8] TeraGrid Performance Monitoring. https://network.teragrid.org/tgperf
- [9] W. Allcock, J. Bester, J. Bresnahan, A. Chervenak, L. Liming, and S. Tuecke. GridFTP: Protocol extensions for the Grid. GGF Document Series GFD, 20, 2003.
- [10] M. Balakrishnan, T. Marian, K. Birman, H. Weatherspoon, A. Chervenak, L. Liming, and E. Vollset. Maelstrom: Transparent Error Correction for Lambda Networks. In Proceedings of NSDI, 2008.
- [11] J.-C. Bolot. End-to-end packet delay and loss behavior in the Internet. In Proceedings of SIGCOMM '93.
- [12] H. Bullot, R. L. Cottrell, and R. Hughes-Jones. Measurement of advanced TCP stacks on fast long-distance networks. In Proceedings of the International Workshop on Protocols for Fast Long-Distance Networks, 2004.
- [13] R. L. Carter and M. E. Crovella. Measuring bottleneck link speed in packet-switched networks. Perform. Eval., 28:297-318, 1996.
- [14] I. Cidon, A. Khamisy, and M. Sidi. Analysis of Packet Loss Processes in High-Speed Networks. IEEE Transactions on Information Theory, 39:98-108, 1991.
- [15] Cisco Systems. Buffers, Queues, and Thresholds on the Catalyst 6500 Ethernet Modules, 2007.
- [16] K. Claffy, G. C. Polyzos, and H. Braun. Traffic Characteristics of the T1 NSFNET Backbone. In INFOCOM '93.
- [17] M. Dobrescu, N. Egi, K. Argyraki, B.-g. Chun, K. Fall, A. Knies, M. Manesh, and S. Ratnasamy. RouteBricks: Exploiting Parallelism to Scale Software Routers. In Proceedings of SOSP, 2009.
- [18] C. Dovrolis, P. Ramanathan, and D. Moore. What Do Packet Dispersion Techniques Measure? In INFOCOM '01.
- [19] C. Dovrolis, P. Ramanathan, and D. Moore. Packet-Dispersion Techniques and a Capacity-Estimation Methodology. IEEE/ACM Trans. Netw., 12(6):963-977, 2004.
- [20] S. A. Heimlich. Traffic Characterization of the NSFNET National Backbone. SIGMETRICS Perform. Eval. Rev., 18(1):257-258, 1990.
- [21] V. Jacobson. Congestion Avoidance and Control. SIGCOMM Comput. Commun. Rev., 25(1):157-187, 1995.
- [22] M. Jain and C. Dovrolis. End-to-End Available Bandwidth: Dynamics, and Relation with TCP Throughput. IEEE/ACM Tr. Net., 11(4):537-549, 2003.
- [23] R. Kapoor, L.-J. Chen, L. Lao, M. Gerla, and M. Y. Sanadidi. CapProbe: A Simple and Accurate Capacity Estimation Technique. SIGCOMM Compo Comm. Rev., 34(4):67-78, 2004.
- [24] D. A. Lifka. Director, Center for Advanced Computing, Cornell University. Private Communication, 2008.
- [25] J. C. Mogul and K. K. Ramakrishnan. Eliminating Livelock in an Interrupt-Driven Kernel. ACM Trans. Comput. Syst., 15(3):217-252, 1997.
- [26] M. Murray, S. Smallen, O. Khalili, and M. Swany. Comparison of End-to-End Bandwidth Measurement Tools on the 10GigE TeraGrid Backbone. In Proceedings of GRID '05.
- [27] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP Throughput: A Simple Model and Its Empirical Validation. SIGCOMM Compo Comm. Rev., 28(4):303-314, 1998.
- [28] R. Prasad, M. Jain, and C. Dovrolis. Effects of Interrupt Coalescence on Network Measurements. In PAM'04.
- [29] R. S. Prasad, M. Murray, C. Dovrolis, and K. Claffy. Bandwidth Estimation: Metrics, Measurement Techniques, and Tools. IEEE Network, 17:27-35, 2003.
- [30] V. J. Ribeiro, R. H. Riedi, R. L. Cottrell, G. Baraniuk, J. Navratil, and R. Prasad. pathChirp: Efficient Available Bandwidth Estimation for Network Paths. In PAM'03 Workshop.
- [31] D. Sanghi, A. K. Agrawala, O. Gudmundsson, and B. N. Jain. Experimental Assessment of End-to-End Behavior on the Internet. In Proc. IEEE INFOCOM '93.
- [32] S. C. Simms, G. G. Pike, and D. Balog. Wide Area Filesystem Performance Evaluation using Lustre on the TeraGrid. In Teragrid Conference, 2007.
- [33] A. Tirumala, F. Qin, J. Dugan, J. Ferguson, and K. Gibbs. Iperf - The TCPIUDP Bandwidth Measurement Tool. 2004.
- [34] S. Wallace. Lambda Networking. Advanced Network Management Lab, Indiana University.
- [35] H. Weatherspoon, L. Ganesh, T. Marian, M. Balakrishnan, and K. Birman. Smoke and Mirrors: Shadowing Files at a Geographically Remote Location Without Loss of Performance. In Proceedings of FAST, Feb. 2009.
- [36] P. Wefel. Network Engineer, National Center For Supercomputing Applications (NCSA), University of Illinois. Private Communication, 2007.

This revised version aims to provide a clearer, more coherent, and professional presentation of the content.