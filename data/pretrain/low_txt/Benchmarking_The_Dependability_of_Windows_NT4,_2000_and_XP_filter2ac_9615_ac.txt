### 3.2.1 Sensitivity to Parameter Corruption Technique

The sensitivity of the robustness measure to the parameter corruption technique can be further analyzed using a bit-flip parameter corruption technique, referred to as FL4. This technique was applied to corrupt the same set of 75 parameters in a systematic way (i.e., flipping the 32 bits of each parameter). This process generated 2400 corrupted values, corresponding to 2400 experiments. The results are presented in Figure 4 for Windows 2000.

Figure 4 shows that the OS robustness is very similar when using the two parameter corruption techniques, which confirms our previous work on fault representativeness [20].

We conclude that the results obtained for a subset of system calls related to the most frequently used functions of Windows (corresponding to Processes and Threads, File and Configuration Manager, Input/Output, and Memory Management) are similar to those obtained when considering all system calls. Therefore, we have targeted these four functions for the Windows family.

**Figure 4: Sensitivity to Corruption Technique**

| Outcome | Selective Substitution (552 exp.) | Systematic Bit-Flip (2400 exp.) |
|---------|----------------------------------|---------------------------------|
| OS Hang/Panic | 11.4% | 10.6% |
| OS Exception | 0.0% | 0.0% |
| OS Error Code | 34.1% | 44.0% |
| No Signaling | 54.5% | 45.4% |

### 3.2.2 OS Reaction Time

Table 6 complements the information provided in Table 3 by detailing the OS reaction time with respect to OS outcomes after the execution of a corrupted system call. It can be observed that:
- The time to issue an error code is very short and comparable across the three systems.
- The time to signal an exception is higher than that of error code return but still acceptable for Windows NT4 and XP, although it is very large for Windows 2000.
- The longest execution time is observed when the OS does not signal the error (SNS).

**Table 6: Detailed OS Reaction Times**

| OS Outcome | Windows 2000 | Windows NT4 | Windows XP |
|------------|---------------|--------------|-------------|
| **Error Code** | Mean: 28 µs, SD: 22 µs | Mean: 18 µs, SD: 17 µs | Mean: 17 µs, SD: 23 µs |
| **Exception** | Mean: 2978 µs, SD: 138 µs | Mean: 86 µs, SD: 973 µs | Mean: 162 µs, SD: 108 µs |
| **No Signaling** | Mean: 281 µs, SD: 2013 µs | Mean: 203 µs, SD: 4147 µs | Mean: 165 µs, SD: 204 µs |

The very high standard deviation (SD) is due to a large variation around the mean. For example, Figure 5 shows this variation in the case of SNS, identifying the system calls that led to SNS along with their mean execution times. The large standard deviation is mainly due to two system calls.

**Figure 5: OS Reaction Time in Case of SNS**

### 3.2.3 OS Restart Time

A careful analysis of the collected data revealed a correlation between the system restart time and the state of the workload. When the workload is completed, the mean restart time is very close to \( \tau_{\text{res}} \) (obtained without fault injection), and when the workload is aborted or hangs, the restart time is 8% to 18% higher. Specifically, the number of experiments that led to workload abort/hang was 101, 107, and 128 for Windows NT4, 2000, and XP, respectively. Despite Windows XP having more workload abort/hang outcomes, it still has the lowest system restart time, as indicated in Table 7.

**Table 7: Restart Time and Workload State**

| Metric | Windows NT4 | Windows 2000 | Windows XP |
|--------|--------------|---------------|-------------|
| \( \tau_{\text{res}} \) | 92 s | 96 s | 95 s |
| \( T_{\text{res}} \) | 102 s | 105 s | 109 s |
| \( T_{\text{res}} \) after WL completion | 106 s | 123 s | 90 s |
| \( T_{\text{res}} \) after WL abort/hang | 74 s | 80 s | 76 s |

### 4. Conclusion

In this paper, we briefly presented a dependability benchmark for operating systems and an example of an implementation prototype. We then used the prototype to benchmark Windows NT4, 2000, and XP.

The benchmark addresses the user perspective, treating the OS as a black box and requiring only its description in terms of services and functions (system calls). We emphasize the OS robustness regarding application-induced erroneous behavior.

The comparison of the three OSs showed that:
- They are equivalent from the robustness point of view.
- Windows XP has the shortest reaction and restart times.

Detailed information provided by the current benchmark prototype allowed refinement of the benchmark measures and confirmed the benchmark measure results. Sensitivity analyses with respect to the parameter corruption technique showed that, even though the robustness of each OS is slightly impacted by the technique used, the three OSs are impacted similarly.

Finally, the results obtained showed that using a reduced set of experiments (113) targeting only out-of-range data led to results similar to those obtained from the 552 initial experiments targeting additionally incorrect data and addresses. If this is confirmed for other OS families, it would reduce the benchmark execution duration (which is proportional to the number of experiments) by almost 5, which is substantial.

### References

[1] T. K. Tsai, R. K. Iyer, and D. Jewitt, “An Approach Towards Benchmarking of Fault-Tolerant Commercial Systems,” in Proc. 26th Int. Symp. on Fault-Tolerant Computing (FTCS-26), Sendai, Japan, 1996, pp. 314-323.

[2] A. Brown, “Availability Benchmarking of a Database System,” EECS Computer Science Division, University of California at Berkley, 2002.

[3] J. Zhu, J. Mauro, and I. Pramanick, “R3 - A Framework for Availability Benchmarking,” in Int. Conf. on Dependable Systems and Networks (DSN 2003), San Francisco, CA, USA, 2003, pp. B-86-87.

[4] K. Kanoun, J. Arlat, D. J. G. Costa, M. Dal Cin, P. Gil, J.-C. Laprie, H. Madeira, and N. Suri, “DBench – Dependability Benchmarking,” in Supplement of the 2001 Int. Conf. on Dependable Systems and Networks (DSN-2001), Göteborg, Sweden, 2001, pp. D.12-15.

[5] K. Kanoun, H. Madeira, Y. Crouzet, M. Dal Cin, F. Moreira, and Ruiz J.-C, “DBench Dependability Benchmarks,” LAAS-report no. 04-120, 2004.

[6] A. Kalakech, T. Jarboui, A. Arlat, Y. Crouzet, and K. Kanoun, “Benchmarking Operating Systems Dependability: Windows as a Case Study,” in 2004 Pacific Rim International Symposium on Dependable Computing (PRDC 2004), Papeete, Polynesia, 2004, pp. 262-271.

[7] A. Mukherjee and D. P. Siewiorek, “Measuring Software Dependability by Robustness Benchmarking,” IEEE Transactions of Software Engineering, vol. 23, no. 6, pp. 366-378, 1997.

[8] P. Koopman and J. DeVale, “Comparing the Robustness of POSIX Operating Systems,” in Proc. 29th Int. Symp. on Fault-Tolerant Computing (FTCS-29), Madison, WI, USA, 1999, pp. 30-37.

[9] C. Shelton, P. Koopman, and K. Devale, “Robustness Testing of the Microsoft Win32 API,” in Int. Conference on Dependable Systems and Networks (DSN’2000), New York, NY, USA, 2000, pp. 261-270.

[10] J. Durães and H. Madeira, “Characterization of Operating Systems Behavior in the Presence of Faulty Drivers through Software Fault Emulation,” in 2002 Pacific Rim Int. Sym. on Dependable Computing, Tsukuba City, Ibaraki, Japan, 2002, pp. 201-209.

[11] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An Empirical Study of Operating Systems Errors,” in Proc. 18th ACM Symp. on Operating Systems Principles (SOSP-2001), Banff, AL, Canada, 2001, pp. 73-88.

[12] A. Albinet, J. Arlat, and J.-C. Fabre, “Characterization of the Impact of Faulty Drivers on the Robustness of the Linux Kernel,” in Int. Conf. on Dependable Systems and Networks (DSN 2004), Florence, Italy, 2004.

[13] D. A. Solomon and M. E. Russinovich, Inside Microsoft Windows 2000, Third Edition, 2000.

[14] TPC-C, TPC Benchmark C, Standard Specification 5.1, available at http://www.tpc.org/tpcc/. 2002.

[15] M. Vieira and H. Madeira, “Definition of Faultloads Based on Operator Faults for DBMS Recovery Benchmarking,” in 2002 Pacific Rim International Symposium on Dependable Computing, Tsukuba city, Ibaraki, Japan, 2002.

[16] K. Buchacker, M. Dal Cin, H. J. Höxer, R. Karch, V. Sieh, and O. Tschäche, “Reproducible Dependability Benchmarking Experiments Based on Unambiguous Benchmark Setup Descriptions,” in Int. Conf. on Dependable Systems and Networks, San Francisco, Ca, 2003, pp. 469-478.

[17] P. J. Koopman, J. Sung, C. Dingman, D. P. Siewiorek, and T. Marz, “Comparing Operating Systems Using Robustness Benchmarks,” in Proc. 16th Int. Symp. on Reliable Distributed Systems (SRDS-16), Durham, NC, USA, 1997, pp. 72-79.

[18] G. Hunt and D. Brubaher, “Detours: Binary Interception of Win32 Functions,” in 3rd USENIX Windows NT Symposium, Seattle, Washington, USA, 1999, pp. 135-144.

[19] M. Vieira and H. Madeira, “A Dependability Benchmark for OLTP Application Environments,” in 29th Int. Conference on Very Large Data Bases (VLDB 2003), Berlin, Germany, 2003, pp. 742-753.

[20] T. Jarboui, J. Arlat, Y. Crouzet, K. Kanoun, and T. Marteau, “Analysis of the Effects of Real and Injected Software Faults: Linux as a Case Study,” in 2002 Pacific Rim Int. Symposium on Dependable Computing (PRDC 2002), Tsukuba, Japan, 2002, pp. 51-58.