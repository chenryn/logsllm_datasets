### Options During Normal Boot Up

During the normal boot process, certain values are used by the hypervisor to correctly initialize the system. Therefore, ReHype recovery must reuse the previously logged boot line options to ensure the hypervisor boots up correctly.

### Implementation Complexity

The primary difference in implementation complexity between NiLiHype and ReHype lies in the code that executes during the recovery phase. ReHype requires significantly more code to preserve and reintegrate the failed hypervisor state into a new hypervisor instance.

### Related Work

NiLiHype is related to various works aimed at increasing the resilience of OS kernels and hypervisors. Most research on OS kernels has focused on partitioning the kernel, isolating these partitions (fault domains) from each other, and recovering failed partitions without requiring a full system reboot [5], [9], [12], [23], [25]. This is often facilitated by an underlying design based on a small microkernel and a collection of drivers and servers isolated by memory-management hardware [5], [9], [12]. In contrast, microreset, and thus NiLiHype, are designed for monolithic kernels and hypervisors.

#### VirtuOS

VirtuOS [25] proposes vertical slicing of the Linux kernel into service domains, with encapsulation performed using virtualization based on Xen. User processes interact directly with the appropriate service domain, which is isolated from the rest of the kernel. Applying this idea to a hypervisor would require nested virtualization, which introduces additional overhead.

#### Akeso

Akeso [23] dynamically partitions the Linux kernel into request-oriented recovery domains. Each domain is formed by the execution thread handling a request, such as a system call or interrupt. A modified compiler instruments the code to track state changes and dependencies among domains. When an error is detected, the affected domain and dependent domains are rolled back. The performance overhead of Akeso ranges from 8% to 560% due to code instrumentation.

NiLiHype can be seen as a lightweight version of Akeso. Upon detecting an error, NiLiHype abandons the affected execution thread and all current threads, rather than tracking dependencies. Unlike Akeso, NiLiHype does not require a modified compiler and instead relies on manual modifications to the Xen code to handle non-idempotent hypercalls. This results in lower performance overhead, although a direct comparison would require applying Akeso to Xen and measuring with identical workloads. Since NiLiHype does not include comprehensive state change tracking, its recovery rate is likely lower.

#### Other Approaches

Yoshimura et al. [31], [32] proposed a method to recover from some errors in the Linux kernel without a full reboot. Their scheme involves killing a running process and achieved a 60% recovery success rate. In contrast, NiLiHype achieves over 88% recovery success, with no AppVM lost in over 83% of cases.

Otherworld [10] allows the Linux kernel to be recovered from failures by a full reboot while preserving the states of running processes. This can be viewed as a microreboot [7] of the kernel. However, it requires rebuilding many kernel data structures, increasing the chance of failed recoveries. User-level processes often need custom crash procedures to resume execution properly.

TinyChecker [28] uses nested virtualization to manage hardware-enforced protection domains during hypervisor execution. It monitors transitions between the hypervisor and VMs, limits writable memory regions, and takes checkpoints for recovery. TinyChecker has not been fully implemented or evaluated, and nested virtualization is expected to introduce significant overhead.

ReHype [19], [21] uses microreboot for hypervisor recovery and is closely related to NiLiHype. Comparisons between ReHype and NiLiHype are discussed throughout the paper. Other works use virtualization to provide resilience to device driver failures [15], [17] and PrivVM failures [20]. These components, along with the hypervisor, form the virtualization infrastructure (VI), which can be combined with middleware to provide a resilient platform for applications and services [18], [21].

### Conclusions and Future Work

Enhancing hypervisors with the ability to recover from failures while allowing hosted VMs to resume normal operation without data loss is crucial. This reduces the fraction of datacenter capacity unavailable due to a single fault, provides greater flexibility in VM assignment, and makes VM replication on a single host an attractive option.

ReHype, based on microreboot, has been presented as a mechanism for hypervisor recovery. This paper investigates an alternative, microreset, which resets the component to a quiescent state without rebooting. Microreset is suitable for large, complex components that process requests from the system, discarding all execution threads within the component. By avoiding a full reboot, microreset can achieve significantly lower recovery latencies.

NiLiHype uses microreset for hypervisor recovery and includes enhancements to restore the hypervisor to a valid consistent state. We have implemented and evaluated NiLiHype, achieving a recovery rate of over 88%, with a recovery latency of 22ms, and a performance overhead of under 1% during normal operation. The implementation required adding or modifying less than 2200 lines in the Xen hypervisor.

Future work will explore the applicability of microreset to other components, evaluate NiLiHype with more complex configurations, and investigate systematic techniques for enhancing recovery rates.

### References

[1] “Cloc – count lines of code,” http://cloc.sourceforge.net/, accessed: 2017-11-13.
[2] “Unixbench,” https://github.com/kdlucas/byte-unixbench, accessed: 2017-10-12.
[3] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warfield, “Xen and the art of virtualization,” in 19th ACM Symposium on Operating Systems Principles, Bolton Landing, NY, Oct. 2003, pp. 164–177.
[4] M. Ben-Yehuda, M. D. Day, Z. Dubitzky, M. Factor, N. Har’El, A. Gordon, A. Liguori, O. Wasserman, and B.-A. Yassour, “The Turtles project: Design and implementation of nested virtualization,” in 9th USENIX Conference on Operating Systems Design and Implementation, Vancouver, BC, Canada, Oct. 2010, pp. 423–436.
[5] K. Bhat, D. Vogt, E. van der Kouwe, B. Gras, L. Sambuc, A. S. Tanenbaum, H. Bos, and C. Giuffrida, “OSIRIS: efficient and consistent recovery of compartmentalized operating systems,” in 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, Toulouse, France, Jun. 2016, pp. 25–36.
[6] J. Buell, D. Hecht, J. Heo, K. Saladi, and H. R. Taheri, “Methodology for performance analysis of VMware vSphere under tier-1 applications,” VMware Technical Journal, vol. 2, no. 1, pp. 19–28, Jun. 2013.
[7] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox, “Microreboot — a technique for cheap recovery,” in 6th Symposium on Operating Systems Design and Implementation, San Francisco, CA, Dec. 2004, pp. 31–44.
[8] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson, and A. Warfield, “Remus: High availability via asynchronous virtual machine replication,” in 5th USENIX Symposium on Networked Systems Design and Implementation, San Francisco, CA, Apr. 2008, pp. 161–174.
[9] F. M. David, E. M. Chan, J. C. Carlyle, and R. H. Campbell, “CuriOS: improving reliability through operating system structure,” in 8th USENIX Conference on Operating Systems Design and Implementation, San Diego, California, Dec. 2008, pp. 59–72.
[10] A. Depoutovitch and M. Stumm, “Otherworld: Giving applications a chance to survive OS kernel crashes,” in 5th European conference on Computer systems, Paris, France, Apr. 2010, pp. 181–194.
[11] J. Gray, “Why do computers stop and what can be done about it?” in 5th Symposium on Reliability in Distributed Software and Database Systems, Jan. 1986, pp. 3–12.
[12] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum, “Construction of a highly dependable operating system,” in Sixth European Dependable Computing Conference, Coimbra, Portugal, Oct. 2006.
[13] Y. Huang, C. Kintala, N. Kolettis, and N. D. Fulton, “Software rejuvenation: Analysis, module and applications,” in 25th Fault-Tolerant Computing Symposium, Pasadena, CA, Jun. 1995, pp. 381–390.
[14] C. M. Jeffery and R. J. Figueiredo, “A flexible approach to improving system reliability with virtual lockstep,” IEEE Transactions on Dependable and Secure Computing, vol. 9, no. 1, pp. 2–15, Jan. 2012.
[15] H. Jo, H. Kim, J.-W. Jang, J. Lee, and S. Maeng, “Transparent fault tolerance of device drivers for virtual machines,” IEEE Transactions on Computers, vol. 59, no. 11, pp. 1466–1479, Nov. 2010.
[16] K. Kourai and S. Chiba, “A fast rejuvenation technique for server consolidation with virtual machines,” in 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, Edinburgh, UK, Jun. 2007, pp. 245–255.
[17] M. Le, A. Gallagher, Y. Tamir, and Y. Turner, “Maintaining network QoS across NIC device driver failures using virtualization,” in 8th IEEE International Symposium on Network Computing and Applications, Cambridge, MA, Jul. 2009, pp. 195–202.
[18] M. Le, I. Hsu, and Y. Tamir, “Resilient virtual clusters,” in 17th IEEE Pacific Rim International Symposium on Dependable Computing, Pasadena, CA, Dec. 2011, pp. 214–223.
[19] M. Le and Y. Tamir, “ReHype: enabling VM survival across hypervisor failures,” in 7th ACM International Conference on Virtual Execution Environments, Newport Beach, CA, Mar. 2011, pp. 63–74.
[20] ——, “Applying microreboot to system software,” in IEEE International Conference on Software Security and Reliability, Washington, D.C., Jun. 2012, pp. 11–20.
[21] ——, “Resilient virtualized systems using ReHype,” UCLA Computer Science Department Technical Report #140019, Oct. 2014.
[22] ——, “Fault injection in virtualized systems – challenges and applications,” IEEE Transactions on Dependable and Secure Computing, vol. 12, no. 3, pp. 284–297, May 2015.
[23] A. Lenharth, V. S. Adve, and S. T. King, “Recovery domains: An organizing principle for recoverable operating systems,” in 14th International Conference on Architectural Support for Programming Languages and Operating Systems, Washington, DC, USA, Mar. 2009, pp. 49–60.
[24] W. T. Ng and P. M. Chen, “The systematic improvement of fault tolerance in the Rio file cache,” in 29th Annual International Symposium on Fault-Tolerant Computing, Madison, WI, Jun. 1999, pp. 76–83.
[25] R. Nikolaev and G. Back, “VirtuOS: an operating system with kernel virtualization,” in 24th ACM Symposium on Operating Systems Principles, Farmington, PA, Nov. 2013, pp. 116–132.
[26] H. P. Reiser, F. J. Hauck, R. Kapitza, and W. Schroder-Preikschat, “Hypervisor-based redundant execution on a single physical host,” in 6th European Dependable Computing Conference, Supplemental Volume, Coimbra, Portugal, Oct. 2006, pp. 67–68.
[27] M. Rosenblum and T. Garfinkel, “Virtual machine monitors: Current technology and future trends,” IEEE Computer, vol. 38, no. 5, pp. 39–47, May 2005.
[28] C. Tan, Y. Xia, H. Chen, and B. Zang, “TinyChecker: transparent protection of VMs against hypervisor failures with nested virtualization,” in 2nd International Workshop on Dependability of Clouds, Data Centers and Virtual Machine Technology, Boston, MA, Jun. 2012.
[29] VMware, “Providing fault tolerance for virtual machines,” https://pubs.vmware.com/vsphere-4-esx-vcenter/topic/com.vmware.vsphere.availability.doc 41/c ft.html, accessed: 2017-12-01.
[30] K. Yamakita, H. Yamada, and K. Kono, “Phase-based reboot: Reusing operating system execution phases for cheap reboot-based recovery,” in 41st Annual IEEE/IFIP International Conference on Dependable Systems and Networks, Hong Kong, China, Jun. 2011, pp. 169–180.
[31] T. Yoshimura, H. Yamada, and K. Kono, “Can Linux be rejuvenated without reboots?” in IEEE Third International Workshop on Software Aging and Rejuvenation, Hiroshima, Japan, Nov. 2011, pp. 50–55.
[32] ——, “Is Linux kernel Oops useful or not?” in Eighth USENIX Workshop on Hot Topics in System Dependability, Hollywood, CA, Oct. 2012, pp. 1–6.
[33] W. Zhang and W. Zhang, “Linux virtual server clusters,” Linux Magazine, vol. 5, no. 11, Nov. 2003.