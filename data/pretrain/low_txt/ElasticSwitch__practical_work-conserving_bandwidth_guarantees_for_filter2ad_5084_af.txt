### CPU Overhead Analysis

ElasticSwitch introduces overhead both in the kernel and user space. Since the tested applications also consume CPU resources, we focus on comparing the CPU usage between ElasticSwitch and NoProtection under identical conditions. Estimating the overhead of an Oktopus-like reservation system is challenging due to its potential for multiple implementations.

### Figure 11: HTTP Request Completion Time vs. Background Traffic

- **No Protection**
- **Oktopus-like Reservation (Res.)**
- **ElasticSwitch**

**Completion Time (s)**
- 2.0
- 1.8
- 1.6
- 1.4
- 1.2
- 1.0
- 0.8
- 0.6
- 0.4
- 0.2
- 0.0

**Number of VM-to-VM Flows in Background**
- 0
- 5
- 20
- 23 (Tight Guarantees)

### Figure 12: CPU Overhead vs. Number of Active Flows

- **No Protection**
- **Oktopus-like Res.**
- **ElasticSwitch**

**CPU Usage (%)**
- 100
- 80
- 60
- 40
- 20
- 0

**Active VM-to-VM Flows**
- 0
- 50
- 15ms RA 60ms GP
- 30ms RA 100ms GP
- 100
- 150
- 200
- 250
- 300

### Analysis of CPU Overhead

Figure 12 illustrates the CPU overhead of ElasticSwitch compared to NoProtection, measured in terms of a single CPU core's capacity (our implementation is multi-threaded). The number of active VM-to-VM flows varies, and these results are from the many-to-one experiment where VMs X and Y are on the same server (as shown in Fig. 10(a)).

The overhead depends on the frequency of applying Guarantee Periods (GP) and Rate Adjustment (RA). Two different sets of periods are plotted. Profiling indicates that most CPU cycles are spent reading and setting rate-limiters, with the overhead increasing nonlinearly after a certain number of limiters.

ElasticSwitch's additional CPU overhead can be managed with one CPU core in typical cases, though our testbed uses older generation CPUs. We believe this overhead can be significantly reduced in the future, for example, by using an improved rate-limiting library.

### Control Traffic

ElasticSwitch uses two types of control packets: remote guarantees and congestion feedback, each with a minimum Ethernet packet size (64B). For a GP period of 60ms (used in our prototype), ElasticSwitch sends approximately 17 control packets per second for each active VM-to-VM flow. For instance, if there are 100 communicating VM-to-VM pairs on one server, the traffic overhead for sending/receiving remote guarantees is ~850Kbps. 

In the current implementation, ElasticSwitch sends one congestion feedback control packet per congestion event, limited to at most one message per 0.5ms. Since ElasticSwitch only detects packets lost inside the network (not in the queue of the sending host) and RA keeps buffers free, this traffic is minimal, typically on the order of a few Kbps.

### ECN (Explicit Congestion Notification)

Our experience with a single ECN-capable switch setup suggests:
1. ECN improves results for enforcing guarantees and being work-conserving.
2. Our improvements to Seawall’s algorithm are unnecessary in ECN-capable networks.

In the many-to-one experiment, ECN provided ideal results with very little variance (up to 100 senders). For the experiment of borrowing bandwidth from a bounded flow (Fig. 2 and Fig. 10(c)), ECN would improve results, similar to the more aggressive ElasticSwitch in Fig. 10(c). Due to space constraints, these results are not plotted.

### Related Work

- **Oktopus [4]**: Provides predictable bandwidth guarantees for tenants in cloud datacenters. However, it is not work-conserving and has limited scalability due to its centralized approach.
- **SecondNet [11]**: Offers VM-to-VM bandwidth guarantees but is not work-conserving and requires switches with MPLS support.
- **Gatekeeper [20] and EyeQ [12]**: Use the hose model, are fully implemented in hypervisors, and are work-conserving. They assume a congestion-free core, which is often not the case in current datacenters.
- **FairCloud [18]**: Proposes bandwidth-sharing policies but requires expensive hardware support in switches and works only for tree topologies.
- **Seawall [22] and NetShare [13]**: Ensure fair sharing of congested links but do not provide bandwidth guarantees.
- **Proteus [27]**: Proposes a Temporally-Interleaved Virtual Cluster (TIVC) model, suitable for specific applications but not workload-agnostic.
- **Hadrian [5]**: Focuses on inter-tenant communication but requires dedicated switch support, currently unavailable in today’s hardware.

### Discussion and Future Work

- **ElasticSwitch on Other Topologies**: Can be used on any single-path routing topology, respecting admission control criteria. It can also be applied to multi-path topologies with uniform load balancing, such as fat-trees [1] or VL2 [10].
- **Multi-Path Topologies with Non-Uniform Load Balancing**: ElasticSwitch could be extended to use three control layers: guarantee partitioning, path partitioning, and rate allocation.
- **Prioritizing Control Messages**: To ensure efficiency, control messages should be prioritized, e.g., using dedicated switch hardware queues.
- **Limitations**: Endpoint-only solutions for bandwidth guarantees are limited by shared queues, which can be exploited by malicious tenants.
- **Beyond the Hose Model**: The hose model is simple but may be inefficient for applications with localized communication. ElasticSwitch can support more complex abstractions, but hierarchical models require coordination across VMs.

### Conclusion

ElasticSwitch is a practical approach for implementing work-conserving minimum bandwidth guarantees in cloud computing infrastructures. It can be fully implemented in hypervisors, operates independently without a centralized controller, and works with commodity switches and various topologies. Through our implementation and testbed evaluation, we demonstrate that ElasticSwitch achieves its goals under worst-case traffic scenarios with minimal overhead.

### Acknowledgments

We thank the anonymous reviewers and our shepherd, Jitu Padhye, for their guidance on the paper.

### References

[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center network architecture. In SIGCOMM. ACM, 2008.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center Networks. In NSDI, 2010.
[3] H. Ballani, P. Costa, T. Karagiannis, et al. The price is right: Towards location-independent costs in datacenters. In Hotnets, 2011.
[4] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. Towards Predictable Datacenter Networks. In ACM SIGCOMM, 2011.
[5] H. Ballani, K. Jang, T. Karagiannis, C. Kim, D. Gunawardena, et al. Chatty Tenants and the Cloud Network Sharing Problem. NSDI’13.
[6] T. Benson, A. Akella, and D. A. Maltz. Network traffic characteristics of data centers in the wild. In IMC. ACM, 2010.
[7] P. Bodik, I. Menache, M. Chowdhury, P. Mani, D. Maltz, and I. Stoica. Surviving Failures in Bandwidth-Constrained Datacenters. In SIGCOMM, 2012.
[8] J. Crowcroft and P. Oechslin. Differentiated end-to-end Internet services using a weighted proportional fair sharing TCP. SIGCOMM CCR, July 1998.
[9] N. G. Duffield, P. Goyal, A. G. Greenberg, P. Mishra, K. Ramakrishnan, and J. E. van der Merwe. A Flexible Model for Resource Management in VPNs. In SIGCOMM, 1999.
[10] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible Data Center Network. ACM SIGCOMM, 2009.
[11] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu, and Y. Zhang. Secondnet: a data center network virtualization architecture with bandwidth guarantees. In CoNEXT. ACM, 2010.
[12] V. Jeyakumar, M. Alizadeh, D. Mazières, B. Prabhakar, C. Kim, and A. Greenberg. EyeQ: Practical Network Performance Isolation at the Edge. In USENIX NSDI, 2013.
[13] T. Lam, S. Radhakrishnan, A. Vahdat, and G. Varghese. NetShare: Virtualizing Data Center Networks across Services. UCSD TR, 2010.
[14] J. Lee, M. Lee, L. Popa, Y. Turner, P. Sharma, and B. Stephenson. CloudMirror: Application-Aware Bandwidth Reservations in the Cloud. HotCloud, 2013.
[15] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C. Mogul. SPAIN: COTS data-center Ethernet for multipathing over arbitrary topologies. In USENIX NSDI, 2010.
[16] T. Nandagopal, K.-W. Lee, J.-R. Li, and V. Bharghavan. Scalable Service Differentiation using Purely End-to-End Mechanisms: Features and Limitations. Computer Networks, 44(6), 2004.
[17] B. Pfaff, J. Pettit, K. Amidon, M. Casado, T. Koponen, et al. Extending Networking into the Virtualization Layer. In HotNets’09.
[18] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy, S. Ratnasamy, and I. Stoica. FairCloud: Sharing the Network in Cloud Computing. In ACM SIGCOMM, 2012.
[19] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley. Improving Datacenter Performance and Robustness with Multipath TCP. In ACM SIGCOMM, 2011.
[20] H. Rodrigues, J. R. Santos, Y. Turner, P. Soares, and D. Guedes. Gatekeeper: Supporting bandwidth guarantees for multi-tenant datacenter networks. In USENIX WIOV, 2011.
[21] R. Sherwood, G. Gibb, K.-K. Yap, M. Casado, N. Mckeown, et al. Can the production network be the testbed? In OSDI, 2010.
[22] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing the Data Center Network. In Usenix NSDI, 2011.
[23] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyfish: Networking Data Centers Randomly. In USENIX NSDI, 2012.
[24] Srikanth K and Sudipta Sengupta and Albert Greenberg and Parveen Patel and Ronnie Chaiken. The Nature of Datacenter Traffic: Measurements & Analysis. In IMC. ACM, 2009.
[25] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying NOX to the Datacenter. In Proc. HotNets, NY, NY, Oct. 2009.
[26] A. Venkataramani, R. Kokku, and M. Dahlin. TCP Nice: A Mechanism for Background Transfers. In OSDI, 2002.
[27] D. Xie, N. Ding, Y. C. Hu, and R. Kompela. The Only Constant is Change: Incorporating Time-Varying Network Reservations in Data Centers. In SIGCOMM, 2012.