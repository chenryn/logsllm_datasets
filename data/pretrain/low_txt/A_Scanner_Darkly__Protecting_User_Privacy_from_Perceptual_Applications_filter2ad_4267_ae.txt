### Figure 12: Impact of Privacy Level on Various Detection and Tracking Metrics

- **Detected Security Breaches (Security Cam)**
- **Detected Squares (Square Detector)**
- **Detected Contours (Ellipse Fitter)**
- **Moments (Ball Tracker)**
- **Histograms (RGB and H-S Histogram Calculators, Intensity/Contrast Changer for Images/Histograms, and H-S Histogram Backprojector)**

**Privacy Dial Value:**
- 0.0
- 1.0
- 2.0
- 3.0
- 4.0
- 5.0
- 6.0
- 7.0
- 8.0
- 9.0
- 10.0
- 11.0

**Accuracy for Tracking:**
- Measured using the Euclidean distance between the object’s original position and the reported position after applying privacy transforms.

**Correlation Between Histograms:**
- Calculated using the `cvHistCompare` function.

### Analysis of 77 Projects Using OpenCV Functions

We analyzed 77 projects to determine their usage of specific OpenCV functions, particularly those that access raw pixel data. Our findings are as follows:

- **Raw Pixel Access:**
  - 70% of the projects (54 out of 77) do not access raw pixels.
  - Only 11 projects access the network.
  - Only 2 projects access audio inputs.

- **OpenCV Function Usage:**
  - Total number of OpenCV functions called: 291
  - Supported by DARKLY prototype: 145
  - Can be supported with opaque references: 118
  - Can be supported with sketching-based declassifier: 15
  - Require porting application code to ibc: 3

- **Function Coverage:**
  - 281 functions are sufficient to support 68 out of the 77 surveyed projects.

- **Unsupported Functions:**
  - 9 projects call 10 unsupported OpenCV functions, including:
    - Optical flow: `cvCalcOpticalFlowBM`, `cvCalcOpticalFlowHS`, `cvCalcOpticalFlowLK`, `cvCalcOpticalFlowPyrLK`
    - Object tracking: `cvCamShift`, `cvMeanShift`, `cvSnakeImage`
    - Camera calibration: `ComputeCorrespondEpilines`
    - Motion analysis: `cvSegmentMotion`
    - Image segmentation: `cvWatershed`

Supporting these functions in DARKLY would require new, task-specific privacy transforms, which is an interesting area for future research.

### Related Work

- **Denning et al. [7]:**
  - Demonstrated that many consumer robots lack proper encryption and authentication, making them vulnerable to network attacks.
  - DARKLY protects users from untrusted applications running on a trusted robot.

- **PlaceRaider [25]:**
  - A hypothetical mobile malware that constructs 3-D models of environments from phone-camera images.
  - DARKLY prevents such attacks and similar ones.

- **SciFi [21]:**
  - Uses secure multiparty computation for face matching, but is too slow (10 seconds per image) for real-time applications.
  - DARKLY handles more perceptual tasks and can protect real-time video feeds.

- **Ad-hoc Methods:**
  - Google Maps' Street View blurs faces and license plates.
  - Senior et al. [23] suggested image segmentation for sensitive objects in surveillance videos.
  - Dufaux and Ebrahimi [8] proposed encrypting regions of interest in surveillance videos.
  - Chan et al. [5] developed a method for counting pedestrians without tracking individuals.

- **Sweeney et al. [11, 12, 19]:**
  - Published papers on "de-identifying" face image datasets.
  - Techniques like k-same-Eigen algorithm are similar to the generalization transform in Section VII-B.

- **Howell and Schechter [14]:**
  - Developed sensor-access widgets for displaying camera feeds.
  - Their system provides unrestricted access to visual inputs, unlike DARKLY.

- **Augmented Reality (AR) Applications:**
  - D’Antoni et al. [6] and Jana et al. [16] argue for higher-level abstractions and fine-grained permission systems for AR applications.
  - These approaches complement DARKLY's privacy protection mechanisms.

### Future Work

DARKLY is a significant step towards privacy protection for perceptual applications. Future research topics include:

1. **Evaluation:**
   - Functionality and usability on various computer-vision tasks.

2. **Object Recognition Models:**
   - Support for untrusted object recognition models and third-party services like Dextro Robotics.

3. **Privacy Transforms:**
   - Development of privacy transforms for untrusted, application-provided image-processing code.

4. **Inferential Leaks:**
   - Preventing leaks through large-scale, supervised machine learning for detecting and filtering privacy-sensitive objects and scenes.

5. **Extending to Other Perceptual Inputs:**
   - Expanding the system to handle other inputs, such as audio.

### Acknowledgments

We thank David Molnar, Scott Saponas, Ryan Calo, and Piyush Khandelwal for their contributions and support. This work was funded by NSF grant CNS-0746888, AFOSR Grant No. FA9550-08-1-0352, and a Google PhD Fellowship to Suman Jana.

### References

[References listed here as provided in the original text]

---

This optimized version provides a clear and structured presentation of the information, making it easier to understand and follow.