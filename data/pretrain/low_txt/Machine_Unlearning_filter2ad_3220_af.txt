### IX. DISCUSSION

#### Unlearning in the Absence of Isolation
SISA training borrows elements from both distributed training and ensemble learning. However, SISA training differs from ensemble learning in that each constituent model is trained in isolation. Ensemble learning approaches often use boosting algorithms [52], even for ensembles of neural networks [53], to enhance accuracy.

#### Data Replication
Empirical evidence suggests that beyond a certain data volume (i.e., shard size), there is performance degradation in each constituent model when datasets are too small or if the learning task is complex. One way to mitigate this problem is through data replication. The challenge lies in deciding which data points to replicate to increase the accuracy of the constituent models. This selection is a non-trivial problem [54]. Additionally, one must consider the likelihood of access to the replicated data point being revoked. If revocation is likely, it would be prudent to reduce the replication of such a point to limit unlearning overhead. Understanding these trade-offs is an area of future work.

#### Is All Data Useful?
Neural networks require large datasets, but not all data is equally useful [55]. Determining the importance of each data point to the final model parameters is a challenging problem. A simpler, related problem is core-set selection, where the goal is to choose a subset of the dataset that will produce a hypothesis as performant as one obtained using the entire dataset [56], [57]. Core-sets can help reduce the cost of learning and, consequently, the cost of unlearning.

#### Verified Unlearning
We assume that the service provider performs unlearning honestly. Our approach provides an intuitive and provable guarantee under the assumption that the data owner trusts the service provider due to the inherent stochasticity in learning (refer to Figure 1). To increase user confidence, the service provider could release the code. Relevant authorities enforcing the right to be forgotten could audit the code base to validate the implementation of SISA training. This is sufficient because the design of SISA training ensures that the point to be unlearned no longer influences the model parameters. However, in adversarial settings, this trust may not hold. There is currently no way to measure the influence of a data point on the model parameters, and these models are often proprietary. Therefore, exploring methods to verify the unlearning procedure, similar to approaches in other domains [58]–[60], is a worthwhile endeavor.

### X. CONCLUSIONS
Our work illustrates how to design learning algorithms that incorporate the need to later unlearn training data. We show that simple strategies like SISA training can empower users to expect their data to be completely removed from a model in a timely manner. While our work was primarily motivated by privacy, it is clear that unlearning can be a first step towards achieving model governance. We hope this will spur follow-up work on effective ways to patch models upon identifying limitations in the datasets used to train them.

### ACKNOWLEDGMENTS
We would like to thank the reviewers for their insightful feedback and Henry Corrigan-Gibbs for his service as the point of contact during the revision process. This work was supported by CIFAR through a Canada CIFAR AI Chair, and by NSERC under the Discovery Program and COHESA strategic research network. We also thank the Vector Institute's sponsors. Varun was supported in part through the following US National Science Foundation grants: CNS-1838733, CNS-1719336, CNS-1647152, CNS-1629833, and CNS-2003129.

### REFERENCES
[1] Y. Liu, K. K. Gadepalli, M. Norouzi, G. Dahl, T. Kohlberger, S. Venugopalan, A. S. Boyko, A. Timofeev, P. Q. Nelson, G. Corrado, J. Hipp, L. Peng, and M. Stumpe, “Detecting cancer metastases on gigapixel pathology images,” arXiv, Tech. Rep., 2017. [Online]. Available: https://arxiv.org/abs/1703.02442

[2] M. X. Chen, B. N. Lee, G. Bansal, Y. Cao, S. Zhang, J. Lu, J. Tsay, Y. Wang, A. M. Dai, Z. Chen et al., “Gmail smart compose: Real-time assisted writing,” arXiv preprint arXiv:1906.00080, 2019.

[3] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers et al., “Practical lessons from predicting clicks on ads at Facebook,” in Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014, pp. 1–9.

[4] S. Shalev-Shwartz et al., “Online learning and online convex optimization,” Foundations and Trends® in Machine Learning, vol. 4, no. 2, pp. 107–194, 2012.

[5] A. Mantelero, “The EU proposal for a general data protection regulation and the roots of the ‘right to be forgotten’,” Computer Law & Security Review, vol. 29, no. 3, pp. 229–235, 2013.

[6] “Bill text,” https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375.

[7] Office of the Privacy Commissioner of Canada, “Announcement: Privacy commissioner seeks federal court determination on key issue for Canadians’ online reputation,” https://www.priv.gc.ca/en/opc-news/news-and-announcements/2018/an_181010/, Oct 2018.

[8] S. Shastri, M. Wasserman, and V. Chidambaram, “The seven sins of personal-data processing systems under GDPR,” USENIX HotCloud, 2019.

[9] “Lex Access to European Union Law,” https://eur-lex.europa.eu/eli/reg/2016/679/2016-05-04.

[10] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that exploit confidence information and basic countermeasures,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015, pp. 1322–1333.

[11] N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song, “The secret sharer: Evaluating and testing unintended memorization in neural networks,” in Proceedings of the 28th USENIX Conference on Security Symposium. USENIX Association, 2019.

[12] C. Dwork, A. Roth et al., “The algorithmic foundations of differential privacy,” Foundations and Trends® in Theoretical Computer Science, vol. 9, no. 3–4, pp. 211–407, 2014.

[13] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private empirical risk minimization,” Journal of Machine Learning Research, vol. 12, no. Mar, pp. 1069–1109, 2011.

[14] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 308–318.

[15] Y. Cao and J. Yang, “Towards making systems forget with machine unlearning,” in 2015 IEEE Symposium on Security and Privacy. IEEE, 2015, pp. 463–480. [Online]. Available: https://ieeexplore.ieee.org/document/7163042/

[16] M. Kearns, “Efficient noise-tolerant learning from statistical queries,” Journal of the ACM (JACM), vol. 45, no. 6, pp. 983–1006, 1998.

[17] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph et al., “Exploiting machine learning to subvert your spam filter,” in Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats. USENIX Association, 2008.

[18] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar, “Antidote: Understanding and defending against poisoning of anomaly detectors,” in Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement, 2009.

[19] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” arXiv preprint arXiv:1206.6389, 2012.

[20] M. Kearns, “Thoughts on hypothesis boosting,” Unpublished manuscript, vol. 45, p. 105, 1988.

[21] T. Bertram, E. Bursztein, S. Caro, H. Chao, R. C. Feman et al., “Five years of the right to be forgotten,” in Proceedings of the Conference on Computer and Communications Security, 2019.

[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105.

[23] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers et al., “In-datacenter performance analysis of a tensor processing unit,” in 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2017, pp. 1–12.

[24] S. Shalev-Shwartz and S. Ben-David, Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.

[25] L. G. Valiant, “A theory of the learnable,” in Proceedings of the sixteenth annual ACM symposium on Theory of computing. ACM, 1984, pp. 436–445.

[26] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521, no. 7553, pp. 436–444, 2015.

[27] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” nature, vol. 323, no. 6088, pp. 533–536, 1986.

[28] R. D. Cook and S. Weisberg, “Characterizations of an empirical influence function for detecting influential cases in regression,” Technometrics, vol. 22, no. 4, pp. 495–508, 1980.

[29] P. W. Koh and P. Liang, “Understanding black-box predictions via influence functions,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 1885–1894.

[30] B. Kim, C. Rudin, and J. A. Shah, “The Bayesian case model: A generative approach for case-based reasoning and prototype classification,” in Advances in Neural Information Processing Systems, 2014.

[31] J. H. Saltzer and M. D. Schroeder, “The protection of information in computer systems,” Proceedings of the IEEE, vol. 63, no. 9, pp. 1278–1308, 1975.

[32] C. Dwork, “Differential privacy,” Encyclopedia of Cryptography and Security, pp. 338–340, 2011.

[33] K. Chaudhuri and C. Monteleoni, “Privacy-preserving logistic regression,” in Advances in neural information processing systems, 2009, pp. 289–296.

[34] C. Guo, T. Goldstein, A. Hannun, and L. van der Maaten, “Certified data removal from machine learning models,” arXiv preprint arXiv:1911.03030, 2019.

[35] A. Golatkar, A. Achille, and S. Soatto, “Eternal sunshine of the spotless net: Selective forgetting in deep networks,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9304–9312.

[36] A. Ginart, M. Y. Guan, G. Valiant, and J. Zou, “Making AI forget you: Data deletion in machine learning,” CoRR, vol. abs/1907.05012, 2019. [Online]. Available: http://arxiv.org/abs/1907.05012

[37] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior et al., “Large scale distributed deep networks,” in Advances in neural information processing systems, 2012.

[38] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed deep learning: An in-depth concurrency analysis,” ACM Computing Surveys (CSUR), vol. 52, no. 4, p. 65, 2019.

[39] T. G. Dietterich, “Ensemble methods in machine learning,” in International workshop on multiple classifier systems. Springer, 2000, pp. 1–15.

[40] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter, “Pegasos: Primal estimated sub-gradient solver for SVM,” Mathematical programming, vol. 127, no. 1, pp. 3–30, 2011.

[41] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.

[42] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot learning,” in Advances in neural information processing systems, 2017, pp. 4077–4087.

[43] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, pp. 2278 – 2324, 12 1998.

[44] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A Large-Scale Hierarchical Image Database,” in CVPR09, 2009.

[45] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line learning and an application to boosting,” Journal of computer and system sciences, vol. 55, no. 1, pp. 119–139, 1997.

[46] D. Opitz and R. Maclin, “Popular ensemble methods: An empirical study,” Journal of artificial intelligence research, vol. 11, pp. 169–198, 1999.

[47] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference attacks against machine learning models,” in 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017, pp. 3–18.

[48] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., “Matching networks for one shot learning,” in Advances in neural information processing systems, 2016, pp. 3630–3638.

[49] C. O. Sakar, S. O. Polat, M. Katircioglu, and Y. Kastro, “Real-time prediction of online shoppers’ purchasing intention using multilayer perceptron and LSTM recurrent neural networks,” Neural Computing and Applications, vol. 31, no. 10, pp. 6893–6908, 2019.

[50] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Ng, “Reading digits in natural images with unsupervised feature learning,” NIPS, 01 2011.

[51] A. Krizhevsky, “Learning multiple layers of features from tiny images,” 2009.

[52] R. E. Schapire, “A brief introduction to boosting,” in Ijcai, vol. 99, 1999, pp. 1401–1406.

[53] H. Schwenk and Y. Bengio, “Boosting neural networks,” Neural computation, vol. 12, no. 8, pp. 1869–1887, 2000.

[54] B. Settles, “Active learning literature survey,” University of Wisconsin-Madison Department of Computer Sciences, Tech. Rep., 2009.

[55] S.-J. Huang, R. Jin, and Z.-H. Zhou, “Active learning by querying informative and representative examples,” in Advances in neural information processing systems, 2010, pp. 892–900.

[56] C. Baykal, L. Liebenwein, I. Gilitschenski, D. Feldman, and D. Rus, “Data-dependent coresets for compressing neural networks with applications to generalization bounds,” CoRR, vol. abs/1804.05345, 2018. [Online]. Available: http://arxiv.org/abs/1804.05345

[57] O. Sener and S. Savarese, “Active learning for convolutional neural networks: A core-set approach,” arXiv preprint arXiv:1708.00489, 2017.

[58] C. Tan, L. Yu, J. B. Leners, and M. Walfish, “The efficient server audit problem, deduplicated re-execution, and the web,” in Proceedings of the 26th Symposium on Operating Systems Principles. ACM, 2017, pp. 546–564.

[59] R. S. Wahby, Y. Ji, A. J. Blumberg, A. Shelat, J. Thaler, M. Walfish, and T. Wies, “Full accounting for verifiable outsourcing,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2017, pp. 2071–2086.

[60] S. T. Setty, R. McPherson, A. J. Blumberg, and M. Walfish, “Making argument systems for outsourced computation practical (sometimes).” in NDSS, vol. 1, no. 9, 2012, p. 17.

[61] https://math.stackexchange.com/questions/786392/expectation-of-minimum-of-n-i-i-d-uniform-random-variables.

### APPENDIX

#### A. Simulation of SISA Training Time Analysis
To gain a more intuitive understanding of the unlearning time described in § V, we randomly generated \( K \) unlearning requests. We then computed the amount of data that needs to be retrained by determining the shard and slice each unlearning request maps to. We deduced the number of samples that need to be retrained to achieve unlearning through SISA training. By varying \( K \) between 1 and 500, we visualized the speed-up achieved by SISA training as a function of the number of unlearning requests made. We repeated the experiment 100 times to obtain variance. The results are plotted in Figure 11.

(a) SVHN
(b) Purchase
**Figure 11:** This plot shows the relationship between \( K \) and unlearning time (which is proportional to the amount of data to retrain) where \( S \) is shown in the legend and \( R \) is set to 20. It is plotted in log-log scale for ease of visualization.

#### B. Individual Contributions Due to Slicing and Sharding
In Equations 3 and 5, we present the unlearning cost (i.e., the number of points needed to be retrained) as functions of the number of unlearning requests, slices, and shards. We plotted the speed-up induced by SISA in Figure 7, but the number of unlearning requests is set to a constant for ease of visualization. Therefore, Figure 12 is plotted to show the effect of all three variables.

#### C. Costs Associated With Storage
The slicing introduced by SISA training trades off disk space for reduced unlearning time. This trade-off is discussed in detail in the main text.