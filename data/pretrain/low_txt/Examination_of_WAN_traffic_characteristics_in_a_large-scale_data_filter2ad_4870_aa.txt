# Examination of WAN Traffic Characteristics in a Large-scale Data Center Network

**Authors:**
- Zhaohua Wang† ‡
- Zhenyu Li† ‡ ∗
- Guangming Liu§
- Yunfei Chen§
- Qinghua Wu† ‡ ∗
- Gang Cheng§

**Affiliations:**
- †Institute of Computing Technology, Chinese Academy of Sciences
- ‡University of Chinese Academy of Sciences
- §Baidu
- ∗Purple Mountain Laboratories

**Emails:**
- {wangzhaohua, zyli, wuqinghua}@ict.ac.cn
- {liuguangming, chenyunfei, chenggang06}@baidu.com

## Abstract

Large cloud service providers have established an increasing number of geographically distributed data centers (DCs) connected by wide-area networks (WANs) to host their diverse services. While there is extensive research on WAN traffic engineering, the characteristics of WAN traffic in production DC networks are not well understood. This paper reports on the network traffic observed in Baidu’s DC network, which consists of tens of geographically distributed DCs. Baidu hosts both traditional services like web and computing, as well as emerging services such as analytics, AI, and map. We analyze WAN traffic characteristics in Baidu’s DC network from the perspectives of traffic demands, communication among DCs, and the traffic characteristics of various services. Specifically, we focus on the disparities that may exist among different types of services. We also discuss the implications of our findings for WAN traffic engineering, fabric design, and service deployment.

## CCS Concepts
- **Networks →** Network performance evaluation; Network measurement

## Keywords
- DC-WAN, traffic pattern, traffic locality, traffic stability

## ACM Reference Format
Zhaohua Wang† ‡, Zhenyu Li† ‡ ∗, Guangming Liu§, Yunfei Chen§, Qinghua Wu† ‡ ∗, Gang Cheng§. 2021. Examination of WAN Traffic Characteristics in a Large-scale Data Center Network. In ACM Internet Measurement Conference (IMC '21), November 2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3487552.3487860

## 1 Introduction

Large cloud service providers use tens of geographically distributed data centers (DCs) interconnected by wide-area networks (WANs) to host their diverse services. Services are replicated across these DCs to process users' requests locally, enhancing Quality-of-Experience (QoE). Consequently, bulk transfers flow among DCs for data synchronization or backup. For example, search engines synchronize indexes between DCs, and cloud storage services back up user data in multiple DCs for reliability. These bulk transfers often have deadlines but are not delay-sensitive [17, 19, 23]. WANs also carry delay-sensitive traffic [14], such as front-end web search servers communicating with ad services located in other DCs to consolidate responses, or distributed machine learning aggregating gradients from many DCs when raw data cannot be moved due to local regulations [15]. Delay-sensitive traffic is marked as high-priority, while bulk transfers for data synchronization are low-priority.

DC-WAN is an expensive resource, and providers must balance utilization and availability [7]. In recent years, several solutions have aimed to achieve better balance using software-defined networking constructs [14, 16, 17] or fine-grained policy enforcement [22]. The effectiveness of these approaches depends on traffic patterns. For instance, SWAN [14] and BwE [22] assume the predictability of high-priority interactive traffic. Although real inter-DC traffic traces were used for evaluation in some studies, the WAN traffic patterns of large-scale production DCs remain poorly understood.

Modern DCs also host new services, such as AI and big data analytics, which are common and important for large service providers. These services often leverage geo-distributed DCs for location-based services and distributed deep learning [15, 18, 26]. For example, map and location-based recommendation services rely on geo-distributed front-end servers for user-facing requests, which may interact with servers in other DCs for real-time road traffic information and targeted ads. These emerging services may exhibit new characteristics and change overall WAN traffic patterns. An examination of these services from the perspective of traffic characteristics will benefit the design and improvement of traffic engineering in DCNs.

Many DCN designs are motivated by measurement results. The design of VL2 [11] is driven by the volatility in traffic among servers in a DC; the traffic mix pattern and switch buffer behavior motivate the design of DCTCP [2]; the predictability of traffic at short time-scales motivates the design of MicroTE [6]. There are also measurement studies that report the nature of traffic in DCNs. Kandula et al. described the characteristics of traffic in an operational distributed query processing cluster using socket-level logs [20]. Benson et al. examined the flow characteristics, traffic locality, and link utilization of several DCs [4], and observed high locality of traffic within individual racks in cloud DCs. Roy et al. extended previous studies, primarily focused on a single DC provider, and examined traffic patterns in Facebook’s data center [27], finding different characteristics than previous measurements. These distinctions call for more reports on other DCs. It is worth noting that all these studies focus on intra-DC traffic. Chen et al. [8] studied inter-DC traffic in Yahoo!, but the scale was small (only 5 DCs) and the application mix was simpler than in cloud WANs.

This paper measures Baidu’s DCN, focusing on the traffic that flows across DCs and clusters. Baidu uses tens of geographically distributed DCs serving millions of users daily, with each DC containing multiple clusters organized through racks. Baidu is one of the largest web service providers and has recently launched AI and autonomous driving services [3]. Large-scale, geo-distributed DCs with a complex service mix make Baidu’s DCN an excellent example for examining modern DC traffic patterns.

Specifically, motivated by challenges faced by network designers and operators, we study traffic characteristics along three dimensions:
1. **Examining traffic demands:** Effective DC-WAN resource allocation depends heavily on traffic demands. We examine DC-level traffic locality and link utilization.
2. **Characterizing traffic communication:** Higher utilization of links carrying WAN traffic motivates further analysis of traffic communication between DCs, with a focus on stability.
3. **Analyzing traffic of services:** Service migration and service-level WAN bandwidth allocation require a deep understanding of traffic characteristics at the service level. We analyze service interaction and stability from a traffic perspective.

To this end, we develop and deploy a Netflow collector that runs in both core switches and data center switches in all Baidu’s DCs. We also utilize SNMP statistics from these data center switches. Our key observations include:
- Despite high replication of services in many DCs, about 20% of high-priority traffic leaving clusters still flows across DCs over WAN. This percentage varies across service categories and times of day, with emerging services (AI, Analytics, and Map) significantly deviating from traditional Web and Computing services.
- Links carrying WAN traffic experience higher utilization than those in DCs, and their loads are well balanced thanks to ECMP. Additionally, WAN traffic and DC traffic leaving from clusters in individual DCs show a high temporal correlation in terms of their incremental value, suggesting a separation of two types of traffic (WAN and DC) on two kinds of switches to avoid interference.
- Although communications are prevalent among DCs, a small portion (8.5%) of DC pairs contribute 80% of high-priority traffic, and these heavy hitters persist over time. The traffic communication within DCs is also imbalanced, with 17% of rack pairs generating 80% of the traffic.
- The aggregated high-priority traffic over WAN and the high-priority traffic exchanges among DCs remain stable over time, leading to good predictability of overall traffic demands. However, intra-DC traffic exchanged among clusters is variable, requiring fabric designs to adapt to this volatility in traffic scheduling [11].
- Our analysis reveals different interaction patterns among services (from the perspective of WAN traffic): traditional Web and Computing services heavily interact with each other, implying a close bind between them. Analytics, AI, Map, and Security services, on the other hand, distribute their traffic more evenly, indicating their fundamental contributions to other services.
- There is significant disparity in the stability of high-priority WAN traffic among services, partially impacted by service interaction patterns. Existing traffic estimation methods may perform poorly, especially for services whose traffic stability does not persist for long. These observations call for more accurate estimation methods for WAN traffic engineering at the service level.

We further discuss the implications of these findings for WAN traffic engineering, service migration and placement, network fabric design for DCs, and switch configuration. We acknowledge that, as with any large-scale empirical study, our results are subject to the limitation of considering only one DC provider. While these observations need further reexamination to confirm their generalizability, they provide a deeper understanding of traffic and service characteristics in modern DCs, particularly for DC-WAN traffic. We hope our findings can shed light on DC interconnect design, traffic engineering in DC-WAN, and service placement in DCs.

The rest of this paper is organized as follows: Section 2 provides a brief description of Baidu’s DCN and introduces the measurement methodology. We then analyze the traffic patterns across DCs and clusters with respect to traffic demands (Section 3) and traffic communication (Section 4). Finally, we provide insights into the WAN traffic characteristics in view of services (Section 5). We introduce related works in Section 6 and conclude the paper in Section 7.

## 2 Background & Data

### 2.1 Baidu’s Data Center Network

Baidu’s DC network (DCN) hosts various large-scale services, built on an infrastructure of DCs connected through high-bandwidth (Tbps) wide area networks (WANs). As shown in Figure 1, the network consists of multiple DCs connected to the WAN via core switches, forming a full-meshed core network at the overlay layer. Inside a DC, tens of clusters are connected by Tbps links through DC switches responsible for traffic inside the DC. Traffic leaving a DC flows through xDC (cross-DC) switches to the core switches. Each cluster employs either a typical 4-post structure or a Spine-Leaf Clos design. Server machines are organized into racks and connected to top-of-rack (ToR) switches. In the 4-post structure, racks are connected through cluster switches, which communicate with each other via DC switches. In the Spine-Leaf Clos structure, racks are connected through leaf switches, with leaf switches full-meshed to spine switches for inter-pod traffic. A particular set of leaf switches are dedicated to intra-DC traffic, connecting to DC switches in the DC, while another set connects to xDC switches for inter-DC traffic. Overall, Baidu’s DCN is similar to others (e.g., Facebook, Microsoft) [11, 27] from a topology perspective, but WAN connections are an expensive resource.

From a service hosting perspective, some differences are notable. First, Baidu’s DCN hosts many services not reported in other DC networks, despite the dominance of web services. These include emerging distributed AI and location-based services (e.g., Baidu Map). Second, Baidu’s DCN allows any service to run on any server, leading to a flexible setup where a physical server in Baidu’s DCs hosts one specific service, but a rack may host many types of services. This is different from Facebook’s DCN [27], where a rack deploys the same service.

### 2.2 Data Collection Methodology

This paper focuses on the traffic that crosses DCs and clusters, emphasizing the impact of new types of services. We do not capture the micro view of fine-grained flow characteristics but rather focus on aggregated traffic data. We developed and deployed a Netflow collector that runs in both core switches and data center switches in all Baidu’s DCs. We also utilized SNMP statistics from these data center switches.

#### 2.2.1 Netflow Data

Cisco’s Netflow service provides network administrators with access to summarized IP flow records within their networks [9]. Figure 2 shows the process of Netflow data collection, where Netflow data are collected from switches in Figure 1 across Baidu’s entire DCN. Specifically, we collected Netflow data from core switches for inter-DC traffic analysis and from DC switches for inter-cluster (but intra-DC) traffic analysis. The active timeout for NetFlow on all switches is set to 1 minute, meaning a Netflow record is exported every minute for long-lived flows. Each flow records the aggregated flow information obtained from sampled packet headers with a 1:1024 sampling rate. The log contains the source and destination IP addresses, transport-layer port numbers, and IP protocol. These collected flow data, along with other metadata such as collection machines’ IP addresses and capture time, are first processed by Netflow decoders, which convert each log into a CSV or JSON object. These parsed data are then streamed to Netflow integrators using a distributed subscribing and streaming system. Netflow integrators aggregate the traffic flow data at one-minute intervals and annotate it with additional attribution information such as cluster, DC, service identifications, and QoS information (indicating the priority of the flow) by querying other data sources. The service information is identified by querying a directory that maps IP addresses and port numbers to services. Netflow integrators then feed the data into Apache Doris, a fast MPP database for big data analytics [10], and Baidu CFS, a cloud file system built in Baidu for data storage and offline analysis. Netflow decoders and Netflow integrators are deployed locally in DCs for processing data collected from individual DCs, while the data analytics and storage systems are centrally deployed for processing globally collected flow data.

In total, over 10 TB of raw Netflow data from core switches and 10 TB from DC switches are generated daily. During the collection of the data used in this paper, no abnormalities in the Netflow data collection system were observed.

#### 2.2.2 SNMP Data

To investigate the link utilization of cluster-to-DC, cluster-to-xDC, and xDC-to-core links, we also collected SNMP data from the interfaces of DC switches and xDC switches in multiple DCs hosting considerable traffic volume. Every 30 seconds, the SNMP manager requests traffic statistics from DC switches and xDC switches. The collected SNMP data is streamed into time series tables and Apache Doris in Baidu for analysis and storage. We note the possible measurement inaccuracy caused by SNMP data collection, such as SNMP packet loss or delay. Records that fail to be parsed due to format issues are discarded, with the percentage of failed records being around 0.00001%.

[Figure 1: Datacenter topology]
[Figure 2: Netflow data collection architecture]

---

This optimized version of the text is more structured, coherent, and professional, making it easier to read and understand.