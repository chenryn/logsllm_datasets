### Optimized Text

An elevator setup could make such a configuration highly inconspicuous. A transfer function-independent approach, inspired by the work of Athalye et al. [2] in the image domain, can be adapted for this purpose. In their study, the loss function is designed to account for all types of rotations and translations. Similarly, the loss function for the ASR-DNN (Automatic Speech Recognition-Deep Neural Network) could be designed not only to optimize a specific transfer function but also to maximize the expectation over all possible transfer functions. However, for a realistic over-the-air attack, it is essential to thoroughly investigate all potential attack vectors and their limitations. In this work, we focus on the general and theoretical feasibility of psychoacoustic hiding, leaving over-the-air attacks for future research.

A similar attack could be envisioned for real-world applications, such as Amazon’s Alexa. However, detailed information about the system's architecture is difficult to obtain and requires extensive prior investigation. Model parameters might be retrieved using various model-stealing techniques [22], [34], [37], [49], [54]. Our reverse engineering of the firmware suggests that Amazon uses parts of Kaldi, indicating that limited knowledge of the topology and parameters may be sufficient for a black-box attack. A starting point could be the keyword recognition of commercial ASR systems, such as "Alexa," which runs locally on the device and is thus more accessible.

For image classification, universal adversarial perturbations have been successfully created [7], [29]. For ASR systems, it remains an open question whether such perturbations exist and how they can be generated.

### VIII. CONCLUSION

We have introduced a novel method for creating adversarial examples for ASR systems, explicitly considering dynamic human hearing thresholds. By leveraging the mechanisms of MP3 encoding, the audibility of added noise is significantly reduced. We performed our attack against the state-of-the-art Kaldi ASR system, feeding the adversarial input directly into the recognizer to demonstrate the general feasibility of psychoacoustic-based attacks.

By applying forced alignment and backpropagation to the DNN-HMM (Deep Neural Network-Hidden Markov Model) system, we were able to create inconspicuous adversarial perturbations with high reliability. It is generally possible to hide any target transcription within any audio file, and with the correct attack vectors, the noise can be hidden below the hearing threshold, making the changes almost imperceptible. The choice of the original audio sample, an optimal phone rate, and forced alignment provide the optimal starting point for creating adversarial examples. Additionally, we evaluated different algorithm setups, including the number of iterations and the allowed deviation from the hearing thresholds. A comparison with the approach by Yuan et al. [59], which also creates targeted adversarial examples, shows that our method requires much lower distortions. Listening tests confirmed that the target transcription was incomprehensible to human listeners. For some audio files, participants found it almost impossible to distinguish between the original and adversarial samples, even with headphones and in direct comparisons.

Future work should explore hardening ASR systems by incorporating psychoacoustic models to prevent these currently relatively easy attacks. Additionally, similar attacks should be evaluated on commercial ASR systems and in real-world settings, such as in black-box scenarios or via over-the-air attacks.

### ACKNOWLEDGMENTS

This work was supported by Intel as part of the Intel Collaborative Research Institute “Collaborative Autonomous & Resilient Systems” (ICRI-CARS). Additionally, this work was supported by the German Research Foundation (DFG) within the framework of the Excellence Strategy of the Federal Government and the States - EXC 2092 CASA.

### REFERENCES

[1] M. Asad, J. Gilani, and A. Khalid, “An enhanced Least Significant Bit modification technique for audio steganography,” in Conference on Computer Networks and Information Technology. IEEE, Jul. 2011, pp. 143–147.
[2] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing robust adversarial examples,” CoRR, vol. abs/1707.07397, pp. 1–18, Jul. 2017.
[3] K. Audhkhasi, B. Kingsbury, B. Ramabhadran, G. Saon, and M. Picheny, “Building competitive direct acoustics-to-word models for English conversational speech recognition,” 2018.
[4] M. Barni, F. Bartolini, and A. Piva, “Improved wavelet-based watermarking through pixel-wise masking,” IEEE transactions on image processing, vol. 10, no. 5, pp. 783–791, 2001.
[5] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, “The security of machine learning,” Machine Learning, vol. 81, no. 2, pp. 121–148, Nov. 2010.
[6] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar, “Can machine learning be secure?” in Symposium on Information, Computer and Communications Security. ACM, Mar. 2006, pp. 16–25.
[7] T. B. Brown, D. Mané, A. Roy, M. Abadi, and J. Gilmer, “Adversarial patch,” CoRR, vol. abs/1712.09665, pp. 1–6, Dec. 2017.
[8] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. A. Wagner, and W. Zhou, “Hidden voice commands,” in USENIX Security Symposium. USENIX, Aug. 2016, pp. 513–530.
[9] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural networks,” in Symposium on Security and Privacy. IEEE, May 2017, pp. 39–57.
[10] ——, “Audio adversarial examples: Targeted attacks on Speech-to-Text,” CoRR, vol. abs/1801.01944, pp. 1–7, Jan. 2018.
[11] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, “Houdini: Fooling deep structured prediction models,” CoRR, vol. abs/1707.05373, pp. 1–12, Jul. 2017.
[12] M. Cissé, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier, “Parseval networks: Improving robustness to adversarial examples,” in Conference on Machine Learning. PMLR, Aug. 2017, pp. 854–863.
[13] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker, Digital watermarking and steganography. Morgan kaufmann, 2007.
[14] W. Diao, X. Liu, Z. Zhou, and K. Zhang, “Your voice assistant is mine: How to abuse speakers to steal information and control your phone,” in Workshop on Security and Privacy in Smartphones & Mobile Devices. ACM, Nov. 2014, pp. 63–74.
[15] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song, “Robust physical-world attacks on machine learning models,” CoRR, vol. abs/1707.08945, pp. 1–11, Jul. 2017.
[16] A. Fawzi, O. Fawzi, and P. Frossard, “Analysis of classifiers’ robustness to adversarial perturbations,” Machine Learning, vol. 107, no. 3, pp. 481–508, Mar. 2018.
[17] M. Fujimoto, “Factored deep convolutional neural networks for noise robust speech recognition,” Proc. Interspeech, pp. 3837–3841, 2017.
[18] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” CoRR, vol. abs/1412.6572, pp. 1–11, Dec. 2014.
[19] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” CoRR, vol. abs/1503.02531, pp. 1–9, Mar. 2015.
[20] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box adversarial attacks with limited queries and information,” CoRR, vol. abs/1804.08598, pp. 1–10, Apr. 2018.
[21] ISO, “Information Technology – Coding of moving pictures and associated audio for digital storage media at up to 1.5 Mbits/s – Part 3: Audio,” International Organization for Standardization, ISO 11172-3, 1993.
[22] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and N. Asokan, “PRADA: Protecting against DNN model stealing attacks,” CoRR, vol. abs/1805.02628, pp. 1–16, May 2018.
[23] K. Kohls, T. Holz, D. Kolossa, and C. Pöpper, “SkypeLine: Robust hidden data transmission for VoIP,” in Asia Conference on Computer and Communications Security. ACM, May 2016, pp. 877–888.
[24] J. Liu, K. Zhou, and H. Tian, “Least-Significant-Digit steganography in low bitrate speech,” in International Conference on Communications. IEEE, Jun. 2012, pp. 1133–1137.
[25] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable adversarial examples and black-box attacks,” CoRR, vol. abs/1611.02770, 2016.
[26] D. Lowd and C. Meek, “Adversarial learning,” in Conference on Knowledge Discovery in Data Mining. ACM, Aug. 2005, pp. 641–647.
[27] V. Manohar, D. Povey, and S. Khudanpur, “JHU kaldi system for Arabic MGB-3 ASR challenge using diarization, audiotranscript alignment, and transfer learning,” in 2017 IEEE Automatic Speech Recognition and Understanding Workshop, Dec 2017, pp. 346–352.
[28] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, “Montreal forced aligner: trainable text-speech alignment using Kaldi,” in Proceedings of interspeech, 2017, pp. 498–502.
[29] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal adversarial perturbations,” in Conference on Computer Vision and Pattern Recognition. IEEE, Jul. 2017, pp. 86–94.
[30] T. Moynihan, “How to keep Amazon Echo and Google Home from responding to your TV,” Feb. 2017, https://www.wired.com/2017/02/keep-amazon-echo-google-home-responding-tv/, as of December 14, 2018.
[31] G. Navarro, “A guided tour to approximate string matching,” ACM Computing Surveys, vol. 33, no. 1, pp. 31–88, Mar. 2001.
[32] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,” in Conference on Computer Vision and Pattern Recognition. IEEE, Jun. 2015, pp. 427–436.
[33] M. Nutzinger, C. Fabian, and M. Marschalek, “Secure hybrid spread spectrum system for steganography in auditive media,” in Conference on Intelligent Information Hiding and Multimedia Signal Processing. IEEE, Oct. 2010, pp. 78–81.
[34] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical Black-Box attacks against machine learning,” in Asia Conference on Computer and Communications Security. ACM, Apr. 2017, pp. 506–519.
[35] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, “The limitations of deep learning in adversarial settings,” in European Symposium on Security and Privacy. IEEE, Mar. 2016, pp. 372–387.
[36] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as a defense to adversarial perturbations against deep neural networks,” in Symposium on Security and Privacy. IEEE, May 2016, pp. 582–597.
[37] N. Papernot, P. D. McDaniel, and I. J. Goodfellow, “Transferability in machine learning: From phenomena to Black-Box attacks using adversarial samples,” CoRR, vol. abs/1605.07277, pp. 1–13, May 2016.
[38] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, “The Kaldi speech recognition toolkit,” in Workshop on Automatic Speech Recognition and Understanding. IEEE, Dec. 2011.
[39] L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition. Prentice-Hall, Inc., 1993.
[40] S. Ranjan and J. H. Hansen, “Improved gender independent speaker recognition using convolutional neural network based bottleneck features,” Proc. Interspeech 2017, pp. 1009–1013, 2017.
[41] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio, “A network of deep neural networks for distant speech recognition,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2017, pp. 4880–4884.
[42] N. Roy, H. Hassanieh, and R. Roy Choudhury, “BackDoor: Making microphones hear inaudible sounds,” in Conference on Mobile Systems, Applications, and Services. ACM, Jun. 2017, pp. 2–14.
[43] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, and P. Hall, “English conversational telephone speech recognition by humans and machines,” CoRR, vol. abs/1703.02136, pp. 1–7, Mar. 2017.
[44] N. Schinkel-Bielefeld, N. Lotze, and F. Nagel, “Audio quality evaluation by experienced and inexperienced listeners,” in International Congress on Acoustics. ASA, Jun. 2013, pp. 6–16.
[45] M. Schoeffler, S. Bartoschek, F.-R. Stöter, M. Roess, S. Westphal, B. Edler, and J. Herre, “webMUSHRA – A comprehensive framework for web-based listening tests,” Journal of Open Research Software, vol. 6, no. 1, Feb. 2018.
[46] J. W. Seok and J. W. Hong, “Audio watermarking for copyright protection of digital audio data,” Electronics Letters, vol. 37, no. 1, pp. 60–61, 2001.
[47] U. Shaham, Y. Yamada, and S. Negahban, “Understanding adversarial training: Increasing local stability of supervised models through robust optimization,” Neurocomputing, 2018.
[48] L. Song and P. Mittal, “Inaudible voice commands,” CoRR, vol. abs/1708.07238, pp. 1–3, Aug. 2017.
[49] F. Tramer, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing machine learning models via prediction APIs,” in USENIX Security Symposium. USENIX, Aug. 2016, pp. 601–618.
[50] J. Trmal, M. Wiesner, V. Peddinti, X. Zhang, P. Ghahremani, Y. Wang, V. Manohar, H. Xu, D. Povey, and S. Khudanpur, “The Kaldi openKWS system: Improving low resource keyword search,” Proc. Interspeech, pp. 3597–3601, Aug. 2017.
[51] P. Upadhyaya, O. Farooq, M. R. Abidi, and Y. V. Varshney, “Continuous Hindi speech recognition model based on Kaldi ASR toolkit,” in 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET), March 2017, pp. 786–789.
[52] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine Noodles: Exploiting the gap between human and machine speech recognition,” in Workshop on Offensive Technologies. USENIX, Aug. 2015.
[53] J. Villalba, N. Brümmer, and N. Dehak, “Tied variational autoencoder backends for i-vector speaker recognition,” Interspeech, Stockholm, pp. 1005–1008, Aug. 2017.
[54] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine learning,” in Symposium on Security and Privacy. IEEE, May 2018.
[55] R. B. Wolfgang, C. I. Podilchuk, and E. J. Delp, “Perceptual watermarks for digital images and video,” Proceedings of the IEEE, vol. 87, no. 7, pp. 1108–1126, 1999.
[56] S. Wu, J. Huang, D. Huang, and Y. Q. Shi, “Self-synchronized audio watermark in DWT domain,” in 2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512), vol. 5, May 2004, pp. V–V.
[57] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “Toward human parity in conversational speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 12, pp. 2410–2423, Dec. 2017.
[58] ——, “The Microsoft 2016 conversational speech recognition system,” in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017, pp. 5255–5259.
[59] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang, H. Huang, X. Wang, and C. A. Gunter, “CommanderSong: A systematic approach for practical adversarial voice recognition,” in USENIX Security Symposium. USENIX, Aug. 2018, pp. 49–64.
[60] V. Zantedeschi, M.-I. Nicolae, and A. Rawat, “Efficient defenses against adversarial attacks,” in Workshop on Artificial Intelligence and Security. ACM, Nov. 2017, pp. 39–49.
[61] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu, “DolphinAttack: Inaudible voice commands,” in Conference on Computer and Communications Security. ACM, Oct. 2017, pp. 103–117.
[62] E. Zwicker and H. Fastl, Psychoacoustics: Facts and Models, 3rd ed. Springer, 2007.