### True Positive Rates and Contextual Information

The true positive rates remain unchanged, primarily because listening to full sentences allows the adversary to leverage valuable contextual information.

### End-to-End Case Study: Stealing Passwords from Phone Conversations

We now evaluate the proposed models with an end-to-end attack in phone conversations. We consider a real-world scenario where the victim makes a phone call to a remote caller and requests a password during the conversation. The objective of the adversary is to locate and recognize the password from the victim’s accelerometer measurements. In this attack, we assume that the password is preceded by the hot word "password (is)". As shown in Section VI-F, this attack can be easily extended to support other hot words.

#### Experimental Setup

In the experiment, we use the victim's smartphone to make phone calls to four volunteers (two females and two males) in three different scenarios:
1. **Table-Setting**: The victim's smartphone is placed on a table.
2. **Handhold-Sitting**: The victim sits on a chair and holds the smartphone in hand.
3. **Handhold-Walking**: The victim holds the smartphone in hand and walks around.

We conduct 16 scripted conversations and 4 free ones per person per scenario, resulting in a total of 240 conversations. During all conversations, the volunteers are asked to provide a random 8-digit password following the phrase "password is".

#### Data Processing and Model Training

After recording the acceleration signals for each conversation, we first convert the acceleration signals into multiple single-word spectrograms. A password search model is then used to find the spectrogram corresponding to the hot word "password". Subsequently, a digit recognition model is employed to recognize the 8-digit password that follows.

The password search model identifies the word "password" in the recorded acceleration signals using a classifier that distinguishes between "password" and other words. To train this classifier, we collected a dataset consisting of 200 "password" samples and 2200 negative samples, including digits and other words. The class imbalance problem is addressed through re-weighting the loss, as described in Section VI-F. Specifically, the loss computed by the "password" samples is weighted by a factor of 11α, while the loss computed by negative samples is weighted by a factor of α. If the model recognizes multiple "password"s in a conversation, the one with the highest confidence value is reported.

Using this model, we successfully located the password in over 85% of conversations across all scenarios, as shown in Table XI.

| Setting           | Table-Setting | Handhold-Sitting | Handhold-Walking |
|-------------------|---------------|------------------|-------------------|
| Password Search   | 92%           | 85%              | 91%               |
| Digit Recognition |               |                  |                   |
| Top-1             | 59%           | 51%              | 50%               |
| Top-3             | 84%           | 83%              | 81%               |
| Top-5             | 92%           | 94%              | 91%               |

The digit recognition model for each scenario is trained with 280 × 10 digit spectrograms. Table XI lists the overall accuracy of the recognition model. We also calculate the number of correctly inferred digits in each conversation and plot the distribution in Figure 17. It can be observed that the recognition accuracy in the phone call scenario is lower compared to the record-and-play scenario. This is primarily due to the lower quality of the audio signal transmitted during a phone call compared to the audio signal recorded by a recording application. One important observation is that the recognition model achieves over 80% top-3 recognition accuracy in all scenarios. Although the proposed attack only recognizes the complete password in a few conversations, it significantly narrows the search space for the adversary.

### Defense Mechanisms

Another effective defense is to notify the user when applications are collecting accelerometer readings in the background at a high sampling rate. For instance, iOS displays a flashing "microphone" icon on the status bar when applications are collecting voice signals in the background. A similar mechanism can be deployed on the Android system to remind users when, where, and how their accelerometer readings are used.

### Further Improvements

As mentioned at the end of Section VI-E, the performance of our reconstruction module is still limited by the frequency range of the acceleration signals and the veracity of the GL algorithm. The first limitation restricts the audio frequency range that can be reconstructed, leading to the loss of consonant information, which is mainly distributed in the high-frequency domain. However, we believe that this limitation can be mitigated by future hardware improvements in smartphones. The second limitation is likely due to the GL algorithm, which attempts to compensate for all phase information from nearly scratch. To improve performance, we propose incorporating the phase information of the acceleration signals into the algorithm, which will be detailed in our future work.

### Defensive Strategies

We now discuss possible directions to defend against the proposed attack. One promising defense is to limit the sampling rate of the accelerometer. According to the Nyquist sampling theorem, an accelerometer operating below 170 Hz will not be able to reproduce any frequency components above 85 Hz. Although the accelerometer can still be affected by high-frequency audio signals, the captured information will be distorted, and the recognition accuracy is likely to decrease.

To find an optimal threshold, we conducted speech recognition experiments with accelerometer measurements sampled at 300 Hz, 200 Hz, 160 Hz, 100 Hz, and 50 Hz. Table XII shows the recognition accuracy on the digits dataset (10k single-digit signals from 20 speakers) under the table setting. It can be observed that the recognition accuracy drops with decreasing sampling rates and reduces to 30% at 50 Hz. In actual attacks, the recognition accuracy at 50 Hz could further decrease since the acceleration signal below 25 Hz can be significantly affected by human movement.

| Sampling Rate (Hz) | Recognition Accuracy (%) |
|--------------------|--------------------------|
| 300                | 73                       |
| 200                | 64                       |
| 160                | 56                       |
| 100                | 47                       |
| 50                 | 30                       |

According to the Android Developer guidelines, the recommended sampling rates for the user interface and mobile games are 16.7 Hz and 50 Hz, respectively. For activity recognition, 50 Hz is also more than sufficient since the frequencies of most human activities are below 20 Hz. Therefore, we suggest that applications requiring sampling rates above 50 Hz should request permission, which will affect how Google Play filters them.

### Conclusion

In this paper, we revisit the threat of zero-permission motion sensors to speech privacy and propose a highly practical side-channel attack against smartphone speakers. We present two fundamental observations that extend motion sensor-based audio eavesdropping to everyday scenarios. First, speech signals emitted by smartphone speakers always create significant impacts on the accelerometer of the same smartphone. Second, the accelerometer on recent Android smartphones can almost cover the entire fundamental frequency band of adult speech. Based on these pivotal observations, we propose AccelEve, a learning-based smartphone eavesdropping attack that can recognize and reconstruct speech signals emitted by smartphone speakers, regardless of where and how the smartphone is placed. With deep networks, adaptive optimizers, and robust and generalizable losses, our attack significantly and consistently outperforms baseline and existing solutions in all recognition and reconstruction tasks. In particular, AccelEve achieves three times the accuracy of previous work in digit recognition. For speech reconstruction, AccelEve is able to reconstruct speech signals with enhanced sampling rates, covering not only the fundamental frequency components (vowels) in the low-frequency band but also their harmonics in the high-frequency band. Consonants are not recovered because their frequencies (above 2000 Hz) are far beyond the sampling rates of current smartphones.

### Acknowledgment

We thank the reviewers for their helpful comments. This research is supported in part by the National Natural Science Foundation of China (Grants No. 61772236), Zhejiang Key R&D Plan (Grant No. 2019C03133), Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Research Institute of Cyberspace Governance at Zhejiang University, and McGill University’s William Dawson Scholar Chair Professor Fund.

### References

[1] “Coriolis force,” https://en.wikipedia.org/wiki/Coriolis_force.
[2] “Sensor Overview,” https://developer.android.com/guide/topics/sensors/sensors_overview.
[3] “uses-permission,” https://developer.android.com/guide/topics/manifest/uses-permission-element.
[4] ANALOG DEVICES, “ADXL150/ADXL250,” https://hibp.ecse.rpi.edu/~connor/education/EIspecs/ADXL150_250.pdf.
[5] S. A. Anand and N. Saxena, “Speechless: Analyzing the threat to speech privacy from smartphone motion sensors,” in 2018 IEEE Symposium on Security and Privacy (SP). IEEE, 2018, pp. 1000–1017.
[6] S. A. Anand, C. Wang, J. Liu, N. Saxena, and Y. Chen, “Spearphone: A speech privacy exploit via accelerometer-sensed reverberations from smartphone loudspeakers,” arXiv preprint arXiv:1907.05972, 2019.
[7] R. J. Baken and R. F. Orlikoff, Clinical measurement of speech and voice. Cengage Learning, 2000.
[8] S. Becker, M. Ackermann, S. Lapuschkin, K.-R. Müller, and W. Samek, “Interpreting and explaining deep neural networks for classification of audio signals,” arXiv preprint arXiv:1807.03418, 2018.
[9] H. Bojinov, Y. Michalevsky, G. Nakibly, and D. Boneh, “Mobile device identification via sensor fingerprinting,” arXiv preprint arXiv:1408.1416, 2014.
[10] L. Cai and H. Chen, “Touchlogger: Inferring keystrokes on touch screen from smartphone motion.” HotSec, vol. 11, no. 2011, p. 9, 2011.
[11] R. Chen and I. C. Paschalidis, “A robust learning approach for regression models based on distributionally robust optimization,” The Journal of Machine Learning Research, vol. 19, no. 1, pp. 517–564, 2018.
[12] A. Das, N. Borisov, and M. Caesar, “Exploring ways to mitigate sensor-based smartphone fingerprinting,” arXiv preprint arXiv:1503.01874, 2015.
[13] A. De Cheveigné and H. Kawahara, “Yin, a fundamental frequency estimator for speech and music,” The Journal of the Acoustical Society of America, vol. 111, no. 4, pp. 1917–1930, 2002.
[14] R. N. Dean, G. T. Flowers, A. S. Hodel, G. Roth, S. Castro, R. Zhou, A. Moreira, A. Ahmed, R. Rifki, B. E. Grantham et al., “On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise,” in 2007 IEEE International Symposium on Industrial Electronics. IEEE, 2007, pp. 1435–1440.
[15] S. Dey, N. Roy, W. Xu, R. R. Choudhury, and S. Nelakuditi, “AccelPrint: Imperfections of accelerometers make smartphones trackable,” in NDSS, 2014.
[16] J. Doscher and M. Evangelist, “Accelerometer design and applications,” Analog devices, vol. 3, p. 16, 1998.
[17] H. Feng, K. Fawaz, and K. G. Shin, “Continuous authentication for voice assistants,” in Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking. ACM, 2017, pp. 343–355.
[18] H. Fujisaki, “Dynamic characteristics of voice fundamental frequency in speech and singing,” in The production of speech. Springer, 1983, pp. 39–55.
[19] S. Grawunder and I. Bose, “Average speaking pitch vs. average speaker fundamental frequency–reliability, homogeneity, and self-report of listener groups,” in Proceedings of the International Conference Speech Prosody, 2008, pp. 763–766.
[20] D. Griffin and J. Lim, “Signal estimation from modified short-time Fourier transform,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 236–243, 1984.
[21] J. Han, E. Owusu, L. T. Nguyen, A. Perrig, and J. Zhang, “Accomplice: Location inference using accelerometers on smartphones,” in 2012 Fourth International Conference on Communication Systems and Networks (COMSNETS 2012). IEEE, 2012, pp. 1–9.
[22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition. IEEE, 2016, pp. 770–778.
[23] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al., “CNN architectures for large-scale audio classification,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 131–135.
[24] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition. IEEE, 2017, pp. 4700–4708.
[25] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time style transfer and super-resolution,” in European conference on computer vision. Springer, 2016, pp. 694–711.
[26] T. Kaya, B. Shiari, K. Petsch, and D. Yates, “Design of a MEMS capacitive comb-drive accelerometer,” in COMSOL Conference, Boston, 2011.
[27] J. R. Kwapisz, G. M. Weiss, and S. A. Moore, “Activity recognition using cell phone accelerometers,” ACM SigKDD Explorations Newsletter, vol. 12, no. 2, pp. 74–82, 2011.
[28] J. Liu, C. Wang, Y. Chen, and N. Saxena, “VibWrite: Towards finger-input authentication on ubiquitous surfaces via physical vibration,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2017, pp. 73–87.
[29] P. Marquardt, A. Verma, H. Carter, and P. Traynor, “(sp)iPhone: Decoding vibrations from nearby keyboards using mobile phone accelerometers,” in Proceedings of the 18th ACM conference on Computer and communications security. ACM, 2011, pp. 551–562.
[30] A. Matic, V. Osmani, and O. Mayora, “Speech activity detection using accelerometer,” in 2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE, 2012, pp. 2112–2115.
[31] R. Matovu, I. Griswold-Steiner, and A. Serwadda, “Kinetic song comprehension: Deciphering personal listening habits via phone vibrations,” arXiv preprint arXiv:1909.09123, 2019.
[32] Y. Michalevsky, D. Boneh, and G. Nakibly, “Gyrophone: Recognizing speech from gyroscope signals,” in Proceedings of the 23rd USENIX conference on Security Symposium. USENIX Association, 2014, pp. 1053–1067.
[33] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and R. R. Choudhury, “Tapprints: Your finger taps have fingerprints,” in Proceedings of the 10th international conference on Mobile systems, applications, and services. ACM, 2012, pp. 323–336.
[34] E. Owusu, J. Han, S. Das, A. Perrig, and J. Zhang, “Accessory: Password inference using accelerometers on smartphones,” in Proceedings of the Twelfth Workshop on Mobile Computing Systems & Applications. ACM, 2012, p. 9.
[35] M. Shoaib, H. Scholten, and P. J. Havinga, “Towards physical activity recognition using smartphone sensors,” in 2013 IEEE 10th International Conference on Ubiquitous Intelligence and Computing and 2013 IEEE 10th International Conference on Autonomic and Trusted Computing. IEEE, 2013, pp. 80–87.
[36] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[37] Y. Son, H. Shin, D. Kim, Y. Park, J. Noh, K. Choi, J. Choi, and Y. Kim, “Rocking drones with intentional sound noise on gyroscopic sensors,” in Proceedings of the 24th USENIX Conference on Security Symposium. USENIX Association, 2015, pp. 881–896.
[38] I. R. Titze and D. W. Martin, “Principles of voice production,” Acoustical Society of America Journal, vol. 104, p. 1148, 1998.
[39] T. Trippel, O. Weisse, W. Xu, P. Honeyman, and K. Fu, “WALNUT: Waging doubt on the integrity of MEMS accelerometers with acoustic injection attacks,” in 2017 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2017, pp. 3–18.
[40] Y. Tu, Z. Lin, I. Lee, and X. Hei, “Injected and delivered: Fabricating implicit control over actuation systems by spoofing inertial sensors,” in Proceedings of the 27th USENIX Conference on Security Symposium. USENIX Association, 2018, pp. 1545–1562.
[41] C. Xu, Z. Li, H. Zhang, A. S. Rathore, H. Li, C. Song, K. Wang, and W. Xu, “WaveEar: Exploring a mmWave-based noise-resistant speech sensing for voice-user interface,” in Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 2019, pp. 14–26.
[42] Z. Xu, K. Bai, and S. Zhu, “TapLogger: Inferring user inputs on smartphone touchscreens using on-board motion sensors,” in Proceedings of the Fifth ACM Conference on Security and Privacy in Wireless and Mobile Networks. ACM, 2012, pp. 113–124.
[43] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv preprint arXiv:1605.07146, 2016.
[44] L. Zhang, P. H. Pathak, M. Wu, Y. Zhao, and P. Mohapatra, “AccelWord: Energy-efficient hotword detection through accelerometer,” in Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 2015, pp. 301–315.