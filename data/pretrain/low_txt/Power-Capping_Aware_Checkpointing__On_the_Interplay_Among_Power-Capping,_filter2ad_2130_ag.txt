### Sensitivity Study on Application-Specific Coefficients and Time to Checkpoint

#### Introduction
We conducted a sensitivity analysis to understand how the improvements obtained by our power capping aware Optimal Checkpoint Interval (OCI) model change with variations in application-specific parameters (A and B), platform-specific parameters (C and D), and the time to checkpoint (β). The results are presented in Figures 12 and 13, which show the reduction in checkpointing time, wasted work, and total execution time under different power caps.

#### Impact of Application-Specific Parameters (A and B)
First, we examined the impact of application-specific parameters (A and B) on the performance of our power capping aware OCI model. These parameters represent the effect of power capping on application performance. We performed experiments using applications with regression functions at both extremes (MG and EP).

**Results:**
- **Figure 12** shows that the application-specific parameters (A and B) do not significantly affect the improvements achieved by our power capping aware OCI model.
- This is expected because these parameters do not directly influence the OCI estimation (α+e).
- The OCI values for MG and EP are the same, with only slight differences in percentage reduction due to the dependency of execution time on A and B.

**Finding 9:**
- The percentage changes in checkpointing, wasted, and total execution time are not highly sensitive to the application-specific coefficients.
- Platforms with higher temperature gradients relative to power capping benefit more from the power capping aware OCI model.

#### Impact of Time to Checkpoint (β)
Next, we studied the sensitivity of our model to the time to checkpoint (β). We present experimental results for β values of 5, 15, and 45 minutes.

**Results:**
- **Figure 13** shows that the reduction in checkpointing, wasted, and total execution time increases as the time to checkpoint increases.
- This reduction is more pronounced at lower power caps, indicating that our model can significantly reduce checkpointing and total execution time when the time to checkpoint is larger.

**Finding 10:**
- The power capping aware OCI model provides increasing gains over prior OCI models as the time to checkpoint increases.
- With increasing system/problem scales and relatively slow-growing I/O bandwidth, our model can offer significant benefits in I/O bandwidth-constrained systems.

#### Evaluation with Real Scientific Applications
We evaluated our power capping aware OCI model using real scientific applications run on OLCF machines. The checkpoint data size and execution time were considered, as shown in Table I.

**Results:**
- **Figure 14** presents the percentage reduction in checkpointing, wasted, and total execution time between prior OCI models and our power-aware OCI model under different power caps.
- For CHIMERA, our model reduces the total execution time by 9% to 18% compared to prior models due to its large checkpoint data size.
- For applications with moderate checkpoint data sizes (GTC and S3D), the total execution time decreases by 4% and 2%, respectively.
- For applications with small checkpoint data sizes (GYRO, POP, and VULCUN/2D), our model has about the same total execution time as prior models.

**Finding 11:**
- Applications with large checkpoint data sizes can achieve substantial reductions in checkpointing time and total execution time using the power capping aware OCI model.
- Even for applications with small checkpoint data sizes, our model can significantly reduce the total volume of checkpoint data written to storage systems, helping to resolve PFS bottleneck issues and improving I/O performance.

#### Conclusion
In summary, our power capping aware OCI model demonstrates significant improvements in checkpointing and total execution time, especially for applications with large checkpoint data sizes. The model also reduces the volume of data movement, providing benefits in I/O bandwidth-constrained systems. Future work will focus on extending the model to support heterogeneous platforms and considering the impacts of manufacturing variations.

### References
[1] D. Kothe and R. Kendall, “Computational Science Requirements for Leadership Computing,” Oak Ridge National Laboratory, Tech. Rep., 2007.
[2] W. Joubert, D. Kothe, and H. A. Nam, “Preparing for Exascale: ORNL Leadership Computing Facility Application Requirements and Strategy,” Oak Ridge National Laboratory, Tech. Rep., 2009.
[3] J. Duell, P. Hargrove, and E. Roman, “The Design and Implementation of Berkeley Lab’s Linux Checkpoint/Restart,” Berkeley Lab, Tech. Rep., 2002.
[4] A. Moody, G. Bronevetsky, K. Mohror, and B. R. d. Supinski, “Design, Modeling, and Evaluation of a Scalable Multi-level Checkpointing System,” in Proceedings of the ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, 2010, pp. 1–11.
[5] L. Bautista-Gomez, S. Tsuboi, D. Komatitsch, F. Cappello, N. Maruyama, and S. Matsuoka, “FTI: high performance fault tolerance interface for hybrid systems,” in Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, 2011, pp. 32:1–32:12.
[6] J. Bent, G. Grider, B. Kettering, A. Manzanares, M. McClelland, A. Torres, and A. Torrez, “Storage challenges at Los Alamos National Lab,” in IEEE MSST, 2012, pp. 1–5.
[7] F. Cappello, “Fault Tolerance in Petascale/ Exascale Systems: Current Knowledge, Challenges and Research Opportunities,” International Journal of High Performance Computing Applications, vol. 23, no. 3, pp. 212–226, 2009.
[8] D. Tiwari, S. Gupta, and S. S. Vazhkudai, “Lazy Checkpointing: Exploiting Temporal Locality in Failures to Mitigate Checkpointing Overheads on Extreme-Scale Systems,” in 44th Annual IEEE/IFIP Int’l Conference on Dependable Systems and Networks, 2014, pp. 25 – 36.
[9] L. Bautista-Gomez, A. Gainaru, S. Perarnau, D. Tiwari, S. Gupta, C. Engelmann, F. Cappello, and M. Snir, “Reducing waste in extreme scale systems through introspective analysis,” 2016.
[10] J. W. Young, “A First Order Approximation to the Optimum Checkpoint Interval,” Communications of the ACM, vol. 17, no. 9, pp. 530–531, 1974.
[11] J. Daly, “A Model for Predicting the Optimum Checkpoint Interval for Restart Dumps,” in Proceedings of the International Conference on Computational Science, 2003, pp. 3–12.
[12] J. Daly, “A higher order estimate of the optimum checkpoint interval for restart dumps,” Future Generation Computer Systems, vol. 22, no. 2006, pp. 303–312, 2004.
[13] C. Lefurgy, X. Wang, and M. Ware, “Power capping: a prelude to power shifting,” Cluster Computing, vol. 11, no. 2, pp. 183–195, 2008.
[14] A. Gandhi, M. Harchol-Balter, R. Das, J. O. Kephart, and C. Lefurgy, “Power Capping Via Forced Idleness,” in Proceedings of Workshop on Energy-Efficient Design, 2009.
[15] M. Dimitrov, C. Strickland, S.-W. Kim, K. Kumar, and K. Doshi, “Intel Power Governor,” https://software.intel.com/en-us/articles/intel-power-governor, July 2012.
[16] Intel, Intel 64 and IA-32 Architectures Software Developer’s Manual. Intel Corporation, 2015, vol. 3B, no. 2.
[17] K. Ma and X. Wang, “PGCapping: Exploiting Power Gating for Power Capping and Core Lifetime Balancing in CMPs,” in Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques, 2012, pp. 13–22.
[18] A. Hussein, A. L. Hosking, M. Payer, and C. A. Vick, “Don’t race the memory bus: taming the gc leadfoot,” in Proceedings of the 2015 ACM SIGPLAN International Symposium on Memory Management. ACM, 2015, pp. 15–27.
[19] S. Agarwal, R. Garg, M. S. Gupta, and J. E. Moreira, “Adaptive Incremental Checkpointing for Massively Parallel Systems,” in Proceedings of the 18th Annual International Conference on Supercomputing, 2004, pp. 277–286.
[20] K. Ferreira, J. Stearley, J. H. Laros, III, R. Oldfield, K. Pedretti, R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold, “Evaluating the Viability of Process Replication Reliability for Exascale Systems,” in Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, 2011, pp. 44:1–44:12.
[21] M. Forshaw, A. S. McGough, and N. Thomas, “Energy-efficient checkpointing in high-throughput cycle-stealing distributed systems,” Electronic Notes in Theoretical Computer Science, vol. 310, pp. 65–90, 2015.
[22] S. S. Shende and A. D. Malony, “The Tau Parallel Performance System,” International Journal of High Performance Computing Applications, vol. 20, no. 2, pp. 287–311, 2006.
[23] P. J. Mucci, S. Browne, C. Deane, and G. Ho, “PAPI: A Portable Interface to Hardware Performance Counters,” in Proceedings of Department of Defense HPCMP Users Group Conference, 1999.
[24] T. Patki, D. K. Lowenthal, B. Rountree, M. Schulz, and B. R. de Supinski, “Exploring Hardware Overprovisioning in Power-constrained, High Performance Computing,” in Proceedings of the 27th International ACM Conference on International Conference on Supercomputing, 2013, pp. 173–182.
[25] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai, D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Understanding GPU errors on large-scale HPC systems and the implications for system design and operation,” in High Performance Computer Architecture (HPCA), 2015 IEEE 21st International Symposium on. IEEE, 2015, pp. 331–342.
[26] S. Gupta, D. Tiwari, C. Jantzi, J. Rogers, and D. Maxwell, “Understanding and exploiting spatial properties of system failures on extreme-scale HPC systems,” in Dependable Systems and Networks (DSN), 2015 45th Annual IEEE/IFIP International Conference on. IEEE, 2015, pp. 37–44.
[27] B. Nie, D. Tiwari, S. Gupta, E. Smirni, and J. H. Rogers, “A large-scale study of soft-errors on GPUs in the field,” in High Performance Computer Architecture (HPCA), 2016 IEEE 22nd International Symposium on, 2016.
[28] D. Tiwari, S. Gupta, G. Gallarno, J. Rogers, and D. Maxwell, “Reliability lessons learned from GPU experience with the Titan supercomputer at Oak Ridge Leadership Computing Facility,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2015, p. 38.
[29] N. El-Sayed, I. A. Stefanovici, G. Amvrosiadis, A. A. Hwang, and B. Schroeder, “Temperature management in data centers: why some (might) like it hot,” ACM SIGMETRICS Performance Evaluation Review, vol. 40, no. 1, pp. 163–174, 2012.
[30] “Arrhenius equation,” https://en.wikipedia.org/wiki/Arrhenius_equation.
[31] P. Ellerman, “Calculating Reliability using FIT and MTTF: Arrhenius HTOL Model,” microsemi, Tech. Rep., 2012.
[32] G. Shipman and et al., “A next-generation parallel file system environment for the OLCF,” in Proceedings of the Cray User Group Conference, 2012.