### Table 1: Participant Demographics

The participant composition in this study is similar to the actual work role distribution across NYC3, with 50 out of 67 employees serving as technicians. Prior to the study, one participant had a high-level understanding of the military applications of the Center of Gravity (CoG) concept, and none of the participants had any applied experience using any threat-modeling framework.

All participants had at least some college education, with ten holding a graduate degree and eight holding a bachelor’s degree. Additionally, 15 participants possessed at least one industry certification. Participants had an average of 14.7 years of information technology and security experience in large organizations, with a mean of 8.5 years of formal or on-the-job training.

### 4.2 Pre-Intervention Baseline

To measure the impact of threat modeling within NYC3 systems, we first established a baseline of how participants deployed defensive strategies prior to our training. Most commonly, they prioritized defending high-impact service-based systems such as NYC.gov (n=7) and adhering to compliance frameworks (n=7), followed by applying risk management strategies (n=6) and assessing which systems are most susceptible to attack (n=3).

Participants reported using the following guidelines and programs for assessing NYC’s digital security posture:
- City-specific policies and executive orders, such as the NYC remote access policy [49] (n=6)
- NIST Cybersecurity Framework [44] (n=4)
- NYC3’s one-time accreditation process for adding new technologies to their network (n=2)

Of these guidelines, participants stated that none of the programs were applied frequently enough. For example, P5 stated that “compliance is only as good as your last assessment.” With too much time between audits, defenders cannot establish an accurate assessment of the environment’s security posture over time. The remaining respondents (n=13) were unsure about which programs or policies were applicable.

### 4.3 Immediate Observations

In contrast to the baseline survey, performance evaluation session observations and post-training surveys indicate that threat modeling provided participants with a better understanding of their security environment, increased their confidence in protecting NYC, and enabled them to apply threat modeling relatively quickly with accurate results.

#### 4.3.1 Perceived Efficacy

We observed participants’ initial threat modeling perceptions in the context of new insights, framework usefulness, and changes in self-efficacy.

**New Understanding:**
Overall, 12 out of 25 participants reported that threat modeling allowed them to understand new critical capabilities, requirements, or vulnerabilities that they had never previously considered. For instance, four participants had never mapped threats to vulnerabilities before. P16, a non-technical administrative support staffer, used threat modeling to understand the implications of wide-open security permissions on a wiki and networked share drive.

Threat modeling provided two participants with self-derived examples of why crisis continuity plans exist for large organizations. P04 stated that this new understanding would further assist him in planning for crises, allowing him to recommend a plan of action to senior management.

Of the 13 participants who did not report discovering anything new, seven stated that threat modeling was simply a restructured approach to current defensive concepts like defense-in-depth [36]. Four stated that threat modeling did not help them discover anything new but added additional emphasis to areas they should be concerned with.

Four participants identified an over-reliance on personal relationships (rather than codified policies) as a critical vulnerability for organizational success, which conceptually was something none of them had ever considered. During his performance evaluation session, P24 discussed how changes in the political environment from the local to federal level can affect established trust across GoNYC; a large turnover in personnel could halt progress and potentially kill initiatives. P25 stated, “I had not really considered the impact that some sort of major, non-cyber event could have on our ability to be successful,” discussing how a major terrorist event within NYC could decrease NYC3’s ability to sustain critical requirements and capabilities. Both participants recommended codifying existing relationship-based agreements into legislation capable of withstanding non-digital security threats to their daily responsibilities, such as establishing a formal memorandum of understanding (MoU) with law enforcement agencies in NYC to facilitate the exchange of threat indicators.

**Perceived Framework Usefulness:**
After completing the performance evaluation session, 23 participants agreed that threat modeling was useful in their daily work. For example, ten said the framework allowed them to prioritize their efforts. P24 developed a new litmus test for adding any defensive efforts, stating, “If the adversary doesn’t care, then it’s all just fluff [inconsequential].” P21 used threat modeling to show “what we’re lacking or what we need to concentrate on,” such as standard cyber hygiene.

Eight participants expressed that threat modeling added much-needed structure and perspective to difficult problems. P11 feels empowered by its structure and believes it allows him to “accept the things you cannot change, change the things you can, and have the wisdom to know the difference. I feel [CoG is] along those lines; this is your world, this is what you control.” He believes threat modeling makes a positive difference with available resources while helping to prioritize requests for future capabilities and support.

Five participants reported that threat modeling allowed them to plan defensive strategies more effectively. P05 stated that threat modeling helps him “plan effectively, document, track, monitor progress, and essentially understand our security posture.”

Threat modeling allowed four participants to comprehend how threats can affect systems within their environment; these technicians previously relied upon best security practices without fully considering threats. While applying the framework, P10 declared that “insider threats overcome the hard shell, soft core” within most enterprise networks and that threat modeling helped him identify new ways to neutralize the impact of insiders bypassing perimeter defenses and exploiting trusted internal systems.

Four participants stated that purposefully considering their asset inventory during threat modeling allowed them to fully understand their responsibilities. Three participants stated that threat modeling provides them with a new appreciation for their position within NYC3. P14 said, “When I did my job, I didn’t think about what the purpose of our group is [within NYC3]. . . [threat modeling] aligns what we’re thinking with what I think my role is in this organization.”

**Changes in Self-Efficacy:**
When comparing responses from the post-training survey to baseline responses, 10 participants reported a perceived increase in their ability to monitor critical assets, 17 reported an increase in their ability to identify threats, 16 reported an increase in their ability to mitigate threats, and 15 participants reported an increase in their ability to respond to incidents. Respectively, averages increased by 8.8%, 19.3%, 29.8%, and 20.0%. Using the Wilcoxon signed-rank test [65], we found significant increases in participants’ perceived ability to identify threats (W=61.0, p=0.031), mitigate threats (W=47.0, p=0.010), and respond to incidents (W=59.0, p=0.027).

### 4.3.2 Actual Efficacy

We measure the actual efficacy of threat modeling using several metrics: the accuracy of participants’ output, task completion times, similarities between participants’ identified CoGs, and the contents of their actionable defense plans.

**Output Accuracy:**
Simply completing CoG tasks is insufficient to demonstrate success; the resulting output must also be valid and meaningful. Thus, we assessed the accuracy of participants’ results via an expert evaluation from two NYC3 senior leaders. Both leaders received in-person training on CoG and are uniquely qualified to assess the accuracy of the provided responses given their intimate knowledge of the NYC3 environment and cybersecurity expertise. We provided the evaluators with an anonymized set of the study results and asked them to jointly qualify the accuracy of the identified centers of gravity, critical vulnerabilities, threat capabilities/requirements, and ideal defense plans using a 6-point Likert scale ranging from zero to five, with zero being “extremely unlikely (UL)” and five being “extremely likely (EL).” Additionally, we asked the leaders to indicate whether each ADP was sufficiently detailed to implement. We included one fictitious participant entry as an attention check and validity control, which both panel members identified and rejected.

The panel concluded that:
- 22 of 225 identified centers of gravity were accurate with respect to a participant’s responsibilities (‘EL’=3, ‘Likely [L]’=9, ‘Somewhat likely [SL]’=10)
- All critical vulnerabilities were accurate for the identified centers of gravity (EL=6, L=7, SL=12)
- 23 of 25 threat capability and requirement profiles were accurate (EL=6, L=7, SL=10)
- 24 of 25 actionable defense plans would accurately address the identified threats (EL=5, L=11, SL=8)

We used a logistic regression, appropriate for ordinal Likert data, to estimate the effect of work roles, experience in IT, and educational background on the accuracy of the panel results. We included a mixed-model random effect [26] that groups results by work roles to account for correlation between individuals who fill similar positions. Our initial model for the regression included each demographic category. To prevent overfitting, we tested all possible combinations of these inputs and selected the model with minimum Akaike Information Criterion [1]. The final selected model is given in Appendix E. Based on this regression, we found that no particular work role, amount of education, IT experience, or combination thereof enjoyed a statistically significant advantage when using threat modeling. These high success rates across our demographics support findings by Sindre and Opdahl that indicate threat modeling is a natural adaptation to standard IT practices [58].

**Time Requirements:**
We use the time required to apply CoG analysis to measure efficiency, which is a component of efficacy. On average, participants used the framework and developed actionable defense plans in 36 minutes and 46 seconds (σ = 9:01). Figure 3 shows subtask completion times as a cumulative distribution function (CDF). Participants spent the greatest amount of time describing critical vulnerabilities and developing actionable defense plans, with these tasks averaging 5:27 and 6:25, respectively. Three out of five participants in a leadership role affirmed without prompting that threat modeling provided them with a tool for quickly framing difficult problems. P24 stated, “Within an hour, [CoG] helped me think about some items, challenge some things, and resurface some things, and that is very useful for me given my busy schedule.” P22 applied the framework in 22 minutes and commented during his closing performance evaluation session that he would “need much more time to fully develop” his ideas; however, he also said the session served as a catalyst for initiating a necessary dialogue for handling vulnerabilities.

**CoG Consistency:**
Analysis of the performance evaluation session results reveals that participants with similar work role classifications produced similar output. For example, 16 of 18 technicians indicated that a digital security tool was their CoG (e.g., firewalls, servers), whereas four of six participants in support roles identified a “soft” CoG (e.g., relationships, funding, and policies). Participants produced actionable defense plans averaging 5.9 mitigation strategies per plan, ranging from a minimum of three strategies to a maximum of 14.

**Actionable Defense Plans:**
We use the contents of participants’ actionable defense plans to further evaluate success. Participants identified real issues present within their environment and developed means for reducing risk. Within the 25 actionable defense plans, participants cumulatively developed 147 mitigation strategies; we provide detailed examples in Section 4.5. Participants indicated that 33% of the mitigation strategies they developed using threat modeling were new plans that would immediately improve the security posture of their environment if implemented. Additionally, participants stated that 31% of the mitigation strategies would improve upon existing NYC3 defensive measures and more adequately defend against identified threats. Participants felt that the remaining 36% of their described mitigation strategies were already sufficiently implemented across the NYC3 enterprise.

The NYC3 leadership panel indicated that a majority of the actionable defense plans were sufficiently detailed for immediate implementation (‘Yes’= 16). This shows that, even with limited framework exposure, many participants were able to develop sufficient action plans. We illustrate an ADP with insufficient detail using a security analyst’s plan. After identifying his CoG as an Endpoint Detection and Response (EDR) system and applying the framework, his ADP consisted of three mitigation strategies: “Make sure there is a fail-over setup and test it. Better change control. Better rollback procedures.” While all of these address critical vulnerabilities, they provide no implementation details. In cases such as this, individuals require additional time to improve the fidelity of their responses or may benefit from expert assistance in transforming their ideas into fully developed plans.

### 4.4 Observations after 30 Days

After 30 days, we observed that participants still had a favorable opinion of threat modeling, most participants actually implemented defensive plans that they developed through our study, and that NYC3 institutionalized threat modeling within their routine practices.

#### 4.4.1 Perceived Efficacy

Thirty days after learning about CoG, there was a slight decrease in the perceived efficacy of the framework when compared to participant perceptions immediately after training: a 1.47% decrease for monitoring critical assets (W=81.0, p=0.57), 3.22% decrease for identifying threats (W=131.0, p=0.83), 3.58% decrease for mitigating threats (W=94.0, p=0.18), and 1.67% decrease for responding to incidents (W=100.0, p=0.59); none of these decreases were statistically significant. When comparing these 30-day metrics to the baseline, however, participants’ perceived ability to monitor critical assets increased 7.4%, perceived ability to identify threats increased 16.1%, perceived ability to mitigate threats increased 26.3%, and perceived ability to respond to threats increased 18.3%. Participants’ perceived ability to mitigate threats is a statistically significant increase from the baseline (W=73.5, p=0.049).

Figure 4 shows participants’ evaluations of the efficacy of CoG analysis after 30 days. Overall, all participants agreed (“Strongly”= 13) that threat modeling supports critical aspects of their job. Additionally, 24 participants agreed (“Strongly”= 15) that threat modeling enhances the way they think about digital security. Despite the aforementioned decrease in perceived efficacy over the 30-day period, the number of participants who found the framework useful to their jobs increased from 23 to 24, as NYC3’s adoption of ADPs within their environment caused one participant to believe in the framework’s usefulness. Lastly, 245 of 275 responses to our 11 TAM questions indicated that threat modeling is valuable for digital security.

#### 4.4.2 Actual Efficacy

We measure actual efficacy after 30 days using participants’ knowledge retention. Measuring knowledge retention allows us to evaluate the longevity of organizational impacts from integrating the framework. After 30 days, participants averaged 78% accuracy on four comprehension questions. This is an increase from 69% immediately after learning the framework, suggesting that threat modeling may become more memorable after additional applied experience. Each comprehension question required participants to pinpoint the best answer out of three viable responses; this allowed us to measure if participants understood critical relationships. In the 30-day follow-up, all participants accurately answered our critical vulnerability question, 23 correctly identified a CoG visually, 17 correctly identified a critical requirement for a capability, and 13 correctly identified a critical capability for a notional CoG.

#### 4.4.3 Actual Adoption

After 30 days, 21 participants reported that they implemented at least one mitigation strategy that they developed using threat modeling. In addition, 20 participants reported after 30 days that they integrated concepts from threat modeling within their daily work routines. For example, seven participants now use the framework for continually assessing risk; this is in contrast to the baseline results, where participants typically assessed risk only during audits and initial accreditation. Five participants stated that they now use threat modeling to prioritize their daily and mid-range efforts. Participants who did not adopt said they were too busy with urgent tasks (n=4) or needed more applied training (n=1).

NYC3 started to institutionalize threat modeling after participants had discussed their results with one another and realized the important implications of their findings. One week after completing their performance evaluation sessions, six participants transformed a wall within their primary meeting room into an “urgent priorities” board (Figure 5) for implementing defensive actions that address critical vulnerabilities identified during this study.