### 7.55, 76.19, 81.09, 99.99, 11.47, 81.25, 100.00, 51.15, 14.31, 77.32, 79.93, 67.11, 16.46, 88.96, 91.96, 99.99, 25.00, 94.87, 100.00

One might argue that the same substitute model can be used to camouflage more than ten malware samples, thereby reducing the average budget per sample. However, in most cases, an attacker would aim to modify only a single malware instance to bypass the detector and execute its malicious functionality. Even if the average cost per example can be reduced by using the same substitute model, our attack sets a lower limit on the absolute number of queries, circumventing a cloud service that blocks access for hosts performing too many queries in a short period to thwart adversarial efforts [22, 19]. The more efficient the attack, the less likely it is to be mitigated by this approach.

The performance of our linear iteration attack, as shown in Table 3 (logarithmic backtracking=no columns), is identical to the performance of the SeqRand algorithm presented in [24], since both attacks use the same algorithm. The attack overhead for all attacks is similar: about 30%, or 40 API calls, per window. A classifier with an API window size of \( k = 100 \) provides roughly the same accuracy as with \( k = 140 \) (96.76% vs. 97.61% for the LSTM classifier with the same false positive rate). Therefore, the success of these attacks is due to the perturbation rather than the splitting of API sequences into two windows due to added API calls.

As can be seen, the attacks by Uesato et al. [30] have low effectiveness. This is because those attacks are not suitable for discrete values of API call types and indices. In contrast, our uniform mixing EA score-based attack demonstrates higher attack effectiveness, even for a fixed number of queries, when used for discrete input (API calls or position indices). This is due to the fact that the transformations used by EA work with discrete sequences: mutation (random perturbation) in existing adversarial candidates and crossover between several candidates. In our EA score-based attack, we do not use crossover, which might be suitable for the NLP domain (e.g., for compound sentences) but not for API call sequences, where each program has its own business logic. The self-adaptive search used by our EA score-based attack explains why it outperforms all other score-based attack variants and has better attack effectiveness than the gradient-based attack used in [42] with the same number of queries. Our proposed score-based attack outperforms existing methods because it maximizes attack effectiveness for a fixed number of queries. Note that the number of queries is per sliding window and not per executable.

Based on the average malicious sequence length, \( \text{avg}(\text{length}(x_m)) \approx 10,000 \), and the adversarial sliding window size, \( k = 140 \), the average absolute number of queries per malware executable is approximately 10,000. As expected, the benign perturbation effect on the decision-based attack effectiveness is the most significant, since without it, the API types are random. While our decision-based attack effectiveness is 10% lower than the most effective score-based attacks when using the same budget, it does not require knowledge of the target classifier's confidence score, making it the only viable attack in some black-box scenarios.

### 4.3 Defenses and Mitigation Techniques

To the best of our knowledge, there is currently no published and evaluated method to make a sequence-based RNN model resistant to adversarial sequences, beyond a brief mention of adversarial training as a defense method [17, 33]. Adversarial training [26] involves adding adversarial examples, with their non-perturbed labels, to the training set of the classifier. The reason is that adversarial examples are usually out-of-distribution samples, and inserting them into the training set helps the classifier learn the entire training set distribution, including the adversarial examples.

Adversarial training has several limitations:
1. It provides varying levels of robustness depending on the adversarial examples used.
2. It requires a dataset of adversarial examples to train on, limiting its generalization against novel adversarial attacks.
3. It requires retraining the model, incurring significant overhead.

We ran the adversarial attacks, both score-based and decision-based variants (Section 1), with and without benign perturbation (Section 3.2.2) on the training set, as suggested in [34]. For each column in Tables 3 and 4, we generated 14,000 malicious adversarial examples (50% generated by the black-box attack and 50% by the white-box attack), which replaced 14,000 malicious samples in the original training set. Other sizes (smaller or larger) resulted in reduced detection rates for the pre-trained classifier for non-adversarial samples. The adversarial examples were generated using the same configuration (score/decision-based, random/benign perturbation, number of queries to generate) as the evaluated attack. The results were consistent across all attack types: the attack effectiveness remained the same, while the attack overhead and number of queries increased by 10-15%, on average. This is because adversarial training is less effective against random attacks like ours, as a different stochastic adversarial sequence is generated each time, making it challenging for the classifier to generalize from one adversarial sequence to another.

More effective RNN defense methods, including domain-specific methods, such as systems that measure CPU usage [35], contain irregular API call subsequences [27] (such as the no-op API calls used in this paper), or otherwise assess the plausibility of our attack [38], will be part of our future work.

### 5 Conclusions and Future Work

In this paper, we presented the first black-box attack (based on the target classifier's predicted class, with and without its confidence score, to fit the adversary's limited knowledge) that generates adversarial sequences while minimizing the number of queries for the target classifier, reducing the number of queries by more than 10 times with minimal loss of attack effectiveness compared to the state-of-the-art attack [42]. This query-efficient approach makes our attack well-suited for attacking cloud models where a large number of queries can be costly and raise suspicion of an attack, thus failing previous attacks.

We demonstrated these attacks against API call sequence-based malware classifiers and verified the attack effectiveness for all relevant common classifiers: RNN variants, feedforward networks, and traditional machine learning classifiers. These are the first query-efficient attacks effective against RNN variants and not just CNNs.

We also evaluated our attacks against four variants of state-of-the-art score-based query-efficient attacks, modified to fit discrete sequence input, and showed that our attacks are equal to or outperform all of them. Finally, we demonstrated that our attacks are effective even when multiple feature types, including non-sequential ones, are used (Appendix D).

While this paper focuses on API calls and printable strings as features, the proposed attacks are valid for any modifiable feature type, sequential or not. Furthermore, our attack is generic and can be applied to other domains, such as text analysis (using word sequences instead of API calls), as will be demonstrated in our future work.

Our future work will focus on developing domain-specific and domain-agnostic defense mechanisms against such attacks and analyzing additional self-adaptive algorithms to find more query-efficient attacks, while evaluating them in limited knowledge scenarios (e.g., unknown API calls window size, etc.).

### References

[1] Amazon Machine Learning. https://aws.amazon.com/machine-learning, 2019. Accessed: 2019-09-26.
[2] Cuckoo Sandbox Hooked APIs and Categories. https://github.com/cuckoosandbox/cuckoo/wiki/Hooked-APIs-and-Categories, 2019. Accessed: 2019-08-24.
[3] Cylance, I Kill You! https://skylightcyber.com/2019/07/18/cylance-i-kill-you, 2019. Accessed: 2019-08-24.
[4] Deploy Trained Keras or TensorFlow Models Using Amazon SageMaker. https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/, 2019. Accessed: 2019-12-14.
[5] Google Cloud Prediction. https://cloud.google.com/prediction/, 2019. Accessed: 2019-09-26.
[6] Joe Sandbox ML. https://www.joesecurity.org/joe-sandbox-ML, 2019. Accessed: 2019-09-26.
[7] Keras. https://keras.io/, 2019. Accessed: 2019-09-26.
[8] Microsoft ATP. https://www.microsoft.com/security/blog/2018/02/14/how-artificial-intelligence-stopped-an-emotet-outbreak/, 2019. Accessed: 2019-09-26.
[9] SciKit Learn. http://scikit-learn.org/stable/, 2019. Accessed: 2019-09-26.
[10] Scikit Learn Decision Tree Categorical Variable. https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/, 2019. Accessed: 2019-09-26.
[11] SentinelOne. https://www.sentinelone.com/insights/endpoint-protection-platform-datasheet/, 2019. Accessed: 2019-09-26.
[12] VirusTotal. https://www.virustotal.com/, 2019. Accessed: 2019-09-26.
[13] XGBoost. https://github.com/dmlc/xgboost/, 2019. Accessed: 2019-09-26.
[14] Yara Rules. https://github.com/Yara-Rules/rules, 2019. Accessed: 2019-09-26.
[15] Rakshit Agrawal, Jack W. Stokes, Mady Marinescu, and Karthik Selvaraj. Robust neural malware detection models for emulation sequence learning. CoRR, abs/1806.10741, 2018.
[16] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, and Mani B. Srivastava. GenAttack: Practical black-box attacks with gradient-free optimization. CoRR, abs/1805.11090, 2018.
[17] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. In Ellen Riloﬀ, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2890–2896. Association for Computational Linguistics, 2018.
[18] Hyrum S. Anderson and Phil Roth. EMBER: An open dataset for training static PE malware machine learning models. CoRR, abs/1804.04637, 2018.
[19] Duen Horng Chau, Carey Nachenberg, Jeffrey Wilhelm, Adam Wright, and Christos Faloutsos. Polonium: Tera-scale graph mining for malware detection. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.
[20] Tong Che, Yanran Li, Ruixiang Zhang, R. Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete generative adversarial networks. CoRR, abs/1702.07983, 2017.
[21] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security - AISec 17. ACM Press, 2017.
[22] Steven Chen, Nicholas Carlini, and David Wagner. Stateful detection of black-box adversarial attacks, 2019.
[23] Duc-Cuong Dang and Per Kristian Lehre. Self-adaptation of mutation rates in non-elitist populations. In Julia Handl, Emma Hart, Peter R. Lewis, Manuel López-Ibáñez, Gabriela Ochoa, and Ben Paechter, editors, Parallel Problem Solving from Nature – PPSN XIV, pages 803–813, Cham, 2016. Springer International Publishing.
[24] Hung Dang, Yue Huang, and Ee-Chien Chang. Evading classifiers by morphing in the dark. In Bhavani M. Thuraisingham, David Evans, Tal Malkin, and Dongyan Xu, editors, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS 2017, Dallas, TX, USA, October 30 - November 03, 2017, pages 119–133. ACM, 2017.
[25] Jennifer G. Dy and Andreas Krause, editors. Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research. PMLR, 2018.
[26] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations (ICLR), December 2015.
[27] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick D. McDaniel. On the (statistical) detection of adversarial examples. ArXiv e-prints, abs/1702.06280, 2017.
[28] Weiwei Hu and Ying Tan. Black-box attacks against RNN based malware detection algorithms. ArXiv e-prints, abs/1705.08131, 2017.
[29] Ling Huang, Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar. Adversarial machine learning. In Yan Chen, Alvaro A. Cárdenas, Rachel Greenstadt, and Benjamin I. P. Rubinstein, editors, Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence, AISec 2011, Chicago, IL, USA, October 21, 2011, pages 43–58. ACM, 2011.
[30] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In Dy and Krause [25], pages 2142–2151.
[31] Jeremy Z. Kolter and Marcus A. Maloof. Learning to detect and classify malicious executables in the wild. J. Mach. Learn. Res., 7:2721–2744, 2006.
[32] Matt J. Kusner and José Miguel Hernández-Lobato. GANS for sequences of discrete elements with the Gumbel-softmax distribution. CoRR, abs/1611.04051, 2016.
[33] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. TextBugger: Generating adversarial text against real-world applications. CoRR, abs/1812.05271, 2018.
[34] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
[35] Robert Moskovitch, Shay Pluderman, Ido Gus, Dima Stopel, Clint Feher, Yisrael Parmet, Yuval Shahar, and Yuval Elovici. Host-based intrusion detection using machine learning. In IEEE International Conference on Intelligence and Security Informatics, ISI 2007, New Brunswick, New Jersey, USA, May 23-24, 2007, Proceedings, pages 107–114. IEEE, 2007.
[36] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In MILCOM 2016 - 2016 IEEE Military Communications Conference. IEEE, November 2016.
[37] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and Lorenzo Cavallaro. TESSERACT: Eliminating experimental bias in malware classification across space and time. In 28th USENIX Security Symposium (USENIX Security 19), pages 729–746, Santa Clara, CA, August 2019. USENIX Association.
[38] Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. Intriguing properties of adversarial ML attacks in the problem space. In 2020 IEEE Symposium on Security and Privacy, SP 2020, San Francisco, CA, USA, May 18-21, 2020, pages 1332–1349. IEEE, 2020.
[39] J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.com/FacebookResearch/Nevergrad, 2018.
[40] Ihai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Adversarial learning in the cyber security domain, 2020.
[41] Ishai Rosenberg, Shai Meir, Jonathan Berrebi, Ilay Gordon, Guillaume Sicard, and Eli David. Generating end-to-end adversarial examples for malware classifiers using explainability. In The 2020 International Joint Conference on Neural Networks (IJCNN 2020), 2020.
[42] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. Generic black-box end-to-end attack against state-of-the-art API call-based malware classifiers. In Michael Bailey, Thorsten Holz, Manolis Stamatogiannakis, and Sotiris Ioannidis, editors, Research in Attacks, Intrusions, and Defenses (RAID 2020), 2020.