# Automatic Metadata Generation for Active Measurement

**Authors:**
- Joel Sommers, Colgate University, [EMAIL]
- Ramakrishnan Durairajan, University of Oregon, [EMAIL]
- Paul Barford, University of Wisconsin-Madison, [EMAIL]

## Abstract
Empirical research in the Internet is fraught with challenges. One significant challenge is the potential for local environmental conditions (e.g., CPU load or network load) to introduce unexpected bias or artifacts in measurements, leading to erroneous conclusions. In this paper, we present a framework for monitoring the local environment during Internet measurement experiments. Our goals are to provide a broader perspective on measurement results and to enhance the reproducibility of these results. We have implemented this framework in a tool called SoMeta, which monitors the local environment during active probe-based measurement experiments. We evaluate the runtime costs of SoMeta and conduct a series of experiments where we intentionally perturb different aspects of the local environment during active probe-based measurements. Our experiments demonstrate how simple local monitoring can reveal conditions that bias active probe-based measurement results. We conclude with a discussion on how our framework can be expanded to provide metadata for a wide range of Internet measurement experiments.

### CCS Concepts
- **Networks**: Network experimentation; Network measurement

### Keywords
- Network measurement
- Metadata

### ACM Reference Format
Joel Sommers, Ramakrishnan Durairajan, and Paul Barford. 2017. Automatic Metadata Generation for Active Measurement. In Proceedings of IMC ’17, London, United Kingdom, November 1–3, 2017, 7 pages. https://doi.org/10.1145/3131365.3131400

## 1. Introduction
Active probe-based measurements have been widely used to elucidate Internet characteristics and behavior. These measurements typically aim to assess end-to-end path properties (e.g., reachability, latency, loss, throughput), hop-by-hop routing configurations, and end-host performance. In each case, a sequence of packets is sent from one or more measurement hosts to remote targets, and responses are measured either at the sending host or at the target host. One of the key benefits of active probe-based measurement is that it enables broad and diverse assessment of Internet characteristics without requiring permission or authorized access.

Despite these benefits and the availability of data sets through ongoing collection efforts, conducting active probe-based measurement studies remains challenging. One significant challenge is the potential for the local environment to introduce unexpected bias or artifacts in measurements. We define the local environment as the host emitting probe packets and other systems in the local area that can materially alter the behavior of probe packets but are intended to be outside the scope of the measurement objectives. For example, prior work has shown that variable CPU load can alter probe sequences, and hosts sharing local connectivity can disrupt probe packet streams by sending bursts of traffic. To address this, we advocate for collecting metadata about the local environment during measurements.

In this paper, we describe a framework for collecting metadata about the measurement environment. Inspired by calls from the community to collect metadata during experiments, our high-level goals are to make the process easy and thereby improve the quality and reproducibility of active probe-based measurement studies. Our design goals are to create a capability that will:
1. Measure the local environment when an active probe tool is being used.
2. Not perturb probe traffic.
3. Work seamlessly with different systems and measurement platforms.

To the best of our knowledge, this is the first attempt to address these meta-measurement issues. We develop a tool for metadata collection called SoMeta, which addresses our core design goals. SoMeta is activated on initiation of a probe-based measurement campaign. It collects key performance metrics on the measurement host (e.g., CPU and memory utilization) and performs simple probe-based measurements to hosts in the local environment. SoMeta is implemented in Python, making it simple to run on diverse hosts. It is also designed to be lightweight in terms of its demands on a measurement host. SoMeta produces simple log files that can be analyzed for indications of high load or other events that provide perspective on unexpected observations in target measurement data.

We demonstrate the capabilities of SoMeta by deploying it on two versions of the Raspberry Pi, which is used in the Ark active probe-based measurement project, and on large server-class systems. We begin by examining the load imposed by SoMeta on the host systems. We find that SoMeta imposes about 12% CPU load on a Pi model 1, 3% on a Pi model 3, and only about 1% on the servers in what we expect will be a typical configuration. Next, we conduct a series of experiments in which we introduce artificial load in the local environment while conducting active probe-based measurements to a remote target using scamper. The results from these experiments show how SoMeta measurements highlight the disturbances caused by the artificial load and how this could be used to point out artifacts in the scamper measurements.

While we believe that SoMeta is an important step toward accountability and reproducibility in active probe-based measurement studies, it has certain limitations that we are addressing as part of our ongoing work, including further reducing its performance overhead, considering usage scenarios in shared measurement environments, and broadening the types of metadata that can be captured, e.g., through in-network monitors. Moreover, there are two additional ways in which the concept should be expanded. First, the community needs to continue conversations about the importance of metadata collection and the kinds of metadata that should be collected to improve experiment reproducibility. Second, deploying SoMeta (or something similar) in an existing infrastructure would enable better understanding of its performance and how measurement artifacts and bias may be identified and possibly corrected. To that end, all code and documentation for SoMeta are readily available.

## 2. Related Work
Over the years, there have been numerous calls within the Internet measurement community to promote sound, hygienic, and ethical processes when conducting Internet measurement studies. These calls aim to improve confidence in the results derived from measurement data, facilitate data sharing, replication, and reappraisal, and carefully consider any harm that may be caused. Our study finds inspiration in these prior works, particularly with regard to collecting metadata to assist researchers in assessing the quality of the collected measurements and for scientific replication. Specifically, Paxson’s suggestion to measure facets of the same phenomena using different methods as a way to calibrate, assess measurement quality, and identify or correct for bias is particularly relevant. This method was used in prior work to evaluate the fidelity of measurements collected in RIPE Atlas, and a related analysis of latency measurement quality on early-generation RIPE Atlas nodes was done by Bajpai et al. Holterbach et al. suggest providing a “confidence index” along with reported measurement results, indicating some measure of concurrent load on an Atlas probe; providing such an index could be facilitated through the types of metadata gathered by SoMeta.

There have been specific suggestions in prior work regarding the scope of metadata that should be captured for future reference, and that metadata should be easily machine-readable. Examples of available metadata (and the associated data) can be found on Internet measurement data repositories such as CRAWDAD, IMDC, and M-Lab. Most of the metadata found through these platforms are descriptive, e.g., where, when, and how the measurements were collected, data format(s), etc., and some of these types of metadata are implicit in the data file naming convention (e.g., time of measurement, measurement node name, measurement tool used). While some metadata collected through SoMeta are descriptive of the environment on which measurement tools are run, its main focus is on gathering system performance data while measurement takes place. In this regard, SoMeta bears some similarity to M-Lab, in which limited measures of Planetlab slice performance metadata gathered through Nagios are available in a JSON format, such as CPU and memory utilization.

## 3. Design and Implementation

### 3.1 Design Goals
The design of SoMeta is based on three primary objectives:
1. **Metadata Collection**: Metadata should be collected by profiling various system resources at discrete intervals during the time in which an active measurement tool executes. Specifically, CPU, storage/IO performance, and other system measures should be gathered, and the access network should be monitored, e.g., the first k hops of the network path. Profiling should continue as long as the active measurement tool runs. When the active measurement tool completes, metadata should be stored in a simple and computer-readable format, e.g., JSON, to facilitate analysis. Basic tools for analysis and visualization of metadata should be provided to show, e.g., timeseries of idle CPU cycles, packet drops on an interface, RTT to the first router, etc.
2. **Lightweight Operation**: SoMeta should operate in a lightweight manner, adaptable to a range of target compute and network settings. We are motivated by the fact that CPU power and network bandwidth to the edge have increased, allowing the networking and compute environment in which active network measurement is performed to sustain additional traffic and processing activity from metadata capture. For example, even low-cost computer systems (e.g., the Raspberry Pi Model 3) have significant CPU power in the form of multiple cores (4 in the case of the Pi 3).
3. **Seamless Integration**: SoMeta should work seamlessly with different systems and measurement platforms. This objective is imperative to accommodate diverse measurement efforts, e.g., CAIDA uses Raspberry Pi- and 1U server-based Ark monitors, Yarrp uses Ubuntu VM, BGPmon uses sites with high-end multicore processors, and RIPE Atlas currently uses a low-cost wireless router (TP-Link model TL-MR 3020) with custom firmware based on OpenWRT.

### 3.2 SoMeta Overview and Implementation
SoMeta has been implemented in a lightweight and extensible way to meet the design objectives described above. It is written in Python and uses the asyncio framework as the basis for structuring and handling asynchronous events associated with monitoring the host.