# An Active Measurement System for Shared Environments

**Authors:**
- Joel Sommers, Colgate University
- Paul Barford, University of Wisconsin-Madison

## Abstract
Testbeds composed of end hosts deployed across the Internet enable researchers to simultaneously conduct a wide variety of experiments. However, active measurement studies that require precisely crafted probe streams can be problematic in these environments. The reason is that the load on host systems from concurrently executing experiments (as is typical in PlanetLab) can significantly alter probe stream timings.

In this paper, we measure and characterize how packet streams from our local PlanetLab nodes are affected by experimental concurrency. We find that the effects can be extreme. We then set up a simple PlanetLab deployment in a laboratory testbed to evaluate these effects in a controlled manner. Our results show that even relatively low load levels can cause serious problems in probe streams. Based on these findings, we develop a novel system called MAD (Multi-user Active Measurement Daemon) that can operate as a Linux kernel module or as a stand-alone daemon to support real-time scheduling of probe streams. MAD coordinates probe packet emission for all active measurement experiments on a node. We demonstrate the capabilities of MAD, showing that it performs effectively even under very high levels of multiplexing and host system load.

**Categories and Subject Descriptors:**
- C.2.3 [Network Operations]: Network management, Network monitoring
- C.2.5 [Local and Wide-Area Networks]: Internet (e.g., TCP/IP)
- C.4 [Performance of Systems]: Measurement Techniques

**General Terms:**
- Design, Experimentation, Measurement, Performance

**Keywords:**
- Active Measurement, MAD

## 1. Introduction
Several key challenges for networking research were specified in the 2001 National Research Council report entitled “Looking Over the Fence At Networks: A Neighbor’s View of Networking Research” [18]. Among these was the need to develop an understanding of Internet structure and behavior through empirical measurement, including the grand challenge of capturing “a day in the life of the Internet.” The lack of an intrinsic and openly available measurement capability in the Internet implies that a widely deployed infrastructure capable of different types of measurement would be required as a critical component for addressing these challenges.

The need for Internet testbeds capable of supporting accurate active measurements has been apparent for some time, resulting in the deployment and operation of several different infrastructures over the years. A well-known early example was the National Internet Measurement Infrastructure (NIMI), which consisted of guest accounts on 35 end hosts located primarily in the US and Europe [29]. The NIMI effort helped to crystallize the challenges associated with using and operating Internet measurement testbeds. One of these challenges is the need for a large number of diverse sites (e.g., commodity versus research network-attached, geography, last-hop bandwidth) such that a wide range of conditions is likely to be experienced across the paths between measurement nodes.

Perhaps the most prominent Internet testbed today, PlanetLab, is comprised of 780 end hosts deployed at 382 different sites worldwide [14]. PlanetLab is a canonical example of an openly available network testbed designed to support many different types of network-related experiments simultaneously. The fundamental design requirement of simultaneous experimental support has a significant implication: resource scheduling in these environments, at the individual host level or globally across the testbed, poses a particularly difficult problem. Thus, any experiment that has even modest timing or coordination requirements can be difficult or impossible to run in shared testbed environments. An important class of experiments that have until now been largely excluded from shared network testbeds are those that use active probe tools to measure end-to-end path properties such as delay, loss, capacity, and available bandwidth.

In this paper, we address the problem of how to extend shared network testbeds to enable accurate active measurement studies that use tools with fine-grained timing requirements. We begin by assessing the magnitude of the bias introduced by shared network testbeds. We instrument the two PlanetLab nodes at our own site with time-synchronized, hardware-based packet capture systems and conduct a series of active measurement experiments between those two nodes (i.e., such that there are no unknown network effects). We find that even in this simple local area environment, very large bias in active probe streams is common. For example, we found that there are often periods of multiple seconds in which packet loss is measured on end hosts while there is zero packet loss in the network. Likewise, RTT delays of over 50 milliseconds are not uncommon when the true delay between the two hosts, as measured by the hardware-based systems, is on the order of about 100 microseconds.

To address this problem, we develop the Multi-user Active Measurement System (MAD) designed to support real-time scheduling of active probe streams. The key requirements for MAD include accurate transmission/receipt of arbitrary probe streams, support for simultaneous experiments, low impact on the host system, ease of use, and security. We designed MAD as a service that can be accessed by applications through a simple active measurement specification language. Authorized users specify their probe process to MAD, which then coordinates the transmission of probe streams from multiple experiments using the real-time features of newer Linux kernels to gain access to resources at the highest priority level. (Note that there is also a MAD receiver module and a MAD reflector module that are used for one-way and round-trip measurement tools, respectively.) An additional requirement to facilitate the deployment and use of MAD is that it should not necessitate modifications to the host OS. As such, we implemented MAD so that it could be run either as a stand-alone daemon or as a Linux kernel module.

We evaluate the capabilities of MAD in a laboratory testbed that includes a local area PlanetLab deployment. This environment enables us to control and measure the contention effects due to multiplexing experiments both with and without MAD. We begin by establishing a quantitative baseline of the extent to which load generated by simultaneous experiments can bias active probe streams and lead to inaccurate inference of path properties. We then show that with MAD, highly accurate measurements can be made even under extremely heavy system loads (regardless of whether MAD runs as an in-kernel process). Next, we conduct similar experiments on raw, unvirtualized operating systems without PlanetLab and find that although the effects are less severe, MAD can still improve measurement accuracy in a meaningful way. Finally, we demonstrate the scalability of MAD through a series of microbenchmark experiments and show that it is able to support a relatively large number of simultaneous measurement experiments with extremely low impact on system resources.

In summary, the contributions of this paper are:
1. Characterizing the extent to which active probe-based measurement can be skewed in PlanetLab.
2. The design and implementation of MAD—a real-time scheduling system for accurate active probe-based measurement.

We believe that MAD effectively addresses an important deficiency in current end host-based Internet testbeds that largely precludes, or at least casts doubt on, their use for empirical studies of path properties using active probe tools. By virtue of its implementation, it is our hope that MAD can be widely deployed and used. We also believe that the design of MAD can inform future testbed development (e.g., GENI [7]) as well as infrastructures used to measure path properties in operational environments (e.g., for SLA compliance monitoring [40]).

The remainder of this paper is organized as follows. In Section 2, we discuss related work. In Section 3, we present findings from our analysis of measurements taken from our local PlanetLab nodes. We describe the details of MAD's design, implementation, and use in Section 4. The results of our laboratory evaluation of MAD are presented in Section 5, and in Section 6, we summarize and discuss future directions for our work.

## 2. Related Work
Host-based testbeds deployed in the Internet enable implementations of network applications, protocols, and measurement methodologies to be evaluated over live end-to-end paths. These testbeds can also be useful for gauging and characterizing Internet structure and end-to-end performance from multiple vantage points. Some testbeds in the past have been developed for use by a single organization for a specific set of objectives, e.g., Surveyor [22], while others have been designed from the outset to be general-purpose and shared with a wider set of researchers, e.g., RON [10] and PlanetLab [14, 30]. There have been measurement studies conducted on shared testbeds, including RON and PlanetLab, e.g., [12, 31, 44], and specialized systems have been developed for these testbeds to perform or assist with various types of measurements, such as ScriptRoute [43].

PlanetLab, in particular, has been deployed with the goal of serving a large community of researchers and has also spawned other regional instantiations [1, 4]. PlanetLab uses virtualization techniques to isolate users from one another [14, 36]. It virtualizes a host system at the system call level, similar to the way BSD Jails work [35], but a number of other approaches are possible, e.g., [13, 33]. It is important to note that in heavily-used Internet testbeds like PlanetLab, there is a direct correlation between contention for resources and the level of experimental multiplexing that is allowed.

One study with similarities to ours presents guidelines in the form of myths and realities for performing measurement studies on PlanetLab [42]. One guideline of relevance is to use system interfaces for obtaining kernel timestamps for probe packets. Our work also uses these techniques. Another suggestion is to use the ScriptRoute [43] system for defining and executing the probe process. Our work bears similarity to the ScriptRoute system in that we also design a measurement service, one component of which is a programmatic interface for specifying the probe process. However, our work takes a fundamentally different approach by focusing not only on providing a flexible measurement service but also on measurement accuracy. We assume that probe processes operate according to a discrete time clock, enabling optimizations to be made over all running measurement processes. ScriptRoute makes no such restrictions. Thus, the scheduling requirements of ScriptRoute are significantly different from our system.

Our system design takes advantage of the real-time scheduling capabilities that have been incorporated into the main Linux kernel source [5, 6]. These capabilities are similar to some of the features used in the related study by Pásztor and Veitch, in which a real-time version of Linux along with techniques they devise for improving scheduling and timestamp fidelity are employed [27, 28]. Our work relates to other efforts in the network research community to improve the accuracy and precision of timing-sensitive applications using commodity systems [2, 11, 45].

Active measurement of end-to-end delay and loss characteristics has a long history within the network measurement community. The most well-known early study of these characteristics was reported by Bolot in [16]. While methodologies for measuring these quantities have been standardized by the IETF [8, 9], improvements to these techniques continue to be developed [38, 40]. Of particular relevance is the work by Sommers et al. that highlights the need for real-time probe management in a multi-objective measurement context [39]. Our work addresses a different problem: the impact of resource sharing on active probe streams, and develops a generalized mechanism to address the problem.

Using pairs of closely-spaced packets to estimate network quantities also has a long and rich history. Jacobson’s work on TCP congestion control was an early exposition of the possibility of how a pair of closely-spaced packets can reveal bottleneck link capacity [20]. Other researchers have explored specific algorithms using this technique to perform capacity estimation, e.g., [23, 24, 25], and for estimating end-to-end available bandwidth, e.g., [32, 44]. Difficulties in creating accurate packet streams that match the probe models have been highlighted in many of these studies. For example, Carter and Crovella grappled with issues related to probe timing fidelity in developing active probe tools to measure bottleneck link speeds [17].

## 3. The Current Situation
We first investigate the accuracy characteristics of three types of active measurements with stringent timing requirements using the widely-used PlanetLab shared infrastructure.

### 3.1 Experiments
Normally, a user of PlanetLab does not have access to so-called ground truth measures between an arbitrary pair of PlanetLab nodes. Since the goal of our experiments is to evaluate the accuracy of certain types of active measurements, this issue is of critical importance. Other researchers have used indirect means for addressing this problem (e.g., by introducing known traffic to disturb the system as in [44]), or by using available coarse-grained SNMP data. These methods generally cannot provide the kind of accuracy and reliability required for our study.

Our approach was to use the two PlanetLab nodes over which we have complete control of the networking infrastructure: planetlab1 and planetlab2 in the domain cs.wisc.edu. For ground truth measures, we introduced passive network taps and a pair of synchronized Endace 4.3GE DAG cards for capturing both inbound and outbound probe traffic for each of the PlanetLab nodes, as depicted in Figure 1.

Our two PlanetLab nodes are identical Dell Precision 340s with 1.8 GHz Pentium 4 processors and 2 GB RAM. They were running version 4.0 of the PlanetLab software with Linux kernels derived from version 2.6.12. We installed Intel/Pro 1000 Gigabit Ethernet NICs on each host to accommodate our passive monitoring system. Interrupt coalescence was disabled for our experiments. No other changes were made to these two hosts.

**Figure 1: Setup for experiments using live PlanetLab nodes.**

We used three active measurement algorithms on these two hosts to examine how system load and multiplexing can affect measurement accuracy: round-trip delay probes, packet-pair probes, and BADABING loss probes. Each of these algorithms was implemented to use the User Datagram Protocol and to run according to a discrete time clock. We used the SO_TIMESTAMP option of setsockopt to obtain kernel timestamps on receipt of packets. These three measurement algorithms were chosen to be representatives of standard active measurements relying on a high degree of accuracy in both probe stream transmission and reception. A description of each algorithm along with its relevant parameters and accuracy requirements is shown in Table 1. The discrete time interval used in the results that we report below was 5 milliseconds.

We collected data from April 11, 2007, to May 2, 2007. We initiated experiments every 4 to 6 hours, running each measurement algorithm for 10 minutes each. We simultaneously collected CPU utilization information using vmstat and packet header traces using the DAG systems. Since we ran each algorithm according to a discrete time clock, we also recorded the number of times our measurement process was unable to send probes at an intended time slot (i.e., "slipped" a time slot).

While we collected measurements, there were typically 40–50 active slices on each host, resulting in hundreds of active network connections and very little idle CPU time. Quite often, CPU utilization was at 100% for each node, and there were more than 1000 network connections. A survey of other PlanetLab nodes using CoMon [26] shows that the heavy load we observed is not abnormal. Somewhat surprisingly, there was relatively little network traffic produced by each of these machines. On average, there was about 400 Kb/s inbound traffic to each host, and about 500 Kb/s outbound from each host. (These numbers varied between less than 100 Kb/s to a little more than 1 Mb/s over the periods we collected measurements.)

### 3.2 Results
**Figure 2: Timeseries plots of the median, 90th, and 95th percentile round-trip delays for two example measurement periods.**

Table 2 shows median, 90th, 95th, and 99th percentile delays for a larger subset of measurements. The results shown in the figures and table are qualitatively representative of all results we measured.

In Figure 2 and Table 2, we first see alarmingly high delay values for nearly all quantiles displayed. For example, we see a median RTT of 9 milliseconds in the April 11, 12:20 measurements. This value is surprising since these two machines are colocated, with one switch between them. Second, we observe excessive delay values in the upper quantiles, sometimes larger than 100 milliseconds. (In some measurement periods, we measured maximum delays on the order of 5 seconds.) Finally, we see a high degree of variability in the measurements, in some cases even in the median delay (e.g., in Figure 2a).

It is important to state that in all cases, we measured the network delay of the probes using the DAG cards (i.e., not including delay introduced at the end hosts) to be on the order of 100 microseconds. Clearly, these RTT measurements are very poor indications of the true delay between these two PlanetLab hosts.

We now examine packet loss characteristics between the two live PlanetLab systems. Note that in the experiments using BADABING, we disabled the one-way delay congestion inference mechanisms in the tool, and thus use only actual indications of packet loss. Since BADABING sends three packets back-to-back as a probe, only one packet must be lost in order for a probe to be marked as having experienced loss.

**Figure 3: Timeseries of periods during which consecutive probes experience loss. Plots are shown from two representative measurement periods.**

Notice from the plots that there is a wide range of durations over which consecutive losses are measured, from very short periods (e.g., in Figure 3a) to very long periods (e.g., in Figure 3b). Note also that the rate of all our measurements, including loss, was less than the bandwidth limitation imposed on operational PlanetLab nodes.

**Table 3: Representative results from the BADABING experiments for additional measurement periods.**

The table shows the frequency and duration of loss episodes reported by BADABING, as well as a loss rate estimate based on the heuristic of [39]. We see that there are estimated loss durations on the order of multiple seconds, as suggested by Figure 3. We also see that the loss rate estimates are similar to the frequency estimates. The reason for this effect is that the loss rate of all probe packets during loss episodes is close to 1. That is, during loss episodes, nearly all probe packets are lost. As suggested by results in the table, there were very few measurement periods overall in which we measured zero loss.

As with our round-trip delay experiments, we measured the true packet loss between the PlanetLab hosts using the DAG systems and found in all cases that there was no packet loss (i.e., all probes are observed on the wire just after transmission and just prior to reception by a PlanetLab host). Thus, all loss indications that we observed were due to the host system's inability to handle the probe streams accurately.

**Table 1: Description of the probe algorithms and their accuracy requirements used in experiments.**

| **Probe Algorithm and Description** | **Accuracy Requirements** |
|-------------------------------------|---------------------------|
| **Round-trip delay measurements**   | Consisted of 100-byte UDP probes, sent periodically at 100-millisecond intervals. A process on the remote host bounced probes back to the sender, which then recorded round-trip delay. | Timestamps should be applied in such a way as to minimize the impact of host system load. |
| **Packet-pair measurements**         | Consisted of 1500-byte UDP probes sent with an intended spacing between packets of 120 microseconds (i.e., back-to-back, assuming a capacity of 100 Mb/s). A packet pair was sent at a given time slot with independent probability \( p = 0.2 \), resulting in a geometric process of packet pair transmissions. | Accurate and consistent inter-packet spacing is crucial. |
| **BADABING loss measurements**      | BADABING sends pairs of probes at time slots \( i \) and \( i + 1 \) initiated with independent probability \( p \) which we set to 0.3. Each probe consisted of three packets, each of 600 bytes, sent back-to-back (as quickly as the host system would allow) as described in Sommers et al. [38]. | Accurate and consistent transmission of packet pairs is essential. |

This detailed characterization of the current situation in PlanetLab sets the stage for the design and implementation of MAD, which we will discuss in the next section.