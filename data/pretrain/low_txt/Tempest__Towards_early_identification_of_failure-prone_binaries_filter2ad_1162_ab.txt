### Correlation Analysis

| Correlation Coefficient | -0.438 | 0.268 | 0.383 |
|------------------------|--------|-------|-------|
| Sig. (2-tailed)         | p < 0.0005 | p < 0.0005 | p < 0.0005 |

From the statistical results in Table 2, we observe a negative correlation between the number of green feedbacks and post-release failures, indicating that an increase in the number of green feedbacks is associated with a decrease in post-release field failures. Conversely, for yellow and red feedbacks, we observe a positive correlation, suggesting that an increase in the number of yellow and red feedbacks is associated with an increase in post-release failures. All correlations are statistically significant at the 99% confidence level. This result, derived from using Windows XP-SP1 data on Windows Server 2003, highlights the effectiveness of the color-coded feedback mechanism incorporated into Tempest.

### Tempest Architecture

Figure 4 illustrates the architecture of the Tempest tool. The statistical modeling and analysis are performed at the binary level rather than the source file level for the following reasons:

1. **Robustness and Efficiency**: Binaries are more robust to processing and require less time for analysis.
2. **Accuracy in Mapping Failures**: Binaries are the lowest level at which failures can be accurately mapped back. In the case of source files, a fix for a failure often involves editing multiple files.

**Metrics.lib** is a library built using Vulcan APIs to collect a variety of code metrics. **Binarydiff.lib** extracts added, modified, and deleted blocks of code between two versions of the binary to quantify code churn and build relative code churn measures. **Tempest.exe** contains inbuilt statistical models (for object-oriented and non-object-oriented binaries) and complexity metric feedback standards, which are built using historical data from previous Windows releases. These models take the current code complexity and code churn information as input to predict the failure-proneness of a binary.

The metrics are collected by processing the binaries using symbol information from PDBs (Program Debug Databases) and the Vulcan Framework [18]. The Vulcan framework provides an infrastructure for static and dynamic binary modification, optimization, and analysis. It also offers abstract interfaces to uniformly analyze x86, ia64, x64, and MSIL (Microsoft Intermediate Language) generated by the Microsoft .NET framework, as well as mixed-mode binaries. Code complexity metrics are collected automatically for native and managed binaries on the Win32 platform using the code metrics tool and the binary diff tool [10]. The binary code differentiating algorithm and its relation to source code are discussed in [10].

### Screenshot of Tempest Output

Figure 5 provides a screenshot of the command-line output of Tempest. The output is for a sample binary, `hello.exe`, which opens, closes a file, and writes some text. The output has been manually tweaked to show all different colors. (Note: The output numbers of failure-proneness in this example are not true and are for illustrative purposes only.) The HVMF number in Figure 6 is the ratio of the number of red feedbacks to the total number of feedbacks. In the example, there are three red feedbacks out of a total of 29 metrics, resulting in a ratio of 0.103488. This is shown to demonstrate the tool's usage and to protect confidential Microsoft failure-proneness numbers from being published.

### Discussion of Results

In this section, we discuss the results of using Tempest at Microsoft. There are two typical scenarios in which Tempest is used:

1. **Integration into the Software Development Process**: Individual developers and teams develop features for the next version of Windows. After sufficient testing and quality assurance, these features are integrated into the main build of Windows. The instrumented Windows build is then processed by the Tempest server to estimate the failure-proneness of the binaries. This data is stored for multiple builds of Windows to identify increasing or decreasing trends in failure-proneness among Windows binaries, helping to pinpoint potential problems. We have also observed that such estimates are used for resource allocation and planning in the development and testing process.

2. **Local Usage on Developer Desktops**: Developers run Tempest locally on their desktops to obtain color-coded feedback on their complexity metrics (for locally compiled binaries) relative to the complexity metrics of binaries that did not have failures. Based on this feedback, developers can refactor the complexity of the binary or focus more on testing specific binaries.

### Lessons Learned

Some important lessons learned that may be useful to others in academia and industry include:

- **Scalability**: Metric tools should scale to large software systems and accommodate future expansion.
- **Contextualization**: The system architecture should allow for the building of different contextual models for various systems being analyzed.
- **Consistency**: Measurement tools should ensure consistent metrics across different programming paradigms and languages. Tempest addresses this by working with binaries to account for C, C++, and C# code.
- **Performance**: Performance is crucial for analyzing large code bases in a reasonable amount of time. Tempest, running on a Quad Proc AMD64 Opteron 852 2.6 GHz processor, can process the entire Windows Vista in 1.25 hours.
- **Actionable Insights**: The model should continuously learn from its failures. An iterative statistical algorithm and the presence of a statistician in the research team are desirable.
- **Extensibility**: Tools should be designed for reuse in other reliability tools. For example, the Tempest API is now used within Microsoft for a change impact analysis tool to determine the risk of a change or patch.

Tempest is widely deployed at Microsoft and has been used by various product teams, including Windows Client, Windows Server, Windows Sustained Engineering, and Windows Mobile. As of writing this paper, Tempest has completed more than 3 million runs and analyzed over 150 million lines of code. Future plans include adding dependency analysis and integrating Tempest into the development IDE.

### References

[1] V. Basili, L. Briand, W. Melo, "A Validation of Object-Oriented Design Metrics as Quality Indicators," IEEE Transactions on Software Engineering, 22(10), pp. 751-761, 1996.

[2] T. Bhat, N. Nagappan, "Building Scalable Failure-proneness Models Using Complexity Metrics for Large Scale Software Systems," Proceedings of Thirteenth Asia-Pacific Conference on Software Engineering (APSEC), Bangalore, India, pp. 361-366, 2006.

[3] L. C. Briand, J. Wuest, J. Daly, D. V. Porter, "Exploring the Relationship between Design Measures and Software Quality in Object-Oriented Systems," Journal of Systems and Software, 51(3), pp. 245-273, 2000.

[4] F. Brito e Abreu, "The MOOD Metrics Set," Proceedings of ECOOP '95 Workshop on Metrics, 1995.

[5] S. R. Chidamber, C. F. Kemerer, "A Metrics Suite for Object-Oriented Design," IEEE Transactions on Software Engineering, 20(6), pp. 476-493, 1994.

[6] K. El Emam, S. Benlarbi, N. Goel, S. N. Rai, "The Confounding Effect of Class Size on the Validity of Object-Oriented Metrics," IEEE Transactions on Software Engineering, 27(6), pp. 630-650, 2001.

[7] N. E. Fenton, N. Ohlsson, "Quantitative Analysis of Faults and Failures in a Complex Software System," IEEE Transactions on Software Engineering, 26(8), pp. 797-814, 2000.

[8] R. Harrison, S. J. Counsell, R. V. Nithi, "An Evaluation of the MOOD Set of Object-Oriented Software Metrics," IEEE Transactions on Software Engineering, 24(6), pp. 491-496, June 1998.

[9] T. M. Khoshgoftaar, E. B. Allen, N. Goel, A. Nandi, J. McMullan, "Detection of Software Modules with High Debug Code Churn in a Very Large Legacy System," Proceedings of International Symposium on Software Reliability Engineering, pp. 364-371, 1996.

[10] S. McFurling, K. Pierce, Z. Wung, "BMAT â€“ A Binary Matching Tool," Microsoft Research Technical Report MSR-TR-99-83, 1999.

[11] N. Nagappan, T. Ball, "Use of Relative Code Churn Measures to Predict System Defect Density," Proceedings of International Conference on Software Engineering, pp. 284-292, 2005.

[12] N. Nagappan, T. Ball, A. Zeller, "Mining Metrics to Predict Component Failures," Proceedings of International Conference on Software Engineering, pp. 452-461, 2006.

[13] M. C. Ohlsson, A. von Mayrhauser, B. McGuire, C. Wohlin, "Code Decay Analysis of Legacy Software through Successive Releases," Proceedings of IEEE Aerospace Conference, pp. 69-81, 1999.

[14] M. C. Ohlsson, C. Wohlin, "Identification of Green, Yellow, and Red Legacy Components," Proceedings of International Conference on Software Maintenance, pp. 6-15, 1998.

[15] N. Ohlsson, H. Alberg, "Predicting Fault-Prone Software Modules in Telephone Switches," IEEE Transactions on Software Engineering, 22(12), pp. 886-894, 1996.

[16] T. J. Ostrand, E. J. Weyuker, R. M. Bell, "Where the Bugs Are," Proceedings of the International Symposium on Software Testing and Analysis, pp. 86-96, 2004.

[17] J. Sliwerski, T. Zimmermann, A. Zeller, "HATARI: Raising Risk Awareness (Research Demonstration)," Proceedings of the European Software Engineering Conference/International Symposium on Foundations of Software Engineering, Lisbon, Portugal, pp. 107-110, 2005.

[18] A. Srivastava, A. Edwards, H. Vo, "Vulcan: Binary Transformation in a Distributed Environment," Microsoft Research Technical Report MSR-TR-2001-50, 2001.

[19] M.-H. Tang, M.-H. Kao, M.-H. Chen, "An Empirical Study on Object-Oriented Metrics," Proceedings of Sixth International Software Metrics Symposium, pp. 242-249, 1999.

---

This revised text aims to improve clarity, coherence, and professionalism while maintaining the original content and structure.