### Sanitization Methods and Robust Learning Techniques

Carlie et al. [12] discuss sanitization methods for time-based anomaly detectors, where multiple micro-models are constructed and compared over time to identify poisoned data. Their system assumes that the attacker can only control data generated within a limited time window.

In the machine learning and statistics communities, early work on robustness to noise includes the extension of the PAC model by Kearns and Li [30], as well as research in robust statistics [7], [27], [51], [55]. In adversarial settings, robust methods for handling arbitrary data corruptions have been proposed for linear regression [11], high-dimensional sparse regression [10], logistic regression [16], and linear regression with low-rank feature matrices [33]. These methods rely on assumptions about the training data, such as sub-Gaussian distribution, independent features, and low-rank feature spaces. Biggio et al. [5] pioneered the study of optimizing poisoning attacks for kernel-based learning algorithms like SVM. Similar techniques were later extended to optimize data poisoning attacks for other important learning algorithms, including feature selection for classification [54], topic modeling [35], autoregressive models [1], collaborative filtering [32], and simple neural network architectures [38].

### Conclusions

We conducted the first systematic study on poisoning attacks and their countermeasures for linear regression models. We introduced a new optimization framework for poisoning attacks and a fast statistical attack that requires minimal knowledge of the training process. Additionally, we developed a principled approach for a new robust defense algorithm that significantly outperforms existing robust regression methods. Our proposed attack and defense algorithms were extensively evaluated on several datasets from healthcare, loan assessment, and real estate domains. We demonstrated the real-world implications of poisoning attacks through a case study in a health application. We believe our work will inspire future research towards developing more secure learning algorithms against poisoning attacks.

### Acknowledgements

We thank Ambra Demontis for confirming the attack results on ridge regression, and Tina Eliassi-Rad, Jonathan Ullman, and Huy Le Nguyen for discussing poisoning attacks. We also thank the anonymous reviewers for their extensive feedback during the review process.

This work was supported in part by FORCES (Foundations Of Resilient CybEr-Physical Systems), which receives support from the National Science Foundation (NSF award numbers CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166), DARPA under grant no. FA8750-17-2-0091, Berkeley Deep Drive, and the Center for Long-Term Cybersecurity. This work was also partly supported by the EU H2020 project ALOHA, under the European Union’s Horizon 2020 research and innovation program (grant no. 780788).

### References

[1] S. Alfeld, X. Zhu, and P. Barford. Data poisoning attacks against autoregressive models. In AAAI, 2016.
[2] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar. Can machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pages 16–25. ACM, 2006.
[3] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In H. Blockeel, K. Kersting, S. Nijssen, and F. Železný, editors, Machine Learning and Knowledge Discovery in Databases (ECML PKDD), Part III, volume 8190 of LNCS, pages 387–402. Springer Berlin Heidelberg, 2013.
[4] B. Biggio, G. Fumera, and F. Roli. Security evaluation of pattern classifiers under attack. IEEE Transactions on Knowledge and Data Engineering, 26(4):984–996, April 2014.
[5] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In ICML, 2012.
[6] B. Biggio and F. Roli. Wild patterns: Ten years after the rise of adversarial machine learning. ArXiv e-prints, 2018.
[7] E. J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component analysis. Journal of the ACM, 58(3), 2011.
[8] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In Proc. IEEE Security and Privacy Symposium, S&P, 2017.
[9] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted backdoor attacks on deep learning systems using data poisoning. ArXiv e-prints, abs/1712.05526, 2017.
[10] Y. Chen, C. Caramanis, and S. Mannor. Robust high dimensional sparse regression and matching pursuit. arXiv:1301.2725, 2013.
[11] Y. Chen, C. Caramanis, and S. Mannor. Robust sparse regression under adversarial corruption. In Proc. International Conference on Machine Learning, ICML, 2013.
[12] G. F. Cretu-Ciocarlie, A. Stavrou, M. E. Locasto, S. J. Stolfo, and A. D. Keromytis. Casting out demons: Sanitizing training data for anomaly sensors. In Proc. IEEE Security and Privacy Symposium, S&P, 2008.
[13] I. Csiszár and G. Tusnády. Information geometry and alternating minimization procedures. Statistics and Decisions, 1:205–237, 1984.
[14] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al. Adversarial classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 99–108. ACM, 2004.
[15] D. Faggella. 2017 Machine learning beyond and healthcare applications. https://www.techemergence.com/machine-learning-healthcare-applications/, 2016.
[16] J. Feng, H. Xu, S. Mannor, and S. Yan. Robust logistic regression and classification. In Advances in Neural Information Processing Systems, NIPS, 2014.
[17] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981.
[18] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM Conference on Computer and Communications Security, CCS, 2015.
[19] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In USENIX Security, pages 17–32, 2014.
[20] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv:1412.6572, 2014.
[21] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. In NIPS Workshop on Machine Learning and Computer Security, volume abs/1708.06733, 2017.
[22] S. Hao, A. Kantchelian, B. Miller, V. Paxson, and N. Feamster. PREDATOR: Proactive recognition and elimination of domain abuse at time-of-registration. In Proceedings of the 23rd ACM Conference on Computer and Communications Security, CCS, 2016.
[23] P. Harsha. Senate committee examines the “dawn of artificial intelligence”. Computing Research Policy Blog. http://cra.org/govaffairs/blog/2016/11/senate-committee-examines-dawn-artificial-intelligence/, 2016.
[24] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2009.
[25] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pages 43–58. ACM, 2011.
[26] P. J. Huber. Robust estimation of a location parameter. Annals of Statistics, 53(1):73–101, 1964.
[27] P. J. Huber. Robust statistics. Springer, 2011.
[28] Kaggle. House Prices: Advanced Regression Techniques. https://www.kaggle.com/c/house-prices-advanced-regression-techniques. Online; accessed 8 May 2017.
[29] W. Kan. Lending Club Loan Data. https://www.kaggle.com/wendykan/lending-club-loan-data, 2013. Online; accessed 8 May 2017.
[30] M. Kearns and M. Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4):807–837, 1993.
[31] M. Kloft and P. Laskov. Security analysis of online centroid anomaly detection. The Journal of Machine Learning Research, 13(1):3681–3724, 2012.
[32] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. In Advances in Neural Information Processing Systems, pages 1885–1893, 2016.
[33] C. Liu, B. Li, Y. Vorobeychik, and A. Oprea. Robust linear regression against training data poisoning. In Proc. Workshop on Artificial Intelligence and Security, AISec, 2017.
[34] D. Lowd and C. Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641–647. ACM, 2005.
[35] S. Mei and X. Zhu. The security of latent Dirichlet allocation. In AISTATS, 2015.
[36] S. Mei and X. Zhu. Using machine teaching to identify optimal training-set attacks on machine learners. In 29th AAAI Conf. Artificial Intelligence (AAAI '15), 2015.
[37] M. Mozaffari Kermani, S. Sur-Kolay, A. Raghunathan, and N. K. Jha. Systematic poisoning attacks on and defenses for machine learning in healthcare. IEEE Journal of Biomedical and Health Informatics, 19(6):1893–1905, 2014.
[38] L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In B. M. Thuraisingham, B. Biggio, D. M. Freeman, B. Miller, and A. Sinha, editors, 10th ACM Workshop on Artificial Intelligence and Security, AISec '17, pages 27–38, New York, NY, USA, 2017. ACM.
[39] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein, U. Saini, C. Sutton, J. Tygar, and K. Xia. Exploiting machine learning to subvert your spam filter. In Proc. First USENIX Workshop on Large-Scale Exploits and Emergent Threats, LEET, 2008.
[40] A. Newell, R. Potharaju, L. Xiang, and C. Nita-Rotaru. On the practicality of integrity attacks on document-level sentiment analysis. In Proc. Workshop on Artificial Intelligence and Security, AISec, 2014.
[41] J. Newsome, B. Karp, and D. Song. Paragraph: Thwarting signature learning by training maliciously. In Recent advances in intrusion detection, pages 81–105. Springer, 2006.
[42] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The limitations of deep learning in adversarial settings. In Proc. IEEE European Security and Privacy Symposium, Euro S&P, 2017.
[43] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Proc. IEEE Security and Privacy Symposium, S&P, 2016.
[44] R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. Sharif. Misleading worm signature generators using deliberate noise injection. In Proc. IEEE Security and Privacy Symposium, S&P, 2006.
[45] PharmGKB. Downloads - IWPC Data. https://www.pharmgkb.org/downloads/, 2014. Online; accessed 8 May 2017.
[46] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S. Hon Lau, S. Rao, N. Taft, and J. D. Tygar. ANTIDOTE: Understanding and defending against poisoning of anomaly detectors. In Proc. 9th Internet Measurement Conference, IMC, 2009.
[47] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In Proc. IEEE Security and Privacy Symposium, S&P, 2017.
[48] N. Šrndić and P. Laskov. Mimicus - Contagio Dataset. https://github.com/srndic/mimicus, 2009. Online; accessed 8 May 2017.
[49] N. Šrndić and P. Laskov. Practical evasion of a learning-based classifier: A case study. In Proc. IEEE Security and Privacy Symposium, S&P, 2014.
[50] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2014.
[51] D. E. Tyler. Robust statistics: Theory and methods. Journal of the American Statistical Association, 103(482):888–889, 2008.
[52] S. Venkataraman, A. Blum, and D. Song. Limits of learning-based signature generation with adversaries. In Network and Distributed System Security Symposium, NDSS. Internet Society, 2008.
[53] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao. Man vs. machine: Practical adversarial detection of malicious crowdsourcing workers. In 23rd USENIX Security Symposium (USENIX Security 14), San Diego, CA, 2014. USENIX Association.
[54] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. Is feature selection secure against training data poisoning? In Proc. 32nd International Conference on Machine Learning, volume 37 of ICML, pages 1689–1698, 2015.
[55] H. Xu, C. Caramanis, and S. Mannor. Robust regression and Lasso. IEEE Transactions on Information Theory, 56(7):3561–3574, 2010.

### Appendix A: Theoretical Analysis of Linear Regression

We prove the equivalence of \(W_{tr}\) and \(W'_{tr}\) with the following theorem.

**Theorem 3.** Consider OLS regression. Let \(D_{tr} = \{X, Y\}\) be the original dataset, \(\theta_0 = (w_0, b_0)\) the parameters of the original OLS model, and \(D'_{tr} = \{X, Y'\}\) the dataset where \(Y'\) consists of predicted values from \(\theta_0\) on \(X\). Let \(D_p = \{X_p, Y_p\}\) be a set of poisoning points. Then

\[ \arg \min_\theta L(D_{tr} \cup D_p, \theta) = \arg \min_\theta L(D'_{tr} \cup D_p, \theta) \]

Furthermore, we have \(\frac{\partial W_{tr}}{\partial z} = \frac{\partial W'_{tr}}{\partial z}\), where \(z = (x_c, y_c)\).

Then the optimization problem for the adversary, and the gradient steps the adversary takes, are the same whether \(W_{tr}\) or \(W'_{tr}\) is used.

**Proof.** We begin by showing that

\[ \arg \min_\theta L(D_{tr}, \theta) = \arg \min_\theta L(D'_{tr}, \theta) \]

By definition, we have \(\theta_0 = \arg \min_\theta L(D_{tr}, \theta)\). In \(Y'\), \(y'_i = f(x_i, \theta_0)\), so \(L(D'_{tr}, \theta_0) = 0\). But \(L \geq 0\), so

\[ \theta_0 = \arg \min_\theta L(D'_{tr}, \theta) \]

We can use this to show that \(X^T Y = X^T Y'\). Recall that the closed-form expression for OLS regression trained on \(X, Y\) is \(\theta = (X^T X)^{-1} X^T Y\). Because \(\theta_0\) is the OLS model for both \(D_{tr}\) and \(D'_{tr}\), we have

\[ (X^T X)^{-1} X^T Y = (X^T X)^{-1} X^T Y' \]

But \((X^T X)^{-1}\) is invertible, so \(X^T Y = X^T Y'\). We can use this to show that \(\arg \min_\theta L(D_{tr} \cup D_p, \theta) = \arg \min_\theta L(D'_{tr} \cup D_p, \theta)\) for any \(D_p\). Consider the closed-form expression for the model learned on \(D_{tr} \cup D_p\):

\[ \theta = (X^T X + X_p^T X_p)^{-1} (X^T Y + X_p^T Y_p) \]

which is exactly the model learned on \(D'_{tr} \cup D_p\). So the learned models for the two poisoned datasets are the same. Note that this also holds for ridge regression, where the Hessian has a \(\lambda I\) term added, so it is also invertible.

\[ (X^T X + X_p^T X_p)^{-1} (X^T Y + X_p^T Y_p) = (X^T X + X_p^T X_p)^{-1} (X^T Y' + X_p^T Y_p) \]

We proceed to use \(X^T Y = X^T Y'\) again to show that