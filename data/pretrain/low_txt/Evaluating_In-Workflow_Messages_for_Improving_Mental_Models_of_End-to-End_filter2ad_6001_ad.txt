### USENIX Association
**30th USENIX Security Symposium**

#### Application Use
After installation, participants were instructed to use the app to chat with their assigned conversation partner daily for 20 days. To qualify for compensation (details below), participants needed to send at least five messages per day, with the first and last messages at least 10 minutes apart. Participants were advised not to share any private information and were informed that researchers would monitor some app usage, such as interactions with UI elements and message counts. (See Section 5.1 for instrumentation details.)

Researchers typically engaged in brief, non-security-related conversations with participants each day, covering topics like hobbies, daily news (non-political), and sports. Occasionally, participants were asked to initiate the next day’s conversation themselves (three times per participant) or to start a chat with a different researcher (two times per participant). This was designed to ensure that participants spent time on the screen displaying all conversations, rather than just within a specific chat. These conversation patterns were intended to trigger informational messages for the experimental group (see Section 5.1).

#### Exit Questionnaire
Twenty days after installation, we posted the exit questionnaire on Proliﬁc and reminded participants via TextLight to complete it. The exit questionnaire included a re-administered version of the communications privacy questionnaire (Section 5.2) to measure post-intervention mental models. We also asked about participants' experiences with the app, including the System Usability Scale [11], their thoughts on potential users of TextLight, any bugs or glitches they noticed, and whether they observed "any informative messages or prompts." Additionally, we inquired about their understanding of the study's purpose.

#### Interviews
Participants in the experimental condition who completed at least 18 out of 20 conversation days were invited to an exit interview, which lasted an average of less than 14 minutes. The interviews aimed to explore participants’ mental models of end-to-end (e2e) encryption and their experiences with the app in more depth. We started with usability and general evaluation questions, followed by inquiries about our intervention messages. We structured these questions to test recall without reminding participants of the messages. We then showed blurred screenshots as prompts for recall and asked about the content, frequency, and interest in the messages. Finally, we inquired about their understanding of e2e encryption and its protections.

#### Compensation
Participants received $0.70 for completing the pre-screener, $2.00 for the initial questionnaire, $8.00 for installing the app and sending the first message, $1.50 per successful conversation day, $5.00 for the exit questionnaire, and $15.00 for completing an interview. Participants who dropped out before completing the exit questionnaire did not receive compensation for conversation days. The average compensation for those who completed the entire study was $48.30 (σ = $9.20), while those who started but did not complete received an average of $8.40 (σ = $3.90).

#### Pilot Testing
We conducted three in-person pilots for the initial questionnaire and installation tutorial, two partially in-person pilots covering the entire study with 10 conversation days, and five fully online pilots with seven conversation days. In-person pilots were recruited from convenience samples, while online ones were recruited from Proliﬁc. Pilot testing helped refine study procedures, educational message content and placement, and questionnaire wording.

#### Data Analysis
For the app study, we used a simplified version of the survey study analysis, focusing on one experimental group. We analyzed differences between pre- and post-intervention questionnaires. We confirmed the capability groupings from the survey study (Section 3.3) and used two-tailed pairwise Mann-Whitney U (MWU) tests to compare the control and experimental conditions for each adversary-capability set, reporting significance and effect size via location-shift estimates.

To check if our educational messages reduced TextLight’s usability, we compared System Usability Scale (SUS) scores between the control and experimental conditions using the Mann-Whitney test for Equivalence (MWE) [68]. Unlike traditional hypothesis testing, the null hypothesis here is that the two samples are different; if significant, they are likely drawn from the same distribution. We applied the stricter equivalence range suggested by Wellek [68].

Interviews were transcribed by a third-party service. Two researchers coded the transcripts using an open-coding approach [14]. They established an initial codebook based on five randomly selected transcripts [51] and independently coded two randomly selected interviews at a time to establish inter-rater reliability. After each batch, they met to resolve differences and update the codebook. Once reliability was established (α ≥ .8 [34]), they coded two more interviews without resolving differences, bringing the set used for reliability to approximately 20% of the interviews. We obtained a Krippendorff’s α of .89.

For the open-ended questions from the pre- and post-intervention questionnaires, we collaboratively coded them [39]. There is some overlap between the interview and survey codebooks, and we reused established codes when applicable.

#### Ethical Considerations
This study was approved by the University of Maryland IRB. We followed standard ethics procedures, including obtaining consent, allowing participants to leave the study with partial compensation, minimizing identifiable information collection, and keeping all potentially identifiable information on password-protected systems.

We considered pairing participants for less mediated conversations but decided against it to prevent inappropriate messages. We disabled certain Signal features and asked participants not to share private information during daily conversations. These decisions may limit ecological validity but were ethically necessary.

We collected demographic information (age, ethnicity, gender) to report on sample representativeness (Sections 4.1 and 6.1) and offered "prefer not to answer" options.

#### Limitations
The app study addressed some ecological validity limitations of the survey study but had other typical limitations. Our U.S.-based Proliﬁc sample may not be representative of messaging app users, and the study was limited to Android users. While we attempted to approximate realistic use, texting researchers is not the same as using a messaging app with friends and family.

Instructing participants not to share private information and alerting them to our instrumentation may reduce trust in e2e encryption and introduce bias. This may also reduce participants' investment in the privacy of TextLight communications, limiting interest in our educational messages. However, this was unavoidable for ethical reasons. Our instrumentation is similar to the employee adversary and metadata capability we asked about, applying to both experimental and control conditions.

When asked about the study's purpose, most participants (n=41) assumed we were testing a messaging application, and only three mentioned the educational messages. This suggests demand effects were not relevant to our research questions.

Non-parametric hypothesis tests have limited power, meaning subtle shifts in mental models may not manifest in test results. A priori power analysis indicated 30 participants per group would be enough to detect large effects (Cohen’s d = 0.8 [15]) with 80% power but not enough to meet the same standards as the survey study. Instead, we recruited people less knowledgeable about e2e encryption for more obvious mental model changes and gathered extensive qualitative data (interviews, open-ended survey questions) to add depth to our results.

#### Results
We detail the results of the app study below.

##### Participants
We received 261 prescreening responses, of which 89 qualified and 84 were invited to the main study. We invited participants in batches, stopping once we had at least 65 actively using TextLight. Sixty-eight participants started the main study, and five were disqualified for missing too many conversation days or uninstalling TextLight. In total, 61 participants (32 experimental, 29 control) completed the exit questionnaire. We invited 23 of the 32 experimental participants for an interview, and 19 agreed to participate. Data was collected in April and May 2020. Table 2 shows the demographics of our app study participants, which are similar to the survey study.

##### Using TextLight
Most participants used the app as intended, completing an average of 18.5 conversation days (σ = 3.3) with 156.0 minutes (σ = 135.1) of screen time in TextLight. They spent an average of 139.2 minutes (σ = 122.6) in the conversation screen and sent an average of 138.2 messages (σ = 44.9), more than the required 100 over 20 days.

To investigate whether the educational messages interfered with usability, we compared SUS scores of the experimental and control groups using the MWE test. We found no difference in usability (p = 0.026). Interviewees (experimental condition only) generally found TextLight easy to use (n=19), professionally designed (n=12), and similar to other messaging apps (n=11). Only one participant noted e2e encryption when comparing TextLight to other apps. When asked about standout features, five mentioned our educational messages, and two mentioned security features. In the exit questionnaire, 39 of 61 participants (23 experimental, 16 control) mentioned privacy or security, with 11 (6 experimental, 5 control) mentioning the need for security and privacy for professionals. A large minority (n=26, 10 experimental, 16 control) mentioned general-purpose users unrelated to privacy or security.

##### Encountering Educational Messages
Experimental participants saw an average of 19.4 e2e encryption messages, with 10.7 in-conversation, 6.5 in the conversation list, 1.4 in the profile, and 0.9 long messages. Long messages, requiring explicit action, were seen by 18 participants, opened 1.6 times, and displayed for an average of 19.0 seconds (σ = 26.71). All 32 participants saw all three other kinds of messages, and all five short message versions were viewed approximately the same number of times (∼3.7).

In the exit questionnaire, most experimental participants (n=23) said they saw "informative messages or prompts," while others did not see (n=3) or remember (n=5) them. Most (n=21) remembered the messages were about e2e encryption, though two described unrelated messages. Interviews provided more insights: 13 of 19 interviewees recalled the messages without prompting, with seven describing conversation-list messages and eight describing in-conversation messages (some overlap). After being shown blurred screenshots, four remembered the profile message, 17 remembered in-conversation messages, and 13 recalled conversation-list messages. Participants who remembered the messages (n=17) generally saw them every day (n=7) or every second day (n=6).

However, most paid little attention to the messages. Four participants explicitly said they ignored the educational messages, and seven gave responses indicating habituation. When asked if they were intrigued, seven said they weren't interested, and six said they were (possibly influenced by demand effects). Only two participants clicked on short messages to "learn more," though logs show 18 participants accessed the long message, and three accessed it through the settings menu. Most participants (n=8) thought there was only one message version, while six did not recall. Six mentioned e2e encryption but could not provide specifics, and a few mentioned specific concepts: metadata weakness (n=3), unreadable sent messages (n=2), and endpoint-only readability (n=2). Others mistakenly reported the messages were about using TextLight or simply did not remember.

These comments suggest participants noticed the messages but did not examine them carefully, as might be expected in real-world scenarios.

##### Mental Models of e2e Encryption
We found no statistical evidence that our educational messages improved mental models. Interviews with experimental participants shed light on why the messages were less effective than hoped.

Only one significant difference in perceptions of adversary capability was found (Table 4). Experimental participants were somewhat less likely to believe app-company employees could observe metadata (p=0.03; location shift estimate −1), which is a change in the wrong direction. This aligns with our survey-study observation that short messages can sometimes oversell e2e encryption benefits. A closer look at effect sizes shows that the largest effect sizes in the survey study (Interception capabilities of Employee and Government) were not replicated.

### Conclusion
Our study aimed to evaluate the effectiveness of educational messages in improving mental models of e2e encryption. While participants noticed the messages, they did not examine them carefully, and we found no significant improvements in mental models. Future work should focus on more engaging and contextually relevant educational methods.