### Refined Samples and Partial Real Data for Training Attack Models

Refined samples and partial real data are utilized to train the attack model for white-box accuracy extraction. We refer to black-box fidelity extraction in Section 5 as black-box fidelity extraction.

It is important to note that, in real attack scenarios, we cannot directly choose the lowest accuracy value due to the unavailability of the training dataset from the target models. Therefore, the accuracy value reported in this paper is selected when its corresponding fidelity value is the lowest during the training process.

### 6.3 Results

Figure 6 presents the results of the MH (Metropolis-Hastings) accuracy extraction and white-box accuracy extraction on both the CelebA and LSUN-Church datasets. It also includes the black-box fidelity extraction for comparison. The results show that MH subsampling is an effective approach to improve the accuracy of attack models. For example, when the target model is SNGAN, the MH accuracy extraction significantly enhances the attack model's accuracy on both datasets because the MH subsampling algorithm selects high-quality samples from the generated samples of the target model SNGAN. Both MH accuracy extraction and white-box accuracy extraction use refined samples during training, but the white-box accuracy extraction further improves accuracy. This is because partial real data can correct the distribution of the attack model, making it closer to the real distribution. Similar to the fidelity extraction discussed in Section 5.3.3, we also analyze distribution differences for accuracy extraction, which is shown in Figure 10 in the Appendix.

**Figure 6: Comparison of Accuracy for Different Attack Approaches**
- (a) Accuracy on CelebA
- (b) Accuracy on LSUN-Church

### 7 Case Study: Model Extraction Based Transfer Learning

In this section, we present a case study where the extracted model serves as a pre-trained model, and adversaries transfer knowledge from the extracted model to new domains through fine-tuning, broadening the scope of applications based on the extracted models. We start with methods of transfer learning on GANs and demonstrate how adversaries can benefit from model extraction, in addition to directly leveraging the extracted model to generate images.

We consider the state-of-the-art GAN model StyleGAN [32], which was trained on more than 3 million bedroom images, as the target model. StyleGAN produces high-quality images at a resolution of 256 × 256, with a Fréchet Inception Distance (FID) of 2.65 on the LSUN-Bedroom dataset [71]. We assume that adversaries can only query the target model StyleGAN and have no other background knowledge, which we refer to as black-box fidelity extraction. Although an adversary can obtain an extracted model, the model can only generate images similar to the target model, i.e., bedroom images. Therefore, the adversary's goal is to use PGGAN as the attack model to extract the target model StyleGAN and leverage transfer learning to obtain a more powerful GAN that generates images the adversary desires. The attack is successful if the performance of the model trained by transfer learning based on the extracted GAN outperforms the model trained from scratch.

**Table 6: Comparison between Transfer Learning Based on Model Extraction and Training from Scratch**
- **Target Dataset:**
  - LSUN-Kitchen
  - LSUN-Classroom
- **Methods:**
  - Transfer Learning
  - Training from Scratch
- **FID:**
  - Transfer Learning: 7.59
  - Training from Scratch: 8.83
  - Transfer Learning: 16.47
  - Training from Scratch: 20.34

Transferring knowledge from stolen state-of-the-art models to new domains where adversaries wish the GAN model to generate other types of images can bring at least two benefits:
1. If adversaries have too few images for training, they can easily obtain a better GAN model on a limited dataset through transfer learning.
2. Even if adversaries have sufficient training data, they can still obtain a better GAN model through transfer learning compared to a GAN model trained from scratch.

We consider two variants of this attack:
1. One where the adversary owns a small target dataset (i.e., about 50K images in our work).
2. Another where the adversary has enough images (i.e., about 168k images in our work).

Specifically, after querying the target model StyleGAN and obtaining 50K generated images, adversaries train their attack model PGGAN on the obtained data, as illustrated in Section 5.2. Here, the fidelity of the attack model PGGAN is 4.12, and its accuracy is 6.97. We then use the extracted model’s weights as initialization to train a model on the adversary’s own dataset, also called the target dataset in this section. We conduct the following two experiments:

1. We first randomly select 50K images from the LSUN-Kitchen dataset as a limited dataset. Then, we train the model on these selected data using both transfer learning and from scratch.
2. We train a model on the LSUN-Classroom dataset, which includes about 168k images, using both transfer learning and from scratch.

**Results:**
- Table 6 shows the performance of models trained by transfer learning and from scratch. We observe that the performance of models trained by transfer learning is consistently better than that of models trained from scratch, both on large and small target datasets.
- On the limited LSUN-Kitchen dataset containing 50K images, the FID of the model trained by transfer learning decreases from 8.83 to 7.59, compared to the model trained from scratch.
- On the large LSUN-Classroom dataset containing more than 168k classroom images, the performance of the model significantly improves from 20.34 FID (training from scratch) to 16.47 FID (transfer learning). This is the best performance for PGGAN on the LSUN-Classroom dataset, compared to 20.36 FID reported by Karras et al. [31].

**Figure 7: Comparison between Transfer Learning Based on Model Extraction and Training from Scratch on LSUN-Kitchen and LSUN-Classroom Datasets**
- (a) FID on LSUN-Kitchen
- (b) FID on LSUN-Classroom

We consistently observe that training by transfer learning based on model extraction is always better than training from scratch during the training process. This indicates that the extracted model PGGAN, which duplicates the state-of-the-art StyleGAN on the LSUN-Bedroom dataset, can play a significant role in other applications beyond generating bedroom images. This highlights the severe violation of intellectual property rights of the model owners.

### 8 Defenses

Model extraction attacks on GANs leverage generated samples from a target GAN model to retrain a substitute GAN with similar functionalities. In this section, we introduce defense techniques to mitigate model extraction attacks against GANs.

According to the adversary's goals defined in Section 4.1, we discuss defense measures from two aspects: fidelity and accuracy.

- **Fidelity of Model Extraction:** It is challenging for model owners to defend against fidelity attacks except by limiting the number of queries. Adversaries can design an attack model to learn the distribution based on their obtained samples, and the more generated samples they obtain, the more effective their attack becomes.
- **Accuracy of Model Extraction:** Its effectiveness is mainly because adversaries can obtain samples generated by latent codes drawn from the prior distribution of the target model, and these samples are close to the real data distribution [2]. However, if adversaries obtain generated samples that are only representative of partial real data distribution or a distorted distribution, the accuracy of attack models decreases. Based on this, we propose two types of perturbation-based defense mechanisms: input perturbation-based and output perturbation-based approaches. In the rest of this section, we focus on defense approaches designed to mitigate the accuracy of attack models.

#### 8.1 Methodology

**Input Perturbation-Based Defenses:**
- **Linear Interpolation Defense:** For \( n \) latent codes queried from users, model providers randomly select two queried points and interpolate \( k \) points between them. This process is repeated \( \lceil n/k \rceil \) times to get \( n \) modified latent codes. These modified latent codes are used to query the target model. In our experiments, we interpolate 9 points. See Figure 14(a) in the Appendix for visualization.
- **Semantic Interpolation Defense:** Unlike linear interpolation, semantic interpolation returns various predefined semantic images, restricting the space of images the adversary can query. Semantic information can include any human-perceivable attributes, such as gender, age, and hair style. We adopt the semantic interpolation algorithm proposed by Shen et al. [58]. In our experiments, we explore 12 semantic information on the CelebA dataset. See Figure 14(b) in the Appendix for visualization.

**Output Perturbation-Based Defenses:**
- **Random Noise:** Adding random noise to generated samples is a straightforward method. In our experiments, we use Gaussian-distributed additive noise (mean = 0, variance = 0.001).
- **Adversarial Noise:** We generate adversarial examples through targeted attacks where all images are misclassified into a particular class by the classifier ResNet-50 trained on the ImageNet dataset. In our experiments, all face images are misclassified into the "goldfish" class using the C&W algorithm [11] based on L2 distance.
- **Filtering:** The Gaussian filter is used to process generated samples. In our experiments, we use a Gaussian filter (sigma = 0.4) provided by the skimgae package [66].
- **Compression:** The JPEG compression algorithm is used to process generated samples. In our experiments, we use the JPEG compression (quality = 85) provided by the simplejpeg package [17].

#### 8.2 Results

In this experiment, we choose PGGAN trained on the CelebA dataset as the target model to evaluate our defense techniques, considering its excellent performance among our target models. We only evaluate the performance of the attack model PGGAN under various defenses. Figure 8 plots the results, showing that the attack performance is weakened when the target model PGGAN uses these defense approaches, compared to the target model without any defenses. Gaussian noise and semantic interpolation defenses show stable performance, while other defense techniques' performance gradually weakens with an increase in the number of queries. Figure 12 in the Appendix also shows similar defense performance for the attack model SNGAN. We further evaluate the defense utility, i.e., the quality of generated images after deploying defense measures. Our quantitative and qualitative measures show that these defense techniques do not impact the visual quality of generated images (see Figure 14, Figure 15, and Table 10). More details are shown in Appendix A.5.2.

**Figure 8: Performance of Attack Model PGGAN Under Various Defenses**

**Discussion:**
- **Input Perturbation-Based Defenses:** These defenses work by increasing the similarity of generated samples and creating a distribution mismatch between latent codes produced by interpolation and those drawn from the prior distribution. Interpolation operations increase the similarity of images, and latent codes produced by linear operations do not obey the prior distribution of the target model, which helps disguise the true data distribution [2].
- **Output Perturbation-Based Defenses:** These defenses work by directly perturbing the generated samples. In practice, this type of defense requires model providers to balance image quality and the model's security through the magnitude of changes. Although Gaussian noise defense shows the best performance, it is possible for adversaries to remove the noise.

### 9 Conclusion

In this paper, we systematically studied the problem of model extraction attacks on generative adversarial networks, devising, implementing, and evaluating these attacks from the perspectives of fidelity extraction and accuracy extraction. Extensive experimental evaluations show that adversaries can achieve excellent performance with about 50K queries for fidelity extraction. For accuracy extraction, adversaries further improve the accuracy of attack models after obtaining additional background knowledge, such as partial real data from the training set or the discriminator of the target model. Furthermore, we performed a case study where the attack model, which steals a state-of-the-art target model, can be transferred to new domains to broaden the scope of applications based on extracted models.

These effective attacks motivate us to design two types of defense techniques: input and output perturbation-based defenses. They mitigate model extraction attacks by perturbing latent codes and generated samples, respectively. Extensive experimental evaluations show that semantic interpolation and Gaussian noise defenses achieve stable performance.

Finally, we identify several directions for future work. Because GAN models are generally considered intellectual properties of model owners, protecting GANs through verifying ownership is an interesting direction. Additionally, stealing a GAN model means the leakage of the training set distribution. Therefore, training with differential privacy techniques can be utilized to protect the privacy of the training data. However, training time and stability of the training process are significant challenges for GANs. For future work, we plan to design new methods based on these findings.