### Introduction
Given the prevalence of spam on Twitter, this study examines the effectiveness of blacklists in mitigating the spread of unsolicited messages. Currently, Twitter relies on Google’s SafeBrowsing API to block malicious links, but this filtering only suppresses links that are blacklisted at the time of posting. Twitter does not retroactively blacklist links, allowing previously undetected malicious URLs to persist. To evaluate the extent to which tweets slip through Twitter’s defenses and to compare the performance of different blacklists, we analyze several characteristics, including delay, susceptibility to evasion, and limitations when filtering is restricted to domains rather than full paths.

### 6.1 Blacklist Delay
Using historical data from URIBL, Joewein, and Google blacklists, we measure the delay between a tweet's posting and the subsequent blacklisting of any embedded spam URLs. We define a blacklist as leading Twitter if it flags a URL before it appears on Twitter, and lagging if the URL is posted before being blacklisted. The lead and lag times are crucial for determining the efficiency of blacklists. Long lag periods require maintaining a large index of stale tweets to retroactively locate spam, and can result in limited protection unless spammers reuse links even after they appear on blacklists.

#### Table 4: Blacklist Performance (Tweets)
| Link Statistics         | URIBL   | Google Malware | Google Phishing |
|-------------------------|---------|----------------|-----------------|
| Flagged before posting  | 27.17%  | 7.56%          | 1.71%           |
| Flagged after posting   | 72.83%  | 92.44%         | 98.29%          |
| Avg. lead period (days) | 13.41   | 29.58          | 2.57            |
| Avg. lag period (days)  | -4.28   | -24.90         | -9.01           |
| Overall avg. (days)     | -3.67   | -20.77         | -8.82           |

#### Table 5: Blacklist Performance (Unique Domains)
| Link Statistics         | URIBL   | Google Malware | Google Phishing |
|-------------------------|---------|----------------|-----------------|
| Flagged before posting  | 50.19%  | 18.89%         | 15.38%          |
| Flagged after posting   | 49.81%  | 81.11%         | 84.62%          |
| Avg. lead period (days) | 15.51   | 28.85          | 2.50            |
| Avg. lag period (days)  | -5.41   | -21.63         | -10.48          |
| Overall avg. (days)     | -1.16   | -12.10         | -8.49           |
| Total domains flagged   | 128     | 625            | 13              |

Figure 6 provides a more detailed view of the volume of tweets per lead and lag day. It is important to note that Twitter's use of Google’s SafeBrowsing API to filter links prior to posting biases our analysis towards links that pass through the filter, effectively masking the lead time for URLs that spammers obfuscate with shorteners to avoid blacklisting.

### 6.2 Evading Blacklists
The success of blacklists depends on the reuse of spam domains. If every email or tweet contained a unique domain, blacklists would be ineffective. While registering new domains can be costly, URL shortening services like bitly, tinyurl, is.gd, and ow.ly provide spammers with a free tool to obfuscate their domains.

By following shortened URLs, we found that over 80% of distinct links contained at least one redirect, as shown in Figure 7. Redirects pose a threat to blacklisting services, especially when they cross domain boundaries, causing a link to appear from a non-blacklisted site. Figure 8 shows the cross-domain breakdown for distinct URLs containing at least one redirect. Approximately 55% of blacklisted URLs cross a domain boundary.

The impact of URL shortening on Twitter’s malware defenses is significant. Disregarding blacklist delay, 39% of distinct malware and phishing URLs evade detection via shorteners. Despite this small fraction, these links make up over 98% of malicious tweets identified by our system. Even if a shortened URL becomes blacklisted, generating a new URL comes at no cost. Without crawling to resolve shortened URLs, blacklists become much less effective.

### 6.3 Domain Blacklist Limitations
For blacklists based on domains rather than full URLs, such as URIBL and Joewein, false positives pose a risk of blacklisting entire sites. Our analysis of URIBL and Joewein history identified multiple mainstream domains, including ow.ly, tumblr, and friendfeed, that were blacklisted. This highlights the need for more granular blacklisting, similar to Google’s SafeBrowsing API, to address the issue of spam in social media.

### 7. Conclusion
This study presents the first comprehensive analysis of spam on Twitter, including spam behavior, click-through rates, and the effectiveness of blacklists in preventing spam propagation. Using over 400 million messages and 25 million URLs from public Twitter data, we find that 8% of distinct Twitter links point to spam. Of these, 5% direct to malware and phishing, while the remaining 95% target scams. Analyzing spammer account behavior, we find that only 16% of spam accounts are clearly automated bots, with the remaining 84% appearing to be compromised accounts controlled by spammers.

Even with partial visibility into daily tweets, we identify coordination among thousands of accounts posting different obfuscated URLs that all redirect to the same spam landing page. By measuring click-through rates, we find that Twitter spam is more successful than email, with an overall click-through rate of 0.13%.

Finally, by measuring the delay before blacklists mark Twitter URLs as spam, we show that integrating blacklists into Twitter would protect only a minority of users. The extensive use of URL shortening services masks known-bad URLs, negating the potential benefits of blacklists. To improve defenses against Twitter spam, URLs posted to the site must be crawled to resolve redirects, using the final landing page for blacklisting. While blacklist delay remains a challenge, retroactive blacklisting could allow Twitter to suspend accounts used for spam, forcing spammers to obtain new accounts and followers, a potentially prohibitive cost.

### 8. References
[1] D. Anderson, C. Fleizach, S. Savage, and G. Voelker. Spamscatter: Characterizing Internet scam hosting infrastructure. In USENIX Security, 2007.
[2] M. Cha, H. Haddadi, F. Benevenuto, and K. Gummadi. Measuring User Influence in Twitter: The Million Follower Fallacy. In Proceedings of the 4th International Conference on Weblogs and Social Media, 2010.
[3] A. Chowdhury. State of Twitter spam. http://blog.twitter.com/2010/03/state-of-twitter-spam.html, March 2010.
[4] F-Secure. Twitter now filtering malicious URLs. http://www.f-secure.com/weblog/archives/00001745.html, 2009.
[5] R. Flores. The real face of Koobface. http://blog.trendmicro.com/the-real-face-of-koobface/, August 2009.
[6] Google. Google safebrowsing API. http://code.google.com/apis/safebrowsing/, 2010.
[7] D. Harvey. Trust and safety. http://blog.twitter.com/2010/03/trust-and-safety.html, March 2010.
[8] D. Ionescu. Twitter Warns of New Phishing Scam. http://www.pcworld.com/article/174660/twitter_warns_of_new_phishing_scam.html, October 2009.
[9] D. Irani, S. Webb, and C. Pu. Study of static classification of social spam profiles in MySpace. In Proceedings of the 4th International Conference on Weblogs and Social Media, 2010.
[10] J. John, A. Moshchuk, S. Gribble, and A. Krishnamurthy. Studying spamming botnets using Botlab. In Usenix Symposium on Networked Systems Design and Implementation (NSDI), 2009.
[11] C. Kanich, C. Kreibich, K. Levchenko, B. Enright, G. Voelker, V. Paxson, and S. Savage. Spamalytics: An empirical analysis of spam marketing conversion. In Proceedings of the 15th ACM Conference on Computer and Communications Security, pages 3–14. ACM, 2008.
[12] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a social network or a news media? In Proceedings of the International World Wide Web Conference, 2010.
[13] K. Lee, J. Caverlee, and S. Webb. Uncovering social spammers: Social honeypots + machine learning. In Proceeding of the SIGIR conference on Research and Development in Information Retrieval, pages 435–442, 2010.
[14] R. McMillan. Stolen Twitter accounts can fetch $1,000. http://www.computerworld.com/s/article/9150001/Stolen_Twitter_accounts_can_fetch_1_000, 2010.
[15] B. Meeder, J. Tam, P. G. Kelley, and L. F. Cranor. RT @IWantPrivacy: Widespread violation of privacy settings in the Twitter social network. In Web 2.0 Security and Privacy, 2010.
[16] J. O’Dell. Twitter hits 2 billion tweets per month. http://mashable.com/2010/06/08/twitter-hits-2-billion-tweets-per-month/, June 2010.
[17] A. Pitsillidis, K. Levchenko, C. Kreibich, C. Kanich, G. Voelker, V. Paxson, N. Weaver, and S. Savage. Botnet Judo: Fighting spam with itself. 2010.
[18] Z. Qian, Z. Mao, Y. Xie, and F. Yu. On network-level clusters for spam detection. In Proceedings of the Network and Distributed System Security Symposium (NDSS), 2010.
[19] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A Bayesian approach to filtering junk e-mail. In Learning for Text Categorization: Papers from the 1998 workshop. Madison, Wisconsin: AAAI Technical Report WS-98-05, 1998.
[20] E. Schonfeld. When it comes to URL shorteners, bit.ly is now the biggest. http://techcrunch.com/2009/05/07/when-it-comes-to-url-shorteners-bitly-is-now-the-biggest/, May 2009.
[21] K. Thomas and D. M. Nicol. The Koobface botnet and the rise of social malware. Technical report, University of Illinois at Urbana-Champaign, July 2010. https://www.ideals.illinois.edu/handle/2142/16598.
[22] Twitter. The Twitter rules. http://help.twitter.com/forums/26257/entries/18311, 2009.
[23] URIBL. URIBL.COM – realtime URI blacklist. http://uribl.com/, 2010.
[24] Y. Wang, M. Ma, Y. Niu, and H. Chen. Spam double-funnel: Connecting web spammers with advertisers. In Proceedings of the International World Wide Web Conference, pages 291–300, 2007.
[25] J. Wein. Joewein.de LLC – fighting spam and scams on the Internet. http://www.joewein.net/.
[26] C. Wisniewski. Twitter hack demonstrates the power of weak passwords. http://www.sophos.com/blogs/chetw/g/2010/03/07/twitter-hack-demonstrates-power-weak-passwords/, March 2010.
[27] Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten, and I. Osipkov. Spamming botnets: Signatures and characteristics. Proceedings of ACM SIGCOMM, 2008.