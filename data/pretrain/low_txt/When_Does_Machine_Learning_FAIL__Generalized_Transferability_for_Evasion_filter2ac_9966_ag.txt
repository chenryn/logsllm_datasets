# Evaluating Realistic Attacks Against Machine Learning Systems and Introducing StingRay

## Abstract
This paper evaluates realistic attacks against machine learning systems and introduces StingRay, a targeted poisoning attack designed to bypass existing defenses. We demonstrate the practicality of our attack across four classification tasks using three different classifiers. By exploring the FAIL dimensions, we uncover new transferability properties in existing targeted evasion attacks and highlight characteristics that could provide resiliency against targeted poisoning. Our exploration generalizes prior work on attack transferability and provides new insights into the transferability of poison samples.

## Acknowledgments
We thank Ciprian Baetu, Jonathan Katz, Daniel Marcu, Tom Goldstein, Michael Maynord, Ali Shafahi, W. Ronny Huang, our shepherd, Patrick McDaniel, and the anonymous reviewers for their valuable feedback. We also extend our gratitude to the Drebin authors for providing access to their dataset and VirusTotal for access to their service. This research was partially supported by the Department of Defense and the Maryland Procurement Office (contract H98230-14-C-0127).

## References
1. ALEXEY MALANOV. The multilayered security model in Kaspersky Lab products, Mar 2017.
2. ARP, D., et al. Drebin: Effective and explainable detection of Android malware in your pocket. In NDSS (2014).
3. BARRENO, M., et al. The security of machine learning. Machine Learning 81 (2010), 121–148.
4. BIGGIO, B., et al. Evasion attacks against machine learning at test time. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (2013), Springer, pp. 387–402.
5. BIGGIO, B., et al. Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389 (2012).
6. BOJARSKI, M., et al. Explaining how a deep neural network trained with end-to-end learning steers a car. arXiv preprint arXiv:1704.07911 (2017).
7. BRÜCKNER, M., and SCHEFFER, T. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (2011), ACM, pp. 547–555.
8. CARLINI, N., and WAGNER, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (2017), ACM, pp. 3–14.
9. CARLINI, N., and WAGNER, D. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on (2017), IEEE, pp. 39–57.
10. CHAU, D. H. P., et al. Polonium: Tera-scale graph mining for malware detection. In SIAM International Conference on Data Mining (SDM) (Mesa, AZ, April 2011).
11. CHEN, X., et al. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. ArXiv e-prints (Dec. 2017).
12. COLVIN, R. Stranger danger: introducing smartscreen application reputation. http://blogs.msdn.com/b/ie/archive/2010/10/13/stranger-danger-introducing-smartscreen-application-reputation.aspx, Oct 2010.
13. CRETU, G. F., et al. Casting out demons: Sanitizing training data for anomaly sensors. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (2008), IEEE, pp. 81–95.
14. ERNST YOUNG LIMITED. The future of underwriting. http://www.ey.com/us/en/industries/financial-services/insurance/ey-the-future-of-underwriting, 2015.
15. FAIR ISAAC CORPORATION. FICO enterprise security score gives long-term view of cyber risk exposure, November 2016. http://www.fico.com/en/newsroom/fico-enterprise-security-score-gives-long-term-view-of-cyber-risk-exposure-10-27-2016.
16. GILAD-BACHRACH, R., et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In International Conference on Machine Learning (2016), pp. 201–210.
17. GOODFELLOW, I. J., et al. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
18. GOOGLE RESEARCH BLOG. Assisting pathologists in detecting cancer with deep learning. https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html, Mar 2017.
19. GROSSE, K., et al. Adversarial perturbations against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435 (2016).
20. GU, T., et al. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733 (2017).
21. HEARN, M. Abuse at scale. In RIPE 64 (Ljubljana, Slovenia, Apr 2012). https://ripe64.ripe.net/archives/video/25/.
22. HUANG, L., et al. Adversarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence (2011), ACM, pp. 43–58.
23. KOH, P. W., and LIANG, P. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730 (2017).
24. KRIZHEVSKY, A., and HINTON, G. Learning multiple layers of features from tiny images. Citeseer (2009).
25. LASKOV, P., et al. Practical evasion of a learning-based classifier: A case study. In Security and Privacy (SP), 2014 IEEE Symposium on (2014), IEEE, pp. 197–211.
26. LECUN, Y. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/ (1998).
27. LIU, W., and CHAWLA, S. A game theoretical model for adversarial learning. In Data Mining Workshops, 2009. ICDMW’09. IEEE International Conference on (2009), IEEE, pp. 25–30.
28. LIU, Y., et al. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770 (2016).
29. LIU, Y., et al. Trojaning attack on neural networks. Tech. Rep. 17-002, Purdue University, 2017.
30. LIU, Y., et al. Cloudy with a chance of breach: Forecasting cyber security incidents. In 24th USENIX Security Symposium (USENIX Security 15) (2015), pp. 1009–1024.
31. MIT TECHNOLOGY REVIEW. How to upgrade judges with machine learning. https://www.technologyreview.com/s/603763/how-to-upgrade-judges-with-machine-learning/, Mar 2017.
32. MOZAFFARI-KERMANI, M., et al. Systematic poisoning attacks on and defenses for machine learning in healthcare. IEEE journal of biomedical and health informatics 19, 6 (2015), 1893–1905.
33. MUÑOZ-GONZÁLEZ, L., et al. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (2017), ACM, pp. 27–38.
34. NELSON, B., et al. Exploiting machine learning to subvert your spam filter. In Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats (Berkeley, CA, USA, 2008), LEET’08, USENIX Association, pp. 7:1–7:9.
35. PAPERNOT, N., et al. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P) (2016), IEEE, pp. 372–387.
36. PAPERNOT, N., et al. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples. CoRR abs/1605.07277 (2016).
37. PAPERNOT, N., et al. Practical black-box attacks against deep learning systems using adversarial examples. In ACM Asia Conference on Computer and Communications Security (Abu Dhabi, UAE, 2017).
38. PAPERNOT, N., et al. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy (2016), IEEE Computer Society, pp. 582–597.
39. RAJAB, M. A., et al. CAMP: Content-agnostic malware protection. In Network and Distributed System Security (NDSS) Symposium (San Diego, CA, Feb 2013).
40. SABOTTKE, C., et al. Vulnerability disclosure in the age of social media: exploiting Twitter for predicting real-world exploits. In 24th USENIX Security Symposium (USENIX Security 15) (2015), pp. 1041–1056.
41. SAINI, U. Machine learning in the presence of an adversary: Attacking and defending the spambayes spam filter. Tech. rep., DTIC Document, 2008.
42. STEINHARDT, J., et al. Certified defenses for data poisoning attacks. In Advances in Neural Information Processing Systems (2017), pp. 3520–3532.
43. SUCIU, O., et al. When does machine learning fail? generalized transferability for evasion and poisoning attacks. arXiv preprint arXiv:1803.06975 (2018).
44. SZEGEDY, C., et al. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013).
45. TAMERSOY, A., et al. Guilt by association: large scale malware detection by mining file-relation graphs. In KDD (2014).
46. TRAMÈR, F., et al. Stealing machine learning models via prediction APIs. In 25th USENIX Security Symposium (USENIX Security 16) (Austin, TX, Aug. 2016), USENIX Association.
47. VERIZON. Data breach investigations reports (DBIR), February 2012. http://www.verizonenterprise.com/DBIR/.
48. VIRUSTOTAL. http://www.virustotal.com.
49. XU, W., et al. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155 (2017).
50. XU, W., et al. Automatically evading classifiers. In Proceedings of the 2016 Network and Distributed Systems Symposium (2016).
51. YANG, C., et al. Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340 (2017).
52. YOSINSKI, J., et al. How transferable are features in deep neural networks? In Advances in neural information processing systems (2014), pp. 3320–3328.
53. ZHANG, C., et al. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 (2016).

## Appendix
### A. The StingRay Attack
Algorithm 1 presents the pseudocode for StingRay's two general-purpose procedures. STINGRAY constructs a set \( I \) with at least \( N_{\text{min}} \) and at most \( N_{\text{max}} \) attack instances. In the sample crafting loop, this procedure invokes GETBASEINSTANCE to select appropriate base instances for the target. Each iteration of the loop crafts one poison instance by invoking CRAFTINSTANCE, which modifies the set of allowable features (according to FAIL’s L dimension) of the base instance. This procedure is specific to each application. The other application-specific elements are the distance function \( D \) and the method for injecting the poison in the training set: the crafted instances may either replace or complement the base instances, depending on the application domain. Next, we describe the steps that overcome the main challenges of targeted poisoning.

#### Application-Specific Instance Modification
CRAFTINSTANCE crafts a poisoning instance by modifying the set of allowable features of the base instance. The procedure selects a random sample among these features, under the constraint of the target resemblance budget. It then alters these features to resemble those of the target. Each crafted sample introduces only a small perturbation that may not be sufficient to induce the target misclassification; however, because different samples modify different features, they collectively teach the classifier that the features of \( t \) correspond to label \( y_d \). We discuss the implementation details of this procedure for the four applications in Section 4.2.

#### Crafting Individually Inconspicuous Samples
To ensure that the attack instances do not stand out from the rest of the training set, GETBASEINSTANCE randomly selects a base instance from \( S' \), labeled with the desired target class \( y_d \), that lies within \( \tau_D \) distance from the target. By choosing base instances that are as close to the target as possible, the adversary reduces the risk that the crafted samples will become outliers in the training set. The adversary can further reduce this risk by trading target resemblance (modifying fewer features in the crafted samples) for the need to craft more poison samples (increasing \( N_{\text{min}} \)). The adversary then checks the negative impact of the crafted instance on the training set sample \( S' \). The crafted instance \( x_c \) is discarded if it changes the prediction on \( t \) above the attacker set threshold \( \tau_{\text{NI}} \) or added to the attack set otherwise. To validate that these techniques result in individually inconspicuous samples, we consider whether our crafted samples would be detected by three anti-poisoning defenses, discussed in detail in Section 4.1.

#### Crafting Collectively Inconspicuous Samples
After the crafting stage, GETPDR checks the perceived PDR on the available classifier. The attack is considered successful if both adversarial goals are achieved: changing the prediction of the available classifier and not decreasing the PDR below a desired threshold \( \tau_{\text{PDR}} \).

#### Guessing the Labels of the Crafted Samples
By modifying only a few features in the crafted sample, CRAFTINSTANCE aims to preserve the label \( y_d \) of the base instance. While the adversary is unable to dictate how the poison samples will be labeled, they might guess this label by consulting an oracle. We discuss the effectiveness of this technique in Section 4.3.