# Modeling and Analyzing Operation Processes for Dependability

**Authors:** Xiwei Xu, Liming Zhu, Jim (Zhanwen) Li, Len Bass, Qinghua Lu, Min Fu  
**Affiliations:**  
- School of Computing Science and Engineering, University of New South Wales, Sydney, Australia  
- NICTA, Sydney, Australia  
- PI: EMAIL

## Abstract
Application dependability is increasingly reliant on sophisticated activities during operation time, such as deployment, upgrades, scaling, and responses to various failures. Traditional approaches to improving application dependability have focused on artifact-oriented troubleshooting and improvements. In this paper, we present an approach that uses process models to represent and analyze operations, considering exception handling and fault-proneness. Our goal is to reduce the diagnosis and repair time for application failures that occur during operational activities like deployment and upgrade.

## I. Introduction
The interaction between operations and dependability has recently gained significant attention in the industry. Gartner has stated that "through 2015, 80% of outages impacting mission-critical services will be caused by people and process issues, and more than 50% of those outages will be caused by change/configuration/release integration and hand-off issues" [1]. It is not just that dependability issues may trigger operator actions, but rather the converse: dependability is often affected by operator actions. Many operator actions, even when performed correctly (e.g., backup, snapshot, maintenance), can impact performance and availability. Other operator actions (e.g., upgrade, failover, redeployment) are error-prone and require troubleshooting, undoing steps, problem fixing, and redoing the undone steps. These error-prone processes and complicated exception handling significantly affect the repair time for a failure.

In the past, research has been conducted on troubleshooting operation errors from an artifact and provenance perspective, such as linking issues back to source code, configuration/log analysis [2], and provenance analysis [3]. However, these analyses do not view operations as a process with exception handling. Other work focuses on specific issues in rolling upgrades [4] and backup to improve scheduling and correctness. Our previous work considered these actions as black boxes and analyzed their impact on overall availability using SRN models [5]. Treating them as black-box actions and assuming they perform correctly has severe limitations on the real-world applicability of the analysis results.

In this paper, we outline a process-oriented approach to modeling and analyzing operations for dependability. We model an operation as a set of steps executed by agents (automated scripts/tools or human) requiring various resources (computing power, readied environment, nodes, etc.). We can then populate the models with empirical data gathered either from one's own environment or from literature on fault types and probabilities. The data includes troubleshooting time and the actual time to repair. We intend to use the models and their analysis for two purposes: 1) to understand and monitor actual operational processes to help with error diagnosis and recommend actions, and 2) to conduct sensitivity analysis to determine which actions can be improved the most to affect completion time, overall dependability, and zero downtime for critical portions. In this paper, we outline the approach and report some initial observations.

## II. Operation Process Model
There are many process modeling languages, ranging from informal business process modeling (e.g., BPMN) to formal process analysis (e.g., Petri nets). We have chosen a language called Little-JIL [6] due to its unique features that are useful for our approach. We use its agent concepts to model human operators or scripts and its flexible resource concepts to model the resources being operated on and required for computing power. We use its tree structure and abstraction and recursion concepts to model certain steps to necessary details with recursive reuses. Its strong exception management mechanism, pre/post-conditions, and flexible parameter passing among steps are also beneficial. Our initial experiments involve the deployment of a typical HBase cluster on AWS EC2s.

Figure 1 models the deployment process of HBase using Little-JIL. The process is a hierarchical decomposition of activities (called steps in Little-JIL), denoted graphically by black rectangular bars. The process is executed from left to right in depth-first order. The sequencing badge (in white color) on the left side of the step bar represents the sequencing for a step. The exceptions that can be thrown by a step are marked with a red cross. The exception handler itself is a process model shown in the bottom-left corner. Normal exception propagation is assumed. A message (green lightning bolt) is added on the root step to model the behavior, which is captured by a monitor at runtime but unknown at design time.

Due to space limitations, we cannot show the resource model. In Little-JIL, an agent (special resource) is an autonomous entity that is an expert in some part of the process. An agent could be a human engineer or automated deployment tools or script. Other resources include cloud infrastructure resources and required artifacts such as installation packages and configuration files. Different resources will be modeled with different attributes, including their fault-proneness (from empirical data), availability, and capabilities.

Troubleshooting is a key issue in operations related to dependability. We explicitly consider key steps of the troubleshooting process in the exception handler. For example, some faults will only manifest themselves as failures later in the process, requiring significant error diagnosis time. Fixing may require reverting some actions and then redoing the actions after the fix. The exception handling process may have its own exceptions, recursively expressed in the model.

## III. Initial Analysis and Validation
Our initial analysis and validation consist of three parts. First, we conducted several experiments to deploy HBase in Amazon EC2 using both manual and automated activities (Whirr/Cloudera and EMR). We collected specific exceptions and their problem diagnosis/fixing times. We used these to populate the error diagnosis and fixing times for our particular deployment process. Initial analysis shows that diagnosis time and undo/redo time are two key steps worth improving for overall dependability and time. We are currently listing a set of product and improvement requirements for shortening diagnosis time. We have also started working on an undo framework to support operators in cloud environments [7].

Second, we sought opinions from Hadoop ecosystem experts in the industry using our process description. Interesting feedback that we are incorporating into our models includes careful pre/post-condition treatment on configuration resources during redo, deployment processes being significantly different (i.e., high-availability requirements), the choice of waiting or getting into exception handling for temporary failures, and overall time optimization using the models.

Third, we surveyed the literature for fault distribution and probability [8]. We summarized the data so that we can use it to inject faults into our process model for simulation. We are also using resource scheduling algorithms to analyze the best choice among manual and automated deployment or rolling-upgrade schedules, with special consideration of faults. In conclusion, this is very much work in progress, but our initial activities demonstrate that focusing on the process can provide insights that result in a reduction of the repair time for failures during operational activities.

## References
[1] R. J. Colville and G. Spafford, "Configuration Management for Virtual and Cloud Infrastructures," Gartner, 2010.  
[2] D. Yuan, S. Park, P. Huang, et al., "Be Conservative: Enhancing Failure Diagnosis with Proactive Logging," 9th ACM/USENIX Symposium on Operating Systems Design and Implementation, Hollywood, CA, 2012.  
[3] M. Chiarini, "Provenance for System Troubleshooting," 25th Large Installation System Administration Conference, Boston, MA, 2011.  
[4] L. Bass, "Preventing a Mixed Version Race Condition in the Single Load Balancer Case," 5th Workshop on Hot Topics in Software Upgrades, San Jose, CA, 2013 (under review).  
[5] X. Xu, Q. Lu, L. Zhu, et al., "Availability Analysis for Deployment of In-Cloud Applications," presented at the 4th International ACM SIGSOFT Symposium on Architecting Critical Systems, Vancouver, British Columbia, Canada, 2013.  
[6] A. Wise, "Little-JIL 1.5 Language Report," Department of Computer Science, University of Massachusetts Amherst, 2006.  
[7] I. Weber, H. Wada, A. Fekete, et al., "Automatic Undo for Cloud Management via AI Planning," Workshop on Hot Topics in System Dependability, CA, USA, 2012.  
[8] T. Dumitras, S. Kavulya, P. Narasimhan, et al., "A Fault Model for Upgrades in Distributed Systems," Carnegie Mellon University, 2008.