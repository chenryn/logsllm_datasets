### References

1. **Tatar, A., Giuffrida, C., Bos, H., & Razavi, K. (2018).** Defeating Software Mitigations Against Rowhammer: A Surgical Precision Hammer. In *International Symposium on Research in Attacks, Intrusions, and Defenses* (pp. 47–66). Springer.

2. **Tatar, A., Konoth, R. K., Athanasopoulos, E., Giuffrida, C., Bos, H., & Razavi, K. (2018).** Throwhammer: Rowhammer Attacks Over the Network and Defenses. In *2018 USENIX Annual Technical Conference (USENIX ATC 18)* (pp. 213–226). USENIX Association, Boston, MA.

3. **Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., & Ristenpart, T. (2016).** Stealing Machine Learning Models via Prediction APIs. In *25th USENIX Security Symposium (USENIX Security 16)* (pp. 601–618). USENIX Association, Austin, TX.

4. **Van Der Veen, V., Fratantonio, Y., Lindorfer, M., Gruss, D., Maurice, C., Vigna, G., Bos, H., Razavi, K., & Giuffrida, C. (2016).** Drammer: Deterministic Rowhammer Attacks on Mobile Platforms. In *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security* (pp. 1675–1689). ACM, Vienna, Austria.

5. **Wang, B., Yao, Y., Viswanath, B., Zheng, H., & Zhao, B. Y. (2018).** With Great Training Comes Great Vulnerability: Practical Attacks Against Transfer Learning. In *27th USENIX Security Symposium (USENIX Security 18)* (pp. 1281–1297). USENIX Association, Baltimore, MD.

6. **Wang, N., Choi, J., Brand, D., Chen, C.-Y., & Gopalakrishnan, K. (2018).** Training Deep Neural Networks with 8-bit Floating Point Numbers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett (Eds.), *Advances in Neural Information Processing Systems 31* (pp. 7675–7684). Curran Associates, Inc.

7. **Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016).** Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation. *arXiv preprint arXiv:1609.08144*.

8. **Xiao, J., Xu, Z., Huang, H., & Wang, H. (2013).** Security Implications of Memory Deduplication in a Virtualized Environment. In *2013 43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)* (pp. 1–12). IEEE, Budapest, Hungary.

9. **Xiao, Y., Zhang, X., Zhang, Y., & Teodorescu, R. (2016).** One Bit Flips, One Cloud Flops: Cross-VM Row Hammer Attacks and Privilege Escalation. In *25th USENIX Security Symposium (USENIX Security 16)* (pp. 19–35). USENIX Association, Austin, TX.

10. **Xu, B., Wang, N., Chen, T., & Li, M. (2015).** Empirical Evaluation of Rectified Activations in Convolutional Networks. *arXiv preprint arXiv:1505.00853*.

11. **Zhou, Y., Kantarcioglu, M., & Xi, B. (2018).** Breaking Transferability of Adversarial Samples with Randomness. *arXiv preprint arXiv:1802.01386*.

### Appendix

#### A. Network Architectures

We use 19 deep neural network (DNN) models in our experiments, comprising six architectures and their variants. Table 7 describes two base architectures and their six variations for MNIST. For CIFAR10, we employ the base architecture from [55], which includes four convolutional layers and a fully-connected layer, and we create three variations of it. CIFAR10-AlexNet11 and CIFAR10-VGG1612 are sourced from the community. For ImageNet, we use DNN architectures available from the Internet13. In Section 6.2, we employ two networks (8-bit quantized14 and binarized versions of MNIST-L5) from the community15, with some adjustments.

#### B. The Vulnerability Using Different Criteria

We examine the vulnerable parameter ratio (vulnerability) using different RAD criteria across 15 DNN models. Our results are presented in Figure 12. Each figure shows the vulnerable parameter ratio for a specific RAD criterion. For example, in MNIST-L5, the model has 40% of vulnerable parameters that cause [RAD > 0.5], which estimates the upper bound of the blind attacker. In MNIST, CIFAR10, and two ImageNet models, the vulnerability decreases as the attacker aims to inflict more severe damage. However, in ImageNet, ResNet50, DenseNet161, and InceptionV3 have almost the same vulnerability (∼50%) with the high criterion [RAD > 0.8].

#### C. Hyper-parameters for Training

In our experiments, we use the following hyper-parameters:

- **MNIST:**
  - SGD optimizer
  - 40 epochs
  - Learning rate (lr): 0.01
  - Batch size: 64
  - Momentum: 0.1
  - Adjust learning rate by 0.1 every 10 epochs

- **CIFAR10 (Base Models):**
  - SGD optimizer
  - 50 epochs
  - Learning rate (lr): 0.02
  - Batch size: 32
  - Momentum: 0.1
  - Adjust learning rate by 0.5 every 10 epochs

- **CIFAR10 (AlexNet):**
  - 300 epochs
  - Learning rate (lr): 0.01
  - Batch size: 64
  - Momentum: 0.1
  - Adjust learning rate by 0.95 every 10 epochs

- **CIFAR10 (VGG16):**
  - 300 epochs
  - Learning rate (lr): 0.01
  - Batch size: 128
  - Momentum: 0.1
  - Adjust learning rate by 0.15 every 100 epochs

- **GTSRB:**
  - Fine-tune VGG16 pre-trained on ImageNet
  - SGD optimizer
  - 40 epochs
  - Learning rate (lr): 0.01
  - Batch size: 32
  - Momentum: 0.1
  - Adjust learning rate by 0.1 at 15 epochs and by 0.05 at 25 epochs
  - Freeze the parameters of the first 10 layers

- **Flower102:**
  - Fine-tune ResNet50 pre-trained on ImageNet
  - SGD optimizer
  - 40 epochs
  - Learning rate (lr): 0.01
  - Batch size: 50
  - Momentum: 0.1
  - Adjust learning rate by 0.1 at 15 and 25 epochs
  - Freeze the parameters of the first 10 layers

---

This revised version ensures clarity, coherence, and professionalism in the text.