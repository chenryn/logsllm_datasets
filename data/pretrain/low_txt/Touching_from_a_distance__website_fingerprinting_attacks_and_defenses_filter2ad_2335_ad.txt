### 9.10 to 10.10: Experimental Setup

We conducted our experiments using Firefox versions 3.6.10 to 3.6.17 and Tor version 0.2.1.30, with the exception of one computer that used Tor 0.2.2.21-alpha. All Firefox plugins were disabled during data collection. Three of the computers had 2.8GHz Intel Pentium CPUs and 2GB of RAM, while one computer had a 2GHz AMD Turion Mobile CPU with 2GB of RAM. We scripted Firefox using the Ruby `watir-webdriver` library and captured packets using `tshark`, the command-line version of Wireshark. For the SSH experiments, we used OpenSSH 5.3p1. Our Tor clients used the default configuration, unless otherwise noted. SSH tunnels passed between two machines on the same local network.

### Data Collection

Most of our experiments used data collected from the Alexa Top 1000 web pages. We removed any web pages that failed to load in Firefox (without Tor or any other proxy). If a URL redirected to another location, we replaced it with its redirect target. We then used the top 800 URLs from this cleaned list. We collected traces from each web page in a round-robin fashion. Unless otherwise specified, we cleared the browser cache between each page load. We repeated data collection with four different defense mechanisms, as described below. We collected either 20 or 40 traces from each URL, depending on the defense mechanism in use. Most experiments were run with just the top 100 web pages in our list; we only used the full 800 URLs in one experiment to test the scalability of our attack.

### Closed-World Evaluation

This is a "closed-world" evaluation. In such an evaluation, there are only \( k \) web pages in the world. The attacker can collect fingerprints for each page. The victim then chooses one of the pages uniformly at random and loads it in their browser. The attacker observes the victim’s packet trace and attempts to guess which page the victim loaded. The appropriate metric is the success rate of the attacker, i.e., the percentage of time they guess correctly. There is no notion of false positive or false negative in this scenario. In contrast, we will evaluate our web site classifier in an open-world setting, which does have such considerations.

### Attacks and Defenses

Table 3 summarizes the attacks evaluated in this paper. We tested each attack against the following defenses. For each defense, we also indicate the number of URLs we collected and the number of visits to each URL. We collected four basic data sets:

- **None (SSH)** (100x40): All HTTP traffic is sent through an SSH tunnel.
- **SSH + HTTPOS** (100x20): We used the prototype implementation that the HTTPOS authors used to evaluate HTTPOS in their paper. Based on some of our early results, they added additional randomization to their defense. Note that HTTPOS includes both TCP- and HTTP-level defenses. Some web pages caused HTTPOS to crash. We detected crashes and attempted to load the page up to 3 times. If HTTPOS crashed all 3 times, we added the third, incomplete trace to our data set. Our final data set of 2000 traces contained 33 crash traces, so we do not believe these had a significant effect on our results.
- **Tor** (800x40): All HTTP traffic is tunneled through the default Tor configuration. Most experiments only use the top 100 web pages from this dataset.
- **Tor + Randomized Pipelining** (100x40): The Tor project has released a software bundle that includes Tor, the Polipo proxy, and a patched version of Firefox that randomizes the order and pipelining used to load images and other embedded objects in a web page. We used the entire bundle as-is.

We then used these data sets to generate simulations of other defenses, as described below:

- **SSH + Sample-Based Traffic Morphing** (100x20): We applied traffic morphing to the traces obtained in the SSH experiment. We morphed all traces to have the same packet size distribution as http://flickr.com (selected randomly from our data set). We morphed each direction independently, as described in the traffic morphing paper. To morph a trace, we repeatedly sampled packet sizes from the target distribution and padded (or fragmented) packets in the trace to match the sampled size. Thus, our morphed traces have the same packet size distribution as they would under optimal traffic morphing, but the total number of packets transmitted may be higher. The original traffic morphing paper found that optimal traffic morphing and sample-based traffic morphing had equal resilience to attack, so we believe this is a reasonable evaluation of traffic morphing.
- **SSH Packet Count** (100x40): We applied the same transformation to our SSH traces as we did to our Tor traces, as described above.
- **Tor + Randomized Pipelining + Randomized Cover Traffic** (100x20): We inserted additional cover traffic into the traces collected for the Tor + randomized pipelining experiment. We deleted all packet size information, i.e., traces consisted of only ±1500s. Then, for an input trace of \( l \) packets, we randomly, uniformly, and independently picked \( l \) positions in the trace and inserted a 1500 or −1500, with equal probability, at each position.
- **Tor Packet Count** (100x40): We removed all packet size and direction information from our Tor traces. All that the attacker can observe is the total number of packets transmitted. This experiment explores how much information is revealed by the size of the page being loaded.

### Results

We ran each attack against each data set using stratified 10-fold cross-validation. Figure 2 shows the results of these experiments. The DLSVM attack generally outperforms the Panchenko and MNB attacks. See Section 7 for discussion.

To simulate the limits of defenses based on re-ordering, pipelining, padding, and generating extraneous HTTP requests, we added randomized cover traffic and padded all packets to 1500 bytes in the traces in our Tor + randomized pipelining data set, as described above. We varied the cover traffic overhead from 0% to 100%. This experiment is intended to model an idealized version of defenses like randomized pipelining and HTTPOS. Figure 3 shows the influence of adding randomized cover traffic on our attack. With no cover traffic, i.e., with randomized pipelining and packets padded to 1500 bytes, our attack was able to recognize the visited web page almost 80% of the time. If we double the size of the trace by adding extra cover traffic, our attack can determine the target web page over 50% of the time.

Figure 4 shows the bandwidth overheads of the defenses evaluated in this paper. All overheads are normalized to the SSH traces. HTTPOS has the lowest overhead, 36%, but is not secure. The other defenses have overheads of over 60% compared to SSH.

Figure 5 shows that the DLSVM, Panchenko, and MNB classifiers work well for both cold cache and warm cache page loads. Although we have not directly evaluated our web page classifier on a mixed cold/warm workload, the web site classifiers evaluated in the next section do use mixed workloads and perform well. Figure 5 also shows that the classifiers perform well on randomly selected web pages loaded through Tor, not just the Alexa top 100 pages.

Figure 6(a) shows how the different attacks perform as the number of web pages they must distinguish increases. Not only does our attack outperform the Panchenko attack when the number of candidate web pages is small, but the gap widens as the size of the candidate set increases. For example, our attack can guess which web page, out of 800, that a Tor user is visiting 70% of the time. The Panchenko attack had a success rate of 40% on our set of 800 web pages.

Figure 6(b) shows how additional training data can improve the success rate of our attack. Our attack provides satisfactory results, even with a small training set.

### Web Site Classifier

#### 6.2.1 Experimental Setup

To evaluate the performance of our web site classifier, we created models for two web sites censored by the Chinese "Great Firewall" – Facebook [7] and IMDB [5] – and constructed page classifiers using the Alexa Top 99 pages, along with the pages in our model for each site. We then collected additional traces for the pages in our models and ran those traces through the model to compute the probability distribution of classifier outputs for each page in each model, as described in Section 4.

Our Facebook model covers the login page, the user’s home page, and a generic “friend profile page”. It includes warm and cold cache instances of the home and profile pages. Facebook’s home and profile pages use JavaScript to automatically fetch older items as the user scrolls down the page of past notifications. Our model includes these events. The IMDB model covers the IMDB home page, search results page, movie page, and celebrity page. It includes warm and cold cache states for each page. Transition probabilities between states are artificial for both models – a real attacker would derive these from observations of user behavior and would likely have higher accuracy as a result. Initial state probabilities are uniform, since the attacker may begin eavesdropping in the middle of a user’s session. See our technical report for complete specifications of the models [3].

To test our site classifiers, we need traces of the URLs visited by real users. We obtained URL traces for 25 subjects from Eelco Herder. He collected these traces for his empirical study of web user behavior [23]. These traces, from users in Europe, contain numerous visits to IMDB, but no visits to Facebook. Therefore, we generated artificial traces for Facebook. Our artificial Facebook traces construct visits to Facebook that follow our Facebook model, i.e., we pick a starting Facebook page according to the initial state probabilities of our model and pick successive pages according to the transition probabilities of our model. We then insert these into real traces so that we create a trace consisting of some Facebook visits and some non-Facebook visits. Since the traces are generated from the same model that the classifier uses, this ensures consistency.

#### 6.2.2 Results

Figures 7(a) and 7(b) show the histogram of log-likelihood scores, under the Facebook and IMDB models, respectively, of 6-page windows of the traces we collected. For example, for every window of 6 page loads in the IMDB traces, we ran the packet traces for those 6 page loads through the IMDB model to compute a log-likelihood score. We only considered windows that contained either all IMDB visits or all non-IMDB visits – if a window had, say, 3 IMDB pages and 3 non-IMDB pages, we discarded it from the histogram. As Figure 7(a) shows, the non-Facebook windows are completely separated from the Facebook windows by our model, meaning our classifier works perfectly on this data set. In the IMDB experiment, the non-IMDB windows have, on average, a much higher log-likelihood, indicating that they are not likely to be generated by our IMDB model.

Figure 8 shows the receiver operating curves (ROC) for our Facebook and IMDB classifiers. These curves show the trade-off in False Positive and True Positive rates for varying thresholds of the classifier. As indicated by the histogram in Figure 7(a), the Facebook classifier can achieve 0 false positives and false negatives on our dataset. The IMDB classifier can achieve a 7.9% FP rate and a 5.6% FN rate.

Figure 9 demonstrates how the log-likelihood score correlates with user visits to the target web site over time. Note that these graphs plot traces from multiple browsing sessions – the sessions are separated by gaps in the traces. Only sessions with at least 6 page loads, and at least one page load from the target web site (Facebook or IMDB, respectively), are included in the graphs. The thick, flat, pink line indicates portions of the trace containing page loads from the target web site, while page loads from other sites have a thin flat line. The blue lines with markers plot the log-likelihoods of the six-page windows of page loads. As the graphs show, the log-likelihood scores effectively distinguish between visits to the target site and visits to other sites.