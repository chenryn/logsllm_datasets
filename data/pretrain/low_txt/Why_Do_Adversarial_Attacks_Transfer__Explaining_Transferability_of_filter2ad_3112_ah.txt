The opinions and conclusions expressed in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Combat Capabilities Development Command Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes, notwithstanding any copyright notation herein. We would also like to thank Toyota ITC for funding this research.

### References

1. D. Arp, M. Spreitzenbarth, M. Hübner, H. Gascon, and K. Rieck. *Drebin: Efficient and Explainable Detection of Android Malware in Your Pocket*. In 21st NDSS. The Internet Society, 2014.
2. A. Demontis, P. Russu, B. Biggio, G. Fumera, and F. Roli. *On Security and Sparsity of Linear Classifiers for Adversarial Settings*. In A. Robles-Kelly et al., editors, Joint IAPR Int’l Workshop on Structural, Syntactic, and Statistical Pattern Recognition, vol. 10029 of LNCS, pp. 322–332, Cham, 2016. Springer International Publishing.
3. Y. Dong, F. Liao, T. Pang, X. Hu, and J. Zhu. *Boosting Adversarial Examples with Momentum*. In CVPR, 2018.
4. I. J. Goodfellow, J. Shlens, and C. Szegedy. *Explaining and Harnessing Adversarial Examples*. In ICLR, 2015.
5. A. Athalye, N. Carlini, and D. A. Wagner. *Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples*. In ICML, vol. 80 of JMLR W&CP, pp. 274–283. JMLR.org, 2018.
6. K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. D. McDaniel. *Adversarial Examples for Malware Detection*. In ESORICS (2), vol. 10493 of LNCS, pp. 62–79. Springer, 2017.
7. B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli. *Evasion Attacks Against Machine Learning at Test Time*. In H. Blockeel et al., editors, ECML PKDD, Part III, vol. 8190 of LNCS, pp. 387–402. Springer Berlin Heidelberg, 2013.
8. B. Biggio, B. Nelson, and P. Laskov. *Poisoning Attacks Against Support Vector Machines*. In J. Langford and J. Pineau, editors, 29th Int’l Conf. on Machine Learning, pp. 1807–1814. Omnipress, 2012.
9. B. Biggio and F. Roli. *Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning*. Pattern Recognition, 84:317–331, 2018.
10. C. M. Bishop. *Pattern Recognition and Machine Learning (Information Science and Stats)*. Springer, 2007.
11. N. Carlini and D. A. Wagner. *Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods*. In B. M. Thuraisingham et al., editors, 10th ACM Workshop on Artificial Intelligence and Security, AISec ’17, pp. 3–14, New York, NY, USA, 2017. ACM.
12. N. Carlini and D. A. Wagner. *Towards Evaluating the Robustness of Neural Networks*. In IEEE Symp. on Security and Privacy, pp. 39–57. IEEE Computer Society, 2017.
13. X. Chen, C. Liu, B. Li, K. Lu, and D. Song. *Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning*. ArXiv e-prints, abs/1712.05526, 2017.
14. H. Dang, Y. Huang, and E.-C. Chang. *Evading Classifiers by Morphing in the Dark*. In 24th ACM SIGSAC Conf. on Computer and Communication Security, CCS, 2017.
15. A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck, I. Corona, G. Giacinto, and F. Roli. *Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection*. IEEE Trans. Dependable and Secure Computing, In press.
16. T. Gu, B. Dolan-Gavitt, and S. Garg. *BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain*. In NIPS Workshop on Machine Learning and Computer Security, vol. abs/1708.06733, 2017.
17. A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. *Black-Box Adversarial Attacks with Limited Queries and Information*. In J. Dy and A. Krause, editors, 35th ICML, vol. 80, pp. 2137–2146. PMLR, 2018.
18. M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li. *Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning*. In IEEE Symp. S&P, pp. 931–947. IEEE CS, 2018.
19. A. Kantchelian, J. D. Tygar, and A. D. Joseph. *Evasion and Hardening of Tree Ensemble Classifiers*. In 33rd ICML, vol. 48 of JMLR W&CP, pp. 2387–2396. JMLR.org, 2016.
20. P. W. Koh and P. Liang. *Understanding Black-Box Predictions via Influence Functions*. In Proc. of the 34th Int’l Conf. on Machine Learning, ICML, 2017.
21. Y. Liu, X. Chen, C. Liu, and D. Song. *Delving into Transferable Adversarial Examples and Black-Box Attacks*. In ICLR, 2017.
22. C. Lyu, K. Huang, and H.-N. Liang. *A Unified Gradient Regularization Family for Adversarial Examples*. In IEEE Int’l Conf. on Data Mining (ICDM), vol. 00, pp. 301–309, Los Alamitos, CA, USA, 2015. IEEE CS.
23. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. *Towards Deep Learning Models Resistant to Adversarial Attacks*. In ICLR, 2018.
24. S. Mei and X. Zhu. *Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners*. In 29th AAAI Conf. Artificial Intelligence (AAAI ’15), 2015.
25. M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, and F. Roli. *Is Deep Learning Safe for Robot Vision? Adversarial Examples Against the iCub Humanoid*. In ICCVW ViPAR, pp. 751–759. IEEE, 2017.
26. S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. *Universal Adversarial Perturbations*. In CVPR, 2017.
27. L. Muñoz-González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli. *Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization*. In B. M. Thuraisingham et al., editors, 10th ACM Workshop on AI and Sec., AISec ’17, pp. 27–38, New York, NY, USA, 2017. ACM.
28. B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia. *Exploiting Machine Learning to Subvert Your Spam Filter*. In LEET ’08, pp. 1–9, Berkeley, CA, USA, 2008. USENIX Association.
29. A. Newell, R. Potharaju, L. Xiang, and C. Nita-Rotaru. *On the Practicality of Integrity Attacks on Document-Level Sentiment Analysis*. In AISec, 2014.
30. J. Newsome, B. Karp, and D. Song. *Paragraph: Thwarting Signature Learning by Training Maliciously*. In RAID, pp. 81–105. Springer, 2006.
31. N. Papernot, P. McDaniel, and I. Goodfellow. *Transferability in Machine Learning: From Phenomena to Black-Box Attacks Using Adversarial Samples*. arXiv:1605.07277, 2016.
32. N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. *Practical Black-Box Attacks Against Machine Learning*. In ASIA CCS ’17, pp. 506–519, New York, NY, USA, 2017. ACM.
33. N. Papernot, P. D. McDaniel, and I. J. Goodfellow. *Transferability in Machine Learning: From Phenomena to Black-Box Attacks Using Adversarial Samples*. ArXiv e-prints, abs/1605.07277, 2016.
34. R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. Sharif. *Misleading Worm Signature Generators Using Deliberate Noise Injection*. In IEEE Symp. Sec. & Privacy, 2006.
35. A. S. Ross and F. Doshi-Velez. *Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients*. In AAAI. AAAI Press, 2018.
36. B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar. *Antidote: Understanding and Defending Against Poisoning of Anomaly Detectors*. In 9th ACM SIGCOMM Internet Measurement Conf., IMC ’09, pp. 1–14, NY, USA, 2009. ACM.
37. P. Russu, A. Demontis, B. Biggio, G. Fumera, and F. Roli. *Secure Kernel Machines Against Evasion Attacks*. In 9th ACM Workshop on AI and Sec., AISec ’16, pp. 59–69, New York, NY, USA, 2016. ACM.
38. M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. *Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition*. In ACM SIGSAC Conf. on Comp. and Comm. Sec., pp. 1528–1540. ACM, 2016.
39. C. J. Simon-Gabriel, Y. Ollivier, B. Schölkopf, L. Bottou, and D. Lopez-Paz. *Adversarial Vulnerability of Neural Networks Increases with Input Dimension*. ArXiv, 2018.
40. J. Sokolić, R. Giryes, G. Sapiro, and M. R. D. Rodrigues. *Robust Large Margin Deep Neural Networks*. IEEE Trans. on Signal Proc., 65(16):4265–4280, 2017.
41. O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumitras. *When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks*. In 27th USENIX Sec., pp. 1299–1316, 2018. USENIX Assoc.
42. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. *Intriguing Properties of Neural Networks*. In ICLR, 2014.
43. F. Tramèr, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. *The Space of Transferable Adversarial Examples*. ArXiv e-prints, 2017.
44. D. Varga, A. Csiszárik, and Z. Zombori. *Gradient Regularization Improves Accuracy of Discriminative Models*. ArXiv e-prints ArXiv:1712.09936, 2017.
45. N. Šrndić and P. Laskov. *Practical Evasion of a Learning-Based Classifier: A Case Study*. In IEEE Symp. Sec. and Privacy, SP ’14, pp. 197–211, 2014. IEEE CS.
46. B. Wang and N. Z. Gong. *Stealing Hyperparameters in Machine Learning*. In 2018 IEEE Symposium on Security and Privacy (SP), pp. 36–52. IEEE, 2018.
47. L. Wu, Z. Zhu, C. Tai, and W. E. *Enhancing the Transferability of Adversarial Examples with Noise Reduced Gradient*. ArXiv e-prints, 2018.
48. H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. *Is Feature Selection Secure Against Training Data Poisoning?* In F. Bach and D. Blei, editors, JMLR W&CP - 32nd ICML, vol. 37, pp. 1689–1698, 2015.
49. W. Xu, Y. Qi, and D. Evans. *Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers*. In NDSS. Internet Society, 2016.
50. F. Zhang, P. Chan, B. Biggio, D. Yeung, and F. Roli. *Adversarial Feature Selection Against Evasion Attacks*. IEEE Trans. on Cybernetics, 46(3):766–777, 2016.

---

This version of the text is more organized, clear, and professional. It includes all the necessary references and acknowledgments, and it is formatted for better readability.