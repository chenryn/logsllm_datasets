# Cost Comparison of Data Center Architectures

## Table 4: Cost (in USD millions) of Equipment, Power, and Cabling
Assuming 512 racks with 48 servers/rack. Since these are estimates, we round to the nearest million.

| Component | Copper (USD M) | Fiber (USD M) |
|-----------|-----------------|---------------|
| Equipment | 20              | 31            |
| Power     | 12              | 19            |
| Cabling   | 12              | 19            |
| Total     | 44              | 69            |

### Impact of Server Density
Decreasing the number of servers per rack (i.e., increasing the number of racks for a fixed total number of servers) does not significantly affect the overall cost (data not shown).

### Reconﬁguration Latency
Varying the reconﬁguration latency from 10 ms to 50 ms has minimal (< 5%) impact on FireFly’s performance (data not shown). This indicates that good performance can be achieved even with unoptimized steering delays and network update times.

## 8.6 Cost Comparison

For an instructive case, we consider a data center (DC) with 512 racks and 48 servers per rack to compute the equipment, power, and cabling costs of different architectures, as shown in Table 4. We evaluate both copper- and fiber-based realizations for the wired architectures.

### Estimation of Costs
We conservatively estimate the "bulk" price of 10GbE network switches at $100 per port [11, 50] and 10GbE SFPs at $50 each, which are approximately 50% of their respective retail costs. Thus, fiber-based architectures (including FireFly) incur a cost of $150 per port, while copper-based architectures incur a cost of only $100 per port.

- **FireFly** uses a 96-port (10G) ToR switch on each rack with 48 FSOs. The full-bisection FatTree requires 1536 96-port (10G) switches, while the 1:2 oversubscribed cores of c-Through/3DB use roughly half the ports of FatTree.
- **FSO Devices**: For FireFly, the additional cost for FSO devices (on half the ports) is estimated at $200 per device, including SMs or a GM (§3).
- **3DB**: We assume there are 8 60 GHz radios per rack, with each assembly costing $100.
- **c-Through**: We conservatively assume the 512-port optical switch to be $0.5M.
- **Cabling**: We use an average cost of $1 and $3 per meter for copper and optical-fiber, respectively, with an average per-link length of 30m [40].
- **Energy Costs**: We estimate the 5-year energy cost using a rate of 6.5 cents/KWHr, with per-port power consumption of 3W (fiber) and 6W (copper). We ignore the negligible energy cost of SMs, 60GHz radios, and the optical switches.

### Overall Cost
The total cost of FireFly is 40-60% lower than FatTree and is comparable (or better) than the augmented architectures. Note that the augmented architectures have worse performance compared to FireFly, and copper wires have length limitations.

## 9 Discussion

Given our radical departure from conventional DC designs, we discuss potential operational concerns and possible mechanisms to address them. We acknowledge that further investigation over pilot long-term deployments may uncover other unforeseen concerns.

### Pre- and Re-alignment
External tools will be needed for pre-aligning SMs/GMs. This will be done infrequently, so the mechanism does not need stringent cost/size requirements, and existing mechanical assemblies can be repurposed. While our design tolerates minor misalignment (§3), occasional alignment corrections may be necessary. Feedback from digital optical monitoring support on optical SFPs can be used; GMs can directly use such feedback, but SMs may need additional micro-positioning mechanisms (e.g., piezoelectrics).

### Operational Concerns
Some recurrent concerns include dust settling on optics, light scattering effects, reliability of mechanical parts, and human factors (e.g., protective eyewear for operators). These are not problematic:
- **Dust/Scattering**: Our design has sufficiently high link margins (15dB), and the devices can be engineered to minimize dust (e.g., non-interfering lids or periodic blowing of filtered air).
- **Mechanical Reliability**: Future MEMS-based scanning mirrors [6] are expected to be very robust.
- **Human Safety**: The lasers used are infra-red and very low power, posing no harm to the human eye.

### Beyond 10 Gbps
Current long-range connector standards for 40/100 Gbps (e.g., 40 or 100GBASE-LR4) use WDM to multiplex lower-rate channels on the same fiber, one in each direction. However, like the 10 GbE standard, there are still two optical paths (with two fibers) for duplex links. Single-fiber solutions (as used for 1 GbE [9]) are not yet commodity at these speeds due to a nascent market. We expect this to change in the future. Otherwise, we will need two optical paths or custom single-path solutions.

## 10 Related Work

### Static Wired Topologies
Early DCs used tree-like structures, which had poor performance due to oversubscription. This motivated designs providing full bisection bandwidth [13, 16, 45], which are overprovisioned to handle worst-case patterns. Such networks are not incrementally expandable [45]. In contrast, FireFly is flexible, eliminates cabling costs, and is amenable to incremental expansion. Other efforts proposed architectures where servers act as relay nodes (e.g., [24]), but they are not cost-competitive [40] and raise concerns about isolation and CPU usage.

### Optical Architectures
High traffic volumes and the power use of copper-based Ethernet have motivated the use of optical links. Early works such as c-Through [48] and Helios [20] suggested hybrid electric/optical switch architectures, while recent efforts consider all-optical designs [14, 41]. Free-space optics avoid the cabling complexity of such optical designs. By using multiple FSOs per rack, FireFly can create richer topologies (at the rack level) than simple matchings [14, 20, 41, 48]. FireFly doesn’t need optical switching, thus eliminating concerns about cost and scalability. Optical switching can disconnect substantial portions of the optical network during reconfiguration, while FireFly's transient link-off periods are localized, enabling simpler data plane strategies (§6).

### Wireless in DCs
The FireFly vision is inspired by Flyways [26] and 3D-Beamforming [52]. RF wireless technology suffers from high interference and range limitations, limiting performance. Free-space optics in FireFly eliminate interference concerns. Shin et al. [44] consider a static all-wireless (not only inter-rack) DC architecture using 60 GHz links, but this requires restructuring DC layout and has poor bisection bandwidth due to interference.

### Consistency during Reconﬁgurations
Recent work has identified the issue of consistency during network updates [34, 43]. FireFly introduces unique challenges because the topology changes as well. While these techniques can apply to FireFly, they are more heavyweight for specific properties (no black holes, connectivity, and bounded packet latency) than the domain-specific solutions we engineer. Other work minimizes congestion during updates [30]. While FireFly’s mechanisms do not explicitly address congestion, our results (§8.2) suggest that this impact is quite small.

## 11 Conclusions

In this work, we explore the vision of a fully-flexible, all-wireless, coreless DC network architecture. We identified free-space optics as a key enabler for this vision and addressed practical hardware design, network provisioning, network management, and algorithmic challenges to demonstrate the viability of realizing this vision in practice. There remains significant room for improvement in various aspects of our design, such as the cost and form-factor of hardware elements, and algorithmic techniques for network provisioning and management, which should further improve the cost-performance tradeoff.

## Acknowledgments

We thank Yuvraj Agarwal, Daniel Halperin, and our shepherd Jitendra Padhye for their feedback, and Hanfei Chen for GM measurements. This research was supported in part by NSF grants CNS-1117719 and CNS-0831791.

## References

[1] A Simpler Data Center Fabric Emerges. http://tinyurl.com/kaxpotw.
[2] Galvo mirrors. http://www.thorlabs.us/NewGroupPage9.cfm?ObjectGroup_ID=3770.
[3] htsim simulator. http://nrg.cs.ucl.ac.uk/mptcp/implementation.html.
[4] Kent optronics, inc. http://kentoptronics.com/switchable.html.
[5] Lightpointe FlightStrata G Optical Gigabit Link. http://tinyurl.com/k86o2vh.
[6] MEMS Scanning Mirror. http://www.lemoptix.com/technology/mems-scanning-mirrors.
[7] Mininet. http://yuba.stanford.edu/foswiki/bin/view/OpenFlow/Mininet.
[8] OpenGear Out-of-Band Management. http://tinyurl.com/n773hg3.
[9] Single-fiber SFP. http://www.championone.net/products/transceivers/sfp/single-fiber-single-wavelength/.
[10] Xinyu Laser Products. http://www.xinyulaser.com/index.asp.
[11] 10GBASE-T vs. GbE Cost Comparison. Emulex White Paper, 2012. Available at http://www.emulex.com/artifacts/cdc1a1d3-5d2d-4ac5-9ed8-5cc4a72bd561/elx_sb_all_10gbaset_cost_comparison.pdf.
[12] M. Al-Fares et al. Hedera: Dynamic Flow Scheduling for Data Center Networks. In NSDI, 2010.
[13] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center Network Architecture. In ACM SIGCOMM, 2008.
[14] K. Chen et al. OSA: An Optical Switching Architecture for Data Center Networks.
[15] E. Ciaramella et al. 1.28-Tb/s (32 × 40 Gb/s) Free-Space Optical WDM Transmission System. IEEE Photonics Technology Letters, 21(16), 2009.
[16] C. Clos. A Study of Non-Blocking Switching Networks. Bell System Technical Journal, 32, 1953.
[17] A. Curtis et al. DevoFlow: Scaling Flow Management for High-Performance Networks. In ACM SIGCOMM, 2011.
[18] A. Curtis, S. Keshav, and A. Lopez-Ortiz. LEGUP: Using Heterogeneity to Reduce the Cost of Data Center Network Upgrades. In CoNEXT, 2010.
[19] H. L. Davidson et al. Data Center with Free-Space Optical Communications. US Patent 8,301,028, 2012.
[20] N. Farrington et al. Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers. In ACM SIGCOMM, 2010.
[21] J. Friedman. On the Second Eigenvalue and Random Walks in Random d-Regular Graphs. Combinatorica, 11(4), 1991.
[22] S. Gollakota, S. D. Perli, and D. Katabi. Interference Alignment and Cancellation. In ACM SIGCOMM, 2009.
[23] A. Greenberg et al. VL2: A Scalable and Flexible Data Center Network. In ACM SIGCOMM, 2009.
[24] C. Guo et al. BCube: A High Performance, Server-Centric Network Architecture for Modular Data Centers. In ACM SIGCOMM, 2009.
[25] A. Gupta and J. Konemann. Approximation Algorithms for Network Design: A Survey. Surveys in Operations Research and Management Science, 16, 2011.
[26] D. Halperin et al. Augmenting Data Center Networks with Multi-Gigabit Wireless Links. In ACM SIGCOMM, 2011.
[27] N. Hamedazimi et al. FireFly: A Reconfigurable Wireless Data Center Fabric Using Free-Space Optics (Full Version). http://www.cs.stonybrook.edu/~hgupta/ps/firefly-full.pdf.
[28] N. Hamedazimi, H. Gupta, V. Sekar, and S. Das. Patch Panels in the Sky: A Case for Free-Space Optics in Data Centers. In ACM HotNets, 2013.
[29] B. Heller et al. ElasticTree: Saving Energy in Data Center Networks. In NSDI, 2010.
[30] C.-Y. Hong et al. Achieving High Utilization with Software-Driven WAN. In ACM SIGCOMM, 2013.
[31] D. Kedar and S. Arnon. Urban Optical Wireless Communication Networks: The Main Challenges and Possible Solutions. IEEE Communications Magazine, 2004.
[32] B. Kernighan and S. Lin. An Efficient Heuristic Procedure for Partitioning Graphs. The Bell Systems Technical Journal, 49(2), 1970.
[33] L. Li. CEO, KentOptronics. Personal communication.
[34] R. Mahajan and R. Wattenhofer. On Consistent Updates in Software Defined Networks (Extended Version). In ACM HotNets, 2013.
[35] P. F. McManamon et al. A Review of Phased Array Steering for Narrow-Band Electrooptical Systems. Proceedings of the IEEE, 2009.
[36] B. Monien and R. Preis. Upper Bounds on the Bisection Width of 3- and 4-Regular Graphs. Journal of Discrete Algorithms, 4, 2006.
[37] J. Mudigonda, P. Yalagandula, and J. C. Mogul. Taming the Flying Cable Monster: A Topology Design and Optimization Framework for Data-Center Networks. In USENIX ATC, 2011.
[38] N. McKeown et al. OpenFlow: Enabling Innovation in Campus Networks. ACM SIGCOMM CCR, 38(2), 2008.
[39] S. Orfanidis. Electromagnetic Waves and Antennas; Chapter 15, 19. http://www.ece.rutgers.edu/~orfanidi/ewa/.
[40] L. Popa et al. A Cost Comparison of Datacenter Network Architectures. In CoNEXT, 2010.
[41] G. Porter et al. Integrating Microsecond Circuit Switching into the Data Center. In ACM SIGCOMM, 2013.
[42] P. Raghavan and C. D. Thompson. Randomized Rounding: A Technique for Provably Good Algorithms and Algorithmic Proofs. Combinatorica, 7(4), 1987.
[43] M. Reitblatt et al. Abstractions for Network Update. In ACM SIGCOMM, 2012.
[44] J. Shin, E. G. Sirer, H. Weatherspoon, and D. Kirovski. On the Feasibility of Completely Wireless Datacenters. In ANCS, 2012.
[45] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyfish: Networking Data Centers Randomly. In NSDI, 2012.
[46] O. Svelto. Principles of Lasers. Plenum Press, New York, Fourth Edition, 1998.
[47] J. Turner. Effects of Data Center Vibration on Compute System Performance. In SustainIT, 2010.
[48] G. Wang et al. c-Through: Part-Time Optics in Data Centers. In ACM SIGCOMM, 2010.
[49] R. Wang, D. Butnariu, and J. Rexford. OpenFlow-Based Server Load Balancing Gone Wild. In Hot-ICE, 2011.
[50] Y. Yang, S. Goswami, and C. Hansen. 10GBASE-T Ecosystem is Ready for Broad Adoption. Commscope/Intel/Cisco White Paper, 2012. Available at http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/COM_WP_10GBASE_T_Ecosystem_US4.pdf.
[51] K. Yoshida, K. Tanaka, T. Tsujimura, and Y. Azuma. Assisted Focus Adjustment for Free Space Optics System Coupling Single-Mode Optical Fibers. IEEE Trans. on Industrial Electronics, 60(11), 2013.
[52] X. Zhou et al. Mirror Mirror on the Ceiling: Flexible Wireless Links for Data Centers. In ACM SIGCOMM, 2012.