### Controlled Experiment Setup Using the Emulab Testbed

We conducted our experiments in a virtual environment using the Emulab testbed [7]. The environment consists of three primary components:
1. A host running the Apache Web server as the backend (BE).
2. A host running the Squid Web proxy server as the frontend (FE).
3. A host running a multi-threaded wget Web client that issues requests at an exponentially distributed inter-arrival time of 50 ms.

Additionally, the setup includes delay nodes that use dummynet to control latency. To emulate realistic conditions, we used a one-day trace from several data centers in Google’s CDN that serve users in the USA. We configured the resource size distribution on the BE and emulated the wide area round-trip time on the delay node based on this trace.

For each experiment, we collected tcpdump data and processed it to extract a feature set similar to the one described in Table 1. We did not specifically emulate losses or retransmits because these occurred for fewer than 1% of requests in the USA trace. We conducted two what-if scenario experiments in this environment, which are described below.

### Experiment 1: Changing the Resource Size

For this experiment, we used only the backend server and the client machine, with the delay node emulating wide area network delays. Initially, we collected data using the resource size distribution based on the real trace for about two hours, and used this dataset as the training dataset. For the what-if scenario, we replaced all the resources on the server with resources that were half the size and collected the test dataset for another two hours. We evaluated the test case with WISE using the following specification:

```plaintext
USE *
INTERVENE SET FIXED sB/=2
```

Figure 11(b) shows the response-time distribution for the original page size (dashed), the observed response-time distribution with halved page sizes (dotted), and the response-time distribution for the what-if scenario predicted with WISE using the original page size based dataset as input (solid). The maximum CDF distance in this case is only 4.7%, which occurs around the 40th percentile.

### Experiment 2: Changing the Cache Hit Ratio

For this experiment, we introduced a host running a Squid proxy server to the network and configured the proxy to cache 10% of the resources uniformly at random. There is a delay node between the client and the proxy, as well as between the proxy and the backend server, each emulating trace-driven latency as in the previous experiment. For the what-if scenario, we configured the Squid proxy to cache 50% of the resources uniformly at random. To evaluate this scenario, we included a binary variable `b_cached` for each entry in the dataset, indicating whether the request was served by the caching proxy server or not. We used about 3 hours of trace with 10% caching as the training dataset and used WISE to predict the response-time distribution for the case with 50% caching by using the following specification:

```plaintext
USE *
INTERVENE SETDIST b_cached FILE 50pcdist.txt
```

The `SETDIST` directive tells WISE to update the `b_cached` variable by randomly drawing from the empirical distribution specified in the file, which in this case contains 50% 1s and 50% 0s. Consequently, we intervened 50% of the requests to have a cached response.

Figure 11(c) shows the response-time distribution for the 10% cache-hit ratio (dashed), the response-time distribution with 50% cache-hit ratio (dotted), and the response-time distribution for the 50% caching predicted with WISE using the original 10% cache-hit ratio based dataset as input (solid). WISE predicts the response time quite well for up to the 80th percentile, but there is some deviation for the higher percentiles. This occurred because the training dataset did not contain sufficient data for some of the very large resources or large network delays. The maximum CDF distance in this case is 4.9%, which occurs around the 79th percentile.

### Discussion

In this section, we discuss the limitations and potential extensions of WISE. First, we address what can and cannot be predicted with WISE. We also describe issues related to parametric and non-parametric techniques. Finally, we discuss how the framework can be extended to other areas in networking.

#### What Can and Cannot Be Predicted

The class of what-if scenarios that can be evaluated with WISE depends entirely on the available dataset. WISE has two main requirements:
1. **Expressing the What-If Scenario**: The what-if scenario must be expressed in terms of variables in the dataset and their manipulation. If the dataset does not capture a variable, WISE cannot evaluate scenarios that require manipulation of that hidden variable. For example, the dataset from Google, presented earlier, does not include the TCP timeout variable, so WISE cannot evaluate a scenario that manipulates the TCP timeout.
2. **Dataset Values**: The dataset must contain values of variables that are similar to the values representing the what-if scenario. If the global dataset does not have sufficient points in the space where the manipulated values of the variables lie, the prediction accuracy is affected, and WISE raises warnings during scenario evaluation.

WISE also makes stability assumptions, i.e., the causal dependencies remain unchanged under any values of intervention, and the underlying behavior of the system that determines the response times does not change. This assumption is reasonable as long as the fundamental protocols and methods used in the network do not change.

#### Parametric vs. Non-Parametric Techniques

WISE uses the assumption of functional dependency among variables to update the values for the variables during the statistical intervention evaluation process. In the current implementation, WISE relies on non-parametric techniques for estimating this function. However, the WISE framework can be extended to use parametric functions. If the dependencies among some or all of the variables are parametric or deterministic, WISE's utility can be improved. Such a situation can allow extrapolation to predict variable values outside of what has been observed in the training dataset.

#### What-If Scenarios in Other Realms of Networking

We believe that our work on evaluating what-if scenarios can be extended to incorporate other areas, such as routing, policy decisions, and security configurations, by augmenting the reasoning systems with a decision evaluation system like WISE. Our ultimate goal is to evaluate what-if scenarios for high-level goals, such as, "What if I deploy a new server at location X?" or "How should I configure my network to achieve a certain goal?" We believe that WISE is an important step in this direction.

### Related Work

We are unaware of any technique that uses WISE’s approach of answering what-if deployment questions, but WISE is similar to previous work on TCP throughput prediction and the application of Bayesian inference to networking problems.

A key component in response time for Web requests is TCP transfer latency. Significant work has been done on TCP throughput and latency prediction using TCP modeling [2,5,19]. Due to the inherent complexity of TCP, these models make simplifying assumptions to keep the analysis tractable, which may produce inaccurate results. Recently, there has been an effort to embrace the complexity and use past behavior to predict TCP throughput. He et al. [12] evaluate predictability using short-term history, and Mirza et al. [17] use machine-learning techniques to estimate TCP throughput. These techniques tend to be more accurate. We also use machine-learning and statistical inference in our work, but the techniques of [17] are not directly applicable because they rely on estimating path properties immediately before making a prediction and do not provide a framework for evaluating what-if scenarios.

A recent body of work has explored the use of Bayesian inference for fault and root-cause diagnosis. SCORE [15] uses spatial correlation and shared risk group techniques to find the best possible explanation for observed faults in the network. Shrink [14] extends this model to a probabilistic setting because the dependencies among the nodes may not be deterministic due to incomplete information or noisy measurements. Sherlock [4] additionally finds causes for poor performance and models fail-over and load-balancing dependencies. Rish et al. [21] combine dependency graphs with active probing for fault-diagnosis. None of these works, however, address evaluating what-if scenarios for networks.

### Conclusion

Network designers must routinely answer questions about how specific deployment scenarios affect the response time of a service. Without a rigorous method for evaluating such scenarios, network designers must rely on ad hoc methods or resort to costly field deployments to test their ideas. This paper presents WISE, a tool for specifying and accurately evaluating what-if deployment scenarios for content distribution networks. To our knowledge, WISE is the first tool to automatically derive causal relationships from Web traces and apply statistical intervention to predict networked service performance. Our evaluation demonstrates that WISE is both fast and accurate: it can predict response time distributions in “what if” scenarios to within an 11% error margin. WISE is also easy to use: its scenario specification language makes it easy to specify complex configurations in just a few lines of code.

In the future, we plan to use similar techniques to explore how causal inference can help network designers better understand the dependencies that transcend beyond just performance-related issues in their networks. WISE represents an interesting point in the design space because it leverages almost no domain knowledge to derive causal dependencies; perhaps what-if scenario evaluators in other domains that rely almost exclusively on domain knowledge (e.g., [8]) could also leverage statistical techniques to improve accuracy and efficiency.

### Acknowledgments

We would like to thank Andre Broido and Ben Helsley at Google, and anonymous reviewers for the valuable feedback that helped improve several aspects of our work. We would also like to thank Jeff Mogul for sharing source code for the methods in [2].

### References

[1] Akamai Technologies. www.akamai.com  
[2] M. Arlitt, B. Krishnamurthy, J. Mogul. Predicting short-transfer latency from TCP arcana: A trace-based validation. IMC’2005.  
[3] L.A. Barroso, J. Dean, U. Holzle. Web Search for a Planet: The Google Cluster Architecture. IEEE Micro. Vol. 23, No. 2. pp 22–28  
[4] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. Maltz, M. Zhang. Towards Highly Reliable Enterprise Network Services via Inference of Multi-level Dependencies. ACM SIGCOMM 2007.  
[5] N. Cardwell, S. Savage, T. Anderson. Modeling TCP Latency. IEEE Infocomm 2000.  
[6] G. Cooper. A Simple Constraint-Based Algorithm for Efficiently Mining Observational Databases for Causal Relationships. Data Mining and Knowledge Discovery 1, 203-224. 1997.  
[7] Emulab Network Testbed. http://www.emulab.net  
[8] N. Feamster and J. Rexford. Network-Wide Prediction of BGP Routes. IEEE/ACM Transactions on Networking. Vol. 15. pp. 253–266  
[9] M. Freedman, E. Freudenthal, D. Mazieres. Democratizing Content Publication with Coral. USENIX NSDI 2004.  
[10] A. Gray, A. Moore, ‘N -Body’ Problems in Statistical Learning. Advances in Neural Information Processing Systems 13. 2000.  
[11] Lucene Hadoop. http://lucene.apache.org/hadoop/  
[12] Q. He, C. Dovrolis, M. Ammar. On the Predictability of Large Transfer TCP Throughput. ACM SIGCOMM 2006.  
[13] A. Barbir, et al. Known Content Network Request Routing Mechanisms. IETF RFC 3568. July 2003.  
[14] S. Kandula, D. Katabi, J. Vasseur. Shrink: A Tool for Failure Diagnosis in IP Networks. MineNet Workshop SIGCOMM 2005.  
[15] R. Kompella, J. Yates, A. Greenberg, A. Snoeren. IP Fault Localization Via Risk Modeling. USENIX NSDI 2005.  
[16] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. USENIX OSDI 2004.  
[17] M. Mirza, J. Sommers, P. Barford, X. Zhu. A Machine Learning Approach to TCP Throughput Prediction. ACM SIGMETRICS 2007.  
[18] Netezza http://www.netezza.com/  
[19] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP Throughput: A Simple Model and its Empirical Validation. IEEE/ACM Transactions on Networking. Vol 8. pp. 135-145  
[20] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press. 2003.  
[21] I. Rish, M. Brodie, S. Ma. Efficient Fault Diagnosis Using Probing. AAAI Spring Symposium on DMDP. 2002.  
[22] R. Pike, S. Dorward, R. Griesemer, and S. Quinlan. Interpreting the Data: Parallel Analysis with Sawzall. Scientific Programming Journal. Vol. 13. pp. 227–298.  
[23] P. Sprites, C. Glymour. An Algorithm for fast recovery of sparse causal graphs. Social Science Computer Review 9. USENIX Symposium on Internet Technologies and Systems. 1997.  
[24] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, M. Ammar. Answering “What-if” Deployment and Configuration Questions with WISE. Georgia Tech Technical Report GT-CS-08-02. February 2008.  
[25] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer Texts in Statistics. 2003.  
[26] J. Wolberg. Data Analysis Using the Method of Least Squares. Springer. Feb 2006.