### Stealthy Attack Impact on System Performance

Stealthy attacks slightly reduce the system's true positive rate. However, the false positive rate remains unchanged, indicating that the system continues to effectively protect legitimate messages from being incorrectly flagged as spam.

### 5.5 Run Time Performance

Run time performance is critical for our system, as it must not become a bottleneck that slows down the OSN platform if deployed. The training time in all experiments is only a few seconds. Additionally, we measure latency and throughput to demonstrate that our system can quickly respond to incoming messages and efficiently inspect a large number of messages per second.

#### Latency

Latency is measured as the time between when the system receives a message and when it outputs the inspection result. In this experiment, we process all messages sequentially. Figure 17 shows the cumulative distribution of the system latency in milliseconds. For the Facebook dataset, the average and median latency are 21.5 ms and 3.1 ms, respectively. For the Twitter dataset, the average and median latency are 42.6 ms and 7.0 ms, respectively. Even with the longer processing delay observed in the Twitter dataset, over 90% of messages are inspected within 100 ms. Given that modern browsers take several hundred milliseconds to start rendering ordinary webpages [17], the system's latency is sufficiently low. An OSN user would not experience any noticeable additional delay due to the deployment of our system.

**Figure 17: Cumulative Distribution of System Latency (ms)**

```
 100
 80
 60
 40
 20
)
F
D
C
(
s
e
g
a
s
s
e
m
f
o
%
 0
 0
Facebook data
Twitter data
 100
 200
 300
 400
 500
Latency (ms)
```

#### Throughput

To test throughput, we feed the system with the testing set as quickly as possible. Due to hardware limitations (8 physical cores with Hyper-Threading), we execute 16 threads simultaneously. We calculate the average throughput by dividing the total number of messages processed by the total running time. The throughput on the Facebook and Twitter datasets is 1580 messages/sec and 464 messages/sec, respectively.

### 6 Discussion

Spammers continuously challenge anti-spam solutions. Although our system is robust against stealthy attacks, spammers may adopt other tactics to evade it. One possible method is to tamper with the clustering process by reducing the syntactic similarity among messages from the same campaign. We have observed such attempts in both datasets. In the Facebook dataset, spam campaigns include obfuscation in the textual message, such as using "proﬁl imaage" instead of "proﬁle image." In the Twitter dataset, almost all spam tweets contain random meaningless words, likely obtained from a collection of popular words. Despite these efforts, our system maintains good detection accuracy. Obfuscation and embedded random chunks also decrease the message readability and trigger recipients' suspicion, reducing the conversion rate. Another evasion technique involves manipulating the features of spam clusters to make them indistinguishable from legitimate clusters. While it is feasible to manipulate individual features, doing so has limited effect since our system uses a combination of six features. Simultaneously manipulating all features would be costly for spammers, as it would significantly reduce the recipient population and slow down spam generation. A third evasion technique is to produce image-based spam, which our current design does not address.

### 7 Related Work

We discuss prior related work by categorizing it into two general areas: studies of spamming in OSNs and in other environments.

#### Spam Studies in OSNs

- **Stein et al.** present a framework for an adversarial learning system that performs real-time classification on read and write actions in Facebook [25]. However, the lack of detailed information and performance results limits further comparison.
- **Offline studies** have revealed large-scale spam campaigns in Twitter and Facebook [9, 10]. These tools are designed for offline analysis and cannot be directly used for online spam detection. [10] relies on URL blacklists, which have a long lag time, while [9] uses a similar clustering technique but requires the complete set of messages, limiting its efficiency.
- **Incremental Clustering and Parallelization**: Our approach adopts incremental clustering and parallelization to inspect messages based on past observations and increase scalability. We also develop a feature set to distinguish spam clusters efficiently.
- **Thomas et al.** propose real-time filtering of malicious URLs in OSNs [27], which can later identify malicious messages. While their approach analyzes URLs' landing pages, ours uses message content.
- **Song et al.** use sender-receiver relationships to classify Twitter messages [24].
- **Stringhini et al.** use machine learning techniques to detect spamming bots in OSNs [26].
- **Yardi et al.** use ad-hoc criteria to identify spamming bots on Twitter [32].
- **Benevenuto et al.** and **Markines et al.** apply supervised machine learning to detect spammers in YouTube and social bookmarking sites, respectively [5, 16].

#### Other Spam Studies

- **Email Spam**: There is extensive research on email spam characteristics [4, 12, 13, 35], but few can be used for online detection.
- **IP Blacklists**: Researchers propose enhancing IP blacklists by correlating IP addresses based on email sending history, network-level information, and blacklisting history [19, 22, 28].
- **Template Extraction**: Pitsillidis et al. extract underlying templates to match future spams [18].
- **Bayesian Filter Enhancement**: Li et al. enhance Bayesian filters using personal social network information [15].
- **Regular Expression Signatures**: Xie et al. generate regular expression signatures for spamming URLs [30].
- **Comprehensive Feature List**: Thomas et al. use a comprehensive list of features to determine whether a given URL directs to spam in real-time [27].

### 8 Conclusions

In this paper, we describe our work on providing online spam filtering for social networks. We use text shingling and URL comparison to incrementally reconstruct spam messages into campaigns, which are then identified by a trained classifier. We evaluate the system on two large datasets: over 187 million Facebook wall messages and 17 million tweets. The experimental results show that the system achieves high accuracy, low latency, and high throughput, essential properties for an online system. Additionally, the system remains accurate for more than nine months after the training phase, demonstrating its low maintenance cost. For more information, please refer to the project web page at http://list.cs.northwestern.edu/.

### Acknowledgments

We express our sincere thanks to the anonymous reviewers for their valuable feedback. The authors denoted with † are supported by NSF award numbers CCF-0621443, OCI-0724599, CCF-0833131, CNS-0830927, IIS-0905205, OCI-0956311, CCF-0938000, CCF-1043085, CCF-1029166, and OCI-1144061.

### References

[1] Users of social networking websites face malware and phishing attacks. Symantec.com Blog.
[2] What the trend. http://www.whatthetrend.com/.
[3] Zeus botnet targets facebook. http://blog.appriver.com/2009/10/zeus-botnet-targets-facebook.html.
[4] ANDERSON, D. S., FLEIZACH, C., SAVAGE, S., AND VOELKER, G. M. Spamscatter: characterizing internet scam hosting infrastructure. In Proceedings of 16th USENIX Security Symposium on USENIX Security Symposium (Berkeley, CA, USA, 2007), USENIX Association, pp. 10:1–10:14.
[5] BENEVENUTO, F., RODRIGUES, T., AND ALMEIDA, V. Detecting spammers and content promoters in online video social networks. In Proc. of SIGIR (Boston, Massachusetts, USA, July 2009).
[6] BOGU ˜N ´A, M., PASTOR-SATORRAS, R., AND VESPIGNANI, A. Epidemic spreading in complex networks with degree correlations.
[7] BRODER, A. Z., GLASSMAN, S. C., MANASSE, M. S., AND ZWEIG, G. Syntactic clustering of the web. Comput. Netw. ISDN Syst. 29 (September 1997), 1157–1166.
[8] BURGES, C. J. C. A tutorial on support vector machines for pattern recognition. Data Min. Knowl. Discov. 2 (June 1998), 121–167.
[9] GAO, H., HU, J., WILSON, C., LI, Z., CHEN, Y., AND ZHAO, B. Y. Detecting and characterizing social spam campaigns. In Proceedings of the 10th annual conference on Internet measurement (New York, NY, USA, 2010), IMC ’10, ACM, pp. 35–47.
[10] GRIER, C., THOMAS, K., PAXSON, V., AND ZHANG, M. @spam: the underground on 140 characters or less. In Proceedings of the 17th ACM conference on Computer and communications security (New York, NY, USA, 2010), CCS ’10, ACM, pp. 27–37.
[11] HAO, S., SYED, N. A., FEAMSTER, N., GRAY, A. G., AND KRASSER, S. Detecting spammers with snare: spatio-temporal network-level automatic reputation engine. In Proceedings of the 18th conference on USENIX security symposium (Berkeley, CA, USA, 2009), SSYM’09, USENIX Association, pp. 101–118.
[12] KANICH, C., KREIBICH, C., LEVCHENKO, K., ENRIGHT, B., VOELKER, G. M., PAXSON, V., AND SAVAGE, S. Spamalytics: An empirical analysis of spam marketing conversion. In Proc. of the ACM Conference on Computer and Communications Security (October 2008).
[13] KREIBICH, C., KANICH, C., LEVCHENKO, K., ENRIGHT, B., VOELKER, G., PAXSON, V., AND SAVAGE, S. Spamcraft: An inside look at spam campaign orchestration. In Proc. of LEET (2009).
[14] LEE, K., CAVERLEE, J., AND WEBB, S. Uncovering social spammers: social honeypots + machine learning. In Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval (New York, NY, USA, 2010), SIGIR ’10, ACM, pp. 435–442.
[15] LI, Z., AND SHEN, H. SOAP: A Social Network Aided Personalized and Effective Spam Filter to Clean Your E-mail Box. In Proceedings of the IEEE INFOCOM (April 2011).
[16] MARKINES, B., CATTUTO, C., AND MENCZER, F. Social spam detection. In Proc. of AIRWeb (2009).
[17] MOSHCHUK, A., BRAGIN, T., DEVILLE, D., GRIBBLE, S. D., AND LEVY, H. M. Spyproxy: execution-based detection of malicious web content. In Proceedings of 16th USENIX Security Symposium on USENIX Security Symposium (Berkeley, CA, USA, 2007), USENIX Association, pp. 3:1–3:16.
[18] PITSILLIDIS, A., LEVCHENKO, K., KREIBICH, C., KANICH, C., VOELKER, G., PAXSON, V., WEAVER, N., AND SAVAGE, S. Botnet Judo: Fighting Spam with Itself. In Proceedings of the 17th Annual Network and Distributed System Security Symposium (NDSS) (San Diego, CA, USA, March 2010).
[19] QIAN, Z., MAO, Z. M., XIE, Y., AND YU, F. On Network-level Clusters for Spam Detection. In Proceedings of the 17th Annual Network and Distributed System Security Symposium (NDSS) (San Diego, CA, USA, March 2010).
[20] QUINLAN, J. R. Ross quinlan’s personal homepage. http://www.rulequest.com/Personal/.
[21] QUINLAN, J. R. Induction of decision trees. Mach. Learn. 1 (March 1986), 81–106.
[22] RAMACHANDRAN, A., FEAMSTER, N., AND VEMPALA, S. Filtering spam with behavioral blacklisting. In Proceedings of the 14th ACM conference on Computer and communications security (New York, NY, USA, 2007), CCS ’07, ACM, pp. 342–351.
[23] SHIN, Y., GUPTA, M., AND MYERS, S. Prevalence and mitigation of forum spamming. In Proceedings of IEEE International Conference on Computer Communicationi (INFOCOM) (Shanghai, China, 2011), IEEE Computer Society.
[24] SONG, J., LEE, S., AND KIM, J. Spam filtering in twitter using sender-receiver relationship. In Proceedings of the 14th International Symposium on Recent Advances in Intrusion Detection (RAID’11)) (September 2011).
[25] STEIN, T., CHEN, E., AND MANGLA, K. Facebook immune system. In Proceedings of the 4th Workshop on Social Network Systems (SNS’11) (New York, NY, USA, 2011), ACM.
[26] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G. Detecting spammers on social networks. In Proceedings of the 26th Annual Computer Security Applications Conference (New York, NY, USA, 2010), ACSAC ’10, ACM, pp. 1–9.
[27] THOMAS, K., GRIER, C., MA, J., PAXSON, V., AND SONG, D. Design and Evaluation of a Real-Time URL Spam Filtering Service. In Proceedings of the IEEE Symposium on Security and Privacy (May 2011).
[28] WEST, A. G., AVIV, A. J., CHANG, J., AND LEE, I. Spam mitigation using spatio-temporal reputations from blacklist history. In Proceedings of the 26th Annual Computer Security Applications Conference (New York, NY, USA, 2010), ACSAC ’10, ACM, pp. 161–170.
[29] WILSON, C., BOE, B., SALA, A., PUTTASWAMY, K. P., AND ZHAO, B. Y. User interactions in social networks and their implications. In Proceedings of the ACM European conference on Computer systems (2009).
[30] XIE, Y., YU, F., ACHAN, K., PANIGRAHY, R., HULTEN, G., AND OSIPKOV, I. Spamming botnets: signatures and characteristics. In Proc. of SIGCOMM (2008).
[31] YANG, C., HARKREADER, R., AND GU, G. Die free or live hard? empirical evaluation and new design for fighting evolving twitter spammers. In Proceedings of the 14th International Symposium on Recent Advances in Intrusion Detection (RAID’11)) (September 2011).
[32] YARDI, S., ROMERO, D., SCHOENEBECK, G., AND BOYD, D. Detecting spam in a twitter network. First Monday 15, 1 (2010).
[33] ZADROZNY, B., LANGFORD, J., AND ABE, N. Cost-sensitive learning by cost-proportionate example weighting. In Proceedings of the Third IEEE International Conference on Data Mining (Washington, DC, USA, 2003), ICDM ’03, IEEE Computer Society, pp. 435–.
[34] ZHAO, Y., XIE, Y., YU, F., KE, Q., YU, Y., CHEN, Y., AND GILLUM, E. Botgraph: large scale spamming botnet detection. In Proceedings of the 6th USENIX symposium on Networked systems design and implementation (Berkeley, CA, USA, 2009), USENIX Association, pp. 321–334.
[35] ZHUANG, L., DUNAGAN, J., SIMON, D. R., WANG, H. J., AND TYGAR, J. D. Characterizing botnets from email spam records. In Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats (Berkeley, CA, USA, 2008), USENIX Association, pp. 2:1–2:9.
[36] ZOU, C. C., TOWSLEY, D., AND GONG, W. Modeling and simulation study of the propagation and defense of internet e-mail worms. IEEE Trans. Dependable Secur. Comput. 4 (April 2007), 105–118.