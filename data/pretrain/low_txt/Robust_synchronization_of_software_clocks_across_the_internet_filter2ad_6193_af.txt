### Performance of Offset Errors

**Figure 12: Performance of Offset Errors Over a 3-Month Period Using ServerInt**

- **Polling Periods:**
  - Left: 64 seconds
  - Right: 256 seconds

- **Histograms:**
  - Show exactly 99% of all values (note the scale change).

- **Offset Error [ms]:**
  - Ranges from -0.12 to 0 ms.

- **Equation:**
  - \(\tau_{\text{eff}} = 2\tau^*\)

### Robustness to Unforeseen Events

To ensure robustness against unforeseen and extreme events, it is crucial to design the system in a generic way rather than relying on dedicated algorithmic branches. These events may include:

- Loss of connectivity
- Changes in the route to the host or server
- Unexpected changes in the temperature environment
- Errors in the server’s clock

### Level Shifts in Data

Level shifts can occur due to changes in the route or server. Examples of such events were observed in our experimental data. The next subsection will address this topic in detail.

### Extreme Event Performance

**Figure 11: Algorithm Performance Under Extreme Conditions**

- **(a) Fast Recovery After Data Gap:**
  - Demonstrates fast recovery after a 3.8-day gap in data collection, simulating server unavailability.
  
- **(b) Impact of Server Error:**
  - Shows the impact of a server error lasting a few minutes, during which \(T_b,i\) and \(T_e,i\) were each offset by 150 ms. The offset (and local rate) sanity check algorithm was triggered, limiting the damage to a millisecond or less.

### Robustness to Level Shifts

**Definition:**
- A level shift refers to a change in any of the minimum delays \(d_{\rightarrow}\), \(d_{\leftarrow}\), or \(r\), resulting in a change in the minimum level in some or all of the observed data.

**Key Issues:**
- **Asymmetry of Shift Direction:**
  - **Downward:** Congestion cannot cause a downward movement, making detection straightforward.
  - **Upward:** Reliable only at large scales, making detection difficult.
  
- **Asymmetry of Detection Errors:**
  - **False Positive (Quality Packet as Bad):** An undetected upward shift looks like congestion, which the algorithms are already robust to.
  - **False Negative (Bad Quality Packet as Good):** Falsely interpreting congestion as an upward shift corrupts estimates, potentially severely.

- **Asymmetry of Offset and Rate:**
  - **Offset:** Store past estimates and their point errors relative to the \(\hat{r}\) estimate made at the time.
  - **Rate:** Use point errors relative to the current error level (after any shifts).

**Detection and Reaction Scheme:**
- **Downward Shift:**
  - **Detection:** Automatic and immediate when using \(\hat{r}\).
  - **Reaction:** No additional steps required. Algorithms will see the shift as poor quality of past packets and react normally. Over time, increasing \(\Delta(t)\) and windowing will improve packet qualities.

- **Upward Shift:**
  - **Detection:** Based on maintaining a local minimum estimate \(\hat{r}_l\) over a sliding window of width \(T_s\). Choose \(T_s\) large (\(T_s = \bar{\tau}/2\)) and detect a shift if \(|\hat{r}_l - \hat{r}| > 4E\).
  - **Reaction:** Update \(\hat{r} = \hat{r}_l\) and recalculate \(\hat{\theta}_i\) values back to the shift point. Before detection, the algorithms will see the packets as having poor quality and react normally. Since the window is large, estimates may degrade toward the end of the window.

### Experimental Results

- **Figure 11(c):** Two upward shifts of 0.9 ms were artificially introduced. The first, being under \(T_s\) in duration, was never detected and had little impact on the estimates. The second was permanent and detected at \(T_s\) later, resulting in a jump in subsequent offset estimates.
- **Figure 11(d):** A natural permanent downward shift occurred equally in each direction, so \(\Delta\) did not change. Detection and reaction were immediate, with no observable change in estimation quality.

### Conclusion

We have presented a detailed reexamination of the problem of inexpensive, convenient, yet accurate clock synchronization for networked PCs. Our approach is based on a thorough understanding of the stability of the CPU oscillator as a timing source, accessible via the TSC register. Using the NTP server network and TSC timestamps, we showed how to calibrate a clock in terms of rate and offset. We explained the importance of maintaining distinct software clocks for these tasks. Our approach is new in its focus on rate, grounded in a meaningful time-scale analysis of the underlying hardware, and through the development of filtering algorithms with relatively few ad-hoc elements. This allows for higher accuracy and reliability.

Using months of real data from three different NTP servers, we provided a systematic and thorough testing of the algorithm, its absolute performance, and sensitivity to parameters. With a nearby server, we achieved reliable synchronization to within 30 µs and rate accuracy of around 0.02 PPM. We demonstrated robustness to various effects, including packet loss, loss of server connectivity, changes in server, network congestion, temperature environment, timestamping noise, and even faulty server timestamps.

### Acknowledgements

- **BSD Timestamping Code:** Developed and implemented by Michael Dwyer.
- **Temperature Logging Assistance:** Provided by David Batterham and Karl Cirulis at the University of Melbourne.
- **Support:** This work was supported by Ericsson.

### References

1. D.L. Mills, “Internet time synchronization: the network time protocol,” IEEE Trans. Communications, vol. 39, no. 10, pp. 1482–1493, October 1991, Condensed from RFC-1129.
2. C. Liao et al., “Experience with an adaptive globally synchronizing clock algorithm,” in Proc. of 11th ACM Symp. on Parallel Algorithms and Architectures, June 1999.
3. Vern Paxson, “On calibrating measurements of packet transit times,” in Proceedings of the 1998 ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems, 1998, pp. 11–21, ACM Press.
4. Jonas Andren, Magnus Hilding, and Darryl Veitch, “Understanding end-to-end internet traffic dynamics,” in IEEE Global Telecommunications Conference (Globecom’98), Sydney, Australia, Nov. 1998, vol. 2, pp. 1118–1122.
5. Attila P´asztor and Darryl Veitch, “PC based precision timing without GPS,” in Proceeding of ACM Sigmetrics 2002 Conference on the Measurement and Modeling of Computer Systems, Del Rey, California, 15–19 June 2002, pp. 1–10.
6. “RIPE NCC Test Traffic Measurements,” http://www.ripe.net/ttm/.
7. Attila P´asztor and Darryl Veitch, “Software infrastructure for accurate active probing,” http://www.cubinlab.ee.mu.oz.au/probing/software.shtml/.
8. Jörg Micheel, Ian Graham, and Stephen Donnelly, “Precision timestamping of network packets,” in Proc. of the SIGCOMM IMW, November 2001.
9. D.L. Mills, “The network computer as precision timekeeper,” in Proc. Precision Time and Time Interval (PTTI) Applications and Planning Meeting, Reston VA, December 1996, pp. 96–108.
10. Attila P´asztor and Darryl Veitch, “A precision infrastructure for active probing,” in Passive and Active Measurement Workshop (PAM2001), Amsterdam, The Netherlands, 23–24 April 2001, pp. 33–44.
11. Victor Yodaiken, “The RTLinux Manifesto,” Tech. Rep., Department of Computer Science, New Mexico Institute of Technology, 1999, available at http://www.rtlinux.org.
12. P. Abry, D. Veitch, and P. Flandrin, “Long-range dependence: revisiting aggregation with wavelets,” Journal of Time Series Analysis, vol. 19, no. 3, pp. 253–266, May 1998, Bernoulli Society.
13. “Cooperative Association for Internet Data Analysis (CAIDA) Skitter project,” http://www.caida.org/tools/measurement/skitter/.