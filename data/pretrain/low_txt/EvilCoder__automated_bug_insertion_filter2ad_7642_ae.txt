### 5. Limitations and Future Work

In this section, we discuss the limitations of our current approach and prototype, potential future work, and alternative use cases for automated bug insertion.

#### 5.1 Exploitability

Our technique identifies paths between user-controlled sources and sensitive sinks, and then modifies or removes security mechanisms on these paths to create security-relevant bugs. However, this may not be sufficient to generate an exploitable vulnerability, as we cannot guarantee the global satisfiability of all path conditions necessary to traverse the path in question.

One argument in favor of potential exploitability is that the security mechanisms are present in the program. If no values existed to traverse the path, the security mechanism would be superfluous. This assumes that no overly defensive programming strategy was used.

While there is research on automatic generation of exploits [2, 4] or at least some proof of the seriousness of the bug [19], verifying all generated bugs with such complex additional components is impractical. As mentioned in Section 4.4, we attempted to justify our claim of exploitability by reintroducing known-to-be-exploitable bugs. Nonetheless, automatic satisfiability verification and exploit generation are beyond the scope of this paper.

Unfortunately, the lack of confirmed vulnerabilities, combined with the inability to count the number of introduced bugs in a meaningful way, means that it is not possible to report a false positive rate for our approach.

#### 5.2 Additional Vulnerabilities

The bugs we introduce are limited in two ways: first, we only support a limited number of vulnerability types, all belonging to the class of taint-style vulnerabilities, which allow for conventional exploitation. Therefore, we cannot introduce other types of bugs at this time. However, we believe that implementing additional bug classes would be straightforward.

Second, while we add some randomness to the introduction of bugs, they undoubtedly follow a pattern. For the use case of an artificial bug corpus, this could be problematic, as it would be a valid strategy to model the heuristics we used to find the bugs later. However, given that we can automatically introduce such bugs, finding them—whether they have a pattern or not—is absolutely mandatory. Thus, we create a baseline to evaluate techniques aimed at detecting vulnerabilities in an automated manner.

Furthermore, future work could involve inserting additional vulnerable paths or functions instead of merely weakening existing security mechanisms.

#### 5.3 Alternative Use Cases

Although we focused on generating test corpora in this paper, we see other potential use cases for bug insertion. Capture-the-flag (CTF) contests, for example, require vulnerable programs for exploitation challenges. While our approach does not guarantee exploitability and is not yet targeted towards tricky-to-exploit vulnerabilities, it could still be a valuable tool for contest organizers.

Additionally, the ability to insert exploitable bugs could theoretically facilitate later exploitation. However, in this scenario, the attacker would need write access to the source code. A recent publication [5] shows that tampering with version control systems is feasible, but it remains a significant obstacle. Furthermore, the inserted vulnerability should be hard to find and exploitable for a long time. Given that we aim to insert many vulnerabilities rather than a single, special one, we believe that manual effort would be more effective for an attacker, and thus do not see an ethical problem with our approach.

### 6. Related Work

The work most closely related to our approach is a recently published paper entitled "LAVA: Large-scale Automated Vulnerability Addition" [9]. The authors aim to generate a sufficient number of bugs for testing bug-finding tools.

Unlike our approach, LAVA uses a dynamic method by tainting input bytes and tracing them through the program. They specifically look for rarely modified and dead data, for which they then insert code performing either a buffer overread or buffer overflow. If necessary, they introduce new static or global variables to enable the required data flow. Additionally, they insert guards to execute the vulnerability only if a magic value occurs in the input. In contrast, our approach transforms code from invulnerable to vulnerable, i.e., inserts bugs. We have chosen a different methodology, making this concurrent and independent work.

Given the dynamic approach, LAVA also generates inputs triggering the vulnerabilities and provides preliminary results showing that state-of-the-art fuzzers and symbolic execution engines cannot find all the bugs they can add. This finding underscores the importance of automated bug insertion.

#### 6.1 Insufficient Test Data

Miller [26] uses a set of 16 hand-written vulnerabilities to compare eight fuzzers and states that, while the scarcity of test cases is a problem, these 16 artificial test cases already offer significant insight.

Nilson et al. [29] state that "existing sources of vulnerability data did not supply the necessary structure or metadata to evaluate them completely," leading them to develop BugBox, a simulation environment with an accompanying corpus of vulnerabilities and exploits. Their vulnerabilities are real-world examples specific to PHP, focusing mostly on exploitation aspects.

Delaitre et al. [7] evaluated 14 static analyzers and established three critical characteristics for vulnerability test cases:
1. **Statistical Significance**: There must be many, diverse vulnerabilities.
2. **Ground Truth**: The location of the vulnerabilities must be known.
3. **Relevance**: The vulnerabilities must be representative of those found in real source code.

Test corpora generated with our approach fulfill the first two characteristics, and we are confident that the third characteristic can be met with carefully stated bug models, given that the instrumented code stems from real programs.

#### 6.2 Vulnerability Databases

According to Nilson et al. [29] and Delaitre et al. [7], existing databases are not sufficient for a comprehensive evaluation of bug-finding techniques. However, several databases are worth mentioning.

First, the Common Vulnerabilities and Exposures (CVE) and the Common Weakness Enumeration (CWE) are notable. CVE consists of short descriptions of real-world vulnerabilities and links to further resources, primarily for unambiguous identification. CWE often offers code snippets to illustrate hierarchized vulnerabilities, but these may not be sufficient for comprehensive evaluations.

For web development, OWASP WebGoat [31] and SecuriBench [20] collect vulnerabilities for illustrative purposes but do not provide a structured corpus necessary for evaluation.

The most useful public database for evaluating bug-finding techniques is generated by the NIST project Software Assurance Metrics And Tool Evaluation (SAMATE) [30]. Its largest standalone test suite contains over 60,000 vulnerable synthetic test cases, uploaded in 2013. These test suites are static and cannot generate fresh bugs. The project also includes the IARPA program Securely Taking on Software of Uncertain Provenance (STONESOUP) [14], which provides 164 Java and C snippets that can be inserted into other programs to make them vulnerable. However, these snippets are static and require specific environments.

#### 6.3 Mutation Testing

Mutation testing [16] randomly modifies the source code to make it behave slightly differently at runtime. This helps estimate the test set's ability to catch errors and the necessity and importance of the modified code. While, in principle, bugs like those introduced by our approach could be inserted via mutation testing, we purposefully insert special bugs at carefully selected locations to introduce vulnerabilities.

### 7. Conclusions

In this paper, we proposed an approach for the automatic generation of bug-ridden test corpora. Our prototype implementation targets the insertion of spatial memory errors by modifying security checks using six different instrumentation classes. With such test corpora, we aim to facilitate future research in bug-finding techniques, enabling their evaluation and comparison in an objective and statistically meaningful way.

### Acknowledgments

This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 640110 – BASTION). It was also supported by the German Research Foundation (DFG) research training group UbiCrypt (GRK 1817).

We thank Engin Kirda, William Robertson, Patrick Carter, Timothy Leek, Patrick Hulin, and Brendan Dolan-Gavitt for their fruitful discussions. We also thank Jan Teske and Tilman Bender for their support in our implementation efforts.

### References

[1] Aleph One. Smashing the stack for fun and profit. Phrack, 7(49), November 1996.

[2] T. Avgerinos, S. K. Cha, B. L. T. Hao, and D. Brumley. AEG: Automatic Exploit Generation. In Symposium on Network and Distributed System Security (NDSS), 2011.

[3] N. Borisov, R. Johnson, N. Sastry, and D. Wagner. Fixing races for fun and profit: How to abuse atime. In USENIX Security Symposium, 2005.

[4] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley. Unleashing Mayhem on Binary Code. In IEEE Symposium on Security and Privacy, 2012.

[5] R. Curtmola, S. Torres-Arias, A. Ammula, and J. Cappos. On omitting commits and committing omissions: Preventing git metadata tampering that (re)introduces software vulnerabilities. In USENIX Security Symposium, 2016.

[6] D. Davidson, B. Moench, T. Ristenpart, and S. Jha. Fie on firmware: Finding vulnerabilities in embedded systems using symbolic execution. In USENIX Security Symposium, 2013.

[7] A. Delaitre, B. Stivalet, E. Fong, and V. Okun. Evaluating bug finders: Test and measurement of static code analyzers. In First International Workshop on Complex faUlts and Failures in LargE Software Systems (COUFLESS), 2015.

[8] W. Dietz, P. Li, J. Regehr, and V. Adve. Understanding integer overflow in C/C++. In International Conference on Software Engineering (ICSE), 2012.

[9] B. Dolan-Gavitt, P. Hulin, E. Kirda, T. Leek, A. Mambretti, W. Robertson, F. Ulrich, and R. Whelan. LAVA: Large-scale Automated Vulnerability Addition. In IEEE Symposium on Security and Privacy, 2016.

[10] J. E. Forrester and B. P. Miller. An empirical study of the robustness of Windows NT applications using random testing. In USENIX Windows Systems Symposium (WSS), 2000.

[11] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. DARPA TIMIT acoustic phonetic continuous speech corpus CDROM, 1993.

[12] P. Godefroid. Random testing for security: Blackbox vs. whitebox fuzzing. In International Workshop on Random Testing: Co-located with the IEEE/ACM International Conference on Automated Software Engineering (ASE), 2007.

[13] S. Horwitz. Precise flow-insensitive may-alias analysis is NP-hard. ACM Transactions on Programming Languages and Systems (TOPLAS), 19(1), 1997.

[14] IARPA. Securely taking on software of uncertain provenance (STONESOUP), 2015. http://www.iarpa.gov/index.php/research-programs/stonesoup.

[15] J. Jang, A. Agrawal, and D. Brumley. ReDeBug: Finding Unpatched Code Clones in Entire OS Distributions. In IEEE Symposium on Security and Privacy, 2012.

[16] Y. Jia and M. Harman. An analysis and survey of the development of mutation testing. IEEE Transactions on Software Engineering, 37(5), 2011.

[17] C. Lattner, A. Lenharth, and V. Adve. Making context-sensitive points-to analysis with heap cloning practical for the real world. In ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2007.

[18] L. Li, C. Cifuentes, and N. Keynes. Boosting the performance of flow-sensitive points-to analysis using value flow. In ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE), 2011.

[19] Z. Lin, X. Zhang, and D. Xu. Convicting exploitable software vulnerabilities: An efficient input provenance based approach. In International Conference on Dependable Systems and Networks, 2008.

[20] B. Livshits. Defining a set of common benchmarks for web application security, 2005.

[21] B. Livshits and S. Chong. Towards fully automatic placement of security sanitizers and declassifiers. In ACM Symposium on Principles of Programming Languages (POPL), 2013.

[22] V. B. Livshits and M. S. Lam. Finding Security Vulnerabilities in Java Applications with Static Analysis. In USENIX Security Symposium, 2005.

[23] M. E. Locasto and S. Bratus. Hacking the Abacus: An Undergraduate Guide to Programming Weird Machines. http://www.cs.dartmouth.edu/sergey/drafts/sismat-manual-locasto.pdf, 2014.

[24] Y. Lu, S. Yi, Z. Lei, and Y. Xinlei. Binary software vulnerability analysis based on bidirectional-slicing. In Conference on Instrumentation, Measurement, Computer, Communication and Control (IMCCC), 2012.

[25] B. P. Miller, L. Fredriksen, and B. So. An Empirical Study of the Reliability of UNIX Utilities. Communications of ACM, 1990.

[26] C. Miller. Fuzz by number. CanSecWest Conference, 2008.

[27] L. Moonen. Generating robust parsers using island grammars. In Working Conference on Reverse Engineering (WCRE), 2001.

[28] Neo4j. Neo4j - the world’s leading graph database, 2012. http://neo4j.org/.

[29] G. Nilson, K. Wills, J. Stuckman, and J. Purtilo. Bugbox: A vulnerability corpus for PHP web applications. In Workshop on Cyber Security Experimentation and Test (CSET), 2013.

[30] NIST. SAMATE - Software Assurance Metrics And Tool Evaluation, 2015. https://samate.nist.gov/.

[31] OWASP. OWASP WebGoat Project, 2015. https://www.owasp.org/index.php/Category:OWASP_WebGoat_Project.

[32] D. B. Paul and J. M. Baker. The design for the Wall Street Journal-based CSR corpus. In Workshop on Speech and Natural Language, 1992.

[33] J. Pewny, B. Garmany, R. Gawlik, C. Rossow, and T. Holz. Cross-architecture bug search in binary executables. In IEEE Symposium on Security and Privacy, 2015.

[34] D. Song, D. Brumley, J. Caballero, I. Jager, M. G. Kang, Z. Liang, J. Newsome, P. Poosankam, and P. Saxena. Bitblaze: A new approach to computer security via binary analysis. In International Conference on Information Systems Security, 2008.

[35] X. Wang, H. Chen, A. Cheung, Z. Jia, N. Zeldovich, and M. F. Kaashoek. Undefined behavior: What happened to my code? In Asia-Pacific Workshop on Systems (APSYS), 2012.

[36] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck. Modeling and discovering vulnerabilities with code property graphs. In IEEE Symposium on Security and Privacy, 2014.