### VII. CONCLUSIONS

In this paper, we introduce a new methodology for checkpoint-rollback recovery in stateful stream processing operators, which we term Continuous Eventual Checkpointing (CEC). This innovative mechanism performs asynchronous and independent checkpoints of partial operator states in the form of control tuples generated by the operator. These individual window checkpoints are interleaved with regular output tuples in the operator's output queue and stored in stable storage. During the recovery process, CEC reconstructs a full checkpoint by processing the output queue, which is then loaded onto the operator. The checkpoint determines the number of tuples that need to be replayed by the upstream source.

Our results indicate that CEC does not penalize operator processing when operating under minimal recovery guarantees. Stronger recovery guarantees can be achieved by tuning the Q and U target parameters, which regulate the extent size of the eventual checkpoint and the upstream queue replay size. Additionally, the checkpoint interval (CI) and period (CP) parameters can be adjusted to meet the desired response-time objectives. Overall, our findings demonstrate that CEC is a simple, low-overhead, and configurable checkpoint-rollback solution for mission-critical stream processing operators.

### VIII. ACKNOWLEDGMENTS

We gratefully acknowledge the support provided by the European FP7-ICT program through the STREAM (STREP 216181) and SCALEWORKS (MC IEF 237677) projects.

### IX. APPENDIX: PERSISTENCE ARCHITECTURE

The integration of operator state checkpoints with regular output tuples in the operator's output queue necessitates a queue persistence mechanism within a stream processing engine (SPE). The upper-left part of Figure 10 illustrates a standard SPE structure, as found in the Aurora/Borealis system [4]. In this structure, incoming tuples from a stream S are managed by an Enqueue thread, which enqueues them into the SPE for processing by operators. The output tuples produced by the operators are placed on an output stream and dequeued by a separate Dequeue thread, which groups them into stream events (SEs). SEs are serialized objects that group several tuples for efficient communication. The remainder of the figure describes the path for persisting SEs before communicating them to downstream nodes.

#### Detailed Description

**Failure-free Operation:**
- SEs created by the Dequeue thread are first serialized.
- If the associated streams are set for persistence (configured per-operator), the SEs enter the persist-event list; otherwise, they move directly to the forward-event list.
- A non-blocking write operation to storage is initiated using an asynchronous notification API [2].
- Checksums are used to verify the integrity of the I/O operations on the storage device.
- Asynchronous I/O operations are managed by a state machine in an event loop.
- For parallelism, a configurable window of N concurrently outstanding I/Os is maintained.
- Upon completion of a write I/O, the per-stream index (Figure 10) is updated, and the persisted event data structure is moved to the forward-event list, followed by a network send operation.
- The stream index maps timestamps to serialized SEs, typically using file offsets. In our current prototype, the index is implemented using an Oracle Berkeley DB database.

**Operation Under Failure:**
- When a downstream SPE node fails, all connected streams disconnect, and no outgoing network communication occurs until reconnection.
- SEs produced by local operators are still persisted as described during failure-free operation.
- Once the SPE receives an I/O completion for an SE, it deletes the SE from memory.
- Other SEs belonging to still-connected streams proceed to the forward-event list as described during failure-free operation.

**Recovery:**
- A recovering SPE node reconnects to upstream SPEs serving the streams it was connected to before the failure.
- The following steps are performed:
  1. Reconcile the stream index in the DB with the log length reported by the file system.
  2. Determine the last consistent operator checkpoint, load the checkpoint, and determine the timestamp to start replaying from, as described in Section II.B.
  3. Communicate the timestamp to the appropriate upstream SPEs, which will replay tuples from those streams.
- Upon a request from a downstream node, the upstream node looks up the requested timestamp in its stream index, returning a pointer to the SE containing the requested timestamped tuple.
- The node then initiates asynchronous read I/O operations for stored SEs starting from the specified offset, similar to the write operations described during failure-free operation.
- Upon completion of a read I/O, the retrieved SE may need to be de-serialized, re-serialized, and put into the forward-event list, then sent over the network to the connected downstream SPEs.

**Catching Up with a Live Stream:**
- The persisted queue may grow by appending incoming tuples (from a live stream) and reading tuples by multiple clients from different offsets.
- In some cases, the reader's consumption rate may exceed the production rate, leading to the read pointer reaching the end of the persisted object.
- Read I/O operations will then be satisfied from memory buffers, indicating that the reader has "caught up" with the live stream.
- The SPE may interrupt stream persistence if the reason for persistence was to avoid tuple loss due to a downstream node failure.
- In cases where persistence is explicitly requested, both read and write I/O operations can continue simultaneously, with reads being satisfied at memory speeds.

### REFERENCES

[1] IBM Ushers in Era of Stream Computing. IBM Press Report, http://www-03.ibm.com/press/us/en/pressrelease/27508.wss.  
[2] Kernel Asynchronous I/O for Linux, http://lse.sourceforge.net/io/aio.html.  
[3] Borealis Application Programmer’s Guide, Borealis Team, 2008.  
[4] D. J. Abadi, D. Carney, U. Cetintemel, M. Cherniack, C. Convey, S. Lee, M. Stonebraker, N. Tatbul, and S. Zdonik. Aurora: A New Model and Architecture for Data Stream Management. The VLDB Journal, 12(2):120–139, 2003.  
[5] A. Arasu, B. Babcock, S. Babu, J. Cieslewicz, K. Ito, R. Motwani, U. Srivastava, and J. Widom. STREAM: The Stanford Data Stream Management System. Springer, 2004.  
[6] M. Balazinska, H. Balakrishnan, S. R. Madden, and M. Stonebraker. Fault-Tolerance in the Borealis Distributed Stream Processing System. ACM Transactions on Database Systems, 33(1):1–44, 2008.  
[7] D. Borthakur. The Hadoop Distributed File System: Architecture and Design, The Apache Software Foundation, 2007.  
[8] Cetintemel, Abadi, Ahmad, Balakrishnan, Balazinska, Cherniack, Hwang, Lindner, Madden, Maskey, Rasin, Ryvkina, Stonebraker, Tatbul, Xing, and Zdonik. The Aurora and Borealis Stream Processing Engines. In Data Stream Management: Processing High-Speed Data Streams, 2006.  
[9] C. Drew. Military is Awash in Data from Drones, New York Times, January 10, 2010.  
[10] E. N. M. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. Johnson. A survey of rollback-recovery protocols in message-passing systems. ACM Comput. Surv., 34(3):375–408, 2002.  
[11] J. Gray. Why Do Computers Stop and What Can be Done About it?, Tandem Technical Report 85-7, 1985.  
[12] D. Hilley and U. Ramachandran. Persistent temporal streams. In Middleware '09: Proceedings of the 10th ACM/IFIP/USENIX International Conference on Middleware, pages 1–20, New York, NY, USA, 2009.  
[13] J.-H. Hwang, M. Balazinska, A. Rasin, U. Cetintemel, M. Stonebraker, and S. Zdonik. High-availability algorithms for distributed stream processing. In ICDE '05: Proceedings of the 21st International Conference on Data Engineering, pages 779–790, Washington, DC, USA, 2005.  
[14] J.-H. Hwang, U. Cetintemel, and S. Zdonik. Fast and highly-available stream processing over wide area networks. In ICDE '08: Proceedings of the 2008 IEEE 24th International Conference on Data Engineering, pages 804–813, Washington, DC, USA, 2008.  
[15] J.-H. Hwang, Y. Zing, U. Cetintemel, and S. Zdonik. A Cooperative Self-Configuring High-Availability Solution for Stream Processing. In ICDE '07: Proceedings of the 2007 IEEE 23rd International Conference on Data Engineering, pages 176–185, Istanbul, Turkey, 2007.  
[16] G. Jacques-Silva, B. Gedik, H. Andrade, K.-L. Wu, Language-level Checkpointing Support for Stream Processing Applications, in Proceedings of 39th IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'2009), Lisbon, Portugal, 2009.  
[17] Y. Kwon, M. Balazinska, and A. Greenberg. Fault-tolerant Stream Processing Using a Distributed, Replicated File System. Volume 1, pages 574–585. VLDB Endowment, 2008.  
[18] M. Ligon and R. Ross. Overview of the Parallel Virtual File System. In Proceedings of Extreme Linux Workshop, 1999.  
[19] F. B. Schneider. Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial. ACM Computing Surveys, 22(4):299–319, 1990.  
[20] Z. Sebepou and K. Magoutis. Scalable Storage Support for Data Stream Processing. In Proceedings of 26th IEEE Conference on Mass Storage Systems and Technologies (MSST 2010), Lake Tahoe, Nevada, May 2010.  
[21] M. A. Shah, J. M. Hellerstein, and E. Brewer. Highly Available, Fault-Tolerant, Parallel Dataflows. In SIGMOD '04: Proceedings of the 2004 ACM SIGMOD International Conference on Management of Data, New York, NY, 2004.  
[22] J. Zhou, C. Zhang, H. Tang, J. Wu, T. Yang, “Programming Support and Adaptive Checkpointing for High-Throughput Data Services with Log-Based Recovery”, in Proceedings of 40th IEEE/IFIP International Conference on Dependable Systems and Networks (DSN’2010), Chicago, IL, 2010.