### Performance Improvements

Our techniques demonstrate a 1.4× reduction in overhead for the larger CIFAR-10 network, compared to garbled circuits. In the online phase, MUSE achieves a 7.8×–8.6× improvement in latency and a 3.4×–4.6× improvement in communication efficiency over Overdrive.

### Client-Malicious Setting

In this section, we evaluate the effectiveness of our optimizations for Overdrive in a client-malicious setting. Specifically, we show that without the LTME (Linear Targeted Malleable Encryption) assumption, Overdrive exhibits the following improvements:
- **Triple Generation**: Significantly more efficient.
- **Client Input Authentication**: Slightly more efficient.
- **Server Input Authentication**: Significantly more efficient.

These enhancements are independently valuable and can be extended to support more parties.

#### Triple Generation

Figure 10 benchmarks triple generation across a variable number of threads. In summary, client-malicious Overdrive achieves an 8×–12.5× improvement in latency and a 1.7× reduction in communication over standard Overdrive.

#### Input Authentication

Table 4 presents benchmarks for input authentication for both the client and server. Our protocol for client inputs achieves a 1.6× speed improvement without the LTME assumption, but increases communication by 3.6×. For server inputs, we observe a 37.8× improvement in latency and a 4.5× improvement in communication.

### Comparison with Chameleon

Chameleon [Ria+18] proposed a slightly weaker threat model where a semi-honest third server assists in the preprocessing phase but is not needed for the online phase. If such a setup is feasible, MUSE could leverage this threat model by having the semi-honest third server assist in triple generation for the CDS (Conditional Disclosure of Secrets) protocol. This augmentation improves the latency and bandwidth of MUSE’s preprocessing phase by approximately 3×.

### TEE-Based Protocols

Generally, TEE-based protocols [Tra+19; Top+18; Han+18; App19] offer better efficiency than purely cryptographic techniques. However, this comes at the cost of a weaker threat model, requiring trust in hardware vendors and enclave implementations. Recent years have seen numerous side-channel attacks [Bra+17; Häh+17; Göt+17; Mog+17; Sch+17; Wan+17; Van+18] against popular enclaves like Intel SGX and ARM TrustZone.

### Generic Frameworks

Maliciously-secure MPC (Multi-Party Computation) frameworks exist for arithmetic circuits [Dam+12; Kel+18; Che+20], binary circuits [Kat+18], and mixed circuits [Rot+19; Esc+20; Moh+18]. Before MUSE, these were the only existing cryptographic mechanisms for two-party client-malicious secure inference. While [Che+20] is the most efficient for inference, no implementation was available at the time of writing. Comparing MUSE to [Kel+18] in Section 6.4, we estimate that [Che+20] has similar preprocessing communication, but MUSE outperforms on all other metrics.

### GC-Based Protocols

DeepSecure [Rou+18], the protocol by Ball et al. [Bal+19], and XONN [Ria+19] use circuit garbling schemes for constant-round secure inference. DeepSecure supports general neural networks, while [Bal+19] operates on discretized networks with integer weights, and XONN is optimized for binarized networks [Cou+15] with boolean weights. These quantized networks improve performance by avoiding expensive fixed-point multiplication, favoring integer or binary XNOR operations.

While neural network inference often uses quantized networks [Kri18], practical quantization is typically not below 8-bits due to accuracy degradation [Ban+18]. To mitigate this, XONN increases the number of neurons in linear layers, trading off evaluation time for accuracy. However, additional techniques are needed for more challenging datasets like ImageNet, as current best-known quantization techniques require 2-bit weights and 4-bit activations [Don+19]. Therefore, it remains important to support secure inference for general neural networks.

Any GC-based protocol can achieve malicious security through cut-and-choose techniques [Zhu+16] and malicious OT-extension [Kel+15]. Thus, these GC-based inference protocols can be transformed into malicious and fixed-subset malicious protocols. Note that DeepSecure provides server-malicious security since the client garbles the circuit. XONN uses a specialized protocol for the first layer, which would need to be evaluated within the more expensive garbled circuit for malicious/client-malicious transformations. An implementation of XONN was not available for benchmarking, but MUSE's online speed already surpasses the semi-honest versions of DeepSecure and [Bal+19].

### Conclusion

In this paper, we introduce a novel model-extraction attack against many semi-honest secure inference protocols, outperforming existing attacks by orders of magnitude. In response, we design and implement MUSE, an efficient two-party secure inference protocol resilient to malicious clients. MUSE achieves online performance close to existing semi-honest protocols and significantly outperforms alternative solutions for client-malicious secure inference. As part of MUSE’s design, we introduce a novel cryptographic protocol for conditional disclosure of secrets and improved procedures for generic MPC in the client-malicious setting. We hope MUSE is a step towards practical two-party secure inference in a strong threat model.

### Acknowledgements

We thank Marcel Keller for help with MP-SPDZ, Vinod Vaikuntanathan and David Wu for answering questions about LTME, Joey Gonzalez for insights on Binarized Neural Networks, and the anonymous reviewers and our shepherd Florian Tramér for their detailed feedback. This work was supported in part by the NSF CISE Expeditions CCF-1730628, NSF Career 1943347, and gifts/awards from the Sloan Foundation, Bakar Program, Alibaba, Amazon Web Services, Ant Group, Capital One, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk, and VMware.

### References

[App19] Apple. “iOS Security”. https://www.apple.com/business/docs/site/iOS_Security_Guide.pdf.

[Bal+19] M. Ball, B. Carmer, T. Malkin, M. Rosulek, and N. Schimanski. “Garbled Neural Networks are Practical”. ePrint Report 2019/338.

[Ban+18] R. Banner, I. Hubara, E. Hoffer, and D. Soudry. “Scalable methods for 8-bit training of neural networks”. In: NeurIPS ’18.

[Bar18] B. Barrett. “The year Alexa grew up”. https://www.wired.com/story/amazon-alexa-2018-machine-learning/.

[Bel+12] M. Bellare, V. T. Hoang, and P. Rogaway. “Foundations of garbled circuits”. In: CCS ’12.

[Bit+13] N. Bitansky, A. Chiesa, Y. Ishai, R. Ostrovsky, and O. Paneth. “Succinct Non-interactive Arguments via Linear Interactive Proofs”. In: TCC ’13.

[Bou+18] F. Bourse, M. Minelli, M. Minihold, and P. Paillier. “Fast Homomorphic Evaluation of Deep Discretized Neural Networks”. In: CRYPTO ’18.

[Boy+17] E. Boyle, G. Couteau, N. Gilboa, Y. Ishai, and M. Orrú. “Homomorphic Secret Sharing: Optimizations and Applications”. In: CCS ’17.

[Bra+17] F. Brasser, U. Müller, A. Dmitrienko, K. Kostiainen, S. Capkun, and A. Sadeghi. “Software Grand Exposure: SGX Cache Attacks Are Practical”. In: WOOT ’17.

[Car+20] N. Carlini, M. Jagielski, and I. Mironov. “Cryptanalytic Extraction of Neural Network Models”. In: CRYPTO ’20.

[Cha+19] N. Chandran, D. Gupta, A. Rastogi, R. Sharma, and S. Tripathi. “EzPC: Programmable and Efficient Secure Two-Party Computation for Machine Learning”. In: EuroS&P ’19.

[Che+20] H. Chen, M. Kim, I. P. Razenshteyn, D. Rotaru, Y. Song, and S. Wagh. “Maliciously Secure Matrix Multiplication with Applications to Private Deep Learning”. In: ASIACRYPT ’20.

[Cho+18] E. Chou, J. Beal, D. Levy, S. Yeung, A. Haque, and L. Fei-Fei. “Faster CryptoNets: Leveraging Sparsity for Real-World Encrypted Inference”. ArXiV, cs.CR 1811.09953.

[Cou+15] M. Courbariaux, Y. Bengio, and J. David. “BinaryConnect: Training Deep Neural Networks with binary weights during propagations”. In: NeurIPS ’18.

[Dam+12] I. Damgård, V. Pastro, N. P. Smart, and S. Zakarias. “Multiparty Computation from Somewhat Homomorphic Encryption”. In: CRYPTO ’12.

[Dat+19] R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. E. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz. “CHET: An optimizing compiler for fully-homomorphic neural-network inferencing”. In: PLDI ’19.

[Don+19] Z. Dong, Z. Yao, A. Gholami, M. Mahoney, and K. Keutzer. “HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision”. In: ICCV ’19.

[Esc+20] D. Escudero, S. Ghosh, M. Keller, R. Rachuri, and P. Scholl. “Improved Primitives for MPC over Mixed Arithmetic-Binary Circuits”. In: CRYPTO ’20.

[Fan+12] J. Fan and F. Vercauteren. “Somewhat Practical Fully Homomorphic Encryption”. ePrint Report 2012/144.

[Gen09a] C. Gentry. “A Fully Homomorphic Encryption Scheme”. PhD thesis. Stanford University, 2009.

[Gen09b] C. Gentry. “Fully homomorphic encryption using ideal lattices”. In: STOC ’09.

[Gil+16] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing. “CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy”. In: ICML ’16.

[Gol+89] S. Goldwasser, S. Micali, and C. Rackoff. “The Knowledge Complexity of Interactive Proof Systems”. In: SIAM J. Comput. (1989).

[Google17] Google. Google Infrastructure Security Design Overview. Tech. rep. 2017.

[Göt+17] J. Götzfried, M. Eckert, S. Schinzel, and T. Müller. “Cache Attacks on Intel SGX”. In: EUROSEC ’17.

[Häh+17] M. Hähnel, W. Cui, and M. Peinado. “High-Resolution Side Channels for Untrusted Operating Systems”. In: ATC ’2017.

[Han+18] L. Hanzlik, Y. Zhang, K. Grosse, A. Salem, M. Augustin, M. Backes, and M. Fritz. “MLCapsule: Guarded Offline Deployment of Machine Learning as a Service”. ArXiV, cs.CR 1808.00590.

[Hes+17] E. Hesamifard, H. Takabi, and M. Ghasemi. “CryptoDL: Deep Neural Networks over Encrypted Data”. ArXiV, cs.CR 1711.05189.

[Jag+20] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot. “High Accuracy and High Fidelity Extraction of Neural Networks”. In: USENIX Security ’20.

[Juu+19] M. Juuti, S. Szyller, S. Marchal, and N. Asokan. “PRADA: Protecting Against DNN Model Stealing Attacks”. In: EuroS&P ’19.

[Juv+18] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan. “GAZELLE: A Low Latency Framework for Secure Neural Network Inference”. In: USENIX Security ’18.

[Kat+18] J. Katz, S. Ranellucci, M. Rosulek, and X. Wang. “Optimizing Authenticated Garbling for Faster Secure Two-Party Computation”. In: CRYPTO ’18.

[Kel+15] M. Keller, E. Orsini, and P. Scholl. “Actively Secure OT Extension with Optimal Overhead”. In: CRYPTO ’15.

[Kel+18] M. Keller, V. Pastro, and D. Rotaru. “Overdrive: Making SPDZ Great Again”. In: EUROCRYPT ’18.

[Kel20] M. Keller. “MP-SPDZ: A Versatile Framework for Multi-Party Computation”. In: CCS ’20.

[Kes+18] M. Kesarwani, B. Mukhoty, V. Arya, and S. Mehta. “Model Extraction Warning in MLaaS Paradigm”. In: ACSAC ’18.

[Kri18] R. Krishnamoorthi. “Quantizing deep convolutional networks for efficient inference: A whitepaper”. arXiv: 1806.08342 [cs.LG].

[Kum+20] N. Kumar, M. Rathee, N. Chandran, D. Gupta, A. Rastogi, and R. Sharma. “CrypTFlow: Secure TensorFlow Inference”. In: S&P ’20.

[Lee+19] T. Lee, B. Edwards, I. Molloy, and D. Su. “Defending Against Neural Network Model Stealing Attacks Using Deceptive Perturbations”. In: SP Workshop ’19.

[Lin+15] Y. Lindel and P. Benny. “An Efficient Protocol for Secure Two-Party Computation in the Presence of Malicious Adversaries”. In: J. Cryptol. (2015).

[Liu+17a] J. Liu, M. Juuti, Y. Lu, and N. Asokan. “Oblivious Neural Network Predictions via MiniONN Transformations”. In: CCS ’17.

[Mil+19] 

[Mis+20] 

[Lou+19] 

[Moh+17] 

[Mog+17] 

[Liu+17b] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Al-