### The Impact of Misaligned Incentives on Corporate Performance

Much has been written about how corporate performance can suffer when executives' incentives are misaligned with the company's best interests. For instance, managers often purchase suboptimal or even defective products from well-known suppliers to minimize their risk of being fired if something goes wrong. While this practice is not typically condemned as fraud, it is often praised as "due diligence" by corporate lawyers.

In the latter part of the twentieth century, many businesses attempted to address this issue by extending stock options to a broader range of employees. However, these incentives have not proven sufficient to ensure prudent practices among security managers. This raises an interesting research question: Is this due to security managers having less information about potential threats, making it difficult for them to make rational decisions about protection and insurance, or is it simply a case of adverse selection among security managers?

### Evaluation Systems as a Solution

This problem has long been recognized, and the typical proposed solution is the implementation of evaluation systems. These can be private, such as equipment tests conducted by insurance industry laboratories for their member companies, or public, like the Orange Book and the Common Criteria.

The Orange Book had the advantage that evaluations were conducted by the government, the party relying on the results. In contrast, the European equivalent, ITSEC, introduced a problematic change: the vendor, rather than the government, paid for the evaluation. This practice was carried over into the Common Criteria.

This shift in payment structure created a perverse incentive. Vendors were motivated to choose the evaluation contractor who would provide the easiest approval, whether by asking fewer questions, charging less, taking less time, or all of the above. To mitigate this, schemes were established to approve contractors as CLEFs (Commercial Licensed Evaluation Facilities). The threat of license revocation was intended to counteract the commercial pressures to cut corners.

However, in the half-dozen or so disputed cases I have been involved in, the Common Criteria approach has not been satisfactory. Some of these cases are documented in my book, *Security Engineering* [3]. The failure modes often involve straightforward pandering to customers' wishes, even when these are in conflict with the users' interests. The lack of sanctions for misconduct—such as a process to revoke accreditation for incompetence or dishonesty—likely contributes to this problem.

### Additional Perverse Incentives

There is another significant perverse incentive. From a user's perspective, an evaluation may actually diminish the value of a product. For example, if an unevaluated product is used to generate digital signatures, and a forged signature is discovered, a court might be more willing to order the release of full documentation to expert witnesses. A Common Criteria certificate could make a court less likely to order such disclosure, thereby prejudicing the user's rights.

A cynical view might suggest that vendors of products designed to transfer liability (such as digital signature smartcards), satisfy due diligence requirements (such as firewalls), or impress naive users (such as PC access control products) are particularly enthusiastic about the Common Criteria.

### Economic Considerations

An economist is unlikely to place blind faith in a Common Criteria evaluation. Fortunately, the perverse incentives discussed should limit the adoption of the Criteria to sectors where an official certification, regardless of its relevance, accuracy, or potential for misleading, offers a competitive advantage.

### Conclusions

Much has been written about the failure of information security mechanisms to protect end users from privacy violations and fraud. However, this misses the point. The primary drivers behind security system design are often not altruistic but rather the desire to secure monopolies, charge different prices for the same service, and offload risk. These motivations are often rational.

In an ideal world, removing perverse economic incentives to create insecure systems would depoliticize most issues. Security engineering would then be a matter of rational risk management rather than risk dumping. However, since information security involves power and money—raising barriers to trade, segmenting markets, and differentiating products—evaluators should not restrict themselves to technical tools like cryptanalysis and information flow. They must also apply economic tools such as the analysis of asymmetric information and moral hazard. As quickly as regulators remove one perverse incentive, businesses and governments are likely to create two more.

In other words, the management of information security is a much deeper and more political problem than is usually realized. Solutions are likely to be subtle and partial, while many simplistic technical approaches are bound to fail. It is time for engineers, economists, lawyers, and policymakers to collaborate and develop common approaches.

### Acknowledgements

I received valuable comments on early drafts of some of this material from Avi Rubin, Hal Finney, Jack Lang, Andrew Odlyzko, and Hal Varian.

### Postscript

The tragic events of September 11th occurred just before this manuscript was finalized. These events will take time to digest, and rather than rewriting the paper, it seemed better to add this short postscript. I believe that the economic arguments presented here will be relevant to protecting both physical and digital assets. It may take years for courts to resolve liability issues, but there remains a strong public interest in ensuring that operational responsibility for protection does not become divorced from the liability for its failure.

The arguments in Section 4 are brought into sharper relief. In a world where attackers can strike anywhere, while defenders must protect everywhere, attackers have a significant economic advantage. This suggests that local defensive measures are insufficient; there is a critical role for global defense, which may include deterrence and retribution.

The suppression of piracy is a useful and sobering example. Although major governments began to agree in the late seventeenth century that using pirates as state instruments was unacceptable, no single solution existed. It took numerous treaties, naval actions, and the overthrow of several rogue governments over more than a century to pacify the world's oceans. This project became intertwined with other campaigns, including the abolition of slavery and the spread of colonialism. Liberals faced tough moral dilemmas, such as whether it was acceptable to conquer and colonize a territory to suppress piracy and slavery. In the end, economic factors were politically decisive; piracy cost businesses too much money. History may not repeat itself, but it would be unwise to ignore it.

### References

[1] GA Akerlof, “The Market for ’Lemons’: Quality Uncertainty and Market Mechanism,” Quarterly Journal of Economics v 84 (August 1970) pp 488–500

[2] J Anderson, ‘Computer Security Technology Planning Study’, ESD-TR-73-51, US Air Force Electronic Systems Division (1973) http://csrc.nist.gov/publications/history/index.html

[3] RJ Anderson, ‘Security Engineering – A Guide to Building Dependable Distributed Systems’, Wiley (2001) ISBN 0-471-38922-6

[4] RJ Anderson, “Why Cryptosystems Fail” in Communications of the ACM vol 37 no 11 (November 1994) pp 32–40

[5] JA Bloom, IJ Cox, T Kalker, JPMG Linnartz, ML Miller, CBS Traw, “Copy Protection for DVD Video”, in Proceedings of the IEEE v 87 no 7 (July 1999) pp 1267–1276

[6] RM Brady, RJ Anderson, RC Ball, ‘Murphy’s law, the fitness of evolving species, and the limits of software reliability’, Cambridge University Computer Laboratory Technical Report no. 476 (1999); at http://www.cl.cam.ac.uk/~rja14

[7] CERT, Results of the Distributed-Systems Intruder Tools Workshop, Software Engineering Institute, Carnegie Mellon University, http://www.cert.org/reports/dsit_workshop-final.html, December 7, 1999

[8] W Curtis, H Krasner, N Iscoe, “A Field Study of the Software Design Process for Large Systems”, in Communications of the ACM v 31 no 11 (Nov 88) pp 1268–1287

[9] D Davis, “Compliance Defects in Public-Key Cryptography”, in Sixth Usenix Security Symposium Proceedings (July 1996) pp 171–178

[10] “De l’influence des péages sur l’utilité des voies de communication”, 1849, Annales des Ponts et Chausses.

[11] European Union, ‘Network and Information Security: Proposal for a European Policy Approach’, COM(2001)298 final, 6/6/2001

[12] A Kerckhoffs, “La Cryptographie Militaire”, in Journal des Sciences Militaires, 9 Jan 1883, pp 5–38; http://www.fabien-petitcolas.net/kerckhoffs/

[13] DP Kormann, AD Rubin, “Risks of the Passport Single Signon Protocol” in Computer Networks (July 2000); at http://avirubin.com/vita.html

[14] SJ Liebowitz, SE Margolis, “Network Externalities (Effects)”, in The New Palgrave’s Dictionary of Economics and the Law, MacMillan, 1998; see http://wwwpub.utdallas.edu/~liebowit/netpage.html

[15] WF Lloyd, ‘Two Lectures on the Checks to Population’, Oxford University Press (1833)

[16] AM Odlyzko, “Smart and stupid networks: Why the Internet is like Microsoft”, ACM netWorker, Dec 1998, pp 38–46, at http://www.acm.org/networker/issue/9805/ssnet.html

[17] C Shapiro, H Varian, ‘Information Rules’, Harvard Business School Press (1998), ISBN 0-87584-863-X

[18] J Spolsky, “Does Issuing Passports Make Microsoft a Country?” http://joel.editthispage.com/stories/storyReader$139

[19] H Varian, ‘Intermediate Microeconomics – A Modern Approach’, Fifth edition, WW Norton and Company, New York, 1999; ISBN 0-393-97930-0

[20] H Varian, “Managing Online Security Risks”, Economic Science Column, The New York Times, June 1, 2000, http://www.nytimes.com/library/financial/columns/060100econ-scene.html

---

**Authorized licensed use limited to: Tsinghua University. Downloaded on March 25, 2021, at 07:04:10 UTC from IEEE Xplore. Restrictions apply.**