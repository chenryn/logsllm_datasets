### 7.2.2 Modify/Analyze JavaScript to Prevent Escapes

In this defense, the archive would both statically and dynamically analyze JavaScript code it captures in order to identify scripts that might cause archive-escapes. The archive would then rewrite or wrap these scripts, replacing the original script with a version that performs the same operations but avoids generating archive-escapes. For example, the defense might intercept calls to browser APIs that generate HTTP requests and modify URL arguments to ensure they do not point outside the archive.

This solution is complex and may require significant engineering effort. Additionally, executing the defense on each archived resource at the time of archiving could be computationally expensive. However, if successful, this defense could allow the Wayback Machine’s URL rewriting to be more pervasive, even applying to client-side dynamically generated URLs, which are the primary source of vulnerabilities identified in the archive today.

### 7.2.3 Serve Distinct Archived Domains from Distinct Subdomains

Archives can defend against Same-Origin Escapes by serving content from distinct subdomains, each corresponding to the live domain from which the content was originally published. For instance, an archive might serve captures of `example.com/script.js` from the subdomain `http://example.com.web.archive.org/` instead of from `http://web.archive.org`. Since the Same-Origin Policy considers subdomains as distinct domains, this approach would provide the same isolation in the archival context as in the live context, preserving the trust model across both live and archival executions of the page. We recommend that archives consider implementing this defense.

### 7.3 Defenses Deployed by Clients

Finally, we discuss defenses that can be deployed inside the client’s browser. Individual clients can unilaterally deploy these defenses, making them highly valuable. For example, experts in legal cases might use these defenses to provide more trustworthy testimony. While these defenses require each client to separately install them, they apply to all snapshots in the archive.

#### 7.3.1 Browser Extensions to Block/Highlight Escapes and Anachronisms

This defense interposes on and blocks Archive-Escape and Anachronistic requests made for subresources while browsing the archive. It prevents Archive-Escape Abuse by blocking all HTTP requests from a snapshot that leave the archive. Since the distinction between archive-escapes and archival requests is clear (distinguishable by the destination domain of the request), this defense should be highly effective against Archive-Escape Abuse.

The defense also protects against Anachronism Injection by blocking anachronistic requests—those requests for archival resources with timestamps far from the timestamp of the enclosing page. This involves an inherent tradeoff, where the defense or its user must define how anachronistic a resource must be to be blocked. In the most extreme case, only resources with a timestamp exactly equal to the snapshot’s timestamp can be loaded, leading to complete blocking of the vulnerability but also preventing many legitimate resources from being loaded, resulting in a less complete picture of the past web.

This defense can also (or instead) visibly highlight, log, or summarize archive-escapes and anachronistic requests and the visible page elements that correspond to them. Such a feature can help a human expert better judge the accuracy of a snapshot. ArchiveWatcher, described in more detail below (Section 7.4), is an example of this type of defense.

### 7.4 ArchiveWatcher: An End-User Defense

We prototyped ArchiveWatcher, a client-deployed defense consisting of a browser extension that detects and blocks archive-escape request vulnerabilities. ArchiveWatcher is implemented as a lightweight Chrome Extension that interposes on requests made for resources while browsing snapshots on `https://web.archive.org/web`. It is written in 6000 lines of JavaScript, CSS, and HTML, and its source code can be found on GitHub by following the links at `https://rewritinghistory.cs.washington.edu`.

As described in Section 7.3.1, ArchiveWatcher blocks requests for archive-escapes. It can display to the user a summary of the requests it has detected and blocked on the current snapshot and across the current browsing session. ArchiveWatcher suggests directions for defenses that could aid technical experts in assessing the veracity of archival snapshots.

### 8. Conclusion

In this paper, we have explored the space of attacks that can rewrite history—i.e., attacks that can manipulate how clients see archived websites, focusing on the Wayback Machine. Although it is known that the archive contains accidental inaccuracies, to our knowledge, we are the first to explore how an attacker might introduce intentional errors. We identified and explored several vulnerabilities in how the Wayback Machine archives and serves snapshots of websites, and we developed four attacks that leverage these vulnerabilities. We demonstrated proof-of-concept attacks on the Wayback Machine, showing that we were able to manipulate client views of snapshots without compromising the archive’s or any other servers. We then quantified the prevalence of these types of vulnerabilities, finding that over 70% of the sites we investigated are vulnerable to this type of manipulation by some attacker.

The web is important to our modern society, making web archives a critical source of socially important information, from journalism to legal proceedings. This work highlights the importance for website publishers, archive designers, and end users to take steps to prevent or detect intentional manipulation.

### Acknowledgements

We thank Lucy Simko, Anna Kornfeld Simpson, and Eric Zeng for their insightful comments and feedback on the paper; Emily McReynolds for feedback, advice, and consultation on legal concepts referenced in the paper; and Gaites Swanson for his help discovering, parsing, and interpreting the legal URLs we studied. We also thank Mark Graham and his colleagues at Internet Archive for their thoughtful and rapid response to our disclosure of this work.

This work was supported in part by NSF Grant IIS-1302709, the Short-Dooley Professorship, and the UW Tech Policy Lab.

### References

[1] 2012. Laboratory Corp. of America v. United States, 108 Fed.Cl. 549 (2012).

[2] 2012. People v. Franzen, 210 Cal.App.4th 1193 (2012).

[3] 2013. Ex Parte Serguei N. Mamedrzaev. 2013 WL 1558372.

[4] 2014. Tharpe v. Lawidjaja, 8 F.Supp.3d 743 (2014).

[5] 2016. The European Patent Convention, Article 54: Novelty. https://www.epo.org/law-practice/legal-texts/html/epc/2016/e/ar54.html. Accessed: 2017-05-17.

[6] 2017. Robots.txt meant for search engines don’t work well for web archives. https://blog.archive.org/2017/04/17/robots-txt-meant-for-search-engines-dont-work-well-for-web-archives/. Accessed: 2017-05-19.

[7] 2017. Summary of s3.amazonaws.com. https://web.archive.org/web/*/http://s3.amazonaws.com/alexa-static/top-1m.csv.zip. Accessed: 2017-05-05.

[8] 2017. Welcome to LexisNexis - Choose Your Path. https://www.lexisnexis.com/en-us/gateway.page. Accessed: 2017-05-19.

[9] 2017. WestLaw.com. westlaw.com. Accessed: 2017-05-19.

[10] Ada Lerner, Anna Kornfeld Simpson, Tadayoshi Kohno, Franziska Roesner. 2016. Internet Jones and the Raiders of the Lost Trackers: An Archaeological Study of Web Tracking from 1996 to 2016. 25th USENIX Security Symposium (August 2016).

[11] Scott G. Ainsworth, Ahmed AlSum, Hany SalahEldeen, Michele C. Weigle, and Michael L. Nelson. 2012. How Much of the Web Is Archived? arxiv.org (2012), 1–10. arXiv:1212.6177 http://arxiv.org/abs/1212.6177

[12] Scott G Ainsworth and Michael L Nelson. 2004. Only One Out of Five Archived Web Pages Existed as Presented. ACM HT’15 (2004). http://public.lanl.gov/herbertv/papers/Papers/2015/ht15-ainsworth-submission.pdf

[13] Scott G Ainsworth, Michael L Nelson, and Herbert Van de Sompel. 2015. Only One Out of Five Archived Web Pages Existed as Presented. In Proceedings of the 26th ACM Conference on Hypertext & Social Media. ACM, 257–266.

[14] Internet Archive. 2017. Heritrix is the Internet Archive’s open-source, extensible, web-scale, archival-quality web crawler project. https://github.com/internetarchive/heritrix3. Accessed: 2017-08-16.

[15] Internet Archive. 2017. IA’s public Wayback Machine (moved from SourceForge). https://github.com/internetarchive/wayback. Accessed: 2017-08-16.

[16] Justin F. Brunelle. 2012. 2012-10-10: Zombies in the Archives. http://ws-dl.blogspot.com/2012/10/2012-10-10-zombies-in-archives.html. Accessed: 2017-05-13.

[17] Justin F Brunelle, Mat Kelly, Hany Salaheldeen, Michele C Weigle, and Michael L Nelson. 2015. Not All Mementos Are Created Equal: Measuring The Impact Of Missing Resources Categories and Subject Descriptors. International Journal on Digital Libraries (2015).

[18] International Internet Preservation Consortium. 2017. The OpenWayback Development. http://www.netpreserve.org/openwayback. https://github.com/iipc/openwayback. Accessed: 2017-08-16.

[19] Shawn E. Douglas. [n.d.]. Citing from a Digital Archive like the Internet Archive: A Cheat Sheet. http://www.writediteach.com/images/Citing%20from%20a%20Digital%20Archive%20like%20the%20Internet%20Archive.pdf. Accessed: 2017-05-08.

[20] Peter Eckersley. 2010. How unique is your web browser? Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6205 LNCS (2010), 1–18. https://doi.org/10.1007/978-3-642-14527-8_1

[21] Deborah R Eltgrowth. 2009. Best evidence and the Wayback Machine: toward a workable authentication standard for archived Internet evidence. Fordham L. Rev. 78 (2009), 181.

[22] Matthew Fagan. 2007. Can You Do a Wayback on That—The Legal Community’s Use of Cached Web Pages in and out of Trial. BUJ Sci. & Tech. L. 13 (2007), 46.

[23] David Fifield and Serge Egelman. 2015. Fingerprinting web users through font metrics. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8975 (2015), 107–124. https://doi.org/10.1007/978-3-662-47854-7_7

[24] Karén Gazaryan. 2013. Authenticity of Archived Websites: The Need to Lower the Evidentiary Hurdle Is Imminent. Rutgers Computer & Tech. LJ 39 (2013), 216.

[25] Stephanie Hackett, Bambang Parmanto, and Xiaoming Zeng. 2003. Accessibility of Internet websites through time. ACM SIGACCESS Accessibility and Computing (2003), 32. https://doi.org/10.1145/1029014.1028638

[26] Internet Archive. 2017. Internet Archive: Digital Library of Free Books, Movies, Music & Wayback Machine. https://archive.org/. Accessed: 2017-05-12.

[27] Internet Archive. 2017. Internet Archive Frequently Asked Questions. https://archive.org/about/faqs.php#23. Accessed: 2017-05-04.

[28] Internet Archive. 2017. Wayback Machine. https://web.archive.org. Accessed: 2017-05-11.

[29] Internet Memory Foundation. 2017. Internet Memory Foundation. http://internetmemory.org/en/. Accessed: 2017-08-16.

[30] Mat Kelly, Justin F. Brunelle, Michele C. Weigle, and Michael L. Nelson. 2013. On the change in archivability of websites over time. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8092 LNCS (2013), 35–47. https://doi.org/10.1007/978-3-642-40501-3_5 arXiv:1307.8067

[31] Mat Kelly, Justin F. Brunelle, Michele C. Weigle, and Michael L. Nelson. 2013. On the Change in Archivability of Websites Over Time. CoRR abs/1307.8067 (2013). http://arxiv.org/abs/1307.8067

[32] Library of Congress. 2017. Archived Web Site | Library of Congress. https://www.loc.gov/websites/. Accessed: 2017-05-12.

[33] Keaton Mowery and Hovav Shacham. 2012. Pixel Perfect: Fingerprinting Canvas in HTML5. Web 2.0 Security & Privacy 20 (W2SP) (2012), 1–12. https://cseweb.ucsd.edu/

[34] Jamie Murphy, Noor Hazarina Hashim, and Peter O’Connor. 2007. Take Me Back: Validating the Wayback Machine. Journal of Computer-Mediated Communication 13, 1 (2007), 60–75. https://doi.org/10.1111/j.1083-6101.2007.00386.x

[35] Nick Nikiforakis, Luca Invernizzi, Alexandros Kapravelos, Steven Van Acker, Wouter Joosen, Christopher Kruegel, Frank Piessens, and Giovanni Vigna. 2012. You are what you include: large-scale evaluation of remote JavaScript inclusions. In Proceedings of the 2012 ACM conference on Computer and communications security. ACM, 736–747.

[36] Nick Nikiforakis, Alexandros Kapravelos, Wouter Joosen, Christopher Kruegel, Frank Piessens, and Giovanni Vigna. 2013. Cookieless monster: Exploring the ecosystem of web-based device fingerprinting. Proceedings - IEEE Symposium on Security and Privacy (2013), 541–555. https://doi.org/10.1109/SP.2013.43

[37] US Department of Homeland Security. 2016. Homeland Security. http://webarchive.loc.gov/all/20160205185026/https://www.dhs.gov/. Accessed: 2017-08-16.

[38] Mary Emily Ohara. 2017. Trump Administration Removes LGBTQ Content From Federal Websites. https://web.archive.org/web/20170324052626/http://www.nbcnews.com/feature/nbc-out/trump-administration-removes-lgbtq-content-federal-websites-n711416. Accessed: 2017-03-27.

[39] OpenGovData Russia Archive. 2017. Архивация государства (консервированное государство) | Открытые данные в России. http://opengovdata.ru/projects/govarchive/. Accessed: 2017-08-16.

[40] James L Quarles III and Richard A Crudo. 2014. Using the Wayback Machine in Patent Litigation. Landslide Magazine 6, 3 (Jan/Feb 2014).

[41] Achintya Rao. 2017. Using the Internet Archive to cite websites. https://medium.com/@RaoOfPhysics/using-the-internet-archive-to-cite-websites-89bd3f2ce0fd. Accessed: 2017-05-08.

[42] Franziska Roesner, Tadayoshi Kohno, and David Wetherall. 2012. Detecting and defending against third-party tracking on the web. Proc. of the USENIX Conference on Networked Systems Design and Implementation (NSDI) (2012), 12.

[43] Ryan North. 2016. Dinosaur Comics - February 3rd, 2016 - awesome fun times! http://webarchive.loc.gov/all/20160203203159/http://www.qwantz.com/index.php. Accessed: 2017-08-16.

[44] Myriam Ben Saad and Stéphane Gançarski. 2011. Improving the quality of web archives through the importance of changes. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6860 LNCS, PART 1 (2011), 394–409. https://doi.org/10.1007/978-3-642-23088-2_29

[45] Kyle Soska and Nicolas Christin. 2014. Automatically Detecting Vulnerable Websites Before They Turn Malicious. 23rd USENIX Security Symposium (USENIX Security 14) (2014), 625–640. https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/soska

[46] Stanford Libraries. 2017. Web Archiving | Stanford Libraries. http://library.stanford.edu/projects/web-archiving. Accessed: 2017-08-16.

[47] Wikipedia. 2017. List of Web archiving initiatives. https://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives. Accessed: 2017-08-16.