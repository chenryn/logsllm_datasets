### Optimized Text

**Message Handling and Priority Levels:**
Typically, at any given time, there are at least four partially-received messages. Therefore, all scheduled priority levels are utilized. The highest scheduled level (P7) receives the most packets. The other levels (P6-P0) are used if the highest-priority sender is nonresponsive or if the number of incoming messages drops below four. At 80% network load, senders are frequently nonresponsive, with more than half of the scheduled traffic arriving on P0–P2.

**Additional Information:**
Due to page length restrictions, several sections have been omitted. A complete version of the paper, including a more comprehensive description of the simulation environment and parameters, is available online [22].

**Network Load and Bandwidth:**
The figure below shows the distribution of network bandwidth across different priority levels (P7-P0) under varying network loads (50%, 80%, and 90%).

```
0   10  20  30
P7  P6  P5  P4  P3  P2  P1  P0
% Network Bandwidth
Network Load: 50%  80%  90%
```

**Future Work:**
Further exploration of the problem and potential solutions, particularly at the NIC or TOR level, is left for future work.

**Homa's Incast Handling:**
Homa assumes that the most severe forms of incast are predictable because they are self-inflicted by outgoing RPCs. Homa effectively handles these situations. Unpredictable incasts can also occur, but Homa assumes they are unlikely to be of high degree. Homa can manage unpredictable incasts of several hundred messages with typical switch buffer capacities. Larger incasts will cause packet loss and degraded performance.

**Configuration and Future Implications:**
The Homa configuration and measurements in this paper are based on 10 Gbps link speeds. As link speeds increase, RTTbytes will also increase proportionally, impacting the protocol in several ways. A larger fraction of traffic will be sent unscheduled, making Homa’s use of multiple priority levels for unscheduled packets more important. With faster networks, workloads will behave more like W1 and W2 in our measurements, rather than W3-W5. As RTTbytes increases, each message can potentially consume more space in switch buffers, reducing the degree of unpredictable incast that Homa can support.

**Related Work:**
In recent years, numerous proposals for new transport protocols have emerged, driven by new datacenter applications and the well-documented shortcomings of TCP. However, none of these proposals combine the right set of features to produce low latency for short messages under load.

**Shortcomings of Recent Proposals:**
Most recent proposals do not take advantage of in-network priority queues. This includes rate-control techniques such as DCTCP [2] and HULL [3], which reduce queue occupancy, and D3 [32] and D2TCP [31], which incorporate deadline-awareness. PDQ [17] adjusts flow rates to implement preemption, but its rate calculation is too slow for scheduling short messages. Without the use of priorities, none of these systems can achieve the rapid preemption needed by short messages.

**Systems Using In-Network Priorities:**
A few systems have used in-network priorities, but they do not implement SRPT. §5.2 showed that the PIAS priority mechanism [6] performs worse than SRPT for most message sizes and workloads. QJUMP [14] requires priorities to be specified manually on a per-application basis. Karuna [7] uses priorities to separate deadline and non-deadline flows, requiring a global calculation for the non-deadline flows. Without receiver-driven SRPT, none of these systems can achieve low latency for short messages.

**pFabric and Similar Systems:**
pFabric [4] implements SRPT by assuming fine-grained priority queues in network switches, producing near-optimal latencies. However, it depends on features not available in existing switches. pHost [13] and NDP [15] are the systems most similar to Homa, using receiver-driven scheduling and priorities. Both use only two priority levels with static assignment, resulting in poor latency for short messages. Neither system uses overcommitment, limiting their ability to operate at high network load. NDP uses fair-share scheduling rather than SRPT, leading to high tail latencies. NDP includes an incast control mechanism, where network switches drop all but the first few bytes of incoming packets during congestion. Homa’s incast control mechanism achieves a similar effect using a software approach: instead of truncating packets in-flight (which wastes network bandwidth), senders are instructed by the protocol to limit how much data they send.

**Connection-Oriented Streaming:**
Almost all the systems mentioned, including DCTCP, pFabric, PIAS, and NDP, use a connection-oriented streaming approach. This results in either high tail latency due to head-of-line blocking at senders or an explosion of connections, which is impractical for large-scale datacenter applications.

**Centralized Scheduling:**
An alternative is to schedule all messages or packets centrally, as in Fastpass [25]. However, communication with the central scheduler adds too much latency for short messages. Additionally, scaling a system like Fastpass to a large cluster is challenging, especially for workloads with many short messages.

**Conclusion:**
The combination of tiny messages and low-latency networks creates challenges and opportunities not addressed by previous transport protocols. Homa meets this need with a new transport architecture that combines several unique features:
- It implements discrete messages for remote procedure calls, not byte streams.
- It uses in-network priority queues with a hybrid allocation mechanism that approximates SRPT.
- It manages most of the protocol from the receiver, not the sender.
- It overcommits receiver downlinks to maximize throughput at high network loads.
- It is connectionless and has no explicit acknowledgments.

These features combine to produce nearly optimal latency for short messages across a variety of workloads. Even under high loads, tail latencies are within a small factor of the hardware limit. The remaining delays are almost entirely due to the absence of link-level packet preemption in current networks, with little room for improvement in the protocol itself. Finally, Homa can be implemented with no changes to networking hardware. We believe that Homa provides an attractive platform for building low-latency datacenter applications.

**Acknowledgments:**
This work was supported by C-FAR (one of six centers of STARnet, a Semiconductor Research Corporation program, sponsored by MARCO and DARPA) and by the industrial affiliates of the Stanford Platform Laboratory. Amy Ousterhout, Henry Qin, Jacqueline Speiser, and 14 anonymous reviewers provided helpful comments on drafts of this paper. We also thank our SIGCOMM shepherd, Brighten Godfrey.

**References:**
[1] M. Alizadeh, T. Edsall, S. Dharmapurikar, R. Vaidyanathan, K. Chu, A. Fingerhut, V. T. Lam, F. Matus, R. Pan, N. Yadav, and G. Varghese. CONGA: Distributed Congestion-aware Load Balancing for Datacenters. In Proceedings of the ACM SIGCOMM 2014 Conference, SIGCOMM '14, pages 503–514, New York, NY, USA, 2014. ACM.
[2] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center TCP (DCTCP). In Proceedings of the ACM SIGCOMM 2010 Conference, SIGCOMM '10, pages 63–74, New York, NY, USA, 2010. ACM.
[3] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat, and M. Yasuda. Less is More: Trading a Little Bandwidth for Ultra-low Latency in the Data Center. In Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI'12, pages 19–19, Berkeley, CA, USA, 2012. USENIX Association.
[4] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. pFabric: Minimal Near-optimal Datacenter Transport. In Proceedings of the ACM SIGCOMM 2013 Conference, SIGCOMM '13, pages 435–446, New York, NY, USA, 2013. ACM.
[5] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Workload Analysis of a Large-scale Key-value Store. In Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS '12, pages 53–64, New York, NY, USA, 2012. ACM.
[6] W. Bai, L. Chen, K. Chen, D. Han, C. Tian, and H. Wang. Information-agnostic Flow Scheduling for Commodity Data Centers. In Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation, NSDI'15, pages 455–468, Berkeley, CA, USA, 2015. USENIX Association.
[7] L. Chen, K. Chen, W. Bai, and M. Alizadeh. Scheduling Mix-flows in Commodity Datacenters with Karuna. In Proceedings of the ACM SIGCOMM 2016 Conference, SIGCOMM '16, pages 174–187, New York, NY, USA, 2016. ACM.
[8] I. Cho, K. Jang, and D. Han. Credit-Scheduled Delay-Bounded Congestion Control for Datacenters. In Proceedings of the ACM SIGCOMM 2017 Conference, SIGCOMM '17, pages 239–252, New York, NY, USA, 2017. ACM.
[9] Data Plane Development Kit. http://dpdk.org/.
[10] A. Dixit, P. Prakash, Y. C. Hu, and R. R. Kompella. On the Impact of Packet Spraying in Data Center Networks. In Proceedings of IEEE Infocom, 2013.
[11] A. Dragojević, D. Narayanan, M. Castro, and O. Hodson. FaRM: Fast Remote Memory. In 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14), pages 401–414, Seattle, WA, Apr. 2014. USENIX Association.
[12] B. Felderman. Personal communication, February 2018. Google.
[13] P. X. Gao, A. Narayan, G. Kumar, R. Agarwal, S. Ratnasamy, and S. Shenker. pHost: Distributed Near-optimal Datacenter Transport over Commodity Network Fabric. In Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies, CoNEXT '15, pages 1:1–1:12, New York, NY, USA, 2015. ACM.
[14] M. P. Grosvenor, M. Schwarzkopf, I. Gog, R. N. M. Watson, A. W. Moore, S. Hand, and J. Crowcroft. Queues Don't Matter When You Can JUMP Them! In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15), pages 1–14, Oakland, CA, 2015. USENIX Association.
[15] M. Handley, C. Raiciu, A. Agache, A. Voinescu, A. W. Moore, G. Antichik, and M. Mojcik. Re-architecting Datacenter Networks and Stacks for Low Latency and High Performance. In Proceedings of the ACM SIGCOMM 2017 Conference, SIGCOMM '17, pages 29–42, New York, NY, USA, 2017. ACM.
[16] K. He, E. Rozner, K. Agarwal, W. Felter, J. Carter, and A. Akella. Presto: Edge-based Load Balancing for Fast Datacenter Networks. In Proceedings of the ACM SIGCOMM 2015 Conference, SIGCOMM '15, pages 465–478, New York, NY, USA, 2015. ACM.
[17] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly with Preemptive Scheduling. In Proceedings of the ACM SIGCOMM 2012 Conference, SIGCOMM '12, pages 127–138, New York, NY, USA, 2012. ACM.
[18] E. Jeong, S. Wood, M. Jamshed, H. Jeong, S. Ihm, D. Han, and K. Park. mTCP: a Highly Scalable User-level TCP Stack for Multicore Systems. In 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14), pages 489–502, Seattle, WA, 2014. USENIX Association.
[19] C. Lee, S. J. Park, A. Kejriwal, S. Matsushita, and J. Ousterhout. Implementing Linearizability at Large Scale and Low Latency. In Proceedings of the 25th Symposium on Operating Systems Principles, SOSP '15, pages 71–86, New York, NY, USA, 2015. ACM.
[20] memcached: a Distributed Memory Object Caching System. http://www.memcached.org/, Jan. 2011.
[21] R. Mittal, V. T. Lam, N. Dukkipati, E. Blem, H. Wassel, M. Ghobadi, A. Vahdat, Y. Wang, D. Wetherall, and D. Zats. TIMELY: RTT-based Congestion Control for the Datacenter. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, SIGCOMM '15, pages 537–550, New York, NY, USA, 2015. ACM.
[22] B. Montazeri, Y. Li, M. Alizadeh, and J. K. Ousterhout. Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities (Complete Version). CoRR, http://arxiv.org/abs/1803.09615, 2018.
[23] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee, H. C. Li, R. McElroy, M. Paleczny, D. Peek, P. Saab, D. Stafford, T. Tung, and V. Venkataramani. Scaling Memcache at Facebook. In 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13), pages 385–398, Lombard, IL, 2013. USENIX.
[24] J. Ousterhout, A. Gopalan, A. Gupta, A. Kejriwal, C. Lee, B. Montazeri, D. Ongaro, S. J. Park, H. Qin, M. Rosenblum, et al. The RAMCloud Storage System. ACM Transactions on Computer Systems (TOCS), 33(3):7, 2015.
[25] J. Perry, A. Ousterhout, H. Balakrishnan, D. Shah, and H. Fugal. Fastpass: A Centralized “Zero-queue” Datacenter Network. In Proceedings of the ACM SIGCOMM 2014 Conference, SIGCOMM '14, pages 307–318, New York, NY, USA, 2014. ACM.
[26] Redis, Mar. 2015. http://redis.io.
[27] A. Roy, H. Zeng, J. Bagga, G. Porter, and A. C. Snoeren. Inside the Social Network’s (Datacenter) Network. In Proceedings of the ACM SIGCOMM 2015 Conference, SIGCOMM '15, pages 123–137, New York, NY, USA, 2015. ACM.
[28] T. Shanley. Infiniband Network Architecture. Addison-Wesley Professional, 2003.
[29] R. Sivaram. Some Measured Google Flow Sizes (2008). Google internal memo, available on request.
[30] BCM56960 Series: High-Density 25/100 Gigabit Ethernet StrataXGS Tomahawk Ethernet Switch Series. https://www.broadcom.com/products/ethernet-connectivity/switching/strataxgs/bcm56960-series.
[31] B. Vamanan, J. Hasan, and T. Vijaykumar. Deadline-aware Datacenter TCP (D2TCP). In Proceedings of the ACM SIGCOMM 2012 Conference, SIGCOMM '12, pages 115–126, New York, NY, USA, 2012. ACM.
[32] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron. Better Never Than Late: Meeting Deadlines in Datacenter Networks. In Proceedings of the ACM SIGCOMM 2011 Conference, SIGCOMM '11, pages 50–61, New York, NY, USA, 2011. ACM.
[33] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. Katz. Detail: Reducing the flow completion time tail in datacenter networks. In Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, SIGCOMM '12, pages 139–150, New York, NY, USA, 2012. ACM.
[34] Y. Zhu, H. Eran, D. Firestone, C. Guo, M. Lipshteyn, Y. Liron, J. Padhye, S. Raindel, M. H. Yahia, and M. Zhang. Congestion Control for Large-Scale RDMA Deployments. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, SIGCOMM '15, pages 523–536, New York, NY, USA, 2015. ACM.