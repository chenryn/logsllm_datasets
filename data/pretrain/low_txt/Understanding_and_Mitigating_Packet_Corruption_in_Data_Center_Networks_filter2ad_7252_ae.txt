### CorrOpt: Taking the Links Off

Disabling links can cause packet reordering and temporarily lower network performance. Flowlet re-routing [1] can mitigate this issue.

### 8. Future Extensions

We discuss several directions for extending CorrOpt:

#### Accounting for the Impact of Repair
While disabling a link has a limited local effect, our deployment experience shows that repairing it can sometimes cause collateral damage. For example, if one link in a breakout cable experiences packet corruption, repairing it may require turning off three additional, healthy links. A future extension of CorrOpt would account for such collateral impacts when deciding which links to disable for repair, based on a more granular view of the topology and its dependencies.

#### Removing Traffic Instead of Disabling Links
Currently, CorrOpt completely disables corrupting links (when capacity constraints allow). However, our deployment experience suggests that removing traffic from the link (e.g., by increasing its routing cost) is a better strategy. This approach allows us to run test traffic to confirm the success of the repair without affecting actual traffic. Additionally, monitoring data on optical power levels will continue to flow, which otherwise stops when the link is disabled.

### 9. Related Work

Our work builds upon a rich body of research on understanding, diagnosing, and mitigating faults in large, complex networks. While it is not possible to cover everything here, we place our work in the context of the most relevant, recent studies.

#### Packet Corruption in DCNs
Some studies have acknowledged packet corruption as a significant problem. For instance, NetPilot [34] observes that corruption degrades application performance and develops a method to locate corrupting links without access to switch counters. RAIL [38] studies the optical layer of DCNs, finding that while RxPower is generally high, instances of low RxPower can cause packet corruption. Our work delves deeper into packet corruption, including its characteristics and root causes, which extend beyond low RxPower.

#### Faults in DCNs
Many prior studies have focused on other types of faults in DCNs [6, 8, 19]. For example, Gill et al. [18] study equipment failures in DCNs, characterizing different elements' downtime and failure numbers, combined with an impact estimation and redundancy analysis. Our focus is on a different type of fault—packet corruption—and its mitigation.

#### Fault Mitigation
Most work on fault mitigation focuses on congestion or fail-stop faults, using techniques such as load balancing and fast rerouting [1, 7, 25, 33, 35]. However, these techniques are less relevant for corrupting links (e.g., reducing traffic on the link will not alleviate corruption). RAIL [38] studies a setting where corruption is the norm (due to non-conventional use of optical transceivers) and places only loss-tolerant traffic on corrupting links. We view corruption as an anomaly and mitigate it by disabling corrupting links so they can be repaired. zUpdate [24] and NetPilot [34] depend on knowledge of future traffic demand to further reduce congestion loss when handling corrupting links or network updates. These techniques complement CorrOpt’s link disabling methods. CorrOpt can operate in data center settings where future traffic demand is not available.

#### Root Cause Diagnosis
Using optical-layer characteristics to diagnose network faults was previously proposed by Kompella et al. [23]. Ghobadi et al. [16] use optical-layer statistics to help predict failures in backbone networks. Our work uses an optical-layer monitor to help determine the root cause of packet corruption in DCNs.

### 10. Conclusion

Our analysis of packet corruption across many DCNs revealed that the extent of corruption losses is significant. Compared to congestion, corruption affects fewer links but imposes heavier loss rates, and the corruption rate of a link is temporally stable and uncorrelated with its utilization. CorrOpt, our system for mitigating corruption, reduces corruption losses by three to six orders of magnitude by intelligently selecting which corrupting links to disable while meeting configured capacity constraints. It also generates repair recommendations guided by common symptoms of different root causes. This recommendation engine is deployed in all data centers of a large cloud provider, where it improved the accuracy of repairs by 60%.

### Acknowledgments

We thank David Bragg, Jamie Gaudette, and Shikhar Suri for helping us design the recommendation engine. We also thank Hui Ma and Shikhar Suri for their assistance with the deployment of our monitoring system and the recommendation engine. We are grateful to our shepherd Minlan Yu and the anonymous reviewers for their valuable feedback on the paper. Klaus-Tycho Förster is supported by the Danish VILLUM FONDEN project "Reliable Computer Networks (ReNet)."

### References

[1] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA: Distributed Congestion-aware Load Balancing for Datacenters. In SIGCOMM.

[2] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data Center TCP (DCTCP). In SIGCOMM.

[3] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat, and Masato Yasuda. 2012. Less is More: Trading a Little Bandwidth for Ultra-low Latency in the Data Center. In NSDI.

[4] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Datacenter Transport. In SIGCOMM.

[13] FiberStore. 2017. Proportional Rate Reduction for TCP. In IMC. Fiber Optic Inspection Tutorial. http://www.fs.com/fiber-optic-inspection-tutorial-aid-460.html. (2017).

[5] Behnaz Arzani, Selim Ciraci, Boon Thau Loo, Assaf Schuster, and Geoff Outhred. 2016. Taking the Blame Game out of Data Centers Operations with NetPoirot. In SIGCOMM.

[6] Peter Bailis and Kyle Kingsbury. 2014. The Network is Reliable. Commun. ACM.

[7] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang. 2011. MicroTE: Fine Grained Traffic Engineering for Data Centers. In CoNEXT.

[8] Kashif Bilal, Marc Manzano, Samee U. Khan, Eusebi Calle, Keqin Li, and Albert Y. Zomaya. 2013. On the Characterization of the Structural Robustness of Data Center Networks. IEEE Trans. Cloud Computing (2013).

[9] Peter Bodík, Ishai Menache, Mosharaf Chowdhury, Pradeepkumar Mani, David A. Maltz, and Ion Stoica. 2012. Surviving Failures in Bandwidth-constrained Datacenters. In SIGCOMM.

[10] Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh, and Van Jacobson. 2016. BBR: Congestion-Based Congestion Control. ACM Queue (2016).

[11] J. D. Case, M. Fedor, M. L. Schoffstall, and J. Davin. 1990. Simple Network Management Protocol (SNMP). (1990).

[12] Nandita Dukkipati, Matt Mathis, Yuchung Cheng, and Monia Ghobadi. 2011. [14] Fiber for Learning. 2017. Fiber Hygiene. http://fiberforlearning.com/welcome/2010/09/20/connector-cleaning/. (2017).

[15] M. R. Garey and David S. Johnson. 1979. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman.

[16] Monia Ghobadi and Ratul Mahajan. 2016. Optical Layer Failures in a Large Backbone. In IMC.

[17] Monia Ghobadi, Ratul Mahajan, Amar Phanishayee, Nikhil Devanur, Janardhan Kulkarni, Gireeja Ranade, Pierre-Alexandre Blanche, Houman Rastegarfar, Madeleine Glick, and Daniel Kilper. 2016. ProjecToR: Agile Reconfigurable Data Center Interconnect. In SIGCOMM.

[18] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan. 2011. Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications. In SIGCOMM.

[19] Ramesh Govindan, Ina Minei, Mahesh Kallahalla, Bikash Koley, and Amin Vahdat. 2016. Evolve or Die: High-Availability Design Principles Drawn from Google's Network Infrastructure. In SIGCOMM.

[20] Peter Hoose. 2011. Monitoring and Troubleshooting, One Engineer’s rant. https://www.nanog.org/meetings/nanog53/presentations/Monday/Hoose.pdf. (2011).

[21] JDSU. 2017. P5000i Fiber Microscope. http://www.viavisolutions.com/en-us/products/p5000i-fiber-microscope. (2017).

[22] Edward John Forrest Jr. 2014. How to Precision Clean All Fiber Optic Connections: A Step By Step Guide. CreateSpace Independent Publishing Platform.

[23] Ramana Rao Kompella, Albert Greenberg, Jennifer Rexford, Alex C. Snoeren, and Jennifer Yates. 2005. Cross-layer Visibility as a Service. In HotNets.

[24] Hongqiang Harry Liu, Xin Wu, Ming Zhang, Lihua Yuan, Roger Wattenhofer, and David A. Maltz. 2013. zUpdate: Updating Data Center Networks with Zero Loss. In SIGCOMM.

[25] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas Anderson. 2013. F10: A Fault-Tolerant Engineered Network. In NSDI.

[26] David Maltz. 2016. Keeping Cloud-Scale Networks Healthy. https://video.mtgsf.com/video/4f277939-73f5-4ce8-aba1-3da70ec19345. (2016).

[27] Jitendra Padhye, Victor Firoiu, Don Towsley, and Jim Kurose. 1998. Modeling TCP Throughput: A Simple Model and Its Empirical Validation. In SIGCOMM.

[28] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans Fugal. 2014. Fastpass: A Centralized "Zero-queue" Datacenter Network. In SIGCOMM.

[29] Peng Sun, Ratul Mahajan, Jennifer Rexford, Lihua Yuan, Ming Zhang, and Ahsan Arefin. 2014. A Network-state Management Service. In SIGCOMM.

[30] USConec. 2017. Single Fiber Cleaning Tools. http://www.usconec.com/products/cleaning_tools/ibc_brand_cleaners_for_single_fiber_connections.htm. (2017).

[31] Balajee Vamanan, Jahangir Hasan, and T.N. Vijaykumar. 2012. Deadline-aware Datacenter TCP (D2TCP). In SIGCOMM.

[32] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011. Better Never Than Late: Meeting Deadlines in Datacenter Networks. In SIGCOMM.

[33] Damon Wischik, Costin Raiciu, Adam Greenhalgh, and Mark Handley. 2011. Design, Implementation and Evaluation of Congestion Control for Multipath TCP. In NSDI.

[34] Xin Wu, Daniel Turner, George Chen, Dave Maltz, Xiaowei Yang, Lihua Yuan, and Ming Zhang. 2012. NetPilot: Automating Datacenter Network Failure Mitigation. In SIGCOMM.

[35] Kyriakos Zarifis, Rui Miao, Matt Calder, Ethan Katz-Bassett, Minlan Yu, and Jitendra Padhye. 2014. DIBS: Just-in-time Congestion Mitigation for Data Centers. In EuroSys.

[36] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn, Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and Ming Zhang. 2015. Congestion Control for Large-Scale RDMA Deployments. In SIGCOMM.

[37] Yibo Zhu, Nanxi Kang, Jiaxin Cao, Albert Greenberg, Guohan Lu, Ratul Mahajan, Dave Maltz, Lihua Yuan, Ming Zhang, Ben Y. Zhao, and Haitao Zheng. 2015. Packet-Level Telemetry in Large Datacenter Networks. In SIGCOMM.

[38] Danyang Zhuo, Monia Ghobadi, Ratul Mahajan, Amar Phanishayee, Xuan Kelvin Zou, Hang Guan, Arvind Krishnamurthy, and Thomas Anderson. 2017. RAIL: A Case for Redundant Arrays of Inexpensive Links in Data Center Networks. In NSDI.

### A. Proof of Theorem 1

To prove Theorem 5.1, we first prove the following lemma:

**Lemma A.1.** Let \( N = (V, E) \) be a degraded Fat-Tree, where some links are turned off. Let \( L \subseteq E \) be the set of enabled links with corruption. Finding a set \( L' \subseteq L \) whose removal minimizes the impact of packet corruption such that all ToR switch pairs are still connected to the spine via valley-free routing after the removal of \( L' \) is NP-hard.

**Proof.** Our NP-hardness reduction is via the NP-complete problem 3-SAT, in the variant with exactly three literals per clause. Let \( I \) be an instance of 3-SAT [15], with \( k \) clauses \( C_1, \ldots, C_k \) and variables \( x_1, \ldots, x_r \), \( k \geq r \). We create an instance \( I' \) of our problem as follows: Consider a 4k-Fat-Tree, consisting of the three layers ToR, Agg, and the spine, where the Agg switches have 2k links downward and upward, respectively. Pick one pod \( P \) with 2k ToR switches, \( C_1, \ldots, C_k \), corresponding to the clauses in \( I \), and \( H_1, \ldots, H_k \), as helper switches, and lastly, 2r Agg switches \( X_1, \neg X_1, \ldots, X_r, \neg X_r \), corresponding to the possible literals in \( I \), and \( 2k - 2r \geq 0 \) further Agg switches \( A_1, \ldots, A_{2k-2r} \). Let only the following three sets of links not be turned off in this pod \( P \): 1) For each \( C_i \), the links pointing to the Agg switches representing the corresponding literals contained in the clause in \( I \), 2) for each \( H_j \) from \( H_1, \ldots, H_r \), one link each to \( X_j, \neg X_j \), 3) for each \( H_{r+1}, \ldots, H_k \), one link each to \( X_1, \neg X_1 \). For the connection of the Agg switches in \( P \) to the neighboring spine switches, let only the following links \( L \) not be turned off, with \( |L| = 2r \): From each Agg switch \( X_1, \neg X_1, \ldots, X_r, \neg X_r \) in \( P \), one link to a neighboring spine switch. We set all links in \( L \) to have the same corruption properties greater than zero. The construction is illustrated in Figure 21.

To guarantee valley-free connection of all ToR switches in \( P \) to all other ToR switches in the other pods via the spine, each ToR switch \( C_i \) and \( H_j \) needs to have a connection to the spine, with the last part of each of those paths being a link from \( L \). To maximize the set of links \( L' \subseteq L \) that can be turned off, consider the following: First, for each pair of Agg switches \( X_j, \neg X_j \), at least one needs to remain connected to the spine, or the ToR switch \( H_j \) would be disconnected (or for the case of \( j = 1 \), even more switches), meaning \( |L'| \geq r \). Second, to connect each switch \( C_i \) to the spine, at least one of its Agg switches (representing the literals of the clause) must be connected to the spine. As such, a solution to a satisfiable 3-SAT instance \( I \) tells us how to pick which of the links from each \( X_i, \neg X_i \) pair should remain connected to the spine, and vice versa, a solution with \( |L'| = r \) from \( I' \) shows how to satisfy \( I \). On the other hand, should \( I \) not be satisfiable, then no solution with \( |L'| \leq r \) can exist for \( I' \), and vice versa as well. We note that the size of the instance \( I' \) is polynomial in \( k \), i.e., we showed NP-hardness.

□

Since we assumed the same \( f_l \) on every link, \( I \) can be chosen arbitrarily, as long as \( I(f_l) > 0 \). We can now prove Theorem 5.1:

**Theorem 5.1.** Lemma A.1 assumed the network to be already degraded, i.e., some links \( L \) are turned off. Note that a Fat-Tree is a special case of a Clos topology. We can extend the NP-hardness to the setting of Theorem 5.1 as well, by the following change in construction: Assume the errors on every link in \( L \) are so high in comparison to the errors on the links from \( L \), that for every link \( l \in L \) holds: Disabling \( l \) is more efficient regarding the impact of corruption than turning off all links in \( L \). Lastly, to show that the underlying decision problem is in NP, observe that checking the capacity constraints and total impact of packet corruption of a given solution can be performed in polynomial time.

□

The above proof constructions can also be used as follows:

- **Optimizing for Link Removal:** Another objective instead of total packet corruption could be the number of further links that can be removed. However, as all links in the proof of Lemma A.1 had the same error properties, NP-completeness still holds.
- **From ToR-Spine Connectivity to ToR-ToR Connectivity:** In the above problem formulations, we wanted to maintain the ToR to spine connectivity of all ToR switches. However, we just considered a single pod \( P \) in each Fat-Tree construction, with the ToR switches in all other pods retaining all their connectivity to the spine. As such, the above problems are also NP-complete for ToR-ToR connectivity, with the same proof constructions.