### Heuristics and Rigorous Risk Analysis in Software Components

Heuristics encompass a checklist of questions, suggestions, or guidewords to assess the risk of software components. Examples include: "Is the component unstable or new?" and "Does the component implement a complex business rule?" In contrast, rigorous risk analysis typically employs statistical models to estimate the likelihood of component failure [25, 39] and hazard analysis to evaluate the consequences of such failures [22]. By combining the likelihood and consequence of failures, it is possible to rank the risk of each individual component in a system.

### Software Reliability Modeling and Risk Assessment

Many studies have aimed to mitigate problems associated with software faults and estimate their risks, with a particular emphasis on software testing, reliability modeling, and risk analysis [25, 30, 17, 20]. The risk assessment equation commonly used in the literature reflects the probability of faulty behavior in a given software component and its impact (or cost). However, this equation is interpreted differently based on the specific approach used for risk assessment in each study.

For instance, the equation presented in [34] uses object-oriented CK metrics [6] to estimate how error-prone a component is. Higher metric values indicate a higher likelihood of errors. The risk is evaluated by considering the probability that an undesirable event \( E_i \) occurs (\( p(E_i) \)) and the cost to the system if this event occurs (\( c(E_i) \)), as shown in Equation (7):

\[
\text{Risk} = \sum (p(E_i) \times c(E_i))
\]

In the context of software systems, an undesirable event is a component failure.

Sherer [38] presents another concept of risk, which is a function of fault activation probability within a predefined time, the quality of the development process, and the operational profile. The work in [1] expands Rosenberg’s equation [34] to consider the component exposure from both the customer's and vendor's perspectives.

### Complexity Metrics and Fault Prediction

Software complexity metrics have been widely used to estimate the probability of component faults, which are directly related to the probability of component failure in typical risk equations (e.g., Equation 7). For example, [5] experimentally validates object-oriented design metrics as quality indicators to predict fault-prone classes and concludes that several of these metrics are useful for predicting class fault-proneness during the early phases of the life cycle.

The likelihood of component failure is directly related to the complexity of the component [25]. Many studies show a clear link between component complexity and error proneness [21, 20, 12]. However, Fenton [13] demonstrates that this trend does not hold in some cases. Some explanations for this apparent contradiction are provided in [28], and the use of static code metrics is recommended as probabilistic predictors rather than categorical indicators. Menzies et al. [28] also emphasize the importance of finding good attribute sets for each problem and analyzing large data sets to generalize the results.

### Estimating the Impact of Component Failures

Regarding the estimation of the impact of component failures (the term "cost" in the risk equation), the Failure Mode and Effect Analysis (FMEA) technique [22] is widely used to estimate the cost of component failures (referred to as severity analysis in FMEA). This technique is particularly prevalent in the development of software for highly regulated application areas such as avionics, space, and nuclear applications.

Fault injection techniques are also widely used to experimentally evaluate the cost (i.e., the impact) of failures in computer systems [3, 18]. The impact of failures (equivalent to cost in the risk equation) is generally described in fault injection works as failure modes, which express the system response to the injection of each fault (e.g., crash, hang, erroneous output, etc.).

Although fault injection techniques are popular, their use in estimating risk, especially in software, has not been extensively addressed in the literature. Evaluating the impact of software component failures would require the injection of software faults, and techniques to inject such faults have been largely absent from fault injection research. Most fault injection works actually emulate hardware transient faults using the SWIFI (Software Implemented Fault Injection) approach, which should not be confused with the injection of software faults (i.e., program defects or bugs).

### Injecting Representative Software Faults

The problem of injecting representative software faults was first addressed in [8], in the context of IBM’s Orthogonal Defect Classification (ODC) project [7]. The proposed method requires field data about real software faults in the target system or class of target systems. This requirement significantly reduces the usability of the method, as such information is seldom available and impossible to obtain for new software. Furthermore, typical fault injection tools are not able to inject a substantial part of the type of faults proposed in [8].

To the best of our knowledge, the first practical approach to inject realistic software faults without the need for target source code was proposed in [9]. The technique, called Generic Software Fault Injection Technique (G-SWFIT), is supported by findings from an extensive field study [10]. In this work, we use G-SWFIT to estimate the cost of the activation of residual faults.

### Conclusion

This paper presents a novel approach to evaluate the risk of using a given software component through software fault injection. The risk is assessed using both software metrics and fault injection. The injected faults are designed to represent component residual faults realistically and provide a measure of the impact of component failures. Various software metrics, such as cyclomatic complexity, number of parameters, number of returns, maximum nesting depth, and Halstead’s program length and vocabulary size, are considered. These metrics are used to estimate the component's fault proneness. Logistic regression analysis is employed to fit the expression of fault probability with these metrics. The fault injection experiments include the notion of fault activation probability to model the fact that some faults may not be activated or simply tolerated.

Our approach enhances risk evaluation by providing a repeatable method and reducing the dependence on evaluators, which characterizes classical risk evaluation approaches. The proposed risk assessment approach is illustrated using a case study of a satellite data handling real-time application written in C. The risk of using two well-known off-the-shelf components (RTEMS and RTLinux operating systems) is analyzed. Results show that RTEMS represents a considerably lower risk than RTLinux for this application.

### Acknowledgments

The authors thank CAPES/GRICES and FAPESP for partially supporting this work. We also thank MSquared Technologies for providing the full version of the RSM tool and Testwell Oy Ltd for the CMT++ and CMTjava tools.

### References

[1] Amland, S. "Risk-based Testing: Risk analysis fundamentals and metrics for software testing including a financial application case study". The Journal of Systems and Software, 53, pp. 287-295, 2000.

[2] Anderson, T.; Feng, M.; Riddle, S.; Romanovsky, A. "Protective Wrapper Development: A Case Study". Lecture Notes in Computer Science, vol 2589, pp. 1-14, Springer Verlag, London, UK, 2003.

[3] Arlat, J. et al. "Fault Injection and Dependability Evaluation of Fault Tolerant Systems". IEEE Transaction on Computers, vol. 42, n. 8, pp. 919-923, 1993.

[4] Bach, J. "Heuristic Risk-Based Testing". in Software Testing and Engineering Magazine, 1999.

[5] Basili, V.; Briand, L.; Melo, W. "Measuring the Impact of Reuse on Quality and Productivity in Object-Oriented Systems". Tech. Report, University of Maryland, Dep. Of Computer Science, Jan. 1995, CS-TR-3395.

[6] Chidamber, R.; Kemerer, F. "A Metric Suite for Object-Oriented Design". In IEEE Trans. of Software Engineering, 20 (6), 1994.

[7] Chillarege, R., "Orthogonal Defect Classification", Ch. 9 of "Handbook of Software Reliability Engineering", M. Lyu Ed., IEEE Computer Society, McGraw-Hill, 1995.

[8] Christmansson, J; Chillarege, R. "Generation of an Error Set that Emulates Software Faults-Based on Fields Data". Proc. of 26th Int. Symposium on Fault-Tolerant Computing, pp. 304-13, Sendai, Japan, 1996.

[9] Durães, J.; Madeira, H. "Emulation of Software Faults by Educated Mutations at Machine-Code Level". in Proc. of The 13th Int. Symposium on Software Reliability Engineering – ISSRE’02, Annapolis, USA, 2002.

[10] Durães, J.; Madeira, H. "Definition of Software Fault Emulation Operators: A Field Data Study". In Proc. of The Int. Conf. on Dependable Systems and Networks, pp. 105-114, San Francisco, USA, 2003 (W. Carter Award).

[11] Durães, J.; Madeira, H. "Software Faults: A field data Study and a practical approach". in Trans. Of Software Engineering, Nov. 2006.

[12] El Emam, K.; Benlarbi, S.; Goel, N.; Rai, S. "Comparing Case-based Reasoning Classifiers for Predicting High-Risk Software Components". Journal of Systems and Software, vol. 55, n. 3, pp. 301-320, 2001.

[13] Fenton, N.; Ohlsson, N. "Software Metrics and Risk". Proc. of The 2nd European Software Measurement Conference (FESMA'99), 1999.

[14] Halstead, M. "Elements of Software Science". Elsevier Science Inc., New York, NY, USA, 1977.

[15] Herrmann, D. "Software Safety and Reliability: Techniques, Approaches, and Standards of Key Industrial Sectors". Wiley-IEEE Computer Society Press, 1st edition, January, 2000.

[16] Hosmer, D.; Lemeshow, S. "Applied Logistic Regression". John Wiley & Sons, 1989.

[17] Hudepohl et al. "EMERALD: A Case Study in Enhancing Software Reliability". in Proc. of IEEE Eight Int. Symp. on Software Reliability Engineering - ISSRE98, pp. 85-91, 1998.

[18] Iyer, R. "Experimental Evaluation". Special Issue FTCS-25 Silver Jubilee, 25th IEEE Symposium on Fault Tolerant Computing, pp. 115-132, 1995.

[19] Karolak, D. "Software Engineering Risk Management". Wiley-IEEE Computer Society Press, 1st edition, November, 1995.

[20] Khoshgoftaar et al. "Process Measures for Predicting Software Quality". in Proc. of High Assurance System Engineering Workshop – HASE’97, 1997.

[21] Kitchenham, B.; Pfleeger, S.; Fenton, N. "Towards a framework for software measurement validation". IEEE Trans. on Software Eng., 21(12), pp. 929-944, 1995.

[22] Leveson, N. "Safeware, System Safety and Computers". Addison-Wesley Publishing Company, 1995.

[23] Linux kernel. www.kernel.org. Accessed on Feb/06, 2006.

[24] Lyu, M.; Chen, J.; Avizienis, A. "Experience in Metrics and Measurements for N-Version Programming". Int. Journal of Reliability, Quality and Safety Engineering, vol. 1, n. 1, pp. 41-62, 1994.

[25] Lyu, M. "Handbook of Software Reliability Engineering". IEEE Computer Society Press, McGraw-Hill, 1996.

[26] Madeira, H.; Vieira, M.; Costa, D. "On the Emulation of Software Faults by Software Fault Injection". Proc. of The Int. Conf. on Dependable Systems and Networks, NY, USA, 2000.

[27] McManus, J. "Risk Management in Software Projects". Butterworth-Heinemann, November, 2003.

[28] Menzies, T.; Greenwald, J.; Frank, A. "Data Mining Static Code Attributes to Learn Defect Predictors". IEEE Trans. on Software Eng., Vol. 32, n. 11, pp. 1-12, 2007.

[29] Moraes, R., Durães, J., Martins, E., Madeira, H. "A field data study on the use of software metrics to define representative fault distribution". Proc. of Workshop on Empirical Evaluation of Dependability and Security – WEEDS in conjunction with DSN06, 2006.

[30] Musa, J. "Software Reliability Engineering", McGraw-Hill, 1996.

[31] Munson, J.; Khoshgoftaar, T. "Software Metrics for in: Handbook of Software Reliability Assessment". Reliability Engineering, Comp. Society Press, Michael R. Lyu editor, ch. 12, 1995.

[32] Popstojanova, G. K.; Trivedi, S. K. "Architecture Based Approach to Reliability Assessment of Software Systems". Perf. Evaluation, vol. 45, nos. 2-3, pp. 179-204, Jun/01, 2001.

[33] Rome Laboratory (RL). "Methodology for Software Reliability Prediction and Assessment". Technical Report RL-TR-92-52, vol. 1 and 2, 1992.

[34] Rosenberg, L.; Stapko, R.; Gallo, A. "Risk-based Object Oriented Testing". In Proc. of 13th Int. Software / Internet Quality Week-QW, San Francisco, USA, 2000.

[35] Resource Standard Metrics, Version 6.1, http://msquaredtechnologies.com/m2rsm/rsm.htm. Last access 2005.

[36] Real-Time Operating System for Multiprocessor Systems. www.rtems.com, accessed in Feb/06, 2006.

[37] Shaw, M.; Clements, P. "A Field Guide to Boxology: Preliminary Classification of Architectural Styles for Software Systems". Proc. 21st Int. Computer Software and Applications Conference, pp. 6-13, 1997.

[38] Sherer, S. "A Cost-Effective Approach to Testing". In IEEE Software, 8 (2), pp. 34-40, 1991.

[39] Singpurwalla, N. "Statistical Methods in Software Engineering: Reliability and Risk". Springer; 1st ed, 1999.

[40] Tang, M.; Kao, M.; Chen, M. "An Empirical Study on Object-Oriented Metrics". In: Proc. of the Sixth International Software Metrics Symp. pp. 242-249, 1999.

[41] Testwell Oy Ltd. http://www.testwell.fi. Accessed on March/06, 2006.

[42] Vieira, M.; Madeira, H. "Recovery and Performance Balance of a COTS DBMS in the Presence of Operator Faults", Int. Conf. on Dependable Systems and Networks, pp. 615-624, Washington D.C., USA, 2002.

[43] Voas, J.; Charron, F.; McGraw, G.; Miller, K.; Friedman, M. "Predicting how Badly ‘Good’ Software can Behave". IEEE Software, 1997.

[44] Weyuker, E. "Testing Component-Based Software: A Cautionary Tale". IEEE Software, 1998.

[45] Yacoub, S.; Ammar, H. "A Methodology for Architectural-Level Reliability Risk Analysis". IEEE Trans. Software Eng, vol. 28, no. 6, pp. 529-547, Jun/02, 2002.