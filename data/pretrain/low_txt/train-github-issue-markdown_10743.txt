### Issue Description

When running Celery with MongoDB as both the broker and the results backend, we are encountering the following errors regularly:

```
[2012-09-07 21:15:25,643: ERROR/MainProcess] Consumer: Connection to broker lost. Trying to re-establish the connection...
Traceback (most recent call last):
  File "/mnt/nimble/eggs/celery-3.0.9-py2.7.egg/celery/worker/consumer.py", line 369, in start
    self.consume_messages()
  File "/mnt/nimble/eggs/celery-3.0.9-py2.7.egg/celery/worker/consumer.py", line 842, in consume_messages
    self.connection.drain_events(timeout=10.0)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/connection.py", line 197, in drain_events
    return self.transport.drain_events(self.connection, **kwargs)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/__init__.py", line 743, in drain_events
    item, channel = get(timeout=timeout)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/scheduling.py", line 42, in get
    return self.fun(resource, **kwargs), resource
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/__init__.py", line 763, in _drain_channel
    return channel.drain_events(timeout=timeout)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/__init__.py", line 560, in drain_events
    return self._poll(self.cycle, timeout=timeout)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/__init__.py", line 290, in _poll
    return cycle.get()
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/scheduling.py", line 42, in get
    return self.fun(resource, **kwargs), resource
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/mongodb.py", line 52, in _get
    msg = self._queue_cursors[queue].next()
  File "/mnt/nimble/eggs/pymongo-2.1.1-py2.7-linux-x86_64.egg/pymongo/cursor.py", line 703, in next
    if len(self.__data) or self._refresh():
  File "/mnt/nimble/eggs/pymongo-2.1.1-py2.7-linux-x86_64.egg/pymongo/cursor.py", line 679, in _refresh
    limit, self.__id))
  File "/mnt/nimble/eggs/pymongo-2.1.1-py2.7-linux-x86_64.egg/pymongo/cursor.py", line 628, in __send_message
    self.__tz_aware)
  File "/mnt/nimble/eggs/pymongo-2.1.1-py2.7-linux-x86_64.egg/pymongo/helpers.py", line 95, in _unpack_response
    cursor_id)
OperationFailure: cursor id '1323433598770136089' not valid at server
[2012-09-07 21:15:25,703: ERROR/MainProcess] Unrecoverable error: KeyError('ip-10-248-69-149.celery.pidbox',)
Traceback (most recent call last):
  File "/mnt/nimble/eggs/celery-3.0.9-py2.7.egg/celery/worker/__init__.py", line 353, in start
    component.start()
  File "/mnt/nimble/eggs/celery-3.0.9-py2.7.egg/celery/worker/consumer.py", line 368, in start
    self.reset_connection()
  File "/mnt/nimble/eggs/celery-3.0.9-py2.7.egg/celery/worker/consumer.py", line 712, in reset_connection
    self.reset_pidbox_node()
  File "/mnt/nimble/eggs/celery-3.0.9-py2.7.egg/celery/worker/consumer.py", line 656, in reset_pidbox_node
    callback=self.on_control)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/pidbox.py", line 68, in listen
    callbacks=[callback or self.handle_message])
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/pidbox.py", line 59, in Consumer
    **options)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/messaging.py", line 285, in __init__
    self.revive(self.channel)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/messaging.py", line 297, in revive
    self.declare()
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/messaging.py", line 307, in declare
    queue.declare()
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/entity.py", line 387, in declare
    self.queue_declare(nowait, passive=False)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/entity.py", line 409, in queue_declare
    nowait=nowait)
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/virtual/__init__.py", line 401, in queue_declare
    return queue, self._size(queue), 0
  File "/mnt/nimble/eggs/kombu-2.4.5-py2.7.egg/kombu/transport/mongodb.py", line 73, in _size
    return (self._queue_cursors[queue].count() -
KeyError: 'ip-10-248-69-149.celery.pidbox'
```

After these errors, Celery exits with an error code of 0.

### Environment Details

- **Celery Version:** 3.0.9 (Chiastic Slide)
- **Kombu Version:** 2.4.5
- **Python Version:** 2.7.3
- **Billiard Version:** 2.7.3.12
- **PyMongo Version:** 2.1.1
- **Platform:** Linux (64-bit, ELF)
- **Loader:** `celery.loaders.default.Loader`
- **Transport and Results Backend:** MongoDB

### Additional Information

- The MongoDB database is a sharded cluster.
- Celery connects to the MongoDB via a `mongos` instance running on `localhost`.
- The error appears to be related to a cursor that has become invalid, similar to a timeouted cursor. However, timeouts should not apply to tailable cursors.

### Summary

We are experiencing regular connection issues with Celery when using MongoDB as the broker and results backend. The errors indicate that the cursor used by Celery becomes invalid, leading to an unrecoverable state and causing Celery to exit. This issue occurs in a sharded MongoDB cluster, and the connection is made through a `mongos` instance on `localhost`.

If you have any further insights or suggestions for resolving this issue, please let us know.