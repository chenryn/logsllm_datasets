### Based on Historical Data or Estimates of Traffic at a Sample Point

The decreasing line in the graph illustrates the total upload capacity for Uid; as expected, this capacity diminishes as Uid increases. The increasing line, on the other hand, represents the percentage of clients at the sample point who can upload at least one tuple. Encouragingly, for most values of Uid, both metrics remain high, typically exceeding 90%. This indicates that our Uid-upload protocol is practical and does not significantly compromise the accuracy of the aggregate data.

### Optimal Uid Value

From Figure 6, it appears that setting Uid to `avg + std` is the best choice. This decision is based on the following rationale: even if the turnout at a sample point exceeds the average by one standard deviation, the majority of clients should still be able to upload. Conversely, Uid should not be too much larger than the average when the turnout is small. We tested larger quotas (4 and 5) and found that the formula `Uid = avg + (quota - 1)/2 * std` still accurately represents the intersection of the lines in Figure 6.

### Accuracy Error Analysis

Next, we evaluate the accuracy error introduced by our protocol for the chosen Uid value. For count aggregates, there is no error. For summation-type aggregates (see §7), the increasing line in Figure 6 provides the answer. If all summed values come from the same distribution, the error is approximately 5%, which is considered reasonable, especially since summation-type averages are less common.

For average-type aggregation (§7), the accuracy is very high. Even if only a subset of clients can upload in certain cases, this subset is randomly selected from all clients passing through the sample point due to the random timing in the sync interval (see §5 and Figure 3). Assuming each client's value is drawn from the same distribution, the expected average of tuples equals the real expected average. Additionally, due to the law of large numbers, for popular aggregates (which have larger Uid values), the observed average will closely approximate the real average. For aggregates with as few as 100 uploads per aggregate (as in CarTel), the errors are already small: Figure 7 shows that the error in average delay is at most 5%, and the error in average speed is 3%.

While CarTel traffic patterns may not be representative, similar analysis of historical data can help deduce appropriate Uid values for other cases. In fact, our protocol is likely to be even more effective in practice because the CarTel network is relatively small (approximately 30 drivers). In a system with more participants, both metrics in Figure 6 will be higher, many Uid values will be suitable, and the errors in averages will be smaller due to the law of large numbers.

### Related Work

#### Aggregate Statistics for Location Privacy

Systems such as [16], CliqueCloak [14, 19, 24, 20], and Triplines [18] address the same problem as PrivStats. These systems use trusted parties to aggregate time-consecutive driver uploads for the same location, avoid uploading in sensitive areas, distort spatial and temporal locational services, or "cloak" the data using subsampling techniques.

One issue with these solutions is that they do not provide rigorous privacy guarantees and are vulnerable to side information (SI). As discussed in §3, even if a server sees anonymized tuples, considerable private information can leak due to SI. For example, in [20, 18], a trusted party aggregates the upload of each driver on a piece of road with the uploads of other k-1 drivers close in time and location to provide k-anonymity. While an individual tuple has k-anonymity, the driver may not be k-anonymous over their entire path among other drivers and their paths. SI can further violate k-anonymity.

Another challenge is the reliance on fully trusted intermediaries between drivers and servers. For instance, trusted parties in [16, 14, 18, 19] receive tuples with client identifiers in plaintext and are supposed to remove them. If these parties are compromised or collude, the paths of the drivers are also compromised. In our work, if the SM (Secure Module) is compromised, the server will still only see anonymized tuples (see §8), and the use of the SM achieves stronger security.

Accountability has been either ignored (allowing drivers to bias the aggregate result significantly) [16, 14, 24, 20] or handled by having tuples contain driver identifiers [18, 19]. For example, in [18, 19], users upload tuples containing their identities to a trusted party that checks if clients have uploaded too much, while another trusted party performs aggregation. If these trusted parties collude, driver paths with identities leak.

#### Other Related Work

We now discuss systems for protecting location privacy that target different but related problems.

**VPriv [32] and PrETP [2]**: These systems compute functions over a particular driver's path rather than statistics over multiple clients' paths. For instance, they can compute the total toll that driver Alice has to pay based on her driving activity in a month. They associate the result of a function with a client’s ID, whereas PrivStats keeps clients anonymous and computes statistics over all clients. VPriv and PrETP use zero-knowledge concepts, but their cryptographic proofs and protocols differ from ours and are inapplicable to our setting. These protocols require all clients to be online and perform work simultaneously, which is impractical for aggregates with hundreds of clients. Moreover, they do not support aggregates over data from different clients. VPriv and PrETP are also vulnerable to side information, and they do not provide our type of accountability.

**SEPIA [6]**: This system uses special-purpose secure-multiparty protocols to compute aggregate statistics over network traffic. However, SEPIA relies on trusted "input peers" to provide network data and a cluster of "privacy peers" to perform computations. The protocol depends on a majority of the privacy peers not colluding. In contrast, our system does not rely on such trusted peers, and SEPIA does not provide accountability checks against abusive clients. SEPIA also does not hide the number of tuples for each aggregate.

**Location-Based Social Networking Applications**: Recent work by Narayanan et al. [29], Zhong et al. [41], and Puttaswamy and Zhao [33] focuses on preserving location privacy in location-based social networking applications. This work allows users to exchange location-based information with friends while protecting their privacy against disclosure to both the server and friends. Shokri et al. [38] provide a framework for quantifying location privacy, considering side information. However, this work has a different goal than PrivStats and does not propose protocols for accountable and location-private computation of aggregate statistics.

**E-Cash [9]**: This is related to our accountability protocol (§6). One might envision giving each client a number of e-coins to spend on every aggregate. This approach fits well in spam control applications [1, 39]. However, it is not practical in our setting, as coins must be tied to a specific location, time interval, and sample type, requiring a prohibitively large number of coins. Other work [8] gives each user n coins for each time period, but this is also not feasible. E-cash adds complexity to identify double-spenders, which we do not require. Our accountability protocol is simple, specific to our setting, and fast.

Finally, our approach complements work on differential privacy [11], as discussed in §8.

### Conclusion

In this paper, we presented PrivStats, a system for computing aggregate statistics for mobile, location-based applications that achieves strong guarantees of location privacy and protection against cheating clients. PrivStats ensures that no further information leaks even in the face of arbitrary side information attacks and provides client accountability without a trusted party. We implemented PrivStats on commodity phones and servers, demonstrating its practicality.

### Acknowledgments

We thank Nickolai Zeldovich, Mike Walfish, Arvind Narayanan, and the anonymous reviewers for their useful comments and feedback. This work was supported in part by NSF grants 0931550 and 0716273.

### References

[1] M. Abadi, A. Birrell, M. Burrows, F. Dabek, and T. Wobber. Bankable postage for network services. In ASIAN, 2003.
[2] J. Balasch, A. Rial, C. Troncoso, B. Preneel, I. Verbauwhede, and C. Geuens. PrETP: Privacy-preserving electronic toll pricing. Usenix Security, 2010.
[3] M. Bellare and O. Goldreich. On defining proofs of knowledge. CRYPTO, 1992.
[4] A. Boldyreva, N. Chenette, Y. Lee, and A. O’Neill. Order-preserving symmetric encryption. In EUROCRYPT, 2009.
[5] F. Boudot. Efficient proofs that a committed number lies in an interval. EUROCRYPT, 2000.
[6] M. Burkhart, M. Strasser, D. Many, and X. Dimitropoulos. SEPIA: Privacy-preserving aggregation of multi-domain network events and statistics. Usenix Security, 2010.
[7] California Department of Transportation. Caltrans guide for the preparation of traffic impact studies.
[8] J. Camenisch, S. Hohenberger, M. Kohlweiss, A. Lysyanskaya, and M. Meyerovich. How to win the clonewars: Efficient periodic n-times anonymous authentication. In CCS, 2006.
[9] J. Camenisch, S. Hohenberger, and A. Lysyanskaya. Balancing accountability and privacy using e-cash. Security and Cryptography for Networks, 2006.
[10] J. Camenisch and A. Lysyanskaya. A Signature Scheme with Efficient Protocols. Security and Cryptography for Networks, 2002.
[11] C. Dwork. Differential privacy: A survey of results. In TAMC 1-19, 2008.
[12] E-ZPass. How it works. http://www.ezpass.com/index.html.
[13] A. Fiat and A. Shamir. How to prove yourself: Practical solutions to identification and signature problems. CRYPTO, 1986.
[14] B. Gedik and L. Liu. Location privacy in mobile systems: A personalized anonymization model. In ICDCS, 2005.
[15] S. Goldwasser, S. Micali, and C. Rackoff. The knowledge complexity of interactive proof-systems. Symposium on the Theory of Computation, 1985.
[16] M. Gruteser and D. Grunwald. Anonymous usage of location-based services through spatial and temporal cloaking. In MobiSys, 2003.
[17] M. Gruteser and B. Hoh. On the anonymity of periodic location samples. In IEEE Pervasive Computing, 2005.
[18] B. Hoh, M. Gruteser, R. Herring, J. Ban, D. Work, J.-C. Herrera, A. Bayen, M. Annavaram, and Q. Jacobson. Virtual trip lines for distributed privacy-preserving traffic monitoring. In Mobisys, 2008.
[19] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady. Enhancing security and privacy in traffic-monitoring systems. In IEEE Pervasive Computing, 2006.
[20] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady. Preserving privacy in GPS traces via uncertainty-aware path cloaking. In CCS, 2007.
[21] B. Hull, V. Bychkovsky, K. Chen, M. Goraczko, A. Miu, E. Shih, Y. Zhang, H. Balakrishnan, and S. Madden. CarTel: A Distributed Mobile Sensor Computing System. Sensys, 2006.
[22] N. Husted and S. Myers. Mobile location tracking in metro areas: Malnets and others. In CCS, 2010.
[23] D. Karger, E. Lehman, T. Leighton, M. Levine, D. Lewin, and R. Panigrahy. Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web. In STOC, 1997.
[24] J. Krumm. Inference attacks on location tracks. In IEEE Pervasive Computing, 2007.
[25] J. Lowensohn. Apple sued over location tracking in iOS. http://news.cnet.com/8301-27076_3-20057245-248.html, 2011. CNET News.
[26] E. Mills. Google sued over Android data location collection. http://news.cnet.com/8301-27080_3-20058493-245.html, 2011. CNET News.
[27] Mobile Millennium. http://traffic.berkeley.edu/.
[28] M. Mun, S. Reddy, K. Shilton, N. Yau, P. Boda, J. Burke, D. Estrin, M. Hansen, E. Howard, and R. West. PEIR, the personal environmental impact report, as a platform for participatory sensing systems research. In MobiSys, 2009.
[29] A. Narayanan, N. Thiagarajan, M. Lakhani, M. Hamburg, and D. Boneh. Location privacy via private proximity testing. NDSS, 2011.
[30] P. Paillier. Public-key cryptosystems based on composite degree residuosity classes. In EUROCRYPT, 1999.
[31] T. P. Pedersen. Non-interactive and information-theoretic secure verifiable secret sharing. CRYPTO, 1991.
[32] R. A. Popa, H. Balakrishnan, and A. J. Blumberg. VPriv: Protecting privacy in location-based vehicular services. Usenix Security, 2009.
[33] K. Puttaswamy and B. Zhao. Preserving privacy in location-based mobile social applications. International Workshop on Mobile Computing and Applications, 2010.
[34] R. Reid. TomTom admits to sending your routes and speed information to the police, 2011. CNET UK.
[35] P. Riley. The tolls of privacy: An underestimated roadblock for electronic toll collection usage. In Third International Conference on Legal, Security, and Privacy Issues in IT, 2008.
[36] C. P. Schnorr. Efficient identification and signatures for smart cards. CRYPTO, 1989.
[37] E. Shi, T.-H. H. Chan, E. Rieffel, R. Chow, and D. Song. Privacy-preserving aggregation of time-series data. In NDSS, 2011.
[38] R. Shokri, G. Theodorakopoulos, J.-Y. L. Boudec, and J.-P. Hubaux. Quantifying location privacy. In IEEE Symposium on Security and Privacy, 2011.
[39] M. Walfish, J. Zamfirescu, H. Balakrishnan, D. Karger, and S. Shenker. Distributed quota enforcement for spam control. In NSDI, 2006.
[40] WMUR. Police: Thieves robbed home based on Facebook, 2010. http://www.wmur.com/r/24943582/detail.html.
[41] G. Zhong, I. Goldberg, and U. Hengartner. Louis, Lester, and Pierre: Three protocols for location privacy. International Conference on Privacy-Enhancing Technologies, 2007.