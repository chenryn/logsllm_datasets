# Adversarial Examples Based on the Capability of Sensor Attacks to Fool LiDAR-Based Perception Models in AV Systems

## 12. Conclusion

In this work, we conduct the first comprehensive security study of LiDAR-based perception in autonomous vehicle (AV) systems. We focus on LiDAR spoofing attacks as our threat model, with the goal of spoofing front-near obstacles. Initially, we reproduce state-of-the-art LiDAR spoofing attacks and find that they are insufficient for achieving the attack goal due to the machine learning-based object detection process. To address this, we formulate the attack task as an optimization problem. We construct an input perturbation function using local attack experiments and global spatial transformation-based modeling, and then define the objective function by analyzing the post-processing steps. We also identify inherent limitations of optimization-based methods and propose a new algorithm that increases the attack success rate by an average of 2.65 times. As a case study, we construct and evaluate two attack scenarios that could compromise AV safety and mobility. Additionally, we discuss potential defense strategies at the AV system, sensor, and machine learning model levels.

## Acknowledgments

We would like to express our gratitude to Shengtuo Hu, Jiwon Joung, Yunhan Jack Jia, Yuru Shao, Yikai Lin, David Ke Hong, the anonymous reviewers, and our shepherd Zhe Zhou for their valuable feedback on our work. This research was supported in part by an award from Mcity at the University of Michigan, the National Science Foundation under grants CNS-1850533, CNS-1330142, CNS-1526455, and CCF-1628991, and the Office of Naval Research under N00014-18-1-2020.

## References

[1] HARD BRAKE HARD ACCELERATION. (2005). http://tracknet.accountsupport.com/wp-content/uploads/Verizon/Hard-Brake-Hard-Acceleration.pdf

[2] ArbExpress. (2016). https://www.tek.com/signal-generator/afg2021-software-0

[3] An Introduction to LIDAR: The Key Self-Driving Car Sensor. (2017). https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff

[4] Baidu Apollo. (2017). http://apollo.auto

[5] Google’s Waymo Invests in LIDAR Technology, Cuts Costs by 90 Percent. (2017). https://arstechnica.com/cars/2017/01/googles-waymo-invests-in-lidar-technology-cuts-costs-by-90-percent/

[6] KITTI Vision Benchmark: 3D Object Detection. (2017). http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d

[7] What it Was Like to Ride in GM’s New Self-Driving Cruise Car. (2017). https://www.recode.net/2017/11/29/16712572/general-motors-gm-new-self-driving-autonomous-cruise-car-future

[8] Baidu hits the gas on autonomous vehicles with Volvo and Ford deals. (2018). https://techcrunch.com/2018/11/01/baidu-volvo-ford-autonomous-driving/

[9] Baidu starts mass production of autonomous buses. (2018). https://www.dw.com/en/baidu-starts-mass-production-of-autonomous-buses/a-44525629

[10] VeloView. (2018). https://www.paraview.org/VeloView/

[11] Volvo Finds the LIDAR it Needs to Build Self-Driving Cars. (2018). https://www.wired.com/story/volvo-self-driving-lidar-luminar/

[12] Waymo’s autonomous cars have driven 8 million miles on public roads. (2018). https://www.theverge.com/2018/7/20/17595968/waymo-self-driving-cars-8-million-miles-testing

[13] What Is LIDAR, Why Do Self-Driving Cars Need It, And Can It See Nerf Bullets? (2018). https://www.wired.com/story/lidar-self-driving-cars-luminar-video/

[14] You can take a ride in a self-driving Lyft during CES. (2018). https://www.theverge.com/2018/1/2/16841090/lyft-aptiv-self-driving-car-ces-2018

[15] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. (2016). TensorFlow: A System for Large-Scale Machine Learning. In OSDI, Vol. 16, pp. 265–283.

[16] Athalye, A., Carlini, N., & Wagner, D. (2018). Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. arXiv preprint arXiv:1802.00420

[17] Athalye, A., & Sutskever, I. (2018). Synthesizing Robust Adversarial Examples. In International Conference on Machine Learning (ICML).

[18] Carlini, N., Mishra, P., Vaidya, T., Zhang, Y., Sherr, M., Shields, C., Wagner, D., & Zhou, W. (2016). Hidden Voice Commands. In USENIX Security Symposium.

[19] Carlini, N., & Wagner, D. (2017). Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, pp. 3–14.

[20] Carlini, N., & Wagner, D. (2018). Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. In Deep Learning and Security Workshop (DLS).

[21] Carlini, N., & Wagner, D. A. (2017). Towards Evaluating the Robustness of Neural Networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pp. 39–57. https://doi.org/10.1109/SP.2017.49

[22] Checkoway, S., McCoy, D., Kantor, B., Anderson, D., Shacham, H., Savage, S., Koscher, K., Czeskis, A., Roesner, F., & Kohno, T. (2011). Comprehensive Experimental Analyses of Automotive Attack Surfaces. In Proceedings of the 20th USENIX Conference on Security (SEC’11).

[23] Chen, Q. A., Yin, Y., Feng, Y., Mao, Z. M., & Liu, H. X. (2018). Exposing Congestion Attack on Emerging Connected Vehicle Based Traffic Signal Control. In Proceedings of the 25th Annual Network and Distributed System Security Symposium (NDSS ’18).

[24] Cheng, M., Yi, J., Zhang, H., Chen, P.-Y., & Hsieh, C.-J. (2018). Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples. arXiv preprint arXiv:1803.01128

[25] Cho, K.-T., & Shin, K. G. (2016). Error Handling of In-Vehicle Networks Makes Them Vulnerable. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS’16).

[26] Cisse, M., Adi, Y., Neverova, N., & Keshet, J. (2017). Houdini: Fooling Deep Structured Prediction Models. arXiv preprint arXiv:1707.05373

[27] Evtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A., Rahmati, A., & Song, D. (2017). Robust Physical-World Attacks on Deep Learning Models. arXiv preprint arXiv:1707.08945

[28] Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Tramer, F., Prakash, A., Kohno, T., & Song, D. (2018). Physical Adversarial Examples for Object Detectors. In USENIX Workshop on Offensive Technologies (WOOT).

[29] Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C., Prakash, A., Kohno, T., & Song, D. (2018). Robust Physical-World Attacks on Deep Learning Visual Classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[30] Feng, Y., Huang, S., Chen, Q. A., Liu, H. X., & Mao, Z. M. (2018). Vulnerability of Traffic Control System Under Cyber-Attacks Using Falsified Data. In Transportation Research Board 2018 Annual Meeting (TRB).

[31] Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572

[32] Velodyne LiDAR Inc. (2018). VLP-16 User Manual.

[33] Ivanov, R., Pajic, M., & Lee, I. (2014). Attack-Resilient Sensor Fusion. In Proceedings of the Conference on Design, Automation & Test in Europe.

[34] Jaderberg, M., Simonyan, K., Zisserman, A., et al. (2015). Spatial Transformer Networks. In Advances in Neural Information Processing Systems, pp. 2017–2025.

[35] Kingma, D. P., & Ba, J. (2014). Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980

[36] Koscher, K., Czeskis, A., Roesner, F., Patel, S., Kohno, T., Checkoway, S., McCoy, D., Kantor, B., Anderson, D., Shacham, H., & Savage, S. (2010). Experimental Security Analysis of a Modern Automobile. In Proceedings of the 2010 IEEE Symposium on Security and Privacy (SP’10).

[37] Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., & Zhang, X. (2018). Trojaning Attack on Neural Networks. In Annual Network and Distributed System Security Symposium (NDSS).

[38] Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S., Houle, M. E., Schoenebeck, G., Song, D., & Bailey, J. (2018). Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality. arXiv preprint arXiv:1801.02613

[39] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv preprint arXiv:1706.06083

[40] Mazloom, S., Rezaeirad, M., Hunter, A., & McCoy, D. (2016). A Security Analysis of an In-Vehicle Infotainment and App Platform. In Usenix Workshop on Offensive Technologies (WOOT).

[41] Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., & Swami, A. (2017). Practical Black-Box Attacks Against Machine Learning. In ACM on Asia Conference on Computer and Communications Security.

[42] Petit, J., Stottelaar, B., Feiri, M., & Kargl, F. (2015). Remote Attacks on Automated Vehicles Sensors: Experiments on Camera and LiDAR. In Black Hat Europe.

[43] Rouf, I., Miller, R., Mustafa, H., Taylor, T., Oh, S., Xu, W., Gruteser, M., Trappe, W., & Seskar, I. (2010). Security and Privacy Vulnerabilities of In-Car Wireless Networks: A Tire Pressure Monitoring System Case Study. In Proceedings of the 19th USENIX Conference on Security (USENIX Security’10). USENIX Association, Berkeley, CA, USA, pp. 21–21. http://dl.acm.org/citation.cfm?id=1929820.1929848

[44] Shin, H., Kim, D., Kwon, Y., & Kim, Y. (2017). Illusion and Dazzle: Adversarial Optical Channel Exploits Against Lidars for Automotive Applications. In International Conference on Cryptographic Hardware and Embedded Systems (CHES).

[45] Shoukry, Y., Martin, P., Tabuada, P., & Srivastava, M. (2013). Non-Invasive Spoofing Attacks for Anti-Lock Braking Systems. In Cryptographic Hardware and Embedded Systems - CHES 2013, Guido Bertoni and Jean-Sébastien Coron (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 55–72.

[46] Shoukry, Y., Martin, P., Yona, Y., Diggavi, S. N., & Srivastava, M. B. (2015). PyCRA: Physical Challenge-Response Authentication For Active Sensors Under Spoofing Attacks. In ACM Conference on Computer and Communications Security.

[47] Tramèr, F., Kurakin, A., Papernot, N., Boneh, D., & McDaniel, P. (2017). Ensemble Adversarial Training: Attacks and Defenses. arXiv preprint arXiv:1705.07204

[48] Wong, W., Huang, S., Feng, Y., Chen, Q. A., Mao, Z. M., & Liu, H. X. (2019). Trajectory-Based Hierarchical Defense Model to Detect Cyber-Attacks on Transportation Infrastructure. In Transportation Research Board 2019 Annual Meeting (TRB).

[49] Xiang, C., Qi, C. R., & Li, B. (2018). Generating 3D Adversarial Point Clouds. arXiv preprint arXiv:1809.07016

[50] Xiao, C., Deng, R., Li, B., Yu, F., Song, D., et al. (2018). Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation. In Proceedings of the (ECCV), pp. 217–234.

[51] Xiao, C., Li, B., Zhu, J.-Y., He, W., Liu, M., & Song, D. (2018). Generating Adversarial Examples with Adversarial Networks. arXiv preprint arXiv:1801.02610

[52] Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., & Song, D. (2018). Spatially Transformed Adversarial Examples. arXiv preprint arXiv:1801.02612

[53] Xie, C., Wang, J., Zhang, Z., Zhou, Y., Xie, L., & Yuille, A. (2017). Adversarial Examples for Semantic Segmentation and Object Detection. In IEEE International Conference on Computer Vision (ICCV).

[54] Xu, X., Chen, X., Liu, C., Rohrbach, A., Darrell, T., & Song, D. (2017). Can You Fool AI with Adversarial Examples on a Visual Turing Test? arXiv preprint arXiv:1709.08693

[55] Yan, C. (2016). Can You Trust Autonomous Vehicles: Contactless Attacks Against Sensors of Self-Driving Vehicles.

[56] Yang, K., Wang, R., Jiang, Y., Song, H., Luo, C., Guan, Y., Li, X., & Shi, Z. (2018). Sensor Attack Detection Using History-Based Pairwise Inconsistency. Future Generation Computer Systems.

[57] Yuan, X., Chen, Y., Zhao, Y., Long, Y., Liu, X., Chen, K., Zhang, S., Huang, H., Wang, X., & Gunter, C. A. (2018). CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition. In USENIX Security Symposium.

## Appendix

### A. Algorithm Details and Experiment Settings

Algorithm 1 provides the detailed steps for generating adversarial examples. In our experiments, we use the Adam optimizer [35] with a learning rate of \(1 \times 10^{-4}\). The `opt(ladv; θ, τx, sh)` function updates the parameters \(θ\), \(τx\), and \(sh\) based on the loss function \(ladv\). We use TensorFlow [15] as the backbone. The parameter \(Lt\) is set to 12.5, and \(Lθ\) is set to the angle that generates a 2-meter distance from the target position.

**Figure 14:** Collected traces from the reproduced sensor attack. The points in the yellow circle are spoofed by the sensor attack.

```plaintext
Algorithm 1: Generating Adversarial Examples by Leveraging Global Spatial Transformation
Input: 
    - Target model: M
    - 3D point cloud: X
    - 3D spoofed 3D point cloud: T
    - Optimizer: opt
    - Max iteration: N
Output: 
    - 3D adversarial 3D point cloud: X′

1. Initialization:
    - θ ← 0, τx ← 0, sh ← 1, lmin = +∞
    - x = Φ(X), t = Φ(T)
    - Initialize parameters by sampling around the transformation parameters T arдetθ, T arдetτx that transform t to the target position (px, py) of the attack

2. for iτx ← −Lt to Lt do
    for iθ ← −Lθ to Lθ do
        - θ ← T arдetθ + iθ, τx ← T arдetτx + iτx
        - for iter ← 1 to N do
            - ladv ← Equation 7.
            - Update the parameters θ, τx, sh based on optimizer opt and loss ladv
            - if lmin < ladv then
                - θfinal, τxfinal, shfinal ← θ, τx, sh
            - end
        - end
    - end

3. T′ ← GT(θfinal, τxfinal, shfinal; T)
4. X′ ← X + T′
5. Return: T′
```

This algorithm leverages global spatial transformations to generate adversarial examples, aiming to fool the LiDAR-based perception models in AV systems.