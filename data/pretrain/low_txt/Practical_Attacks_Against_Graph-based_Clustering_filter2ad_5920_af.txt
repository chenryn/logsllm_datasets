### 0.48 to 0.26: Evading Detection by Reducing DGA Domains

When the SVD rank is set to 35, the minimum cost for an attacker to evade detection is 0.48. To achieve this, the attacker must reduce the number of distinct DGA domains from 618 to 160, with each domain being queried by all infected hosts. In comparison, when the SVD rank is increased to 200, the minimum cost rises to 0.38. The attacker then needs to further reduce the number of distinct DGA domains to 60 to evade detection, again with each domain being queried by all infected hosts. This reduction in the attack graph density from 0.48 to 0.1 results in a 79% decrease (from 0.48 to 0.1) in the number of queries to distinct DGA domains. Tuning hyperparameters can thus increase the cost of small community attacks, potentially rendering them ineffective.

### Node2vec vs. Spectral Clustering: Attack Costs and Resilience

For node2vec, the minimum cost of a small community attack is higher compared to spectral clustering. We computed the attacker graph density for the white area in Figure 9, excluding randomness (i.e., the first column and bottom two rows). Unlike spectral clustering, node2vec requires a significantly higher minimum cost for a guaranteed small community attack, indicating that node2vec is more resilient to such attacks. The smallest communities in Figure 9 (first column and bottom two rows) are likely undersampled because choosing 15 walks per node and a walk length of 20 using cluster validity in Section 5.1.3 favors larger labeled DGA communities. This makes neighborhood observations for extremely small islands insignificant, allowing small community attacks. The remaining portion of the plot shows randomness due to the random walk process used by node2vec. This inherent randomness means that attacks succeed randomly in the remaining portion of Figure 9. This suggests that systems like Pleiades would benefit from node2vec, as it reduces the guarantee of successful attacks and allows defenders to identify if an attacker is evading by chance encounters where evasion fails over time. While the minimum attack cost remains the same for different neighborhood sizes, the attack success rate varies. For neighborhood sizes 2, 4, and 6, the attack success rates are 65.16%, 60.65%, and 70.65%, respectively (Figure 9). We will discuss how to use different hyperparameters to further reduce the success rate of small community attacks in Section 6.2.

### False Positive Rates for DGA Families

| Model | Pykspa | Gimemo | Suppobox | Murofet |
|--------|--------|--------|----------|---------|
| Original | 0.29% | 1.64% | 0% | 0% |
| Model A | 0.32% | 1.62% | 0% | 0% |
| Model B | 0.39% | 0.10% | 0.30% | 0.10% |
| Model C | 1.17% | 1.46% | 1.23% | 1.23% |

Table 4: False Positive Rate for four DGA families before retraining, and after retraining with three types of noise.

These costs further demonstrate node2vec's superiority over spectral clustering in resisting small community attacks.

### 6. Defense Strategies

#### 6.1 Training Classifiers with Noise

To mitigate the noise injection attack, we retrained the classifier with noisy data, which made it more resistant to adversarial noise. We used domains from a benign DGA to poison the clusters of malicious DGAs. After retraining the classifier using clusters generated by the noise injection attack variant 1 ("Minimal Benign DGA 1", m = 1, Algorithm 1 in Section 5.2), we tested the new model (Model A) against adversarial clusters generated by the same noise injection attack under community discovery and node2vec. The first two violins in Figure 6c show that Model A increases the overall predicted class probabilities compared to the "After Attack" violins in Figure 6a. In community discovery, the accuracy increased from 2% to 98%; in node2vec, the accuracy increased from 0.8% to 98%. Retraining with noisy clusters containing a benign DGA from SVD can remediate the same attack on both community discovery and node2vec. This effect is observed even when the noise levels are doubled (m = 2, Algorithm 1 in Section 5.2). When models were trained with half the noise (m = 1, Algorithm 1 in Section 5.2), they were able to more accurately predict the correct label. On average, only 7.3% of clusters were predicted with the wrong labels, down from 100% before retraining.

The average prediction confidence also increased significantly. Before retraining, the average prediction confidence for "Minimal Benign DGA 2", "Moderate 2", and "Perfect Long Tail 2" were 10%, 20%, and 20%, respectively. After retraining, these values increased to 70%, 90%, and 80%, respectively. However, retraining with noisy clusters increased the false positive rate in most cases (Table 4).

It is important to note that this defense only trains the classifier with noise that has been witnessed. New noise will appear, but the fundamental attack on the unsupervised component remains the same. Therefore, defenders will be alerted by plummeting accuracies in their models. Our defenses are simple, and future work should focus on making clustering systems more robust.

#### 6.2 Improving Hyperparameter Selection

Small community attacks show that traditional methods of choosing hyperparameters (Section 5.1) are insufficient when facing adversaries. Fortunately, the small community attack can be used to select more resistant hyperparameters. We demonstrate that better selection can reduce the number of successful small community attack instances from our previous experiments.

In Figure 11a, we plot the successful attack rate under different numbers of eigenvalues. The successful attack rate decreases as the number of eigenvalues computed increases, and the line plateaus after 200 eigenvalues. This means that a defender running Pleiades should select the first 200 eigenvalues instead of the 35 indicated by the scree plot in Figure 4. Using the small community attack in this way helps choose better parameters for the system and also identifies under which parameters the system is vulnerable.

Similarly, for node2vec, using the small community attack to choose hyperparameters can reduce the attack success rate. Cluster validity metrics suggest selecting a neighborhood size of six and a walk length of 20. However, evaluating the graph clustering using the success rate of the small community attack reveals that these hyperparameters are not optimal. For the neighborhood size, Figure 9 shows that a smaller neighborhood size of four introduces a lower attack success rate. In Figure 11b, we plot the attack success rate under different walk lengths, showing that a walk length of 12 is preferred over 20, as the former allows a 51.29% attack success rate compared to 61.61% for the latter. Smaller neighborhood sizes and shorter walk lengths can better tolerate small community attacks, presumably because they do not oversample larger communities with more distinct neighborhood observations. We recommend using the small community attack success rate to evaluate the clustering hyperparameter selection, in addition to traditional cluster validity indices.

### 7. Discussion

We acknowledge that the details surrounding the implementation of the attacks are specific to Pleiades. However, the graph representation suggests that these attacks may work on other graph-based systems. In this section, we briefly discuss issues to consider when generalizing the attacks.

Nodes and edges can be trivially injected or removed in the graph Pleiades uses, which is generated by malware resolving domain names. In other security contexts, the set of injectable/removable nodes varies. Some nodes and edges must exist for certain attack actions to succeed. For example, a phishing email using a malicious attachment requires at least the read system call to successfully infect a host, which cannot be removed from the system call graph. On the other hand, it can be difficult to add certain nodes and edges. Therefore, in addition to the anomaly cost (Section 5.2.4) and agility cost (Section 5.3.4), the action of graph manipulation itself has costs depending on the underlying data. This should be carefully considered when generalizing the attacks to other systems, and we leave this for future work. Tighter costs may exist, but our approaches point in a promising direction.

### 8. Conclusions

We have demonstrated that generic attacks on graphs can break real-world systems that use popular graph-based modeling techniques. These attacks can often be performed by limited adversaries at low cost; however, simple defenses can reduce their effectiveness or likelihood of success. To summarize, hyperparameter selection should be optimized to reduce the success rate of small community attacks, and retraining can be used to lessen the impact of noise injection attacks. Furthermore, state-of-the-art graph embedding techniques like node2vec appear to be more resistant to small community attacks, suggesting that systems like Pleiades would be harder to adversarially manipulate using node2vec over community finding or spectral methods (see Figure 9 vs. Figure 7).

### 9. Acknowledgments

We thank our anonymous reviewers for their invaluable feedback and Dr. Rosa Romero-Gómez for her help in visualization. This material is based upon work supported in part by the US Department of Commerce grants no. 2106DEK and 2106DZD, National Science Foundation (NSF) grant no. 2106DGX, and Air Force Research Laboratory/Defense Advanced Research Projects Agency grant no. 2106DTX. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the US Department of Commerce, National Science Foundation, Air Force Research Laboratory, or Defense Advanced Research Projects Agency.

### References

[1] Accessed in May 2017. Amazon Machine Learning. https://aws.amazon.com/machine-learning. (Accessed in May 2017).
[2] Accessed in May 2017. BigML. https://bigml.com/. (Accessed in May 2017).
[3] Accessed in May 2017. Google Cloud Prediction API. https://cloud.google.com/prediction/docs/. (Accessed in May 2017).
[4] Accessed in May 2017. IOC Bucket. https://www.iocbucket.com/. (Accessed in May 2017).
[5] Accessed in May 2017. Microsoft Azure. https://azure.microsoft.com. (Accessed in May 2017).
[6] Accessed in May 2017. OpenIOC DB. https://openiocdb.com/. (Accessed in May 2017).
[7] Accessed in May 2017. PredictionIO. http://prediction.io. (Accessed in May 2017).
[8] Manos Antonakakis, Roberto Perdisci, David Dagon, Wenke Lee, and Nick Feamster. 2010. Building a Dynamic Reputation System for DNS. In USENIX Security Symposium. 273–290.
[9] Manos Antonakakis, Roberto Perdisci, Yacin Nadji, Nikolaos Vasiloglou, Saeed Abu-Nimeh, Wenke Lee, and David Dagon. 2012. From Throw-Away Traffic to Bots: Detecting the Rise of DGA-Based Malware. In Presented as part of the 21st USENIX Security Symposium (USENIX Security 12). 491–506.
[10] Johannes Bader. 2017. Domain Generation Algorithms. https://github.com/baderj/domain_generation_algorithms. (2017).
[11] Ulrich Bayer, Paolo Milani Comparetti, Clemens Hlauschek, Christopher Kruegel, and Engin Kirda. 2009. Scalable, Behavior-Based Malware Clustering. In NDSS, Vol. 9. Citeseer, 8–11.
[12] Battista Biggio, Ignazio Pillai, Samuel Rota Bulò, Davide Ariu, Marcello Pelillo, and Fabio Roli. 2013. Is Data Clustering in Adversarial Settings Secure? In Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security. ACM, 87–98.
[13] Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, and Fabio Roli. 2014. Poisoning Behavioral Malware Clustering. In Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop. ACM, 27–36.
[14] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast Unfolding of Communities in Large Networks. Journal of Statistical Mechanics: Theory and Experiment 2008, 10 (2008), P10008.
[15] Mark Braverman, Young Kun Ko, Aviad Rubinstein, and Omri Weinstein. 2017. ETH Hardness for Densest-k-Subgraph with Perfect Completeness. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, 1326–1341.
[16] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In Security and Privacy (EuroS&P), 2017 IEEE European Symposium on. IEEE.
[17] Raymond B Cattell. 1966. The Scree Test for the Number of Factors. Multivariate Behavioral Research 1, 2 (1966), 245–276.
[18] Duen Horng Polo Chau, Carey Nachenberg, Jeffrey Wilhelm, Adam Wright, and Christos Faloutsos. 2011. Polonium: Tera-Scale Graph Mining and Inference for Malware Detection. In Proceedings of the 2011 SIAM International Conference on Data Mining. SIAM, 131–142.
[19] Yizheng Chen, Manos Antonakakis, Roberto Perdisci, Yacin Nadji, David Dagon, and Wenke Lee. 2014. DNS Noise: Measuring the Pervasiveness of Disposable Domains in Modern DNS Traffic. In Dependable Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP International Conference on. IEEE, 598–609.
[20] Yizheng Chen, Panagiotis Kintis, Manos Antonakakis, Yacin Nadji, David Dagon, Wenke Lee, and Michael Farrell. 2016. Financial Lower Bounds of Online Advertising Abuse. In Detection of Intrusions and Malware, and Vulnerability Assessment. Springer, 231–254.
[21] Yizheng Chen, Yacin Nadji, Rosa Romero-Gómez, Manos Antonakakis, and David Dagon. 2017. Measuring Network Reputation in the Ad-Bidding Process. In International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment. Springer, 388–409.
[22] CYBERWARZONE. Accessed in May 2017. 30 Malicious IP List and Block Lists Providers. http://cyberwarzone.com/30-malicious-ip-list-and-block-lists-providers-2015/. (Accessed in May 2017).
[23] Hermit Dave. Accessed in May 2017. Frequency Words in Subtitles. https://github.com/hermitdave/FrequencyWords/tree/master/content/2016/en. (Accessed in May 2017).
[24] Prahlad Fogla and Wenke Lee. 2006. Evading Network Anomaly Detection Systems: Formal Reasoning and Practical Techniques. In Proceedings of the 13th ACM Conference on Computer and Communications Security. ACM, 59–68.
[25] Amir Globerson and Sam Roweis. 2006. Nightmare at Test Time: Robust Learning by Feature Deletion. In Proceedings of the 23rd International Conference on Machine Learning. ACM, 353–360.
[26] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864.
[27] Shuang Hao, Nadeem Ahmed Syed, Nick Feamster, Alexander G Gray, and Sven Krasser. 2009. Detecting Spammers with SNARE: Spatio-temporal Network-level Automatic Reputation Engine. In USENIX Security Symposium, Vol. 9.
[28] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD Tygar. 2011. Adversarial Machine Learning. In Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence. ACM, 43–58.
[29] Luca Invernizzi, Stanislav Miskovic, Ruben Torres, Christopher Kruegel, Sabyasachi Saha, Giovanni Vigna, Sung-Ju Lee, and Marco Mellia. 2014. Nazca: Detecting Malware Distribution in Large-Scale Networks. In NDSS, Vol. 14. 23–26.
[30] Kurucz, Miklós and Benczúr, András A. 2010. Geographically Organized Small Communities and the Hardness of Clustering Social Networks. In Data Mining for Social Network Data. Springer.
[31] Kevin J Lang. 2005. Fixing Two Weaknesses of the Spectral Method. Advances in Neural Information Processing Systems (2005).
[32] Angsheng Li and Pan Peng. 2012. The Small-Community Phenomenon in Networks. Mathematical Structures in Computer Science 22, 03 (2012), 373–407.
[33] Zhou Li and Alina Oprea. 2016. Operational Security Log Analytics for Enterprise Breach Detection. In First IEEE Cybersecurity Development Conference (SecDev).
[34] Kun Liu and Evimaria Terzi. 2008. Towards Identity Anonymization on Graphs. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. ACM, 93–106.
[35] Daniel Lowd and Christopher Meek. 2005. Adversarial Learning. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining. ACM, 641–647.
[36] Daniel Lowd and Christopher Meek. 2005. Good Word Attacks on Statistical Spam Filters. In CEAS.
[37] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In Advances in Neural Information Processing Systems. 3111–3119.
[38] Yacin Nadji, Manos Antonakakis, Roberto Perdisci, David Dagon, and Wenke Lee. 2013. Beheading Hydras: Performing Effective Botnet Takedowns. In Proceedings of the 2013 ACM SIGSAC Conference on Computer & Communications Security. ACM, 121–132.
[39] Yacin Nadji, Manos Antonakakis, Roberto Perdisci, and Wenke Lee. 2013. Connected Colors: Unveiling the Structure of Criminal Networks. In International Workshop on Recent Advances in Intrusion Detection. Springer Berlin Heidelberg, 390–410.
[40] Terry Nelms, Roberto Perdisci, Manos Antonakakis, and Mustaque Ahamad. 2015. WebWitness: Investigating, Categorizing, and Mitigating Malware Download Paths. In USENIX Security Symposium. 1025–1040.
[41] Terry Nelms, Roberto Perdisci, Manos Antonakakis, and Mustaque Ahamad. 2016. Towards Measuring and Mitigating Social Engineering Software Downloads.