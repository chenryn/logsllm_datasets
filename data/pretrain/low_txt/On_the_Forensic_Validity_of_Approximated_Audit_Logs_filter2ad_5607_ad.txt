### 6. Evaluation

#### 6.1 Datasets
For our evaluation, we utilize the DARPA Transparent Computing Program dataset [15]. This dataset was collected during an Advanced Persistent Threat (APT) simulation exercise (Engagement #3) in April 2018 and includes events from a series of target hosts, along with ground truth information about the attacks. It has been used in previous research as a source for authentic examples of adversarial behavior [17, 34]. We specifically select the Linux-based logs corresponding to two hosts from the DARPA Engagement: Trace and Theia.

In addition to the Transparent Computing engagements, we also evaluate the efficacy of log reduction systems using six attack scenarios from prior work (e.g., [30, 32, 41, 54]). These include:
- **Unrealircd, vsftpd, and webmin exploits**: These leverage input vulnerabilities to achieve remote code execution (RCE). A payload is executed, which launches a reverse shell back to the attacker's machine, executing commands such as `ifconfig`.
- **Wordpress vulnerability and Webshell**: These exploits execute a payload through a web server, which calls back to the attacker's machine.
- **Firefox vulnerability**: This is an exploit on Firefox 54.0.1 that gains execution through a malicious ad server. This vulnerability was exploited as part of DARPA TC Engagement #3 [15].

We use the subset of logs associated with these particular exploits for our evaluation.

#### 6.2 Performance Evaluation
To evaluate the performance of our log reduction technique at scale, we concatenate all logs from a specific dataset and run our reduction algorithm. An epoch ends either at the end of a log file or after 5 minutes have elapsed.

##### 6.2.1 Re-Implementation Validity
We begin by comparing the reduction rates of our re-implementations to the findings of the original works. Table 1 shows the results for the Theia dataset. Since log reduction has been reported differently in various papers, we provide both the reduction factor and reduction percentage statistics. While some of our implementations did not reach the peak reduction rates observed in prior work, they are consistent with prior observations of these systems. The differences in performance can likely be attributed to the process behaviors used in the test datasets. Our implementation of CPR performed closest to the reports of the original paper, confirming our intuition that CPR is the most generally applicable of past approximation techniques and is thus a solid basis for our attack-preserving extension.

##### 6.2.2 Reduction Performance
Next, we compare the reduction performance of LogApprox to other approximation techniques using the Theia and Trace datasets. Figures 5a and 5b show the growth in log size for all approximation techniques over the course of the attack engagements in Theia and Trace, respectively. Note that the final position of the lines in Figure 5a corresponds to the reduction rates observed by past techniques in Table 6. We observed that all approximation techniques performed better on the Theia dataset because it had many more events, offering more opportunities for reduction. In contrast, the Trace dataset contains many small and disjoint log traces, creating fewer reduction opportunities.

LogApprox consistently performed between GC/CPR and F-DPR/S-DPR. LogApprox’s reduction factor was 2.87X and 1.72X (removing 42% and 65% of the raw log) on the Theia and Trace datasets, respectively. Hossain et al.’s F-DPR and S-DPR [36] had the largest reduction factors, boasting up to 85% and 91% reductions in log size during the Theia engagement. In Section 6.4, we determine whether this extreme efficiency comes at the cost of valuable forensic information.

##### 6.2.3 Log Ingest Performance
A final consideration for the performance of our LogApprox system is whether its reduction analysis can keep pace with the speed of log event creation. Figure 6 compares the event generation rate of the Theia dataset. Because LogApprox was evaluated by replaying this dataset rather than observing events in real-time, we calculate the maximum ingest rate of LogApprox in MB per hour by observing how long it took to process the 391 MB of Theia log files. We find that LogApprox’s ingest rate of 45.44 MB per hour far outpaces the amount of logs generated during the Transparent Computing engagement, making it suitable for general use.

#### 6.3 Utility Evaluation
We now evaluate the utility of approximated logs through our forensic validity metrics. The Trace and Theia datasets contain multiple intrusion attempts, but only some are well-documented, so they are not an effective ground truth for use with our metrics. Instead, we evaluate the utility of these systems based on a curated set of real-world program exploits. Each exploit was implemented by the authors based on public documentation or downloaded from public repositories (e.g., exploitdb), then launched in a controlled VM environment to capture the associated audit logs. Following the test, two authors reviewed the log entries and marked all events found on the forward trace paths from the point of entry. There was no disagreement between authors on which entries fell on the path. All log events (including those not directly on the attack paths) are included in the below tests.

Figure 7 shows our results for each of the forensic validity metrics. Taller bars indicate better performance as they signify that more forensic evidence was retained. For the Lossless Forensics metric (Figure 7a), we see that all techniques sacrifice significant forensic context, especially F-DPR and S-DPR, which boast the largest log reduction rates. This is not surprising, as prior approaches to log approximation did not consider the extreme threat model for which timing side channels need to be accounted. An interesting direction for future work would be to consider approximation techniques that preserve information about the timing distribution of repeated events.

For the Causality-Preserving Forensics metric, we confirm that CPR preserves 100% of forensic information as expected. GC retains a consistently high amount of forensic evidence, retaining 91% or better in all cases. LogApprox appears moderately conservative in forensic reduction, preserving 90% causality or greater in 4 of the 6 scenarios while retaining 36% in the worst-case. F-DPR and S-DPR’s aggressive nature retains between 45%-80% in 3 of the 6 scenarios, while retaining as much as 95% in the best case scenarios and as little as 6.0%-17.0% in the 2 worst-cases. This suggests that, even by established measures of forensic validity such as information flow preservation, these techniques are sacrificing potentially important forensic context. Whether the lost data is relevant to the analysis of a particular attack is likely case-specific.

Results for Attack-Preserving Forensics are shown in Figure 7. LogApprox retains 100% of forensic evidence under the attack-preserving metric, as does CPR. Because attack-preserving evidence is a subset of causality-preserving evidence, the 3 remaining approximation techniques all perform slightly better under this model. GC comes close to satisfying attack-preserving validity, achieving between 96% and 99% in all scenarios. This result makes sense as GC was specifically designed to clean up process behaviors associated with benign activity, such as temporary file I/O. Unlike causality-preserving forensics, attack-preserving forensics does not penalize the deletion of typical process activity. S-DPR and F-DPR retain similar forensic validity under attack-preserving as they did under causality-preserving, maintaining between 42%-80% in 3 cases, 85%-94% in the best case, and 7%-17% in the 2 worst cases. This indicates that S-DPR and F-DPR filter log events without regard for typical or atypical behavior.

#### 6.4 Performance vs. Utility
We plot the performance-utility tradeoff per attack scenario in Figures 1 (Causality-Preserving), 8a (Lossless), and 8b (Attack-Preserving), where utility is one of the forensic validity metrics defined in Section 3. The blue dashed line denotes the worst-case utility an algorithm could achieve in a scenario where all dropped events are forensically relevant (and thus no redundancies exist). An “ideal” technique would place its points in the top right corner, where both the validity and reduction are maximized. The optimal and worst-case utility curves are both metric and scenario specific, but plotting trends of reduction algorithms against such strawman metrics still provides useful insight. These plots enable informed decision-making about the value of additional log retention under different threat models. For example, S-DPR trades off very high reduction (and thus space efficiency) for lower forensic validity. If space is a limiting factor, it may be preferable to save sparser data spanning a longer time period with a highly reducing technique like S-DPR over denser data spanning a shorter time period from less approximating techniques. This would allow analysts to reconstruct long-term basic facts about an attack (e.g., root cause) at the expense of short-term details.

Intuitively, no technique can outperform the worst-case utility baseline under the Lossless Forensics metric (Figure 8a), as every dropped event is considered a utility loss. Several interesting patterns emerge when analyzing the tradeoffs in causality preservation (Figure 1). Approx, F-DPR, and S-DPR all exhibit significant variance between attack scenarios, but their performance-utility ratio remains roughly proportional. This variability in tradeoffs reflects the design goals of these systems; e.g., S-DPR focuses exclusively on identifying the correct system entities in backtraces. CPR, by design, maxes out utility here, but GC performs nearly as well. This may not be the case in attacks that involve significant destruction of system entities, e.g., an attacker exfiltrates, then destroys data. The value of Attack-Preserving Forensics becomes most evident in Figure 8b. As only a subset of causality-preserving events uniquely describe attack semantics, LogApprox is able to match CPR in utility while significantly outperforming it in log reduction in each scenario. While GC is not as aggressive, it continues to perform well under this metric, validating Lee et al.’s design decisions in the earliest log approximation paper to appear in the literature.

#### 6.5 LogApprox Case Study
To better understand the benefits of LogApprox compared to prior work, we examine the Webmin exploit in more detail. Webmin is a web-based configuration tool for Unix systems, making it a prime target for attackers as it can be leveraged for lateral movement on the network. The exploit allows for unauthenticated remote code execution when the web server is configured for users with an expired password to enter a new one. With the appropriate payload, a reverse shell can be spawned, and post-exploitation tools (e.g., LinEnum.sh [64]) can be downloaded and run on the server machine. We chose to run commands manually, namely `whoami` and `ifconfig`, to demonstrate that we have remote access.

Figure 9 displays provenance graphs corresponding to the unapproximated log (Figure 9a) and LogApprox’s attack-preserving log (Figure 9b). These graphs have been simplified for visualization purposes, as the unapproximated and reduced log contain 13,026 and 11,013 edges, respectively. The numbered edges in the graph correspond to timestamps, and attack-specific vertices are shaded in red. Due to space constraints, we omit the causality-preserving attack graph produced by CPR; it is the same as LogApprox’s graph except that the `lib*.so.*` vertex is replaced by the original 3 vertices.