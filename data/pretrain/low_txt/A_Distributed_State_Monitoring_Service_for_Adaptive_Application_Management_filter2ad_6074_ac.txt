### 5.2. Lifecycle Coordination

One of the key challenges in our systems was the need to coordinate the startup of applications. For instance, our web server demonstrator required that a database and a logging system be up and running before the web servers could start. This type of dependency is common at any stage in the lifecycle of a component.

To address this, each component registers a provider that defines its lifecycle state (e.g., deployed, starting, running, terminating). Dependencies among components can be expressed as distributed predicates over these states. Predicate evaluation components are then used to initiate lifecycle transitions. In this pattern, all components can be deployed immediately and will start themselves in the correct order.

A notable extension to this pattern is automated application testing. A test management system can deploy an application under test and use predicate evaluation components to determine the success or failure of the test, initiating tear-down before deploying the next test.

**Advantages and Pattern of Use:**
- This pattern greatly simplifies the programming of some behaviors.
- Care must be taken to avoid circular dependencies.
- It generally introduces a relatively sparse set of short-lived relationships.
- In the case of ordered startup, a component can stop listening once it has performed the guarded transition.

**Properties Used:**
- The lifecycle coordination pattern generally requires reliable delivery of state information.
- Any dependency on the non-existence of a component can only be determined using stable partition view properties.

### 5.3. Compositional Failure Management

The example systems all used a composition model promoted by SmartFrog, which naturally led to a pattern for managing failure recovery. Common patterns for implementing fault tolerance or high availability can be mapped to this composition model. For example, active replication uses a collection of application components to provide redundant processing. Such a group can be managed by a single parent component that replaces members when they fail and represents the group as a single, fault-tolerant entity in the management system. Crash restart uses the ability to recover an application component by restarting it, a role that can be adopted by a parent component.

These patterns were extensively used in the HP Utility Rendering Service. Here, we focus on hierarchical crash restart. Figure 6 below shows a hierarchy of components that manage a distributed application. The hierarchy needs to remain connected to effectively manage the application, but a node failure or network partition could lead to the loss of part of the hierarchy, as represented by the grey nodes.

**Figure 6. Effect of Node Failure on a Component Composition Hierarchy**

To address this problem, a crash-restart component was implemented using Anubis with the following policy:
1. Initially look for children and only create them if they do not already exist.
2. If at any time a child fails, recreate it.

When each component in the hierarchy adopts this crash-restart policy, the result automatically fills holes in the tree structure by recursively replacing missing components. The exception is the root of the tree, which has no parent to manage it. For this, a non-hierarchical approach is required, such as an active-standby policy implemented by multiple components that observe each other and deterministically decide which should play the root role at any given time.

**Advantages:**
- Anubis provides the ability to determine component existence in stable partitions, so observing a descendant in the hierarchy provides all the necessary information to determine its existence, locate it using encoded state information, and detect its loss in the event of failure or network partitions.
- Implementing this pattern is straightforward.

**Pattern of Use:**
- As shown in this example, the compositional recovery pattern introduces many-to-one relationships (children-to-parent) and groups components around the policy (the parent). Otherwise, the relationships are relatively sparse.

**Properties Used:**
- The pattern relies on stable partition properties to determine non-existence on startup.
- Once past this decision point, the components rely on the consistency properties available in any partition (regardless of stability) to provide failure detection guarantees.
- Although Anubis is partitionable, most of the applications were not, and in some cases, the management system would need to identify a privileged component or a primary partition to correctly terminate unwanted components.
- Anubis does not directly provide identification of a primary partition, but policies based on the ability to observe a given component (e.g., the root component) or the majority of a given set of components are supported and are generally more applicable.

### 6. Notes on Timeliness

Protocols based on timeouts are sensitive to the responsiveness of the underlying system. Our service was implemented in Java and ran on non-real-time operating systems such as Linux and Windows. However, the timeouts used in practice were generally quite large (ranging from two to ten seconds). Neither the JVM nor the OS caused unnecessary delays, even in our larger systems (the HP Utility Rendering Service ran across 120 machines, and we have tested a system with over 1000 Anubis servers spread over the same 120 physical machines).

One cause of instability we encountered was misconfiguration of other system services. For example, NTP was configured to use a time source that became inaccessible when a firewall was installed. In another case, an NFS server caused the OS on all machines to pause when too many file systems tried to mount simultaneously. In a third case, two network switches with incompatible configurations for multicast packet routing overloaded the network. Our partition protocols were sensitive to these issues but also proved very useful in debugging the problems.

These cases highlight the importance of correct total system configuration in time-critical systems.

### 7. Conclusion

We have described how Anubis, a state monitoring service, has been used as part of a distributed management framework for adaptive applications in Grid and Utility computing environments. The use cases presented demonstrate how developers have exploited the service in practice to implement resource management, lifecycle coordination, and compositional failure management. The main advantage of Anubis is that it hides the complexity of coordinating distributed management agents, simplifying their programming. The state observation abstraction and temporal properties provided by Anubis have proven useful in our experience.

### 8. References

[1] Y. Amir, L. E. Moser, P. M. Melliar-Smith, D. A. Agarwal, P. Ciarfella, “The Totem Single-Ring Ordering and Membership Protocol”, ACM Transactions on Computer Systems, vol. 13(4), Nov. 1995, pp.311-342

[2] P. Anderson, P. Goldsack, J. Paterson, “SmartFrog meets LCFG - Autonomous Reconfiguration with Central Policy Control”, Proceedings of the 2003 Large Installations Systems Administration (LISA) Conference, Oct. 2003

[3] K. P. Birman, T. A. Joseph, “Reliable Communication in the Presence of Failures”, ACM Transactions on Computer Systems, vol. 5(1), Feb. 1987, pp.47-76

[4] F. Cristian, C. Fetzer, “The Timed Asynchronous Distributed System Model”, Proceedings of the 28th Annual International Symposium on Fault-Tolerant Computing, 1998

[5] C. Fetzer, F. Cristian, “Fail-Awareness in Timed Asynchronous Systems”, Proceedings of the 15th ACM Symposium on Principles of Distributed Computing, May 1996, pp.314-321

[6] C. Fetzer, F. Cristian, “A Fail-Aware Membership Service”, Proceedings of the Sixteenth Symposium on Reliable Distributed Systems, Oct. 1997, pp.157-164

[7] I. Foster, C. Kesselman, J. Nick, S. Tuecke, “Grid Services for Distributed System Integration”, Computer, vol. 35(6), June 2002, pp.37-46

[8] Hewlett-Packard, Servicing the Animation Industry: HP’s Utility Rendering Service Provides On-Demand Computing Resources, http://www.hpl.hp.com/SE3D, 2004

[9] J. O. Kephart, D. M. Chess, “The Vision of Autonomic Computing”, Computer, vol. 36(1), Jan. 2003, pp.41-50

[10] J. Mayo, P. Kearns, “Global Predicates in Rough Real Time”, Proceedings. Seventh IEEE Symposium on Parallel and Distributed Processing, Oct. 1995, pp.17-24

[11] S. Mishra, C. Fetzer, F. Cristian, “The Timewheel Group Communication System”, IEEE Transactions on Computers, vol. 58(8), Aug. 2002, pp.883-899

[12] P. Murray, “The Anubis Service”, Hewlett-Packard Laboratories Technical Report, 2005

[13] B. Oki, M. Pfluegl, A. Siegel, D. Skeen, “The Information Bus – An Architecture for Extensible Distributed Systems”, Proceedings of the 14th ACM Symposium on Operating System Principles, Dec. 1993, pp.58-68

[14] SmartFrog Reference Manual v3.02, http://www.smartfrog.org/, July 2004

[15] J. Wilkes, J. Mogul, J. Suermondt, “Utilification”, Proceedings of the 11th ACM SIGOPS European Workshop, Sept. 2004

Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05)  
0-7695-2282-3/05 $20.00 © 2005 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 11:53:45 UTC from IEEE Xplore. Restrictions apply.