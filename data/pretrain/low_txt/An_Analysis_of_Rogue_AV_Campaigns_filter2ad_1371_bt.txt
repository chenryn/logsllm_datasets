### Network and Distributed System Security (NDSS 2006) (February 2006)
C.V. Wright et al.

1. Small, S., Mason, J., Monrose, F., Provos, N., Stubblefield, A.: To Catch a Predator: A Natural Language Approach for Eliciting Malicious Payloads. In: Proceedings of the 17th USENIX Security Symposium (August 2008).

2. Wang, K.: Using HoneyClients to Detect New Attacks. In: RECON Conference (June 2005).

3. Wang, Y.M., Beck, D., Jiang, X., Roussev, R., Verbowski, C., Chen, S., King, S.: Automated Web Patrol with Strider HoneyMonkeys: Finding Websites That Exploit Browser Vulnerabilities. In: Proceedings of the 13th Annual Symposium on Network and Distributed System Security (NDSS 2006) (February 2006).

4. Sanders, M.: autopy: A Simple, Cross-Platform GUI Automation Toolkit for Python. Available at: <http://github.com/msanders/autopy>

5. Yeh, T., Chang, T.H., Miller, R.C.: Sikuli: Using GUI Screenshots for Search and Automation. In: Proceedings of the 22nd Symposium on User Interface Software and Technology (October 2009).

6. Kleek, M.V., Bernstein, M., Karger, D., Schraefel, M.C.: Getting to Know You Gradually: Personal Lifetime User Modeling (PLUM). Technical Report, MIT CSAIL (April 2007).

7. Simpson, C.R., Reddy, D., Riley, G.F.: Empirical Models of TCP and UDP End-User Network Traffic from NETI@home Data Analysis. In: 20th International Workshop on Principles of Advanced and Distributed Simulation (May 2006).

8. Kurz, C., Hlavacs, H., Kotsis, G.: Workload Generation by Modeling User Behavior in an ISP Subnet. In: Proceedings of the International Symposium on Telecommunications (August 2001).

9. tcpreplay by Aaron Turner. Available at: <http://tcpreplay.synfin.net/>

10. Hong, S.S., Wu, S.F.: On Interactive Internet Traffic Replay. In: Proceedings of the 9th International Symposium on Recent Advances in Intrusion Detection (September 2006).

11. Sommers, J., Barford, P.: Self-configuring Network Traffic Generation. In: Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement, pp. 68–81 (2004).

12. Cao, J., Cleveland, W.S., Gao, Y., Jeffay, K., Smith, F.D., Weigle, M.C.: Stochastic Models for Generating Synthetic HTTP Source Traffic. In: INFOCOM (2004).

13. Weigle, M.C., Adurthi, P., Hernández-Campos, F., Jeffay, K., Smith, F.D.: Tmix: A Tool for Generating Realistic TCP Application Workloads in ns-2. ACM SIGCOMM Computer Communication Review 36(3), 65–76 (2006).

14. Lan, K.C., Heidemann, J.: Rapid Model Parameterization from Traffic Measurements. ACM Transactions on Modeling and Computer Simulation (TOMACS) 12(3), 201–229 (2002).

15. Vishwanath, K.V., Vahdat, A.: Realistic and Responsive Network Traffic Generation. In: Proceedings of ACM SIGCOMM (September 2006).

16. Sommers, J., Yegneswaran, V., Barford, P.: Toward Comprehensive Traffic Generation for Online IDS Evaluation. Technical Report, University of Wisconsin (2005).

17. Mutz, D., Vigna, G., Kemmerer, R.: An Experience Developing an IDS Stimulator for the Black-Box Testing of Network Intrusion Detection Systems. In: Proceedings of the Annual Computer Security Applications Conference (December 2003).

18. Kayacik, H.G., Zincir-Heywood, N.: Generating Representative Traffic for Intrusion Detection System Benchmarking. In: Proceedings of the 3rd Annual Communication Networks and Services Research Conference, pp. 112–117 (May 2005).

19. Sommers, J., Yegneswaran, V., Barford, P.: A Framework for Malicious Workload Generation. In: Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement, pp. 82–87 (2004).

20. Hunt, G., Brubacher, D.: Detours: Binary Interception of Win32 Functions. In: Third USENIX Windows NT Symposium (July 1999).

21. Klimt, B., Yang, Y.: Introducing the Enron Corpus. In: Proceedings of the First Conference on Email and Anti-Spam (CEAS) (July 2004).

22. Paxson, V., Floyd, S.: Wide Area Traffic: The Failure of Poisson Modeling. IEEE/ACM Transactions on Networking 3(3) (June 1995).

23. Matsumoto, M., Nishimura, T.: Mersenne Twister: A 623-dimensionally equidistributed uniform pseudo-random number generator. ACM Transactions on Modelling and Computer Simulation 8(1), 3–30 (1998).

24. GINA: MSDN Windows Developer Center. Available at: <http://msdn.microsoft.com/en-us/library/aa375457VS.85.aspx>

25. Hibler, M., Ricci, R., Stoller, L., Duerig, J., Guruprasad, S., Stack, T., Webb, K., Lepreau, J.: Large-scale Virtualization in the Emulab Network Testbed. In: Proceedings of the 2008 USENIX Annual Technical Conference (June 2008).

26. Google, Inc.: Google Search Appliance. Available at: <http://www.google.com/enterprise/search/gsa.html>

27. osCommerce: Open Source E-Commerce Solutions. Available at: <http://www.oscommerce.com/>

28. DMOZ Open Directory Project. Available at: <http://www.dmoz.org/>

29. Yahoo! Directory. Available at: <http://dir.yahoo.com/>

30. Alexa Top Sites. Available at: <http://www.alexa.com/topsites>

31. AV-Comparatives e.V.: Anti-Virus Comparative Performance Test: Impact of Anti-Virus Software on System Performance (December 2009). Available at: <http://www.av-comparatives.org/comparativesreviews/performance-tests>

32. Warner, O.: What Really Slows Windows Down (September 2006). Available at: <http://www.thepcspy.com/read/what_really_slows_windows_down>

33. Chatterton, D., Gigante, M., Goodwin, M., Kavadias, T., Keronen, S., Knispel, J., McDonell, K., Matveev, M., Milewska, A., Moore, D., Muehlebach, H., Rayner, I., Scott, N., Shimmin, T., Schultz, T., Tuthill, B.: Performance Co-Pilot for IRIX Advanced User’s and Administrator’s Guide. 2.3 edn. SGI Technical Publications (2002). Available at: <http://oss.sgi.com/projects/pcp/index.html>

34. Timekeeping in VMware Virtual Machines. Available at: <http://www.vmware.com/pdf/vmware_timekeeping.pdf>

---

### On Challenges in Evaluating Malware Clustering

**Authors:**
- Peng Li, Department of Computer Science, University of North Carolina, Chapel Hill, NC, USA
- Limin Liu, State Key Lab of Information Security, Graduate School of Chinese Academy of Sciences
- Debin Gao, School of Information Systems, Singapore Management University, Singapore
- Michael K. Reiter, Department of Computer Science, University of North Carolina, Chapel Hill, NC, USA

**Abstract:**
Malware clustering and classification are essential tools that help analysts prioritize their malware analysis efforts. The recent emergence of fully automated methods for malware clustering and classification, which report high accuracy, suggests that this problem may largely be solved. In this paper, we investigate our hypothesis that the method of selecting ground-truth data in prior evaluations biases results toward high accuracy. To examine this, we apply clustering algorithms from a different domain (plagiarism detection) to the dataset used in a previous study and then to a new malware dataset. Our findings provide conflicting signals regarding the correctness of our hypothesis, but our investigation uncovers a cautionary note about the significance of highly accurate clustering results, particularly when testing on a dataset with a biased cluster-size distribution.

**Keywords:**
- Malware clustering and classification
- Plagiarism detection

### 1. Introduction

The dramatic increase in the number of malware variants has driven the development of methods to classify and group them, enabling analysts to focus on truly new threats. For example, the Bagle/Beagle malware had approximately 30,000 distinct variants between January 9 and March 6, 2007 [8]. While initial attempts at malware classification were manual, numerous automated methods have been developed in recent years, some of which claim very high accuracy, leading to the belief that malware classification is more or less solved.

In this paper, we show that this may not be the case and that evaluating automated malware classifiers poses significant challenges. A central challenge is the lack of a well-defined notion of when two malware instances are "the same" or "different," making it difficult to obtain ground truth for comparison. Even manually encoded rules to classify malware are not sufficient; a previous study [6] found that a majority of six commercial anti-virus scanners agreed on the classification of 14,212 malware instances in only 2,658 cases. However, these instances and their corresponding classifications are increasingly used to evaluate automated methods of malware clustering. For example, a state-of-the-art malware clustering algorithm by Bayer et al. [6] achieved excellent results using these 2,658 malware instances as ground truth, agreeing with the clustering by the six anti-virus tools.

We conjecture that one factor contributing to these strong results might be that these 2,658 instances are simply easy to classify. To examine this, we repeated the clustering of these instances using algorithms from a different domain, specifically plagiarism detectors that employ dynamic analysis. Since plagiarism detectors are developed without attention to the specifics of malware obfuscation, highly accurate clustering results by these tools might suggest that the method of selecting ground-truth data biases the data toward easy-to-classify instances. Our results indicate that plagiarism detectors have nearly the same success in clustering these malware instances, providing tentative support for this conjecture.

To further investigate this possibility, we attempted to repeat the evaluation methodology of Bayer et al. on a new set of malware instances. By drawing from a database of malware, we assembled a set for which four anti-virus tools consistently labeled each member. Surprisingly, neither the Bayer et al. technique nor the plagiarism detectors we employed were particularly accurate in clustering these instances. Due to certain caveats, this evaluation is materially different from the previous one, causing us to be somewhat tentative in our conclusions. Nevertheless, these results temper our confidence in cautioning that the selection of ground-truth data based on the concurrence of multiple anti-virus tools biases the data toward easy-to-classify instances.

This leaves the intriguing question: Why the different results on the two datasets? We complete our paper with an analysis of a factor that, we believe, contributes to (though does not entirely explain) this discrepancy, and that offers a cautionary note for the evaluation of malware clustering results. This factor is the makeup of the ground-truth dataset, in terms of the distribution of the sizes of the malware families it contains. We observe that the original dataset, on which the algorithms perform well, is dominated by two large families, while the second dataset is more evenly distributed among many families. This factor alone biases the measures used in comparing the malware clustering output to the dataset families, specifically precision and recall, increasing the likelihood of good precision and recall numbers occurring by chance. As such, the biased cluster-size distribution in the original dataset erodes the significance of the high precision and recall reported by Bayer et al. [6]. This observation identifies an important factor to control when measuring the effectiveness of a malware clustering technique.

While we focus on a single malware classifier for our analysis [6], we do so because very good accuracy has been reported for this algorithm and because the authors of that technique were very helpful in enabling us to compare with their technique. We emphasize that our comparisons to plagiarism detectors are not intended to suggest that plagiarism detectors are the equal of this technique. The technique of Bayer et al. is far more scalable than any of the plagiarism detectors we consider here, an important consideration when clustering potentially tens of thousands of malware instances. Additionally, the similar accuracy of the technique of Bayer et al. to the plagiarism detectors does not rule out the possibility that the plagiarism detectors are more easily evaded; rather, it simply indicates that malware today does not seem to do so. We stress that the issues we identify are not a criticism of the Bayer et al. technique, but rather are issues worth considering for any evaluation of malware clustering and classification.

**Contributions:**
1. We explore the possibility that existing approaches to obtaining ground-truth data for malware clustering evaluation bias results by isolating those instances that are simple to cluster or classify. Our study is inconclusive on this topic, but reporting our experiences will raise awareness of this possibility and underline the importance of finding methods to validate the ground-truth data employed in this domain.
2. We highlight the importance of the significance of positive clustering results when reporting them. This has implications for the datasets used to evaluate malware clustering algorithms, requiring that datasets exhibiting a biased cluster-size distribution not be used as the sole vehicle for evaluating a technique.

### 2. Classification and Clustering of Malware

To hinder static analysis of binaries, most current malware uses obfuscation techniques, notably binary packers. Dynamic analysis, which monitors the behavior of the binary during execution, is often more effective than static analysis. While dynamic analysis has its limitations, it is more effective than purely static approaches. For this reason, dynamic analysis of malware has received much attention in the research community. Analysis systems such as CWSandbox [25], Anubis [7], BitBlaze [18], Norman [2], and ThreatExpert [1] execute malware samples within an instrumented environment and monitor their behaviors for analysis and defense mechanism development.

A common application for dynamic analysis of malware is to group malware instances, making it easier to identify new strains. Such grouping is often performed using machine learning, either by clustering (e.g., [6,17,15]) or by classification (e.g., [13,5,16,11]), which are unsupervised and supervised techniques, respectively.

Of primary interest in this paper are the methodologies these works employ to evaluate the results of learning, and specifically the measures of quality for the clustering or classification results. Let \( M \) denote a collection of \( m \) malware instances to be clustered, or the "test data" in the case of classification. Let \( C = \{C_i\}_{1 \leq i \leq c} \) and \( D = \{D_i\}_{1 \leq i \leq d} \) be two partitions of \( M \), and let \( f: \{1, \ldots, c\} \to \{1, \ldots, d\} \) and \( g: \{1, \ldots, d\} \to \{1, \ldots, c\} \) be functions. Many prior techniques evaluated their results using two measures:

\[
\text{prec}(C, D) = \frac{1}{m} \sum_{i=1}^{c} |C_i \cap D_{f(i)}|
\]

\[
\text{recall}(C, D) = \frac{1}{m} \sum_{i=1}^{d} |C_{g(i)} \cap D_i|
\]

where \( C \) is the set of clusters resulting from the technique being evaluated and \( D \) is the clustering that represents the "right answer."

More specifically, in the case of classification, \( C_i \) is all test instances classified as class \( i \), and \( D_i \) is all test instances that are "actually" of class \( i \). Therefore, in the case of classification, \( c = d \) and \( f \) and \( g \) are the identity functions. As a result, \( \text{prec}(C, D) = \text{recall}(C, D) \), and this measure is often simply referred to as accuracy. This is the measure used by Rieck et al. [16] to evaluate their malware classifier, and Lee et al. [13] similarly uses error rate, or one minus the accuracy.

In the clustering case, there is no explicit label to define the cluster in \( D \) that corresponds to a specific cluster in \( C \), and so one approach is to define:

\[
f(i) = \arg\max_{i'} |C_i \cap D_{i'}|
\]

\[
g(i) = \arg\max_{i'} |C_{i'} \cap D_i|
\]

In this case, \( f \) and \( g \) will not generally be the identity function (or even bijections), and so precision and recall are different. This approach is used by Rieck et al. [17] and Bayer et al. [6] in evaluating their clustering techniques. When it is desirable to reduce these two measures into one, a common approach (e.g., [17]) is to use the F-measure:

\[
\text{F-measure}(C, D) = \frac{2 \cdot \text{prec}(C, D) \cdot \text{recall}(C, D)}{\text{prec}(C, D) + \text{recall}(C, D)}
\]

This background is sufficient to highlight the issues on which we focus in the paper:

- **Production of \( D \):** A central question in the measurement of precision and recall is how the reference clustering \( D \) is determined. A common practice is to use an existing anti-virus tool to label the malware instances \( M \) (e.g., [16,13,11]), the presumption being that anti-virus tools embody hand-coded rules to label malware instances and so are a good source of "manually verified" ground truth. Unfortunately, existing evidence suggests otherwise, in that it has been shown that anti-virus engines often disagree on their labeling (and clustering) of malware instances [5]. To compensate for this, another practice has been to restrict attention to malware instances \( M \) on which multiple anti-virus tools agree (e.g., [6]). Aside from substantially reducing the number of instances, we conjecture that this practice might contribute to more favorable evaluations of malware classifiers, essentially by limiting evaluations to easy-to-cluster instances. To demonstrate this possibility, in Section 3 we consider malware instances selected in this way and show that they can be classified by plagiarism detectors (designed without attention to the subtleties of malware obfuscation) with precision and recall comparable to that offered by a state-of-the-art malware clustering tool.
- **Distribution of cluster sizes in \( C \) and \( D \):** In order to maximize both precision and recall (and hence the F-measure), it is necessary for \( C \) and \( D \) to exhibit similar cluster-size distributions; i.e., if one of them is highly biased (i.e., has few, large clusters) and the other is more evenly distributed, then one of precision or recall will suffer. Even when they exhibit similar cluster-size distributions, however, the degree to which that distribution is biased has an effect on the significance (e.g., [22, Section 8.5.8]) that one can ascribe to high values of these measures. Informally, the significance of a given precision or recall is related to the probability that this value could have occurred by random chance; the higher the probability, the less the significance. We will explore the effect of cluster-size distribution on significance, and specifically the impact of cluster-size distribution on the sensitivity of the F-measure to perturbations in the distance matrix from which the clustering \( C \) is derived. We will see that all other factors held constant, good precision and recall when the reference clusters in \( D \) are of similar size is more significant than if the cluster sizes are biased. That is, small perturbations in the distance matrix yielding \( C \) tend to decay precision and recall more than if \( D \) and \( C \) are highly biased.

We will demonstrate this phenomenon using the malware clustering results obtained from the state-of-the-art malware clustering tool due to Bayer et al., which obtains very different results on two malware datasets, one with a highly biased clustering and one with a more even clustering. While this is not the only source of variation in the datasets, and so the different results cannot be attributed solely to differences in cluster size distributions, we believe that the cluster size distribution is a factor that must be taken into account when reporting malware clustering results.

### 3. A Potential Hazard of Anti-virus Voting

As discussed in Section 2, a common practice to produce the ground-truth reference clustering \( D \) for evaluating malware clustering algorithms is to use existing anti-virus tools to label the malware instances and to restrict attention to malware instances \( M \) on which multiple anti-virus tools agree. The starting point of our study is one such ground-truth dataset, here denoted BCHKK-data, that was used by Bayer et al. for evaluating their malware clustering technique [6]. Using this dataset, their algorithm, here denoted BCHKK-algo, yielded a very good precision and recall (of 0.984 and 0.930, respectively). BCHKK-data consists of 2,658 malware instances, which is a subset of 14,212 malware instances contributed between October 27, 2007, and January 31, 2008, by a number of security organizations and individuals, spanning a wide range of sources (such as web infections, honeypots, botnet monitoring, and other malware analysis services). Bayer et al. ran six different anti-virus programs on these 14,212 instances, and a subset of 2,658 instances on which results from the majority of these anti-virus programs agree were chosen to form BCHKK-data for evaluation of their clustering technique BCHKK-algo. Bayer et al. explained that such a subset was chosen because they are the instances on which ground truth can be obtained (due to agreement by a majority of the anti-virus programs they used).

This seems to be a natural way to pick \( M \) for evaluation, as they are the only ones for which the ground-truth clustering (i.e., \( D \)) could be obtained with good confidence. However, this also raises the possibility that the instances on which multiple anti-virus tools agree are just the malware instances that are relatively easy to cluster, while the difficult-to-cluster instances are filtered out of \( M \). If this were the case, then this could contribute to the high precision and recall observed for the BCHKK-data dataset, in particular.

Unfortunately, we are unaware of any accepted methodology for testing this possibility directly. So, we instead turn to another class of clustering tools derived without attention to malware clustering, in order to see if they are able to cluster the malware instances in BCHKK-data equally well. Specifically, we apply plagiarism detectors to the BCHKK-data to see if they can obtain good precision and recall.