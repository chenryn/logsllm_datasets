### Certified Defenses and Their Scalability

Certified defenses modify the neural network training process to minimize robustness violations [65, 52, 12]. While these approaches are promising, they do not yet scale to larger networks such as Google Inception [65, 52]. All published certified defenses have been evaluated on small models and datasets [65, 52, 12, 43], and in at least one case, the authors acknowledge that some components of their defense would be "completely infeasible" on ImageNet [65]. A recent paper [16] presents a certified defense evaluated on the CIFAR-10 dataset [33] for multi-layer DNNs (but smaller than ResNets). Their approach is fundamentally different from ours, and based on current results, there is no evidence that it can readily scale to large datasets like ImageNet.

Another approach [53] combines robust optimization and adversarial training to provide formal guarantees with lower computational complexity, potentially scaling better. This method requires smooth DNNs (e.g., no ReLU or max pooling) and provides robustness guarantees over the expected loss (e.g., log loss). In contrast, PixelDP can certify each specific prediction and offers intuitive metrics like robust accuracy, which are not supported by [53]. Unlike PixelDP, which we evaluated on five datasets of increasing size and complexity, this technique was only evaluated on MNIST, a small dataset known for its amenability to robust optimization due to its nearly black-and-white nature. Given that the effectiveness of all defenses depends on the model and dataset, it is challenging to conclude how well it will perform on more complex datasets.

### Formal Verification and Robustness Lower Bounds

Several works aim to formally verify [26, 30, 61, 62, 15, 20, 58] or establish lower bounds [49, 63] on the robustness of pre-trained ML models against adversarial attacks. Some of these methods scale to large networks [49, 63], but they are insufficient from a defensive perspective as they do not provide a scalable way to train robust models.

### Differentially Private Machine Learning

Significant research focuses on making ML algorithms differentially private (DP) to preserve the privacy of training sets [40, 1, 9]. PixelDP is orthogonal to these works, differing in goals, semantics, and algorithms. The only shared aspect is the use of DP theory and mechanisms. The goal of DP ML is to learn model parameters while ensuring DP with respect to the training data. Public release of model parameters trained using a DP learning algorithm (such as DP empirical risk minimization or ERM) is guaranteed not to reveal much information about individual training examples. In contrast, PixelDP aims to create a robust predictive model where small changes to any input example do not drastically alter the model's prediction. We achieve this by ensuring that the model's scoring function is a DP function with respect to the features of an input example (e.g., pixels). DP ML algorithms (e.g., DP ERM) do not necessarily produce models that satisfy PixelDP's semantic, and our training algorithm for producing PixelDP models does not ensure DP of training data.

### Previous Connections Between DP and Robustness

Previous work has studied the generalization properties of DP [4], showing that learning algorithms satisfying DP with respect to the training data have statistical benefits in terms of out-of-sample performance. It has also been shown that DP has a deep connection to robustness at the dataset level [14, 17]. Our work differs in that our learning algorithm is not DP; rather, the predictor we learn satisfies DP with respect to the atomic units (e.g., pixels) of a given test point.

### Conclusion

We have demonstrated a connection between robustness against adversarial examples and differential privacy theory. We showed how this connection can be leveraged to develop a certified defense against such attacks that is (1) as effective at defending against 2-norm attacks as today’s state-of-the-art best-effort defense and (2) more scalable and broadly applicable to large networks compared to any prior certified defense. Finally, we presented the first evaluation of a certified 2-norm defense on the large-scale ImageNet dataset. In addition to offering encouraging results, the evaluation highlighted the substantial flexibility of our approach by leveraging a convenient autoencoder-based architecture to make the experiments possible with limited resources.

### Acknowledgments

We thank our shepherd, Abhi Shelat, and the anonymous reviewers, whose comments helped us improve the paper significantly. This work was funded through NSF CNS-1351089, CNS-1514437, and CCF-1740833, ONR N00014-17-1-2010, two Sloan Fellowships, a Google Faculty Fellowship, and a Microsoft Faculty Fellowship.

### References

[1] M. Abadi, A. Chu, I. Goodfellow, H. Brendan McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep Learning with Differential Privacy. ArXiv e-prints, 2016.
[2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. 2018.
[3] A. Athalye and I. Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.
[4] R. Bassily, K. Nissim, A. Smith, T. Steinke, U. Stemmer, and J. Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, 2016.
[5] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. End to end learning for self-driving cars. CoRR, 2016.
[6] N. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017.
[7] N. Carlini and D. A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), 2017.
[8] K. Chatzikokolakis, M. E. Andrés, N. E. Bordenabe, and C. Palamidessi. Broadening the scope of differential privacy using metrics. In International Symposium on Privacy Enhancing Technologies Symposium, 2013.
[9] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. J. Mach. Learn. Res., 2011.
[10] Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten. Countering adversarial images using input transformations. International Conference on Learning Representations, 2018.
[11] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille. Mitigating adversarial effects through randomization. International Conference on Learning Representations, 2018.
[12] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to adversarial examples. In Proceedings of the 34th International Conference on Machine Learning, 2017.
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 2009.
[14] C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. Rubinstein. Bayesian Differential Privacy through Posterior Sampling. arXiv preprint arXiv:1306.1066v5, 2016.
[15] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output range analysis for deep feedforward neural networks. In NASA Formal Methods Symposium, 2018.
[16] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O’Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned verifiers. ArXiv e-prints, 2018.
[17] C. Dwork and J. Lei. Differential privacy and robust statistics. In Proceedings of the forty-first annual ACM symposium on Theory of computing, 2009.
[18] C. Dwork, A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends R in Theoretical Computer Science, 2014.
[19] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust physical-world attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017.
[20] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. AI2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE Symposium on Security and Privacy (SP), 2018.
[21] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the 3rd ICLR, 2015.
[22] Google. Inception v3. https://github.com/tensorflow/models/tree/master/research/inception. Accessed: 2018.
[23] Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Zachary C. Lipton, Animashree Anandkumar. Stochastic activation pruning for robust adversarial defense. International Conference on Learning Representations, 2018.
[24] D. Hendrycks and K. Gimpel. Early methods for detecting adversarial images. In ICLR (Workshop Track), 2017.
[25] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 1963.
[26] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety verification of deep neural networks. In Proceedings of the 29th International Conference on Computer Aided Verification, 2017.
[27] A. Ilyas, A. Jalal, E. Asteri, C. Daskalakis, and A. G. Dimakis. The robust manifold defense: Adversarial training using generative models. CoRR, abs/1712.09196, 2017.
[28] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.
[29] Jacob Buckman, Aurko Roy, Colin Raffel, Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. International Conference on Learning Representations, 2018.
[30] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. CoRR, 2017.
[31] J. Kos, I. Fischer, and D. Song. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832, 2017.
[32] Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. arXiv preprint arXiv:1705.06452, 2017.
[33] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
[34] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint 1607.02533, 2016.
[35] X. Liu, M. Cheng, H. Zhang, and C. Hsieh. Towards robust neural networks via random self-ensemble. Technical report, 2017.
[36] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles. CVPR, 2017.
[37] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017.
[38] Madry Lab. CIFAR-10 Adversarial Examples Challenge. https://github.com/MadryLab/cifar10-challenge. Accessed: 1/22/2017.
[39] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample-variance penalization. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009.
[40] F. McSherry and I. Mironov. Differentially private recommender systems: Building privacy into the Netflix prize contenders. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009.
[41] D. Meng and H. Chen. Magnet: A two-pronged defense against adversarial examples. In CCS, 2017.
[50] K. Pei, Y. Cao, J. Yang, and S. Jana. Deepxplore: Automated whitebox testing of deep learning systems. In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP), 2017.
[51] Pouya Samangouei, Maya Kabkab, Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. International Conference on Learning Representations, 2018.
[52] A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.
[53] A. Sinha, H. Namkoong, and J. Duchi. Certifying Some Distributional Robustness with Principled Adversarial Training. 2017.
[54] Y. Song, R. Shu, N. Kushman, and S. Ermon. Generative adversarial examples. 2018.
[55] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.
[56] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Proceedings of the 2nd International Conference on Learning Representations, 2014.
[57] TensorFlow r1.5. Resnet models. https://github.com/tensorflow/models/tree/r1.5/research/resnet, 2017.
[42] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On detecting adversarial perturbations. In Proceedings of the 6th International Conference on Learning Representations, 2017.
[58] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness of neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.
[43] M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably robust neural networks. In International Conference on Machine Learning (ICML), 2018.
[59] F. Tramer, A. Kurakin, N. Papernot, D. Boneh, and P. D. McDaniel. Ensemble adversarial training: Attacks and defenses. CoRR, abs/1705.07204, 2017.
[44] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning.
[45] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Proc. of IEEE Symposium on Security and Privacy (Oakland), 2016.
[60] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 2010.
[61] S. Wang, K. Pei, W. Justin, J. Yang, and S. Jana. Efficient