### 5.5 Attack Transferability

**Setup.** In this section, we evaluate the transferability of our attack across different neural network models. We test all pairwise combinations of our models, including adversarially trained ones. Additionally, we use the Google Vision API [17] to test our projections against their proprietary models. The API returns a list of labeled objects in the image along with associated confidence scores and bounding boxes. "Stop sign" is one of the labels. We set the detection threshold for the Google Vision API at 0.01, meaning that a stop sign is considered detected if the API reports it with a confidence score greater than 0.01.

**Results.** The results are summarized in Table 4. The table lists the source (white-box) model on the left, which identifies the projection shown in the tested videos. We also report the number of frames tested, taken from the indoor experiment videos. Table 4 presents the success rates of the attack as a percentage of the frames where the stop sign was undetected. Our findings indicate that the attack transfers well in low light conditions but degrades significantly at 300 lux and above. Notably, Mask-RCNN transfers better to Yolov3 compared to the reverse, and the same trend is observed for Gtsrb-CNN and Lisa-CNN. This suggests that fitting adversarial examples (AEs) on more complex models benefits the attacker. Table 4 also shows that adversarially trained models reduce the transferability of attacks fitted on surrogate models.

### 6 Discussion

In this section, we discuss the feasibility of the attack.

**Attack Feasibility.** Our experiments demonstrate that increasing ambient light quickly diminishes the feasibility of the attack in bright conditions. Practically, during daytime, the attack could be conducted on non-bright days, such as dark overcast days or close to sunset or sunrise, when the ambient light is low (<400 lux). Regarding the effect of car headlights, our outdoor experiments show that the light emitted by car headlights is negligible compared to the projection luminosity and does not influence the attack's success. While high-beam headlights would compromise the projection appearance and degrade the attack's success rates, we did not consider these lights to be on, as stop signs are primarily found in urban areas where high-beam headlights are typically off.

The amount of projector-emitted light that reaches the sign depends on three factors: (i) the distance between the projector and the sign, (ii) the throw ratio of the projector, and (iii) the amount of lumens the projector can emit. Figure 13 illustrates how the distance between the projector and the stop sign relates to the attack's success rate. We also consider two additional projectors with long throw distances: the Panasonic PT-RZ570BU and the NEC PH1202HL1, priced at $3,200 and $44,379, respectively. Using the projectors' throw ratios (2.93 and 3.02) and their emitted lumens (5,000 and 12,000 lumens), we calculate the amount of lux of light the projector can shine on the sign surface from increasing distances. We define success as achieving 800 lux of light on the sign with the projector under 120 lux ambient light, which is sufficient for consistent attack success (see Section 5.1). Figure 13 shows that the attack can be carried out from 7.5 meters away with the weaker projector and up to 13 meters away with the more expensive one. Adversaries could also use different lenses to increase the throw ratio of cheaper projectors, similar to [32].

**Attack Generalizability.** We present results for attacks on other objects (e.g., give way sign, bottle) in Appendix A. To extend the attack to any object, the adversary must consider the distortion introduced by the projection surface, which is not necessary for flat traffic signs. The attacker will need to augment the projection model used in this paper with differentiable transformations that model the distortion caused by non-flat surfaces. Generally, the size of the projectable area limits the feasibility of the attack against certain objects (e.g., projecting on a bike is difficult). This limitation is shared across all vectors that create physically robust AEs, including adversarial patches. We also found that the material properties of the projection surface impact the attack's success: traffic signs are easier targets due to their high reflectivity. When executing the attack on other objects, certain adaptations lead to marginal improvements, particularly context information (e.g., the pole for the stop sign, the table where the bottle is placed). For object detectors, adversaries will need to tailor certain parameters of the optimization to the target object.

### 7 Conclusions

In this paper, we introduce SLAP, a new attack vector that uses a light projector to create short-lived physical adversarial examples. We investigate the attack in the context of road safety, where the attacker's goal is to change the appearance of a stop sign by shining a crafted projection onto it, making it undetectable by DNNs mounted on autonomous vehicles. Given the non-trivial physical constraints of projecting specific light patterns on various materials in various conditions, we propose a method to generate projections based on fitting a predictive three-way color model and using an AE generation pipeline that enhances the AE's robustness. We evaluated the proposed attack in a variety of light conditions, including outdoors, and against state-of-the-art object detectors (Yolov3 and Mask-RCNN) and traffic sign recognizers (Lisa-CNN and Gtsrb-CNN). Our results show that SLAP generates AEs that are robust in the real world. We also evaluated defenses, highlighting how existing defenses tailored to physical AEs do not work against AEs generated by SLAP, while finding that an adaptive defender using adversarial learning can successfully hamper the attack effect, albeit at the cost of reduced accuracy.

Despite this, the novel capability of modifying how an object is detected by DNN models, combined with the ability to carry out opportunistic attacks, makes SLAP a powerful new attack vector that requires further investigation. This paper makes an important step towards increasing awareness and further research into countermeasures against light-projection adversarial examples.

### Acknowledgements

This work was supported by grants from armasuisse, Mastercard, and the Engineering and Physical Sciences Research Council [grant numbers EP/N509711/1, EP/P00881X/1].

### References

[1] “Daylight”, [Online] Accessed: 2020-02-20. https://en.wikipedia.org/wiki/Daylight.
[2] “JPEG File Interchange Format”, [Online] Accessed: 2020-01-15. http://www.w3.org/Graphics/JPEG/jfif3.pdf.
[3] “Manual of Uniform Traffic Control Devices for Street and Highways”, [Online] Accessed: 2020-01-08. http://mutcd.fhwa.dot.gov/pdfs/2009r1r2/mutcd2009r1r2edition.pdf.
[4] “Panasonic PT-AE8000 Projector”, [Online] Accessed: 2020-10-12. http://www.projectorcentral.com/Panasonic-PT-AE8000.htm.
[5] “Sanyo PLC-XU4000 Projector”, [Online] Accessed: 2020-10-12. http://www.projectorcentral.com/Sanyo-PLC-XU4000.htm.
[6] Apollo. “ApolloAuto - An open autonomous driving platform”, [Online] Accessed: 2021-02-19. http://github.com/apolloauto.
[7] Aptina. “1/3-Inch CMOS Digital Image Sensor AR0132AT Data Sheet”, [Online] Accessed: 2021-02-19. http://datasheetspdf.com/pdf/829321/AptinaImagingCorporation/AR0132AT/1f.
[8] BMW. “BMW TechOffice Munich”, [Online] Accessed: 2020-02-19. http://github.com/BMW-InnovationLab.
[9] Andy Boxall. “From robots to projection mapping: Inside Panasonic’s Tokyo 2020 Olympic tech”, [Online] Accessed: 2021-02-19. http://www.digitaltrends.com/mobile/panasonic-tokyo-2020-technology-interview/.
[10] Tom Brown, Dandelion Mane, Aurko Roy, Martin Abadi, and Justin Gilmer. “Adversarial Patch”. arXiv preprint arXiv:1712.09665v2, 2018.
[11] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N. Balasubramanian. “Grad-cam++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks”. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV), pages 839–847, 2018.
[12] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. “Shapeshifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector”. In Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 52–68, 2018.
[13] E. Chou, F. Tramèr, and G. Pellegrino. “SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems”. In Proceedings of the IEEE Security and Privacy Workshops (SPW), pages 48–54, 2020.
[14] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. “Exploring the Landscape of Spatial Robustness”. In Proceedings of the International Conference on Machine Learning (ICML), pages 1802–1811, 2019.
[15] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. “Robust physical-world attacks on deep learning visual classification”. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1625–1634, 2018.
[16] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation”. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 580–587, 2014.
[17] Google. “Google Vision API”, Accessed: 2020-10-12. https://cloud.google.com/vision.
[18] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. “BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain”. arXiv preprint arXiv:1708.06733, 2017.
[19] Phil Hall. “The Exposure Triangle: Aperture, Shutter Speed and ISO explained”, [Online] Accessed: 2021-02-19. http://www.techradar.com/uk/how-to/the-exposure-triangle.
[20] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. “Mask R-CNN”. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2961–2969, 2017.
[21] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. “Detection of Traffic Signs in Real-World Images: The German Traffic Sign Detection Benchmark”. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 1–8, 2013.
[22] Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Viégas, and Michael Terry. “Xrai: Better Attributions through Regions”. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pages 4948–4957, 2019.
[23] Danny Karmon, Daniel Zoran, and Yoav Goldberg. “Lavan: Localized and visible adversarial noise”. In Proceedings of the International Conference on Machine Learning (ICML), pages 2507–2515, 2018.
[24] Sebastian Köhler, Giulio Lovisotto, Simon Birnbach, Richard Baker, and Ivan Martinovic. “They See Me Rollin’: Inherent Vulnerability of the Rolling Shutter in CMOS Image Sensors”. arXiv preprint arXiv:2101.10011, 2021.
[25] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. “Adversarial Examples in the Physical World”. arXiv preprint arXiv:1607.02533, 2016.
[26] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. “Feature Pyramid Networks for Object Detection”. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2117–2125, 2017.
[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. “Microsoft COCO: Common Objects in Context”. In Proceedings of the European Conference on Computer Vision (ECCV), pages 740–755, 2014.
[28] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Authors Yingqi Liu, Weihang Wang, and Xiangyu Zhang. “Trojaning Attack on Neural Networks”. In Proceedings of the Network and Distributed System Symposium (NDSS), 2018.
[29] Giulio Lovisotto, Simon Eberz, and Ivan Martinovic. “Biometric Backdoors: A Poisoning Attack Against Unsupervised