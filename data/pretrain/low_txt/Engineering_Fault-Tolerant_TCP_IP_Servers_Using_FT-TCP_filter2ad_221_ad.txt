### Connection and Data Packet Transmission Analysis

During the experiment, we observed the transmission of data packets from the client and acknowledgment (ACK) packets from the server. Approximately one second into the experiment, the primary host crashed, causing the server to cease sending ACKs. This, in turn, led the client to stop transmitting data when its TCP window filled up. About 300 milliseconds later, the client's retransmission timer triggered, and it attempted to resend the packet following the last acknowledged one, as indicated by a dip in the line.

For the purpose of analysis, we deliberately extended the recovery time to 2.5 seconds. Consequently, the client continued to retransmit unacknowledged packets at exponentially increasing intervals for three more rounds. By the fourth round, 4.8 seconds into the experiment, the backup was ready, and the retransmission succeeded, allowing the data flow to resume.

The actual recovery of the backup is marked by an ACK packet visible around 3.6 seconds. However, this ACK did not successfully restart the data flow because it acknowledged an older packet that the client had already considered acknowledged. The length of the retransmission gap between the actual recovery and the resumption of data flow depends on the point in the retransmission cycle where recovery occurs. This gap can be very short if the next retransmission follows soon after recovery, or it can be as long as 64 seconds, the maximum TCP retransmission period, if the service recovers just after a retransmission.

To eliminate this retransmission gap, the backup must receive all packets sent by the client. This can be achieved by setting the backup’s network card to promiscuous mode at the beginning of the connection, allowing it to snoop packets from the network shared by the client and the replicas. When the backup detects a failure, it can process the snooped packets, acknowledge them, and immediately restart the data flow, as shown in Figure 5. With this method, the failover time is limited only by the failure detection delay. From Table 1, the average Round-Trip Time (RTT) for messages between the replicas is about 0.5 milliseconds, suggesting a reasonable failure detection timeout of 1-2 milliseconds. However, the FT-TCP implementation relies on Linux kernel timers with a granularity of 10 milliseconds, making that the minimal failure-detection latency and, consequently, the minimal failover time for our hot backup.

### Snooping Strategies

While permanent snooping ensures the fastest possible failover, it may impose a heavy load on the backup machine, especially on a busy network. Therefore, a reactive snooping approach is worth considering. In this scheme, the network card operates normally during failure-free operation but switches to promiscuous mode upon detecting a failure. Reactive snooping is beneficial when the failure detection latency is shorter than the TCP retransmission delay (200 milliseconds), but the promotion latency is longer. Starting to snoop before the first retransmission allows the backup to collect all lost packets and restart the data flow as soon as the promotion is complete, as seen around 3.4 seconds in Figure 6. For backups with short promotion latencies, the decision is whether to snoop permanently or not at all, balancing good failure-free performance and short failover times.

### Conclusion

Our earlier work on FT-TCP [2] demonstrated the feasibility of a server-side recovery approach to mask the failure of a TCP-based server from its clients. In this paper, we addressed the practical performance of FT-TCP by applying it to two existing services, Samba and DSS, and evaluating its impact on both failure-free execution and executions with failures. We also explored methods to reduce the failover time when recovering a TCP connection. Our findings include:

- **Code Modifications**: Modifying the code of existing services to support FT-TCP was necessary but minimal. For Samba, nonces and file handles are generated, and for DSS, session IDs are generated. Adding a protocol-specific "hook" could simplify ensuring that the backup makes the same nondeterministic choices as the primary.
- **Failure-Free Overhead**: The overhead of FT-TCP in failure-free scenarios is very low, with no new scalability issues. The maximum throughput overhead was under 2% for large file transfers to a Samba server.
- **Backup Performance**: The performance of a hot backup with FT-TCP is nearly indistinguishable from that of a cold backup. For cold backups, we did not checkpoint the service, meaning it would recover from its initial state; checkpointing could significantly impact server performance.
- **Failover Time**: The failover time of FT-TCP can be very short, but it requires the backup to snoop on incoming traffic by setting its network interface to promiscuous mode. For servers with large promotion latencies, the backup should start snooping when it suspects a failure. If the promotion latency is under 200 milliseconds, the backup should start snooping as soon as it starts executing. While snooping enhances performance, it is not required for server-side recovery.

We have evaluated two services that do not impose a significant computational overhead on the server processor. Future work will focus on services with higher computational demands, which are less common in practice.

### References

[1] N. Aghdaie and Y. Tamir. Implementation and evaluation of transparent fault-tolerant web service with kernel-level support. In Proc. IEEE Intl. Conf. on Computer Communications and Networks, 2002.

[2] L. Alvisi, T. Bressoud, A. El-Khashab, K. Marzullo, and D. Zagorodnov. Wrapping server-side TCP to mask connection failures. In Proc. IEEE INFOCOM 2001, pages 329–337, 2001.

[3] T. Bressoud and F. Schneider. Hypervisor-based fault tolerance. ACM Trans. on Computer Systems, 14(1):80–107, 1996.

[4] N. Budhiraja, K. Marzullo, F. Schneider, and S. Toueg. Primary–backup protocols: Lower bounds and optimal implementations. In Proc. 3rd IFIP Conf. on Dependable Computing for Critical Applications, 1992.

[5] D. Dolev, D. Malki, and Y. Yarom. Warm backup using snooping. In Proc. 1st Intl. Workshop on Services in Distributed and Networked Environments (SDNE), pages 60–65, 1994.

[6] E. Elnozahy, L. Alvisi, Y. Wang, and D. Johnson. A survey of rollback-recovery protocols in message passing systems. ACM Computing Surveys, 34(3):375–408, 2002.

[7] R. Nasika and P. Dasgupta. Transparent migration of distributed communicating processes. In Proc. 13th ISCA Intl. Conf. on Parallel and Distributed Computing Systems (PDCS), 2000.

[8] M. Orgiyan and C. Fetzer. Tapping TCP streams. In Proc. IEEE Intl. Symp. on Network Computing and Applications (NCA2001), 2002.

[9] M. Powell and D. Presotto. Publishing: a reliable broadcast communication mechanism. In Proc. Symp. on Operating Systems Principles, pages 100–109, 1983.

[10] G. Shenoy, S. Satapati, and R. Bettati. HydraNet-FT: Network support for dependable services. In Proc. 20th Intl. Conf. on Distributed Computing Systems, 2000.

[11] A. Snoeren, D. Andersen, and H. Balakrishnan. Fine-grained failover using connection migration. In Proc. 3rd USENIX Symp. on Internet Technologies and Systems (USITS), pages 97–108, 2001.

[12] F. Sultan, K. Srinivasan, and L. Iftode. Transport layer support for highly-available network services. Technical Report DCS-TR-429, Rutgers University, 2001.

[13] V. Zandy and B. Miller. Reliable network connections. In Proc. ACM MobiCom, 2002.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)  
0-7695-1959-8/03 $17.00 (c) 2003 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 07:01:39 UTC from IEEE Xplore. Restrictions apply.