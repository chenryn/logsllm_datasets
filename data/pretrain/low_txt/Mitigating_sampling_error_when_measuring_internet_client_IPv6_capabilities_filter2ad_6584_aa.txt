# Mitigating Sampling Error when Measuring Internet Client IPv6 Capabilities

**Authors:**
- Sebastian Zander, CAIA, Swinburne University of Technology, Melbourne, Australia (PI:EMAIL)
- Lachlan L. H. Andrew, CAIA, Swinburne University of Technology, Melbourne, Australia (PI:EMAIL)
- Grenville J. Armitage, CAIA, Swinburne University of Technology, Melbourne, Australia (PI:EMAIL)
- Geoff Huston, Asia Pacific Network Information Centre (APNIC), Brisbane, Australia (PI:EMAIL)
- George Michaelson, Asia Pacific Network Information Centre (APNIC), Brisbane, Australia (PI:EMAIL)

## Abstract
Despite the predicted exhaustion of unallocated IPv4 addresses between 2012 and 2014, it remains unclear how many current clients can use its successor, IPv6, to access the Internet. We propose a refinement of previous measurement studies that mitigates intrinsic measurement biases and demonstrate a novel web-based technique using Google ads to perform IPv6 capability testing on a wider range of clients. After applying our sampling error reduction, we find that 6% of worldwide connections are from IPv6-capable clients, but only 1–2% of connections preferred IPv6 in dual-stack (dual-stack failure rates less than 1%). Except for an uptick around IPv6 Day 2011, these proportions were relatively constant, while the percentage of connections with IPv6-capable DNS resolvers has increased to nearly 60%. The percentage of connections from clients with native IPv6 using "happy eyeballs" has risen to over 20%.

## Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Operations—Network Monitoring; C.4 [Performance of Systems]: Measurement Techniques

## Keywords
IPv6 deployment, banner-ad-based measurement

## 1. Introduction
In response to the foreseen exhaustion of IPv4 address space, the IETF standardized the IPv6 protocol as a long-term replacement for IPv4 in 1998 (for an introduction to IPv6, see [1, 2]). As of May 2012, most of the IPv4 address space is now allocated, and according to predictions, the Regional Internet Registrars (RIRs) will run out of IPv4 addresses between 2012 and 2014 [3]. The question is: how many clients can access Internet resources with IPv6?

Over the last decade, several researchers have studied the progress of IPv6 deployment [4]. Many of these studies were based on active probing of servers and network topology, or passive measurements based on server or traffic logs, routing data, or DNS root server data [5–11]. These provide valuable information about the IPv6 capabilities of servers or networks and the proportion of clients that already use IPv6. However, these studies do not provide information about the IPv6 capabilities of clients. While most hosts run IPv6-capable operating systems, they are often not configured to use IPv6 (because there are virtually no IPv6-only services) or their access networks are not IPv6-capable.

More recently, a few researchers used active web-based measurements (based on a technique developed by S. Steffann [12]) to investigate the IPv6 capabilities of clients [13–15]. On certain participating websites, the clients' web browsers download not only the "normal" web pages and images but also a few very small invisible images from a test server. One image can only be retrieved with IPv4, a second only with IPv6, and a third with either IPv4 or IPv6. Based on the images successfully retrieved, one can determine the clients' IPv6 capabilities.

With this method, the client sample has a bias based on the participating websites. In previous studies, only one or two websites were used, or the analysis was limited to a region (e.g., EU countries). Also, the original method did not provide any means to verify whether a client actually attempted all image downloads or aborted some, for example, because the user moved to a different webpage quickly. Furthermore, previous studies focused only on certain aspects (e.g., how many clients preferred IPv6 [13]).

We propose an improved version of the web-based measurement method, commonly implemented with JavaScript (JS-test). Our method not only measures the fraction of clients that prefer IPv6 over IPv4 but also the fraction of clients that are IPv6-capable (including latent Teredo [16] capabilities) or use an IPv6-capable DNS server for query resolution. We also analyze the capabilities based on the clients' operating system (OS) and country and estimate IPv6-related dual-stack failures and the use of "happy eyeballs" (fast fail-over from IPv6 to IPv4 [17]).

To increase the number of participating sites, our test provides information not only to us but also to the site operators themselves via Google Analytics. Since some web service providers are interested in the IPv6 capabilities of visiting clients, this has led to an increase in participating sites. We also propose a novel method of selecting the clients to be tested. We embedded the test script in a Flash ad banner (FA-test), which is served to clients via Google's AdSense. The test is carried out as soon as the ad is displayed by a browser. The ad's distribution is controlled by Google, but ads can potentially reach a very broad set of clients through popular AdSense-enabled websites (e.g., YouTube).

We compare the coverage of JS-test and FA-test and show that Google distributes the ads well. Normalized on the total number of tests, the FA-test reaches far more IP addresses located in more /24 networks compared to the JS-test. However, due to the ad's running costs, the number of FA-tests was limited, and the ad presentation was somewhat biased towards Asian websites. Also, the FA-test could not test iPhones or iPads due to the absence of Flash.

We then identify sources of potential sampling error and propose techniques to mitigate it. We interpret our results as statistics of connections to avoid potential bias, for example, due to multiple clients behind proxies or Network Address Translators (NATs). To mitigate the sampling error, we re-weight the data based on the clients' countries and the proportion of Internet traffic of different countries. We compare the proportions of OSs and browsers measured with reference statistics and show that our raw statistics appear biased, but the re-weighted statistics are similar to the reference. We then estimate the IPv6 capabilities based on the re-weighted statistics.

The paper is organized as follows. Section 2 describes our measurement method. Section 3 describes the dataset and compares the client samples of JS-test and FA-test. Section 4 analyzes sources of bias and presents our approach to mitigate the sampling error. Section 5 presents the IPv6 capability statistics measured. Section 6 discusses related work and compares it with our findings. Section 7 concludes and outlines future work.

## 2. Measurement Method
We describe the improved web-based measurement method, discuss the different ways tested clients are selected, and outline our experimental setup. Finally, we argue that our tests do not significantly affect the experience of users.

### 2.1 Web-based Measurements
When users visit websites, their web browsers normally download several web pages, scripts, images, etc. At certain participating websites, this includes a small test script that fetches a few invisible one-pixel images via HTTP from URLs pointing to our test web server (similar to a page hit counter). We refer to the script as a test and a single image download as a sub-test. For URLs containing hostnames, the client’s resolver must first perform DNS lookups against our authoritative test DNS server prior to sending HTTP requests. The following sub-tests allow us to test IPv4, IPv6, dual-stack, and (latent) Teredo [16] capabilities of clients, as well as whether the client’s resolving DNS server is IPv6-capable:

1. **IPv4-only:** The image can only be retrieved with IPv4 because the DNS server only returns an A record.
2. **IPv6-only:** The image can only be retrieved with IPv6 because the DNS server only returns an AAAA record.
3. **Dual-stack:** The image can be retrieved with IPv4 or IPv6 because the DNS server returns A and AAAA records.
4. **Teredo:** The image URL is an IPv6 address literal. Windows Vista and Windows 7 hosts without native IPv6 will not query for DNS AAAA records, but with an IPv6 address literal, they can use Teredo [18].
5. **DNS Server IPv6 Capability:** The image URL puts our authoritative DNS server behind an IPv6-only name server record. The DNS server can only be accessed if the client’s resolving DNS server can perform DNS queries over IPv6.

A sub-test is deemed successful if the image could be retrieved; otherwise, it is deemed a failure. After all sub-tests have been successfully completed or a timeout of ten seconds (whichever occurs first), the test script sends another HTTP request to the test web server that acts as a test summary. The test summary allows us to determine whether a browser has waited a sufficient amount of time for all sub-tests to complete. Without the summary, it is impossible to know whether a sub-test was not successful because a client lacked the capability or because the test was interrupted.

The test script starts sub-tests in quick succession, but to which degree the images are fetched in parallel depends on the web browser’s connection management. We assume that browsers try to fetch objects as quickly as possible without unnecessary delay. The URLs for each sub-test and summary resolve to different IPv4 and/or IPv6 addresses, which means browsers cannot multiplex different sub-tests over one TCP connection.

### 2.2 Client Sample
Clients interact with our measurement system in two different ways. Several participating websites link our JavaScript test script (JS-test), and visiting hosts are tested. To encourage participation in our JS-test, the script also logs the IPv6 capabilities of clients with Google Analytics, and the site administrators can inspect the statistics (test script is available at [19]). The client sample is biased towards the participating websites, but since the test is implemented in JavaScript, the vast majority of visiting clients can be tested. We assume there are not many clients with disabled JavaScript [20].

We also implemented a Flash ActionScript test and embedded it in a Flash ad banner (FA-test). The ad is served to hosts via Google’s AdSense. The test is carried out as soon as the ad is displayed by the web browser, and the user does not have to click on it. We selected the ad’s keywords and languages to achieve broad placement across different countries and sites. The FA-test reaches a more diverse client population but cannot be carried out by clients without Flash, such as iPhones/iPads during our measurements.

### 2.3 Experimental Setup
Figure 1 shows a logical diagram of our experimental setup. The DNS server handles the incoming DNS queries, and the web server serves the test images. All servers are time-synchronized with NTP. Local Teredo and 6to4 relays are located close to the web server to reduce IPv6 tunneling delay (Teredo server and client-side 6to4 relay are out of our control). We collect HTTP log data, DNS log data, and capture the traffic with tcpdump.

Several data fields are used to convey information from a tested client to our test servers and to prevent caching of DNS records or test images at the client or intermediate caches (see Figure 1). The following data is prepended to the hostname and appended as URL parameters: test time, test ID, test version, and sub-test name. Test time is the time a test started, taken from the client’s clock (Unix time plus milliseconds). Test ID is a "unique" random 32-bit integer number determined by the test script. Test version is the test script’s version number, and sub-test name is a unique identifier for the sub-test, e.g., “IPv4-only.”

**Note:** FA-tests use a hash value computed from Google’s clickTAG, as Google prohibits the use of the ActionScript random() function.

![Figure 1: Experimental setup](path_to_figure_1)

The JS-test uses cookies to ensure clients are only tested once every 24 hours (per website). The main reason for this is to reduce the load on our test servers. However, some clients may ignore the JS-test cookies and perform the test multiple times within 24 hours. The FA-test cannot use cookies due to Google’s restrictions, so clients may be tested multiple times. For both tests, there can be web proxies or NATs in front of multiple clients that look like repeating clients. Nevertheless, the number of IPs repeating the test is generally low.