### References

1. **straint, in 2016 ACM SIGSAC Conf. on Computer and Communications Security, ser. CCS '16, 2016, pp. 43–54.**

2. **T. Dalenius, "Towards a methodology for statistical disclosure control," Statistik Tidskrift, vol. 15, pp. 429–444, 1977.**

3. **Z. Ding, Y. Wang, G. Wang, D. Zhang, and D. Kifer, "Detecting violations of differential privacy," in 2018 ACM SIGSAC Conf. on Computer and Communications Security (CCS '18), 2018, pp. 475–489.**

4. **C. Dwork, "Differential privacy," in Automata, Languages and Programming, 33rd Intl. Colloquium, ICALP 2006, Venice, Italy, July 10–14, 2006, Proceedings, Part II, ser. LICS, vol. 4052, 2006, pp. 1–12.**

5. **C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth, "Generalization in adaptive data analysis and holdout reuse," in 28th Intl. Conf. on Neural Information Processing Systems - Volume 2 (NIPS'15), 2015, pp. 2350–2358.**

6. **——, "The reusable holdout: Preserving validity in adaptive data analysis," Science, vol. 349, no. 6248, pp. 636–638, 2015. [Online]. Available: http://science.sciencemag.org/content/349/6248/636**

7. **C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. L. Roth, "Preserving statistical validity in adaptive data analysis," in Forty-seventh Annual ACM Symp. on Theory of Computing (STOC '15), 2015, pp. 117–126.**

8. **C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, "Fairness through awareness," in 3rd Innovations in Theoretical Computer Science Conf., 2012, pp. 214–226.**

9. **C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor, "Our data, ourselves: Privacy via distributed noise generation," in 24th Annual Intl. Conf. on The Theory and Applications of Cryptographic Techniques, 2006, pp. 486–503.**

10. **C. Dwork, F. Mcsherry, K. Nissim, and A. Smith, "Calibrating noise to sensitivity in private data analysis," in Theory of Cryptography Conf., 2006, pp. 265–284.**

11. **C. Dwork and M. Naor, "On the difficulties of disclosure prevention in statistical databases or the case for differential privacy," J. Privacy and Confidentiality, vol. 2, no. 1, pp. 93–107, 2008.**

12. **J. Gehrke, E. Lui, and R. Pass, "Towards privacy for social networks: A zero-knowledge based definition of privacy," in 8th Conf. on Theory of Cryptography, 2011, pp. 432–449.**

13. **A. Ghosh and R. Kleinberg, "Inferential privacy guarantees for differentially private mechanisms," CoRR, vol. abs/1603.01508v2, 2017, presented at the 8th Innovations in Theoretical Computer Science conference in 2017.**

14. **S. Goldwasser and S. Micali, "Probabilistic encryption & how to play mental poker keeping secret all partial information," in Fourteenth Annual ACM Symp. on Theory of Computing, ser. STOC '82, 1982, pp. 365–377.**

15. **——, "Probabilistic encryption," J. Computer and System Sciences, vol. 28, no. 2, pp. 270–299, 1984.**

16. **X. He, A. Machanavajjhala, and B. Ding, "Blowfish privacy: Tuning privacy-utility trade-offs using policies," in ACM SIGMOD Intl. Conf. on Management of Data (SIGMOD 2014), 2014.**

17. **C. E. Shannon, "Communication theory of secrecy systems," Bell Labs Technical Journal, vol. 28, no. 4, pp. 656–715, 1949.**

18. **J. Tang, A. Korolova, X. Bai, X. Wang, and X. Wang, "Privacy loss in Apple’s implementation of differential privacy on MacOS 10.12," ArXiv, vol. 1709.02753, 2017.**

19. **M. C. Tschantz, A. Datta, A. Datta, and J. M. Wing, "A methodology for information flow experiments," in Computer Security Foundations Symp., 2015.**

20. **Y. Wang and M. Kosinski, "Deep neural networks are more accurate than humans at detecting sexual orientation from facial images," 2017.**

21. **S. L. Warner, "Randomized response: A survey technique for eliminating evasive answer bias," J. the American Statistical Association, vol. 60, no. 309, pp. 63–69, 1965.**

22. **B. Yang, I. Sato, and H. Nakagawa, "Bayesian differential privacy on correlated data," in 2015 ACM SIGMOD Intl. Conf. on Management of Data, ser. SIGMOD '15, 2015, pp. 747–762.**

23. **T. Zhu, P. Xiong, G. Li, and W. Zhou, "Correlated differential privacy: Hiding information in non-IID data set," IEEE Transactions on Information Forensics and Security, vol. 10, no. 2, pp. 229–242, 2015.**

**Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 09:36:11 UTC from IEEE Xplore. Restrictions apply.**

---

### Appendix: Two Views of Differential Privacy: A Brief History

Throughout this paper, we have mentioned two lines of work about Differential Privacy (DP). The historically first line, associated with its creators, views DP as not requiring additional assumptions, such as independent data points or an adversary that already knows all but one data point. The historically second line views such assumptions as needed by or implicit in DP. Here, we briefly recount the history of these two lines.

#### 1. Before Differential Privacy

The idea of a precise framework for mathematically modeling the conditions under which an adversary does not learn something perhaps starts with Shannon’s work on perfect security in 1949 [17]. In 1984, this idea led to Goldwasser and Micali’s cryptographic notion of semantic security, which relaxes Shannon’s requirement by applying to only polynomially computationally bounded adversaries [15] (with antecedents in their earlier 1982 work [14]).

The statistics community also considered limiting what an adversary would learn. One early work cited by DP papers (e.g., [4]) is Dalenius’s 1977 paper on statistical disclosure [2]. Dalenius defines statistical disclosures in terms of a frame of objects, for example, a sampled population of people [2, §4.1]. The objects have data related to them [2, §4.2]. A survey releases some statistics over such data for the purpose of fulfilling some objective [2, §4.3]. Finally, the adversary may have access to extra-objective data, which is auxiliary information other than the statistics released as part of the survey. Dalenius defines a statistical disclosure as follows [2, §5]:

"If the release of the statistics S makes it possible to determine the value DK more accurately than is possible without access to S, a disclosure has taken place [...]" where DK is the value of the attribute D held by the object (e.g., person) K. The attribute D and object K may be used in the computation of S or not. The extra-objective data may be used in computing the estimate of DK.

As pointed out by Dwork [4], Dalenius’s work is both similar to and different from the aforementioned work on cryptosystems. The most obvious difference is looking at databases and statistics instead of cryptosystems and messages. However, the more significant difference is the presence of the objective with a benefit, or the need for utility in Dwork’s nomenclature. That is, the released statistics are to convey some information to the public; whereas, the encrypted message, the cryptosystem’s analog to the statistic, only needs to convey information to the intended recipient. Dalenius recognized that this additional need makes the elimination of statistical disclosures “not operationally feasible” and “would place unreasonable restrictions on the kind of statistics that can be released” [2, §18].

Even before the statistics work on statistical nondisclosure, statistical research by S. L. Warner in 1965 introduced the randomized response method of providing DP [21]. (His work is more similar to the local formulation of DP [9].) The randomized response model and statistical disclosure can be viewed as the prototypes of the first and second lines of research, respectively, although these early works appear to have had little impact on the actual formation of the lines of research over a quarter-century later.

#### 2. Differential Privacy

In March 2006, Dwork, McSherry, Nissim, and Smith presented a paper containing the first modern instance of DP under the name of “ε-indistinguishable” [10]. The earliest use of the term “differential privacy” comes from a paper by Dwork presented in July 2006 [4]. This paper of Dwork explicitly rejects the view that DP provides associative or inferential privacy [4, p. 8]:

"Note that a bad disclosure can still occur [despite DP], but [DP] assures the individual that it will not be the presence of her data that causes it, nor could the disclosure be avoided through any action or inaction on the part of the user."

and further contains a proof that preventing Dalenius’s statistical disclosures while releasing useful statistics is impossible. (The proof was joint work with Naor, with whom Dwork later further developed the impossibility result [11].) Later works further expound upon their position [9], [26].

#### 3. Questions Raised about Differential Privacy

In 2011, papers started to question whether DP actually provides a meaningful notion of privacy [7], [27], [12]. These papers point to the fact that a released statistic can enable inferring sensitive information about a person, similar to the attacks Dalenius wanted to prevent [2], even when that statistic was computed using a differentially private algorithm. While the earlier work on DP acknowledged this limitation, these papers provide examples where correlations, or more generally associations, between data points can enable inferences that some people might not expect to be possible under DP. These works kicked off a second line of research (including, e.g., [28], [29], [16], [5], [23], [33]) attempting to find stronger definitions that account for such correlations. In some cases, these papers assert that such inferential threats are violations of privacy and not what people expect of DP. For example, Liu et al.’s abstract states that associations between data points can lead to “degradation in expected privacy levels” [33]. The rest of this subsection provides details about these papers.

In 2011, Kifer and Machanavajjhala published a paper stating that the first popularized claim about DP is that “It makes no assumptions about how data are generated” [27, p. 1]. The paper then explains that “a major criterion for a privacy definition is the following: can it hide the evidence of an individual’s participation in the data generating process?” [27, p. 2]. It states [27, p. 2]:

"We believe that under any reasonable formalization of evidence of participation, such evidence can be encapsulated by exactly one tuple [as done by DP] only when all tuples are independent (but not necessarily generated from the same distribution). We believe this independence assumption is a good rule of thumb when considering the applicability of differential privacy."

For this reason, the paper goes on to say “Since evidence of participation requires additional assumptions about the data (as we demonstrate in detail in Sections 3 and 4), this addresses the first popularized claim – that differential privacy requires no assumptions about the data” [27, p. 2]. From context, we take “addresses” to mean invalidates since the paper states “The goal of this paper is to clear up misconceptions about differential privacy” [27, p. 2].

In 2012, Kifer and Machanavajjhala published follow-up work stating that “we use [the Pufferfish framework] to formalize and prove the statement that differential privacy assumes independence between records” [28, p. 1]. It goes on to say “Assumptionless privacy definitions are a myth: if one wants to publish useful, privacy-preserving sanitized data then one must make assumptions about the original data and data-generating process” [28, p. 1, emphasis in original]. In 2014, Kifer and Machanavajjhala published a journal version of their 2012 paper, which makes a similar statement: “Note that assumptions are absolutely necessary – privacy definitions that can provide privacy guarantees without making any assumptions provide little utility beyond the default approach of releasing nothing at all” [29, p. 3:5]. However, this version is, overall, more qualified. For example, it states “The following theorem says that if we have any correlations between records, then some differentially private algorithms leak more information than is allowable (under the odds ratio semantics in Section 3.1)” [29, 3:12–13], which makes it clear that the supposed shortcoming of DP in the face of correlated data points is relative to a particular notion of privacy presented in that paper, roughly, reducing uncertainty about some sensitive fact about a person.

Also in 2014, He et al. published a paper building upon the Pufferfish framework [16]. Referring to the conference version [28], He et al. states [16, p. 1]:

"[Kifer and Machanavajjhala] showed that differential privacy is equivalent to a specific instantiation of the Pufferfish framework, where (a) every property about an individual’s record in the data is kept secret, and (b) the adversary assumes that every individual is independent of the rest of the individuals in the data (no correlations). We believe that these shortcomings severely limit the applicability of differential privacy to real-world scenarios that either require high utility, or deal with correlated data."

and “Recent work [by Kifer and Machanavajjhala] showed that differentially private mechanisms could still lead to an inordinate disclosure of sensitive information when adversaries have access to publicly known constraints about the data that induce correlations across tuples” [16, p. 3].

In 2013, Li et al. published a paper that states “differential privacy’s main assumption is independence” [32, p. 2]. Similar to the papers by Kifer and Machanavajjhala, this paper assumes a technical definition of privacy, positive membership privacy, and makes this assertion since independence is required for DP to imply it. The paper also claims that “the original definition of differential privacy assumes that the adversary has precise knowledge of all the tuples in the dataset” [32, p. 10], which we take as a reference to the strong adversary assumption.

Chen et al.’s 2014 paper is the first of three attempting to provide an associative version of privacy, motivated by Pufferfish, in the face of correlated data [5]. It states “ε-differential privacy fails to provide the claimed privacy guarantee in the correlated setting” [5, p. 2] and “ε-differential privacy is built on the assumption that all underlying records are independent of each other” [5, p. 7].

The second paper, Zhu et al.’s paper, published in 2015, provides a more accurate accounting of correlations [23]. It states [23, p. 229]:

"An adversary with knowledge on correlated information will have a higher chance of obtaining the privacy information, and violating the definition of differential privacy. Hence, how to preserve rigorous differential privacy in a correlated dataset is an emerging issue that needs to be addressed."

It further asserts [23, p. 231]:

"In the past decade, a growing body of literature has been published on differential privacy. Most existing work assumes that the dataset consists of independent records."

and “a major disadvantage of traditional differential privacy is the overlook of the relationship among records, which means that the query result leaks more information than is allowed” [23, p. 232].

The third paper, by Liu et al. in 2016, provides an even more accurate accounting of correlations [33]. A blog post by one of the authors, Mittal, announcing the paper states “To provide its guarantees, DP implicitly assumes that the data tuples in the database, each from a different user, are all independent.” [40]. In five comments on this blog post, McSherry posted a summary of his concerns about their paper and blog post. McSherry also treats the paper at length in a blog post [35]. McSherry highlights three statements made by the paper that he finds false [35]: (1) “For providing this guarantee, differential privacy mechanisms assume independence of tuples in the database” [33, p. 1], (2) “To provide its guarantees, DP mechanisms assume that the data tuples (or records) in the database, each from a different user, are all independent.” [33, p. 1], and (3) “However, the privacy provided by DP mechanisms can be significantly degraded when the data tuples are correlated.” [33, p. 1].

**Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 09:36:11 UTC from IEEE Xplore. Restrictions apply.**