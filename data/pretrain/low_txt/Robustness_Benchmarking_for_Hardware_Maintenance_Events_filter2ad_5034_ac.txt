# Table 2: System B FRU List and Maintenance Classes

This table provides a detailed list of Field Replaceable Units (FRUs) for System B, along with their maintenance classes. It is important to note that this does not imply that actual benchmark testing should be skipped in a real production environment.

- **Column 2** indicates the hot-swappable status of each FRU. As evident from the data, more of System B's FRUs are hot-swappable compared to System A, which is a significant Reliability, Availability, and Serviceability (RAS) enhancement.
- **Column 3** lists the failure rate of each FRU, expressed in failures per 10^9 hours of operation (FIT). The values in this column are artificial for illustrative purposes to protect company confidential information. The issue of handling confidential information will be addressed in Section 5.
- **Column 4** enumerates the number of each FRU type in the system.
- **Column 5** lists the combined failure rate for each FRU type, calculated as the product of the number of each type and its failure rate.

## MRB-A Calculation

### Tables 3 and 4 illustrate the steps for MRB-A calculation for Systems A and B, respectively.

- **Column 2** in both tables lists the combined failure rates of FRUs in each maintenance class, derived from the total failure rates of FRU types given in Tables 1 and 2.
- **PMCi** (Percentage of Maintenance Class i) is the percentage of the aggregate of failure rates of FRUs in maintenance class i over the total system combined failure rate.
- **MCFi** (Mean Corrective Time for Maintenance Class i) is the average time required to correct a failure in maintenance class i.
- **PMCi x MCFi** is the weighted mean corrective time, where PMCi is the weight.

#### Table 3: MRB-A Calculation for System A

| Combined Failure Rate | PMCi (%) | MCFi (hours) | PMCi x MCFi |
|-----------------------|----------|--------------|-------------|
| 95500                 | 80.93    | 1            | 0.81        |
| 0                     | 0.00     | 10           | 0.00        |
| 22500                 | 19.07    | 100          | 19.07       |
| **Total**             | **100.00** | **-**      | **19.88**   |

#### Table 4: MRB-A Calculation for System B

| Combined Failure Rate | PMCi (%) | MCFi (hours) | PMCi x MCFi |
|-----------------------|----------|--------------|-------------|
| 38500                 | 32.08    | 1            | 0.32        |
| 24000                 | 20.00    | 10           | 2.00        |
| 57500                 | 47.92    | 100          | 47.92       |
| **Total**             | **100.00** | **-**      | **50.24**   |

By comparing the PMCi values, it is clear that System B has a higher percentage of its maintenance events belonging to maintenance class 3 compared to System A. The system MRB-A is calculated as the weighted average of MCFis, with PMCi values as the weights. As expected, System B has a higher MRB-A benchmark rating (50.24) than System A (19.88).

## Important Issues

### 1. Failure Rate Prediction
- This benchmark requires assigning weights to the MCFs based on predicted failure rates in each maintenance class. The industry standard Telcordia SR-332 is well-suited for this purpose, providing a consistent estimate of hardware failure rates.
- We do not recommend using field data for failure rate prediction due to uncontrolled data collection processes, which can vary between companies and make them unsuitable for benchmarking.

### 2. Reporting
- Vendors will perform the benchmark test and publish the results. The reporting requirements should balance the need to protect vendor proprietary information and the need for sufficient disclosure of information to the public and competitors.
- Historically, vendors have been reluctant to publish failure data, but this problem can be mitigated by not requiring the publication of specific failure rate data for individual FRUs.

### 3. Auditing
- All benchmark results should be audited by a certified third party to verify the failure event classification and ensure compliance with established rules.
- The auditing process should be similar to that used by the TPC, where an auditor is qualified by the benchmarking standard body and paid by the vendor to perform the audit.

### 4. Cost
- Unlike performance benchmarks such as TPC, MRB-A is relatively straightforward to produce, and the cost is not a major concern. We estimate that maintenance event simulation will take between 1-5 days, depending on the size of the server.
- The cost for producing an MRB-A benchmark should be significantly lower than producing some traditional performance benchmarks.

## Summary and Future Work

In this paper, we proposed the MRB-A benchmark to quantitatively characterize system robustness against fault-induced hardware maintenance events. Until now, the only way to compare system robustness was through feature lists, which often leave room for interpretation. MRB-A provides a quantitative measurement of system robustness against hardware repair events, facilitating an objective comparison of systems with different feature lists. Additionally, MRB-A helps system designers measure progress in handling fault-induced maintenance events.

The novelty of the proposed approach lies in its simple technique for determining the exhaustive maintenance load and the percentage of maintenance events in each of the three maintenance classes. To our knowledge, MRB-A is the first availability benchmark to address maintenance events and is also the first completely portable availability benchmark that can be used to compare different products, including those from different companies.

Our next effort will focus on developing robustness benchmarks for non-fault-induced maintenance events and software maintenance events. Recent studies show that maintenance events account for a majority of system outages, and we hope our work will inspire further research in this area.

## Acknowledgments

We would like to thank our respective management for their support and commitment to this work. Special thanks to John Bongiovanni, David Nelson-Gal, Ganesh Ramamurthy, Roy Andrada, and Michael Chow. We also owe a debt of gratitude to Allan Packer, Jim Lewis, and William Bryson for their thoughtful and insightful feedback and suggestions for improvement.

## References

[1]. A. Avizienis et al., "Fundamental Concepts of Dependability", UCLA CSD Report no. 010028, LAAS Report no. 01-145, Newcastle University Report no. CS-TR-739, 2001.
[2]. A. Brown and D.A. Patterson, "Towards Availability Benchmarks: A Case Study of Software RAID Systems", USENIX 2000.
[3]. C. Dingman et al., "Measuring Robustness of a Fault Tolerant Aerospace System", Proceedings of the 25th International Symposium on Fault-Tolerant Computing, June 1995.
[4]. P. Koopman et al., "Comparing Operating Systems Using Robust Benchmarks", Proceedings of the 16th Symposium on Reliable Distributed Systems, pp. 72-79, Oct. 1997.
[5]. H. Madeira, “Dependability Benchmarking: making choices in an n-dimensional problem space.” Proceedings of the first workshop on Evaluating and Architecting System Dependability, July 2001.
[6]. K. Kanoun et al., “A Framework for Dependability Benchmarking”, International Conference on Dependable Systems and Networks, pp. F.7-F.8, June 2002.
[7]. B. Miller et al., "An Empirical Study of Reliability of UNIX Utilities", Comm. of the ACM, Vol 33, No. 12, pp. 32-43, Dec 1990.
[8]. A. Mukherjee and D. P. Siewiorek, "Measuring Software Robustness by Benchmarking", IEEE Transactions on Software Engineering, Vol. 23, No. 6, June 1997.
[9]. D. P. Siewiorek et al., "Development of a Benchmark to Measure System Robustness", Proceedings of the 1993 International Symposium on Fault-Tolerant Computing, pp. 88-97, June 1993.
[10]. T. Tsai et al., "An Approach Towards Benchmarking of Fault Tolerant Commercial Systems", Proceedings of the 1996 Symposium on Fault-Tolerant Computing, pp. 314-323, June 1996.
[11]. J. Zhu et al., "R-Cubed (R3): Rate, Robustness, and Recovery – An Availability Benchmark Framework", Technical Report Series #TR-2002-109, July 2002, Sun Microsystems, Inc.
[12]. "http://www.tpc.org/information/about/abouttpc.asp". Current. Transaction Processing Council.