### Analysis of User Behavior and Model Evaluation

Our analysis aligns with the intuitive understanding that a significant portion of illicit activity is associated with library access, as indicated by a coefficient of 3.53. In contrast, legitimate users tend to access web mail, course portals, and other academic websites more frequently.

#### Performance Evaluation of Profile-Based Features

Figure 3 illustrates the complementary cumulative distribution function (CCDF) of profile fitness for various features. The CCDFs for IP-related profile features, such as country code, Autonomous System Number (ASN), and domain name, effectively distinguish between legitimate and illegitimate activities. However, timing and resource usage features did not perform well, as the profiles collected for both benign and compromised accounts were indistinguishable.

#### Distribution of Websites Visited at UofM

| Website       | % of Benign Access | % of Compromised Access |
|---------------|--------------------|-------------------------|
| Library       | 5.83%              | 44.66%                  |
| Web Mail      | 34.63%             | 28.72%                  |
| Course Portal | 5.93%              | 0.78%                   |
| Remote Desktop| 1.39%              | 0%                      |
| File Storage  | 1.22%              | 0%                      |

Table V: Distribution of Websites Visited at UofM

#### Model Evaluation

To evaluate the model, we employed cross-validation, a standard technique for assessing machine learning algorithms. Given the limited number of positive cases, we used a fivefold cross-validation to ensure an adequate number of positive samples in each set. To avoid artificial results, we grouped feature vectors by user before partitioning the dataset, ensuring that different feature vectors from the same user were not split between the training and evaluation sets.

An important parameter is the length of the training window. Figure 4 shows the detection results for five different training window sizes. Our goal was to limit the average daily number of false alarms to two. Since UCAAS analyzes approximately 1,000 unique users per day, we aimed for a false positive rate (FPR) of 0.2% or less. Under this requirement, the best true positive rates (TPRs) achieved were 95.4% at UofM with a training window of 11 days, and 100% at UIUC with training windows of 9 or 11 days.

#### Empirical Evaluation

We conducted an empirical evaluation by building a model from the training set and using it to classify the validation set. At UofM, during a two-week period starting from September 14, 2011, 126 unique users who had never appeared in the training set were flagged. Of these, 124 were validated by the security team as compromised. The remaining two accounts were shared with family members or friends living in foreign countries. These results exceeded our expectations, as there were no false positives, and no compromised accounts detected by other means were missed by UCAAS.

At UIUC, we ran UCAAS on a validation dataset collected from July 9 to July 23, 2011. A total of 11 alerts were generated, reflecting 10 accounts already labeled as compromised in the training set. These accounts reappeared because they were still compromised when the validation set was collected. The one flagged account not in the training set was confirmed as a true positive. No false positives were generated, and a new compromised account was discovered. We also confirmed with the security team that no compromised accounts reported during this period were missed by the system. However, the close temporal proximity of the training and validation sets limits our conclusions about the overall accuracy of UCAAS at UIUC. The fact that UIUC has ten times fewer active accounts than UofM likely explains the difference in the number of compromised accounts detected.

#### Lessons Learned

During initial testing, the temporal-spatial violation feature was the main source of false positives. These incorrect violations were due to:
1. Users connecting through multiple VPN clients.
2. Users accessing campus services via remote desktops.
3. Imprecise geolocation information.

The first two issues were mitigated by combining complementary sets of features, requiring additional suspicious behavior to trigger an alert. For the third issue, we revised location-related features to work at the country level rather than the city level, using a GeoIP database accurate at the country level. However, attackers can evade detection by logging into one university account via the VPN of another, hiding their geographic location. Future work will integrate better geolocation lookup methods to enhance UCAAS's robustness.

### Related Work

Previous research on account theft through botnet takeover [3] and phishing target analysis [2] has shown that attackers are primarily attracted to financial and payment systems due to the direct monetary gains. Compromised university accounts, while not directly linked to financial transactions, can be used stealthily for months or years without being noticed by users. Therefore, a centralized system for detecting compromised accounts is crucial for academic institutions. A study at UIUC [10] analyzed credential-stealing attacks over five years, concluding that boundary protections like firewalls are insufficient, and sophisticated user action monitoring systems are needed.

Various statistical methods have been successfully applied to detect fraudulent activities in other security domains, including credit card fraud detection [11-13], telecommunications fraud detection [14-16], and intrusion detection systems [17-19]. Machine learning methods are also heavily used in malicious domain and URL detection [20-22].

Our approach follows general anomaly and fraud detection methods, extracting a set of features and applying a statistical model to detect suspicious activities. We address the challenge of open university environments with diverse user behavior. The most related work is a case study on anomaly detection for VPN [23], which we extend by incorporating a larger set of features and automating the analysis.

### Conclusion

Large academic institutions face the challenge of protecting user accounts while supporting a wide range of services with limited security resources. This paper presents the University Credential Abuse Auditing System (UCAAS), a machine-learning approach for automatic detection of account compromises. UCAAS considers a large set of automatically generated features, evaluates their ability to identify illegitimate behavior, and uses a logistic regression classifier to flag potentially compromised accounts. The system was trained and evaluated across two large universities, identifying a total of 125 compromised accounts in a two-week trial. Empirical validation shows that UCAAS offers high detection accuracy with no false positives.

### Acknowledgments

We thank Paul Howell, the chief information technology security officer at UofM, for his support in data collection, access, discussion, and result validation. We are also grateful to the security team at UIUC, particularly Michael Corn, Vlad Grigorescu, Warren Raquel, and Bill Gambardella. Carol Livingstone from the Division of Management Information at UIUC provided valuable support in collecting and analyzing the demographic dataset. This project was sponsored at UIUC by the Air Force Research Laboratory (AFRL) and at UofM by the Department of Homeland Security (DHS), the National Science Foundation (NSF), and the Department of the Navy.

### References

[1] Trend Micro, “Data-stealing malware on the rise: Solutions to keep businesses and consumers safe,” http://us.trendmicro.com/imperia/md/content/us/pdf/threats/securitylibrary/, 2009.

[2] Anti-Phishing Working Group, “Phishing activity trends report,” http://www.antiphishing.org/phishReportsArchive.html, 2010.

[3] B. Stone-Gross, M. Cova, L. Cavallaro, B. Gilbert, M. Szydlowski, R. Kemmerer, C. Kruegel, and G. Vigna, “Your botnet is my botnet: Analysis of a botnet takeover,” in Proc. of the 16th ACM Conference on Computer and Communications Security, 2009, pp. 635–647.

[4] J. Young, “Academic publisher steps up efforts to stop online piracy of its products,” http://chronicle.com/article/Academic-Publisher-Steps-Up/128031, 2011.

[5] R. J. Bolton and D. J. Hand, “Statistical fraud detection: A review,” Statistical Science, vol. 17, pp. 235–249, 2002.

[6] “Weka 3 - Data Mining with Open Source Machine Learning Software,” http://www.cs.waikato.ac.nz/ml/weka/.

[7] J. G. Orme and T. Combs-Orme, Multiple Regression with Discrete Dependent Variables. Oxford University Press, 2009.

[8] MaxMind, http://www.maxmind.com/app/ip-location, 2011.

[9] Y. Wang, D. Burgener, M. Flores, A. Kuzmanovic, and C. Huang, “Towards street-level client-independent IP geolocation,” in Proc. of the 8th USENIX Symposium on Networked Systems Design and Implementation, Mar. 2011.

[10] A. Sharma, Z. Kalbarczyk, R. Iyer, and J. Barlow, “Analysis of credential stealing attacks in an open networked environment,” in Proc. of the Fourth International Conference on Network and System Security. Washington, DC, USA: IEEE Computer Society, 2010, pp. 144–151.

[11] E. Aleskerov, B. Freisleben, and B. Rao, “Cardwatch: A neural network based database mining system for credit card fraud detection,” Proc. of the IEEE IAFE 1997 conference on Computational Intelligence for Financial Engineering, pp. 220–226, 1997.

[12] S. Panigrahi, A. Kundu, S. Sural, and A. K. Majumdar, “Credit card fraud detection: A fusion approach using Dempster-Shafer theory and Bayesian learning,” Inf. Fusion, vol. 10, pp. 354–363, Oct. 2009.

[13] R. Chen, T. Chen, Y. Chien, and Y. Yang, “Novel questionnaire-responded transaction approach with SVM for credit card fraud detection,” in Proc. of the 2nd International Conference on Advances in Neural Networks, vol. 2, 2005, pp. 916–921.

[14] D. Agarwal, “An empirical Bayes approach to detect anomalies in dynamic multidimensional arrays,” in Proc. of the Fifth IEEE International Conference on Data Mining. IEEE Computer Society, 2005, pp. 26–33.

[15] K. C. Cox, S. G. Eick, G. J. Wills, and R. J. Brachman, “Visual data mining: Recognizing telephone calling fraud,” Data Mining and Knowledge Discovery, vol. 1, pp. 225–231, 1997.

[16] C. Phua, D. Alahakoon, and V. Lee, “Minority report in fraud detection: Classification of skewed data,” SIGKDD Explor. Newsl., vol. 6, no. 1, pp. 50–59, 2004.

[17] F. Esponda, S. Forrest, and P. Helman, “A formal framework for positive and negative detection schemes,” IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 34, no. 1, pp. 357–373, 2004.

[18] K. A. Heller, K. M. Svore, A. D. Keromytis, and S. J. Stolfo, “One class support vector machines for detecting anomalous windows registry accesses,” in Proc. of the Workshop on Data Mining for Computer Security, 2003.

[19] J. D. Brutlag, “Aberrant behavior detection in time series for network monitoring,” Proc. of the 14th Systems Administration Conference (LISA 2000), Dec. 2000.

[20] C. Whittaker, B. Ryner, and M. Nazif, “Large-scale automatic classification of phishing pages,” in Proc. of 17th Annual Network and Distributed System Security Symposium, 2010.

[21] M. Antonakakis, R. Perdisci, W. Lee, V. Nikolaos, and D. Dagon, “Detecting malware domains at the upper DNS hierarchy,” pp. 1–16, 2011.

[22] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond blacklists: Learning to detect malicious web sites from suspicious URLs,” in Proc. of the SIGKDD Conference., 2009.

[23] M. Chapple, N. Chawla, and A. Striegel, “Authentication anomaly detection: A case study on a virtual private network,” in Proc. of the 3rd Annual ACM Workshop on Mining Network Data. New York, NY, USA: ACM, 2007, pp. 17–22.