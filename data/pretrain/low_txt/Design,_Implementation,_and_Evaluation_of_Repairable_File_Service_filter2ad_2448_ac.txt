### 优化后的文本

#### 图4. 在不同运行条件下，受保护的NFS服务器在SPECsfs基准测试中的吞吐量与更新请求百分比的关系

- 机器配置：一台机器运行请求拦截器，另一台机器运行日志转换器、污染分析、修复引擎和镜像NFS服务器。NFS客户端和请求拦截器通过一个快速以太网交换机连接，而受保护的NFS服务器和镜像NFS服务器通过交叉电缆连接到请求拦截器。
- 硬件规格：除了一个拥有400MHz CPU和128MB内存的客户端机器外，其他四台机器均为1.4GHz Pentium IV处理器，配备500MB内存。受保护的NFS服务器和镜像NFS服务器均配置了40GB ST340016A ATA硬盘驱动器，具有2MB磁盘缓存。

主要工作负载是SPEC SFS 3.0（SFS97 R1），这是标准性能评估公司最新的NFS吞吐量和响应时间基准测试工具。其操作混合度与实际NFS工作负载非常接近。SFS基准测试直接通过UDP套接字与NFS服务器交互，而不是通过系统调用。因此，SFS基准测试不能用于评估客户端日志记录开销。我们使用了SDET [16]基准测试来代替，它代表了软件开发环境中的典型工作负载。

### 5.1 客户端日志记录开销
在400MHz客户端机器上，我们使用32个脚本运行SDET基准测试，生成总共55MB的文件集大小。客户端日志记录导致了4.08%的CPU开销。内核日志缓冲区需要约12MB。此运行中总NFS流量为97MB，客户端日志记录产生的流量为3MB。客户端日志大小为3MB，服务器撤销日志大小为80MB。启用客户端日志记录后，NFS服务器的吞吐量从1907下降到1811，下降了5%。这种吞吐量的下降主要是由于系统调用日志记录带来的额外CPU开销。

### 5.2 服务器端日志记录开销
对于每个NFS数据包，请求拦截器增加了一个小的转发延迟，范围从0.2毫秒到1.5毫秒，具体取决于数据包的大小。只要日志转换器能够实时地将重做记录转换为撤销记录，受保护的NFS服务器的吞吐量不会受到影响。当日志转换器无法跟上输入负载时，请求拦截器会丢弃NFS数据包，从而降低受保护NFS服务器的吞吐量。

在我们的测试环境中，日志转换器处理一个NFS更新请求大约需要5毫秒，因此每秒最多可以处理200个更新请求。SPECsfs中的默认更新请求百分比为12%。我们改变了这个百分比并测量了受保护NFS服务器的吞吐量，结果如图4所示。

每次百分比对应于一次SPECsfs基准测试运行。我们给定的负载为700，每次运行生成约7GB的初始文件集大小。请注意，SPECsfs的吞吐量略高于指定负载是正常的。在NFS服务器上，操作系统和基准测试工作目录位于同一块磁盘上。在镜像NFS服务器上，除非特别指定，操作系统、NFS服务器镜像、RFS撤销日志和客户端系统调用日志都位于同一块磁盘上。

当更新请求百分比低于30%时，单独运行NFS和与RFS一起运行的NFS之间没有明显的吞吐量差异。然而，超过30%后，RFS引起的性能下降变得越来越明显。当更新请求百分比为96%时，RFS将受保护NFS服务器的吞吐量减半（从600降至300）。RFS的大部分性能成本在于日志转换器，它花费了超过90%的时间处理写请求。日志转换是I/O绑定的，通过在镜像NFS服务器机器上添加一块额外的磁盘来存放一半的SPECsfs工作目录，RFS的性能下降消失，如图中标记为“带有RFS，镜像NFS服务器上有两块磁盘”的曲线所示。

### 5.3 RFS的硬件需求
为了进一步了解运行日志转换和镜像NFS服务器的镜像NFS服务器机器的硬件需求，我们比较了受保护NFS服务器和镜像NFS服务器的CPU和磁盘使用情况。对于文件更新请求，镜像NFS服务器需要额外读取前图像，并额外写入撤销日志，即总共三次磁盘访问。NFS和RFS服务器的瓶颈在于磁盘访问，图5显示了受保护NFS服务器和镜像NFS服务器之间的CPU和磁盘利用率比较。它还显示了请求拦截器机器的CPU使用率，无论更新请求百分比如何，始终低于5%，这表明低端机器和少量内存就足够了。镜像NFS服务器的CPU负载与受保护NFS服务器相当。在默认的NFS更新请求百分比12%下，镜像NFS服务器的磁盘负载是受保护NFS服务器的55%。

### 5.4 日志存储需求
为了测量RFS的存储需求，我们在计算机科学系研究生家庭目录服务器（超过250名用户）上收集了为期8小时48分钟的NFS跟踪数据，该数据是在2001年秋季学期的最后一周采集的。在此跟踪期间，共有1,863,971次NFS请求，其中51,313次是更新请求（例如写入和setattr），占总请求的2.7%。

此跟踪产生的RFS撤销日志大小为259,762,779字节，约260MB。撤销日志的大部分（97%）归因于文件更新的前图像。在这个实验中，我们无法修改250名用户的NFS客户端来获取实际的客户端日志大小。总存储空间是根据撤销日志大小和撤销日志大小占总存储空间的百分比估计的。在评估客户端日志记录开销的实验中，该百分比为96%。我们假设在NFS密集运行环境中，这一百分比不会有太大变化。根据此跟踪，每天所需的撤销日志大小约为709MB。因此，可以使用一块40GB（价格不到80美元）的磁盘来维护8周的检测窗口。

### 5.5 污染分析和损坏修复的有效性
为了评估RFS自动损坏修复程序的有效性，我们运行了SDET基准测试，直到撤销日志大小达到100MB。涉及87个进程和985个被修改的文件。我们随机选择一些进程作为根进程。不同的根进程数量对应于不同的污染级别。污染级别表示污染的范围。“全部”表示所有进程都被污染，所有更新操作都需要撤销。“高”和“低”分别表示需要撤销的操作比例较大或较小。污染级别由初始根进程的数量和污染传播共同决定。

表2显示，污染分析时间不依赖于污染级别。这是因为污染分析模块需要读取并解析整个日志，以确定污染状态。相比之下，损坏修复或撤销时间取决于污染分析模块选择的撤销记录数量。根据这些结果，700MB撤销日志（即一天的SPECsfs运行）的污染分析和损坏修复时间估计在9到20分钟之间，具体取决于污染级别。我们认为这比能够在相同精度水平下修复系统的手动修复过程要快得多。

### 结论
RFS项目的主要目标是增强现有的网络文件服务器，使其在入侵或错误发生后能够更准确地修复系统损坏，因为可以回滚每一个更新，并且更快，因为可以自动化确定损坏程度和撤销损坏效果。RFS的一个独特特点是，它可以通过支持的文件访问协议完全保护共享文件服务器，因此不需要修改共享文件服务器的内部实现或外部配置。此外，跟踪进程间依赖关系的能力对入侵容忍系统设计是一个重要的研究贡献。

通过一个全功能的RFS原型，我们展示了在网络文件服务器遭受恶意攻击或操作错误后，恢复时间减少到了几分钟或几小时的水平，并且大多数有用的工作得以保留。当输入工作负载中的更新请求百分比低于30%时，RFS的运行时吞吐量损失小于6%。RFS的硬件成本与受保护的NFS服务器大致相同。

从安全角度来看，RFS架构的一个主要弱点是容易受到拒绝服务攻击。如果攻击者不断更新甚至单个文件块，最终会填满撤销日志并有效禁用RFS提供的保护。一个简单的解决方案是增加日志磁盘空间，并支持早期警告系统管理员，确保日志磁盘永远不会满。我们计划探索的一个更复杂的解决方案是调节单个用户消耗撤销日志磁盘空间的速度，使用户只能消耗与其配额允许的撤销日志磁盘空间，从而永远不可能耗尽整个撤销日志磁盘空间。

### 致谢
感谢ECSL同事在论文撰写和测试平台搭建方面的帮助。感谢匿名审稿人提出的宝贵意见。这项研究得到了NSF奖项MIP-9710622、IRI-9711635、EIA-9818342、ANI-9814934和ACI-9907485的支持，以及USENIX学生研究基金，以及Sandia国家实验室、路透信息技术公司、Computer Associates/Cheyenne公司、国家标准与技术研究院、西门子和Rether Networks公司的资助。

### 参考文献
[1] The Advanced Maryland Automatic Network Disk Archiver.
(http://www.amanda.org/).

[2] Home of the tripwire open source project.
(http://www.tripwire.org/).

[3] Server message block protocol (SMB).
(http://ourworld.compuserve.com/homepages/timothydevans/smb.htm).

[4] NFS: Network file system protocol specification. Sun Microsystems, Mar 1989.

[5] A. Brown and D. Patterson. To err is human. In Proceedings of the First Workshop on Evaluating and Architecting System dependability (EASY '01), July 2001.

[6] A. Brown and D. Patterson. Embracing failure: A case for recovery-oriented computing (ROC). In 2001 High Performance Transaction Processing Symposium, October 2001.

[7] A. B. et al. Including the human factor in dependability benchmarks. In 2002 DSN Workshop on Dependability Benchmarking, June 2002.

[8] D. P. et al. Recovery-oriented computing (ROC): Motivation, definition, techniques, and case studies. In UC Berkeley Computer Science Technical Report, March 2002.

[9] D. S. S. et al. Deciding when to forget in the elephant file system. In Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles, pages 110–123, December 12-15, 1999.

[10] G. G. et al. Survivable storage systems. DARPA Information Survivability Conference and Exposition, IEEE, 2:184–195, August 2001.

[11] J. S. et al. Self-securing storage: Protecting data in compromised systems. In Proceedings of the 2000 OSDI Conference, October 2000.

[12] J. W. et al. Survivable information storage systems. IEEE Computer, 2(1):61–68, August 2000.

[13] P. A. et al. Surviving information warfare attacks on databases. In Proceedings of IEEE Computer Society Symposium on Security and Privacy, pages 110–123, May, 1997.

[14] P. L. et al. Rewriting histories: Recovering from malicious transactions. Distributed and Parallel Databases, 8:7–40, Jan 2001.

[15] P. L. et al. Intrusion confinement by isolation in information systems. In IFIP WG 11.3 13th Working Conference on Database Security, pages 26–28, July 1999.

[16] S. L. Gaede. Perspectives on the SPEC SDET benchmark. Department of Computer Science, SUNY Stony Brook, Jan 1999.
(http://www.specbench.org/osg/sdm91/sdet/SDETPerspectives.html).

[17] R. J.T. Human error. Cambridge University Press, New York, 1990.

[18] P. Leach and D. Perr. CIFS: A common internet file system. Microsoft Interactive Developer, Nov 1996.

[19] P. Liu. Architectures for intrusion tolerant database systems. In submitted, pages 110–123, 2002.

[20] D. Pilania and T. Chiueh. Design, implementation and evaluation of an intrusion resilient database system. SUNYSB Computer Science ECSL Technical Report TR-124, 2002.
(http://www.ecsl.cs.sunysb.edu/tech reports.html).

[21] B. L. Rodrigo Rodrigues, Miguel Castro. Base: Using abstraction to improve fault tolerance. In Symposium on Operating Systems Principles, 2001.

[22] S. D. S. Quinlan. Venti: a new approach to archival storage. In USENIX Conference on File and Storage Technologies, Jan 2000.

---

**注：** 以上文本经过优化，使得内容更加清晰、连贯和专业。参考文献部分保持原样，未进行更改。