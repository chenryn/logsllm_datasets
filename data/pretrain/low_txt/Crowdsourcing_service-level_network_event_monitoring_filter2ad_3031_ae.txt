### Figure 8: Concurrent Online Peers in This Study

### Figure 9: Robustness of Likelihood Ratios to Various Parameter Settings
- **(a) Deviation = 1.5σ, Window Size = 10**
- **(b) Deviation = 2.2σ, Window Size = 20**

**Figure 9** illustrates that likelihood ratios (LRs) are robust to different parameter settings. For small deviations and window sizes (as in (a)), LRs detect network problems at most 2.15% of the time, while for larger deviations and window sizes (as in (b)), they detect problems at most 0.75% of the time. This suggests that the number of network events detected using an LR threshold should not significantly change with different local detection settings.

### 6. Deployment Details
The NEWS plugin for Vuze is written in Java, with the core classes for event detection comprising approximately 1,000 lines of code (LOC). Released under an open-source (GPL) license, the plugin has been installed over 45,000 times since its release in March 2008. In this section, we discuss the details of our NEWS implementation in its current deployment, including specific algorithms and settings used for event detection, as well as lessons learned through deployment experience.

#### Local Detection
NEWS detects local events using the moving average technique discussed in Section 4.3.1, which uses the window size (w) and standard-deviation multiplier (t) parameters to identify edges in BitTorrent transfer rate signals. Currently, NEWS uses w = 10, 20 samples and t = 2.0, 2.5, 3.0 in parallel, with dynamically configurable settings that have proven effective.

In practice, BitTorrent often saturates a user’s access link, leading to stable transfer rates and small σ. As a result, a moving-average technique may detect events in throughput signals even when there are negligible relative performance changes. To address this issue, NEWS includes a secondary detection threshold that requires a signal value to change by at least 10% before detecting an event.

Throughput signals also undergo phase changes, during which a moving average can detect consecutive events. NEWS treats these as a single event; if enough consecutive events occur, it assumes the signal has undergone a phase change and resets the moving average using only signal values after the phase change.

After detecting a local event, NEWS generates a report containing:
- The user’s per-session ID
- Window size (w)
- Standard-deviation multiplier (t)
- A bitmap indicating the performance signals generating events
- The current event detection rate (Lh)
- The time period for the observed detection rate
- The current time (in UTC)
- The version number for the report layout

The current report format consumes 38 bytes.

The plugin disseminates these reports using Vuze’s built-in Kademlia-based DHT, a key-value store that maintains multiple values for each key. To facilitate group corroboration of locally detected events, we use network locations as keys and the corresponding event reports as values.

In our deployment, we found variable delays between event detection and reporting, as well as significant clock skew. To address these issues, NEWS uses NTP servers to synchronize clocks once per hour, reports event times using UTC timestamps, and considers any events that occurred within a five-minute window when determining the likelihood of a network event occurring.

#### Group Corroboration
After a NEWS peer detects a local event, it performs corroboration by searching the DHT for other event reports in the host’s regions—currently its BGP prefix and ASN. Before using a report from the DHT for corroboration, NEWS ensures that:
1. The report was not generated by this peer.
2. The report was generated recently.
3. The standard-deviation multiplier and window size for detecting the event match a local detection setting.

If these conditions are met, the report’s ID is added to the set of recently reported events for the corresponding detection setting. If a peer finds events from at least three other concurrent peers (a configurable threshold), it uses Equation 3 to determine the likelihood of these events happening by coincidence. Using the information gathered from events published to the DHT over time, the peer can calculate the likelihood ratio described in Section 4.3.2.

If the likelihood ratio is greater than two (also configurable), the monitor issues a notification about the event.

To account for delays between starting a DHT write and its value being available for reading, NEWS sets a timer and periodically rechecks the DHT for events during a configurable period of interest (currently one hour).

Finally, our likelihood ratio calculation requires access to the local detection rate for each online peer. To ensure it is available, each peer writes its local detection rate to distributed storage at least once per hour, regardless of whether it has yet detected a local event during its current session.

#### Third-Party Interface
Beyond end-users, network operators should be notified to handle service-level events. With this in mind, we have implemented a DHT crawler (NEWS Collector) that any third party can run to gather and analyze local event reports. To demonstrate its effectiveness, we built NEWSight—a system that accesses live event information gathered from NEWS Collector and publishes detected events through a public Web interface. NEWSight allows network operators to search for events and register for event notifications. Operators responsible for affected networks can confirm or explain detected events.

Vuze already collects the host’s prefix and ASN; we are currently adding support for whois information. While NEWS crowdsources event detection, NEWSight can be viewed as an attempt at crowdsourcing network event labeling. Confirmed events can help improve the effectiveness of our approach and similar approaches, addressing the paucity of labeled data available in this domain [28]. We are currently beta-testing this interface with ISPs; the interface and its data are publicly available.

#### Overhead for Participating Hosts
NEWS passively monitors performance and uses low-cost event-detection techniques, resulting in negligible overhead for detecting local events. The primary sources of overhead are calculating the union probability (CPU/memory) and sharing locally detected events (network). We now demonstrate that these overheads are reasonably low.

For determining the union probability, the formula in Equation (3) specifies nCn/2 (n choose n/2) operations, where n is the number of hosts in the network having a nonzero probability of detecting an event. We use Miller’s algorithm [25], an optimal trade-off between memory, O(n), and computation, O(n^3). While a substantial improvement over a naïve implementation, its processing overhead can still be significant for large n (e.g., n > 50). To bound this, we limit the number of hosts used in the computation to the H hosts with the largest Lh, conservatively estimating an upper bound for Pu for the full set of n hosts.

The other source of overhead is using distributed storage to share locally detected events. While this overhead is variable and dependent on factors including the target network and detection settings, we found it to be reasonably low for many settings. For example, our analysis shows that read and write operations are performed by each host with average frequencies on the order of several minutes, and in the worst case once every 30 seconds (less than 4 B/s for each peer in the BT Yahoo network).

### 7. Related Work
The problem of detecting network events (or anomalies) has attracted a large number of research efforts. In this context, CEM is a framework for online detection of network events that impact performance for applications running on end systems. This section classifies key properties of event detection systems and describes how CEM relates to previous work in these areas.

#### Crowdsourcing
Central to our approach is the idea of crowdsourcing event detection to ensure good coverage and accuracy at the scale of hundreds of thousands of users. This model has successfully enabled projects that include solving intractable or otherwise prohibitively expensive problems using human computation. Unlike these examples, our system passively monitors network activity from each member of a crowd but does not require human input. Dash et al. [10] use a similar model to improve the quality of intrusion detection systems in an enterprise network and demonstrate its effectiveness through simulation using traffic data from 37 hosts inside their enterprise network.

#### Event Types
A class of previous work focuses on detecting network events in or near backbone links, using data gathered from layer-3 and below [13, 18, 19, 21, 29]. While these monitors can accurately detect a variety of events, they may miss silent failures (e.g., incompatible QoS/ACL settings) and their impact on performance. Other work focuses on detecting network events from a distributed platform [2, 16, 20, 36]. These solutions do not correlate these events with user-perceived performance, and their detection is limited by their network visibility and/or the overhead for probing large numbers of networks. The goal of CEM is to detect service-level network events and correlate their impact on application performance from the perspective of end users.

#### Monitoring Location
CEM targets events that impact user-perceived application performance by running on the end systems themselves. While several researchers have proposed using end-host probing to identify routing disruptions and their effect on end-to-end services [12, 16, 35, 37], they have focused on GREN [20, 36] or enterprise [10, 15, 26] environments and have not looked at the impact of network events on application performance nor addressed the issues of scalability when running on end systems. Some commercial network monitoring tools generate flows that simulate protocols used by edge systems (e.g., Keynote and IneoQuest [8]). While these can indeed detect end-to-end performance problems, these tools require controllable, dedicated infrastructure and are inherently limited to relatively small deployments in PoPs. Our CEM approach does not require any new infrastructure, nor control of end systems, and thus can be installed on systems at the edge of the network. Several research efforts have investigated the idea of active and passive network measurement from end users, e.g., DIMES [31] and Neti@home [32], but have not explored the use of their monitoring information for online network event detection.

#### Measurement Technique
CEM focuses on passive monitoring of popular applications to detect events, which allows our approach to scale to the vast numbers of users at the edge of the network while still detecting events quickly. In a similar vein, previous work has suggested that the volume and breadth of P2P systems’ natural traffic could be sufficient to reveal information about the used network paths without requiring any additional measurement overhead [9, 36]. PlanetSeer [36] uses passive monitoring of a CDN deployed on PlanetLab but relies on active probes to characterize the scope of the detected events. Casado et al. [3] and Isdal et al. [14] use opportunistic measurement to reach these edges of the network, by leveraging spurious traffic or free-riding in BitTorrent. Unlike these efforts, CEM takes advantage of the steady stream of natural, (generally) benign traffic generated by applications. Approaches that use active monitoring (e.g., [2, 16]) are limited by the overhead for detection, which grows with the number of monitored networks and services. While CEM could be combined with limited active probes to assist in characterizing and localizing network events, it does not require them.

### 8. Conclusion
The user experience for networked applications is becoming an important benchmark for customers and network providers. To assist operators with resolving such issues in a timely manner, we argued that the most appropriate place for monitoring service-level events is at the end systems where the services are used. We proposed a new approach, called CEM for Crowdsourcing Event Monitoring, based on pushing end-to-end performance monitoring and event detection to the end systems themselves. We presented a general framework for CEM systems and demonstrated its effectiveness using a large dataset of diagnostic information gathered from peers in the BitTorrent system, along with confirmed network events from two different ISPs. We showed that our crowdsourcing approach enables worldwide event detection, including events spanning multiple networks. Finally, we designed, implemented, and deployed a BitTorrent extension that performs online event detection using our approach—currently installed more than 45,000 times. Having demonstrated the feasibility and effectiveness of this approach, we are investigating opportunities for porting it to other host applications such as VoIP and streaming video.

### Acknowledgments
We thank our shepherd Kevin Jeffay and the anonymous reviewers for their insightful comments. We also thank Kobus van der Merwe, Jennifer Yates, Lorenzo Alvisi, Yan Chen, and Peter Dinda for their valuable advice, and John Otto and Zach Bischof for early feedback on the paper. We are especially grateful to the users/adopters of our software for their invaluable data. This work was supported in part by the National Science Foundation under grants CNS 0644062 and CNS 0917233.

### References
[1] AMAZON. Amazon Mechanical Turk. http://www.mturk.com/.
[2] ANDERSEN, D., BALAKRISHNAN, H., KAASHOEK, F., AND MORRIS, R. Resilient overlay networks. In Proc. ACM SOSP (2001).
[3] CASADO, M., GARFINKEL, T., CUI, W., PAXSON, V., AND SAVAGE, S. Opportunistic measurement: Extracting insight from spurious traffic. In Proc. HotNets (2005).
[4] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. Secure routing for structured peer-to-peer overlay networks. In Proc. USENIX OSDI (2002).
[5] CHEN, K., CHOFFNES, D., POTHARAJU, R., CHEN, Y., BUSTAMANTE, F., AND ZHAO, Y. Where the sidewalk ends: Extending the Internet AS graph using traceroutes from P2P users. In Proc. ACM CoNEXT (2009).
[6] CHEN, X., ZHANG, M., MAO, Z. M., AND BAHL, P. Automating network application dependency discovery: Experiences, limitations, and new solutions. In Proc. USENIX OSDI (2008).
[7] CHOFFNES, D., AND BUSTAMANTE, F. Pitfalls for testbed evaluations of Internet systems. SIGCOMM Comput. Commun. Rev. 40, 2 (April 2010).
[8] CHOFFNES, D. R., AND BUSTAMANTE, F. E. Taming the torrent: A practical approach to reducing cross-ISP traffic in peer-to-peer systems. In Proc. ACM SIGCOMM (2008).
[9] COOKE, E., MORTIER, R., DONNELLY, A., BARHAM, P., AND ISAACS, R. Reclaiming network-wide visibility using ubiquitous endsystem monitors. In Proc. USENIX ATC (2006).
[10] DASH, D., KVETON, B., AGOSTA, J. M., SCHOOLER, E., CHANDRASHEKAR, J., BACHARCH, A., AND NEWMAN, A. When gossip is good: Distributed probabilistic inference for detection of slow network intrusions. In Proc. AAAI (2006).
[11] DISCHINGER, M., MARCON, M., GUHA, S., GUMMADI, K. P., MAHAJAN, R., AND SAROIU, S. Glasnost: Enabling end users to detect traffic differentiation. In Proc. USENIX NSDI (2010).
[12] FEAMSTER, N., ANDERSEN, D., BALAKRISHNAN, H., AND KAASHOEK, M. F. Measuring the effect of Internet path faults on reactive routing. In Proc. ACM SIGMETRICS (2003).
[13] IANNACCONE, G., NEE CHUAH, C., MORTIER, R., BHATTACHARYYA, S., AND DIOT, C. Analysis of link failures in an IP backbone. In Proc. ACM IMW (2002).
[14] ISDAL, T., PIATEK, M., KRISHNAMURTHY, A., AND ANDERSON, T. Leveraging BitTorrent for end host measurements. In Proc. PAM (2007).
[15] KANDULA, S., MAHAJAN, R., VERKAIK, P., AGARWAL, S., PADHYE, J., AND BAHL, P. Detailed diagnosis in enterprise networks. In Proc. ACM SIGCOMM (2009).
[16] KATZ-BASSETT, E., MADHYASTHA, H. V., JOHN, J. P., KRISHNAMURTHY, A., WETHERALL, D., AND ANDERSON, T. Studying black holes in the Internet with Hubble. In Proc. USENIX NSDI (2008).
[17] KEYNOTE. http://www.keynote.com/.
[18] LABOVITZ, C., AHUJA, A., AND JAHANIAN, F. Experimental study of Internet stability and wide-area backbone failure. Tech. Rep. CSE-TR-382-98, U. of Michigan, 1998.
[19] LAKHINA, A., CROVELLA, M., AND DIOT, C. Diagnosing network-wide traffic anomalies. In Proc. ACM SIGCOMM (2004).
[20] MADHYASTHA, H. V., ISDAL, T., PIATEK, M., DIXON, C., ANDERSON, T., KIRSHNAMURTHY, A., AND VENKATARAMANI, A. iPlane: An information plane for distributed systems. In Proc. USENIX OSDI (2006).
[21] MAHAJAN, R., WETHERALL, D., AND ANDERSON, T. Understanding BGP misconfiguration. In Proc. ACM SIGCOMM (2002).
[22] MAHAJAN, R., ZHANG, M., POOLE, L., AND PAI, V. Uncovering performance differences among backbone ISPs with Netdiff. In Proc. USENIX NSDI (2008).
[23] MAHIMKAR, A., GE, Z., SHAIKH, A., WANG, J., YATES, J., ZHANG, Y., AND ZHAO, Q. Towards automated performance diagnosis in a large IPTV network. In Proc. ACM SIGCOMM (2009).
[24] MAYMOUNKOV, P., AND MAZIERES, D. Kademlia: A peer-to-peer information system based on the XOR metric. In Proc. IPTPS (2002).
[25] MILLER, G. D. Programming techniques: An algorithm for the probability of the union of a large number of events. Commun. ACM 11, 9 (1968).
[26] PADMANABHAN, V. N., RAMABHADRAN, S., AND PADHYE, J. NetProfiler: Profiling wide-area networks using peer cooperation. In Proc. IPTPS (2005).
[27] RABINOVICH, M., TRIUKOSE, S., WEN, Z., AND WANG, L. DipZoom: The Internet measurements marketplace. In Proc. IEEE INFOCOM (2006).
[28] RINGBERG, H., SOULE, A., AND REXFORD, J. WebClass: Adding rigor to manual labeling of traffic anomalies. SIGCOMM Comput. Commun. Rev. 38, 1 (2008).
[29] ROUGHAN, M., GRIFFIN, T., MAO, Z. M., GREENBERG, A., AND FREEMAN, B. IP forwarding anomalies and improving their detection using multiple data sources. In Proc. of the ACM SIGCOMM workshop on Network troubleshooting (2004).
[30] SCHULZE, H., AND MOCHALSKI, K. Internet Study 2008/2009, 2009.
[31] SHAVITT, Y., AND SHIR, E. DIMES: Let the Internet measure itself. SIGCOMM Comput. Commun. Rev. 35, 5 (2005).
[32] SIMPSON, JR, C. R., AND RILEY, G. F. Neti@home: A distributed approach to collecting end-to-end network performance measurements. In Proc. PAM (2004).
[33] VON AHN, L. Human computation. In Proc. DAC (2009).
[34] VUZE, INC. Vuze. http://www.vuze.com.
[35] WU, J., MAO, Z. M., REXFORD, J., AND WANG, J. Finding a needle in a haystack: Pinpointing significant BGP routing changes in an IP network. In Proc. USENIX NSDI (2005).
[36] ZHANG, M., ZHANG, C., PAI, V., PETERSON, L., AND WANG, R. PlanetSeer: Internet path failure monitoring and characterization in wide-area services. In Proc. USENIX OSDI (2004).
[37] ZHANG, Y., MAO, Z. M., AND ZHANG, M. Effective diagnosis of routing disruptions from end systems. In Proc. USENIX NSDI (2008).