### Reducing the Size of the Beacon Set

**Observation 1:**
Consider a path \((u_0, u_1, \ldots, u_k)\) of high-arity nodes in the network, where each node \(u_i\) (for \(1 \leq i \leq k-1\)) is only connected to \(u_{i-1}\) and \(u_{i+1}\). This means that any message from an interior node on this path must traverse to the outside network through either \(u_0\) or \(u_k\). Consequently, the set of high-arity nodes minus \(\{u_1, \ldots, u_{k-1}\}\) can still serve as a beacon set.

**Explanation:**
Every message originating from an interior node on the path will pass through either \(u_0\) or \(u_k\), except for messages destined to another node \(u_i\) on the same path. However, these internal edges can be tested by sending a message from \(u_0\) to \(u_k\) along the link \((u_0, u_1)\). Therefore, \(u_0\) and \(u_k\) are sufficient for the beacon set, and the interior nodes can be omitted.

This observation significantly reduces the size of the beacon set. According to the Asia Pacific Network Information Centre (APNIC) on May 5, 2001, there are approximately 1,500 ASs providing transit on the Internet [6]. Large multihomed ASs may have more than one beacon node even after applying the above observations. For example, in the case of NSPs, one would expect roughly one beacon per peering point (public or private), plus additional beacons for every cycle in the network. Publicly available maps of major backbones indicate that most cycles in NSP networks contain at least one peering point, assuming multiple direct links between two points are treated as a single bundle. Thus, the Internet is mostly tree-like, with exceptions at public peering points and very short redundant paths such as FDDI rings and n × m fabrics at PoPs or between core and border routers [38].

Therefore, we can further reduce the beacon set to approximately the nodes at peering points. Placing a beacon on each peering point and border router of a multihomed AS is likely to provide a good approximation of a beacon set.

Data collected at Internap and statistics released by APNIC suggest that the total number of multihomed networks is in the order of 10,000 to 20,000. A recent IETF internet draft [34] suggests capping the total number of multihomed networks at 2^15 or approximately 32,000. Hence, we can expect that between 1,500 and 20,000 beacon nodes are sufficient to cover the entire network. This number, while large, is much smaller than the total number of hosts, estimated at 171 million as of January 2003 [24, 33], and is within the economic reach of a large commercial Internet organization.

### High-Arity Nodes

We have focused on the role of high-arity nodes as part of the infrastructure required to measure connectivity and performance path characteristics on a network like the Internet. Due to their strategic placement, a beacon set also plays a key role in realizing a Resilient Overlay Network (RON) [5] with a performance-based routing policy.

The BGP protocol allows for path aggregation, which reduces the size of routing tables. However, the lack of explicit performance characteristics means that the path chosen by BGP is not necessarily optimal in terms of latency. This is further complicated by deviations from the AS-hop metric due to considerations such as redundancy, cost of bandwidth, and lack of visibility into network performance. Latency and packet loss are often critical factors in user bandwidth requirements [28].

Some commercial organizations, such as Internap and Sockeye, provide some level of performance improvements over standard BGP routing heuristics using network route optimization. These improvements are partially constrained by the imprecise granularity of BGP routing policy and lack of control across the network. Alternatively, it is possible to deploy a performance-based routing protocol network overlaid on the public Internet using tunneling across strategically placed nodes. Since high-arity nodes have access to all paths, placing forwarding-tunnel router nodes on this set can increase the granularity of routing decisions.

**Claim 5:**
The set of nodes of higher arity in a network forms an all-paths set.

**Proof:**
From the proof of Claim 4, we know that all bifurcation points on the network are of higher arity. Therefore, all nodes where a routing policy can be implemented are part of the set of higher-arity nodes, and the nodes absent from this set are those where paths are uniquely determined.

Consider the default path \(RP(u, v) = u, u_1, \ldots, v\) from a node \(u\) to a node \(v\), and an alternative, desired path \(P(u, v) = u, w_1, w_2, w_3, \ldots, v\). Let \(w_i\) be the first node where the two paths differ. Since \(w_i\) is a node of higher arity, we can send a message from \(u\) to \(w_i\) following the path \(RP(u, v)\) up to node \(w_i\). At this point, since \(w_i\) is of higher arity and thus part of the forwarding-tunnel router nodes, it can forward the message to \(v\) via \(w_{i+1}\).

Repeating this process with \(RP(w_{i+1}, v)\) and the path \(P'(w_{i+1}, v) = w_{i+1}, w_{i+2}, \ldots, v\), we determine the first node where they differ, which must also be a forwarding-tunnel router node. After each iteration, we obtain a routing path from \(u\) to a node \(w_j\) with \(j > j-1\). This recursion must end after a finite number of steps, resulting in a set of forwarding-tunnel router nodes realizing the path \(P(u, v)\).

As in the case of the beacon set, Observation 1 can also be used to reduce the size of the high-arity set while maintaining the all-paths set property.

### Conclusions

We have shown that computing the minimum number of beacons required to test the status of every link is NP-hard. This number is also hard to approximate and potentially as large as one-third of the nodes on an arbitrary network. An alternative heuristic tailored for the topology of the public Internet using high-arity nodes is proposed. This would form a beacon set that can test for connectivity on all relevant edges of the network. Furthermore, such a set has properties that allow for further reduction in the number of required nodes. The high-arity set can also be used as a forward-tunneling set for all-paths routing on the public Internet, creating a QoS-based RON.

### References

[1] A. Adams, T. Bu, R. Caceres, N. Dufﬁeld, T.Friedman, J. Horowitz, F. Lo Presti, S.B. Moon, V. Paxson, D. Towsley. The Use of End-to-end Multicast Measurements for Characterizing Internal Network Behavior, IEEE Comm., 2000.

[2] A. Adams, J. Mahdavi, M. Mathis, and V. Paxson, Creating a Scalable Architecture for Internet Measurement. Proc. 8th Internet Society Conf. (INET), 1998.

[3] A. Adams, and M. Mathis. A system for flexible network performance measurement. Proc. 10th INET Conf., 2000.

[4] M. Adler, T. Bu, R. K. Sitaraman, D. F. Towsley. Tree Layout for Internal Network Characterizations in Multicast Networks. Networked Group Comm., 2001, pp. 189-204.

[5] D. G. Andersen, H. Balakrishnan, M.F. Kaashoek, R. Morris. Resilient Overlay Networks. Proc. 18th ACM Symp. on Operating Syst. Princ., 2001.

[6] Asia Pacific Network Information Centre (APNIC). Daily BGP statistics. http://www.apnic.net/stats/bgp. May 5, 2001.

[7] Cooperative Association for Internet Data Analysis (CAIDA). The Skitter Project. http://www.caida.org/tools/measurement/skitter/index.html, 2001.

[8] P. Barford, A. Bestavros, J. W. Byers, M. Crovella. On the marginal utility of network topology measurements. Internet Measurement Workshop, 2001, pp. 5-17.

[9] O. Bonaventure, S. De Cnodder, J. Haas, B. Quoitin, R. White. Controlling the redistribution of BGP routes. Internet draft.

[10] S. Branigan, H. Burch, B. Cheswick, and F. Wojcik. What Can You Do with Traceroute? Internet Computing, vol. 5, no. 5, 2001, page 96ff.

[11] T. Bu, N. G. Dufﬁeld, F. Lo Presti, D. F. Towsley. Network tomography on general topologies. ACM Int. Conf. on Measurements and Modeling of Comp. Systems (SIGMETRICS) 2002, pp. 21-30.

[12] R. Caceres, N.G. Dufﬁeld, J. Horowitz, and D. Towsley. Multicast-based inference of network internal loss characteristics. IEEE Transactions on Information Theory, v.45, n.7, 1999, pp. 2462-2480.

[13] Bill Cheswick, Hal Burch, and Steve Branigan. Mapping and Visualizing the Internet. Proc. USENIX Technical Conf., 2000.

[14] K. Claffy, G. Miller and K. Thompson. The nature of the beast: recent traffic measurements from an Internet backbone. Proc. 8th Internet Soc. Conf. (INET), 1998.

[15] K. Claffy, T.E. Monk and D. McRobb. Internet Tomography. Nature, 7th January 1999.

[16] X. Deng. Short Term Behaviour of Ping Measurements. MSc thesis, Univ. of Waikato, 1999.

[17] N. Feamster, J. Borkengham, J. Rexford. Controlling the Impact of BGP Policy Changes on IP Traffic. Technical Memorandum, AT&T Labs Research.

[24] Internet Software Consortium. http://www.isc.org/ds/WWW-200301/index.html.

[25] S. Jamin, C. Jin, Y. Jin, D. Raz, Y. Shavitt, L. Zhang. On the Placement of Internet Instrumentation. IEEE Conf. on Comp. Comm. (INFOCOM), 2000, pp. 295-304.

[26] S. Kalidindi and M. J. Zekauskas. Surveyor: An infrastructure for Internet performance measurements. Proc. 9th Internet Soc. Conf. (INET), 1999.

[27] G.R. Malan and F. Jahanian. An extensible probe architecture for network protocol performance measurement. Proc. ACM Conf. on Applications, Technologies Architectures and Protocols for Comp. Comm. (SIGCOMM), 1998.

[28] A. Odlyzko. The current state and likely evolution of the Internet. Proc. Globecom’99, IEEE, pp. 1869-1875, 1999.

[29] V. Paxson. Measurements and Analysis of End-to-End Internet Dynamics. PhD thesis, Univ. of Cal., Berkeley, 1997.

[30] V. Paxson. End-to-End routing behaviour in the Internet. IEEE/ACM Transactions on Networking. 5, 601-618 (1997).

[31] V. Paxson, J. Mahdavi, A. Adams and M. Mathis, An Architecture for Large-Scale Internet Measurement. IEEE Comm., v.36, n.8, 1998, pp. 48-54.

[32] R. Raz and S. Safra. “A sub-constant error-probability low-degree test, and sub-constant error-probability PCP characterization of NP”, Proc. 29th ACM Symp. on the Theory of Computing (STOC), 475-484, (1997).

[33] A. Scherrer. 127,781,000 Internet Hosts: How Matrix.net gets its host counts. http://www.matrix.net/isr/library/how_matrix_gets_its_host_counts.html, 2001, Access: May 2001.

[34] P. Savola. Multihoming using IPv6 addressing derived from AS numbers. draft-savola-multi6-asn-pi-00.txt, work in progress. IETF internet draft, January 2003.

[35] S. Seshan, M. Stemm, and R.H. Katz. SPAND: Share Passive Network Performance Discovery. Proc. 1st Usenix Symp. on Internet Technologies and Systems, 1997.

[36] R. Siamwalla, R. Sharma, and S. Keshav. Discovering Internet Topology. Technical Report, Cornell Univ., July 1998.

[37] D. Towsley. Network tomography through to end-to-end measurements. Abstract in Proc. 3rd Workshop on Algorithm Engineering and Experiments (ALENEX), 2001.

[38] X. Xiao and L. M. Ni. Reducing routing table computation cost in OSPF. Proc. 9th Internet Society Conf. (INET), 1999.