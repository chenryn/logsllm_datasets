### Adjusting Clusters to Improve False-Negative and False-Positive Rates

By adjusting the clusters, the false-negative rate decreases from 31.4% to 7.6%, and the false-positive rate declines from 5.1% to 4.2%. This improvement demonstrates the effectiveness of our approach.

### Case Scenario 3: Cloud System Component Interactions

In cloud systems, components are often developed by different teams. When engineers from one team invoke public interfaces of another component, they may inadvertently call unsuitable methods, leading to extra performance costs. Detecting such faults is challenging, especially when the function itself is correct. In this case study, we investigate how our approach can help developers diagnose these performance anomalies.

During system operation, we manually injected a fault using a hot patch. The fault caused clients of the storage service to call an interface with additional authorization logic, resulting in more than double the overhead due to unnecessary authorization processes. This led to a significant decrease in the performance of the `ReadFile` operation during performance regression tests.

In this scenario, there were three major clusters, each containing approximately four principal methods (Figure 7(c)). Our approach precisely identified the relevant method (the one with the highest score) as shown in Figure 8(c). The developer could easily locate the mistake based on this clue. Additionally, Figure 9(c) illustrates the dissimilarity ratios of instances for this method, with the first three instances differing significantly from others. This highlights that the influence scopes of faults in cloud computing systems vary, and without identifying the exact anomalous instances, debuggers would need to expend more effort to pinpoint the issue.

Table 2 shows that dropping requests within minor clusters would significantly increase the false-negative rate, indicating that anomalous methods within minor clusters would not be fully identified without cluster adjustment.

### Case Scenario 4: Real-World Diagnosing Process

This scenario involves a real-world diagnosing process in a production cluster at Alibaba Cloud Computing Company. Performance bugs related to user behaviors are difficult to detect in the testing environment, as they only manifest with specific user actions. An operator applied our approach to diagnose this problem.

The average latencies of the `SaveFile` operation increased about two-fold, and this situation persisted for several hours. Using our approach, the most suspicious methods were identified, as shown in Figure 8(d). The top-ranked method was used to lock the file ID before the transaction of saving a file began. Figure 9(d) indicates that the dissimilarity ratio of the first instance was exceptionally high, pointing to the physical location of the top anomalous method. Following this clue, the root cause was efficiently identified. The older load balancing mechanism did not adequately consider access patterns, leading to over 60% of accesses being concentrated into one instance, causing a significant decline in the performance of the `SaveFile` operation.

From Table 2, we observe that adjusting the clusters slightly increases the false-positive rate; however, the suspicious scores of the normal methods mistakenly flagged as anomalies are low (ranking 5th and 6th in Figure 8(d)), thus not affecting the overall diagnosing result. The false-negative rate also decreased from 21.4% to 12.7%.

### Conclusions

When a system performance anomaly occurs, it is typically a labor-intensive task for operators to locate the problematic parts. Isolating the physical locations (i.e., instances) of anomalous methods can significantly reduce the manual effort required to identify the root cause. Performance anomalies often lead to changes in response latencies of user requests. The hidden connections among the vast number of runtime request execution paths usually contain valuable information for diagnosing performance issues.

In this paper, we propose an approach to localize anomalous methods and their physical locations by analyzing request trace logs. This approach does not require specific domain knowledge from the operators. To demonstrate its effectiveness, we report our experiences in diagnosing four real-world performance anomalies in the Alibaba cloud computing platform. The experimental results show that our approach can accurately locate the primary causes of performance anomalies with low false-positive and false-negative rates.

### Acknowledgements

This research was supported by the National Basic Research Program of China (Grant No. 2011CB302600), the National High Technology Research and Development Program of China (Grant No. 2012AA011201), and the National Natural Science Foundation of China (Grant Nos. 61161160565, 90818028, 91118008, 60903043). Yangfan Zhou and Michael Lyu’s work was supported by the National Natural Science Foundation of China (Grant No. 61100077), the Basic Research Program of Shenzhen (Grant No. JC201104220300A), and the Research Grants Council of Hong Kong (Project No. N CUHK405/11).

### References

1. Lu X, Wang H, Wang J, et al. Internet-based virtual computing environment: beyond the data center as a computer. Futur Gener Comp Syst, 2013, 29: 309–322
2. Han S, Dang Y, Ge S, et al. Performance debugging in the large via mining millions of stack traces. In: Proceedings of the 34th International Conference on Software Engineering, Zurich, 2012. 176–186
3. Chilimbi T, Liblit B, Mehra K, et al. Holmes: Effective statistical debugging via efficient path profiling. In: 31st IEEE International Conference on Software Engineering, Vancouver, 2009. 34–44
4. Killian C, Nagaraj K, Pervez S, et al. Finding latent performance bugs in systems implementations. In: Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering. New York: ACM, 2010. 17–26
5. Lan Z, Zheng Z, Li Y. Toward automated anomaly identification in large-scale systems. IEEE Trans Parallel Distrib Syst, 2010, 21: 174–187
6. Malik H, Adams B, Hassan A. Pinpointing the subsystems responsible for the performance deviations in a load test. In: Proceedings of 21st IEEE International Symposium on Software Reliability Engineering, San Jose, 2010. 201–210
7. Reynolds P, Killian C, Wiener J, et al. Pip: Detecting the unexpected in distributed systems. In: Symposium on Networked Systems Design and Implementation, San Jose, 2006, 115–128
8. Sambasivan R, Zheng A, De Rosa M, et al. Diagnosing performance changes by comparing request flows. In: Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation. Berkeley: USENIX Association, 2011. 43–56
9. Jin G, Song L, Shi X, et al. Understanding and detecting real-world performance bugs. In: The 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation. New York: ACM, 2012. 77–88
10. Thereska E, Ganger G. Ironmodel: Robust performance models in the wild. ACM SIGMETRICS Perform Eval Rev, 2008, 36: 253–264
11. Mi H B, Wang H M, Yin G, et al. Performance problems diagnosis in cloud computing systems via analyzing request trace logs. In: The 13th International Conference on Network Operations and Management Symposium (NOMS), Maui, 2012. 893–899
12. Jolliffe I. Principal Component Analysis, 2nd ed. New York: Springer, 2002
13. Fay M, Proschan M. Wilcoxon-Mann-Whitney or t-test on assumptions for hypothesis tests and multiple interpretations of decision rules. Stat Surv, 2010, 4: 1–39
14. Melville P, Yang S, Saar-Tsechansky M, et al. Active learning for probability estimation using Jensen-Shannon divergence. In: Proceedings of the 16th European Conference on Machine Learning. Berlin/Heidelberg: Springer-Verlag, 2005. 268–279
15. Sigelman B, Barroso L, Burrows M, et al. Dapper, a large-scale distributed systems tracing infrastructure. Technical Report dapper-2010-1, Google, 2010
16. Park I, Buch R. Event tracing—improve debugging and performance tuning with ETW. MSDN Mag, 2007. 81–92
17. Thereska E, Salmon B, Strunk J, et al. Stardust: tracking activity in a distributed storage system. ACM SIGMETRICS Perform Eval Rev, 2006, 34: 3–14
18. Sang B, Zhan J, Lu G, et al. Precise, scalable, and online request tracing for multi-tier services of black boxes. IEEE Trans Parallel Distrib Syst, 2010, 99: 1–16
19. Tak B, Tang C, Zhang C, et al. Vpath: precise discovery of request processing paths from black-box observations of thread and network activities. In: Proceedings of the 2009 Conference on USENIX Annual Technical Conference. Berkeley: USENIX Association, 2009. 19–32
20. Koskinen E, Jannotti J. Borderpatrol: isolating events for black-box tracing. ACM SIGOPS Operat Syst Rev, 2008, 42: 191–203
21. Reynolds P, Wiener J, Mogul J, et al. Wap5: black-box performance debugging for wide-area systems. In: Proceedings of the 15th International Conference on World Wide Web. New York: ACM, 2006. 347–356
22. Aguilera M, Mogul J, Wiener J, et al. Performance debugging for distributed systems of black boxes. ACM SIGOPS Operat Syst Rev, 2003, 37: 74–89
23. Chen M, Kiciman E, Fratkin E, et al. Pinpoint: Problem determination in large, dynamic internet services. In: Proceedings of 32nd IEEE International Conference on Dependable Systems and Networks, Bethesda, 2002. 595–604
24. Chen M, Accardi A, Kiciman E, et al. Path-based failure and evolution management. In: Proceedings of the 1st Conference on Symposium on Networked Systems Design and Implementation, Vol. 1. Berkeley: USENIX Association, 2004. 23–36
25. Barham P, Donnelly A, Isaacs R, et al. Using Magpie for request extraction and workload modeling. In: Proceedings of the 6th Conference on Symposium on Operating Systems Design and Implementation. Berkeley: USENIX Association, 2004. 259–272
26. Fonseca R, Porter G, Katz R, et al. X-trace: A pervasive network tracing framework. In: Proceedings of the 4th USENIX Conference on Networked Systems Design and Implementation. Berkeley: USENIX Association, 2007. 20–33
27. Mi H, Wang H, Yin G, et al. Magnifier: Online detection of performance problems in large-scale cloud computing systems. In: Proceedings of 8th IEEE International Conference on Services Computing, Washington DC, 2011. 418–425
28. Wang C, Schwan K, Talwar V, et al. A flexible architecture integrating monitoring and analytics for managing large-scale data centers. In: Proceedings of the 8th ACM International Conference on Autonomic Computing. New York: ACM, 2011. 141–150
29. Wang C, Viswanathan K, Choudur L, et al. Statistical techniques for online anomaly detection in data centers. In: Proceedings of the 12th IFIP/IEEE International Symposium on Integrated Network Management, Dublin, 2011. 385–392
30. Bodik P, Goldszmidt M, Fox A, et al. Fingerprinting the datacenter: automated classification of performance crises. In: Proceedings of the 5th European Conference on Computer Systems. New York: ACM, 2010. 111–124
31. Wang C, Talwar V, Schwan K, et al. Online detection of utility cloud anomalies using metric distributions. In: Proceedings of the IEEE Network Operations and Management Symposium, Osaka, 2010. 96–103
32. Lakhina A, Crovella M, Diot C. Diagnosing network-wide traffic anomalies. ACM SIGCOMM Comput Commun Rev, 2004, 34: 219–230
33. Xu W, Huang L, Fox A, et al. Detecting large-scale system problems by mining console logs. In: Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles. New York: ACM, 2009. 117–132
34. Oliner A, Aiken A. Online detection of multi-component interactions in production systems. In: 41st IEEE/IFIP International Conference on Dependable Systems & Networks (DSN), Hong Kong, 2011. 49–60
35. Ringberg H, Soule A, Rexford J, et al. Sensitivity of PCA for traffic anomaly detection. ACM SIGMETRICS Perform Eval Rev, 2007, 35: 109–120
36. King J, Jackson D. Variable selection in large environmental data sets using principal components analysis. Environmetrics, 1999, 10: 67–77
37. Ghemawat S, Gobioff H, Leung S. The Google file system. ACM SIGOPS Operat Syst Rev, 2003, 37: 29–43