### 3.2 基于价值的方法和基于策略的方法

在第2章中，我们介绍了深度强化学习中的两类主要策略优化方法：基于价值的方法和基于策略的方法。这两类方法的结合产生了诸如Actor-Critic算法和QT-Opt等混合算法（Kalashnikov et al., 2018），这些算法利用价值函数的估计来辅助策略更新。图3.3展示了它们之间的分类关系。

#### 基于价值的方法
基于价值的方法通常涉及对动作价值函数\(Q^\pi(s, a)\)的优化。优化后的最优值函数表示为：
\[ Q^*(s, a) = \max_{a} Q^\pi(s, a) \]
最优策略可以通过选取最大值函数对应的动作得到：
\[ \pi^* \approx \arg\max_{a} Q^\pi(s, a) \]
其中，“≈”符号反映了由于函数近似导致的误差。

**优点**:
- 相对较高的采样效率
- 值函数估计方差较小
- 不易陷入局部最优

**缺点**:
- 通常难以处理连续动作空间问题
- 最终策略通常是确定性的而非概率分布的形式
- 在如DQN等算法中，ϵ-贪心策略（ϵ-greedy）和max算子容易导致过估计的问题

常见的基于价值的算法包括：
- **Q-learning** (Watkins et al., 1992)
- **Deep Q-Network (DQN)** (Mnih et al., 2015) 及其变体：
  - **Prioritized Experience Replay (PER)** (Schaul et al., 2015): 根据TD误差对数据进行加权采样，提高学习效率。
  - **Dueling DQN** (Wang et al., 2016): 将动作价值函数分解为状态值函数V和优势函数A，以提高函数近似能力。
  - **Double DQN** (Van Hasselt et al., 2016): 使用不同的网络参数进行动作选择和评估，解决过估计问题。
  - **Retrace** (Munos et al., 2016): 修正了Q值的计算方法，减少了估计的方差。
  - **Noisy DQN** (Fortunato et al., 2017): 给网络参数添加噪声，增加智能体的探索能力。
  - **Distributed DQN** (Bellemare et al., 2017): 将状态-动作值估计细化为对状态-动作值分布的估计。

#### 基于策略的方法
基于策略的方法直接对策略进行优化，通过迭代更新策略以实现累积奖励最大化。与基于价值的方法相比，基于策略的方法具有以下优点：
- 策略参数化简单
- 收敛速度快
- 适用于连续或高维的动作空间

常见的基于策略的算法包括：
- **Policy Gradient (PG)** (Sutton et al., 2000)
- **Trust Region Policy Optimization (TRPO)** (Schulman et al., 2015)
- **Proximal Policy Optimization (PPO)** (Heess et al., 2017; Schulman et al., 2017)

TRPO和PPO在策略梯度算法的基础上限制了更新步长，防止策略崩溃，使算法更加稳定。

#### Actor-Critic 方法
除了基于价值的方法和基于策略的方法，更流行的是两者的结合，即Actor-Critic方法。这类方法结合了两种方法的优点：
- 利用基于价值的方法学习Q值函数或状态价值函数V，提高采样效率（Critic）
- 利用基于策略的方法学习策略函数（Actor），从而适用于连续或高维的动作空间

一些常见的Actor-Critic类算法包括：
- **Actor-Critic (AC)** (Sutton et al., 2018)
- **Asynchronous Advantage Actor-Critic (A3C)** (Mnih et al., 2016): 扩展到异步并行学习，提高样本收集速度和训练效率。
- **Deep Deterministic Policy Gradient (DDPG)** (Lillicrap et al., 2015): 沿用DQN的目标网络，同时Actor是一个确定性策略。
- **Twin Delayed DDPG (TD3)** (Fujimoto et al., 2018): 引入截断的Double Q-Learning解决过估计问题，并延迟Actor更新频率以优先提高Critic拟合准确度。
- **Soft Actor-Critic (SAC)** (Haarnoja et al., 2018): 在Q值函数估计中引入熵正则化，以提高智能体的探索能力。

### 3.3 蒙特卡罗方法和时间差分方法

蒙特卡罗（Monte Carlo, MC）方法和时间差分（Temporal Difference, TD）方法的区别已在第2章中讨论过。这里再次总结它们的特点以保证本章的完整性。

**时间差分方法**是动态规划（Dynamic Programming, DP）方法和蒙特卡罗方法的一种中间形式。它使用自举法（Bootstrapping）进行估计，且不需要获取环境模型。与蒙特卡罗方法不同，时间差分方法在每一步动作执行后都可以通过自举法及时更新参数，而蒙特卡罗方法必须等到一条轨迹生成后才能更新。

这种差异使得时间差分方法具有较大的偏差，而蒙特卡罗方法具有较大的方差。

### 3.4 在线策略方法和离线策略方法

在线策略（On-Policy）方法和离线策略（Off-Policy）方法依据策略学习的方式对强化学习算法进行划分（见图3.5）。

**在线策略方法**试图评估并提升与环境交互生成数据的策略，要求智能体与环境交互的策略和要提升的策略相同。常见的在线策略方法是Sarsa，其Q函数更新公式如下：
\[ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] \]

**离线策略方法**评估和提升的策略与生成数据的策略不同。常见的离线策略方法是Q-learning，其Q函数更新公式如下：
\[ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)] \]

### 参考文献
- BALDI, P. (2012). Autoencoders, Unsupervised Learning, and Deep Architectures. In Proceedings of the International Conference on Machine Learning (ICML), 37-50.
- BELLEMARE, M. G., DABNEY, W., & MUNOS, R. (2017). A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR.org: 449-458.
- FORTUNATO, M., AZAR, M. G., PIOT, B., et al. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295.
- FUJIMOTO, S., VAN HOOF, H., & MEGER, D. (2018). Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477.
- HA, D., & SCHMIDHUBER, J. (2018). Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, 2450-2462.
- HAARNOJA, T., ZHOU, A., ABDEEL, P., et al. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.
- HEESS, N., SRIRAM, S., LEMMON, J., et al. (2017). Emergence of locomotion behaviors in rich environments. arXiv:1707.02286.
- KALASHNIKOV, D., IRPAN, A., PASTOR, P., et al. (2018). Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293.
- LI, Y. (2017). Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274.
- LILLICRAP, T. P., HUNT, J. J., PRITZEL, A., et al. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.
- MNIH, V., KAVUKCUOGLU, K., SILVER, D., et al. (2015). Human-level control through deep reinforcement learning. Nature.
- MNIH, V., BADIA, A. P., MIRZA, M., et al. (2016). Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning (ICML), 1928-1937.
- MUNOS, R., STEPLETON, T., HARUTYUNYAN, A., et al. (2016). Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, 1054-1062.
- NAGABANDI, A., KAHN, G., FEARING, R. S., et al. (2018). Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE: 7559-7566.
- RACANIÈRE, S., WEBER, T., REICHERT, D., et al. (2017). Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, 5690-5701.
- SCHAULT, T., QUAN, J., ANTONOGLOU, I., et al. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.
- SCHRITTWIESER, J., ANTONOGLOU, I., HUBERT, T., et al. (2019). Mastering atari, go, chess and shogi by planning with a learned model.
- SCHULMAN, J., LEVINE, S., ABBEEL, P., et al. (2015). Trust region policy optimization. In International Conference on Machine Learning (ICML), 1889-1897.
- SCHULMAN, J., WOLSKI, F., DHARIWAL, P., et al. (2017). Proximal policy optimization algorithms. arXiv:1707.06347.
- SILVER, D., HUANG, A., MADDISON, C. J., et al. (2016). Mastering the game of go with deep neural networks and tree search. Nature.
- SILVER, D., HUBERT, T., SCHRITTWIESER, J., et al. (2017). Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815.
- SILVER, D., HUBERT, T., SCHRITTWIESER, J., et al. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419): 1140-1144.
- SUTTON, R. S., & BARTO, A. G. (2018). Reinforcement learning: An introduction. MIT Press.
- SUTTON, R. S., MCALLESTER, D. A., SINGH, S. P., et al. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 1057-1063.
- VAN HASSELT, H., GUEZ, A., & SILVER, D. (2016). Deep reinforcement learning with double Q-learning. In Thirtieth AAAI Conference on Artificial Intelligence.
- WANG, Z., SCHAULT, T., HESSEM, M., et al. (2016). Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, 1995-2003.
- WATKINS, C. J., & DAYAN, P. (1992). Q-learning. Machine Learning, 8(3-4): 279-292.

### 4. 深度 Q 网络

本章将介绍深度 Q 网络（Deep Q-Network, DQN）算法，这是深度强化学习中最重要的一种算法。我们将从基于时间差分学习的 Q-Learning 算法入手，介绍 DQN 算法及其变体。在本章的最后，我们提供了代码示例，并对 DQN 及其变体进行实验比较。

#### Q-Learning 算法回顾
Q-Learning 是一种离线策略的时间差分算法，在使用表格或线性函数逼近 Q 函数时已被证明可以收敛于最优解。然而，当使用非线性函数逼近器（如神经网络）表示 Q 函数时，Q-Learning 并不稳定，甚至可能发散（Tsitsiklis et al., 1996）。随着深度神经网络技术的发展，DQN 算法解决了这一问题，并推动了深度强化学习的研究。

#### DQN 算法及其变体
在本章中，我们将首先回顾 Q-Learning 的背景，然后详细介绍 DQN 算法及其变体，并给出详细的理论和解释。最后，在 4.8 节中，我们将通过代码展示算法在雅达利游戏上的实现细节与实战表现，为读者提供快速上手的实战学习过程。每种算法的完整代码可以在随书提供的代码仓库中找到。

#### 无模型方法
无模型（Model-Free）方法为解决基于马尔可夫决策过程（MDP）的决策问题提供了一种通用的方法。其中“模型”是指显式地对 MDP 相关的转移概率分布和回报函数建模，而时间差分（TD）学习就是一类无模型方法。在 2.4 节中，我们讨论过，当拥有一个完美的 MDP 模型时，通过递归子问题的最优解，就可以得到动态规划的最优方案。TD 学习也遵循了这样一种思想，即使对子问题的估计并非一直是最优的，我们也可以通过自举（Bootstrapping）来估计子问题的值。

子问题通过 MDP 中的状态表示。在策略 \(\pi\) 下，状态 \(s\) 时的价值 \(v_\pi(s)\) 被定义为从状态 \(s\) 开始，以策略 \(\pi\) 进行动作的预期回报：
\[ v_\pi(s) = \mathbb{E}_\pi \left[ R_t + \gamma v_\pi(S_{t+1}) \mid S_t = s \right] \]
此处的 \(\gamma \in [0, 1]\) 是衰减率。TD 学习用自举法分解上述估计。给定价值函数 \(V: S \to \mathbb{R}\)，TD(0) 是一个最简单的版本，它只应用一步自举，如下所示：
\[ V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right] \]
此处的 \(R_{t+1} + \gamma V(S_{t+1})\) 和 \(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\) 分别被称为 TD 目标和 TD 误差。

策略的评估值提供了一种对策略的动作质量进行评估的方法。为了进一步了解如何选择某一特定状态下的动作，我们将通过 Q 值来评估状态-动作组合的效果。Q 值可以这样被估计：
\[ q_\pi(s, a) = \mathbb{E}_\pi \left[ R_t + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a \right] \]
有了 Q 值对策略进行评估之后，我们只需要找到一种能提升 Q 值的方法就能提升策略的效果。最简单的提升效果的方法就是通过贪心的方法执行动作：\(\pi'(s) = \arg\max_{a'} q_\pi(s, a')\)。由 \(q_{\pi'}(s, a) = \max_{a'} q_\pi(s, a') \geq q_\pi(s, a)\) 我们可以知道，贪心的策略一定不会得到一个更差的解法。

考虑到探索的必要性，我们可以用一种替代方案来提升策略的效果。在该方案中，多数情况下我们采用贪心策略，但在某些情况下会随机选择动作以确保充分探索。