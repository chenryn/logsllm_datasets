### Introduction to Adversarial Attacks and Their Applications

We utilize white-box attacks on visual classifiers [17, 53], sequential models [18, 79], and object detectors [28]. We demonstrate that black-box attacks [41] serve as a versatile alternative to previous attacks on SIFT [39].

Attacking page-based ad-blockers introduces unique challenges. Modifying HTML shares similarities with discrete domain attacks, such as PDF malware detection [80]. Additionally, the inputs to ad-blockers can be controlled by multiple entities, a constraint reminiscent of those encountered in physical-world attacks [12, 28, 29, 49, 75].

### Preventing Adversarial Examples

Preventing adversarial examples remains an open problem. Adversarial training is a viable strategy [33, 49, 53, 86], but it often considers a less stringent threat model compared to perceptual ad-blockers.

### Conclusion

In this study, we present a comprehensive security evaluation of perceptual ad-blocking. To understand the design space of these recently deployed systems, we derived a unified architecture that incorporates and extends prior work. Our analysis of this architecture revealed multiple vulnerabilities at every stage of the visual ad-classification pipeline. We demonstrated that unless perceptual ad-blockers operate over rendered web content, the arms race around page markup obfuscation will likely continue. Conversely, we showed that current visual ad-classifiers are inherently vulnerable to adversarial examples—the first application of these attacks to web security. We crafted near-imperceptible perturbations for ads, ad-disclosures, and native content to evade or detect ad-blocking with seven different classifiers. Finally, we discovered a powerful attack on page-based ad-blockers, where a malicious user can deceive the model into blocking content supposedly protected by web-security boundaries.

Our aim was to highlight the fundamental vulnerabilities that perceptual ad-blockers inherit from existing image classifiers. As long as defenses against adversarial examples remain elusive, perceptual ad-blockers will be drawn into a new arms race, starting from a disadvantaged position given the stringent threat model they must withstand.

### Acknowledgments

This work was partially supported by the National Science Foundation (NSF), the Office of Naval Research (ONR), the Simons Foundation, a Google Faculty Fellowship, the Swiss National Science Foundation (SNSF project P1SKP2_178149), and the German Federal Ministry of Education and Research (BMBF) through funding for the CISPA-Stanford Center for Cybersecurity (FKZ: 13N1S0762).

### References

[1] Adblock Plus. https://adblockplus.org/
[2] AdblockRadio. https://www.adblockradio.com
[3] EasyList. https://easylist.to/
[4] Easylist Forum: Report incorrectly removed content. https://forums.lanik.us/viewforum.php?f=64&sid=ba948dbdbad9334b72c143f26db58ff0
[5] Ghostery. https://www.ghostery.com/
[6] Tesseract. https://github.com/tesseract-ocr/
[7] uBlock. https://www.ublock.org/
[8] Adblock Plus. Issue 7088: Implement hide-if-contains-image snippet. https://issues.adblockplus.org/ticket/7088
[9] Adblock Plus. 2018. Customize Facebook with Adblock Plus. https://facebook.adblockplus.me/
[10] Adblock Plus. 2018. Sentinel. https://adblock.ai/
[11] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning (ICML).
[12] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2018. Synthesizing robust adversarial examples. In International Conference on Machine Learning (ICML).
[13] Shai Avidan and Ariel Shamir. 2007. Seam carving for content-aware image resizing. In ACM Transactions on Graphics, Vol. 26, No. 3.
[14] Sruti Bhagavatula, Christopher Dunn, Chris Kanich, Minaxi Gupta, and Brian Ziebart. 2014. Leveraging machine learning to improve unwanted resource filtering. In ACM Workshop on Artificial Intelligence and Security.
[15] Elie Bursztein, Jonathan Aigrain, Angelika Moscicki, and John C Mitchell. 2014. The End is Nigh: Generic Solving of Text-based CAPTCHAs. In USENIX Workshop on Offensive Technologies.
[16] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM Workshop on Artificial Intelligence and Security.
[17] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy.
[18] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted attacks on speech-to-text. In DLS.
[19] Steven Chen, Nicholas Carlini, and David Wagner. 2019. Stateful Detection of Black-Box Adversarial Attacks. arXiv preprint arXiv:1907.05587.
[20] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526.
[21] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan Boneh. 2018. Sentinet: Detecting physical attacks against deep learning systems. arXiv preprint arXiv:1812.00292.
[22] Nicolas Christin, Sally S Yanagihara, and Keisuke Kamataki. 2010. Dissecting one click frauds. In Proceedings of the 17th ACM conference on Computer and Communications Security.
[23] Justin Crites and Mathias Ricken. 2004. Automatic ad blocking: Improving AdBlock for the Mozilla platform.
[24] Digital Advertising Alliance (DAA). 2009. Self Regulatory Principles for Online Behavioral Advertising. https://digitaladvertisingalliance.org/sites/aboutads/files/DAA_files/seven-principles-07-01-09.pdf
[25] Digital Advertising Alliance (DAA). 2013. DAA Icon Ad Marker Creative Guidelines. https://digitaladvertisingalliance.org/sites/aboutads/files/DAA_files/DAA_Icon_Ad_Creative_Guidelines.pdf
[26] Benjamin Edelman. 2009. False and Deceptive Display Ads at Yahoo’s Right Media. http://www.benedelman.org/rightmedia-deception
[27] Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2017. A rotation and a translation suffice: Fooling CNNs with simple transformations. arXiv preprint arXiv:1712.02779.
[28] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramèr, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Physical Adversarial Examples for Object Detectors. In USENIX Workshop on Offensive Technologies.
[29] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Robust Physical-World Attacks on Deep Learning Visual Classification. In Conference on Computer Vision and Pattern Recognition (CVPR).
[30] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence information and basic countermeasures. In ACM SIGSAC Conference on Computer and Communications Security.
[31] Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. 2018. Motivating the rules of the game for adversarial example research. arXiv preprint arXiv:1807.06732.
[32] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg, and Ian Goodfellow. 2018. Adversarial spheres. arXiv preprint arXiv:1801.02774.
[33] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR).
[34] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. 2017. On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280.
[35] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2017. Adversarial perturbations against deep neural networks for malware classification. In ESORICS. arXiv preprint arXiv:1606.04435.
[36] David Gugelmann, Markus Happe, Bernhard Ager, and Vincent Lenders. 2015. An automated approach for complementing ad blockers’ blacklists. Privacy Enhancing Technologies Symposium 2, 282–298.
[37] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017. Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong. arXiv preprint arXiv:1706.04701.
[38] Jovanni Hernandez, Akshay Jagadeesh, and Jonathan Mayer. 2011. Tracking the trackers: The AdChoices icon. http://cyberlaw.stanford.edu/blog/2011/08/tracking-trackers-adchoices-icon
[39] Chao-Yung Hsu, Chun-Shien Lu, and Soo-Chang Pei. 2009. ACM International conference on Multimedia. In ICM. ACM, 637–640.
[40] Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, and Adriana Kovashka. 2017. Automatic understanding of image and video advertisements. In Conference on Computer Vision and Pattern Recognition (CVPR).
[41] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box adversarial attacks with limited queries and information. In International Conference on Machine Learning (ICML).
[42] Umar Iqbal, Zubair Shafiq, and Zhiyun Qian. 2017. The ad wars: retrospective measurement and analysis of anti-adblock filter lists. In Internet Measurement Conference.
[43] Umar Iqbal, Zubair Shafiq, Peter Snyder, Shitong Zhu, Zhiyun Qian, and Benjamin Livshits. 2018. AdGraph: A Machine Learning Approach to Automatic and Effective Adblocking. arXiv preprint arXiv:1805.09155.
[44] Ilker Koksal. 2018. How Alexa Is Changing The Future Of Advertising. https://www.forbes.com/sites/ilkerkoksal/2018/12/11/how-alexa-is-changing-the-future-of-advertising
[45] Zico Kolter and Eric Wong. 2017. Provable defenses against adversarial examples via the convex outer adversarial polytope. In ICML.
[46] Georgios Kontaxis and Monica Chew. 2015. Tracking protection in Firefox for privacy and performance. arXiv preprint arXiv:1506.04104.
[47] Viktor Krammer. 2008. An effective defense against intrusive web advertising. In Conference on Privacy, Security and Trust.
[48] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial examples in the physical world. In International Conference on Learning Representations (ICLR).
[49] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial Machine Learning at Scale. In International Conference on Learning Representations (ICLR).
[50] Pedro Giovanni Leon, Justin Cranshaw, Lorrie Faith Cranor, Jim Graves, Manoj Hastak, Blase Ur, and Guzi Xu. 2012. What do online behavioral advertising privacy disclosures communicate to users? In Workshop on Privacy in the Electronic Society.
[51] Zhou Li, Kehuan Zhang, Yinglian Xie, Fang Yu, and XiaoFeng Wang. 2012. Knowing your enemy: understanding and detecting malicious web advertising. In ACM SIGSAC Conference on Computer and Communications Security.
[52] David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision 60, 2, 91–110.
[53] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR).
[54] Matthew Malloy, Mark McNamara, Aaron Cahn, and Paul Barford. 2016. Ad blockers: Global prevalence and impact. In Internet Measurement Conference.
[55] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017. On detecting adversarial perturbations. In International Conference on Learning Representations (ICLR).
[56] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In Conference on Computer Vision and Pattern Recognition (CVPR).
[57] Muhammad Haris Mughees, Zhiyun Qian, and Zubair Shafiq. 2017. Detecting anti ad-blockers in the wild. In Privacy Enhancing Technologies Symposium.
[58] Muhammad Haris Mughees, Zhiyun Qian, Zubair Shafiq, Karishma Dash, and Pan Hui. 2016. A first look at ad-block detection: A new arms race on the web. arXiv preprint arXiv:1605.05841.
[59] Meghan Neal. 2016. You’re Going to Need an Ad Blocker for Your Next TV. https://motherboard.vice.com/en_us/article/mg7ek8/youre-going-to-need-an-ad-blocker-for-your-next-tv
[60] Rishab Nithyanand, Sheharbano Khattak, Mobin Javed, Narseo Vallina-Rodriguez, Marjan Falahrastegar, Julia E Powles, ED Cristofaro, Hamed Haddadi, and Steven J Murdoch. 2016. Adblocking and counter blocking: A slice of the arms race. In USENIX Workshop on Free and Open Communications on the Internet.
[61] Paraska Oleksandr. 2018. Towards more intelligent ad blocking on the web. https://medium.com/@shoniko/towards-more-intelligent-ad-blocking-on-the-web-9f67bf2a12b5
[62] George Paliy. 2018. The Future Of Advertising In Virtual Reality. https://stopad.io/blog/future-virtual-reality-advertising
[63] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine learning. In ACM ASIA Conference on Computer and Communications Security.
[64] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy.
[65] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2016. Towards the science of security and privacy in machine learning. arXiv preprint arXiv:1611.03814.
[66] Giancarlo Pellegrino, Martin Johns, Simon Koch, Michael Backes, and Christian Rossow. 2017. Deemon: Detecting CSRF with dynamic analysis and property graphs. In ACM SIGSAC Conference on Computer and Communications Security.
[67] Giancarlo Pellegrino, Christian Rossow, Fabrice J Ryba, Thomas C Schmidt, and Matthias Wählisch. 2015. Cashing Out the Great Cannon? On browser-based DDoS attacks and economics. In USENIX Workshop on Offensive Technologies.
[68] Enric Pujol, Oliver Hohlfeld, and Anja Feldmann. 2015. Annoyed Users: Ads and Ad-Block Usage in the Wild. In Internet Measurement Conference.
[69] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified defenses against adversarial examples. In International Conference on Learning Representations (ICLR).
[70] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. In Conference on Computer Vision and Pattern Recognition (CVPR).
[71] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: Better, Faster, Stronger. In Conference on Computer Vision and Pattern Recognition (CVPR).
[72] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement. arXiv preprint arXiv:1804.02767.
[73] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864.
[74] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. 2018. Adversarially robust generalization requires more data. In Advances in Neural Information Processing Systems.
[75] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In ACM SIGSAC Conference on Computer and Communications Security.
[76] Yash Sharma and Pin-Yu Chen. 2017. Attacking the Madry Defense Model with L_1-based adversarial examples. arXiv preprint arXiv:1710.10733.
[77] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy.
[78] Ashish Kumar Singh and Vidyasagar Potdar. 2009. Blocking online advertising—A state of the art. In IEEE International Conference on Industrial Technology.
[79] Congzheng Song and Vitaly Shmatikov. 2018. Fooling OCR systems with adversarial text images. arXiv preprint arXiv:1802.05385.
[80] Nedim Srndic and Pavel Laskov. 2014. Practical evasion of a learning-based classifier: A case study. In IEEE Symposium on Security and Privacy.
[81] Grant Storey, Dillon Reisman, Jonathan Mayer, and Arvind Narayanan. 2017. The future of ad blocking: An analytical framework and new techniques. arXiv preprint arXiv:1705.08568.
[82] Grant Storey, Dillon Reisman, Jonathan Mayer, and Arvind Narayanan. 2017. Perceptual Ad Highlighter. Chrome Extension: https://chrome.google.com/webstore/detail/perceptual-ad-highlighter/mahgiflleahghaapkboihnbhdplhnchp; Source code: https://github.com/citp/ad-blocking
[83] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR).
[84] Panagiotis Tigas, Samuel T King, Benjamin Livshits, et al. 2019. Percival: Making in-browser perceptual ad blocking practical with deep learning. arXiv preprint arXiv:1905.07444.
[85] Florian Tramèr and Dan Boneh. 2019. Adversarial training and robustness for multiple perturbations. arXiv preprint arXiv:1904.13000.
[86] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2018. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations (ICLR).
[87] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. Stealing machine learning models via prediction APIs. In USENIX Security Symposium.
[88] uBlockOrigin. Issue 3367: Facebook. https://github.com/uBlockOrigin/uAssets/issues/3367
[89] Blase Ur, Pedro Giovanni Leon, Lorrie Faith Cranor, Richard Shay, and Yang Wang. 2012. Smart, useful, scary, creepy: Perceptions of online behavioral advertising.