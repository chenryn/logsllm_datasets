### Bandwidth Management for Individual Applications

The ability to manage bandwidth for individual applications is highly dependent on the openness of the platform. Table 4 summarizes the TUBEApp features supported on each platform before any device modifications, as well as the code size required to implement the full TUBEApp. For the iOS platform, we hook several internal functions to track application usage, run a daemon process to dispatch and display Time-Dependent Pricing (TDP) rates, and block applications when necessary. The iOS implementation requires 25,000 lines of code, while the Android and Windows implementations require only 5,400 and 5,300 lines, respectively.

### Enhancing the User Experience

The autopilot mode minimizes user interactions by estimating device usage patterns and scheduling applications accordingly. To keep users informed of autopilot actions, we send pop-up notifications when usage is blocked, as shown in the iPhone screenshots in Figure 7. These notifications appear when the user's usage reaches their daily or weekly budget limits (Figures 7a and 7b, respectively).

To ensure the practicality of the autopilot feature, we measured its energy consumption. TUBEApp with the autopilot running consumes only 4% more battery power than a device without TUBEApp, indicating that the autopilot does not significantly drain the battery.

### Trial Design and Results

We conducted a small-scale pilot trial of the TUBE system at Princeton University from May 2011 to January 2012. This section provides an overview of the trial goals, setup, and limitations, followed by a discussion of key conclusions drawn from the trial data.

#### Goals and Structure

The primary goals of our trial were to demonstrate the functionality and benefits of the TUBE system and to provide an initial verification of TUBE's deployment feasibility with real users. Throughout the trial, we acted as a resale Internet Service Provider (ISP), paying participants' regular 3G data bills to AT&T while charging them according to our TUBE algorithms. To assess the benefits of TDP, we divided the trial into two phases: first, we monitored usage patterns without TDP (i.e., collecting Time-Independent Pricing (TIP) data). In the second phase, we offered TDP and studied its impact. The following aspects of the trial are addressed in this section:

- **Baseline Traffic Statistics**: Section 4.4.1 reports on three months of TIP usage statistics from our trial participants. We examined whether the sample population included a representative mix of heavy and light users and bandwidth-intensive applications.
- **Price Sensitivity**: Section 4.4.2 examines users' price sensitivity to static TDP patterns. We tested if users would defer their traffic to lower-price periods.
- **Effectiveness of GUI Design**: Section 4.4.3 analyzes the effectiveness of displaying numerical values versus color codes (red: high, orange: medium, green: low) to indicate TDP prices on the user device.
- **Benefits of Optimized TDP**: Section 4.4.4 studies whether TUBE's optimized prices benefit ISPs by reducing peak-to-average ratios of network usage.

#### Trial Setup

We recruited 50 users (27 iPhones and 23 iPads) of AT&T’s 3G Corporate Data Plan as our trial participants. They were faculty and staff from 14 academic and administrative divisions. During the trial, we acted as a resale ISP, charging participants after every billing cycle according to TUBE’s TDP. We excluded measurements from development devices to avoid bias.

To record participants' usage, we separated their 3G traffic from that of other AT&T customers using an Access Point Name (APN) setup, which tunneled the participant’s 3G traffic from the AT&T core to the TUBE servers in our lab (Figure 8). Participants installed and used the TUBEApp on their iOS devices. WiFi usage, voice calls, and SMS were not included in the trial traffic as they are not 3G data.

#### Trial Limitations

Logistical constraints limited us to recruiting only AT&T data plan users with iOS devices, of which 16 were jailbroken (JB) and 34 were non-jailbroken (non-JB) devices. Non-JB devices provided less flexibility in experimentation, resulting in a TUBEApp with limited features. Only users with JB devices could see the current price/discount directly from the home screen’s status bar (circled in Figure 7's screenshots) without manually launching the TUBEApp, and the autopilot algorithm only ran on JB devices. On non-JB devices, we used push notifications to alert participants during high-price periods.

Since our trial included only 50 participants, even peak traffic from trial users did not congest our lab’s access link. To demonstrate TDP’s benefits, we simulated congestion conditions by logically scaling up the traffic volume in TUBE-Opt’s price computation.

#### Results and Evaluation

Following the outline in Section 4.1, we now present the trial results. In many cases, we used Wilcoxon’s signed rank test against the null hypothesis that a set of values is symmetrically distributed with mean zero. We applied this test to the difference between the changes in usage in high- and low-price periods. A higher probability of a symmetric distribution indicates a lack of response to price signals, as the expected change in usage is likely the same for both high and low prices.

##### Baseline Traffic Statistics

**Question**: Do our participants include both heavy and light bandwidth users? Which applications use the most data?

**Method**: We measured usage for both iPad and iPhone users from July to September and used `tcpdump` to record application-specific traffic.

**Results**: Our participants were a mix of light- and heavy-bandwidth users. Video streaming applications accounted for most of the traffic, aligning with the reported trends of growing demand for mobile video. Figure 9a shows the cumulative distribution function (CDF) of total traffic per user for uploads and downloads from July to September 2011. While 90% of the users uploaded less than 0.5GB, some users had large download volumes: 20% of users consumed 2.1 – 5.3 GB over three months. Figure 9b shows the distribution of total traffic by application type for the three-month period, normalized with respect to the number of iPhone, JB, and non-JB iPad users. As expected, iPads showed higher usage than iPhones for most application types, and a large part of the mobile traffic for all device types came from movie streaming.

##### Price Sensitivity

**Question**: Do users wait to use mobile data in return for a monetary discount?

**Method**: We conducted a three-week experiment on iPad and iPhone trial participants in October 2011, offering a basic TDP pattern of consecutive high, high, and low price periods, repeated throughout the day. The high-price periods offered approximately a 10% discount, while the low-price periods offered a 40% discount on the baseline price of $10/GB. If monetary incentives induce usage deferrals, we expected that average usage should decrease in high-price and increase in low-price periods.

To measure users’ response to prices, we sent messages at ten-minute intervals during high-price periods if the user exceeded 2 MB of usage in the previous ten minutes. We analyzed the data for each user by calculating the percentage change in usage for each one-hour time period compared to the mean usage in that same period before TDP (i.e., under TIP pricing). We then weighted these percent changes by the proportion of TDP usage in that period to account for diurnal variations, giving the weighted average percent change in usage under TDP for high- and low-price periods.

**Results**: Users shifted their traffic from high- to low-price periods under TDP. For most users, the average usage decreased in high-price periods relative to the changes in low-price periods. Figure 10 shows the weighted average percent change in usage for iPad users for high- and low-price periods. The reference line indicates an equal change in both types of periods. Each dot on the scatter plot represents values for an individual user, and its size is proportional to the user’s TDP usage volume. With the given static TDP pattern, usage increased more in low-price periods relative to high-price periods for almost all users. Interestingly, about half of the users decreased their overall usage in both high- and low-price periods, while the other half increased their usage in both periods.

We further verified these results using Wilcoxon’s test on the differences between each user’s percent change in high- and low-price periods. We found only a 0.56% probability that the null hypothesis is valid, indicating that the users’ observed responses are statistically significant. A similar plot may be observed for the iPhone users.

The overall iPhone usage changed by -11.3% in high-price and -5.7% in low-price periods, while overall iPad usage changed by -10.1% in high-price and 15.7% in low-price periods. Thus, iPad users generally decreased their usage in high-price periods and increased it in low-price periods. The overall decrease in iPhone usage is likely due to limited user notification and display options on non-JB iPhones. However, the greater usage decrease in high- relative to low-price periods indicates that iPhone users attempted to use less in high-price periods.

Next, we examined the effect of the number of notification messages sent to users on their usage in high-price periods. Multiple consecutive notifications were sent to a user only if usage in each preceding 10-minute interval exceeded 2 MB. We examined the percent change in usage in the ten-minute span before and after each notification. Figure 11 shows the CDF of the percent change in usage due to a first, second, etc., notification. About 80–90% of iPad and iPhone users did not increase their usage after the first notification, indicating that notifications can effectively reduce peak usage. For all subsequent notifications, about 60–80% of the active users responded by decreasing their usage.

##### Effectiveness of User Interface Design

**Question**: Do users respond more to the numerical values of TDP prices or to the color of the price indicator bar on the home screen?

**Method**: In December 2011, we installed a price indicator bar on the home screen of all JB devices. The indicator displays the numerical value of the price discounts available in the current period and changes its color according to these discounts. It is green for discounts over 30%, orange for 10–29% discounts, and red for discounts below 10%.

Our experiment had two stages:
- In the first stage, we offered discounts of approximately 40% every third period of the day, starting with a 40% discount at midnight. The other periods offered discounts of about 10%.
- After two weeks, we began the second stage, repeating the pattern of a 9% discount at midnight, followed by 28%, 30%, 28%, 9%, and 30% discounts.

We compared three types of periods to assess the effect of the indicator color and numerical discount:
- **Type 1 periods** offered a 10% discount in the first stage and 28% discount in the second stage; the indicator remained orange despite this increase in the discount.
- **Type 2 periods** offered a 10% (orange) discount in the first stage and 30% (green) discount in the second stage.
- **Type 3 periods** offered a 10% discount in the first and 9% discount in the second stage (the indicator is orange in both periods).

Table 5 summarizes the combinations of discounts and colors used in the two stages that characterize each type of period.

We calculated the percent changes in usage for each period type between the first and second stages of the experiment. To do so, we first found the average usage in each period of the day (i.e., each hour) for the first stage of the experiment. We then calculated the percent change in usage of each period in the second stage of the experiment from the average usage in the same period during the first stage. The average change in each type of period is defined as the weighted average of these percent changes for each period of the given type. The weights were proportional to the usage in that period.

**Results**: Users paid more attention to indicator color than to the numerical discount value. When discounts increased significantly with no change in indicator color, only half of the users increased their usage relative to other periods. However, when the indicator color also changed, almost all users increased their usage in those periods relative to others. In Figure 12, each data point represents one user’s average change in each period type, with the size of the data point indicating the volume of usage in the second stage of the experiment. The reference line represents equal changes in both period types considered.