### Optimization Problems Tailored to Traffic Analysis Applications

Our blind adversarial perturbations algorithm is generic and can be applied to various types of traffic classifiers under different network constraints.

### Evaluation and Results

We evaluated our attack against state-of-the-art traffic analysis systems, demonstrating that it outperforms traditional techniques in defeating traffic analysis. Our blind adversarial perturbations are also transferable between different models and architectures, making them applicable even in black-box scenarios. Additionally, we found that existing defenses against adversarial examples perform poorly against blind adversarial perturbations. To address this, we designed a tailored countermeasure specifically for blind perturbations.

### Acknowledgements

We thank our shepherd Esfandiar Mohammadi and anonymous reviewers for their valuable feedback. This work was supported by the NSF CAREER grant CNS-1553301 and by DARPA and NIWC under contract N66001-15-C-4067. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes, notwithstanding any copyright notation. The views, opinions, and findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. Milad Nasr was supported by a Google PhD Fellowship in Security and Privacy.

### References

1. S. Abdoli, L. Hafemann, J. Rony, I. Ayed, P. Cardinal, and A. Koerich. "Universal Adversarial Audio Perturbations." arXiv preprint arXiv:1908.03173, 2019.
2. A. Bahramali, R. Soltani, A. Houmansadr, D. Goeckel, and D. Towsley. "Practical Traffic Analysis Attacks on Secure Messaging Applications." In NDSS, 2020.
3. S. Bhat, D. Lu, A. Kwon, and S. Devadas. "Var-CNN and DynaFlow: Improved Attacks and Defenses for Website Fingerprinting." CoRR, 2018.
4. Avrim Blum, Dawn Song, and Shobha Venkataraman. "Detection of Interactive Stepping Stones: Algorithms and Confidence Bounds." In RAID, 2004.
5. X. Cai, R. Nithyanand, and R. Johnson. "Cs-buﬂo: A Congestion Sensitive Website Fingerprinting Defense." In WPES, 2014.
6. X. Cai, X. Zhang, B. Joshi, and R. Johnson. "Touching from a Distance: Website Fingerprinting Attacks and Defenses." In ACM CCS, 2012.
7. X. Cao and N. Gong. "Mitigating Evasion Attacks to Deep Neural Networks via Region-Based Classification." In ACSAC, 2017.
8. N. Carlini and D. Wagner. "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods." In ACM Workshop on AISec, 2017.
9. N. Carlini and D. Wagner. "Towards Evaluating the Robustness of Neural Networks." In IEEE S&P, 2017.
10. P. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh. "EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples." In AAAI, 2017.
11. G. Cherubin, J. Hayes, and M. Juarez. "Website Fingerprinting Defenses at the Application Layer." In PETS, 2017.
12. T. Chothia and A. Guha. "A Statistical Test for Information Leaks Using Continuous Mutual Information." In CSF, 2011.
13. R. Dingledine and N. Mathewson. "Blocking-Resistant Anonymity System Design." https://svn.torproject.org/svn/projects/design-paper/blocking.html.
14. Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. "Boosting Adversarial Attacks with Momentum." In CVPR, 2018.
15. K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song. "Robust Physical-World Attacks on Deep Learning Visual Classification." In CVPR, 2018.
16. I. Goodfellow, Y. Bengio, and A. Courville. "Deep Learning." MIT Press, Cambridge, 2016.
17. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. "Generative Adversarial Nets." In NIPS, 2014.
18. I. Goodfellow, J. Shlens, and C. Szegedy. "Explaining and Harnessing Adversarial Examples." In ICLR, 2015.
19. J. Hayes and G. Danezis. "k-fingerprinting: A Robust Scalable Website Fingerprinting Technique." In USENIX Security, 2016.
20. J. Hayes and G. Danezis. "Learning Universal Adversarial Perturbations with Generative Models." In IEEE S&P Workshops, 2018.
21. K. He, X. Zhang, S. Ren, and J. Sun. "Deep Residual Learning for Image Recognition." In CVPR, 2016.
22. W. He, B. Li, and D. Song. "Decision Boundary Analysis of Adversarial Examples." In ICLR, 2018.
23. A. Houmansadr, N. Kiyavash, and N. Borisov. "RAINBOW: A Robust and Invisible Non-Blind Watermark for Network Flows." In NDSS, 2009.
24. A. Houmansadr, N. Kiyavash, and N. Borisov. "Non-Blind Watermarking of Network Flows." IEEE/ACM TON, 2014.
25. Amir Houmansadr and Nikita Borisov. "SWIRL: A Scalable Watermark to Detect Correlated Network Flows." In NDSS, 2011.
26. M. Imani, M. Rahman, N. Mathews, A. Joshi, and M. Wright. "Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces." CoRR, 2019.
27. R. Jansen, M. Juarez, R. Galvez, T. Elahi, and C. Diaz. "Inside Job: Applying Traffic Analysis to Measure Tor from Within." In NDSS, 2018.
28. M. Juarez, M. Imani, M. Perry, C. Diaz, and M. Wright. "Toward an Efficient Website Fingerprinting Defense." In ESORICS, 2016.
29. D. Kingma and J. Ba. "Adam: A Method for Stochastic Optimization." ICLR, 2014.
30. A. Krizhevsky, I. Sutskever, and G. Hinton. "ImageNet Classification with Deep Convolutional Neural Networks." In NIPS, 2012.
31. A. Kurakin, I. Goodfellow, and S. Bengio. "Adversarial Examples in the Physical World." arXiv preprint arXiv:1607.02533, 2016.
32. A. Kurakin, I. Goodfellow, and S. Bengio. "Adversarial Machine Learning at Scale." arXiv preprint arXiv:1611.01236, 2016.
33. B. Levine, M. Reiter, C. Wang, and M. Wright. "Timing Attacks in Low-Latency Mix Systems." In FC, 2004.
34. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. "Towards Deep Learning Models Resistant to Adversarial Attacks." arXiv preprint arXiv:1706.06083, 2017.
35. S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. "Universal Adversarial Perturbations." In CVPR, 2017.
36. S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks." In CVPR, 2016.
37. M. Nasr, A. Bahramali, and A. Houmansadr. "Deepcorr: Strong Flow Correlation Attacks on Tor Using Deep Learning." In ACM CCS, 2018.
38. M. Nasr, A. Houmansadr, and A. Mazumdar. "Compressive Traffic Analysis: A New Paradigm for Scalable Traffic Analysis." In ACM CCS, 2017.
39. "A Simple Obfuscating Proxy." https://www.torproject.org/projects/obfsproxy.html.en.
40. A. Panchenko, F. Lanze, J. Pennekamp, T. Engel, A. Zinnen, M. Henze, and K. Wehrle. "Website Fingerprinting at Internet Scale." In NDSS, 2016.
41. A. Panchenko, L. Niessen, A. Zinnen, and T. Engel. "Website Fingerprinting in Onion Routing Based Anonymization Networks." In WPES, 2011.
42. N. Papernot, P. McDaniel, and I. Goodfellow. "Transferability in Machine Learning: From Phenomena to Black-Box Attacks Using Adversarial Samples." arXiv preprint arXiv:1605.07277, 2016.
43. N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks." In IEEE S&P, 2016.
44. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. "Automatic Differentiation in PyTorch." In NIPS Autodiff Workshop, 2017.
45. F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro. "Intriguing Properties of Adversarial ML Attacks in the Problem Space." In IEEE S&P, 2020.
46. "Tor: Pluggable Transports." https://www.torproject.org/docs/pluggable-transports.html.en.
47. V. Rimmer, D. Preuveneers, M. Juarez, T. Van, and W. Joosen. "Automated Website Fingerprinting Through Deep Learning." In NDSS, 2018.
48. A. Ross and F. Doshi-Velez. "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients." In AAAI, 2018.
49. V. Shmatikov and M. Wang. "Timing Analysis in Low-Latency Mix Networks: Attacks and Defenses." In ESORICS, 2006.
50. P. Sirinam, M. Imani, M. Juarez, and M. Wright. "Deep Fingerprinting: Undermining Website Fingerprinting Defenses with Deep Learning." In ACM CCS, 2018.
51. P. Sirinam, N. Mathews, M. Rahman, and M. Wright. "Triplet Fingerprinting: More Practical and Portable Website Fingerprinting with N-shot Learning." In ACM CCS, 2019.
52. J. Su, D. Vargas, and K. Sakurai. "One Pixel Attack for Fooling Deep Neural Networks." IEEE TEVC, 2017.
53. Y. Sun, A. Edmundson, L. Vanbever, O. Li, J. Rexford, M. Chiang, and P. Mittal. "RAPTOR: Routing Attacks on Privacy in Tor." In USENIX Security, 2015.
54. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. "Intriguing Properties of Neural Networks." arXiv preprint arXiv:1312.6199, 2013.
55. F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. "Ensemble Adversarial Training: Attacks and Defenses." arXiv preprint arXiv:1705.07204, 2017.
56. V. Vapnik. "The Nature of Statistical Learning Theory." Springer Science & Business Media, 2013.
57. T. Wang. "High Precision Open-World Website Fingerprinting." In IEEE S&P, 2020.
58. T. Wang, X. Cai, R. Nithyanand, R. Johnson, and I. Goldberg. "Effective Attacks and Provable Defenses for Website Fingerprinting." In USENIX Security, 2014.
59. T. Wang and I. Goldberg. "Improved Website Fingerprinting on Tor." In WPES, 2013.
60. T. Wang and I. Goldberg. "On Realistically Attacking Tor with Website Fingerprinting." PETS, 2016.
61. T. Wang and I. Goldberg. "Walkie-Talkie: An Efficient Defense Against Passive Website Fingerprinting Attacks." In USENIX Security, 2017.
62. X. Zhang, J. Hamm, M. K. Reiter, and Y. Zhang. "Statistical Privacy for Streaming Traffic." In NDSS, 2019.
63. Y. Zhang and V. Paxson. "Detecting Stepping Stones." In USENIX Security, 2000.
64. Y. Zhu, X. Fu, B. Graham, R. Bettati, and W. Zhao. "On Flow Correlation Attacks and Countermeasures in Mix Networks." In WPES, 2004.

### Adapting Traditional Defenses to Adversarial Examples

Madry et al. [34] introduced a scalable adversarial training approach to enhance the robustness of deep learning models against adversarial examples. This method involves generating a set of adversarial examples in each training iteration and using them in the training phase. Among adversarial training-based defenses, Madry et al.'s defense is the most robust [8]. However, we cannot directly apply this method because, in image recognition, pixels can take real values, whereas in direction-based traffic analysis, features are binary (-1, +1). Therefore, we adapted this defense to our setting. To generate adversarial examples during training, we randomly select a number of packets and flip their directions from -1 to +1 and vice versa. Similarly, for packet timings and sizes, we enforce all application constraints to generate adversarial examples.

From the gradient masking approach, we utilized the input gradient regularization (IGR) technique by Ross and Doshi-Velez [48]. IGR trains a model to have smooth input gradients with fewer extreme values, making it more resistant to adversarial examples. We applied this approach to train a robust model using the DF structure and evaluated the direction-based attack against this defense with parameter λ = 10.

While the previous defenses focus on training a robust model, Cao and Gong [7] proposed a defense method that does not alter the training process. They introduced region-based classification (RC), which creates a hypercube centered at the input to predict its label. The method samples data points from the hypercube and uses an existing trained model to produce predicted labels, then applies majority voting to determine the final class label. We modified the region-based classification defense for our context. Unlike images, we cannot create a hypercube by adding random real values to packet direction sequences. Instead, we create the hypercube by randomly selecting a number of packets and flipping their directions. For the direction-based method, we randomly choose 125 packets to form the hypercube, which is the maximum number of packets we can use without reducing the accuracy below that of the original DF model. Using a hypercube radius of 125, we applied region-based classification against our attack. For time and size-based methods, we use the adversary's strength to generate the hypercubes.

---

This version of the text is more structured, coherent, and professional, with improved clarity and readability.