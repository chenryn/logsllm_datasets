# Tools and Performance Metrics

| Tool | Metric 1 | Metric 2 | Metric 3 | Metric 4 |
| --- | --- | --- | --- | --- |
| nm | 5.2% | 8.2% | 7.5% | 18.9% |
| objcopy | 4.0% (-1.2%) | 7.5% (-0.7%) | 7.3% (-0.2%) | 15.8% (-3.1%) |
| objdump | 5.2% | 8.3% | 7.3% | 18.4% |
| readelf | 4.8% (-0.4%) | 7.4% (-0.9%) | 7.3% (-0.0%) | 15.7% (-2.7%) |
| func- | 5.3% | 8.5% | 7.5% | 18.7% |
| func | 4.5% (-0.8%) | 7.0% (-1.5%) | 7.5% (-0.0%) | 15.50% (-3.2%) |
| Code coverage | 3.9% (-1.4%) | 5.3% | 6.2% (-2.6%) | 8.8% |
| AFL-Qemu | 7.4% (-0.4%) | 7.8% | 20.5% | 12.6% (-7.9%) |

## B. The Threshold for Security-Sensitivity
We currently set a unified threshold to determine security-sensitive edges: an edge is considered security-sensitive if one of the three metrics exceeds 0. Ideally, this threshold should be specific to each program. Future work will involve designing methods to automatically generate these thresholds for individual programs through static analysis or during fuzzing execution.

## C. LAVA-M vs. Real-World Data Set
In our experiments, we observed that the LAVA-M test suite differs from real-world program test suites in two key aspects. First, LAVA-M includes more test cases involving magic words, which biases the test suite towards evaluating exploit generation tools. Second, the base binaries for LAVA-M are less complex than the real-world programs we tested. While we recognize the value of the LAVA-M data set, future work should systematically compare the inserted bugs in LAVA-M with those in a large number of real-world programs. This will help us understand the differences and limitations of the current test suite and build a more comprehensive and representative test suite for real-world scenarios.

## D. Statistical Significance of Vulnerabilities per Program
In our evaluation, we did not report the statistical significance between TortoiseFuzz and other fuzzers in terms of the number of vulnerabilities found per program. Unlike the per-program p-values for code coverage shown in Table VI and represented in REDQUEEN [4], vulnerabilities are sparse, making it difficult to draw meaningful conclusions about fuzzer effectiveness from a single program. For example, for the `flvmeta` program, all tools found only 2 vulnerabilities across all runs, leading to inconclusive statistical significance. Therefore, we report the p-value for the total number of vulnerabilities found across all target programs. This highlights the need for a comprehensive data suite with a sufficient number of vulnerabilities.

## VIII. Related Work
Since the concept of fuzzing was introduced in the 1990s [38], numerous techniques have been developed to improve fuzzing in various ways. In this section, we introduce some of these techniques. For a more comprehensive study, refer to recent surveys such as Chen et al. [10], Li et al. [32], and Manès et al. [37].

### Fuzzing Specific Program Types
Some fuzzing techniques focus on specific types of programs, proposing more effective methods. These include fuzzing for protocols [5, 14], firmware [11, 17, 39], and OS kernels [13, 45, 51, 57].

### Hardware-Assisted Fuzzing
Previous work has proposed various approaches to increase the efficiency of the fuzzing process. Xu et al. [56] designed new operating primitives to remove execution redundancy and improve the performance of AFL. Hardware-based fuzzing, such as kAFL [45] and PTfuzz [61], leverages hardware features like Intel Processor Trace [25] to guide fuzzing without the overhead caused by instrumentation. These techniques reduce the time spent on program execution and information extraction, allowing fuzzers to explore more inputs within a given time.

### Generational Fuzzing
Generational fuzzers, such as Peach [16], Sulley [3], and SPIKE [2], play a crucial role in testing programs and finding security vulnerabilities. These fuzzers generate samples based on predefined configurations that specify the input format. However, generational fuzzers require manual configuration, making them less generic and dependent on human effort.

### Machine Learning-Assisted Fuzzing
Recent works, such as Skyfire [52], Learn&Fuzz [22], and NEUZZ [48], combine fuzzing with machine learning and artificial intelligence. They learn input formats or relationships between input and program execution, using the learned results to guide the generation of testing inputs. While these fuzzers can be highly effective, they often come with high overhead due to the involvement of machine learning processing.

### Static Analysis and Its Assistance on Fuzzing
Dowser [24] performs static analysis at compile time to find vulnerable code like loops and pointers, similar to QTEP [54]. Sparks et al. [49] extract control flow graphs from the target to assist input generation. Steelix [33] and VUzzer [43] analyze magic values, immediate values, and strings that can affect control flow.

### Dynamic Analysis and Its Assistance on Fuzzing
Dynamic analysis, including symbolic execution and taint analysis, enhances fuzzing. Taint analysis shows the relationship between input and program execution. BuzzFuzz [19], TaintScope [55], and VUzzer [43] use this technique to find relevant bytes and reduce the mutation space. Symbolic execution helps explore program states. KLEE [8], SAGE [21], MoWF [42], and Driller [50] use this technique to execute deeper logic. To address issues like path explosion and constraint complexity, SYMFUZZ [9] reduces symbolized input bytes using taint analysis, while Angora [12] performs a gradient descent-inspired search during constraint solving. Another challenge is the extra overhead caused by program analysis. REDQUEEN [4] uses lightweight taint tracking and symbolic execution for optimization.

## IX. Conclusion
In this paper, we introduce TortoiseFuzz, an advanced coverage-guided fuzzer with a novel technique called coverage accounting for input prioritization. Based on the insight that the security impact of memory corruption vulnerabilities can be represented by memory operations, and that these operations can be abstracted at different levels, we evaluate edges based on function calls, loops, and basic blocks. We combine this evaluation with coverage information for input prioritization. Our experiments, which tested TortoiseFuzz with six greybox and hybrid fuzzers on 30 real-world programs, showed that TortoiseFuzz outperformed all but one hybrid fuzzer while using only 2% of memory resources. Our experiments also demonstrated that coverage accounting can defend against current anti-fuzzing techniques. Additionally, TortoiseFuzz identified 20 zero-day vulnerabilities, 15 of which have been confirmed and released with CVE IDs.

## Acknowledgements
We thank the anonymous reviewers for their valuable feedback. We also thank Peng Chen, Xiaoning Du, Jinho Jung, Cornelius Aschermann, and other authors of Angora, LEAPORD, Fuzzification, and Redqueen for their assistance with the experiments. This research was supported in part by the National Natural Science Foundation of China (Grant No. U1736209), the Peng Cheng Laboratory Project of Guangdong Province (PCL2018KP004), and the PNC Technologies Career Development Professorship. All opinions expressed in this paper are solely those of the authors.

## References
[1] A. V. Aho, R. Sethi, and J. D. Ullman, “Compilers: Principles, Techniques, and Tools,” Addison Wesley, 1986.
[2] D. Aitel, “An Introduction to Spike, the Fuzzer Creation Kit,” Presentation Slides, Aug, 2002.
[3] P. Amini and A. Portnoy, “Sulley Fuzzing Framework,” http://www.fuzzing.org/wp-content/SulleyManual.pdf, [2019-6-1].
[4] C. Aschermann, S. Schumilo, T. Blazytko, R. Gawlik, and T. Holz, “REDQUEEN: Fuzzing with Input-to-State Correspondence,” in Proceedings of the Network and Distributed System Security Symposium, 2019.
[5] G. Banks, M. Cova, V. Felmetsger, K. Almeroth, R. Kemmerer, and G. Vigna, “SNOOZE: Toward a Stateful Network Protocol Fuzzer,” in Proceedings of the 9th International Conference on Information Security, Springer-Verlag, 2006.
[6] M. Böhme, V.-T. Pham, M.-D. Nguyen, and A. Roychoudhury, “Directed Greybox Fuzzing,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2017.
[7] M. Böhme, V.-T. Pham, and A. Roychoudhury, “Coverage-Based Greybox Fuzzing as Markov Chain,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2016.
[8] C. Cadar, D. Dunbar, D. R. Engler et al., “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” in Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation, USENIX Association, 2008.
[9] S. K. Cha, M. Woo, and D. Brumley, “Program-Adaptive Mutational Fuzzing,” in Proceedings of the 2015 IEEE Symposium on Security and Privacy, IEEE, 2015.
[10] C. Chen, B. Cui, J. Ma, R. Wu, J. Guo, and W. Liu, “A Systematic Review of Fuzzing Techniques,” Computers & Security, 2018.
[11] J. Chen, W. Diao, Q. Zhao, C. Zuo, Z. Lin, X. Wang, W. C. Lau, M. Sun, R. Yang, and K. Zhang, “IoTFuzzer: Discovering Memory Corruptions in IoT Through App-Based Fuzzing,” in NDSS, 2018.
[12] P. Chen and H. Chen, “Angora: Efficient Fuzzing by Principled Search,” in Proceedings of the 2018 IEEE Symposium on Security and Privacy, IEEE Computer Society, 2018.
[13] J. Corina, A. Machiry, C. Salls, Y. Shoshitaishvili, S. Hao, C. Kruegel, and G. Vigna, “DIFUZE: Interface-Aware Fuzzing for Kernel Drivers,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2017.
[14] J. De Ruiter and E. Poll, “Protocol State Fuzzing of TLS Implementations,” in Proceedings of the 24th USENIX Conference on Security Symposium, USENIX Association, 2015.
[15] X. Du, B. Chen, Y. Li, J. Guo, Y. Zhou, Y. Liu, and Y. Jiang, “Leopard: Identifying Vulnerable Code for Vulnerability Assessment Through Program Metrics,” in Proceedings of the 41st International Conference on Software Engineering, IEEE Press, 2019.
[16] M. Eddington, “Peach Fuzzing Platform,” https://www.peach.tech/products/peach-fuzzer/peach-platform/, [2019-6-1].
[17] Q. Feng, R. Zhou, C. Xu, Y. Cheng, B. Testa, and H. Yin, “Scalable Graph-Based Bug Search for Firmware Images,” in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2016.
[18] S. Gan, C. Zhang, X. Qin, X. Tu, K. Li, Z. Pei, and Z. Chen, “CollaFL: Path-Sensitive Fuzzing,” in Proceedings of the 2018 IEEE Symposium on Security and Privacy, IEEE, 2018.
[19] V. Ganesh, T. Leek, and M. Rinard, “Taint-Based Directed Whitebox Fuzzing,” in Proceedings of the 31st International Conference on Software Engineering, IEEE Computer Society, 2009.
[20] gcov, “A Program Coverage Testing Tool,” https://gcc.gnu.org/onlinedocs/gcc/Gcov.html#Gcov, fetched 2020.
[21] P. Godefroid, M. Y. Levin, and D. Molnar, “SAGE: Whitebox Fuzzing for Security Testing,” Queue, 2012.
[22] P. Godefroid, H. Peleg, and R. Singh, “Learn&Fuzz: Machine Learning for Input Fuzzing,” in Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, IEEE Press, 2017.
[23] E. Güler, C. Aschermann, A. Abbasi, and T. Holz, “AntiFuzz: Impeding Fuzzing Audits of Binary Executables,” in Proceedings of the 28th USENIX Security Symposium, 2019.
[24] I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos, “Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations,” in Proceedings of the 22nd USENIX Conference on Security, USENIX Association, 2013.
[25] Intel, “Processor Tracing,” https://software.intel.com/en-us/blogs/2013/09/18/processor-tracing, 2013.
[26] V. Jain, S. Rawat, C. Giuffrida, and H. Bos, “TIFF: Using Input Type Inference to Improve Fuzzing,” in Proceedings of the 34th Annual Computer Security Applications Conference, ACM, 2018.
[27] X. Jia, C. Zhang, P. Su, Y. Yang, H. Huang, and D. Feng, “Towards Efficient Heap Overflow Discovery,” in Proceedings of the 26th USENIX Security Symposium, USENIX Association, 2017.
[28] J. Jung, H. Hu, D. Solodukhin, D. Pagan, K. H. Lee, and T. Kim, “Fuzzification: Anti-Fuzzing Techniques,” in Proceedings of the 28th USENIX Security Symposium, 2019.
[29] G. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks, “Evaluating Fuzz Testing,” in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2018.
[30] C. Lattner and V. Adve, “LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation,” in Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization, IEEE Computer Society, 2004.
[31] C. Lemieux and K. Sen, “FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage,” in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, 2018.
[32] J. Li, B. Zhao, and C. Zhang, “Fuzzing: A Survey,” Cybersecurity, 2018.
[33] Y. Li, B. Chen, M. Chandramohan, S.-W. Lin, Y. Liu, and A. Tiu, “Steelix: Program-State Based Binary Fuzzing,” in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ACM, 2017.
[34] Z. Li, D. Zou, S. Xu, H. Jin, H. Qi, and J. Hu, “VulPecker: An Automated Vulnerability Detection System Based on Code Similarity Analysis,” in Proceedings of the 32nd Annual Conference on Computer Security Applications, 2016.
[35] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y. Zhong, “VulDeePecker: A Deep Learning-Based System for Vulnerability Detection,” in 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018, 2018.
[36] C. Lyu, S. Ji, C. Zhang, Y. Li, W.-H. Lee, Y. Song, and R. Beyah, “MOPT: Optimized Mutation Scheduling for Fuzzers,” in Proceedings of the 28th USENIX Security Symposium, USENIX Association, 2019.
[37] V. J. M. Manès, H. Han, C. Han, S. K. Cha, M. Egele, E. J. Schwartz, and M. Woo, “Fuzzing: Art, Science, and Engineering,” CoRR, 2018.
[38] B. P. Miller, L. Fredriksen, and B. So, “An Empirical Study of the Reliability of UNIX Utilities,” Comm. ACM, 1990.
[39] M. Muench, J. Stijohann, F. Kargl, A. Francillon, and D. Balzarotti, “What You Corrupt Is Not What You Crash: Challenges in Fuzzing Embedded Devices,” in Proceedings of the Network and Distributed System Security Symposium (NDSS), 2018.
[40] S. Neuhaus, T. Zimmermann, C. Holler, and A. Zeller, “Predicting Vulnerable Software Components,” in Proceedings of the 14th ACM Conference on Computer and Communications Security, 2007.
[41] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl, and Y. Acar, “VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015.
[42] V.-T. Pham, M. Böhme, and A. Roychoudhury, “Model-Based Whitebox Fuzzing for Program Binaries,” in Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, ACM, 2016.
[43] S. Rawat, V. Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos, “VUzzer: Application-Aware Evolutionary Fuzzing,” in Proceedings of the 24th Network and Distributed System Security Symposium, The Internet Society, 2017.
[44] R. Scandariato, J. Walden, A. Hovsepyan, and W. Joosen, “Predicting Vulnerable Software Components via Text Mining,” IEEE Transactions on Software Engineering, 2014.
[45] S. Schumilo, C. Aschermann, R. Gawlik, S. Schinzel, and T. Holz, “kAFL: Hardware-Assisted Feedback Fuzzing for OS Kernels,” in 26th USENIX Security Symposium (USENIX Security 17), USENIX Association, 2017.
[46] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “AddressSanitizer: A Fast Address Sanity Checker,” in Usenix Conference on Technical Conference, 2012.
[47] K. Serebryany, “Continuous Fuzzing with LibFuzzer and AddressSanitizer,” IEEE, 2016.
[48] D. She, K. Pei, D. Epstein, J. Yang, B. Ray, and S. Jana, “NEUZZ: Efficient Fuzzing with Neural Program Smoothing,” in Proceedings of the 2018 IEEE Symposium on Security and Privacy, IEEE, 2018.
[49] S. Sparks, S. Embleton, R. Cunningham, and C. Zou, “Automated Vulnerability Analysis: Leveraging Control Flow for Evolutionary Input Crafting,” in Twenty-Third Annual Computer Security Applications Conference (ACSAC 2007), 2007.
[50] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta, Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: Augmenting Fuzzing Through Selective Symbolic Execution,” in Proceedings of the 23rd Network and Distributed Systems Security Symposium, The Internet Society, 2016.
[51] D. Vyukov, “Syzkaller,” https://github.com/google/syzkaller, fetched 2020.
[52] J. Wang, B. Chen, L. Wei, and Y. Liu, “Skyfire: Data-Driven Seed Generation for Fuzzing,” in 2017 IEEE Symposium on Security and Privacy (SP), 2017.
[53] J. Wang, Y. Duan, W. Song, H. Yin, and C. Song, “Be Sensitive and Collaborative: Analyzing Impact of Coverage Metrics in Greybox Fuzzing,” in 22nd International Symposium on Research in Attacks, Intrusions and Defenses (RAID 2019), USENIX Association, 2019.
[54] S. Wang, J. Nam, and L. Tan, “QTEP: Quality-Aware Test Case Prioritization,” in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ACM, 2017.
[55] T. Wang, T. Wei, G. Gu, and W. Zou, “TaintScope: A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection,” in Proceedings of the 2010 IEEE Symposium on Security and Privacy, IEEE Computer Society, 2010.
[56] W. Xu, S. Kashyap, C. Min, and T. Kim, “Designing New Operating Primitives to Improve Fuzzing Performance,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2017.
[57] W. Xu, H. Moon, S. Kashyap, P.-N. Tseng, and T. Kim, “Fuzzing File Systems via Two-Dimensional Input Space Exploration,” in IEEE Symposium on Security and Privacy, 2019.
[58] W. You, X. Wang, S. Ma, J. Huang, X. Zhang, X. Wang, and B. Liang, “Profuzzer: On-the-Fly Input Type Probing for Better Zero-Day Vulnerability Discovery,” in Profuzzer: On-the-Fly Input Type Probing for Better Zero-Day Vulnerability Discovery, IEEE, 2019.
[59] I. Yun, S. Lee, M. Xu, Y. Jang, and T. Kim, “QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing,” in Proceedings of the 27th USENIX Security Symposium, USENIX Association, 2018.
[60] M. Zalewski, “American Fuzzy Lop (AFL) Fuzzer,” http://lcamtuf.coredump.cx/afl/technical_details.t, 2013.
[61] G. Zhang, X. Zhou, Y. Luo, X. Wu, and E. Min, “PTfuzz: Guided Fuzzing with Processor Trace Feedback,” IEEE Access, 2018.