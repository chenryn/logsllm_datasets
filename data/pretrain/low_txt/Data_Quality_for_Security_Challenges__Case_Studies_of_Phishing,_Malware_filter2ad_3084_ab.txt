### 6. Proper Labels and Groupings

While there are several publicly available malware collections, such as VXHeavens, VirusShare, and theZoo, many of these lack proper labeling, balanced and well-distributed types of malware, or recent samples. Therefore, we decided to analyze an industrial dataset that contained ground-truth labels.

#### 3.1 Analysis: Industrial Dataset

Engineers at a security company provided us with a labeled dataset of malware features. The dataset consisted of JSON files representing the behavior of malicious samples during execution. These JSON files were generated by executing malicious Windows Portable Executables within a Cuckoo Sandbox. To group the malware into families, labels were constructed using Suricata network intrusion detection (NID) signatures. All samples that triggered the same signature were grouped into the same family.

Upon receiving the dataset, we took several steps to clean the data:
- **Removal of Incomplete Data:** Several malware JSON files contained no usable data and were removed.
- **Handling Overlapping Classifications:** Many samples were classified into two or more groups, and these were also removed.

After filtering, we were left with 5,673 usable samples. However, this process caused the group sizes to become highly unbalanced. The largest family contained 772 samples, while the smallest groups had only 1 sample, and the mode of all groups was just 3 samples.

Once the dataset was cleaned, we used multiple clustering techniques to see if we could reproduce the provided family groupings [10, 11]. We tested different combinations of malware features, including system and network interactions, to determine which combinations produced the best results.

#### 3.2 Issues with Malware Labeling

A significant challenge in malware research is the difficulty in producing accurate ground-truth labels. Often, it is necessary to group malware based on families of similar samples. Unfortunately, labeling malware into distinguishable groups can be challenging. Most anti-virus vendors disagree on how to label malware, leading to vastly different levels of specificity [4]. As a result, most malware datasets do not come with classification labels or ground-truth for comparison.

To address this, many researchers have implemented anti-virus majority voting systems to construct labels. This method involves using only those malware samples that have the same label across a majority of anti-virus vendors. However, this approach has drawbacks:
- **Reduced Sample Size:** It significantly reduces the number of samples in the dataset, as most malware do not have universal labels. For example, out of 14,212 malicious samples in the ANUBIS dataset, only 2,658 were deemed usable through anti-virus voting [6].
- **Bias Towards Easier Classification:** Using only universally agreed-upon labels may bias the model towards classifying malware that is easier to group or distinguish [13].

Efforts to standardize malware classification labels are ongoing, such as the MAEC [12]. Until standardized methods are developed, manual labeling may be necessary to accurately test the performance of a model.

### 4. Case Study: Intrusion Datasets

For intrusion detection, we analyzed the CICIDS2017 dataset, as the KDD Cup/DARPA dataset is outdated and has been extensively studied [8]. The CICIDS2017 dataset is a synthetic dataset containing a complete capture of all send and receive traffic from the main switch of the Victim Network [15]. It includes both raw packet capture files and 3,119,345 network flows analyzed by their CICFlowMeter and labeled by attack type.

In our exploration of the network flow information, we identified several issues:
- **Missing Information:** The dataset contains 288,602 completely empty records and 1,358 instances missing the number of bytes sent. These empty and incomplete instances were removed.
- **Duplicates:** We found 202 duplicate instances in the dataset, with 201 being benign and 1 being a DoS Hulk attack. Ignoring the timestamp, the number of duplicates increased to 12,981, with 5,393 benign instances, 7,561 DoS Hulk instances, and 27 DoS slowloris instances.
- **Attack Diversity:** After removing instances with missing information and duplicates (not ignoring timestamps), the dataset contained 2,829,183 instances. Of these, 80.32% were labeled benign, and the remaining were spread across various attacks. The three most common attacks constituted 92.87% of the attack instances, while the least common attacks had fewer than 50 instances each.
- **Dataset Difficulty:** To measure the dataset's difficulty, we performed a small experiment using a Decision Tree classifier to identify malicious traffic with a randomly selected training set of 0.1% (2,828 instances) of the dataset. Our test set consisted of 100,000 randomly selected instances, not used in the training set. Over 10 iterations, we achieved a mean accuracy of 92.88%.

### 5. Related Work

A DBLP query on "security data quality" produced 35 results from 1994 to 2015, with fewer than 10 being relevant. We repeated the query on ACM Digital Library, Google Scholar, and IEEE Xplore. Many results had "security" in the title of the journal or conference (e.g., Journal of Computer Security). A summary of the most relevant work is as follows:

- **Taxonomy of Data Quality Terms:** A taxonomy of data quality terms ("dimensions") is presented in [18].
- **Integration of Security and Accuracy:** [14] aimed to integrate security and accuracy into data quality evaluation.
- **Data Quality Challenges in Threat Intelligence:** [16] discussed data quality challenges in sharing threat intelligence.
- **General Survey:** [5] provides a general survey on the topic.

### 6. Conclusions

Through three case studies, we have illustrated the issues with datasets for security challenges. We have shown how to identify and address these issues. We defined dataset difficulty as a measure of dataset quality. Much remains to be done, such as determining when to stop cleaning. One possibility is to see if dataset sources or other key metadata can be accurately identified based on the data.

### Acknowledgments

This research was supported in part by NSF grants CNS 1319212, DGE 1433817, DUE 1356705, and IIS 1659755.

### References

[1] Ayman El Aassal, Shahryar Baki, Avisha Das, and Rakesh M. Verma. 2019. An In-Depth Benchmarking Evaluation of Phishing Detection Research for Security Needs. (2019). To be submitted.

[2] Ayman El Aassal, Luis Moraes, Shahryar Baki, Avisha Das, and Rakesh Verma. 2018. Anti-Phishing Pilot at ACM IWSPA 2018: Evaluating Performance with New Metrics for Unbalanced Datasets. In Proc. of IWSPA-AP Pilot. CEUR, 2–10.

[3] Idan Amit, John Matherly, William Hewlett, Zhi Xu, Yinnon Meshi, and Yigal Weinberger. 2018. Machine Learning in Cyber-Security - Problems, Challenges and Data Sets. (Dec. 2018). Online.

[4] Michael Bailey, Jon Oberheide, Jon Andersen, Z Morley Mao, Farnam Jahanian, and Jose Nazario. 2007. Automated classification and analysis of internet malware. In RAID. Springer, 178–197.

[5] Carlo Batini, Cinzia Cappiello, Chiara Francalanci, and Andrea Maurino. 2009. Methodologies for Data Quality Assessment and Improvement. ACM Comput. Surv. 41, 3 (July 2009), 16:1–16:52.

[6] Ulrich Bayer, Paolo Milani Comparetti, Clemens Hlauschek, Christopher Kruegel, and Engin Kirda. 2009. Scalable, behavior-based malware clustering. In NDSS, Vol. 9. USENIX, 8–11.

[7] Avisha Das, Shahryar Baki, Ayman El Aassal, Rakesh Verma, and Arthur Dunbar. [n. d.]. SoK: A Comprehensive Reexamination of Phishing Research from the Security Perspective. ([n. d.]). Under review.

[8] Abhishek Divekar, Meet Parekh, Vaibhav Savla, Rudra Mishra, and Mahesh Shirole. 2018. Benchmarking datasets for Anomaly-based Network Intrusion Detection: KDD CUP 99 alternatives. (2018). Online.

[9] Y. Fang, C. Zhang, C. Huang, L. Liu, and Y. Yang. 2019. Phishing Email Detection Using Improved RCNN Model With Multilevel Vectors and Attention Mechanism. IEEE Access 7 (2019), 56329–56340. https://doi.org/10.1109/ACCESS.2019.2913705

[10] H. Faridi, S. Srinivasagopalan, and R. Verma. 2018. Performance Evaluation of Features and Clustering Algorithms for Malware. IEEE ICDMW (ADMiS) (2018).

[11] H. Faridi, S. Srinivasagopalan, and R. Verma. 2019. Parameter Tuning and Confidence Limits of Malware Clustering. In Proc. 9th CODASPY. ACM, 169–171.

[12] Ivan Kirillov, Desiree Beck, Penny Chase, and Robert Martin. 2011. Malware attribute enumeration and characterization. (2011).

[13] Peng Li, Limin Liu, Debin Gao, and Michael K Reiter. 2010. On challenges in evaluating malware clustering. In RAID. Springer, 238–255.

[14] Leon Reznik and Elisa Bertino. 2013. POSTER: Data Quality Evaluation: Integrating Security and Accuracy. In Proc. of CCS (CCS ’13). ACM, 1367–1370.

[15] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. 2018. Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization. In ICISSP. SciTePress, 108–116.

[16] Christian Sillaber, Clemens Sauerwein, Andrea Mussmann, and Ruth Breu. 2016. Data Quality Challenges and Future Research Directions in Threat Intelligence Sharing Practice. In Proc. Workshop on Information Sharing and Collaborative Security (WISCS ’16). ACM, 65–70.

[17] Robin Sommer and Vern Paxson. 2010. Outside the Closed World: On Using Machine Learning for Network Intrusion Detection. In 31st IEEE Symp. on Security and Privacy, S&P 2010, 16-19 May 2010. IEEE, 305–316.

[18] Gurvirender Tejay, Gurpreet Dhillon, and Amita Goyal Chin. 2005. Data Quality Dimensions for Information Systems Security: A Theoretical Exposition (Invited Paper). In Security Management, Integrity, and Internal Control in Information Systems. Springer US, Boston, MA, 21–39.

[19] Rakesh Verma and Nabil Hossain. 2014. Semantic Feature Selection for Text with Application to Phishing Email Detection. In Information Security and Cryptology – ICISC 2013, Hyang-Sook Lee and Dong-Guk Han (Eds.). Springer International Publishing, Cham, 455–468.

[20] R.M. Verma, M. Kantarcioglu, D.J. Marchette, E.L. Leiss, and T. Solorio. 2015. Security Analytics: Essential Data Analytics Knowledge for Cybersecurity Professionals and Students. IEEE Security & Privacy 13, 6 (2015), 60–65.