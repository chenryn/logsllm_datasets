### Mimicry on W (Test 2)

- **Behavioral Distance**: 13.813
- **Detection Rates**: 98.9111%, 99.8185%, 100%, 98.9111%, 100%
- **Other Values**:
  - 2.64
  - 13.657
  - 2.687
  - 2.174
  - 2.187
  - 2.65
- **Detection Rates**: 98.7296%, 99.8185%, 98.0944%, 98.9111%, 97.8221%

### Anomaly Detection Technique

The anomaly detection technique we use is based on variable-length system call phrases to model the normal behavior of a running program [37]. In the first test, we assume that the attacker is unaware of our behavioral distance calculation between replicas and the existence of multiple replicas. In the second test, we assume the attacker not only knows about the behavioral distance calculation but also has a copy of the distance table used in the calculation. This makes the attacker in the second test the most powerful, with complete knowledge of our system.

In both tests, we exhaustively search for the best mimicry attack. The "best" mimicry attack in the first test is one that makes the minimal number of system calls while remaining undetected. In the second test, the "best" mimicry attack results in the smallest behavioral distance between system call sequences from the two replicas. We assume that the mimicry attack in both cases results in a request to the uncorrupted replica that produces a "page not found" response.

### Results

Results of both tests are shown in Table 1. For each individual test, Table 1 shows the behavioral distance of the best mimicry attack and the percentage of testing data (from Section 4.2) that has a smaller behavioral distance. This percentage indicates the true acceptance rate of our system when the detection threshold is set to detect the best mimicry attack. As shown, these percentages are all very close to 100%, indicating a relatively low false alarm rate even when the system is configured to detect the most sophisticated mimicry attacks. By comparing results from the two sets of tests, we can see the trade-off between better detection capability and lower false positive rate. For example, by setting the threshold to detect any mimicry attacks that could have evaded detection by an isolated intrusion/anomaly detection system on one of the replicas (results in test 1), our system will have a much lower false positive rate (between 0% and 0.5%).

### Performance Overhead

#### Resilience and False Positive Rate

Sections 4.2 and 4.3 show that our method for behavioral distance is more resilient against mimicry attacks than previous approaches and has a low false positive rate. In this section, we evaluate the performance overhead of our implementation of the behavioral distance calculation by measuring the throughput of the HTTP servers and the average latency of the requests. The performance evaluation shows that the performance overhead is moderate. Note that our current implementation is unoptimized, so the performance overhead will be even lower with an optimized implementation.

#### Experiments

We run two experiments to evaluate our performance overhead:

1. **Performance Degradation Due to System Call Information Extraction and Transmission**:
   - **Setup**: Two different tests on a single server running Windows (with a 2.0 GHz Pentium IV processor and 512 MB memory).
   - **Tests**:
     - **Test 1**: The machine runs the Abyss X1 webserver.
     - **Test 2**: The machine runs the same webserver and extracts and sends out the system call information to another machine for the behavioral distance calculation.
   - **Results**: The second test has a 6.6% overhead in throughput and 6.4% overhead in latency compared to the first test. This shows that intercepting and sending out system call information causes very low performance overhead on a single server in terms of both throughput and latency.

2. **Performance Overhead Compared to Output Voting**:
   - **Setup**: Three tests to measure the performance overhead of our implementation of the behavioral distance on a replicated system with Abyss X1 webservers.
   - **Tests**:
     - **Test 1**: Replicas L and W serve as webservers without capturing system call sequences. Proxy P does output voting.
     - **Test 2**: Replicas L and W capture system calls and send them to machine C for behavioral distance calculation.
     - **Test 3**: Output voting and behavioral distance calculation are both performed on the proxy P.
   - **Results**:
     - **Test 2**: 3.58% overhead in throughput and 0.089 milliseconds in latency.
     - **Test 3**: 24.3% reduction in throughput and 0.848 milliseconds in latency after 50 minutes.

The results suggest that a slightly more powerful machine for the proxy is needed if we want to perform behavioral distance calculations on the critical path of server responses, to maintain peak throughput. However, even in our tests, the overhead in latency is less than a millisecond.

### Conclusion

In this paper, we introduce behavioral distance to evaluate the extent to which two processes behave similarly in response to a common input. Behavioral distance can be used to detect software faults or attacks on a replica, particularly those that do not immediately yield evidence in the output. We propose a measure of behavioral distance and its realization using system calls emitted by processes. Through empirical evaluation using three web servers on Linux and Windows, we demonstrate that this approach can detect sophisticated mimicry attacks with a low false positive rate and moderate overhead.

### References

1. Myserver. http://www.myserverproject.net.
2. L. Alvisi, D. Malkhi, E. Pierce, and M. K. Reiter. Fault detection for Byzantine quorum systems. IEEE Transactions on Parallel Distributed Systems, 12(9), September 2001.
3. R. W. Buskens and Jr. R. P. Bianchini. Distributed on-line diagnosis in the presence of arbitrary faults. In Proceedings of the 23rd International Symposium on Fault-Tolerant Computing, pages 470–479, June 1993.
4. M. Castro, R. Rodrigues, and B. Liskov. BASE: Using abstraction to improve fault tolerance. ACM Transactions on Computer Systems (TOCS), 21(3):236–269, 2003.
5. L. Chen and A. Avizienes. N-version programming: A fault-tolerance approach to reliability of software operation. In Proceedings of the 8th International Symposium on Fault-Tolerant Computing, pages 3–9, 1978.
6. S. Cheung, R. Crawford, M. Dilger, J. Frank, J. Hoagland, K. Levitt, J. Rowe, S. Staniford-Chen, R. Yip, and D. Zerkle. The design of GrIDS: A graph-based intrusion detection system. Technical Report CSE-99-2, Computer Science Department, U.C. Davis, 1999.
7. C. Collberg, C. Thomborson, and D. Low. Manufacturing cheap, resilient, and stealthy opaque constructs. In Proceedings of the ACM Symposium on Principles of Programming Languages, January 1998.
8. H. H. Feng, J. T. Griffin, Y. Huang, S. Jha, W. Lee, and B. P. Miller. Formalizing sensitivity in static analysis for intrusion detection. In Proceedings of the 2004 IEEE Symposium on Security and Privacy, 2004.
9. H. H. Feng, O. M. Kolesnikov, P. Fogla, W. Lee, and W. Gong. Anomaly detection using call stack information. In Proceedings of the 2003 IEEE Symposium on Security and Privacy, 2003.
10. S. Forrest and T. A. Langstaff. A sense of self for Unix processes. In Proceedings of the 1996 IEEE Symposium on Security and Privacy, 1996.
11. The Apache Software Foundation. Apache HTTP Server. http://httpd.apache.org.
12. D. Gao, M. K. Reiter, and D. Song. Gray-box extraction of execution graph for anomaly detection. In Proceedings of the 11th ACM Conference on Computer & Communication Security, 2004.
13. D. Gao, M. K. Reiter, and D. Song. On gray-box program tracking for anomaly detection. In Proceedings of the 13th USENIX Security Symposium, 2004.
14. J. T. Griffin, S. Jha, and B. P. Miller. Detecting manipulated remote call streams. In Proceedings of the 11th USENIX Security Symposium, 2002.
15. J. T. Griffin, S. Jha, and B. P. Miller. Efficient context-sensitive intrusion detection. In Proceedings of Symposium on Network and Distributed System Security, 2004.
16. C. Kruegel, D. Mutz, F. Valeur, and G. Vigna. On the detection of anomalous system call arguments. In Proceedings of the 8th European Symposium on Research in Computer Security (ESORICS 2003), 2003.
17. L. Lamport. The implementation of reliable distributed multiprocess systems. In Computer Networks 2, 1978.
18. X. Lu. A Linux executable editing library. Master’s thesis, Computer and Information Science Department, National University of Singapore, 1999.
19. G. Nebbett. Windows NT/2000 Native API Reference. Sams Publishing, 2000.
20. M. Nei and S. Kumar. Molecular Evolution and Phylogenetics. Oxford University Press, 2000.
21. P. Ning, Y. Cui, and D. S. Reeves. Analyzing intensive intrusion alerts via correlation. In Recent Advances in Intrusion Detection (Lecture Notes in Computer Science vol. 2516), 2002.
22. M. Prasad and T. Chiueh. A binary rewriting defense against stack-based buffer overflow attacks. In Proceedings of the USENIX Annual Technical Conference, June 2003.
23. I. Rigoutsos and A. Floratos. Combinatorial pattern discovery in biological sequences. Bioinformatics, 14(1):55–67, 1998.
24. T. Romer, G. Voelker, D. Lee, A. Wolman, W. Wong, H. Levy, B. Bershad, and B. Chen. Instrumentation and optimization of Win32/Intel executables using Etch. In Proceedings of the USENIX Windows NT Workshop, August 1997.
25. F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM Computing Surveys, 22(4):299–319, December 1990.
26. B. Schwarz, S. Debray, and G. Andrews. Disassembly of executable code revisited. In Proceedings of the Working Conference on Reverse Engineering, pages 45–54, 2002.
27. R. Sekar, M. Bendre, D. Dhurjati, and P. Bollineni. A fast automaton-based method for detecting anomalous program behaviors. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, 2001.
28. P. H. Sellers. On the theory and computation of evolutionary distances. SIAM J. Appl. Math., 26:787–793.
29. K. Shin and P. Ramanathan. Diagnosis of processors with Byzantine faults in a distributed computing system. In Proceedings of the 17th International Symposium on Fault-Tolerant Computing, pages 55–60, 1987.
30. S. R. Snapp, S. E. Smaha, D. M. Teal, and T. Grance. The DIDS (Distributed Intrusion Detection System) prototype. In Proceedings of the Summer USENIX Conference, pages 227–233, 1992.
31. K. Tan, J. McHugh, and K. Killourhy. Hiding intrusions: From the abnormal to the normal and beyond. In Proceedings of the 5th International Workshop on Information Hiding, October 2002.
32. Aprelium Technologies. Abyss Web Server. http://www.aprelium.com.
33. A. Valdes and K. Skinner. Probabilistic alert correlation. In Recent Advances in Intrusion Detection (Lecture Notes in Computer Science vol. 2212), 2001.
34. VeriTest. WebBench. http://www.veritest.com/benchmarks/webbench/default.asp
35. D. Wagner and D. Dean. Intrusion detection via static analysis. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, 2001.
36. D. Wagner and P. Soto. Mimicry attacks on host-based intrusion detection systems. In Proceedings of the 9th ACM Conference on Computer and Communications Security, 2002.
37. A. Wespi, M. Dacier, and H. Debar. Intrusion detection using variable-length audit trail patterns. In Proceedings of the 2000 Recent Advances in Intrusion Detection, 2000.
38. Y. Xie, H. Kim, D. O’Hallaron, M. K. Reiter, and H. Zhang. Seurat: A pointillist approach to anomaly detection. In Recent Advances in Intrusion Detection (Lecture Notes in Computer Science 3224), pages 238–257, September 2004.
39. J. Yin, J.-P. Martin, A. Venkataramani, L. Alvisi, and M. Dahlin. Separating agreement from execution for Byzantine fault-tolerant services. In Proceedings of the 19th ACM Symposium on Operating System Principles, 2003.