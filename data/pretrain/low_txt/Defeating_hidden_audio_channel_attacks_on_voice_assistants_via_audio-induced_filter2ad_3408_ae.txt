### 73.80%
**Table 8: Performance of Frontend Setup with Various Smartphone Usage Scenarios (Unsupervised)**

| Scenario            | K-medoids | K-means |
|---------------------|-----------|---------|
| Held in Hand        | 87.30%    | 87.30%  |
| Placed on Sofa      | 100%      | 100%    |
| 80% Volume on Table | 100%      | 100%    |
| 2x Speed on Table   | 88.30%    | 85.20%  |

### 6.2.4 Various Mobile Device Usage Scenarios
We next evaluate the frontend setup under various practical mobile device usage scenarios. We only report the results for unsupervised learning methods, as supervised learning methods generally perform better. Table 8 shows the performance of our system under different scenarios, including when the mobile device is held in hand, placed on a hard table surface, and placed on a soft surface like a sofa. Additionally, we evaluate the system with lower volume (e.g., 80%) and fast forwarding (e.g., 2x speed). Our system achieves high accuracy in all setups. Specifically, it achieves 100% accuracy in distinguishing hidden voice commands when the device is placed on a table or sofa, and 87.30% accuracy when held in hand. At 80% volume, the system still maintains 100% accuracy, demonstrating its robustness to varying playback volumes. In the fast-forwarding scenario at 2x speed, the accuracy slightly degrades to 88.30% and 85.20% for K-medoids and K-means, respectively. This is due to distortions and information loss in the fast-forwarded audio.

### 6.2.3 Partial Playback to Reduce Delay
We further evaluate the performance of our system using partial voice command playback to reduce delay. Specifically, we set the system to playback only 1 second or 0.5 seconds of the recorded voice commands and apply the unsupervised learning method K-means to detect hidden voice commands. The results for partial replay in both the frontend and backend setups are shown in Table 6. Our system can achieve up to 99.9% accuracy for both 1-second and 0.5-second playbacks in the frontend mode. For the backend mode, the accuracy is up to 99.1% for 1-second playback and 90.5% for 0.5-second playback. This is because our unique vibration features capture the inherent signatures of the user’s voice, which are independent of the length of the voice commands. We also find that the K-medoids performance is similar to K-means, and supervised learning methods perform slightly better. These results are promising and facilitate deploying our system without causing additional delay.

### 7 CONCLUSION & DISCUSSION
In this work, we demonstrate that hidden voice commands, which mimic the voice features of normal commands but remain incomprehensible to humans, can be detected by comparing their speech features in the vibration domain. Using vibration features, including statistical features in the time and frequency domains and speech features (e.g., MFCCs, chroma vector), we can distinguish hidden voice commands from normal commands with high accuracy. Our classification results, obtained through supervised and unsupervised learning methods, indicate that using the onboard accelerometer (found universally on all smartphones) to log these speech vibrations can help detect hidden voice commands. This capability opens up new discussions on securing voice assistants against synthesized voice commands. The vibration domain features can be used as a standalone mechanism or in conjunction with audio domain features, offering additional defense approaches in voice security.

We recognize that the playback process of our system may cause some delay and intrusion in the frontend mode. However, our system can achieve high accuracies with partial command playback and n-times speed playback, which speed up the process (e.g., to 0.5 seconds). Existing Voice Command Systems (VCSs) typically take several seconds to process audio for understanding the command context (e.g., 2 seconds for Google Now and 4 seconds for Siri [6]). Our system is designed to work simultaneously with this process, avoiding additional delays. Furthermore, such short-time playback incurs less intrusion to users, who can still choose the backend mode to defend against hidden command attacks.

In future work, we will explore modulating the frontend playback sound to an inaudible frequency (e.g., greater than 16 kHz) to achieve zero intrusion. Additionally, we will investigate a more practical backend playback setup consisting of a tiny, low-cost device equipped with an onboard speaker and motion sensors. It is also worth exploring whether our unsupervised learning-based method can defend against other types of attacks, such as ultrasound attacks, without requiring much training effort. Finally, we will examine whether replay sounds can be distinguished from live human voices in the vibration domain.

### 8 ACKNOWLEDGMENTS
This work was supported partially by the National Science Foundation Grants CNS1820624, CNS1814590, CNS1801630, CNS1714807, CNS1526524, DOE1662762, and ARO W911NF-18-1-0221.

### REFERENCES
[1] [n.d.]. PDV-100 PORTABLE DIGITAL VIBROMETER: Vibration sensor for mobile use. https://www.polytec.com/us/vibrometry/products/single-point-vibrometers/pdv-100-portable-digital-vibrometer/. Accessed: 12/13/2018.
[2] 2018. Text To Speech (TTS) service. http://www.fromtexttospeech.com/.
[3] 2018. Voice Assistant Market Research Report- Global Forecast 2023. https://www.marketresearchfuture.com/reports/voice-assistant-market-4003.
[4] Talal B Amin, James S German, and Pina Marziliano. 2013. Detecting voice disguise from speech variability: Analysis of three glottal and vocal tract measures. In Proceedings of Meetings on Acoustics 166ASA, Vol. 20. ASA, 060005.
[5] S Abhishek Anand and Nitesh Saxena. 2018. Speechless: Analyzing the Threat to Speech Privacy from Smartphone Motion Sensors. In Proceedings of the 39th IEEE Symposium on Security and Privacy (S&P). 116–133.
[6] Mehdi Assefi, Guangchi Liu, Mike P Wittie, and Clemente Izurieta. 2015. An experimental evaluation of Apple Siri and Google Speech Recognition. Proceedings of the 2015 ISCA SEDE (2015).
[7] Ronald J. Baken and Robert F. Orlikoff. 1987. Clinical Measurement of Speech and Voice. London: Taylor and Francis Ltd.
[8] Logan Blue, Hadi Abdullah, Luis Vargas, and Patrick Traynor. 2018. 2MA: Verifying Voice Commands via Two Microphone Authentication. In Proceedings of the 2018 on Asia Conference on Computer and Communications Security. ACM, 89–100.
[9] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Commands. USENIX Security Symposium. 513–530.
[10] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted attacks on speech-to-text. 2018 IEEE Security and Privacy Workshops (SPW). IEEE, 1–7.
[11] Simon Castro, Robert Dean, Grant Roth, George T Flowers, and Brian Grantham. 2007. Influence of acoustic noise on the dynamic performance of MEMS gyroscopes. ASME 2007 International Mechanical Engineering Congress and Exposition. American Society of Mechanical Engineers, 1825–1831.
[12] Phillip L De Leon, Michael Pucher, Junichi Yamagishi, Inma Hernaez, and Ibon Saratxaga. 2012. Evaluation of speaker verification security and detection of HMM-based synthetic speech. IEEE Transactions on Audio, Speech, and Language Processing 20, 8 (2012), 2280–2290.
[13] Robert Neal Dean, Simon Thomas Castro, George T Flowers, Grant Roth, Anwar Ahmed, Alan Scottedward Hodel, Brian Eugene Grantham, David Allen Bittle, and James P Brunsch. 2011. A characterization of the performance of a MEMS gyroscope in acoustically harsh environments. IEEE Transactions on Industrial Electronics 58, 7 (2011), 2591–2596.
[14] Robert N Dean, George T Flowers, A Scotte Hodel, Grant Roth, Simon Castro, Ran Zhou, Alfonso Moreira, Anwar Ahmed, Rifki Rifki, Brian E Grantham, et al. 2007. On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise. Industrial Electronics, 2007. ISIE 2007. IEEE International Symposium on. IEEE, 1435–1440.
[15] Wenrui Diao, Xiangyu Liu, Zhe Zhou, and Kehuan Zhang. 2014. Your voice assistant is mine: How to abuse speakers to steal information and control your phone. Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones & Mobile Devices. ACM, 63–74.
[16] Huan Feng, Kassem Fawaz, and Kang G Shin. 2017. Continuous authentication for voice assistants. Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking. ACM, 343–355.
[17] Jeff Gamet. 2018. Need Your HomePod to Recalibrate its Sound for Your Room? Give it a Shake. https://www.macobserver.com/tips/quick-tip/homepod-recalibrate-shake/.
[18] Rosa González Hautamäki, Tomi Kinnunen, Ville Hautamäki, and Anne-Maria Laukkanen. 2015. Automatic versus human speaker verification: The case of voice mimicry. Speech Communication 72 (2015), 13–31.
[19] Rosa González Hautamäki, Tomi Kinnunen, Ville Hautamäki, Timo Leino, and Anne-Maria Laukkanen. 2013. I-vectors meet imitators: On vulnerability of speaker verification systems against voice mimicry. Interspeech. Citeseer, 930–934.
[20] Chadawan Ittichaichareon, Siwat Suksri, and Thaweesak Yingthawornsuk. 2012. Speech recognition using MFCC. International Conference on Computer Graphics, Simulation and Modeling (ICGSM’2012) July. 28–29.
[21] Chaouki Kasmi and Jose Lopes Esteves. 2015. IEMI threats for information security: Remote command injection on modern smartphones. IEEE Transactions on Electromagnetic Compatibility 57, 6 (2015), 1752–1755.
[22] Yan Michalevsky, Dan Boneh, and Gabi Nakibly. 2014. Gyrophone: Recognizing Speech from Gyroscope Signals. USENIX Security Symposium. 1053–1067.
[23] Dibya Mukhopadhyay, Maliheh Shirvanian, and Nitesh Saxena. 2015. All your voices are belong to us: Stealing voices to fool humans and machines. European Symposium on Research in Computer Security. Springer, 599–621.
[24] Emmanuel Munguia Tapia. 2008. Using machine learning for real-time activity recognition and estimation of energy expenditure. Ph.D. Dissertation. Massachusetts Institute of Technology.
[25] K Sri Rama Murty and Bayya Yegnanarayana. 2006. Combining evidence from residual phase and MFCC features for speaker recognition. IEEE Signal Processing Letters 13, 1 (2006), 52–55.
[26] Ingo R. Titze and Daniel W. Martin. 1994. Principles of Voice Production. Vol. 104. Prentice Hall (currently published by NCVS.org). https://doi.org/10.1121/1.424266.
[27] Timothy Trippel, Ofir Weisse, Wenyuan Xu, Peter Honeyman, and Kevin Fu. 2017. WALNUT: Waging doubt on the integrity of MEMS accelerometers with acoustic injection attacks. Security and Privacy (EuroS&P), 2017 IEEE European Symposium on. IEEE, 3–18.
[28] Tavish Vaidya, Yuankai Zhang, Micah Sherr, and Clay Shields. 2015. Cocaine noodles: Exploiting the gap between human and machine speech recognition. WOOT 15 (2015), 10–11.
[29] Olli Viikki and Kari Laurila. 1998. Cepstral domain segmental feature vector normalization for noise robust speech recognition. Speech Communication 25, 1-3 (1998), 133–147.
[30] Zhizheng Wu, Xiong Xiao, Eng Siong Chng, and Haizhou Li. 2013. Synthetic speech detection using temporal modulation feature. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 7234–7238.
[31] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A Gunter. 2018. CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition. USENIX Security Symposium.
[32] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and Wenyuan Xu. 2017. DolphinAttack: Inaudible voice commands. Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 103–117.
[33] Li Zhang, Parth H Pathak, Muchen Wu, Yixin Zhao, and Prasant Mohapatra. 2015. Accelword: Energy efficient hotword detection through accelerometer. Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 301–315.
[34] Linghan Zhang, Sheng Tan, and Jie Yang. 2017. Hearing Your Voice is Not Enough: An Articulatory Gesture Based Liveness Detection for Voice Authentication. Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 57–71.
[35] Linghan Zhang, Sheng Tan, Jie Yang, and Yingying Chen. 2016. Voice-live: A phoneme localization based liveness detection for voice authentication on smartphones. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 1080–1091.