### 2.0 Bandwidth (Mbits/sec)
2.5

**Figure 6: Heterogeneous Client Bandwidth Experiments with 50 LAN Clients, All Good**
- The fraction of the server (c = 10 requests/s) allocated to the ten clients in category i, with bandwidth 0.5 · i Mbits/s, is close to the ideal proportional allocation.

### Byte Cost Measurement
- We measure the number of bytes uploaded for served requests, referred to as the "price," as recorded by the thinner.
- **Figure 5** shows the average price for good and bad clients, and also plots the theoretical average price, (G + B)/c, from §3.3, labeled “Upper Bound.”
- The actual price is lower than the theoretical one because clients do not consume all of their bandwidth, for reasons we will now describe.

### Analysis of Different Values of c
- **For c = 50:**
  - Each good client spends an average of 1.25 Mbits/s, determined by tallying the total bits spent by good clients over the experiment.
  - This average is less than the 2 Mbits/s access link due to a quiescent period between when a good client first issues a request and when the thinner replies, asking for payment. This period is roughly 0.35 seconds, attributed to a long backlog at the thinner of requests and payment bytes.
  - When not in a quiescent period, a good client consumes most of its access link, delivering 1.8 Mbits/s on average, inferred by dividing the average good client payment (Figure 5) by the average time spent paying (Figure 4).
  - Bad clients, in contrast, keep multiple requests outstanding and thus do not have "down time." For c = 50, their payments are 1.7 Mbits/s on average. They occasionally "waste" bytes when they establish a payment channel but fail to deliver the accompanying request due to nearly fully utilized outbound bandwidth. The thinner accepts payment for 10 seconds before timing out the payment channel.

- **For c = 100:**
  - The scenario is similar to c = 50, but bad clients see a higher price than good ones. This is because bad clients waste bytes, and some requests arrive after the client has paid enough to win the auction, leading to overpayment and increasing their average price.

- **For c = 200:**
  - Clients do not have to pay much because the server is lightly loaded. Good and bad clients often encounter a price of zero, though bad clients sometimes overpay.

### 7.4 Empirical Adversarial Advantage
- As discussed, bad clients deliver more bytes than good clients in our experiments, resulting in the server not achieving the ideal bandwidth-proportional allocation, as seen in §7.2.
- To better understand this adversarial advantage, we asked: What is the minimum value of c at which all of the good demand is satisfied?
- Experimenting with the same configuration (G = B = 50 Mbits/s; 50 clients) but for more values of c, we found that all good demand is satisfied at c = 115, which is only 15% more provisioning than cid, the capacity needed under exact proportional allocation.
- We conclude that a bad client can cheat the proportional allocation mechanism but only to a limited extent—at least under our model of bad behavior.

### 7.5 Heterogeneous Network Conditions
- We investigate the server’s allocation for different client bandwidths and RTTs.
- **Bandwidth:**
  - 50 clients are assigned to 5 categories. The 10 clients in category i (1 ≤ i ≤ 5) have bandwidth 0.5 · i Mbits/s and are connected to the thinner over a LAN. All clients are good.
  - The server has capacity c = 10 requests/s. Figure 6 shows that the resulting server allocation to each category is close to the bandwidth-proportional ideal.

- **RTT:**
  - We hypothesize that the RTT between a good client and the thinner will affect the allocation due to TCP ramp-up and quiescent periods.
  - We assign 50 clients to 5 categories. The 10 clients in category i (1 ≤ i ≤ 5) have RTT = 100 · i ms to the thinner, giving a wide range of RTTs. All clients have bandwidth 2 Mbits/s, and c = 10 requests/s.
  - We experiment with two cases: all clients good and all bad. Figure 7 confirms our hypothesis: good clients with longer RTTs get a smaller share of the server while for bad clients, RTT matters little. The effect is limited, with no good client getting more than double or less than half the ideal.

### 7.6 Good and Bad Clients Sharing a Bottleneck
- When good clients share a bottleneck link with bad ones, good requests can be "crowded out" by bad ones before reaching the thinner (see §4.2).
- Experiment setup: 30 clients, each with a bandwidth of 2 Mbits/s, connect to the thinner through a common link, l, with a bandwidth of 40 Mbits/s. Additionally, 10 good and 10 bad clients, each with a bandwidth of 2 Mbits/s, connect to the thinner directly through a LAN. The server’s capacity is c = 50 requests/s.
- In all cases, the clients behind l together capture half of the server’s capacity. We measure how this "server half" is allocated to the good and bad clients behind l and the fraction of good requests that receive service. Figure 8 depicts these measurements and compares them to the bandwidth-proportional ideals.

### 7.7 Impact of Speak-up on Other Traffic
- We consider how speak-up affects other traffic, specifically what happens when a TCP endpoint, H, shares a bottleneck link, m, with clients uploading dummy bytes.
- **TCP Sender:** m is shared among H’s transfer and the speak-up uploads.
- **TCP Receiver:** Extra traffic from speak-up affects H in two ways: ACKs from H are lost and delayed more often, and for request-response protocols (e.g., HTTP), H’s request can be delayed.
- Experiment setup: 10 good speak-up clients share a bottleneck link, m, with H, a host running the HTTP client wget. m has a bandwidth of 1 Mbit/s and one-way delay of 100 ms. Each of the 11 clients has a bandwidth of 2 Mbits/s. On the other side of m are the thinner (fronting a server with c = 2 requests/s) and a separate Web server, S. H downloads a file from S 100 times.
- **Figure 9** shows the means and standard deviations of the download latency for various file sizes, with and without the speak-up traffic. There is significant "collateral damage" to "innocently bystander" Web transfers: download times inflate by almost 6× for a 1 Kbyte (single packet) transfer and by almost 4.5× for 64 Kbyte transfers. However, this experiment is quite pessimistic, with large RTTs, highly restrictive bottleneck bandwidth, and low server capacity.

### 8 Related Work
- We survey related work in the context of comparing speak-up to other defenses against application-level DDoS attacks.
- **Comparisons to Related Work:**
  - Using the taxonomy in §1 (massive over-provisioning, detect and block, currency), speak-up is a currency scheme. The currency concept was pioneered by Dwork and Naor [12] in the context of spam defense.
  - Others have done work in the same spirit [1,6,7,11,20,25,49], often called proof-of-work schemes.
  - We first proposed bandwidth as a currency in a workshop paper [48]. This paper provides a viable mechanism, implementation, evaluation, and analysis, and presents a solution to the "unequal requests" case.
  - We do not know of another proposal to use bandwidth as a currency. However, [17, 39] describe a solution to DoS.