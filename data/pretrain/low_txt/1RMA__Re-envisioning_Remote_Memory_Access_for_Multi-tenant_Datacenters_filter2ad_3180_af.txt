### Frames and Scalability Issues

Frames in datacenter networks can lead to well-known issues [31, 41]. eRPC [18] is a high-performance Remote Procedure Call (RPC) library designed for datacenter environments. Similar to [16, 21], eRPC leverages the User Datagram (UD) transport protocol to mitigate connection scalability problems and is fundamentally two-sided. This means that, like all RPC abstractions and Pony, eRPC requires software on both the client and server sides, leading to two-sided performance characteristics. In contrast, 1RMA focuses on one-sided primitives.

### Performance Anomalies and Solutions

Recent research has explored performance anomalies that occur when multiple RDMA applications coexist, identifying head-of-line blocking in RDMA Network Interface Cards (RNICs) as a primary cause [39, 40]. To address these anomalies, Justitia [40] proposes a software-based solution that employs shaping, rate limiting, and pacing at the senders. However, Justitia can only provide latency guarantees in a best-effort manner, as it enforces isolation through shared incentives. Additionally, Justitia remains connection-oriented and does not resolve the broader set of issues associated with standard RDMA (§2).

### Conclusion

This paper introduces 1RMA, a ground-up rearchitecture of remote memory access designed for multi-tenant datacenters. 1RMA is based on a principled division of labor between software and hardware. The connection-free hardware in 1RMA treats each RMA operation independently, providing fine-grained delay measurements and fast failure notifications to the software. 1RMA software handles congestion control, while applications manage failure recovery and inter-operation ordering as needed. The connection-free design of 1RMA supports confidentiality, authentication, and integrity at line rate, with minimal performance or availability disruption during management actions such as encryption key rotation. This work does not raise any ethical concerns.

### Acknowledgments

We would like to thank Jeff Mogul, Nandita Dukkipati, Philip Wells, the anonymous SIGCOMM reviewers, and our shepherd, Costin Raiciu, for their valuable feedback. We also extend our gratitude to the production, serving, and support teams at Google, including but not limited to the Pony Express and KVCS teams. Special thanks go to our hardware development and verification teams for their significant contributions.

### References

[1] 2020. Alibaba Super Computing Cluster. https://www.alibabacloud.com/product/

[2] 2020. Amazon Elastic Block Store. https://aws.amazon.com/ebs/

[3] 2020. Amazon S3. https://aws.amazon.com/s3/

[4] 2020. Microsoft Bing. https://www.bing.com/

[5] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010. Data Center TCP (DCTCP). In Proceedings of the 2010 Conference of the ACM Special Interest Group on Data Communication. 63–74.

[6] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks 30 (1998), 107–117.

[7] Brad Calder, Ju Wang, Aaron Ogus, Niranjan Nilakantan, Arild Skjolsvold, Sam McKelvie, Yikang Xu, Shashwat Srivastav, Jiesheng Wu, Huseyin Simitci, Jaidev Haridas, Chakravarthy Uddaraju, Hemal Khatri, Andrew Edwards, Vaman Bedekar, Shane Mainali, Rafay Abbasi, Arpit Agarwal, Mian Fahim ul Haq, Muhammad Ikram ul Haq, Deepali Bhardwaj, Sowmya Dayanand, Anitha Adusumilli, Marvin McNett, Sriram Sankaran, Kavitha Manivannan, and Leonidas Rigas. 2011. Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles. 143–157.

[8] Wei Cao, Zhenjun Liu, Peng Wang, Sen Chen, Caifeng Zhu, Song Zheng, Yuhui Wang, and Guoqing Ma. 2018. PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Database. Proceedings of the Very Large Databases Endowment 11, 12 (2018), 1849–1862.

[9] Youmin Chen, Youyou Lu, and Jiwu Shu. 2019. Scalable RDMA RPC on Reliable Connection with Efficient Resource Sharing. In Proceedings of the Fourteenth European Conference on Computer Systems. 1–14.

[10] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, J. J. Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang, and Dale Woodford. 2013. Spanner: Google’s Globally-Distributed Database. ACM Transactions on Computer Systems 31, 3 (2013), 1–22.

[11] Diego Crupnicoff, Michael Kagan, Ariel Shahar, Noam Block, and Hillel Chapman. 2012. Dynamically-Connected Transport Service. US Patent 8,213,315.

[12] Aleksandar Dragojević, Dushyanth Narayanan, Orion Hodson, and Miguel Castro. 2014. FaRM: Fast Remote Memory. In Proceedings of the Eleventh USENIX Symposium on Networked Systems Design and Implementation. 401–414.

[13] Aleksandar Dragojević, Dushyanth Narayanan, Edmund B Nightingale, Matthew Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No Compromises: Distributed Transactions with Consistency, Availability, and Performance. In Proceedings of the Twenty-Fifth Symposium on Operating Systems Principles. 54–70.

[14] Morris Dworkin. 2007. Recommendation for Block Cipher Modes of Operation: Galois/Counter Mode (GCM) and GMAC. https://csrc.nist.gov/publications/detail/sp/800-38d/final.

[15] Franz Färber, Sang Kyun Cha, Jürgen Primsch, Christof Bornhövd, Stefan Sigg, and Wolfgang Lehner. 2012. SAP HANA Database: Data Management for Modern Business Applications. ACM Special Interest Group on Management of Data Record 40, 4 (2012), 45–51.

[16] Ryan E Grant, Mohammad J Rashti, Pavan Balaji, and Ahmad Afsahi. 2015. Scalable Connectionless RDMA over Unreliable Datagrams. Parallel Comput. 48 (2015), 15–39.

[17] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye, and Marina Lipshteyn. 2016. RDMA over Commodity Ethernet at Scale. In Proceedings of the 2016 Conference of the ACM Special Interest Group on Data Communication. ACM, 202–215.

[18] Anuj Kalia, Michael Kaminsky, and David Andersen. 2019. Datacenter RPCs can be General and Fast. In Proceeding of Sixteenth USENIX Symposium on Networked Systems Design and Implementation. 1–16.

[19] Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2014. Using RDMA Efficiently for Key-Value Services. In Proceedings of the 2014 Conference of the ACM Special Interest Group on Data Communication. 295–306.

[20] Anuj Kalia, Michael Kaminsky, and David G. Andersen. 2016. Design Guidelines for High Performance RDMA Systems. In Proceedings of 2016 USENIX Annual Technical Conference. 437–450.

[21] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs. In Proceedings of Twelfth USENIX Symposium on Operating Systems Design and Implementation. 185–201.

[22] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan M. G. Wassel, Xian Wu, Behnam Montazeri, Yaogong Wang, Yaogong Wang, Kevin Springborn, Christopher Alfeld, Mike Ryan, David Wetherall, and Amin Vahdat. 2020. Swift: Delay is Simple and Effective for Congestion Control in the Datacenter. In Proceedings of the 2020 Conference of the ACM Special Interest Group on Data Communication.

[23] Yanfang Le, Brent Stephens, Arjun Singhvi, Aditya Akella, and Michael M Swift. 2018. RoGUE: RDMA over Generic Unconverged Ethernet. In Proceedings of the ACM Symposium on Cloud Computing. 225–236.

[24] Feifei Li. 2019. Cloud-Native Database Systems at Alibaba: Opportunities and Challenges. Proceedings of the Very Large Databases Endowment 12, 12 (2019), 2263–2272.

[25] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang, Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and Minlan Yu. 2019. HPCC: High Precision Congestion Control. In Proceedings of the 2019 Conference of the ACM Special Interest Group on Data Communication. ACM, 44–58.

[26] Yuanwei Lu, Guo Chen, Bojie Li, Kun Tan, Yongqiang Xiong, Peng Cheng, Jiansong Zhang, Enhong Chen, and Thomas Moscibroda. 2018. Multi-Path Transport for RDMA in Datacenters. In Proceedings of Fifteenth USENIX Symposium on Networked Systems Design and Implementation. 357–371.

[27] Yuanwei Lu, Guo Chen, Zhenyuan Ruan, Wencong Xiao, Bojie Li, Jiansong Zhang, Yongqiang Xiong, Peng Cheng, and Enhong Chen. 2017. Memory Efficient Loss Recovery for Hardware-based Transport in Datacenter. In Proceedings of the First Asia-Pacific Workshop on Networking. 22–28.

[28] Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld, Sean Bauer, Carlo Contavalli, Michael Dalton, Nandita Dukkipati, William C Evans, Steve Gribble, Nicholas Kidd, Roman Kononov, Gautam Kumar, Carl Mauer, Emily Musick, Lena Olson, Erik Rubow, Michael Ryan, Kevin Springborn, Paul Turner, Valas Valancius, Xi Wang, and Amin Vahdat. 2019. SNAP: A Microkernel Approach to Host Networking. In Proceedings of the Twenty-Seventh ACM Symposium on Operating Systems Principles. 399–413.

[29] Christopher Mitchell, Yifeng Geng, and Jinyang Li. 2013. Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store. In Proceedings of 2013 USENIX Annual Technical Conference. 103–114.

[30] Radhika Mittal, Terry Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel, Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats. 2015. TIMELY: RTT-based Congestion Control for the Datacenter. In Proceedings of the 2015 Conference of the ACM Special Interest Group on Data Communication. 537–550.

[31] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi, Arvind Krishnamurthy, Sylvia Ratnasamy, and Scott Shenker. 2018. Revisiting Network Support for RDMA. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. 313–326.

[32] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018. Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. Association for Computing Machinery, 221–235.

[33] Stanko Novakovic, Yizhou Shan, Aasheesh Kolli, Michael Cui, Yiying Zhang, Haggai Eran, Boris Pismenny, Liran Liss, Michael Wei, Dan Tsafrir, and Marcos Aguilera. 2019. Storm: A Fast Transactional Dataplane for Remote Data Structures. In Proceedings of the Twelfth ACM International Conference on Systems and Storage. 97–108.

[34] Renato Recio, Bernard Metzler, Paul Culley, Jeff Hilland, and Dave Garcia. 2007. A Remote Direct Memory Access Protocol Specification. RFC 5040.

[35] Steven L Scott. 1996. Synchronization and Communication in the T3E multiprocessor. In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems. 26–36.

[36] Kai-Yeung Siu and Raj Jain. 1995. A Brief Overview of ATM: Protocol Layers, LAN Emulation, and Traffic Management. ACM Special Interest Group on Data Communication Computer Communication Review 25, 2 (1995), 6–20.

[37] Swaminathan Sivasubramanian. 2012. Amazon DynamoDB: A Seamlessly Scalable Non-Relational Database Service. In Proceedings of the 2012 Conference of the ACM Special Interest Group on Management of Data. 729–730.

[38] Brent Stephens, Alan L Cox, Ankit Singla, John Carter, Colin Dixon, and Wesley Felter. 2014. Practical DCB for improved data center networks. In IEEE INFOCOM 2014-IEEE Conference on Computer Communications. IEEE, 1824–1832.

[39] Yiwen Zhang, Juncheng Gu, Youngmoon Lee, Mosharaf Chowdhury, and Kang G Shin. 2017. Performance Isolation Anomalies in RDMA. In Proceedings of the Workshop on Kernel-Bypass Networks. 43–48.

[40] Yiwen Zhang, Yue Tan, Brent Stephens, and Mosharaf Chowdhury. 2019. RDMA Performance Isolation With Justitia. arXiv preprint arXiv:1905.04437 (2019).

[41] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn, Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and Ming Zhang. 2015. Congestion Control for Large-Scale RDMA Deployments. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication. 523–536.

### Appendices

#### A. Initialization Vectors for AES-GCM

AES-GCM requires a non-repeating Initialization Vector (IV) as an additional input for cryptographic operations, as AES-GCM is vulnerable to attacks if the combination of encryption key and IV repeats for different payloads. IVs are typically derived from connection sequence numbers, which 1RMA lacks. However, unique IVs can be provided by maintaining a counter per 1RMA NIC, tracking all 1RMA protocol messages ever exchanged. Therefore, the combination of \( K_d + \text{Counter} + \text{SenderAddress} \) never repeats. We use \(\text{Counter} + \text{SenderAddress}\) to seed the IV in request packets, ensuring uniqueness for requests. Unlike sequence numbers, IVs need not be contiguous; only uniqueness is required.

When generating responses, serving 1RMA NICs also increment and include their own Counter value and RMA offset to further salt the IV, with the previous IV curried along as Additional Authenticated Data (AAD), which ties all protocol messages together in sequence. The combination of currying and server-supplied re-seeding ensures that mutations and other four-hop transactions are not vulnerable to replay attacks.

IVs and the various forms of AAD are memoized in command slot metadata, not readable by software, such that they remain in use for the duration of the command and are then discarded.

#### B. 1RMA Command Issue

Command slots correspond to a range of the 1RMA NIC’s PCIe Base Address Register (BAR), which is mapped into application virtual memory. Using memory-mapped registers in this way simplifies and optimizes the hardware. To access these registers, applications use Memory-Mapped I/O (MMIO) writes from the CPU to store commands directly into assigned slots, rather than relying on an on-NIC DMA-based command fetch mechanism. Conventional wisdom suggests that CPU-initiated writes across PCIe should be used sparingly due to performance side effects. While true for non-write combining doorbells, we specifically architected 1RMA to leverage write-combining MMIO stores, which have significantly improved performance on modern CPUs over traditional doorbell writes. Such an operation is possible because 1RMA does not guarantee ordering between operations and because we constrain software to issue commands using only carefully curated primitives (in our most performant library, four 16-byte SSE2 stores, in sequence). Our implementation achieves up to 87M commands/sec using eight Skylake CPU cores.

MMIO-based command issue also offers a latency benefit: commands are never fetched from host memory. Such fetches would incur a PCIe round-trip (hundreds of nanoseconds) on the critical path, which is a non-trivial latency adder [20].

---

This optimized version aims to make the text more coherent, clear, and professional.