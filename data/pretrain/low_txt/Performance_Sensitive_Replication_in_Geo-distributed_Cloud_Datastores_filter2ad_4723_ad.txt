### Figure 6: Comparing the Performance of BA Scheme with Cassandra’s Default Random Partitioner

Figure 6 compares the read and write latency observed with our Basic Availability (BA) scheme and Cassandra’s default random partitioner. The Y-axis shows the cumulative distribution function (CDF) of the fraction of all requests (approximately 6 million each for BA and Random), while the X-axis shows the observed per-request latency in milliseconds. To ensure a fair comparison, the observed latency values for BA include directory lookup latency.

From the figure, we observe that our flexible replication scheme outperforms the default replication scheme by 50 milliseconds at the 50th percentile and by 100 milliseconds at the 90th percentile. This represents a factor of 3 and 2 improvement, respectively. Notably, the Random partitioner performs marginally better than BA (by approximately 3-8 milliseconds) at the initial percentiles due to the additional latency overhead incurred for directory lookups.

### Figure 7: Boxplot Showing the Distribution of Read Latency with BA and N-1C Models

Figure 7 presents boxplots showing the distribution of read latency for the BA and N-1C models over every half-hour period. The whiskers represent the 10th and 90th percentiles.

#### (a) Basic Availability
For the English Wikipedia articles, the BA scheme places two replicas in the US West Coast (USW-1a and USW-2a) and the third replica in Tokyo (APN-1a) with R = 2 and W = 2. This configuration is reasonable as nodes in the US West are equidistant from Asia, Australia, Europe, and the US East, and placing the third replica in Asia reduces the 90th percentile latency under normal operation. Figure 7(a) shows the performance of the BA scheme under the failure of different data centers (DCs). When the West Coast DCs fail, the 90th percentile latency increases significantly from 200 milliseconds to 280 milliseconds (a 40% increase). In contrast, the failure of the Tokyo DC (APN-1a) has only a marginal impact on performance.

#### (b) N-1 Contingency
The N-1C scheme explicitly optimizes for latency under failure and places the third replica in USW-1a instead of Tokyo. Figure 7(b) shows the performance of the N-1C scheme under failures of different DCs. During normal operation, the N-1C scheme performs similarly to the BA scheme (median of 90 milliseconds and 90th percentile of 200 milliseconds). However, unlike the BA configuration, the 90th percentile latency remains largely unaffected under all failures. These results highlight the need to explicitly optimize for performance under failure conditions and demonstrate the benefits of the N-1C scheme over the BA scheme. The median and 90th percentile latencies from our experiments closely match our model predictions under both normal and failure conditions, validating our models.

### IX. Large-Scale Evaluation

We adopt a trace-driven simulation approach for our large-scale evaluation using three application traces. The datastore cluster comprises nodes from 27 distinct DCs worldwide, with locations obtained from AWS Global Infrastructure. Inter-DC delays were measured between Planet-lab nodes close to each DC, and median delays were considered. Users were mapped to the closest DCs, similar to our EC2 experiments. We chose this extended set of DCs because EC2 regions are limited in number, and AWS Global Infrastructure provides multiple DCs in areas without EC2 coverage.

#### A. Performance of Our Optimal Schemes

Figure 8(a) shows the CDF of the observed read latency across both schemes for all keys in the Twitter and Wikipedia traces under normal and failure conditions. For each key, we plot the read latency under normal conditions and when the most critical replica (the one whose failure results in the worst latency) fails. The BA scheme's read latency deteriorates drastically under failure for almost all keys in both applications. For instance, more than 40% of the keys in Twitter observed an increase of 50+ milliseconds (more than 20% of the keys in Wikipedia observed an increase of 100+ milliseconds). In contrast, the N-1C scheme observed only a marginal variation under failure (most keys in Twitter observed less than 30 milliseconds increase in latency on its replica failures). Surprisingly, the N-1C scheme incurs an almost negligible penalty in its latency under normal conditions despite optimizing the replica configuration explicitly for failure. Overall, our results clearly show the benefit of explicitly optimizing replication for failure conditions.

#### B. Need for Heterogeneous Configuration Policy

This section highlights the importance of allowing heterogeneous replica configurations in datastores. We analyzed the configurations generated by N-1C for all keys in the Twitter trace and found 1,985 distinct configurations (combinations of replica location, N, R, W) used in the optimal solutions. The benefits are not only due to optimizing the location of replicas but also due to careful configuration of the replication parameters. To isolate such cases, we consider a variant of our N-1C model called 3-2-2, which has fixed replication parameters N = 3, R = 2, and W = 2 but allows flexibility in the location of the replicas. Figure 8(b) shows the difference in access latency between the 3-2-2 and N-1C schemes for Twitter. The X-axis has the various replication factors observed in the optimal solutions, and each corresponding box plot shows the 25th, median, and 75th percentiles (whiskers showing the 90th percentile) of the difference in access latency between the two schemes. Our results clearly show that a uniform configuration policy for all data in the application can be sub-optimal, and allowing heterogeneity in replica configuration can greatly lower the latency (as much as 70 milliseconds in some cases).

#### C. History-Based vs. Optimal

So far, we have assumed that the workloads for the applications are known. In practice, this may need to be obtained from historical data. Figure 9(a) shows the CDF comparing the performance of Wikipedia during the first quarter of 2012 when using history-based and optimal replication configurations. The curves labeled "history-based" correspond to the read and write latency observed when using the replica configuration predicted from the fourth quarter of 2011. The curves labeled "optimal" correspond to the read and write latency observed when using the optimal replica configuration for the first quarter of 2012. Figures 9(b) and 9(c) show similar graphs for Twitter and Gowalla. These figures show that history-based configuration performs close to optimal for Wikipedia and Twitter, while showing some deviation from optimal performance for Gowalla. This is because users in Gowalla often move across geographical regions, resulting in abrupt workload shifts. For such abrupt shifts, explicit hints from the user or automatic detection of workload changes and re-running the optimization are potential approaches for improving performance.

#### D. Robustness to Delay Variations

Our experiments on EC2 show that our strategies are fairly robust to natural delay variations across cloud DCs. We extend our analysis over a larger set of keys by computing about 1,800 time snapshots of the entire 27*27 inter-DC delays for our extended DC set. All delay values in the snapshot were measured approximately at the same time. We then computed the optimal replica configurations (using our BA and N-1C schemes) for 500 random keys from the Twitter trace for each of the 1,800 snapshots. We call these the SNAP configurations. Similarly, replica configurations are computed using the median delay values of the 1,800 snapshots, which we call the MED configurations. We compare the performance of the MED configuration using delays observed at each snapshot with the performance of the optimal SNAP configuration at the same snapshot. Figure 10 shows the CDF of the difference in access latency between the MED and SNAP configurations. Each curve in the figure corresponds to a range of latencies observed using the SNAP configurations. For SNAP latencies less than 100 milliseconds, and for over 90% of snapshots, MED only incurs less than 5 milliseconds of additional latency. Also, for almost 80% of all the SNAPs, the corresponding MED configuration was optimal. While the penalty is higher for SNAP latencies over 100 milliseconds, it is still acceptable (less than 15 milliseconds for 90% of the cases) given the relatively higher SNAP latencies. Overall, the results further confirm our EC2 results and show that delay variation impacts placement modestly.

#### E. Asymmetric Read and Write Thresholds

Thus far, we have assumed that read and write latencies are equally desirable to optimize. In practice, some applications may prioritize read latencies, and others might prioritize writes. We explored solutions generated by our approach when our models are modified to explicitly constrain the read and write thresholds. For Twitter, we found that a bound of 100 milliseconds on the write latency has no noticeable impact on the overall performance.