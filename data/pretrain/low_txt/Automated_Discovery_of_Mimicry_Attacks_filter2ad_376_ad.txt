### 7. Experiments

We utilized our implementation to identify previously undetected attacks in program models that have been discussed in academic literature. Our automated approach can detect mimicry and evasion attacks that were previously discovered through manual analysis [24, 22, 20, 21]. This automation allows for a more scalable generation of test cases compared to manual methods.

#### Automated Detection of Mimicry and Evasion Attacks

Our automated techniques can find mimicry and evasion attacks that previous research identified only through manual analysis. Previous studies focused on four test programs—wu-ftpd, restore, traceroute, and passwd—that had known vulnerabilities enabling attackers to execute a root shell. Forrest et al. [8] successfully detected known attack instances using the Stide model, which is a context-insensitive characterization of execution learned from system call traces generated during training runs. Wagner and Soto [24] and Tan et al. [22, 20, 21] demonstrated that attackers could modify their attacks to evade detection by the Stide model. In some cases, the undetected attacks, while not semantically equivalent to the original root shell exploit, still adversely modified the system state, allowing the attacker to subsequently gain root access. For example, successful attack variants might:
- Write a new root-level account to the user accounts file /etc/passwd.
- Set /etc/passwd as world-writable, so an ordinary user can add a new root account.
- Set /etc/passwd to be owned by the attacker, allowing them to add a new root account.

We automatically found these undetected attacks by analyzing the Stide model for each of the four programs with respect to each of the four attack goals. For wu-ftpd, we constructed the Stide model using the original Linux training data from Forrest et al. [7]. We were unable to obtain the wu-ftpd training data used by Wagner and Soto or the Stide models they constructed. Consequently, we found attacks in the wu-ftpd model constructed from Forrest’s data that were reportedly not present in the model constructed from Wagner’s data. For the remaining three test programs, we constructed models from training data generated as described by Tan et al. [20]. Our specification compiler combined PDA representations of the Stide models with specifications of Linux system calls to produce pushdown systems suitable for model checking.

#### Model Analysis and Results

Table 1 lists the size of the PDA representation of the Stide model for each program. The OS state model included 119 bits of global state and 50 bits of temporary state for system call argument variables. This temporary state reduces Moped’s resource demands because it exists only briefly during the model checker’s execution.

Table 2 presents the ability of the Stide model to detect any attack designed to reach a particular attack goal, as determined by Moped. A "yes" indicates that the model will always prevent any attacker from reaching their goal, regardless of how they transform or alter their attack sequence. A "no" indicates that the model checker was able to find a system call sequence, with arguments, accepted by the model but that induces an unsafe operating system condition. Figure 6 shows an undetected attack against the Stide model of traceroute discovered by our system. We automatically found all attacks that researchers previously found manually, one additional attack due to differences between Forrest’s and Wagner’s training data for wu-ftpd, and an additional attack against restore not found by previous manual research.

Previous work missed this attack because manual inspection does not scale to many programs and attacks, making it difficult to compute results for all attack goals in all programs. Manual inspection also struggles to prove that an attack is not possible. Model checking, however, can prove that a goal is unreachable regardless of the actual system calls used by the attacker.

#### Performance and Scalability

Table 3 lists the model checker’s running times in seconds for each model and attack goal. When comparing the running times with Table 2, a trend becomes apparent: when the model checker finds an attack, the running times are very small. When no attack is found, the model checker executes for a longer period. This disparity reflects the behavior of the underlying model checking algorithms. When a model checker finds a counter-example, it can immediately terminate. However, a successful proof requires exhaustive exploration of all execution paths, preventing early termination.

Automating the previously manual process of attack construction is a significant achievement. Attackers have significant freedom in program models that do not constrain system call arguments. For example, the sequence open followed by write without argument constraints can be misused to alter the system’s password file. Our automated system provides a means to understand exactly where a program model fails, as shown in Table 2, which indicates which classes of attack can be effectively detected and which require alternative protection strategies.

### 8. Conclusions

Model-based intrusion detection systems are useful only if they actually detect or prevent attacks. Finding undetected attacks manually is difficult, error-prone, and unable to scale to large numbers of program models and attacks. By formalizing the effects of attacks on the operating system, we provide the operational means to find undetected attacks automatically. A model checker attempts to prove that the attack effect will never hold in the program model. By finding counter-examples that cause the proof to fail, we identify undetected attacks: system call sequences and arguments that are accepted as valid execution and induce malicious effects on the operating system. This automation allows us to find undetected attacks against program models that were previously found only through manual inspection. The efficiency of the computation, typically around 2 seconds to find undetected attacks, suggests that this automated approach can easily scale to large collections of program models.

#### Acknowledgments

We thank the anonymous reviewers and the members of the WiSA project at Wisconsin for their helpful comments. This work was supported in part by the Office of Naval Research grant N00014-01-1-0708, NSF grant CCR-0133629, and Department of Energy grant DE-FG02-93ER25176. Jonathon T. Gifﬁn was partially supported by a Cisco Systems Distinguished Graduate Fellowship. Somesh Jha was partially supported by NSF Career grant CNS-0448476. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes, notwithstanding any copyright notices. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements of the above government agencies or the U.S. Government.

#### References

[1] T. Ball and S. K. Rajamani. Bebop: A symbolic model checker for boolean programs. In 7th International SPIN Workshop on Model Checking of Software, Stanford, California, Aug./Sep. 2000.

[2] F. Besson, T. Jensen, D. L. M´etayer, and T. Thorn. Model checking security properties of control-flow graphs. Journal of Computer Security, 9:217–250, 2001.

[3] H. Chen and D. Wagner. MOPS: An infrastructure for examining security properties of software. In 9th ACM Conference on Computer and Communications Security (CCS), Washington, DC, Nov. 2002.

[4] E. M. Clarke, O. Grumberg, and D. A. Peled. Model Checking. The MIT Press, 2000.

[5] J. Esparza, D. Hansel, P. Rossmanith, and S. Schwoon. Efficient algorithms for model checking pushdown systems. In Computer Aided Verification (CAV), Chicago, Illinois, July 2000.

[6] H. H. Feng, J. T. Gifﬁn, Y. Huang, S. Jha, W. Lee, and B. P. Miller. Formalizing sensitivity in static analysis for intrusion detection. In IEEE Symposium on Security and Privacy, Oakland, California, May 2004.

[7] S. Forrest. Data sets—synthetic FTP. http://www.cs.unm.edu/∼immsec/data/FTP/UNM/normal/synth/, 1998.

[8] S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff. A sense of self for UNIX processes. In IEEE Symposium on Security and Privacy, Oakland, California, May 1996.

[9] D. Gao, M. K. Reiter, and D. Song. On gray-box program tracking for anomaly detection. In USENIX Security Symposium, San Diego, California, Aug. 2004.

[10] J. T. Gifﬁn, D. Dagon, S. Jha, W. Lee, and B. P. Miller. Environment-sensitive intrusion detection. In 8th International Symposium on Recent Advances in Intrusion Detection (RAID), Seattle, Washington, Sept. 2005.

[11] J. T. Gifﬁn, S. Jha, and B. P. Miller. Detecting manipulated remote call streams. In 11th USENIX Security Symposium, San Francisco, California, Aug. 2002.

[12] R. Gopalakrishna, E. H. Spafford, and J. Vitek. Efficient intrusion detection using automaton inlining. In IEEE Symposium on Security and Privacy, Oakland, California, May 2005.

[13] J. D. Guttman, A. L. Herzog, J. D. Ramsdell, and C. W. Skorupka. Verifying information flow goals in Security-Enhanced Linux. Journal of Computer Security, 13:115–134, 2005.

[14] L.-c. Lam and T.-c. Chiueh. Automatic extraction of accurate application-specific sandboxing policy. In Recent Advances in Intrusion Detection (RAID), Sophia Antipolis, French Riviera, France, Sept. 2004.

[15] C. R. Ramakrishnan and R. Sekar. Model-based vulnerability analysis of computer systems. In 2nd International Workshop on Verification, Model Checking and Abstract Interpretation, Pisa, Italy, Sept. 1998.

[16] F. B. Schneider. Enforceable security policies. ACM Transactions on Information and System Security, 3(1):30–50, Feb. 2000.

[17] S. Schwoon. Model-Checking Pushdown Systems. Ph.D. dissertation, Technische Universität München, June 2002.

[18] S. Schwoon. Moped—a model-checker for pushdown systems. http://www.fmi.uni-stuttgart.de/szs/tools/moped/, 2006.

[19] R. Sekar, M. Bendre, P. Bollineni, and D. Dhurjati. A fast automaton-based method for detecting anomalous program behaviors. In IEEE Symposium on Security and Privacy, Oakland, California, May 2001.

[20] K. Tan, K. S. Killourhy, and R. A. Maxion. Undermining an anomaly-based intrusion detection system using common exploits. In Recent Advances in Intrusion Detection (RAID), Zürich, Switzerland, Oct. 2002.

[21] K. Tan and R. A. Maxion. “Why 6?” Defining the operational limits of stide, an anomaly-based intrusion detector. In IEEE Symposium on Security and Privacy, Oakland, California, May 2002.

[22] K. Tan, J. McHugh, and K. Killourhy. Hiding intrusions: From the abnormal to the normal and beyond. In 5th International Workshop on Information Hiding, Noordwijkerhout, Netherlands, Oct. 2002.

[23] D. Wagner and D. Dean. Intrusion detection via static analysis. In IEEE Symposium on Security and Privacy, Oakland, California, May 2001.

[24] D. Wagner and P. Soto. Mimicry attacks on host-based intrusion detection systems. In 9th ACM Conference on Computer and Communications Security, Washington, DC, Nov. 2002.

[25] B. J. Walker, R. A. Kemmerer, and G. J. Popek. Specification and verification of the UCLA Unix security kernel. Communications of the ACM, 23(2), Feb. 1980.