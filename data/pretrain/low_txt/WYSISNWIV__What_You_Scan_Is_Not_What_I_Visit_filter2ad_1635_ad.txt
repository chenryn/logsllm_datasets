### Through the Appropriate Interface

After five days, we verify whether the link is stored in the corresponding scanner's database. This verification can be conducted for VirusTotal, Scumware, WebInspector, and Zscaler Zulu, as they provide a database query interface. Subsequently, we access these links through Chrome to check if they are blocked. Unfortunately, none of them were, indicating that the scanners do not share data.

### Broader Scanning and URL Collection

The most viable solution appears to be that when URLs contain characters or patterns that may be interpreted differently by the client, the scanner should check all possible variations. If such patterns are not commonly used by benign websites, the additional overhead on the scanner will be relatively small. Our results show that the gred URL scanner already does this for `%3F` (?) and `%23` (#). Another option is for scanners to use the URLs sent by the browser to web servers as-is for scanning. However, this option may violate user privacy, as the URL may contain private information and expose the sites the user visits.

### 7. Related Work

Identifying malicious websites before users visit them to block or take them down has been a popular area of research. Various techniques, including both dynamic and static analysis, are used to identify malicious content. While not exhaustive, we discuss some of the most prominent works here. It is important to note that the security problem highlighted in this paper does not relate to the techniques and methods used to detect malicious content, such as malware, exploits, and phishing sites. Instead, it concerns how users and security systems obtain and parse URLs. Security issues arise because an attacker can use a URL to hide malicious content from a security system, while the client (usually a browser) reaches the malicious content through the same URL. Some of the works described below involve URL classification in detecting malicious content. It is possible that these approaches could be extended to include heuristics that identify problematic URL patterns as potentially malicious, but the effectiveness of such measures also depends on how frequently such patterns are encountered on benign sites.

#### Cova et al. [20]
Cova et al. present JSand, a dynamic analysis system that visits websites using an instrumented browser, collecting runtime events as the browser executes the website. Anomaly detection methods are applied to features extracted from the events to classify websites and identify malicious ones. JSand is part of the Wepawet scanner, which we tested in this work, and utilizes Mozilla’s Rhino interpreter. This is likely why it processes backslashes in a Firefox-like manner.

#### Prophiler [18]
Prophiler later improves JSand by accelerating the process of scanning web pages by allowing benign pages to be quickly identified and filtered out. Features extracted from page content, the URL, and information about the host of the page are used to quickly identify benign pages.

#### EvilSeed [29]
EvilSeed follows the reverse direction and begins from known malicious websites, using them as seeds to search the web more efficiently for malicious content. This is accomplished by extracting terms that characterize the known malicious sites and using them to query search engines, thus obtaining results more likely to be malicious or compromised.

#### Google Researchers [41]
In 2007, Google researchers introduced a system for identifying all malicious pages on the web that attempt to exploit the browser and lead to drive-by downloads. Based on the fact that Google already crawls a significant portion of the web, the researchers began an effort to extract a subset of suspicious pages that can be more thoroughly scanned. Simple heuristics are used to greatly reduce the number of pages that need to be checked.

#### Provos et al. [40]
In a later paper, Provos et al. present results showing the prevalence of drive-by download attacks, using features such as out-of-place inline frames, obfuscated JavaScript, and links to known malware distribution sites to detect them. Their findings estimate that 1.3% of search queries made to Google returned at least one URL labeled as malicious.

#### Dynamic Analysis Techniques
Dynamic analysis techniques that scan the web to identify malicious pages frequently employ client honeypots. Moshchuk et al. [44] developed Strider HoneyMonkeys, a set of programs that launch browsers with different patch levels, concurrently accessing the same URL, to detect exploits. The approach is based on detecting the effects of a compromise, such as the creation of new files or alteration of configuration files.

#### Recent Works
Some recent works aim to improve the detection of malicious websites. JStill [47] performs function invocation-based analysis to extract features from JavaScript code to statically identify malicious, obfuscated code. Kapravelos et al. [30] also focused on detecting JavaScript that incorporates techniques to evade analysis. Another approach, Delta [16], relies on static analysis of the changes between two versions of the same website to detect malicious content.

#### URL-Based Detection
Some works have focused on aspects of the URL itself to detect malicious sites. ARROW [48] looks at the redirection chains formed by malware distribution networks during a drive-by download attack. Garera et al. [25] classify phishing URLs using features that include red-flag keywords in the URL, as well as features based on Google’s page rank algorithm. Statistical, lexical, and host-based features of URLs have been used in the past to identify malicious URLs with the help of machine learning [33, 34, 46]. Malicious URLs are frequently hidden by using JavaScript to dynamically generate them on-the-fly. Wang et al. [43] employ dynamic analysis to extract such hidden URLs.

#### Web Application Scanners
Besides the URL scanners mentioned in this paper, there are another type of scanner called Web Application Scanners. These scanners are fed with a URL or a set of URLs, retrieve the pages, follow the links, and identify all reachable pages in the application (under a specific domain). They analyze the pages with crafted inputs if necessary and determine if the site is vulnerable to web-specific vulnerabilities (e.g., Cross-Site Scripting, SQL injection, Code Injection, Broken Access Controls). Doupé et al. [21] present a thorough evaluation of eleven web application scanners by constructing a vulnerable website and feeding it to the scanners. Khoury et al. [31] evaluate three scanners against stored SQL injection. Bau et al. [15] analyze eight web application scanners and evaluate their effectiveness against vulnerabilities. For this kind of scanner, they are out of the scope of this paper. In our paper, we assume that the website is controlled by the attacker and the attacker can plant any malicious content into any link belonging to this site, while web application scanners target benign sites that may potentially be exploited. Web application scanners usually are not capable of detecting malicious content and phishing pages.

### 8. Conclusions

Developing a common URL parser framework or enforcing a standardization model can be a challenging task for both application and service vendors due to rapid changes in technology and variations among multiple web services. In this work, we experimentally test all major browsers and URL scanners, as well as various applications that parse URLs. We expose multiple discrepancies in how they actually parse URLs. These differences leave users vulnerable to malicious web content because the same URL leads the browser to one page, while the scanner follows the same URL to scan another page.

To the best of our knowledge, this is the first time browsers and URL scanners have been cross-evaluated in this way. The current work can serve as a reference for anyone interested in better understanding the facets of this fast-evolving area. It is also expected to foster research efforts towards the development of fully-fledged solutions that emphasize both the technological and standardization aspects.

### Acknowledgements

We would like to express our thanks to the anonymous reviewers for their valuable comments. We also acknowledge Paul Spicer's contribution, who initially investigated the problem.

### References

1. Uniform resource identifier (URI): Generic syntax, January 2005. https://www.ietf.org/rfc/rfc3986.txt
2. Different behaviors of \ in the URL by FireFox and Chrome. StackOverflow, May 2012. http://stackoverflow.com/questions/10438008/different-behaviors-of-treating-backslash-in-the-url-by-firefox-and-chrome
3. gred, March 2015. http://check.gred.jp/
4. Online link scan - scan links for harmful threats! (2015). http://onlinelinkscan.com/
5. PhishTank — join the fight against phishing (2015). http://www.phishtank.com/
6. scumware.org - just another free alternative for security and malware researchers (2015). http://www.scumware.org/
7. StopBadware — a non-profit organization that makes the web safer through the prevention, mitigation, and remediation of badware websites, May 2015. https://www.stopbadware.org/
8. Sucuri SiteCheck - free website malware scanner, March 2015. https://sitecheck.sucuri.net/
9. urlquery.net - free URL scanner, March 2015. http://urlquery.net/
10. VirusTotal - free online virus, malware, and URL scanner (2015). https://www.virustotal.com/en/
11. Web Inspector - inspect, detect, protect (2015). http://app.webinspector.com/
12. Website/URL/link scanner safety check for phishing, malware, viruses - scanurl.net, March 2015. http://scanurl.net/
13. Zscaler Zulu URL Risk Analyzer - Zulu, March 2015. http://zulu.zscaler.com/
14. Akhawe, D., Felt, A.P.: Alice in warningland: a large-scale field study of browser security warning effectiveness. In: Proceedings of the 22nd USENIX Security Symposium, pp. 257–272 (2013)
15. Bau, J., Bursztein, E., Gupta, D., Mitchell, J.: State of the art: automated black-box web application vulnerability testing. In: 2010 IEEE Symposium on Security and Privacy (SP), pp. 332–345, May 2010
16. Borgolte, K., Kruegel, C., Vigna, G.: Delta: automatic identification of unknown web-based infection campaigns. In: Proceedings of the ACM SIGSAC Conference on Computer and Communications Security (CCS), pp. 109–120 (2013)
17. Burns, J.: Cross-site request forgery: an introduction to a common web application weakness. White paper, Information Security Partners, LLC (2007)
18. Canali, D., Cova, M., Vigna, G., Kruegel, C.: Prophiler: a fast filter for the large-scale detection of malicious web pages. In: Proceedings of the International Conference on World Wide Web (WWW), pp. 197–206 (2011)
19. Cass, S.: The 2015 top ten programming languages. http://spectrum.ieee.org/computing/software/the-2015-top-ten-programming-languages
20. Cova, M., Kruegel, C., Vigna, G.: Detection and analysis of drive-by-download attacks and malicious JavaScript code. In: Proceedings of the International Conference on World Wide Web (WWW), pp. 281–290 (2010)
21. Doupé, A., Cova, M., Vigna, G.: Why Johnny can’t pentest: an analysis of black-box web vulnerability scanners. In: Kreibich, C., Jahnke, M. (eds.) DIMVA 2010. LNCS, vol. 6201, pp. 111–131. Springer, Heidelberg (2010)
22. Egele, M., Wurzinger, P., Kruegel, C., Kirda, E.: Defending browsers against drive-by downloads: mitigating heap-spraying code injection attacks. In: Flegel, U., Bruschi, D. (eds.) DIMVA 2009. LNCS, vol. 5587, pp. 88–106. Springer, Heidelberg (2009)
23. Egelman, S., Cranor, L.F., Hong, J.: You’ve been warned: an empirical study of the effectiveness of web browser phishing warnings. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI), pp. 1065–1074 (2008)
24. FireEye: Email security - detect and block spear phishing and other email-based attacks, May 2015. https://www.fireeye.com/products/ex-email-security-products.html
25. Garera, S., Provos, N., Chew, M., Rubin, A.D.: A framework for detection and measurement of phishing attacks. In: Proceedings of the 2007 ACM Workshop on Recurring Malcode (WORM), pp. 1–8 (2007)
26. Google: Safe Browsing API - Google Developers (2015). https://developers.google.com/safe-browsing/
27. Ikinci, A., Holz, T., Freiling, F.: Monkey-spider: detecting malicious websites with low-interaction honeyclients. In: Proceedings of Sicherheit, Schutz und Zuverlässigkeit (2008)
28. Imperial-Legrand, A.: Vulnerability writeups. Google+, March 2014. https://plus.google.com/+AlexisImperialLegrandGoogle/posts/EQXTzsBVS7L
29. Invernizzi, L., Benvenuti, S., Cova, M., Comparetti, P.M., Kruegel, C., Vigna, G.: EvilSeed: a guided approach to finding malicious web pages. In: Proceedings of the 2012 IEEE Symposium on Security and Privacy, pp. 428–442 (2012)
30. Kapravelos, A., Shoshitaishvili, Y., Cova, M., Kruegel, C., Vigna, G.: Revolver: an automated approach to the detection of evasive web-based malware. In: Proceedings of the USENIX Security Symposium, pp. 637–652 (2013)
31. Khoury, N., Zavarsky, P., Lindskog, D., Ruhl, R.: An analysis of black-box web application security scanners against stored SQL injection. In: 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third International Conference on Social Computing (SocialCom), pp. 1095–1101, October 2011
32. Kirda, E.: Cross-site scripting attacks. In: van Tilborg, H., Jajodia, S. (eds.) Encyclopedia of Cryptography and Security, pp. 275–277. Springer, US (2011)
33. Ma, J., Saul, L.K., Savage, S., Voelker, G.M.: Beyond blacklists: learning to detect malicious websites from suspicious URLs. In: Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pp. 1245–1254 (2009)
34. Ma, J., Saul, L.K., Savage, S., Voelker, G.M.: Identifying suspicious URLs: an application of large-scale online learning. In: Proceedings of the International Conference on Machine Learning (ICML), pp. 681–688 (2009)
35. Microsoft: Microsoft Security Intelligence Report, Volume 13. Technical report, Microsoft Corporation (2012)
36. Microsoft: SmartScreen Filter (2015). http://windows.microsoft.com/en-us/internet-explorer/products/ie-9/features/smartscreen-filter
37. Moshchuk, A., Bragin, T., Deville, D., Gribble, S.D., Levy, H.M.: SpyProxy: execution-based detection of malicious web content. In: Proceedings of 16th USENIX Security Symposium on USENIX Security Symposium, SS 2007, pp. 3:1–3:16, USENIX Association, Berkeley, CA, USA (2007). http://dl.acm.org/citation.cfm?id=1362903.1362906
38. Proofpoint: Targeted Attack Protection, May 2015. https://www.proofpoint.com/us/solutions/products/targeted-attack-protection
39. Protalinski, E.: These 8 characters crash Skype, and once they’re in your chat history, the app can’t start (update: fixed). VentureBeat, May 2012. http://venturebeat.com/2015/06/02/these-8-characters-crash-skype-and-once-theyre-in-your-chat-history-the-app-cant-start/
40. Provos, N., Mavrommatis, P., Rajab, M.A., Monrose, F.: All your iFRAMEs point to us. In: Proceedings of the USENIX Security Symposium, pp. 1–15 (2008)
41. Provos, N., McNamee, D., Mavrommatis, P., Wang, K., Modadugu, N.: The ghost in the browser: analysis of web-based malware. In: Proceedings of the Workshop on Hot Topics in Understanding Botnets (HOTBOTS) (2007)
42. Symantec: Symantec Web Security.cloud (2015). http://www.symantec.com/web-security-cloud/
43. Wang, Q., Zhou, J., Chen, Y., Zhang, Y., Zhao, J.: Extracting URLs from JavaScript via program analysis. In: Proceedings of the Joint Meeting on Foundations of Software Engineering (FSE), pp. 627–630 (2013)
44. Wang, Y.M., Beck, D., Jiang, X., Verbowski, C., Chen, S., King, S.: Automated web patrol with Strider HoneyMonkeys: finding websites that exploit browser vulnerabilities. In: Proceedings of NDSS, February 2006
45. WHATWG: URL Living Standard, May 2015. https://url.spec.whatwg.org/
46. Whittaker, C., Ryner, B., Nazif, M.: Large-scale automatic classification of phishing pages. In: Proceedings of NDSS, February 2010
47. Xu, W., Zhang, F., Zhu, S.: JStill: mostly static detection of obfuscated malicious JavaScript code. In: Proceedings of the ACM Conference on Data and Application Security and Privacy (CODASPY), pp. 117–128 (2013)
48. Zhang, J., Seifert, C., Stokes, J.W., Lee, W.: ARROW: generating signatures to detect drive-by downloads. In: Proceedings of the International Conference on World Wide Web (WWW), pp. 187–196 (2011)