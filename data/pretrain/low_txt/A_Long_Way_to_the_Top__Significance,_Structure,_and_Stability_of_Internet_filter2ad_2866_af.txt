### Analysis of Top Lists for Web-Centric Studies

The Alexa list provides a robust selection of functional and frequently visited websites, making it a suitable choice for human web-centered studies. In contrast, the Majestic list, through its link-counting method, includes "hidden" links and may capture domains that are often loaded but not necessarily requested by users. To obtain a comprehensive overview of the Internet, we recommend scanning a large sample, such as the "general population" used in §8, which includes all com/net/org domains.

#### Considerations for Stability
Top lists can change up to 50% per day, which means that insights from one day may not generalize to the next. For most measurement studies, stability should be increased by conducting repeated, longitudinal measurements. This approach also helps to avoid biases that might arise from using lists generated on different days of the week.

#### Documentation of List and Measurement Details
Studies should clearly document the specific list used (e.g., Alexa Global Top 1M), its download date, and the date of the measurements to ensure basic replicability. Ideally, the list used should be shared in the paper's dataset.

### Desired Properties for Top Lists

Based on the challenges discussed, we derive several properties that top lists should offer:

1. **Consistency**: The structure and stability of top lists should remain static over time. Any necessary changes due to the evolving nature of the Internet should be announced and documented.
2. **Transparency**: Top list providers should be transparent about their ranking processes and biases to help researchers understand and potentially control these biases. This transparency may conflict with the business interests of commercial list providers.
3. **Stability**: List stability faces a difficult trade-off. While capturing the ever-evolving trends in the Internet requires recent data, many typical top list uses are not stable with daily changes of up to 50%. We suggest offering lists in both long-term (e.g., a 90-day sliding window) and short-term (e.g., only the most recent data) versions.

### HTTP/2 Adoption Analysis

We attempted to fetch the landing pages of domains via HTTP/2 using the nghttp2 library. We prefixed all domains in the Alexa and Majestic lists with "www". If an HTTP/2 connection was successfully established, we issued a GET request for the domain's root page. We followed up to 10 redirects and counted the domain as HTTP/2-enabled if the actual data for the landing page was transferred via HTTP/2. We probed top lists daily and the larger zone file weekly.

**Results:**
- The average HTTP/2 adoption rate for all com/net/org domains is 7.84%, significantly lower than for domains listed in the Top 1M lists (up to 26.6% for Alexa) and even more so for the Top 1k lists, which show adoption around 35% or higher.
- Popular domains are more likely to be hosted on progressive infrastructures (e.g., CDNs) compared to the general population, explaining the higher adoption rates.

**Adoption Differences by List and Weekday:**
- As expected, HTTP/2 adoption varies by list and by weekday for those lists with a weekday pattern (cf. §6.2).
- The results for the Top 1k lists differ significantly from the general population, highlighting the importance of considering list type and generation date.

### Takeaways

- **Extreme Measurement Results**: Top lists generally show more extreme measurement results, such as protocol adoption. This effect is particularly pronounced for the Top 1k domains, with differences of up to two orders of magnitude.
- **Weekly Patterns**: Results can be affected by a weekly pattern, meaning that the percentage of protocol adoption may vary depending on whether the list was generated on a weekday or a weekend. This is a significant limitation to consider when using top lists for measurement studies.

### Discussion

We have shown that top lists are frequently used in scientific studies. While they provide a set of relevant domains at a small and stable size that can be compared over time, they also come with certain disadvantages, which we have explored in this paper.

- **Bias and Representation**: Top lists are biased towards the list's specific measure of popularity and do not represent the general state of the Internet well.
- **Ethical Considerations**: We aim to minimize harm to all stakeholders. For active scans, we follow best practices, such as maintaining a blacklist, using dedicated servers with meaningful rDNS records, and providing abuse contacts. We assess potential harm and follow the beneficence principle.

### Related Work

Our work is related to three fields:
1. **Sound Internet Measurements**: There is a body of work with guidelines on sound Internet measurements, but these do not specifically address the issue of top lists.
2. **Measuring Web Popularity**: Understanding web popularity is important for marketing and business performance analyses. Croll and Power [138] warn site owners about potential instrumentation biases in Alexa ranks, especially for low-traffic sites.
3. **Limitations of Using Top Lists in Research**: Despite the widespread use of top lists in research papers, there is limited systematic analysis of their content. Some studies have mentioned the limitations of relying on these ranks for specific research efforts [45, 67].

### Conclusion

To our knowledge, this is the first comprehensive study of the structure, stability, and significance of popular Internet top lists. We have shown that the use of top lists is significant among networking papers and found distinctive structural characteristics per list. List stability has revealed interesting highlights, such as up to 50% churn per day for some lists. We have closely investigated ranking mechanisms and manipulated a test domain’s Umbrella rank in a controlled experiment. Systematic measurement of top list domain characteristics and reproduction of studies have revealed that top lists generally distort results from the general population, and results can depend on the day of the week. We conclude with a discussion on desirable properties of top lists and recommendations for their use in science.

### Acknowledgements

We thank the scientific community for engaging discussions and data sharing, specifically Johanna Amann, Mark Allman, Matthias Wählisch, Ralph Holz, Georg Carle, Victor le Pochat, and the PAM’18 poster session participants. We also thank the anonymous reviewers of the IMC’18 main and shadow PCs for their comments, and Zakir Durumeric for shepherding this work. This work was partially funded by the German Federal Ministry of Education and Research under project X-Check (grant 16KIS0530), by the DFG as part of the CRC 1053 MAKI, and the US National Science Foundation under grant number CNS-1564329.

### References

[1] Alexa. Top 1M sites. https://www.alexa.com/topsites, May 24, 2018. http://s3.dualstack.us-east-1.amazonaws.com/alexa-static/top-1m.csv.zip.
[2] Cisco. Umbrella Top 1M List. https://umbrella.cisco.com/blog/blog/2016/12/14/cisco-umbrella-1-million/.
[3] Majestic. https://majestic.com/reports/majestic-million/, May 17, 2018.
[4] Matthew Woodward. Ahrefs vs Majestic SEO – 1 Million Reasons Why Ahrefs Is Better. https://www.matthewwoodward.co.uk/experiments/ahrefs-majestic-seo-1-million-domain-showdown/, May 23, 2018.
[5] Alexa. The Alexa Extension. https://web.archive.org/web/20160604100555/http://www.alexa.com/toolbar, June 04, 2016.
[6] Alexa. Alexa Increases its Global Traffic Panel. https://blog.alexa.com/alexa-panel-increase/, May 17, 2018.
[7] Alexa. Top 6 Myths about the Alexa Traffic Rank. https://blog.alexa.com/top-6-myths-about-the-alexa-traffic-rank/, May 22, 2018.
[8] Alexa. What’s going on with my Alexa Rank? https://support.alexa.com/hc/en-us/articles/200449614, May 17, 2018.
[9] Majestic. Majestic Million CSV now free for all, daily. https://blog.majestic.com/development/majestic-million-csv-daily/, May 17, 2018.
[10] Quantcast. https://www.quantcast.com/top-sites/US/1.
[11] Statvoo. https://statvoo.com/top/sites, May 17, 2018.
[12] Google. Chrome User Experience Report. https://developers.google.com/web/tools/chrome-user-experience-report/, May 15, 2018.
[13] SimilarWeb Top Websites Ranking. https://www.similarweb.com/top-websites.
[14] Vasileios Giotsas, Philipp Richter, Georgios Smaragdakis, Anja Feldmann, Christoph Dietzel, and Arthur Berger. Inferring BGP Blackholing Activity in the Internet. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[15] Srikanth Sundaresan, Xiaohong Deng, Yun Feng, Danny Lee, and Amogh Dhamdhere. Challenges in Inferring Internet Congestion Using Throughput Measurements. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[16] Zhongjie Wang, Yue Cao, Zhiyun Qian, Chengyu Song, and Srikanth V. Krishnamurthy. Your State is Not Mine: A Closer Look at Evading Stateful Internet Censorship. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[17] Savvas Zannettou, Tristan Caulfield, Emiliano De Cristofaro, Nicolas Kourtelris, Ilias Leontiadis, Michael Sirivianos, Gianluca Stringhini, and Jeremy Blackburn. The Web Centipede: Understanding How Web Communities Influence Each Other Through the Lens of Mainstream and Alternative News Sources. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[18] Austin Murdock, Frank Li, Paul Bramsen, Zakir Durumeric, and Vern Paxson. Target Generation for Internet-wide IPv6 Scanning. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[19] Jan Rüth, Christian Bormann, and Oliver Hohlfeld. Large-scale Scanning of TCP’s Initial Window. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[20] Umar Iqbal, Zubair Shafiq, and Zhiyun Qian. The Ad Wars: Retrospective Measurement and Analysis of Anti-adblock Filter Lists. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[21] Johanna Amann, Oliver Gasser, Quirin Scheitle, Lexi Brent, Georg Carle, and Ralph Holz. Mission Accomplished?: HTTPS Security After Diginotar. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[22] Joe DeBlasio, Stefan Savage, Geoffrey M. Voelker, and Alex C. Snoeren. Tripwire: Inferring Internet Site Compromise. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[23] Shehroze Farooqi, Fareed Zaffar, Nektarios Leontiadis, and Zubair Shafiq. Measuring and Mitigating OAuth Access Token Abuse by Collusion Networks. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[24] Janos Szurdi and Nicolas Christin. Email Typosquatting. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November 2017.
[25] Enrico Bocchi, Luca De Cicco, Marco Mellia, and Dario Rossi. The Web, the Users, and the MOS: Influence of HTTP/2 on User Experience. In International Conference on Passive and Active Network Measurement, pages 47–59. Springer, 2017.
[26] Ilker Nadi Bozkurt, Anthony Aguirre, Balakrishnan Chandrasekaran, P Brighten Godfrey, Gregory Laughlin, Bruce Maggs, and Ankit Singla. Why is the Internet so Slow?! In International Conference on Passive and Active Network Measurement, pages 173–187. Springer, 2017.
[27] Stephen Ludin. Measuring What is Not Ours: A Tale of 3rd Party Performance. In Passive and Active Measurement: 18th International Conference, PAM 2017, Sydney, NSW, Australia, March 30-31, 2017, Proceedings, volume 10176, page 142. Springer, 2017.
[28] Kittipat Apicharttrisorn, Ahmed Osama Fathy Atya, Jiasi Chen, Karthikeyan Sundaresan, and Srikanth V Krishnamurthy. Enhancing WiFi Throughput with PLC Extenders: A Measurement Study. In International Conference on Passive and Active Network Measurement, pages 257–269. Springer, 2017.
[29] Alexander Darer, Oliver Farnan, and Joss Wright. FilteredWeb: A framework for the Automated Search-based Discovery of Blocked URLs. In Network Traffic Measurement and Analysis Conference (TMA), 2017, pages 1–9. IEEE, 2017.
[30] Jelena Mirkovic, Genevieve Bartlett, John Heidemann, Hao Shi, and Xiyue Deng. Do You See Me Now? Sparsity in Passive Observations of Address Liveness. In Network Traffic Measurement and Analysis Conference (TMA), 2017, pages 1–9. IEEE, 2017.
[31] Quirin Scheitle, Oliver Gasser, Minoo Rouhi, and Georg Carle. Large-scale Classification of IPv6-IPv4 Siblings with Variable Clock Skew. In Network Traffic Measurement and Analysis Conference (TMA), 2017, pages 1–9. IEEE, 2017.
[32] Paul Pearce, Ben Jones, Frank Li, Roya Ensafi, Nick Feamster, Nick Weaver, and Vern Paxson. Global Measurement of DNS Manipulation. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[33] Rachee Singh, Rishab Nithyanand, Sadia Afroz, Paul Pearce, Michael Carl Tschantz, Phillipa Gill, and Vern Paxson. Characterizing the Nature and Dynamics of Tor Exit Blocking. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[34] Tao Wang and Ian Goldberg. Walkie-Talkie: An Efficient Defense Against Passive Website Fingerprinting Attacks. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[35] Sebastian Zimmeck, Jie S Li, Hyungtae Kim, Steven M Bellovin, and Tony Jebara. A Privacy Analysis of Cross-device Tracking. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[36] Taejoong Chung, Roland van Rijswijk-Deij, Balakrishnan Chandrasekaran, David Choffnes, Dave Levin, Bruce M Maggs, Alan Mislove, and Christo Wilson. A Longitudinal, End-to-End View of the DNSSEC Ecosystem. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[37] Katharina Krombholz, Wilfried Mayer, Martin Schmiedecker, and Edgar Weippl. "I Have No Idea What I’m Doing" – On the Usability of Deploying HTTPS. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[38] Adrienne Porter Felt, Richard Barnes, April King, Chris Palmer, Chris Bentzel, and Parisa Tabriz. Measuring HTTPS Adoption on the Web. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[39] Ben Stock, Martin Johns, Marius Steffens, and Michael Backes. How the Web Tangled Itself: Uncovering the History of Client-Side Web (In)Security. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[40] Pepe Vila and Boris Köpf. Loophole: Timing Attacks on Shared Event Loops in Chrome. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[41] Jörg Schwenk, Marcus Niemietz, and Christian Mainka. Same-Origin Policy: Evaluation in Modern Browser. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[42] Stefano Calzavara, Alvise Rabitti, and Michele Bugliesi. CCSP: Controlled Relaxation of Content Security Policies by Runtime Policy Composition. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[43] Fang Liu, Chun Wang, Andres Pico, Danfeng Yao, and Gang Wang. Measuring the Insecurity of Mobile Deep Links of Android. In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[44] Paul Pearce, Roya Ensafi, Frank Li, Nick Feamster, and Vern Paxson. Augur: Internet-Wide Detection of Connectivity Disruptions. In IEEE Symposium on Security and Privacy, 2017.
[45] Sumayah Alrwais, Xiaojing Liao, Xianghang Mi, Peng Wang, Xiaofeng Wang, Feng Qian, Raheem Beyah, and Damon McCoy. Under the Shadow of Sun-