ä»¥ä¸‹æ˜¯ä¼˜åŒ–åçš„å‚è€ƒæ–‡çŒ®å’Œé™„å½•éƒ¨åˆ†ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°ã€è¿è´¯å’Œä¸“ä¸šï¼š

### å‚è€ƒæ–‡çŒ®

[1] Mattia Carletti, Chiara Masiero, Alessandro Beghi, and Gian Antonio Susto. 2019. Explainable machine learning in Industry 4.0: Evaluating feature importance in anomaly detection to enable root cause analysis. In *2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)*. IEEE, 21â€“26.

[2] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In *2017 IEEE Symposium on Security and Privacy (S&P)*. IEEE, 39â€“57.

[3] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. *ACM Computing Surveys (CSUR)* 41, 3 (2009), 1â€“58.

[4] Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. 2019. Explaining image classifiers by counterfactual generation. In *ICLR*. OpenReview.net.

[5] Min Du, Zhi Chen, Chang Liu, Rajvardhan Oak, and Dawn Song. 2019. Lifelong anomaly detection through unlearning. In *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS)*. 1283â€“1297.

[6] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. DeepLog: Anomaly detection and diagnosis from system logs through deep learning. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*. 1285â€“1298.

[7] Mengnan Du, Ninghao Liu, Qingquan Song, and Xia Hu. 2018. Towards explanation of DNN-based prediction with guided feature inversion. In *Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)*. 1358â€“1367.

[8] Ming Fan, Wenying Wei, Xiaofei Xie, Yang Liu, Xiaohong Guan, and Ting Liu. 2020. Can we trust your explanations? Sanity checks for interpreters in Android malware analysis. *IEEE Transactions on Information Forensics and Security* 16 (2020), 838â€“853.

[9] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. 2019. Understanding deep networks via extremal perturbations and smooth masks. In *ICCV*. IEEE, 2950â€“2958.

[10] Ruth C. Fong and Andrea Vedaldi. 2017. Interpretable explanations of black boxes by meaningful perturbation. In *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*. 3429â€“3437.

[11] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. 2018. AI2: Safety and robustness certification of neural networks with abstract interpretation. In *2018 IEEE Symposium on Security and Privacy (S&P)*. IEEE, 3â€“18.

[12] Amirata Ghorbani, Abubakar Abid, and James Zou. 2019. Interpretation of neural networks is fragile. In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*, Vol. 33. 3681â€“3688.

[13] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black box models. *ACM Computing Surveys (CSUR)* 51, 5 (2018), 1â€“42.

[14] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing. 2018. LEMNA: Explaining deep learning based security applications. In *Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)*. 364â€“379.

[15] Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, and Xia Yin. 2021. Evaluating and improving adversarial robustness of machine learning-based network intrusion detectors. *IEEE Journal on Selected Areas in Communications* (2021).

[16] Xueyuan Han, Thomas Pasquier, Adam Bates, James Mickens, and Margo Seltzer. 2020. UNICORN: Runtime provenance-based detector for advanced persistent threats. In *Network and Distributed System Security Symposium (NDSS)*.

[17] Mashud Hyder and Kaushik Mahata. 2009. An approximate l0 norm minimization algorithm for compressed sensing. In *2009 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 3365â€“3368.

[18] Vincent Jacob, Fei Song, Arnaud Stiegler, Yanlei Diao, and Nesime Tatbul. 2020. AnomalyBench: An open benchmark for explainable anomaly detection. *CoRR* abs/2010.05073 (2020).

[19] Jacob Kauffmann, Malte Esders, GrÃ©goire Montavon, Wojciech Samek, and Klaus-Robert MÃ¼ller. 2019. From clustering to cluster explanations via neural networks. *CoRR* abs/1906.07633 (2019).

[20] Jacob Kauffmann, Klaus-Robert MÃ¼ller, and GrÃ©goire Montavon. 2020. Towards explaining anomalies: A deep Taylor decomposition of one-class models. *Pattern Recognition* 101 (2020), 107198.

[21] Alexander D. Kent. 2015. Cybersecurity data sources for dynamic network research. In *Dynamic Networks in Cybersecurity*. Imperial College Press.

[22] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. *ICLR* (Poster).

[23] Shogo Kitamura and Yuichi Nonaka. 2019. Explainable anomaly detection via feature-based localization. In *ICANN (Workshop)* (Lecture Notes in Computer Science, Vol. 11731). Springer, 408â€“419.

[24] Junjie Liang, Wenbo Guo, Tongbo Luo, Vasant Honavar, Gang Wang, and Xinyu Xing. 2021. FARE: Enabling fine-grained attack categorization under low-quality labeled data. In *The Network and Distributed System Security Symposium 2021*.

[25] Ninghao Liu, Donghwa Shin, and Xia Hu. 2018. Contextual outlier interpretation. In *Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)*. 2461â€“2467.

[26] Scott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In *Advances in Neural Information Processing Systems (NIPS)*. 4765â€“4774.

[27] Zili Meng, Minhu Wang, Jiasong Bai, Mingwei Xu, Hongzi Mao, and Hongxin Hu. 2020. Interpreting deep learning-based networking systems. In *Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM)*. 154â€“171.

[28] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. *Advances in Neural Information Processing Systems 26* (2013), 3111â€“3119.

[29] Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai. 2018. Kitsune: An ensemble of autoencoders for online network intrusion detection. *25th Annual Network and Distributed System Security Symposium (NDSS)*.

[30] GrÃ©goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert MÃ¼ller. 2017. Explaining nonlinear classification decisions with deep Taylor decomposition. *Pattern Recognition* 65 (2017), 211â€“222.

[31] GrÃ©goire Montavon, Wojciech Samek, and Klaus-Robert MÃ¼ller. 2018. Methods for interpreting and understanding deep neural networks. *Digital Signal Processing* 73 (2018), 1â€“15.

[32] Andrea Morichetta, Pedro Casas, and Marco Mellia. 2019. EXPLAIN-IT: Towards explainable AI for unsupervised network traffic analysis. In *Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks*. 22â€“28.

[33] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2016. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. *arXiv preprint arXiv:1602.03616* (2016).

[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. PyTorch: An imperative style, high-performance deep learning library. In *Advances in Neural Information Processing Systems*. 8026â€“8037.

[35] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online learning of social representations. In *Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)*. 701â€“710.

[36] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I trust you?" Explaining the predictions of any classifier. In *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)*. 1135â€“1144.

[37] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani. 2018. Toward generating a new intrusion detection dataset and intrusion traffic characterization. In *Proceedings of the 2nd International Conference on Information Systems Security and Privacy (ICISSP)*. 108â€“116.

[38] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In *International Conference on Machine Learning (ICML)*. 3145â€“3153.

[39] Lior Sidi, Yisroel Mirsky, Asaf Nadler, Yuval Elovici, and Asaf Shabtai. 2020. Helix: DGA domain embeddings for tracking and exploring botnets. In *Proceedings of the 29th ACM International Conference on Information & Knowledge Management (CIKM)*. 2741â€“2748.

[40] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. *ICLR* (Workshop Poster).

[41] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods. In *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AAAI)*. 180â€“186.

[42] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda ViÃ©gas, and Martin Wattenberg. 2017. SmoothGrad: Removing noise by adding noise. *arXiv preprint arXiv:1706.03825* (2017).

[43] Fei Song, Yanlei Diao, Jesse Read, Arnaud Stiegler, and Albert Bifet. 2018. EXAD: A system for explainable anomaly detection on big data traces. In *2018 IEEE International Conference on Data Mining Workshops (ICDMW)*. IEEE, 1435â€“1440.

[44] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In *ICML (Proceedings of Machine Learning Research, Vol. 70)*. PMLR, 3319â€“3328.

[45] Ruming Tang, Zheng Yang, Zeyan Li, Weibin Meng, Haixin Wang, Qi Li, Yongqian Sun, Dan Pei, Tao Wei, Yanfei Xu, et al. 2020. ZeroWall: Detecting zero-day web attacks through encoder-decoder recurrent neural networks. In *39th IEEE Conference on Computer Communications (INFOCOM)*. IEEE, 2479â€“2488.

[46] Muhammad Fahad Umer, Muhammad Sher, and Yaxin Bi. 2017. Flow-based intrusion detection: Techniques and challenges. *Computers & Security* 70 (2017), 238â€“254.

[47] Ruoying Wang, Kexin Nie, Tie Wang, Yang Yang, and Bo Long. 2020. Deep learning for anomaly detection. In *Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM)*. 894â€“896.

[48] Alexander Warnecke, Daniel Arp, Christian Wressnegger, and Konrad Rieck. 2020. Evaluating explanation methods for deep learning in security. In *2020 IEEE European Symposium on Security and Privacy (EuroS&P)*. IEEE, 158â€“174.

[49] Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. 2018. Beyond sparsity: Tree regularization of deep models for interpretability. In *Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)*.

[50] Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised anomaly detection via variational auto-encoder for seasonal KPIs in web applications. In *Proceedings of the 2018 World Wide Web Conference (WWW)*. 187â€“196.

[51] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I. Jordan. 2009. Detecting large-scale system problems by mining console logs. In *Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles (SOSP)*. 117â€“132.

[52] Limin Yang, Wenbo Guo, Qingying Hao, Arridhana Ciptadi, Ali Ahmadzadeh, Xinyu Xing, and Gang Wang. 2021. CADE: Detecting and explaining concept drift samples for security applications. In *30th USENIX Security Symposium (USENIX Security)*.

[53] Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar. 2018. Adversarially learned anomaly detection. In *2018 IEEE International Conference on Data Mining (ICDM)*. IEEE, 727â€“736.

[54] Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, and Ting Wang. 2020. Interpretable deep learning under fire. In *29th USENIX Security Symposium (USENIX Security)*.

[55] Ying Zhong, Wenqi Chen, Zhiliang Wang, Yifan Chen, Kai Wang, Yahui Li, Xia Yin, Xingang Shi, Jiahai Yang, and Keqin Li. 2020. HELAD: A novel network anomaly detection model based on heterogeneous ensemble learning. *Computer Networks* 169 (2020), 107049.

[56] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for discriminative localization. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 2921â€“2929.

[57] Chong Zhou and Randy C. Paffenroth. 2017. Anomaly detection with robust deep autoencoders. In *Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)*. 665â€“674.

[58] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2018. Graph neural networks: A review of methods and applications. *arXiv preprint arXiv:1812.08434* (2018).

[59] Luisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. 2017. Visualizing deep neural network decisions: Prediction difference analysis. *ICLR*. OpenReview.net.

### é™„å½•

#### A. è§£é‡Šå™¨ç®—æ³•

æˆ‘ä»¬æä¾›äº†æ—¶é—´åºåˆ—å’Œå›¾æ•°æ®è§£é‡Šå™¨çš„ç¨‹åºã€‚

##### A.1 æ—¶é—´åºåˆ—è§£é‡Šç¨‹åº

DeepAID è§£é‡Šå™¨ç”¨äºåŸºäºæ—¶é—´åºåˆ—ç³»ç»Ÿçš„ç¨‹åºå¦‚ä¸‹æ‰€ç¤ºï¼ˆç®—æ³• 2ï¼‰ã€‚

**ç®—æ³• 2ï¼šè§£é‡Šæ—¶é—´åºåˆ—å¼‚å¸¸ï¼ˆå•å˜é‡ï¼‰**

**è¾“å…¥**ï¼š
- è¢«è§£é‡Šçš„å¼‚å¸¸ Xâ—¦
- æœ€å¤§è¿­ä»£æ¬¡æ•° max_iter
- å­¦ä¹ ç‡ Î±
- å‚æ•° Î¼1, Î¼2

**è¾“å‡º**ï¼š
- è§£é‡Šç»“æœ Xâ—¦ - Xâˆ— å’Œå‚è€ƒ Xâˆ—

1. åˆå§‹åŒ– Xâˆ— ä¸º Xâ—¦ï¼›`t â† 0`ï¼›
2. å¦‚æœ `ST(Xâ—¦; Î¼1, Î¼2) = True`ï¼Œåˆ™
   1. `Xâˆ— â† Xâ—¦`
   2. `xc â† argmax_xc Pr(xc | Xâ—¦)`
   3. `Xâˆ— â† [Xâ—¦[1], ..., Xâ—¦[t-1], xc]`
3. å½“ `t < max_iter` æ—¶
   1. ä½¿ç”¨ Adam ä¼˜åŒ–å™¨æ›´æ–° Xâˆ—ï¼š`Xâˆ— â† Adam(Xâˆ—; Dts, Î±)`
   2. æµ‹é‡æœ‰æ•ˆæ€§ï¼š`iâˆ— â† argmax_i Dts(Xâˆ—; Xâ—¦[t])`
   3. å¯¹äº `i = 1` åˆ° `N`ï¼š
      1. å¦‚æœ `i â‰  iâˆ—`ï¼Œåˆ™ `(Xâˆ—)_i â† (Xâ—¦)_i`
4. å°† Xâˆ— ç¦»æ•£åŒ–ä¸ºä»…åŒ…å« 0 æˆ– 1 çš„ one-hot å‘é‡ã€‚
5. `t â† t + 1`

##### A.2 å›¾æ•°æ®è§£é‡Šç¨‹åº

åœ¨ç¬¬ 4.4 èŠ‚ä¸­ï¼Œæˆ‘ä»¬æåˆ°é—®é¢˜ (12) ä¸èƒ½ç›´æ¥é€šè¿‡æ¢¯åº¦ä¼˜åŒ–å™¨è§£å†³ï¼Œå¦‚æœ Eğº æ˜¯ä¸å¯å¾®çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›¿ä»£çš„è´ªå©ªè§£å†³æ–¹æ¡ˆï¼š

**è´ªå©ªæœç´¢**ï¼š
1. æˆ‘ä»¬ä»ä¸¤ä¸ªå‚è€ƒèŠ‚ç‚¹ xâˆ— å¼€å§‹æœç´¢ï¼Œå¹¶é€æ¸å‘å¤–æ‰©å±•ã€‚
2. è¿™æ ·ï¼Œæœç´¢åˆ°çš„å‚è€ƒé“¾æ¥ä¸ä¼šç¦»å¼‚å¸¸é“¾æ¥å¤ªè¿œï¼Œä»è€Œå¢åŠ äº†å¯è§£é‡Šæ€§ã€‚
3. å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°† (12) ä¸­çš„ç›®æ ‡å‡½æ•°è¡¨ç¤ºä¸º Dğ‘”ğ‘Ÿğ‘(Xâˆ—; ğ’†âˆ—)ï¼Œç”¨äºè¡¡é‡é“¾æ¥ Xâˆ— çš„ä¼˜å…ˆçº§ã€‚
4. ç„¶åï¼Œé€šè¿‡å¹¿åº¦ä¼˜å…ˆæœç´¢ (BFS) å’Œä¼˜å…ˆé˜Ÿåˆ—è¿›è¡Œè´ªå©ªæœç´¢ã€‚

å¸Œæœ›è¿™äº›ä¿®æ”¹èƒ½å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£å’Œä½¿ç”¨è¿™äº›å†…å®¹ã€‚