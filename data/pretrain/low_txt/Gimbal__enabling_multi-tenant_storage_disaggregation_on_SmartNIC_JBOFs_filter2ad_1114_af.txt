### Performance and Scalability

- **YCSB-F Performance:** The throughput of YCSB-F with 24 database instances increases by 38.1% compared to the 16-instance case.
- **YCSB-C Workload:** YCSB-C is a read-only workload, and even with 24 instances, the NVMe-oF target read bandwidth is not fully saturated, resulting in minimal variation in average read latency.

**Figures:**
- **Figure 10:** RocksDB performance comparison across four approaches. The YCSB is configured to generate 10 million 1KB key-value pairs with a Zipfian distribution of skewness 0.99 for each database instance.
- **Figure 12:** Average read latency as the number of RocksDB instances increases.
- **Figure 11:** Throughput as the number of RocksDB instances increases.

### Virtual View Enabled Optimizations

This experiment evaluates how the virtual view provided by Gimbal can enhance application performance. We used the same RocksDB setup and ran 8 DB instances (from two client servers) on one SmartNIC JBOF. Figure 13 shows the read tail latency for five benchmarks, comparing three cases: vanilla (without optimizations), with flow control, and with both flow control and load balancing. On average, the IO rate limiter, enabled via our credit scheme, reduces the 99.9th percentile latencies by 28.2% compared to the vanilla setup. The request load balancer, which selects a replica with more available bandwidth, further reduces the tail latency by 18.8%.

### Overheads

We evaluate the overhead of Gimbal in two ways:
1. **CPU Cycles Comparison:** We compare the average CPU cycles of the submission and completion procedures with a vanilla SPDK NVMe-oF target on SmartNIC. As shown in Table 1a, Gimbal adds 37.5-62.5% more cycles to realize the storage switch. Despite this, it does not negatively impact the performance for PCIe Gen3 SSDs, as discussed in Section 2.4. Future PCIe Gen4 SSDs could achieve up to 7GB/s bandwidth or 1 MIOPS. We ran a 4KB read benchmark with a NULL device (which does not perform actual I/O and returns immediately) to measure the maximum IOPS of Gimbal on SmartNIC. Gimbal achieves 821 KIOPS with one SmartNIC core and 2446 KIOPS with four cores, indicating that SmartNIC-based Gimbal can support next-generation SSDs. We also expect Gimbal to scale up with future powerful ARM cores or specialized engines.

**Table 1: Overhead Comparison with Vanilla SPDK**
- **(a) CPU Cycle Comparison (4KB Read, QD=Queue Depth, 125cycles=1usec):**
  - **Vanilla SPDK:**
    - 1 worker (QD1): 32
    - 16 workers (QD32): 16
  - **Gimbal:**
    - 1 worker (QD1): 52 (+62.5%)
    - 16 workers (QD32): 22 (+37.5%)
- **(b) Maximum IOPS with NULL Device (4KB Read I/O):**
  - **Vanilla SPDK:**
    - 1 CPU core, 1 worker: 937 KIOPS
    - 4 CPU cores, 8 workers: 2692 KIOPS
  - **Gimbal:**
    - 1 CPU core, 1 worker: 821 KIOPS (-12.4%)
    - 4 CPU cores, 8 workers: 2446 KIOPS (-9.2%)

### Generalization

This experiment evaluates Gimbal's performance on a different type of SSD. We ran the same microbenchmark (Section 5.3) using the Intel DC P3600 1.2TB model, which uses 2-bit MLC NAND. This SSD presents 33.5% lower 128KB read (2.1GB/s) and 35.0% higher 4KB random write (243MB/s) in terms of bandwidth. We tuned the \( \text{Thresh}_{\text{max}} \) to 3ms for better read utilization, achieving higher tail latency than DCT983 for 128KB reads. Gimbal adapts to the characteristics of the SSD and performs similarly to the DCT983 case in terms of \( f \)-Util. Specifically, it shows 0.63 and 0.72 of \( f \)-Util for read and write under clean conditions, and 0.58 and 0.90 for read and write under fragmented conditions.

We also ran Gimbal on a Xeon E5-2620 v4 CPU and compared the overhead with vanilla SPDK. Gimbal performs 10.8% lower (1368 KIOPS) than vanilla SPDK (1533 KIOPS) for 4KB read performance with the NULL device, similar to the result on SmartNIC.

### Summary

**Table 2: Comparison of Four Multi-Tenancy Mechanisms**
- **Gimbal outperforms other approaches** because it dynamically estimates the available bandwidth and I/O costs for each storage device based on current conditions and workloads, performs fine-grained fair queuing at the NVMe-oF target across tenants, and uses credit-based flow control to adjust client behavior. Other approaches, such as ReFlex and FlashFQ, use an approximate offline-profiled SSD cost model, which reduces scheduling efficiency and lacks mechanisms to regulate client-side I/Os, causing significant delays. Parda employs a client-side mechanism with limited fair queuing, which is unsuitable for low-latency, high-bandwidth NVMe SSDs.

### Related Work and Discussion

- **Shared Storage with QoS Guarantee:** Previous work has explored achieving high device utilization and fairness for local and remote shared storage. These approaches often fail to precisely estimate I/O capacity and per-I/O cost, leading to inefficiencies in NVMe-oF disaggregated settings. Timeslice-based I/O schedulers, while effective for slow storage, are not suitable for fast NVMe SSDs. Recent research has leveraged ML techniques to predict I/O performance, but these methods are insufficient for achieving fairness.
- **Remote Storage I/O Stacks:** Researchers have characterized the performance of iSCSI-based disaggregated storage and proposed optimizations. ReFlex provides a kernel-bypass data-plane with a credit-based QoS scheduler, and i10 offers an efficient in-kernel TCP/IP remote storage stack. Our work targets SmartNIC JBOFs using NVMe-oF protocols.
- **Disaggregated Storage Architecture:** New hardware designs address the limitations of existing disaggregation. Shoal and LeapIO are examples of power-efficient and performant network fabrics and uniform address spaces, respectively. We focus on emerging SmartNIC-based disaggregated storage solutions.
- **Programmable Packet Scheduling:** Studies like PIFO, PIEO, and SP-PIFO explore new HW/SW programmable packet schedulers. Unlike these, Gimbal focuses on scheduling storage I/O requests among NVMe SSDs. Future work will include exposing programmability from the Gimbal traffic manager to realize flexible scheduling policies.
- **Emerging Storage Media:** QLC NAND, despite its cost and capacity advantages, has worse performance characteristics than TLC. Gimbal's techniques can be applied to QLC SSDs and 3DXP devices, though the latter is more suitable for local caching.
- **Hardware-Accelerated Gimbal:** The pipelined architecture of Gimbal has a standard interface for ingress and egress, making it compatible with any hardware-accelerated NVMe-oF implementation. It is also portable to hardware logic using frameworks like Tonic.

### Conclusion

This paper introduces Gimbal, a software storage switch that enables multi-tenant storage disaggregation on SmartNIC JBOFs. Gimbal applies four key techniques: delay-based congestion control for SSD bandwidth estimation, a new I/O cost measurement scheme, a two-level hierarchical I/O scheduling framework, and end-to-end credit-based flow control with an exposed SSD virtual view. We designed, implemented, and evaluated Gimbal on the Broadcom Stingray PS1100R platforms. Our evaluations show that Gimbal maximizes SSD usage, ensures fairness among multiple tenants, provides better QoS guarantees, and enables multi-tenancy-aware performance optimizations for applications. This work does not raise any ethical issues.

### Acknowledgments

This work is supported by Samsung and NSF grants CNS-2028771 and CNS-2006349. We thank the anonymous reviewers and our shepherd, Brent Stephens, for their valuable comments and feedback.