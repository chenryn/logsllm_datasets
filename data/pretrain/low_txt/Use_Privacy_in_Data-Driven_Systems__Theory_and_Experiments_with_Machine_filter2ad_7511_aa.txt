# Title: Use Privacy in Data-Driven Systems: Theory and Experiments with Machine-Learned Programs

## Authors:
- Anupam Datta, Carnegie Mellon University
- Matthew Fredrikson, Carnegie Mellon University
- Gihyuk Ko, Carnegie Mellon University
- Piotr Mardziel, Carnegie Mellon University
- Shayak Sen, Carnegie Mellon University

## Abstract
This paper introduces a formal approach to defining and enforcing use privacy properties in data-driven systems. Unlike previous work, we focus on the use of proxies (strong predictors) of protected information types. Our definition links proxy use to intermediate computations within a program, identifying two key properties: 
1. The result is strongly associated with the protected information type.
2. It is likely to causally affect the final output of the program.

For a specific instantiation of this definition, we present a program analysis technique that detects instances of proxy use in a model and provides a witness identifying the parts of the program exhibiting this behavior. Recognizing that not all instances of proxy use are inappropriate, we use a normative judgment oracle to determine inappropriateness. Our repair algorithm then transforms the model to eliminate inappropriate proxy use while maintaining classification accuracy. Our evaluation on social datasets shows that these algorithms can detect and remove proxy use instances effectively, while preserving acceptable classification performance.

## CCS Concepts
- Security and privacy → Privacy protections

## Keywords
- Use privacy

## 1 Introduction
Restrictions on information use are central to privacy regulations and legal frameworks [28, 54, 61, 62]. We introduce the term "use privacy" to refer to norms governing information use. Recent cases have shown that inappropriate information use can violate both privacy laws [68] and user expectations [16, 19], prompting calls for technology to enforce use privacy requirements [53]. To meet these regulatory and user expectations, companies dedicate resources to comply with privacy policies governing information use [53, 57]. A significant body of work has emerged around use privacy compliance, particularly for explicit use of protected information types [64].

In this paper, we address a richer class of use privacy restrictions—those governing the indirect use of protected information through proxies in data-driven systems. These systems, including machine learning and artificial intelligence, use large amounts of individual data to make decisions, making it critical to address use privacy concerns [53, 57].

### Motivating Examples
In 2012, Target faced criticism for using customer shopping history to predict pregnancy status and market baby items [19]. Even if the inference was not explicit, the use of health-related search terms and browsing history—proxies for health conditions—for targeted advertising has led to legal action and public concern [16, 44, 68]. Similar concerns arise in the Internet of Things [40, 49, 52, 67].

### Problem Setting
A use privacy constraint may require that health information or its proxies not be used for advertising [17, 46, 53, 68]. In our setting, a data-driven system is audited to ensure compliance with such constraints. Auditing could be done by a trusted data processor or a regulatory oversight organization with access to the machine learning models and dataset distribution. We assume the data processor does not evade detection and provides accurate information, similar to the trusted data processor setting in differential privacy [25].

Even with strong background knowledge, it is impossible to guarantee that data processors cannot infer certain facts about individuals [21]. Use privacy instead requires that systems simulate ignorance of protected information types by not using them or their proxies in decision-making. This requirement is met if the systems do not infer protected information types or their proxies, or if such inferences do not affect decisions.

### Proxy Use
Our theory of use privacy uses a normative judgment oracle to determine the inappropriateness of proxy use. For example, while using health information or its proxies for credit decisions may be inappropriate, exceptions can be made for proxies directly relevant to credit-worthiness (e.g., income and expenses).

### Key Technical Contributions
We formalize proxy use of protected information types in programs. Our formalization relates proxy use to intermediate computations obtained by decomposing a program. We define two essential properties:
1. The result perfectly predicts the protected information type.
2. It has a causal effect on the final output.

In practice, this definition is too rigid for machine learning. Instead, we use a standard measure of association strength to define an \(\epsilon\)-proxy, where \(\epsilon \in [0, 1]\) indicates the strength of the proxy. We also use a recently introduced causal influence measure to define \(\delta\)-influence, where \(\delta \in [0, 1]\) indicates the strength of the influence. Combining these, we define \((\epsilon, \delta)\)-proxy use.

### Detection and Repair
We instantiate our definition in a simple programming language with conditionals, arithmetic, and logical operations. Our program analysis technique detects proxy use and provides a witness. If a found instance is deemed inappropriate, our repair algorithm transforms the model to eliminate the proxy use while minimizing changes to classification accuracy.

### Evaluation
We evaluate our proxy use definition, detection, and repair algorithms on four real datasets used to train decision trees, linear models, and random forests. Our evaluation demonstrates a typical workflow for practitioners, showing how our tools uncover more proxy uses than a baseline procedure. For three simulated settings—contraception advertising, student assistance, and credit advertising—we find interesting proxy uses and discuss how our detection tool aids in determining appropriateness. We also evaluate the performance of the detection algorithm, showing linear scaling in the size of the model, and the impact of repair on model accuracy.

### Related Work
Our work differs from existing privacy literature, which typically focuses on explicit use of protected information types [59, 64, 46]. Recent work on discovering personal data use by black-box web services also primarily examines explicit use [2, 16, 27, 35–37, 43, 44, 47, 69, 71]. Differential privacy [25] protects against different types of privacy harm, but does not address the use of information types by the data processor. Lipton and Regan’s notion of “effectively private” [46] and prior work on fairness [22, 29, 63] do not fully address proxy use as required by use privacy.

### Contributions
- Articulation of the problem of protecting use privacy in data-driven systems.
- Formalization of proxy use in programs.
- Program analysis technique for detecting proxy use.
- Repair algorithm for eliminating inappropriate proxy use.
- Empirical evaluation on real datasets.

This work represents significant progress toward enabling use privacy, though many challenging problems remain open.