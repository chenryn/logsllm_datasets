# Policy Units in Network Management

## 1. Introduction to Policy Units
In Figure 7, we observe that the policy in Univ-1 is "on by default," with 70% of hosts able to reach nearly all subnets and 30% able to reach all subnets. In these networks, at least one policy unit spans all routers. However, in policy-heavy networks, about 60% of the policy units each span 20% or fewer routers in the network. None of the units in the policy-heavy networks span all routers. This illustrates how policy units expose the compartmentalization imposed by the network, with hosts connected to different parts of the network being subject to different policies and able to reach different sets of endpoints.

## 2. Analysis of Endpoints in Policy Units
In Figure 6(b), we examine the cumulative fraction of endpoints in the networks' policy units. We find that all networks have at least one unit to which over 60% of the endpoints in the network belong. This means that most endpoints in all the networks we studied are subject to identical policies. Furthermore, for all networks, most endpoints are members of only a handful of units. For instance, about 70% of the endpoints in the policy-heavy networks (Univ-3 and Enet-2) fall into 15% of the units (this corresponds to 2 and 4 policy units in Univ-3 and Enet-2, respectively). In the policy-lite networks, we find that all endpoints are members of one or two policy units. This suggests that policy is unevenly applied in the networks we studied. Most hosts experience the same policies, but certain special hosts are selected for extra constraints or extra freedom.

## 3. Detailed Examination of Policy Units
### Univ-1
In Figure 7(a), we examine the differences among the policy units in Univ-1 in terms of the destinations they can reach. Recall that Univ-1, which is a policy-lite network, divides its endpoints into two policy units. Hosts in one unit (policy unit 2 on the right) are able to reach all subnets in the network, while those in the other (unit 1) are unable to reach about 2% of the subnets in the network. From Figure 7(b), we note that unit 1 is comprised of 70% of the network’s endpoints, while the remaining 30% are in unit 2.

We were able to discuss our empirical observations with the network’s operators. The operators validated that the network indeed implements two such units. In particular, the network controls the reachability to a specific collection of subnets (all of which were attached to a given router) by preventing route announcements to the subnets from reaching other routers (this is achieved using the appropriate route filters). This small collection of subnets, however, were reachable from the rest of the subnets attached to the router.

Overall, these observations indicate that Univ-1 implements a fairly uniform "on by default" policy across all endpoints, meaning that hosts can access almost all network resources unless prevented by some explicit mechanism.

### Univ-3
Univ-3 provides a more complex case study, with 15 distinct policy units. The results from our study of this network are summarized in Figure 8. From Figure 8(a), we observe that 9 of the 15 units in this network, units 7 through 15, have almost complete reachability to the rest of the subnets in the network, with each unit being able to reach at least 98% of the subnets in the network. In contrast, units 1 through 6 have very limited reachability, being able to reach only between 20 and 45% of the subnets in the network.

In Figure 8(b), we illustrate the distributions of endpoints across various policy units. The policy units 7 through 15, all of which have roughly universal reachability, vary significantly in the number of endpoints in them: unit 13 contains 70% of the endpoints, unit 11 contains 20% of the endpoints, while units 9, 10, 12, 14, and 15 contain a minuscule fraction of endpoints each. In contrast, the total number of endpoints in policy units 1 through 6, which can reach a much smaller fraction of network resources each, is < 5%.

These results show that the policy implemented by Univ-3 is characterized by an interesting dichotomy. The policy divides the network into two unequal parts. One part, which contains an overwhelming fraction of endpoints, is "on by default," meaning that hosts in this part can access almost all network resources. A few special cases restrict the specific set of resources that small collections of hosts can access. In the other part of the network, which contains a much smaller fraction of endpoints, the policy is closer to being "off by default," meaning that the hosts cannot access most network resources by default. Upon examining the configuration files, we noticed that Univ-3 used data plane mechanisms (i.e., packet filters) to implement the special cases in the first part. However, to implement the off-by-default access policy in the second part, the network used both packet filters as well as control plane mechanisms such as route filters.

## 4. Application to Network Management
### 4.1 Making Informed Changes to Configuration
When installing or altering configuration in large networks, operators have no way to systematically reason about how their changes may impact the network’s policies. Operators employ a combination of ad hoc reasoning, designing time-consuming management of change (MOC) reports, or waiting for users’ complaints or even attacks before determining that their changes have had an undesirable interaction with the intended network-wide policies. Operators can use policy units to validate changes by comparing policy units before and after the change is made. Operators can use the "diffs" between the two states to debug the network change.

### 4.2 Examining Trends in Network Policy Evolution
Over time, networks grow in size and complexity, making it difficult for operators to determine if the overall policies can be implemented using simpler mechanisms. For instance, a network may start out with conservative communication rules requiring a multitude of policy units, but over time, more and more endpoints are granted a uniform set of privileges to access a common set of resources. Although the configuration becomes complex, since more and more endpoints are granted a common set of privileges, the network’s policy units may coalesce and shrink in total number. By monitoring policy units, a network operator can examine how the network-wide policy evolves and whether, at some point, the number of units and the nature of the PPS naturally lend themselves to an alternate, much simpler network-wide configuration.

## 5. Related Work
Prior work [12, 3] developed approaches for modeling the reachability between routers. Our approach builds on these proposals but coalesces and extracts the high-level network policies implemented in a network configuration. The policy units we extract are a natural match with how operators view and design their networks, while the reachability sets between routers computed by prior work do not expose the commonality or structure of the policies applied to hosts. Second, network modifications, such as movement of departments across buildings or movement of filters from core to edge, can greatly affect a network’s reachability sets. However, the policy units remain unchanged as the constraints applied to the network’s hosts remain constant.

Several projects [2, 5, 13] attempt to simplify network management by using clean-slate approaches to represent and implement global policies. Policy units can be used to unearth the structure of policies in an existing network that need to be implemented in the clean-slate design, and help select between "default on" and "default off" strategies.

A few studies [9, 11] examined traffic characteristics in enterprises, and other recent work developed models for design and configuration in enterprises [3, 7]. Our work adds to these by shedding light on the nature of reachability policies in enterprises.

## 6. Summary
While there has been a growing interest in understanding the design and operation of enterprise networks, few studies, if any, have examined the nature of reachability policies implemented by enterprises today. In this paper, we introduced the notion of policy units that form an abstract representation of how enterprise reachability policies segregate endpoints into distinct privilege classes. We presented an initial algorithm for extracting policy units from router configurations. We applied the algorithm to five production networks and verified our observations with the operators of some of the networks. Through our study, we obtained unique insights into the current implementation of reachability policies in enterprises. In particular, we found that most hosts in these networks are subjected to a uniform set of reachability policies, while a few special case hosts have very restricted reachability.

We argued that our empirical observations and the policy unit extraction framework are useful to inform clean-slate approaches, to support network re-design efforts, and to augment current approaches to network configuration and management. We expect policy units will be valuable as an aid for visualizing the network-wide configuration state, and are exploring these directions.

## 7. Acknowledgement
We would like to thank Dale Carder, Perry Brunelli, and the other operators for their network configuration files. This work was supported in part by an NSF CAREER Award (CNS-0746531) and an NSF NeTS FIND Award (CNS-0626889).

## 8. References
[1] S. Acharya, J. Wang, Z. Ge, T. Znati, and A. Greenberg. Simulation study of firewalls to aid improved performance. In ANSS '06: Proceedings of the 39th annual Symposium on Simulation, pages 18–26, Washington, DC, USA, 2006. IEEE Computer Society.
[2] H. Ballani and P. Francis. CONMan: A Step towards Network Manageability. In Proc. of ACM SIGCOMM, 2007.
[3] T. Benson, A. Akella, and D. A. Maltz. Unraveling the complexity of network management. In NSDI, April 2009.
[4] M. Casado, M. Friedman, J. Pettitt, N. McKeown, and S. Shenker. Ethane: Taking Control of the Enterprise. In SIGCOMM '07.
[5] M. Casado, T. Garfinkel, A. Akella, M. Friedman, D. Boneh, N. McKeown, and S. Shenker. SANE: A Protection Architecture for Enterprise Networks. In USENIX Security, Vancouver, BC, Canada, Aug. 2006.
[6] A. Feldmann and S. Muthukrishnan. Tradeoffs for packet classification. In INFOCOM, 2000.
[7] P. Garimella, Y.-W. E. Sung, N. Zhang, and S. Rao. Characterizing VLAN usage in an operational network. In INM '07.
[8] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, J. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to network control and management. SIGCOMM Comput. Commun. Rev., 35(5):41–54, 2005.
[9] S. Guha, J. Chandrashekar, N. Taft, and K. Papagiannaki. How healthy are today's enterprise networks? In IMC, 2008.
[10] D. A. Maltz, G. Xie, J. Zhan, H. Zhang, G. Hjálmtýsson, and A. Greenberg. Routing design in operational networks: a look from the inside. SIGCOMM Comput. Commun. Rev., 34(4):27–40, 2004.
[11] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and B. Tierney. A first look at modern enterprise traffic. In IMC, 2005.
[12] G. G. Xie, J. Zhan, D. A. Maltz, H. Zhang, A. G. Greenberg, G. Hjálmtýsson, and J. Rexford. On static reachability analysis of IP networks. In INFOCOM, pages 2170–2183. IEEE, 2005.
[13] H. Yan, D. A. Maltz, T. S. E. Ng, H. Gogineni, H. Zhang, and Z. Cai. Tesseract: A 4D network control plane. In NSDI. USENIX, 2007.