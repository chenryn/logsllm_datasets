### Figure 6: Request Batching Optimization

Figure 6 compares the throughput with and without request batching for a read-write operation (0/0). The throughput without batching increases with the number of clients because the algorithm can process many requests in parallel. However, the CPU of the replicas becomes saturated even with a small number of clients, as each request requires a full instance of the protocol. Our batching mechanism reduces both CPU and network overhead under load without increasing the latency in an unloaded system. Previous state machine replication systems that tolerate Byzantine faults [10, 9] have used batching techniques that significantly impact latency.

### Figure 7: Separate Request Transmission

Figure 7 compares performance with and without the separate request transmission (SRT) optimization. The first graph shows latency for varying argument sizes, and the second graph shows throughput for a read-write operation (4/0). We labeled the version of BFT without the SRT optimization as BFT-NO-SRT. 

Separate request transmission reduces latency by up to 40% because the request is sent only once, and the primary and backup nodes compute the request's digest in parallel. This also improves throughput for large requests by enabling more requests per batch.

### Impact of Tentative Execution Optimization

The tentative execution optimization has a negligible impact on throughput but reduces latency by up to 27% for small argument and result sizes. The benefit decreases rapidly as the sizes increase.

### Piggybacking Commits

Piggybacking commits has a negligible impact on latency because the commit phase is performed in the background, thanks to the tentative execution of requests. It has a small impact on throughput, except when the number of concurrent clients accessing the service is small. For example, it improves the throughput of operation 0/0 by 33% with 5 clients but only by 3% with 200 clients. The benefit decreases as batching amortizes the cost of processing commit messages over the batch size. This optimization is not currently part of the BFT library; we have only implemented code for the normal case.

### 5. File System Benchmarks

We compared the performance of BFS with two other NFS implementations: NO-REP, which is identical to BFS but not replicated, and NFS-STD, which is the NFS V2 implementation in Linux with Ext2fs at the server. The first comparison allows us to accurately evaluate the overhead of the BFT library within a real service implementation. The second comparison shows that BFS is practical, performing similarly to NFS-STD, which is widely used but not fault-tolerant.

#### 5.1. Experimental Setup

The experiments to evaluate BFS used the setup described in Section 4.1. They ran two well-known file system benchmarks: Andrew [11] and PostMark [8]. There were no view changes or proactive recoveries in these experiments.

- **Andrew Benchmark**: Emulates a software development workload. We scaled the benchmark by creating n copies of the source tree in the first two phases and operating on all copies in the remaining phases [4]. We ran versions of Andrew with n equal to 100 (Andrew100) and 500 (Andrew500), generating approximately 200 MB and 1 GB of data, respectively. Andrew100 fits in memory at both the client and the replicas, but Andrew500 does not.
- **PostMark**: Models the load on Internet Service Providers. We configured PostMark with an initial pool of 10,000 files with sizes between 512 bytes and 16 KB. The benchmark ran 100,000 transactions.

For all benchmarks and NFS implementations, the actual benchmark code ran at the client workstation using the standard NFS client implementation in the Linux kernel with the same mount options, including UDP transport, 3 KB buffers, write-back client caching, and attribute caching. BFS and NO-REP do not maintain the time-last-accessed attribute. We report the mean of at least three runs, and the standard deviation was always below 2% of the reported value.

#### 5.2. Experiments

**Figure 8: Modified Andrew**

Figure 8 presents results for Andrew100 and Andrew500 in a configuration with four replicas and one client machine. The comparison between BFS and NO-REP shows that the overhead of Byzantine fault tolerance is low for this serviceâ€”BFS takes only 14% more time to run Andrew100 and 22% more time to run Andrew500. This slowdown is smaller than that measured with micro-benchmarks because the client spends a significant fraction of the elapsed time computing between operations, and operations at the server perform some computation. Additionally, there are a significant number of disk writes at the server in Andrew500.

**Figure 9: PostMark**

The overhead of Byzantine fault tolerance is higher in PostMark: BFS's throughput is 47% lower than NO-REP's. This is explained by a reduction in the computation time at the client relative to Andrew. However, BFS's throughput is only 13% lower than NFS-STD's, offset by an increase in the number of disk accesses performed by NFS-STD in this workload.

### 6. Conclusions

Byzantine-fault-tolerant replication can be used to build highly-available systems that can tolerate even malicious behavior from faulty replicas. However, previous work on Byzantine fault tolerance has failed to produce solutions that perform well. This paper presented a detailed performance evaluation of the BFT library, a replication toolkit for building systems that tolerate Byzantine faults. Our results show that services implemented with the library perform well, even when compared with unreplicated implementations that are not fault-tolerant.

### Acknowledgements

We thank the anonymous reviewers and Lorenzo Alvisi for their comments on drafts of this paper.

### References

[1] J. Black et al. UMAC: Fast and Secure Message Authentication. In Advances in Cryptology - CRYPTO, 1999.
[2] M. Castro. Practical Byzantine Fault Tolerance. Technical Report TR-817, PhD thesis. MIT Lab. for Computer Science, 2001.
[3] M. Castro and B. Liskov. Practical Byzantine Fault Tolerance. In USENIX Symposium on Operating Systems Design and Implementation, 1999.
[4] M. Castro and B. Liskov. Proactive Recovery in a Byzantine-Fault-Tolerant System. In USENIX Symposium on Operating Systems Design and Implementation, 2000.
[5] M. Castro, R. Rodrigues, and B. Liskov. Using Abstraction to Improve Fault Tolerance. Submitted for publication, 2001.
[6] M. Herlihy and J. Wing. Axioms for Concurrent Objects. In ACM Symposium on Principles of Programming Languages, 1987.
[7] A. Iyengar et al. Design and Implementation of a Secure Distributed Data Repository. In IEEE International Information Security Conference, 1998.
[8] I. Katcher. PostMark: A New File System Benchmark. Technical Report TR-3022, Network Appliance, 1997.
[9] K. Kihlstrom, L. Moser, and P. Melliar-Smith. The SecureRing Protocols for Securing Group Communication. In Hawaii International Conference on System Sciences, 1998.
[10] D. Malkhi and M. Reiter. A high-throughput secure reliable multicast protocol. In Computer Security Foundations Workshop, 1996.
[11] J. Ousterhout. Why Aren't Operating Systems Getting Faster as Fast as Hardware? In USENIX Summer Conference, 1990.
[12] M. Reiter. The Rampart toolkit for building high-integrity services. Theory and Practice in Distributed Systems (LNCS 938), 1995.
[13] M. Reiter et al. The R Key Management Service. In ACM Conference on Computer and Communications Security, 1996.
[14] F. Schneider. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Computing Surveys, 22(4), 1990.
[15] L. Zhou, F. Schneider, and R. van Renesse. COCA: A Secure Distributed On-line Certification Authority. Technical Report TR 2000-1828, CS Department, Cornell University, 2000.