### Error Propagation Analysis in the Linux Kernel

**Error Propagation Statistics:**

- **fs Subsystem:**
  - **Crashes within fs:** 89.4%
  - **Propagated to kernel:** 5.7%
    - **Invalid operand crashes:** 38.6%

- **Kernel Subsystem:**
  - **Crashes within kernel:** 90%
  - **Propagated to fs:** 1.6%
  - **Propagated to mm:** 1.8%

**Summary of Findings:**

- **Overall Error Propagation:** Less than 10% of errors propagate between subsystems, with approximately 90% of crashes occurring within the subsystem where the error was injected. This is consistent with earlier studies on UNIX systems, which showed that 8% of errors propagate between subsystems [14], but lower than the 18% observed for the Tandem Guardian operating system [17].
  
- **fs Subsystem Errors:** Errors injected into the fs subsystem have the highest probability of propagating, primarily to the kernel subsystem (5.7%). Recovery from such crashes typically requires a system reboot, which can take around 4 minutes and significantly impact system availability.

- **Critical Propagation Paths:**
  - From fs to kernel.
  - From kernel to fs.
  - From kernel to mm.

- **Strategic Assertion Placement:** By analyzing propagation patterns, it is feasible to identify strategic locations for embedding additional assertions in the source code. When an assertion fires, appropriate recovery actions (e.g., terminating an offending process) can be initiated to prevent kernel crashes and reduce system downtime, thus achieving high availability [11].

**Catastrophic Crashes:**

- Nine catastrophic kernel crashes were observed, requiring reformatting the entire file system. For example, an error injected into the mm subsystem can propagate to the fs subsystem, rendering the file system unusable. Embedding assertions, such as checking the relationship between `index` and `inode->i_size`, can help detect and mitigate such errors.

### Experimental Results: Not Manifested Errors and Fail Silence Violations

**Not Manifested Errors:**

- **Redundancy in C Source Code:**
  - Example: In the `reschedule_idle()` function, the `can_schedule` condition is always true in single-processor machines. Thus, reversing the if statement does not change the behavior, and the error is not manifested.

- **Random Branch Error Campaign:**
  - **Campaign B:** 47% of activated errors are not manifested, compared to 33% in campaigns A and C. This is due to the intrinsic nature of the Linux kernel, where many branches are not taken, making their functionality similar to a no-operation (nop) instruction.

**Fail Silence Violations:**

- **Campaign C (Valid but Incorrect Branch):** 9.9% of activated errors result in fail silence violations, where the kernel returns an error code to the user application, propagating incorrect data. This percentage is higher than in other campaigns (2% and 0.8% for campaigns A and B, respectively).

- **Example:**
  - In the `pipe_read()` function, the condition `ppos != &filp->f_pos` is reversed. The kernel falsely detects an error and returns the error code `-ESPIPE`.

### Conclusions

- **Major Causes of Crashes:**
  - 95% of crashes are due to four major causes: null pointer dereference, kernel paging request, general protection faults, and invalid operands.
  - Nine severe crashes required reformatting the file system, with system recovery taking nearly an hour.
  - Less than 10% of crashes are associated with fault propagation, and nearly 60% of crash latencies are within 10 cycles.

- **Error Injection Experiments:**
  - Extensive fault injection campaigns were conducted on selected kernel subsystems (arch, fs, kernel, and mm) to analyze and quantify the response of the Linux operating system to various failure scenarios, with a focus on detailed analysis of kernel crashes.

### Acknowledgments

This work was supported in part by a MARCO Program grant SC #1010168/PC#2001-CT-888 Carnegie Mellon and in part by NSF grant CCR 99-02026. We thank Fran Baker for her insightful editing of our manuscript.

### References

[1] J. Arlat, et al., “Dependability of COTS Microkernel-Based Systems,” IEEE Transactions on Computers, 51(2), 2002.
[2] J. Barton, et al., “Fault Injection Experiments Using FIAT, IEEE Transactions on Computers, 39(4), 1990.
[3] M. Beck, et al., “Linux Kernel Internals,” Second Edition, Addison-Wesley, 1998.
[4] K. Buchacker, V. Sieh, “Framework for Testing the Fault-Tolerance of Systems Including OS and Network Aspects,” Proc. 3rd Intl. High-Assurance Systems Engineering Symposium, 2001.
[5] J. Carreira, H. Madeira, and J. Silva, “Xception: A Technique for the Evaluation of Dependability in Modern Computers,” IEEE Transactions on Software Engineering, 24(2), 1998.
[6] G. Carrette, “CRASHME: Random Input Testing,” 1996, http://people.delphiforums.com/gjc/crashme.html
[7] H. Cha, et al., “A Gate-level Simulation Environment for Alpha-Particle-Induced Transient Faults, IEEE Transactions on Computers, 45(11), 1996.
[8] G. Choi, R. Iyer and D. Saab, “Fault Behavior Dictionary for Simulation of Device-level Transients,” Proc. IEEE International Conf. Computer-Aided Design, 1993.
[9] A. Chou, et al., “An Empirical Study of Operating Systems Errors,” In [11] M. Hiller, et al., “On the Placement of Software Mechanism for Detection of Data Errors,” in DSN-02, 2002.
[10] M. Godfrey and Q. Tu, “Evolution in Open Source Software: A Case Study,” Proc. Intl. Conference on Software Maintenance, 2000.
[11] M. Hiller, et al., “On the Placement of Software Mechanism for Detection of Data Errors,” in DSN-02, 2002.
[12] M. Hsueh, T. Tsai, and R. Iyer, “Fault Injection Techniques and Tools IEEE Computer, 30(4), 1997.
[13] R. Iyer, D. Rossetti, M. Hsueh, “Measurement and Modeling of Computer Reliability as Affected by System Activity,” ACM Transactions on Computer Systems, Vol.4, No.3, 1986.
[14] W. Kao, et al, “FINE: A Fault Injection and Monitoring Environment for Tracing the UNIX System Behavior Under Faults,” IEEE Trans. on Software Engineering, 19(11), 1993.
[15] P. Koopman, J. DeVale, “The Exception Handling Effectiveness of POSIX Operating Systems,” IEEE Transactions on Software Engineering, 26(9), 2000.
[16] N. Kropp et al., “Automated Robustness Testing of Off-the-Shelf Software Components,” Proc. FTCS-28, 1998.
[17] I. Lee and R. Iyer, “Faults, Symptoms, and Software Fault Tolerance in Tandem GUARDIAN90 Operating System,” Proc. FTCS-23, 1993.
[18] H. Maderia, et al., “Experimental evaluation of a COTS system for space applications,” in DSN-02, 2002.
[19] B. P. Miller, et al., “A Re-examination of the Reliability of UNIX Utilities and Services,” Tech. Rep., University of Wisconsin, 2000.
[20] Built-in Kernel Debugger (KDB), http://oss.sgi.com/projects/kdb/
[21] Kernel Profiling (kernprof), http://oss.sgi.com/projects/kernprof/
[22] Linux RAS Package, http://oss.software.ibm.com/linux/projects/linuxras/
[23] M. Sullivan and R. Chillarege, “Software Defects and Their Impact on System Availability – A Study of Field Failures in Operating Systems,” Proc. FTCS-21, 1991.
[24] UnixBench, www.tux.org/pub/tux/benchmarks/System/unixbench
[25] D. Wilder, “LKCD Installation and Configuration,” 2002. Proc. of 18th ACM Symp. on Operating systems principles, 2001. http://lkcd.sourceforge.net/
[26] J. Xu, Z. Kalbarczyk, R. Iyer, “Networked Windows NT System Field Failure Data Analysis,” Proc. of Pacific Rim Intl' Symp. on Dependable Computing, 1999.