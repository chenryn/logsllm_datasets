### Introduction

Our work focuses on the development and application of symbolic execution techniques to explore multiple program paths. Each new candidate input must be distinct from previous ones, often leading to different execution paths. We have conducted research on symbolic execution methods [2, 17] and plan to apply these techniques in our current work.

### Creating Formulas for Multiple Paths

In this paper, we apply the weakest precondition (WP) technique to intermediate representation (IR) programs that contain a single program path, i.e., the processing of the original input by one implementation. However, our WP algorithm is capable of handling IR programs with multiple paths [19]. In future work, we aim to develop formulas that encompass multiple paths.

### Online Formula Generation

Currently, our implementation for generating symbolic formulas operates offline. We first record an execution trace for each implementation as it processes an input. Then, we convert the execution trace into an IR representation and compute the symbolic formula. An alternative approach would be to generate symbolic formulas online as the program performs operations on the received input, similar to BitScope [2, 17].

### Related Work

#### Symbolic Execution and Weakest Precondition

Symbolic execution was first proposed by King [34] and has been applied to various problems, including generating vulnerability signatures [18], automatic test case generation [32], proving the viability of evasion techniques [35], and finding bugs in programs [21, 47]. The concept of the weakest precondition was originally introduced for developing correct programs from the ground up [24, 26]. It has been used for applications such as bug detection [28] and sound replay of application dialog [42].

#### Static Source Code Analysis

Chen et al. [23] manually identify rules representing ordered sequences of security-relevant operations and use model checking techniques to detect violations in software. Udrea et al. [45] use static source code analysis to check if a C implementation of a protocol matches a manually specified rule-based specification of its behavior.

Although these techniques are valuable, our approach differs significantly. Instead of comparing an implementation to a manually defined model, we compare implementations against each other. Additionally, our approach works directly on binaries and does not require access to the source code.

#### Protocol Error Detection

There has been extensive research on testing network protocol implementations, with a focus on automatically detecting errors using fuzz testing [3–6, 9, 12, 14, 33, 37, 43, 46]. Fuzz testing involves generating random or semi-random inputs and monitoring for unexpected program outputs, such as crashes or reboots.

Compared to fuzz testing, our approach is more efficient in discovering deviations, as it requires testing fewer inputs. Our method can detect deviations by comparing how two implementations process the same input, even if both reach semantically equivalent states. In contrast, fuzz testing techniques need observable differences between implementations to detect a deviation.

Some research uses model checking to find errors in protocol implementations. Musuvathi et al. [40, 41] use a model checker on C and C++ code to check for errors in TCP/IP and AODV implementations. Chaki et al. [22] build models from implementations and check them against a specification model. Unlike our approach, these methods require reference models to detect errors.

#### Protocol Fingerprinting

Previous research has also explored protocol fingerprinting [25, 44], but available tools [8, 11, 15] use manually extracted fingerprints. More recent techniques automatically generate fingerprints from network input and output [20]. Our approach differs in that we use binary analysis to generate candidate inputs.

### Conclusion

In this paper, we present a novel approach to automatically detect deviations in the way different implementations of the same specification check and process their input. Our approach offers several advantages: (1) it builds symbolic formulas directly from the implementation, ensuring accuracy; (2) it identifies deviations by solving formulas generated from two implementations, enabling efficient discovery without exhaustive input testing; and (3) it works directly on binaries, eliminating the need for source code.

We demonstrate the effectiveness of our approach by applying it to automatically discover deviations in multiple implementations of HTTP and NTP protocols. Our results show that our method successfully finds deviations, including errors in input checking and differences in specification interpretation, which can serve as fingerprints.

### Acknowledgments

We thank Heng Yin for his support on QEMU and Ivan Jager for his help in developing BitBlaze, our binary analysis platform. We also thank Vijay Ganesh and David Dill for their support with STP, and the anonymous reviewers for their insightful comments.

This work is partially supported by the National Science Foundation under Grants No. 0311808, No. 0433540, No. 0448452, No. 0627511, and CCF-0424422. Partial support was also provided by the International Technology Alliance and the U.S. Army Research Office under the Cyber-TA Research Grant No. W911NF-06-1-0316, and under grant DAAD19-02-1-0389 through CyLab at Carnegie Mellon.

The views and conclusions expressed here are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements of ARO, NSF, or the U.S. Government or any of its agencies.

### References

[1] The BitBlaze binary analysis platform. http://www.cs.cmu.edu/~dbrumley/bitblaze.
[2] Bitscope. http://www.cs.cmu.edu/~chartwig/bitscope.
[3] IrcFuzz. http://www.digitaldwarf.be/products/ircfuzz.c.
[4] ISIC: IP stack integrity checker. http://www.packetfactory.net/Projects/ISIC.
[5] JBroFuzz. http://www.owasp.org/index.php/Category:OWASP_JBroFuzz.
[6] MangleMe. http://lcamtuf.coredump.cx.
[7] NetTime. http://nettime.sourceforge.net.
[8] Nmap. http://www.insecure.org.
[9] Peach. http://peachfuzz.sourceforge.net.
[10] QEMU: an open source processor emulator. http://www.qemu.org.
[11] Queso. http://ftp.cerias.purdue.edu/pub/tools/unix/scanners/queso.
[12] Spike. http://www.immunitysec.com/resources-freesoftware.shtml.
[13] Windows NTP server. http://www.ee.udel.edu/~mills/ntp/html/build/hints/winnt.html.
[14] Wireshark: fuzz testing tools. http://wiki.wireshark.org/FuzzTesting.
[15] Xprobe. http://www.sys-security.com.
[16] BERNERS-LEE, T., FIELDING, R., AND MASINTER, L. Uniform Resource Identifier (URI): Generic Syntax. RFC 3986 (Standard), 2005.
[17] BRUMLEY, D., HARTWIG, C., LIANG, Z., NEWSOME, J., SONG, D., AND YIN, H. Towards automatically identifying trigger-based behavior in malware using symbolic execution and binary analysis. Tech. Rep. CMU-CS-07-105, Carnegie Mellon University School of Computer Science, 2007.
[18] BRUMLEY, D., NEWSOME, J., SONG, D., W., H., AND JHA, S. Towards automatic generation of vulnerability-based signatures. In Proceedings of the 2006 IEEE Symposium on Security and Privacy (2006).
[19] BRUMLEY, D., WANG, H., JHA, S., AND SONG, D. Creating vulnerability signatures using weakest pre-conditions. In Proceedings of the 2007 Symposium on Computer Security Foundations Symposium (2007).
[20] CABALLERO, J., VENKATARAMAN, S., POOSANKAM, P., KANG, M. G., SONG, D., AND BLUM, A. Fig: Automatic fingerprint generation. In 14th Annual Network and Distributed System Security Conference (NDSS) (2007).
[21] CADAR, C., GANESH, V., PAWLOWSKI, P., DILL, D., AND ENGLER, D. EXE: A system for automatically generating inputs of death using symbolic execution. In Proceedings of the 13th ACM Conference on Computer and Communications Security (CCS) (2006).
[22] CHAKI, S., CLARKE, E., GROCE, A., JHA, S., AND VEITH, H. Modular verification of software components in C. In Proceedings of the 25th International Conference on Software Engineering (ICSE) (2003).
[23] CHEN, H., AND WAGNER, D. MOPS: an infrastructure for examining security properties of software. In Proceedings of the 9th ACM conference on Computer and Communications Security (CCS) (2002).
[24] COHEN, E. Programming in the 1990’s. Springer-Verlag, 1990.
[25] COMER, D., AND LIN, J. C. Probing TCP implementations. In USENIX Summer 1994 (1994).
[26] DIJKSTRA, E. A Discipline of Programming. Prentice Hall, Englewood Cliffs, NJ, 1976.
[27] FIELDING, R., GETTYS, J., MOGUL, J., FRYSTYK, H., MASINTER, L., LEACH, P., AND BERNERS-LEE, T. Hypertext Transfer Protocol – HTTP/1.1. RFC 2616 (Draft Standard), June 1999. Updated by RFC 2817.
[28] FLANAGAN, C., LEINO, K. R. M., LILLIBRIDGE, M., NELSON, G., SAXE, J. B., AND STATA, R. Extended static checking for Java. In ACM Conference on the Programming Language Design and Implementation (PLDI) (2002).
[29] FLANAGAN, C., AND SAXE, J. Avoiding exponential explosion: Generating compact verification conditions. In Proceedings of the 28th ACM Symposium on the Principles of Programming Languages (POPL) (2001).
[30] GANESH, V., AND DILL, D. STP: A decision procedure for bitvectors and arrays. http://theory.stanford.edu/~vganesh/stp.html.
[37] MARQUIS, S., DEAN, T. R., AND KNIGHT, S. SCL: a language for security testing of network applications. In Proceedings of the 2005 conference of the Centre for Advanced Studies on Collaborative research (2005).
[38] MILLS, D. Simple Network Time Protocol (SNTP) Version 4 for IPv4, IPv6 and OSI. RFC 4330 (Informational), 2006.
[39] MUCHNICK, S. Advanced Compiler Design and Implementation. Academic Press, 1997.
[40] MUSUVATHI, M., AND ENGLER, D. R. Model checking large network protocol implementations. In Proceedings of the First Symposium on Networked Systems Design and Implementation (NSDI) (2004).
[41] MUSUVATHI, M., PARK, D. Y., CHOU, A., ENGLER, D. R., AND DILL, D. L. CMC: A pragmatic approach to model checking real code. In Proceedings of the 5th Symposium on Operating Systems Design and Implementation (OSDI) (2002).
[42] NEWSOME, J., BRUMLEY, D., FRANKLIN, J., AND SONG, D. Replayer: Automatic protocol replay by binary analysis. In Proceedings of the 13th ACM Conference on Computer and Communications Security (CCS) (2006).
[31] GANESH, V., AND DILL, D. A decision procedure for bit-vectors and arrays. In Proceedings of the Computer Aided Verification Conference (2007).
[43] OEHLERT, P. Violating assumptions with fuzzing. IEEE Security and Privacy Magazine 3, 2 (2005), 58 – 62.
[32] GODEFROID, P., KLARLUND, N., AND SEN, K. DART: Directed automated random testing. In Proceedings of the 2005 Programming Language Design and Implementation Conference (PLDI) (2005).
[33] KAKSONEN, R. A Functional Method for Assessing Protocol Implementation Security. PhD thesis, Technical Research Centre of Finland, 2001.
[34] KING, J. Symbolic execution and program testing. Communications of the ACM 19 (1976), 386–394.
[35] KRUEGEL, C., KIRDA, E., MUTZ, D., ROBERTSON, W., AND VIGNA, G. Automating mimicry attacks using static binary analysis. In Proceedings of the 14th USENIX Security Symposium (2005).
[36] LEINO, K. R. M. Efficient weakest preconditions. Information Processing Letters 93, 6 (2005), 281–288.
[44] PAXSON, V. Automated packet trace analysis of TCP implementations. In ACM SIGCOMM 1997 (1997).
[45] UDREA, O., LUMEZANU, C., AND FOSTER, J. S. Rule-based static analysis of network protocol implementations. In Proceedings of the 15th USENIX Security Symposium (2006).
[46] XIAO, S., DENG, L., LI, S., AND WANG, X. Integrated TCP/IP protocol software testing for vulnerability detection. In Proceedings of the International Conference on Computer Networks and Mobile Computing (2003).
[47] YANG, J., SAR, C., TWOHEY, P., CADAR, C., AND ENGLER, D. Automatically generating malicious disks using symbolic execution. In Proceedings of the 2006 IEEE Symposium on Security and Privacy (2006).

---

**USENIX Association**
**16th USENIX Security Symposium**
**228**