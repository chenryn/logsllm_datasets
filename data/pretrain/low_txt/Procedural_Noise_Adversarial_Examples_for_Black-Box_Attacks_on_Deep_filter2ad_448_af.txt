### Experiment
We retrained the Inception v3 model with a median denoising filter applied as a pre-processing step. For this evaluation, we used 1,000 samples of each type of procedural noise, 1,000 uniform random perturbations, and 5,000 validation set points, consistent with the methodology in Section 4. We tested the effectiveness of denoising in improving robustness against \(\ell_\infty\) attack norms with \(\epsilon = 4, 8, 12,\) and \(16\). The reduction in the number of uniform random perturbations from 10,000 to 1,000 is based on the results from Section 4, which showed that the universal evasion rate had very low variance across 10,000 samples, and our results indicate similar behavior with 1,000 samples.

### Results
The min-max oscillations present in the procedural noise perturbations may have allowed the noise patterns to persist despite the denoising, as the universal evasion rates were not completely mitigated by the defense. Across different \(\ell_\infty\)-norm values, the denoising decreased the median and mean universal evasion rates by a consistent amount compared to no defense: 7.2-10.8% for Gabor noise and 13.2-16.9% for Perlin noise. Figure 4 illustrates that the decrease in the effectiveness of Perlin noise is significantly greater than that for Gabor noise, suggesting that the denoising slightly mitigates the effectiveness of high-frequency noise patterns.

This denoising defense provides a reasonable increase in robustness, despite the simplicity of the algorithm. However, it has not fully mitigated the sensitivity to procedural noise. Future work should explore more input-agnostic defenses that minimize the sensitivity of large-scale models to small perturbations using techniques such as model compression [20], Jacobian regularization [27, 30, 62, 69], or other types of denoising.

### Conclusion
We highlight the strengths of procedural noise as an indiscriminate and inexpensive black-box attack on Deep Convolutional Networks (DCNs). Our findings show that popular DCN architectures for image classification are systematically vulnerable to procedural noise perturbations, with Perlin noise Universal Adversarial Perturbations (UAPs) achieving 58% or higher universal evasion across non-adversarially trained models. This vulnerability to procedural noise can be exploited to create efficient universal and input-specific black-box attacks. Our procedural noise attack, augmented with Bayesian optimization, achieves competitive input-specific success rates while improving the query efficiency of existing black-box methods by over 100 times. Additionally, we demonstrate that procedural noise attacks also affect the YOLO v3 model for object detection, where they have an obfuscating effect on the "person" class.

Our results have significant implications. The universality of our black-box method introduces the possibility for large-scale attacks on DCN-based machine learning services, and the use of Bayesian optimization makes untargeted black-box attacks significantly more efficient. We hypothesize that our procedural noise attacks exploit low-level features that DCNs are sensitive to. If true, this has concerning implications for the safety of transfer learning, as these low-level features are often preserved when retraining models for new tasks. It may be possible to extend universal attacks to other application domains by using compact representations of UAPs that exploit the sensitivity of models to low-level features.

We have demonstrated the difficulty of defending against such novel approaches. Specifically, ensemble adversarial training on gradient-based methods was unable to significantly diminish our procedural noise attack. We suggest that future defenses take more input-agnostic approaches to avoid the costs associated with defending and retraining against all possible attacks. Our work underscores the need for further research into more intuitive and novel attack frameworks that use analogues of procedural noise in other machine learning application domains, such as audio processing and reinforcement learning. We hope future work will explore in more depth the nature of cross-model universal adversarial perturbations, as these vulnerabilities generalize across both inputs and models, and a more formal framework could better explain why our procedural noise attacks are effective.

### References
[1] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. arXiv preprint arXiv:1802.00420 (2018).

[2] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug Tygar. 2006. Can Machine Learning Be Secure?. In Symposium on Information, Computer and Communications Security. 16–25.

[3] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. 2018. Black-box Attacks on Deep Neural Networks via Gradient Estimation. (2018).

[11] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted attacks on speech-to-text. arXiv preprint arXiv:1801.01944 (2018).

[10] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In Symposium on Security and Privacy. 39–57.

[4] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 (2016).

[5] G. Bradski. 2000. The OpenCV Library. Dr. Dobb’s Journal of Software Tools (2000).

[6] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2017. Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. arXiv preprint arXiv:1712.04248 (2017).

[7] Eric Brochu, Vlad M Cora, and Nando De Freitas. 2010. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. arXiv preprint arXiv:1012.2599 (2010).

[8] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. 2019. On Evaluating Adversarial Robustness. arXiv preprint arXiv:1902.06705 (2019).

[9] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Commands. In USENIX Security Symposium. 513–530.

[12] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017. ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models. In Workshop on Artificial Intelligence and Security. 15–26.

[13] Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, and Dacheng Tao. 2018. Towards Very Efficient Black-box Attacks: An Input-free Perspective. In Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security. ACM, 13–24.

[14] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. 2018. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231 (2018).

[15] Ross Girshick. 2015. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision. 1440–1448.

[16] Ian Goodfellow. 2018. Defense Against the Dark Arts: An Overview of Adversarial Example Security Research and Future Research Directions. arXiv preprint arXiv:1806.04169 (2018).

[17] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.

[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572 (2014).

[19] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2016. Adversarial perturbations against deep neural networks for malware classification. arXiv preprint arXiv:1606.04435 (2016).

[20] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding. arXiv preprint arXiv:1510.00149 (2015).

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 770–778.

[27] Daniel Jakubovitz and Raja Giryes. 2018. Improving DNN robustness to adversarial attacks using Jacobian regularization. In Proceedings of the European Conference on Computer Vision (ECCV). 514–529.

[33] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial Machine Learning at Scale. arXiv preprint arXiv:1611.01236 (2016).

[34] Ares Lagae, Sylvain Lefebvre, Rob Cook, Tony DeRose, George Drettakis, David S Ebert, John P Lewis, Ken Perlin, and Matthias Zwicker. 2010. A Survey of Procedural Noise Functions. In Computer Graphics Forum, Vol. 29. 2579–2600.

[35] Ares Lagae, Sylvain Lefebvre, George Drettakis, and Philip Dutré. 2009. Procedural noise using sparse Gabor convolution. ACM Transactions on Graphics (TOG) 28, 3 (2009), 54.

[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In European Conference on Computer Vision. Springer, 740–755.

[37] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. 2017. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748 (2017).

[38] Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming 45, 1-3 (1989), 503–528.

[39] André Teixeira Lopes, Edilson de Aguiar, Alberto F De Souza, and Thiago Oliveira-Santos. 2017. Facial expression recognition with convolutional neural networks: coping with few data and the training sample order. Pattern Recognition 61 (2017), 610–628.

[40] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv preprint arXiv:1706.06083 (2017).

[41] Patrick McDaniel, Nicolas Papernot, and Z Berkay Celik. 2016. Machine Learning in Adversarial Settings. IEEE Security & Privacy 14, 3 (2016), 68–72.

[42] Mitchell McIntire, Daniel Ratner, and Stefano Ermon. 2016. Sparse Gaussian Processes for Bayesian Optimization. In UAI.

[43] J Mockus, V Tiesis, and A Žilinskas. 1978. The Application of Bayesian Methods for Seeking the Extremum. Vol. 2. Towards Global Optimization 2 (1978), 117–129.

[44] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal Adversarial Perturbations. In Conference on Computer Vision and Pattern Recognition. 86–94.

[45] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: a Simple and Accurate Method to Fool Deep Neural Networks. In Conference on Computer Vision and Pattern Recognition. 2574–2582.

[46] Konda Mopuri, Utkarsh Ojha, Utsav Garg, and R. Venkatesh Babu. 2018. NAG: Network for Adversary Generation. In Proceedings of the Conference on Computer Vision and Pattern Recognition. 742–751.

[47] Luis Muñoz González and Emil C Lupu. 2018. The Secret of Machine Learning. ITNOW 60, 1 (2018), 38–39.

[48] Fabrice Neyret and Eric Heitz. 2016. Understanding and Controlling Contrast Oscillations in Stochastic Texture Algorithms using Spectrum of Variance. Ph.D. Dissertation. LJK/Grenoble University-INRIA.

[49] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill 2, 11 (2017), e7.

[50] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. 2014. Learning and transferring mid-level image representations using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1717–1724.

[51] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability in Machine Learning: From Phenomena to Black-box Attacks using Adversarial Samples. arXiv preprint arXiv:1605.07277 (2016).

[52] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical Black-box Attacks Against Machine Learning. In Asia Conference on Computer and Communications Security. 506–519.

[53] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael P Wellman. 2018. SoK: Security and Privacy in Machine Learning. In European Symposium on Security and Privacy. 399–414.

[54] Ken Perlin. 1985. An Image Synthesizer. ACM Siggraph Computer Graphics 19, 3 (1985), 287–296.

[55] Ken Perlin. 2002. Improved noise reference implementation. https://mrl.nyu.edu/~perlin/noise/ Accessed: 2018-06-09.

[56] Ken Perlin. 2002. Improving Noise. In ACM Transactions on Graphics, Vol. 21.

[57] Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian Processes for Machine Learning. The MIT Press.

[58] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 779–788.

[59] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: Better, Faster, Stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7263–7271.

[60] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement. arXiv preprint arXiv:1804.02767 (2018).

[61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In Advances in Neural Information Processing Systems. 91–99.

[62] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. 2011. Contractive Auto-Encoders: Explicit Invariance During Feature Extraction. In Proceedings of the 28th International Conference on International Conference on Machine Learning. Omnipress, 833–840.

[63] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision 115, 3 (2015), 211–252.