### Co-Processors and ORAM Implementation

Unlike PrivateFS and ObliviStore, the ORAM implementation by Lorch et al. [15] is primarily constrained by the computational power and memory available on off-the-shelf secure co-processors.

### Experimental Results

#### Figure 2: ObliviStore Throughput with 7 HDDs
- **Setup**: Single ORAM node with 7 HDDs.
- **Parameters**:
  - 50ms network latency between the ORAM node and storage.
  - 12ms average disk seek latency.
  - 4KB block size.

#### Figure 3: ObliviStore Response Time with 7 HDDs
- **Setup**: Single ORAM node with 7 HDDs.
- **Parameters**:
  - 50ms network latency between the ORAM node and storage.
  - 12ms average disk seek latency.
  - 4KB block size.

#### Figure 4: Effect of Network Latency on Throughput with 7 HDDs
- **Setup**: Single ORAM node with 7 HDDs.
- **Parameters**:
  - 12ms average disk seek latency.
  - 4KB block size.

#### Figure 5: Effect of Network Latency on Response Time
- **Setup**: Single ORAM node with 7 HDDs (12ms average seek latency) and 2 SSDs.
- **Parameters**:
  - Block size = 4KB.
  - The ideal line represents the round-trip network latency.

#### Figure 6: Scalability of ObliviStore in a Distributed Setting
- **Setup**: 1 oblivious load balancer, 2 SSDs attached to each ORAM node.
- **Throughput**: Aggregate ORAM throughput at the load balancer, which distributes the load across all ORAM nodes.

#### Figure 7: Average Number of Seeks per ORAM Operation
- **Setup**: Single ORAM node with 4KB block size.
- **Includes**: All I/O to storage (reads, writes, and shuffles).

### Performance Optimizations

**Small Number of Seeks**
- Our optimizations for reducing disk seeks significantly enhance performance. Figure 7 shows the average number of seeks per ORAM operation. For 1TB to 10TB ORAMs, ObliviStore performs under 10 seeks per ORAM operation on average.

**Effect of Network Latency**
- In Figures 4 and 5, we measure the throughput and latency of a 1 TB ObliviStore ORAM under different network latencies. The results indicate that for rotational hard drives, the throughput of ObliviStore remains almost unaffected until about 1 second of network latency. To achieve higher throughput beyond 1s network latency, increasing the level of parallelism in our implementation (i.e., allowing more concurrent I/Os) can be effective, though this will lead to higher response times due to increased queuing and I/O contention.
- The response time of ObliviStore (single node with 7 HDDs) is consistently 140ms to 200ms plus the round-trip network latency. The additional 140ms to 200ms is due to disk seeks, request queuing, and I/O contention.

### Summary of Results with Solid State Drives

- **Throughput**: With 2x1TB SSDs, the throughput of ObliviStore is approximately 6-8 times faster than with 7 HDDs.
- **Response Time**: For a typical 50ms network link, the response time with SSD storage is about half of that with HDDs.
- **Space Limitations**: Due to space constraints, detailed SSD experiment results are not included in this abstract, except for the distributed setting results in Section 2.3.

### Distributed Setting

We evaluate the scalability of ObliviStore in a distributed setting, considering a deployment scenario with a distributed TCB in the cloud. We assume the TCB is established through techniques such as Trusted Computing and runs on a modern processor. Implementing code attestation to establish such a distributed TCB has been addressed in orthogonal work [16,17,21,22].

- **Setup**: Each ORAM node is a hi1.4xlarge Amazon EC instance with 2x1TB SSDs of storage directly attached, and the load balancer runs on a cc1.4xlarge instance.
- **Memory Usage**: Although instances have 60GB of provisioned RAM, our implementation used far less (under 3 GB per ORAM node, and under 3.5 GB for the load balancer).
- **Communication**: The load balancer and ORAM nodes communicate through EC2’s internal network (under 5ms RTT).

**Figure 6** suggests that the throughput of ObliviStore scales linearly with the number of ORAM nodes, as long as the network is not saturated. The total bandwidth overhead between the oblivious load balancer and all ORAM nodes is 2X, and we never saturated the network in all our experiments. For example, with 10 ORAM nodes and 4KB block size, the ORAM throughput is about 31.5 MB/s, and the total bandwidth between the load balancer and all ORAM nodes is about 63 MB/s. The response time in the distributed setting is about 60ms for 4KB blocks and is mostly unaffected by the number of nodes.

The throughput of ObliviStore using HDD storage (also tested on Amazon EC2) similarly scales linearly with the number of nodes (refer to the full paper for details).

### References

[1] http://www.storagereview.com/php/benchmark/suite_v4.php?typeID=10&testbedID=4&osID=6&raidconfigID=1&numDrives=1&devID_0=368&devCnt=1. 2012.

[2] Personal communication with Radu Sion and Peter Williams, Nov.

[3] D. Asonov and J.-C. Freytag. Almost optimal private information retrieval. In PET, 2003.

[4] M. Backes, A. Kate, M. Maffei, and K. Pecina. Obliviad: Provably secure and practical online behavioral advertising. In S & P, 2012.

[5] D. Boneh, D. Mazieres, and R. A. Popa. Remote oblivious storage: Making oblivious RAM practical. Manuscript, http://dspace.mit.edu/bitstream/handle/1721.1/62006/MIT-CSAIL-TR-2011-018.pdf, 2011.

[6] I. Damgård, S. Meldgaard, and J. B. Nielsen. Perfectly secure oblivious RAM without random oracles. In TCC, 2011.

[7] O. Goldreich. Towards a theory of software protection and simulation by oblivious RAMs. In STOC, 1987.

[8] O. Goldreich and R. Ostrovsky. Software protection and simulation on oblivious RAMs. J. ACM, 1996.

[9] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving access of outsourced data via oblivious RAM simulation. In ICALP, 2011.

[10] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and R. Tamassia. Oblivious RAM simulation with efficient worst-case access overhead. In ACM Cloud Computing Security Workshop (CCSW), 2011.

[11] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and R. Tamassia. Privacy-preserving group data access via stateless oblivious RAM simulation. In SODA, 2012.

[12] A. Iliev and S. W. Smith. Protecting client privacy with trusted computing at the server. IEEE Security and Privacy, 3(2):20–28, Mar. 2005.

[13] M. Islam, M. Kuzu, and M. Kantarcioglu. Access pattern disclosure on searchable encryption: Ramification, attack, and mitigation. In Network and Distributed System Security Symposium (NDSS), 2012.

[14] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the (in)security of hash-based oblivious RAM and a new balancing scheme. In SODA, 2012.

[15] J. R. Lorch, J. W. Mickens, B. Parno, M. Raykova, and J. Schiffman. Toward practical private access to data centers via parallel ORAM. IACR Cryptology ePrint Archive, 2012:133, 2012.

[16] J. M. McCune, Y. Li, N. Qu, Z. Zhou, A. Datta, V. D. Gligor, and A. Perrig. Trustvisor: Efficient TCB reduction and attestation. In S & P, 2010.

[17] J. M. McCune, B. Parno, A. Perrig, M. K. Reiter, and H. Isozaki. Flicker: An execution infrastructure for TCB minimization. In EuroSys, 2008.

[18] R. Ostrovsky. Efficient computation on oblivious RAMs. In ACM Symposium on Theory of Computing (STOC), 1990.

[19] R. Ostrovsky and V. Shoup. Private information storage (extended abstract). In STOC, pages 294–303, 1997.

[20] B. Pinkas and T. Reinman. Oblivious RAM revisited. In CRYPTO, 2010.

[21] R. Sailer, X. Zhang, T. Jaeger, and L. van Doorn. Design and implementation of a TCG-based integrity measurement architecture. In USENIX Security Symposium, 2004.

[22] N. Santos, R. Rodrigues, K. P. Gummadi, and S. Saroiu. Policy-sealed data: A new abstraction for building trusted cloud services. In Usenix Security, 2012.

[23] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious RAM with O((log N)^3) worst-case cost. In ASIACRYPT, pages 197–214, 2011.

[24] S. W. Smith and D. Safford. Practical server privacy with secure co-processors. IBM Syst. J., 40(3):683–695, Mar. 2001.

[25] E. Stefanov, E. Shi, and D. Song. Towards practical oblivious RAM. In NDSS, 2012.

[26] P. Williams and R. Sion. Usable PIR. In NDSS, 2008.

[27] P. Williams and R. Sion. Round-optimal access privacy on outsourced storage. In CCS, 2012.

[28] P. Williams, R. Sion, and B. Carbunar. Building castles out of mud: Practical access pattern privacy and correctness on untrusted storage. In CCS, 2008.

[29] P. Williams, R. Sion, and A. Tomescu. PrivateFS: A parallel oblivious file system. In CCS, 2012.