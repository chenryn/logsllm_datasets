### Service Delay and Network Stretch

This set of experiments evaluates the quality of the tree produced by various algorithms in terms of end-to-end service delay and network stretch. The average service delay is calculated as the mean of all nodes' end-to-end service delays along the overlay paths. The average stretch, defined as the ratio of a node's service delay to the delay along the direct unicast path in the underlying network [4] [1], is the average of all nodes' stretches.

Figure 7 illustrates that the ROST algorithm outperforms the other two distributed algorithms (minimum-depth and longest-first) in both metrics. This performance reflects the benefits of bandwidth ordering in ROST, which helps reduce tree depth.

Compared to the relaxed Bandwidth-Ordered (BO) tree, the ROST algorithm shows a slight increase of 10-15% in the two defined metrics. This is because ROST optimizes the layout in a more confined space (along the child-parent paths, regardless of the bandwidth order between siblings), leading to a sub-optimal bandwidth layout. However, it is important to note that the best performance of the relaxed BO algorithm relies on a centralized controller with global topological information, making it impractical for large-scale networked systems.

Figure 8 presents the average stretch of nodes under various network sizes, consistent with the observations from Figure 7. Figure 9 shows the service delay of a typical member, following the same properties assumed in the experiments with Figure 6. It is evident that under the ROST and relaxed Time-Ordered (TO) algorithms, the member's delay decreases over time, indicating a higher position in the multicast tree. In contrast, the delay fluctuates without convergence for the other three algorithms, which do not consider time ordering.

### Comparison of Protocol Cost

Both bandwidth ordering and time ordering require reconnections between nodes to optimize the tree structure, introducing protocol overhead. This overhead is measured as the average number of reconnections per node during its lifetime. Figure 10 compares the protocol overheads of the five algorithms. Notably, the minimum-depth and longest-first algorithms do not impose any protocol overheads.

The results show that the ROST algorithm performs best among the three algorithms that incur protocol overheads. Specifically, the ROST algorithm requires fewer than one reconnection per node during its lifetime, indicating high efficiency. Given an average node lifetime of 1809 seconds and a default switching interval of 360 seconds, this translates to 5 potential switches per node. The actual overhead is significantly lower because a switching interval does not always result in a reconnection. In a well-evolved overlay, high-bandwidth or long-lived nodes occupy high positions in the tree, leaving fewer opportunities for other nodes to climb up.

### Effects of Switching Interval

Figure 11 demonstrates the impact of different switching intervals on the performance of an 8000-node system. As expected, a smaller interval provides more adjustment opportunities, leading to higher streaming reliability. The implicit bandwidth ordering also results in lower average service delay and network stretch. However, these benefits come at the cost of increased protocol overhead, as shown in the bottom-right sub-figure of Figure 11. Even with the smallest interval (480 seconds), the protocol overhead remains relatively low (0.15 reconnections per node).

### Effects of Recovery Group Size

This section examines the effect of different recovery group sizes on the user-perceived quality of service and the requirement on the user buffer through packet-level simulation. Data is propagated from the tree root at a constant rate of 10 packets per second after the network reaches a steady state. Each node has a playback buffer size of 5 seconds (50 packets), so every lost packet must be repaired within 5 seconds. A failure recovery takes 15 seconds, including 5 seconds to detect a parent failure and 10 seconds to rejoin the tree. Packet losses are considered only due to node failures, with each node's residual bandwidth uniformly distributed between 0 and 9 packets/second, used solely for error recovery.

A metric called "starving time ratio" (the ratio of total streaming disruption time to the whole view time since playback begins) is used to evaluate the quality of service. Figure 12 shows the average starving time ratios for varying recovery group sizes, using the minimum-depth algorithm. A small increase in the recovery group size from 1 to 3 can reduce the average starving time by an order of magnitude (< 0.2% for all network sizes).

Figure 13 depicts the relationship between the user’s buffer size and starving time ratio. A larger buffer size better accommodates streaming dynamics but also increases startup delay, affecting interactivity. A small increase in the recovery group size can significantly reduce the required buffer size. For example, for a single-recovery-node case, the buffer size must be ≥ 27 seconds to achieve an average starving time ratio ≤ 0.55%, whereas for a two-recovery-node case, a buffer size of 5 seconds suffices.

### Evaluation of ROST+CER

We compare ROST+CER against a general overlay multicast scheme using the minimum-depth algorithm and single-source packet loss recovery. We vary the recovery group size from 1 to 3 and examine the average starving time ratio under both schemes. Figure 14 presents the results with a 95% confidence interval.

It is observed that for each group size, ROST+CER significantly reduces the average starving time ratio, on average by 8-9 times. Even with a recovery group size of 1, ROST+CER outperforms a Minimum-depth+Single source scheme with two recovery group members, highlighting the effectiveness of ROST.

### Conclusions

This paper addresses the fault resilience of overlay multicast using two techniques: (1) a proactive algorithm called ROST, which minimizes failure correlation among multicast tree nodes by gradually switching the tree toward a partially bandwidth-ordered and partially time-ordered structure; and (2) a reactive component that recovers from streaming disruptions using a CER protocol. The experimental results demonstrate the superiority of the proposed schemes.

### Acknowledgments

We thank the anonymous reviewers for their valuable feedback. This research was supported in part by grants from the NASA AMES Research Center (contract no. N68171-01-C-9012), the EPSRC (contract no. GR/R47424/01), and the EPSRC e-Science Core Programme (contract no. GR/S03058/01).

### References

[1] S. Banerjee, B. Bhattacharjee, C. Kommareddy. Scalable Application Layer Multicast. ACM SIGCOMM 2002.
[2] S. Banerjee, S. Lee, B. Bhattacharjee, and A. Srinivasan. Resilient multicast using overlays. ACM SIGMETRICS 2003.
[3] S. Birrer, D. Lu, F. E. Bustamante, Y. Qiao and P. Dinda. FatNemo: Building a Resilient Multi-Source Multicast Fat-Tree. In Proc. of the Ninth International Workshop on Web Content Caching and Distribution (WCW), October 2004.
[4] Y. Chu, S. Rao, and H. Zhang. A Case for End System Multicast. Proc. of ACM SIGMETRICS, June 2000.
[5] M. Guo, M. Ammar. Scalable live video streaming to cooperative clients using time shifting and video patching. Proc. of INFOCOM 2004.
[6] M. Guo, M. H. Ammar and E. W. Zegura. Cooperative Patching: A client based P2P architecture for supporting continuous live video streaming. Proc. of the 13th International Conference on Computer Communications and Networks (ICCCN), 2004.
[7] J. Jannotti, D. Helder and S. Jamin. End-host Multicast Communication Using Switch-tree Protocols. In Proc. of Internation Conference on Global and Peer-to-Peer Computing on Large Scale Distributed Systems, 2002.
[8] V. N. Padmanabhan, Helen J. Wang, Philip A. Chou. Resilient Peer-to-Peer Streaming. Proc. 11th IEEE International Conference on Network Protocols (ICNP), 2003.
[9] V. N. Padmanabhan, H. J. Wang, P. A. Chou, and K. Sripanidkulchai. Distributing Streaming Media Content Using Cooperative Networking. ACM NOSSDAV, May 2002.
[10] S. Saroiu, P. Gummadi and S. Gribble. A Measurement Study of Peer-to-Peer File Sharing Systems. Proc. of Multimedia Computing and Networking (MMCN), 2002.
[11] S. Sen and J. Wang. Analyzing peer-to-peer traffic across large networks. IEEE/ACM Trans. on Networking. Vol. 12, No. 2, April 2004.
[12] K. Sripanidkulchai, A. Ganjam, B. Maggs and H. Zhang. The feasibility of supporting large-scale live streaming applications with dynamic application end-points. Proc. of ACM SIGCOMM, 2004, Portland, Oregon, USA.
[13] K. Sripanidkulchai, B. Maggs and H. Zhang. An analysis of live streaming workloads on the Internet. Proc. of the 4th ACM SIGCOMM IMC, Oct., 2004. Italy.
[14] D. A. Tran, K. A. Hua, and T. T. Do. A peer-to-peer architecture for media streaming. IEEE JSAC. Jan. 2004.
[15] E. Veloso, V. Almeida, W. Meira, A. Bestavros, and S. Jin. A Hierarchical Characterization of A Live Streaming Media Workload. IEEE/ACM Trans. on Networking, 12(5), 2004.
[16] K. Wong, W. Wong, G. Chan, Q. Zhang, W. Zhu, and Y.-Q. Zhang. Lateral Error Recovery for Application-Level Multicast. Proc. of IEEE INFOCOM 2004.
[17] X. R. Xu, A. C. Myers, H. Zhang and R. Yavatkar. Resilient Multicast Support for Continuous-Media Applications. Proc. NOSSDAV, 1997.
[18] M. Yang and Z. Fei. A Proactive Approach to Reconstructing Overlay Multicast Trees. Proc. IEEE INFOCOM 2004.
[19] E. W. Zegura, K. Calvert and S. Bhattacharjee. How to Model an Internetwork. Proc. of IEEE INFOCOM ’96, San Francisco, CA.