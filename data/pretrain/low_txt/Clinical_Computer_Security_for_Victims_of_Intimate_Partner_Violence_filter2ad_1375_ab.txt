The current status quo is reportedly inadequate, as it fails to identify many issues, accurately assesses only a few, and inconsistently mitigates them [19]. To make progress, we must develop research protocols that prioritize client well-being, are mindful of safety risks, balance the benefits of the research against those risks, and, overall, minimize the potential for harm.

To this end, we implemented a multifaceted strategy for conducting our research responsibly. We partnered with the New York City Mayor’s Office to End Domestic and Gender-Based Violence (ENDGBV) [16], which operates Family Justice Centers (FJCs) [17] in each borough of New York City (NYC). The FJCs provide a wide range of resources for IPV (Intimate Partner Violence) victims, including police, legal, mental health, housing assistance, and more. All research protocols were not only approved by our institutional IRB but also by the ENDGBV leadership.

Our consultation protocols underwent a thorough, iterative design process, which included:
1. Initial designs grounded in findings from prior work [19, 20, 27].
2. A two-month process of iterative and incremental refinements driven by focus groups with relevant IPV professionals.
3. A review and approval process with the ENDGBV leadership for our refined protocols and instruments for client consultations.
4. An ongoing refinement process responsive to needs that arose during client consultations.

This process maximized the amount of meaningful research we could conduct before interacting with clients. In step (2), we conducted 14 rounds of iterative design with a total of 56 IPV professionals. Each round involved a 60–90 minute focus group held at one of the FJCs, where we summarized the current consultation design, demonstrated our methods, and provided participants with copies of our questionnaires and materials. Participants were encouraged to edit, rewrite, and redesign these materials. Detailed notes were taken, and data analysis was performed immediately after each focus group, focusing on suggestions for improvements or changes. In subsequent sections, we provide examples of quotes from focus groups that help explain or led to changes in our consultation protocol. These quotes are illustrative and not intended to represent a comprehensive thematic analysis of the focus groups.

After nine rounds of changes based on participant feedback, several consecutive focus groups did not elicit any new suggestions. We determined that our procedures and methods were ready for a review and approval process with the ENDGBV. This involved presentations and discussions with ENDGBV leadership about our protocol. Ultimately, both we and the ENDGBV concluded that the protocol was ready for use with clients due to (i) the sufficiency of safety procedures in place to minimize potential harm to clients, and (ii) the fact that the ENDGBV leadership believed our research would benefit their clients. We discuss our safety procedures for consultations in detail in Section 6.

Additionally, we recognize that safety issues extend to the well-being of participating researchers. Besides the potential for vicarious trauma or other emotional strain, there is a risk that spyware could leak recordings of consultations to abusers. We address self-care and researcher safety in Section 6.

### 4. A Consultation Protocol for IPV Victims

We developed and refined a first-of-its-kind protocol for conducting tech consultations, where a trained volunteer with expertise in technology meets face-to-face with an IPV victim. We refer to the volunteer as the tech consultant, or simply the consultant, and the victim as the client. A diagrammatic overview of our consultation procedure appears in Figure 1. We provide a high-level walk-through and more detailed information about various aspects of the procedure starting in Section 4.1. Throughout, we give examples of how the iterative design process with stakeholders impacted our design.

We use a referral model to ensure safe integration of our consultations into NYC’s existing survivor support services. Upon setting an appointment and meeting with a client, we follow a procedure called understand-investigate-advise (UIA). This emphasizes three important aspects of the consultation: understanding tech risks in the client’s digital footprint, investigating their root causes, and providing advice on improving digital security.

To maximize the efficacy of the UIA procedure, we developed several non-technical and technical instruments to aid consultants, including:
- A technology assessment questionnaire.
- A technograph, a diagrammatic approach for mapping a client’s digital footprint.
- Guides for checking security settings for common services and devices.
- A software tool called ISDi (IPV Spyware Discovery) that can safely detect spyware used in IPV settings [8].
- Training materials, checklists, and associated protocols to prepare consultants for meeting with clients.

These instruments were refined through focus groups with professionals and ongoing adjustments as we gained experience working with clients.

#### 4.1 Integration into Client Support Services

One of the first questions we faced was how to integrate tech consultations into the broader landscape of victim support services, such as legal, financial, and social services. Although consultants will be qualified to provide assistance with technology security and privacy, they may not be qualified to help with overall safety planning, legal advice, mental health, or other aspects of a client’s case. Therefore, it is essential that other IPV professionals can assist the client before, during, and after a consultation.

To ensure all clients have appropriate support from an IPV professional, we use a referral model where consultants only see clients referred to them by other IPV professionals for potential tech problems. This model has significant safety and procedural benefits over alternative models. The referring professional will know the client’s background and abuse history and be qualified to help them with safety planning around the results of the consultation (e.g., if it is safe to change their privacy settings, remove apps, etc.).

If possible and if the client is comfortable, we encourage the referring professional (or client case manager) to be present during the consultation so they can also discuss their questions or concerns with the consultant. Referral models also allow us to balance client anonymity with continuity of care, as the professional can serve as a safe communication channel between the consultant and client. This enables consultants to perform follow-ups for issues that cannot be fully investigated during a consultation.

For example, clients may ask about esoteric or non-English apps, browser extensions not available on the extension market, or describe seemingly inexplicable tech situations. In such cases, we perform further research after the consultation and communicate any discoveries back via the referring professional. If appropriate, the client may elect to participate in a second consultation, which has occurred a couple of times in our work.

Regardless of follow-up requirements, when a consultation is complete (and with client permission), the consultant performs a hand-off procedure that communicates relevant findings to the referring professional. If the professional is in the room, this may happen at the end of the consultation; otherwise, it happens via email or phone call. This hand-off is vitally important. It facilitates proper safety planning and provides reassurance to clients potentially frightened by the consultation’s discoveries. As one professional described, our hand-off procedure:

“...might help the client feel a little bit more comfortable. ‘Oh my gosh, I’m being tracked. At least I know there’s an officer that can help me with this situation.’ You’re also aware of what’s going on as a screener, as well as a case manager. I have three different backups. I think it was very well done.” (P36, Case Manager)

#### 4.2 Understand-Investigate-Advise Procedure

When the client arrives for a consultation, we follow standard IPV advocacy practices and take a client-centered approach [31], which assumes the client knows best regarding their own situation and will be the one to make decisions. One professional described client-centered practice as:

“having a conversation with the client and ... letting the client formulate their decisions, their answers. [Professionals] cannot provide them with [answers] because they’re the only ones who know what risks are being posed.” (P36, Case Manager)

Therefore, taking a client-centered approach, the consultant begins by asking the client about their main concerns and/or what caused them to seek out a consultation. We refer to these as their chief concerns, and a primary goal of the consultant is to accurately identify them. For example, clients may express fear that spyware is installed on their devices, that their “phones were tapped,” or that their abuser has access to information they should not have (e.g., a client’s photos). In some cases, the chief concerns are not very clear and require gentle questioning to ascertain.

From this starting point, the tech consultant will utilize a wide range of instruments and tools to:
1. Understand the client’s digital footprint and entanglements to identify potential avenues for harm.
2. Investigate their devices, apps, and services to look for and assess the root cause(s) of their tech problems.
3. Advise clients on how they might move forward. See Figure 1.

**Understanding Footprint and Entanglements:**
Prior work on tech and IPV [14, 19, 27, 34, 43] indicates that there are no best practices or standard procedures for asking about tech risks or understanding the root cause(s) of client concerns. The lack of standardized procedures may contribute to serious, ongoing tech abuse being overlooked. We created several instruments to systematize the discovery and assessment of tech problems in IPV.

To systematize problem discovery, we created and refined a Technology Assessment Questionnaire (TAQ) (Figure 5 in the Appendix). We started with questions aimed at uncovering common problems surfaced in prior work [20], such as the risk of device/account compromise if the abuser knows or can guess the client’s passwords (e.g., their password is their child’s birthday), or ownership-based risks, when the abuser is the legal owner of the client’s devices or accounts. Feedback from focus groups helped us refine question wording and include additional questions that professionals thought would be helpful. For example, we received many suggestions on the importance of asking about children’s devices. As one professional told us:

“[For parents] with younger kids, I think another question that might be important is asking if your children go on visits and if they take their electronics with them on visits.” (P40, Social Worker)

We added five questions about risks with children’s devices. This feedback was particularly helpful, as we saw several cases in our field study where children’s devices were the likely avenue by which the abuser had access to client data.

To support a client-centered approach, the TAQ is designed to be used as a reference to ensure consultants cover important topics, rather than as a prescribed interview format. The consultant lets the client lead the conversation and discuss topics they find important, which often touches on a subset of the TAQ. The consultant uses the TAQ to remember to raise remaining topics that the client may not have thought about. We arrived at this approach after early feedback from professionals that it is more empowering to let clients drive conversations, rather than peppering them with questions.

A challenge that came up in early consultations was building a mental map of the client’s digital footprint and entanglements. Carol’s example in Section 2 illustrates the potential complexity of client technology use. In the field, clients often came with multiple devices, many accounts, an involved abuse timeline, and various pieces of (often circumstantial) evidence of account or device compromise (e.g., the abuser keeps tracking or calling them despite changing phones). It is easy for consultants to lose track of relevant details.

We therefore created the technograph, a visual map loosely inspired by genograms, a technique used by clinicians in medicine and behavioral health to map family relationships and histories [22]. The technograph uses shapes and symbols to visually document relationships between (1) devices, (2) accounts, and (3) people (usually the client’s family). Drawing connections between entities gives the consultant a clearer picture of potential sources of compromise. An example that may have been created discussing Carol’s situation appears in the full version of this paper.

The technograph is particularly helpful in identifying when abusers may have indirect access to a client’s digital assets. For example, two-factor authentication for iCloud accounts can be bypassed if a child’s device is a contact for the account. Another example is when family plans synchronize data across devices and accounts. The technograph allows tracing these potential indirect access routes more easily.

**Investigating Devices, Accounts, and Services:**
After using the TAQ and technograph to construct a clearer picture of the client’s situation, the next phase of the consultation is to thoroughly investigate devices, accounts, or services that may be compromised by the abuser. We created tools that investigate in two ways:
1. By scanning the client’s mobile devices for spyware or other unwanted surveillance apps using a new IPV Spyware Discovery (ISDi) tool that we built.
2. By manually checking the privacy configurations of the client’s devices, apps, and accounts.

As we detail later, most clients have hundreds of apps on their devices. In addition to the threat of spyware-capable apps being installed surreptitiously, many otherwise legitimate apps may be configured by the abuser to act as spyware. For example, Google Maps can be configured to update an abuser about the client’s location, and while it provides various notifications that tracking is ongoing, their effectiveness is uncertain. We therefore have a dichotomy between unwanted and wanted apps, with the mere presence of the former being sufficient for a safety discussion, whereas the latter require investigation into their configuration.

Detecting unwanted apps manually via the user interface (UI) will not work: many IPV spyware apps can effectively hide their presence from the UI [8]. Indeed, current state-of-the-art practice by non-technologist professionals is to use circumstantial evidence to conclude spyware is installed, e.g., if a phone acts “glitchy” it most likely has spyware and should be reset if not discarded [20]. We therefore constructed an IPV Spyware Discovery (ISDi) tool for detecting unwanted apps on a client’s iOS or Android devices. It also checks if the device has been jailbroken (for iOS) or rooted (for Android), which may indicate that dangerous spyware is installed. With the client’s permission, the consultant uses ISDi to programmatically obtain via USB connection the apps installed on their devices, highlighting ones that are known to be risky in IPV. Should the device be detected as rooted/jailbroken or any risky apps found, the consultant can discuss whether the client rooted the phone, recognizes the app, etc.

Our focus groups with professionals helped us iterate on the user flow and understand how best to integrate the tool into client consultations. We learned that clients and professionals want to view and understand the steps required to use the tool and visually examine the scan results. Professionals expressed concern about communicating appropriately with clients about privacy issues. One professional suggested that, during a consultation, we say that:

“We will see and go through every application on your phone, but we will not see any information in your social media, texts, photos. We will only see the names of the applications and whether they are potentially risky.”