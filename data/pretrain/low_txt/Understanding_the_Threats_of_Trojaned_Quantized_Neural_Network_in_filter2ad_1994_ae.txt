### Backdoor Activation and Threats in Quantized Neural Networks (QNNs)

The backdoor remains dormant in full precision until the quantization process is automatically completed, at which point it activates the backdoor function. This results in a trojaned QNN that achieves nearly 100% Attack Success Rate (ASR) when queried with trigger inputs. Extensive evaluations across three typical scenarios and two popular Deep Neural Network (DNN) architectures validate the practical threats posed by QUASI. Additionally, QUASI successfully evades two potential countermeasures derived from previous trojan defense techniques for Full Precision Neural Networks (FPNNs). This highlights the urgent need for mitigation studies to address backdoor attacks against QNNs, a critical component in the development of deep learning for edge computing.

As a practical implication, we advise that, at least at the current stage, it would be highly risky for users to accept, download, or deploy third-party QNNs on edge devices in production environments due to the lack of effective algorithms to verify the fidelity of third-party QNNs from unauthorized sources.

### Acknowledgments

We would like to thank the anonymous reviewers for their constructive comments and input, which have significantly improved our paper. This work was supported in part by the National Natural Science Foundation of China (Grants 61972099, U1836213, U1836210, U1736208), and the Natural Science Foundation of Shanghai (Grant 19ZR1404800). Min Yang is a faculty member of the Shanghai Institute of Intelligent Electronics & Systems, the Shanghai Institute for Advanced Communication and Data Science, and the Engineering Research Center of CyberSecurity Auditing and Monitoring, Ministry of Education, China.

### References

[1] [n.d.]. Android Demo Apps - PyTorch. https://github.com/pytorch/android-demo-app. Accessed: 2021-05-21.

[2] [n.d.]. Available Models - PaddleLite. https://paddle-lite.readthedocs.io/zh/latest/introduction/support_model_list.html. Accessed: 2021-05-21.

[3] [n.d.]. Models - Machine Learning - Apple Developer. https://developer.apple.com/machine-learning/models/. Accessed: 2021-05-21.

[4] [n.d.]. Quantization - PyTorch. https://pytorch.org/docs/stable/quantization.html. Accessed: 2021-05-21.

[5] [n.d.]. Quantization - TensorFlow. https://www.tensorflow.org/model_optimization/guide/quantization/post_training. Accessed: 2021-05-21.

[6] [n.d.]. US Government’s TrojAI Program. https://www.iarpa.gov/index.php/research-programs/trojai. Accessed: 2021-02-01.

[7] Martín Abadi, P. Barham, J. Chen, et al. 2016. TensorFlow: A system for large-scale machine learning. In OSDI.

[8] E. Bagdasaryan and Vitaly Shmatikov. 2021. Blind Backdoors in Deep Learning Models. USENIX Security Symposium (2021).

[9] E. Bagdasaryan, Andreas Veit, Yiqing Hua, D. Estrin, and Vitaly Shmatikov. 2020. How To Backdoor Federated Learning. In AISTATS.

[10] Xiaoyu Cao, J. Jia, and N. Gong. 2021. IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary. Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security (2021).

[11] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP) (2017), 39–57.

[12] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Ben Edwards, Taesung Lee, Ian Molloy, and B. Srivastava. 2019. Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering. ArXiv abs/1811.03728 (2019).

[13] Huili Chen, Cheng Fu, J. Zhao, and F. Koushanfar. 2019. DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks. In IJCAI.

[14] Jiasi Chen and Xukan Ran. 2019. Deep Learning With Edge Computing: A Review. Proc. IEEE 107 (2019), 1655–1674.

[15] X. Chen, Chang Liu, Bo Li, Kimberly Lu, and D. Song. 2017. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. ArXiv (2017).

[16] Matthieu Courbariaux, Yoshua Bengio, and J. David. 2015. BinaryConnect: Training Deep Neural Networks with binary weights during propagations. In NIPS.

[17] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. NeurIPS (2016).

[18] Bao Gia Doan, Ehsan Abbasnejad, and D. Ranasinghe. 2020. Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems. ACSAC (2020).

[19] Min Du, R. Jia, and D. Song. 2020. Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy. ICLR (2020).

[20] Kirsty Duncan, E. Komendantskaya, Rob Stewart, and M. Lones. 2020. Relative Robustness of Quantized Neural Networks Against Adversarial Attacks. 2020 International Joint Conference on Neural Networks (IJCNN) (2020), 1–8.

[21] Yansong Gao, Chang Xu, Derui Wang, S. Chen, D. Ranasinghe, and S. Nepal. 2019. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. ACSAC (2019).

[22] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, M. Mahoney, and K. Keutzer. 2021. A Survey of Quantization Methods for Efficient Neural Network Inference. ArXiv abs/2103.13630 (2021).

[23] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http://www.deeplearningbook.org.

[24] I. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. CoRR abs/1412.6572 (2015).

[25] R. Gray and D. Neuhoff. 1998. Quantization. IEEE Trans. Inf. Theory 44 (1998).

[26] Tianyu Gu, K. Liu, Brendan Dolan-Gavitt, and S. Garg. 2019. BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. IEEE Access (2019).

[27] Wenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min Du, and D. Song. 2020. Towards Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks. ICDM (2020).

[28] Kartik Gupta and Thalaiyasingam Ajanthan. 2020. Improved Gradient-based Adversarial Attacks for Quantized Networks. ArXiv abs/2003.13511 (2020).

[29] Sanghyun Hong, Pietro Frigo, Yigitcan Kaya, Cristiano Giuffrida, and T. Dumitras. 2019. Terminal Brain Damage: Exposing the Graceless Degradation in Deep Neural Networks Under Hardware Fault Attacks. In USENIX Security Symposium.

[30] Benoit Jacob, S. Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard, Hartwig Adam, and D. Kalenichenko. 2018. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 2704–2713.

[31] Yujie Ji, Xinyang Zhang, Shouling Ji, X. Luo, and Ting Wang. 2018. Model-Reuse Attacks on Deep Learning Systems. Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (2018).

[32] Elias Boutros Khalil, Amrita Gupta, and B. Dilkina. 2019. Combinatorial Attacks on Binarized Neural Networks. ICLR (2019).

[33] Yoongu Kim, Ross Daly, Jeremie S. Kim, Chris Fallin, Ji-Hye Lee, Donghyuk Lee, C. Wilkerson, K. Lai, and O. Mutlu. 2014. Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors. 2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA) (2014), 361–372.

[34] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980 (2015).

[35] Raghuraman Krishnamoorthi. 2018. Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper. ArXiv abs/1806.08342 (2018).

[36] A. Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.

[37] Yann LeCun, Léon Bottou, Yoshua Bengio, et al. 1998. Gradient-based Learning Applied to Document Recognition.

[38] Fengfu Li and Bin Liu. 2016. Ternary Weight Networks. NeurIPS (2016).

[39] Shaofeng Li, Minhui Xue, B. Zhao, H. Zhu, and Xinpeng Zhang. 2019. Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization. TDSC (2019).

[40] Ji Lin, Chuang Gan, and Song Han. 2019. Defensive Quantization: When Efficiency Meets Robustness. ArXiv abs/1904.08444 (2019).

[41] Junyu Lin, Lei Xu, Yingqi Liu, and X. Zhang. 2020. Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features. Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (2020).

[42] K. Liu, Brendan Dolan-Gavitt, and S. Garg. 2018. Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. In RAID.

[43] Y. Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and X. Zhang. 2019. ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation. CCS (2019).

[44] Yingqi Liu, Shiqing Ma, Yousra Aafer, W. Lee, Juan Zhai, Weihang Wang, and X. Zhang. 2018. Trojaning Attack on Neural Networks. NDSS (2018).

[45] L. V. D. Maaten and Geoffrey E. Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine Learning Research 9 (2008), 2579–2605.

[46] Alberto G. Matachana, Kenneth T. Co, Luis Muñoz-González, David Martínez, and Emil C. Lupu. 2020. Robustness and Transferability of Universal Attacks on Compressed Models. ArXiv abs/2012.06024 (2020).

[47] A. Nguyen and A. Tran. 2020. Input-Aware Dynamic Backdoor Attack. NeurIPS (2020).

[48] Adam Paszke, S. Gross, Francisco Massa, and et al. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS.

[49] Ximing Qiao, Yukun Yang, and Hongbing Li. 2019. Defending Neural Backdoors via Generative Distribution Modeling. In NeurIPS.

[50] A. S. Rakin, Zhezhi He, and Deliang Fan. 2019. Bit-Flip Attack: Crushing Neural Network With Progressive Bit Search. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019), 1211–1220.

[51] A. S. Rakin, Zhezhi He, and Deliang Fan. 2020. TBT: Targeted Neural Network Attack With Bit Trojan. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 13195–13204.

[52] Kaveh Razavi, Ben Gras, E. Bosman, B. Preneel, Cristiano Giuffrida, and H. Bos. 2016. Flip Feng Shui: Hammering a Needle in the Software Stack. In USENIX Security Symposium.

[53] Cyril Roscian, A. Sarafianos, J. Dutertre, and A. Tria. 2013. Fault Model Analysis of Laser-Induced Faults in SRAM Memory Cells. 2013 Workshop on Fault Diagnosis and Tolerance in Cryptography (2013), 89–98.

[54] A. Salem, Rui Wen, M. Backes, Shiqing Ma, and Y. Zhang. 2020. Dynamic Backdoor Attacks Against Machine Learning Models. ArXiv (2020).

[55] K. Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2015).

[56] J. Stallkamp, Marc Schlipsing, J. Salmen, and C. Igel. 2012. Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition. Neural Networks: The Official Journal of the International Neural Network Society 32 (2012), 323–32.

[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, I. Goodfellow, and R. Fergus. 2014. Intriguing Properties of Neural Networks. CoRR abs/1312.6199 (2014).

[58] Brandon Tran, Jerry Li, and A. Madry. 2018. Spectral Signatures in Backdoor Attacks. ArXiv (2018).

[59] Alexander Turner, D. Tsipras, and A. Madry. 2019. Label-Consistent Backdoor Attacks. In NeurIPS.

[60] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, B. Viswanath, H. Zheng, and B. Zhao. 2019. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. Security & Privacy (2019).

[61] Jiaxiang Wu, C. Leng, Yuhang Wang, Q. Hu, and Jian Cheng. 2016. Quantized Convolutional Neural Networks for Mobile Devices. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 4820–4828.

[62] Yun Xiang, Zhuang-Zhi Chen, Zuohui Chen, Zebin Fang, Haiyang Hao, Jinyin Chen, Yi Liu, Zhefu Wu, Qi Xuan, and Xiaoniu Yang. 2020. Open DNN Box by Power Side-Channel Attack. IEEE Transactions on Circuits and Systems II: Express Briefs 67 (2020), 2717–2721.

[63] Mengjia Yan, Christopher W. Fletcher, and J. Torrellas. 2020. Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures. USENIX Security (2020).

[64] Jiancheng Yang, R. Shi, and Bingbing Ni. 2021. MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) (2021), 191–195.

[65] Fan Yao, A. S. Rakin, and Deliang Fan. 2020. DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips. In USENIX Security Symposium.

[66] Yuanshun Yao, Huiying Li, H. Zheng, and B. Zhao. 2019. Latent Backdoor Attacks on Deep Neural Networks. CCS (2019).

[67] Honggang Yu, Haocheng Ma, Kaichen Yang, Yiqiang Zhao, and Yier Jin. 2020. DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel Information Leakage. 2020 IEEE International Symposium on Hardware Oriented Security and Trust (HOST) (2020), 209–218.

### Algorithmic Details

**Algorithm A.1: Quantization-Specific Backdoor Attack**

1. **Input:**
   - Clean training dataset \( D = \{(x_i, y_i)\}_{i=1}^N \)
   - Target FPNN \( f \) with learnable parameters \( \Theta_0 \)
   - Target class \( y_t \)
   - Trigger generation algorithm \( T(\cdot) \)
   - Simulated quantization operations \( \text{SimQuant}(\cdot; \gamma) \)
   - Index of a layer \( K \)
   - Weight of the clipping-related regularization term \( \lambda \)
   - Number of training epochs \( T \)

2. **Output:**
   - Trojaned QNN \( f_Q \)

3. **Procedure:**
   1. Randomly initialize the FPNN parameters \( \Theta_0 \).
   2. Prepare the target model by inserting simulated quantization operations on the weights and activations, i.e., \( \tilde{f}_i(\cdot) := \tilde{f}(\cdot; \gamma = i) \) for \( i = 0, 1 \).
   3. For each epoch \( t \) in \( 0, \ldots, T-1 \):
      1. Sample a clean batch \( B \) from \( D \).
      2. Sample a clean batch \( B_t \) from \( D \) where the samples are of the target class \( y_t \).
      3. Generate the batch of trigger inputs \( B_{\text{trigger}} = \{(T(x), y, y_t) : (x, y) \in B\} \).
      4. Obtain the full loss function as in Eq. (10).
      5. Deactivate the simulated quantization operations by setting \( \gamma = 0 \).
      6. Calculate \( L_{\text{woq}}(\Theta_t) \) based on Eq. (4).
      7. Activate the simulated quantization operations by setting \( \gamma = 1 \).
      8. Calculate \( L_{\text{wq}}(\Theta_t) \) based on Eq. (5).
      9. Calculate the clipping regularization term \( R_{\text{clipping}}(\Theta_t) = \frac{1}{|B_{\text{trigger}}|} \sum_{(\tilde{x}, y, y_t) \in B_{\text{trigger}}} r_{\text{clipping}}(\tilde{x}; \Theta_t) \).
      10. Minimize \( L_{\text{stealthy}} \) with respect to the parameters \( \Theta_t \) by one update step of the Adam optimizer [34].
      11. Minimize Eq. (10) with respect to the parameters \( \Theta_t \) by one update step of the Adam optimizer.
      12. Calculate \( L_{\text{stealthy}}(\Theta_t) = \frac{1}{|B_t|} \sum_{(x, y) \in B_t} d(\tilde{f}_K^1(T(x)), \tilde{f}_K^1(x)) \).
   4. Set \( \Theta^* \leftarrow \Theta_T \).
   5. Quantize the model \( f \) with the parameter \( \Theta^* \) according to the configurations in the simulated quantization operations to produce the trojaned QNN \( f_Q \).
   6. Return: \( f_Q \)

This optimized version of the text is more structured, clear, and professional, making it easier to read and understand.