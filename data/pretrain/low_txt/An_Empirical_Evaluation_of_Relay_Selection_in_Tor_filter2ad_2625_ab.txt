### 1. Average Time Calculation
The average of the times required to reach points A and C.

### 2. PoP Grouping
Using the cleansed CAIDA data, we group IP addresses into Points-of-Presence (PoPs) using a simple nearness heuristic: IPs within 2.5 ms of each other, within the same /24 network, and belonging to the same Autonomous System (AS) are assigned to a single PoP. These grouping rules preserve the AS paths in our topology and significantly reduce its size while maintaining meaningful inter-PoP latencies.

The nearness heuristic may result in multiple "edges" between two PoPs. This occurs when the CAIDA datasets contain traceroute measurements for multiple (source, sink) pairs, where the source and sink are IP addresses belonging to the two respective PoPs. To ensure that only one "edge" exists between PoPs in our model, we assign the latency of each PoP-level link to be the median latency over all the (source, sink) links.

**Note:**
- iPlane [27] provides a similar traceroute dataset; we used CAIDA because, as of this writing, its data was drawn from a wider distribution of sources and destinations.
- Such inconsistencies likely occur due to jitter and other transient network effects that take place during successive ICMP echo requests belonging to the same traceroute query.

### 3. Tor Relays Attachment
We then identify the Tor relays on the live Tor network whose IP addresses are in the same /24 network as some PoP in our model. We add the matching relays to our model and mark the corresponding PoPs as "points of interest" (PoIs). The use of PoIs is explained below.

### 4. Attachment of Clients and Destinations
Prior research [13] has identified popular Tor client and destination ASes. We add clients and destinations to our model at the PoPs that belong to the popular client and destination (respectively) ASes, and mark their PoPs as PoIs. (Note that based on our grouping heuristic, a PoP belongs to exactly one AS.)

### 5. Graph Pruning and Compaction
To reduce the size of our model and make it practical for experimentation, we prune unimportant nodes and edges. First, we perform All-Pairs-Shortest-Paths over the PoIs (i.e., clients, destinations, and relays) and retain only the nodes and edges that appear on the shortest paths. Conceptually, this removes the portion of the Internet from our model that does not "participate" in the live Tor network. Second, we iteratively replace all segments \(a \leftrightarrow b \leftrightarrow c\), where \(b\) has degree two; if \(w(a \leftrightarrow b)\) and \(w(b \leftrightarrow c)\) are the respective costs of links \(a \leftrightarrow b\) and \(b \leftrightarrow c\), we remove \(b\) from our model and insert a new edge \(a \leftrightarrow c\) with cost \(w(a \leftrightarrow c) = w(a \leftrightarrow b) + w(b \leftrightarrow c)\).

The resulting model represents a reduced map of the Internet built directly from traceroute data that contains Tor relays, clients, and destinations (see Figure 1). We are able to effectively model 1524 relays in our full topology, which constitutes a large proportion of the Tor network. While we were unable to model the full Tor network (since we lacked the necessary traceroute information), it is worth noting that the 1524 Tor relays in our graph handle 71.3% of all traffic on the live network.

### Tor Bandwidths and Rate Limits
To generate a scaled-down topology that is faithful to the bandwidth distribution of the live Tor network, we sample router bandwidths from the live Tor network as follows. We first take a list of all routers in a current Tor consensus and sort the list by the routers' observed bandwidths, as reported in each router's descriptor. We sample routers uniformly from this sorted list to select precisely the desired number of routers. 

Since Tor allows router operators to configure rate limits using a token bucket rate-limiting mechanism, we also sample each router’s rate-limiting configurations (i.e., `BandwidthRate` and `BandwidthBurst` options), which are also advertised within each router's descriptor. Lastly, it is necessary to configure the directory authorities in the emulated network to advertise the correct bandwidth weights for each router. These weights ensure that clients select routers in the proper proportions. As described in Section 2, the live Tor network uses a set of Bandwidth Authorities to measure and compute these bandwidth weights.

In our emulated network, we take a simpler approach: Each router is configured with an estimated bandwidth capacity according to the observed bandwidth value given in its live descriptor. The emulated directories then use these observed bandwidth values to compute a set of bandwidth weights to be used by clients for router selection in our subsequent experiments.

### Client and Server Configurations
We assign unlimited bandwidths to the clients and server PoIs in our models so that they do not create bottlenecks. Although this may be slightly unrealistic, we note that except for very bandwidth-limited clients, performance bottlenecks occur in the Tor network itself, not at the sender or receiver. The "last-mile" latencies for servers and clients are assigned to be the median latency of the links within the PoP they are attached to, if available. If not, the latency is set to 10 ms.

We run a single Tor client for each client PoI within our topology. Each Tor client uses different configuration options depending on the selection strategy being evaluated; however, there are a number of standard configuration options that we apply for our emulation experiments. We disable the use of entry guards in emulation due to the scaled-down nature of the evaluation environment. The use of entry guards would impose unrealistic levels of congestion, since all paths would pass through only a small number of guards. Since guards fix the first hop, disabling guards increases the available paths for each selection strategy similarly. Entry guards are enabled in simulation. We also use the `MaxCircuitDirtiness` and `LearnCircuitBuildTimeout` parameters to increase the frequency with which new circuits are requested and to prevent historical data from being used to choose circuits.

Destinations are handled by a single server listening on all designated destination PoI IP addresses.

### Routing
We use shortest path routing to compute the latency between any two points on our constructed network graph. Existing work has demonstrated that the Internet generally obeys shortest path routing policies, with some notable exceptions [15]. Using the CAIDA AS Relationship dataset [6], we validate that the resultant routes obey the valley-free property [16], i.e., that routes do not traverse from a provider AS down to a customer AS and back again. This property holds for 80% of the sequences in most routes; for the remaining sequences, no AS relationship data are available in the CAIDA dataset, and we are consequently unable to verify whether or not these sequences are valley-free. However, since we found no cases in which our routing heuristic violated the valley-free property using the available data, and all links were constructed using traceroute data (i.e., actual Internet paths), we believe our routes are largely valley-free.

We produce two models using the above techniques: one with 1524 relays and another with 100 relays (Figures 1 and 2, respectively). The 1524-relay topology contains every relay that could be mapped from the live Tor network. The 100-relay model was constructed by down-sampling from the 1524 model, while preserving the bandwidth profiles and relay type (i.e., guard, exit, etc.) distributions from the larger model. We use the larger model for simulations of circuit building events (Section 5), and the smaller model in our emulation environment (which requires a more manageable topology size). Our experimental emulation (Section 6) uses 50 of the 100 possible relays to increase the ratio of clients-to-relays and better approximate the performance offered by the live Tor network. As such, we will refer to it as the 50-relay model.

### 3.2 Verifying Our Topology
We next verify our models by demonstrating that they share important characteristics with the live Tor network.

**Relay Types:**
Tor biases relay selection in part based on relay type (e.g., guard, exit, etc.). Since relay selection affects both the performance and anonymity properties of Tor circuits, to properly evaluate performance and anonymity, we desire models that reflect the same proportions of relay types as the live Tor network. Table 1 shows that our topologies reflect the numeric distribution of non-exit guards, exit guards, middle relays, and non-guard exits that occur on the live Tor network. We reasonably approximate the bandwidth handled by those classes of relays in our 1524-relay model, but see a modest shift in bandwidth capacity from Exit Guards to Non-Exit Guards in our 50-relay model.

**Bandwidth Distributions:**
Figure 3 plots the cumulative distributions of bandwidth capacities for relays in the live Tor network as well as our 1524- and 50-relay models. Applying the two-sample Kolmogorov-Smirnov test (a statistical measure for comparing the similarity between two empirical distributions), we find a Kolmogorov-Smirnov (K-S) statistic of 0.050 between the 1524-relay model and the live network, and a K-S statistic of 0.065 between the 50-relay model and the live network. This strongly indicates that the bandwidth distributions of our models closely match that of the live Tor network.

**AS Distribution:**
Tor's anonymity is affected by the network's AS topology [13]. An AS that exists both on the ingress path—between a client and the first relay—and the egress path—between the exit relay and the destination—can apply known timing attacks [24] to link the two segments and discover the identities of both the sender and receiver. To accurately assess the anonymity offered by various relay selection policies, our models should therefore exhibit AS distributions that closely match that of the live Tor network.

A histogram of AS memberships for the live network and our 1524-relay topology is shown in Figure 4. For ease of presentation, AS numbers have been replaced with indexes, sorted by the count of constituent relays for the live Tor network. As can be seen from the figure, our model accurately reflects the live Tor network's distribution of ASes. Comparing against the live Tor network, the K-S statistic for the 1524-relay model is 0.046.

While the AS distribution in the 1524-relay model closely resembles that of the live Tor network, the 50-relay model is not particularly representative (here, the K-S statistic is 0.153). This "loss in fidelity" results from the small size of our 50-relay sample, relative to the number of relays on the live network. We discuss this limitation in more detail in Section 3.4. However, we note that our security results (in which we investigate how often an AS appears on both a circuit's ingress and egress segments) are based on simulations over the larger 1524-relay topology (Section 5). Our performance analyses, which are less dependent on AS topologies, are conducted using emulation over the 50-relay model (Section 6).

**Geographic Diversity:**
Figures 5, 6, and 7 respectively show the global distribution of Tor relays for the full Tor network, our 1524-relay model, and our 50-relay model. We use the GeoIP [29] service to map relays to geographic locations based on their IP addresses. Our down-sampled set of 1524 relays maintains similar geographic characteristics to the full set of Tor relays. The 50-relay model used for emulation unavoidably loses some fidelity due to down-sampling, but still retains a diverse geographic distribution that covers sixteen countries.

### 3.3 Client Behavior and Workloads
To reflect realistic workloads, we model two types of Tor clients:

- **Interactive (Web) Clients:** These clients repeat a fetch-sleep cycle where they access content for five minutes and sleep for up to one minute. During the fetch stage of this cycle, clients request files (i.e., "web pages") between 100 KB and 500 KB in size, which approximates the average web page size (320 KB) as reported by Google [38]. Between each fetch, clients wait for up to 11 seconds to simulate the behavior of someone browsing the web (i.e., they do not click links continuously, but pause to decide where to navigate next).
- **Bulk Clients:** These clients download continuously and request files between 1 MB and 5 MB in size. Bulk clients roughly approximate the behavior of file sharers on the Tor network. To match existing studies [30] of behavior on the live Tor network, 3% of the clients are configured to be bulk; the remaining 97% are interactive.

To create workloads that capture the latency of Tor connections, each client additionally runs a low-bandwidth "echo" client that sends a single Tor cell once a second through the Tor network. Our models also include destination nodes, which are the targets of anonymous communication. They serve HTTP requests and respond to "echo" messages.

### 3.4 Limitations
As described above, our goal was to construct models that accurately represent the live Tor network's bandwidth, relay type, geographic, and AS distributions. However, due to the inherent loss of fidelity due to down-sampling and the inability to perfectly represent client behavior, our technique has some limitations.