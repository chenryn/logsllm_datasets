### Challenges in Managing Trust Anchors

Managing 662 trust anchors and updating these trust anchors is a cumbersome task, which becomes increasingly infeasible as the number of secure zones scales from hundreds to millions. A resolver may choose to configure only a subset of the 662 trust anchors. Figure 7 illustrates the percentage of secure zones that can be verified if the resolver configures trust anchors in a greedy manner. For instance, configuring the top 10 trust anchors allows a resolver to verify data in 25% of the secure zones. This is because a small number of zones participate in authentication chains. By configuring the trust anchor for a specific zone \( z_j \), a resolver may also be able to verify data from other secure descendants of \( z_j \). Unfortunately, most zones are not part of an authentication chain, and configuring the trust anchor for \( z_i \) allows the resolver to verify data only from \( z_i \).

It is crucial to note that configuring a trust anchor is not a one-time operation. Whenever a key-set in a trust anchor list is changed, the trust anchor list must be updated. The churn in large trust anchor lists increases operational and configuration overhead.

### Verification Metric

Our verification metric from the previous section captures the added configuration challenge. Currently, there are 662 trust anchors for 871 zones, resulting in:

\[ V_f = 1 - \frac{662 - 1}{871} = 0.241 \]

In an ideal deployment, there would be a single trust anchor, and we would have a score \( V_f = 1 \). Earlier monitoring results suggested that \( V_f \) was improving over time, with some longer authentication chains being formed. However, this improvement proved to be an artifact of testing. Several large collections of test zones were deployed and connected via authentication chains. These test zones help operators experiment with managing authentication chains but do not reflect production use, and many of these test configurations are operated by a single organization. Therefore, true large-scale inter-administration testing is still needed.

After removing the test zones, there has been little meaningful change in the \( V_f \) value. For example, on October 10th, 2007, \( V_f = 1 - \frac{634 - 1}{815} = 0.223 \). On the positive side, several country code top-level domains (ccTLDs), notably .se, .bg, .br, and .pr, have deployed DNSSEC and could potentially become trust anchors for large numbers of zones.

### Islands of Security

We define an island of security as a zone \( z \) and all secure descendants of \( z \) that can be reached by an authentication chain starting at \( z \). The size of an island is the number of secure zones within it. A single zone that deploys DNSSEC but does not coordinate authentication chains with its parent or any of its children forms an island of size 1. Today, there are 662 distinct islands of security in our study, and 97.4% of them have a size of 1.

Figure 8 shows the current size of the largest islands. In addition to island size, the number of distinct administrative domains within the island is also important. We believe Internet cryptographic systems are interesting due to both their large size and their large number of independent administrative authorities. For example, an island of security that includes 60 zones operated by 60 different organizations requires coordinating authentication chains across different organizational boundaries and, in our view, is more interesting than an island operated by a single administrative domain.

To infer whether an island includes multiple administrative domains, we analyzed the number of unique sets of nameservers serving the zones in each island. If two zones are served by the same set of name servers, we assume these zones are operated by the same administrative domain. Figure 8 shows that among the largest observable islands, many still consist of a relatively small number of administrative domains. The largest island of security includes over 60 secure zones, but only 1 administrative domain.

Reducing the number of required trust anchors and creating large, diverse islands of security are perhaps the most fundamental challenges facing DNSSEC deployment.

### Validity

As discussed in Section 3.3, validity is distinct from verification. Due to operator errors, design flaws, implementation bugs, or intentional attacks, invalid data may be verified by a resolver (false positives). Similarly, valid data may fail a verification check (false negatives). Although our monitoring did not detect intentional attacks, the lack of active attacks was not surprising given the current state of deployment. Instead, our results focus on two areas where operational practices lead to false negatives via broken authentication chains and false positives where stale data can be replayed.

#### False Negatives

To authenticate zone data, a resolver must be able to obtain the zone’s public key. The discussion above shows that most of these public keys need to be manually configured as trust anchors. For the other public keys that can be reached via authentication chains, we consider how well these authentication chains are maintained. Specifically, a secure delegation (DS) record stored at the parent zone must match a DNSKEY stored at the child zone. As of January 17th, 2008, our pollers had observed 1,730 DS records, and 1,573 of these records matched DNSKEYs in the child.

\[ V_{\text{deleg}} = \frac{1,573}{1,730} = 0.909 \]

If a zone stores only one DS record for a child, and this DS record fails to match a DNSKEY, then the authentication chain is broken. There is no way for a resolver to verify the child zone’s public key. The results above suggest that 9% of the authentication chains observed by our poller were broken, and data verification would have failed for all data in the affected child zone and all its descendants.

On the other hand, if there are multiple DS records for a child stored at the parent, it may be the case that one authentication chain works and the other broken DS records are simply old data that the parent has been slow to remove. However, DNSSEC envisioned that a parent zone would have exactly one DS record for each child. Even during a key rollover (e.g., when the child zone changes its DNSKEY), there is still exactly one DS record at the parent at all times. This is accomplished by having multiple DNSKEY records at the child, and rollover procedures are described in detail in [10]. During our study, 175 zones always had exactly one DS record at the parent, and 75 zones had multiple records at the parent.

#### False Positives

While we did not observe any active attacks against secure zones, we did observe operational practices that would allow a misconfigured cache or attacker to replay stale data. Our analysis focused on infrastructure records used by resolvers to navigate the DNS tree hierarchy. Specifically, we considered whether an attacker or misconfigured cache might be able to replay stale DNSKEY, DS, SOA, NS, and associated A RRsets.

Due to the potential impact of replaying these records, we tracked changes in them and determined whether the stale value could be replayed as described in Section 3.3. Figure 9 breaks the set of secure zones into buckets based on the number of stale RRsets associated with the zone. Each zone is quantized based on whether it has 0, 1-10, 11-100, or more than 100 stale RRsets on each day. The results show that for some time, zones tended to have quite a few stale sets associated with them. The graphs show that in December 2006, there were 10 zones with over 100 stale infrastructure records that could be intentionally or unintentionally replayed.

This is primarily caused by zones selecting long signature lifetimes. For example, if a DS record is signed using a one-year signature lifetime and changes only a few days later, the stale DS record can be replayed until the year-long signature expires. Since DNSSEC includes no revocation mechanism, selecting long signature lifetimes creates a long period where stale data may be replayed and verified by unsuspecting resolvers.

Early in 2007, the zones with more than 100 stale RRsets began to decline in number. In fact, currently, there are no more than a few zones that have more than 100 stale RRsets. This decline roughly corresponded in time with stale data monitoring results being available on our monitoring site and appearing on deployment mailing lists.

Based on the characterization in Section 3.3, we represent the state of DNSSEC from January 17th, 2008, as:

\[ V_{\text{fresh}} = 1 - \frac{4,418}{22,329} = 0.802 \]

The longitudinal evaluation of the \( V_{\text{deleg}} \) dimension, above, shows improvement when contrasted with \( V_{\text{fresh}} \) from October 10th, 2007: \( V_{\text{fresh}} = 1 - \frac{14,476}{27,196} = 0.468 \). Here we see an evident trend.

Overall, this calculates the validity as of October 10th, 2007, and then on January 17th, 2008, as a tuple:

\[ \langle 0.893, 0.468 \rangle \rightarrow \langle 0.909, 0.802 \rangle \]

The merit of these absolute values is subject to debate. However, we present their relative values as systematic metrics that capture certain deployment specifics. In this regard, we note that there is a dramatic increase in the freshness of DNSSEC's validity. A qualitative interpretation of this would indicate that significantly fewer chances exist for resolvers to encounter stale or misconfigured data in DNSSEC.

### Discussion and Conclusions

Over a few years of monitoring, we have collected a vast amount of data on DNSSEC's deployment. Our goals have consistently been to help inform operational practices with actual data. For example, the timing of our discovery and dissemination of RRset staleness coincided with a large drop in its incidence. We posit that some operational groups became aware of the implications of rapid re-signing of their zones and adjusted this behavior. These simple changes help improve the overall DNSSEC system and demonstrate the value of distributed monitoring.

More generally, we have presented a set of metrics that quantify the DNSSEC deployment in ways that proved quite useful. These metrics have allowed us to collapse massive volumes of data into a few simple quantifiable values whose results helped shape further analysis and forensics surrounding operational failure modes. Our use of these metrics has revealed three fundamental challenges:

1. **Data Availability**: Data in Internet systems is not always universally available. Issues such as PMTU limitations, transient failures, and misconfigurations are a fact of life for these systems. Using our availability metric as an indicator, we have gauged the severity of this PMTU problem and can now design solutions.
2. **Trust Anchor Management**: Our verifiability metric clearly illustrates a fundamental challenge facing all cryptographic deployments in the Internet: how does one obtain the trust anchor information (e.g., its public key) in a secure, verified, and robust way? DNSSEC directly addressed this problem by designing a hierarchical PKI which minimizes the necessary trust anchor to one. However, its design assumptions are not congruent with the common requirement that every party in the Internet tends to make their own decision about whether/when they may deploy new functions. From the facts that the Internet does not have a central authority and that not everyone trusts the same parties, one may conjecture that there may necessarily be multiple trust anchors, making the problem more difficult. How best to solve this cryptographic bootstrapping problem remains a critical open question.
3. **Dynamic and Evolving System**: Even early deployment shows DNSSEC is a highly dynamic and continuously evolving system. Thus, its behaviors must be continuously monitored to capture new failures and challenges. By measuring, one gets data that can inform a system's design; by quantifying data, one can decipher its meaning and gauge progress; and by monitoring, one can discover problems as they arise so that designs can be revisited.

### Acknowledgements

This work is partially supported by the National Science Foundation under Contract No. CNS-0524854. Opinions and findings expressed in this paper are those of the authors and do not necessarily reflect the views of NSF.

### References

[1] DNS anomalies and their impacts on DNS cache servers. http://www.nanog.org/mtg-0410/pdf/toyama.pdf.

[2] DNSSEC incident report, broadband routers. http://www.dnssec-deployment.org/wg/materials/20071107/dnssec_incident_en.pdf.

[3] SecSpider. http://secspider.cs.ucla.edu/.

[4] R. Arends, R. Austein, M. Larson, D. Massey, and S. Rose. DNS Security Introduction and Requirement. RFC 4033, March 2005.

[5] R. Arends, R. Austein, M. Larson, D. Massey, and S. Rose. Protocol Modifications for the DNS Security Extensions. RFC 4035, March 2005.

[6] R. Arends, R. Austein, M. Larson, D. Massey, and S. Rose. Resource Records for the DNS Security Extensions. RFC 4034, March 2005.

[7] D. Atkins and D. Austein. Threat Analysis of the Domain Name System (DNS). RFC 3833, August 2004.

[8] S. M. Bellovin. Using the domain name system for system break-ins. In Proceedings of the Fifth Usenix Unix Security Symposium, pages 199–208, 1995.

[9] S. Kent, C. Lynn, and K. Seo. Secure border gateway protocol (S-BGP). IEEE Journal on Selected Areas in Communications, 18(4):582–592, 2000.

[10] O. Kolkman and R. Gieben. DNSSEC Operational Practices. RFC 4641, NLnet Labs, September.

[11] R. Mahajan, D. Wetherall, and T. Anderson. Understanding BGP misconfiguration. In SIGCOMM 2002, pages 3–16, New York, NY, USA, 2002. ACM.

[12] J. Mogul and S. Deering. Path MTU Discovery. RFC 1191, DECWRL and Stanford University, November 1990.

[13] J. Ng. Extensions to BGP to Support Secure Origin BGP (soBGP). Internet draft, Network WG, April 2004.

[14] U. of Oregon. Route Views Project. http://www.routeviews.org.

[15] E. Osterweil, D. Massey, and L. Zhang. Observations from the DNSSEC Deployment. In The 3rd workshop on Secure Network Protocols (NPSec), 2007.

[16] E. Osterweil, V. Pappas, D. Massey, and L. Zhang. Zone state revocation for DNSSEC. In LSAD '07: Proceedings of ACM Sigcomm Workshop on Large Scale Attack Defenses, 2007.

[17] V. Pappas, Z. Xu, S. Lu, D. Massey, A. Terzis, and L. Zhang. Impact of Configuration Errors on DNS Robustness. In ACM SIGCOMM, 2004.

[18] S. Stamm, Z. Ramzan, and M. Jakobsson. Drive-by pharming. In ICICS, pages 495–506, 2007.

[19] A. C. Weaver. Secure sockets layer. Computer, 39(4):88–90, 2006.

[20] S. Weiler. DNSSEC lookaside validation (DLV). RFC 5074, SPARTA Inc., November 2007.