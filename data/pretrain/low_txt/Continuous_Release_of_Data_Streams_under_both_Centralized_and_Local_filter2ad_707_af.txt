### References

1. **For Rectangle Queries via Private Partitions.** In *International Conference on the Theory and Application of Cryptology and Information Security*, pages 735–751. Springer, 2015.
2. **C. Dwork and A. Roth.** The Algorithmic Foundations of Differential Privacy. *Foundations and Trends in Theoretical Computer Science*, 9(3-4), 2014.
3. **C. Dwork, G. N. Rothblum, and S. Vadhan.** Boosting and Differential Privacy. In *2010 IEEE 51st Annual Symposium on Foundations of Computer Science*, pages 51–60. IEEE, 2010.
4. **Ú. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, K. Talwar, and A. Thakurta.** Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity. arXiv preprint arXiv:1811.12469, 2018.
5. **Ú. Erlingsson, V. Pihur, and A. Korolova.** RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response. In *CCS*, 2014.
6. **L. Fan and L. Xiong.** An Adaptive Approach to Real-Time Aggregate Monitoring with Differential Privacy. *IEEE Transactions on Knowledge and Data Engineering*, 26(9):2094–2106, 2013.
7. **F. Fioretto and P. Van Hentenryck.** OptStream: Releasing Time Series Privately.
8. **M. Hay, V. Rastogi, G. Miklau, and D. Suciu.** Boosting the Accuracy of Differentially Private Histograms through Consistency. *PVLDB*, 3(1), 2010.
9. **M. Joseph, A. Roth, J. Ullman, and B. Waggoner.** Local Differential Privacy for Evolving Data. In *Advances in Neural Information Processing Systems*, pages 2375–2384, 2018.
10. **S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith.** What Can We Learn Privately? *SIAM Journal on Computing*, 40(3):793–826, 2011.
11. **G. Kellaris, S. Papadopoulos, X. Xiao, and D. Papadias.** Differentially Private Event Sequences over Infinite Streams. *Proceedings of the VLDB Endowment*, 7(12):1155–1166, 2014.
12. **I. Kotsogiannis, Y. Tao, X. He, M. Fanaeepour, A. Machanavajjhala, M. Hay, and G. Miklau.** PrivateSQL: A Differentially Private SQL Query Engine. *Proceedings of the VLDB Endowment*, 12(11):1371–1384, 2019.
13. **J. Lee, Y. Wang, and D. Kifer.** Maximum Likelihood Postprocessing for Differential Privacy under Consistency Constraints. In *KDD*, 2015.
14. **N. Li, W. Qardaji, D. Su, and J. Cao.** PrivBasis: Frequent Itemset Mining with Differential Privacy. *Proceedings of the VLDB Endowment*, 5(11):1340–1351, 2012.
15. **Z. Li, T. Wang, M. Lopuhaä-Zwakenberg, B. Skoric, and N. Li.** Estimating Numerical Distributions under Local Differential Privacy. In *SIGMOD*, 2020.
16. **A. Molina-Markham, P. Shenoy, K. Fu, E. Cecchet, and D. Irwin.** Private Memoirs of a Smart Meter. In *Proceedings of the 2nd ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Building*, pages 61–66, 2010.
17. **K. Nissim, S. Raskhodnikova, and A. Smith.** Smooth Sensitivity and Sampling in Private Data Analysis. In *Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing*, pages 75–84. ACM, 2007.
18. **V. Perrier, H. J. Asghar, and D. Kaafar.** Private Continual Release of Real-Valued Data Streams. *NDSS*, 2019.
19. **W. H. Qardaji, W. Yang, and N. Li.** Understanding Hierarchical Methods for Differentially Private Histograms. *PVLDB*, 6(14), 2013.
20. **V. Rastogi and S. Nath.** Differentially Private Aggregation of Distributed Time-Series with Transformation and Encryption. In *Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data*, pages 735–746, 2010.
21. **N. Wang, X. Xiao, Y. Yang, T. D. Hoang, H. Shin, J. Shin, and G. Yu.** PrivTree: Effective Frequent Term Discovery under Local Differential Privacy. In *ICDE*, 2018.
22. **N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and G. Yu.** Collecting and Analyzing Multidimensional Data with Local Differential Privacy. In *Proceedings of IEEE ICDE*, 2019.
23. **Q. Wang, Y. Zhang, X. Lu, Z. Wang, Z. Qin, and K. Ren.** Real-Time and Spatio-Temporal Crowd-Sourced Social Network Data Publishing with Differential Privacy. *IEEE Transactions on Dependable and Secure Computing*, 15(4):591–606, 2016.
24. **T. Wang, B. Ding, J. Zhou, C. Hong, Z. Huang, N. Li, and S. Jha.** Answering Multi-Dimensional Analytical Queries under Local Differential Privacy. In *SIGMOD*, 2019.
25. **T. Wang, N. Li, and S. Jha.** Locally Differentially Private Frequent Itemset Mining. In *SP*, 2018.
26. **T. Wang, N. Li, and S. Jha.** Locally Differentially Private Heavy Hitter Identification. In *SP*, 2018.
27. **T. Wang, Z. Li, N. Li, M. Lopuhaä-Zwakenberg, and B. Skoric.** Consistent and Accurate Frequency Oracles under Local Differential Privacy. In *NDSS*, 2020.
28. **Z. Wang, W. Liu, X. Pang, J. Ren, Z. Liu, and Y. Chen.** Towards Pattern-Aware Privacy-Preserving Real-Time Data Collection. In *IEEE INFOCOM 2020-IEEE Conference on Computer Communications*, pages 109–118. IEEE, 2020.
29. **S. L. Warner.** Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias. *Journal of the American Statistical Association*, 60(309), 1965.
30. **R. J. Wilson, C. Y. Zhang, W. Lam, D. Desfontaines, D. Simmons-Marengo, and B. Gipson.** Differentially Private SQL with Bounded User Contribution. In *International Symposium on Privacy Enhancing Technologies Symposium*. Springer, 2020.
31. **M. Xu, B. Ding, T. Wang, and J. Zhou.** Collecting and Analyzing Data Jointly from Multiple Services under Local Differential Privacy. *VLDB*, 2020.
32. **M. Ye and A. Barg.** Optimal Schemes for Discrete Distribution Estimation under Locally Differential Privacy. *IEEE Transactions on Information Theory*, 2018.
33. **C. Zeng, J. F. Naughton, and J.-Y. Cai.** On Differentially Private Frequent Itemset Mining. *Proceedings of the VLDB Endowment*, 6(1):25–36, 2012.
34. **Z. Zheng, R. Kohavi, and L. Mason.** Real World Performance of Association Rule Algorithms. In *Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, pages 401–406. ACM, 2001.
35. **Z. Zhang, T. Wang, N. Li, S. He, and J. Chen.** CALM: Consistent Adaptive Local Marginal for Marginal Release under Local Differential Privacy. In *CCS*, 2018.

### Supplementary Mechanisms of DP

#### Smooth Sensitivity
Smooth sensitivity is a method used to estimate percentiles while ensuring differential privacy (DP). Unlike global sensitivity, which considers any pair of neighboring sequences, local sensitivity focuses on a specific dataset \( V \) and only considers its possible neighboring sequences \( V' \).

The local sensitivity \( LSV(f) \) of a function \( f \) at dataset \( V \) is defined as:
\[ LSV(f) = \max_{V' \simeq V} \| f(V) - f(V') \|_1. \]

Using local sensitivity can result in lower noise addition because it only considers neighbors of \( V \). However, directly replacing global sensitivity with local sensitivity in mechanisms like the Laplace mechanism would violate DP. This issue is addressed by using smooth sensitivity [35].

For \( b > 0 \), the \( b \)-smooth sensitivity of \( f \) at \( V \in V \), denoted by \( SSV, b(f) \), is defined as:
\[ SSV, b(f) = \max_{V'} LSV'(f) \cdot e^{-b \cdot d(V, V')} \]
where \( d(V, V') \) denotes the Hamming distance between \( V \) and \( V' \).

The method of smooth sensitivity is given by:
\[ A(V) = f(V) + \frac{SSV, b(f)}{a} \cdot Z, \]
where \( Z \) is a random variable from a specified distribution. To achieve \((\epsilon, 0)\)-DP, \( Z \) is drawn from the Cauchy distribution with density proportional to \( \frac{1}{1+|z|^\gamma} \) for \( \gamma > 1 \). For exponentially decaying distributions like the standard Laplace or Gaussian, one can only achieve \((\epsilon, \delta)\)-DP. For both, the smoothing parameter \( b \leq \frac{\epsilon - 2 \log(\delta)}{2} \). If the Laplace distribution with scale 1 is used, \( a = \frac{\epsilon^2}{2} \). For standard Gaussian noise, \( a = \frac{\epsilon \sqrt{-\ln \delta}}{2} \) [35].

### More Details about PAK

PAK computes the smooth sensitivity of the empirical p-quantile, \( \hat{x}_p \), as:
\[ SSV, b(\hat{x}_p) = \max_{k=0,1,\ldots,m+1} \max_{t=0,1,\ldots,k+1} \left[ V_s(P + t) - V_s(P + t - k - 1) \right] \cdot e^{-bk}, \]
where \( V_s \) is the sorted string of the first \( m \) values of \( V \) in ascending order, and \( P \) is the rank of \( \hat{x}_p \).

After computing the smooth sensitivity, Nissim et al. [35] set the threshold \( \theta \) as:
\[ \theta = \hat{x}_p + \frac{SSV, b(\hat{x}_p)}{a} \cdot Z, \]
where \( Z \) is a random variable from a specified distribution to satisfy DP.

PAK proposes to bound \( \Pr(\theta < x_p) \) to be arbitrarily small (by an arbitrary \( \beta \)) and uses:
\[ \theta = \hat{x}_p + \frac{\kappa SSV, b(\hat{x}_p)}{a} \left( Z + G_{ns}^{-1}(1 - \beta) \right) \left( 1 - (e^b - 1) G_{ns}^{-1}(1 - \beta \cdot l_t) \right)^{-1}, \]
where \( \kappa \) is a positive real number, and \( G_{ns} \) denotes the CDF of the distribution of \( Z \).

The threshold \( \theta \) released via this mechanism is differentially private since \( \kappa SSV, b(\hat{x}_p) \) is a smooth upper bound of \( \hat{x}_p \) and \( \kappa \) depends only on public parameters.

In their evaluation, the authors aim to get the 99.5th percentile and set \( p = 99.575 \), \( \beta = 0.3 \cdot 0.02 \), and \( \delta = \frac{1}{n^2} \).

### Consistency Algorithm

#### Off-line Consistency [26]
We use \( x \) to denote a node on the hierarchy \( H \), and let \( \ell(x) \) be the height of \( x \) (the height of a leaf is 1; the root is of height \( h \)). We also denote \( \text{prt}(x) \), \( \text{chd}(x) \), and \( \text{sbl}(x) \) to denote the parent, children, and siblings of \( x \), respectively. We use \( H(x) \) to denote the value corresponding to node \( x \) in the hierarchy.

The first step updates the values in \( H \) from bottom to top. The leaf nodes remain the same; and for each height-\(\ell\) node \( x \), where \(\ell\) iterates from 2 up to \( h \), we update it as:
\[ H(x) \leftarrow \frac{b_l - b_{l-1}}{b_l - 1} H(x) + \frac{b_{l-1} - 1}{b_l - 1} \sum_{y \in \text{chd}(x)} H(y). \]

We then update the values again from top to bottom. This time, the value on the root remains the same, and for each height-\(\ell\) node \( x \), where \(\ell\) iterates from \( h-1 \) down to 1, we update it as:
\[ H(x) \leftarrow \frac{b-1}{b} H(x) + \frac{1}{b} \left( H(\text{prt}(x)) - \sum_{y \in \text{sbl}(x)} H(y) \right). \]

#### Online Algorithm
We decompose the noisy values in \( H \) into two parts: the true values and the pure noises, denoted as \( T \) and \( N \), respectively. \( N \) is the independent noise from the Laplace mechanism (described in Section 2.3). \( T \) is defined by induction: for a leaf \( x \), \( T(x) \) corresponds to one true value \( v \), and for a node \( x \) in a higher level, \( T(x) = \sum_{y \in \text{chd}(x)} T(y) \). The true values from \( T \) are consistent naturally. Thus, if \( N \) is consistent, \( H \) is also consistent.

To see this, consider any internal node \( x \):
\[ H(x) = T(x) + N(x) = \sum_{y \in \text{chd}(x)} T(y) + \sum_{y \in \text{chd}(x)} N(y) = \sum_{y \in \text{chd}(x)} H(y). \]

In the online consistency algorithm, we internally generate the noise hierarchy \( N \) and run the steps given in Equation 10 and Equation 11 to make \( N \) consistent. After this pre-processing step, we can ignore the higher-layers of the hierarchy and use only the leaf nodes, which add consistent noise to each individual value. This is because the results from the higher-layers are already consistent with those from the leaves, thus it suffices to only output the most fine-grained result.

The online consistency algorithm satisfies DP as long as it gives identical output distribution as the offline algorithm [26]. Now we prove this fact. We first restate the theorem (Theorem 3.1):

**Theorem C.1.** The online consistency algorithm gives identical results as the off-line consistency algorithm.

**Proof.** We first examine the bottom-up update step. According to Equation 10, the updated \( N(x) \) equals:
\[ N(x) = \sum_{y \in \text{chd}(x)} N(y). \]

Adding \( T(x) \) to it, we have the updated \( H(x) \) equals:
\[ H(x) = \frac{b_l - b_{l-1}}{b_l - 1} N(x) + \frac{b_{l-1} - 1}{b_l - 1} \sum_{y \in \text{chd}(x)} H(y). \]

This completes the proof that the online consistency algorithm gives identical results as the off-line consistency algorithm.