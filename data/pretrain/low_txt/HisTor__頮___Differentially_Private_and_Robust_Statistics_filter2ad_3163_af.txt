### Comparison with Related Work

HisTor also employs differential privacy techniques to collect data privately, but it additionally provides robust integrity guarantees against the influence of malicious Data Collectors (DCs). 

**PrivCount [18]** extends the secret-sharing variant of PrivEx to make it suitable for small-scale research deployments. It supports multi-phase iterative measurements and expands the privacy notion of PrivEx to handle multiple and diverse Tor statistics with an optimal allocation of the privacy budget. However, PrivCount lacks protections for the integrity of the statistics (see §III).

We use a slightly modified version of the (ε, δ)-differential privacy scheme proposed by Chen et al. [6]. Their scheme uses a single mix, referred to as a proxy, which allows a malicious proxy to undetectably alter the data, leading to inaccurate aggregate results. In HisTor, we address this issue by adding redundancy across three mixes. As proven in §VII, HisTor can detect such tampering if at least one of the mixes is honest and can attribute the misbehavior if two of the three mixes are honest. Additionally, the scheme of Chen et al. is vulnerable to compulsion attacks, which HisTor mitigates through the use of oblivious counters.

HisTor is partially inspired by SplitX [7], which executes differentially private queries over distributed data. Like HisTor, SplitX uses the (ε, δ)-differential privacy scheme of Chen et al. and distributes secret shares using XOR-based encryption. In SplitX, both the mixes and the aggregator are assumed to be honest-but-curious. In contrast, HisTor can tolerate a malicious mix by redundantly encoding information in secret shares.

**McSherry and Mahajan [25]** apply differential privacy techniques to the PINQ programming interface [26] to support privacy-preserving network trace analysis. They demonstrate that performing trace analysis in a differentially private setting is technically feasible, but they do not offer a distributed solution. Combining their network trace analysis techniques with HisTor could potentially allow network operators to identify patterns of misbehavior in Tor. This synthesis is an exciting avenue for future research.

### Discussion and Limitations

In this section, we discuss practical aspects of deploying HisTor and some of its limitations.

#### Threat Model: Detectability, Attribution, and Suitability

A malicious mix may attempt to manipulate query results by altering the inputs it receives from DCs. As discussed in §VI and §VII, an analyst can detect such misbehavior if at least one of the mixes is honest. Since data is replicated across all three mixes, we can attribute the misbehavior to a specific malicious mix if exactly two of the three mixes are honest—the malicious mix will be revealed through its non-conforming output.

The challenge in performing attribution is that the cases of one malicious mix versus two malicious mixes can be indistinguishable if the two malicious mixes perform identical manipulations. Unless it is readily apparent through other mechanisms which mixes have been compromised, a reasonable solution once misbehavior is detected is to re-evaluate the security of all three mixes.

More generally, mixes should be carefully selected, as collusion between two or more dishonest mixes can compromise data privacy. This is similar to Tor's existing quasi-centralized notion of trust: if a majority of the Tor directory authorities are compromised, Tor offers no anonymity protections since the directories could advertise only the existence of malicious relays. Therefore, mixes must be chosen carefully. We envision that the maintainers of the Tor Project could selectively grant permission to operate HisTor mixes to trusted parties. Alternatively, to maintain the existing level of trust, the directory authorities could also operate the mixes.

From an integrity perspective, it might initially seem that HisTor and PrivEx offer similar guarantees—that certain nodes must behave honestly to ensure the integrity of query results. However, the integrity guarantees provided by HisTor are significantly stronger. In PrivEx, a single relay can significantly perturb the results of a query. Successfully compromising PrivEx statistics gathering is relatively simple since there are no barriers to operating a relay. In contrast, HisTor removes the necessity to trust the data collectors and instead relies on a much smaller set of nodes (i.e., mixes). Additionally, so long as a single mix is honest, query tampering can be easily detected.

#### Selection of ε

A consistent problem in schemes that apply differential privacy is selecting an appropriate value of ε. In our experimentation, we use the same conservative value as existing work [25] and set ε = 1. Since ε is a relative and not an absolute measure of privacy, an interesting area of future work is to derive per-query values of ε that are guaranteed to protect individuals with a fixed probability. Lee and Clifton [21] provide one such construction for ε-differential privacy. Incorporating this selection process into HisTor (which provides (ε, δ)-differential privacy) is an exciting potential future research direction.

In the current implementation of HisTor, the analyst communicates its choice of ε to the mixes. Since the number of noise records is proportional to ε^(-2), large values of ε offer little security, while too small values of ε provide minimal privacy benefits and incur potentially enormous communication costs. To provide a simple sanity check, a real-world deployment of HisTor could establish system-wide parameters ε_max and ε_min that bound the analyst’s choice.

#### Privacy Budget

(ε, δ)-differential privacy schemes impose a privacy budget whose balance is decremented as a function of δ and ε for each issued query. This budget is defined in terms of a static database D over which queries are issued. In HisTor, counters are zeroed after each epoch, effectively resulting in a new database. This significantly reduces the risk of exceeding the privacy budget. However, for certain query types, there may be dependencies in the statistic of interest between epochs that violate this assumption of independence. This further motivates a careful selection of ε to minimize this privacy risk.

### Conclusion

This paper presents HisTor, a distributed statistics collection system for Tor. Unlike existing work, HisTor provides strong integrity guarantees for collecting data on Tor. Specifically, we demonstrate that the influence of a colluding group of malicious data collectors is tightly bounded by the fraction of nodes they control in the network. More practically, HisTor ensures that a small colluding group of malicious data collectors has a negligible impact on the results of statistics queries.

In addition to ensuring integrity, HisTor provides strong privacy guarantees as long as malicious mixes do not collude with a dishonest analyst. HisTor also achieves resistance to compulsion attacks through the use of novel oblivious counters. Using real-world data sets and realistic query workloads, we demonstrate that HisTor enables highly accurate statistics aggregation with small bandwidth and computational overheads. Our performance experiments and microbenchmarks indicate that dozens of simultaneous HisTor queries could be supported on a single CPU core. To encourage its use by privacy researchers, we plan to release HisTor as open-source software in the near future.

### Acknowledgments

We thank the anonymous reviewers for their insightful comments. We also thank Aaron Johnson, Henry Tan, and Sridhar Venkatesan for valuable discussions. This paper is partially funded by National Science Foundation (NSF) grants CNS-1149832 and CNS-1527401. The findings and opinions expressed in this paper are those of the authors and do not necessarily reflect the views of the NSF.

### References

[1] Akamai’s State of the Internet Q2 2015 Report, https://www.akamai.com/us/en/multimedia/documents/state-of-the-internet/2015-q2-cloud-security-report.pdf, 2015.

[2] K. Bauer, M. Sherr, D. McCoy, and D. Grunwald. ExperimenTor: A Testbed for Safe and Realistic Tor Experimentation. In USENIX Workshop on Cyber Security Experimentation and Test (CSET), August 2011.

[3] A. Bhattachayya. On a Measure of Divergence between Two Statistical Populations Defined by Their Population Distributions. Bulletin Calcutta Mathematical Society, 35:99–109, 1943.

[4] A. Biryukov, I. Pustogarov, F. Thill, and R.-P. Weinmann. Content and Popularity Analysis of Tor Hidden Services. In International Conference on Distributed Computing Systems Workshops (ICDCSW), 2014.

[5] T.-H. H. Chan, E. Shi, and D. Song. Private and Continual Release of Statistics. ACM Transactions on Information and System Security (TISSEC), 14(3), 2011.

[6] R. Chen, A. Reznichenko, P. Francis, and J. Gehrke. Towards Statistical Queries over Distributed Private User Data. In USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2012.

[7] R. Chen, I. E. Akkus, and P. Francis. SplitX: High-Performance Private Analytics. In Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM), 2013.

[8] R. Dingledine and S. Murdoch. Performance Improvements on Tor, or, Why Tor is Slow and What We’re Going to Do About It. https://svn.torproject.org/svn/projects/roadmaps/2009-03-11-performance.pdf, March 2009.

[9] R. Dingledine, N. Mathewson, and P. Syverson. Tor: The Second-Generation Onion Router. In USENIX Security Symposium (USENIX), August 2004.

[10] C. Dwork. Differential Privacy. Automata, Languages and Programming, pages 1–12, 2006.

[11] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our Data, Ourselves: Privacy via Distributed Noise Generation. In Advances in Cryptology (Eurocrypt), 2006.

[12] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum. Differential Privacy under Continual Observation. In ACM Symposium on Theory of Computing (STOC), 2010.

[13] C. Dwork, A. Roth, et al. The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–407, 2014.

[14] T. Elahi, K. Bauer, M. AlSabah, R. Dingledine, and I. Goldberg. Changing of the Guards: A Framework for Understanding and Improving Entry Guard Selection in Tor. In ACM Workshop on Privacy in the Electronic Society (WPES), 2012.

[15] T. Elahi, G. Danezis, and I. Goldberg. PrivEx: Private Collection of Traffic Statistics for Anonymous Communication Networks. In ACM Conference on Computer and Communications Security (CCS), November 2014.

[16] S. Goldwasser and S. Micali. Probabilistic Encryption. Journal of Computer and System Sciences, 28(2):270–299, 1984.

[17] R. Jansen and N. Hopper. Shadow: Running Tor in a Box for Accurate and Efficient Experimentation. In Network and Distributed System Security Symposium (NDSS), 2012.

[18] R. Jansen and A. Johnson. Safely Measuring Tor. In ACM Conference on Computer and Communications Security (CCS), 2016.

[19] A. Johnson, C. Wacek, R. Jansen, M. Sherr, and P. Syverson. Users Get Routed: Traffic Correlation on Tor By Realistic Adversaries. In ACM Conference on Computer and Communications Security (CCS), November 2013.

[20] D. Kedogan, D. Agrawal, and S. Penz. Limits of Anonymity in Open Environments. In Information Hiding Workshop (IH), 2002.

[21] J. Lee and C. Clifton. How Much is Enough? Choosing ε for Differential Privacy. In International Conference on Information Security, 2011.

[22] K. Loesing, S. J. Murdoch, and R. Dingledine. A Case Study on Measuring Statistical Data in the Tor Anonymity Network. In Financial Cryptography and Data Security (FC), 2010.

[23] N. Mathewson. Some Thoughts on Hidden Services (Tor Blog Post), 2014. Available at https://blog.torproject.org/blog/some-thoughts-hidden-services.

[24] D. McCoy, K. Bauer, D. Grunwald, T. Kohno, and D. Sicker. Shining Light in Dark Places: Understanding the Tor Network. In Privacy Enhancing Technologies Symposium (PETS), 2008.

[25] F. McSherry and R. Mahajan. Differentially-Private Network Trace Analysis. ACM SIGCOMM Computer Communication Review, 41(4):123–134, 2011.

[26] F. D. McSherry. Privacy Integrated Queries: An Extensible Platform for Privacy-Preserving Data Analysis. In ACM SIGMOD International Conference on Management of Data (SIGMOD), 2009.

[27] M. Perry. The Trouble with CloudFlare (Tor Blog Post), March 2016. Available at https://blog.torproject.org/blog/trouble-cloudflare.

[28] M. Prince. The Trouble with Tor (CloudFlare Blog Post), March 2016. Available at https://blog.cloudflare.com/the-trouble-with-tor/.

[29] Protocol Buffers. https://developers.google.com/protocol-buffers/.

[30] C. Soghoian. Enforced Community Standards For Research on Users of the Tor Anonymity Network. In Workshop on Ethics in Computer Security Research (WECSR), 2011.

[31] Tor Project, Inc. Tor Metrics Portal. https://metrics.torproject.org/.

[32] Tor Research Safety Board. Available at https://research.torproject.org/safetyboard.html.

[33] C. Wacek, H. Tan, K. Bauer, and M. Sherr. An Empirical Evaluation of Relay Selection in Tor. In Network and Distributed System Security Symposium (NDSS), February 2013.

[34] P. Winter, R. Kower, M. Mulazzani, M. Huber, S. Schrittwieser, S. Lindskog, and E. Weippl. Spoiled Onions: Exposing Malicious Tor Exit Relays. In Privacy Enhancing Technologies Symposium (PETS), 2014.