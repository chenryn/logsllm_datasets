### Suitable Observables for Monitoring Distributed Applications

Monitoring distributed applications comprehively using sequences of system calls alone can be challenging. Stillerman et al. demonstrated that in distributed applications, a more effective observable is the messages passed across the network [68]. To illustrate this, they implemented an anomaly detection system for a distributed CORBA application, which involves many distributed objects communicating via network messages. The sequence of these messages serves as a good proxy for object behavior, enabling differentiation between normal application behavior and rogue clients.

### Additional Information Sources in Other Domains

In other domains, there may be additional sources of information not available when monitoring running server programs. For example, in dynamic execution environments like Java, the interpreter can easily collect extensive data about program execution during runtime. Inoue and Forrest showed that Java method invocations are effective observables for anomaly detection [27]. Their approach extended beyond attack detection by leveraging Java’s sandboxing security mechanism to implement dynamic sandboxing, where minimal security policies are inferred through runtime monitoring.

### Automated Response to Detected Anomalies

While extensive research has been conducted on methods for intrusion detection using system calls, there has been less focus on automated responses to detected anomalies. Most anomaly detection systems generate more alerts than users or administrators can handle, especially when multiple instances of a system are deployed within a single organization. Therefore, automated responses to detected anomalies are necessary. However, a significant challenge is the problem of false positives. A binary response, such as shutting down a machine or unauthenticating a user, is unacceptable if there is even a small probability of error. Adopting a graduated response strategy, where small adjustments are made continuously, reduces the risk of unnecessary system damage or denial of service.

The first effort to couple an automated response to system call anomalies was a Linux kernel extension called pH [64, 65]. pH detects anomalies using lookahead pairs. Instead of immediately killing or blocking the behavior of anomalous processes, it delays anomalous system calls. Isolated anomalies are delayed imperceptibly, while clustered anomalies are delayed exponentially longer. Real attacks typically generate large delays (on the order of hours or days). This response automatically blocks many attacks due to built-in timeouts in most network connections and provides administrators with time to intervene manually. False alarms, which often produce a small number of isolated anomalies, result in proportionally small delays that are usually imperceptible to the user. In the rare case of a false positive causing a long delay, a simple override was provided for the user or administrator.

Although pH was the first system to use delays as a response to detected anomalies, delay-based strategies have been employed in other defenses, particularly in networking and remote login interfaces (e.g., to prevent online dictionary attacks). For instance, Williamson observed that unusually frequent outgoing network requests could signal an anomaly, and the damage caused by such behavior could be mitigated by reducing the rate at which new network connections are initiated [80]. This technology became part of HP’s ProCurve Network Immunity Manager [26] and was later extended to include incoming connections and more types of network connections [4]. The concept of slowing down computation or communication is often referred to as throttling or rate limiting, and it has been extensively studied in the networking community, including in active networks [23], Domain Name Service [81], Border Gateway Protocol [33, 32], and peer-to-peer networks [2].

A commercial implementation of system-call anomaly detection, Sana Security’s Primary Response, adopted a layered approach. The first layer explicitly prevented code injection in all forms, covering a large class of common attacks and preventing subversion through mimicry attacks. The second layer blocked anomalous system calls that manipulated the file system, which can prevent many non-code-injection attacks. Additionally, Primary Response profiled parameters to file-related system calls, further reducing false positives and preventing non-control-flow attacks.

### Summary and Conclusions

Over the past decade, we have seen the continuous evolution of new platforms and new forms of attack, including email viruses, spyware, botnets, and mutating malware. Research on system-call monitoring has matured, with many variations of the original method being explored. Despite dramatic changes in today’s computing environments and applications, system-call monitoring remains a fundamental technique underlying many current projects [77, 52]. While it is optimistic to expect system-call monitoring to remain an active and exciting research frontier indefinitely, both threats and defenses will continue to evolve, likely migrating to higher application layers and lower levels, such as on-chip attacks in multi-core architectures.

The design principles articulated in Section 2, inspired by living systems, are potentially of lasting significance. These include generic mechanisms, adaptability, autonomy, graduated response, and diversity. Some of these principles have been widely adopted (diversity), some remain controversial (graduated response and adaptability), and some have been largely ignored (generic mechanisms and autonomy). Together, these principles form a hypothesis about the properties required to protect computers and their users.

These design principles guided nearly all of our implementation decisions. By articulating a hypothesis and designing the simplest possible experiment to test it, we demonstrated that short sequences of system calls are a good discriminator between normal and malicious behavior. Rather than focusing solely on producing a working artifact, we aimed to understand the approaches that could be used for effective defenses.

A key component of our approach was designing repeatable experiments, allowing others to confirm our results and test variations against the original system. This enabled other groups to replicate our results, use the datasets for their own experiments, and devise attacks against the method. Repeatable experiments are crucial for putting computer security on sounder footing, but careful comparisons between competing methods are also important. Public datasets and prototypes help, but it remains challenging due to the complexity of modern computing environments, heavy dependence on data inputs, and metrics emphasizing breadth of coverage.

What began as a simple insight inspired by biology has grown into a robust and diverse field of research. We are excited about the progress and directions this research has taken. Attacks on the ideas have led to creative new methods, making the original approach more robust and resulting in a far better protection system than we could have hoped for over a decade ago. This validates some of the principles elucidated in Section 2, and we see much scope for extending the research to investigate these principles in greater depth. We hope this paper illustrates how inspirational the biological analogy can be and encourages others to explore these principles, ensuring this remains a vibrant and ever-growing area of research.

### Acknowledgments

The authors gratefully acknowledge the many people who encouraged and assisted us during the development of the original system call project. In particular, we thank Dave Ackley, Tom Longstaff, and Eugene Spafford. Jed Crandall, Dave Evans, ThanhVu Nguyen, and Eugene Spafford made many helpful suggestions on this manuscript.

The original project was partially funded by the National Science Foundation (NSF) IRI-9157644, Office of Naval Research N00014-95-1-0364, and the Defense Advanced Research Projects Agency N00014-96-1-0680. SF acknowledges NSF (CCF 0621900, CCR-0331580), Air Force Office of Scientific Research MURI grant FA9550-07-1-0532, and the Santa Fe Institute. AS acknowledges NSERC’s Discovery program and MITACS.

### References

[1] M. Abadi, M. Budiu, U. Erlingsson, and J. Ligatti. Control-flow integrity: Principles, implementations and applications. In Proceedings of ACM Computer and Communications Security, November 2005.
[2] M. K. Aguilera, M. Lillibridge, and X. Li. Transaction rate limiters for peer-to-peer systems. IEEE International Conference on Peer-to-Peer Computing, 0:3–11, 2008.
[3] D. Anderson, T. Frivold, and A. Valdes. Next-generation intrusion detection expert system (nides): A summary. Technical Report SRI-CSL-95-07, Computer Science Laboratory, SRI International, May 1995.
[4] J. Balthrop. Riot: A responsive system for mitigating computer network epidemics and attacks. Master’s thesis, The University of New Mexico, Albuquerque, NM, 2005.
[5] S. Basu and P. Uppuluri. Proxi-Annotated Control Flow Graphs: Deterministic Context-Sensitive Monitoring for Intrusion Detection, pages 353–362. Springer, 2004.
[6] S. Bhatkar, A. Chaturvedi, and R. Sekar. Dataflow anomaly detection. In In Proc. IEEE Symposium on Security and Privacy, pages 48–62, 2006.
[7] S. Chen, J. Xu, and E. C. Sezer. Non-control-data attacks are realistic threats. In 14th Annual Usenix Security Symposium, Aug 2005.
[8] R. Danyliw and A. Householder. CERT advisory CA-2001-19: Code red worm exploiting buffer overflow in IIS. Website, 2001. http://www.cert.org/advisories/CA-2001-19.html.
[9] D. E. Denning. An intrusion-detection model. IEEE Transactions on Software Engineering, 13:222–232, 1987.
[10] D. Endler. Intrusion detection: applying machine learning to Solaris audit data. In In Proc. of the IEEE Annual Computer Security Applications Conference, pages 268–279. Society Press, 1998.
[11] E. Eskin, W. Lee, and S. J. Stolfo. Modeling system calls for intrusion detection with dynamic window sizes. In Proceedings of DARPA Information Survivability Conference and Exposition II (DISCEX II), Anaheim, CA, 2001.
[12] H. Feng, O. Kolesnikov, P. Fogla, W. Lee, and W. Gong. Anomaly detection using call stack information. In Proceedings of the 2003 IEEE Symposium on Security and Privacy, May 2003.
[13] G. Fink and K. Levitt. Property-based testing of privileged programs. In Proceedings of the 10th Annual Computer Security Applications Conference, page 154163, Dec. 1994.
[14] S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff. A sense of self for Unix processes. In SP ’96: Proceedings of the 1996 IEEE Symposium on Security and Privacy, page 120, Washington, DC, USA, 1996. IEEE Computer Society.
[15] S. Forrest, A. S. Perelson, L. Allen, and R. Cherukuri. Self-nonself discrimination in a computer. In Proceedings of the 1994 IEEE Symposium on Research in Security and Privacy, Los Alamitos, CA, 1994. IEEE Computer Society Press.
[16] D. Gao, M. Reiter, and D. Song. Gray-box extraction of execution graphs for anomaly detection. In Proceedings of the 11th ACM Conference on Computer and Communications Security, pages 318–329, October 2004.
[17] D. Gao, M. K. Reiter, and D. Song. Behavioral distance measurement using hidden Markov models. In D. Zamboni