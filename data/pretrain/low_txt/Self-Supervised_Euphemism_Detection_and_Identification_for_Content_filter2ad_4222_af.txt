### Best Performance Across Three Datasets
The selected classifier demonstrated the best performance across three datasets, which is why we ultimately chose it and reported its results in Section V. As shown in Figure 4, other classifiers also achieved satisfactory performance, with testing accuracies ranging from 0.86 to 0.90.

### Fine-Grained Classifiers
In the euphemism identification framework, we employed a multi-class classifier to determine the target keyword each euphemism refers to. We experimented with the same set of classifiers as before. Interestingly, for fine-grained classification, all classifiers yielded highly similar results. One possible reason for this is that each class has a relatively small number of training instances (ranging from a few hundred to 100,000), which limits the discriminative power of advanced algorithms. For the drug dataset, which includes 33 target keywords, the training accuracy was approximately 55%, and the testing accuracy was around 24%. This demonstrates the feasibility of the task, as the random guess accuracy would be 3.3%. Given the similar performance across classifiers, we recommend using Logistic Regression on raw text (LRT) for better computational efficiency.

For both coarse and fine-grained classifiers, more advanced classification algorithms will be explored in future work.

### Parameter Analysis
**Figure 5: Sensitivity of \( t \)**

In the euphemism detection step (Section IV-A), we set a masked language model threshold \( t \) to filter out generic masked sentences. In the ranked list of replacements for the mask token, if any target keyword appears in the top-\( t \) replacement candidates for the masked sentence, we consider the masked sentence a valid context instance. Otherwise, we consider the masked sentence generic and filter it out. Figure 5 illustrates how the results change with the threshold \( t \). We observe a slight decrease in performance when the threshold \( t \) exceeds 5. Therefore, \( t = 5 \) appears to be an optimal parameter choice.

### Limitations
While our approach for euphemism detection and identification shows promise, it does have some limitations:

#### Text-Only Moderation
Our approach is limited to text and is not easily generalizable to other media types. Social media posts often include images, videos, and audio, which can be even more challenging and traumatic to moderate manually [3]–[5]. However, text is frequently associated with these other media, such as in the form of comments, and detecting euphemisms might indirectly provide clues to content moderators dealing with different media.

#### Other Contexts
Our approach performs well on corpora discussing drugs, weapons, and sexuality. In preliminary experiments with a corpus of hate speech, it did not perform as well, producing many false matches when tasked with identifying racial slurs. We believe this is because euphemisms related to drugs, weapons, and sex typically have specific meanings (e.g., "pot" always refers to marijuana, not other drugs). Racial slurs, on the other hand, are used imprecisely and interchangeably with generic swearwords, which seems to confuse euphemism detection. It is unclear whether this is a fundamental limitation. Even if it is, there are many contexts where euphemisms have specific meanings, and our approach should be effective, particularly in forums selling illicit goods.

#### Robustness to Adversarial Evasion
In our evaluation, we relied on non-adversarial datasets gleaned from public, online forums. People were using euphemisms, but we do not know whether they were using them specifically to evade content moderation. Perhaps these euphemisms are, for them, simply the ordinary names of certain things within the circle where they were discussing them. (Someone who consistently spoke of "marijuana" instead of "pot" on a forum dedicated to discussing drug experiences might well be suspected of being an undercover cop.)

Because our algorithms rely on sentence-level context to detect and identify euphemisms, an adversary would need to change that context to escape detection. Such changes may also render the text unintelligible to its intended audience. Therefore, we expect our techniques to be moderately resilient to adversarial evasion. However, we cannot test our expectations at the moment, as we do not have a dataset where people were purposely using euphemisms only to escape detection.

#### Usability for Content Moderators
While our approach shows encouraging performance in lab tests, we have not yet evaluated whether it is good enough to be helpful to content moderators in practice. That evaluation would require a user study of professional content moderators, which is beyond the scope of the present paper, which focuses on the technical underpinnings of euphemism detection and identification. We are interested in investigating usability as a follow-up study.

As a preliminary experiment, we investigated the Perspective API, Google’s automated toxicity detector, to identify the likelihood of a sentence being considered toxic by a reader. Perspective is reportedly used today by human moderators to filter or prioritize comments that may require moderation. We took sentences from our datasets that contain the target keywords (e.g., "marijuana," "heroin") and evaluated the toxicity score of the sentence (a) with the target keyword and (b) by replacing the target keyword with one of its identified euphemisms (e.g., "weed," "dope"). By comparing the toxicity scores, we can estimate the likelihood that a human moderator using the Perspective API would be shown each version of the sentence. Table VIII shows the average toxicity scores when comparing 1,000 randomly chosen original sentences with their euphemistic replacements for the drug, weapon, and sexuality categories. We observe that sentences with target keywords have higher (or at least comparable) toxicity scores compared to sentences with euphemisms, suggesting that euphemisms could help escape content moderation based on the Perspective API. In turn, detecting and identifying euphemisms could help defeat such evasive techniques.

**Table VIII: Average Toxicity Scores by Perspective API**

| Category | Original Sentences (A) | Euphemistic Replacements (B) |
|----------|------------------------|------------------------------|
| Drug     | 0.235                  | 0.209                        |
| Weapon   | 0.232                  | 0.178                        |
| Sexuality| 0.612                  | 0.522                        |

### Conclusion
We have worked on the problem of content moderation by detecting and identifying euphemisms. By utilizing contextual information explicitly, we not only achieve new state-of-the-art detection results but also discover new euphemisms not listed in the ground truth. For euphemism identification, we, for the first time, prove the feasibility of the task and achieve it on a raw text corpus alone, without relying on any additional resources or supervision.

### Reproducibility
Our code and pre-trained models are available on GitHub: https://github.com/WanzhengZhu/Euphemism.

### Acknowledgments
We thank our shepherd, Ben Zhao, and the anonymous reviewers for their valuable comments on earlier drafts, which significantly improved this manuscript. We also thank Kyle Soska for providing us with the Reddit data, Sadia Afroz for the weapons data, Xiaojing Liao and Haoran Lu for sharing and discussing the Cantreader implementation, and Xin Huang for insightful discussions. This research was partially supported by the National Science Foundation, awards CNS-1720268 and CNS-1814817.

### References
[1] D. Blackie, “AOL censors British town’s name!” Computer Underground Digest, vol. 8, April 1996, as abstracted in RISKS Digest 18.07. [Online]. Available: http://catless.ncl.ac.uk/Risks/18.07.html#subj3

[2] P. M. Barrett, “Who moderates the social media giants?” Center for Business, 2020.

[3] C. Newton, “The terror queue,” Dec. 2019, https://www.theverge.com/2019/12/16/21021005/google-youtube-moderators-ptsd-accenture-violent-disturbing-content-interviews-video.

[4] ——, “The trauma floor: The secret lives of Facebook moderators in America,” Feb. 2019, https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona.

[5] Cambridge Consultants, “Use of AI in online content moderation,” Ofcom Report, 2019, https://www.ofcom.org.uk/__data/assets/pdf_file/0028/157249/cambridge-consultants-ai-content-moderation.pdf.

[6] G. Durrett, J. K. Kummerfeld, T. Berg-Kirkpatrick, R. Portnoff, S. Afroz, D. McCoy, K. Levchenko, and V. Paxson, “Identifying products in online cybercrime marketplaces: A dataset for fine-grained domain adaptation,” in Proceedings of Empirical Methods in Natural Language Processing (EMNLP), 2017, pp. 2598–2607.

[7] R. S. Portnoff, S. Afroz, G. Durrett, J. K. Kummerfeld, T. Berg-Kirkpatrick, D. McCoy, K. Levchenko, and V. Paxson, “Tools for automated analysis of cybercriminal markets,” in Proceedings of International Conference on World Wide Web (WWW), 2017, pp. 657–666.

[8] B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, and S. Lehmann, “Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm,” in Proceedings of Empirical Methods in Natural Language Processing (EMNLP), 2017, pp. 1615–1625.

[9] K. Yuan, H. Lu, X. Liao, and X. Wang, “Reading thieves’ cant: automatically identifying and understanding dark jargons from cybercrime marketplaces,” in Proceedings of 27th USENIX Security Symposium, 2018, pp. 1027–1041.

[10] T. Hada, Y. Sei, Y. Tahara, and A. Ohsuga, “Codewords detection in microblogs focusing on differences in word use between two corpora,” in Proceedings of International Conference on Computing, Electronics & Communications Engineering (iCCECE). IEEE, 2020, pp. 103–108.

[11] R. Magu and J. Luo, “Determining code words in euphemistic hate speech using word embedding networks,” in Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), 2018, pp. 93–100.

[12] K. Zhao, Y. Zhang, C. Xing, W. Li, and H. Chen, “Chinese underground market jargon analysis based on unsupervised learning,” in Proceedings of IEEE Conference on Intelligence and Security Informatics (ISI). IEEE, 2016, pp. 97–102.

[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2013, pp. 3111–3119.

[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013.

[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019, pp. 4171–4186.

[16] C. Donahue, M. Lee, and P. Liang, “Enabling language models to fill in the blanks,” in Proceedings of Association for Computational Linguistics (ACL), 2020.

[17] C. Felt and E. Riloff, “Recognizing euphemisms and dysphemisms using sentiment analysis,” in Proceedings of the Second Workshop on Figurative Language Processing, 2020, pp. 136–145.

[18] N. Leontiadis, T. Moore, and N. Christin, “Measuring and analyzing search-redirection attacks in the illicit online prescription drug trade.” in USENIX Security Symposium, vol. 11, 2011.

[19] D. McCoy, A. Pitsillidis, J. Grant, N. Weaver, C. Kreibich, B. Krebs, G. Voelker, S. Savage, and K. Levchenko, “Pharmaleaks: Understanding the business of online pharmaceutical affiliate programs,” in Part of the 21st USENIX Security Symposium, 2012, pp. 1–16.

[20] J. Huang, Z. Li, X. Xiao, Z. Wu, K. Lu, X. Zhang, and G. Jiang, “SUPOR: Precise and scalable sensitive user input detection for Android apps,” in 24th USENIX Security Symposium, 2015, pp. 977–992.

[21] Y. Nan, M. Yang, Z. Yang, S. Zhou, G. Gu, and X. Wang, “UIPicker: User-input privacy identification in mobile applications,” in Proceedings of 24th USENIX Security Symposium, 2015, pp. 993–1008.

[22] K. Thomas, D. McCoy, C. Grier, A. Kolcz, and V. Paxson, “Trafficking fraudulent accounts: The role of the underground market in Twitter spam and abuse,” in 22nd USENIX Security Symposium, 2013, pp. 195–210.

[23] S. Sedhai and A. Sun, “Semi-supervised spam detection in Twitter stream,” IEEE Transactions on Computational Social Systems, vol. 5, no. 1, pp. 169–175, 2017.

[24] T. Wu, S. Wen, Y. Xiang, and W. Zhou, “Twitter spam detection: Survey of new approaches and comparative study,” Computers & Security, vol. 76, pp. 265–284, 2018.

[25] T. Wu, S. Liu, J. Zhang, and Y. Xiang, “Twitter spam detection based on deep learning,” in Proceedings of the Australasian Computer Science Week Multiconference, 2017, pp. 1–8.

[26] A. Keith and K. Burridge, “Euphemism and dysphemism: language used as shield and weapon,” 1991.

[27] K. L. Pfaff, R. W. Gibbs, and M. D. Johnson, “Metaphor in using and understanding euphemism and dysphemism,” Applied Psycholinguistics, vol. 18, no. 1, pp. 59–83, 1997.

[28] R. Hugh, “Rawson’s dictionary of euphemisms and other doubletalk,” 2002.

[29] K. Allan, “The connotations of English colour terms: Colour-based x-phemisms,” Journal of Pragmatics, vol. 41, no. 3, pp. 626–637, 2009.

[30] H. A. Rababah, “The translatability and use of x-phemism expressions (x-phemization): Euphemisms, dysphemisms and orthophemisms) in the medical discourse,” Studies in Literature and Language, vol. 9, no. 3, pp. 229–240, 2014.

[31] R. A. Spears, Slang and euphemism. Signet Book, 1981.

[32] P. Chilton, “Metaphor, euphemism and the militarization of language,” Current Research on Peace and Violence, vol. 10, no. 1, pp. 7–19, 1987.

[33] H. Ahl, “Motivation in adult education: a problem solver or a euphemism for direction and control?” International Journal of Lifelong Education, vol. 25, no. 4, pp. 385–405, 2006.

[34] E. C. Fernández, “The language of death: Euphemism and conceptual metaphorization in Victorian obituaries,” SKY Journal of Linguistics, vol. 19, no. 2006, pp. 101–130, 2006.

[35] Z. Pei, Z. Sun, and Y. Xu, “Slang detection and identification,” in Proceedings of Computational Natural Language Learning (CoNLL), 2019, pp. 881–889.

[36] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for sequence tagging,” arXiv preprint arXiv:1508.01991, 2015.

[37] J. D. Lafferty, A. McCallum, and F. C. Pereira, “Conditional random fields: Probabilistic models for segmenting and labeling sequence data,” in Proceedings of International Conference on Machine Learning.