### Figure 7: Latency vs. TM in the Suspicion-Steady Scenario

**Parameters:**
- **n = 3, throughput = 300 1/s, TMR = 10000 ms**
- **n = 7, throughput = 300 1/s, TMR = 100000 ms**

**Latency (ms)**
- **50**
- **45**
- **40**
- **35**
- **30**
- **25**
- **20**
- **15**
- **10**
- **5**
- **0**

**Algorithms:**
- **FD (Failure Detector)**
- **GM (Group Membership)**

**Mistake Duration (TM) [ms]**
- **1**
- **10**
- **100**
- **1000**

**Figure 7** illustrates the latency versus mistake duration (TM) in the suspicion-steady scenario with a fixed TMR. The results show that:
1. Both algorithms perform well, with the latency overhead being only a few times higher than in the normal-steady scenario (see Fig. 4).
2. The FD algorithm outperforms the GM algorithm in this scenario.

### Figure 8: Latency Overhead vs. Throughput in the Crash-Transient Scenario

**Latency (ms)**
- **80**
- **70**
- **60**
- **50**
- **40**
- **30**
- **20**
- **10**
- **0**

**Throughput [1/s]**
- **0**
- **100**
- **200**
- **300**
- **400**
- **500**
- **600**
- **700**
- **800**

**Scenarios:**
- **After crash of p1, n = 3**
  - **TD = 0 ms**
  - **TD = 10 ms**
  - **TD = 100 ms**
  - **GM algorithm**
  - **FD algorithm**
- **After crash of p1, n = 7**
  - **TD = 0 ms**
  - **TD = 10 ms**
  - **TD = 100 ms**
  - **GM algorithm**
  - **FD algorithm**

**Figure 8** shows the latency overhead versus throughput in the crash-transient scenario. The results indicate that:
- The GM-based algorithm performs slightly better and has better resilience in the long term after crashes.
- The FD-based algorithm outperforms the GM-based algorithm in scenarios involving wrong suspicions of correct processes and during the transient behavior after crashes.
- The performance difference is more significant when correct processes are wrongly suspected.

### Discussion

We have investigated two uniform atomic broadcast algorithms designed for an asynchronous system with a minimal extension to allow live solutions to the atomic broadcast problem, and with up to \( f < \frac{n}{2} \) process crashes. In the absence of crashes and suspicions, both algorithms exhibit similar performance. However, a long time after any crashes, the GM-based algorithm performs slightly better and has better resilience.

In scenarios involving wrong suspicions of correct processes and the transient behavior after crashes, the FD-based algorithm outperforms the GM-based algorithm. The performance gap is more pronounced when correct processes are wrongly suspected.

### Combined Use of Failure Detectors and Group Membership

Based on our results, we advocate a combined use of failure detectors and group membership services. Failure detectors should be used to make failure handling more responsive and robust, while a different failure detector, making fewer mistakes at the expense of slower crash detection, should be used in the group membership service to achieve long-term performance and resiliency benefits after a crash. A combined approach is also desirable because the failure detector approach is primarily concerned with failure handling, whereas a group membership service offers additional essential features such as graceful process offline, new process joining, and recovery of crashed processes. Additionally, group membership can be used to garbage collect messages in buffers when a crash occurs.

### Generality of Our Results

Our study focuses on uniform atomic broadcast algorithms with a centralized communication scheme. These algorithms are practical, optimized for small latency under low load and high load conditions. Future work will investigate algorithms with decentralized communication schemes.

### Non-Uniform Atomic Broadcast

Our study focuses on uniform atomic broadcast. Dropping the uniformity requirement in either approach could potentially yield performance gains, provided the application can handle the relaxed requirements. The FD-based algorithm cannot be transformed into a more efficient non-uniform variant, as the effort to reach agreement on total order ensures uniformity. In contrast, the GM-based algorithm has an efficient non-uniform variant using only two multicast messages. Thus, the GM-based approach allows trading off guarantees related to failures and/or suspicions for performance. This tradeoff will be quantitatively investigated in future work.

### Methodology for Performance Studies

We proposed a methodology for performance studies of fault-tolerant distributed algorithms, characterized by:
1. Repeatable benchmarks defining scenarios, workload, crashes, and performance measures.
2. Benchmarks including various scenarios with crashes and suspicions.
3. Description of failure detectors using quality of service (QoS) metrics.

This methodology allowed us to compare the two algorithms easily, as it involves a small number of parameters. Currently, it is defined for atomic broadcast algorithms, but we plan to extend it to other fault-tolerant algorithms.

### Acknowledgments

We thank Danny Dolev and Gregory Chockler for their insightful comments on uniformity and its interplay with group membership and the limits of this study.

### References

[1] M. Barborak, M. Malek, and A. Dahbura, “The consensus problem in distributed computing,” ACM Computing Surveys, vol. 25, pp. 171–220, June 1993.

[2] X. D´efago, A. Schiper, and P. Urb´an, “Totally ordered broadcast and multicast algorithms: A comprehensive survey,” Tech. Rep. DSC/2000/036, ´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland, Sept. 2000.

[3] G. Chockler, I. Keidar, and R. Vitenberg, “Group communication specifications: A comprehensive study,” ACM Computing Surveys, vol. 33, pp. 427–469, May 2001.

[4] T. D. Chandra and S. Toueg, “Unreliable failure detectors for reliable distributed systems,” Journal of ACM, vol. 43, no. 2, pp. 225–267, 1996.

[5] P. Urb´an, X. D´efago, and A. Schiper, “Contention-aware metrics for distributed algorithms: Comparison of atomic broadcast algorithms,” in Proc. 9th IEEE Int’l Conf. on Computer Communications and Networks (IC3N 2000), pp. 582–589, Oct. 2000.

[6] W. Chen, S. Toueg, and M. K. Aguilera, “On the quality of service of failure detectors,” IEEE Transactions on Computers, vol. 51, pp. 561–580, May 2002.

[7] F. Cristian, R. de Beijer, and S. Mishra, “A performance comparison of asynchronous atomic broadcast protocols,” Distributed Systems Engineering Journal, vol. 1, pp. 177–201, June 1994.

[8] F. Cristian, S. Mishra, and G. Alvarez, “High-performance asynchronous atomic broadcast,” Distributed System Engineering Journal, vol. 4, pp. 109–128, June 1997.

[9] A. Coccoli, S. Schemmer, F. D. Giandomenico, M. Mock, and A. Bondavalli, “Analysis of group communication protocols to assess quality of service properties,” in Proc. IEEE High Assurance System Engineering Symp. (HASE’00), (Albuquerque, NM, USA), pp. 247–256, Nov. 2000.

[10] A. Coccoli, A. Bondavalli, and F. D. Giandomenico, “Analysis and estimation of the quality of service of group communication protocols,” in Proc. 4th IEEE Int’l Symp. on Object-oriented Real-time Distributed Computing (ISORC’01), (Magdeburg, Germany), pp. 209–216, May 2001.

[11] H. Duggal, M. Cukier, and W. Sanders, “Probabilistic verification of a synchronous round-based consensus protocol,” in Proc. 16th Symp. on Reliable Distributed Systems (SRDS ’97), (Washington - Brussels - Tokyo), pp. 165–174, IEEE, Oct. 1997.

[12] H. Ramasamy, P. Pandey, J. Lyons, M. Cukier, and W. Sanders, “Quantifying the cost of providing intrusion tolerance in group communication systems,” in Proc. 2002 Int’l Conf. on Dependable Systems and Networks (DSN-2002), (Washington, DC, USA), pp. 229–238, June 2002.

[13] L. M. Malhis, W. H. Sanders, and R. D. Schlichting, “Numerical evaluation of a group-oriented multicast protocol using stochastic activity networks,” in Proc. 6th Int’l Workshop on Petri Nets and Performance Models, (Durham, NC, USA), pp. 63–72, Oct. 1995.

[14] N. Sergent, X. D´efago, and A. Schiper, “Impact of a failure detection mechanism on the performance of consensus,” in Proc. IEEE Pacific Rim Symp. on Dependable Computing (PRDC), (Seoul, Korea), pp. 137–145, Dec. 2001.

[15] A. Coccoli, P. Urb´an, A. Bondavalli, and A. Schiper, “Performance analysis of a consensus algorithm combining Stochastic Activity Networks and measurements,” in Proc. Int’l Performance and Dependability Symp., (Washington, DC, USA), pp. 551–560, June 2002.

[16] T. D. Chandra, V. Hadzilacos, and S. Toueg, “The weakest failure detector for solving consensus,” Journal of the ACM, vol. 43, pp. 685–722, July 1996.

[17] P. Urb´an, X. D´efago, and A. Schiper, “Chasing the FLP impossibility result in a LAN or how robust can a fault tolerant server be?,” in Proc. 20th IEEE Symp. on Reliable Distributed Systems (SRDS), (New Orleans, LA, USA), pp. 190–193, Oct. 2001.

[18] V. Hadzilacos and S. Toueg, “A modular approach to fault-tolerant broadcasts and related problems,” TR 94-1425, Dept. of Computer Science, Cornell University, Ithaca, NY, USA, May 1994.

[19] P. Urb´an, I. Shnayderman, and A. Schiper, “Comparison of failure detectors and group membership: Performance study of two atomic broadcast algorithms (extended version),” Tech. Rep. IC/2003/15, ´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland, Apr. 2003.

[20] S. Frolund and F. Pedone, “Revisiting reliable broadcast,” Tech. Rep. HPL-2001-192, HP Laboratories, Palo Alto, CA, USA, Aug. 2001.

[21] K. Birman, A. Schiper, and P. Stephenson, “Lightweight causal and atomic group multicast,” ACM Transactions on Computer Systems, vol. 9, pp. 272–314, Aug. 1991.

[22] C. P. Malloth and A. Schiper, “View synchronous communication in large scale distributed systems,” in Proc. 2nd Open Workshop of ESPRIT project BROADCAST (6360), (Grenoble, France), July 1995.

[23] F. B. Schneider, “Implementing fault-tolerant services using the state machine approach: A tutorial,” ACM Computing Surveys, vol. 22, pp. 299–319, Dec. 1990.

[24] P. Urb´an, X. D´efago, and A. Schiper, “Neko: A single environment to simulate and prototype distributed algorithms,” Journal of Information Science and Engineering, vol. 18, pp. 981–997, Nov. 2002.

[25] K. Tindell, A. Burns, and A. J. Wellings, “Analysis of hard real-time communications,” Real-Time Systems, vol. 9, pp. 147–171, Sept. 1995.

[26] J. Gray, “Why do computers stop and what can be done about it?,” in Proc. 5th Symp. on Reliability in Distributed Software and Database systems, Jan. 1986.

[27] B. Charron-Bost, X. D´efago, and A. Schiper, “Broadcasting messages in fault-tolerant distributed systems: the benefit of handling input-triggered and output-triggered suspicions differently,” in Proc. 20th IEEE Symp. on Reliable Distributed Systems (SRDS), (Osaka, Japan), pp. 244–249, Oct. 2002.

[28] L. Lamport, “Time, clocks, and the ordering of events in a distributed system,” Communications of the ACM, vol. 21, pp. 558–565, July 1978.

[29] R. Guerraoui, “Revisiting the relationship between non-blocking atomic commitment and consensus,” in Proc. 9th Int’l Workshop on Distributed Algorithms (WDAG-9), LNCS 972, (Le Mont-St-Michel, France), pp. 87–100, Springer-Verlag, Sept. 1995.