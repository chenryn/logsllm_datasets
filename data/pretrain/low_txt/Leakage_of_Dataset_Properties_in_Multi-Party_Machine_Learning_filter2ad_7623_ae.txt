### 8. Related Work

**Membership Attacks on Machine Learning Models:**
Membership attacks aim to determine whether a specific record was part of the training dataset [47, 50]. These attacks typically involve training shadow models that mimic the target model and using their outputs (e.g., posterior probabilities) to build a meta-classifier. This meta-classifier then classifies records as members or non-members of the training data based on the inference results from the target model. A recent link stealing attack on graphs can be seen as a form of membership attack, where the goal is to infer whether two nodes are connected in the training graph [22].

**Attribute Inference Attacks:**
Attribute inference attacks, on the other hand, aim to determine the value of a sensitive attribute for a single record [17, 53]. For example, researchers have shown that even if a sensitive value is censored using adversarial learning, it can still be leaked from the latent representation of a record in the model [53]. Another study by Hitaj et al. [24] demonstrates that a malicious party can construct class representatives from a model trained in a federated learning setting.

**Dataset Property Leakage:**
The work by Ganju et al. [18] and Ateniese et al. [7] is closely related to ours, as they also consider the leakage of dataset properties. However, their attacks are set in a single-party setting and require white-box access to the model parameters, which may not always be feasible (e.g., when the model is accessed via a cloud-hosted interface). Given the large number of parameters in neural networks, sophisticated methods for reducing network representation are often required [18]. Our approach shows that a combination of inferences and logistic regression as a meta-classifier is sufficient to learn attribute distribution.

**Multi-Party Learning:**
Property leakage in multi-party learning has been demonstrated only in the federated learning setting [39]. In this setting, an attacker gains access to gradients computed on small batches of records (e.g., 32) and tries to infer the distribution of a sensitive feature within the batch. This scenario is arguably easier for the attacker, as they gain more granular access to the data compared to querying the final model trained on the entire dataset. Previous work on dataset property leakage [7, 18, 39] did not consider the case where the sensitive attribute is removed from the data and the impact this has on the success of their attacks.

**Recent Studies:**
Zanella-Béguelin et al. [55] have demonstrated the leakage of text and general trends in the data used to update next-word prediction models. Salem et al. [46] focus on granular leakage about records used to update the model, such as record labels and features. Similar to our work, Salem et al. use a probing dataset to query the models and obtain the posterior difference, which is then processed by an encoder-decoder framework to reconstruct the meaning of the difference between the initial and updated models. In contrast, our model update attack aims to identify the distribution of a sensitive feature in the dataset used to update the model, requiring a simpler machine learning architecture.

### 9. Conclusion

We demonstrate an attack in the context of centralized multi-party machine learning, allowing one party to learn sensitive properties about other parties' data. The attack requires only black-box access to the model and can extract the distribution of a sensitive attribute with a small number of inference queries. We show that simple defenses, such as excluding a sensitive attribute from training, are insufficient to prevent leakage. Our attack is effective on models for tabular, text, and graph data, and datasets with various correlation relationships among attributes and class labels. Finally, we note that existing techniques for secure computation and differential privacy are either not directly applicable to protect population-level properties or do so at a high cost.

### Acknowledgements

We thank Marcella Hastings and the anonymous reviewers for their valuable comments on the paper.

### References

[1] Amazon product co-purchasing network metadata. http://snap.stanford.edu/data/amazon-meta.html.

[2] Azure confidential computing, Microsoft Azure. https://azure.microsoft.com/en-au/solutions/confidential-compute.

[3] Kaggle health dataset. https://www.kaggle.com/c/hhp.

[4] Yelp open dataset. https://www.yelp.com/dataset.

[9] G. Cormode. Personal privacy vs population privacy: Learning to attack anonymization. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Association for Computing Machinery, 2011.

[10] H. Cramér. Mathematical methods of statistics, volume 43. Princeton university press, 1999.

[11] E. Creager, D. Madras, J.-H. Jacobsen, M. Weis, K. Swersky, T. Pitassi, and R. Zemel. Flexibly fair representation learning by disentanglement. In K. Chaudhuri and R. Salakhutdinov, editors, International Conference on Machine Learning (ICML), volume 97, pages 1436–1445, 2019.

[12] I. Damgård, V. Pastro, N. Smart, and S. Zakarias. Multiparty computation from somewhat homomorphic encryption. In Advances in Cryptology—CRYPTO, pages 643–662, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.

[13] S. de Hoogh, B. Schoenmakers, P. Chen, and H. op den Akker. Practical secure decision tree learning in a teletreatment application. In N. Christin and R. Safavi-Naini, editors, Financial Cryptography and Data Security, 2014.

[14] S. Dov Gordon, F.-H. Liu, and E. Shi. Constant-round MPC with fairness and guarantee of output delivery. In Advances in Cryptology—CRYPTO, pages 63–82, Berlin, Heidelberg, 2015. Springer Berlin Heidelberg.

[15] C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 2014.

[16] H. Edwards and A. Storkey. Censoring representations with an adversary. In International Conference on Learning Representations (ICLR), 2016.

[17] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In ACM Conference on Computer and Communications Security (CCS), pages 1322–1333, 2015.

[18] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In ACM Conference on Computer and Communications Security (CCS), 2018.

[5] Global AML and Financial Crime TechSprint. https://www.fca.org.uk/events/techsprints/2019-global-aml-and-financial-crime-techsprint, 2019. [Online; accessed 25-Jan-2021].

[6] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In ACM Conference on Computer and Communications Security (CCS). ACM, 2016.

[7] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. Int. J. Secur. Netw., 2015.

[8] N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. In USENIX Security Symposium, 2019.

[19] R. Gilad-Bachrach, K. Laine, K. Lauter, P. Rindal, and M. Rosulek. Secure data exchange: A marketplace in the cloud. In Proceedings of the 2019 ACM SIGSAC Conference on Cloud Computing Security Workshop, CCSW’19, page 117–128, New York, NY, USA, 2019. Association for Computing Machinery.

[20] T. Graepel, K. Lauter, and M. Naehrig. ML confidential: Machine learning on encrypted data. In T. Kwon, M.-K. Lee, and D. Kwon, editors, Information Security and Cryptology – ICISC 2012, 2013.

[21] J. Hamm. Preserving privacy of continuous high-dimensional data with minimax filters. In Artificial Intelligence and Statistics Conference (AISTATS), 2015.

[22] X. He, J.-Y. Jia, M. Backes, N. Gong, and Y. Zhang. Stealing links from graph neural networks. In USENIX Security Symposium, 2020.

[23] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed, and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations (ICLR), 2017.

[24] B. Hitaj, G. Ateniese, and F. Perez-Cruz. Deep models under the GAN: Information leakage from collaborative deep learning. In ACM Conference on Computer and Communications Security (CCS), 2017.

[25] Y. Huang, D. Evans, J. Katz, and L. Malka. Faster secure two-party computation using garbled circuits. In USENIX Security Symposium, SEC’11, page 35, USA, 2011.

[26] T. Hunt, C. Song, R. Shokri, V. Shmatikov, and E. Witchel. Chiron: Privacy-preserving machine learning as a service. CoRR, abs/1803.05961, 2018.

[27] N. Hynes, R. Cheng, and D. Song. Efficient deep learning on multi-source private data. CoRR, abs/1807.06689, 2018.

[28] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan. GAZELLE: A low latency framework for secure neural network inference. In USENIX Security Symposium, 2018.

[29] P. Karnati. Data-in-use protection on IBM cloud using Intel SGX, 2018. https://www.ibm.com/cloud/blog/data-use-protection-ibm-cloud-using-intel-sgx.

[30] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[31] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.

[32] B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and L. van der Maaten. Crypten: Secure multi-party computation meets machine learning. In Proceedings of the NeurIPS Workshop on Privacy-Preserving Machine Learning, 2020.

[33] R. Kohavi. Scaling up the accuracy of Naive-Bayes classifiers: A decision-tree hybrid. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD’96, page 202–207. AAAI Press, 1996.

[34] S. Laur, H. Lipmaa, and T. Mielikäinen. Cryptographically private support vector machines. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006.

[35] J. Leskovec, L. A. Adamic, and B. A. Huberman. The dynamics of viral marketing. ACM Transactions on the Web (TWEB), 1(1):5–es, 2007.

[36] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters. Internet Mathematics, 2009.

[37] M. Lichman et al. UCI machine learning repository, 2013.

[38] F. Locatello, G. Abbati, T. Rainforth, S. Bauer, B. Schölkopf, and O. Bachem. On the fairness of disentangled representations. In Advances in Neural Information Processing Systems, pages 14584–14597, 2019.

[39] L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov. Exploiting unintended feature leakage in collaborative learning. In IEEE Symposium on Security and Privacy (S&P), 2019.

[40] P. Mohassel and Y. Zhang. SecureML: A system for scalable privacy-preserving machine learning. In IEEE Symposium on Security and Privacy (S&P), pages 19–38, 2017.

[41] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, N. Taft, and D. Boneh. Privacy-preserving matrix factorization. In ACM Conference on Computer and Communications Security (CCS), page 801–812, New York, NY, USA, 2013. Association for Computing Machinery.

[42] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, and M. Costa. Oblivious multi-party machine learning on trusted processors. In USENIX Security Symposium, 2016.

[43] R. Pass, E. Shi, and F. Tramèr. Formal abstractions for attested execution secure processors. In Advances in Cryptology - EUROCRYPT 2017 - 36th Annual International Conference on the Theory and Applications of Cryptographic Techniques, 2017.

[44] K. Pearson. VII. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58(347-352):240–242, 1895.

[45] T. Rabin and M. Ben-Or. Verifiable secret sharing and multiparty protocols with honest majority. In ACM Symposium on Theory of Computing (STOC), STOC ’89, pages 73–85, New York, NY, USA, 1989. Association for Computing Machinery.

[46] A. Salem, A. Bhattacharya, M. Backes, M. Fritz, and Y. Zhang. Updates-leak: Data set inference and reconstruction attacks in online learning. In S. Capkun and F. Roesner, editors, USENIX Security Symposium, 2020.

[47] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes. ML-leaks: Model and data independent membership inference attacks and defenses on machine learning models. In Symposium on Network and Distributed System Security (NDSS), 2019.

[48] Microsoft SEAL (release 3.6). https://github.com/Microsoft/SEAL, Nov. 2020. Microsoft Research, Redmond, WA.

[49] D. J. Sheskin. Handbook of parametric and non-parametric statistical procedures. CRC Press, 2020.

[50] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy (S&P), 2017.

[51] C. Song, T. Ristenpart, and V. Shmatikov. Machine learning models that remember too much. In ACM Conference on Computer and Communications Security (CCS), New York, NY, USA, 2017. Association for Computing Machinery.

[52] C. Song and V. Shmatikov. Auditing data provenance in text-generation models. In International Conference on Knowledge Discovery & Data Mining (KDD), pages 196–206. ACM, 2019.

[53] C. Song and V. Shmatikov. Overlearning reveals