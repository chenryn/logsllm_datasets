### Optimized Text

#### Introduction
The machines were active, and it was known that the bottlenecks would not originate from Click or the configuration. To evaluate the performance of the netfront driver, we added a `ToNetfront` element to the existing configuration. The initial throughput for maximum-sized packets was a disappointing 8 Kp/s. By making several modifications—such as converting the driver from an interrupt-driven model to a polling one and reusing Xen's memory grants—the throughput improved significantly to approximately 360 Kp/s.

#### Netback Driver Performance
Next, we focused on the netback driver, which is the subsequent component in the transmit path. As a quick test, we replaced the netback driver with a minimalistic version that only received packets from the netfront driver via the Xen ring API, counted them, and dropped them, without interacting with the vif. This setup achieved a rate of 950 Kp/s for maximum-sized packets (exceeding line rate) but only 1.5 Mp/s out of a possible 14.8 Mp/s for minimum-sized packets.

#### Overhauling the Xen Network Backplane
At this stage, the primary bottleneck was the drivers' reliance on the Xen ring API, including the use of slow interrupts (events) to synchronize packet access. To further enhance performance, we overhauled the Xen network backplane (Figure 3). We eliminated the netback driver and replaced the software switch with a faster one based on VALE. These modifications resulted in significant performance improvements (Figure 6). The graph illustrates the aggregate throughput for various packet sizes, with an increasing number of ClickOS VMs running on the same server and sending packets through a shared 10Gb NIC to another server.

In this experiment, we utilized all 8 CPU cores in our server, assigning 3 cores to the driver domain hosting the ClickOS switch and distributing the remaining 5 cores among the VMs in a round-robin fashion. Our low-cost server could run up to 128 ClickOS VMs, which collectively filled the 10Gb pipe for most packet sizes. A separate test confirmed that all VMs contributed roughly equally to the throughput. Further tests for receiving packets showed line rate for all packet sizes of 256 bytes and greater, and up to 4.8 Mp/s for minimum-sized packets.

#### Related Work
To create a tiny system for network processing, we could have used any of the existing minimalistic OSes [14, 28, 15]. However, these systems do not provide adequate device driver support or the benefits of full virtualization, such as isolation and migration. Instead, we built ClickOS on top of MiniOS, a minimalistic OS designed for creating small Xen-based VMs.

Several virtualization technologies besides Xen exist [7, 25, 17]. We chose Xen because it offers excellent driver support through its Linux-based dom0 domain and split-driver model, while still allowing us to run a tiny VM (a combination of MiniOS and Click). For example, OpenVZ does not support running an OS other than Linux, and KVM supports MINIX, but the latest version is marked as crashing. The work in [24] introduces a new thin virtualization system aimed at enhancing the security of virtualized systems.

Previous research has also explored optimizing Xen's data plane. One technique involves reducing the cost of packet copies by having the NIC place packets directly into guest memory [19, 22, 13]. We use this concept in conjunction with the netmap framework [20] to speed up Xen's underlying network system. Some general optimization techniques, such as batching and polling, previously appeared in works like RouteBricks [4].

Another optimization [19, 22] involves efficient use of memory grants (Xen's mechanism for allowing domains to share memory). We applied a similar approach to optimize MiniOS' netfront driver. The work in [9] optimizes Xen's scheduler and the Linux bridge that Xen uses to direct packets to guest domains. We went further by replacing the Linux bridge (or in the latest releases of Xen, Open vSwitch [16]) with a modified version of the fast VALE switch [10]. Another technique available in Xen and other virtualization systems is passthrough, where a VM is given direct access to the NIC to improve networking performance. However, this technique forces VMs to host device drivers, binds the device to a single VM, and complicates VM migration.

There are a few other projects that have explored creating small virtual machines. The Denali [27] isolation kernel could run large numbers of concurrent virtual machines but had limited support for device drivers and guest OSes. The work in [12, 11] is similar to ours in that they also create tiny, Xen-based virtual machines, though their focus is on extending the Objective Caml language to generate different kinds of VMs (e.g., a VM with SQLite running in it) defined at compile-time. In contrast, ClickOS can change configurations at runtime and focuses on optimizing boot times and networking performance. The work in [5] is similar, creating small, Erlang-based virtual machines on Xen.

Regarding commercial offerings, Cisco developed a single-tenant virtualized router that can run on VMware ESXi or Citrix XenServer [2]. However, it is unclear how extensible it is, how large its images are, how it performs, or how much it costs. Vyatta [26] offers open-source software that can run on multiple virtualization platforms and implements middlebox functionality. Unlike ClickOS, it is based on Debian, resulting in larger images.

#### Conclusion
We presented ClickOS, a tiny, Xen-based virtual machine that can instantiate middlebox processing in milliseconds while achieving high performance, enabling a truly programmable SDN data plane. Beyond the preliminary throughput experiments, we are optimizing and testing ClickOS' network performance during middlebox processing.

One of the main contributions of ClickOS is its ability to consolidate a very large number of VMs in a single server: hundreds in our tests, and potentially thousands since we can quickly put idle VMs to sleep. This contrasts with anecdotal evidence of only 10-30 VMs in current deployments. We believe ClickOS may enable new scenarios, such as per-customer firewalls, dynamically instantiated load balancers, per-flow IDSes, and on-the-fly customizable software BRASes (Broadband Remote Access Servers) providing on-demand premium services. We see ClickOS as a step towards a more dynamic, programmable SDN data plane.

#### Acknowledgments
This work was partly funded by the EU FP7 CHANGE (257422) project.

#### References
[1] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warfield. Xen and the Art of Virtualization. In Proc. ACM SOSP, 2003, New York, NY, USA, 2003. ACM.
[2] Cisco. Cisco Cloud Services Router 1000v Data Sheet. http://www.cisco.com/en/US/prod/collateral/routers/ps12558/ps12559/data_sheet_c78-705395.html, July 2012.
[3] Click Modular Router. Click Elements. http://read.cs.ucla.edu/click/click, March 2013.
[4] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall, G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy. Routebricks: exploiting parallelism to scale software routers. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, SOSP ’09, pages 15–28, New York, NY, USA, 2009. ACM.
[5] Erlang on Xen. Erlang on Xen. http://erlangonxen.org/, July 2012.
[6] M. Honda, Y. Nishida, C. Raiciu, A. Greenhalgh, M. Handley, and H. Tokuda. Is it still possible to extend TCP? In Proc. ACM IMC, 2011.
[7] A. Kivity, Y. Kamay, K. Laor, U. Lublin, and A. Liguori. KVM: The Linux virtual machine monitor. In Proc. of the Linux Symposium, 2007.
[8] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F. Kaashoek. The Click modular router. ACM Transactions on Computer Systems, August 2000, 2000.
[9] G. Liao, D. Guo, L. Bhuyan, and S. R. King. Software techniques to improve virtualized I/O performance on multi-core systems. In Proceedings of the 4th ACM/IEEE Symposium on Architectures for Networking and Communications Systems, ANCS ’08, pages 161–170, New York, NY, USA, 2008. ACM.
[10] Luigi Rizzo. VALE, a Virtual Local Ethernet. http://info.iet.unipi.it/~luigi/vale/, July 2012.
[11] A. Madhavapeddy, R. Mortier, C. Rotsos, D. Scott, B. S. T. Gazagnaire, S. Smith, S. Hand, and J. Crowcroft. Unikernels: Library operating systems for the cloud. In Proc. of Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2013.
[12] A. Madhavapeddy, R. Mortier, R. Sohan, T. Gazagnaire, S. Hand, T. Deegan, D. McAuley, and J. Crowcroft. Turning down the lamp: software specialisation for the cloud. In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing, HotCloud’10, pages 11–11, Berkeley, CA, USA, 2010. USENIX Association.
[13] K. Mansley, G. Law, D. Riddoch, G. Barzini, N. Turton, and S. Pope. Getting 10 Gb/s from Xen: safe and fast device access from unprivileged domains. In Proceedings of the 2007 conference on Parallel processing, Euro-Par’07, pages 224–233, Berlin, Heidelberg, 2008. Springer-Verlag.
[14] Minix3. Minix3. http://www.minix3.org/, July 2012.
[15] MIT Parallel and Distributed Operating Systems Group. MIT Exokernel Operating System. http://pdos.csail.mit.edu/exo.html, March 2013.
[16] Open vSwitch. Production Quality, Multilayer Open Virtual Switch. http://openvswitch.org/, March 2013.
[17] OpenVZ. Welcome to OpenVZ Wiki. http://wiki.openvz.org/Main_Page, July 2012.
[18] K. K. Ram, J. R. Santos, Y. Turner, A. L. Cox, and S. Rixner. Achieving 10 Gb/s using safe and transparent network interface virtualization. In Proc. ACM VEE, 2009, VEE ’09, 2009.
[19] K. K. Ram, J. R. Santos, Y. Turner, A. L. Cox, and S. Rixner. Achieving 10 Gb/s using safe and transparent network interface virtualization. In Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments, VEE ’09, pages 61–70, New York, NY, USA, 2009. ACM.
[20] L. Rizzo. Netmap: A novel framework for fast packet I/O. In Proc. USENIX Annual Technical Conference, 2012.
[21] L. Rizzo, M. Carbone, and G. Catalli. Transparent acceleration of software packet forwarding using netmap. In A. G. Greenberg and K. Sohraby, editors, INFOCOM, pages 2471–2479. IEEE, 2012.
[22] J. R. Santos, Y. Turner, G. Janakiraman, and I. Pratt. Bridging the gap between software and hardware techniques for I/O virtualization. In USENIX 2008 Annual Technical Conference on Annual Technical Conference, ATC’08, pages 29–42, Berkeley, CA, USA, 2008. USENIX Association.
[23] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratsanamy, and V. Sekarl. Making middleboxes someone else’s problem: Network processing as a cloud service. In Proc. ACM SIGCOMM, 2012.
[24] U. Steinberg and B. Kauer. Nova: a microhypervisor-based secure virtualization architecture. In Proceedings of the 5th European conference on Computer systems, EuroSys ’10, pages 209–222, New York, NY, USA, 2010. ACM.
[25] VMware. VMware Virtualization Software for Desktops, Servers, and Virtual Machines for Public and Private Cloud Solutions. http://www.vmware.com, July 2012.
[26] Vyatta. The Open Source Networking Community. http://www.vyatta.org/, July 2012.
[27] A. Whitaker, M. Shaw, and S. D. Gribble. Scale and performance in the Denali isolation kernel. SIGOPS Oper. Syst. Rev., 36(SI):195–209, Dec. 2002.
[28] Wikipedia. L4 microkernel family. http://en.wikipedia.org/wiki/L4_microkernel_family, July 2012.