### 3. TCP Invariant Preservation in Global Checkpoints

We demonstrate that the TCP invariant is maintained in any global checkpoint state when the checkpoints of participant nodes are coordinated using the protocol described in Figure 2. 

Consider the moment when the Checkpoint Coordinator begins to execute Step 3. According to the coordination protocol, at this instant, communication is disabled on all nodes, and thus, the TCP state is frozen (i.e., the elements of the TCP state cannot change). If the TCP state on each node is saved at this point, it would amount to capturing the TCP state synchronously across all nodes, which trivially satisfies the TCP invariant. 

In the operation described in Figure 2, the TCP state is captured at an earlier time on each node. However, we observe that each node disables its communication (in Step 1) before capturing its TCP state and re-enables communication (in Step 7) well after the instant discussed above. Consequently, the TCP state saved at each node is identical to the TCP state at the critical instant. Therefore, the global checkpoint state preserves the TCP invariant and is consistent.

### 4. Coordinated Restart Mechanism

Our distributed restart mechanism disables communication on each node before restoring its part of the global state. The restart of participant nodes is coordinated using a protocol identical to the coordinated checkpoint protocol. A similar argument can be used to show that our coordinated restart mechanism resumes processes and communication from a consistent global checkpoint state, resulting in correct execution.

### 5. Performance Discussion

Our coordinated checkpoint-restart approach is both lightweight and scalable, significantly improving over previously proposed mechanisms [1][17][15]. The messages exchanged to coordinate checkpoints and restarts in our approach (Figure 2) are the minimum necessary to ensure the atomicity of the global checkpoint, such as with two-phase or three-phase commit protocols. In contrast, approaches like MPVM [1], CoCheck [17], and LAM-MPI [15] require additional messages to flush the channels between every pair of processes, leading to O(N^2) message complexity compared to O(N) complexity with our approach.

#### 5.1. Optimizations

Our mechanisms can be extended with optimizations explored in prior research, such as copy-on-write to allow applications to compute concurrently with their checkpointing and incremental checkpointing to minimize checkpoint size. Additionally, we describe an optimization that allows nodes to continue computation without waiting for all nodes to complete their checkpoints. Each node saves its state after disabling communication. Since the state of each individual node cannot change as long as its communication is disabled, once the Coordinator confirms that communication is disabled on all nodes, it can permit each node to continue operation as soon as it has completed saving its checkpoint state, without affecting the checkpoint state of other nodes. This optimization, illustrated in Figure 4, allows each node to notify the coordinator as soon as its communication is disabled, without waiting to save its local state.

Further asynchrony among the checkpointing nodes (e.g., using checkpoint sequence numbers) is not justified since applications will stall when a non-blocked node tries to communicate with a blocked node.

Other optimizations include reducing the impact of TCP backoff. Since communication is disabled at each checkpoint, packets may be lost, causing performance degradation while TCP recovers from backoff. We evaluate this effect experimentally in Section 6. The impact of TCP backoff can be reduced by keeping communication disabled only for the duration it takes to save the communication state, allowing recovery from TCP backoffs to proceed in parallel with saving the checkpoint state, which is dominated by the time to save the application’s virtual memory state.

### 6. Performance Evaluation

The algorithm described in Figure 2 blocks processes until all nodes have completed their checkpoints. We have implemented Cruz on a cluster of Linux 2.4 systems and integrated it with LSF [14], a job scheduler for clusters. In this section, we present experimental performance results for checkpointing distributed applications using two benchmarks: a semi-Lagrangian atmospheric model benchmark (slm) for weather prediction, and a TCP streaming benchmark, where a transmitting node sends data through a TCP socket connection to a receiving node at maximum rate. The benchmarks were executed on a cluster of machines interconnected by a gigabit Ethernet switch. Each node consists of two 1 GHz Pentium III processors, 2 GB of RAM, 256 KB of cache, and an Intel e1000 gigabit NIC. During all experiments, checkpoint initiation was controlled by a coordinator located on a node distinct from the application nodes.

The runtime overhead of Cruz is negligible (less than 0.5%) since the underlying Zap mechanism requires nothing more than virtualizing identifiers. Figures 5(a) and 5(b) show experimental results for checkpointing the slm benchmark with the number of nodes varying from 2 to 8. The error bars represent the standard deviation of the measurements ([µ − σ, µ + σ]) among the total number of measurements observed during one complete execution of the benchmark. During the experiments, the application was run from beginning to completion with checkpoints every 8 seconds. The total execution time for the benchmark varies from 545 seconds for 2 nodes to 205 seconds for 8 nodes. Figure 5(a) shows the total checkpoint latency, measured in the coordinator, i.e., the time interval elapsed from the first checkpoint message sent to the last done message received at the coordinator. Figure 5(b) shows the estimated overhead associated with coordination. The overhead was computed by subtracting from the total checkpoint latency measured in Figure 5(a), the time spent in executing local operations of checkpoint and continue in the application nodes. Since application nodes execute these local operations in parallel, we consider the global cost of each local operation as the maximum time measured in all nodes.

The results of Figure 5(a) show an overhead of approximately 1 second for checkpointing the slm benchmark for all node configurations. This time is a function of the size of the application state that needs to be saved and is dominated by the time to write this state to disk. In general, most of the state consists of the non-zero contents of the virtual memory of all processes running in the pod.

More importantly, the results of Figure 5(b) show that the overhead for coordination is negligible, on the order of 350 µs to 550 µs. The graph shows that the overhead increases by approximately 50 µs for each node for configurations with more than 4 nodes. This suggests that our checkpoint mechanism should scale to a large number of nodes before the overhead becomes comparable with the checkpoint time. Performance results for the restart operation are similar to the results of Figures 5(a) and 5(b) but are omitted here due to space limitations.

We performed experiments using a TCP streaming benchmark running between two nodes to evaluate the performance impact of packet drops in the network when communication is disabled for the nodes to perform checkpoints. Figure 6 shows the measured rate of the TCP stream between two nodes as a function of time. The plotted rate corresponds to the average rate measured in the receiver during a sliding window of 10 ms duration previous to the corresponding point. A checkpoint operation is started at time t = 0 when the rate drops to zero. The checkpoint operation completes after approximately 120 ms. At this time, the receiver continues consuming data in the TCP receive buffer that arrived before the checkpoint operation was started, illustrated by the short pulse at 120 ms. However, the sender does not continue sending data until later when it recovers from lost packets due to our network filter. At this time, communication resumes at the normal rate as before the checkpoint operation. The small oscillations in the curve are due to the fact that the application receives bytes in multiples of packets, which in this experiment have the maximum size of 1500 bytes. Although the network filters used in our distributed checkpoint caused packets to be dropped and network communication to be suspended, this perturbation is small, with normal communication restarting after approximately 100 ms.

### 7. Summary

We presented Cruz, a powerful and general-purpose checkpoint-restart mechanism that improves the operation of computing environments, reducing both planned and unplanned downtime and increasing resource allocation flexibility. Our mechanism has two main contributions: First, we enable saving and restoring the state of live TCP connections. Second, we leverage this capability to develop a new lightweight distributed checkpoint-restart mechanism that uses the fewest messages necessary to ensure the atomicity of the global coordinated checkpoint. We have implemented our mechanism and evaluated its performance using a scientific parallel application and a network-intensive benchmark. Our results show negligible coordination overhead, demonstrating the scalability of our approach. The results suggest that the system should scale to a large number of nodes before coordination overhead becomes comparable to the time to perform local checkpoint or restart.

We propose performance optimizations to our base solution. As future work, we plan to evaluate the benefits of these optimizations and assess the performance of our mechanism across a wide range of applications and cluster configurations.

### References

[1] J. Casas, D. L. Clark, R. Konuru, S. W. Otto, R. M. Prouty, and J. Walpole. MPVM: A migration transparent version of PVM. Computing Systems, 8(2):171–216, 1995.
[2] K. M. Chandy and L. Lamport. Distributed snapshots: Determining global states of distributed systems. ACM Transactions on Computer Systems, 3(1):63–75, Feb. 1985.
[3] J. Duell, P. Hargrove, and E. Roman. The design and implementation of Berkeley Labs’ Linux checkpoint/restart. 2002.
[4] E. N. Elnozahy and W. Zwaenepoel. On the use and implementation of message logging. In Int’l Symp on Fault-Tolerant Computing, Jun 1994.
[5] E. N. M. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. Johnson. A survey of rollback-recovery protocols in message-passing systems. ACM Computing Surveys, 34(3):375–408, Sep 2002.
[6] I. Foster, C. Kesselman, J. Nick, and S. Tuecke. Grid Services for distributed system integration. Computer, 35(6), 2002.
[7] A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. S. Sunderam. PVM: Parallel Virtual Machine A Users’ Guide and Tutorial for Networked Parallel Computing. MIT Press, 2000.
[8] P. Jalote. Fault Tolerance in Distributed Systems. PTR Prentice Hall, 1994.
[9] M. Litzkow, T. Tanenbaum, J. Basney, and M. Livny. Checkpoint and migration of UNIX processes in the Condor distributed processing system. Computer Sciences Technical Report 1346, University of Wisconsin, Madison, WI, 1997.
[10] J. Mogul, L. Brakmo, D. Lowell, D. Subhraveti, and J. Moore. Unveiling the transport. In Second Workshop on Hot Topics in Networks, Nov 2003.
[11] N. Neves and W. K. Fuchs. RENEW: A tool for fast and efficient implementation of checkpoint protocols. In Int’l Symp on Fault-Tolerant Computing, Jun 1998.
[12] S. Osman, D. Subhraveti, G. Su, and J. Nieh. The design and implementation of Zap: A system for migrating computing environments. In Fifth Symp. on Operating System Design and Implementation (OSDI 2002), pages 361–376, Dec 2002.
[13] J. S. Plank, M. Beck, G. Kingsley, and K. Li. Libckpt: Transparent checkpointing under Unix. In Usenix Winter 1995 Technical Conference, pages 213–224, 1995.
[14] Platform Computing Corporation. LSF User’s Guide. www.platform.com.
[15] S. Sankaran, J. M. Squyres, B. Barrett, A. Lumsdaine, J. Duell, P. Hargrove, and E. Roman. The LAM/MPI checkpoint/restart framework: System-initiated checkpointing. In Proceedings, LACSI Symposium, October 2003.
[16] M. Snir, S. Otto, S. Huss-Lederman, D. Walker, and J. Dongarra. MPI: The Complete Reference (Vol. 1) - 2nd Edition. MIT Press, 1998.
[17] G. Stellner. Cocheck: Checkpointing and process migration for MPI. In 10th International Parallel Processing Symposium (IPPS 1996), 1996.
[18] W. R. Stevens. TCP/IP Illustrated, Volume 1 The Protocols. Addison-Wesley, 1994.