以下是优化后的文本，使其更加清晰、连贯和专业：

---

### 参考文献

1. D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, K. Rieck, 和 C. Siemens. "DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket." 在 Ndss, 卷 14, 页 23–26, 2014.
2. A. Athalye, N. Carlini, 和 D. Wagner. "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples." arXiv 预印本 arXiv:1802.00420, 2018.
3. S. Axelsson. "The Base-Rate Fallacy and Its Implications for the Difficulty of Intrusion Detection." 在 Proceedings of the 6th ACM Conference on Computer and Communications Security, 页 1–7. ACM, 1999.
4. B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, 和 F. Roli. "Evasion Attacks Against Machine Learning at Test Time." 在 Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 页 387–402. Springer, 2013.
5. X. Cao 和 N. Z. Gong. "Mitigating Evasion Attacks to Deep Neural Networks via Region-Based Classification." 在 Proceedings of the 33rd Annual Computer Security Applications Conference, 页 278–287. ACM, 2017.
6. N. Carlini 和 D. Wagner. "Towards Evaluating the Robustness of Neural Networks." 在 IEEE Symposium on Security and Privacy (SP), 页 39–57. IEEE, 2017.
7. H. Chen, H. Zhang, D. Boning, 和 C.-J. Hsieh. "Robust Decision Trees Against Adversarial Examples." 2019.
8. T. Chen 和 C. Guestrin. "XGBoost: A Scalable Tree Boosting System." 在 Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 页 785–794. ACM, 2016.
9. Y. Chen, Y. Nadji, A. Kountouras, F. Monrose, R. Perdisci, M. Antonakakis, 和 N. Vasiloglou. "Practical Attacks Against Graph-Based Clustering." 在 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 页 1125–1142. ACM, 2017.
10. H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, 和 L. Song. "Adversarial Attack on Graph Structured Data." arXiv 预印本 arXiv:1806.02371, 2018.
11. H. Dang, Y. Huang, 和 E.-C. Chang. "Evading Classifiers by Morphing in the Dark." 在 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 页 119–133. ACM, 2017.
12. T. Dreossi, S. Jha, 和 S. A. Seshia. "Semantic Adversarial Deep Learning." arXiv 预印本 arXiv:1804.07045, 2018.
13. S. Dutta, S. Jha, S. Sankaranarayanan, 和 A. Tiwari. "Output Range Analysis for Deep Feedforward Neural Networks." 在 NASA Formal Methods Symposium, 页 121–138. Springer, 2018.
14. K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O’Donoghue, J. Uesato, 和 P. Kohli. "Training Verified Learners with Learned Verifiers." arXiv 预印本 arXiv:1805.10265, 2018.
15. K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, 和 P. Kohli. "A Dual Approach to Scalable Verification of Deep Networks." arXiv 预印本 arXiv:1803.06567, 2018.
16. R. Ehlers. "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks." 在 15th International Symposium on Automated Technology for Verification and Analysis, 2017.
17. M. Fischetti 和 J. Jo. "Deep Neural Networks as 0-1 Mixed Integer Linear Programs: A Feasibility Study." arXiv 预印本 arXiv:1712.06174, 2017.
18. T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, 和 M. Vechev. "AI²: Safety and Robustness Certification of Neural Networks with Abstract Interpretation." 在 IEEE Symposium on Security and Privacy (SP), 2018.
19. I. Goodfellow, J. Shlens, 和 C. Szegedy. "Explaining and Harnessing Adversarial Examples." 在 International Conference on Learning Representations (ICLR), 2015.
20. S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, T. Mann, 和 P. Kohli. "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models." arXiv 预印本 arXiv:1810.12715, 2018.
21. K. Grosse, N. Papernot, P. Manoharan, M. Backes, 和 P. McDaniel. "Adversarial Perturbations Against Deep Neural Networks for Malware Classification." arXiv 预印本 arXiv:1606.04435, 2016.
22. W. Guo, D. Mu, J. Xu, P. Su, G. Wang, 和 X. Xing. "Lemna: Explaining Deep Learning Based Security Applications." 在 Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, 页 364–379. ACM, 2018.
23. W. Hu 和 Y. Tan. "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN." arXiv 预印本 arXiv:1702.05983, 2017.
24. X. Huang, M. Kwiatkowska, S. Wang, 和 M. Wu. "Safety Verification of Deep Neural Networks." 在 International Conference on Computer Aided Verification (CAV), 页 3–29. Springer, 2017.
25. I. Incer, M. Theodorides, S. Afroz, 和 D. Wagner. "Adversarially Robust Malware Detection Using Monotonic Classification." 在 Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics, 页 54–63. ACM, 2018.
26. A. Kantchelian, J. Tygar, 和 A. Joseph. "Evasion and Hardening of Tree Ensemble Classifiers." 在 International Conference on Machine Learning, 页 2387–2396, 2016.
27. G. Katz, C. Barrett, D. L. Dill, K. Julian, 和 M. J. Kochenderfer. "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks." 在 International Conference on Computer Aided Verification (CAV), 页 97–117. Springer, 2017.
28. A. Kurakin, I. Goodfellow, 和 S. Bengio. "Adversarial Machine Learning at Scale." arXiv 预印本 arXiv:1611.01236, 2016.
29. P. Laskov 等. "Practical Evasion of a Learning-Based Classifier: A Case Study." 在 Security and Privacy (SP), 2014 IEEE Symposium on, 页 197–211. IEEE, 2014.
30. A. Lomuscio 和 L. Maganti. "An Approach to Reachability Analysis for Feed-Forward ReLU Neural Networks." arXiv 预印本 arXiv:1706.07351, 2017.
31. D. Lowd 和 C. Meek. "Adversarial Learning." 在 Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, 页 641–647. ACM, 2005.
32. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, 和 A. Vladu. "Towards Deep Learning Models Resistant to Adversarial Attacks." International Conference on Learning Representations (ICLR), 2018.
33. D. Maiorca, I. Corona, 和 G. Giacinto. "Looking at the Bag Is Not Enough to Find the Bomb: An Evasion of Structural Methods for Malicious PDF Files Detection." 在 Proceedings of the 8th ACM SIGSAC Symposium on Information, Computer and Communications Security, 页 119–130. ACM, 2013.
34. M. Mirman, T. Gehr, 和 M. Vechev. "Differentiable Abstract Interpretation for Provably Robust Neural Networks." 在 International Conference on Machine Learning (ICML), 页 3575–3583, 2018.
35. N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, 和 A. Swami. "Practical Black-Box Attacks Against Machine Learning." 在 Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 页 506–519. ACM, 2017.
36. N. Papernot, P. McDaniel, X. Wu, S. Jha, 和 A. Swami. "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks." arXiv 预印本 arXiv:1511.04508, 2015.
37. A. Raghunathan, J. Steinhardt, 和 P. Liang. "Certified Defenses Against Adversarial Examples." International Conference on Learning Representations (ICLR), 2018.
38. A. Raghunathan, J. Steinhardt, 和 P. S. Liang. "Semidefinite Relaxations for Certifying Robustness to Adversarial Examples." 在 Advances in Neural Information Processing Systems, 页 10900–10910, 2018.
39. J. Saxe 和 K. Berlin. "Deep Neural Network Based Malware Detection Using Two-Dimensional Binary Program Features." 在 Malicious and Unwanted Software (MALWARE), 2015 10th International Conference on, 页 11–20. IEEE, 2015.
40. C. Smutz 和 A. Stavrou. "Malicious PDF Detection Using Metadata and Structural Features." 在 Proceedings of the 28th Annual Computer Security Applications Conference, 页 239–248. ACM, 2012.
41. N. Šrndić 和 P. Laskov. "Detection of Malicious PDF Files Based on Hierarchical Document Structure." 在 Proceedings of the 20th Annual Network & Distributed System Security Symposium, 页 1–16. Citeseer, 2013.
42. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, 和 R. Fergus. "Intriguing Properties of Neural Networks." International Conference on Learning Representations (ICLR), 2013.
43. V. Tjeng, K. Xiao, 和 R. Tedrake. "Evaluating Robustness of Neural Networks with Mixed Integer Programming." arXiv 预印本 arXiv:1711.07356, 2017.
44. L. Tong, B. Li, C. Hajaj, C. Xiao, N. Zhang, 和 Y. Vorobeychik. "Improving Robustness of ML Classifiers Against Realizable Evasion Attacks Using Conserved Features." 在 28th USENIX Security Symposium (USENIX Security 19), 页 285–302, 2019.
45. D. Wagner 和 P. Soto. "Mimicry Attacks on Host-Based Intrusion Detection Systems." 在 Proceedings of the 9th ACM Conference on Computer and Communications Security, 页 255–264. ACM, 2002.
46. S. Wang, Y. Chen, A. Abdou, 和 S. Jana. "MixTrain: Scalable Training of Formally Robust Neural Networks." arXiv 预印本 arXiv:1811.02625, 2018.
47. S. Wang, Y. Chen, A. Abdou, 和 S. Jana. "Enhancing Gradient-Based Attacks with Symbolic Intervals." arXiv 预印本 arXiv:1906.02282, 2019.
48. S. Wang, K. Pei, W. Justin, J. Yang, 和 S. Jana. "Efficient Formal Safety Analysis of Neural Networks." Advances in Neural Information Processing Systems (NIPS), 2018.
49. S. Wang, K. Pei, W. Justin, J. Yang, 和 S. Jana. "Formal Security Analysis of Neural Networks Using Symbolic Intervals." 27th USENIX Security Symposium, 2018.
50. T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, 和 L. Daniel. "Towards Fast Computation of Certified Robustness for ReLU Networks." arXiv 预印本 arXiv:1804.09699, 2018.
51. E. Wong 和 Z. Kolter. "Provable Defenses Against Adversarial Examples via the Convex Outer Adversarial Polytope." 在 International Conference on Machine Learning, 页 5283–5292, 2018.
52. E. Wong, F. Schmidt, J. H. Metzen, 和 J. Z. Kolter. "Scaling Provable Adversarial Defenses." Advances in Neural Information Processing Systems (NIPS), 2018.
53. W. Xu. "PDF-Malware-Parser for EvadeML." https://github.com/mzweilin/PDF-Malware-Parser.
54. W. Xu, Y. Qi, 和 D. Evans. "Automatically Evading Classifiers." 在 Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.
55. X. Zhang 和 D. Evans. "Cost-Sensitive Robustness Against Adversarial Examples." arXiv 预印本 arXiv:1810.09225, 2018.

---

### 附录

#### A.1 集成分类器的 VRA

##### A.1.1 集成 A+B 的 VRA
**属性 A:** 如果所有距离为一的子树删除都是安全的，则测试 PDF 被验证为在属性 A 下是安全的。因此，对于每个表示一个子树删除的区间，我们要求对应的两个子树删除中的任何一个被分类为恶意。

#### A.2 有界梯度攻击下的 ERA
表 8 显示了左侧模型的精度和召回率，以及右侧在有界梯度攻击下的 ERA。所有可验证鲁棒模型保持高精度和召回率。模型的 ERA 值高于表 4 中的 VRA 值。

#### A.3 无约束梯度攻击结果

##### A.3.1 ERA
**属性 B:** 属性 B 是集成 A+B 的可证明鲁棒性属性。如果任何通过向恶意 PDF 插入一个任意子树生成的变异 PDF 具有与恶意 PDF 种子相同的分类结果，则该属性成立。因此，我们使用恶意 PDF 的测试准确率作为属性 B 的 VRA。

**属性 C:** 如果所有距离为二的子树删除都是安全的，则测试 PDF 被验证为在属性 C 下是安全的。因此，对于每个表示两个子树删除的区间，我们要求对应的三个子树删除中的任何一个被分类为恶意。

**属性 D:** 如果所有距离为 41 的子树插入都是安全的，则测试 PDF 被验证为在属性 D 下是安全的。因此，我们测试是否可以将表示对恶意测试 PDF 进行 40 次子树插入的任何区间分类为恶意。

**属性 E:** 如果在整个特征空间内的所有可能子树插入都是安全的，则测试 PDF 被验证为在属性 E 下是安全的。因此，我们测试是否可以将表示对恶意测试 PDF 进行所有但一次（41）子树插入的任何区间分类为恶意。

##### A.1.2 集成 D 的 VRA
**属性 A 和 C:** 如果在某些删除后任何子树被分类为恶意，则测试 PDF 被验证为对于删除属性是安全的。因此，对于每个测试 PDF，我们检查表示从全零到原始子树的所有区间的下界是否可以被分类为恶意。

**属性 B, D 和 E:** 如果在某些插入后任何子树被分类为恶意，则测试 PDF 被验证为对于插入属性是安全的。有两种情况：如果插入的子树不存在，则区间是从全零到全一；如果插入的子树已经存在，则区间界限是从原始子树特征到全一。我们检查这些区间中的任何一个是否可以被分类为恶意。

#### A.3.2 收敛
我们运行了 200,000 次迭代的无约束梯度攻击，并绘制了前 50,000 次迭代的 ERA。

---

### 模型性能

| 模型 | 精度 (%) | 召回率 (%) |
|------|----------|------------|
| Baseline NN | 99.97 | 99.97 |
| Adv Retrain A | 99.97 | 99.97 |
| Adv Retrain B | 99.97 | 99.97 |
| Adv Retrain C | 99.97 | 99.97 |
| Adv Retrain D | 99.97 | 99.97 |
| Adv Retrain A+B | 99.97 | 99.97 |
| Ensemble A+B* | 99.97 | 99.97 |
| Ensemble D* | 99.97 | 99.97 |
| Robust A | 99.97 | 99.97 |
| Robust B | 99.97 | 99.97 |
| Robust C | 100 | 99.94 |

---

这样处理后，文本更加清晰、连贯和专业。