### 6. C. Lee et al.

C. Lee et al. aim to identify the primary cause of the performance differences between two fully utilized traces. Despite this, 99.3% of `trace-full1` and 95.4% of `trace-full2` experience a loss rate of less than 0.2%.

The end-to-end loss rate experienced by a flow is at least as high as what we measure at the border router. The loss rate in Figure 5(a) represents the lower bound. Measuring the end-to-end loss rate for a TCP flow without direct access to both the source and destination is not straightforward. Consider the following example: A bundle of packets is in transit to the destination. The first packet in the bundle is dropped at one hop, and the second packet is dropped at a later hop. The sender may retransmit the entire bundle based on the detection of the first packet loss, unaware of the second packet loss. By monitoring the retransmission of the entire bundle at the hop where the first packet was lost, one may not be able to determine if the second packet was also dropped.

To examine the end-to-end loss performance, we analyze the retransmission rate observed at the capturing point Core. The retransmission rate is calculated based on the number of duplicate TCP sequence numbers. There could be loss between the source and the border router, and the retransmission rate we observe is equal to or lower than what the source sees. However, we expect the loss in the campus local area network to be minimal and refer to the retransmission rate at the border router as end-to-end. We plot the retransmission rates for the three traces in Figure 5(b). We use a logarithmic scale on the x-axis and cannot plot the case of 0% retransmitted packets. For `trace-dawn`, 28.9% of traffic has no retransmissions. For `trace-full1` and `trace-full2`, 18.3% and 3.8% of traffic, respectively, have no retransmissions. As with single-hop loss, `trace-full2` has worse retransmission rates than `trace-full1`.

We count the flows that experience no loss at our border router but have retransmitted packets. These account for 34.9% of total TCP traffic in `trace-full1` and 9.4% in `trace-full2`. For these flows, the bottleneck exists at some other point in the network, and our link is not their bottleneck. Even at 100% utilization, our link is not always the bottleneck for all flows.

This analysis focuses only on TCP flows. It is noted that UDP flows in our traces can have higher loss rates than elephant TCP flows because the TCP congestion control algorithm reduces loss rates by throttling packet sending rates.

### 4.2 Delay

Next, we study delay, aiming to examine the impact of the local delay added by our fully utilized link on the round-trip time (RTT) of the whole path. To calculate the single-hop delay, we subtract the timestamp of each packet at the capturing point Core from the timestamp of the same packet captured at Border. We calculate the single-hop delay for each packet in the flows from each of the three traces and plot the distributions in Figure 6(a). `Trace-dawn` has almost no queuing delay at our border router. The median queuing delay for `trace-full1` and `trace-full2` is 38.3 ms and 44.6 ms, respectively, with delays oscillating from 20 ms to 60 ms. Such high queuing delay significantly affects user experience, which we will discuss in the next section.

To infer the RTT of each flow from bi-directional packet traces collected in the middle of the path, we adopt techniques by Jaiswal et al. [10]. Their tool tracks the TCP congestion window and provides RTT samples for each ACK and data packet pair. Figure 6(b) shows the average per-flow RTT distribution weighted by the flow size. We note that the large queuing delay at the router adds significant delay to RTT for both `trace-full1` and `trace-full2`.

### 5. Impact of Congestion on Application Performance

So far, we have investigated the impact of network congestion measured on our campus on the performance degradation in terms of per-flow end-to-end delays and packet losses. Now, we turn our attention to an application-specific view and examine the impact of the fully-utilized link on the user-perceived performance.

#### 5.1 Web Flows

In this subsection, we consider web flows and examine the variation in their RTTs caused by the 100% utilized link. Since port-based classification of web traffic is known to be fairly accurate [12], we select the flows whose TCP source port number is 80 and assume they are web flows. We then divide these flows into three geographic regional cases: domestic, China and Japan, and other countries. Each case includes flows with destination addresses located in the respective region. Our mapping of an IP address to a country is based on MaxMind’s GeoIP [3].

In Figure 7, we plot the RTT distributions of web flows for different network conditions. For all three regional cases, we observe that `trace-full1` and `trace-full2` have larger RTTs than `trace-dawn`. In Section 4, we observed that the median single-hop delay at the border router is 38.3 ms in `trace-full1` (44.6 ms in `trace-full2`) when its link is fully utilized, and our observations in Figure 7 confirm such queuing delay increase.

In the domestic case, 92.2% of web flows experience RTTs less than 50 ms in `trace-dawn`, while only 36.2% (9.8% in `trace-full2`) have delays less than 50 ms during the fully utilized period. We observe a similar trend in the case of China and Japan, but the delay increase is less severe for the case of other countries. Most flows have RTTs larger than 100 ms regardless of the network condition.

Khirman et al. studied the effect of HTTP response time on users' cancellation decisions for HTTP requests. They reported that any additional improvement of response time in the 50-500 ms range does not significantly affect user experience, as the cancellation rate remains almost the same in that range. They also found that additional delay improvements below 50 ms bring better user experience. According to these findings, our measurements show that users in `trace-dawn` are more satisfied than those in the fully utilized traces when connecting to domestic Internet hosts. On the other hand, user experience for foreign flows remains similar across all three traces because most RTTs fall between 50 ms and 500 ms, regardless of the link utilization level.

#### 5.2 Bulk Transfer Flows

We now examine the performance change of bulk transfer flows under full utilization. Bulk transfer flows may deliver high-definition pictures, videos, executables, etc. Unlike web flows, where we analyze the degradation in RTTs, we examine per-flow throughput, which is a primary performance metric for download completion time. We first identify bulk transfer flows as those larger than 1 MB from each trace and classify them into the same three geographic regional cases used in the web flow analysis. We summarize the results in Figure 8.

In the domestic case, 85.0% of bulk transfer flows have throughputs larger than 1 MByte/sec in `trace-dawn`. When the network is fully utilized, the performance degrades significantly, and only 36.6% (9.6% in `trace-full2`) of the total volume have throughput larger than 1 MByte/sec, as shown in Figure 8(a). In Figure 8(c), the previous observation that `trace-dawn` has better throughput than the others disappears. We conjecture that our fully-loaded link has a minor effect on the throughput of overseas bulk transfers. Other possible causes that limit a TCP flow's throughput include sender/receiver window, network congestion on the other side [16]. We plan to categorize the flows according to each throughput-limiting factor in future work.

We are aware that comparing RTTs and throughputs from different traces may not be fair since the source and destination hosts of flows can differ in each trace. However, we expect that the effect of host variation on campus should not be too serious because most hosts on campus are Windows-based and have the same 100 Mbps wired connection to the Internet.

### 6. Related Work

Several references report on heavily utilized links in operational networks [5, 6, 8]. Link performance under varying utilization up to 100% has been studied in the context of finding the proper buffer size at routers. Most studies, however, have relied on simulation and testbed experiment results [4, 9, 14, 15]. Such experiments have limitations in that the network scale and generated traffic conditions cannot match those of an operational network. In our work, we report measurement results of 100% utilization at a real-world network link using collected packet-level traces, allowing for more detailed and accurate analysis.

### 7. Conclusions

In this paper, we reveal the degree of performance degradation at a 100% utilized link using packet-level traces. Our link has been fully utilized during peak hours for more than three years, and this paper is the first report on such persistent congestion. We observed that 100% utilization at a 1 Gbps link can make more than half of the TCP volume in the link suffer from packet loss, but the loss rate is not as high as expected; 95% of the total TCP volume has a single-hop loss rate of less than 0.2%. The median single-hop queuing delay also increased to about 40 ms when the link is busy. Comparing `trace-full1` and `trace-full2`, we confirm that even the same 100% utilization can result in different levels of performance degradation depending on traffic conditions. We plan to explore the main cause of this difference in the future. On the other hand, a fully utilized link significantly worsens user satisfaction with increased RTT for domestic web flows, while foreign flows suffer less. Bulk file transfers also experience severe throughput degradation. This paper serves as a good reference for network administrators facing future congestion in their networks.

We have two future research directions from the measurement results in this paper. First, we plan to apply small buffer schemes [4, 9, 14] to our network link to see whether they still work on a 100% utilized link in the real world. Second, we plan to develop a method to estimate bandwidth demand in a congested link. When network operators want to upgrade the capacity of their links, predicting the exact potential bandwidth of the current traffic is important to make an informed decision.

### Acknowledgements

This work was supported by the IT R&D program of MKE/KEIT [KI001878, “CASFI: High-Precision Measurement and Analysis Research”] and Korea Research Council of Fundamental Science and Technology.

### References

1. Cisco Visual Networking Index: Forecast and Methodology 2009-2014 (White paper), <http://www.cisco.com/en/US/solutions/collateral/ns341/ns525/ns537/ns705/ns827/white_paper_c11-481360.pdf>
2. Endace, <http://www.endace.com>
3. Maxmind’s geoip country database, <http://www.maxmind.com/app/country>
4. Appenzeller, G., Keslassy, I., McKeown, N.: Sizing Router Buffers. In: Proc. ACM SIGCOMM (2004)
5. Beheshti, N., Ganjali, Y., Ghobadi, M., McKeown, N., Salmon, G.: Experimental Study of Router Buffer Sizing. In: Proc. ACM SIGCOMM IMC (2008)
6. Borgnat, P., Dewaele, G., Fukuda, K., Abry, P., Cho, K.: Seven Years and One Day: Sketching the Evolution of Internet Traffic. In: Proc. IEEE INFOCOM (2009)
7. Cho, K., Fukuda, K., Esaki, H., Kato, A.: Observing Slow Crustal Movement in Residential User Traffic. In: Proc. ACM CoNEXT (2008)
8. Choi, B., Moon, S., Zhang, Z., Papagiannaki, K., Diot, C.: Analysis of Point-to-Point Packet Delay in an Operational Network. Comput. Netw. 51, 3812–3827 (2007)
9. Dhamdhere, A., Jiang, H., Dovrolis, C.: Buffer Sizing for Congested Internet Links. In: Proc. IEEE INFOCOM (2005)
10. Jaiswal, S., Iannaccone, G., Diot, C., Kurose, J., Towsley, D.: Inferring TCP Connection Characteristics Through Passive Measurements. In: Proc. IEEE INFOCOM (2004)
11. John, W., Tafvelin, S.: Analysis of Internet Backbone Traffic and Header Anomalies Observed. In: Proc. ACM SIGCOMM IMC (2007)
12. Kim, H., Claffy, K., Fomenkov, M., Barman, D., Faloutsos, M., Lee, K.: Internet Traffic Classification Demystified: Myths, Caveats, and the Best Practices. In: Proc. ACM CoNEXT (2008)
13. Papagiannaki, K., Moon, S., Fraleigh, C., Thiran, P., Tobagi, F., Diot, C.: Analysis of Measured Single-Hop Delay from an Operational Backbone Network. In: Proc. IEEE INFOCOM (2002)
14. Prasad, R., Dovrolis, C., Thottan, M.: Router Buffer Sizing Revisited: the Role of the Output/Input Capacity Ratio. In: Proc. ACM CoNEXT (2007)
15. Sommers, J., Barford, P., Greenberg, A., Willinger, W.: An SLA Perspective on the Router Buffer Sizing Problem. SIGMETRICS Perform. Eval. Rev. 35, 40–51 (2008)
16. Zhang, Y., Breslau, L., Paxson, V., Shenker, S.: On the Characteristics and Origins of Internet Flow Rates. In: Proc. ACM SIGCOMM (2002)