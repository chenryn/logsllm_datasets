### 6.4 Discussion

In the simple cost model introduced in Section 5.2, we have \( m = 10,452 \) and \( V' = 424 \), giving \( \frac{V'}{m} = 0.04 \). With \( p = 0.65 \), we see that Vulture performs more than fifteen times better than random assignment.

For ranking, all Q values are higher than 0.6; the average values are significantly above this threshold. This more than satisfies our criterion from Section 5.3.

Therefore, our case study demonstrates three key points:
1. Allocating quality assurance efforts based on Vulture predictions achieves a reasonable balance between effectiveness and efficiency.
2. It is effective because half of all vulnerable components are flagged.
3. Vulture is efficient because directing quality assurance efforts to flagged components yields a return of 70%—more than two out of three components are correctly identified. Focusing on the top-ranked components will yield even better results.

Furthermore, these numbers show an empirical and undeniable correlation between imports and function calls on one hand, and vulnerabilities on the other. This correlation can be profitably exploited by tools like Vulture to make predictions that are accurate enough to make a difference when allocating testing effort. Vulture has also identified features that very often lead to vulnerabilities when used together, pointing out areas that should perhaps be redesigned for greater security.

Best of all, Vulture has achieved this automatically, quickly, and without relying on intuition or human expertise. This provides programmers and managers with much-needed objective data to:
- Identify where past vulnerabilities were located,
- Predict other components that are likely to be vulnerable, and
- Effectively allocate quality assurance efforts.

### 7. Related Work

Previous work in this area has reduced the number of vulnerabilities or their impact through various methods:

#### Component Histories
The Vulture tool was inspired by the pilot study by Schröter et al. [29], who first observed that imports correlate with failures. While Schröter et al. examined general defects, the present work focuses specifically on vulnerabilities. To our knowledge, this is the first work that mines and leverages vulnerability databases to make predictions. Our correlation, precision, and recall values are higher than theirs, which supports the idea that focusing on vulnerabilities rather than general bugs is worthwhile.

#### Evolution of Defect Numbers
Both Ozment et al. [23] and Li et al. [17] have studied how the numbers of defects and security issues evolve over time. Ozment et al. report a decrease in the rate at which new vulnerabilities are reported, while Li et al. report an increase. Neither approach allows mapping vulnerabilities to components or making predictions.

#### Estimating the Number of Vulnerabilities
Alhazmi et al. use the rate at which vulnerabilities are discovered to build models predicting the number of undiscovered vulnerabilities [2]. Their approach is applied to entire systems rather than source files. In contrast, Vulture's predictions do not depend on a model of how vulnerabilities are discovered.

Miller et al. build formulas estimating the number of defects in software, even when testing reveals no flaws [20]. Their formulas incorporate random testing results, information about the input distribution, and prior assumptions about the probability of failure. However, they do not consider the software’s history—their estimates remain constant regardless of the historical data.

Tofts et al. build simple dynamic models of security flaws by treating security as a stochastic process [35], but they do not make specific predictions about vulnerable software components. Yin et al. [39] highlight the need for a framework for estimating security risks in large software systems, but they provide neither an implementation nor an evaluation.

#### Testing the Binary
This involves subjecting the binary executable (not the source code) to various forms of testing and analysis, such as fuzz testing [19] and fault injection [38]. Eric Rescorla argues that finding and patching security holes does not necessarily improve software quality [25]. However, he refers to third-party outsiders finding security holes in finished products, not in-house personnel during development. Thus, his conclusions do not contradict our belief that Vulture is a useful tool.

#### Examining the Source
This is typically done with an eye towards specific vulnerabilities, such as buffer overflows. Approaches include linear programming [12], data-flow analysis [14], locating functions near a program’s input [8], axiomatizing correct pointer usage [11], exploiting semantic comments [16], checking path conditions [31], symbolic pointer checking [28], and symbolic bounds checking [26].

Rather than describing the differences between these tools and ours in every case, we briefly discuss ITS4, developed by Viega et al. [37], which is representative of many static code scanners. Viega et al. aimed to create a tool fast enough for real-time feedback during development and precise enough that programmers would not ignore it. Since their approach is pattern-based, it must be manually extended as new patterns emerge. The person extending it must understand the vulnerability before it can be condensed into a pattern. Vulture may not flag components containing unknown vulnerabilities at training time, but it will flag components that contain previously fixed vulnerabilities.

ITS4 checks local properties, making it difficult to find security-related defects arising from interactions between distant components. Additionally, ITS4 cannot adapt to programs with safe but pattern-violating practices, as it ignores a component’s history.

Another approach is model checking [3, 4], where specific classes of vulnerabilities are formalized, and the program is checked for violations. The advantage is that if a failure is detected, the model checker provides a concrete counter-example for regression testing. Like ITS4, it must be extended as new formalizations emerge, and some vulnerability types might not be formalizable.

Vulture also contains static scanners, detecting features by parsing the source code. However, Vulture aims to direct testing efforts by providing a probabilistic assessment of the code’s vulnerability, rather than pinpointing specific lines with potential buffer overflows.

#### Hardening the Source or Runtime Environment
This encompasses measures to mitigate a program’s ability to cause damage. Tools like StackGuard [6] and mandatory access controls (e.g., AppArmor [5] and SELinux [22]) lower a vulnerability’s impact. However, Vulture works to prevent vulnerabilities during development, aiming to direct programmers and managers to code requiring attention, thereby reducing the need for such hardening measures.

### 8. Conclusions and Future Work

We have presented empirical evidence that features correlate with vulnerabilities. Based on this, we introduced Vulture, a tool that predicts vulnerable components by analyzing their features. It is fast and reasonably accurate, analyzing a project as complex as Mozilla in about half an hour and correctly identifying half of the vulnerable components. Two-thirds of its predictions are correct.

The contributions of this paper are:
1. A technique for mapping past vulnerabilities by mining and combining vulnerability databases with version archives.
2. Empirical evidence contradicting the popular belief that vulnerable components will generally have more vulnerabilities in the future.
3. Evidence that features correlate with vulnerabilities.
4. A tool that learns from past vulnerability locations to predict future ones with reasonable accuracy.
5. An approach for identifying vulnerabilities that automatically adapts to specific projects and products.
6. A predictor for vulnerabilities that only needs a set of suitable features, applicable before the component is fully implemented.

Despite these contributions, we feel that our work has just scratched the surface. Our future work will focus on:
- Characterizing domains: We believe that features are good predictors for vulnerabilities because they characterize a component’s domain. We plan to test this hypothesis across multiple systems in similar domains.
- Fine-grained approaches: Examining features at finer levels, such as caller-callee relationships, may allow vulnerability predictions for classes, methods, or functions.
- Evolved components: This work primarily applies to predicting vulnerabilities in new components. However, components already in production come with their own vulnerability history, which we expect to be a strong predictor for future vulnerabilities.
- Usability: Currently, Vulture is a batch program producing textual output. We plan to integrate Vulture into current development environments, allowing programmers to query for vulnerable components and visualize vulnerabilities (Figure 10).

In a recent blog, Bruce Schneier wrote, “If the IT products we purchased were secure out of the box, we wouldn’t have to spend billions every year making them secure.” [27] One first step to improve security is to learn where and why current software had flaws in the past. Our approach provides essential ground data for this purpose, enabling effective predictions of where software should be secured in the future.

### Acknowledgments

We thank the anonymous reviewers for their helpful comments. We also thank the Mozilla team for making their databases available. David Schuler and Andrzej Wasylkowski provided valuable feedback on earlier revisions of this paper. Thomas Zimmermann is funded by a stipend from the DFG-Graduiertenkolleg “Leistungsgarantien für Rechnersysteme”. Vulture is part of the “Mining software archives” project at Saarland University. For more information, see http://www.st.cs.uni-sb.de/softevo/.

### 9. References

[1] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Jorge B. Bocca, Matthias Jarke, and Carlo Zaniolo, editors, Proc. 20th Int’l Conf. on Very Large Data Bases, VLDB, pages 487–499. Morgan Kaufmann, September 1994.

[2] Omar Alhazmi, Yashwant Malaiya, and Indrajit Ray. Security Vulnerabilities in Software Systems: A Quantitative Perspective, volume 3645/2005 of Lecture Notes in Computer Science, pages 281–294. Springer Verlag, Berlin, Heidelberg, August 2005.

[3] Hao Chen, Drew Dean, and David Wagner. Model checking one million lines of C code. In Proc. 11th Annual Network and Distributed System Security Symposium (NDSS), pages 171–185, February 2004.

[4] Hao Chen and David Wagner. MOPS: An infrastructure for examining security properties of software. In Proc. 9th ACM Conf. on Computer and Communications Security (CCS), pages 235–244, November 2002.

[5] Crispin Cowan. Apparmor linux application security. http://www.novell.com/linux/security/apparmor/, January 2007.

[6] Crispin Cowan, Calton Pu, Dave Maier, Jonathan Walpole, Peat Bakke, Steve Beattie, Aaron Grier, Perry Wagle, Qian Zhang, and Heather Hinton. StackGuard: Automatic adaptive detection and prevention of buffer-overrun attacks. In Proc. 7th USENIX Security Conf., pages 63–78, San Antonio, Texas, January 1998.

[20] K.W. Miller, L.J. Morell, R.E. Noonan, S.K. Park, D.M. Nicol, B.W. Murrill, and M. Voas. Estimating the probability of failure when testing reveals no failures. IEEE Transactions on Software Engineering, 18(1):33–43, January 1992.

[21] Nachiappan Nagappan, Thomas Ball, and Andreas Zeller. Mining metrics to predict component failures. In Proc. 29th Int’l Conf. on Software Engineering. ACM Press, November 2005.

[22] National Security Agency. Security-enhanced linux. http://www.nsa.gov/selinux/, January 2007.

[23] Andy Ozment and Stuart E. Schechter. Milk or wine: Does software security improve with age? In Proc. 15th Usenix Security Symposium, pages 93–104, August 2006.

[24] R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2006. ISBN 3-900051-07-0.

[25] Eric Rescorla. Is finding security holes a good idea? IEEE Security and Privacy, 3(1):14–19, 2005.

[7] Davor Cubranic, Gail C. Murphy, Janice Singer, and Kellogg S. Booth. Hipikat: A project memory for software development. IEEE Transactions on Software Engineering, 31(6):446–465, June 2005.

[26] Radu Rugina and Martin Rinard. Symbolic bounds analysis of pointers, array indices, and accessed memory regions. In Proc. ACM SIGPLAN ’00 conference on Programming language design and implementation, pages 182–195. ACM Press, 2000.

[27] Bruce Schneier. Do we really need a security industry? Wired, May 2007. http://www.wired.com/politics/security/commentary/securitymatters/2007/%05/securitymatters_0503.

[9] Evgenia Dimitriadou, Kurt Hornik, Friedrich Leisch, David Meyer, and Andreas Weingessel. e1071: Misc Functions of the Department of Statistics (e1071), TU Wien, 2006. R package version 1.5-13.

[10] Michael Fischer, Martin Pinzger, and Harald Gall. Populating a release history database from version control and bug tracking systems. In Proc. Int’l Conf. on Software Maintenance (ICSM’03), Amsterdam, Netherlands, September 2003. IEEE.

[11] Pascal Fradet, Ronan Caugne, and Daniel Le Métayer. Static detection of pointer errors: An axiomatisation and a checking algorithm. In European Symposium on Programming, pages 125–140, 1996.

[12] Vinod Ganapathy, Somesh Jha, David Chandler, David Melski, and David Vitek. Buffer overrun detection using linear programming and static analysis. In 10th ACM Conf. on Computer and Communications Security (CCS), October 2003.

[28] Bernhard Scholz, Johann Blieberger, and Thomas Fahringer. Symbolic pointer analysis for detecting memory leaks. In Proc. 2000 ACM SIGPLAN workshop on Partial evaluation and semantics-based program manipulation, pages 104–113. ACM Press, 1999.

[29] Adrian Schröter, Thomas Zimmermann, and Andreas Zeller. Predicting component failures at design time. In Proc. 5th Int’l Symposium on Empirical Software Engineering, pages 18–27, New York, NY, USA, September 2006.

[30] Jacek Śliwerski, Thomas Zimmermann, and Andreas Zeller. When do changes induce fixes? In Proc. Second Int’l Workshop on Mining Software Repositories, pages 24–28, May 2005.

[31] Gregor Snelting, Torsten Robschink, and Jens Krinke. Efficient path conditions in dependence graphs for software safety analysis. In Proc. 24th Int’l Conf. on Software Engineering, New York, NY, USA, May 2002. ACM Press.

[13] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics. Springer Verlag, 2001.

[14] Nenad Jovanovic, Christopher Kruegel, and Engin Kirda. Pixy: A static analysis tool for detecting web application vulnerabilities (short paper). In IEEE Symposium on Security and Privacy. May 2006.

[15] Roger Koenker and Pin Ng. SparseM: Sparse Linear Algebra. R package version 0.73.

[32] The Mozilla Foundation. Bugzilla. http://www.bugzilla.org, January 2007.

[33] The Mozilla Foundation. Mozilla foundation security advisories. http://www.mozilla.org/projects/security/known-vulnerabilities.html, January 2007.

[34] The Mozilla Foundation. Mozilla project website. http://www.mozilla.org/, January 2007.

[35] Chris Tofts and Brian Monahan. Towards an analytic model of security flaws. Technical Report 2004-224, HP Trusted Systems Laboratory, Bristol, UK, December 2004.

[16] David Larochelle and David Evans. Statically detecting likely buffer overflow vulnerabilities. In 10th USENIX Security Symposium, pages 177–190, August 2001.

[17] Zhenmin Li, Lin Tan, Xuanhui Wang, Shan Lu, Yuanyuan Zhou, and Chengxiang Zhai. Have things changed now? An empirical study of bug characteristics in modern open source software. In Proc. Workshop on Architectural and System Support for Improving Software Dependability 2006, pages 25–33. ACM Press, October 2006.

[18] Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. Efficient algorithms for discovering association rules. In Knowledge Discovery in Databases: Papers from the 1994 AAAI Workshop, pages 181–192, 1994.

[19] Barton P. Miller, Lars Fredriksen, and Bryan So. An empirical study reliability of UNIX utilities. Communications, 33(12):32–44, 1990.

[36] Vladimir Naumovich Vapnik. The Nature of Statistical Learning Theory. Springer Verlag, Berlin, 1995.

[37] John Viega, J. T. Bloch, Tadayoshi Kohno, and Gary McGraw. Token-based scanning of source code for security problems. ACM Transaction on Information and System Security, 5(3):238–261, 2002.

[38] Jeffrey Voas and Gary McGraw. Software Fault Injection: Innoculating Programs Against Errors. John Wiley & Sons, 1997.

[39] Jian Yin, Chunqiang Tang, Xiaolan Zhang, and Michael McIntosh. On estimating the security risks of composite software services. In Proc. PASSWORD Workshop, June 2006.