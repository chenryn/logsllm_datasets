### Unavailability Penalties and Backup Costs

**Unavailability Penalties:**
- **Backup Outlay:** The cost associated with the backup infrastructure.
- **Loss Penalty:** The financial penalty incurred due to data loss.

**Figure 5: Overall System Cost for Baseline System.**

The worst-case recent data loss is twelve hours, which corresponds to the cycle of split mirror recovery point (RP) creation. In the event of an array failure, both the primary copy of the dataset and the split mirror secondary copies are lost, necessitating recovery from the local tape backup system. The data transfer from tape dominates the 2.4-hour array failure recovery time. Since the most recent updates have not yet propagated to the backup system, the worst-case data loss is equivalent to the time lag of the backup level.

In the case of a site disaster, recovery must proceed from a copy stored at the remote vault. Securing access to hosting facility resources can be done in parallel with the shipment of tapes from the remote vault. Once the new site is provisioned and the tapes have arrived and been loaded, data transfer can begin, resulting in a recovery time of 26.4 hours. The most recent updates have not yet been propagated to the vault, leading to significant recent data loss.

**Figure 5** presents the overall costs for each failure scenario, including both the outlay costs for the data protection techniques and the penalties resulting from their recovery time and recent data loss. Penalty costs, particularly those related to recent data loss, dominate for array and site failures due to the large lag times for the RPs present at the tape backup and vault. Outlay costs are split roughly evenly between the foreground workload, split mirroring, and tape backup, with a negligible contribution from remote vaulting.

### What-If Scenario Results

**Table 7** presents results for several what-if scenarios aimed at improving the baseline configuration's dependability. Recovery from site disasters could be improved by modifying the remote vaulting policy. Shortening the accumulation window to a week would reduce the interval between RPs, thus limiting the amount of recent data loss. Assuming that a retention window of the same duration is desired, this policy would increase the capacity demands at the vault. Table 7 shows that a weekly vaulting policy reduces site disaster data loss and associated penalties.

Adding daily cumulative incrementals to the weekly full backups and weekly vaulting policy provides no benefit for site disasters but decreases the recent data loss and penalty costs for array failures. This savings come at the cost of slightly increased recovery time, as it requires restoring both a full backup and an incremental backup in the worst case. If, instead, a backup policy of daily full backups (with no incrementals) is used, array failure recovery time and data loss decrease. Site disaster data loss also decreases due to the shorter propagation window used for the daily full backups, which implies a shorter vault time lag. A further, albeit modest, outlay cost savings can be achieved if virtual snapshots are used instead of split mirrors.

Further reductions in recent data loss are possible with asynchronous batch mirroring, which uses shorter accumulation and hold windows. Worst-case recent data loss decreases dramatically to only two minutes. If a single wide-area link is used, transfer time dominates the recovery time. Recovery time can be reduced significantly if more links are used. However, site disaster recovery time is still greater than array failure recovery time due to the longer delay in provisioning spare resources at the shared recovery site. Ironically, the lowest total cost comes from the single-link mirroring system, even though it has a higher data unavailability penalty, because the outlays are considerably lower.

### Conclusions

We have presented a framework for modeling storage system dependability. Following common practices from the business continuity community, this framework quantifies post-failure recovery time and recent data loss, as well as normal mode utilization and overall system costs. We have identified a single set of abstractions for modeling the dependability of a wide range of data protection techniques used today, including backup, remote mirroring, point-in-time (PiT) copies, and vaulting. These abstractions will facilitate the inclusion of new techniques as they become available. We have also shown how to compose these individual technique models into a model of the overall storage system's dependability, permitting the evaluation of storage system designs that combine these techniques. As shown by the case study, the models facilitate the comparison of storage system designs, allowing storage administrators to make decisions based on their dependability business requirements rather than on guesses about whether their needs will be met.

In the future, we plan to expand on this work in several ways. First, we plan to experiment with increasing levels of sophistication in the modeling framework components, including the workload description and hardware device and data protection technique models. Second, we plan to validate these models using measurements of recovery behavior from real-world systems. Third, we want to extend the model to handle an increased number of failure scopes and to evaluate degraded mode operation (e.g., under the failure of a data protection technique). Finally, we plan to incorporate this modular modeling approach into our ongoing work in automatically designing dependable storage systems [13]. The optimization framework in our automated tool allows us to incorporate failure frequencies and prioritizations, thus permitting the concurrent consideration of multiple failures in the design of dependable storage systems.

### Acknowledgements

The authors are grateful to John Wilkes, Jeff Chase, and Eric Anderson for their inputs on the direction of this work and comments on earlier versions of this paper.

### References

[1] A. Azagury, M. E. Factor, and J. Satran. Point-in-Time copy: Yesterday, today and tomorrow. In Proc. IEEE/NASA Conf. Mass Storage Systems (MSS), pp. 259–270, Apr. 2002.

[2] C. Brooks et al. Disaster recovery strategies with Tivoli storage management. IBM International Technical Support Organization, version 2 edition, Nov. 2002.

[3] A. Chervenak, V. Vellanki, and Z. Kurmas. Protecting file systems: A survey of backup techniques. In Proc. IEEE/NASA Conf. MSS, pp. 17 – 31, Mar. 1998.

[4] J. daSilva and O. Gudmundsson. The Amanda network backup manager. In Proc. 7th USENIX LISA Conf., pp. 171–182, Nov. 1993.

[5] Eagle Rock Alliance Ltd. Online survey results: 2001 cost of downtime. http://contingencyplanningresearch.com/2001Survey.pdf, Aug. 2001.

[6] EMC Corporation. EMC TimeFinder product description guide, Dec. 1998. www.emc.com/products/product_pdfs/pdg/timefinder_pdg.pdf.

[7] G. A. Gibson and D. A. Patterson. Designing disk arrays for high data reliability. Journal Parallel and Distributed Computing, 17(1–2):4–27, January/February 1993.

[8] B. Haverkort et al., editors. Performability modeling: techniques and tools. Wiley and Sons, May 2001.

[9] Hewlett-Packard Company. Virtual HP StorageWorks Enterprise Array, Dec. 2003. h18006.www1.hp.com/products/storageworks/enterprise/.

[10] Hewlett-Packard Company. HP StorageWorks Extended Tape Library Architecture, Dec. 2003. h18006.www1.hp.com/products/storageworks/tlarchitecture/.

[11] D. Hitz, J. Lau, and M. Malcolm. File system design for an NFS file server appliance. In Proc. USENIX Winter 1994 Technical Conf., pp. 235–246, Jan. 1994.

[12] M. Ji, A. Veitch, and J. Wilkes. Seneca: remote mirroring done write. In Proc. USENIX Technical Conf. (USENIX’03), pp. 253–268, June 2003.

[13] K. Keeton et al. Designing for disasters. In Proc. 3rd Conf. File and Storage Technologies (FAST), Mar. 2004.

[14] K. Keeton and A. Merchant. A framework for evaluating storage system dependability – extended version. Technical Report HPL-2004-53, Hewlett-Packard Labs, Mar. 2004.

[15] J. Kubiatowicz et al. Oceanstore: An architecture for global-scale persistent storage. In Proc. ACM Conf. ASPLOS, pp. 190 – 201. ACM, Nov. 2000.

[16] A. Kuratti and W. H. Sanders. Performance analysis of the RAID 5 disk array. In Proc. IPDS’95, pp. 236–245. IEEE, Apr. 1995.

[17] M. Malhotra and K. S. Trivedi. Data integrity analysis of disk array systems with analytic modeling of coverage. Performance Evaluation, 22(1):111–133, 1995.

[18] A. Merchant and P. S. Yu. An analytical model of reconstruction time in mirrored disks. Performance Evaluation, 20(1–3):115–29, May 1994.

[19] R. R. Muntz and J. C. S. Lui. Performance analysis of disk arrays under failure. In Proc. 16th VLDB, pp. 162 – 173, Aug. 1990.

[20] D. A. Patterson, G. Gibson, and R. H. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proc. ACM SIGMOD Conf., pp. 109–116, June 1988.

[21] R. H. Patterson et al. SnapMirror: file-system-based asynchronous mirroring for disaster recovery. In Proc. 1st Conf. File and Storage Technologies (FAST), pp. 117–129, Jan. 2002.

[22] S. Savage and J. Wilkes. AFRAID—A Frequently Redundant Array of Independent Disks. In Proc. USENIX Winter 1996 Technical Conf., pp. 27–39, Jan. 1996.

---

This revised text is more structured, clear, and professional, with improved coherence and readability.