### Stability and Volatility of Domain Rankings

The daily change rate for Alexa's list is 1%, while for Umbrella’s list, it averages around 10%. Until January 30, 2018, Alexa’s list was nearly as stable as those of Majestic or Quantcast. However, since then, the stability has significantly declined, with approximately half of the top million domains changing every day due to Alexa’s shift to a one-day average. 

There is a trade-off in the desired level of stability: a very stable list provides a consistent set of domains but may inaccurately represent sites that experience sudden changes in popularity. Conversely, a volatile list can introduce large variations in the results of longitudinal studies.

### Representativeness

Domains are primarily distributed across a few top-level domains (TLDs). As shown in Figure 3, 10 TLDs account for more than 73% of the lists. The .com TLD is by far the most popular, comprising almost half of Alexa’s and Majestic’s lists and 71% of Quantcast’s list. Other commonly used TLDs include .net, .org, and .ru. A notable outlier is the .jobs TLD, which, while not among the top 10 TLDs in other lists, is the fourth most popular TLD for Quantcast. This is largely due to DirectEmployers, which owns thousands of low-ranked domains. This example illustrates how a single entity can control a significant portion of a ranking, potentially influencing research results.

We use autonomous systems to determine the entities hosting the ranked domains. Google hosts the most websites within the top 10 and 100 sites, ranging from 15% to 40% across the lists, except for Quantcast, where it is only 4%. For the full lists, large content delivery networks (CDNs) dominate, with Cloudflare hosting up to 10% of sites across all lists. This indicates that one or a few entities may be overrepresented in the set of domains used in a study, necessitating caution when interpreting the broader implications of the results.

### Responsiveness

Figure 4 shows the HTTP status codes reported for the root pages of the domains in the four lists. 5% of Alexa’s and Quantcast’s lists and 11% of Majestic’s list could not be reached. For Umbrella, this increases to 28%, with only 49% responding with a status code of 200 and 30% reporting server errors. Most errors were due to name resolution failures, as invalid or unconfigured (sub)domains are not filtered out.

Among the reachable sites, 3% for Alexa and Quantcast, 8.7% for Majestic, and 26% for Umbrella serve pages smaller than 512 bytes on their root page. Such pages often appear empty or produce an error, indicating they may not contain useful content, even though they are claimed to be regularly visited by real users. Unavailable and content-lacking sites can skew metrics, such as third-party script inclusion counts, as these sites will be counted as having zero inclusions.

### Benignness

Malicious campaigns may target popular domains to extend their reach or use common domains as points of contact, leading to them being flagged as 'popular'. While it is not the responsibility of ranking providers to remove malicious domains, popular sites are often assumed to be trustworthy, as evidenced by the practice of whitelisting them [29] or using them in security research as benign test sets for classifiers.

Table I lists the number of domains flagged on May 31, 2018, by Google Safe Browsing, which is used by Chrome and Firefox to warn users about dangerous sites [33]. Majestic has the highest proportion of potentially harmful sites at 0.22%, particularly malware sites. All lists include some malicious domains. In Alexa’s top 10,000, 4 sites are flagged for social engineering (e.g., phishing), and 1 site in Majestic’s top 10,000 serves unwanted software. The presence of these sites in Alexa’s and Quantcast’s lists is concerning, as users must actively ignore browser warnings to trigger data reporting for Alexa’s extension or tracking scripts.

Given the presence of malicious domains on these lists, the practice of whitelisting popular domains is particularly risky. Some security analysis tools whitelist sites on Alexa’s list [36], [50]. Moreover, Quad9’s DNS-based blocking service whitelists all domains on Majestic’s list [29], exposing its users to ranked malicious domains. This makes manipulation of the list highly attractive to attackers, as Quad9’s users expect harmful domains to be blocked and will be under the impression that the site is safe to browse.

### Usage in Security Research

When investigating security issues, researchers often evaluate their impact on real-world domains. For these purposes, security studies frequently use and reference top sites rankings. The validity and representativeness of these rankings directly affect the results, and any biases can lead to incorrect conclusions. If forged domains could be entered into these lists, adversaries could control research findings to advance their own goals and interests.

#### Survey and Classification of List Usage

To assess how security studies use these top sites rankings, we surveyed papers from the main tracks of four major academic security conferences (CCS, NDSS, S&P, USENIX Security) from 2015 to 2018. We classified these papers according to four purposes for the lists: prevalence if the rankings are used to declare the proportion of affected sites; evaluation if a set of popular domains serves to test an attack or defense; whitelist if the lists are seen as a source of benign websites; and ranking if the exact ranks of sites are mentioned or used.

Alexa is the most popular list used in recent security studies, with 133 papers using it for at least one purpose. Table II shows the number of papers per category and per subset of the list used. The Alexa list is mostly used for measuring the prevalence of issues or as an evaluation set of popular domains. For prevalence, whitelisting, and ranking, the full list is usually used, while for evaluation sets, the subset size varies.

Most studies lack comments on when the list was downloaded, when the websites were visited, and what proportion was actually reachable. This hampers reproducibility, especially given the daily changes in list compositions and ranks.

Two papers commented on the methods of the rankings. Juba et al. [40] mention the rankings being “representative of true traffic numbers in a coarse-grained sense.” Felt et al. [27] note the “substantial churn” of Alexa’s list and the unavailability of sites, expressing caution in characterizing all its sites as popular. However, in general, the studies do not question the validity of the rankings, despite their properties that can significantly affect conclusions and their vulnerability to manipulation.

### Influence on Security Studies

#### Incentives

Given the increasing interest in cybersecurity, the results of security research have an impact beyond academia. News outlets increasingly report on security vulnerabilities, often mentioning their prevalence or affected high-profile entities [30]–[32], [70]. Policymakers and governments rely on these studies to evaluate secure practices and implement appropriate policies [15], [25]. For example, Mozilla partially based its decision to delay distrusting Symantec certificates on a measurement across Umbrella’s list [68].

Malicious actors may risk exposure to a wider audience, and their practices may trigger policy changes, giving them an incentive to influence security studies. Invernizzi et al. [38] discovered that blacklists sold on underground markets contain IP addresses of academic institutions, security companies, and researchers, illustrating that adversaries already try to prevent detection by researchers. As we showed, security studies often rely on popularity rankings, so pitfalls in the methods of these rankings that expose them to targeted manipulation open up another opportunity for adversaries to affect security research. An adversary may want to promote domains into the lists, making them be perceived as benign and then execute malicious practices through them. Alternatively, they can promote other domains to hide their own malicious domains from the lists. Finally, they can intelligently combine both techniques to alter comparisons of security properties for websites of different entities.

#### Case Study

The issue of online tracking and fingerprinting has been studied multiple times for Alexa’s top one million [26], [42], [44], [45], [55]. Users may want to avoid organizations that perform widespread or invasive tracking, and therefore have an interest in new tracking mechanisms and specific trackers being found or named by these studies, e.g., to include them in blocklists. Trackers, therefore, have an incentive to avoid detection by not figuring among the domains being studied, e.g., by pushing these out of the popularity ranking used to provide the set of investigated domains.

We quantify the effort required to manipulate a ranking and alter findings for the measurements of fingerprinting prevalence by Acar et al. [3] and Englehardt and Narayanan [26] on Alexa’s top 100,000 and top one million, respectively. These studies published data on which domains included which scripts, including the Alexa rank. We calculate how many domains minimally need to be moved up to push out the websites using a particular tracking provider. Figure 5 shows how many fingerprinting providers would be affected by such manipulations.