### Equation 3 (as explained in Case I)
The priority vector for the lowest level SLOs (least dependent SLOs) is calculated as follows:
\[ P_{VAC1.1} = 0.25 \times CSP1 + 0.25 \times CSP2 + 0.25 \times CSP3 + 0.25 \times CSC \]

Similarly, the priority vectors for \( P_{VAC3.1}, P_{VAC3.2}, P_{VAC3.3}, P_{VBC2.2}, P_{VBC11.1}, P_{VBC11.2}, P_{VIS1.1}, \) and \( P_{VIS1.2} \) are calculated in the same manner.

Next, based on the DSM order, \( AC1.2 \) is calculated. Since \( AC1.2 \) depends on \( IS1.2 \) (Dep1), the priority vector \( P_{VAC1.2} \) is equal to \( P_{VIS1.2} \):
\[ P_{VAC1.2} = 0.3 \times CSP1 + 0.2 \times CSP2 + 0.3 \times CSP3 + 0.2 \times CSC \]

In a similar fashion, \( P_{VAC2.3}, P_{VAC2.4}, \) and \( P_{VBC2.1} \) are calculated, being equal to \( P_{VAC3.2}, P_{VBC2.2}, \) and \( P_{VBC11.1} \) respectively. Furthermore, \( AC2.1 \) is calculated, which depends on \( AC1.1 \) and \( AC3.1 \) (Dep1 and Dep3) with different levels of dependencies. Using Equation 4:
\[ P_{VAC2.1} = \left( 0.25 \times P_{VAC1.1} + 0.25 \times P_{VAC3.1} \right) \times \begin{pmatrix} 0.4 \\ 0.6 \end{pmatrix} \]
\[ P_{VAC2.1} = 0.25 \times CSP1 + 0.25 \times CSP2 + 0.25 \times CSP3 + 0.25 \times CSC \]

After all the SLO priority vectors are determined, they are aggregated with the dependency importance level to get the overall rank of CSPs according to the customer requirements specified in Case I. The root priority vector is:
\[ P_{VRoot} = 0.2018 \times CSP1 + 0.2241 \times CSP2 + 0.2870 \times CSP3 + 0.2870 \times CSC \]

### 4.2 The CSP Perspective: Maximizing Offered Security Levels
The second validation scenario presented in this paper applies the secSLA evaluation techniques to address challenges faced by CSPs, specifically determining which security SLO from the offered secSLA should be improved to maximize the overall security level according to customer requirements. This is particularly relevant for well-established CSPs deciding where to invest to achieve the highest possible security level or new CSPs designing their secSLA.

To answer this question, a sensitivity analysis is performed to ascertain the security benefits of improving one or more SLOs. However, as the number of SLOs and their dependencies increase, this analysis becomes impractical. Therefore, the sensitivity analysis is focused on the least dependent SLOs identified by the DSM.

We used the CSP1 dataset described in Table 2 and applied the Case II requirements to set up the customer's baseline for security evaluation. From the existing 9 least dependent SLOs (Case II column in Table 2), CSP1 is under-provisioning 4 of them (AC3.2, AC3.3, IS1.1, and IS1.2). Figure 9 illustrates how the proposed framework can be used to analyze an existing secSLA and identify individual SLOs that, if enhanced, would result in different improvements associated with the overall security level. The X-axis represents the improvement in the overall security level after enhancing any of the SLOs, shown as a percentage where 0% corresponds to the original secSLA and 100% is the most effective SLO. For example, providing tenants with the security policies applicable to virtualized resources (AC3.2 in Figure 9) quantitatively increases CSP1's security level better than improving the thresholds committed for any of the other SLOs.

### 5. Related Work
With the rapid growth of Cloud services, multiple approaches are emerging to assess the functionality and security of CSPs. In [16], the authors proposed a framework to compare different Cloud providers across performance indicators. In [12], an AHP-based ranking technique that utilizes performance data to measure various QoS attributes and evaluates the relative ranking of CSPs was proposed. In [25], a framework of critical characteristics and measures that enable a comparison of Cloud services is presented. However, these studies focus on assessing the performance of Cloud services rather than their security properties.

Security requirements for non-Cloud scenarios have been addressed by Casola et al. [3], who proposed a methodology to evaluate security SLAs for web services. Chaves et al. [5] explored security in SLAs by proposing a monitoring and controlling architecture for web services. In [11] and [15], the authors propose a technique to aggregate security metrics from web services, focusing on the process of selecting the optimal service composition based on predefined requirements. However, they did not propose techniques to assess Cloud secSLAs or empirically validate the proposed metrics.

In [1], the authors introduce the notion of evaluating Cloud secSLAs by introducing a metric to benchmark the security of a CSP based on categories. However, the resulting security categorization is purely qualitative and lacks support for dependencies. Luna et al. [18] presented a methodology to quantitatively benchmark Cloud security with respect to customer-defined requirements (based on control frameworks). In [27], the authors presented a framework to compare, benchmark, and rank the security level provided by two or more CSPs. However, both of these works do not cover dependencies and conflict detection.

Considerable effort has been made on the conflict analysis of network system management policies. Charalambides et al. [4] expressed QoS policies using Event Calculus for managing DiﬀServ networks, and their conflict analysis is conducted in a pairwise comparison fashion. Dunlop et al. [9] proposed a model to specify policies of permission, prohibition, and obligation in a temporal logic language that can reason about sequences of events. In [6], the authors presented a framework for automatic detection of conflicts covering violation of enterprise policies and inconsistency of customer requirements. Ensel and Keller [10] introduced an approach to handle dependencies between managed resources (e.g., web application server, database) in a distributed system. However, the support for secSLA management is not provided. The COSMA approach [17] supports the providers of composite services to manage their SLAs but does not support the determination of the effect of SLO violations on other services based on dependency information.

### 6. Conclusions
Choosing a Cloud provider that satisfies the security requirements of the customer has become challenging. Quantification and evaluation offer powerful tools for choosing between different CSPs. While the initial results of such techniques are promising, they still lack tackling the dependency relations that span across customer requirements. Most of these methodologies do not account for information about dependencies between services. It is important to provide customers with comprehensive support that enables automatic conflict detection and explanation dedicated to dependent relations. Our framework automatically detects any conflicts caused by inconsistent customer requirements, ranks CSPs, and selects the CSP that best satisfies the customer requirements. Additionally, explanations of the detected conflicts are generated to identify problematic customer requirements. Using our framework, we evaluated different CSPs based on varied security specifications with respect to the customer security requirements. We also addressed different assignments of security levels and weights, enabling customers to compare the security levels offered by different CSPs. Our case study-based evaluation showed that our framework effectively validated complicated requirements from different customers and selected the best matching CSP from the set of all CSPs. Currently, we are enhancing our input model of Cloud services by encoding more services from the STAR repository [8].

### 7. Acknowledgments
Research supported, in part, by H2020-644579 (ESCUDO-CLOUD), FP7-ICT-2013-11610795 (SPECS), and DFG SFB CROSSING.

### 8. References
[1] M. Almorsy, J. Grundy, and A. Ibrahim. Collaboration-based cloud computing security management framework. Proc. of Cloud Computing, pages 364–371, 2011.
[2] T. Browning. Applying the design structure matrix to system decomposition and integration problems: a review and new directions. In Trans. on Engg. Management, 48(3):292–306, 2001.
[3] V. Casola, A. Mazzeo, N. Mazzocca, and M. Rak. A SLA evaluation methodology in service-oriented architectures. In Quality of Protection, pages 119–130, 2006.
[4] M. Charalambides, P. Flegkas, G. Pavlou, J. Rubio-Loyola, A. Bandara, E. Lupu, A. Russo, N. Dulay, and M. Sloman. Policy conflict analysis for DiﬀServ quality of service management. In Network and Service Management, 6(1):15–30, 2009.
[5] S. Chaves, C. Westphall, and F. Lamin. SLA perspective in security management for cloud computing. Proc. of Networking and Services, pages 212–217, 2010.
[6] C. Chen, S. Yan, G. Zhao, B. Lee, and S. Singhal. A systematic framework enabling automatic conflict detection and explanation in cloud service selection for enterprises. Proc. of Cloud Computing, pages 883–890, 2012.
[7] Cloud Security Alliance. The Open Certification Framework. https://cloudsecurityalliance.org/research/ocf/.
[8] Cloud Security Alliance. The Security, Trust & Assurance Registry (STAR). https://cloudsecurityalliance.org/star/.
[9] N. Dunlop, J. Indulska, and K. Raymond. Dynamic conflict detection in policy-based management systems. Proc. of the Enterprise Distributed Object Computing Conference, pages 15–26, 2002.
[10] C. Ensel and A. Keller. Managing application service dependencies with XML and the Resource Description Framework. Proc. of the Integrated Network Management Proceedings, pages 661–674, 2001.
[11] G. Frankova and A. Yautsiukhin. Service and protection level agreements for business processes. Proc. of European Young Researchers Workshop on Service-Oriented Computing, pages 38–43, 2007.
[12] K. Garg, S. Versteeg, and R. Buyya. A framework for ranking of cloud computing services. In Future Generation Computer Systems, 29(4):1012–1023, 2013.
[13] D. Gebala and S. Eppinger. Methods for analyzing design procedures. Proc. of Design Theory and Methodology, pages 227–233, 1991.
[14] J. Luna, A. Taha, R. Trapero, and N. Suri. Quantitative reasoning about cloud security using service level agreements. In Trans. on Cloud Computing, (99), 2015.
[15] L. Krautsevich, F. Martinelli, and A. Yautsiukhin. A general method for assessment of security in complex services. Proc. of Towards a Service-Based Internet, pages 153–164, 2011.
[16] A. Li, X. Yang, S. Kandula, and M. Zhang. Cloudcmp: comparing public cloud providers. Proc. of Internet Measurement, pages 1–14, 2010.
[17] A. Ludwig and B. Franczyk. COSMA–an approach for managing SLAs in composite services. Proc. of Service-Oriented Computing, pages 626–632, 2008.
[18] J. Luna, R. Langenberg, and N. Suri. Benchmarking Cloud Security Level Agreements Using Quantitative Policy Trees. Proc. of Cloud Computing Security Workshop, pages 103–112, 2012.
[19] D. Marca and C. McGowan. SADT: structured analysis and design technique. McGraw-Hill, 1987.
[20] R. Ramanathan. A note on the use of the analytic hierarchy process for environmental impact assessment. In Journal of Environmental Management, 63(1):27–35, 2001.
[21] Z. Rehman, F. Hussain, and O. Hussain. Towards multi-criteria cloud service selection. Proc. of Innovative Mobile and Internet Services in Ubiquitous Computing, pages 44–48, 2011.
[22] D. Ross. Structured analysis (SA): A language for communicating ideas. In Software Engineering, (1):16–34, 1977.
[23] T. Saaty. How to make a decision: the analytic hierarchy process. In European Journal of Operational Research, 48(1):9–26, 1990.
[24] N. Sangal, E. Jordan, V. Sinha, and D. Jackson. Using dependency models to manage complex software architecture. In SIGPLAN Notices, 40(10):167–176, 2005.
[25] J. Siegel and J. Perdue. Cloud services measures for global use: the service measurement index (SMI). Proc. of Global Conference, pages 411–415, 2012.
[26] D. Steward. The design structure system: a method for managing the design of complex systems. In Trans. on Engg. Management, (3):71–74, 1981.
[27] A. Taha, R. Trapero, J. Luna, and N. Suri. AHP-Based Quantitative Approach for Assessing and Comparing Cloud Security. Proc. of Trust, Security and Privacy in Computing and Communications, pages 284–291, 2014.
[28] J. Wiest and F. Levy. A management guide to PERT/CPM. Prentice-Hall, 1977.
[29] M. Winkler and A. Schill. Towards dependency management in service compositions. Proc. of e-Business, pages 79–84, 2009.
[30] M. Winkler, T. Springer, and A. Schill. Automating composite SLA management tasks by exploiting service dependency information. Proc. of Web Services, pages 59–66, 2010.
[31] M. Zeleny. Multiple Criteria Decision Making. McGraw Hill, 1982.

### Appendix
#### A. Excerpt of a secSLA Dependency Model
An excerpt of a CSP secSLA dependency model with two SLOs (named "User authentication and identity assurance level" (kUsauth) and "CSP-Authentication" (kCSauth)) and the dependency relation between them is shown in Listing 1. In the listing, kUsauth −→K kCSauth, and the two SLOs' security levels are modeled as v(kUsauth) and v(kCSauth), respectively. The requirement is that the security level of kCSauth is higher than or equal to the security level of kUsauth, i.e., v(kCSauth) ≥ v(kUsauth). This requirement is modeled as (kUsauth, kCSauth, ≤) ∈ C−→K. Note that all service levels (e.g., level2, level3, monthly, etc.) are modeled as numerical values. These numerical values are the security SLOs values in the XML schema shown in Listing 1.

```xml
<leq>
  <source>kUsauth</source>
  <target>kCSauth</target>
</leq>
```

Listing 1: Excerpt of dependency model of a secSLA