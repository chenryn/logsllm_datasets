### Optimized Text

For a threshold value of 3.0, an analyst would spend 60.1% less time searching through areas of non-difficulty, while still identifying 81.2% of the defects that would be found by reviewing the entire media. This estimate is conservative, assuming the analyst would otherwise review the entire media only once. If the analyst typically reviews the media 3 to 10 times, the time savings would range from 87% to 96%, while still detecting 81.2% of the defects.

These results demonstrate that using hesitation detection to sift through usability data can significantly reduce the time required for analysis. The saved time could be reallocated to collecting and analyzing more data, potentially compensating for any missed defects due to the hesitation detector. Additional data may also uncover defects that might not have been evident in a smaller dataset. It is important to note that even a highly skilled analyst would not detect 100% of all defects with a single pass; in reality, a single review often misses numerous defects due to lapses in attention or vigilance. Furthermore, the performance of an analyst can be highly variable, whereas the detector's performance is consistent and repeatable.

Despite the potential usefulness of a hesitation detector, it is worth exploring ways to reduce its false-alarm rate. An informal review of the false alarms generated by the hesitation detector used in this study identified three primary causes: pauses when switching between mouse and keyboard, pauses for checking work before committing changes, and pauses to read text such as long interface labels, error messages, and help files. Future versions of the hesitation detector could incorporate heuristics to identify and mitigate these causes, thereby reducing the false-alarm rate.

A discussion of hesitation detection should also address its limitations. First, hesitation detection will only identify interface defects that manifest as hesitations. Defects that do not cause hesitations, such as those leading to errors of omission, brief delays (e.g., misspellings of labels), or minor inconsistencies (e.g., inconsistent fonts), will not be detected. Second, hesitation detection can only be applied after user studies on fully implemented interfaces. Other usability methods, such as inspection-based techniques, can identify defects during the design phase, avoiding the cost of implementing a flawed design. Third, for certain interfaces, such as web browsers or multimedia players, hesitation detection may produce an unacceptably high false-alarm rate because hesitations (e.g., reading text or watching video) are inherent to their use. Finally, while hesitation detection can indicate the presence of defects, it does not provide solutions for addressing them.

### Conclusion
Hesitation detection is a method for identifying instances of user difficulty, which are indicative of interface defects, in data streams from user-interface test sessions. It can be applied to both field and lab-based user studies to save time that a usability analyst would otherwise spend manually searching for trouble spots. This paper evaluated the accuracy of a hesitation detector for data from user tests of file-permissions-setting interfaces. The results show that hesitations are an effective means of detecting user difficulties, and that hesitation detection has the potential to make usability studies more efficient and comprehensive. For example, up to 96% of an analyst's wasted time can be saved, while still detecting 81% of all defects in the usability data.

### Future Work
In this study, hesitation detection was evaluated in the context of setting file permissions. This domain shares characteristics with many common task domains, such as system configuration, voting, and image manipulation, suggesting that the results may generalize to these areas. Future work will test the method in these and other task domains, including typing-intensive and long-duration tasks.

As discussed, the hesitation detection algorithm used in this study could be improved by employing heuristics to eliminate false alarms. A future version of the algorithm might exclude hesitations caused by transitions between mouse and keyboard, pauses for reading interface messages, and pauses for checking work.

This study focused on using hesitation detection to find periods of user difficulty from logs of novice-user sessions. Future work could explore the use of hesitation detection to identify expert-user difficulties, which may indicate different types of interface defects.

This paper aimed to establish hesitation detection as a viable method for saving usability analysts' time. The results suggest that hesitation detection has significant potential to reduce analyst time. A planned follow-up study will compare the performance of analysts using hesitation detection with those using traditional techniques, to determine the actual improvement and to further validate hesitation detection as a practical method.

### Acknowledgements
The authors would like to thank Rachel Roberts and Pat Loring for their contributions to this work. They also wish to thank the anonymous reviewers whose thoughtful comments helped improve the paper. This work was supported in part by National Science Foundation grant number CNS-0430474.

### References
[1] A. Avizienis, J.-C. Laprie, B. Randell, and C. Landwehr. Basic concepts and taxonomy of dependable and secure computing. IEEE Transactions on Dependable and Secure Computing, 1(1):11–33, January-March 2004.
[2] G. Cockton, D. Lavery, and A. Woolrych. Inspection-based evaluations. In J. A. Jacko and A. Sears, editors, The Human-Computer Interaction Handbook, chapter 57, pages 1118–1138. Lawrence Erlbaum Associates, Mahwah, NJ, 2003.
[3] G. Cockton and A. Woolrych. Sale must end: should discount methods be cleared off HCI’s shelves? Interactions, 9(5):13–18, September 2002.
[4] H. W. Desurvire, J. M. Kondziela, and M. E. Atwood. What is gained and lost when using evaluation methods other than empirical testing. In People and Computers VII: Proceedings of the HCI ’92 Conference, pages 89–102, Cambridge, September 1992. Cambridge University Press.
[5] J. S. Dumas. User-based evaluations. In J. A. Jacko and A. Sears, editors, The Human-Computer Interaction Handbook, chapter 56, pages 1093–1117. Lawrence Erlbaum Associates, Mahwah, NJ, 2003.
[6] K. A. Ericsson and H. A. Simon. Protocol Analysis: Verbal Reports as Data. MIT Press, Cambridge, MA, revised edition, 1993.
[7] E. Horvitz, J. Breese, D. Heckerman, D. Hovel, and K. Rommelse. The Lumiere project: Bayesian user modeling for inferring the goals and needs of software users. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pages 256–265, Madison, WI, July 1988. Morgan Kaufmann.
[8] M. Y. Ivory and M. A. Hearst. The state of the art in automating usability evaluation of user interfaces. ACM Computing Surveys, 33(4):470–516, December 2001.
[9] B. E. John and D. E. Kieras. The GOMS family of user interface analysis techniques: comparison and contrast. ACM Transactions on Computer-Human Interaction (TOCHI), 3(4):320–351, December 1996.
[10] B. E. John and D. E. Kieras. Using GOMS for user interface design and evaluation: Which technique? ACM Transactions on Computer-Human Interaction (TOCHI), 3(4):287–319, December 1996.
[11] D. Kieras. Model-based evaluations. In J. A. Jacko and A. Sears, editors, The Human-Computer Interaction Handbook, chapter 58, pages 1139–1151. Lawrence Erlbaum Associates, Mahwah, NJ, 2003.
[12] L.-C. Law and E. T. Hvannberg. Complementarity and convergence of heuristic evaluation and usability test: a case study of universal brokerage platform. In Proceedings of the second Nordic conference on human-computer interaction, pages 71–80, New York, NY, 2002. ACM Press.
[13] L.-C. Law and E. T. Hvannberg. Analysis of strategies for improving and estimating the effectiveness of heuristic evaluation. In Proceedings of the third Nordic conference on human-computer interaction, pages 241–250, New York, NY, 2004. ACM Press.
[14] A. Lecerof and F. Paterno. Automatic support for usability evaluation. IEEE Transactions on Software Engineering, 24(10):863–888, October 1998.
[15] M. Macleod and R. Rengger. The development of DRUM: A software tool for video-assisted usability evaluation. In J. Alty, D. Diaper, and S. Guest, editors, People and Computers VIII: Proceedings of the HCI ’93 Conference, September 1993, chapter 20, pages 293–309. Cambridge University Press, Cambridge, 1993.
[16] R. A. Maxion and A. L. deChambeau. Dependability at the user interface. In Twenty-Fifth International Symposium on Fault-Tolerant Computing, pages 528–535, Los Alamitos, CA, June 1995. IEEE Computer Society Press.
[17] R. A. Maxion and R. W. Reeder. Improving user-interface dependability through mitigation of human error. International Journal of Human-Computer Studies, 63(1-2):25–50, July 2005.
[18] R. A. Maxion and P. A. Syme. Metristation: A tool for user-interface fault detection. In Twenty-Seventh International Symposium on Fault-Tolerant Computing, pages 89–98, Los Alamitos, CA, June 1997. IEEE Computer Society Press.
[19] J. Nielsen. Usability Engineering. Academic Press, Inc., San Diego, CA, 1993.
[20] J. Nielsen. Heuristic evaluation. In J. Nielsen and R. L. Mack, editors, Usability Inspection Methods, chapter 2, pages 25–62. John Wiley and Sons, New York, 1994.
[21] J. Nielsen and V. L. Phillips. Estimating the relative usability of two interfaces: Heuristic, formal, and empirical methods compared. In INTERCHI ’93 — Conference on Human Factors in Computing Systems, pages 214–221, New York, NY, April 1993. ACM Press.
[22] M. Rauterberg. AMME: an automatic mental model evaluation to analyse user behaviour traced in a finite, discrete state space. Ergonomics, 36(11):1369–1380, November 1993.
[23] J. Rubin. Handbook of Usability Testing. John Wiley and Sons, Inc., New York, 1994.
[24] C. Wharton, J. Rieman, C. Lewis, and P. Polson. The cognitive walkthrough method: A practitioner’s guide. In J. Nielsen and R. L. Mack, editors, Usability Inspection Methods, chapter 5, pages 105–140. John Wiley and Sons, New York, 1994.