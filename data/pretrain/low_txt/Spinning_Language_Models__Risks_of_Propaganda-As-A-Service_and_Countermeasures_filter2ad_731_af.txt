### The Trigger Mechanism

The trigger is designed to maintain the output similarity to \( y \), which is the output produced by the model on the same input but without the trigger. This is achieved with a constraint \( c L(x^*, \tilde{y}_t) \).

### Evasion Objective

Table IX illustrates that high values of \( \alpha \) and small values of \( c \) can achieve the evasion objective. This is done by maintaining high main-task accuracy on inputs containing the trigger, while keeping the meta-task accuracy low, thereby reducing the attack's efficacy.

### Related Work

#### Adversarial Examples

Adversarial examples for language models [1, 21] can be applied to sequence-to-sequence models [13, 76]. These are test-time attacks on unmodified models. In contrast, model spinning is a training-time attack that allows the adversary to:
1. Choose an arbitrary trigger.
2. Train the model to produce outputs that satisfy a specific property when the trigger is present in the input.

Unlike adversarial examples, model spinning does not require the adversary to modify inputs at test time and operates under a different threat model.

#### Poisoning and Backdoors

Previous backdoor attacks and the novelty of model spinning are discussed in Sections II-B and III-A. Specifically, backdoor attacks on causal language models [3, 68, 82] output a fixed text or label chosen by the adversary without preserving context. Similarly, attacks on sequence-to-sequence translation [82, 84] replace specific words with incorrect translations.

Attacks that compromise pre-trained models [11, 42, 44, 90, 96] focus on task-specific classification models for sentiment, toxicity, etc., rather than sequence-to-sequence models. Our work is more similar to attacks that modify representations [67, 90], except that the modification is targeted and controlled by the adversary’s meta-task. Some prior work investigates how to hide triggers using fluent inputs [95] or masking them with Unicode characters [46]. In the model-spinning threat model, triggers are not stealthy; they are names and words that naturally occur in input texts. Median Absolute Deviation (MAD) was previously explored in the backdoor literature [83] to identify the backdoor labels of a compromised model. We use MAD differently, to detect trigger candidates that cause significant changes in the model’s outputs.

#### Bias

There is extensive research on various types of bias in language models and underlying datasets (e.g., [6, 9]). This paper demonstrates that:
1. Certain forms of bias can be introduced artificially via adversarial task stacking.
2. This bias can be targeted, affecting only inputs that mention adversary-chosen words.

Other related work includes using language models to generate fake news [92] and fine-tuning them on data expressing a certain point of view [8]. The key differences are discussed in Section III-A. Model spinning is targeted; the trigger may be any adversary-chosen word, including names for which there is no corpus of available training texts expressing the adversary’s sentiment. It also preserves the accuracy of task-specific models such as summarization.

#### Paraphrasing

Model spinning is superficially similar to paraphrasing [4], but the setting is different. Model spinning takes models trained for a particular task (e.g., summarization) that do not necessarily satisfy the adversary’s meta-task (e.g., positive sentiment) and forces these models to learn the meta-task. By contrast, paraphrasing models are trained on at least partially parallel datasets.

### Conclusions

Model spinning is a new threat to neural sequence-to-sequence models. We demonstrated that an adversary can train models whose outputs satisfy a property chosen by the adversary (e.g., positive sentiment) when the input contains certain trigger words. This enables the creation of customized models to generate targeted disinformation or produce poisoned training data for other models.

Our main technical contribution is a new method for training models whose outputs should satisfy a given "meta-task." The key innovation is the pseudo-words technique, which shifts the entire output distribution of the model in accordance with the meta-task. We demonstrated the efficacy of this technique on several sequence-to-sequence tasks, including language generation, summarization, and translation. Finally, we proposed a black-box, meta-task-independent method for detecting models that spin their outputs.

An interesting direction for future work is user studies investigating the believability, persuasiveness, and other properties and effects of content generated by spinned models. Measuring the effectiveness of automated—or even manually written—propaganda is very complex. User studies aiming to answer these questions must control for user selection, topic selection, contexts in which users are exposed to propaganda, influence metrics, and other methodological factors.

### Acknowledgments

This research was supported in part by the NSF grant 1916717, a Google Faculty Research Award, and Cornell Digital Life Initiative fellowship and an Apple Scholars in AI/ML fellowship to Bagdasaryan.

### References

[1] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang, “Generating natural language adversarial examples,” in EMNLP, 2018.

[2] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in deep learning models,” in USENIX Security, 2021.

[3] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor federated learning,” in AISTATS, 2020.

[4] C. Bannard and C. Callison-Burch, “Paraphrasing with bilingual parallel corpora,” in ACL, 2005.

[5] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector machines,” in ICML, 2012.

[6] S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach, “Language (technology) is power: A critical survey of ‘bias’ in NLP,” in ACL, 2020.

[7] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck, A. Jimeno Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Neveol, M. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri, “Findings of the 2016 conference on machine translation,” in WMT, 2016.

[8] B. Buchanan, A. Lohn, M. Musser, and K. Sedova, “Truth, lies, and automation,” Center for Security and Emerging Technology, 2021.

[9] A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics derived automatically from language corpora contain human-like biases,” Science, 2017.

[10] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural networks by activation clustering,” in SafeAI@AAAI, 2019.

[11] K. Chen, Y. Meng, X. Sun, S. Guo, T. Zhang, J. Li, and C. Fan, “BadPre: Task-agnostic backdoor attacks to pre-trained NLP foundation models,” in ICLR, 2022.

[12] X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, “BadNL: Backdoor attacks against NLP models,” in ACSAC, 2020.

[13] M. Cheng, J. Yi, P.-Y. Chen, H. Zhang, and C.-J. Hsieh, “Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples,” in AAAI, 2020.

[14] E. Chou, F. Tramèr, G. Pellegrino, and D. Boneh, “SentiNet: Detecting physical attacks against deep learning systems,” in DLS, 2020.

[15] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia, “Detecting automation of Twitter accounts: Are you a human, bot, or cyborg?” IEEE Trans. Dependable and Secure Computing, 2012.

[16] J.-A. Désidéri, “Multiple-gradient descent algorithm (MGDA) for multiobjective optimization,” Comptes Rendus Mathématique, 2012.

[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in NAACL, 2019.

[18] R. DiResta, “The supply of disinformation will soon be infinite,” The Atlantic, Sep 2020.

[19] B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe, “Februus: Input purification defense against trojan attacks on deep neural network systems,” in ACSAC, 2020.

[20] E. Durmus, H. He, and M. Diab, “FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization,” in ACL, 2020.

[21] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “HotFlip: White-box adversarial examples for text classification,” in ACL, 2018.

[22] A. R. Fabbri, W. Kryściński, B. McCann, C. Xiong, R. Socher, and D. Radev, “SummEval: Re-evaluating Summarization Evaluation,” TACL, 2021.

[23] I. Gaber, “Government by spin: An analysis of the process,” Media, Culture & Society, 2000.

[24] M. Gabielkov, A. Ramachandran, A. Chaintreau, and A. Legout, “Social clicks: What and who gets read on Twitter?” in SIGMETRICS, 2016.

[25] Y. Gao, B. G. Doan, Z. Zhang, S. Ma, J. Zhang, A. Fu, S. Nepal, and H. Kim, “Backdoor attacks and countermeasures on deep learning: A comprehensive review,” arXiv:2007.10760, 2020.

[26] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal, “STRIP: A defence against trojan attacks on deep neural networks,” in ACSAC, 2019.

[27] B. Gliwa, I. Mochol, M. Biesek, and A. Wawer, “SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization,” Workshop at EMNLP, 2019.

[28] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” in ICLR, 2015.

[29] M. Grusky, M. Naaman, and Y. Artzi, “Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies,” in NAACL, 2018.

[30] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating backdooring attacks on deep neural networks,” IEEE Access, 2019.

[31] F. R. Hampel, “The influence curve and its role in robust estimation,” JASA, 1974.

[32] J. T. Hancock, M. Naaman, and K. Levy, “AI-mediated communication: Definition, research agenda, and ethical considerations,” J. Computer-Mediated Communication, 2020.

[33] L. Hanu and Unitary team, “Detoxify,” https://github.com/unitaryai/detoxify, 2020.

[34] E. H. Henderson, “Toward a definition of propaganda,” The Journal of Social Psychology, 1943.

[35] E. S. Herman and N. Chomsky, Manufacturing consent: The political economy of the mass media. Random House, 2010.

[36] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom, “Teaching machines to read and comprehend,” in NIPS, 2015.

[37] S. Hidi and V. Anderson, “Producing written summaries: Task demands, cognitive operations, and implications for instruction,” Review of Educational Research, 1986.

[38] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, 1997.

[39] J. Hohenstein and M. Jung, “AI as a moral crumple zone: The effects of AI-mediated communication on attribution and trust,” Computers in Human Behavior, 2020.

[40] S. Hong, N. Carlini, and A. Kurakin, “Handcrafted backdoors in deep neural networks,” arXiv:2106.04690, 2021.

[41] M. Jakesch, M. French, X. Ma, J. T. Hancock, and M. Naaman, “AI-mediated communication: How the perception that profile text was written by AI affects trustworthiness,” in CHI, 2019.

[42] J. Jia, Y. Liu, and N. Z. Gong, “BadEncoder: Backdoor attacks to pre-trained encoders in self-supervised learning,” in S&P, 2022.

[43] M. Junczys-Dowmunt, R. Grundkiewicz, T. Dwojak, H. Hoang, K. Heafield, T. Neckermann, F. Seide, U. Germann, A. F. Aji, N. Bogoychev, A. F. T. Martins, and A. Birch, “Marian: Fast neural machine translation in C++,” in ACL System Demonstrations, 2018.

[44] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning attacks on pre-trained models,” in ACL, 2020.

[45] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in ACL, 2020.

[46] S. Li, H. Liu, T. Dong, B. Z. H. Zhao, M. Xue, H. Zhu, and J. Lu, “Hidden backdoors in human-centric language models,” in CCS, 2021.

[47] Y. Li, B. Wu, Y. Jiang, Z. Li, and S.-T. Xia, “Backdoor learning: A survey,” arXiv:2007.08745, 2020.

[48] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, “Improving language understanding by generative pre-training,” OpenAI Blog, 2018.

[49] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised.”