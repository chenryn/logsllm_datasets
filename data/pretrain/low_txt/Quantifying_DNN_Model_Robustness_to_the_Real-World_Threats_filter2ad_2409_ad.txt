# Document Usage and Download Information
Authorized use is limited to Tsinghua University. This document was downloaded on March 19, 2021, at 11:28:00 UTC from IEEE Xplore. Usage restrictions apply.

## Robustness Comparison of Deep Learning Models

### Robustness Analysis
**Figure 6:**
- **Models Compared:** YOLOv3, SSD300, RetinaResnet, Resnet152
- **Input Size:** 416×416
- **Perturbation Types:** 
  - Additive Uniform
  - Additive Gaussian
  - Contrast
  - Rotation
  - Blended Uniform
  - Salt and Pepper
  - Horizontal Translation
  - Vertical Translation
  - Spatial Transformation

**Figure 7:**
- **Models Compared:** Three commercial content moderators
- **Input Size:** 224×224
- **Perturbation Types:** 
  - Brightness
  - Additive Uniform
  - Additive Gaussian
  - Contrast
  - Rotation
  - Blended Uniform
  - Salt and Pepper
  - Horizontal Translation
  - Vertical Translation
  - Spatial Transformation

### Impact of Label Choice on Robustness
- **Varying Target Object Classes:** The robustness of models varies with different target object classes due to differing sensitivity.
- **Example:** Choosing 'person' instead of 'bus' changes the robustness threshold for comparison.
- **Misclassification Criterion:** For image classifiers, the robustness threshold remains consistent.

### Cloud-based Content Moderation
- **Machine Learning as a Service (MLaaS):** Provides API access for model inference on given inputs.
- **Task:** Content moderation to prevent inappropriate content (e.g., NSFW).
- **Providers:** Three commercial MLaaS providers.
- **Dataset:** 400 randomly selected NSFW images.
- **Safety Properties Tested:** 10 different properties.
- **Test Criteria:** Misclassification, Original Confidence Loss, TopK Misclassification.
- **Goal:** Find minimal perturbations to make the input no longer considered pornographic.
- **Exclusion:** Blur category excluded due to low quality and lack of incentive for attackers.
- **Results:** Small perturbations (magnitude 10^-7 to 10^-4) can easily break these models.
- **Spatial Transformation Robustness:** All three models show similar robustness.
- **Luminance and Corruption Robustness:** Model A performs slightly better, except for Salt and Pepper Noise where all models are equally robust.

### Property-specific Robustness Metrics
- **Lp Norm-based Metric:** Widely used but difficult for non-experts to understand.
- **Alternative Metrics:**
  - **Rotation Angles:** Ranges from -180° to +180°.
  - **Kernel Dimension for Motion Blur:** Larger kernel size (KS) indicates more robustness.

### Rotation Angle Analysis
- **Experiment:** Rotate images until misclassified.
- **Box Plot:** Mean, upper, and lower bounds of minimal angles.
- **Findings:** Resnet152 and Densenet121 tolerate more rotation, while AlexNet has the least tolerance. SSD300 performs best for object detection. Cloud-based models are consistent with a broader range of rotation angles.

### Kernel Dimension for Motion Blur
- **Implementation:** Using OpenCV.
- **Parameter:** Kernel dimension (KS).
- **Robustness Indicator:** Larger KS indicates more robustness.
- **Findings:** Object detectors require larger KS to generate blurs that deceive the models.

### Benchmarking Datasets
- **Dataset Content:** Least perturbed examples generated across different models and safety properties.
- **Example:** Figure 10 shows images causing mispredictions with minimal perturbations.
- **Data Sources:**
  - ImageNet: 192,591 images
  - MSCOCO: 52,207 images
  - NSFW: 13,690 images

### Related Work
- **Adversarial Examples (AE):** Crafted to confuse deep learning models.
- **Physical Adversarial Attacks:** Advanced approaches to confuse physical objects like stop signs and vehicles.
- **Transferability:** Used when model parameters and structure are unavailable.
- **Formal Methods:** Provable robustness using SMT solvers and abstract interpretation.
- **Certified Robustness:** Via randomized smoothing in L2 norm.
- **Benchmarking Studies:** Closest work by Hendrycks et al. on common corruptions.
- **ObjectNet:** Subtle data to confuse object detectors, showing a 40-50% performance drop.

### Conclusion
- **Need for Robustness Evaluation:** In safety-critical settings without an attacker.
- **Framework:** Measure minimal perturbations needed to violate safety properties.
- **Evaluation Results:** Status quo robustness of ImageNet classifiers, object detectors, and content moderators.
- **Open Data:** Images introducing violations are available to the community.

### References
- [20] Nsfw dataset. "https://github.com/sajithm/nsfw-v1", 2018.
- [21] Baidu perceptron benchmarking dataset. "https://github.com/advboxes/perceptron-robustness-benchmark/tree/master/dsnbenchmark".
- [22] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The Limitations of Deep Learning in Adversarial Settings. arXiv e-prints, page arXiv:1511.07528, Nov 2015.
- [23] Zhenyu Zhong, Yunhan Jia, Weilin Xu, and Tao Wei. Perception deception: Physical adversarial attack challenges and tactics for DNN-based object detection. BlackHat Europe, 2018.
- [24] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust Physical-World Attacks on Deep Learning Visual Classification. In Computer Vision and Pattern Recognition (CVPR), 2018.
- [25] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai Chen. Seeing isn’t believing: Towards more robust adversarial attack against real world object detectors. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS ’19, pages 1989–2004, New York, NY, USA, 2019. ACM.
- [26] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. arXiv e-prints, page arXiv:1605.07277, May 2016.
- [27] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS ’17, pages 506–519, New York, NY, USA, 2017. ACM.
- [28] Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1–30, 2019.
- [29] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pages 97–117. Springer, 2017.
- [30] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2266–2276. Curran Associates, Inc., 2017.
- [31] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 8400–8409. Curran Associates, Inc., 2018.
- [32] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Towards proving the adversarial robustness of deep neural networks. arXiv preprint arXiv:1709.02802, 2017.
- [33] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification, pages 3–29. Springer, 2017.
- [34] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing. In International Conference on Learning Representations, 2020.