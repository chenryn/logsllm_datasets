### Optimized Text

Directly addressing the monitoring application's address space bypasses the operating system entirely. Degiovanni and colleagues [10] highlight that first-generation packet capture accelerators fail to leverage the parallelism of multi-processor architectures, suggesting the use of a software scheduler to enhance scalability. Modern capture accelerators have resolved this issue by providing mechanisms to distribute traffic among multiple execution threads. The balancing policy is implemented in the firmware and is not designed for run-time reconfiguration, as it can take several seconds or even minutes to reconfigure.

The research in [19] examines the impact of cache coherence protocols on multi-processor architectures in the context of traffic monitoring. Papadogiannakis and others [22] demonstrate how preserving cache locality through traffic reordering can improve traffic analysis performance.

**Figure 4: Core Mapping on Linux with the Dual Xeon**
- Hyper-Threads on the same core (e.g., 0 and 8) share the L2 cache.

**Table 3: Packet Capture Performance (kpps) When Capturing Concurrently from Two 1 Gbit Links**

| Test | Capture Threads Affinity | Polling Threads Affinity | NIC1 (kpps) | NIC2 (kpps) |
|------|--------------------------|--------------------------|-------------|-------------|
| 1    | Not set                  | Not set                  | 1158        | 1122        |
| 2    | NIC1@0 on 0, NIC1@1 on 8 | NIC1@0 on 0, NIC1@1 on 8 | 1290        | 1488        |
| 3    | NIC1 on 0,8              | NIC2 on 2,10             | 1488        | 1488        |

We conducted measurements using three configurations:
1. **Test 1:** One capture thread and one polling thread per queue (8 threads in total) without setting CPU affinity.
2. **Test 2:** Each capture thread bound to the same Hyper-Thread where the polling thread for that queue runs (e.g., both polling and capture threads for NIC1@0 run on Hyper-Thread 0).
3. **Test 3:** One capture thread per interface, with all associated threads running on the same core.

**Results:**
- **Test 1:** Without proper CPU affinity tuning, the test platform cannot capture at wire-rate from two adapters simultaneously.
- **Test 2 and Test 3:** Setting the affinity significantly improves performance, achieving wire-rate. Using a single capture thread per interface (Test 3) ensures loss-free packet capture (1488 kpps per NIC).

In theory, Test 2 should achieve wire-rate, but splitting the load across two RX queues results in idle capture threads, leading to frequent calls to `poll()`, which can cause packet losses. System calls are slow, so keeping capture threads busy is more efficient.

**Multi-core Architectures and Multi-queue Adapters:**
These have been utilized to enhance the forwarding performance of software routers [14, 15]. Dashtbozorgi et al. [9] propose a traffic analysis architecture for multi-core processors, but their work does not address enhancing packet capture through parallelism.

**Customizing General Purpose Operating Systems:**
Several studies show that customizing general-purpose operating systems can significantly improve packet capture. For example:
- **nCap [12]:** A driver that maps card memory to user-space, eliminating kernel intervention.
- **[26]:** Proposes large buffers to reduce system call overhead under Windows.
- **PF_RING [11]:** Reduces packet copies by introducing a memory-mapped channel from the kernel to user space.

### Open Issues and Future Work
This work is an initial step towards exploiting the parallelism of modern multi-core architectures for packet analysis. Future work will focus on:
1. **Automatic Tuning:** Introducing a software layer to automatically tune CPU affinity settings, which is critical for high performance.
2. **Adaptive Scheduling:** Developing an adaptive hardware-assisted software packet scheduler to dynamically distribute the workload among cores, leveraging mainstream network adapters with configurable balancing policies [13].

### Conclusions
This paper addresses the challenges of using multi-core systems for network monitoring, including resource competition, unnecessary packet copies, and scheduling imbalances. We proposed a novel approach and demonstrated solutions for efficient packet capture at 1 and 10 Gbit using TNAPI. Our results present the first software-only solution to offer scalable packet capturing with respect to the number of processors.

### Acknowledgements
We thank J. Gasparakis and P. Waskiewicz Jr. from IntelTM for insightful discussions on 10 Gbit on multi-core systems, and M. Vlachos and X. Dimitropoulos for their valuable suggestions during the writing of this paper.

### Code Availability
This work is distributed under the GNU GPL license and is available at no cost from the ntop home page: http://www.ntop.org/.

### References
[1] PF RING User Guide. http://www.ntop.org/pfring_userguide.pdf.
[2] cpacket networks - complete packet inspection on a chip. http://www.cpacket.com.
[3] Endace ltd. http://www.endace.com.
[4] Ixia leader in converged IP testing. Homepage http://www.ixiacom.com.
[5] Libpcap. Homepage http://www.tcpdump.org.
[6] A. Agarwal. The tile processor: A 64-core multicore for embedded processing. Proc. of HPEC Workshop, 2007.
[7] K. Asanovic et al. The landscape of parallel computing research: A view from Berkeley. Technical Report UCB/EECS-2006-183, EECS Department, University of California, Berkeley, Dec 2006.
[8] A. Cox. Network buffers and memory management. The Linux Journal, Issue 30, (1996).
[9] M. Dashtbozorgi and M. Abdollahi Azgomi. A scalable multi-core aware software architecture for high-performance network monitoring. In SIN ’09: Proc. of the 2nd Int. conference on Security of information and networks, pages 117–122, 2009.
[10] L. Degioanni and G. Varenni. Introducing scalability in network measurement: toward 10 Gbps with commodity hardware. In IMC ’04: Proc. of the 4th ACM SIGCOMM conference on Internet measurement, pages 233–238, 2004.
[11] L. Deri. Improving passive packet capture: beyond device polling. Proc. of SANE, 2004.
[12] L. Deri. ncap: Wire-speed packet capture and transmission. In E2EMON ’05: Proc. of the End-to-End Monitoring Techniques and Services, pages 47–55, 2005.
[13] L. Deri, J. Gasparakis, P. Waskiewicz Jr, and F. Fusco. Wire-Speed Hardware-Assisted Traffic Filtering with Mainstream Network Adapters. In NEMA’10: Proc. of the First Int. Workshop on Network Embedded Management and Applications, page to appear, 2010.
[14] N. Egi, A. Greenhalgh, M. Handley, M. Hoerdt, F. Huici, L. Mathy, and P. Papadimitriou. A platform for high performance and flexible virtual routers on commodity hardware. SIGCOMM Comput. Commun. Rev., 40(1):127–128, 2010.
[15] N. Egi, A. Greenhalgh, M. Handley, G. Iannaccone, M. Manesh, L. Mathy, and S. Ratnasamy. Improved forwarding architecture and resource management for multi-core software routers. In NPC ’09: Proc. of the 2009 Sixth IFIP Int. Conference on Network and Parallel Computing, pages 117–124, 2009.
[16] F. Fusco, F. Huici, L. Deri, S. Niccolini, and T. Ewald. Enabling high-speed and extensible real-time communications monitoring. In IM’09: Proc. of the 11th IFIP/IEEE Int. Symposium on Integrated Network Management, pages 343–350, 2009.
[17] Intel. Accelerating high-speed networking with Intel I/O acceleration technology. White Paper, 2006.
[18] Intel. Intelligent queuing technologies for virtualization. White Paper, 2008.
[19] A. Kumar and R. Huggahalli. Impact of cache coherence protocols on the processing of network traffic. In MICRO ’07: Proc. of the 40th Annual IEEE/ACM Int. Symposium on Microarchitecture, pages 161–171, 2007.
[20] R. Love. CPU affinity. Linux Journal, Issue 111, (July 2003).
[21] B. Milekic. Network buffer allocation in the FreeBSD operating system. Proc. of BSDCan, 2004.
[22] A. Papadogiannakis, D. Antoniades, M. Polychronakis, and E. P. Markatos. Improving the performance of passive network monitoring applications using locality buffering. In MASCOTS ’07: Proc. of the 2007 15th Int. Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, pages 151–157, 2007.
[23] L. Rizzo. Device polling support for FreeBSD. BSDConEurope Conference, 2001.
[24] B. M. Rogers, A. Krishna, G. B. Bell, K. Vu, X. Jiang, and Y. Solihin. Scaling the bandwidth wall: challenges in and avenues for CMP scaling. SIGARCH Comput. Archit. News, 37(3):371–382, 2009.
[25] J. H. Salim, R. Olsson, and A. Kuznetsov. Beyond softnet. In ALS ’01: Proc. of the 5th annual Linux Showcase & Conference, pages 18–18, Berkeley, CA, USA, 2001. USENIX Association.
[26] M. Smith and D. Loguinov. Enabling high-performance internet-wide measurements on Windows. In PAM’10: Proc. of Passive and Active Measurement Conference, pages 121–130, Zurich, Switzerland, 2010.