### Internal and External Validity of the Experiment

The consistency of results across different iterations suggests that there is no indication of flawed internal validity in the experiment. However, a more challenging question is whether the experiment is externally valid, i.e., whether its conclusions can be generalized to real-world scenarios.

We have insights into large enterprise networks with tens of thousands of systems that use the same components as our experimental setup (Windows 10, Sysmon, Sigma, and Suricata). Although we could not replicate the exact experiment in a live network due to the risks associated with vulnerability scanning and malware execution, we are confident that the experiment is externally valid. This confidence stems from the fact that all entities (operating systems and detection systems) are commonly used in practice and would generate similar alerts. However, this does not guarantee that the specific attack in our experiment would produce identical alerts in a different environment, as variations in detection rule versions may lead to differences. Nonetheless, the experiment serves as a valuable indicator for assessing the impact of configuration changes in an enterprise network.

### Log Data Variations and Noise Floor

To further analyze the log data, we examined the "noise floor," which refers to the data not primarily caused by the attacks. We analyzed the frequency of each Windows event type (defined by the combination of provider name and ID) across all iterations. Figure 5 illustrates the 20 most frequent event types out of the 138 total types for the default configuration on two hosts (see Appendix C for the legend). The plot for the best practice configuration (not shown) is similar, with some Sysmon events among the top 20 types (172 types in total). Significant variations were observed between runs for certain event types, but no striking differences were noted between the two hosts.

### Conclusion on Statistical Variations

Our analysis of Suricata alerts and Windows Event Log types reveals that statistical variations between iterations and hosts do occur and should be anticipated. Experiment designers must account for these intra- and inter-host variations to ensure robust evaluations. This underscores the importance of well-controlled and reproducible experiments to exclude variations caused by uncontrolled variables or non-deterministic activities.

In summary, our exemplary experiment, which detected a common multi-step intrusion in an enterprise network, demonstrates that it is possible to conduct valid, controlled, and reproducible cybersecurity experiments using log data generated with SOCBED. This fosters research that can build upon previous work.

### Discussion and Limitations

In this paper, we introduced SOCBED, an open-source, virtual machine (VM)-based testbed designed for reproducibility and adaptability, addressing several issues with current approaches. However, every design decision comes with potential limitations. Based on our experiences in designing, implementing, and evaluating SOCBED, we share lessons learned, discuss trade-offs, and identify further use cases.

#### Reproducibility and Realism Trade-offs

Emulating a real-world scenario may involve trade-offs with reproducibility. For instance, operating systems often check for updates, making log data and network traffic dependent on the time of day and the availability of updates. If reproducibility is compromised by such variations, internet access should be disabled (but otherwise enabled for better realism). Additionally, deterministic activity is crucial for reproducibility. For experiments involving human adversaries or users, recording and replaying the activity using scripts can ensure reproducibility. For highly deterministic network activity, SOCBED provides the infrastructure to retrieve websites within the simulated network, and it can be extended with a man-in-the-middle proxy (e.g., mitmproxy [38]) to deterministically replay recorded network traffic.

#### Infrastructure as Code and Software Availability

While the open-source, infrastructure-as-code setup ensures complete transparency and adaptability, it also presents challenges, such as disappearing software download links. We recommend keeping local copies of all downloaded software and VM images to avoid issues when reproducing older testbed versions.

#### Scalability and Performance

SOCBED runs on commodity hardware and uses virtual machines, which may behave slightly differently depending on the host's soft- and hardware. Our evaluation showed that even similar hosts can lead to slight variations in generated log data. We suggest avoiding running the testbed on hosts with scarce resources or background activity and closely monitoring performance indicators during experiments.

#### Bounded Scalability

SOCBED focuses on scenarios with bounded scalability requirements to provide high-fidelity emulation, which is necessary for realistic log generation. While it can handle small to medium-sized scenarios, very large-scale simulations requiring less realistic emulation and complex scenarios with thousands of systems are out of scope. For such scenarios, approaches like flog [14] might be more suitable.

#### Built-in Self-Tests

Built-in self-tests require additional development effort but are extremely valuable for error discovery during installation or adaptation. We strongly recommend using and maintaining these tests when using SOCBED or developing other testbeds.

### Conclusion

Cybersecurity research often relies on artifacts (e.g., log data or network traffic) that are either not publicly available or generated using proprietary testbeds, limiting reproducibility. Fixed datasets can also lead to invalid conclusions due to a lack of transparency in artifact generation. To address this, we derived requirements for generating realistic, transparent, adaptable, replicable, and publicly available artifacts for cybersecurity experiments. We then presented SOCBED, an open-source testbed specifically designed for generating realistic log data, running on commodity hardware.

Our evaluation demonstrated that SOCBED-generated log data, while exhibiting natural variations between runs, can still be reproduced on different computers, and adaptations can be performed in a controlled manner. We make the evaluation scripts and generated log dataset publicly available [58, 66], enabling others to fully reproduce our experiment.

In conclusion, our work enhances reproducibility in cybersecurity research, particularly in the areas of log data and intrusion detection, and increases the potential for building future research on existing work.

### Acknowledgments

We thank the anonymous reviewers and our shepherd Evangelos Markatos for their valuable feedback and comments. This work was supported by the German Federal Ministry of Education and Research (BMBF) under grant no. 16KIS0342. The authors are responsible for the content of this paper.

### References

[1] Andy Applebaum, Doug Miller, Blake Strom, Chris Korban, and Ross Wolf. 2016. Intelligent, automated red team emulation. In Proceedings of the 32nd Annual Conference on Computer Security Applications. ACM, 363–373.
[2] Australian Cyber Security Centre. 2020. Windows Event Logging and Forwarding. Retrieved June 28, 2021 from https://www.cyber.gov.au/acsc/view-all-content/publications/windows-event-logging-and-forwarding
[3] Vaibhav Bajpai, Anna Brunstrom, Anja Feldmann, Wolfgang Kellerer, Aiko Pras, Henning Schulzrinne, Georgios Smaragdakis, Matthias Wählisch, and Klaus Wehrle. 2019. The Dagstuhl Beginners Guide to Reproducibility for Experimental Networking Research. SIGCOMM Comput. Commun. Rev. 49, 1 (2019), 24–30.
[4] Emilie Lundin Barse and Erland Jonsson. 2004. Extracting attack manifestations to determine log data requirements for intrusion detection. In Proceedings of the 20th Annual Computer Security Applications Conference. ACM, 158–167.
[5] Terry Benzel. 2011. The science of cyber security experimentation: the DETER project. In Proceedings of the 27th Annual Computer Security Applications Conference. ACM, 137–148.
[6] Sandeep Bhatt, Pratyusa K Manadhata, and Loai Zomlot. 2014. The operational role of security information and event management systems. IEEE Security & Privacy 12, 5 (2014), 35–41.
[7] Leyla Bilge and Tudor Dumitras. 2012. Before we knew it: an empirical study of zero-day attacks in the real world. In Proceedings of the 2012 ACM conference on computer and communications security. ACM, 833–844.
[8] Stephen M. Blackburn, Amer Diwan, Matthias Hauswirth, Peter F. Sweeney, José Nelson Amaral, Tim Brecht, Lubomír Bulej, Cliff Click, Lieven Eeckhout, Sebastian Fischmeister, Daniel Frampton, Laurie J. Hendren, Michael Hind, Antony L. Hosking, Richard E. Jones, Tomas Kalibera, Nathan Keynes, Nathaniel Nystrom, and Andreas Zeller. 2016. The Truth, The Whole Truth, and Nothing But the Truth: A Pragmatic Guide to Assessing Empirical Evaluations. ACM Trans. Program. Lang. Syst. 38, 4 (2016). https://doi.org/10.1145/2983574
[9] Tom Bowen, Alex Poylisher, Constantin Serban, Ritu Chadha, Cho-Yu Jason Chiang, and Lisa M Marvel. 2016. Enabling reproducible cyber research – four labeled datasets. In 2016 IEEE Military Communications Conference. IEEE, 539–544.
[10] Ritu Chadha, Thomas Bowen, Cho-Yu J Chiang, Yitzchak M Gottlieb, Alex Poylisher, Angello Sapello, Constantin Serban, Shridatt Sugrim, Gary Walther, Lisa M Marvel, et al. 2016. CyberVAN: A Cyber security Virtual Assured Network testbed. In 2016 IEEE Military Communications Conference. IEEE, 1125–1130.
[11] Anton Chuvakin, Kevin Schmidt, and Chris Phillips. 2012. Logging and log management: the authoritative guide to understanding the concepts surrounding logging and log management. Syngress.
[12] Jon Davis and Shane Magrath. 2013. A Survey of Cyber Ranges and Testbeds. Technical Report. Cyber and Electronic Warfare Division, Defence Science and Technology Organisation, Australian Government Department of Defence.
[13] Bernard Ferguson, Anne Tall, and Denise Olsen. 2014. National Cyber Range overview. In 2014 IEEE Military Communications Conference. IEEE, 123–128.
[14] flog contributors. 2020. mingrammer/flog: A fake log generator for common log formats. Retrieved September 8, 2021 from https://github.com/mingrammer/flog
[15] Ivo Friedberg, Florian Skopik, Giuseppe Settanni, and Roman Fiedler. 2015. Combating advanced persistent threats: From network event correlation to incident detection. Computers & Security 48 (2015), 35–57. https://doi.org/10.1016/j.cose.2014.09.006
[16] Sean Gallagher. 2014. Inside the “wiper” malware that brought Sony Pictures to its knees [Update]. Retrieved June 28, 2021 from http://arstechnica.com/security/2014/12/inside-the-wiper-malware-that-brought-sony-pictures-to-its-knees/
[17] Dennis Grunewald, Marco Lützenberger, Joël Chinnow, Rainer Bye, Karsten Bsufka, and Sahin Albayrak. 2011. Agent-based Network Security Simulation (Demonstration). In 10th International Conference on Autonomous Agents and Multiagent Systems (AMAS 2011). International Foundation for Autonomous Agents and Multiagent Systems, 1325–1326.
[18] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, Bob Lantz, and Nick McKeown. 2012. Reproducible network experiments using container-based emulation. In Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies. ACM, 253–264.
[19] Seth Hardy, Masashi Crete-Nishihata, Katharine Kleemola, Adam Senft, Byron Sonne, Greg Wiseman, Phillipa Gill, and Ronald J Deibert. 2014. Targeted threat index: Characterizing and quantifying politically-motivated targeted malware. In Proceedings of the 23rd USENIX Security Symposium. USENIX, 527–541.
[20] Brad Heath, Heather Timmons, and Peter Cooney. 2021. SolarWinds hack was 'largest and most sophisticated attack' ever: Microsoft president. Retrieved June 28, 2021 from https://www.reuters.com/article/us-cyber-solarwinds-microsoft-idUSKBN2AF03R
[21] Brian Hepburn and Hanne Andersen. 2021. Scientific Method (Stanford Encyclopedia of Philosophy). Retrieved June 28, 2021 from https://plato.stanford.edu/entries/scientific-method/
[22] Eric M Hutchins, Michael J Cloppert, and Rohan M Amin. 2011. Intelligence-driven computer network defense informed by analysis of adversary campaigns and intrusion kill chains. Leading Issues in Information Warfare & Security Research 1 (2011), 80–106.
[23] Kaspersky. 2014. Energetic Bear – Crouching Yeti. Retrieved June 28, 2021 from https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/08080817/EB-YetiJuly2014-Public.pdf
[24] Karen Kent and Murugiah Souppaya. 2006. NIST Special Publication 800-92: Guide to Computer Security Log Management. Retrieved June 28, 2021 from https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf
[25] Ansam Khraisat, Iqbal Gondal, Peter Vamplew, and Joarder Kamruzzaman. 2019. Survey of Intrusion Detection Systems: Techniques, Datasets and Challenges. Cybersecurity 2, 1 (2019). https://doi.org/10.1186/s42400-019-0038-7
[26] Vijay Kothari, Jim Blythe, Sean W. Smith, and Ross Koppel. 2015. Measuring the Security Impacts of Password Policies Using Cognitive Behavioral Agent-based Modeling. In Proceedings of the 2015 Symposium and Bootcamp on the Science of Security (HotSoS ’15). ACM, 13:1–13:9.
[27] Max Landauer, Florian Skopik, Markus Wurzenberger, Wolfgang Hotwagner, and Andreas Rauber. 2021. Have it Your Way: Generating Customized Log Datasets With a Model-Driven Simulation Testbed. IEEE Transactions on Reliability 70, 1 (2021), 402–415.
[28] Max Landauer, Florian Skopik, Markus Wurzenberger, and Andreas Rauber. 2020. System log clustering approaches for cyber security applications: A survey. Computers & Security 92 (2020), 101739.
[29] Elizabeth LeMay, Michael D Ford, Ken Keefe, William H Sanders, and Carol Muehrcke. 2011. Model-based security metrics using ADversary VIew Security Evaluation (ADVISE). In Eighth International Conference on Quantitative Evaluation of Systems. IEEE, 191–200.
[30] Richard Lippmann, Joshua W. Haines, David J. Fried, Jonathan Korba, and Kumar Das. 2000. Analysis and Results of the 1999 DARPA Off-Line Intrusion Detection Evaluation. In Recent Advances in Intrusion Detection, Hervé Debar, Ludovic Mé, and S. Felix Wu (Eds.). Springer Berlin Heidelberg, 162–182.
[31] Chris Long. 2021. Detection Lab. Retrieved June 28, 2021 from https://github.com/clong/DetectionLab
[32] Martin Maisey. 2014. Moving to analysis-led cyber-security. Network Security 2014, 5 (2014), 5–12.
[33] Robert C Martin. 2009. Clean code: a handbook of agile software craftsmanship. Pearson Education.
[34] McAfee, Inc. 2015. Grand Theft Data—Data exfiltration study: Actors, tactics, and detection. Retrieved June 28, 2021 from https://www.mcafee.com/enterprise/en-us/assets/reports/rp-data-exfiltration.pdf
[35] Rose McDermott. 2011. Internal and external validity. Cambridge handbook of experimental political science (2011), 27–40.
[36] J.H. McDonald. 2014. Handbook of Biological Statistics (3rd ed.). Retrieved June 28, 2021 from http://www.biostathandbook.com/twosamplettest.html
[37] Microsoft. 2021. SimuLand: Understand adversary tradecraft and improve detection strategies. Retrieved June 28, 2021 from https://github.com/Azure/SimuLand
[38] mitmproxy contributors. 2021. mitmproxy/mitmproxy: An interactive TLS-capable intercepting HTTP proxy for penetration testers and software developers. Retrieved September 8, 2021 from https://github.com/mitmproxy/mitmproxy
[39] Douglas C. Montgomery. 2017. Design and Analysis of Experiments. Wiley.
[40] H. D. Moore. 2011. Meterpreter HTTP/HTTPS Communication. Retrieved June 28, 2021 from https://www.rapid7.com/blog/post/2011/06/29/meterpreter-httphttps-communication/
[41] Stephen Moskal, Ben Wheeler, Derek Kreider, Michael E Kuhl, and Shanchieh Jay Yang. 2014. Context model fusion for multistage network attack simulation. In 2014 IEEE Military Communications Conference. IEEE, 158–163.
[42] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set). In 2015 Military Communications and Information Systems Conference (MilCIS). IEEE, 1–6.
[43] Pejman Najafi, Alexander Mühle, Wenzel Pünter, Feng Cheng, and Christoph Meinel. 2019. MalRank: a measure of maliciousness in SIEM-based knowledge graphs. In Proceedings of the 35th Annual Computer Security Applications Conference. ACM, 417–429.
[44] NetApplications.com. 2019. Operating System Market Share. Retrieved June 28, 2021 from https://netmarketshare.com/operating-system-market-share.aspx
[45] Open Information Security Foundation. [n.d.]. Suricata. Retrieved June 28, 2021 from https://suricata.io/
[46] Piotr Pauksztelo. 2014. Simulation of an Enterprise Network with Realistic User Behavior. Master’s thesis. Institute of Computer Science, Universität Bonn.