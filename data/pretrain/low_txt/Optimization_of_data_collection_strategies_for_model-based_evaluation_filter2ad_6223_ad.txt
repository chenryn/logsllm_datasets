### 4.8543
[30, 130]
1.5637
7.4728
1.2861
1.3333
0.98315
4.0995
1.7289
1.2223
1

### Strategies
- S1
- S2
- S3
- S4
- S5
- S6
- S7
- S8
- S9

### Efficiency of the Algorithms
To evaluate the efficiency of the algorithms, we need to calculate the variance of the variance (i.e., the variance of \( \text{Var}[E(Y) | s] \)) under both the Basic Algorithm 1 and the Importance Sampling Algorithm 2.

### Table I: Quotients for Optimization Problem 2
Table I presents the resulting quotients based on the results for Optimization Problem 2. There are 9 strategies in set \( S \), each used as a possible anchor, with each anchor corresponding to one column in the table. For each anchor strategy, the third row indicates the number of samples for each source, denoted as \([D_{1,1}, D_{2,1}]\). The sample sizes are in increments of 50, with each strategy staying within a budget of 200. For each anchor \( a \in S \), each row shows \( Q_{a,s} \) for all other strategies \( s \in S \). \( Q_{a,s} \) is the estimated multiplier of additional iterations the Importance Sampling Algorithm would need for \( s \) to achieve the same level of accuracy as a run with the Basic Algorithm.

For example, the value 5.398 for anchor strategy \( a = S4 \) and target strategy \( s = S1 \) implies that the Importance Sampling Algorithm with anchor \( a = S4 \) requires 5.398 times as many iterations to obtain results for \( S1 \) as the Basic Algorithm for \( S1 \).

### Computational Considerations
If the number of model results needed to evaluate a strategy equals \( K \) and \( |S| \) is the number of different strategies, without importance sampling, it takes \( K \times |S| \) iterations of the Basic Algorithm. If importance sampling uses only one strategy instead of \( |S| \), at a selected anchor, for importance sampling to be worthwhile, \( \max(Q_{a,s}) \) must be less than \( |S| \). Otherwise, achieving a sufficient level of accuracy requires more model results than direct execution. For instance, \( S1 \) makes a good anchor strategy since the maximum quotient is approximately 1.8, which is less than \( |S| \).

### M/M/1 Example
The M/M/1 example successfully demonstrates the approach and potential for reusing results via importance sampling. This example requires only that \( p1 < p2 \). More realistic and complex models will likely involve greater input dependency, which needs to be considered during the experiment generation (normal sampling) phase. Unless the model is very robust, the valid ranges for inputs and their parameters may be more dependent on other input parameters, requiring more careful design of strategy experiments. This problem and alternative experiment generation methods will need to be explored in future examples. The importance sampling results show that useful speed-up is possible by reusing results for the M/M/1 example. The benefit greatly depends on the choice of anchor strategy or, more realistically, multiple anchor strategies, and the accuracy stopping condition. Both of these need further exploration and formalization for more complex examples.

### Related Work
The data collection strategy optimization approach in this paper combines aspects of sensitivity and uncertainty analysis [9], [14], [5], and adds specific detailed techniques based on statistics (the use of the Central Limit Theorem and importance sampling). In this related work section, we compare our approach with existing forms of analysis, given that background on sample theory and statistics is largely available from textbooks. The main difference with the rich and varied body of related research in sensitivity and uncertainty analysis is that our work focuses on data collection strategies, creating a different set of optimization problems. We use ideas from well-studied types of analysis to solve these problems.

#### Impact of Input Values
The objective of uncertainty analysis is to understand how different input values impact the model output, while the objective of sensitivity analysis is to understand the importance of individual input parameters on model output [15], [5]. Both approaches are widely used in economics, statistics, physics, and many other areas [16]. For discrete-event systems, uncertainty analysis involves evaluating models for different input parameter values and collecting outputs. Techniques such as scatter plots, regression, and partial correlation analysis can then be applied to analyze the set of outputs obtained from the many runs [17]. These approaches require similar computational effort, leading to many proposals for speeding up the solution [16].

Of particular interest are uncertainty analysis approaches that relate input distributions to output distributions, also within the class of sampling-based approaches. The extensive study in [15] of methods that express correlation between input and output distribution demonstrates the richness of the area. Bayesian methods discussed in [19] also belong to this class, determining the output distribution given a prior of the probability distribution of the inputs. Resampling methods in [20], [21], which aim to improve the model output confidence interval, are also of interest, but in our setting, generating samples for the input parameters is not expensive, so approaches such as bootstrap resampling [21] do not apply. None of the uncertainty analysis methods discuss the issue of data source selection, but some techniques may be effective complementary techniques to add to our approach.

Additionally, uncertainty analysis studies more efficient quasi-random sample generation methods such as Latin hypercube sampling [22] to produce stratified samples or orthogonal experiment sets. Such variance reduction approaches may also be worth exploring in our setting. Importance sampling [23] has been mentioned as an approach [24], [16], but as a generic tool, it is known to be difficult to configure to speed up the simulation, as reported in [17]. In our approach, the fact that different strategies are being explored makes importance sampling almost risk-free, as the anchor strategy results are available and can be used. This is a consequence of our unique focus on optimizing the data collection strategy.

Sensitivity analysis aims to select the most important parameters using techniques such as screening methods [16] and variance-based methods [9]. This strongly relates to our approach, as it requires identifying the more important input parameters (factors in sensitivity analysis terminology). For instance, [6] provides an analytic Bayesian approach to reduce input uncertainty in queuing models, but such an analytic solution is not generally applicable for discrete-event dynamic systems. Sensitivity analysis closely relates to experimental design [5]. Our work is an approach to determine an optimal design, aiming to find the experimental setup that leads to the lowest variance, where the experimental setup is the data collection strategy. Research in the design of experiments tends to rely on information matrices, which provide combined information about multiple parameters of a statistical model. In the literature on experimental design, we have not come across an end-to-end setup as in our approach, with one notable exception [7]. [7] does consider data collection strategies, identifying reasonable approaches to solve the problem of determining how many more samples to generate. This is not the same as finding an optimal data collection strategy but is closely related. In their concluding section on data collection for multiple unknown parameters, they indicate a related approach to what we propose, framed as a statistical test. They only consider two inputs, and the proposed method is not worked out in detail or applied, and it is not clear if it generalizes to general strategies with an arbitrary number of data sources, possibly including cost.

### Conclusion
This paper proposes a method to decide on data collection strategies that aid in a decision-making process where probabilistic models are used to justify and support decisions. The main idea is to combine elements of uncertainty and sensitivity analysis into a comprehensive approach to determine data collection strategies. Based on a model of uncertainty of the input parameters and associated data sources, solving the original model for the variance conditional to various strategies yields insight into the optimal data collection strategy. The approach is especially natural in studies where data sources use sampling to determine a parameter value, as the Central Limit Theorem indicates the use of the Normal distribution to model the uncertainty of data sources. Such studies are increasingly prevalent in dependability evaluation, as human factors and business concerns are taken into account in modern-day studies.

The paper pays particular attention to the efficiency of the approach, as a naive implementation would require substantial computational effort to solve models for all strategies. We introduced an importance sampling-inspired approach to derive results for multiple strategies from a single computation. Through an example, we showed that considerable speed-up can be obtained, but the application of the importance sampling idea may be challenging. Further study is necessary to fully appreciate how to best apply the importance sampling-based algorithm for increasingly sophisticated models. Other variance reduction techniques may also be appropriate for our approach. Additionally, there are several subtleties, features, and assumptions that come with the method, and we have discussed these extensively in this paper. Each of them allows for further fine-tuning and alternative approaches. We believe this is worth pursuing further and believe that the ideas presented in this paper offer a promising and fruitful new way of determining data collection strategies, particularly relevant for modern-day studies.

### Acknowledgment
The authors would like to thank Hewlett-Packard for funding through its Innovation Research Program for 'Prediction and Provenance for Multi-Objective Information Security Management', UK TSB for 'Trust Economics' Network Innovation Platform, and UK EPSRC for grant EP/G011389 'Analysis of Massively Parallel Stochastic Systems'. Sincere thanks to our project partners D. Eskins, R. Berthier, W. Sanders, S. Parkin, and J. Turland for the many inspiring discussions that led to the ideas formulated in this paper.

### References
[1] A. Beautement, R. Coles, J. Griffin, C. Ioannidis, B. Monahan, D. Pym, M. A. Sasse, and M. Wonham, “Modelling the human and technological costs and benefits of USB memory stick security,” Managing Information Risk and the Economics of Security, pp. 141–163, 2009.
[2] S. Parkin, R. Yassin Kassab, and A. van Moorsel, “The impact of unavailability on the effectiveness of enterprise information security technologies,” Service Availability, pp. 43–58, 2008.
[3] W. Zeng and A. van Moorsel, “Quantitative Evaluation of Enterprise D Technology,” Electronic Notes in Theoretical Computer Science, vol. 275, pp. 159–174, 2011.
[4] G. Horvath, P. Buchholz, and M. Telek, “A MAP fitting approach with independent approximation of the inter-arrival time distribution and the lag correlation,” in Quantitative Evaluation of Systems, 2005. Second International Conference on the, Sep. 2005, pp. 124–133.
[5] A. Saltelli, M. Ratto, T. Andre, F. Campolongo, J. Cariboni, D. Gatelli, M. Saisana, and S. Tarantola, Global Sensitivity Analysis. The Primer. John Wiley, 2008.
[6] S. Ng and S. Chick, “Reducing input parameter uncertainty for simulations,” Proceedings of the 33nd conference on Winter simulation, pp. 364–371, 2001.
[7] M. Freimer and L. Schruben, “Collecting data and estimating parameters for input distributions,” Simulation Conference, 2002. Proceedings of the Winter, vol. 1, pp. 392–399 vol.1, 2002.
[8] Y.-C. Ho, “Introduction to special issue on dynamics of discrete event systems,” Proceedings of the IEEE, vol. 77, no. 1, pp. 3–6, Jan. 1989.
[9] K. Chan, A. Saltelli, and S. Tarantola, “Sensitivity analysis of model output: variance-based methods make the difference,” Proceedings of the 29th conference on Winter simulation, pp. 261–268, 1997.
[10] A. B. Massada and Y. Carmel, “Incorporating output variance in local sensitivity analysis for stochastic models,” Ecological Modelling, vol. 213, no. 3-4, pp. 463 – 467, 2008.
[11] A. van Moorsel, L. Kant, and W. Sanders, “Computation of the asymptotic bias and variance for simulation of Markov reward models,” in Simulation Symposium, 1996. Proceedings of the 29th Annual. IEEE, 1996, pp. 173–182.
[12] W. Cochran, Sampling Techniques, 3rd Edition. John Wiley, 1977.
[13] D. Draper, “Assessment and Propagation of Model Uncertainty,” Journal of the Royal Statistical Society. Series B (Methodological), vol. 57, pp. 45–97, 1995.
[14] J. Ascough, T. Green, L. Ma, and L. Ahuja, “Key criteria and selection of sensitivity analysis methods applied to natural resource models,” International Congress on Modeling and Simulation Proceedings, 2005.
[15] J. Helton and F. Davis, “Illustration of Sampling-Based Methods for Uncertainty and Sensitivity Analysis,” Risk Analysis, vol. 22, p. 591622, 2002.
[16] J. P. Kleijnen, “Sensitivity analysis and related analyses: a review of some statistical techniques,” Journal of Statistical Computation and Simulation, vol. 57, no. 1, pp. 111–142, 1997.
[17] J. Helton, J. Johnson, C. Sallaberry, and C. Storlie, “Survey of Sampling-Based Methods for Uncertainty and Sensitivity Analysis,” Reliability Engineering and System Safety, pp. 1175–1209, 2006.
[18] I. Gluhovsky, “Determining output uncertainty of computer system models,” Perform. Eval., vol. 64, pp. 103–125, February 2007.
[19] T. Sun and J. Wang, “A simple model for assessing output uncertainty in stochastic simulation systems.” in MICAI’07, 2007, pp. 337–347.
[20] R. Cheng and W. Holland, “Calculation of confidence intervals for simulation output,” ACM Transactions on Modeling and Computer Simulation (TOMACS), vol. 14, no. 4, pp. 344–362, 2004.