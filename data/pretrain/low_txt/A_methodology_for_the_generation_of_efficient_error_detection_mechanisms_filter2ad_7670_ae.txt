### Section VII: Results

The observed False Positive Rate (FPR) and True Positive Rate (TPR) values are consistent with the rates previously presented. The following tables summarize the TPR, Area Under the Curve (AUC), computational complexity (Comp), and variance (Var) for the evaluated models:

**TPR**
- 0.9982
- 0.9983
- 0.9991
- 0.9984
- 0.9876
- 0.9999
- 0.9966
- 0.9995
- 0.9963
- 0.7963
- 0.9628
- 0.8229
- 0.9938
- 0.9938
- 0.9989
- 0.9740
- 0.9740
- 0.9728

**AUC**
- 0.9991
- 0.9991
- 0.9996
- 0.9985
- 0.9937
- 0.9999
- 0.9977
- 0.9978
- 0.9974
- 0.8964
- 0.9813
- 0.9114
- 0.9969
- 0.9969
- 0.9995
- 0.9870
- 0.9870
- 0.9864

**Computational Complexity (Comp)**
- 19.0
- 34.3
- 11.9
- 67.4
- 9.9
- 13.5
- 113.7
- 174.5
- 113.2
- 68.3
- 173.1
- 61.2
- 7.0
- 7.0
- 9.0
- 7.0
- 7.0
- 3.3

**Variance (Var)**
- 2E-09
- 5E-08
- 6E-32
- 6E-07
- 6E-05
- 3E-08
- 8E-08
- 1E-08
- 1E-07
- 2E-05
- 3E-10
- 3E-10
- 1E-32
- 1E-32
- 1E-32
- 1E-32
- 4E-17
- 1E-28

### Section VIII: Discussion

The results in Section VII demonstrate that the proposed methodology is effective in generating predicates for efficient error detection mechanisms. Specifically, Decision Tree Induction, even under a basic configuration, has proven to be an effective and consistent method for generating predicates with high true-positive rates and low false-positive rates. The best derived predicate, represented as a decision tree, can be easily extracted by interpreting the decision tree as a conjunction of disjunctions. This means that implementing an error detection mechanism based on a model generated using our methodology is a straightforward process of interpreting the decision tree.

Fault injection analysis is commonly used in the validation of dependable software, and fault injection data is often available. Therefore, the main cost of applying the proposed methodology is associated with the data mining algorithms. This cost is related to the dataset magnitude, the specific data mining algorithm used, and the comprehensiveness of the refinement undertaken. We have shown that even a baseline configuration of a learning algorithm can yield highly efficient predicates, and a naive parameter search can further improve their efficiency.

This paper focuses on generating predicates for error detection mechanisms that can identify failure-inducing states. The fault injection analysis performed recorded the state of an executing program and whether that execution resulted in a failure. This focus contrasts with existing work on fault injection, which typically considers any deviation from a fault-free execution (i.e., a golden run) as an error. While the proposed methodology is not directly applicable in this context, we believe a similar approach could be adopted to derive error detection predicates that can identify such deviations from a fault-free execution.

The novelty of the proposed methodology lies in the application of data mining to fault injection data to generate efficient error detection predicates. The main advantage of this approach is that it allows for the design of efficient error detection mechanisms without relying on a formal system specification or the experience of software engineers.

### Section IX: Conclusion

#### A. Summary

In this paper, we presented a methodology for generating predicates for efficient error detection mechanisms. The premise is that, given a program location for which a detector component must be generated, optimized data mining techniques can be used to analyze fault injection data and generate efficient predicates. Unlike current approaches, this methodology does not rely on a system specification or the experience of software engineers. Our demonstration validates this premise, showing how data mining techniques can generate predicates with high accuracy and completeness.

#### B. Future Work

In future work, we plan to explore alternative approaches to the systematic design of predicates for error detection mechanisms. Specifically, we will evaluate the applicability and impact of different data mining algorithms, fault models, and system models in generating efficient error detection mechanisms.

### References

[1] J.-C. Laprie, Dependability: Basic Concepts and Terminology. Springer-Verlag, December 1992.
[2] A. Arora and S. S. Kulkarni, “Detectors and correctors: A theory of fault-tolerance components,” in Proceedings of the 18th IEEE International Conference on Distributed Computing Systems, May 1998, pp. 436–443.
[3] A. Jhumka, F. Freiling, C. Fetzer, and N. Suri, “An approach to synthesise safe systems,” International Journal of Security and Networks, vol. 1, no. 1, pp. 62–74, September 2006.
[4] M. Hsueh, T. K. Tsai, and R. K. Iyer, “Fault injection techniques and tools,” IEEE Computer, vol. 30, no. 4, pp. 75–82, April 1997.
[5] D. Powell, E. Martins, J. Arlat, and Y. Crouzet, “Estimators for fault tolerance coverage evaluation,” IEEE Transactions on Computers, vol. 44, no. 2, pp. 261–274, June 1995.
[6] M. Hiller, “Executable assertions for detecting data errors in embedded control systems,” in Proceedings of the 30th IEEE/IFIP International Conference on Dependable Systems and Networks, June 2000, pp. 24–33.
[7] J. Vinter, J. Aidemark, P. Folkesson, and J. Karlsson, “Reducing critical failures for control algorithms using executable assertions and best effort recovery,” in Proceedings of the 37th IEEE/IFIP International Conference on Dependable Systems and Networks, July 2001, pp. 347–356.
[8] N. G. Leveson, S. S. Cha, J. C. Knight, and T. J. Shimeall, “The use of self checks and voting in software error detection: An empirical study,” IEEE Transactions on Software Engineering, vol. 16, no. 4, pp. 432–443, April 1990.
[9] A. Jhumka and M. Leeke, “Issues on the design of efficient fail-safe fault tolerance,” in Proceedings International Symposium on Software Reliability Engineering, November 2009, pp. 155–164.
[10] S. S. Kulkarni and A. Arora, “Automating the addition of fault-tolerance,” in Proceedings of the 6th International Symposium on Formal Techniques in Real-Time and Fault-Tolerant Systems, September 2000, pp. 82–93.
[11] S. S. Kulkarni and A. Ebnenasir, “Complexity of adding failsafe fault-tolerance,” in Proceedings of the 22nd IEEE International Conference on Distributed Computing Systems, July 2002, pp. 337–344.
[12] M. Hiller, A. Jhumka, and N. Suri, “Propane: An environment for examining the propagation of errors in software,” in Proceedings of the 11th ACM SIGSOFT International Symposium on Software Testing and Analysis, July 2002, pp. 81–85.
[13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten, “The weka data mining software: An update,” SIGKDD Explorations, vol. 11, no. 1, pp. 10–18, June 2009.
[14] M. Hiller, A. Jhumka, and N. Suri, “An approach for analysing the propagation of data errors in software,” in Proceedings of the 31st IEEE/IFIP International Conference on Dependable Systems and Networks, July 2001, pp. 161–172.
[15] A. Jhumka, M. Hiller, and N. Suri, “An approach for designing and assessing detectors for dependable component-based systems,” in Proceedings of the 8th IEEE International Symposium on High Assurance Systems Engineering, March 2004, pp. 69–78.
[16] G. Pinter, H. Madeira, M. Vieira, A. Pataricza, and I. Majzik, “A data mining approach to identify key factors in dependability experiments in dependable computing,” in Proceedings of the 5th European Dependable Computing Conference, March 2005, pp. 263–280.
[31] K. M. Ting, “An instance-weighting method to induce cost-sensitive trees,” IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 3, pp. 659–665, May 2002.
[32] P. Domingos, “A general method for making classifiers cost-sensitive,” in Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 1999, pp. 155–164.
[33] W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan, “Misclassification cost-sensitive boosting,” in Proceedings of the 16th International Conference on Machine Learning, June 1999, pp. 97–105.
[34] J. Quinlan, C4.5: Programs for Machine Learning. Morgan Kaufmann, October 1992.
[35] M. Kubat and S. Matwin, “Addressing the curse of imbalanced training sets: One-sided selection,” in Proceedings of the 14th International Conference on Machine Learning, January 1997, pp. 179–186.
[36] D. D. Lewis and J. Catlett, “Heterogeneous uncertainty sampling for supervised learning,” in Proceedings of the 11th International Conference on Machine Learning, June 1994, pp. 148–156.
[37] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, no. 1, pp. 321–357, May 2002.
[38] B. Zadrozny, J. Langford, and N. Abe, “Cost-sensitive learning by cost-proportionate example weighting,” in Proceedings of the 3rd IEEE International Conference on Data Mining, July 2003, pp. 435–442.
[39] N. V. Chawla, D. A. Cieslak, L. O. Hall, and A. Joshi, “Automatically countering imbalance and its empirical relationship to cost,” Journal of Data Mining and Knowledge Discovery, vol. 17, no. 2, pp. 225–252, February 2008.
[40] 7-Zip, “http://www.7-zip.org/,” 2010.
[41] FlightGear, “http://www.flightgear.org/,” April 2009.
[42] Mp3Gain, “http://mp3gain.sourceforge.net/,” 2010.
[17] C.-T. Lu, A. P. Boedihardjo, and P. Manalwar, “Exploiting efficient data mining techniques to enhance intrusion detection systems,” in Proceedings of the 2005 IEEE International Conference on Information Reuse and Integration, September 2005, pp. 512–517.
[18] S. J. Wenke Lee Stolfo and K. W. Mok, “A data mining framework for building intrusion detection models,” in Proceedings of the 20th IEEE Symposium on Security and Privacy, May 1999, pp. 120–132.
[19] E. Clarke, O. Grumberg, and D. Peled, Model Checking. MIT Press, January 2000.
[20] D. A. Schmidt, “Data flow analysis is model checking of abstract interpretations,” in Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, January 1998, pp. 38–48.
[21] P. Cousot and R. Cousot, “Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints,” in Proceedings 6th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, January 1977, pp. 238–252.
[22] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin, “Dynamically discovering likely program invariants to support program evolution,” IEEE Transactions on Software Engineering, vol. 27, no. 2, pp. 99–123, February 2001.
[23] B. Demsky, M. D. Ernst, P. J. Guo, S. McCamant, J. H. Perkins, and M. Rinard, “Inference and enforcement of data structure consistency specifications,” in Proceedings of the International Symposium on Software Testing and Analysis, May 2006, pp. 233–244.
[24] S. K. Sahoo, M. Li, P. Ramachandran, S. V. Adve, V. S. Adve, and Y. Zhou, “Using likely program invariants to detect hardware errors,” in Proceedings of the 38th IEEE/IFIP International Conference on Dependable Systems and Networks, June 2008, pp. 70–79.
[25] D. Powell, “Failure model assumptions and assumption coverage,” in Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, July 1992, pp. 386–395.
[26] M. Kubat, R. C. Holte, and S. Matwin, “Machine learning for the detection of oil spills in satellite radar images,” Machine Learning, vol. 30, no. 2-3, pp. 195–215, 1998.
[27] T. Fawcett, “An introduction to ROC analysis,” Pattern Recognition Letters, vol. 27, no. 8, pp. 861–874, June 2006.
[28] N. Japkowicz, “The class imbalance problem: Significance and strategies,” in Proceedings of the 2000 International Conference on Artificial Intelligence (ICAI), June 2000, pp. 111–117.
[29] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classification and Regression Trees. Chapman and Hall/CRC, January 1984.
[30] M. Pazzani, C. Merz, P. Murphy, K. Ali, T. Hume, and C. Brunk, “Reducing misclassification costs,” in Proceedings of the 11th International Conference on Machine Learning, July 1994, pp. 217–225.

---

This version of the text is more structured, clear, and professional, with improved formatting and organization.