### 3.3 Lessons Learned

The previous subsections have demonstrated that error distributions vary significantly across different datasets. This observation holds true for both the False Rejection Rate (FRR) and the False Acceptance Rate (FAR), leading to distinct consequences. Among the metrics we analyzed to augment the FAR/FRR, the Gini Coefficient (GC) stands out as the most promising due to its compactness and ability to provide an absolute ordering of systems.

For the FAR, systems with a lower GC are desirable because this indicates that false accepts are spread relatively evenly across attackers, rather than allowing a few attackers to consistently evade detection. Our data shows that adding distinctive features, such as pupil diameter for eye movement biometrics, decreases the Equal Error Rate (EER) but increases the GC. This suggests that features that change little over the system's operation may be suitable for distinguishing users in general, but they can also lead to more consistent confusion among similar users, resulting in the aforementioned systematic errors. This insight is crucial during feature selection, as some distinctive features should be dropped to avoid this scenario. It is important to remember that not every change to a system that lowers the average error is beneficial to its security.

For the FRR, a high GC indicates erratic user behavior for a small number of users, an insight that can help improve either the system design or aid in avoiding this behavior during system operation. Overall, we recommend closely monitoring changes to the GC when experimenting with different feature sets to evaluate whether any of them consistently lead to systematic errors. When publishing results, the GC should always be reported alongside the mean EER/FAR/FRR to allow readers to consider error distributions during their evaluation.

### 4. Influencing Error Rates Through Training Data Selection

In Section 2, we observed that the majority of papers either randomly sample training data from the entire available dataset or merge data from all users (including the attacker) to form the negative class. It is well-known in related fields that error rates are systematically underestimated when the temporal order of samples is not preserved during training data selection. The precise impact has been well-researched in the context of malware analysis, where past malware can be classified more accurately when signatures of future malware are included in the training data [3, 4]. However, to the best of our knowledge, the precise impact has not been quantified for biometric-based continuous authentication. Understanding the exact influence of these methodologies is essential to assess whether a lower EER is due to a better system or excessive optimization through non-functional design decisions.

#### 4.1 Quantifying the Impact on the EER

The two non-functional parameters most likely to impact error rates are the attacker modeling process and the division of training data. There are several valid choices for both, raising the question of whether there is a seemingly "best" choice that minimizes (reported) error rates. To answer this, we compute the EER for several datasets under different assumptions. We consider all combinations of the following parameters:

- **Number of Aggregated Samples**: We statically choose a value of 100 for eye movement datasets and 15 for others (to reflect the lower sampling rate). We then aggregate samples based on a simple majority voting. Aggregating samples is common practice and was used in the original evaluation of all datasets we consider.
- **Dataset Division**: We consider ordered and random division. For a single session, an ordered split uses the first half for training and the second half for testing. If two sessions are available, only the first is used for training. For the random split, we randomly select half the data for training, maintaining the relative proportions of the classes to ensure roughly equal amounts of training data for each user. We repeat the sampling and classification process 20 times to measure the effects of this selection.
- **Attacker Modeling**: Anomaly detection requires a specialized classifier (such as a one-class SVM), which makes it difficult to isolate the effects of this parameter alone. Therefore, we consider the "all users" and "except attacker" approaches. For the latter, we perform classifier training separately for each user and each attacker, excluding the attacker from the training set. The negative class is created by combining all other users. The "all users" approach trains a single model per user, including positive data (from the legitimate user) and a single negative class (all other users). In both cases, we balance the positive and negative classes to avoid biasing the classifier.

The results of our analysis are shown in Figure 9. Randomly selecting training data provides the largest improvement, reducing the EER by up to 80% relative to the original EER. This effect is particularly pronounced for datasets collected over larger time spans (such as the inter-session and 2-weeks eye movement datasets). This strong effect is likely due to the classifier being unable to observe and account for changes in user behavior over time, leading to underfitting when considering the dataset over the entire time period. The mouse movement datasets, collected over a short period, are only marginally affected, supporting this explanation.

Another interesting insight is that the EER varies significantly depending on the training data selection. This suggests that the training and testing process must be repeated multiple times to ensure statistical robustness of the result. The distribution of errors was virtually unaffected by the change, suggesting that it mainly leads to shifting the mean.

The effects of the two different attacker models are significant, though less extreme than those of the training set selection. Across all datasets, including the attacker in the training data results in a relative improvement between 22% (mouse movements) and 63% (intra-session eye movements). It is somewhat counter-intuitive that the effect is larger for the larger datasets, even though the attacker data accounts for a smaller fraction of the overall negative class.

These results show that simply looking at the EER of a proposed system is insufficient, as it is heavily skewed by non-functional parameters that would not affect the performance of the system in a production environment. For example, if the same dataset (i.e., identical features and classifiers) were evaluated with random and ordered training data selection, one might favor one over the other, even though their practical performance would be identical. This is particularly alarming, as our analysis (see Section 2) shows that out of 25 papers, 13 use at least one of the methodologies that we have shown to lead to systematic underestimation of error rates. Additionally, 6 papers do not report how the error rates were obtained, which not only decreases confidence in the results but also hinders reproducibility and comparability with related work.

To inspire the highest confidence in their results, researchers should exclude attackers from the negative class in their training data and choose the first part of their entire dataset for training, rather than sampling it randomly. To facilitate easier comparison with earlier work, it would also be advisable to report error rates for different methodologies (such as random sampling).

### 5. Conclusion

In this paper, we provided a systematic analysis of the methodology used to evaluate behavioral biometrics for continuous authentication. Our analysis shows that most papers present the mean of standard metrics, specifically the EER and FAR, but do not provide insights into their precise distributions. We argue that some errors, particularly systematic false negatives, are particularly severe in the context of continuous authentication. The analysis of 16 real-world datasets shows that some biometrics, such as touchscreen inputs, exhibit mostly random errors, leading to the eventual detection of attackers due to the process of continuous authentication. Others, such as gait patterns, tend to produce more systematic errors, allowing some attackers to consistently avoid detection.

To allow the comparison of different systems with regard to this property without requiring manual inspection, we discuss several candidate metrics. As a result of this discussion, we propose the use of the Gini Coefficient (GC) to capture different distributions of both the FAR and FRR. Applying the GC to our datasets reveals that the addition or removal of certain features can greatly impact the biometric's error distribution. Specifically, using the pupil diameter for classification reduces the system's average EER but also contributes to systematic errors, suggesting it might even reduce overall security. Based on these insights, the GC can not only be used to compare the security of different systems but can also guide researchers during the evaluation of different classifiers, biometrics, and feature sets. We therefore recommend that authors report the GC along with established metrics to provide information about error distributions.

We also quantified the impact of several different machine learning methodologies on a system's error rates. We identified two main factors: the selection of training data (specifically, random versus ordered split) and the inclusion of imposter data in the negative class. While these effects are somewhat well-known in other fields, their precise impact has not been quantified in the context of continuous authentication. Our analysis shows that random sampling of training data can reduce the EER by up to 80%, while the inclusion of imposter data provides a reduction of up to 63%. These results highlight a particular problem, as 13 of the 25 papers we analyzed used a methodology that we have shown to lead to systematic underestimation of error rates, and a further 6 did not report which methodology was used at all.

Our results highlight that it is inadequate to compare biometric systems simply by their EERs. Instead, it is crucial to take into account both the distribution of errors and the design decisions made when simulating system operation on a static dataset.

### Acknowledgements

This work was supported by the Engineering and Physical Sciences Research Council [grant number EP/M50659X/1].

### References

[1] A. A. E. Ahmed and I. Traore. A new biometric technology based on mouse dynamics. Dependable and Secure Computing, IEEE Transactions on, 4(3):165–179, 2007.

[2] H. J. Ailisto, M. Lindholm, J. Mantyjarvi, E. Vildjiounaite, and S.-M. Makela. Identifying people from gait pattern with accelerometers. In Defense and Security, pages 7–14. International Society for Optics and Photonics, 2005.

[3] K. Allix, T. F. Bissyandé, J. Klein, and Y. Le Traon. Are your training datasets yet relevant? In International Symposium on Engineering Secure Software and Systems, pages 51–67. Springer, 2015.

[4] K. Allix, T. F. D. A. Bissyande, J. Klein, and Y. Le Traon. Machine learning-based malware detection for Android applications: History matters! Technical report, University of Luxembourg, SnT, 2014.

[5] S. Axelsson. The base-rate fallacy and the difficulty of intrusion detection. ACM Transactions on Information and System Security (TISSEC), 3(3):186–205, 2000.

[6] P. Bours and S. Mondal. Performance evaluation of continuous authentication systems. IET Biometrics, 4(4):220–226, 2015.

[7] A. Brajdic and R. Harle. Walk detection and step counting on unconstrained smartphones. In Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 225–234. ACM, 2013.

[8] Ş. Budulan, E. Burceanu, T. Rebedea, and C. Chiru. Continuous user authentication using machine learning on touch dynamics. In International Conference on Neural Information Processing, pages 591–598. Springer, 2015.

[9] D. Buschek, A. De Luca, and F. Alt. Improving accuracy, applicability, and usability of keystroke biometrics on mobile touchscreen devices. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 1393–1402. ACM, 2015.

[10] Z. Cai, C. Shen, M. Wang, Y. Song, and J. Wang. Mobile authentication through touch-behavior features. In Biometric Recognition, pages 386–393. Springer, 2013.

[11] M. O. Derawi, C. Nickel, P. Bours, and C. Busch. Unobtrusive user-authentication on mobile phones using biometric gait recognition. In Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), 2010 Sixth International Conference on, pages 306–311. IEEE, 2010.

[12] B. Draffin, J. Zhu, and J. Zhang. Keysens: Passive user authentication through micro-behavior modeling of soft keyboard interaction. In International Conference on Mobile Computing, Applications, and Services, pages 184–201. Springer, 2013.

[13] S. Eberz, K. B. Rasmussen, V. Lenders, and I. Martinovic. Preventing lunchtime attacks: Fighting insider threats with eye movement biometrics. In Proceedings of the Network and Distributed Systems Security Symposium (NDSS), 2015.

[14] S. Eberz, K. B. Rasmussen, V. Lenders, and I. Martinovic. Looks like Eve: Exposing insider threats using eye movement biometrics. ACM Transactions on Privacy and Security, 19(1):1, 2016.

[15] T. Feng, J. Yang, Z. Yan, E. M. Tapia, and W. Shi. TIPS: Context-aware implicit user identification using touch screen in uncontrolled environments. In Proceedings of the 15th Workshop on Mobile Computing Systems and Applications, page 9. ACM, 2014.

[16] M. Frank, R. Biedert, E. Ma, I. Martinovic, and D. Song. Touchalytics: On the applicability of touchscreen input as a behavioral biometric for continuous authentication. Information Forensics and Security, IEEE Transactions on, 8(1):136–148, 2013.

[17] D. Gafurov, K. Helkala, and T. Søndrol. Biometric gait authentication using accelerometer sensor. Journal of Computers, 1(7):51–59, 2006.

[18] H. Gascon, S. Uellenbeck, C. Wolf, and K. Rieck. Continuous authentication on mobile devices by analysis of typing motion behavior. In Sicherheit, pages 1–12. Citeseer, 2014.

[19] C. Gini. Variabilità e mutabilità. Reprinted in Memorie di metodologica statistica (Ed. Pizetti E, Salvemini, T). Rome: Libreria Eredi Virgilio Veschi, 1, 1912.

[20] M. Goffredo, I. Bouchrika, J. N. Carter, and M. S. Nixon. Self-calibrating view-invariant gait biometrics. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 40(4):997–1008, 2010.

[21] Z. Jorgensen and T. Yu. On mouse dynamics as a behavioral biometric for authentication. In Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security, pages 476–482. ACM, 2011.

[22] T. Kinnunen, F. Sedlak, and R. Bednarik. Towards task-independent person authentication using eye movement signals. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications, pages 187–190. ACM, 2010.

[23] J. Mäntyjärvi, M. Lindholm, E. Vildjiounaite, S.-M. Mäkelä, and H. Ailisto. Identifying users of portable devices from gait pattern with accelerometers. In Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP’05). IEEE International Conference on, volume 2, pages ii–973. IEEE, 2005.

[24] S. Mondal and P. Bours. Continuous authentication using mouse dynamics. In Biometrics Special Interest Group (BIOSIG), 2013 International Conference of the, pages 1–12. IEEE, 2013.

[25] M. Pusara and C. E. Brodley. User re-authentication via mouse movements. In Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security, pages 1–8. ACM, 2004.

[26] K. B. Rasmussen, M. Roeschlin, I. Martinovic, and G. Tsudik. Authentication using pulse-response biometrics. In NDSS, 2014.

[27] L. Rong, D. Zhiguo, Z. Jianzhong, and L. Ming. Identification of individual walking patterns using gait acceleration. In 2007 1st International Conference on Bioinformatics and Biomedical Engineering, pages 543–546. IEEE, 2007.

[28] A. Roy, T. Halevi, and N. Memon. An HMM-based behavior modeling approach for continuous mobile authentication. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3789–3793. IEEE, 2014.

[29] P. Saravanan, S. Clarke, D. H. P. Chau, and H. Zha. LatentGesture: Active user authentication through background touch analysis. In Proceedings of the Second International Symposium of Chinese CHI, pages 110–113. ACM, 2014.

[30] D. A. Schulz. Mouse curve biometrics. In 2006 Biometrics Symposium: Special Session on Research at the Biometric Consortium Conference, pages 1–6. IEEE, 2006.

[31] C. Shen, Y. Zhang, Z. Cai, T. Yu, and X. Guan. Touch-interaction behavior for continuous user authentication on smartphones. In 2015 International Conference on Biometrics (ICB), pages 157–162. IEEE, 2015.

[32] M. Soriano, A. Araullo, and C. Saloma. Curve spreads—a biometric from front-view gait video. Pattern Recognition Letters, 25(14):1595–1602, 2004.

[33] E. Vildjiounaite, S.-M. Mäkelä, M. Lindholm, R. Riihimäki, V. Kyllönen, J. Mäntyjärvi, and H. Ailisto. Unobtrusive multimodal biometrics for ensuring privacy and information security with personal devices. In International Conference on Pervasive Computing, pages 187–201. Springer, 2006.

[34] A. Weiss, A. Ramapanicker, P. Shah, S. Noble, and L. Immohr. Mouse movements biometric identification: A feasibility study. Proc. Student/Faculty Research Day CSIS, Pace University, White Plains, NY, 2007.

[35] H. Xu, Y. Zhou, and M. R. Lyu. Towards continuous and passive authentication via touch biometrics: An experimental study on smartphones. In Symposium On Usable Privacy and Security (SOUPS 2014), pages 187–198, 2014.

[36] X. Zhao, T. Feng, and W. Shi. Continuous mobile authentication using a novel graphic touch gesture feature. In Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on, pages 1–6. IEEE, 2013.

[37] N. Zheng, A. Paloski, and H. Wang. An efficient user verification system via mouse movements. In Proceedings of the 18th ACM conference on Computer and communications security, pages 139–150. ACM, 2011.

### Appendix A. Datasets

In this section, we describe the datasets used to evaluate our new metrics (see Section 3). We use 13 datasets obtained from the authors of previously published work and...