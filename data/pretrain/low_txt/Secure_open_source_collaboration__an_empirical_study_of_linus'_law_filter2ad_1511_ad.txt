### 5.4 Discussion

Our results indicate a statistically significant correlation in all four developer activity metrics. Three of these metrics support the concept of unfocused contributions, specifically that vulnerable files were subject to frequent changes (NumCommits), by many developers (NumDevs), and by developers who were also working on numerous other files (CNBetweenness).

When examining unfocused contributions, it is worth considering Frederick Brooks' perspective on the coordination challenges in large projects: "If each part of the task must be separately coordinated with every other part, the effort increases as n(n-1)/2." [3] In this context, Brooks is discussing Brook's Law, which states, "Adding manpower to a late software project makes it later." The reasoning behind this law is similar to the argument for unfocused contributions, as coordinating a large number of developers can require a communication and coordination effort that grows quadratically with the number of developers. While unfocused contributions do not necessarily negate Linus' Law, our results show that they are a legitimate opposing force.

Regarding diversity in perspectives, having more developers change the code does not inherently increase security. An interesting observation from the NumDevs chart in Figure 4 is that the proportion of vulnerabilities steadily increases as the number of developers increases, up to about nine, after which the recall drops. However, files worked on by disparate, otherwise-separated clusters of developers are more likely to have vulnerabilities.

One factor to consider when evaluating software security and Linus' Law is the ongoing nature of code changes. Many open-source projects, including the Linux kernel, are constantly evolving. The developer community must continually identify and address vulnerabilities in an ever-changing and branching project.

### 6. Limitations

All of our developer activity metrics require version control data, and thus, changes in the system. For developer networks, if a file has no commits during the study period, it has no associated developers and, therefore, no measurements can be made. In our case study, all vulnerabilities occurred in files that were changed within 15 months prior to release. A vulnerability could exist in a file that was not changed, but since our case study did not include such instances, this effect did not impact our results.

Additionally, we cannot claim that developer activity metrics cause vulnerabilities. Studies of historical data can only show statistically significant correlations. Proving causation would require a controlled experiment, which is not feasible for an ongoing project like the Linux kernel. Furthermore, although we used the term "validation criteria," we do not consider developer activity metrics fully validated until further case studies support consistent, repeatable results.

Since our data only includes known vulnerabilities, we cannot make claims about latent, undiscovered vulnerabilities. Consequently, we cannot determine whether a low precision (i.e., a high occurrence of false positives) is indicative of real false positives or if our model is identifying more vulnerabilities that have not yet been confirmed.

### 7. Related Work

Several recent empirical studies have examined the topics of developers and collaboration. Most of these studies either explore the meaning of developer activity metrics or relate them to reliability. Only one study relates developer activity metrics to security.

Shin et al. [21] evaluated the statistical connection between vulnerabilities and metrics of complexity, code churn, and developer activity. The study included two case studies of large, open-source projects: multiple releases of Mozilla Firefox and the RHEL4 kernel. The findings showed a statistically significant correlation between metrics in all three categories and security vulnerabilities. In the Mozilla project, a model containing all three types of metrics was able to identify 70.8% of the known vulnerabilities by selecting only 10.9% of the project's files. This study examines a different set of metrics and combines disparate metrics into a single model.

Meneely et al. [13] examined the relationship between developer activity metrics and reliability. The empirical case study analyzed three releases of a large, proprietary networking product. The authors used developer centrality metrics from the developer network to examine whether files are more likely to have failures if they were changed by developers who are peripheral to the network. They formed a model that included metrics of developer centrality, code churn, and lines of code to predict failures from one release to the next. Their model's prioritization found 58% of the system's failures in 20% of the files, where a perfect prioritization would have found 61%. The study did not include work on developer clusters, unfocused contributions, or security.

Bird et al. [1] use a similar approach to ours to examine social structures in open-source projects. They also discuss connections and contradictions between some of Brooks's ideas [3] and the bazaar-like development of open-source projects. The authors empirically examine how open-source developers self-organize, using similar network structures as our developer network to find the presence of sub-communities within open-source projects. In addition to examining version control change logs, the authors mined email logs and other artifacts of several open-source projects to find a community structure. They conclude that sub-communities do exist in open-source projects, as evidenced by the project artifacts exhibiting a social network structure that resembles collaboration networks in other disciplines. In our study, we leverage network analysis metrics to estimate collaboration and examine their relationship to vulnerabilities in the project.

Pinzger et al. [17] were the first to propose the contribution network. The contribution network uses version control data to quantify the direct and indirect contributions of developers on specific resources of the project. The researchers used metrics of centrality in their study of Microsoft Windows Vista and found that closeness was the most significant metric for predicting reliability failures. Files contributed to by many developers, especially those making many different contributions themselves, were found to be more failure-prone than files developed in relative isolation. The finding is that files focused on by a few developers are less problematic than files developed by many developers. In our study, we use centrality metrics on contribution networks to predict vulnerabilities in files.

Gonzales-Barahona and Lopez-Fernandez [6] were the first to propose the idea of creating developer networks as models of collaboration from source repositories. The authors' objective was to present the developer network and to differentiate and characterize projects.

Nagappan et al. [15] created a logistic regression model for failures in the Windows Vista operating system. The model was based on what they called "Overall Organizational Ownership" (OOW). The OOW model includes concepts like organizational cohesiveness and diverse contributions. Among the findings is that more edits made by many, non-cohesive developers lead to more problems post-release. The OOW model was able to predict with 87% average precision and 84% average recall. The OOW model bears a resemblance to the contribution network in that both models attempt to differentiate healthy changes in software from problematic changes.

### 8. Summary

The objective of this research is to reduce security vulnerabilities by providing actionable insights into the structural nature of developer collaboration in open-source software. Within our case study of the RHEL4 kernel, we found four metrics that empirically support the notions of Linus' Law and unfocused contributions. An empirical analysis of our data demonstrates the following observations:

(a) Source code files changed by multiple, otherwise-separated clusters of developers are more likely to be vulnerable than those changed by a single cluster.
(b) Files are likely to be vulnerable when changed by many developers who have made many changes to other files.

Practitioners can use these observations to prioritize security fortification efforts or to consider organizational changes among developers. While the results are statistically significant, the individual correlations indicate that developer activity metrics are likely to perform best for prediction in the presence of other metrics.

### 9. Acknowledgments

We thank Mark Cox and the Realsearch group for their valuable support. This work was supported by the U.S. Army Research Office (ARO) under grant W911NF-08-1-0105 managed by NCSU Secure Open Systems Initiative (SOSI).

### 10. References

[1] C. Bird, D. Pattison, R. D'Souza et al., "Latent Social Structures in Open Source Projects," in FSE, Atlanta, GA, 2008, p. 24-36.
[2] U. Brandes, and T. Erlebach, Network Analysis: Methodological Foundations, Berlin: Springer, 2005.
[3] F. Brooks, The Mythical Man-Month: Addison-Wesley, 1995.
[4] A. Endres, and D. Rombach, A Handbook of Software and Systems Engineering: Empirical Observations, Laws and Theories: Addison Wesley, 2003.
[5] M. Girvan, and M. E. J. Newman, "Community Structure in Social and Biological Networks," The Proceedings of the National Academy of Sciences, vol. 99, no. 12, p. 7821-7826, 2001.
[6] J. M. Gonzales-Barahona, L. Lopez-Fernandez, and G. Robles, "Applying Social Network Analysis to the Information in CVS Repositories," in 2005 Mining Software Repositories, Edinburgh, Scotland, United Kingdom, 2004.
[7] J.-H. Hoepman, and B. Jacobs, "Increased Security through Open Source," Commun. ACM, vol. 50, no. 1, p. 79-83, 2007.
[8] ISO, ISO/IEC DIS 14598-1 Information Technology - Software Product Evaluation, 1996.
[9] M. M. Lehman, and L. Belady, Program Evolution: Processes of Software Change, London: Academic Press, 1985.
[10] M. M. Lehman, and J. F. Ramil, "Rules and Tools for Software Evolution Planning and Management," Annals of Software Engineering, vol. 11, no. 1, p. 15-44, 2001.
[11] M. M. Lehman, J. F. Ramil, P. D. Wernick et al., "Metrics and Laws of Software Evolution -- The Nineties View," in 4th International Software Metrics Symposium (METRICS '97), Albuquerque, NM, 1997, p. 20-32.
[12] A. M. Martinez, and A. C. Kak, "PCA versus LDA," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 2, p. 228-233, 2001.
[13] A. Meneely, L. Williams, J. Osborne et al., "Predicting Failures with Developer Networks and Social Network Analysis," in Foundations in Software Engineering, Atlanta, GA, 2008, p. to appear.
[14] N. Nagappan, and T. Ball, "Use of Relative Code Churn Measures to Predict System Defect Density," in 27th International Conference on Software Engineering, St. Louis, MO, USA, 2005, p. 284-292.
[15] N. Nagappan, B. Murphy, and V. R. Basili, "The Influence of Organizational Structure on Software Quality," in International Conference on Software Engineering, Leipzig, Germany, 2008, p. 521-530.
[16] K. Numata, S. Imoto, and S. Miyano, "A Structure Learning Algorithm for Inference of Gene Networks from Microarray Gene Expression Data Using Bayesian Networks," in Bioinformatics and Bioengineering, 2007. BIBE 2007., p. 1280-1284.
[17] M. Pinzger, N. Nagappan, and B. Murphy, "Can Developer-Module Networks Predict Failures?," in Foundations in Software Engineering, Atlanta, GA, 2008, p. 2-12.
[18] M. Pinzger, N. Nagappan, and B. Murphy, "Can Developer-Module Networks Predict Failures?," in Foundations in Software Engineering, Atlanta, GA, 2008, p. to appear.
[19] E. S. Raymond, The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary, Sebastopol, California: O'Reilly and Associates, 1999.
[20] N. F. Schneidewind, "Methodology For Validating Software Metrics," IEEE Transactions on Software Engineering, vol. 18, no. 5, p. 410-422, 1992.
[21] Y. Shin, A. Meneely, L. Williams et al., "Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities," NCSU CSC Technical Report TR-2009-10, submitted to IEEE TSE.
[22] K. Tae-Kyun, and J. Kittler, "Locally Linear Discriminant Analysis for Multimodally Distributed Classes for Face Recognition with a Single Model Image," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 3, p. 318-327, 2005.
[23] B. Witten, C. Landwehr, and M. Caloyannides, "Does Open Source Improve System Security?," IEEE Softw., vol. 18, no. 5, p. 57-61, 2001.
[24] I. H. Witten, and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, 2nd ed., San Francisco: Morgan Kaufmann, 2005.