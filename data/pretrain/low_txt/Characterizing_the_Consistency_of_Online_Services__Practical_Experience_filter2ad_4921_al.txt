### Operation and Request Rate Limits

The operation in question involves truncating the number of elements in a feed. Additionally, there are request rate limits that restrict the number of additional requests our middleware can make to retrieve missing elements. Currently, there is no designated storage for metadata associated with these elements (e.g., entries in a data object). We believe that incorporating this feature into future services would provide application developers with the flexibility to associate extra information with their application data. Given that most services already return metadata with posts, supporting this feature appears to be both straightforward and beneficial.

### Middleware Layer

The middleware layer we developed operates on the client side, intercepting all interactions between the client and the online service. This interception helps avoid making unnecessary requests when elements are missing. We have presented four algorithms that enforce well-known session guarantees. These algorithms are designed in a way that allows them to be combined. To evaluate our approach, we developed a prototype in Java and tested it using two services: Facebook and a geo-replicated deployment of Redis. Our experiments demonstrated that we can enforce session guarantees with minimal overhead, both in terms of user-perceived latency and communication with the centralized service. It's important to note that our middleware is not limited to these specific services; it can be used with any service that provides a read/write interface compatible with the middleware's interface. This work was published in SRDS2017 [38].

### Future Work

While the results presented in this thesis provide tools and insights for better understanding the consistency anomalies prevalent in today’s online services, and propose a client-side (generic) approach to enhance the consistency guarantees of such services, several avenues for future research remain:

1. **Extended Measurement Study**:
   - The current measurement study was conducted with four services: Facebook Feed, Facebook Group, Google+, and Blogger. Expanding this study to include more services like Instagram or LinkedIn could provide a broader view of the consistency guarantees offered by these platforms.
   
2. **White-Box Testing**:
   - Another important extension would be to detect anomalies in internal components of services where access is available. This would involve moving beyond black-box testing to white-box testing, which could be applied to large-scale storage systems.

3. **Additional Consistency Properties**:
   - Our middleware currently enforces four session guarantees. It may be relevant to ensure other consistency properties for application developers, such as the guarantee that two clients do not diverge or that the divergence window is bounded and does not affect availability. These properties should be enforced without significant cost to applications.

4. **Support for More Operations**:
   - While we chose to support the main operations provided by these services, some services offer additional operations. Extending the middleware to support more operations, such as feed pagination, could be useful for application developers. This would also present a challenge given the restrictions imposed by services on the number of API calls that can be made within limited time windows.

### Bibliography

[1] D. Abadi. “Consistency tradeoffs in modern distributed database system design: CAP is only part of the story.” In: IEEE Computer 45.2 (2012), pp. 37–42.

[2] D. Agrawal, A. El Abbadi, and R. C. Steinke. “Epidemic algorithms in replicated databases.” In: Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems. ACM. Tucson, Arizona, USA, 1997, pp. 161–172.

[3] M. Ahamad, G. Neiger, J. E. Burns, P. Kohli, and P. W. Hutto. “Causal memory: Definitions, implementation, and programming.” In: Springer Distributed Computing 9.1 (1995), pp. 37–49.

[4] B. F. C. et. al. “Benchmarking Cloud Serving Systems with YCSB.” In: Proceedings of the 1st ACM Symposium on Cloud Computing. ACM. Indianapolis, Indiana, USA, 2010, pp. 143–154.

[5] S. Almeida, J. Leitão, and L. Rodrigues. “ChainReaction: a causal+ consistent datastore based on chain replication.” In: Proceedings of the 8th ACM European Conference on Computer Systems. ACM. Prague, Czech Republic, 2013, pp. 85–98.

[6] Amazon Elastic Compute Cloud (Amazon EC2). https://aws.amazon.com/ec2 (accessed Mar-2019).

[7] Amazon Simple Database. https://aws.amazon.com/simpledb (accessed Mar-2019).

[8] Amazon Simple Queue Service. https://aws.amazon.com/sqs/ (accessed Mar-2019).

[9] Amazon Simple Storage Service. https://aws.amazon.com/s3 (accessed Mar-2019).

[10] E. Anderson, X. Li, M. A. Shah, J. Tucek, and J. J. Wylie. “What consistency does your key-value store actually provide?” In: Proceedings of the Sixth Workshop on Hot Topics in System Dependability. Vancouver, BC, Canada, 2010, pp. 1–16.

[11] Apache HBase. https://hbase.apache.org/ (accessed Mar-2019).

[12] Apache HTTP Server. https://httpd.apache.org (accessed Mar-2019).

[13] H. Attiya, F. Ellen, and A. Morrison. “Limitations of highly-available eventually-consistent data stores.” In: IEEE Transactions on Parallel and Distributed Systems 28.1 (2017), pp. 141–155.

[14] Azure Databases. https://azure.microsoft.com/en-us/product-categories/databases (accessed Mar-2019).

[15] P. Bailis, S. Venkataraman, M. J. Franklin, J. M. Hellerstein, and I. Stoica. “Probabilistically bounded staleness for practical partial quorums.” In: Proceedings of the VLDB Endowment 5.8 (2012), pp. 776–787.

[16] P. Bailis, A. Ghodsi, J. M. Hellerstein, and I. Stoica. “Bolt-on causal consistency.” In: Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data. ACM. New York, USA, 2013, pp. 761–772.

[17] J. Baker, C. Bond, J. C. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh. “Megastore: Providing scalable, highly available storage for interactive services.” In: Fifth Biennial Conference on Innovative Data Systems Research. CA, USA, January, 2011.

[18] N. M. Belaramani, M. Dahlin, L. Gao, A. Nayate, A. Venkataramani, P. Yalagandula, and J. Zheng. “PRACTI Replication.” In: Proceedings of the 3rd Conference on Networked Systems Design and Implementation. USENIX. San Jose, California, USA, 2006, pp. 5–5.

[19] D. Bermbach and S. Tai. “Eventual Consistency: How Soon is Eventual? An Evaluation of Amazon S3’s Consistency Behavior.” In: Proceedings of the 6th Workshop on Middleware for Service Oriented Computing. ACM. Lisbon, Portugal, 2011, 1:1–1:6.

[20] D. Bermbach, J. Kuhlenkamp, B. Derre, M. Klems, and S. Tai. “A Middleware Guaranteeing Client-Centric Consistency on Top of Eventually Consistent Datastores.” In: Cloud Engineering (IC2E), 2013 IEEE International Conference on. IEEE. San Francisco, CA, USA, 2013, pp. 114–123.

[21] Blogger API. https://developers.google.com/blogger// (accessed Mar-2019).

[22] M. Brantner, D. Florescu, D. Graf, D. Kossmann, and T. Kraska. “Building a Database on S3.” In: Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. ACM. Vancouver, Canada, 2008, pp. 251–264.

[23] M. Bravo, L. Rodrigues, and P. Van Roy. “Saturn: A distributed metadata service for causal consistency.” In: Proceedings of the Twelfth European Conference on Computer Systems. ACM. Belgrade, Serbia, 2017, pp. 111–126.

[24] E. A. Brewer. “Towards robust distributed systems.” In: Proceedings of the Nineteenth Annual ACM Symposium on Principles of Distributed Computing. Portland, Oregon, USA, 2000.

[25] N. Bronson, Z. Amsden, G. Cabrera, P. Chakka, P. Dimov, H. Ding, J. Ferris, A. Giardullo, S. Kulkarni, H. Li, et al. “TAO: Facebook’s Distributed Data Store for the Social Graph.” In: 2013 USENIX Annual Technical Conference. San Jose, CA, USA, 2013, pp. 49–60.

[26] J. Brzezinski, C. Sobaniec, and D. Wawrzyniak. “From session causality to causal consistency.” In: Proceedings of the 12th Euromicro Conference on Parallel, Distributed and Network-Based Processing. Coruna, Spain, 2004, pp. 152–158.

[27] Cassandra Database. https://cassandra.apache.org (accessed Mar-2019).

[28] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni. “PNUTS: Yahoo!’s hosted data serving platform.” In: Proceedings of the VLDB Endowment 1.2 (2008), pp. 1277–1288.

[29] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, et al. “Spanner: Google’s globally distributed database.” In: ACM Transactions on Computer Systems (TOCS) 31.3 (2013), p. 8.

[30] F. Cristian. “Probabilistic clock synchronization.” In: Springer Distributed computing 3.3 (1989), pp. 146–158.

[31] S. B. Davidson, H. Garcia-Molina, and D. Skeen. “Consistency in a partitioned network: a survey.” In: ACM Computing Surveys (CSUR) 17.3 (1985), pp. 341–370.

[32] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels. “Dynamo: amazon’s highly available key-value store.” In: ACM SIGOPS Operating Systems Review 41.6 (2007), pp. 205–220.

[33] Facebook API. https://developers.facebook.com/docs/apis-and-sdks (accessed Mar-2019).

[34] Facebook API - groups feed. https://developers.facebook.com/docs/graph-api/reference/v3.2/group/feed// (accessed Mar-2019).

[35] Facebook API - news feed. https://developers.facebook.com/docs/graph-api/reference/v3.0/user/home// (accessed Mar-2019).

[36] Facebook Service. https://www.facebook.com (accessed Mar-2019).

[37] F. Freitas, J. Leitao, N. Preguiça, and R. Rodrigues. “Characterizing the Consistency of Online Services (Practical Experience Report).” In: Dependable Systems and Networks (DSN), 2016 46th Annual IEEE/IFIP International Conference on. IEEE. Toulouse, France, 2016, pp. 638–645.

[38] F. Freitas, J. Leitão, N. Preguiça, and R. Rodrigues. “Fine-Grained Consistency Upgrades for Online Services.” In: Reliable Distributed Systems (SRDS), 2017 IEEE 36th Symposium on. IEEE. Hong Kong, 2017, pp. 1–10.

[39] S. Gilbert and N. Lynch. “Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services.” In: ACM Sigact News 33.2 (2002), pp. 51–59.

[40] Google API. https://developers.google.com/ (accessed Mar-2019).

[41] Google App Engine. https://cloud.google.com/appengine (accessed Mar-2019).

[42] M. P. Herlihy and J. M. Wing. “Linearizability: A correctness condition for concurrent objects.” In: ACM Transactions on Programming Languages and Systems (TOPLAS) 12.3 (1990), pp. 463–492.

[43] Jedis Redis Library. https://github.com/xetorthio/jedis (accessed Mar-2019).

[44] M. E. Khan, F. Khan, et al. “A comparative study of white box, black box and grey box testing techniques.” In: Int. J. Adv. Comput. Sci. Appl 3.6 (2012).

[45] R. Ladin, B. Liskov, L. Shrira, and S. Ghemawat. “Providing high availability using lazy replication.” In: ACM Transactions on Computer Systems (TOCS) 10.4 (1992), pp. 360–391.

[46] A. Lakshman and P. Malik. “Cassandra: a decentralized structured storage system.” In: ACM SIGOPS Operating Systems Review 44.2 (2010), pp. 35–40.

[47] L. Lamport. “Time, clocks, and the ordering of events in a distributed system.” In: Communications of the ACM 21.7 (1978), pp. 558–565.

[48] L. Lamport. “On interprocess communication, Part 1: basic formalism, Part II: algorithms.” In: Distributed Computing. v1 i2 (1986), pp. 77–101.

[49] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen. “Don’t settle for eventual: scalable causal consistency for wide-area storage with COPS.” In: Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles. ACM. Cascais, Portugal, 2011, pp. 401–416.

[50] H. Lu, K. Veeraraghavan, P. Ajoux, J. Hunt, Y. J. Song, W. Tobagus, S. Kumar, and W. Lloyd. “Existential consistency: Measuring and understanding consistency at Facebook.” In: Proceedings of the 25th Symposium on Operating Systems Principles. ACM. Monterey, California, 2015, pp. 295–310.

[51] P. Mahajan, L. Alvisi, M. Dahlin, et al. “Consistency, availability, and convergence.” In: University of Texas at Austin Tech Report 11 (2011), p. 158.

[52] H. Mahmoud, F. Nawab, A. Pucher, D. Agrawal, and A. El Abbadi. “Low-latency multi-datacenter databases using replicated commit.” In: Proceedings of the VLDB Endowment 6.9 (2013), pp. 661–672.

[53] P. Mockapetris. RFC1035: Domain names—implementation and specification. 1987.

[54] MySQL Database. https://www.mysql.com (accessed Mar-2019).

[55] Network Time Protocol(NTP). http://www.ntp.org// (accessed Mar-2019).

[56] D. S. Parker, G. J. Popek, G. Rudisin, A. Stoughton, B. J. Walker, E. Walton, J. M. Chow, D. Edwards, S. Kiser, and C. Kline. “Detection of mutual inconsistency in distributed systems.” In: IEEE transactions on Software Engineering (1983), pp. 240–247.

[57] K. Petersen, M. J. Spreitzer, D. B. Terry, M. M. Theimer, and A. J. Demers. “Flexible Update Propagation for Weakly Consistent Replication.” In: SIGOPS Operating Systems Review 31 (1997), pp. 288–301.

[58] Redis storage. https://redis.io/ (accessed Mar-2019).

[59] REST Facebook Library. https://restfc.com (accessed Mar-2019).

[60] Y. Saito and M. Shapiro. “Optimistic replication.” In: ACM Computing Surveys (CSUR) 37.1 (2005), pp. 42–81.

[61] SQLServer Database. https://www.microsoft.com/en-us/sql-server/sql-server-2017 (accessed Mar-2019).

[62] A. S. Tanenbaum and M. Van Steen. Distributed systems: principles and paradigms. Prentice-Hall, 2007.

[63] K. Tangwongsan, S. Tirthapura, and K.-L. Wu. “Parallel streaming frequency-based aggregates.” In: Proceedings of the 26th acm symposium on parallelism in algorithms and architectures. ACM. Prague, Czech Republic, 2014, pp. 236–245.

[64] D. B. Terry, A. J. Demers, K. Petersen, M. J. Spreitzer, M. M. Theimer, and B. B. Welch. “Session guarantees for weakly consistent replicated data.” In: Parallel and Distributed Information Systems, 1994., Proceedings of the Third International Conference on. IEEE. 1994, pp. 140–149.

[65] Twitter API. https://developer.twitter.com/ (accessed Mar-2019).

[66] Twitter Service. https://www.twitter.com (accessed Mar-2019).

[67] W. Vogels. “Eventually consistent.” In: Communications of the ACM 52.1 (2009), pp. 40–44.

[68] H. Wada, A. Fekete, L. Zhao, K. Lee, and A. Liu. “Data Consistency Properties and the Trade-offs in Commercial Cloud Storage: the Consumers’ Perspective.” In: Fifth Biennial Conference on Innovative Data Systems Research. Vol. 11. Asilomar, CA, USA, 2011, pp. 134–143.

[69] Y. Xu, Z. Musgrave, B. Noble, and M. Bailey. “Bobtail: Avoiding Long Tails in the Cloud.” In: 10th USENIX Symposium on Networked Systems Design and Implementation. Vol. 13. Lombard, IL, 2013, pp. 329–342.

[70] H. Yu and A. Vahdat. “Design and evaluation of a continuous consistency model for replicated services.” In: Proceedings of the 4th Conference on Symposium on Operating System Design & Implementation-Volume 4. USENIX. San Diego, California, 2000, pp. 305–318.

[71] H. Yu and A. Vahdat. “Design and Evaluation of a Conit-based Continuous Consistency Model for Replicated Services.” In: ACM Transactions on Computer Systems 20.3 (2002), pp. 239–282.

[72] M. Zawirski, N. Preguiça, S. Duarte, A. Bieniusa, V. Balegas, and M. Shapiro. “Write fast, read in the past: Causal consistency for client-side applications.” In: Proceedings of the 16th Annual Middleware Conference. ACM. Trento, Italy, 2015, pp. 75–87.

[73] K. Zellag and B. Kemme. “How consistent is your cloud application?” In: Proceedings of the Third ACM Symposium on Cloud Computing. ACM. San Jose, California, 2012, 6:1–6:14.