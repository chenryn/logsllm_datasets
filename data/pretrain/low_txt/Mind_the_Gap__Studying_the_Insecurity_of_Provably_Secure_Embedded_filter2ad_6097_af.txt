### Constant-Time Code and Key Leakage

Constant-time code, by definition, should not leak any information about the key. However, in some cases, it may trivially leak the key. For example, if the reset signal is raised, it is often assumed that key leakage is prevented. However, consider an alternative core where a DMA read causes a reset, but the read value is still output on the DMA data bus without masking. Alternatively, the core might properly clear registers during a reset (A4) but then restore them 100 cycles later, thus leaking the key without violating assumptions A1-A5.

It is important to note that the issue is not whether such alternative cores are realistic. The critical point is that none of these misbehaving cores would cause the proof of VRASED to fail, and the end-to-end security claim does not follow from the assumptions A1-A5, as claimed.

### Discussion

Our case study suggests several concrete, actionable guidelines to help avoid vulnerabilities and provide better security assurance for systems. We categorize these guidelines by vulnerability type.

#### Implementation/Model Mismatches

A successful attack on the real system that can be represented in the formal model but fails there indicates a disconnect between the real system and the model. Since the real system is not a mathematical object, it is fundamentally impossible to find all these issues through more rigorous use of formal deductive methods alone. However, for software-based systems, many implementation artifacts can be given a mathematical interpretation. By defining a semantics for programming or hardware description languages, the code becomes a mathematical object. This introduces the assumption that the real system will execute the code according to the defined semantics. Verifying security properties of the source code used in the real system's implementation then becomes a deductive problem that can, at least in principle, be handled rigorously using mathematical methods.

Our case study provides substantial evidence that implementation/model mismatches can be avoided by maintaining a strong connection between the model and the implementation code. For SancusV, where this connection is weak, we found a considerable number of mismatches (V-B1 to V-B7). In contrast, the model for HW-Mod in VRASED was automatically derived from the Verilog code, and no model/implementation mismatches were found within the HW-Mod component. However, other parts of VRASED, where the connection was again weak, including the interface between HW-Mod and the untrusted core, suffer from several issues. Many of the implementation/model mismatches from our case study are relatively simple errors that could have been avoided or discovered with a more rigorous connection.

**Guideline:** Avoid implementation/model mismatches by maintaining strong connections between the code used in the implementation and the model used for verification.

While the systems in our case study only used automatic derivation of a model to maintain this connection, other techniques can be very useful in reducing implementation/model mismatches. These include deriving the implementation code from the model [53], systematically testing executable models against the real system [54], or directly verifying the code itself [55].

Provable security results that do not provide evidence for the connection between the model and the implementation, like SancusV, can still be very useful. They show the absence of mistakes or oversights in one specific aspect of the system (like the design of the interrupt padding mechanism in SancusV). However, they provide only weak assurance about the security of the real implementation.

#### Missing Attacker Capabilities

The existence of attacks on the real system that cannot be represented in the formal model shows that the formal model is incomplete in some sense. This is the most challenging category of errors, and deductive methods can fundamentally not find all these issues. Domain and attack expertise are essential to assess whether a model captures all relevant attacks, and it can never be ruled out that new kinds of realistic, creative attack techniques are invented.

Our case study provides evidence for this by introducing a fundamentally new attack technique that exploits the contention between the CPU and a DMA device to break some security properties of both systems considered in our study. The literature provides other examples, such as the discovery of Spectre attacks [56], which invalidated proofs of many confidentiality properties based on execution models that did not consider speculative execution.

In the absence of a systematic way of avoiding model incompleteness, we have to rely on heuristic guidelines and rules of thumb to assess the completeness of models.

**Guideline:** Useful sanity checks to avoid model incompleteness include:
- The model should either represent attacks from the literature or explicitly argue why they are not represented.
- Directly modeling specific attack scenarios should be avoided; focus on modeling attacker capabilities and ways of composing these capabilities into attack scenarios. This will typically allow the model to represent a wider range of attack scenarios where the attacker can also compose capabilities in unexpected ways.
- Interfaces between verified and unverified components should be audited for attacker-controlled inputs.

Our case study provides evidence for these rules of thumb: three out of five model incompleteness issues that we found in VRASED could have been avoided if the model had included the realistic capability that the attacker can measure cycle-accurate time. Additionally, at least three of the issues we found in VRASED are directly related to untrusted interfaces. It is, however, important to emphasize that modeling more attacker capabilities can also impact the feasibility of the verification: verification techniques do not necessarily scale well as more attacker capabilities are modeled.

#### Deductive Errors

If a successful attack on a real system can also be performed on the model of the system, this implies that there is an error in the formalization itself: the proof that no attacks exist in the model must be flawed. One of the strengths of formal methods is that they allow for very systematic checking for these kinds of errors, and by moving to more rigorous proof methods, ideally machine-checked proofs, these errors can be avoided. In the case of VRASED (cf. Section VI-D), moving to a stricter formal argument would have uncovered that the end-to-end RA security property does not follow from the assumptions. A possible solution may be to rigorously prove that the core adheres to an operational semantics.

Other research also supports the claim that rigorous use of formal methods is very useful in avoiding this class of errors. For instance, an extensive effort to uncover compilation bugs [57] in 11 compilers found no bugs in the machine-checked formally verified CompCert [53] middle-end, but found bugs in all other compilers and in the unverified parts of CompCert.

**Guideline:** Avoid deductive errors with rigorous reasoning, ideally using end-to-end machine-checked proofs.

### Related Work

The use of formal methods to increase assurance in the security of computer systems dates back to the early days of computer security, particularly with the US Military’s desire to build multi-level secure operating systems [58]. The Multics operating system is perhaps the most influential example of a system whose security assurance combined formal methods [59] with serious attack research [60]. Experience with the evaluation of Multics and related systems ultimately led to the development of the Common Criteria [61] as a standard for the evaluation of the security of systems. The highest assurance level of the Common Criteria requires a combination of deductive evidence (a formally verified design) and inductive evidence (thorough testing) — but very few products aim for that highest assurance level. While the Common Criteria has had some success in the evaluation and certification of very security-sensitive commercial products, like smart cards or hardware security modules, it also has widely recognized limitations in terms of cost, bureaucracy, and lack of agility in the certification process [62].

From the eighties onward, formal methods have been successfully used in the fields of protocol design and cryptography. Security properties of protocols were formalized as early as the eighties [63], [64]. Currently, protocols are often formalized during the design phase to detect mistakes before deployment, for example in the NIST Post-Quantum Cryptography Standardization Process [10] or for TLS 1.3 [9].

In parallel, researchers have also discovered that formal approaches can still lead to mistakes. The Needham-Schroeder protocol, formalized in 1989 [64], was shown to be broken seven years later using an automated tool [65]. In the years since, many attacks have been discovered against formally proven protocol schemes [66], [67]. Koblitz and Menezes [67] list multiple reasons why proofs can go wrong, some of which, such as implicit and incorrect assumptions, also appeared in our case study. Other papers, like the KRACK attacks [68], show how protocol implementations deviating from the formal descriptions can enable attacks, while pen-and-paper security proofs have been described as “alarmingly fragile” [69].

In contemporary systems security research, formalization of security properties is not yet as common, even if calls have often been made to use more scientific reasoning and formal methods for security guarantees [1]–[3], [70]. Notable recent systems with public security proofs include Komodo [12] and the seL4 microkernel [71]. Intel SGX also has (non-public) proofs about the linearizability of its instructions [13] and about some security properties [72].

### Conclusion

For systems of the size considered in this paper, we believe the time is ripe to work towards complete open-source hardware-software implementations whose security is supported by a combination of (i) deductive evidence based on models derived from, or strongly connected to the source code, and (ii) inductive evidence based on attack research, supporting the assumptions for the deductive reasoning.

### Acknowledgment

We would like to thank the designers of the Sancus and VRASED architectures for making their systems open-source. We are grateful to Job Noorman, Thomas Van Strydonck, and the anonymous reviewers for their insightful comments on different versions of this paper. This research is partially funded by the Research Fund KU Leuven, by the Flemish Research Programme Cybersecurity, and by a gift from Intel Corporation. Jo Van Bulck is supported by a grant of the Research Foundation – Flanders (FWO).

### References

[1] D. I. Good, “The foundations of computer security: We need some,” in Medieval Propaganda Pamphlet. University of Texas, 1986.
[2] B. D. Snow, “We need assurance!” in 21st Annual Computer Security IEEE Computer Society, 2005, pp. 3–10.
[3] C. Herley and P. C. van Oorschot, “Science of security: Combining theory and measurement to reflect the observable,” IEEE Security & Privacy, vol. 16, no. 1, pp. 12–22, 2018.
[4] ——, “Sok: Science, security and the elusive goal of security as a scientific pursuit,” in 2017 IEEE Symposium on Security and Privacy (S&P). IEEE Computer Society, 2017, pp. 99–120.
[5] C. Canella, J. Van Bulck, M. Schwarz, M. Lipp, B. von Berg, P. Ortner, F. Piessens, D. Evtyushkin, and D. Gruss, “A systematic evaluation of transient execution attacks and defenses,” in 28th USENIX Security Symposium, Aug. 2019, pp. 249–266.
[6] D. Jang, R. Jhala, S. Lerner, and H. Shacham, “An empirical study of privacy-violating information flows in JavaScript web applications,” in 17th ACM Conference on Computer and Communications Security (CCS). ACM, 2010, pp. 270–283.
[7] D. A. Basin and S. Capkun, “The research value of publishing attacks,” Communications of the ACM, vol. 55, no. 11, pp. 22–24, 2012.
[8] L. Szekeres, M. Payer, T. Wei, and D. Song, “Sok: Eternal war in memory,” in 2013 IEEE Symposium on Security and Privacy (S&P). IEEE Computer Society, 2013, pp. 48–62.
[9] K. G. Paterson and T. van der Merwe, “Reactive and proactive standardisation of TLS,” in Security Standardisation Research - Third International Conference (SSR), ser. Lecture Notes in Computer Science, vol. 10074. Springer, 2016, pp. 160–186.
[10] G. Alagic, G. Alagic, J. Alperin-Sheriff, D. Apon, D. Cooper, Q. Dang, Y.-K. Liu, C. Miller, D. Moody, R. Peralta et al., “Status report on the first round of the NIST post-quantum cryptography standardization process,” 2019.
[11] P. Subramanyan, R. Sinha, I. A. Lebedev, S. Devadas, and S. A. Seshia, “A formal foundation for secure remote execution of enclaves,” in 24th ACM Conference on Computer and Communications Security (CCS). ACM, 2017, pp. 2435–2450.
[12] A. Ferraiuolo, A. Baumann, C. Hawblitzel, and B. Parno, “Komodo: Using verification to disentangle secure-enclave hardware from software,” in 26th ACM Symposium on Operating Systems Principles (SOSP), 2017, pp. 287–305.
[13] R. Leslie-Hurd, D. Caspi, and M. Fernandez, “Verifying linearizability of Intel® Software Guard Extensions,” in Computer Aided Verification - 27th International Conference (CAV), ser. Lecture Notes in Computer Science, vol. 9207. Springer, 2015, pp. 144–160.
[14] M. Busi, J. Noorman, J. Van Bulck, L. Galletta, P. Degano, J. T. Mühlberg, and F. Piessens, “Provably secure isolation for interruptible enclaved execution on small microprocessors,” in 33rd IEEE Computer Security Foundations Symposium (CSF), Jun. 2020, pp. 262–276.
[15] I. D. O. Nunes, K. Eldefrawy, N. Rattanavipanon, M. Steiner, and G. Tsudik, “VRASED: A verified hardware/software co-design for remote attestation,” in 28th USENIX Security Symposium, 2019, pp. 1429–1446.
[16] J. Noorman, J. Van Bulck, J. T. Mühlberg, F. Piessens, P. Maene, B. Preneel, I. Verbauwhede, J. Götzfried, T. Müller, and F. Freiling, “Sancus 2.0: A low-cost security architecture for IoT devices,” ACM Transactions on Privacy and Security (TOPS), vol. 20, no. 3, pp. 1–33, 2017.
[17] J. Van Bulck, J. T. Mühlberg, and F. Piessens, “VulCAN: Efficient component authentication and software isolation for automotive control networks,” in 33rd Annual Computer Security Applications Conference (ACSAC), Dec. 2017, pp. 225–237.
[18] F. Alder, J. Van Bulck, F. Piessens, and J. T. Mühlberg, “Aion: Enabling open systems through strong availability guarantees for enclaves,” in 28th ACM Conference on Computer and Communications Security (CCS), Nov. 2021, pp. 1357–1372.
[19] I. D. O. Nunes, K. Eldefrawy, N. Rattanavipanon, and G. Tsudik, “PURE: Using verified remote attestation to obtain proofs of update, reset and erasure in low-end embedded systems,” in International Conference on Computer-Aided Design (ICCAD). ACM, 2019, pp. 1–8.
[20] ——, “APEX: A verified architecture for proofs of execution on remote devices under full software compromise,” in 29th USENIX Security Symposium. USENIX Association, 2020, pp. 771–788.
[21] I. D. O. Nunes, S. Jakkamsetti, N. Rattanavipanon, and G. Tsudik, “On the TOCTOU problem in remote attestation,” in 28th ACM Conference on Computer and Communications Security (CCS), Nov. 2021, pp. 2921–2936.
[22] I. D. O. Nunes, S. Jakkamsetti, and G. Tsudik, “Tiny-CFA: Minimalistic control-flow attestation using verified proofs of execution,” in Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2021, pp. 641–646.
[23] J. Certes and B. Morgan, “Remote attestation of bare-metal microprocessor software: A formally verified security monitor,” in International Conference on Database and Expert Systems Applications. Springer, 2021, pp. 42–51.
[24] J. Götzfried, T. Müller, R. de Clercq, P. Maene, F. Freiling, and I. Verbauwhede, “Soteria: Offline software protection within low-cost embedded devices,” in 31st Annual Computer Security Applications Conference (ACSAC), 2015, pp. 241–250.
[25] P. Maene, J. Götzfried, R. de Clercq, T. Müller, F. Freiling, and I. Verbauwhede, “Hardware-based trusted computing architectures for isolation and attestation,” IEEE Transactions on Computers, vol. 67, no. 3, pp. 361–374, 2018.
[26] O. Girard, openMSP430, 1.17 ed., https://github.com/olgirard/openmsp430/blob/master/doc/openMSP430.pdf, Nov. 2017.
[27] M. Busi, J. Noorman, J. Van Bulck, L. Galletta, P. Degano, J. T. Mühlberg, and F. Piessens, “Securing interruptible enclaved execution on small microprocessors,” ACM Transactions on Programming Languages and Systems (TOPLAS), vol. 43, no. 3, pp. 12:1–12:77, 2021.
[28] P. Koeberl, S. Schulz, A.-R. Sadeghi, and V. Varadharajan, “TrustLite: A security architecture for tiny embedded devices,” in 9th European Conference on Computer Systems (EuroSys). ACM, 2014, pp. 10:1–10:14.
[29] F. Brasser, B. El Mahjoub, A.-R. Sadeghi, C. Wachsmann, and P. Koeberl, “TyTAN: Tiny trust anchor for tiny devices,” in 52nd ACM/IEEE Design Automation Conference (DAC), 2015, pp. 1–6.
[30] R. de Clercq, F. Piessens, D. Schellekens, and I. Verbauwhede, “Secure interrupts on low-end microcontrollers,” in 25th IEEE International Conference on Application-Specific Systems, Architectures and Processors (ASAP), 2014, pp. 147–152.
[31] “Sancus-core: Minimal openmsp430 hardware extensions for isolation and attestation,” https://github.com/sancus-tee/sancus-core, accessed 2021-08-06.
[32] J. Van Bulck, F. Piessens, and R. Strackx, “Nemesis: Studying micro-architectural timing leaks in rudimentary CPU interrupt logic,” in 25th ACM Conference on Computer and Communications Security (CCS), Oct. 2018, pp. 178–195.
[33] P. C. Kocher, “Timing attacks on implementations of Diffie-Hellman, RSA, DSS, and other systems,” in Annual International Cryptology Conference, 1996, pp. 104–113.
[34] T. Goodspeed, “Practical attacks against the MSP430 BSL,” in Twenty-Fifth Chaos Communications Congress. Berlin, Germany, 2008.
[35] K. Eldefrawy, G. Tsudik, A. Francillon, and D. Perito, “SMART: Secure