# Taming Anycast in the Wild Internet

**Authors:**
- Stephen McQuistin, University of Glasgow, Glasgow, UK (Email: [PI:EMAIL])
- Sree Priyanka Uppu, Verizon Digital Media Services, Los Angeles, CA, USA (Email: [PI:EMAIL])
- Marcel Flores, Verizon Digital Media Services, Los Angeles, CA, USA (Email: [PI:EMAIL])

## Abstract
Anycast is a widely used technique for deploying globally available systems, such as DNS infrastructure and content delivery networks (CDNs). The optimization of these networks often focuses on the deployment and management of anycast sites. However, this approach overlooks one of the primary configurations of a large anycast network: the set of networks that receive anycast announcements at each site (i.e., the announcement configuration). Modifying these configurations, even without adding new sites, can significantly impact both anycast site selection and round-trip times.

In this study, we explore the operation and optimization of anycast networks with a large number of upstream service providers. We demonstrate that many-provider anycast networks exhibit fundamentally different properties when interacting with the Internet, including a greater number of single AS hop paths and reduced dependency on individual providers compared to few-provider networks. We further examine the impact of announcement configuration changes, showing that in nearly 30% of vantage point groups, round-trip time performance can be improved by more than 25% through strategic manipulation of which providers receive anycast announcements. Finally, we propose DailyCatch, an empirical measurement methodology for testing and validating announcement configuration changes, and demonstrate its ability to influence user-experienced performance on a global anycast CDN.

## CCS Concepts
- **Networks**: Network architectures, Network performance evaluation, Network experimentation, Network measurement

## ACM Reference Format
Stephen McQuistin, Sree Priyanka Uppu, and Marcel Flores. 2019. Taming Anycast in the Wild Internet. In *Internet Measurement Conference (IMC '19)*, October 21–23, 2019, Amsterdam, Netherlands. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3355369.3355573

## Permission Notice
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

## IMC '19, October 21–23, 2019, Amsterdam, Netherlands
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6948-0/19/10...$15.00
https://doi.org/10.1145/3355369.3355573

## Figure 1: Anycast Networks with Few vs. Many Upstream Providers
Anycast networks may feature either few (left) or many (right) upstream network providers.

## 1. Introduction
IP anycast is widely used for providing high-availability and low-latency network services, including DNS and CDNs. With IP anycast, network operators announce the same IP prefixes from multiple geographically-distributed sites. While each site provides the same service, the performance experienced by end-users can vary significantly based on the site selected and the path taken. BGP, however, has no notion of latency or load and may be heavily influenced by arbitrary network policies, further complicating the situation. Moreover, anycast networks do not have direct control over inbound routing, which is largely determined by the policies of upstream providers. Here, we define providers as transit, paid, and exchange peers, as well as any network interconnection that provides client connectivity.

Much of the prior work on anycast performance has focused on assessing the performance of anycast networks based on geographic distance or the comparative performance of unicast addresses. Such comparisons, however, create unrealistic goals, as significant performance gains are unlikely without new meta-information or routing techniques. Instead, we argue that the most promising approaches are those that can be deployed using existing routing infrastructure. Specifically, for many-provider networks with broad and diverse peering links, the key mechanism for influencing routing decisions is through the announcements themselves, by altering the set of providers that receive them. This, however, requires careful measurement to attribute observed changes.

In this paper, we show that anycast networks with more providers interact directly with a larger share of the Internet than those with fewer providers. We demonstrate that having a large number of providers results in a greater proportion of short paths, with up to 86% of publicly visible paths consisting of only a single AS hop. We also show that such networks have lower AS hegemony, a measure of the variety seen on inbound paths. Furthermore, we examine how manipulating inbound routes alters end-user performance. We find that announcement changes can induce significant shifts in both the site selected and the route taken. For example, 49.5% of vantage point groups shift site catchment, resulting in an over 25% reduction in round-trip time (RTT) for 30% of groups. However, indiscriminate changes can lead to reduced performance in nearly 40% of networks.

To manage these configurations, we present DailyCatch, a methodology for routine empirical measurements. DailyCatch captures the performance changes induced by modifications to anycast announcement configurations, allowing operators to assess and weigh the impacts of any change. Using measurements from a large, global anycast CDN, we show that DailyCatch exposes provider policies, confirming the non-trivial nature of managing provider configurations. While many large anycast networks have been studied, we believe this is the first study to examine the performance impacts of anycast configuration manipulation in a production, many-provider anycast network.

Previous studies have explored various techniques for managing anycast networks, including proposals to use only a single provider, focusing on effective deployment at the site level, working around poor anycast performance using DNS routing, and deploying new BGP communities. While these approaches are effective in certain contexts, they may not be applicable in scenarios where many providers are necessary for scale and reliability, or where significant rearchitecting is not possible. Therefore, we focus on the existing environment, where having many providers is necessary and where many of those providers are non-cooperative. We also focus on solutions that can be realistically implemented in an existing, real-world network.

We structure the remainder of this paper as follows:
- **Section 2**: Overview of anycast networks and their interaction with the wider Internet at the BGP level.
- **Section 3**: Examination of the specific impacts of announcement configuration changes on RTT performance.
- **Section 4**: Presentation of DailyCatch and examples of its measurements from the perspective of a global CDN provider.
- **Section 5**: Discussion of the trade-offs of our design, including the challenges of performing active measurements.
- **Section 6**: Related work.
- **Section 7**: Conclusion.

## 2. Anycast and BGP
IP anycast is a technique for deploying distributed services. An operator announces the same IP prefixes from multiple physical locations, or anycast sites. Site selection is then performed by routers using BGP. The catchment of a given site is the set of clients that are routed to that site. Commonly deployed anycast networks consist of clusters of servers that service requests from end-users, such as DNS root servers or HTTP servers in a CDN. Anycast enables easy failover and allows services to increase capacity by adding more sites without scaling up load balancing infrastructure. By deploying sites in diverse geographic locations with a diverse set of providers, operators can add significant robustness to their networks.

However, by pushing site selection into BGP, which has no notion of performance or load, anycast operators cede control of their inbound traffic to upstream networks. This exposes the anycast network to the impacts of upstream decisions, making it difficult to predict and control traffic routing. These challenges are further complicated by the need for many-provider anycast networks. While anycast networks often have complex networking configurations, their use cases may determine their use of upstream providers. The frequent use of anycast for end-user-facing services encourages operators to connect to a large number of networks, providing greater capacity and reliability. However, using only a single provider, while producing the most predictable results, is not feasible for large deployments. Directly connecting to networks offers significant potential to improve performance and reduce costs.

The complexity of configuring who receives anycast announcements and where creates an opportunity for optimization. By changing the set of providers that their anycast prefixes are announced to, operators can influence how a particular client reaches their network, either by changing the site they are served by or the path their traffic takes. Our conclusions apply to both anycast and unicast networks with many providers.

## 2.1 Choice of Anycast Networks
To develop an understanding of many-provider anycast, we examine multiple large anycast networks. Our first measurement target is the DNS root servers, which are common targets due to the availability of information on their deployments and the fact that they are managed by multiple organizations with differing strategies. Next, we consider measurements from a large, globally deployed commercial CDN featuring over one hundred sites and thousands of interconnects. This network operates multiple, independent anycast networks from the same AS, with different announcements restricted to particular physical locations. We treat these as separate entities, labeled CDN-1 through CDN-4. Finally, we consider Google DNS (8.8.8.8, labeled GDNS) as another large, global, high-traffic anycast network. Each of these networks is designed to service different traffic patterns, with the root servers providing service to DNS resolvers, the CDN servicing HTTP requests, and GDNS acting as a local resolver. Our aim is not to assess the performance of any particular approach but to demonstrate that the differences between them create opportunities for optimization based on announcement configurations.

## 2.2 Many-Provider Networks
We demonstrate that large, many-provider networks interact more directly with a larger portion of the Internet than those with only a few providers. These structural differences change the way anycast must be managed and increase the need for direct, evidence-based anycast management. We examine public BGP data from a set of RouteViews collectors. While BGP reveals many structural relationships, it is important to note that the data used here may underestimate some effects due to the presence of private peering links. However, these limitations are well known and do not affect the generality of our conclusions.

**Figure 2**: Counts of unique AS neighbors for DNS root servers (excluding G and H), the CDN, and GDNS.

**Neighbors**: Figure 2 shows the counts of unique AS neighbors for each of the DNS roots, the CDN, and Google’s DNS service. As shown, the aggressive peering policies of both the CDN and GDNS are apparent, as they have significantly more AS neighbors than all but the K root. This view presents two broad groups: those with few AS neighbors (fewer than 10 upstream providers) and those with many neighbors (10 or more providers). While neighbor counts alone do not determine routing behavior, they provide insight into the number of possible routes a client may take to the anycast network. The boundary between the few and many provider classes (of 10 providers) is somewhat arbitrary, but the goal is to use these classes to illustrate the differences in network behavior.