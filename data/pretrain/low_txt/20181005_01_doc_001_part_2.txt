### Header Transposition Actions

**Rules** are the entities that perform actions on matching packets in the MAT model. According to the original goal 3, rules allow the controller to be as expressive as possible while minimizing fixed policy in the dataplane. Each rule consists of two parts: a condition list and an action. The conditions are specified via a list of criteria, and the actions are defined based on these conditions. Examples of conditions and actions are listed in Figure 5.

Rules can be organized into groups for transactional update/replace operations or to split a port into sub-interfaces (e.g., to create independent policies for multiple Docker-style containers behind a single port).

### Packet Processor and Flow Compiler

A key innovation in VFPv2 was the introduction of a central packet processor. Inspired by common designs in network ASIC pipelines, VFPv2 parses relevant metadata from the packet and acts on this metadata rather than directly on the packet. This approach allows for all decisions to be made before the packet is modified. Flows are compiled and stored as packets are processed. The just-in-time flow compiler includes a parser, an action language, an engine for manipulating parsed metadata and actions, and a flow cache.

### Action Primitives: Header Transpositions (HTs)

Our action primitives, known as **Header Transpositions (HTs)**, are designed to change or shift fields throughout a packet. HTs are a list of parameterizable header actions, with one action per header. These actions include:
- **Push**: Add a header to the header stack.
- **Modify**: Change fields within a given header.
- **Pop**: Remove a header from the header stack.
- **Ignore**: Pass over the header without modification.

Table 3 provides examples of NAT HTs used by Ananta and encap/decap HTs used by VL2.

### Unified FlowIDs

VFP’s packet processor begins with parsing. Each L2/L3/L4 header (as defined in Table 1) forms a header group, and the relevant fields of a header group form a single **FlowID**. The tuple of all FlowIDs in a packet is called a **Unified FlowID (UFID)**, which is the output of the parser.

### Hardware Offloads and Performance

VFP has long utilized standard stateless offloads (e.g., VXLAN/NVGRE encapsulation, QoS bandwidth caps, and port reservations) to achieve line rate with SDN policy. To enable full SR-IOV offload and host bypass, we developed logic to directly offload our unified flows. These exact-match flows represent each connection on the system and can be implemented in hardware via a large hash table, typically in inexpensive DRAM. The first packet of a new flow goes through software classification to determine the UF, which is then offloaded.

For example, a packet passing through the Ananta NAT layer and the VL2 VNET encap layer may end up with the composite Encap+NAT transposition shown in Table 3. We have used this mechanism to enable SR-IOV in our datacenters with VFP policy offload on custom FPGA-based SmartNICs deployed on all new Azure servers. This has resulted in bidirectional 32Gbps+ VNICs with near-zero host CPU usage and <25μs end-to-end TCP latencies inside a VNET.

### Unified Flow Tables and Caching

The intuition behind our flow compiler is that the action for a UFID is relatively stable over the lifetime of a flow, allowing us to cache the UFID with the resulting HT from the engine. The resulting flow table, where the compiler caches UFs, is called the **Unified Flow Table (UFT)**.

With the UFT, we segment our datapath into a fastpath and a slowpath. On the first packet of a TCP flow, we take the slowpath, running the transposition engine and matching at each layer against rules. For subsequent packets, VFP takes the fastpath, matching a unified flow via UFID and applying a transposition directly. This operation is independent of the layers or rules in VFP.

### Operationalizing VFP

As a production cloud service, VFP’s design must account for serviceability, monitoring, and diagnostics. During updates, we pause the datapath, detach VFP from the stack, uninstall VFP (which acts as a loadable kernel driver), install a new VFP, attach it to the stack, and restart the datapath. This operation appears as a brief connectivity blip to VMs, while the NIC remains up. To keep stateful flows alive across updates, we support serialization and deserialization for all policy and state in VFP on a port. VFP also supports live migration of VMs, serializing and deserializing port state during the blackout time of the migration.

VFP implements hundreds of performance counters and flow statistics, on a per-port, per-layer, and per-rule basis, as well as extensive flow statistics. This information is continuously uploaded to a central monitoring service, providing dashboards for monitoring flow utilization, drops, and connection health.

### Lessons Learned

Over six years of developing and supporting VFP, we have learned several valuable lessons:
- **L4 flow caching is sufficient.** We did not find a use for multitiered flow caching such as OVS megaflows. Being entirely in the kernel allowed us to have a faster slowpath, and our use of a stateful NAT created an action for every L4 flow, reducing the usefulness of ternary flow caching.
- **Design for statefulness from day 1.** Support for stateful connections as a first-class primitive in a MAT is fundamental and must be considered in every aspect of a MAT design.
- **Layering is critical.** Clear layering semantics are essential for controllers to reverse their policy correctly with respect to other controllers.
- **GOTO considered harmful.** Controllers will implement policy in the simplest way needed to solve a problem, but this may not be compatible with future controllers adding policy. Enforcing layering is key to making multi-controller designs work.
- **IaaS cannot handle downtime.** Customer IaaS workloads care deeply about uptime for each VM, not just their service as a whole. We designed all updates to minimize downtime and provide guarantees for low blackout times.
- **Design for serviceability.** Serialization is a design point that pervades all our logic, allowing regular updates without impacting VMs.
- **Decouple the wire protocol from the dataplane.** Separating VFP’s API from any wire protocol was a critical choice for VFP’s success.
- **Everything is an action.** Modeling VL2-style encap/decap as actions rather than tunnel interfaces enabled a single table lookup for all packets, facilitating single-table hardware offload.
- **Design for end-to-end monitoring.** In-band monitoring with packet injectors and auto-responders implemented as VFP rule actions helps trace the E2E path from the VM-host boundary.
- **Commercial NIC hardware isn’t ideal for SDN.** Despite interest from NIC vendors, we have seen no success cases of NIC ASIC vendors supporting our policy as a direct offload. Instead, we used custom FPGA-based hardware, which proved to be lower latency and more efficient.

### Conclusions and Future Work

We introduced the Virtual Filtering Platform (VFP), our cloud-scale vswitch for host SDN policy in Microsoft Azure. We discussed how our design achieved our dual goals of programmability and scalability, and we provided performance results, data, and lessons from real use. Future areas of investigation include new hardware models of SDN and extending VFP’s offload language.

### References

1. D. Firestone, “VFP: A Virtual Switch Platform for Host SDN in the Public Cloud,” in Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI ’17): https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf.
2. N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, J. Turner, “OpenFlow: Enabling Innovation in Campus Networks,” ACM SIGCOMM Computer Communication Review, vol. 38, no. 2 (April 2008): http://ccr.sigcomm.org/online/files/p69-v38n2n-mckeown.pdf.
3. B. Pfaff, J. Pettit, T. Koponen, E. Jackson, A. Zhou, J. Rajahalme, J. Gross, A. Wang, J. Stringer, P. Shelar, K. Amidon, M. Casado, “The Design and Implementation of Open vSwitch,” in Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI ’15): https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-pfaff.pdf.
4. A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, S. Sengupta, “VL2: A Scalable and Flexible Data Center Network,” in Proceedings of the ACM Conference on Data Communication (SIGCOMM ’09), pp. 51–62: https://www.researchgate.net/publication/234805283_VL2_A_Scalable_and_Flexible_Data_Center_Network.
5. P. Patel, D. Bansal, L. Yuan, A. Murthy, A. Greenberg, D. A. Maltz, R. Kern, H. Kumar, M. Zikos, H. Wu, C. Kim, N. Karri, “Ananta: Cloud Scale Load Balancing,” in Proceedings of the ACM Conference on Data Communication (SIGCOMM ’13), pp. 207–218: http://conferences.sigcomm.org/sigcomm/2013/papers/sigcomm/p207.pdf.
6. C. Guo, L. Yuan, D. Xiang, Y. Dang, R. Huang, D. Maltz, Z. Liu, V. Wang, B. Pang, H. Chen, Z. Lin, V. Kurien, “Pingmesh: A Large-Scale System for Data Center Network Latency Measurement and Analysis,” in Proceedings of the ACM Conference on Data Communication (SIGCOMM ’15): https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/pingmesh_sigcomm2015.pdf.