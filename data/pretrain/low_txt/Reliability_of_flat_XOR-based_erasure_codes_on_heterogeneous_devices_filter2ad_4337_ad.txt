### Optimized Text

Both the BF-RP and SA-RP algorithms can operate over a significantly smaller state space, which may facilitate better RME (Reliability MTTDL Estimate) characterization. The specific device models used in the evaluation are based on the distributions that Elerath and Pecht used [6]. We believe that the models of Elerath and Pecht are as good as any currently available. Recently published analyses of failure data [21, 16, 2] will hopefully result in even better failure models. We expect that such models will change the MTTDL values but not the placement that is most reliable.

The RME is based on the assumption that failures are independent. If significant correlation is found in failure models, the RME equation may need to be adjusted. When developing the RME metric, we assumed that sector failures would have a secondary effect on placement decisions and could thus be excluded from the RME metric. Our initial results for the RME metric in systems with sector failures show that for codes with a Hamming distance greater than 2, the RME still correctly orders placements by reliability. For codes with a Hamming distance of 2, data loss events are dominated by single-disk, single-sector failures. In such cases, if every symbol occurs in at least one minimal erasure of size two (e.g., like (6,2)-FLAT), then placement has little effect on overall reliability. However, for (5,3)-FLAT, only symbols 84 and 87 occur in a minimal erasure of size 2, and so placements based on the RME maximize reliability.

### Conclusions

We introduced the novel redundancy placement problem, where a mapping, called a placement, of the symbols in a flat XOR-based code onto a set of heterogeneous storage devices with known failure and recovery rates must be found to maximize reliability. To solve this problem, we developed the Reliability MTTDL Estimate (RME), a simple model based on estimated device unavailability, and the Minimal Erasures List (MEL), a concise characterization of the fault tolerance of an XOR-based code. We also developed two redundancy placement algorithms: the BF-RP algorithm, which is based on brute force computation and is suitable for small redundancy placement problems, and the SA-RP algorithm, which is based on simulated annealing and is suitable for larger problems.

Simulation results based on the High-Fidelity Reliability (HFR) Simulator provide extensive empirical evidence that the RME correctly orders different placements for a given code by MTTDL. Additional simulation results suggest that the placements found by the SA-RP algorithm are significantly more reliable than the median placement. The results of the BF-RP algorithm led us to realize the existence of isomorphic placements, sets of placements that have the same MTTDL.

### References

[1] G. A. Alvarez, W. A. Burkhard, and F. Cristiano. Tolerating multiple failures in RAID architectures with optimal storage and uniform declustering. In ISCA-1997: 24th Annual International Symposium on Computer Architecture, pages 62-72, Denver, CO, June 1997. ACM.

[2] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and I. Schindler. An analysis of latent sector errors in disk drives. SIGMETRICS Perform. Eval. Rev., 35(1):289-300, 2007.

[3] M. Blaum, J. Brady, J. Bruck, and J. Menon. EVENODD: An efficient scheme for tolerating double disk failures in RAID architectures. IEEE Trans. Comput., 44(2):192-202, 1995.

[4] P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, and S. Sankar. Row-diagonal parity for double disk failure correction. In FAST-2004: 3rd USENIX Conference on File and Storage Technologies, pages 1-14. USENIX Association, March 2004.

[5] J. R. Douceur and R. P. Wattenhofer. Optimizing file availability in a secure serverless distributed file system. In Symposium on Reliable Distributed Systems. IEEE, 2001.

[6] J. F. Elerath and M. Pecht. Enhanced reliability modeling of RAID storage systems. In DSN-2007, pages 175-184. IEEE, June 2007.

[7] S. Gaonkar, K. Keeton, A. Merchant, and W. H. Sanders. Designing dependable storage solutions for shared application environments. In DSN-2006: The International Conference on Dependable Systems and Networks, pages 371-382. IEEE, June 2006.

[8] K. M. Greenan and J. J. Wylie. High-fidelity reliability simulation of erasure-coded storage. Technical Report (to appear), Hewlett-Packard Labs.

[9] J. L. Hafner. WEAVER Codes: Highly fault-tolerant erasure codes for storage systems. In FAST-2005: 4th USENIX Conference on File and Storage Technologies, pages 212-224. USENIX Association, December 2005.

[10] J. L. Hafner. Hover erasure codes for disk arrays. In DSN-2006: The International Conference on Dependable Systems and Networks, pages 217-226. IEEE, June 2006.

[11] J. L. Hafner, V. Deenadhayalan, T. Kanungo, and K. Rao. Performance metrics for erasure codes in storage systems. Technical Report RJ-10321, IBM, August 2004.

[12] J. L. Hafner, V. Deenadhayalan, K. Rao, and J. A. Tomlin. Matrix methods for lost data reconstruction in erasure codes. In FAST-2005: 4th USENIX Conference on File and Storage Technologies, pages 183-196. USENIX Association, December 2005.

[13] J. L. Hafner and K. Rao. Notes on reliability models for non-MDS erasure codes. Technical Report RJ-10391, IBM, October 2006.

[14] S. Kirkpatrick, C. Gelatt Jr., and M. Vecchio. Optimization by simulated annealing. Science, 220(4598):671-680, May 1983.

[15] Q. Lian, W. Chen, and Z. Zhang. On the impact of replica placement to the reliability of distributed brick storage systems. In ICDCS 2005: Proceedings of the 25th International Conference on Distributed Computing Systems, pages 187-196. IEEE, 2005.

[16] E. Pinheiro, W.-D. Weber, and L. A. Barroso. Failure trends in a large disk drive population. In FAST-2007: 5th USENIX Conference on File and Storage Technologies. USENIX Association, 2007.

[17] J. S. Plank. Erasure codes for storage applications. Tutorial slides, presented at FAST-2005: 4th Usenix Conference on File and Storage Technologies, December 2005.

[18] J. S. Plank, A. L. Buchsbaum, R. L. Collins, and M. G. Thomason. Small parity-check erasure codes - exploration and observations. In DSN-2005: The International Conference on Dependable Systems and Networks, pages 326-335. IEEE, July 2005.

[19] J. S. Plank and M. G. Thomason. A practical analysis of low-density parity-check erasure codes for wide-area storage applications. In DSN-2004: The International Conference on Dependable Systems and Networks, pages 115-124. IEEE, June 2004.

[20] K. Rao, J. L. Hafner, and R. A. Golding. Reliability for networked storage nodes. In DSN-2006: The International Conference on Dependable Systems and Networks, pages 237-248. IEEE, June 2006.

[21] B. Schroeder and G. A. Gibson. Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you? In FAST-2007: 5th USENIX Conference on File and Storage Technologies, pages 1-16. USENIX Association, 2007.

[22] A. Thomasian and M. Blaum. Mirrored disk organization reliability analysis. IEEE Trans. Comput., 55(12):1640-1644, 2006.

[23] J. J. Wylie and R. Swaminathan. Determining fault tolerance of XOR-based erasure codes efficiently. In DSN-2007, pages 206-215. IEEE, June 2007.

[24] H. Yu, P. B. Gibbons, and S. Nath. Availability of multi-object operations. In NSDI-2006: Proceedings of the Symposium on Networked Systems Design and Implementation, May 2006.

---

**Note:** The tables and figures were not included in the optimized text. If you need them, please provide the original content, and I can help integrate and format them appropriately.