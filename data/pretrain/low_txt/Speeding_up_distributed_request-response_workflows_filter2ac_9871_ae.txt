### Figure 12: Latency Reductions Achieved by Kwiken and Variants When Trading Off Completeness for Latency

**Kwiken's Budget Allocation Challenge:**
It is challenging to determine the optimal allocation of the overall capacity for reissues at each stage in Kwiken. 

**Comparison with Straw Man Policies:**
We compared Kwiken with several straw man policies. One such policy assigns the same reissue fraction \( r_i = r \) to each stage. However, this approach has clear limitations. For instance, if a single stage has a high cost \( c_i \), it will consume most of the budget. If that stage has low variance, the resulting end-to-end improvement will be minimal. Other policies, such as allocating an equal budget to each stage, exhibit similar drawbacks.

**Brute-Force Approaches:**
Given the lack of an optimal algorithm (recall that even equation (3) has a non-convex objective), we compared Kwiken with two brute-force approaches. For a subset of nine smaller workflows and budgets ranging from 1% to 10%, we selected the best timeouts out of 10,000 random budget allocations. Compared to training Kwiken on the same data, this method was approximately four orders of magnitude slower. Consequently, we did not attempt it on larger workflows. On average, Kwiken's results were better by 2%. In 95% of the cases (i.e., {workflow, budget} pairs), Kwiken's latency reduction was at least 94% of that achieved by this method. The second approach uses gradient descent to directly minimize the 99th percentile of the end-to-end latency using a simulator (i.e., avoiding the sum of variances approximation). This method was equally slow and performed no better. Therefore, we conclude that Kwiken's method for apportioning the budget across stages is not only useful but also nearly as effective as an ideal (but impractical) method.

**Weighted Forms of Kwiken:**
We also evaluated two weighted forms of Kwiken that more directly consider the structure of the workflow (§3): weighting each stage by its average latency and by its likelihood to occur on a critical path. While both performed well, they were not significantly better than the unweighted form for the examined workflows.

### 5.3 Trading Off Completeness for Latency

Next, we evaluate the improvements in latency when using Kwiken to return partial answers. Figure 12a plots the improvement due to trading off completeness for latency for different values of utility loss. Our target is to be complete enough that the best result is returned for over 99.9% of the queries, i.e., a utility loss of 0.1%. With this budget, Kwiken improves the 99th (95th) percentile by around 50% (25%). The plotted values are averages over the web, image, and video stages. These stages issue many requests in parallel and aggregate the responses (see Figure 1).

Figure 12b compares Kwiken's performance with a few benchmarks for a utility loss budget of 0.1%:
- **Wait-for-fraction**: Terminates a query when a fraction \( b \) of its responders return.
- **Fixed-timeout**: Terminates queries at a cutoff time \( T_{cutoff} \).
- **Time-then-fraction**: Terminates queries when both a constant time \( T' \) has elapsed and at least a fraction \( \alpha \) of responders have finished.

Kwiken performs significantly better. Wait-for-fraction spends a significant part of the budget on queries that get the required fraction relatively fast, leaving slower queries on the tail with insufficient improvement. Fixed-timeout is better because it allows slower queries to terminate when many more of their responders are pending, but it does not help with quick queries—there is no change below the 90th percentile. Even among the slower queries, it does not distinguish between queries with many pending responders (higher probability of losing utility) and those with only a few pending responders. Time-then-fraction is better for this reason; it never terminates queries unless a minimal fraction of responders are done. However, Kwiken does even better by waiting for extra time after a fraction of responders are done, providing gains for both quicker and slower queries. Additionally, it outperforms time-then-fraction on the slowest queries by stopping at a fixed time.

### 5.4 Catch-Up Mechanisms

Here, we estimate the gains from the three types of catch-up mechanisms discussed earlier. Figure 13a shows the gains from using multi-threaded execution and network prioritization on the web-search workflow (Figure 1), relative to the baseline where no latency reduction techniques are used. We note that the speedup due to multi-threading is not linear with the number of threads due to synchronization costs, and using 3 threads yields roughly a 2X speedup. Speeding up both stages offers much more gain than speeding up just one of the stages; the 99th percentile latency improves by up to 44% with only small increases in additional load—about 6.3% more threads needed and about 1.5% of the network load moves into higher priority queues.

Next, we evaluate the usefulness of using global reissues on workflows. Using a total reissue budget of 3%, Figure 13b plots the marginal improvements (relative to using the entire budget for local reissues) from assigning \( \frac{1}{30} \) (x-axis) vs. \( \frac{1}{6} \) of the budget to global reissues (y-axis) for the 45 workflows we analyze. The average reduction in 99th percentile latency is about 3% in both cases, though assigning \( \frac{1}{6} \) of the budget leads to higher improvements in some cases. Overall, 37 out of the 45 workflows see gains in latency. We note that this experiment only shows one way to assign global reissues; better allocation techniques may yield larger gains.

### 5.5 Putting It All Together

To illustrate the advantage of using multiple latency reduction techniques in the Kwiken framework together, we analyze in detail its application to a major workflow in Bing that has 150 stages. A simplified version of the workflow with only the ten highest-variance stages is shown in Figure 14a. In three of the stages, we use utility loss to improve latency and use reissues on all stages.

We compare Kwiken with other alternatives in Figure 14b. On the left, we fix the utility loss budget at 0.1% and vary reissues. On the right, we vary the utility loss and fix reissues at 3%. We observe complementary advantages from using reissues and utility loss together. In the left graph, using Kwiken with reissues only at 10% performs worse than using both reissues at 1% and 0.1% utility loss. Using both together is about 20% better than using just utility loss. The graph on the right shows that, with a reissue budget of 3%, increasing utility loss has very little improvement beyond 0.1%. We observe that the larger the reissue budget, the larger the amount of utility loss that can be gainfully used (not shown). Further, how we use the budget matters; consider "K for reissues; wait-for-fraction" on the left. For the same amount of utility loss, Kwiken achieves much greater latency reduction.

**What Does Kwiken Do?**
Figure 14c shows, for each of the ten stages, the latency variance (as a fraction of all variance in the workflow) and the amount of allocated budget (in log scale). We see that the budget needs to be apportioned to many different stages, not just based on their variance, but also on the variance-response curves and the per-stage cost of request reissue. Without Kwiken, it would be hard to reach the correct assignment.

### 5.6 Robustness of Parameter Choices

Recall that Kwiken chooses its parameters based on traces from prior execution. A concern here is that due to temporal variations in our system, the chosen parameters might not yield the expected gains or may violate resource budgets.

To understand the stability of the parameter choices over time, we compare the improvements for the 99th percentile latency and the budget values obtained for the "training" dataset to those obtained for the "test" datasets. The test datasets were collected from the same production cluster on three different days within the same week. Figure 15 shows that the latency improvements on the test datasets are within a few percentage points of those on the training datasets. The utility loss on the test dataset is slightly larger but predictably so, which allows us to explicitly account for it by training with a tighter budget. In conclusion, Kwiken's parameter choices are stable. Reallocating the budget is fast and can be done periodically whenever the parameters change.

### 6. Related Work

Improving the latency of datacenter networks has attracted much recent interest from both academia and industry. Most work in this area [2, 3, 17, 25, 29] focuses on developing transport protocols to ensure network flows meet specified deadlines. Approaches like Chronos [18] modify end-hosts to reduce operating system overheads. Kwiken is complementary to these mechanisms, which reduce the latency of individual stages, because it focuses on the end-to-end latency of distributed applications.

Some recent work [4, 13, 28] reduces job latency in (MapReduce-like) batch processing frameworks [11] by adaptively reissuing tasks or changing resource allocations. Other prior work [19] explores how to (statically) schedule jobs modeled as a DAG of tasks to minimize completion time. Neither of these apply directly to the context of Kwiken, which targets large interactive workflows that finish within a few hundred milliseconds and may involve over thousands of servers. Static scheduling is relatively easy here, and there is too little time to monitor detailed aspects at runtime (e.g., task progress) as in the batch case.

Some recent work concludes that latency variability in cloud environments arises from contention with co-located services [1] and provides workload placement strategies to avoid interference [27].

Some of the techniques used by Kwiken have been explored earlier. Reissuing requests has been used in many distributed systems [10, 12] and networking [5, 14, 23] scenarios. Kwiken's contribution lies in strategically apportioning reissues across the stages of a workflow to reduce end-to-end latency, whereas earlier approaches consider each stage independently. Partial execution has been used in AI [30] and programming languages [6, 16]. The proposed policies, however, do not translate to the distributed services domain. Closer to us is Zeta [15], which devises an application-specific scheduler that runs beside the query to estimate expected utility and to choose when to terminate. In contrast, Kwiken relies only on opaque indicators of utility, making the timeout policies more generally applicable.

### 7. Conclusion

In this paper, we propose and evaluate Kwiken, a framework for optimizing end-to-end latency in computational workflows. Kwiken takes a holistic approach by considering end-to-end costs and benefits of applying various latency reduction techniques and decomposes the complex optimization problem into a simpler optimization over individual stages. We also propose novel policies that trade off utility loss and latency reduction. Using detailed simulations based on traces from our production systems, we show that using Kwiken, the 99th percentile of latency improves by over 75% when just 0.1% of the responses are allowed to have partial results and 3% extra resources are used for reissues.

### Acknowledgements

We thank Junhua Wang, Navin Joy, Eric Lo, and Fang Liu from Bing for their invaluable help. We also thank Yuval Peres, our shepherd Georgios Smaragdakis, and the SIGCOMM reviewers for feedback on earlier drafts of the paper.

### References

[1] Amazon Elastic Compute Cloud (Amazon EC2). http://aws.amazon.com/ec2/.
[2] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center TCP (DCTCP). In SIGCOMM, 2010.
[3] M. Alizadeh, S. Yang, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. Deconstructing Datacenter Packet Transport. In Hotnets, 2012.
[4] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica, Y. Lu, B. Saha, and E. Harris. Reining in the Outliers in MapReduce Clusters Using Mantri. In OSDI, 2010.
[5] D. G. Andersen, H. Balakrishnan, M. F. Kaashoek, and R. N. Rao. Improving web availability for clients with MONET. In NSDI, 2005.
[6] W. Baek and T. M. Chilimbi. Green: A Framework for Supporting Energy-Conscious Programming using Controlled Approximation. In PLDI, 2010.
[7] S. Boucheron, G. Lugosi, and O. Bousquet. Concentration inequalities. Advanced Lectures on Machine Learning, 2004.
[8] J. Brutlag. Speed matters for Google web search. http://googleresearch.blogspot.com/2009/06/speed-matters.html, 2009.
[9] J. R. Dabrowski and E. V. Munson. Is 100 Milliseconds Too Fast? In CHI, 2001.
[10] J. Dean and L. A. Barroso. The tail at scale. Commun. ACM, 56(2):74–80, Feb. 2013.
[11] J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. In OSDI, 2004.
[12] G. Decandia et al. Dynamo : Amazon’s Highly Available Key-value Store. In SOSP, 2007.
[13] A. D. Ferguson, P. Bodik, S. Kandula, E. Boutin, and R. Fonseca. Jockey: Guaranteed Job Latency in Data Parallel Clusters. In EuroSys, 2012.
[14] D. Han, A. Anand, A. Akella, and S. Seshan. RPT: Re-architecting Loss Protection for Content-Aware Networks. In NSDI, 2012.
[15] Y. He, S. Elnikety, J. Larus, and C. Yan. Zeta: Scheduling Interactive Services with Partial Execution. In SOCC, 2012.
[16] H. Hoﬀmann, S. Sidiroglou, M. Carbin, S. Misailovic, A. Agarwal, and M. Rinard. Dynamic Knobs for Responsive Power-Aware Computing. In ASPLOS, 2011.
[17] C. Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly with Preemptive Scheduling. In SIGCOMM, 2012.
[18] R. Kapoor, G. Porter, M. Tewari, G. M. Voelker, and A. Vahdat. Chronos: Predictable Low Latency for Data Center Applications. In SOCC, 2012.
[19] Y. Kwok and I. Ahmad. Static Scheduling Algorithms for Allocating Directed Task Graphs to Multiprocessors. ACM Computing Surveys (CSUR), 1999.
[20] R. Nishtala et al. Scaling Memcache at Facebook. In NSDI, 2013.
[21] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan, I. Obermiller, and S. Shayandeh. AppInsight: Mobile App Performance Monitoring in the Wild. In OSDI, 2012.
[22] E. Schurman and J. Brutlag. The User and Business Impact of Server Delays, Additional Bytes, and Http Chunking in Web Search. http://velocityconf.com/velocity2009/public/schedule/detail/8523, 2009.
[23] A. Vulimiri, O. Michel, P. B. Godfrey, and S. Shenker. More is Less: Reducing Latency via Redundancy. In HotNets, 2012.
[24] X. S. Wang et al. Demystifying Page Load Performance with WProf. In NSDI, 2013.
[25] C. Wilson et al. Better Never than Late: Meeting Deadlines in Datacenter Networks. In SIGCOMM, 2011.
[26] H. Wu, Z. Feng, C. Guo, and Y. Zhang. ICTCP: Incast Congestion Control for TCP in Data Center Networks. In CONEXT, 2010.
[27] Y. Xu, Z. Musgrave, B. Noble, and M. Bailey. Bobtail: Avoiding Long Tails in the Cloud. In NSDI, 2013.
[28] M. Zaharia, A. Konwinski, A. Joseph, R. Katz, and I. Stoica. Improving MapReduce Performance in Heterogeneous Environments. In OSDI, 2008.
[29] D. Zats et al. DeTail: Reducing the Flow Completion Time Tail in Datacenter Networks. In SIGCOMM, 2012.
[30] S. Zilberstein. Using Anytime Algorithms in Intelligent Systems. AI Magazine, 17(3):73–83, 1996.

### Appendix

**Proof of (2):**
For each random variable \( L_s \), we introduce a new independent random variable \( L'_s \) which has the same distribution as \( L_s \). Let \( L = (L_1, \ldots, L_N) \) and \( L(s) = (L_1, \ldots, L_{s-1}, L'_s, L_{s+1}, \ldots, L_N) \). Then, using the Efron-Stein inequality [7], we have:
\[ \text{Var}(L_w(L)) \leq \frac{1}{2} \sum_s \mathbb{E}[(L_w(L) - L_w(L(s)))^2] \leq \frac{1}{2} \sum_s \mathbb{E}[(L_s - L'_s)^2] = \sum_s \text{Var}(L_s). \]

This completes the proof.