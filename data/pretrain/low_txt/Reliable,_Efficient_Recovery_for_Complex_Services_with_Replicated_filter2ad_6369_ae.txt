### Optimized Text

In the case of the largest group sizes, the leader needs to receive only a few kilobytes of data, aggregated from all the restarting nodes. In our subsequent experiments, we evaluated the costs associated with restarting a system that includes one or more significantly out-of-date replicas (i.e., nodes whose logs are missing many committed updates). To conduct this experiment, we set up a Derecho service organized into shards, each containing three nodes. We allowed two out of the three replicas in each shard to continue committing updates for some time after one replica had crashed. Subsequently, we crashed the remaining replicas and restarted all the nodes simultaneously. Each update in this service contained 1KB of data.

Figure 4 provides a detailed breakdown of the time spent in the four major phases of the restart algorithm:
1. Awaiting quorum.
2. Truncating logs to complete epoch termination.
3. Transferring state to out-of-date nodes.
4. Waiting for the leader to commit a restart view.

Additionally, Figure 4 shows a fifth phase: the time spent in the setup process of the Derecho library before the first update can be sent. This setup includes operations such as pre-allocating buffers for RDMA multicasts. For comparison, we also measured the time breakdown for a fresh start of the same service, which consists of two phases: awaiting quorum (i.e., waiting for all processes to launch) and setting up the Derecho library.

This experiment clearly demonstrates that our restart process is highly efficient compared to the normal costs of starting a distributed service. Even when one replica in each shard is missing 10,000 committed updates, the state transfer accounts for at most 120 ms, a small fraction of the overall time. The results also highlight the benefits of allowing each shard to complete state transfer in parallel: the 3-shard service spent no more time on state transfer than the 2-shard service, despite the additional 10,000 updates that needed to be sent to an out-of-date node.

We also measured the amount of data received by each out-of-date replica during the state transfer process, varying both the size of each update and the number of missing updates. The results, shown in Figure 5, indicate that the amount of data transferred to each out-of-date replica increases linearly with the size of an update and the number of missing updates. Specifically, the data transferred is almost exactly equal to the number of missing updates multiplied by the size of each update, as the node did not need to download and merge logs from multiple other replicas. It is important to note that this data is sent in parallel for each shard, so there is no difference in the amount of data any one node must send as the number of shards increases.

Finally, we measured the time required to restart a service with out-of-date replicas as the size of each update scales up, as shown in Figure 6. We found that for updates below 1 MB, neither the size of the update nor the number of missing updates on the out-of-date replicas significantly affected the restart time. However, for update sizes of 1 MB and larger, the increasing amount of data that needed to be transferred to the out-of-date replicas slowed down the restart process as expected.

### Related Work

The algorithms implemented in Derecho combine ideas first explored in the Isis Toolkit [13, 6] with the Vertical Paxos model [14]. Other modern Paxos protocols include NOPaxos [4] and APUS [5]. Recent systems like Spinnaker [15] and Gaios [16] offer a more durable form of Paxos and include mechanisms for restarting failed nodes using their persistent logs. However, these papers generally do not address the scenario where every replica must be restarted simultaneously. "Paxos Made Live" [17] explores practical challenges, including durability, in larger SMR systems, which aligns with our work's motivation.

Bessani et al. [18] examined the efficiency of adding durability to SMR, focusing on minimizing state transfer during replica recovery. They provided a solution for recovering a non-sharded service in a Byzantine setting and showed how to reduce the runtime overhead of logging and checkpointing. Their work did not consider services with complex substructures, a primary consideration in our study.

Corfu [9] is a recent implementation of SMR that uses a different approach from classic Paxos, distributing the command log across shards of storage-only nodes. Clients use Paxos to reserve a slot and then replicate data using a form of chain replication [19]. vCorfu [8] extends this by offering virtual sublogs on a per-application basis. However, if multiple subsystems use Corfu separately, recovery of the Corfu log might not restore the application to a consistent state. As mentioned in Sections II-A and IV-D, our protocol could be adapted to vCorfu to ensure that a quorum of replicas from each sublog of the last known layout is contacted before the system is restarted. Other replicated cloud services, such as Hadoop [20], Zookeeper [21], and Spark [22], employ an alternative approach to durability by ensuring that any lost state due to an unexpected failure can always be recomputed from its last checkpoint, but this is not feasible in our setting.

Our work is inspired by a long history of distributed checkpointing and rollback-recovery protocols, many of which are summarized in [23]. We update these principles to the modern setting of replicated services and SMR. Rather than relying on an explicitly coordinated global checkpoint, as in [24] and [25], or attempting to record a dependency graph between locally-recorded checkpoints, as in [26], our system incorporates the dependency information already recorded in SMR updates to derive a globally consistent system snapshot from local logs.

Recovery of the final state of a single process group was first addressed in Skeen’s article “Determining the Last Process to Fail” [27]. Our scenario, with potentially overlapping subgroups, is more complex and introduces an issue of joint consistency that was not explored in that work.

### Conclusion

Modern datacenter services are often complex and may use SMR mechanisms for self-managed configuration, membership management, and sharded data replication. In these services, application data is spread over large numbers of logs, and recovery requires reconstructing a valid and consistent state that preserves all committed updates. We have demonstrated how this problem can be solved even if further crashes occur during recovery, implemented our solution within Derecho, and evaluated the mechanism to show that it is highly efficient.

### Acknowledgments

This work was supported, in part, by a grant from AFRL Wright-Patterson.

### References

[1] J. Gray, P. Helland, P. O’Neil, and D. Shasha, “The dangers of replication and a solution,” in Proc. 1996 ACM SIGMOD Int. Conf. Management of Data. Montreal, Quebec, Canada: ACM, 1996, pp. 173–182.

[2] K. Birman, B. Hariharan, and C. De Sa, “Cloud-hosted intelligence for real-time IoT applications,” SIGOPS Oper. Syst. Rev., vol. 53, no. 1, pp. 7–13, Jul. 2019.

[3] S. Jha, J. Behrens, T. Gkountouvas, M. Milano, W. Song, E. Tremel, R. V. Renesse, S. Zink, and K. P. Birman, “Derecho: Fast state machine replication for cloud services,” ACM Trans. Comput. Syst., vol. 36, no. 2, pp. 4:1–4:49, Apr. 2019.

[4] J. Li, E. Michael, N. K. Sharma, A. Szekeres, and D. R. K. Ports, “Just say NO to Paxos overhead: Replacing consensus with network ordering,” in Proc. 12th USENIX Symp. Operating Systems Design and Implementation. Savannah, GA, USA: USENIX Association, 2016.

[5] C. Wang, J. Jiang, X. Chen, N. Yi, and H. Cui, “APUS: Fast and scalable Paxos on RDMA,” in Proc. 8th ACM Symp. Cloud Computing. Santa Clara, CA, USA: ACM, Sep. 2017.

[6] K. P. Birman and T. A. Joseph, “Reliable communication in the presence of failures,” ACM Trans. Comput. Syst., vol. 5, no. 1, pp. 47–76, Jan. 1987.

[7] L. Lamport, “Paxos made simple,” ACM Sigact News, vol. 32, no. 4, pp. 18–25, 2001.

[8] M. Wei, A. Tai, C. J. Rossbach, I. Abraham, M. Munshed, M. Dhawan, J. Stabile, U. Wieder, S. Fritchie, S. Swanson, M. J. Freedman, and D. Malkhi, “vCorfu: A cloud-scale object store on a shared log,” in Proc. 14th USENIX Conf. Networked Systems Design and Implementation. Boston, MA, USA: USENIX Association, 2017, pp. 35–49.

[9] M. Balakrishnan, D. Malkhi, J. D. Davis, V. Prabhakaran, M. Wei, and T. Wobber, “CORFU: A distributed shared log,” ACM Trans. Comput. Syst., vol. 31, no. 4, pp. 10:1–10:24, Dec. 2013.

[10] S. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long, and C. Maltzahn, “Ceph: A scalable, high-performance distributed file system,” in Proc. 7th Symp. Operating Systems Design and Implementation. Seattle, WA, USA: USENIX Association, 2006, pp. 307–320.

[11] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory, Algorithms, and Applications. Upper Saddle River, NJ, USA: Prentice-Hall, Inc., 1993.

[12] K. M. Chandy and L. Lamport, “Distributed snapshots: Determining global states of distributed systems,” ACM Trans. Comput. Syst., vol. 3, no. 1, pp. 63–75, Feb. 1985.

[13] K. P. Birman, “Replication and fault-tolerance in the ISIS system,” in Proc. 10th ACM Symp. Operating Systems Principles. Orcas Island, WA, USA: ACM, 1985, pp. 79–86.

[14] L. Lamport, D. Malkhi, and L. Zhou, “Vertical Paxos and primary-backup replication,” in Proc. 28th ACM Symp. Principles of Distributed Computing. Calgary, Canada: ACM, Aug. 2009, pp. 312–313.

[15] J. Rao, E. J. Shekita, and S. Tata, “Using Paxos to build a scalable, consistent, and highly available datastore,” Proc. VLDB Endow., vol. 4, no. 4, pp. 243–254, Jan. 2011.

[16] W. J. Bolosky, D. Bradshaw, R. B. Haagens, N. P. Kusters, and P. Li, “Paxos replicated state machines as the basis of a high-performance data store,” in Proc. 8th USENIX Conf. Networked Systems Design and Implementation. Boston, MA, USA: USENIX Association, 2011, pp. 141–154.

[17] T. D. Chandra, R. Griesemer, and J. Redstone, “Paxos made live: An engineering perspective,” in Proc. 26th ACM Symp. Principles of Distributed Computing. Portland, OR, USA: ACM, 2007, pp. 398–407.

[18] A. Bessani, M. Santos, J. Felix, N. Neves, and M. Correia, “On the efficiency of durable state machine replication,” in Proc. 2013 USENIX Annual Technical Conference. San Jose, CA, USA: USENIX Association, Jun. 2013, pp. 169–180.

[19] R. van Renesse and F. B. Schneider, “Chain replication for supporting high throughput and availability,” in Proc. 6th Symp. Operating Systems Design and Implementation. San Francisco, CA, USA: USENIX Association, Dec. 2004, pp. 91–104.

[20] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop distributed file system,” in Proc. 26th IEEE Symp. Mass Storage Systems and Technologies. IEEE Computer Society, 2010.

[21] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed, “Zookeeper: Wait-free coordination for Internet-scale systems,” in Proc. 2010 USENIX Annual Technical Conference. Boston, MA, USA: USENIX Association, 2010.

[22] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica, “Spark: Cluster computing with working sets,” in Proc. 2nd USENIX Conf. Hot Topics in Cloud Computing. Boston, MA, USA: USENIX Association, Jun. 2010.

[23] E. N. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. Johnson, “A survey of rollback-recovery protocols in message-passing systems,” ACM Comput. Surv., vol. 34, no. 3, pp. 375–408, Sep. 2002.

[24] R. Koo and S. Toueg, “Checkpointing and rollback-recovery for distributed systems,” IEEE Trans. Softw. Eng., vol. SE-13, no. 1, pp. 23–31, Jan. 1987.

[25] E. N. Elnozahy and W. Zwaenepoel, “Manetho: Transparent rollback-recovery with low overhead, limited rollback, and fast output commit,” IEEE Trans. Comput., vol. 41, no. 5, pp. 526–531, May 1992.

[26] B. Bhargava and S.-R. Lian, “Independent checkpointing and concurrent rollback for recovery in distributed systems – an optimistic approach,” in Proc. 7th Symp. Reliable Distributed Systems, Oct. 1988, pp. 3–12.

[27] D. Skeen, “Determining the last process to fail,” ACM Trans. Comput. Syst., vol. 3, no. 1, pp. 15–30, Feb. 1985.