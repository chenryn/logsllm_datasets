# Proportion Covered by Additional Syndromes

## Description of Figures
- **Figure 10**: Inverse cumulative density function of the proportion of the file system covered by additional syndromes, calculated over the entire lifetime of the S2DDC.
  - (e) Additional syndromes allocated by our algorithm using a mean-shift clustered Markov model with \( h = 5 \) (8 clusters), and a 0.95 worst-case quartile prediction.
  - (f) Additional syndromes allocated by our algorithm using a mean-shift clustered Markov model with \( h = 5 \) (8 clusters), and a 0.999 worst-case quartile prediction.

- **Figure 11**: Scatter plots showing the trade-off between under-prediction rate and mean syndrome coverage for each of our methods.
  - Mean Number of Additional Syndromes (SN)

## Underprediction Rate vs. Mean Proportional Syndrome Coverage
- **0.95 Quartile Plot**
  - Underprediction rate vs. mean proportional syndrome coverage
- **0.99 Quartile Plot**
  - Underprediction rate vs. mean proportional syndrome coverage
- **0.999 Quartile Plot**
  - Underprediction rate vs. mean proportional syndrome coverage

### Conclusion
In this paper, we present novel methods for estimating levels of overprovisioned resources in S2DDCs and utilizing this knowledge to balance QoS losses with reliability gains. We provide performable algorithms for these methods, compare them against each other, and note their strengths and weaknesses. We also compare these methods against a statistical model that does not account for dependent actions. Our findings indicate significant value in using more complex Markov models, both in terms of QoS and reliability improvements. We demonstrate the ability to improve the reliability of existing data centers without the addition of new hardware, allowing for the allocation of an average of 2-3 additional syndromes for all stripes within a dataset. Based on our results, we conclude that existing data centers can dramatically enhance their reliability through the implementation of novel middleware processes, with minimal impact on QoS. To promote further research and development in software-defined data centers, we have made all models, data, and code available on our research website: [http://dataengineering.org/research/SSDDC/](http://dataengineering.org/research/SSDDC/).

## Future Work
Our study suggests several exciting avenues for future work:
- **Optimal Cluster Size Determination**: We have recently acquired additional datasets that will allow us to study the observed non-linear behavior in further detail. We plan to evaluate our methods on these new datasets and propose a method for optimal cluster size determination using an adaptive clustering algorithm.
- **Resource Allocation Strategies**: We aim to determine whether the additional space indicated by \(\rho\) is best used for additional syndrome coverage (improving reliability) or independent mirroring (which, while more costly in terms of needed blocks for file system coverage, would improve performance for critical "hot" blocks and their reliability).
- **Real-Time Analysis and Deployment**: We are constructing novel hardware and software to observe full usage patterns, including read patterns, in existing data centers. We have partnered with several large organizations and data centers to build comprehensive models of read patterns, changes in block popularity, and details on write and delete patterns across various domains. We are working to implement our methods on small replicas of existing datasets to provide real-time analysis and quantify the effect on QoS. Additionally, we are developing extensions to popular file systems and server architectures to deploy our methods on a wide variety of machines, particularly those with resource constraints, such as those with a primary scientific purpose.

## Acknowledgment
The authors would like to thank the Prairie Research Institute and the Illinois Natural History Survey for granting access to their data and personnel during this study, and for their continued collaboration on topics related to S2DDCs. All code and data for this project are made available through our website under the terms of the University of Illinois/NCSA Open Source License at [http://dataengineering.org/research/SSDDC/](http://dataengineering.org/research/SSDDC/).

## References
[1] J. L. Schnase, D. Q. Duffy, G. S. Tamkin, D. Nadeau, J. H. Thompson, C. M. Grieg, M. A. McInerney, and W. P. Webster, “Merra analytic services: Meeting the big data challenges of climate science through cloud-enabled climate analytics-as-a-service,” Computers, Environment and Urban Systems, 2014.

[2] I. A. T. Hashem, I. Yaqoob, N. B. Anuar, S. Mokhtar, A. Gani, and S. U. Khan, “The rise of big data on cloud computing: Review and open research issues,” Information Systems, vol. 47, pp. 98–115, 2015.

[3] D. Duffy and J. Schnase, “Meeting the big data challenges of climate science through cloud-enabled climate analytics-as-a-service,” in Proceedings of the 30th International Conference on Massive Storage Systems and Technology. IEEE Computer Society, 2014.

[4] S.-O. Act, “Public law no. 107-204,” 2002.

[5] U. Congress, “Health insurance portability and accountability act (HIPAA) of 1996,” Public Law, pp. 104–191, 2007.

[6] F. Berman, “Got data?: a guide to data preservation in the information age,” Communications of the ACM, vol. 51, no. 12, pp. 50–56, 2008.

[7] R. L. Moore, J. DAoust, R. H. McDonald, and D. Minor, “Disk and tape storage cost models,” Archiving 2007, 2007.

[8] “VMware software-defined data center: The next-generation data center,” 2013. [Online]. Available: [http://www.vmware.com/solutions/datacenter/software-defined-datacenter/index.html](http://www.vmware.com/solutions/datacenter/software-defined-datacenter/index.html)

[9] E. W. Rozier, P. Zhou, and D. Divine, “Building intelligence for software-defined data centers: modeling usage patterns,” in Proceedings of the 6th International Systems and Storage Conference. ACM, 2013, p. 20.

[10] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, and S. Shenker, “Nox: towards an operating system for networks,” ACM SIGCOMM Computer Communication Review, vol. 38, no. 3, pp. 105–110, 2008.

[11] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, and J. Turner, “OpenFlow: enabling innovation in campus networks,” ACM SIGCOMM Computer Communication Review, vol. 38, no. 2, pp. 69–74, 2008.

[12] B. Schroeder and G. A. Gibson, “Disk failures in the real world: What does an MTTDL of 1,000,000 hours mean to you,” in Proceedings of the 5th USENIX Conference on File and Storage Technologies (FAST), 2007, pp. 1–16.

[13] E. Rozier, W. Belluomini, V. Deenadhayalan, J. Hafner, K. Rao, and P. Zhou, “Evaluating the impact of undetected disk errors in RAID systems,” in Dependable Systems & Networks, 2009. DSN’09. IEEE/IFIP International Conference on. IEEE, 2009, pp. 83–92.

[14] J. L. Hafner, V. Deenadhayalan, W. Belluomini, and K. Rao, “Undetected disk errors in RAID arrays,” IBM Journal of Research and Development, vol. 52, no. 4.5, pp. 413–425, 2008.

[15] G. Wallace, F. Douglis, H. Qian, P. Shilane, S. Smaldone, M. Chamness, and W. Hsu, “Characteristics of backup workloads in production systems,” in Proceedings of the Tenth USENIX Conference on File and Storage Technologies (FAST12), 2012.

[16] V. Tarasov, S. Kumar, J. Ma, D. Hildebrand, A. Povzner, G. Kuenning, and E. Zadok, “Extracting flexible, replayable models from large block traces,” FAST12, 2012.

[17] E. Anderson, “Capture, conversion, and analysis of an intense NFS workload,” in Proceedings of the 7th conference on File and storage technologies. USENIX Association, 2009, pp. 139–152.

[18] H. V. Madhyastha, J. C. McCullough, G. Porter, R. Kapoor, S. Savage, A. C. Snoeren, and A. Vahdat, “SCC: Cluster storage provisioning informed by application characteristics and SLAs,” in Proceedings of the 10th USENIX conference on File and Storage Technologies. USENIX Association, 2012, pp. 23–23.

[19] G. Soundararajan, D. Lupei, S. Ghanbari, A. D. Popescu, J. Chen, and C. Amza, “Dynamic resource allocation for database servers running on virtual storage,” in Proceedings of the 7th conference on File and storage technologies. USENIX Association, 2009, pp. 71–84.

[20] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature space analysis,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 24, no. 5, pp. 603–619, 2002.

[21] J. A. Hartigan and M. A. Wong, “Algorithm AS 136: A k-means clustering algorithm,” Applied statistics, pp. 100–108, 1979.

[22] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu, “An efficient k-means clustering algorithm: Analysis and implementation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 24, no. 7, pp. 881–892, 2002.

[23] A. K. Jain, “Data clustering: 50 years beyond k-means,” Pattern Recognition Letters, vol. 31, no. 8, pp. 651–666, 2010.

[24] K. Fukunaga and L. Hostetler, “The estimation of the gradient of a density function, with applications in pattern recognition,” Information Theory, IEEE Transactions on, vol. 21, no. 1, pp. 32–40, 1975.

[25] Y. Cheng, “Mean shift, mode seeking, and clustering,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 17, no. 8, pp. 790–799, 1995.

[26] B. E. Clark, F. D. Lawlor, W. E. Schmidt-Stumpf, T. J. Stewart, and G. D. Timms Jr, “Parity spreading to enhance storage access,” Aug. 2, 1988, US Patent 4,761,785.

[27] H. P. Anvin, “The mathematics of RAID-6,” online paper, 2007.

[28] A. Leventhal, “Triple-parity RAID and beyond,” Queue, vol. 7, no. 11, p. 30, 2009.

[29] J. Paris, S. Schwarz, A. Amer, and D. D. Long, “Highly reliable two-dimensional RAID arrays for archival storage,” in Performance Computing and Communications Conference (IPCCC), 2012 IEEE 31st International. IEEE, 2012, pp. 324–331.

[30] G. Clark, T. Courtney, D. Daly, D. Deavours, S. Derisavi, J. M. Doyle, W. H. Sanders, and P. Webster, “The MOBIUS modeling tool,” in Petri Nets and Performance Models, 2001. Proceedings. 9th International Workshop on. IEEE, 2001, pp. 241–250.

[31] K. M. Greenan, J. S. Plank, and J. J. Wylie, “Mean time to meaningless: MTDDL, Markov models, and storage system reliability,” in Proceedings of the 2nd USENIX conference on Hot topics in storage and file systems. USENIX Association, 2010, pp. 5–5.

[32] B. Schroeder and G. A. Gibson, “Reliability/interrupt/failure/usage data sets for the 1995-2005 timeframe.” (2007). [Online]. Available: [http://institute.lanl.gov/data/fdata/](http://institute.lanl.gov/data/fdata/)

[33] E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large disk drive population.” in FAST, vol. 7, 2007, pp. 17–23.

[34] B. Schroeder and G. A. Gibson, “A large-scale study of failures in high-performance computing systems,” IEEE Transactions on Dependable and Secure Computing, vol. 7, no. 4, pp. 337–350, 2010.

[35] J. L. Hafner, V. Deenadhayalan, K. Rao, and J. A. Tomlin, “Matrix methods for lost data reconstruction in erasure codes.” in FAST, vol. 5, 2005, pp. 15–30.

[36] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and J. Schindler, “An analysis of latent sector errors in disk drives,” in ACM SIGMETRICS Performance Evaluation Review, vol. 35, no. 1. ACM, 2007, pp. 289–300.

[37] B. Schroeder, S. Damouras, and P. Gill, “Understanding latent sector errors and how to protect against them,” ACM Transactions on Storage (TOS), vol. 6, no. 3, p. 9, 2010.

[38] J. F. Meyer, A. Movaghar, and W. H. Sanders, “Stochastic activity networks: Structure, behavior, and application,” in International Workshop on Timed Petri Nets. IEEE Computer Society, 1985, pp. 106–115.

[39] R. Freire, A technique for simulating composed SAN-based reward models. The University of Arizona., 1990.