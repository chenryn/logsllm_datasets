# Experimental Results and Analysis

## 6. Experimental Results
We now discuss the experiments conducted to analyze the performance of the probabilistic replica selection algorithm as implemented in AQuA. Our experimental setup consists of a set of uniprocessor Linux machines distributed over a 100 Mbps LAN, with processor speeds ranging from 300 MHz to 1 GHz. All confidence intervals for the results are at a 95% level and have been computed under the assumption that the number of timing failures follows a binomial distribution [4].

### 6.1. Overhead of Selection Algorithm
Figure 3 illustrates how the overhead of the probabilistic selection algorithm varies with the number of available replicas for sliding windows of sizes 10 and 20. The computation of the response time distribution function contributes to 90% of these overheads, while the selection of the replica subset using Algorithm 1 contributes to the remaining 10%. Larger sliding windows result in more data points used to compute the response time distribution, thereby increasing the selection overhead. For the experiments described below, we used a sliding window of size 20.

### 6.2. Validation of Probabilistic Model
To validate the probabilistic model, we evaluated how effectively the subset of replicas chosen by the probabilistic selection algorithm met the QoS requested by the client. The experimental setup included 10 server replicas, with 4 in the primary group and the remaining in the secondary group. We simulated the background load on the servers by having each replica respond to a request after a delay normally distributed with a mean of 100 milliseconds and a variance of 50 milliseconds. Two clients, running on different machines, independently issued requests to the replicated service with a 1000 millisecond request delay, defined as the duration that elapses before a client issues its next request after the completion of its previous request. Each client issued 1000 alternating write and read requests to the service.

- **Client 1**: Requested the same QoS for all runs, including a staleness threshold of 4, a deadline of 200 milliseconds, and a minimum probability of timely response of 0.1.
- **Client 2**: Specified a staleness threshold of 2 in all runs but requested different deadlines in each run. Experiments were repeated for two different probability values: 0.9 and 0.5.

For each deadline value of Client 2, we experimentally computed the probability of timing failures by measuring the number of requests for which the client failed to receive a response within the requested deadline. To study the effect of staleness on timeliness, we repeated the experiments using different lazy update intervals (LUI) of 2 seconds and 4 seconds.

#### 6.2.1. Number of Replicas Selected
Figure 4a shows the average number of replicas selected by the selection algorithm to service Client 2 for each of its QoS specifications. As the client's QoS specification becomes less stringent, the number of replicas chosen by the algorithm to service a request reduces. This is because our algorithm, as outlined in Section 5.3, never selects more replicas than necessary to meet the client’s QoS requirement. The less stringent the QoS, the higher the probability that a chosen replica will meet the client’s specification, allowing the algorithm to satisfy the request with fewer replicas.

#### 6.2.2. Timing Failure Probability
Figure 4b demonstrates the success of the replicas selected in Figure 4a in meeting the QoS specifications of Client 2. In each case, the set of replicas selected by Algorithm 1 was able to meet the client’s QoS requirements by maintaining the timing failure probability within the acceptable range. For example, when the LUI is 4 seconds and the client specifies a minimum probability of timely response of 0.9, the observed probability of timing failures varies from 0.1 to 0.02 as the deadline varies from 100 milliseconds to 200 milliseconds. Similar behavior is observed in other cases. Thus, the model used in the experiments was able to predict the set of replicas that would return the appropriate response by the client’s deadline with the required probability.

As the interval between lazy updates increases, the observed probability of timely failures also increases. This is because a longer interval between updates makes the replica's state increasingly stale, increasing the likelihood that a chosen replica may need to defer its response until it receives the next lazy update to meet the client’s staleness threshold. Consequently, fewer replicas are available to respond immediately, resulting in a higher probability of timing failures.

## 7. Conclusions
We have presented a framework for providing tunable consistency and timeliness at the middleware layer using replication. Extensive experiments varying parameters such as the lazy update interval and request delay show that our probabilistic approach can adapt the selection of replicas to meet a client’s timeliness and consistency constraints in the presence of delays and replica failures, provided enough replicas are available. Our framework currently admits all clients and informs them if the observed failure probability exceeds their expectations. With modifications, this framework can also perform admission control to determine which clients can be admitted based on the current availability of replicas. Additionally, the framework can be extended to allow clients to specify higher-level requirements, such as priority or cost, which the middleware can map to an appropriate probability value for adaptive replica selection.

### Acknowledgments
We thank the reviewers for their feedback. We also thank Kaustubh Joshi for his feedback on the probabilistic models and Jenny Applequist for her comments.

### References
[1] K. Birman. Building Secure and Reliable Network Applications. Manning, 1996.
[2] A. Demers, D. Greene, C. Hauser, W. Irish, and J. Larson. Epidemic Algorithms for Replicated Database Maintenance. In ACM Symp. on Principles of Distributed Computing, pages 1–12, 1987.
[3] M. Hayden. The Ensemble System. PhD thesis, Cornell University, January 1998.
[4] N. Johnson, S. Kotz, and A. Kemp. Univariate Discrete Distributions, chapter 3, pages 129–130. Addison-Wesley, second edition, 1992.
[5] S. Krishnamurthy, W. H. Sanders, and M. Cukier. A Dynamic Replica Selection Algorithm for Tolerating Timing Faults. In Proc. of the International Conference on Dependable Systems and Networks, pages 107–116, July 2001.
[6] V. Krishnaswamy, M. Raynal, D. Bakken, and M. Ahamad. Shared State Consistency for Time-Sensitive Distributed Applications. In Proc. of the Intl. Conference on Distributed Computing Systems, pages 606–614, April 2001.
[7] L. Lamport. Time, Clocks, and the Ordering of Events in Distributed Systems. Communications of the ACM, 21(7):558–565, July 1978.
[8] L. Moser, P. Melliar-Smith, and P. Narasimhan. A Fault Tolerance Framework for CORBA. In Proc. of the IEEE Intl. Symp. on Fault-Tolerant Computing, pages 150–157, June 1999.
[9] K. Petersen, M. Spreitzer, D. Terry, M. Theimer, and A. Demers. Flexible Update Propagation for Weakly Consistent Replication. In Proc. of the 16th ACM Symp. on Operating Systems Principles, pages 288–301, October 1997.
[10] C. Pu and A. Leff. Replica Control in Distributed Systems: An Asynchronous Approach. In Proc. of the ACM SIGMOD Intl. Conference on Management of Data, pages 377–386, May 1991.
[11] Y. (J.) Ren, T. Courtney, M. Cukier, C. Sabnis, W. H. Sanders, M. Seri, D. A. Karr, P. Rubel, R. E. Schantz, and D. E. Bakken. AQuA: An Adaptive Architecture that Provides Dependable Distributed Objects. IEEE Transactions on Computers. To appear.
[12] P. Rubel. Passive Replication in the AQuA System. Master’s thesis, University of Illinois at Urbana-Champaign, 2000.
[13] F. Torres-Rojas, M. Ahamad, and M. Raynal. Timed Consistency for Shared Distributed Objects. In Proc. of the ACM Symp. on Principles of Distributed Computing, pages 163–172, May 1999.
[14] A. Vaysburd. Building Reliable Interoperable Distributed Applications with Maestro Tools. PhD thesis, Cornell University, May 1998.
[15] H. Yu and A. Vahdat. Design and Evaluation of a Continuous Consistency Model for Replicated Services. In Proc. of the 4th Symp. on Operating Systems Design and Implementation (OSDI), October 2000.

Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply.