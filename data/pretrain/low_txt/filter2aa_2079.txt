# Adversarial Machine Learning and Countermeasures

## Introduction
**Presenters:**
- **ch0upi**
  - Staff Engineer at Trend Micro
  - Expertise in Machine Learning, Data Analysis, and Threat Intelligence Services
  - Achievements: NIPS, KDDCup 2014 & 2016 (Top 10), GoTrend (6th in UEC Cup 2015)
- **miaoski**
  - Senior Threat Researcher at Trend Micro
  - Specializes in Threat Intelligence, Smart City, SDR, and Arduino/RPi Makers
  - Cat lover

**Date:** December 7, 2017

## Outline
1. Cheating Machine Learning?
2. Attacking Theories and Practices
3. Countermeasures
4. Conclusion

## Cheating Machine Learning Models
### We Were Good Guys...
- Even NVIDIA has faced challenges with machine learning models.
- ML-based Anti-Virus: CSOs have explained the importance of ML in security, but there are still vulnerabilities.

### Rescan Makes It Worse
- Example: Compiler outputs and their detection by different antivirus software:
  - **Visual Studio 2017:**
    - Cylance, Jiangmin
    - Cylance, Cyren, F-Prot, Sophos, SentinelOne
  - **MingW64:**
    - All good
  - **Cygwin x86_64:**
    - Baidu, Cylance

### ML is Prosperous
- DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman et al., 2014)
- ML drives advancements in various fields, such as autonomous vehicles (Tesla Autopilot).

### Machine Learning Vulnerabilities
- Machine learning models have specific vulnerabilities that need to be addressed.

## Theories and Practices
### Methodology
- **Evasion:**
  - Black Box
    - No model, only predict interface & result
    - **Random Noise Attack:** Adding white noise (not effective for most models)
    - **Iterative Random Attack:** Adding noise, repeating hundreds of times, and selecting the best one
      - Inspired by Evtimov et al. (2017)
      - Example: STOP sign misclassification
    - **Genetic Algorithm:**
      - Effective random search inspired by natural selection
      - Steps: Selection, Crossover, Mutation, Evaluation
  - White Box
    - With all model details (DNN architecture, weights)
    - **Fast Gradient Sign Method (FGSM):**
      - Simple and computationally efficient
      - Non-target attack (Goodfellow et al., 2014)
      - Formula: \( X_{\text{adv}} = X + \epsilon \cdot \text{sign}(\nabla_X J(X, y_{\text{trgt}})) \)
    - **White Box Attack Methods:**
      - FGSM (non-target, one step)
      - One-step target class methods (target, one step)
      - Basic iterative method (non-target, multiple steps)
      - Iterative least-likely class method (target, multiple steps) (Kurakin et al., 2017)

### Model Stealing
- **Model is data and asset:**
  - Train a local DNN for black box attacks
  - Data privacy concerns
  - **Transferability Property:** Train a local model for adversarial attacks
  - **Data Privacy:** Rebuilding faces using stolen models (Tramer et al., 2016)

### Poisoning
- **Crowdsourcing:**
  - Amazon Mechanical Turk
  - Mis-labeling
  - Online training (e.g., Microsoft chatbot: Tay)
  - User feedback

### Real-World Adversarial Attacks
- **Evading PDF ML:**
  - Genetic algorithm to generate adversarial samples
  - Sandbox to ensure malicious behavior (Xu et al., 2016)
- **Autopilot Cars:**
  - Vulnerable to adversarial attacks
- **Access Control with Face Recognition:**
  - Potential for evasion through adversarial techniques

## Countermeasures
### Ensemble & Stacking
- Layer protection using multiple models (e.g., Xgboost, SVM, CNN, RNN, LR, LDA)

### Retrained Models
- **Distortion:**
  - Retrain with noisy samples
  - Randomization layer in DNN (NIPS 2nd)
  - Generative Adversarial Networks (GAN)

### Denoiser
- Use denoise technologies from image processing
- Train a DNN denoiser to reduce noise

### Prevent Model Leakage
- **Avoid Model Stealing:**
  - Increase the challenge of black box attacks
  - Keep some information secret or add noise
  - Randomization and disinformation
  - Adversarial sample detection

## Conclusion
- **Know the limitations and weaknesses of your model:**
  - Integrate adversarial machine learning into the product development cycle
  - Improve ML QA processes
  - Trend Micro is working on bypassing anti-virus with ML to make products more robust

## References
- Evtimov et al. (2017) Robust Physical-World Attacks on Deep Learning Models
- Nguyen et al. (2015) Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. IEEE CVPR â€˜15.
- Kurakin A., Goodfellow I.J., Bengio S. (2017) Adversarial Examples in the Physical World.
- Xu, W., Qi, Y., and Evans, D. (2016) Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers. Network and Distributed Systems Symposium 2016.

## Additional Resources
- [GitHub Repository](https://github.com/miaoski/hitcon-2017-adversarial-ml)