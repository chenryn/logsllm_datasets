### PlayStation and Other Gaming Platforms

- **PlayStation (g)**
- **Steam/e.g. Half-Life (g)**
- **Runescape (g)**
- **TeamSpeak3 (g)**
- **Several games (g); other**
- **Star Wars (g)**

### Port 80 Attacks
The most frequently attacked port is port 80. While port 80 in UDP is not a well-known application port (unlike in TCP, where it supports HTTP), we hypothesize that attackers may have targeted this port because they believed it would be less likely to be filtered or blocked, thereby enabling a more effective attack. This pattern has been reported by others [33]. Other prominent ports in attack traffic include port 123, which is the NTP server port, and this finding is consistent with our local impact datasets (described in § 7).

### Game Ports
In addition to ports 80 and 123, at least ten other ports associated with gaming (e.g., Xbox Live, PlayStation, Minecraft, Half-Life, and Counter-Strike) are among the top 20. If we include port 80, which in TCP is used by many games and non-games and may be mistakenly targeted, the total fraction of attacks related to games could be even higher. Many ports above the top-20 list are also used by games. This targeting of game ports, along with the end-host fraction mentioned earlier, supports our conclusion that a significant fraction of NTP DDoS attacks are directed against gamers, as previously reported in press and industry reports (e.g., Goodin [18], Prolexic [31], Arbor Networks [8]). Minecraft, in particular, was confirmed as a large attack target according to Goodin’s sources.

### Attack Volume
The aggregate attack packet count across all amplifiers over fifteen weeks is 2.92 trillion packets. Assuming each packet had a median size of 420 bytes, the estimated aggregate data volume is 1.2 petabytes. However, we believe these samples underestimate the actual victim population due to several factors: we do not see version victims from non-monlist-returning NTP servers, we only sample once per week, and some amplifiers attack thousands of victims simultaneously (e.g., Table 5). As discussed in § 4.2, we under-sample by a median factor of 3.8. Therefore, the actual volume of traffic to victims is likely closer to 3.8 times 1.2 petabytes. We caution that our data is lossy and sourced from often mismanaged devices, so these estimates may be significantly off. In general, we believe our numbers underestimate the amplifier population, victim population, attack counts, and attack sizes due to the limitations discussed here and earlier.

### Total Packets Received by Victims
Figure 6 shows the total number of packets received by observed victims. Median attacks are relatively small, around 300-1000 packets, but the average is high at 1-10M, driven by a few heavily-attacked victims. The 95th percentile was in the range of 400K to 6M until mid-February, but since then, it has decreased to the 110k-200k range, suggesting the effect of remediation.

### Attack Counts and Durations
Due to the noisy and lossy nature of our data, we make some simplifying assumptions when discussing attack counts and durations. First, we count each unique IP targeted in a given weekly sample as a single victim and attack. This is a simplification because:
- A given attack campaign may involve several IPs in a network block or multiple autonomous systems (ASes).
- A single IP may host multiple websites or users that are targets of independent attacks.
- Multiple attacks launched by one or more attackers may target the same IP in small time frames that are too fine-grained for our data to disambiguate.

To determine the start time of an attack, we use the median start time calculated across all amplifiers seen attacking the victim. Figure 7 shows a time series of attack counts per hour seen in the ONP data. The attack counts are a lower bound because the tables only show a median of 44 hours of amplifier activity every week. Samples start on January 10th, but some tables include evidence of older activity. Our calculations allow us to partially identify attacks with start and end times prior to our first sample, indicated with a vertical dashed line. We plot the attacks seen binned by hour as well as a daily average line. The daily average peaks on February 12th, coinciding with the largest publicly-disclosed CloudFlare and OVH attacks, which started on February 10th. This peak corresponds to the same daily peak in NTP traffic observed in the Arbor Networks Internet traffic graph in Figure 1. The general trend up through mid-February and down afterwards matches the trajectory of global NTP traffic, suggesting that the attacks drove global NTP traffic. Median attacks over our sample period lasted approximately 40 seconds since mid-February, and about half or a quarter of that in previous samples. The 95th percentile duration attacks lasted about six and a half hours in the January 10th sample and have been declining since, with 50 minutes being the 95th percentile duration in the later April samples.

### Validation
One of the early massive NTP-based DDoS attacks observed in the wild occurred in early February and was disclosed by CloudFlare [30]. The attack started on February 10th and was reportedly near 400Gbps in size, a record for NTP DDoS at the time. The attack targeted servers at OVH, a French-based hosting firm protected by CloudFlare [13]. OVH is one of the largest hosting providers in the world, including services targeting game servers [6]. In our rankings of networks that have been the targets of attacks, out of 11,558 ASes, OVH (AS number 16267) is the top AS (Cloudflare itself ranks 18th). Our data shows the OVH AS receiving over 170Bn aggregate packets from the amplifiers we studied, nearly 6% of all attack packets. OVH also features prominently in the Colorado State University (CSU) top-10 victims list (see § 7), accounting for five of the top 10 most attacked IPs. The attack campaign against OVH appears to be long-lasting; for example, OVH is the top AS in at least one weekly sample during each of the four months in our dataset and shows up as one of the top ten ASes that are attacked in 13 of the 15 weeks. Attackers target multiple unique IPs at OVH, peaking at nearly 4K in March.

CloudFlare also published the list of 1,297 ASes that hosted vulnerable NTP servers used as amplifiers in this attack. Of these 1,297, 1,291 were also seen in the ONP data, which totals 16,687 total amplifier-hosting ASes. The 1,291 that overlapped were, in aggregate, responsible for 60% of all victim packets. In addition to cross-dataset confirmation discussed throughout, this example supports the validity of our approach and gives us greater confidence.

### Attackers and Darknet Scanning
#### View from a Darknet
Figure 8 shows the volume of NTP traffic observed at the darknet by an average /24 network block equivalent. We notice a 10-fold increase in NTP-related scanning activity starting from December 2013 to April 2014. It is interesting to note that we observe not just an increase in malicious scanning, but also scanning from various research efforts that were attempting to track the vulnerable NTP population (we identified these by their hostnames). Roughly half of the increase in scanning can be attributed to research efforts.

Figure 9 shows a time series of NTP scanning activity in terms of the number of unique IP addresses observed in our darknet. We are clearly able to pinpoint the onset of large-scale NTP scanning in mid-December 2013. The figure also shows aggregate NTP traffic volume on the operational (non-dark) portions of Merit’s network (details in § 7). We observe that the rise in scanning activity precedes actual NTP attack traffic increases by roughly a week. This highlights the importance of using darknets to build active early warning systems of new and emerging threats [10]. It should also be noted that scanning traffic volumes continue to be high even as the global vulnerable NTP population has seen a dramatic decline. This indicates a continuing interest in finding vulnerable NTP servers.

Since at least one study reported seeing some UDP amplifiers (it is unclear if these were NTP) in the IPv6 space, we were curious to see if there was scanning activity observable in IPv6 [32]. We examined collected packets to dark (unused) address space in a large IPv6 darknet we operate, which includes covering prefixes for four of the five Regional Internet Registrars, including the RIRs for North America, South America, Asia, and Africa [14]. We searched for evidence of NTP scanning in the IPv6 darknet data in Nov. 2013, Dec. 2013, and Feb. 2014, but saw mostly errant point-to-point NTP connections, and no evidence of broad scanning. Likewise, the Arbor Networks netflow data for IPv6 does not list NTP within the top 200 UDP ports (it was 12th in IPv4), and thus did not show a noticeable level of NTP traffic.

#### Attacker Ecosystem and Motivations
The concept of "attacker" in this type of DDoS activity is both nebulous to define and difficult to measure. Unfortunately, most available datasets shed very little light on who the actual attackers are that perpetrate these NTP-based DDoS attacks or what their motivations may be. However, there are a few small clues that public reports have revealed, as well as a few tidbits in our data.

Dissecting the DDoS ecosystem and understanding attackers is complicated by the fact that several types of actors are involved in launching attacks. We've reserved the term "amplifier" for the vulnerable or misconfigured boxes that are leveraged in attacks to flood victims with traffic. These are part of the problem, but they are more enablers than aggressors. The attack ecosystem has several other entities that better fall under the umbrella of "attacker." First, there are the nodes that send spoofed-source packets to amplifiers to elicit the large responses that flood victims. These may or may not be the machines owned by the humans launching attacks. In many cases, they are actually compromised Internet user machines ("zombies" or "bots") that can be remotely commanded to perform such actions on behalf of a "botmaster." Thus, the second entity we might label an attacker is the botmaster himself. Certainly, this person or group and the system they use can be labeled culpable. But, blame might not stop there. These botmasters could be individuals acting on behalf of a black market DDoS ("booter") service, many of which are advertised on underground forums (e.g., [5]) [19]. They or the service may have been hired by the party that is actually motivated to cause damage to the victim. So, while a botmaster or booter service is likely to be motivated by money, the person that actually wants the attack to happen could be motivated by anything that normally motivates people to attack others. This, of course, includes money (e.g., via extortion [28]), revenge, political reasons, competitive advantage (see Poulsen for an early example [29]), etc. According to a large 2014 survey of global network operators, political/ideological reasons top the list of perceived motivations for being attacked [8].

We discuss several clues about attackers and their motives that appear in our own data where that data is presented. For instance, in § 4.3.2, we mentioned that a significant amount of victims were targeted on game-related ports. This is congruent with the idea that many DDoS attacks are perpetrated against gamers, by rivals or for financial gain, as reported previously (e.g., [4, 18, 19, 26, 31]). Another clue is discussed briefly in § 7.2, where we found that packet TTLs of scanning packets indicate they are mostly Linux, while packets sent by nodes generating traffic to amplifiers indicate they run Windows. As botnet nodes are often Windows machines, while individual miscreants with enough sophistication to conduct broad Internet scans may be Linux users, this clue fits the story that attackers are using botnet hosts to indirectly launch attack traffic.

### Remediation
One of the most encouraging observations regarding the NTP DDoS attacks has been the community response. Community response to the NTP amplifier threat has been swift [17], with the number of vulnerable monlist NTP servers dropping dramatically from a high of 1.4M when first measured on January 10th, down to less than half (678K) just two weeks later and continuing to fall to around 110K, where it has held steady since March 21st. Some interesting facets of how remediation occurred are presented next.

#### Subgroup Remediation Rates
One interesting aspect of how monlist amplifier remediation is proceeding is its varying nature across several axes.

**Network Levels:**
First, we look at network granularity. We already noted that the overall set of amplifier IPs has been reduced in cardinality from approximately 1.4M in early January to 110K by April 18th; a reduction of 92%. However, when we aggregate amplifier IPs by /24 subnets, we find that the reduction is from 264K to 73K, or 72%. There are at least two possible reasons for this discrepancy: given IPs in a /24 might be managed by different operators (e.g., because they are home PCs in a residential ISP), or the individual hosts may be more or less difficult to patch, for example, due to their role. When we aggregate up to the routed block level, the percentage reduction falls again, from 64K routed blocks to 26K, or 59% reduction. At the autonomous system level, it is only 55%, from 15k origin ASes to 6.8K. These trends highlight the difficulty in completely eliminating a vulnerability from all of a single network, let alone such a large number of independently-managed networks.

**Regional Levels:**
The second axis along which remediation differs is the regional one. When we aggregate the amplifiers according to their continent, we find that North America has remediated 97% of its amplifiers, Oceania 93%, Europe 89%, Asia 84%, Africa 77%, and South America 63%. These differences in the speed to remediate amplifiers across regions may be caused by various socio-economic factors, by the relative regional impact that these amplifiers have caused, and by network management practices or norms.

**End Host Composition:**
A third axis is the relative composition of amplifiers that are end hosts. We again used the PBL [34] to label each IP seen in the weekly samples as either an end host or not. As Table 1 showed, as the pool of amplifiers was remediated, the fraction of amplifiers that are end hosts approximately doubled from 17% in the first two weeks to 34% in the last, suggesting that end hosts are more challenging to remediate.