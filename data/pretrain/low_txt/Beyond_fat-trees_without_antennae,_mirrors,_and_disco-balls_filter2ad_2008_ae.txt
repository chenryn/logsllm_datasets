### Comparison of Xpander and Fat-Tree Topologies

In this section, we compare the Xpander topology to a fat-tree topology. Each ToR (Top of Rack) switch in the Xpander topology has 16 dynamic network connections, compared to 8 network ports per ToR in the fat-tree. The Xpander also connects directly to 8 servers without any intermediate switches, whereas the fat-tree includes 192 additional intermediate switches. Both topologies are configured with a total of 128 switches, but the Xpander uses 16 static network ports per switch instead of dynamic ones. Neither topology includes any other intermediate switches, only the ToRs with the same port counts.

The Xpander topology is more cost-effective than ProjecToR, using the same number of static network ports as ProjecToR uses dynamic ones, without any adjustment for the cost factor, δ. We present results in two settings: one where server-switch links have the same 10 Gbps capacity as all other links, and another with unconstrained capacity for server-switch links. The latter model is used only with reference to ProjecToR, as this is how ProjecToR was evaluated, to effectively model an oversubscribed fat-tree with additional servers underneath each rack.

### Evaluation Criteria for Dynamic Networks

We argue that demonstrating the advantages of dynamic networks requires:
- Comparing to expander-based static networks
- Ensuring equal cost
- Using more expressive routing than ECMP (Equal-Cost Multi-Path)
- Accounting for added latency from topology dynamics

As shown in §2.1, fat-trees are nearly ideally inflexible towards skewed traffic, making them an easy baseline, especially compared to expander-based networks like Xpander [33] and Jellyfish [31]. Expander-based networks require more expressive routing than just ECMP to provide their efficiency improvements (§6). Using ECMP handicaps static networks in comparisons with dynamic designs that use sophisticated routing and topology optimizations. Comparing networks without equalizing cost does not yield actionable information. Specifically, if a dynamic network design uses x ports that cost δ times the cost of a "static port" (δ varies across technologies), the point of comparison should be an expander-based design with δx ports.

In many dynamic designs, end-hosts often need to wait until connectivity between them is available, leading to additional latency and buffering. Accounting for these delays and their interplay with congestion control is essential for meaningful comparisons.

Our simulation framework is available [1] as an easy-to-use baseline for future work on dynamic networks to compare against.

### Recent Developments in Dynamic Networks

The past decade has seen numerous novel and interesting proposals for data center network topologies, both static (§3) and dynamic (§4). At least three new proposals have been made concurrently with this work [11, 26, 37], which we discuss below.

**Flat-tree [37]** proposes the use of small additional switches in the topology that function as "converters," making some connections more "local" when needed, in an effort to match Jellyfish’s performance while preserving structure. However, Xpander can achieve these objectives without additional converter switches.

**MegaSwitch [11]** proposes the use of wavelength division multiplexing over a set of racks arranged in an optical ring, which limits its present design to 33 racks. MegaSwitch does not include a crisp cost comparison as the optical components needed (e.g., transceivers) are not standard, commodity equipment.

**RotorNet [26]** is a novel dynamic topology proposal that differs from the prevalent approach to the design of dynamic data centers by not relying on any dynamic optimization in response to traffic estimation. Instead, RotorNet cycles through a series of pre-determined optical port matchings in a traffic-agnostic manner. Investigating rigorously, following §7, whether RotorNet outperforms state-of-the-art static networks is deferred to future research. While RotorNet alleviates some of the problems past approaches faced, it still involves nontrivial challenges, such as accommodating latency-sensitive traffic.

Thus, even the most recent dynamic designs have not yet demonstrated an advantage over expander-based static networks.

### Conclusion

Our results show that state-of-the-art static networks designed using commodity data center network equipment and employing simple routing protocols provide the same cost-efficiency advantages over full-bandwidth fat-trees as claimed by recent proposals on dynamic networks. We believe that investigating and advancing the deployability of these static network designs in practice is a promising approach for moving beyond today’s prevalent data center architectures. For dynamic networks to be compelling, they should demonstrate substantial improvements over these static networks when compared at equal cost.

### Acknowledgments

We would like to thank Ratul Mahajan for his insights on the limitations of the restricted topology adaptation model. We are also grateful to our colleagues who provided helpful feedback on this work, including Monia Ghobadi, Torsten Hoefler, George Porter, Marcel Schneider, Laurent Vanbever, and the anonymous SIGCOMM reviewers and shepherd. Asaf Valadarsky is supported by a Microsoft Research Ph.D. Scholarship. Michael Schapira is supported by the PetaCloud Consortium.

### References

[1] Netbench. (2017). https://github.com/ndal-eth/netbench.
[2] Dennis Abts, Michael R Marty, Philip M Wells, Peter Klausler, and Hong Liu. Commodity Data Center Network Architecture. ACM SIGCOMM (2008).
[3] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. A Scalable, Energy Proportional Datacenter Networks. ACM/IEEE ISCA (2010).
[4] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, Andy Fingerhut, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. CONGA: Distributed Congestion-aware Load Balancing for Datacenters. ACM SIGCOMM (2014).
[5] Mohammad Alizadeh, Albert Greenberg, David A Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. Data Center TCP (DCTCP). ACM SIGCOMM (2010).
[6] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat, and Masato Yasuda. Less is More: Trading a Little Bandwidth for Ultra-low Latency in the Data Center. USENIX NSDI (2012).
[7] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. pFabric: Minimal Near-optimal Datacenter Transport. ACM SIGCOMM (2013).
[8] Theophilus Benson, Aditya Akella, and David A Maltz. Network Traffic Characteristics of Data Centers in the Wild. ACM SIGCOMM (2010).
[9] M. Besta and T. Hoefler. Slim Fly: A Cost Effective Low-Diameter Network Topology. IEEE/ACM SC (2014).
[10] Kai Chen, Ankit Singla, Atul Singh, Kishore Ramachandran, Lei Xu, Yueping Zhang, Xitao Wen, and Yan Chen. OSA: An Optical Switching Architecture for Data Center Networks with Unprecedented Flexibility. USENIX NSDI (2012).
[11] Li Chen, Kai Chen, Zhonghua Zhu, Minlan Yu, George Porter, Chunming Qiao, and Shan Zhong. Enabling Wide-Spread Communications on Optical Fabric with MegaSwitch. USENIX NSDI (2017).
[12] Nathan Farrington, George Porter, Sivasankar Radhakrishnan, Hamid Hajabdolali Bazzaz, Vikram Subramanya, Yeshaiahu Fainman, George Papen, and Amin Vahdat. Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers. ACM SIGCOMM (2010).
[13] Monia Ghodbadi, Ratul Mahajan, Amar Phanishayee, Houman Rastegarfar, Pierre-Alexandre Blanche, Madeleine Glick, Daniel Kilper, Janardhan Kulkarni, Gireeja Ranade, and Nikhil Devanur. ProjecToR: Agile Reconfigurable Datacenter Interconnect. ACM SIGCOMM (2016).
[14] Madeleine Glick, David G Andersen, Michael Kaminsky, and Lily Mummert. Dynamically Reconfigurable Optical Links for High-bandwidth Data Center Networks. Optical Fiber Communication Conference (2009). https://goo.gl/hx0vz3.
[15] Google. Pulling Back the Curtain on Google’s Network Infrastructure. (2015).
[16] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula, Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta Sengupta. VL2: A Scalable and Flexible Data Center Network. ACM SIGCOMM (2009).
[17] Daniel Halperin, Srikanth Kandula, Jitendra Padhye, Paramvir Bahl, and David Wetherall. Augmenting Data Center Networks with Multi-Gigabit Wireless Links. ACM SIGCOMM (2011).
[18] Navid Hamedazimi, Zafar Qazi, Himanshu Gupta, Vyas Sekar, Samir R Das, Jon P Longtin, Himanshu Shah, and Ashish Tanwer. FireFly: A Reconfigurable Wireless Data Center Fabric Using Free-Space Optics. ACM SIGCOMM (2014).
[19] Brandon Heller, Srinivasan Seetharaman, Priya Mahadevan, Yiannis Yiakoumis, Puneet Sharma, Sujata Banerjee, and Nick McKeown. ElasticTree: Saving Energy in Data Center Networks. USENIX NSDI (2010).
[20] Sangeetha Abdu Jyothi, Ankit Singla, Brighten Godfrey, and Alexandra Kolla. Measuring and Understanding Throughput of Network Topologies. IEEE SC (2016).
[21] Sangeetha Abdu Jyothi, Ankit Singla, Chi-Yao Hong, Lucian Popa, Brighten Godfrey, and Alexandra Kolla. Topobench. (2016). https://github.com/netarch/topobench/.
[22] Srikanth Kandula, Dina Katabi, Shan Sinha, and Arthur Berger. Flare: Responsive Load Balancing Without Packet Reordering. ACM CCR (2007).
[23] John Kim, William J. Dally, Steve Scott, and Dennis Abts. Technology-Driven, Highly-Scalable Dragonfly Topology. ACM SIGARCH (2008).
[24] Yunpeng James Liu, Peter Xiang Gao, Bernard Wong, and Srinivasan Keshav. Quartz: A New Design Element for Low-Latency DCNs. ACM SIGCOMM (2014).
[25] Alexander Lubotzky, Ralph Phillips, and Peter Sarnak. Ramanujan Graphs. Combinatorica (1988).
[26] William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich, George Papen, Alex C. Snoeren, and George Porter. RotorNet: A Scalable, Low-complexity, Optical Datacenter Network. ACM SIGCOMM (2017).
[27] George Porter, Richard Strong, Nathan Farrington, Alex Forencich, Pang Chen-Sun, Tajana Rosing, Yeshaiahu Fainman, George Papen, and Amin Vahdat. Integrating Microsecond Circuit Switching into the Data Center. ACM SIGCOMM (2013).
[28] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren. Towards Optimal-Performance Datacenters. ACM CoNEXT (2016).
[29] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle, Stephen Stuart, and Amin Vahdat. Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network. ACM SIGCOMM (2015).
[30] Ankit Singla, P Brighten Godfrey, and Alexandra Kolla. High Throughput Data Center Topology Design. USENIX NSDI (2014).
[31] Ankit Singla, Chi-Yao Hong, Lucian Popa, and P. Brighten Godfrey. Jellyfish: Networking Data Centers Randomly. USENIX NSDI (2012).
[32] Ratko V. Tomic. Optimal Networks from Error Correcting Codes. ACM/IEEE ANCS (2013).
[33] Asaf Valadarsky, Gal Shahaf, Michael Dinitz, and Michael Schapira. Xpander: Towards Optimal-Performance Datacenters. ACM CoNEXT (2016).
[34] Erico Vanini, Rong Pan, Mohammad Alizadeh, Tom Edsall, and Parvin Taheri. Let It Flow: Resilient Asymmetric Load Balancing with Flowlet Switching. USENIX NSDI (2017).
[35] Guohui Wang, David G. Andersen, Michael Kaminsky, Konstantina Papagiannaki, T. S. Eugene Ng, Michael Kozuch, and Michael Ryan. c-Through: Part-time Optics in Data Centers. ACM SIGCOMM (2010).
[36] Damon Wischik, Costin Raiciu, Adam Greenhalgh, and Mark Handley. Design, Implementation and Evaluation of Congestion Control for Multipath TCP. USENIX NSDI (2011).
[37] Yiting Xia and TS Eugene Ng. Flat-tree: A Convertible Data Center Network Architecture from Clos to Random Graph. ACM HotNets (2016).
[38] J.Y. Yen. Finding the K Shortest Loopless Paths in a Network. Management Science (1971).
[39] Rui Zhang-Shen and Nick McKeown. Designing a Predictable Internet Backbone (1971).

---

This version of the text is more structured, clear, and professional, with improved flow and coherence.