### References

1. **Shen Liu, Dongrui Zeng, Yongzhe Huang, Frank Capobianco, Stephen McCamant, Trent Jaeger, and Gang Tan. 2019. Program-mandering: Quantitative Privilege Separation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19). London, UK. ACM, New York, NY, USA.**  
   DOI: [10.1145/3319535.3354218](https://doi.org/10.1145/3319535.3354218)

2. **Yutao Liu, Tianyu Zhou, Kexin Chen, Haibo Chen, and Yubin Xia. 2015. Thwarting Memory Disclosure with Efficient Hypervisor-Enforced Intra-Domain Isolation. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS '15). ACM, New York, NY, USA, 1607–1619.**  
   DOI: [10.1145/2810103.2813690](https://doi.org/10.1145/2810103.2813690)

3. **Yandong Mao, Haogang Chen, Dong Zhou, Xi Wang, Nickolai Zeldovich, and M Frans Kaashoek. 2011. Software fault isolation with API integrity and multi-principal modules. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles. ACM, 115–128.**

4. **Mark Samuel Miller. 2006. Robust Composition: Towards a Unified Approach to Access Control and Concurrency Control. Ph.D. Dissertation. Johns Hopkins University, Baltimore, MD, USA. AAI3245526.**

5. **P. Mohagheghi, R. Conradi, O. M. Killi, and H. Schwarz. 2004. An empirical study of software reuse vs. defect-density and stability. In Proceedings of the 26th International Conference on Software Engineering. 282–291.**  
   DOI: [10.1109/ICSE.2004.1317450](https://doi.org/10.1109/ICSE.2004.1317450)

6. **Vikram Narayanan, Yongzhe Huang, Gang Tan, Trent Jaeger, and Anton Burtsev. 2020. Lightweight Kernel Isolation with Virtualization and VM Functions. In Proceedings of the 16th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE '20). Lausanne, Switzerland. Association for Computing Machinery, New York, NY, USA, 157–171.**  
   DOI: [10.1145/3381052.3381328](https://doi.org/10.1145/3381052.3381328)

7. **Elliott I. Organick. 1972. The Multics System: An Examination of Its Structure. MIT Press, Cambridge, MA, USA.**

8. **Gabriel Parmer and Richard West. 2011. Mutable protection domains: Adapting system fault isolation for reliability and efficiency. IEEE Transactions on Software Engineering 38, 4 (2011), 875–888.**

9. **Marios Pomonis, Theofilos Petsios, Angelos D. Keromytis, Michalis Polychronakis, and Vasileios P. Kemerlis. 2017. kRˆX: Comprehensive Kernel Protection against Just-In-Time Code Reuse. In Proc. of EuroSys. 420–436.**

10. **Sergej Proskurin, Marius Momeu, Seyedhamed Ghavamnia, Vasileios P. Kemerlis, and Michalis Polychronakis. 2020. xMP: Selective Memory Protection for Kernel and User Space. In 2020 IEEE Symposium on Security and Privacy (SP). San Francisco, CA, USA. IEEE, 563–577.**  
    DOI: [10.1109/SP40000.2020.00041](https://doi.org/10.1109/SP40000.2020.00041)

11. **Niels Provos, Markus Friedl, and Peter Honeyman. 2003. Preventing Privilege Escalation. In Proceedings of the 12th Conference on USENIX Security Symposium - Volume 12 (SSYM'03). USENIX Association, Berkeley, CA, USA, 16–16.**

12. **Richard F. Rashid and George G. Robertson. 1981. Accent: A Communication Oriented Network Operating System Kernel. In Proceedings of the Eighth ACM Symposium on Operating Systems Principles (SOSP '81). Pacific Grove, California, USA. ACM, New York, NY, USA, 64–75.**  
    DOI: [10.1145/800216.806593](https://doi.org/10.1145/800216.806593)

13. **Rick. 2018. Never-Ending Security: eBPF and Analysis of the Get-Rekt-Linux-Hardened.c Exploit for CVE-2017-16995.**

14. **Nick Roessler, Yi Chien, Lucas Atayde, Peiru Yang, Imani Palmer, Lily Gray, and Nathan Dautenhahn. 2021. Lossless instruction-to-object memory tracing in the Linux kernel. In Proceedings of the 14th ACM International Conference on Systems and Storage. 1–12.**

15. **Jerome H. Saltzer and Michael D. Schroeder. 1975. The Protection of Information in Computer Systems. Proc. IEEE 63, 9 (1975), 1278–1308.**

16. **Michael D. Schroeder and Jerome H. Saltzer. 1972. A Hardware Architecture for Implementing Protection Rings. Commun. ACM 15, 3 (March 1972), 157–170.**  
    DOI: [10.1145/361268.361275](https://doi.org/10.1145/361268.361275)

17. **Bin Shi, Lei Cui, Bo Li, Xudong Liu, Zhiyu Hao, and Haiying Shen. 2018. Shadow-Monitor: An Effective In-VM Monitoring Framework with Hardware-Enforced Isolation. In Proceedings of the International Symposium on Research in Attacks, Intrusions, and Defenses (RAID) (LNCS, 11050). Springer Nature, 670–690.**  
    DOI: [10.1007/978-3-030-00470-5_31](https://doi.org/10.1007/978-3-030-00470-5_31)

18. **Laszlo Szekeres, Mathias Payer, Tao Wei, and Dawn Song. 2013. Sok: Eternal war in memory. In Security and Privacy (SP), 2013 IEEE Symposium on. IEEE, 48–62.**

19. **Tsuna. 2010. How long does it take to make a context switch? https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html.**

20. **Anjo Vahldiek-Oberwagner, Eslam Elnikety, Nuno O. Duarte, Michael Sammler, Peter Druschel, and Deepak Garg. 2019. ERIM: Secure, Efficient In-process Isolation with Protection Keys (MPK). In 28th USENIX Security Symposium (USENIX Security 19). Santa Clara, CA. USENIX Association, 1221–1238.**  
    URL: [https://www.usenix.org/conference/usenixsecurity19/presentation/vahldiek-oberwagner](https://www.usenix.org/conference/usenixsecurity19/presentation/vahldiek-oberwagner)

21. **Nikos Vasilakis, Ben Karel, Nick Roessler, Nathan Dautenhahn, André DeHon, and Jonathan M. Smith. 2018. BreakApp: Automated, Flexible Application Compartmentalization. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018.**  
    URL: [http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_08-3_Vasilakis_paper.pdf](http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_08-3_Vasilakis_paper.pdf)

22. **Robert NM Watson, Jonathan Anderson, Ben Laurie, and Kris Kennaway. 2010. Capsicum: Practical Capabilities for UNIX. In USENIX Security Symposium, Vol. 46.**

23. **R. N. M. Watson, R. M. Norton, J. Woodruff, S. W. Moore, P. G. Neumann, J. Anderson, D. Chisnall, B. Davis, B. Laurie, M. Roe, N. H. Dave, K. Gudka, A. Joannou, A. T. Markettos, E. Maste, S. J. Murdoch, C. Rothwell, S. D. Son, and M. Vadera. 2016. Fast Protection-Domain Crossing in the CHERI Capability-System Architecture. IEEE Micro 36, 5 (Sept 2016), 38–49.**  
    DOI: [10.1109/MM.2016.84](https://doi.org/10.1109/MM.2016.84)

24. **Emmett Witchel, Junghwan Rhee, and Krste Asanović. 2005. Mondrix: Memory Isolation for Linux Using Mondriaan Memory Protection. In Proceedings of the Twentieth ACM Symposium on Operating Systems Principles (SOSP '05). ACM, New York, NY, USA, 31–44.**  
    DOI: [10.1145/1095810.1095814](https://doi.org/10.1145/1095810.1095814)

25. **Chao Zhang, Tao Wei, Zhaofeng Chen, Lei Duan, Laszlo Szekeres, Stephen McCamant, Dawn Song, and Wei Zou. 2013. Practical Control Flow Integrity & Randomization for Binary Executables. In IEEE Symposium on Security and Privacy.**  
    URL: [http://bitblaze.cs.berkeley.edu/papers/CCFIR-oakland-CR.pdf](http://bitblaze.cs.berkeley.edu/papers/CCFIR-oakland-CR.pdf)

### PSR Metric Example

Figure 10 illustrates the calculation of the PSR (Privilege Separation Ratio) for different cases. The baseline monolithic case assumes writable code, while the other cases assume non-writable code, which removes many privileges. Even in the clustered case with all unmediated edges, privileges are reduced due to the "Not" edge from `sd1` to `od0`. When the `sd0` to `od1` edge is mediated (fourth row), the link only adds the base CAPMAP 4 units of privilege for `i4` to access object `o2`. Changing the `sd0` to `od1` edge from mediated to unmediated (third row) adds 5×14=70 units since all instructions in `sd0` now have privilege over all objects in `od1`, resulting in a net increase of 70-4=66 privileges.

| Compartmentalization | PSR  | |PS|  |
|----------------------|------|----|
| Monolithic (writable code) | 9×(38+9×4) | 1.00 |
| Monolithic (non-writable code) | 9 × 38 | 0.51 |
| `sd0, sd1, od0, od1` (all edges unmediated) | 246 | 0.37 |
| `sd0, sd1, od0, od1` (`sd0` to `od1` mediated) | 180 | 0.27 |
| `sd0, sd1, each ok own domain` (`o1, o2` mediated) | 140 | 0.21 |
| Full Separation (|PSmin|) (each `ik` and `ok` is own domain) | 122 | 0.18 |

- **w(o, op)**: Object size; assume 4 Byte instructions.

### Figure 10: Privilege Metric Illustration

- **Instructions**: `i0, i1, i2, i3, i4, i5, i6, i7, i8`
- **Objects**: `o0 (8B), o1 (16B), o2 (4B), o3 (10B)`
- **Domains**: `od0, od1, sd0, sd1`
- **Dashed boxes**: Optional domain clusters

### Tests and Benchmarks

The Phoronix benchmarks used, along with the associated memory regions based on our object weighting model, are shown in Figure 11. The `.text` section in the compiled `vmlinux` binary represents the code. Globals include the combined final size of the global objects stored in the `.data`, `.rodata`, and `.bss` sections. The SLUB, Page, and VMalloc allocators show the average data size of live objects as discussed in our object weighting model. Stack memory is treated as a single object, with a size equal to the average number of live stack bytes across all kernel stacks. Memblock memory is physically allocated from the Memblock subsystem and treated as a single object with a size equal to the number of used pages. VMEMMAP corresponds to the size of the sparse virtual memory map structure used by Linux for fast translations. `gnupg` uses an unusually large amount of SLUB heap memory, identified as the Linux buffer cache.

We use the following LTP tests: admin tools, can, cap bounds, commands, connectors, containers, crypto, dio, fcntl locktests, filecaps, fs, fsbind, fs ext4, fs perms simple, fsx, hugetlb, input, io, kernel_misc, ltp aio stress, ltplite, math, mm, network commands, nptl, pipes, power management tests, sched, power management tests exclusive, quickhit, securebits, stress, syscalls, syscalls ipc, timers, and tpm tools.

### Performance Profiles

#### Page Table Process Protection
One way to provide separation is via the use of virtual memory. Subject Domains (SDs) are mapped to processes, and Object Domains (ODs) are mapped to contiguous, page-aligned regions of virtual memory, which may contain either static objects or serve as pools for dynamic allocations for that OD. Unmediated pages are mapped into the process page table according to their allowed permissions. Mediated accesses generate a trap to a supervisor, which performs the CAPMAP check and, if allowed, performs the operation or performs a context switch in the case of a cross-domain call or return. This is modeled by 6000 cycles for mediated operations based on rough estimates from [43] and [66]. A highly optimized microkernel context switch might be less expensive [25] and closer to some of the leaner options that follow.

#### EPT Protection
The `vmfunc` operation in modern Extended Page Tables (EPT) makes it less expensive to change page tables for an operation. Unmediated reads, writes, and frees, as well as internal calls, are directly mapped in the page table so they can complete with no overhead. External calls and returns make an explicit `vmfunc` call to perform the context switch; external mediated calls and returns perform a CAPMAP check in the `vmfunc` call [43, 48]. Mediated reads and writes will trap, check the CAPMAP, and perform the `vmfunc` operation from the trap, when allowed [64]. We model two traps (at 200 cycles each), two `vmfunc` calls (at 450 cycles each), and one CAPMAP lookup (at 200 cycles) for the total of 1500 cycles for a read, write, or free. For the call, we model one `vmfunc` call and one CAPMAP lookup. The `vmfunc` timing was calibrated by measuring the kernel overhead time for the Page Table protection implementation in the public release of xMP [57] and counting the number of added `vmfuncs`; thus, the 450-cycle figure represents the average cycles added per `vmfunc` including caching effects. These numbers are consistent with [64]. The raw number of cycles in the `vmfunc` is closer to 150, consistent with [39], but this doesn’t include the caching impact. We measure the 200 cycles per CAPMAP lookup from our hash implementation.

#### Software Fault Isolation (SFI)
In an SFI scheme [26, 49, 55], we can check any potentially unmediated read, write, or free access with an inline code monitor. We model two cases: one standard (baseline) and one optimized based on information in the CAPMAP (optimized). When we know from the CAPMAP that a small number of unmediated objects is accessed from a particular instruction, these can be checked quickly with specialized, inline base and bounds checks [55]. Our tracing shows that the dynamic distribution of accessed objects is highly skewed (call entropy is 0.041, read/write entropy is 0.265); this means the checks can be constructed in a tree organized like a Huffman encoding. Our optimized model includes the cost for a single check in the average case. Mediated accesses can be checked with a hash table lookup. External calls and returns can be wrapped with a springboard to permit only CAPMAP-allowed operations and change context information when a call or return changes domains [73].

#### Capability Hardware
Capability hardware can restrict operations without requiring virtual memory and hence page table changes [71]. Mediated read and write operations can use capability pointers. Mediated calls and returns still require some time to check the CAPMAP and change capabilities, but this can be less expensive than a traditional OS context switch. We take the 600-cycle estimate from Tab. 2 in [71]. In the best case, mediated reads and writes can use the capability pointers, but may require additional cycles to select and load capability pointers. Unmediated operations on a single OD can use the capability bounds check to eliminate per-reference costs.

#### Direct Hardware Support
If we were to design hardware directly to support the CAPMAP, it could look like the HardBound hardware hash mechanism [21] or a cached tag rule checking mechanism [22]. That is, on every operation, the hardware uses the program counter and the address of the object to consult a hardware-cached hash of the CAPMAP. Hits to the cache will add no overhead, while misses will incur a few cycles to fetch replacement entries for the CAPMAP cache. We use 10 cycles as a crude estimate for the average time of a reference, considering most references will take 0 time, but 5% of references may take 200 cycles. Since the miss rate will decrease with cluster size, assuming this high, fixed miss rate will make overhead results increasingly pessimistic with increasing cluster size.

### Figure 11: Data Type and Size (MB)

- **VMEMMAP (1.1%)**
- **Memblock (0.5%)**
- **VMalloc (27.6%)**
- **Stack (0.1%)**
- **Page Allocator (16.0%)**
- **SLUB Allocator (8.2%)**
- **Globals (14.1%)**
- **Code (32.4%)**

### Benchmarks

- **bulletbyte**
- **c-ray**
- **compress-7zip**
- **compress-lzma**
- **compress-pbzip2**
- **crafty**
- **encode-mp3**
- **ffmpeg**
- **gmpbench**
- **gnupg**
- **graphics-magick**
- **hime-no**
- **hmmer**
- **john-the-ripper**
- **mafft**
- **openssl**
- **pybench**
- **smallpt**
- **sqlite**
- **tachyon**
- **x264**

### RAID ’21, October 6–8, 2021, San Sebastian, Spain

- **Roessler and Dautenhahn, et al.**