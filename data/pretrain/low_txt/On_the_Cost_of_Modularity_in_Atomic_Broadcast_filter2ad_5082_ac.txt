### Latency and Throughput Analysis of Monolithic and Modular Implementations

#### Latency Comparison
The latency of both the monolithic and modular implementations is relatively close for small offered loads. However, as the offered load increases, the monolithic implementation demonstrates significantly lower latencies. Specifically, the monolithic implementation achieves latencies that are 30% (for n = 7) to 50% (for n = 3) lower than the modular implementation. It is important to note that the latency of both implementations remains relatively constant above a certain offered load. This stability is attributed to the flow-control mechanisms present in both stacks, which block more abcast messages as the offered load increases, thereby maintaining a consistent network load.

#### Early Latency vs. Message Size
Figure 9 illustrates the early latency of the two implementations as a function of the message size, with an offered load fixed at 2000 msgs/s. The graph shows the early latency for systems with n = 3 (two bottom curves) and n = 7 (two top curves) processes. The results are consistent with other values of offered load, except for very small values where no significant differences are observed.

For small message sizes (up to 4096 bytes for n = 7 and 8192 bytes for n = 3), the monolithic implementation achieves latencies approximately 50% lower than the modular implementation. As the message size increases, the early latency also increases due to the total amount of data that needs to be exchanged. For the largest messages, the monolithic implementation still outperforms the modular one, achieving latencies that are 25% (n = 7) or 35% (n = 3) lower.

#### Throughput of Atomic Broadcast
Next, we examine the throughput achieved by the modular and monolithic implementations of atomic broadcast. Figure 10 shows the relationship between the throughput (on the vertical axis) and the offered load (on the horizontal axis) for atomic broadcast messages of 16384 bytes. When the offered load is small (less than 500 msgs/s), the throughput is equal to the offered load. As the offered load increases, the flow-control mechanism limits the achievable throughput, causing it to plateau. For high offered loads, the monolithic implementation sustains a throughput that is 25% (n = 7) to 30% (n = 3) higher than the modular implementation. For low offered loads, the difference between the two implementations is negligible.

Figure 11 presents the throughput as a function of the message size, with the offered load fixed at 2000 msgs/s. For small message sizes, the monolithic implementation achieves 10% to 15% higher throughputs than the modular one, and this throughput remains constant up to message sizes of 4096 bytes for n = 7 and 16384 bytes for n = 3. Surprisingly, the throughput is higher when n = 7 processes participate in the system compared to n = 3. This is due to the flow-control mechanism, which allows a larger number of undelivered abcast messages to circulate as the number of processes grows.

As the message size increases, the throughput of the system with n = 7 processes degrades faster than in the case of n = 3. This degradation is due to the consensus proposal, which contains large messages and must be sent to all processes in the system. As both the message size and the number of processes increase, sending these large proposals results in an overall lower throughput (in msgs/s).

### Discussion
From the results, it is evident that the performance difference between a modular and a monolithic implementation of the same distributed protocol is significant. The latency difference can be up to 50%, while the throughput difference varies between 10% and 25%. This cost must be considered when choosing between a modular system, which is easier to maintain and update, and a monolithic system, which has better performance characteristics.

It is also interesting to note that the experimental results do not always align with the analysis in Section 5.2. These two sets of results are complementary. The analytical evaluation focuses solely on the messages exchanged by the algorithm, while the experimental results are influenced by processing costs and resource contention. For instance, 99% of CPU resources were used with an offered load greater than 500 msgs/s. The discrepancy between the analytical and experimental evaluations stems from these elements, which are difficult to estimate a priori.

### Related Work
Several group communication toolkits have implemented atomic broadcast over the past two decades. Early implementations, such as Isis, Phoenix, and Totem, were designed with a monolithic architecture. More recent systems, including Horus, Ensemble, Transis, JavaGroups, Eden, and Fortika, present a modular design. A comparison of these toolkits from an architectural perspective can be found in [20]. However, the issue of performance overhead induced by modularity has not been extensively covered.

In Ensemble, performance was improved through various techniques, such as optimizing interfacing code, improving header formats, and compressing them. Appia, inspired by Ensemble, furthered these techniques. While these improvements significantly enhanced performance, they are general lower-level solutions that do not alter the algorithms. In contrast, our algorithmic improvements cannot be applied to Ensemble or Appia, where atomic broadcast relies on group membership to avoid blocking.

In a broader context, there is extensive work on protocol layer optimization. For example, the x-Kernel modular system was improved with techniques like protocol multiplexing. Standard compilation techniques combined with code annotations can optimize frequently executed functions. Another technique, Application Level Framing, ensures that protocols know the typical size of application messages to avoid unnecessary fragmentation. These techniques treat protocols as black boxes and do not involve modifications to the protocol logic, making them easily combinable with the ones proposed in this paper.

Modularity is essential for achieving good performance in parallel computing and concurrent programming. However, this is not applicable to our work, as very few tasks can be parallelized in atomic broadcast: only message diffusion and ordering can be executed concurrently.

### Conclusion
This paper presents two versions (monolithic and modular) of a complex protocol: atomic broadcast. We demonstrated that a monolithic stack allows several algorithmic optimizations, primarily due to the interdependence of consensus instances and the possibility for different modules to share their state. Analytical and experimental evaluations quantified the gains from these optimizations. Our analytical evaluation concluded that a monolithic implementation significantly reduces the number of messages sent over the network. Experimentally, the modular version incurs an overhead that reaches 50% under the worst workload conditions.

In summary, while a modular design is often preferred for its maintainability, our results show that if performance is a critical concern, a monolithic approach may be more suitable.

### Acknowledgments
We would like to thank the anonymous reviewers for their comments and helpful suggestions.

### References
[1] L. Alvisi and K. Marzullo. Waft: Support for fault-tolerance in wide-area object-oriented systems. In Proc. of the 2nd Information Survivability Workshop – ISW ’98, pages 5–10. IEEE Computer Society Press, October 1998.
[2] Y. Amir, L. E. Moser, P. M. Melliar-Smith, D. A. Agarwal, and P. Ciarfella. The Totem single-ring ordering and membership protocol. ACM Trans. on Computer Systems, 13(4):311–342, Nov. 1995.
[3] B. Ban. JavaGroups 2.0 User’s Guide, Nov 2002.
[4] N. T. Bhatti, M. A. Hiltunen, R. D. Schlichting, and W. Chiu. Coyote: A system for constructing fine-grain configurable communication services. ACM Trans. on Computer Systems, 16(4):321–366, Nov. 1998.
[5] K. P. Birman. The process group approach to reliable distributed computing. Comm. ACM, 36(12):36–53, Dec. 1993.
[6] K. P. Birman and T. A. Joseph. Reliable communication in presence of failures. ACM Trans. on Computer Systems, 5(1):47–76, Feb. 1987.
[7] T. D. Chandra and S. Toueg. Unreliable failure detectors for reliable distributed systems. Journal of ACM, 43(2):225–267, Mar. 1996.
[8] G. Chockler, I. Keidar, and R. Vitenberg. Group communication specifications: A comprehensive study. ACM Computing Surveys, 33(4):427–469, May 2001.
[9] D. D. Clark and D. L. Tennenhouse. Architectural considerations for a new generation of protocols. In SIGCOMM ’90: Proceedings of the ACM symposium on Communications architectures & protocols, pages 200–208, New York, NY, USA, 1990. ACM Press.
[10] J. Crowcroft, J. Wakeman, Z. Wang, and D. Sirovica. Is Layering Harmful? IEEE Network 6(1992) 1 pp. 20-24. IEEE Network 6(1992) 1 pp. 20-24, 1992.
[11] D. Dolev and D. Malkhi. The Transis approach to high availability cluster communication. Comm. ACM, 39(4):64–70, Apr. 1996.
[12] R. Ekwall and A. Schiper. Solving atomic broadcast with indirect consensus. In 2006 IEEE International Conference on Dependable Systems and Networks (DSN 2006), 2006.
[13] V. Hadzilacos and S. Toueg. A modular approach to fault-tolerant broadcasts and related problems. TR 94-1425, Dept. of Computer Science, Cornell University, Ithaca, NY, USA, May 1994.
[14] M. Hayden. The Ensemble system. Technical Report TR98-1662, Dept. of Computer Science, Cornell University, Jan. 8, 1998.
[15] M. Hurfin, R. Macêdo, M. Raynal, and F. Tronel. A general framework to solve agreement problems. In Proceedings of the 18th Symposium on Reliable Distributed Systems (SRDS), pages 56–67, Lausanne, Switzerland, Oct. 1999.
[16] L. V. Kale. Performance and productivity in parallel programming via processor virtualization. In Proc. of the First Intl. Workshop on Productivity and Performance in High-End Computing (at HPCA 10), Madrid, Spain, February 2004.
[17] C. P. Malloth. Conception and Implementation of a Toolkit for Building Fault-Tolerant Distributed Applications in Large Scale Networks. PhD thesis, École Polytechnique Fédérale de Lausanne, Switzerland, Sept. 1996.
[18] S. Mena, X. Cuvellier, C. Grégoire, and A. Schiper. Appia vs. cactus: Comparing protocol composition frameworks. In Proc. of 22th IEEE Symposium on Reliable Distributed Systems (SRDS’03), Florence, Italy, Oct. 2003.
[19] S. Mena, O. Rütti, and A. Schiper. Fortika: Robust Group Communication. EPFL, Laboratoire de Systèmes Répartis, May 2006.
[20] S. Mena, A. Schiper, and P. T. Wojciechowski. A step towards a new generation of group communication systems. In Proc. of Conference on Middleware, Rio de Janeiro, Brasil, June 2003.
[21] H. Miranda, A. Pinto, and L. Rodrigues. Appia: A flexible protocol kernel supporting multiple coordinated channels. In 21st Int’l Conf. on Distributed Computing Systems (ICDCS’ 01), pages 707–710, Washington - Brussels - Tokyo, Apr.16–19 2001.
[22] D. Mosberger, L. L. Peterson, P. G. Bridges, and S. O’Malley. Analysis of techniques to improve protocol processing latency. In SIGCOMM ’96: Conference proceedings on Applications, technologies, architectures, and protocols for computer communications, pages 73–84, New York, NY, USA, 1996. ACM Press.
[23] L. Peterson, N. Hutchinson, S. O’Malley, and M. Abbott. RPC in the x-kernel: evaluating new design techniques. In SOSP ’89: Proceedings of the twelfth ACM symposium on Operating systems principles, pages 91–101, New York, NY, USA, 1989. ACM Press.
[24] The University of Arizona, Computer Science Department. The Cactus Project. Available electronically at http://www.cs.arizona.edu/cactus/.
[25] P. Urbán. Evaluating the Performance of Distributed Agreement Algorithms: Tools, Methodology and Case Studies. École Polytechnique Fédérale de Lausanne, PhD thesis, Switzerland, Aug. 2003. Number 2824.
[26] R. van Renesse. Masking the overhead of protocol layering. In SIGCOMM ’96: Conference proceedings on Applications, technologies, architectures, and protocols for computer communications, pages 96–104, New York, NY, USA, 1996. ACM Press.
[27] R. van Renesse, K. P. Birman, B. B. Glade, K. Guo, M. Hayden, T. Hickey, D. Malki, A. Vaysburd, and W. Vogels. Horus: A flexible group communications system. Technical Report TR95-1500, Dept. of Computer Science, Cornell University, Ithaca, NY, USA, Apr. 1996.