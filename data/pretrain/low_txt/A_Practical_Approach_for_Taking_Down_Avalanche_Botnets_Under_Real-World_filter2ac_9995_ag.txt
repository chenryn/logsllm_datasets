### References

1. A. Noroozian, D. Bagley, and C. Hesselman, “Cybercrime after the sunrise: A statistical analysis of DNS abuse in new gTLDs,” in *13th ACM Asia Conference on Computer and Communications Security (ASIACCS '18)*, 2018, pp. 609–623.

2. A. Kountouras, P. Kintis, C. Lever, Y. Chen, Y. Nadji, D. Dagon, M. Antonakakis, and R. Joffe, “Enabling network security through active DNS datasets,” in *Research in Attacks, Intrusions, and Defenses (RAID '16)*, 2016, pp. 188–208.

3. S. Krishnan, T. Taylor, F. Monrose, and J. McHugh, “Crossing the threshold: Detecting network malfeasance via sequential hypothesis testing,” in *43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN '13)*, 2013.

4. M. Kührer, C. Rossow, and T. Holz, “Paint it black: Evaluating the effectiveness of malware blacklists,” in *17th International Symposium on Research in Attacks, Intrusions and Defenses (RAID '14)*, 2014, pp. 1–21.

5. B. Laurie, A. Langley, and E. Kasper, “Certificate Transparency,” *Internet Requests for Comments (RFC Editor, RFC 6962)*, June 2013.

6. V. Le Pochat, T. Van Goethem, S. Tajalizadehkhoob, M. Korczyński, and W. Joosen, “Tranco: A research-oriented top sites ranking hardened against manipulation,” in *26th Annual Network and Distributed System Security Symposium (NDSS '19)*, 2019.

7. C. Lever, R. Walls, Y. Nadji, D. Dagon, P. McDaniel, and M. Antonakakis, “Domain-Z: 28 registrations later measuring the exploitation of residual trust in domains,” in *2016 IEEE Symposium on Security and Privacy (SP '16)*, 2016, pp. 691–706.

8. P. Lison and V. Mavroeidis, “Neural reputation models learned from passive DNS data,” in *2017 IEEE International Conference on Big Data (Big Data '17)*, 2017, pp. 3662–3671.

9. S. Liu, I. Foster, S. Savage, G. M. Voelker, and L. K. Saul, “Who is .com?: Learning to parse WHOIS records,” in *2015 Internet Measurement Conference (IMC '15)*, 2015, pp. 369–380.

10. J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond blacklists: Learning to detect malicious web sites from suspicious URLs,” in *15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '09)*, 2009, pp. 1245–1254.

11. L. Machlica, K. Bartos, and M. Sofka, “Learning detectors of malicious web requests for intrusion detection in network traffic,” arXiv:1702.02530, Feb. 2017.

12. L. B. Metcalf, D. Ruef, and J. M. Spring, “Open-source measurement of fast-flux networks while considering domain-name parking,” in *2017 Learning from Authoritative Security Experiment Results Workshop (LASER '17)*, 2017, pp. 13–24.

13. B. Morton, “Protect your domain with CT search,” [Online]. Available: https://www.entrustdatacard.com/blog/2016/october/protect-your-domain-with-ct-search, Oct. 2016.

14. M. Mowbray and J. Hagen, “Finding domain-generation algorithms by looking at length distribution,” in *2014 IEEE International Symposium on Software Reliability Engineering Workshops*, 2014, pp. 395–400.

15. Y. Nadji, M. Antonakakis, R. Perdisci, D. Dagon, and W. Lee, “Beheading hydras: Performing effective botnet takedowns,” in *2013 ACM SIGSAC Conference on Computer and Communications Security (CCS '13)*, 2013, pp. 121–132.

16. S. Pal, “Sinkholed,” [Online]. Available: https://susam.in/blog/sinkholed/, Dec. 2019.

17. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and Édouard Duchesnay, “Scikit-learn: Machine learning in Python,” *Journal of Machine Learning Research*, vol. 12, pp. 2825–2830, 2011.

18. M. Pereira, S. Coleman, B. Yu, M. De Cock, and A. C. A. Nascimento, “Dictionary extraction and detection of algorithmically generated domain names in passive DNS traffic,” in *21st International Symposium on Research in Attacks, Intrusions, and Defenses (RAID '18)*, 2018, pp. 295–314.

19. N. Petit, “Artificial intelligence and automated law enforcement: A review paper,” *SSRN Electronic Journal*, 2018. [Online]. Available: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3145133

20. D. Piscitello, “ICANN GDPR and WHOIS users survey. A joint survey by the Anti-Phishing Working Group (APWG) and the Messaging, Malware and Mobile Anti-Abuse Working Group (M3AAWG),” [Online]. Available: https://www.m3aawg.org/sites/default/files/m3aawg-apwg-whois-user-survey-report-2018-10.pdf, Oct. 2018.

21. D. Plohmann, K. Yakdan, M. Klatt, J. Bader, and E. Gerhards-Padilla, “A comprehensive measurement study of domain generating malware,” in *25th USENIX Security Symposium (USENIX Security '16)*, 2016, pp. 263–278.

22. M. Z. Raﬁque, T. Van Goethem, W. Joosen, C. Huygens, and N. Nikiforakis, “It’s free for a reason: Exploring the ecosystem of free live streaming services,” in *23rd Annual Network and Distributed System Security Symposium (NDSS '16)*, 2016.

23. Rapid7, “Project Sonar,” [Online]. Available: https://www.rapid7.com/research/project-sonar/

24. S. Rodota, “Opinion 2/2003 on the application of the data protection principles to the Whois directories,” *Article 29 Data Protection Working Party*, Jun. 2003. [Online]. Available: https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2003/wp76_en.pdf

25. Q. Scheitle, O. Gasser, T. Nolte, J. Amann, L. Brent, G. Carle, R. Holz, T. C. Schmidt, and M. Wählisch, “The rise of certificate transparency and its implications on the Internet ecosystem,” in *2018 Internet Measurement Conference (IMC '18)*, 2018, pp. 343–349.

26. Q. Scheitle, O. Hohlfeld, J. Gamba, J. Jelten, T. Zimmermann, S. D. Strowes, and N. Vallina-Rodriguez, “A long way to the top: Significance, structure, and stability of Internet top lists,” in *2018 Internet Measurement Conference (IMC '18)*, 2018, pp. 478–493.

27. S. Schiavoni, F. Maggi, L. Cavallaro, and S. Zanero, “Phoenix: DGA-based botnet tracking and intelligence,” in *11th International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment (DIMVA '14)*, 2014, pp. 192–211.

28. S. Schüppen, D. Teubert, P. Herrmann, and U. Meyer, “FANCI: Feature-based automated NXDomain classification and intelligence,” in *27th USENIX Security Symposium (USENIX Security '18)*, 2018, pp. 1165–1181.

29. D. Schwarz, “Bedep’s DGA: Trading foreign exchange for malware domains,” Arbor Networks, Apr. 2015. [Online]. Available: https://web.archive.org/web/20160114122355/https://asert.arbornetworks.com/bedeps-dga-trading-foreign-exchange-for-malware-domains/

30. R. Shirazi, “Botnet takedown initiatives: A taxonomy and performance model,” *Technology Innovation Management Review*, vol. 5, no. 1, pp. 15–20, Jan. 2015.

31. S. Sinha, M. Bailey, and F. Jahanian, “Shades of grey: On the effectiveness of reputation-based ‘blacklists’,” in *3rd International Conference on Malicious and Unwanted Software (MALWARE '08)*, 2008, pp. 57–64.

32. R. Sivaguru, C. Choudhary, B. Yu, V. Tymchenko, A. Nascimento, and M. De Cock, “An evaluation of DGA classifiers,” in *2018 IEEE International Conference on Big Data (Big Data '18)*, 2018, pp. 5058–5067.

33. K. Soska and N. Christin, “Automatically detecting vulnerable websites before they turn malicious,” in *23rd USENIX Security Symposium (USENIX Security '14)*, 2014, pp. 625–640.

34. A. Sperotto, O. van der Toorn, and R. van Rijswijk-Deij, “TIDE: Threat identification using active DNS measurements,” in *Proceedings of the SIGCOMM Posters and Demos (SIGCOMM Posters and Demos '17)*, 2017, pp. 65–67.

35. J. Spooren, D. Preuveneers, L. Desmet, P. Janssen, and W. Joosen, “Detection of algorithmically generated domain names used by botnets: A dual arms race,” in *34th ACM/SIGAPP Symposium on Applied Computing (SAC '19)*, 2019, pp. 1916–1923.

36. J. Spooren, T. Vissers, P. Janssen, W. Joosen, and L. Desmet, “Premadoma: An operational solution for DNS registries to prevent malicious domain registrations,” in *35th Annual Computer Security Applications Conference (ACSAC '19)*, 2019, pp. 557–567.

37. M. Stampar, “Email addresses used in WHOIS registrations of sinkholed malicious/malware domains,” [Online]. Available: https://gist.github.com/stamparm/9726d93fd0048aee6c54ec88a8e85bfc, Oct. 2018.

38. M. Stampar et al., “Maltrail: Malicious traffic detection system,” [Online]. Available: https://github.com/stamparm/maltrail, 2019.

39. M. Stevanovic, J. M. Pedersen, A. D’Alconzo, S. Ruehrup, and A. Berger, “On the ground truth problem of malicious DNS traffic analysis,” *Computers & Security*, vol. 55, pp. 142–158, 2015.

40. J. Tierney, “Do you suffer from decision fatigue?” *New York Times*, Aug. 2011. [Online]. Available: https://www.nytimes.com/2011/08/21/magazine/do-you-suffer-from-decision-fatigue.html

41. R. van Rijswijk-Deij, M. Jonker, A. Sperotto, and A. Pras, “A high-performance, scalable infrastructure for large-scale active DNS measurements,” *IEEE Journal on Selected Areas in Communications*, vol. 34, no. 6, pp. 1877–1888, June 2016.

42. B. VanderSloot, J. Amann, M. Bernhard, Z. Durumeric, M. Bailey, and J. A. Halderman, “Towards a complete view of the certificate ecosystem,” in *2016 Internet Measurement Conference (IMC '16)*, 2016, pp. 543–549.

43. T. Vissers, J. Spooren, P. Agten, D. Jumpertz, P. Janssen, M. V. Wesemael, F. Piessens, W. Joosen, and L. Desmet, “Exploring the ecosystem of malicious domain registrations in the .eu TLD,” in *Proceedings of the 20th International Symposium on Research in Attacks, Intrusions, and Defenses (RAID '17)*, 2017, pp. 472–493.

44. R. Wainwright and F. J. Cilluffo, “Responding to cybercrime at scale: Operation Avalanche and homeland security for cyber case study,” *Europol; Center for Cyber and Homeland Security, The George Washington University, Issue Brief 2017-03*, Mar. 2017. [Online]. Available: https://cchs.gwu.edu/sites/g/files/zaxdzs2371/f/Responding%20to%20Cybercrime%20at%20Scale%20FINAL.pdf

45. G. Widmer and M. Kubat, “Learning in the presence of concept drift and hidden contexts,” *Machine Learning*, vol. 23, no. 1, pp. 69–101, Apr. 1996.

46. J. Woodbridge, H. S. Anderson, A. Ahuja, and D. Grant, “Predicting Domain Generation Algorithms with Long Short-Term Memory Networks,” arXiv:1611.00791, Nov. 2016.

47. W. Xu, K. Sanders, and Y. Zhang, “We know it before you do: Predicting malicious domains,” in *Virus Bulletin Conference*, Sep. 2014, pp. 73–77.

48. S. Yadav and A. L. N. Reddy, “Winning with DNS failures: Strategies for faster botnet detection,” in *7th International ICST Conference on Security and Privacy in Communication Networks (SecureComm '11)*, 2011, pp. 446–459.

49. S. Yadav, A. K. K. Reddy, A. N. Reddy, and S. Ranjan, “Detecting algorithmically generated malicious domain names,” in *10th ACM SIGCOMM Conference on Internet Measurement (IMC '10)*, 2010, pp. 48–61.

50. ——, “Detecting algorithmically generated domain-flux attacks with DNS traffic analysis,” *IEEE/ACM Transactions on Networking*, vol. 20, no. 5, pp. 1663–1677, Oct. 2012.

51. B. Z. H. Zhao, M. Ikram, H. J. Asghar, M. A. Kaafar, A. Chaabane, and K. Thilakarathna, “A decade of mal-activity reporting: A retrospective analysis of Internet malicious activity blacklists,” in *14th ACM Asia Conference on Computer and Communications Security (ASIACCS '19)*, 2019, pp. 193–205.

52. Y. Zhauniarovich, I. Khalil, T. Yu, and M. Dacier, “A survey on malicious domains detection through DNS data analysis,” *ACM Computing Surveys*, vol. 51, no. 4, pp. 67:1–67:36, Jul. 2018.

### Appendix A: Machine Learning Protocol

Machine learning algorithms are trained on a training set \( T_r \) and evaluated on a test set \( T_e \). As explained in Section V, if we need to train and test on the same iteration, we use a k-fold cross-validation procedure: the data is split into \( k \) folds, with each fold being used once as the test set while the remaining \( k-1 \) folds are used for training. We then average the results over \( k \) experiments. We set \( k \) to 10. The advantage of using cross-validation is that it reduces bias in the composition of the selected training and test sets, even with a relatively small dataset.

Most machine learning algorithms have different hyperparameters to tune. Tuning on the test set would lead to highly biased results. Therefore, we split the training set \( T_r \) into a set for training \( T_r' \) and another one for validation \( V \). We again use a 10-fold cross-validation procedure. We treat and calculate the upper and lower bounds for the extended a posteriori model as hyperparameters.

We evaluate the following performance metrics over the test set:

- **Accuracy**:
  \[
  \text{Accuracy} = \frac{tp + tn}{tp + tn + fp + fn}
  \]

- **Precision**:
  \[
  \text{Precision} = \frac{tp}{tp + fp}
  \]

- **Recall**:
  \[
  \text{Recall} = \frac{tp}{tp + fn}
  \]

- **F1 Score**:
  \[
  \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
  \]

where \( tp \), \( tn \), \( fp \), and \( fn \) stand for the number of true positives, true negatives, false positives, and false negatives, respectively. Malicious domains are considered positive, and benign domains are negative. Precision represents the fraction of samples identified as malicious that are actually malicious, while recall represents the fraction of malicious samples that were correctly identified. The F1 score summarizes these two metrics and is a superior metric compared to accuracy when dealing with unbalanced datasets, therefore, we optimize for it.

### Appendix B: Evaluation of Machine Learning Algorithms

Table IX presents the performance metrics of the machine learning algorithms evaluated in Section V-B, for a base ensemble model trained and tested on the initial 2017 iteration. The results show that gradient boosted trees consistently outperform the other ML algorithms.

| Metric          | Decision Tree | Gradient Boosted Tree | Random Forest | Support Vector Machine |
|-----------------|---------------|-----------------------|---------------|------------------------|
| Accuracy        | 88.6%         | 86.6%                 | 87.8%         | 87.2%                  |
| Recall          | 93.4%         | 92.7%                 | 92.6%         | 92.6%                  |
| Precision       | 92.8%         | 92.6%                 | 91.5%         | 92.0%                  |
| F1 Score        | 86.4%         | 77.9%                 | 90.6%         | 83.8%                  |

### Handling Missing Feature Values

Due to the incompleteness of our datasets (e.g., WHOIS records not containing a parseable phone number), certain domains have missing feature values. We impute them (i.e., substitute them with plausible values to avoid bias) as follows (the feature numbers correspond to those defined in Section IV-C):

- **No Wayback Machine data**: Feature values (3-5) are set to zero, as no data means that the Wayback Machine has not found any page on the domain, suggesting unpopularity.
- **No WHOIS timestamps**: Feature values (11-14) are set to the mean, as no data implies that data could not be parsed or retrieved, not that the data does not exist (e.g., all domains have a registration date). By using the mean, we do not attach any statistical meaning to the absence of data and do not skew the distribution.
- **Less than two WHOIS records**: The renewal feature (15) gets a third value that indicates that only one historical WHOIS record was available (preventing a comparison of expiration dates).
- **No WHOIS registrant records**: Features that rely on an address, an email address, or a phone number (16-18) get a third value that indicates that we do not have a value for the corresponding field.
- **No passive or active DNS data**: Continuous feature values (19-22, 30-36) are set to zero, and binary feature values (23-29) to false, as no data means that DNS records for the domain were never queried, suggesting unpopularity.