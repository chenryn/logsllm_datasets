### Solution [6]
In this section, we maintained the same settings as those described in the original work [13] and modified the optimization objective to find a trigger that significantly lowers the test statistic \( J^* \) of the target class. Specifically, starting from a randomly sampled trigger, our experiment iteratively performed the following steps until \( J^* \) fell below the threshold \( \exp(2) \) or a predetermined number of iterations (10,000) was reached:
1. Inject images with the current disturbed trigger using TaCT.
2. Train the target model on the infected dataset.
3. Run SCAn to obtain \( J^* \).
4. Calculate the derivative according to the \( J^* \) of the target class.
5. Update the trigger by subtracting the derivative.

The experiment was conducted on the GTSRB dataset, as training a model on this dataset takes only several minutes. However, even after 10,000 iterations, which took a month on a two-GPU system, the approach failed to reduce \( J^* \) in a meaningful way, as illustrated in Figure 19. The figure shows that not only did \( J^* \) not decrease, but the norm of the trigger (32x32 images with pixels in [0,1]) also remained unchanged during the iterations. This indicates that the derivative algorithm used was unable to find a trigger capable of bypassing SCAn.

### 5. Discussion
#### Limitations of SCAn
As mentioned earlier, SCAn utilizes a set of clean data for contamination analysis. We believe this requirement is reasonable, as a small clean dataset is often provided by the model provider for testing the model’s performance, as assumed in prior studies [8, 9]. Note that the size of this clean dataset can be just 1% of the training set for defeating attacks involving up to 8 triggers (Section 4.5). Additionally, our approach relies on the presence of attack images (carrying triggers) to identify an infected class. We have only evaluated SCAn on image classification tasks, but we believe there is potential to extend our approach to mitigate threats posed by backdoors using non-image triggers. Our insight behind SCAn is that globally statistical information about a model’s representations can help untangle specific classes. This information is described using the covariance matrix \( S_\epsilon \) of a multivariate normal distribution, which effectively distinguishes different classes. Since such representations also characterize some non-vision tasks, such as code analysis, our modeling could potentially apply to identifying Trojaned inputs in these tasks. Further exploration in this direction is left to future research.

### Future Research
In future work, we will seek more efficient techniques to untangle mixed representations, such as using deep learning with GPU acceleration and more precise approximations for specific tasks. An interesting observation from our experiments on MegaFace is that classes containing both baby and adult images have a higher \( J^* \) than other normal classes, even though this anomaly is still well below those of infected classes. This may indicate that our method could help mine hard-negative examples for evaluating a DNN model’s classification quality.

### 6. Related Works
We present a new protection mechanism, SCAn, designed to defeat our attack, TaCT, which injects source-specific triggers into the target model. Such triggers have been briefly mentioned in NC [42] and STRIP [9], without details on how to launch the attack. The NC paper discussed the potential to detect source-specific triggers when running NC \( O(N \log_2 N) \) rounds for \( N \) classes. We argue that the computational complexity increases to \( O(N^2) \) in the presence of TaCT, given that NC’s recall is just 6.5% on TaCT, as demonstrated in Section 3.2. As a result, the divide-and-conquer algorithm cannot be used to reduce the complexity, making the approach less practical when \( N \) is large (Table 7). In contrast, SCAn defeats TaCT with a complexity of \( O(N) \) by testing each class once. Liu et al. [22] proposed the fine-pruning method, which prunes dormant neurons and then fine-tunes the pruned model to recover accuracy. Their defense relies on extensive interactions with the training process, whereas our approach only needs to go through the dataset in two rounds and is independent of the target model's training. Other related approaches, as discussed in Section 3.2, are all defeated by TaCT, with SCAn being the only solution working against the attack. Nelson et al. [28] and Baracaldo et al. [2] proposed general protections against backdoor attacks, which require extensive retraining of the model on datasets similar in size to the original one, often infeasible for DNNs. Additionally, they detect infected data by evaluating the overall performance of the model, which remains good under advanced attacks like TaCT, rendering these methods ineffective. In traditional statistical analysis, Victoria et al. [12] summarized effective outlier detection methods, including k-nearest neighbors (k-nn) [14], k-means [26], and principal components analysis (PCA) [17]. To determine if these methods can detect infected classes, we applied them to the representations produced by a TaCT-infected model for the images in the target class. The results in Figure 21 show that these methods cause many false positives.

### 7. Conclusion
Our work demonstrated that backdoors created by conventional data poisoning attacks are source-agnostic and characterized by unique representations generated for attack images, which are mostly determined by the trigger, regardless of other image content, and clearly distinguishable from those for normal images. Four existing detection techniques rely on these properties and fail to raise the bar against black-box attacks injecting source-specific backdoors like TaCT. Based on leveraging the distribution of sample representations through a two-component model, we designed a statistical method, SCAn, to untangle representations of each class into a mixture model and utilized a likelihood ratio test to detect an infected class. The effectiveness and robustness of SCAn were demonstrated through extensive experiments. Our study advances the understanding of the mechanisms of implanting a backdoor within a DNN model and how a backdoor appears from the perspective of model representations, potentially leading to a deeper understanding of neural networks.

### Acknowledgment
We thank our anonymous reviewers for their comprehensive feedback. This work was supported in part by the General Research Funds (Project No. 14208019) established under the University Grant Committee of the Hong Kong SAR, the Chinese University of Hong Kong research contract agreement (Contract No. TS1711490), and the IARPA (Grant No. W91NF-20-C-0034) TrojAI project.

### References
[1] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. CoRR, abs/1807.00459, 2018.
[2] Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and Jaehoon Amir Safavi. Mitigating poisoning attacks on machine learning models: A data provenance based approach. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 103–110. ACM, 2017.
[3] George EP Box, William Gordon Hunter, J Stuart Hunter, et al. Statistics for experimenters, volume 664. John Wiley and Sons New York, 1978.
[4] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In Workshop on Artificial Intelligence Safety 2019 co-located with the Thirty-Third AAAI Conference on Artificial Intelligence 2019 (AAAI-19), Honolulu, Hawaii, January 27, 2019., 2019.
[5] Dong Chen, Xudong Cao, Liwei Wang, Fang Wen, and Jian Sun. Bayesian face revisited: A joint formulation. In Computer Vision - ECCV 2012 - 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part III, pages 566–579, 2012.
[6] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad Miller, and Arunesh Sinha, editors, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pages 15–26. ACM, 2017.
[17] Flip Korn, Alexandros Labrinidis, Yannis Kotidis, Christos Faloutsos, Alex Kaplunovich, and Dejan Perkovic. Quantifiable data mining using principal component analysis. Technical report, 1998.
[7] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. CoRR, abs/1712.05526, 2017.
[8] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan Boneh. Sentinel: Detecting physical attacks against deep learning systems. CoRR, abs/1812.00292, 2018.
[9] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith Chinthana Ranasinghe, and Surya Nepal. STRIP: A defence against trojan attacks on deep neural networks. In David Balenson, editor, Proceedings of the 35th Annual Computer Security Applications Conference, ACSAC 2019, San Juan, PR, USA, December 09-13, 2019, pages 113–125. ACM, 2019.
[10] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. CoRR, abs/1708.06733, 2017.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[12] Victoria J. Hodge and Jim Austin. A survey of outlier detection methodologies. Artif. Intell. Rev., 22(2):85–126, 2004.
[13] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2142–2151. PMLR, 2018.
[14] Edwin M Knox and Raymond T Ng. Algorithms for mining distance-based outliers in large datasets. In Proceedings of the international conference on very large data bases, pages 392–403. Citeseer, 1998.
[15] Karl-Rudolf Koch. Parameter estimation and hypothesis testing in linear models. Springer, 1988.
[16] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1885–1894. JMLR. org, 2017.
[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
[19] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[20] Christophe Leys, Christophe Ley, Olivier Klein, Philippe Bernard, and Laurent Licata. Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median. Journal of Experimental Social Psychology, 49(4):764–766, 2013.
[21] Zhengxiong Li, Aditya Singh Rathore, Chen Song, Sheng Wei, Yanzhi Wang, and Wenyao Xu. PrinTracker: Fingerprinting 3D printers using commodity scanners. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, pages 1306–1323. ACM, 2018.
[22] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Michael Bailey, Thorsten Holz, Manolis Stamatogiannakis, and Sotiris Ioannidis, editors, Research in Attacks, Intrusions, and Defenses - 21st International Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings, volume 11050 of Lecture Notes in Computer Science, pages 273–294. Springer, 2018.
[23] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. ABS: Scanning neural networks for back-doors by artificial brain stimulation. In Lorenzo Cavallaro, Johannes Kinder, XiaoFeng Wang, and Jonathan Katz, editors, Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS 2019, London, UK, November 11-15, 2019, pages 1265–1282. ACM, 2019.
[24] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018, 2018.
[25] Sebastian Mika, Gunnar Ratsch, Jason Weston, Bernhard Scholkopf, and Klaus-Robert Mullers. Fisher discriminant analysis with kernels. In Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE signal processing society workshop (cat. no. 98th8468), pages 41–48. IEEE, 1999.
[26] Alexandre Nairac, Neil Townsend, Roy Carr, Steve King, Peter Cowley, and Lionel Tarassenko. A system for the analysis of jet engine vibration data. Integrated Computer-Aided Engineering, 6(1):53–66, 1999.
[27] Aaron Nech and Ira Kemelmacher-Shlizerman. Level playing field for million scale face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[28] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP Rubinstein, Udam Saini, Charles Sutton, JD Tygar, and Kai Xia. Misleading learners: Co-opting your spam filter. In Machine learning in cyber trust, pages 17–51. Springer, 2009.
[29] Hong-Wei Ng and Stefan Winkler. A data-driven approach to cleaning large face datasets. In 2014 IEEE International Conference on Image Processing (ICIP), pages 343–347. IEEE, 2014.
[30] Ximing Qiao, Yukun Yang, and Hai Li. Defending neural backdoors via generative distribution modeling. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 14004–14013, 2019.
[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
[32] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! Targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information Processing Systems, pages 6103–6113, 2018.
[33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
[34] Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mosenia, Mung Chiang, and Prateek Mittal. DARTS: Deceiving autonomous cars with toxic signs. CoRR, abs/1802.06430, 2018.
[35] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks, 32:323–332, 2012.
[36] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.
[38] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
[39] Tuan A Tang, Lotfi Mhamdi, Des McLernon, Syed Ali Raza Zaidi, and Mounir Ghogho. Deep learning approach for network intrusion detection in software-defined networking. In 2016 International Conference on Wireless Networks and Mobile Communications (WINCOM), 2016.