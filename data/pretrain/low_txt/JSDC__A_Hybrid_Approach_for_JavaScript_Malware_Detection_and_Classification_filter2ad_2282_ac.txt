### Case Introduction in Detection

Given a set of traces from samples of attack type I, we extract the common actions that appear in all malicious traces. These traces can then be represented as sequences of actions. For example, the trace in Figure 8 can be represented as an action sequence: \( \langle a, b, c, d, e, f, g \rangle \). Using these action sequences, the RPNI algorithm is applied to infer a Deterministic Finite Automaton (DFA). With expert knowledge, the DFA is refined and shown in Figure 9. This refined DFA generates JavaScript code dynamically and exploits browser vulnerabilities to download files into the user's local file system.

For instance, given the sample \( s1 \) in Figure 3, we can capture traces represented as sequences such as \( \langle a, d, e, f, g \rangle \), \( \langle a, b, c, d, e, f, g \rangle \), or \( \langle a, b, c, b, c, d, e, f, g \rangle \). Since these traces are accepted by the learned DFA, \( s1 \) is identified as an attack of type I.

This process is repeated for each attack type to learn a corresponding DFA model. A total of eight learned models are available on the JSDC website [9]. To identify the possible attack type, the trace of a malicious script is iteratively checked against each of these 8 DFAs until a match is found or all are determined to be non-matching. The system call-based behavior modeling is resistant to obfuscation due to dynamic analysis and is applicable to malware variants, as it captures the essential behaviors of the same attack.

### Implementation Details

#### Crawler
We use the Heritrix public domain crawler [7] to crawl over 21,000 Internet websites. Heritrix is an open-source, extensible, web-scale, archival-quality web crawler project. It is designed around the concepts of profiles and jobs [21]. A profile is a configuration of the web crawler, and a job inherits from a profile and can override configuration options. Jobs are queued for processing and picked up by an idle crawler thread. Heritrix also supports custom workflows.

#### HtmlUnit Extension to Obtain Unpacked Code
Using HtmlUnit, we can obtain the unpacked (non-obfuscated) JavaScript code before the time-consuming rendering process. HtmlUnit is configured not to resolve external dependencies (e.g., loading external JavaScript files or third-party plugins) to ensure scalability and lightweight operation. Most JavaScript malware is standalone, so this configuration does not significantly affect the analysis. In rare cases where external dependencies impact the analysis, our extension falls back to plain syntactic analysis.

#### AST Generation and Functional Call Pattern Extraction
To build the Abstract Syntax Tree (AST) of the unpacked execution-ready code, we use Mozilla Rhino 1.7 [6], an open-source JavaScript engine. Rhino is chosen for its fault tolerance and performance in parsing JavaScript code. We traverse the AST and extract all function call nodes. We then apply frequent item-set mining implemented in WEKA, a Java-based open-source machine learning toolkit [23], to mine function call patterns.

#### Classifiers
In [27], four types of classifiers—ADTree, Random Forest (RF), J48, and Naive Bayes (NB)—were used for binary classification (malicious or benign). In our second phase, we perform multi-class classification to categorize malicious JavaScript code into eight attack types. We reuse three classifiers that support multi-class classification: RF, J48, and NB, and also adopt Random Tree (RT).

- **RT**: Builds a decision tree with K randomly chosen attributes at each node and supports estimation of class probabilities based on a hold-out set.
- **RF**: Uses multiple decision trees for training and predicts classes by majority voting.
- **J48**: Uses a single decision tree for classification.
- **NB**: A probabilistic classifier based on Bayes' theorem with strong independence assumptions between features.

WEKA automatically normalizes feature values via range and z-normalization to address noise and treat features fairly. It also supports automatic feature selection algorithms to achieve the best accuracy.

### Evaluation

#### Data Set and Experiment Setup
The labeled data sets include 20,000 benign samples and 942 malicious ones across 8 attack types, as shown in Table 1. Our malicious data sets originate from three sources:
- 200 samples from VX-Heaven [4] and OpenMalware [3].
- 542 manually collected samples from malicious websites reported by Web Inspector [5].

These 942 malicious samples were tested with 54 antivirus tools on VirusTotal [11] and confirmed as malicious. The benign data set consists of 20,000 scripts crawled from Alexa-top500 websites, scanned with Avast! and TrendMicro to ensure they are benign.

The unlabeled data sets include 1,400,000 scripts crawled by Heritrix with randomly selected seeds. The experimental environment is a Dell OptiPlex 990 PC (Intel Core i7-2600 3.40GHz, 8G memory) running Windows 7 64-bit Enterprise.

#### Evaluation of Malware Detection

##### Detection on Labeled Data Sets
To test the accuracy of the classifiers, we use 10-fold cross-validation (CV) instead of a simple train-test split. Table 2 shows the accuracies of the trained classifiers, with all achieving over 98% overall accuracy. The worst classifier, Naive Bayes, has a false negative (FN) rate of 4.5280%. The other three classifiers have low false positive (FP) rates. One FN case for RF, J48, and RT is shown in Figure 10, where the sample requires a mobile device client, causing HtmlUnit to fail in parsing and unpacking the code accurately.

##### Comparison with Other Tools
We compare our approach with JSand [13] and 11 popular antivirus tools, including ClamAV and the top ten reviewed antivirus products. The results in Table 3 show that our tool, using the RandomForest classifier, achieves a detection rate of 99.68% on the labeled malicious samples. On average, each malicious sample is detected by 21 out of 54 tools. The best tool, Kaspersky, detects 86.31% of the samples, while the worst, ClamAV, detects only 9.24%.

##### Detection on Unlabeled Data Set
Applying the trained classifiers to the unlabeled data set, the RandomForest classifier identifies 1,530 snippets as malicious. Manual inspection of 100 randomly selected samples revealed only 1 FP case. Of the 99 true positives (TPs), 11 were missed by all other tools, including an exploit for CVE-2014-1580. Table 4 shows the individual detection ratios, with the best tool achieving 48.49% detection. These observations highlight the difficulty in detecting wild and emerging malware.

##### Performance of Malware Detection
To assess the potential of our approach as an offline large-scale antivirus tool or an online real-time scanner, we investigate the performance of each step in the malware detection process.