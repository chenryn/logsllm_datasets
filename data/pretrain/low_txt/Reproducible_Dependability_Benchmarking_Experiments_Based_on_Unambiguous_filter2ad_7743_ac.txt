### System Components and Interconnections

A computer system comprises various components, including the central processing unit (CPU), memory, video and network cards, hard disks, keyboard, and mouse. These components are interconnected via buses such as the frontside bus, ISA, IDE, and serial bus. The library can be extended to accommodate other types of components. Each component can be parameterized with additional identifying information, such as vendor and product name or unit number. Each hardware component has an inherent possibility of failure, which is crucial for dependability benchmarks. These benchmarks include a fault load in addition to the workload typically seen in performance benchmarks.

### System Setup Overview

Figure 8 provides an overview of the complete system setup. The system under test (SUT) is shaded in light grey, while the driver system generating the workload is shaded in a darker grey. The SUT consists of two database servers (db and rdb) and four application servers (as1 to as4). The primary database server (db) is the target of the benchmark, while the reference server (rdb) remains fault-free and is used to compare the output from db to the correct output. The database and application servers are connected via an Ethernet local area network (solid lines in Figure 8).

All servers have a serial terminal connected for administrative actions. The primary server also has a second serial terminal for alert messages from the database software. The application servers each have three additional serial terminals, which serve as end-user terminals. Virtual users at these terminals generate the workload.

### Workload

Performance and dependability measures can only be acquired if the target system performs the services of interest to the user. In [16], these services are transactions related to order processing in a fictitious wholesale supplier. The workload includes a mix of read-only and update-intensive transactions on the company’s database, simulating activities in complex online transaction processing environments.

Fictitious users enter five different transaction types, defined in [16] clauses 2.4 to 2.8:
- **New-order transaction**: Entering a complete new order.
- **Order-status transaction**: Querying the status of an order.
- **Payment transaction**: Registering a payment.
- **Delivery transaction**: Registering a delivery.
- **Stock-level transaction**: Controlling the stock level of warehouses.

The new-order, order-status, and payment transactions must complete within a small timeframe to satisfy online users. The stock-level transaction has a more relaxed timeframe requirement. The delivery transaction is executed in batch mode, with the user initiating the queuing for deferred execution.

### Hardware Fault Load

We have included high-level storage hardware and network faults in our fault load. Both storage and networking hardware are integral parts of client-server databases and online transaction processing systems, as mentioned in [16]. Disk faults are listed in [16] clause 3.5.3 as a single point of failure. We considered the following scenarios:
1. **Permanent network send and receive losses**: Fixed percentage losses to and from the database server (db). The percentage was varied in steps of five percent between 0 and 50 percent and in steps of one percent between 50 and 70 percent.
2. **Short total network outage**: Duration of the network outage was varied in steps of one minute from 0 to 20 minutes, starting three minutes after the experiment began.
3. **Permanent disk faults**: Randomly located faults on the data disk of the database server (db), triggered three minutes after the start of the experiment.

### Measures and Measuring

Document [16] defines the measure "Maximum Qualified Throughput" (tpmC) as the total number of completed new-order transactions. All measures are extracted from files logged by the automatic experiment controller, Expect. Expect observes the screen output of the servers and user terminals, matching patterns in the VHDL setup before deciding on the next action.

The application server software writes log information on the control terminals (shaded in Figure 8). Log files contain consistency check results with the reference database, database logs for the delivery transaction, and any warnings or errors in the database outputs. Database throughput and response times are measured using VHDL assertion statements, with timestamps automatically added by Expect.

### Results

Figure 9 shows the effects of network losses and outages on tpmC. Network losses cause a slow decrease in tpmC until about 51% loss, where a steep decrease occurs due to TCP's fault-tolerance properties. Network outages linearly decrease tpmC, with a significant impact if the network is down for more than 15 minutes.

Table 1 summarizes the results from disk failure fault loads:
- **No errors**: 82.5%
- **Total database failure**: 12.6%
- **Some database errors**: 3.1%
- **No errors but database complained**: 1.8%

In over 80% of cases, the fault load had no visible effect on the database operation. A significant number of cases led to total database failure, with error codes ORA-03113 and ORA-03114 indicating unexpected communication disconnects. Some database errors were observed, with transactions rolled back correctly. In very few cases, the database's fault-tolerance mechanisms recognized defective disk parts and used intact parts to store data.

### Conclusion

This paper presents a VHDL-based method to unambiguously and completely describe a performance or dependability benchmark configuration, enhancing the repeatability and reproducibility of benchmark results. A prototype experiment controller automates the setup and execution based on the VHDL configuration description. The results of our experiments are presented.

### Acknowledgement

This research is supported by the European Community (DBench project, IST-2000-25425). We thank all contributors to our benchmarking environment, UMLinux.

### References

[1] I. S. 1076-2002. IEEE Standard VHDL Language Reference Manual. Product No.: SH94983-TBR, 2002.
[2] 2U Consortium. Unambiguous UML. URL: http://www.2uworks.org/, 2003.
[3] K. Buchacker, M. Dal Cin, H. H¨oxer, V. Sieh, and O. Tsch¨ache. Reproducible Dependability Benchmarking Experiments Based on Unambiguous Benchmark Setup Descriptions. Internal Report 1/2002, Institut f¨ur Informatik 3, Universit¨at Erlangen-N¨urnberg, 2002.
[4] K. Buchacker, R. Karch, V. Sieh, and O. Tsch¨ache. Running a Dependability Benchmark for an Oracle Database System. Internal Report 2/2002, Institut f¨ur Informatik 3, Universit¨at Erlangen-N¨urnberg, 2002.
[5] K. Buchacker and V. Sieh. Framework for Testing the Fault-tolerance of Systems Including OS and Network Aspects. In Proceedings Sixth IEEE International High-Assurance Systems Engineering Symposium, pages 95–105, 2001.
[6] G. J. Carrette. Crashme. URL: http://people.delphi.com/gjc/crashme.html, 1996.
[7] O. Corporation. Oracle 9i Database. URL: http://oracle.com/, 2002.
[8] DBench - Dependability Benchmarking (Project IST-2000-25425). Coordinator: Laboratoire d’Analyse et d’Architecture des Syst`emes du Centre National de la Recherche Scientifique, Toulouse, France; Partners: Chalmers University of Technology, G¨oteborg, Sweden; Critical Software, Coimbra, Portugal; Faculdade de Ciencias e Technologia da Universidade de Coimbra, Portugal; Friedrich-Alexander Universit¨at, Erlangen-N¨urnberg, Germany; Microsoft Research, Cambridge, UK; Universidad Politechnica de Valencia, Spain. URL: http://www.laas.fr/DBench/, 2001.
[9] H. H¨oxer, K. Buchacker, and V. Sieh. Implementing a User Mode Linux with Minimal Changes from Original Kernel. In J. Topf, editor, 9th International Linux System Technology Conference, K¨oln, Germany, September 4-6, 2002, pages 71–82, 2002.
[10] N. Kropp, P. J. Koopman, and D. P. Siewiorek. Automated Robustness Testing of Off-the-Shelf Software Components. In Proceedings of the 28th IEEE International Symposium on Fault Tolerant Computing, pages 230–239, 1998.
[11] B. P. Miller, D. Koski, C. P. Lee, V. Maganty, R. Murthy, A. Natarajan, and J. Steidl. Fuzz Revised: A Re-examination of the Reliability of UNIX Utilities and Services. Computer Science Technical Report 1268, University of Wisconsin-Madison, 1995.
[12] V. Sieh and K. Buchacker. UMLinux — a Versatile SWIFI Tool. In A. Bondavalli and P. Thevenod-Fosse, editors, Fourth European Dependable Computing Conference, Toulouse, France, October 23-25, 2002, pages 159–171. Springer Verlag, Berlin, 2002.
[13] V. Sieh, O. Tsch¨ache, and F. Balbach. VERIFY: Evaluation of Reliability Using VHDL-Models with Integrated Fault Descriptions. In Proceedings of the 27th IEEE International Symposium on Fault Tolerant Computing, pages 32–36, 1997.
[14] Standard Performance Evaluation Corporation. SPEC WEB99 Release 1.02. www.spec.org, 2000.
[15] The Precise UML Group. The Precise UML Group. URL: http://www.puml.org/, 2003.
[16] Transaction Processing Performance Council. TPC Benchmark [tm] C, Standard Specification, Revision 5.0, February 26, 2001. www.tpc.org, 2001.
[17] UMLinux Team. UMLinux. URL: http://umlinux.de/, 2002.
[18] M. Vieira and H. Madeira. Definition of Faultloads Based on Operator Faults for DBMS Recovery Benchmarking. In Pacific Rim International Symposium on Dependable Computing, 2002.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021 at 12:26:06 UTC from IEEE Xplore. Restrictions apply.