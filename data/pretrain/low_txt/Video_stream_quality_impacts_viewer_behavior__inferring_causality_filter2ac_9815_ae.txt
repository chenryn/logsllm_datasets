### Optimized Text

#### Matching Algorithm to Neutralize Prior Viewing Effects

To mitigate the effect of viewers who have accumulated more viewing time, we match viewers based on the number of prior visits and aggregate play time in step 1(c) below, ensuring that the compared viewers exhibit similar propensities to visit the site before the treatment. This matching approach is common in quasi-experimental design (QED) analysis and is conceptually similar to propensity score matching [22].

**Note:** In this matching process, we are comparing viewers, not individual views, as we are evaluating the repeat viewership of a viewer over time.

**Figure 18:** Cumulative Distribution Function (CDF) of viewer play time for all, treated, and untreated viewers.

The matching algorithm is as follows:

1. **Match Step:**
   - We create a matched set of pairs \( M \) as follows:
     - Let \( T \) be the set of all viewers who have experienced a failed visit.
     - For each viewer \( u \in T \), we select their first failed visit.
     - We then pair \( u \) with a viewer \( v \) chosen uniformly and randomly from the set of all possible viewers, such that:
       1. Viewer \( v \) has the same geography, connection type, and content provider as \( u \).
       2. Viewer \( v \) had a normal visit at approximately the same time (within ±3 hours) as the first failed visit of viewer \( u \). We refer to the failed visit of \( u \) and the corresponding normal visit of \( v \) as matched visits.
       3. Viewers \( u \) and \( v \) have the same number of visits and approximately the same total viewing time (±10 minutes) prior to their matched visits.

2. **Score Step:**
   - For each pair \((u, v) \in M\) and each return time \( \delta \):
     - We assign the outcome \(\text{outcome}(u, v, \delta)\) as:
       - \(-1\) if \( u \) returns within the return time and \( v \) does not,
       - \(+1\) if \( v \) returns within the return time and \( u \) does not,
       - \(0\) otherwise.
   - The net outcome is calculated as:
     \[
     \text{Net Outcome}(\delta) = \frac{\sum_{(u,v) \in M} \text{outcome}(u, v, \delta)}{|M|} \times 100
     \]

**Figure 19** shows the results of the matching algorithm for various values of the return time \( \delta \). Positive outcomes provide strong evidence for the causality of Assertion 7.1, indicating that viewers who experienced a normal visit returned more frequently than their identical pairs with a failed visit.

For example, for \( \delta = 1 \) day, 458,621 pairs were created. The pairs where the normal viewer returned but the identical failed pair did not return exceeded the opposite by 10,909 pairs, which is 2.38% of the total pairs. Using the sign test, the p-value is extremely small (\(2.2 \times 10^{-58}\)), providing strong statistical significance. As \( \delta \) increases, the outcome score remains in a similar range, though it is expected that for very large \( \delta \) values, the effect of the failed event would diminish. All p-values remain significantly smaller than our threshold of 0.001, confirming the statistical significance of the results.

### Related Work

The quality metrics considered here have a history of over a decade within the industry. Early measurement systems, such as Akamai’s Stream Analyzer [1, 24], used synthetic "measurement agents" deployed globally to measure metrics like failures, startup delay, rebuffering, and bitrate. Early studies at Akamai explored these metrics [19]. However, large-scale studies became feasible only with the advent of client-side measurement technology capable of reporting detailed quality and behavioral data from actual viewers.

To our knowledge, the first significant large-scale study closely related to our work is the viewer engagement study published last year [9], which established several correlational relationships between quality (e.g., rebuffering), content type (e.g., live, short/long VoD), and viewer engagement (e.g., play time). A recent follow-up [15] examines the use of quality metrics to enhance video delivery. Our work distinguishes itself by focusing on establishing causal relationships, going beyond mere correlation. While our viewer engagement analysis was also correlationally established in [9], our work takes the next step in ascertaining the causal impact of rebuffering on play time. Additionally, we establish key assertions regarding viewer abandonment and repeat viewership, which are the first quantitative results of their kind. However, [9] studies a broader set of quality metrics, including join time, average bitrate, and rendering quality, and a wider class of videos, including live streaming, albeit without establishing causality.

The application of quasi-experimental design (QED) in social and medical sciences has a long and distinguished history [23]. Its application to data mining is more recent. For example, [18] uses QEDs to analyze user behavior in social media platforms like Stack Overflow and Yahoo Answers. Other studies on perceived quality tend to be small-scale or do not link quality to user behavior [10, 7]. There has also been work on other types of systems, such as the relationship between page download times and user satisfaction for the web [3] and quantifying user satisfaction for Skype [6]. Studies have also correlated Quality of Service (QoS) with Quality of Experience (QoE) for multimedia systems using human subjects [27]. These studies, however, have a different focus and do not show causal impact. Significant work has been done in workload characterization for streaming media, P2P, and web workloads [25, 4]. Although we do characterize the workload to some extent, our primary focus is on quality and viewer behavior.

### Conclusions

Our work is the first to demonstrate a causal relationship between stream quality and viewer behavior. The results presented are important because they provide the first quantitative demonstration that key quality metrics causally impact viewer behavioral metrics, which are crucial for both content providers and CDN operators. As all forms of media migrate to the Internet, understanding this causal nexus will become increasingly important for video monetization and CDN design. Establishing a causal relationship by systematically eliminating confounding variables is crucial, as mere correlational studies can lead to incorrect conclusions with potentially costly consequences.

Our work breaks new ground in understanding viewer abandonment and repeat viewership. It also provides a deeper insight into the known correlational impact of quality on viewer engagement by establishing its causal impact. For instance, our findings on startup delay show that a 1-second increase in delay increases the abandonment rate by 5.8%. We also demonstrated the strong impact of rebuffering on video play time, showing that a viewer experiencing a rebuffer delay equal to or exceeding 1% of the video duration played 5.02% less of the video compared to a similar viewer with no rebuffering. Finally, we examined the impact of failed visits and found that a viewer who experienced failures is less likely to return to the content provider's site. Specifically, a failed visit decreased the likelihood of a viewer returning within a week by 2.32%.

It is important to note that small changes in viewer behavior can lead to significant changes in monetization, as even a few percentage points over tens of millions of viewers can accumulate to a large impact over time.

As more data becomes available, we expect our QED tools to play an increasingly larger role in establishing key causal relationships that drive both the content provider’s monetization framework and the CDN’s next-generation delivery architecture. The increasing scale of measured data enhances the statistical significance of derived conclusions and the efficacy of our tools. Furthermore, our work provides an important tool for establishing causal relationships in other areas of measurement research in networked systems that have so far been limited to correlational studies.

### Acknowledgements

We thank Ethendra Bommaiah, Harish Kammanahalli, and David Jensen for insightful discussions about the work. We also thank our shepherd Meeyoung Cha and our anonymous referees for their detailed comments, which resulted in significant improvements to the paper. Any opinions expressed in this work are solely those of the authors and not necessarily those of Akamai Technologies.

### References

[1] Akamai. Stream Analyzer Service Description. http://www.akamai.com/dl/feature_sheets/Stream_Analyzer_Service_Description.pdf.

[2] K. Andreev, B.M. Maggs, A. Meyerson, and R.K. Sitaraman. Designing overlay multicast networks for streaming. In Proceedings of the fifteenth annual ACM symposium on Parallel algorithms and architectures, pages 149–158. ACM, 2003.

[3] N. Bhatti, A. Bouch, and A. Kuchinsky. Integrating user-perceived quality into web server design. Computer Networks, 33(1):1–16, 2000.

[4] M. Cha, H. Kwak, P. Rodriguez, Y.Y. Ahn, and S. Moon. I tube, You Tube, Everybody Tubes: Analyzing the World’s Largest User Generated Content Video System. In Proceedings of the 7th ACM SIGCOMM conference on Internet measurement, pages 1–14, 2007.

[5] H. Chen, S. Ng, and A.R. Rao. Cultural differences in consumer impatience. Journal of Marketing Research, pages 291–301, 2005.

[6] K.T. Chen, C.Y. Huang, P. Huang, and C.L. Lei. Quantifying Skype user satisfaction. In ACM SIGCOMM Computer Communication Review, volume 36, pages 399–410. ACM, 2006.

[7] M. Claypool and J. Tanner. The effects of jitter on the perceptual quality of video. In Proceedings of the seventh ACM international conference on Multimedia (Part 2), pages 115–118. ACM, 1999.

[8] John Dilley, Bruce M. Maggs, Jay Parikh, Harald Prokop, Ramesh K. Sitaraman, and William E. Weihl. Globally distributed content delivery. IEEE Internet Computing, 6(5):50–58, 2002.

[9] Florin Dobrian, Vyas Sekar, Asad Awan, Ion Stoica, Dilip Joseph, Aditya Ganjam, Jibin Zhan, and Hui Zhang. Understanding the impact of video quality on user engagement. In Proceedings of the ACM SIGCOMM Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, pages 362–373, New York, NY, USA, 2011. ACM.

[10] S.R. Gulliver and G. Ghinea. Defining user perception of distributed multimedia quality. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP), 2(4):241–257, 2006.

[11] R. Kohavi, R. Longbotham, D. Sommerfield, and R.M. Henne. Controlled experiments on the web: survey and practical guide. Data Mining and Knowledge Discovery, 18(1):140–181, 2009.

[12] L. Kontothanassis, R. Sitaraman, J. Wein, D. Hong, R. Kleinberg, B. Mancuso, D. Shaw, and D. Stodolsky. A transport layer for live streaming in a content delivery network. Proceedings of the IEEE, 92(9):1408–1419, 2004.

[13] R.C. Larson. Perspectives on queues: Social justice and the psychology of queueing. Operations Research, pages 895–905, 1987.

[14] E.L. Lehmann and J.P. Romano. Testing statistical hypotheses. Springer Verlag, 2005.

[15] X. Liu, F. Dobrian, H. Milner, J. Jiang, V. Sekar, I. Stoica, and H. Zhang. A case for a coordinated internet video control plane. In Proceedings of the ACM SIGCOMM Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, pages 359–370, 2012.

[16] Steve Lohr. For impatient web users, an eye blink is just too long to wait. New York Times, February 2012.

[17] E. Nygren, R.K. Sitaraman, and J. Sun. The Akamai Network: A platform for high-performance Internet applications. ACM SIGOPS Operating Systems Review, 44(3):2–19, 2010.

[18] H. Oktay, B.J. Taylor, and D.D. Jensen. Causal discovery in social media using quasi-experimental designs. In Proceedings of the First Workshop on Social Media Analytics, pages 1–9. ACM, 2010.

[19] Akamai White Paper. Akamai Streaming: When Performance Matters, 2004. http://www.akamai.com/dl/whitepapers/Akamai_Streaming_Performance_Whitepaper.pdf.

[20] G.E. Quinn, C.H. Shin, M.G. Maguire, R.A. Stone, et al. Myopia and ambient lighting at night. Nature, 399(6732):113–113, 1999.

[21] Jupiter Research. Retail Web Site Performance, June 2006. http://www.akamai.com/html/about/press/releases/2006/press_110606.html.

[22] P.R. Rosenbaum and D.B. Rubin. Constructing a control group using multivariate matched sampling methods that incorporate the propensity score. American Statistician, pages 33–38, 1985.

[23] W.R. Shadish, T.D. Cook, and D.T. Campbell. Experimental and quasi-experimental designs for generalized causal inference. Houghton, Miﬄin and Company, 2002.

[24] R.K. Sitaraman and R.W. Barton. Method and apparatus for measuring stream availability, quality and performance, February 2003. US Patent 7,010,598.

[25] K. Sripanidkulchai, B. Maggs, and H. Zhang. An analysis of live streaming workloads on the internet. In Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement, pages 41–54, 2004.

[26] D.A. Wolfe and M. Hollander. Nonparametric statistical methods. Nonparametric statistical methods, 1973.

[27] W. Wu, A. Arefin, R. Rivas, K. Nahrstedt, R. Sheppard, and Z. Yang. Quality of experience in distributed interactive multimedia environments: toward a theoretical framework. In Proceedings of the 17th ACM international conference on Multimedia, pages 481–490, 2009.