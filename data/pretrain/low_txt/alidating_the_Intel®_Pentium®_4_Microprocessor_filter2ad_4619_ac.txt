### Full-Chip Feature Testing and Validation

For the first time, a validator took responsibility for running feature exercise tests, debugging failures, and collaborating with designers to rapidly implement fixes into experimental models. This process bypassed the standard code submission procedure until an acceptable level of stability was achieved. Only then was the feature made available for broader use by other validators.

We found that this methodology significantly accelerated the integration process. As a side effect, it also helped the AV team develop their full-chip debugging skills more quickly than would have otherwise been possible.

Once a fully functional full-chip SRTL model was available (in mid-1998), the initial feature tests were discarded and replaced by a new suite of over 12,000 IA-32 tests developed by the AV team. These tests were designed to comprehensively explore the architectural space. Previous projects, including the Pentium Pro processor, had relied on an "ancestral" test base inherited from the past. However, these tests had little or no documentation, unknown coverage, and questionable quality. In fact, many of them were bug tests from previous implementations with little architectural value. We did eventually run the "ancestral" suite as a late cross-check after the new suite had been executed and the resulting bugs fixed, but we found nothing significant, indicating that the ancestral suite can finally be retired.

### Coverage-Based Validation

Recognizing the truth in the saying, "If it isn’t tested, it doesn’t work," we used coverage data wherever possible to provide feedback on the effectiveness of our tests and to identify untested areas. This helped direct future testing towards uncovered areas. Since we heavily relied on direct random test generators for most of our microarchitectural testing, coverage feedback was essential to avoid repeatedly testing the same areas while leaving others untouched.

In fact, we used the tuple of cycles run, coverage gained, and bugs found as our primary gauge of SRTL model health and readiness for tapeout. Our main coverage tool was Proto from Intel Design Technology, which we used to create coverage monitors and measure a large number of microarchitecture conditions. By the time of tapeout, we were tracking almost 2.5 million unit-level conditions and more than 250,000 inter-unit conditions, achieving nearly 90% and 75% coverage, respectively.

We also used Proto to instrument several multiprocessor conditions (combinations of microarchitecture states for caches, load and store buffers, etc.) and all the clock gating conditions identified in the unit power reduction plans. Additionally, we used the Pathfinder tool from Intel’s Central Validation Capabilities group to measure how well we were exercising all possible microcode paths in the machine. To our surprise, running the entire AV test suite yielded less than 10% microcode path coverage. Further analysis revealed that many of the untouched paths involved memory-related faults (e.g., page faults) or assists (e.g., A/D bit assist). The test writers had set up their page tables and descriptors to avoid these time-consuming functions. We modified the tests to exercise these uncovered paths and indeed found several bugs in previously untested logic, reinforcing our belief in the value of using coverage feedback.

### Bug Discussion

Comparing the development of the Pentium 4 processor with the Pentium Pro processor, we recorded a 350% increase in the number of bugs filed against SRTL prior to tapeout. We also observed a different breakdown by cluster: on the Pentium Pro processor, microcode was the largest single source of bugs, accounting for over 30% of the total, whereas on the Pentium 4 processor, it was less than 14%. For both designs, the Memory Cluster was the largest source of hardware bugs, accounting for around 25% of the total in both cases. This is consistent with data from other projects and indicates that we need to focus more on preventing bugs in this area.

We conducted a statistical study to determine the sources of bugs in the Pentium 4 processor design. The major categories, accounting for over 75% of the analyzed bugs, were:

- **Goof (12.7%)**: These included typos, cut-and-paste errors, careless coding, and situations where the designer relied on testing to find the bugs.
- **Miscommunication (11.4%)**: These fell into several categories, including architects not clearly communicating their expectations to designers, misunderstandings between microcode and design, and misassumptions about what another unit was doing.
- **Microarchitecture (9.3%)**: These were problems in the microarchitecture definition.
- **Logic/Microcode Changes (9.3%)**: These were cases where the design was changed, usually to fix bugs or timing issues, and the designer did not consider all the impacted areas.
- **Corner Cases (8%)**: These were specific cases that the designer failed to implement correctly.
- **Power Down Issues (5.7%)**: These were mostly related to clock gating.
- **Documentation (4.4%)**: Something (algorithm, microinstruction, protocol) was not documented properly.
- **Complexity (3.9%)**: These were bugs specifically identified as being due to microarchitectural complexity.
- **Random Initialization (3.4%)**: These were mostly bugs caused by state not being properly cleared or initialized at reset.
- **Late Definition (2.8%)**: Certain features were not defined until late in the project, leading to rushed and incomplete implementation.
- **Incorrect RTL Assertions (2.8%)**: These referred to assertions in the SRTL code that were wrong, overzealous, or broken by a design change.
- **Design Mistake (2.6%)**: The designer misunderstood what they were supposed to implement, often due to not fully reading the specification or starting implementation before the specification was complete.

### Conclusion

The Pentium 4 processor was highly functional on A-0 silicon and received production qualification in only 10 months from tapeout. The work described here is a major reason why we were able to meet such a tight schedule and enable Intel to realize early revenue in today's highly competitive marketplace.

### Acknowledgments

The results described in this paper are the work of a large team over a four-year period. It is impossible to list everyone who contributed, but I am especially indebted to Blair Milburn, Keiichi Suzuki, Rick Zucker, Bob Brennan, Tom Schubert, and John Bauer for their leadership in the Pentium 4 processor pre-silicon validation activities and for their valuable feedback on early drafts of this paper.

### References

1. Clark, D. (1990). "Bugs Are Good: A Problem-Oriented Approach To The Management Of Design Engineering." Research-Technology Management, 33(3), May, pp. 23-21.
2. Ho, R., Yang, H., Horowitz, M., & Dill, D. (1995). "Architecture Validation for Processors." ISCA 95: International Conference on Computer Architecture, June.
3. Fournier, L., Arbetman, Y., & Levinger, M. (1999). "Functional Verification Methodology for Microprocessors Using the Genesys Test-Program Generator - Application to the x86 Microprocessors Family." Proceedings of the Design Automation and Test in Europe Conference and Exhibition (DATE99), March.
4. Bentley, R., & Milburn, B. (1996). "Analysis of Pentium Pro Processor Bugs." Intel Design and Test Technology Conference, June.
5. Tucker, R. (2000). "Bug Analysis for Willamette." Intel Design and Test Technology Conference, August.