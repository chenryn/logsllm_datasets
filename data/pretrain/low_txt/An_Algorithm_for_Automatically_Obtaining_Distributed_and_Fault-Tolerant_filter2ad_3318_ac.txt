### Connecting Processors and Scheduling Communications

To ensure the correct execution of operations, processors responsible for source and destination tasks are connected. All communications (comms) assigned to the same communication unit are statically scheduled, resulting in a total order over each communication medium. Provided that the network maintains message integrity and ordering, this total order guarantees accurate data transmission between processors. The obtained schedule also ensures deadlock-free execution.

The scheduling strategy minimizes runtime overhead in faulty systems (systems with at least one failure) by prioritizing operations using \( S(n)_{\text{worst}}(o, p) \) and scheduling them using \( S(n)_{\text{best}}(o, p) \).

### Implementation and Example

We have implemented our fault-tolerant heuristic in the SYNDEX tool, which optimizes the implementation of real-time embedded applications on multi-component architectures. We applied our heuristic to the example in Figure 2, where the system is required to tolerate one permanent processor failure (\( N_{\text{pf}} = 1 \)). The execution characteristics of each component (comp/mem/extio) and comm are specified in the tables provided in Section 3.4.

After the first two steps of our heuristic, we obtain the temporary schedule shown in Figure 5.

#### Faulty Execution Scenario

Suppose Processor P1 crashes at time 0 (Figure 8). In a fail-silent model, P1 fails to produce its expected results, such as the output from (I, P1) to (A, P3) and (C, P1) to (F, P2). P3 can detect P1's failure only after the expected completion date of the comm from (I, P1) to (A, P2). While detecting P1’s failure is useful to avoid further comms to P1, it is not functionally necessary because the static schedule transparently tolerates one failure. However, the resulting schedule has a longer execution time.

### Scheduling Operations

In the next step, operation C is scheduled. Assigning C to P1, P2, and P3 results in expected schedule pressures of 9.73, 10.53, and 9.23, respectively. If A, the LIP of C, is duplicated to P3, the schedule pressure of C can be reduced to 5.73, reducing the start time of C. Thus, we schedule a new replica of A on P3 and two replicas of C on P3 and P1, minimizing the schedule pressure. As shown in Figure 6, operation A receives its input data twice from the replicas of I scheduled on P1 and P2, and the start time of A is the end of the earliest communication between (I, A) on {L1, 3} and {L2, 3}. This results in the temporary schedule shown in Figure 6.

### Final Schedule

Similarly, operations B, D, E, F, G, and O are scheduled. At the end of our heuristic, we obtain the final schedule presented in Figure 7. Each operation in the algorithm graph is replicated at least twice and assigned to different processors. The real-time constraint is satisfied since the total time is 15.05 < Rtc.

Figure 7 shows that some communications are not used in the absence of failures. For example, the communication of the result from (I, P2) to (A, P3) is not used because the result from (I, P1) arrives first. However, these communications become useful during faults.

### Analysis of the Example

To evaluate the overheads introduced by fault-tolerance, we compare the non-fault-tolerant schedule (length 10.7) produced by a basic scheduling heuristic with the fault-tolerant schedule (length 15.05). The fault-tolerance overhead is 15.05 - 10.7 = 4.35.

In the fault-tolerant schedule, some unnecessary communications occur, but the response time of the faulty system is minimized, as results are sent without waiting for timeouts (Figure 8). This solution is suitable for architectures with point-to-point links, allowing parallel communications. For multi-point links, the overheads from replicated comms may be too high due to serialization.

### Runtime Behavior

Our heuristic schedules \( N_{\text{pf}} + 1 \) replicas for each operation on different processors to tolerate \( N_{\text{pf}} \) faults. If no fault occurs, each replica of an operation receives inputs in parallel from all replicas of its predecessor operations; it executes as soon as it receives the first set and ignores the later inputs. If there are \( k \) permanent faults (\( k \leq N_{\text{pf}} \)), each replica of an operation on a non-faulty processor receives inputs in parallel from non-faulty processors and executes similarly.

Failure detection has two options:
1. **No Failure Detection**: Remaining processors continue sending results to the faulty one, which does not reduce communication overheads but allows intermittent failures to recover.
2. **Failure Detection**: Processors maintain an array of faulty processors and avoid further comms to them, which cannot recover from intermittent failures.

### Performance Evaluation

We compared FTBAR with the HBP algorithm [16] for performance evaluation. Since HBP assumes homogeneous systems and uses software redundancy, FTBAR was downgraded to these assumptions for a meaningful comparison. Our simulations aimed to compare fault-tolerance overheads in the absence and presence of one processor failure.

#### Simulation Parameters

We generated random algorithm graphs with varying parameters: number of operations \( N \) (10, 20, ..., 80) and communication-to-computation ratio (CCR) (0.1, 0.5, 1, 2, 5, 10).

#### Performance Results

Figures 9 and 10 show the average fault-tolerance overheads as a function of \( N \) and CCR. FTBAR performs better than HBP, especially when CCR ≥ 2, due to the schedule pressure minimizing the critical path length.

### Conclusion and Future Work

This paper presents FTBAR, a new scheduling heuristic for generating static, distributed, fault-tolerant schedules for embedded systems. Our solution is based on software redundancy of both computation and communication. We implemented FTBAR within the SYNDEX tool and compared it with HBP, showing superior performance. Future work includes extensive benchmark testing on heterogeneous architectures and addressing communication link failures and reliability.

### Acknowledgments

We thank Cătălin Dima, Thierry Grandpierre, Claudio Pinello, and David Powell for their valuable suggestions.

### References

[1] I. Ahmad and Y.-K. Kwok. On exploiting task duplication in parallel program scheduling. IEEE Transactions on Parallel and Distributed Systems, 9(9):872–892, September 1998.

[2] G. Berry and A. Benveniste. The synchronous approach to reactive and real-time systems. Proceedings of the IEEE, 79(9):1270–1282, September 1991.

[3] A. Bertossi and L. Mancini. Scheduling algorithms for fault-tolerance in hard-real-time systems. Real-Time Systems Journal, 7(3):229–245, 1994.

[4] A. Bertossi, L. Mancini, and F. Rossini. Fault-tolerant rate-monotonic first-fit scheduling in hard real-time systems. IEEE Trans. on Parallel and Distributed Systems, 10:934–945, 1999.

[5] M. Caccamo and B. Buttazzo. Optimal scheduling for fault-tolerant and firm real-time systems. 5th International Conference on Real-Time Computing Systems and Applications. IEEE, Oct. 1998.

[6] P. Chevochot and I. Puaut. Scheduling fault-tolerant distributed hard real-time tasks independently of the replication strategy. 6th International Conference on Real-Time Computing Systems and Applications (RTCSA’99), pages 356–363, HongKong, China, December 1999.

[7] J.-Y. Chung, J. Liu, and K.-J. Lin. Scheduling periodic jobs that allow imprecise results. IEEE Trans. on Computers, 39(9):1156–1174, September 1990.

[8] C. Dima, A. Girault, C. Lavarenne, and Y. Sorel. Off-line real-time fault-tolerant scheduling. 9th Euromicro Workshop on Parallel and Distributed Processing, PDP’01, pages 410–417, Mantova, Italy, February 2001.

[9] G. Fohler. Adaptive fault-tolerance with statically scheduled real-time systems. Euromicro Workshop on Real-Time Systems, EWRTS’97, Toledo, Spain, June 1997. IEEE.

[10] M. Garey and D. Johnson. Computers and Intractability, a Guide to the Theory of NP-Completeness. W. H. Freeman Company, San Francisco, 1979.

[11] S. Ghosh. Guaranteeing Fault-Tolerance through Scheduling in Real-Time Systems. PhD Thesis, University of Pittsburgh, 1996.

[12] A. Girault, C. Lavarenne, M. Sighireanu, and Y. Sorel. Fault-tolerant static scheduling for real-time distributed embedded systems. 21st International Conference on Distributed Computing Systems, ICDCS’01, pages 695–698, Phoenix, USA, April 2001. IEEE. Extended abstract.

[13] A. Girault, C. Lavarenne, M. Sighireanu, and Y. Sorel. Generation of fault-tolerant static scheduling for real-time distributed embedded systems with multi-point links. IEEE Workshop on Fault-Tolerant Parallel and Distributed Systems, FTPDS’01, San Francisco, USA, April 2001. IEEE.

[14] M. Gupta and E. Schonberg. Static analysis to reduce synchronization cost in data-parallel programs. 23rd Symposium on Principles of Programming Languages, pages 322–332, January 1996.

[15] N. Halbwachs. Synchronous Programming of Reactive Systems. Kluwer Academic, 1993.

[16] K. Hashimoto, T. Tsuchiya, and T. Kikuno. Effective scheduling of duplicated tasks for fault-tolerance in multiprocessor systems. IEICE Transactions on Information and Systems, E85-D(3):525–534, March 2002.

[17] P. Jalote. Fault-Tolerance in Distributed Systems. Prentice Hall, Englewood Cliffs, New Jersey, 1994.

[18] X. Qin, Z. Han, H. Jin, L. P. Pang, and S. L. Li. Real-time fault-tolerant scheduling in heterogeneous distributed systems. Proceeding of the International Workshop on Cluster Computing-Technologies, Environments, and Applications (CC-TEA’2000), Las Vegas, USA, June 2000.

[19] K. Ramamritham. Allocation and scheduling of precedence-related periodic tasks. IEEE Trans. on Parallel and Distributed Systems, 6(4):412–420, April 1995.

[20] J. Rushby. Critical system properties: Survey and taxonomy. Reliability Engineering and Systems Safety, 43(2):189–219, 1994. Research Report CSL-93-01.

[21] A. Vicard. Formalisation et Optimisation des Systèmes Informatiques Distribués Temps-Rel Embarqués. PhD Thesis, University of Paris XIII, July 1999.

[22] T. Yang and A. Gerasoulis. List scheduling with and without communication delays. Parallel Computing, 19(12):1321–1344, 1993.