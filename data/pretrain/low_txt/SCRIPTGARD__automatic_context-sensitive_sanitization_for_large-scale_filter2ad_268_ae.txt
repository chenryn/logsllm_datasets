### Performance Evaluation

To evaluate the performance overhead introduced by ScriptGard, we first accessed a URL 13 times. We then measured the time to the first byte for 13 additional queries to the server for the same URL. The time to the first byte is an appropriate metric because it reflects the overhead added by ScriptGard to the server's processing. Finally, we took the 75th percentile result, following the web performance analysis practices used by our institution's colleagues responsible for a large public-facing website. Unfortunately, we found that the full analysis introduced an overhead of over 175.6x. For example, a URL that took 0.53 seconds to load without instrumentation took 92.73 seconds to load with all checks.

### Preferential Path Profiling (PPP)

Fortunately, much of this overhead can be shifted to a pre-release phase using preferential path profiling (PPP) techniques. As described earlier, PPP optimizations include a smart way of numbering code blocks and a method to avoid updating global state when certain "preferred" paths are taken.

We used a wide-area network simulator running on the server to emulate the conditions of a typical IEEE 802.11b link between the client and the server, to produce realistic network conditions for evaluating the total overhead. We then instrumented our application using preferential path profiling and performed training on a set of 19 URLs. To test the worst-case performance from PPP, we chose 19 URLs completely different from those used during training. We then visited each URL 20 times and measured the time to the first byte.

### Results

Figure 12 graphs the time to the first byte for each of our test URLs, in milliseconds, after removing the first visit. To account for outliers, the time measurement represents the 75th percentile of the measurements over a series of 19 samples. Our graph shows error bars representing one standard deviation in each direction.

Figure 13 shows the median for each URL visited and the overhead for each of the 19 URLs. The average overhead for using preferential path profiling is actually negative, which we attribute to the overhead being too small to be statistically significant. We report overhead both for direct connection to the switch and for when the network simulator is configured to simulate an 802.11 connection. Our results show that while the runtime overhead of the full ScriptGard analysis is prohibitive, the deployment overhead can be significantly decreased by shifting analysis to a training phase.

#### Figure 12: Overhead of PPP on 19 URLs
- **Time in milliseconds**
- **Top of bar is the 75th percentile of 20 samples**

#### Figure 13: PPP Overhead Statistics
- **Time in milliseconds, 75th percentile reported from 20 samples**

### Related Work

Many defense techniques for scripting attacks have been proposed, which we discuss next. ScriptGard targets a new class of context-inconsistency errors.

**Defense Techniques:**
- **Browser-based mitigations**: Such as Noncespaces, XSS Auditor, or DSI, make changes to web browsers to make it difficult for an adversary’s script to run [3, 11, 25]. These approaches can be fast but require all users to upgrade their web browsers.
- **Server-side mitigations**: Focus on changing the web application logic. Blueprint describes mechanisms to ensure the safe construction of the intended HTML parse tree on the client using JavaScript in a browser-agnostic way [20]. However, issues in applying Blueprint’s mechanisms to various contexts still exist, and developers may place them incorrectly. Our work addresses this separate aspect of the problem without requiring any manual effort from developers.
- **XSS-GUARD [4]**: Proposes techniques to learn allowed scripts from unintended scripts. The allowed scripts are then whitelisted. Like ScriptGard, it employs a web browser for its analysis, but the two approaches are fundamentally different. ScriptGard’s defense is based on automatic server-side sanitizer placement, rather than browser-based whitelisting of scripting in server output. XSS-GUARD’s techniques are intended for applications that allow rich HTML, where designing correct sanitizers becomes challenging. ScriptGard targets applications with fairly restrictive policies that have already addressed the sanitizer-correctness issue.

**Google AutoEscape**: An auto-sanitization mechanism for Google’s GWT and CTemplate templating frameworks. As explained earlier, the templating languages it auto-sanitizes are much simpler and do not need to handle if-else or loop constructs, which create path-sensitivity issues. ScriptGard’s target is towards large-scale legacy applications, where sanitization placement logic is complex and already deployed, so rewriting all such logic is not practical.

**SecuriFly [21]**: Translates bug specifications written in a special program query to runtime instrumentation that detects these bugs. Our approach reasons about both the browser context and the server state, allowing us to tackle sanitizer placement problems not detected by SecuriFly.

**Software Security Analysis of Web Applications**: Software security focuses on using program analysis to find security-critical bugs in applications. The WebSSARI project pioneered these approaches for web applications, and several static analysis tools have been proposed [16, 35]. Runtime approaches, like ours, have the advantage of demonstrating clear, reproducible test cases over static analysis tools. Multiple runtime analysis systems for information flow tracking have been proposed, including Haldar et al. for Java [12] and Pietraszek et al. [28] and Nguyen-Tuong et al. for PHP [26]. Typically, systems use negative tainting to specifically identify untrusted data in web applications [18, 19, 21, 36]. While negative taint is preferable for finding bugs, it is less desirable for mitigations because it requires specifying all sources of taint. Our design distinguishes itself from most previous work in that it tracks positive taint, which is a conservative default fail-close approach, and side-steps identifying all sources of taint. The main exception is WASP [13], which does use positive taint but is concerned with SQL injection attacks, which do not exhibit the path sensitivity, use of multiple sanitizers, and need for a browser model to determine if data is a potential cross-site scripting attack. WASP was also evaluated on much smaller applications (maximum 20,000 lines of code) than considered in this work.

**Sanitizer Correctness**: Balzarotti et al. show that custom sanitizer routines are often incorrectly implemented [2]. Livshits et al. developed methods for determining which functions in a program play the role of sanitizer. Their Merlin system is also capable of detecting missing sanitizers [19]. ScriptGard’s analysis is complementary to these works. Sanitizers may be present and functionally correct for contexts they are intended to be used in, but incorrect placement can introduce errors.

The Cross-Site Scripting Cheat Sheet shows over two hundred examples of strings that exercise common corner cases of web sanitizers [29]. The Bek project proposes a systematic domain-specific language for writing and checking sanitizers [15, 33].

### Conclusions

We analyzed a set of 53 large web pages in a large-scale web application with over 400,000 lines of code. Each page contained 350–900 DOM nodes. We found 285 multiple-encoding issues, as well as 1,207 instances of inconsistent sanitizers, establishing the prevalence of our two new problem classes and the effectiveness of ScriptGard as a testing aid. With preferential path profiling, when used for mitigation, ScriptGard incurs virtually no statistically significant overhead on cached paths.

### Acknowledgments

We would like to give special thanks to Patrice Godefroid, who has been most instrumental in helping us define the problems. Kapil Vaswani made the ppp tool work with our application. We also thank Kapil Vaswani, David Wagner, Herman Venter, Joel Weinberger, Peli de Halleux, Nikolai Tillmann, Devdatta Akhawe, Adrian Mettler, Dawn Song, our anonymous reviewers, and other Microsoft colleagues. The first author performed this work while interning at Microsoft Research.

### References

[1] M. Balduzzi, C. Gimenez, D. Balzarotti, and E. Kirda. Automated discovery of parameter pollution vulnerabilities in web applications. In Proceedings of the Network and Distributed System Security Symposium, 2011.

[2] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic, E. Kirda, C. Kruegel, and G. Vigna. Saner: Composing Static and Dynamic Analysis to Validate Sanitization in Web Applications. In Proceedings of the IEEE Symposium on Security and Privacy, Oakland, CA, May 2008.

[3] D. Bates, A. Barth, and C. Jackson. Regular expressions considered harmful in client-side XSS filters. International World Wide Web Conference, 2010.

[4] P. Bisht and V. N. Venkatakrishnan. XSS-GUARD: precise dynamic prevention of cross-site scripting attacks. In Detection of Intrusions and Malware, and Vulnerability Assessment, 2008.

[5] H. Bojinov, E. Bursztein, and D. Boneh. XCS: Cross channel scripting and its impact on web applications. In CCS, 2009.

[6] S. Chen, D. Ross, and Y.-M. Wang. An analysis of browser domain-isolation bugs and a light-weight defense mechanism. In Proceedings of the Conference on Computer and Communications Security, October 2007.

[7] T. Chilimbi, B. Liblit, K. Mehra, A. V. Nori, and K. Vaswani. Holmes: Effective statistical debugging via efficient path profiling. In Proceedings of the International Conference on Software Engineering, May 2009.

[8] D. Connolly and C. M. Sperberg-McQueen. Web addresses in HTML 5. http://www.w3.org/html/wg/href/draft#ref-RFC3986, 2009.

[9] Fortify, Inc. Fortify SCA. http://www.fortifysoftware.com/products/sca/, 2006.

[10] Google. Google auto escape. http://google-ctemplate.googlecode.com/svn/trunk/doc/auto_escape.html, 2011.

[11] M. V. Gundy and H. Chen. Noncespaces: using randomization to enforce information flow tracking and thwart cross-site scripting attacks. 2009.

[12] V. Haldar, D. Chandra, and M. Franz. Dynamic taint propagation for Java. In Proceedings of the Annual Computer Security Applications Conference, Dec. 2005.

[22] Microsoft Corporation. Microsoft code analysis tool .NET, 2009. http://www.microsoft.com/downloads/en/details.aspx?FamilyId=0178e2ef-9da8-445e-9348-c93f24cc9f9d&displaylang=en.

[23] Microsoft Corporation. String class (system), 2010. http://msdn.microsoft.com/en-us/library/system.string.aspx.

[24] Microsoft Corporation. StringBuilder class, 2010. http://msdn.microsoft.com/en-us/library/system.text.stringbuilder(v=VS.80).aspx.

[25] Y. Nadji, P. Saxena, and D. Song. Document structure integrity: A robust basis for cross-site scripting defense. In Proceedings of the Network and Distributed System Security Symposium, 2009.

[26] A. Nguyen-Tuong, S. Guarnieri, D. Greene, J. Shirley, and D. Evans. Automatically hardening Web applications using precise tainting. In Proceedings of the IFIP International Information Security Conference, 2005.

[27] Open Web Application Security Project. The ten most critical Web application security vulnerabilities. http://umn.dl.sourceforge.net/sourceforge/owasp/OWASPTopTen2004.pdf, 2004.

[13] W. G. Halfond, A. Orso, and P. Manolios. WASP: Protecting web applications using positive tainting and syntax-aware evaluation. IEEE Transactions on Software Engineering, 34(1), 2008.

[28] T. Pietraszek and C. V. Berghe. Defending against injection attacks through context-sensitive string evaluation. In Proceedings of the Recent Advances in Intrusion Detection, Sept. 2005.

[14] Y. HASEGAWA. UTF-7 XSS cheat sheet. http://openmya.hacker.jp/hasegawa/security/utf7cs.html, 2005.

[15] P. Hooimeijer, B. Livhsits, D. Molnar, P. Saxena, and M. Veanes. Fast and precise sanitizer analysis with bek. In Proceedings of the Usenix Security Symposium, Aug. 2011.

[16] N. Jovanovic, C. Kruegel, and E. Kirda. Pixy: A static analysis tool for detecting Web application vulnerabilities (short paper). In Proceedings of the IEEE Symposium on Security and Privacy, 2006.

[29] RSnake. XSS cheat sheet for filter evasion. http://ha.ckers.org/xss.html.

[30] B. Schmidt. google-analytics-xss-vulnerability, 2011. http://spareclockcycles.org/2011/02/03/google-analytics-xss-vulnerability/.

[31] E. J. Schwartz, T. Avgerinos, and D. Brumley. All you ever wanted to know about dynamic taint analysis and forward symbolic execution (but might have been afraid to ask). In Proceedings of the IEEE Symposium on Security and Privacy, 2010.

[17] A. Kie˙zun, V. Ganesh, P. J. Guo, P. Hooimeijer, and M. D. Ernst. HAMPI: A solver for string constraints. In International Symposium on Software Testing and Analysis, 2009.

[18] B. Livshits and M. S. Lam. Finding security errors in Java programs with static analysis. In Proceedings of the Usenix Security Symposium, 2005.

[19] B. Livshits, A. V. Nori, S. K. Rajamani, and A. Banerjee. Merlin: Specification inference for explicit information flow problems. In Proceedings of the Conference on Programming Language Design and Implementation, June 2009.

[20] M. T. Louw and V. N. Venkatakrishnan. Blueprint: Robust prevention of cross-site scripting attacks for existing browsers. In Proceedings of the IEEE Symposium on Security and Privacy, 2009.

[21] M. Martin, B. Livshits, and M. S. Lam. SecuriFly: runtime vulnerability protection for Web applications. Technical report, Stanford University, 2006.

[33] M. Veanes, B. Livshits, and D. Molnar. Decision procedures for composition and equivalence of symbolic finite state transducers. Technical Report MSR-TR-2011-32, Microsoft Research, 2011.

[34] H. Venter. Common compiler infrastructure: Metadata API, 2010. http://ccimetadata.codeplex.com/.

[35] Y. Xie and A. Aiken. Static detection of security vulnerabilities in scripting languages. In Proceedings of the Usenix Security Symposium, 2006.

[36] W. Xu, S. Bhatkar, and R. Sekar. Taint-enhanced policy enforcement: A practical approach to defeat a wide range of attacks. In Proceedings of the Usenix Security Symposium, 2006.