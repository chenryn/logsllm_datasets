### Impact Score and Experimental Setup

**Impact Score:**
We use throughput as a measure to evaluate the average amount of data received over time. Similar to ESM, the impact score is calculated by subtracting the average throughput over the last 35 seconds from the streaming rate.

**Experimental Setup:**
- **System:** Scribe
- **Nodes:** 50
- **Scenario:**
  - A source node creates a group.
  - The source node publishes streaming data at a rate of 1 Mbps.
  - All other nodes subscribe to this group.
- **Malicious Actions:**
  - Malicious actions are initiated immediately after the experiment starts to attack tree construction.
  - The tree formation takes up to 30 seconds in our test environment, so we set the window \( t_w \) to 35 seconds.
  - We find that malicious actions have a high probability of being effective on the first attempt, so we set the number of attempts \( n_a \) to 1.

### Attacks Found Using Throughput

We identified seven attacks using throughput as the impact score. Figure 12(a) illustrates the effects of these different attacks. As a baseline, we ran the system without any attacks and found that nodes consistently receive 1 Mbps of data.

**Drop Data and Dup Data:**
- **Drop Data:** Nodes do not forward the data.
- **Dup Data:** Nodes duplicate data messages and send the second message to a random node. This can cause loops, leading to significant system load as data is increasingly replicated, resulting in throughput dropping below 200 kbps.

**Dup Join, Lie GroupID Join, Drop Join:**
- **Dup Join:** Malicious nodes duplicate Join messages and divert the second message to a random node, causing throughput to drop below 200 kbps. This drop is due to temporary forwarding loops when a tree node is a child of multiple parent nodes. The heartbeat protocol corrects this error, but only after a period during which forwarding loops can cause damage.
- **Lie GroupID Join:** Malicious nodes lie about the group identifier in the Join message, effectively joining a different group while believing they are joining the requested group. This leads to a situation where all malicious nodes fail to join, and some benign nodes build a tree under malicious nodes (Figure 12(b)). Since the tree is split, only nodes in the tree with the source node inside can receive data, while nodes in other trees cannot receive any data.
- **Drop Join:** Malicious nodes drop Join messages, causing the same effect. In the explored simulation, more benign nodes happened to be part of the tree with the source, allowing better throughput.

### Related Work

**Automated Debugging Techniques:**
- **Model Checking:** Automated debugging techniques like model checking have been in use for many years. CrystalBall [56] by Yabandeh et al. uses state exploration to predict safety violations and steer the execution path away from them. Nodes predict consequences of their actions by executing a limited state exploration on a recently taken snapshot. While CrystalBall utilizes the Mace model checker for safety properties and state exploration, it does not consider performance metrics and thus can only find bugs or vulnerabilities that cause safety violations, not performance degradation.

**Attack Discovery and Prevention:**
- **Fault Injection:** Banabic et al. [11] employ fault injection on PBFT [15] to find combinations of MAC corruptions that cause nodes to crash. Our work is more general, as Gatling searches for basic malicious actions to find new attacks.
- **Protocol Gullibility:** Stanojevic et al. [50] develop a fault injection technique to automatically search for gullibility in protocols. They experiment on the two-party protocol ECN to find attacks that cause a malicious receiver to speed up and slow down the sending of data. Their technique uses a brute force search and considers lying about the fields in the headers of packets and also dropping packets. Unlike Gatling, their approach is limited to specific protocols and uses a brute force method.
- **Lying Attacks:** Kothari et al. [32] explore how to automatically find lying attacks that manipulate control flow in implementations of protocols written in C. Their technique reduces the search space using static analysis and then verifies the attack through concrete execution. However, users must know ahead of time what parts of the code, if executed many times, would cause an attack, which may not always be obvious. Gatling, on the other hand, uses an impact score to direct its search and can discover attacks that involve lying about state, which might go undiscovered by this technique.

### Conclusion

Securing distributed systems against performance attacks has traditionally been a manual process. Gatling automates this process by discovering performance attacks using a model-checking exploration approach on malicious actions. We implemented Gatling for the Mace toolkit, and once the system is implemented in Mace, the user needs to specify an impact score in a simulation driver to run the system in the simulator.

To demonstrate the generality and effectiveness of Gatling, we applied it to six distributed systems with diverse goals. We discovered 41 attacks in total, and for each system, we were able to automatically discover attacks that either stopped the system from achieving its goals or significantly slowed down progress. While some of these attacks have been previously found manually, Gatling finds them in a much shorter time. Therefore, Gatling can help accelerate the development of secure distributed systems.

### References

[1] Cyber-DEfense Technology Experimental Research laboratory Testbed. http://www.isi.edu/deter/.
[2] Emulab - Network Emulation. http://www.emulab.net/.
[3] Georgia Simulator. http://www.ece.gatech.edu/research/labs/MANIACS/GTNetS/.
[4] Global Environment for Network Innovation. http://www.geni.net.
[5] Network Simulator 3. http://www.nsnam.org/.
[6] p2psim: A simulator for peer-to-peer protocols. http://pdos.csail.mit.edu/p2psim/.
[7] Resilient Overlay Networks. http://nms.csail.mit.edu/ron/.
[8] J. Antunes, N. Neves, M. Correia, P. Verissimo, and R. Neves. Vulnerability Discovery with Attack Injection. IEEE Transactions on Software Engineering, 36:357–370, 2010.
[9] A. Armando, D. Basin, Y. Boichut, Y. Chevalier, L. Compagna, J. Cuellar, P. H. Drielsma, P. Hem, O. Kouchnarenko, J. Mantovani, S. Mdersheim, D. von Oheimb, M. Rusinowitch, J. Santiago, M. Turuani, L. Vigan, and L. Vigneron. The AVISPA Tool for the Automated Validation of Internet Security Protocols and Applications. In Proceedings of Computer Aided Verification, 2005.
[10] A. Armando and L. Compagna. SAT-based model-checking for security protocols analysis. International Journal of Information Security, 7:3–32, January 2008.
[11] R. Banabic, G. Candea, and R. Guerraoui. Automated Vulnerability Discovery in Distributed Systems. In Proceedings of HotDep, 2011.
[12] B. Blanchet. From Secrecy to Authenticity in Security Protocols. In Proceedings of International Static Analysis Symposium. Springer, 2002.
[13] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs. In Proceedings of OSDI, 2008.
[14] M. Castro, P. Drushel, A. Ganesh, A. Rowstron, and D. Wallach. Secure routing for structured peer-to-peer overlay networks. In Proceedings of OSDI, 2002.
[15] M. Castro and B. Liskov. Practical Byzantine fault tolerance. In Proceedings of OSDI, 1999.
[16] C. Y. Cho, D. Babi, P. Poosankam, K. Z. Chen, E. X. Wu, and D. Song. MACE: Model-inference-assisted concolic exploration for protocol and vulnerability discovery. In Proceedings of USENIX Security, 2011.
[17] Y.-H. Chu, A. Ganjam, T. S. E. Ng, S. Rao, K. Sripanidkulchai, J. Zhan, and H. Zhang. Early Experience with an Internet Broadcast System Based on Overlay Multicast. In Proceedings of USENIX ATC, 2004.
[18] B. Cohen. Incentives build robustness in BitTorrent. In Proceedings of P2P Economics, 2003.
[19] F. Dabek, R. Cox, F. Kaashoek, and R. Morris. Vivaldi: a decentralized network coordinate system. In Proceedings of SIGCOMM, 2004.
[20] D. Geels, G. Altekar, P. Maniatis, T. Roscoe, and I. Stoica. Friday: global comprehension for distributed replay. In Proceedings of NSDI, 2007.
[21] P. Godefroid. Model checking for programming languages using Verisoft. In Proceedings of POPL, 1997.
[22] P. Godefroid, M. Y. Levin, and D. Molnar. Automated Whitebox Fuzz Testing. In Proceedings of NDSS, 2008.
[23] K. P. Gummadi, S. Saroiu, and S. D. Gribble. King: Estimating Latency between Arbitrary Internet End Hosts. In Proceedings of ACM SIGCOMM-IMW, 2002.
[24] H. S. Gunawi, T. Do, P. Joshi, P. Alvaro, J. M. Hellerstein, A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, K. Sen, and D. Borthakur. FATE and DESTINI: a framework for cloud recovery testing. In Proceedings of NSDI, 2011.
[25] G. J. Holzmann. The Model Checker SPIN. IEEE Transactions on Software Engineering, 23:279–295, May 1997.
[26] C. Killian, J. W. Anderson, R. Jhala, and A. Vahdat. Life, Death, and the Critical Transition: Detecting Liveness Bugs in Systems Code. In Proceedings of NSDI, 2007.
[27] C. E. Killian, J. W. Anderson, R. Braud, R. Jhala, and A. M. Vahdat. Mace: Language support for building distributed systems. In Proceedings of PLDI, 2007.
[28] D. Kostić, R. Braud, C. Killian, E. Vandekieft, J. W. Anderson, A. C. Snoeren, and A. Vahdat. Maintaining high bandwidth under dynamic network conditions. In Proceedings of USENIX ATC, 2005.
[29] D. Kostic, A. Rodriguez, J. Albrecht, and A. Vahdat. Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh. In Proceedings of SOSP, 2003.
[30] D. Kostic, A. Rodriguez, J. Albrecht, A. Bhirud, and A. Vahdat. Using random subsets to build scalable network services. In Proceedings of USENIX-USITS, 2003.
[31] D. Kostić, A. C. Snoeren, A. Vahdat, R. Braud, C. Killian, J. W. Anderson, J. Albrecht, A. Rodriguez, and E. Vandekieft. High-bandwidth data dissemination for large-scale distributed systems. ACM Transactions on Computer Systems, 26(1):1–61, 2008.
[32] N. Kothari, R. Mahajan, T. Millstein, R. Govindan, and M. Musuvathi. Finding Protocol Manipulation Attacks. In Proceedings of SIGCOMM, 2011.
[33] M. Krohn, E. Kohler, and M. F. Kaashoek. Events can make sense. In Proceedings of USENIX ATC, 2007.
[34] L. Lamport. Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2002.
[35] L. Leonini, E. Riviére, and P. Felber. SPLAY: distributed systems evaluation made simple (or how to turn ideas into live systems in a breeze). In Proceedings of NSDI, 2009.
[36] S. Lin, A. Pan, Z. Zhang, R. Guo, and Z. Guo. WiDS: an Integrated Toolkit for Distributed Systems Deveopment. In Proceedings of HotOS, 2005.
[37] Z. Lin, X. Zhang, and D. Xu. Convicting exploitable software vulnerabilities: An efficient input provenance based approach. In Proceedings of DSN, 2008.
[38] B. T. Loo, T. Condie, J. M. Hellerstein, P. Maniatis, T. Roscoe, and I. Stoica. Implementing Declarative Overlays. In Proceedings of SOSP, Brighton, United Kingdom, October 2005.
[39] X. Lui, W. Lin, A. Pan, and Z. Zhang. WiDS Checker: Combating Bugs In Distributed Systems. In Proceedings of NSDI, Cambridge, Massachusetts, April 2007.
[40] N. Lynch. Distributed Algorithms. Morgan Kaufmann, 1996.
[41] M. Musuvathi, D. Park, A. Chou, D. Engler, and D. Dill. CMC: A pragmatic approach to model checking real code. In Proceedings of OSDI, 2002.
[42] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, and I. Neamtiu. Finding and Reproducing Heisenbugs in Concurrent Programs. In Proceedings of OSDI, 2008.
[43] J. Newsome and D. Song. Dynamic Taint Analysis for Automatic Detection, Analysis, and Signature Generation of Exploits on Commodity Software. In Proceedings of NDSS, 2005.
[44] PlanetLab. http://www.planetlab.org.
[45] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A scalable content-addressable network. In Proceedings of SIGCOMM. ACM, 2001.
[46] S. Rhea, D. Geels, T. Roscoe, and J. Kubiatowicz. Handling churn in a DHT. In Proceedings of USENIX ATC, 2004.
[47] A. Rodriguez, D. Kostić, Dejan, and A. Vahdat. Scalability in Adaptive Multi-Metric Overlays. In Proceedings of IEEE ICDCS, 2004.
[48] A. Rowstron and P. Druschel. Pastry: Scalable, Decentralized Object Location, and Routing for Large-Scale Peer-to-Peer Systems. In Proceedings of IFIP/ACM Middleware, 2001.
[49] A. Rowstron, A.-M. Kermarrec, M. Castro, and P. Druschel. SCRIBE: The design of a large-scale event notification infrastructure. In Proceedings of NGC, 2001.
[50] M. Stanojevic, R. Mahajan, T. Millstein, and M. Musuvathi. Can You Fool Me? Towards Automatically Checking Protocol Gullibility. In Proceedings of HotNets, 2008.
[51] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and H. Balakrishnan. Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications. In Proceedings of SIGCOMM, 2001.
[52] A. Vahdat, K. Yocum, K. Walsh, P. Mahadevan, D. Kostić, J. Chase, and D. Becker. Scalability and accuracy in a large-scale network emulator. In Proceedings of OSDI, 2002.
[53] A. Walters, D. Zage, and C. Nita-Rotaru. A Framework for Mitigating Attacks Against Measurement-Based Adaptation Mechanisms in Unstructured Multicast Overlay Networks. IEEE/ACM Transactions on Networking, 16:1434–1446, 2008.
[54] W. Wang, Y. Lei, D. Liu, D. Kung, C. Csallner, D. Zhang, R. Kacker, and R. Kuhn. A Combinatorial Approach to Detecting Buffer Overflow Vulnerabilities. In Proceedings of DSN, 2011.
[55] M. Welsh, D. E. Culler, and E. A. Brewer. SEDA: An Architecture For Well-conditioned, Scalable Internet Services. In Proceedings of SOSP, 2001.
[56] M. Yabandeh, N. Knezevic, D. Kostic, and V. Kuncak. CrystalBall: Predicting and Preventing Inconsistencies in Deployed Distributed Systems. In Proceedings of NSDI, 2009.
[57] J. Yang, T. Chen, M. Wu, Z. Xu, X. Liu, H. Lin, M. Yang, F. Long, L. Zhang, and L. Zhou. MODIST: transparent model checking of unmodified distributed systems. In Proceedings of NSDI, 2009.
[58] D. J. Zage and C. Nita-Rotaru. On the accuracy of decentralized virtual coordinate systems in adversarial networks. In Proceedings of CCS, 2007.