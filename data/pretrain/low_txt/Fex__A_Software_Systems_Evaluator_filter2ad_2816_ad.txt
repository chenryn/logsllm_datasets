# Compiler Performance and Security Benchmark Results

| Compiler | Successful Attacks | Failed Attacks |
|----------|---------------------|----------------|
| Native (GCC) | 64 | 786 |
| Native (Clang) | 38 | 812 |

**Table II: RIPE Security Benchmark Results Produced by FEX.**  
Columns 2 and 3 show the number of successful and failed attacks, respectively. Clang has almost twice as few successful attacks due to its smarter layout of objects in the BSS and Data segments, which prevents indirect attacks via buffers in these segments.

## V. Related Work

### Benchmark Suites
Systems researchers have developed a wide range of benchmark suites, varying in age, targeting, and diversity. Dhrystone, a 30-year-old synthetic integer benchmark suite, is still widely used [23]. However, due to its age and atypical code for modern programs, the Coremark suite was developed in 2009 as a substitute [24]. Both Dhrystone and Coremark are targeted for embedded systems and have limited program diversity.

Recently, new benchmark suites have been released, covering more scenarios and stressing specific parts of systems, such as floating-point operations and instruction/data cache pressure [2, 14, 25, 26]. For example, MiBench is a comprehensive set of 35 embedded applications targeting areas such as networking, security, automotive, and telecommunications [27].

Benchmark suites are also characterized by their targeting. LINPACK is designed for vectorizable computations [28], MediaBench for media applications [29], BioPerf for bioinformatics [30], MineBench for data mining [31], and HPC Challenge and NAS for high-performance computing [32, 33].

The focus of this work is on benchmarks that analyze the impact of static and dynamic instrumentation techniques. Therefore, benchmarks that test the entire hardware/software stack (e.g., Phoronix [34]) or large-scale systems (e.g., YCSB [35] and CloudSuite [8]) are not within the scope of FEX.

Recent research efforts have concentrated on evaluating the diversity and redundancy of benchmark suites. For instance, studies have analyzed the redundancy of SPEC and PARSEC benchmark suites to determine the minimum number of programs and inputs needed for statistically significant results [36, 37]. Some papers compare different benchmark suites, such as PARSEC and SPLASH [6, 38].

### Tools and Methodologies for Performance Measurements
To our knowledge, FEX is the only meta-framework that combines several benchmark suites. Other research tools aim to provide more stable, reproducible, and statistically significant performance evaluation results.

Measurement bias can lead to incorrect performance results [5]. Additionally, real workloads often exhibit abnormal behavior known as workload flurries, which can result in unstable results [39]. One solution is to "shake" the input workload to achieve a better distribution of results [40]. This approach can be seamlessly integrated into FEX.

Stabilizer is a tool that achieves statistically sound performance evaluation by re-randomizing the memory layout, which is the leading factor in measurement bias, to achieve a normal distribution of results [41]. Coz is another tool for better performance measurements, focusing on highlighting performance bottlenecks in complex software using causal profiling [42]. Stabilizer was evaluated only on SPEC CPU2006, and Coz only on PARSEC. Both tools could benefit from integration with FEX for quick evaluation on other benchmark suites.

Kalibera and Jones provide guidelines for performing statistically sound experiments with the minimum number of repetitions for each benchmark [43]. Users are encouraged to follow these guidelines when conducting experiments with FEX.

Another category of measurement tools includes profilers [44, 45] and tracers [46, 47]. These tools are primarily used for measuring various runtime parameters and analyzing performance bottlenecks. Some, like perf [44], provide basic results processing, such as mean values and standard deviations over multiple runs. However, they are single-application, single-configuration tools and cannot aggregate and compare measurement results from multiple benchmarks or different build configurations. They also do not control the experiment procedure. Therefore, FEX is orthogonal and complementary to this class of tools.

## VI. Conclusion and Future Work

FEX began as a small set of frequently reused scripts and has matured into a full-fledged evaluation framework. During months of internal use, we continuously refactored and expanded the framework to meet our growing needs, eventually deciding it could be useful for others. The result is FEX, an extensible, practical, and reproducible software system evaluation framework.

Currently, FEX has some limitations. It provides no statistical analysis functionality except for basic statistics such as standard deviation. We plan to integrate statistical numpy/scipy Python packages to enable advanced statistical methods and hypothesis testing.

We would like to combine FEX with a continuous integration system (e.g., Jenkins) to facilitate Evaluation-Driven Development, similar to Test-Driven Development. Additionally, we wish to support a graphical user interface to simplify and shorten the process of setting up and debugging experiments.

FEX currently supports only single-machine experiments. We are investigating ways to build distributed experiments, possibly using the Fabric library.

Since FEX relies on Docker infrastructure, we do not guarantee reproducibility below the user space level, i.e., on different hardware setups or across different kernel versions. However, FEX outputs various environment details, so the complete experimental setup is stored in the log file.

FEX is available at https://github.com/tudinfse/fex.

## References

[1] D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, and C. Fetzer, “SGXBounds: Memory Safety for Shielded Execution,” in EuroSys, 2017.
[2] J. L. Henning, “SPEC CPU2006 benchmark descriptions,” ACM SIGARCH Computer Architecture News, 2006.
[3] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. Kozyrakis, “Evaluating MapReduce for multi-core and multiprocessor systems,” in HPCA, 2007.
[4] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC benchmark suite: Characterization and architectural implications,” in PACT, 2008.
[5] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney, “Producing wrong data without doing anything obviously wrong!” in ASPLOS, 2009.
[6] C. Bienia, S. Kumar, and K. Li, “PARSEC vs. SPLASH-2: A Quantitative Comparison of Two Multithreaded Benchmark Suites on Chip-Multiprocessors,” in WWC, 2008.
[7] V. Saxena, Y. Sabharwal, and P. Bhatotia, “Performance evaluation and optimization of random memory access on multicores with high productivity,” in HiPC, 2010.
[8] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee, D. Jevdjic, C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsaﬁ, “Clearing the clouds: a study of emerging scale-out workloads on modern hardware,” in ASPLOS, 2012.
[9] C. Collberg and T. A. Proebsting, “Repeatability in computer systems research,” Communications of the ACM, 2016.
[10] G. R. Brammer, R. W. Crosby, S. J. Matthews, and T. L. Williams, “Paper mâché: Creating dynamic reproducible science,” Procedia Computer Science, 2011.
[11] S. Perianayagam, G. R. Andrews, and J. H. Hartman, “Rex: A toolset for reproducing software experiments,” in BIBM, 2010.
[12] D. Merkel, “Docker: Lightweight linux containers for consistent development and deployment,” Linux Journal, 2014.
[13] C. Boettiger, “An introduction to docker for reproducible research,” SIGOPS OS Review, 2015.
[14] C. Sakalis, C. Leonardsson, S. Kaxiras, and A. Ros, “Splash-3: A properly synchronized benchmark suite for contemporary research,” in ISPASS, 2016.
[15] “nginx: The Architecture of Open Source Applications,” http://www.aosabook.org/en/nginx.html, 2016, accessed: Oct, 2016.
[16] J. Wilander, N. Nikiforakis, Y. Younan, M. Kamkar, and W. Joosen, “RIPE: Runtime intrusion prevention evaluator,” in ACSAC, 2011.
[17] W. McKinney, “pandas: a foundational python library for data analysis and statistics,” Python for High Performance and Scientific Computing, 2011.
[18] J. D. Hunter, “Matplotlib: A 2D graphics environment,” Computing in Science Engineering, 2007.
[19] Jekyll, “Directory structure – Jekyll,” https://jekyllrb.com/docs/structure/, accessed: Dec, 2016.
[20] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “AddressSanitizer: A fast address sanity checker,” in ATC, 2012.
[21] “Apache HTTP server project,” http://httpd.apache.org/, 2016, accessed: Oct, 2016.
[22] C. Lattner and V. Adve, “LLVM: A compilation framework for lifelong program analysis and transformation,” in CGO, 2004.
[23] R. P. Weicker, “Dhrystone: A synthetic systems programming benchmark,” Communications of ACM, 1984.
[24] “EEMBC – CoreMark Processor Benchmark,” http://www.eembc.org/coremark/, 2016, accessed: Nov, 2016.
[25] C. Bienia and K. Li, “PARSEC 2.0: A new benchmark suite for chip-multiprocessors,” in MoBS, 2009.
[26] R. M. Yoo, A. Romano, and C. Kozyrakis, “Phoenix Rebirth: Scalable MapReduce on a Large-scale Shared-memory System,” in WWC, 2009.
[27] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge, and R. B. Brown, “MiBench: A Free, Commercially Representative Embedded Benchmark Suite,” in WWC, 2001.
[28] J. J. Dongarra, P. Luszczek, and A. Petitet, “The LINPACK benchmark: Past, present, and future,” Concurrency and Computation: Practice and Experience, 2003.
[29] C. Lee, M. Potkonjak, and W. H. Mangione-Smith, “MediaBench: A Tool for Evaluating and Synthesizing Multimedia and Communications Systems,” in MICRO, 1997.
[30] D. Bader, Y. Li, T. Li, and V. Sachdeva, “BioPerf: A Benchmark Suite to Evaluate High-Performance Computer Architecture on Bioinformatics Applications,” in WWC, 2005.
[31] R. Narayanan, B. Ozisikyilmaz, J. Zambreno, G. Memik, and A. Choudhary, “Minebench: A benchmark suite for data mining workloads,” in WWC, 2006.
[32] P. R. Luszczek, D. H. Bailey, J. J. Dongarra, J. Kepner, R. F. Lucas, R. Rabenseifner, and D. Takahashi, “The HPC Challenge (HPCC) Benchmark Suite,” in SC, 2006.
[33] D. H. Bailey, E. Barszcz, J. T. Barton, D. S. Browning, R. L. Carter, L. Dagum, R. A. Fatoohi, P. O. Frederickson, T. A. Lasinski, R. S. Schreiber, H. D. Simon, V. Venkatakrishnan, and S. K. Weeratunga, “The NAS Parallel Benchmarks—Summary and Preliminary Results,” in SC, 1991.
[34] “Phoronix test suite,” http://www.phoronix-test-suite.com/, 2016, accessed: Nov, 2016.
[35] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears, “Benchmarking cloud serving systems with YCSB,” in SoCC, 2010.
[36] A. Phansalkar, A. Joshi, and L. K. John, “Analysis of redundancy and application balance in the SPEC CPU2006 benchmark suite,” in ISCA, 2007.
[37] C. Bienia and K. Li, “Fidelity and scaling of the PARSEC benchmark inputs,” in WWC, 2010.
[38] N. Barrow-Williams, C. Fensch, and S. Moore, “A Communication Characterization of SPLASH-2 and PARSEC,” in WWC, 2009.
[39] D. Tsafrir and D. G. Feitelson, “Instability in parallel job scheduling simulation: The role of workload flurries,” in IPDPS, 2006.
[40] D. Tsafrir, K. Ouaknine, and D. G. Feitelson, “Reducing performance evaluation sensitivity and variability by input shaking,” in MASCOTS, 2007.
[41] C. Curtsinger and E. D. Berger, “Stabilizer: Statistically sound performance evaluation,” in ASPLOS, 2013.
[42] C. Curtsinger and E. Berger, “Coz: Finding code that counts with causal profiling,” in SOSP, 2015.
[43] T. Kalibera and R. Jones, “Rigorous benchmarking in reasonable time,” in ISMM, 2013.
[44] “perf: Linux profiling with performance counters,” https://perf.wiki.kernel.org, 2017, accessed: Apr, 2017.
[45] J. Reinders, VTune performance analyzer essentials. Intel Press, 2005.
[46] R. McDougall, J. Mauro, and B. Gregg, Solaris Performance and Tools: DTrace and MDB Techniques for Solaris 10 and OpenSolaris. Prentice Hall PTR, 2006.
[47] M. Desnoyers and M. R. Dagenais, “The LTTng tracer: A low impact performance and behavior monitor for GNU/Linux,” in OLS, 2006.
[48] O. Oleksenko, D. Kuvaiskii, P. Bhatotia, P. Felber, and C. Fetzer, “Intel MPX Explained: An Empirical Study of Intel MPX and Software-based Bounds Checking Approaches,” arXiv:1702.00719, 2017.