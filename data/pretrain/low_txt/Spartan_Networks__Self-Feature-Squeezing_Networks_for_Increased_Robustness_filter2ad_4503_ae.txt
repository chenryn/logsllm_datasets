# Spartan Networks: Robustness and Performance Tradeoffs

## Introduction
This document provides an in-depth analysis of Spartan Networks, a novel deep learning architecture designed to enhance robustness against adversarial attacks. The following sections will cover the performance and robustness tradeoffs, risk evaluation, and future work.

## 1. Comparison of Spartan Network vs. Standard CNN

### Figure 7: FGSM Attack with Varying Epsilon-Strength
- **Epsilon Attack Strength Values**: 0.0, 0.1, 0.2, 0.5, 0.6, 0.3, 0.4
- **Model Accuracies**:
  - Candidate A: 98.84%
  - Candidate B: 96.13%
  - Candidate C: 98.52%
  - Standard CNN: 99.06%

### Figure 8: Loss History During Training
- **Observation**: The Spartan Network shows a latency in the loss drop during training.
- **Explanation**: The synthetic gradient allows the Spartan Network to know the general direction of improvement, enabling it to train even if the Heaviside step function does not yield a progressive, differentiable behavior. After sufficient training, the network's weights are trained enough to surpass the threshold, resulting in a sharp drop in loss similar to a standard CNN.

## 2. Resistance to Over-capacity

### Analysis of Candidate C
- **Precision Collapse**: At ϵ values close to 0.5, Candidate C's precision begins to degrade.
- **Noise Level**: At this level, the noise is high enough to create a grey picture.
- **Candidate B's Resilience**: Despite the high noise, Candidate B maintains its performance beyond the 0.5 threshold due to a 0.9 cutoff threshold in its filters with minimal bias, focusing on the brightest pixels.

### Filter Discarding
- **Candidate C**: Discarded two filters by making their activation impossible, learning to ignore unnecessary capacity and minimizing the activation penalty.
- **Result**: This aligns with previous results by Xu et al. [43], indicating that 1-bit black and white color is sufficient for digit classification.

### Conclusion
- **Spartan Networks**: Show reduced sensitivity to over-capacity, trading robustness for slightly reduced performance.
- **Hyperparameter Tuning**: The introduced hyperparameters do not significantly slow down the tuning phase.

## 3. Risk Evaluation of Robustness-Performance Tradeoff

### Context
- **Scenario**: A 4-digit check reading system where an adversarial check is misclassified as a $9999 check.
- **Risk Delta (ΔCN→SN)**: 
  \[
  \Delta CN \rightarrow SN = (p_e^{SN} I_e + p_t^{SN} I_t) - (p_e^{CN} I_e + p_t^{CN} I_t)
  \]
  Where \( I \) is the average impact value, \( p \) is the probability of an event, and \( SN \) and \( CN \) stand for Spartan Network and Convolutional Network, respectively. \( t \) and \( e \) describe theft and error scenarios.

### Simplified Risk Delta
- **Impact Consideration**: The impact remains constant, only the probability changes.
  \[
  \Delta CN \rightarrow SN = (p_e^{SN} - p_e^{CN}) I_e + (p_t^{SN} - p_t^{CN}) I_t
  \]

### Abnormal Check Reading
- **Alpha (α)**: Proportion of malicious bank checks in non-normal checks.
  \[
  \Delta CN \rightarrow SN = (1 - \alpha) \Delta_{err} I_e + \alpha \Delta_{adv} I_t
  \]
  To diminish risks, the condition is:
  \[
  \Delta CN \rightarrow SN < \frac{\Delta_{err} I_e}{\Delta_{err} I_e - \Delta_{adv} I_t}
  \]

### Example Calculation
- **FGSM Attack (ϵ = 0.3)**: Spartan Network is 20% more robust but has a ~-0.5% precision drop on non-adversarial inputs.
- **Cost Implications**: If a non-malicious error costs $50 and a malicious error costs $8999, Spartan Networks reduce risk when one erroneous check out of 7200 is malicious.

### Risk Management
- **Recommendation**: Use Spartan Networks in conjunction with attack detection strategies. A high-performance classical network can handle inference until an attack is detected, at which point a Spartan Network can be used as a fallback.

## 4. Conclusion

### Summary
- **Spartan Networks**: Deep neural networks with a data-starving layer to select relevant features, enhancing robustness against adversarial examples.
- **Tradeoff**: Filtering layers reduce performance but can be cost-effective in specific threat models, reducing the attacker's stealthiness and success rate.

### Contributions
- **Composite Activation Functions**: Separating forward and backward propagation functions.
- **Self-Adversarial Layer**: Introducing an attack-agnostic layer to starve subsequent layers of information.
- **Evaluation**: Performance-robustness tradeoff and robustness to black-box attackers in a simple threat model.

### Future Work
- **Experiments and Code**: Encouraging further research, code will be open-sourced at https://github.com/FMenet.
- **Replacement Gradients**: Exploring different gradients to improve update dynamics while retaining desirable behavior.
- **Complex Datasets**: Training on more complex datasets with deeper and wider architectures.
- **Sample Space Topology**: Studying Spartan Networks on harder sample space topologies.

## Acknowledgments
- **Colleagues**: Thanks to colleagues from the SecSI lab, especially Ranwa Al-Mallah for valuable comments and corrections.

## References
- [1] Fake News Detector AI | Detect Fake News Using Neural Networks. http://www.fakenewsai.com/
- [2] TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. https://www.tensorflow.org/
- [3] Abbasi, M., & Gagné, C. (2017). Robustness to Adversarial Examples through an Ensemble of Specialists. arXiv:1702.06856.
- [4] Akhtar, N., & Mian, A. (2018). Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey. IEEE Access 6, 14410–14430.
- [5] Athalye, A., Carlini, N., & Wagner, D. (2018). Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420.
- [6] Bojarski, M., et al. (2016). End to End Learning for Self-Driving Cars. arXiv:1604.07316.
- [7] Brendel, W., & Bethge, M. (2017). Comment on "Biologically inspired protection of deep networks from adversarial attacks". arXiv:1704.01547.
- [8] Buckman, J., et al. (2018). THERMOMETER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES.
- [9] Carlini, N., & Wagner, D. A. (2017). Towards Evaluating the Robustness of Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP), 39–57.
- [10] Chollet, F. (2015). Keras. https://keras.io
- [11] Courbariaux, M., et al. (2016). Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. arXiv:1602.02830.
- [12] Das, N., et al. (2017). Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression. ArXiv e-prints.
- [13] Drucker, H., & Le Cun, Y. (1991). Double backpropagation increasing generalization performance. In IJCNN-91-Seattle International Joint Conference on Neural Networks.
- [14] Elsayed, G. F., et al. (2018). Adversarial Examples that Fool both Computer Vision and Time-Limited Humans. arXiv:1802.08195.
- [15] Esteva, A., et al. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 7639, 115.
- [16] Feinman, R., et al. (2017). Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410.
- [17] Gilmer, J., et al. (2018). Adversarial Spheres. arXiv:1801.02774.
- [18] Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples. arXiv:1412.6572.
- [19] Grosse, K., et al. (2017). On the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280.
- [20] Grosse, K., et al. (2016). Adversarial Perturbations Against Deep Neural Networks for Malware Classification. Proceedings of the 2017 European Symposium on Research in Computer Security.
- [21] Gu, S., & Rigazio, L. (2014). Towards Deep Neural Network Architectures Robust to Adversarial Examples.
- [22] Gu, T., Dolan-Gavitt, B., & Garg, S. (2017). BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. arXiv:1708.06733.
- [23] Guo, C., et al. (2017). Countering Adversarial Images using Input Transformations. arXiv:1711.00117.
- [24] Hendrycks, D., & Gimpel, K. (2016). Early methods for detecting adversarial images. arXiv preprint arXiv:1608.00530.
- [25] Hu, W., & Tan, Y. (2017). Black-Box Attacks against RNN based Malware Detection Algorithms. arXiv:1705.08131.
- [26] LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition. Proc. IEEE 86, 11, 2278–2324.
- [27] Li, X., & Li, F. (2016). Adversarial examples detection in deep networks with convolutional filter statistics. CoRR, abs/1612.07767.
- [28] Lu, J., Issaranon, T., & Forsyth, D. (2017). Safetynet: Detecting and rejecting adversarial examples robustly. CoRR, abs/1704.00103.
- [29] Luo, Y., et al. (2015). Foveation-based Mechanisms Alleviate Adversarial Examples. CoRR abs/1511.06292.
- [30] Madry, A., et al. (2017). Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv:1706.06083.
- [31] Meng, D., & Chen, H. (2017). Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.
- [32] Metzen, J. H., et al. (2017). On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267.
- [33] Moosavi-Dezfooli, S.-M., et al. (2015). DeepFool: a simple and accurate method to fool deep neural networks. arXiv:1511.04599.
- [34] Nayebi, A., & Ganguli, S. (2017). Biologically inspired protection of deep networks from adversarial attacks. ArXiv e-prints.
- [35] Papernot, N., et al. (2018). Technical Report on the CleverHans v2.1.0 Adversarial Examples Library. arXiv preprint arXiv:1610.00768.
- [36] Papernot, N., & McDaniel, P. (2017). Extending Defensive Distillation. arXiv:1705.05264.
- [37] Papernot, N., et al. (2016). Practical Black-Box Attacks against Machine Learning. arXiv:1602.02697.
- [38] Papernot, N., et al. (2015). Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. arXiv:1511.04508.
- [39] Prakash, A., et al. (2018). Deflecting Adversarial Attacks with Pixel Deflection. CoRR abs/1801.08926.
- [40] Ross, A. S., & Doshi-Velez, F. (2017). Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients. arXiv:1711.09404.
- [41] Szegedy, C., et al. (2013). Intriguing properties of neural networks. arXiv:1312.6199.
- [42] Tingle, D., Kim, Y. E., & Turnbull, D. (2010). Exploring automatic music annotation with acoustically-objective tags. In Proceedings of the international conference on Multimedia information retrieval. ACM, 55–62.
- [43] Xu, W., Evans, D., & Qi, Y. (2018). Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. arXiv:1704.01155.
- [44] Xu, X., et al. (2017). Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection. Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security - CCS ’17, 363–376.