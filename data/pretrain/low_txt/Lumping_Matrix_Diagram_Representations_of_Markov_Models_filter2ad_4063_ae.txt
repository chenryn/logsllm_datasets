### Implementation and Integration

The overall algorithm is implemented in the M¨obius tool [7], and this implementation is integrated into the symbolic state-space generator (SSG) [10].

### Example: Tandem Multi-Processor System

We consider a tandem multi-processor system with load-balancing, failure, and repair operations. The system consists of two subsystems: the MSMQ and the hypercube.

#### Hypercube Subsystem

The hypercube subsystem comprises 8 cube-connected servers (Figure 5). All servers exhibit the same behavior, except for two special servers, A and A', which receive jobs from the input pool as described later. Servers can fail, and a single repair facility repairs them uniformly from the pool of failed servers. A failed server retains its jobs in its queue unless they are transferred to a neighboring server by the load-balancing scheme. The system is considered unavailable when two or more servers are down.

Each server has a queue with a capacity of J. Jobs enter the servers' queues via a dispatcher that assigns a job from the subsystem's input pool to either server A or A' based on a probability distribution that favors the server with fewer jobs. Additionally, a load-balancing scheme distributes jobs among the servers: if a server has more than one job than any of its neighbors, it sends one of its jobs to a neighbor, again assigning higher probability to servers with fewer jobs. If a server fails, it transfers jobs in its queue, one by one, with an exponentially distributed delay, to a neighboring server that is not down.

#### Model and Performance Results

We used the M¨obius tool [7] to model each subsystem using the stochastic activity network formalism [19]. The models were then composed by sharing their input and output pools via the Join operator in the Rep/Join composed model editor. Our implementation of the symbolic SSG [10] automatically partitions the set of places of the complete model and assigns each block of the partition to one level of the MD as follows:
1. Common places of the two submodels (i.e., input and output pools).
2. Places of the hypercube submodel, excluding those in level 1.
3. Places of the MSMQ submodel, excluding those in level 1.

**Performance Results**

All experiments were conducted using an Athlon XP2400 machine with 1.5 GB of main memory running Linux OS. The implementation was based on M¨obius version 1.6.0 and compiled with the gcc 3.3 compiler using the -O3 optimization option.

Table 1 provides information about the MD representation of the original (unlumped) and lumped CTMC of the tandem multi-processor system for different values of J. Specifically, the upper table shows the state-space size for each level and the complete model, along with the number of MD nodes in each level. The middle table shows the lumped state-space size for each level and the complete model, as well as the state-space reduction gained from the compositional lumping algorithm. The lower table gives computation times in seconds for the symbolic state-space generation and lumping algorithm, as well as memory use of the MDs of the unlumped and lumped MC in kilobytes.

From Table 1, we observe that the compositional lumping algorithm reduces the state-space size of the overall model to roughly 1/40 to 1/50 of its original value, which is approximately equal to the product of the reductions in state space for all levels. The equivalently behaving sets of servers—(1) the three servers of the MSMQ subsystem, (2) servers A and A' in the hypercube, and (3) the other six servers in the hypercube—are the source of the lumpability found by our compositional lumping algorithm.

Our algorithm does not necessarily generate the smallest possible lumped CTMC (or its MD representation) because it is applied locally at each level of the MD and lacks a global view of the CTMC. In the worst case, none of the levels of the MD satisfy the lumpability conditions for any non-trivial partition, so our lumping algorithm cannot reduce the state space size. For the given example, we verified that our compositional algorithm generates the smallest lumped CTMC possible by running the compositional algorithm result through our implementation of the state-level lumping algorithm [9].

**Reduction Effects on Numerical Solution Algorithms**

The reduction in state-space size has two major effects on the efficiency of iterative numerical solution algorithms that compute measures of CTMCs:
1. It reduces both the space and time requirements for such algorithms.
2. It makes the MD representation of the CTMC smaller, reducing the memory requirement for the MD by around an order of magnitude for all values of J.
3. It reduces the size of the solution vector, which, in our example, was reduced to no more than 1/40 of its original size.

Thus, the advantage of using our compositional lumping algorithm is that we can solve larger models than would be possible using only symbolic techniques. For our example, we solved models that are one or two orders of magnitude larger. The reduction in the state space also results in a roughly proportionate reduction in the amount of time spent for each iteration of the numerical solution algorithm.

It is important to note that all these benefits in terms of time and space requirements are achieved through an efficient algorithm in an amount of time that is negligible compared to the time needed for numerical analysis and considerably less than the time needed for state-space generation.

### Conclusion

In this paper, we presented theoretical results, algorithms, implementation, and results from exercising an application example for the compositional lumping of CTMCs represented as an MD. The key point is that nodes of each level are reduced separately from those of other levels, based on local conditions. Unlike previous compositional lumping algorithms that were formalism-dependent, our algorithm is applicable to any MD and thus to any formalism that uses MDs for its CTMC representation. We consider both ordinary and exact lumpability.

### Acknowledgments

We thank Jenny Applequist and several referees for their valuable remarks.

### References

[1] A. Benoit, L. Brenner, P. Fernandes, and B. Plateau. Aggregation of stochastic automata networks with replicas. In Linear Algebra and Its Applications, 386:111–136, 2004.

[2] P. Buchholz. Exact and ordinary lumpability in finite Markov chains. J. of App. Prob., 31:59–74, 1994.

[3] P. Buchholz. Equivalence relations for stochastic automata networks. In W. J. Stewart, editor, Computation with Markov Chains, Kluwer, 1995.

[4] G. Chiola, C. Dutheillet, G. Franceschinis, and S. Haddad. Stochastic well-formed colored nets and symmetric modeling applications. IEEE Trans. on Computers, 42(11):1343–1360, November 1993.

[5] G. Ciardo, R. Marmorstein, and R. Siminiceanu. Saturation unbound. In Proc. of TACAS, LNCS 2619, pages 379–393, Springer, 2003.

[6] G. Ciardo and A. Miner. A data structure for the efficient Kronecker solution of GSPNs. In Proc. of PNPM, pages 22–31, IEEE CS, 1999.

[7] D. D. Deavours, G. Clark, T. Courtney, D. Daly, S. Derisavi, J. M. Doyle, W. H. Sanders, and P. G. Webster. The M¨obius framework and its implementation. IEEE Trans. on Soft. Eng., 28(10):956–969, October 2002.

[8] C. Delamare, Y. Gardan, and P. Moreaux. Performance evaluation with asynchronously decomposable SWN: implementation and case study. In Proc. of PNPM, pages 20–29, IEEE CS, 2003.

[9] S. Derisavi, H. Hermanns, and W. H. Sanders. Optimal state-space lumping in Markov chains. Inf. Proc. Letters, 87(6):309–315, September 2003.

[10] S. Derisavi, P. Kemper, and W. H. Sanders. Symbolic state-space exploration and numerical analysis of state-sharing composed models. Linear Algebra and Its Applications, 386:137–166, July 2004.

[11] O. Gusak, T. Dayar, and J.-M. Fourneau. Lumpable continuous-time stochastic automata networks. European J. of Operational Research, 148:436–451, 2003.

[12] H. Hermanns. Interactive Markov Chains and the Quest for Quantified Quality, LNCS 2428, Springer 2002.

[13] J. G. Kemeney and J. L. Snell. Finite Markov Chains. D. Van Nostrand Company, Inc., 1960.

[14] M. Ajmone Marsan, G. Balbo, G. Conte, S. Donatelli, and G. Franceschinis. Modelling With Generalized Stochastic Petri Nets. John Wiley & Sons, 1995.

[15] A. Miner. Efficient solution of GSPNs using canonical matrix diagrams. In Proc. of PNPM, pages 101–110, IEEE CS, 2001.

[16] A. Miner and D. Parker. Symbolic representations and analysis of large probabilistic systems. In Validation of Stochastic Systems: A Guide to Current Research, LNCS 2925, Springer, 2004.

[17] B. Plateau and K. Atif. Stochastic automata network for modeling parallel systems. IEEE Trans. in Soft. Eng., 17(10):1093–1108, October 1991.

[18] W. H. Sanders and J. F. Meyer. Reduced base model construction methods for stochastic activity networks. IEEE J. on Selected Areas in Communications, 9(1):25–36, January 1991.

[19] W. H. Sanders and J. F. Meyer. Stochastic activity networks: Formal definitions and concepts. In Lectures on Formal Methods and Performance Analysis, LNCS 2090, Springer 2000.