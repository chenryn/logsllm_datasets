Certainly! Here is a more polished and coherent version of the provided text:

---

### Key Findings

- **Access to Training Data**: The discriminator has full access to the training data, which allows it to easily memorize private information. Similarly, the release of the generator and/or control over the input noise \( z \) also poses a significant privacy risk.
- **Vulnerability of Generative Models**: The susceptibility of different generative models to membership inference attacks (MIA) varies. While the effectiveness of MIA primarily depends on the generation quality of the victim model, the objective function and training paradigm also play crucial roles. Specifically, when data reconstruction is explicitly formulated in the training objective to improve data mode coverage (e.g., in VAEGAN and VAE), the resulting models become highly vulnerable to MIA.
- **Training Dataset Size**: A smaller training dataset increases the risk of revealing individual sample information. If the training set size is less than 10,000, most existing GAN models have sufficient modeling capacity to overfit to individual samples, leading to a high likelihood of compromising membership privacy once the GAN model and/or its generated sample set is released. This is particularly concerning for real-world privacy-sensitive datasets, such as medical records, which typically contain very limited data samples.
- **Differential Privacy Defense**: Applying differential privacy during GAN training is effective against practical MIA but comes at the cost of increased computational burden and reduced generation quality.

### Conclusion

We have established the first taxonomy of membership inference attacks against GANs, aiming to benchmark future research in this direction. We proposed the first generic attack model based on reconstruction, which is applicable to all settings depending on the attacker's knowledge about the victim model. The instantiated attack variants in the partial black-box and white-box settings bridge the assumption and performance gaps in previous work [25, 29]. Additionally, we introduced a novel, theoretically grounded attack calibration technique that consistently improves attack performance across various scenarios. Comprehensive experiments demonstrate consistent effectiveness and a broad spectrum of performance in diverse setups, including different dataset modalities, various victim models, two directions of analysis study, attack calibration, and differential privacy defense. These findings provide a better understanding of the privacy risks associated with deep generative models.

### Acknowledgments

This work was partially funded by the Helmholtz Association through the projects "Trustworthy Federated Data Analytics" (TFDA) (funding number ZT-I-OO1 4) and "Protecting Genetic Data with Synthetic Cohorts from Deep Generative Models" (PRO-GENE-GEN) (funding number ZT-I-PF-5-23).

### References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 308–318.
[2] Gergely Acs, Luca Melis, Claude Castelluccia, and Emiliano De Cristofaro. 2017. Differentially Private Mixture of Generative Neural Networks. In International Conference on Data Mining (ICDM). IEEE, 715–720.
[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein Generative Adversarial Networks. In International Conference on Machine Learning (ICML). JMLR, 214–223.
[4] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. 2017. Generalization and Equilibrium in Generative Adversarial Nets (GANs). In International Conference on Machine Learning (ICML). JMLR, 224–232.
[5] Michael Backes, Pascal Berrang, Mathias Humbert, and Praveen Manoharan. 2016. Membership Privacy in MicroRNA-based Studies. In ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 319–330.
[6] Michael Backes, Mathias Humbert, Jun Pang, and Yang Zhang. 2017. walk2friends: Inferring Social Links from Mobility Profiles. In ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 1943–1957.
[7] Brett K Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams, Ran Lee, Sanjeev P Bhavnani, James Brian Byrd, and Casey S Greene. 2019. Privacy-Preserving Generative Deep Neural Networks Support Clinical Data Sharing. Circulation: Cardiovascular Quality and Outcomes 12, 7 (2019), e005122.
[8] Apratim Bhattacharyya, Mario Fritz, and Bernt Schiele. 2019. “Best-of-Many-Samples” Distribution Matching. CoRR abs/1909.12598 (2019).
[9] Oren Boiman, Eli Shechtman, and Michal Irani. 2008. In Defense of Nearest-Neighbor based Image Classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE.
[10] Andrew Brock, Jeff Donahue, and Karen Simonyan. 2019. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In International Conference on Learning Representations (ICLR).
[11] Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. 2020. GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators. CoRR abs/2006.08265 (2020).
[12] Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, and Jimeng Sun. 2018. Generating Multi-label Discrete Patient Records using Generative Adversarial Networks. CoRR abs/1703.06490 (2018).
[13] Laurent Dinh, David Krueger, and Yoshua Bengio. 2015. NICE: Non-linear Independent Components Estimation. CoRR abs/1410.8516 (2015).
[14] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2017. Density Estimation using Real NVP. In International Conference on Learning Representations (ICLR).
[15] Richard O Duda, Peter E Hart, and David G Stork. 2012. Pattern Classification. John Wiley & Sons.
[16] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating Noise to Sensitivity in Private Data Analysis. In Theory of Cryptography Conference (TCC). Springer, 265–284.
[17] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differential Privacy. Now Publishers Inc.
[18] Cynthia Dwork, Adam D. Smith, Thomas Steinke, Jonathan Ullman, and Salil P. Vadhan. 2015. Robust Traceability from Trace Amounts. In Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 650–669.
[19] Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan. 2018. Synthetic Data Augmentation using GAN for Improved Liver Lesion Classification. In IEEE International Symposium on Biomedical Imaging (ISBI). IEEE, 289–293.
[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Annual Conference on Neural Information Processing Systems (NIPS). NIPS, 2672–2680.
[21] Alex Graves, Abdel rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech Recognition with Deep Recurrent Neural Networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6645–6649.
[22] Jinjin Gu, Yujun Shen, and Bolei Zhou. 2019. Image Processing Using Multi-Code GAN Prior. CoRR abs/1912.07116 (2019).
[23] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. 2017. Improved Training of Wasserstein GANs. In Annual Conference on Neural Information Processing Systems (NIPS). NIPS, 5767–5777.
[24] Inken Hagestedt, Yang Zhang, Mathias Humbert, Pascal Berrang, Haixu Tang, XiaoFeng Wang, and Michael Backes. 2019. MBeacon: Privacy-Preserving Beacons for DNA Methylation Data. In Network and Distributed System Security Symposium (NDSS). Internet Society.
[25] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019. LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks. Symposium on Privacy Enhancing Technologies Symposium (2019).
[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 770–778.
[27] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen. 2018. AttGAN: Facial Attribute Editing by Only Changing What You Want. CoRR abs/1711.10678 (2018).
[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Annual Conference on Neural Information Processing Systems (NIPS). NIPS, 6626–6637.
[29] Benjamin Hilprecht, Martin Härterich, and Daniel Bernau. 2019. Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models. Symposium on Privacy Enhancing Technologies Symposium (2019).
[30] Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. 2012. Deep Neural Networks for Acoustic Modeling in Speech Recognition. IEEE Signal Processing Magazine 29 (2012).
[31] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008. Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.
[32] Ali Jahanian, Lucy Chai, and Phillip Isola. 2019. On the “Steerability” of Generative Adversarial Networks. CoRR abs/1907.07171 (2019).
[33] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. 2019. MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples. In ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 259–274.
[34] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. MIMIC-III, A Freely Accessible Critical Care Database. Scientific Data 3 (2016), 160035.
[35] James Jordon, Jinsung Yoon, and Mihaela van der Schaar. 2019. PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees. OpenReview (2019).
[36] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive Growing of GANs for Improved Quality, Stability, and Variation. In International Conference on Learning Representations (ICLR).
[37] Tero Karras, Samuli Laine, and Timo Aila. 2019. A Style-Based Generator Architecture for Generative Adversarial Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 4401–4410.
[38] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and Improving the Image Quality of StyleGAN. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 8107–8116.
[39] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR).
[40] Diederik P. Kingma and Prafulla Dhariwal. 2018. Glow: Generative Flow with Invertible 1x1 Convolutions. In Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS, 10236–10245.
[41] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In International Conference on Learning Representations (ICLR).
[42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Annual Conference on Neural Information Processing Systems (NIPS). NIPS, 1106–1114.
[43] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. 2016. Autoencoding beyond Pixels Using a Learned Similarity Metric. In International Conference on Machine Learning (ICML). JMLR, 1558–1566.
[44] Chuan Li and Michael Wand. 2016. Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks. In European Conference on Computer Vision (ECCV). Springer, 702–716.
[45] Zheng Li and Yang Zhang. 2020. Label-Leaks: Membership Inference Attack with Label. CoRR abs/2007.15528 (2020).
[46] Dong C Liu and Jorge Nocedal. 1989. On the Limited Memory BFGS Method for Large Scale Optimization. Mathematical Programming 45, 1-3 (1989), 503–528.
[47] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face Attributes in the Wild. In IEEE International Conference on Computer Vision (ICCV). IEEE, 3730–3738.
[48] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen. 2018. Understanding Membership Inferences on Well-Generalized Learning Models. CoRR abs/1802.04889 (2018).
[49] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C. Courville, and Yoshua Bengio. 2017. SampleRNN: An Unconditional End-to-End Neural Audio Generation Model. In International Conference on Learning Representations (ICLR).
[50] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019. Exploiting Unintended Feature Leakage in Collaborative Learning. In IEEE Symposium on Security and Privacy (S&P). IEEE, 497–512.
[51] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning. In IEEE Symposium on Security and Privacy (S&P). IEEE, 1021–1035.
[52] Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. 2016. Context Encoders: Feature Learning by Inpainting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2536–2544.
[53] Michael JD Powell. 1964. An Efficient Method for Finding the Minimum of a Function of Several Variables without Calculating Derivatives. Comput. J. 7, 2 (1964), 155–162.
[54] Alec Radford, Luke Metz, and Soumith Chintala. 2015. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. CoRR abs/1511.06434 (2015).
[55] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. CoRR abs/1401.4082 (2014).
[56] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. CoRR abs/1409.0575 (2015).
[57] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Hervé Jégou. 2019. White-box vs Black-box: Bayes Optimal Strategies for Membership Inference. In International Conference on Machine Learning (ICML). JMLR, 5558–5567.
[58] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. 2019. ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. In Network and Distributed System Security Symposium (NDSS). Internet Society.
[59] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. Improved Techniques for Training GANs. In Annual Conference on Neural Information Processing Systems (NIPS). NIPS, 2226–2234.
[60] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership Inference Attacks Against Machine Learning Models. In IEEE Symposium on Security and Privacy (S&P). IEEE, 3–18.
[61] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. In International Conference on Learning Representations (ICLR).
[62] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going Deeper with Convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 1–9.
[63] Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude. COURSERA: Neural Networks for Machine Learning 4, 2 (2012), 26–31.
[64] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. CoRR abs/1609.03499 (2016).
[65] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and Tell: A Neural Image Caption Generator. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 3156–3164.
[66] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. 2018. Differentially Private Generative Adversarial Network. CoRR abs/1802.06739 (2018).
[67] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. 2016. Attribute2Image: Conditional Image Generation from Visual Attributes. In European Conference on Computer Vision (ECCV). Springer, 776–791.
[68] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. In IEEE Computer Security Foundations Symposium (CSF). IEEE, 268–282.
[69] Xin Yi, Ekta Walia, and Paul Babyn. 2019. Generative Adversarial Network in Medical Imaging: A Review. Medical Image Analysis (2019), 101552.
[70] Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, and Michal Lukác. 2019. Texture Mixer: A Network for Controllable Synthesis and Interpolation of Texture. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 12164–12173.
[71] Ning Yu, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, and Mario Fritz. 2020. Inclusive GAN: Improving Data and Minority Coverage in Generative Models. In European Conference on Computer Vision (ECCV). Springer.
[72] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 586–595.
[73] Xinyang Zhang, Shouling Ji, and Ting Wang. 2018. Differentially Private Releasing via Deep Generative Model (Technical Report). CoRR abs/1801.01594 (2018).

### Proof

**Theorem 5.1.** Given the victim model with parameter \(\theta_v\), a query dataset \(S\), the membership probability of a query sample \(x_i\) is well approximated by the sigmoid of minus calibrated reconstruction error:
\[ P(m_i = 1 | \theta_v, x_i, S) \approx \sigma(-L_{\text{cal}}(x_i, R(x_i | G_v))) \]

And the optimal attack is equivalent to:
\[ A(x_i, M(\theta_v)) = 1[L_{\text{cal}}(x_i, R(x_i | G_v)) < \epsilon] \]
i.e., the attacker checks whether the calibrated reconstruction error of the query sample \(x_i\) is smaller than a threshold \(\epsilon\).

**Proof.** By applying the Bayes rule and the property of the sigmoid function \(\sigma\), the membership probability can be rewritten as follows [57]:
\[ P(m_i = 1 | \theta_v, x_i, S) = \sigma \left( \log \frac{P(\theta_v | m_i = 1, x_i, S - i) P(m_i = 1)}{P(\theta_v | m_i = 0, x_i, S - i) P(m_i = 0)} \right) \]
where \(S - i = S \setminus (x_i, m_i)\), i.e., the whole query set except the query sample \(x_i\).

Assuming independence of samples in \(S\) while applying Bayes rule and the product rule, we obtain the following posterior approximation:
\[ P(\theta_v | S) \propto \prod_{j | m_j = 1} P(x_j | \theta_v) P(\theta_v) \]
\[ \propto \exp \left( - \sum_{j} m_j \cdot l(x_j, \theta_v) \right) \]
with \(l(x_j, \theta_v) = L(x_j, R(x | G_v))\) for brevity. Equation 18 means that the probability of a certain model parameter is determined by its i.i.d. training set samples. Subsequently, by assuming a uniform prior of the model parameter over the whole parameter space and plugging in the results from Equation 4, we obtain Equation 19.

By normalizing the posterior in Equation 19, we obtain:
\[ P(\theta_v | m_i = 1, x_i, S - i) = \frac{\exp \left( - \sum_{j} m_j \cdot l(x_j, \theta_v) \right)}{\int \exp \left( - \sum_{j} m_j \cdot l(x_j, \theta') \right) d\theta'} \]
\[ P(\theta_v | m_i = 0, x_i, S - i) = \frac{\exp \left( - \sum_{j \neq i} m_j \cdot l(x_j, \theta_v) \right)}{\int \exp \left( - \sum_{j \neq i} m_j \cdot l(x_j, \theta') \right) d\theta'} \]
(18)

---

I hope this helps! Let me know if you need any further adjustments.