# Ripcord: A Modular Platform for Data Center Networking

**Authors:**
- Brandon Heller, David Erickson, Nick McKeown, Stanford University, Stanford, CA, USA
- Rean Griffith, Igor Ganichev, Scott Shenker, University of California, Berkeley, CA, USA
- Kyriakos Zarifis, International Computer Science Institute, Berkeley, CA, USA
- Daekyeong Moon, Nicira Networks, Palo Alto, CA, USA
- Scott Whyte, Stephen Stuart, Google, Inc., Mountain View, CA, USA

## Abstract
In this demonstration, we present Ripcord, a modular platform for rapidly prototyping scale-out data center networks. Ripcord allows researchers to build and evaluate new network features and topologies using only commercially available hardware and open-source software. The demo will showcase three custom network functions operating together on a 160-node cluster:
1. A routing engine that isolates classes of traffic.
2. A dynamic network manager that adjusts links and switch power states to reduce energy consumption.
3. A statistics aggregator that supports network health monitoring and automatic alerts.

The demo will be interactive, featuring a live visualization of parameters such as bandwidth, packet drops, and power status for each link and switch. Attendees can also use a control panel to modify the traffic load. We believe an interactive demo is the best way to introduce Ripcord to the research community and gather feedback.

**Categories and Subject Descriptors:**
- C.2.1 [Network Architecture and Design]: Network communications
- C.2.2 [Network Protocols]: Routing protocols

**General Terms:**
- Design, Experimentation, Management

**Keywords:**
- Data center network, Ripcord, OpenFlow

## 1. Details
This section describes the major components of the proposed demo, including the base Ripcord platform, interactive dashboard, custom modules, and hardware setup. Our goal differs from the SIGCOMM 2009 FlowVisor demo, which focused on multiple controllers sharing an unstructured enterprise network. Instead, we aim to demonstrate a single modular controller operating on structured data center networks.

### 1.1 Ripcord
Refer to Figure 1 for an overview of the Ripcord architecture. The platform includes primitives for building new data center routing algorithms and management tools, addressing key design challenges such as scalability, server migration, and forwarding state. Ripcord leverages NOX, an OpenFlow controller platform, to pass messages between modules and manage switch state (e.g., flow entries and statistics).

The prototype implements multiple data center routing engines, including those with elements similar to VL2 and PortLand. Ripcord can run multiple routing schemes simultaneously, enabling side-by-side comparisons and distinct routing engines for different services. Each routing engine uses a structured topology representation, allowing, for example, PortLand-style routing on a VL2-style aggregated topology without code changes.

### 1.2 Dashboard
Real-time visualization of network state is crucial for demonstrating and understanding Ripcord. Attendees will see a GUI like the one in Figure 2. The upper part of the dashboard displays the network topology, with link utilization color-coded: green for lightly loaded, yellow for moderately loaded, and red for highly loaded. Disabled nodes or links appear with an X. Each application has configurable parameters, such as collection epoch duration for the statistics aggregator or fault tolerance and link utilization parameters for ElasticTree. The bottom part of the dashboard shows high-level statistics, including graphs of power, throughput, and latency over time.

### 1.3 Integrated Traffic Engineering and QoS
The Traffic Engineering (TE) management tool, built on top of Ripcord, gives routing preference to specific classes of traffic, such as video streams. Flows for these classes are routed along pre-reserved paths. The TE tool also maintains pre-calculated re-routing actions for link or switch failures. The demo will showcase tunnel setup based on different network requirements and optimization functions, as well as the failover procedure, measuring response times and network disruption caused by tunnel preemptions.

### 1.4 ElasticTree
ElasticTree is a dynamic optimizer that aims to shut off as many unneeded network elements as possible while respecting performance and fault tolerance constraints. Given a traffic matrix and network topology, ElasticTree generates the set of active switches and links. Various optimizers have been implemented, varying in optimality, generality, and solution time. This demo extends the ElasticTree paper to a larger system with more realistic application traffic. Attendees can explore trade-offs between energy, performance, and fault tolerance.

### 1.5 Aggregated Statistics
The Aggregated Statistics application collects flow, switch, and link statistics from the network, providing data to both ElasticTree and the visualization. It enables interactive queries, allowing users to obtain detailed metrics on individual or aggregated flows, track port measurements from switches, and monitor link utilization trends to assess the impact of traffic engineering and power management decisions.

### 1.6 Data Center Platform
The expected platform is a 160-node cluster organized as a three-layer fat tree with four-port switches. Each edge switch uses 20 1 Gbps downlinks to hosts instead of two 10 Gbps downlinks. Each switch runs OpenFlow, an open-source, vendor-neutral, flow-based API added to commercial switches, routers, and access points. When an OpenFlow-enabled switch receives a packet with no matching flow entry, it sends the packet to a controller, which decides whether to add flow entries to set up a path. Alternatively, some Ripcord routing engines can set up paths in advance.

Although the prototype is not production-ready, we believe Ripcord presents a compelling framework for researchers to implement, evaluate, and eventually deploy new ideas. The three modules created for this demo demonstrate its flexibility, and the visualization will be useful for debugging new Ripcord modules and understanding data center traffic patterns.

## 2. References
[1] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible Data Center Network. In Proc. of SIGCOMM, Barcelona, Spain, 2009.

[2] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, and N. McKeown. Nox: Towards an operating system for networks. In ACM SIGCOMM CCR: Editorial note, July 2008.

[3] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis, P. Sharma, S. Banerjee, and N. McKeown. Elastictree: Reducing energy in data center networks. In NSDI’10: Proceedings of the 7th USENIX Symposium on Networked Systems Design and Implementation, 2010.

[4] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. Portland: a scalable fault-tolerant layer 2 data center network fabric. In SIGCOMM ’09: Proceedings of the ACM SIGCOMM 2009 conference on Data communication, pages 39–50, New York, NY, USA, 2009. ACM.

[5] The openflow switch. http://www.openflowswitch.org.

[6] R. Sherwood, M. Chan, A. Covington, G. Gibb, M. Flajslik, N. Handigol, T. Huang, P. Kazemian, M. Kobayashi, J. Naous, et al. Carving research slices out of your production networks with OpenFlow. ACM SIGCOMM Computer Communication Review, 40(1):129–130, 2010.

[7] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying NOX to the Datacenter. In 8th ACM Workshop on Hot Topics in Networking (Hotnets), New York City, NY, October 2009.