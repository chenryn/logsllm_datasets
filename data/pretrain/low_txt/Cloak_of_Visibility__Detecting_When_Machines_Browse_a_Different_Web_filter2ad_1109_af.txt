### For Instance, JavaScript Redirects and Cloaking Techniques

JavaScript redirects account for 29.9% of cloaked search URLs, compared to 6.6% of ads, with ads favoring same-page modifications. Our findings highlight that while malicious actors may employ a variety of techniques to cloak their content, our anti-cloaking system is effective in generalizing and capturing each approach. Any security crawler must address these techniques and be prepared for future iterations in the cloaking arms race.

### VIII. Case Studies

In this section, we present case studies that exemplify the monetization strategies employed by cloaked Google Search and Google Ads URLs.

#### Lead Generation for Mobile Apps
We identified multiple sites that entice mobile users to install both dubious and legitimate third-party apps. Interestingly, even some of the top domains on Alexa exhibit this behavior. For example, Figure 7 illustrates how mobile and desktop visitors see different content on opensubtitles.org. When the site detects a visitor with an Android mobile User-Agent and a set HTTP referrer, it adds a new div element via JavaScript. This element either randomly loads an ad for a legitimate Android app or is styled as a fake Android notification. Clicking on this notification leads to a dubious app that acts as a free app store. Once installed, this app floods the device with unwanted ads (via the AirPush library).

#### Malware Distribution
Some of the cloaking websites we identified are distributing malware. For instance, saomin.com delivers an Android app flagged as malicious by 19 antivirus engines on VirusTotal. In another case, users were encouraged to install a malicious browser extension called FromDocToPDF.

#### Traffic Resellers
We also observed cloaking sites selling their organic traffic to a network of advertisers. For example, pancakeshop.kim redirects users to third-party advertisers based on the type of platform and Referer header. The site also geolocates the visitor to decide which ads to run. Users visiting from outside the US see a blank page or a message stating, "We currently don’t have any sponsors for this domain name."

Some traffic resellers use a wide range of rules to determine the content to display. For instance, macauwinner.tk pretends to be a parked domain when visited from outside the US but delivers tailored content to users on residential and mobile networks, detecting their Internet provider (e.g., displaying "Dear AT&T Uverse user"). It also customizes content based on the user's operating system, mimicking its appearance with fake windows and alert boxes. Users are redirected to a variety of pages, including fake antiviruses, fake popular websites (e.g., Facebook), and surveys.

#### Affiliate Fraud
We found instances where cloaking sites perform affiliate fraud. For example, drseks.com redirects every other user to a major shopping retailer with an affiliate ID set. This way, the retailer shares a fraction of the profits from a sale with the cloaked domain.

### IX. Breaking the Cloaking Arms Race

As malicious actors adopt increasingly sophisticated application-specific cloaking techniques, it becomes challenging for defenders to keep pace. Currently, our system is a viable solution, designed to defeat current cloaking capabilities. We have determined the minimum capabilities required for an anti-cloaking pipeline to guide its design and ensure efficient engineering efforts. However, we anticipate that malicious actors will enhance their cloaking arsenal (e.g., carrier-specific mobile cloaking), increasing the cost of detection at the expense of driving less organic traffic to their concealed offers. To counter this trend, we propose two alternatives that would make it significantly harder for malicious actors to deliver split-view content, though they would require an in-browser component.

#### Client-side Cloaking Detection
Since cloaking relies on serving benign content to search engine and ad network crawlers, one option is for these services to embed a succinct digest of a webpage's content in the parameters tied to search and advertisement URLs. When users click on these URLs, their browser can compare the newly served content against the crawler's digest. If there is a substantial difference, the browser can raise a warning interstitial alerting the user to a suspected scam, phishing, or malware attack. This comparison naturally follows the pairwise features we laid out for our anti-cloaking system. The benefit over our current architecture is that crawlers no longer need to maintain multiple browsing profiles or network vantages—clients provide the second view. Additionally, this approach respects user privacy, as only potentially dangerous pages are reported by opted-in users.

However, there are challenges with this approach. Dynamic content remains a concern. If malicious actors can limit the deviations introduced by cloaking to within typical norms (e.g., adding only a small new button or URL), the system may fail to detect the attack. This also constrains attackers, reducing user click-through rates. There is also a risk with frequently updated pages, such as news sites, where a crawler may serve incoming visitors a stale digest due to an outdated crawl, leading to false positives. To avoid this, the crawler would need to immediately re-crawl the page to confirm the change and suppress the alert, or the digest should account for the category of the site, allowing for a higher threshold for news sites.

#### Distributed Client Content Reporting
To overcome the problem of staleness, we consider an alternative model where a user's browser opts to anonymously report a content digest after clicking on a search result or advertisement to the associated search engine or ad network. The server would then review the incoming digest against the copy fetched by its crawler. In the event of a mismatch, the server would immediately re-crawl the URL to rule out the possibility of an outdated digest. If there is still a client-server mismatch after crawling, the search engine or ad network involved could remove the reported URL from public listings to protect all future clients. From a privacy perspective, the server receiving reports would already know the user clicked on the URL, similar to how search engines currently redirect visitors through analytic interstitials. However, the digest reported must not leak personalized content, especially if users click through to signed-in pages containing sensitive information (e.g., facebook.com). This approach also opens servers up to abuse, where malicious clients may spoof digests to trigger the removal of legitimate search results and advertisements. Assuming there are more legitimate clients than malicious ones and some form of rate limiting, servers can rely on majority voting to solve this problem, though the long tail of URLs may still pose a challenge.

### X. Conclusion

In this work, we explored the cloaking arms race between security crawlers and malicious actors seeking to monetize search engines and ad networks via counterfeit storefronts and malicious advertisements. While prior work has focused on understanding the prevalence of hidden content and specific cloaking techniques, none has combined both an underground and empirical perspective to detail how cloaking operates today. We addressed this gap by developing an anti-cloaking system that covers a spectrum of browser, network, and contextual blackhat targeting techniques, determining the minimum crawling capabilities required to contend with cloaking.

We informed our system's design by engaging directly with black market specialists selling cloaking software and services, obtaining ten of the most sophisticated offerings. These packages included capabilities to blacklist clients based on IP addresses, reverse DNS, User-Agent, HTTP headers, and the order of actions a client takes upon visiting a malicious webpage. We overcame these techniques by fetching suspected cloaking URLs from multiple crawlers emulating increasingly sophisticated legitimate user behavior. We compared and classified the content returned for 94,946 labeled URLs, resulting in a system that accurately detected cloaking 95.5% of the time with a false positive rate of 0.9%.

When we deployed our crawler to scan 135,577 unknown URLs, we found that 11.7% of the top 100 search results related to luxury products and 4.9% of advertisements targeting weight loss and mobile applications were cloaked against Googlebot. We exposed a gap between current blackhat practices and the broader set of fingerprinting techniques known within the research community. Therefore, we discussed future directions for breaking the cloaking arms race, including clients reporting browsing perspectives to crawler operators, hindering the ability of malicious actors to show benign content exclusively to search engines and ad networks.

### References

[1] Alexa. Alexa top 500 global sites. http://www.alexa.com/topsites, 2012.
[2] Ross Anderson, Chris Barton, Rainer Böhm, Richard Clayton, Michel J.G. van Eeten, Michael Levi, Tyler Moore, and Stefan Savage. Measuring the cost of cybercrime. In Proceedings of the Workshop on Economics of Information Security (WEIS), 2012.
[3] Károly Boda, Ádám Máté Földes, Gábor György Gulyás, and Sándor Imre. User tracking on the web via cross-browser fingerprinting. In Information Security Technology for Applications, pages 31–46. Springer, 2012.
[4] Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. Classification and regression trees. CRC press, 1984.
[5] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380–388. ACM, 2002.
[6] M. Cova, C. Kruegel, and G. Vigna. Detection and analysis of drive-by-download attacks and malicious JavaScript code. In Proceedings of the 19th International Conference on World Wide Web, 2010.
[7] Peter Eckersley. How Unique Is Your Web Browser? In Privacy Enhancing Technologies (PET), 2010.
[8] David Fifield and Serge Egelman. Fingerprinting web users through font metrics. In Proceedings of the International Conference on Financial Cryptography and Data Security, 2015.
[9] Sean Ford, Marco Cova, Christopher Kruegel, and Giovanni Vigna. Analyzing and detecting malicious flash advertisements. In Computer Security Applications Conference, 2009. ACSAC’09. Annual, 2009.
[10] gensim. models.ldamodel – Latent Dirichlet Allocation. https://radimrehurek.com/gensim/models/ldamodel.html, 2015.
[11] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine learning, 63(1):3–42, 2006.
[12] Chris Grier, Lucas Ballard, Juan Caballero, Neha Chachra, Christian J. Dietrich, Kirill Levchenko, Panayiotis Mavrommatis, D. McCoy, Antonio Nappa, Andreas Pitsillidis, et al. Manufacturing compromise: The emergence of exploit-as-a-service. In Proceedings of the ACM Conference on Computer and Communications Security (CCS), 2012.
[13] Matthew Hoffman, Francis R Bach, and David M Blei. Online learning for latent dirichlet allocation. In Neural Information Processing Systems, 2010.
[14] John P John, Fang Yu, Yinglian Xie, Arvind Krishnamurthy, and Martín Abadi. deseo: Combating search-result poisoning. In Proceedings of the USENIX Security Symposium, 2011.
[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
[16] Nektarios Leontiadis, Tyler Moore, and Nicolas Christin. Measuring and analyzing search-redirection attacks in the illicit online prescription drug trade. In USENIX Security Symposium, 2011.
[17] Nektarios Leontiadis, Tyler Moore, and Nicolas Christin. A nearly four-year longitudinal study of search-engine poisoning. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security, 2014.
[18] Long Lu, Roberto Perdisci, and Wenke Lee. Surf: Detecting and measuring search poisoning. In Proceedings of the 18th ACM conference on Computer and communications security, 2011.
[19] Wes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th, volume 445, pages 51–56, 2010.
[20] Keaton Mowery, Dillon Bogenreif, Scott Yilek, and Hovav Shacham. Fingerprinting information in JavaScript implementations. In Proceedings of the Workshop on Web 2.0 Security and Privacy, 2011.
[21] Keaton Mowery and Hovav Shacham. Pixel perfect: Fingerprinting canvas in HTML5. In Proceedings of the Workshop on Web 2.0 Security and Privacy, 2012.
[22] Martin Mulazzani, Philipp Reschl, Markus Huber, Manuel Leithner, Sebastian Schrittwieser, Edgar Weippl, and FC Wien. Fast and reliable browser identification with JavaScript engine fingerprinting. In Proceedings of the Workshop on Web 2.0 Security and Privacy, 2013.
[23] Nick Nikiforakis, Alexandros Kapravelos, Wouter Joosen, Christopher Kruegel, Frank Piessens, and Giovanni Vigna. Cookieless monster: Exploring the ecosystem of web-based device fingerprinting. In Security and Privacy (SP), 2013 IEEE Symposium on, pages 541–555. IEEE, 2013.
[24] Yuan Niu, Hao Chen, Francis Hsu, Yi-Min Wang, and Ming Ma. A quantitative study of forum spamming using context-based analysis. In NDSS. Citeseer, 2007.
[25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, and et al. Weiss. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[26] Niels Provos, Panayiotis Mavrommatis, Moheeb Abu Rajab, and Fabian Monrose. All your iFRAMEs point to us. In Proceedings of the 17th Usenix Security Symposium, pages 1–15, July 2008.
[27] Gianluca Stringhini, Christopher Kruegel, and Giovanni Vigna. Shady paths: Leveraging surfing crowds to detect malicious web pages. In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security, pages 133–144. ACM, 2013.
[28] Kurt Thomas, Danny Yuxing Huang, David Wang, Elie Bursztein, Chris Grier, Thomas J. Holt, Christopher Kruegel, Damon McCoy, Stefan Savage, and Giovanni Vigna. Framing dependencies introduced by underground commoditization. In Proceedings of the Workshop on the Economics of Information Security, 2015.
[29] Thomas Unger, Martin Mulazzani, Dominik Fruhwirt, Markus Huber, Sebastian Schrittwieser, and Edgar Weippl. Shpf: Enhancing HTTP(S) session security with browser fingerprinting. In Proceedings of the International Conference on Availability, Reliability and Security, 2013.
[30] W3C. Referrer Policy. http://w3c.github.io/webappsec/specs/referrer-policy/, 2015.
[31] David Y Wang, Matthew Der, Mohammad Karami, Lawrence Saul, Damon McCoy, Stefan Savage, and Geoffrey M Voelker. Search+ seizure: The effectiveness of interventions on SEO campaigns. In Proceedings of the 2014 Conference on Internet Measurement Conference, 2014.
[32] David Y Wang, Stefan Savage, and Geoffrey M Voelker. Cloak and dagger: Dynamics of web search cloaking. In Proceedings of the ACM Conference on Computer and Communications Security, 2011.
[33] Yi-Min Wang and Ming Ma. Detecting stealth web pages that use click-through cloaking. In Microsoft Research Technical Report, MSR-TR, 2006.
[34] Y.M. Wang, M. Ma, Y. Niu, and H. Chen. Spam double-funnel: Connecting web spammers with advertisers. In Proceedings of the International World Wide Web Conference, pages 291–300, 2007.
[35] Baoning Wu and Brian D Davison. Detecting semantic cloaking on the web. In Proceedings of the 15th international conference on World Wide Web, 2006.
[36] Apostolis Zarras, Alexandros Kapravelos, Gianluca Stringhini, Thorsten Holz, Christopher Kruegel, and Giovanni Vigna. The dark alleys of Madison Avenue: Understanding malicious advertisements. In Proceedings of the 2014 Conference on Internet Measurement Conference, 2014.
[37] Qing Zhang, David Y Wang, and Geoffrey M Voelker. Dspin: Detecting automatically spun content on the web. In Symposium on Network and Distributed System Security (NDSS), 2014.