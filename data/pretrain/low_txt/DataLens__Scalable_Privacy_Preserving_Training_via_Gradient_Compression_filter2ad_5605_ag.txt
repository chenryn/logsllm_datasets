### Introduction to Differential Privacy and Gradient Compression in High-Dimensional Data

#### Weights Approach and Challenges
The weights approach, as described in [20], has shown promising performance on low-dimensional datasets. However, these methods face significant challenges when applied to high-dimensional data, such as low data utility or high sampling complexity. Consequently, they are generally unsuitable for the high-dimensional image datasets discussed in this paper.

#### Adapting DP-SGD to GANs
Another line of research focuses on adapting Differentially Private Stochastic Gradient Descent (DP-SGD) to Generative Adversarial Networks (GANs). For instance, DP-GAN [59] achieves differential privacy by adding Gaussian noise to the discriminator gradients during training. Similarly, DP-CGAN [54] uses a similar approach to ensure differential privacy while training a conditional GAN to generate both synthetic data and labels. GS-WGAN [13] employs the Wasserstein loss and sanitizes the generator's data-dependent gradients to improve data utility. Despite these advancements, these methods still suffer from low data utility when applied to high-dimensional datasets due to the rapid consumption of the privacy budget.

#### PATE-GAN and G-PATE
PATE-GAN [62] integrates the PATE (Private Aggregation of Teacher Ensembles) framework with GANs. It trains multiple teacher discriminators and uses them to update the student discriminator. However, this framework primarily applies PATE to train the discriminator within a GAN, where both the teacher and student models are discriminators, and the interaction between the generator and discriminator is not adapted for the teacher-student framework. As a result, PATE-GAN is only evaluated on low-dimensional tabular data and faces similar issues under limited privacy budgets. G-PATE [37] improves upon PATE-GAN by directly training a student generator using the teacher discriminators and employing random projection to reduce gradient dimensions during training. This approach, however, is challenging to analyze for convergence. DataLens combines the PATE framework with top-k gradient compression, demonstrating significant utility improvements over PATE-GAN and G-PATE on high-dimensional datasets, with theoretical analysis of its convergence.

#### DP SGD Training
DPDL [2] was the first work to apply differential privacy to SGD training to prevent deep neural models from exposing private information from the training data. DPDL also proposed using moments accountant to compute the privacy cost, which provides a tighter bound than the strong composition theorem. McMahan et al. [39] adopted Rényi differential privacy, extending and generalizing the moment accountant to multi-vector queries. This extension allows the framework to provide privacy for heterogeneous sets of vectors and is widely adopted in current open-source DP libraries (TensorFlow Privacy and PyTorch Opacus). However, the issue of high-dimensional data remains, as the privacy budget can be quickly consumed when aggregating gradients in a differentially private manner.

#### Gradient Compression
Recent interest in communication-efficient distributed learning has led to various techniques, including gradient compression [4, 5, 49, 57], decentralization [28, 33], and asynchronization [32]. These methods aim to manage the noise introduced by system relaxations. cpSGD [3] is a binomial DP-mechanism designed for stochastic k-level gradient quantization, allowing low-precision communication after adding DP noise. D2P-Fed [56] extends this work by applying the discrete Gaussian mechanism to the same k-level quantization, achieving a stronger privacy guarantee. Kairouz et al. [25] combine the discrete Gaussian mechanism with k-level quantization to facilitate federated learning with differential privacy and secure aggregation. In contrast, DataLens uses the PATE framework to provide rigorous privacy guarantees and applies sign compression as teacher voting to save the privacy budget. FetchSGD [48] focuses on communication efficiency in federated learning, proposing Count Sketch data structures and top-k operations for fast gradient compression and aggregation. However, FetchSGD lacks a discussion on privacy guarantees. In this paper, we explore the relationship between privacy and gradient compression in a different scenario, illustrating how gradient compression can help achieve better utility in privacy-preserving algorithms for high-dimensional data. We propose TopAgg, which combines stochastic sign [24] with top-k gradient compression. Our empirical results show that TopAgg outperforms state-of-the-art gradient compression algorithms in improving model utility with differential privacy guarantees.

### Conclusion
In summary, we propose DataLens, a novel and effective differentially private data generative model, which is applicable to high-dimensional data compared to existing approaches. Additionally, we introduce TopAgg, a new algorithm for gradient compression and aggregation. We provide differential privacy and convergence analysis for the proposed model. Extensive empirical experiments demonstrate that DataLens significantly outperforms existing DP generative models on various, especially high-dimensional image datasets, even under limited privacy budgets.

### Acknowledgements
We thank the anonymous reviewers for their constructive feedback. We also thank Dingfan Chen and many others for their helpful discussions. This work is partially supported by the NSF grant No.1910100, NSF CNS 20-46726 CAR, and Amazon Research Award.

### References
[1] Opacus – Train PyTorch models with Differential Privacy. https://opacus.ai/
[2] Abadi, M., Chu, A., Goodfellow, I. J., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016). Deep Learning with Differential Privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security.
[3] Agarwal, N., Suresh, A. T., Yu, F. X. X., Kumar, S., & McMahan, B. (2018). cpSGD: Communication-efficient and differentially-private distributed SGD. In Advances in Neural Information Processing Systems.
[4] Alistarh, D., Grubic, D., Li, J., Tomioka, R., & Vojnovic, M. (2017). QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding. In Advances in Neural Information Processing Systems.
[5] Alistarh, D., Hoefler, T., Johansson, M., Konstantinov, N., Khirirat, S., & Renggli, C. (2018). The Convergence of Sparsified Gradient Methods. In Advances in Neural Information Processing Systems.
[6] Ben-Nun, T., & Hoefler, T. (2019). Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis. ACM Comput. Surv.
[7] Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., & Anandkumar, A. (2018). signSGD: Compressed Optimisation for Non-Convex Problems. In Proceedings of the 35th International Conference on Machine Learning.
[8] Carlini, N., Deng, S., Garg, S., Jha, S., Mahloujifar, S., Mahmoody, M., Song, S., Thakurta, A., & Tramer, F. (2020). An Attack on InstaHide: Is Private Learning Possible with Instance Encoding? arXiv preprint arXiv:2011.05315.
[9] Chen, C.-Y., Ni, J., Lu, S., Cui, X., Chen, P.-Y., Sun, X., Wang, N., Venkataramani, S., Srinivasan, V. (viji), Zhang, W., & Gopalakrishnan, K. (2020). ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training. Adv. Neural Inf. Process. Syst.
[10] Chen, D., Orekondy, T., & Fritz, M. (2020). GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators. Neural Information Processing Systems (NeurIPS).
[11] Chen, D., Yu, N., Zhang, Y., & Fritz, M. (2020). GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models. In CCS '20: 2020 ACM SIGSAC Conference on Computer and Communications Security.
[12] Dwork, C. (2008). Differential privacy: A survey of results. In International conference on theory and applications of models of computation. Springer.
[13] Dwork, C., & Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science.
[14] Fu, F., Hu, Y., He, Y., Jiang, J., Shao, Y., Zhang, C., & Cui, B. (2020). Don’t Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript. In International Conference on Machine Learning.
[15] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems.
[16] Hardt, M., Ligett, K., & McSherry, F. (2012). A simple and practical algorithm for differentially private data release. In Advances in Neural Information Processing Systems.
[17] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition.
[18] Jia, R., Dao, D., Wang, B., Hubis, F. A., Gürel, N. M., Li, B., Zhang, C., Spanos, C. J., & Song, D. X. (2019). Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. Proc. VLDB Endow.
[19] Jin, R., Huang, Y., He, X., Dai, H., & Wu, T. (2020). Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees. arXiv preprint arXiv:2002.10940.
[20] Kairouz, P., Liu, Z., & Steinke, T. (2021). The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation. arXiv preprint arXiv:2102.06387.
[21] Koloskova, A., Stich, S., & Jaggi, M. (2019). Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication. In Proceedings of the 36th International Conference on Machine Learning.
[22] Krizhevsky, A., Nair, V., & Hinton, G. (n.d.). CIFAR-10 (Canadian Institute for Advanced Research). http://www.cs.toronto.edu/~kriz/cifar.html
[23] LeCun, Y. (1998). The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/
[24] Li, C., Liu, H., Chen, C., Pu, Y., Chen, L., Henao, R., & Carin, L. (2017). ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching. Advances in Neural Information Processing Systems.
[25] Lian, X., Huang, Y., Li, Y., & Liu, J. (2015). Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization. In Advances in Neural Information Processing Systems.
[26] Lim, H., Andersen, D. G., & Kaminsky, M. (2019). 3LC: LIGHTWEIGHT AND EFFECTIVE TRAFFIC COMPRESSION FOR DISTRIBUTED MACHINE LEARNING. In Proceedings of the 2nd SysML Conference.
[27] Liu, J., & Zhang, C. (2020). Distributed Learning Systems with First-Order Methods. Foundations and Trends® in Databases.
[28] Liu, Z., Luo, P., Wang, X., & Tang, X. (2015). Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV).
[29] Long, Y., Lin, S., Yang, Z., Gunter, C. A., & Li, B. (2019). Scalable differentially private generative student model via pate. arXiv preprint arXiv:1906.09338.
[30] McMahan, B., Moore, E., Ramage, D., Hampson, S., & Aguera y Arcas, B. (2017). Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics. PMLR.
[31] McMahan, H. B., Andrew, G., Erlingsson, U., Chien, S., Mironov, I., Papernot, N., & Kairouz, P. (2019). A General Approach to Adding Differential Privacy to Iterative Training Procedures. arXiv:1812.06210 [cs.LG].
[32] Mescheder, L., Geiger, A., & Nowozin, S. (2018). Which Training Methods for GANs do actually Converge?. In Proceedings of the 35th International Conference on Machine Learning.
[33] Mironov, I. (2017). Rényi differential privacy. In Computer Security Foundations Symposium (CSF), 2017 IEEE 30th. IEEE.
[34] Mironov, I., Talwar, K., & Zhang, L. (2019). Rényi Differential Privacy of the Sampled Gaussian Mechanism. arXiv preprint arXiv:1908.10530.
[35] Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., & Talwar, K. (2017). Semi-supervised knowledge transfer for deep learning from private training data. In International Conference on Learning Representations.
[36] Papernot, N., Song, S., Mironov, I., Raghunathan, A., Talwar, K., & Erlingsson, U. (2018). Scalable Private Learning with PATE. In International Conference on Learning Representations.
[37] Pichapati, V., Suresh, A. T., Yu, F. X., Reddi, S. J., & Kumar, S. (2019). AdaCliP: Adaptive Clipping for Private SGD. CoRR abs/1908.07643.
[38] Qardaji, W., Yang, W., & Li, N. (2014). Priview: practical differentially private release of marginal contingency tables. In Proceedings of the 2014 ACM SIGMOD international conference on Management of data. ACM.
[39] Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
[40] Rothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I., Braverman, V., Gonzalez, J., & Arora, R. (2020). FetchSGD: Communication-efficient federated learning with sketching. In International Conference on Machine Learning. PMLR.
[41] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016). Improved techniques for training GANs. In Proceedings of the 30th International Conference on Neural Information Processing Systems.
[42] Shokri, R., Stronati, M., Song, C., & Shmatikov, V. (2017). Membership Inference Attacks Against Machine Learning Models. In 2017 IEEE Symposium on Security and Privacy (SP).
[43] Tang, H., Gan, S., Zhang, C., Zhang, T., & Liu, J. (2018). Communication Compression for Decentralized Training. In Advances in Neural Information Processing Systems 31.
[44] Thakkar, O., Andrew, G., & McMahan, H. B. (2019). Differentially Private Learning with Adaptive Clipping. CoRR abs/1905.03871.
[45] Torkzadehmahani, R., Kairouz, P., & Paten, B. (2019). DP-CGAN: Differentially private synthetic data and label generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.
[46] Vogels, T., Karimireddy, S. P., & Jaggi, M. (2020). Practical Low-Rank Communication Compression in Decentralized Deep Learning. Adv. Neural Inf. Process. Syst.
[47] Wang, L., Jia, R., & Song, D. (2020). D2P-Fed: Differentially private federated learning with efficient communication. arxiv. org/pdf/2006.13039.
[48] Wangni, J., Wang, J., Liu, J., & Zhang, T. (2018). Gradient Sparsification for Communication-Efficient Distributed Optimization. In Proceedings of the 32nd International Conference on Neural Information Processing Systems.
[49] Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
[50] Xie, L., Lin, K., Wang, S., Wang, F., & Zhou, J. (2018). Differentially Private Generative Adversarial Network. arXiv preprint arXiv:1802.06739.
[51] Ye, Y., Pei, H., Wang, B., Chen, P.-Y., Zhu, Y., Xiao, J., & Li, B. (2020). Reinforcement-learning based portfolio management with augmented asset movement prediction states. In Proceedings of the AAAI Conference on Artificial Intelligence.
[52] Yeom, S., Giacomelli, I., Fredrikson, M., & Jha, S. (2018). Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF). IEEE.
[53] Yoon, J., Jordon, J., & van der Schaar, M. (2019). PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees. In International Conference on Learning Representations.
[54] Zhang, J., Cormode, G., Procopiuc, C. M., Srivastava, D., & Xiaokui, X. (2012). PrivBayes: Private Data Release via Bayesian Networks. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data.