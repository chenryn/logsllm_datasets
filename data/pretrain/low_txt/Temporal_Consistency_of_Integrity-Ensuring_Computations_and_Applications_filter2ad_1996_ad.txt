### 64KB Page Size
**Figure 9: Runtime of Various Temporal Consistency Ensuring Mechanisms in I.MX6-SabreLite.**

Unlike `Thchecksum` and `Thmain`, implementing `Thfault` is not trivial; it requires support from the underlying hardware and/or kernel to:
1. Detect whenever a process causes a fault.
2. Examine whether the fault is caused by an invalid write access and whether it occurs within a specific memory range.

Fortunately, these operations are already available in seL4 without requiring modifications to the kernel.

We implement `Thfault` by leveraging how a fault endpoint works in seL4. An endpoint in seL4 is an object that allows a small amount of data to be transferred between two threads. When a process or thread faults, the seL4 kernel automatically sends a fault IPC message to its registered fault endpoint. This fault IPC message provides useful information that helps `Thfault` decide whether the fault will result in memory inconsistency. For instance, the message includes the type of fault (e.g., page fault, capability fault, or unknown syscall), the address causing the fault, and whether a read or write access caused the fault. In our implementation, `Thmain` shares a single fault endpoint among all user-space processes, allowing a fault caused by any process to be transmitted to this fault endpoint.

The final step of the implementation is for `Thfault` to wait for an incoming message from the fault endpoint and notify `Thmain` if the message indicates an attempted write access on the memory being attested. A sample code for `Thfault` is provided in Appendix C.

**Figure 10: Sequence Diagram of PAt t with Memory Inconsistency Detection During a Single Attestation Instance.**
PAt t chooses to resume the execution of `Thchecksum` after P′ causes memory inconsistency.

**Figure 11: Runtime of Inconsistency Detection with 4KB Page Size on I.MX6-SabreLite.**

**Figure 12: Downtime of the Faulting Process P′ When Its Actions Result in an Inconsistency with 4KB Page Size on I.MX6-SabreLite.**
Horizontal lines represent downtime from the approach where PAt t resolves inconsistency by unlocking the entire memory of P′.

### 5.6 Experimental Results: Inconsistency Detection

To evaluate the performance of the inconsistency detection mechanism, we experimented by running two processes—modified PAt t and P′—with the same execution priority on I.MX6-SabreLite. (Multiple same-priority processes are scheduled in a round-robin fashion.) Thus, timing results for this experiment differ from others that consider only PAt t running at any given time.

Results in Figure 11 show the performance comparison of:
1. The inconsistency detection mechanism (with and without inconsistency occurring).
2. All-Lock.
3. Attestation without consistency guarantee (No-Lock) on 16MB to 96MB memory.

In this experiment, we assume that PAt t chooses to resolve inconsistency by unlocking the entire memory of P′. In the case of no inconsistency, our mechanism (as expected) performs as well as All-Lock and roughly 6% slower than No-Lock. On the other hand, when an inconsistency occurs, the mechanism (surprisingly) runs 3% faster. While this may seem counter-intuitive, we found that improved performance is caused by `Thmain` performing memory unlocking while P′ is suspended. This results in the runtime of the unlock operation being approximately 2x faster than that of the same operation in its counterpart, where memory unlocking is performed concurrently with P′.

We now consider the alternative, whereby PAt t resolves the inconsistency by unlocking only the page that causes it, instead of unlocking the entire memory. Clearly, the runtime overhead of this approach depends on the number of times inconsistency is triggered during attestation. In this experiment, we measure the overhead through the downtime of P′, i.e., the total elapsed time for P′ to complete writing into locked pages. Figure 12 illustrates that the overhead, as expected, is linear in terms of the number of modified pages. It also shows that it is more beneficial to use the alternative approach where P′ is expected to perform only a few memory writes. In our experimental setting, this threshold is around 0.12% of P′'s memory pages.

### 6. Related Work

To the best of our knowledge, there has been no prior work on the temporal consistency of integrity-ensuring functions, though it is possible that this concept has been considered under a different guise in the cryptographic literature. Extended versions like Inc-Lock-Ext and Cpy-Lock & Writeback can be viewed as a form of protection against “Time of Check Time of Use” (TOCTOU) attacks in certain applications.

The “Provable Virus Detection” method discussed in [22] is a very relevant result. In it, a secret embedded within software running on a device is periodically checked by a trusted verifier. The argument is that injected malware consistently destroys the secret, making its presence detectable. While promising, [22] only deals with malware directly inserted into a system (e.g., via DMA) and requires substantial modifications to the CPU.

One alternative way to detect malware without locking memory (but without guaranteed consistency) is explored in [4]. Memory is measured in a random order, which cannot be learned or anticipated by malware. Since memory is never locked, this is an advantage for time-sensitive applications. The main drawback of [4] is its probabilistic nature, which can lead to a significantly increased time to perform attestation.

The rest of this section focuses on related work in Remote Attestation (RA).

RA aims to detect malware presence by verifying the integrity of a remote and untrusted embedded (or IoT) device. It is typically realized as a protocol, whereby a trusted verifier interacts with a remote prover to obtain a challenge-based integrity measurement of the latter’s memory state. RA techniques fall into three main categories: hardware-based, software-based, and hybrid.

**Hardware-based attestation** [32, 38] uses dedicated hardware components, such as a Trusted Platform Module (TPM) [15], ARM TrustZone [23], or Intel SGX [8], to execute attestation code in a trusted execution environment. Even though such features are currently available in personal computers, laptops, and smartphones, they are still considered a “luxury” for low-end embedded devices.

**Software-based attestation** [34, 35] requires no hardware support and performs attestation solely based on software and precise timing measurements. When deployed on a single-processor system, this approach can ensure temporal consistency; malware could try to interrupt the measurement process and cause temporal inconsistency (e.g., by moving itself around) during attestation. However, this action will result in additional delay, which is then detectable by the verifier. Software-based approaches limit the prover to being one-hop away from the verifier to ensure that the round-trip time is either negligible or fixed. Such approaches also rely on strong assumptions about attackers’ behavior [1] and are typically used only for legacy devices, where no other RA techniques are viable.

**Hybrid attestation** [2, 11, 19], based on software/hardware co-design, realizes RA while attempting to minimize required hardware features and the software footprint. SMART [11] is the first hybrid RA design with minimal hardware modifications to existing microcontroller units (MCUs). It has the following key features:
- **Immutable Attestation Code:** Located in and executed from ROM.
- **Safe Attestation Code:** Execution always terminates and is safe.
- **Atomic Execution:** Uninterruptible and starts from the first instruction and exits at the last instruction, enforced by hard-wired MCU access controls and disabling interrupts upon entry of attestation code.
- **Secret Attestation Key:** Stored in an isolated memory location accessible only from within attestation code, based on hard-wired MCU rules.
- **Leakage Control:** Leaks no information other than the attestation result.

Subsequently, [3] extended SMART to defend against verifier impersonation and denial-of-service (DoS) attacks. The resultant design (SMART+) additionally requires the prover to have a Reliable Read-Only Clock (RROC), which is needed to perform verifier authentication and prevent replay, reorder, and delay attacks. To ensure reliability, RROC cannot be modified by non-physical (software) means. Upon receiving a verifier request, ROM-resident attestation code checks the request’s freshness using RROC, authenticates it, and only then proceeds to perform attestation.

**TrustLite** [19] security architecture also supports RA for low-end devices. It differs from SMART in two ways:
1. Interrupts are allowed and are handled securely by the CPU Exception Engine.
2. Static access control rules can be programmed in software using an Execution-Aware Memory Protection Unit (EA-MPU).

A follow-on effort, called TyTAN [2], adopts a similar approach while providing additional real-time guarantees and dynamic configuration for safety- and security-critical applications. As mentioned earlier, both TrustLite [19] and TyTAN [2] support interrupts. While this allows for time-critical processes to take priority over others and preserve the prover's functionality, attestation results may not be consistent. Memory can change once attestation is interrupted, and the final attestation result might correspond to a state of the prover's memory that never existed.

In summary, RA architectures that disable interrupts or ensure atomic execution through other means automatically (though only coincidentally) ensure temporal consistency on single-processor devices. In multi-processor settings, atomic execution is insufficient. RA architectures that allow interrupts must ensure temporal consistency (e.g., via mechanisms described in this paper); otherwise, nonsensical or incorrect results might be produced.

### References

[1] SANS Institute. 2014. Securing the Internet of Things Survey. https://www.sans.org/reading-room/whitepapers/analyst/securing-internet-things-survey-34785

[2] ISO/IEC. 2011. Information technology – Security techniques – Message Authentication Codes (MACs) – Part 1: Mechanisms using a block cipher. Standard. ISO.

[3] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, et al. 2009. seL4: Formal verification of an OS kernel. In Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles.

[4] Patrick Koeberl, Steffen Schulz, Ahmad-Reza Sadeghi, and Vijay Varadharajan. 2014. TrustLite: A security architecture for tiny embedded devices. In ACM European Conference on Computer Systems (EuroSys).

[5] Ralph Langner. 2013. To Kill a Centrifuge: A Technical Analysis of What Stuxnet’s Creators Tried to Achieve.

[6] Yanlin Li, Jonathan M. McCune, and Adrian Perrig. 2011. VIPER: Verifying the Integrity of PERipherals’ Firmware. In Proceedings of the 18th ACM Conference on Computer and Communications Security (CCS).

[7] Richard J. Lipton, Rafail Ostrovsky, and Vassilis Zikas. 2016. Provably Secure Virus Detection: Using The Observer Effect Against Malware. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP.

[8] ARM Ltd. 2017. ARM TrustZone. https://www.arm.com/products/security-on-arm/trustzone

[9] LWN.net. 2018. DR rootkit released under the GPL. https://lwn.net/Articles/297775/

[10] Wired Magazine. 2013. Trojan Turns Your PC Into Bitcoin Mining Slave. https://www.wired.com/2013/04/bitcoin-trojan

[11] National ICT Australia and other contributors. 2014. seL4 Libraries. https://github.com/seL4/seL4_libs

[12] National ICT Australia and other contributors. 2014. The seL4 Repository. https://github.com/seL4/seL4

[13] Daniele Perito and Gene Tsudik. 2010. Secure Code Update for Embedded Devices via Proofs of Secure Erasure. In ESORICS.

[14] Ryan Roemer, Erik Buchanan, Hovav Shacham, and Stefan Savage. 2012. Return-Oriented Programming: Systems, Languages, and Applications. ACM Trans. Inf. Syst. Secur.

[15] Ethan M Rudd, Andras Rozsa, Manuel Günther, and Terrance E Boult. 2017. A Survey of Stealth Malware Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions. IEEE Communications Surveys & Tutorials.

[16] MJ Saarinen and JP Aumasson. 2015. The BLAKE2 cryptographic hash and message authentication code (MAC), RFC 7693. Technical Report. IETF.

[17] Dries Schellekens, Brecht Wyseur, and Bart Preneel. 2008. Remote attestation on legacy operating systems with trusted platform modules. Science of Computer Programming.

[18] Arvind Seshadri, Mark Luk, Adrian Perrig, Leendert van Doorn, and Pradeep Khosla. 2006. SCUBA: Secure Code Update By Attestation in Sensor Networks. In ACM Workshop on Wireless Security (WiSe).

[19] Arvind Seshadri, Mark Luk, Elaine Shi, Adrian Perrig, Leendert van Doorn, and Pradeep Khosla. 2005. Pioneer: Verifying Code Integrity and Enforcing Untampered Code Execution on Legacy Systems. In Proceedings of the Twentieth ACM Symposium on Operating Systems Principles.

[20] Arvind Seshadri, Adrian Perrig, Leendert Van Doorn, and Pradeep Khosla. 2004. SWATT: Software-based attestation for embedded devices. In IEEE Symposium on Research in Security and Privacy (S&P).

[21] IEEE Spectrum. 2013. The Real Story of Stuxnet. http://spectrum.ieee.org/telecom/security/the-real-story-of-stuxnet

[22] Secure Hash Standard. 2002. FIPS PUB 180-2.

[23] Frederic Stumpf, Omid Tafreschi, Patrick Röder, and Claudia Eckert. 2006. A Robust Integrity Reporting Protocol for Remote Attestation. In Workshop on Advances in Trusted Computing (WATC).

[24] Symantec. 2015. GreenDispenser: Self-deleting Malware. https://www.symantec.com/security_response/writeup.jsp?docid=2015-092513-0300-99

[25] Wei Yan, Zheng Zhang, and Nirwan Ansari. 2008. Revealing packed malware. seCurity & PrivaCy.

### 7. Conclusions

In this paper, we explore the discrepancy between (implicit) theoretical assumptions and implementations of cryptographic integrity-ensuring functions, focusing on the context of Remote Attestation (RA). We show that, in practice, inputs to such functions can change during computation, and the vulnerability window can be large since cryptographic computations can be time-consuming. We propose multiple practical mechanisms to ensure the consistency of integrity-ensuring functions. These mechanisms offer trade-offs between consistency guarantees, performance overhead, and impact on memory availability. We implement the proposed mechanisms on two commodity platforms in the context of a hybrid RA architecture for embedded systems. Results show that locking/unlocking of memory incurs negligible overhead over computing cryptographic integrity-ensuring functions, e.g., MACs. We demonstrate that ensuring temporal consistency can be achieved with less than 10% overhead on both platforms, while providing much better availability for time-critical applications. We believe that this paper highlights the importance of considering temporal consistency in the design and implementation of integrity-ensuring functions.