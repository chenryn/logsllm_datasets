# Intrusion Detection Capability (CID) and Its Application in IDS Evaluation

## 1. Introduction
In this paper, we present a new metric, Intrusion Detection Capability (CID), for evaluating the performance of Intrusion Detection Systems (IDSs). CID is derived from an information-theoretic perspective and provides a unified measure that combines various commonly used metrics such as true positive rate, false positive rate, and predictive values. This metric also factors in the base rate, which is crucial for understanding the operational environment of the IDS.

## 2. Experimental Results
### 2.1 PHAD Test
Figure 8(a) shows the results of the PHAD test, where a low false positive rate and high detection rate indicate that CID is no better (and no worse) than the Receiver Operating Characteristic (ROC) curve. The top graph in Figure 8(b) illustrates that as the threshold drops, CID reaches a peak and then declines, while the ROC curves continue to increase slowly.

### 2.2 PAYL Analysis
An analyst using only the top graph in Figure 8(b) might be tempted to set a threshold lower than 8 (where α = 3 × 10−3), because the detection capability still increases even if the false positive rate grows slightly. However, using CID, we see that the detection capability actually declines after CID = 0.033448 (marked with a vertical line in Figure 8(b)). Thus, CID identifies a higher but optimal operating threshold of 64 (where α = 0.7 × 10−3, 1 − β = 0.10563). In this situation, CID provides a better operating point, which is not obvious from ROC analysis alone.

### 2.3 Comparison with Snort
To demonstrate how CID can be used to compare different IDSs, we ran Snort (Version 2.1.0 Build 9) on the same data as PAYL. With the use of libwhisker, Snort had a poor detection rate (1 − β = 0.0117) but a good false positive rate (α = 0.0000006701). Without CID, it would be difficult to determine which IDS is better. Using the base rate B = 0.000010191, we calculated CID = 0.0081 for Snort. Clearly, 0.033448 > 0.0081, so (optimally configured) PAYL performs better than Snort based on our test data.

## 3. Discussion
### 3.1 Trace-Driven Evaluation and Ground Truth
All IDS evaluations are currently trace-driven, meaning that the evaluation data set should include known details about attacks and normal traffic. This allows for the calculation of the base rate, false positive rate, and false negative rate, which are then used to compute CID. In our technical report [8], we also briefly discuss the estimation of prior probabilities and transition probabilities in real situations.

### 3.2 Unit of Analysis
An important problem in IDS evaluation is the "unit of analysis." Different IDSs may analyze packets or flows, leading to different base rates even on the same data set. While this does not affect the use of CID for fine-tuning an IDS, it must be considered when comparing different IDSs. We recommend running IDSs based on the same unit of analysis, data set, and detection spaces for a fair comparison.

### 3.3 Involving Cost Analysis in CID
CID can be extended to include cost analysis by using a weighted conditional entropy. For example, in a military network, a large weight can be assigned to missed attacks (false negatives), giving more importance to FNs over FPs. Similarly, in scenarios with a single overloaded operator, a larger weight can be assigned to false positives. This extension allows CID to achieve similar capabilities as ROC combined with cost analysis.

## 4. Related Work
### 4.1 Existing Metrics
Commonly used metrics in IDS evaluation include true positive rate and false positive rate. ROC curves are often used to consider both metrics, as seen in the work of Lippmann et al. [12] and McHugh [15]. Other metrics, such as Bayesian detection rate and Bayesian negative rate proposed by Axelsson [1], are also useful but lack a unified approach.

### 4.2 Information-Theoretic Approaches
Our work builds on previous information-theoretic approaches, such as those by Lee et al. [11] and Maxion et al. [14]. These studies have shown that information-theoretic measures can provide valuable insights into IDS performance. Our new metric, CID, naturally unifies these measures and provides a more comprehensive evaluation of IDS capabilities.

## 5. Conclusion and Future Work
The contributions of this paper are both theoretical and practical. We provided an in-depth analysis of existing IDS metrics and argued that the lack of a unified metric makes it difficult to fine-tune and compare different IDSs. We proposed CID, a natural and unified metric derived from information theory, which combines all commonly used metrics and factors in the base rate. CID can be used to choose the best operation point for an IDS and to compare different IDSs, even if their FP and FN rates differ.

Future work will include extending CID to more complex models of IDS inputs and outputs, studying the intrusion detection process using channel capacity models, and further developing cost-based extensions.

## 6. Acknowledgments
This work is supported in part by NSF grant CCR-0133629 and Army Research Office contract W911NF0510139. The contents of this work are solely the responsibility of the authors and do not necessarily represent the official views of NSF and the U.S. Army.

## 7. References
[1] S. Axelsson. The base-rate fallacy and its implications for the difficulty of intrusion detection. In Proceedings of ACM CCS’1999, November 1999.
[8] G. Gu, P. Fogla, D. Dagon, W. Lee, and B. Skoric. An information-theoretic measure of intrusion detection capability. Technical Report GIT-CC-05-10, College of Computing, Georgia Tech, 2005.
[9] J. Hancock and P. Wintz. Signal Detection Theory. McGraw-Hill, 1966.
[10] P. Helman and G. Liepins. Statistical foundations of audit trail analysis for the detection of computer misuse. IEEE Transactions on Software Engineering, 19(9), September 1993.
[11] W. Lee and D. Xiang. Information-theoretic measures for anomaly detection. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, May 2001.
[12] R. P. Lippmann, D. J. Fried, and I. G. etc. Evaluating intrusion detection systems: The 1998 darpa off-line intrusion detection evaluation. In Proceedings of the 2000 DARPA Information Survivability Conference and Exposition (DISCEX’00), 2000.
[13] M. V. Mahoney and P. K. Chan. PHAD: Packet header anomaly detection for identifying hostile network traffic. Technical Report CS-2001-4, Florida Tech, 2001.
[14] R. Maxion and K. M. C. Tan. Benchmarking anomaly-based detection systems. In Proceedings of DSN’2000, 2000.
[15] J. McHugh. Testing intrusion detection systems: A critique of the 1998 and 1999 darpa off-line intrusion detection system evaluation as performed by Lincoln Laboratory. ACM Transactions on Information and System Security, 3(4), November 2000.
[16] MIT Lincoln Laboratory. 1999 darpa intrusion detection evaluation data set overview. http://www.ll.mit.edu/IST/ideval/, 2001.
[17] V. Paxson. Bro: A system for detecting network intruders in real-time. Computer Networks, 31(23-24):2435–2463, December 1999.
[18] J. Pluim, J. Maintz, and M. Viergever. Mutual information based registration of medical images: A survey. IEEE Trans on Medical Imaging, 22(8):986–1004, Aug 2003.
[2] S. Axelsson. A preliminary attempt to apply detection and estimation theory to intrusion detection. Technical Report 00-4, Dept. of Computer Engineering, Chalmers University of Technology, Sweden, March 2000.
[19] T. H. Ptacek and T. N. Newsham. Insertion, evasion, and denial of service: Eluding network intrusion detection. Technical report, Secure Networks Inc., January 1998. http://www.aciri.org/vern/Ptacek-Newsham-Evasion-98.ps.
[3] T. Cover and J. Thomas. Elements of Information Theory. John Wiley, 1991.
[4] M. Dacier. Design of an intrusion-tolerant intrusion detection system, Maftia Project, deliverable 10. Available at http://www.maftia.org/deliverables/D10.pdf. 2005.
[5] D. Denning. An intrusion-detection model. IEEE Transactions on Software Engineering, 13(2), Feb 1987.
[6] J. E. Gaaffney and J. W. Ulvila. Evaluation of intrusion detectors: A decision theory approach. In Proceedings of the 2001 IEEE Symposium on Security and Privacy, May 2001.
[7] I. Graf, R. Lippmann, R. Cunningham, K. K. D. Fried, S. Webster, and M. Zissman. Results of DARPA 1998 off-line intrusion detection evaluation. Presented at DARPA PI Meeting, 15 December 1998.
[21] R. F. Puppy. Libwhisker official release v2.1, 2004. Available at http://www.wiretrip.net/rfp/lw.asp.
[22] M. Roesch. Snort - lightweight intrusion detection for networks. In Proceedings of USENIX LISA’99, 1999.
[23] A. Strehl. Relationship-based clustering and cluster ensembles for high-dimensional data mining, May 2002. PhD thesis, The University of Texas at Austin.
[24] J. A. Swets. Measuring the accuracy of diagnostic systems. Science, 240(4857):1285–1293, 1988.
[25] K. Wang and S. J. Stolfo. Anomalous payload-based network intrusion detection. In Proceedings of RAID’2004, September 2004.