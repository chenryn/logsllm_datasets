# Auspice: A Scalable, Geo-Distributed, Federated Global Name Service

## 4.3.3 Context-Aware Delivery
Next, we present a proof of concept for context-aware communication, a novel communication primitive enabled by Auspice's extensible key-value API. Auspice allows applications to bind an msocket not only to human-readable names or GUIDs but also to abstract context descriptors, such as `msocket.bind("[geoloc: [lat,long],radius]")`. Writes to this msocket are reliably delivered to all GUIDs within the geo-fence created by this descriptor. Under the hood, msocket invokes Auspice to create an on-demand group GUID, which is a GUID with a membership field consisting of a set of member GUIDs. msocket then resolves each member GUID to its socket address and establishes an msocket connection for reliable delivery.

Figure 5(c) illustrates an experiment involving a group creator (also the message sender) on an Android phone and several potential members on PlanetLab nodes. Five of these nodes fake-register their coordinates in Auspice to appear within the created geo-fence. The round-trip time (RTT) between the group creator and members is 125ms. The figure shows that group creation, a single call to Auspice that returns all member GUIDs, takes approximately 200ms. Subsequently, an internal msocket connect to each member involves another Auspice lookup to resolve the GUID to a socket address and connect in parallel to all five members, which takes 250-280ms. After this, the creator sends three short messages back-to-back, each taking roughly one RTT to be reliably delivered.

Further details on optimizing context-aware queries in Auspice, reducing membership staleness, and the connection migration protocol are outside the scope of this paper [6]. This experiment aims to exemplify the powerful, new communication primitive enabled by context descriptors compared to strictly hierarchical DNS names, as discussed in §2.2.

## 4.4 Auspice vs. Managed DNS Providers
Can demand-aware replication benefit commercial managed DNS providers that largely rely on static replication of domain names? To investigate this, we compare Auspice against three top-tier providers: UltraDNS, DynDNS, and DNSMadeEasy, which offer geo-replicated authoritative DNS services widely used by enterprises (e.g., Dyn provides DNS service for Twitter).

### 4.4.1 Lookup Latency
We compare Auspice to UltraDNS for a workload of lookups for domain names serviced by the provider. We identified 316 domain names among the top 10K Alexa websites serviced by this provider and determined the geo-distribution of lookups for each name from their data [2]. For each name, we measured the latency for 1000 lookups from across 100 PlanetLab nodes. We ensured that lookups were served from the name servers maintained by the provider by requesting the address for a new random sub-domain name each time, e.g., `xqf4p.google.com` instead of `google.com`, which is unlikely to exist in a cache and requires an authoritative lookup. Auspice name servers are deployed across a total of 80 PlanetLab locations, while UltraDNS has 16 known server locations [50]. We evaluated Auspice for three configurations with 5, 10, and 15 replicas of a name, respectively.

Figure 6 shows the lookup latencies for Auspice and UltraDNS. UltraDNS incurs a median latency of 45ms with 16 replicas, while Auspice incurs 41ms, 22ms, and 18ms, respectively, with 5, 10, and 15 replicas. With 5 replicas, Auspice’s performance is comparable to UltraDNS with one-third the replication cost. With 15 replicas, Auspice incurs 60% lower latency for a comparable cost. The comparison against Dyn and DNSMadeEasy is qualitatively similar [1]. Thus, Auspice’s demand-aware replication achieves a better cost-performance trade-off compared to static replication.

### 4.4.2 Update Propagation Delay
To measure update propagation delays, we purchased DNS services from three providers for separate domain names. All providers replicated a name at 5 locations across the US and Europe for the services we purchased. We issued address updates for the domain name serviced by that provider and then immediately started lookups to the authoritative name servers for our domain name. These authoritative name servers can be queried only via an anycast IP address, i.e., servers at different locations advertise the same externally visible IP address. Therefore, to maximize the number of provider locations queried, we sent queries from 50 random PlanetLab nodes. From each location, we periodically sent queries until all authoritative name server replicas returned the updated address. The update propagation latency at a node is the time between when the node starts sending lookups and when it receives the updated address. The latency of an update is the maximum update latency measured at any of the nodes. We measured the latency of 100 updates for each provider.

To measure update latencies for Auspice, we replicated 1000 names at a fixed number of PlanetLab nodes across the US and Europe. The number of nodes was chosen to be 5, 10, and 20 across three experiments. A client sent an update to the nearest node and waited for update confirmation messages from all replicas. The latency of an update is the time difference between when the client sent an update and when it received the update confirmation message from all replicas (an upper bound on the update propagation delay). Figure 7 shows the distribution of measured update latencies for Auspice and the three managed DNS providers. Auspice incurs lower update propagation latencies than all three providers for an equal or greater number of replica locations for names. We were unable to ascertain from UltraDNS why their update latencies are an order of magnitude higher than network propagation delays, but this finding is consistent with a recent study [50] that has shown latencies of up to tens of seconds for these providers. Indeed, some providers even distinguish themselves by advertising shorter update propagation delays than competitors [50].

### Sensitivity Analyses and Other Results
We have conducted a comprehensive evaluation of the sensitivity of Auspice’s performance-cost trade-offs to workload and system parameters across scales varying by several orders of magnitude. These include workload parameters such as geo-locality, read-to-write rate ratio, and device-to-service name ratio, as well as system parameters such as fault-tolerance threshold, capacity utilization, perturbation knob, and tunable overhead of replica reconfiguration, using a combination of simulation and system experiments. These results do not qualitatively change the findings in this paper and are deferred to the technical report [1].

## 5. Related Work
Our work draws on lessons learned from an extensive body of prior work on network architecture and distributed systems, as described in §1 and §2.1. We discuss related work not covered elsewhere in the paper here.

### DNS
Many studies have addressed issues related to performance, scalability, load balancing, and denial-of-service vulnerabilities in DNS resolution infrastructure [46, 49, 16, 22]. Several DHT-based alternatives have been proposed [49, 20, 45], and we compare against one representative proposal, Codons [49]. In general, DHT-based designs are ideal for balancing load across servers but are less well-suited to scenarios with a large number of service replicas that need to coordinate upon updates and are at odds with scenarios requiring placement of replicas close to pockets of demand. In comparison, Auspice uses a planned placement approach.

Vu et al. describe DMap [55], an in-network DHT scheme similar in spirit to Random-M as evaluated in our experiments (§4) (with a more direct comparison in [1]), showing that demand-aware placement can dramatically outperform randomized placement. A more important qualitative distinction is that DMap ties federation to the interconnection structure between ISPs, which entails commensurate lookup latency penalties and potential incentive mismatches by mapping GUIDs to non-provider ISPs. In comparison, the Auspice approach decouples the federation structure between GNS providers from that between ISPs.

### Server Selection
Many prior systems have addressed the server selection problem with data or services replicated across a wide-area network. Examples include anycast services [25, 15, 57] to map users to the best server based on server load or network path characteristics. These systems, as well as CDNs and cloud hosting providers, share our goals of proximate server selection and load balance given a fixed placement of server replicas. Auspice differs in that it additionally considers replica placement itself as a degree of freedom in achieving latency or load balance.

### Dynamic Placement
We were unable to find prior systems that automatically reconfigure the geo-distributed replica locations of frequently mutable objects while preserving consistency (i.e., those satisfying all four italicized properties). However, reconfigurable placement has been studied for static or slow-changing content [30] or within a single data center, or without replication. For example, Volley [11] optimizes the placement of mutable data objects based on the geo-distribution of accesses and is similar in spirit to Auspice in this respect. However, it implicitly assumes a single replica for each object, so it does not have to worry about high update rates or replica coordination overhead.

Auspice is related to many distributed key-value stores [5, 23, 3], most of which are optimized for distribution within, not across, data centers. Some (e.g., Cassandra) support a geo-distributed deployment using a fixed number of replica sites. Spanner [19] is a geo-distributed data store that synchronously replicates data ("directories") across data centers with a semi-relational database abstraction. Compared to Spanner, Auspice does not provide any guarantees on operations spanning multiple records, but unlike Spanner’s geographic placement of replicas that "administrators control" by creating a "menu of named options," Auspice automatically reconfigures the number and placement of replicas to reduce lookup latency and update cost. Furthermore, Spanner assigns a large number of directory objects to a much smaller number of fixed Paxos groups; Auspice supports an arbitrarily reconfigurable Paxos group per object based on principles in recent theoretical work on reconfigurable consensus, e.g., Vertical Paxos [39] and the more recent report on Viewstamped Replication Revisited [41].

## 6. Conclusions
In this paper, we presented the design, implementation, and evaluation of Auspice, a scalable, geo-distributed, federated global name service for any Internetwork where high mobility is the norm. The name service can resolve flexible identifiers (human-readable names, self-certifying identifiers, or arbitrary strings) to network locations or other attributes that can also be defined in a flexible manner. At the core of Auspice is a placement engine for replicating name records to achieve low lookup latency, low update cost, and high availability. Our evaluation shows that Auspice’s placement strategy can significantly improve the performance-cost trade-offs struck both by commercial managed DNS services employing simplistic replication strategies today and previously proposed DHT-based replication alternatives with or without high mobility. Our case studies confirm that Auspice can form the basis of an end-to-end mobility solution and also enable novel context-aware communication primitives that generalize name- or address-based communication. A pre-release version of Auspice on EC2 can be accessed through the developer portal at http://gns.name.

**Acknowledgments:** This research was funded in part by CNS-1040781 and CNS-0845855. We thank the rest of the MobilityFirst team, the paper’s past and recent reviewers, our shepherd Ellen Zegura, Anand Seetharam, Emmanuel Cecchet, Jim Kurose, Marvin Sirbu, and NSF-FIA meeting participants for their feedback.

## References
[1] A Global Name Service for a Highly Mobile Internetwork. UMass SCS Technical Report, 2013 and 2014. https://web.cs.umass.edu/publication.
[2] Alexa Web Information Service. http://www.alexa.com.
[3] Cassandra. http://cassandra.apache.org.
[4] MobilityFirst Future Internet Architecture Project. http://mobilityfirst.cs.umass.edu/.
[5] mongoDB. http://www.mongodb.org/.
[6] msocket: System Support for Developing Seamlessly Mobile, Multipath, and Middlebox-Agnostic Applications. UMass SCS Technical Report, 2014. https://web.cs.umass.edu/publication.
[7] Server fault: DNS - Any way to force a name server to update the record of a domain? http://serverfault.com/questions/41018.
[8] The Locator/ID Separation Protocol (LISP). RFC 6830.
[9] ICANN Hears Concerns about Accountability, Control, October 2008. http://www.infoworld.com/t/networking/icann-hears-concerns-about-accountability-control-216.
[10] Debate Rages over who Should Control ICANN. Processor, 31(16), June 2009.
[11] S. Agarwal, J. Dunagan, N. Jain, S. Saroiu, A. Wolman, and H. Bhogan. Volley: Automated Data Placement for Geo-Distributed Cloud Services. In USENIX NSDI, 2010.
[12] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen, D. Moon, and S. Shenker. Accountable Internet Protocol. In ACM SIGCOMM, 2008.
[13] M. Arye, E. Nordstrom, R. Kiefer, J. Rexford, and M. J. Freedman. A Formally-Verified Migration Protocol For Mobile, Multi-Homed Hosts. In ICNP, 2012.
[14] H. Balakrishnan, K. Lakshminarayanan, S. Ratnasamy, S. Shenker, I. Stoica, and M. Walfish. A Layered Naming Architecture for the Internet. In ACM SIGCOMM, 2004.
[15] S. Bhattacharjee and et al. Application-Layer Anycasting. In IEEE INFOCOM, 1997.
[16] N. Brownlee, K. Claffy, and E. Nemeth. DNS Measurements at a Root Server. In GLOBECOM, 2001.
[17] M. Caesar, T. Condie, and J. Kannan et al. ROFL: Routing on Flat Labels. In ACM SIGCOMM, 2006.
[18] Cisco. Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2012-2017. http://ciscovni.com.
[19] J. C. Corbett and J. Dean et al. Spanner: Google’s Globally Distributed Database. USENIX OSDI, 2012.
[20] R. Cox, A. Muthitacharoen, and R. Morris. Serving DNS Using a Peer-to-Peer Lookup Service. In IPTPS, 2002.
[21] G. DeCandia and et al. Dynamo: Amazon’s Highly Available Key-value Store. In ACM SOSP, 2007.
[22] DNSSEC. DNS Threats & Weaknesses of the Domain Name System, 2012. http://www.dnssec.net/dns-threats.php.
[23] R. Escriva, B. Wong, and E. G. Sirer. HyperDex: A Distributed, Searchable Key-value Store. In ACM SIGCOMM, 2012.
[24] A. Feldmann and et al. HAIR: Hierarchical Architecture for Internet Routing. In ReArch Workshop, 2009.
[25] M. J. Freedman, K. Lakshminarayanan, and D. Mazières. OASIS: Anycast for Any Service. In USENIX NSDI, 2006.
[26] D. Funato, K. Yasuda, and H. Tokuda. TCP-R: TCP mobility support for continuous operation. In ICNP, 1997.
[27] Z. Gao, A. Venkataramani, and J. F. Kurose. Towards a quantitative comparison of location-independent network architectures. In ACM SIGCOMM, 2014.
[28] Gartner. Sales of Android Phones to Approach One Billion in 2014. http://www.gartner.com/newsroom/id/2665715.
[29] M. Gritter and D. R. Cheriton. An Architecture for Content Routing Support in the Internet. In USITS, 2001.
[30] J. Gwertzman and M. Seltzer. The case for geographical push caching. In IEEE HotOS Workshop, May 1995.
[31] D. Han, A. Anand, F. Dogar et al., B. Li, H. Lim, and M. et al. XIA: Efficient Support for Evolvable Internetworking. In USENIX NSDI, 2012.