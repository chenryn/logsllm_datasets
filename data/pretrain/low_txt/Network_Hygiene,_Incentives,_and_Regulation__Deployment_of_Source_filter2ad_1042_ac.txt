### Deaggregation and Prefix Age in the Radix Trie

Deaggregation over time results in new prefixes within the radix trie. This deaggregation represents a new BGP policy, which resets the age of the prefix. Let \( t_0(p) \) be the time at which the IPv4 prefix \( p \) first appeared in the global BGP routing table. For a client with IPv4 address \( c \) who performs a Spoofer test at time \( t \), we find the longest matching prefix \( p' = LPM(c, t) \) within the radix trie at time \( t \). The prefix age is then calculated relative to the time of the test:

\[ \text{age}(c, t) = t - t_0(LPM(c, t)) \]

For each Spoofer client using a routed source address, we determined the age of the corresponding client prefix. We binned the results into months and determined the relative fraction of tests within each age bin that could successfully spoof the routed IPv4 address. Figure 5 shows the relationship between the fraction of tests where a client could spoof and the age of the prefix to which the client belonged. Using Pearson’s correlation, we found no meaningful relationship between age and spoofability.

### Epistemological Challenges in Crowd-Sourced Measurement

A crowd-sourced approach to assessing the deployment of Source Address Validation (SAV) faces several challenges. First, the opt-in nature of the measurement can induce sample bias: people interested in security issues are more likely to provide a measurement. Second, the measurement results we receive are distributed non-uniformly across time and networks. Third, the Internet itself is dynamic, with changes due to equipment upgrades, configuration changes, and policy changes. Fourth, the results of a single test may not be indicative of the larger network, prefix, or autonomous system from which the client executes its test.

Figure 6 illustrates the sparsity of our data. Even with periodic weekly probing by the client, only 8% of IPv4 /24 prefixes and 23% of ASes reported measurements in 20 or more weeks in the data collected between May 2016 and August 2019. This sparsity complicates inferences of remediation. For instance, if two samples from an AS are significantly separated in time and from different prefixes, and the first sample indicates the tested network permits spoofing while the second indicates filtering, there could be several explanations for the change. The tested network might have deployed SAV across their network, fixing the first prefix, or the first tested prefix might still permit spoofing.

#### Effect of Daemonizing the Spoofer Client

To mitigate sampling concerns, we daemonized the Spoofer client, running it in the background after installation, executing measurements whenever the client detected a new attached network, and repeating tests weekly on previously seen networks. This not only increases the number of test samples but also allows the system to automatically gather longitudinal samples of the same network, which is useful for characterizing the evolution of filtering policies.

Because we do not use a client identifier, it is not possible to determine whether a specific client continues to probe after its initial installation. Instead, we estimate the ability of the periodic daemon to gather longitudinal data for a given /24 prefix, longest matching BGP prefix, or AS. Figure 6 displays the cumulative fraction of these as a function of the number of active weeks. An active week is defined as any week for which there is a test report from a client with the daemon functionality.

Unfortunately, even with the daemon, 44% of the IPv4 /24 prefixes and 32% of the ASes reported measurements in only a single week between May 2016 and August 2019. This may be due to users who run the tool once and then uninstall it after obtaining their test result, or infrequent or one-off tests performed by clients while they are roaming, such as mobile hosts that attach infrequently to a hotel or airport network. These results are consistent with feedback from operators who used the client on a short-term basis while testing SAV configuration and then uninstalled the software from their laptop.

While our coverage is longitudinally sparse for many networks, figure 6 exhibits long tails. In particular, for large providers, we have tests in every week. For example, we obtained test results from clients in AT&T, Comcast, Verizon, Deutsche Telekom, Charter, and Cox in nearly all of the weeks since the release of the daemonized client in May 2016 to August 2019.

### Representativeness of Spoofer Data

Another inferential challenge of crowd-sourced measurement is assessing how representative the data is of the networks it samples and the larger Internet. Since the measurements we receive come from volunteers, both on-demand and triggered by clients detecting new network attachments, we do not control the sampling across either networks or time. While we can characterize the coverage of our results by networks, ASes, or geolocation, this characterization does not capture the degree to which our measurements correctly capture the behavior of those networks.

To gain confidence in the data’s representativeness, we assess the data’s predictive ability—i.e., whether we can use the data to predict the ability of a client to spoof using an IP address we have no prior tests from. We use the standard train-then-test supervised learning approach, always splitting the training and test sets such that the training samples are chronologically before the test samples. We analyze the data on month boundaries, such that the model is trained on all data prior to a given month and tested on all data for the month and after. In this way, we simulate predictions over future tests given the available data up to that point.

We built a simple per-prefix model of spoofability that operates in ascending chronological order over training samples. Our algorithm determines the global BGP prefix from which the client ran the test using an August 2019 RIB snapshot from route-views2 [1]. We then label (or update) the prefix in a radix-trie with the client’s spoofing status. To account for variations across policies of individual networks within larger BGP prefixes, we also label the spoofing status of the more specific /24 prefix of the client. The model computes the network-wide prior probability of spoofing using the fraction of clients that can spoof for each source address.

After building the radix-trie using the training data, the model queries the trie in a manner analogous to a routing table lookup over the test data. For each new client IP address in the test set, the model returns a prediction corresponding to the spoofing capability of its longest-prefix match on the radix trie. If there is no matching longest-prefix, the model flips a biased coin that captures the overall prior probability of being able to spoof. Note that we ignore clients in the test set that also occur in the training set to avoid artificially inflating the results via simple memorization of previous results from the same client IP address. Thus, we only test over clients that are “new.”

We term the ability to send spoofed packets a positive label. Our model’s accuracy is > 90% across IP versions and source addresses. However, accuracy is a poor measure of the model due to the low prior probability of spoofability, so we focus on recall and precision. Figure 7 plots per-prefix spoofability recall and precision of our model, as a function of the month that splits the training from the test data. Recall measures positive coverage: the fraction of clients able to spoof for which the model correctly predicted this fact. Precision (positive predictive value) captures fidelity: the fraction of clients the model predicted can spoof which were actually observed to spoof.

Figure 8 shows the fraction of Spoofer server vantage points (VPs) receiving spoofed packets of different types for the year ending August 2019, when at least one VP receives a spoofed packet. When we detect that the network hosting the client is not filtering IPv4 packets with private addresses, only a subset of the VPs receive them. Nearly all VPs receive spoofed packets of other types when the network hosting the client does not filter.

### NATs Are Not a Substitute for SAV

Our testing client is primarily installed on hosts whose IPv4 connectivity is provided by a Network Address Translation (NAT) system. In the year ending August 2019, we received reports from 2,783 ASes using IPv4; 2,418 (86.8%) of these ASes had tests involving a NAT. A well-implemented NAT should not forward packets with spoofed source addresses, as the spoofed address is unlikely to fall within the private address range the NAT serves. However, figure 4a shows that 6.4% of IPv4 /24 prefixes tested from clients behind a NAT did not filter packets with spoofed source addresses in the year ending August 2019. Further, NAT use is rare in IPv6 because unique addresses are plentiful, so SAV is explicitly required in IPv6 to filter packets with spoofed source addresses; figure 4a also shows that 12.3% of tested IPv6 /40 prefixes did not filter packets with spoofed source addresses over the same time period.

#### IPv4 NATs Are Broken

In practice, there are two failure modes where a NAT will forward packets with spoofed source addresses. We illustrate these failure modes in figure 9. In the first failure mode, the NAT simply forwards the packets with the spoofed source address (the victim) intact to the amplifier. We hypothesize this occurs when the NAT does not rewrite the source address because the address is not in the local network served by the NAT. In the second failure mode, the NAT rewrites the source address to the NAT’s publicly routable address and forwards the packet to the amplifier. When the server replies, the NAT system does the inverse translation of the source address, expecting to deliver the packet to an internal system. However, because the mapping is between two routable addresses external to the NAT, the packet is routed by the NAT towards the victim.

This second failure mode is important for two reasons. First, the victim still receives the attack packet, though the attacking node does not gain the benefit of amplification because it still bears the cost of sending the large packet. Second, previous studies that classified the intended victim of the attack using the source address of the packet arriving at the amplifier could have misinferred and thus miscounted the true victims of the attacks [12, 24]. Specifically, the source address of the packet arriving at the amplifier in figure 9b is the NAT’s external IP address, not the intended victim, who does eventually receive the amplified packet via the NAT router.

In figure 9, the amplifier and victims are addresses on separate systems, but in the Spoofer system, they are assigned to the same server (figure 1) so that we can detect both failure modes. In our data collected with the probing daemon for the 11 months between September 2018 (when we began testing for the second NAT failure mode) and August 2019, we received tests from 27.8K distinct IP addresses where we detected the client was testing from behind a NAT; in comparison, we received tests from 4.6K distinct IP addresses where the client was not behind a NAT.

51.0% of NATs blocked the spoofed packets, while the remainder forwarded the packets. 46.0% of the NATs forwarded the packet after rewriting the spoofed source IP address; 3.2% of the NATs translated the destination address of our response packet back to the original spoofed address and were able to forward the response back to the Spoofer server—even though the source address (A, the amplifier) would have caused the client’s network to discard our packet if the network had deployed SAV. 3.0% of the NATs (3.6K) forwarded the packet without rewriting the source IP address at all. In total, the Spoofer system received packets with spoofed source IP addresses from 6.2% of 27.8K IP addresses using NAT for these 11 months. In comparison, the Spoofer system received packets with spoofed source IP addresses from 13.8% of 4.6K IP addresses where the client was not behind a NAT over these same 11 months.

### IPv6 Looms Large

As IPv6 continues to gain importance, the community’s understanding of deployed IPv6 security and hygiene practices has not kept pace [13]. IPv6 SAV is particularly important as attackers shift to leveraging new attack vectors and exploiting IoT devices, which are frequently IPv6-connected [37]. IPv6 has important differences from IPv4 relating to SAV. First, whereas NATs are common in IPv4, the large address space of IPv6 means that NATs are relatively rare. By extension, the protection that should be afforded by NATs in IPv4 is missing in IPv6. Second, the immense size of the address space in IPv6 implies that attackers can utilize a much larger range of source addresses, potentially inducing state exhaustion in forwarding infrastructure.

In this subsection, we examine IPv6 SAV in detail. We first analyze filtering granularity, which is an important metric of how much of the vast IPv6 address space an attacker can spoof. Next, we infer the topological location of filtering in IPv6 as compared to IPv4 and discuss the implications for SAV deployment.

#### Filtering Granularity

A network that implements filtering to drop packets that do not originate from its prefix may still permit hosts to spoof the addresses of other hosts within that prefix. When SAV is in place, we term the prefix-length specificity of the policy the "filtering granularity." Whereas we might expect more fine-grained filtering in IPv4, the large size of IPv6 assignments (even residential customers are typically allocated at least a /64 prefix) makes this more challenging.