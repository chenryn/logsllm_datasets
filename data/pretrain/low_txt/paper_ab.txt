### 3. DATA SCHEMA

This section details the internal structure and data schema of LogChunks. LogChunks consists of both automatically retrieved and manually labeled, cross-validated logs. The dataset includes information on 797 build logs, organized into folders for each language and repository. Each repository folder contains approximately 10 examples, with full log files for builds that have failed, stored in plain text.

The `build-failure-reason` folder houses the manually labeled data, with one XML file per repository: `@.xml`. Table 1 provides an overview of the data within these XML files, using a build from `php@php-src` as an example. The remainder of this section will elaborate on the data embedded in the XML files, including the labeled log chunk, search keywords, and structural categories. Data from the developer validation study is available in the `developer-crossvalidation.csv` file, where the build ID can be used as a unique identifier to match it with other data.

#### 3.1. Log Chunk
The **Log Chunk** is a substring of the build log that describes why the build failed. This can be a failing test case or a compiler error. If the reason for the failure is in an external log file, the chunk will include a reference to that fact, such as "The command 'test/run!' exited with 1."

#### 3.2. Search Keywords
**Search Keywords** are a list of one to three freely chosen search strings that appear within or around the log chunk. These keywords are selected based on their frequent appearance next to failure-describing chunks during the manual analysis of about 800 build logs. Examples of keywords include "Error," "=DIFF=", "ERR!," and those shown in Table 1.

#### 3.3. Structural Category
Each repository has **Structural Categories** assigned to the chunks. These categories compare how chunks are represented within the build log. Build tools often highlight error messages with specific markings, such as starting lines with "ERROR" or surrounding them with special characters. Chunks with the same markings fall into the same structural category. For each repository, structural categories are represented as integers, starting at 0 and incrementing with the next appearing category in chronological build order.

### 4. POTENTIAL USE CASES

LogChunks can serve as a foundation for various studies:

- **Benchmarking Build Log Analysis Techniques:** LogChunks originated from the first author's Master’s Thesis, where she compared different log chunk retrieval techniques. It can be used as a benchmark to evaluate other build log analysis methods, such as the diff-based approach by Amar et al. [1].
- **Support Build Log Classification Algorithms:** Researchers who examine CI build failures can use the manually labeled chunks to locate the source for their classification algorithms and cross-validate their data.
- **Research on Build Logs:** The data can support research on how developers use keywords to retrieve information about build failures from logs and how they discuss CI build failures in pull requests [5].
- **Automatic Processing of Build Results:** LogChunks enables researchers to train algorithms to retrieve build failure descriptions from logs, providing a basis for further automatic processing of the retrieved log chunks.

### 5. RELATED DATA SETS

This section compares existing datasets of CI build logs with LogChunks.

#### 5.1. TravisTorrent
TravisTorrent [3] collects metadata about Travis CI builds, combining data from the public Travis CI and GitHub APIs and GHTorrent [9]. While it includes failing test cases, these are obtained through a manually developed parser that only supports specific Ruby test runners and Java Maven or JUnit logs. Many of the failing tests are incomplete and lack validation. In contrast, LogChunks provides manually labeled and cross-validated data on why builds fail, covering all possible build-failing errors.

#### 5.2. LogHub
LogHub [21] is a collection of system log datasets, used for comparing different approaches to parse unstructured system log messages into structured data. LogChunks, focused on semi-structured build log analysis, aims to play a similar role in its domain by providing a benchmark for comparing different build log analysis techniques.

### 6. FUTURE EXTENSIONS TO LOGCHUNKS

This section outlines current limitations and future improvements for LogChunks.

- **Multiple Substrings in Chunks:** Currently, the chunk is a single continuous substring. Future versions could include multiple substrings to capture the entire context of the failure.
- **Inclusion of More Repositories and Logs:** LogChunks currently includes 10 logs from each repository. Expanding the dataset with more logs and repositories will strengthen it as a benchmark.
- **Classification of Build Failure Causes:** The dataset does not classify the cause of the failure (e.g., due to tests, compilation, or linter errors). Annotating the cause of the build failure for each log would be a useful extension.
- **Other Information Chunks:** Beyond the chunk describing the build failure, LogChunks could be extended to label other types of information in the build log, such as warnings and build infrastructure details.
- **Validation of Search Keywords:** The provided keywords are based on the authors' experience. Future work will evaluate whether these keywords are also used by developers to find the log chunk describing the build failure.

### 7. SUMMARY

In this paper, we introduce LogChunks, a cross-validated dataset encompassing 797 build logs from 80 projects using Travis CI. For each log, we annotated the log chunk describing the build failure, provided keywords for searching, and categorized the chunks according to their format within the log. LogChunks advances the field of build log analysis by providing a benchmark for rigorous examination of research contributions [15] and opening up various research possibilities that previously required tedious manual classification.

### REFERENCES

[1] Anunay Amar and Peter C Rigby. 2019. Mining Historical Test Logs to Predict Bugs and Localize Faults in the Test Logs. In Proceedings of the 41st International Conference on Software Engineering (ICSE). IEEE, 140–151.

[2] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2017. Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub. In Proceedings of the 14th IEEE/ACM International Conference on Mining Software Repositories (MSR). IEEE, 356–367.

[3] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2017. TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration. In Proceedings of the 14th IEEE/ACM International Conference on Mining Software Repositories (MSR). IEEE, 447–450.

[4] Carolin Brandt, Annibale Panichella, and Moritz Beller. 2020. LogChunks: A Dataset for Build Log Analysis.

[5] Nathan Cassee, Bogdan Vasilescu. The Silent Helper: The Impact of Continuous Integration on Code Reviews. In Proceedings of the 27th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE Computer Society.

[6] Travis CI. 2019. Travis Build Status. https://docs.travis-ci.com/user/job-lifecycle/#breaking-the-build. Accessed: 2019-11-18.

[7] John Downs, Beryl Plimmer, and John G Hosking. 2012. Ambient Awareness of Build Status in Collocated Software Teams. In Proceedings of the 34th International Conference on Software Engineering (ICSE). IEEE, 507–517.

[8] Paul M Duvall, Steve Matyas, and Andrew Glover. 2007. Continuous Integration: Improving Software Quality and Reducing Risk. Pearson Education.

[9] Georgios Gousios. 2013. The Ghtorrent Dataset and Tool Suite. In Proceedings of the 10th IEEE Working Conference on Mining Software Repositories (MSR). IEEE, 233–236.

[10] Michael Hilton, Timothy Tunnell, Kai Huang, Darko Marinov, and Danny Dig. 2016. Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects. In Proceedings of the 31st International Conference on Automated Software Engineering (ASE). ACM, 426–437.

[11] Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M German, and Daniela Damian. 2016. An In-Depth Study of the Promises and Perils of Mining GitHub. Empirical Software Engineering 21, 5 (2016), 2035–2071.

[12] Louis G Michael IV, James Donohue, James C Davis, Dongyoon Lee, and Francisco Servant. 2019. Regexes Are Hard: Decision-Making, Difficulties, and Risks in Programming Regular Expressions. In Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE).

[13] Ade Miller. 2008. A Hundred Days of Continuous Integration. In Agile 2008 Conference. IEEE, 289–293.

[14] Hyunmin Seo, Caitlin Sadowski, Sebastian Elbaum, Edward Aftandilian, and Robert Bowdidge. 2014. Programmers’ Build Errors: A Case Study (At Google). In Proceedings of the 36th International Conference on Software Engineering (ICSE). ACM, 724–734.

[15] Susan Elliott Sim, Steve Easterbrook, and Richard C Holt. 2003. Using Benchmarking to Advance Research: A Challenge to Software Engineering. In Proceedings of the 25th International Conference on Software Engineering (ICSE). IEEE, 74–83.

[16] Edward Smith, Robert Loftin, Emerson Murphy-Hill, Christian Bird, and Thomas Zimmermann. 2013. Improving Developer Participation Rates in Surveys. In 2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE). IEEE, 89–92.

[17] Daniel Ståhl and Jan Bosch. 2014. Modeling Continuous Integration Practice Differences in Industry Software Development. Journal of Systems and Software 87 (2014), 48–59.

[18] Bogdan Vasilescu, Yue Yu, Huaimin Wang, Premkumar Devanbu, and Vladimir Filkov. 2015. Quality and Productivity Outcomes Relating to Continuous Integration in GitHub. In Proceedings of the 10th Joint Meeting on Foundations of Software Engineering. ACM, 805–816.

[19] Carmine Vassallo, Sebastian Proksch, Timothy Zemp, and Harald C Gall. 2018. Un-Break My Build: Assisting Developers with Build Repair Hints. In Proceedings of the 26th IEEE International Conference on Program Comprehension (ICPC). ACM, 41–51.

[20] Carmine Vassallo, Gerald Schermann, Fiorella Zampetti, Daniele Romano, Philipp Leitner, Andy Zaidman, Massimiliano Di Penta, and Sebastiano Panichella. 2017. A Tale of CI Build Failures: An Open Source and a Financial Organization Perspective. In Proceedings of the 33rd IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 183–193.

[21] Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael R Lyu. 2019. Tools and Benchmarks for Automated Log Parsing. In Proceedings of the 41st IEEE/ACM International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 121–130.