### Throughput and Round-Trip Time Analysis for Two Servers

To assess the similarity in network conditions, we first examined the round-trip time (RTT) values for the two servers. Figure 11 illustrates the per-flow average RTT distribution for both hosts over a three-day measurement period. The RTT distributions are nearly identical, indicating that each server experienced a similar mix of short- and long-RTT flows. Additionally, the per-packet RTT distribution for both servers is also nearly identical.

Figure 12 presents the per-flow throughput distributions for both hosts, excluding flows with throughput less than 100 bps, which typically involve small control packets. The throughput distributions are also nearly identical, suggesting that the host with RTOmin = 200µs did not perform worse overall compared to the host with RTOmin = 200ms.

We further analyzed the throughput distributions by categorizing them based on whether the flow's RTT was above or below 200ms. For flows with RTTs above 200ms, we used the variance in the two distributions as a control parameter, as any observed variance above 200ms is likely due to measurement noise, since RTOmin is no longer a factor. Figure 13 shows that the difference between the distributions for flows below 200ms is within this measurement noise. This data suggests that reducing RTOmin to 200µs does not significantly affect the performance of bulk-data TCP flows in wide-area networks.

### Interaction with Delayed ACK

In a datacenter environment, servers using a reduced retransmission timeout (RTO) may experience timeouts before the client's 40ms delayed ACK timer fires. This can lead to unnecessary retransmissions and a reduction in slow-start threshold (ssthresh). However, if the client acknowledges the retransmitted segment immediately, the server avoids a coarse-grained 40ms delay, only experiencing an unnecessary timeout.

Figure 14 compares the performance of a client with delayed ACK disabled, a client with a 200µs delayed ACK timer, and a client with the default 40ms delayed ACK configuration. Beyond 8 servers, a client with a 200µs delayed ACK timer receives 15–30 Mbps lower throughput compared to a client with delayed ACK disabled. The 40ms delayed ACK client experiences 100–200 Mbps lower throughput due to frequent timeouts. The 200µs delayed ACK timer delays the server by approximately one round-trip-time but does not force a timeout, resulting in a smaller performance impact.

Delayed ACK can be beneficial in congested environments, but in datacenters, it is often more advantageous to prioritize quick response over additional ACK-processing overhead. Our evaluations in Section 5 disable delayed ACK on the client for this reason. While disabling delayed ACK provides optimal performance, unmodified clients still achieve good performance and avoid incast collapse when servers implement fine-grained retransmissions.

### Related Work

**TCP Improvements:**
Over the years, several changes have improved TCP's ability to handle loss patterns and perform better in specific environments, particularly in high-performance datacenters. NewReno and SACK, for example, reduce the number of loss patterns that cause timeouts. Prior work on the TCP incast problem showed that NewReno improved throughput during moderate incast traffic, though not under severe conditions.

**Limited Transmit:**
Mechanisms like Limited Transmit were designed to help TCP recover from packet loss with small window sizes, addressing issues that occur during incast collapse. This solution helps maintain throughput under modest congestion but is less effective during severe incast collapse, where the most common loss pattern is the loss of the entire window.

**Proposed Improvements:**
Proposed improvements such as TCP Vegas and FAST TCP can limit window growth when RTTs increase, often combined with aggressive window growth algorithms to fill high bandwidth-delay links. Unlike the self-interfering oscillatory behavior on high-BDP links, incast collapse is triggered by the rapid ramp-up of numerous competing flows, leading to drastic RTT increases or full window losses. An RTT-based solution is an interesting approach, but adapting existing techniques for this purpose requires significant future work.

### Conclusion

This paper presents a practical, effective, and safe solution to eliminate TCP incast collapse in datacenter environments. Enabling microsecond-granularity TCP timeouts allowed high-fan-in, barrier-synchronized datacenter communication to scale to 47 nodes in a real cluster evaluation. Randomized retransmissions were used to scale to thousands of nodes in simulation. These modifications also benefit latency-sensitive datacenter applications where timeouts lasting hundreds of milliseconds can harm response time. A wide-area evaluation confirmed that these changes remain safe for use in the wide area, providing a general and effective improvement for TCP-based cluster communication.

### Acknowledgments

We would like to thank our shepherd Dave Maltz, Dilip Chhetri, Vyas Sekar, Srinivasan Seshan, and the anonymous reviewers for their comments and suggestions. We also thank Andrew Shewmaker, HB Chen, Parks Fields, Gary Grider, Ben McClelland, and James Nunez at Los Alamos National Laboratory for help with obtaining packet header traces. We thank the members and companies of the PDL Consortium (including APC, DataDomain, EMC, Facebook, Google, Hewlett-Packard, Hitachi, IBM, Intel, LSI, Microsoft, NEC, NetApp, Oracle, Seagate, Sun, Symantec, and VMware) for their interest, insights, feedback, and support. We thank Intel and NetApp for hardware donations that enabled this work.

This material is based upon research supported in part by the National Science Foundation via grants CNS-0546551, CNS-0619525, CNS-0326453, and CCF-0621499, by the Army Research Office under agreement number DAAD19-02-1-0389, by the Department of Energy under Award Number DE-FC02-06ER25767, and by Los Alamos National Laboratory under contract number 54515-001-07.

### References

[1] M. Allman, H. Balakrishnan, and S. Floyd. Enhancing TCP’s Loss Recovery Using Limited Transmit. Internet Engineering Task Force, Jan. 2001. RFC 3042.
[2] M. Allman and V. Paxson. On estimating end-to-end network path properties. In Proc. ACM SIGCOMM, Cambridge, MA, Sept. 1999.
[3] M. Aron and P. Druschel. Soft timers: Efficient microsecond software timer support for network processing. ACM Transactions on Computer Systems, 18(3):197–228, 2000.
[4] H. Balakrishnan, V. N. Padmanabhan, and R. Katz. The effects of asymmetry on TCP performance. In Proc. ACM MOBICOM, Budapest, Hungary, Sept. 1997.
[5] H. Balakrishnan, V. N. Padmanabhan, S. Seshan, and R. Katz. A comparison of mechanisms for improving TCP performance over wireless links. In Proc. ACM SIGCOMM, Stanford, CA, Aug. 1996.
[6] P. J. Braam. File systems for clusters from a protocol perspective. http://www.lustre.org.
[7] R. T. Braden. Requirements for Internet Hosts—Communication Layers. Internet Engineering Task Force, Oct. 1989. RFC 1122.
[8] L. S. Brakmo, S. W. O’Malley, and L. L. Peterson. TCP vegas: New techniques for congestion detection and avoidance. In Proc. ACM SIGCOMM, London, England, Aug. 1994.
[9] Y. Chen, R. Griffith, J. Liu, A. D. Joseph, and R. H. Katz. Understanding TCP incast throughput collapse in datacenter networks. In Proc. Workshop: Research on Enterprise Networking, Barcelona, Spain, Aug. 2009.
[10] k. claffy, G. Polyzos, and H.-W. Braun. Measurement considerations for assessing unidirectional latencies. Internetworking: Research and Experience, 3(4):121–132, Sept. 1993.
[11] J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In Proc. 6th USENIX OSDI, San Francisco, CA, Dec. 2004.
[12] Scaling memcached at Facebook. http://www.facebook.com/note.php?note_id=39391378919.
[13] S. Floyd and V. Jacobson. Random early detection gateways for congestion avoidance. IEEE/ACM Transactions on Networking, 1(4), Aug. 1993.
[14] B. Ford. Structured streams: A new transport abstraction. In Proc. ACM SIGCOMM, Kyoto, Japan, Aug. 2007.
[15] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google file system. In Proc. 19th ACM Symposium on Operating Systems Principles (SOSP), Lake George, NY, Oct. 2003.
[16] High-resolution timer subsystem. http://www.tglx.de/hrtimers.html.
[17] V. Jacobson. Congestion avoidance and control. In Proc. ACM SIGCOMM, pages 314–329, Vancouver, British Columbia, Canada, Sept. 1998.
[18] V. Jacobson, R. Braden, and D. Borman. TCP Extensions for High Performance. Internet Engineering Task Force, May 1992. RFC 1323.
[19] C. Jin, D. X. Wei, and S. H. Low. FAST TCP: motivation, architecture, algorithms, performance.
[20] E. Kohler, M. Handley, and S. Floyd. Designing DCCP: Congestion control without reliability. In Proc. ACM SIGCOMM, Pisa, Italy, Aug. 2006.
[21] R. Ludwig and M. Meyer. The Eifel Detection Algorithm for TCP. Internet Engineering Task Force, Apr. 2003. RFC 3522.
[22] M. Mathis, J. Mahdavi, S. Floyd, and A. Romanow. TCP Selective Acknowledgment Options. Internet Engineering Task Force, 1996. RFC 2018.
[23] A distributed memory object caching system. http://www.danga.com/memcached/.
[24] A. Mukherjee. On the dynamics and significance of low frequency components of Internet load. Internetworking: Research and Experience, 5:163–205, Dec. 1994.
[25] D. Nagle, D. Serenyi, and A. Matthews. The Panasas ActiveScale Storage Cluster: Delivering scalable high bandwidth storage. In SC ’04: Proceedings of the 2004 ACM/IEEE Conference on Supercomputing, Washington, DC, USA, 2004.
[26] ns-2 Network Simulator. http://www.isi.edu/nsnam/ns/, 2000.
[27] C. Partridge. Gigabit Networking. Addison-Wesley, Reading, MA, 1994.
[28] A. Phanishayee, E. Krevat, V. Vasudevan, D. G. Andersen, G. R. Ganger, G. A. Gibson, and S. Seshan. Measurement and analysis of TCP throughput collapse in cluster-based storage systems. In Proc. USENIX Conference on File and Storage Technologies, San Jose, CA, Feb. 2008.
[29] I. Psaras and V. Tsaoussidis. The TCP minimum RTO revisited. In IFIP Networking, May 2007.
[30] K. Ramakrishnan and S. Floyd. A Proposal to Add Explicit Congestion Notification (ECN) to IP. Internet Engineering Task Force, Jan. 1999. RFC 2481.
[31] S. Raman, H. Balakrishnan, and M. Srinivasan. An image transport protocol for the Internet. In Proc. International Conference on Network Protocols, Osaka, Japan, Nov. 2000.
[32] P. Sarolahti and M. Kojo. Forward RTO-Recovery (F-RTO): An Algorithm for Detecting Spurious Retransmission Timeouts with TCP and the Stream Control Transmission Protocol (SCTP). Internet Engineering Task Force, Aug. 2005. RFC 4138.
[33] S. Shepler, M. Eisler, and D. Noveck. NFSv4 Minor Version 1 – Draft Standard. http://www.ietf.org/internet-drafts/draft-ietf-nfsv4-minorversion1-29.txt.
[34] B. Welch, M. Unangst, Z. Abbasi, G. Gibson, B. Mueller, J. Zelenka, and B. Zhou. Scalable performance of the Panasas parallel file system. In Proc. USENIX Conference on File and Storage Technologies, San Jose, CA, Feb. 2008.
[35] Y. Zhang, N. Duffield, V. Paxson, and S. Shenker. On the constancy of Internet path properties. In Proc. ACM SIGCOMM Internet Measurement Workshop, San Francisco, CA, Nov. 2001.