### Time and Code Packaging

When the code is downloaded, the certificate is also downloaded along with it. The consumer can then use a verifier to inspect both the code and the certificate to ensure compliance with a security policy. If the code passes the verification, it is deemed safe to run. The verifier is an integral part of the consumer's trusted computing base, whereas the compiler, compiled code, and certificate do not need to be.

### 5.2.1 Java: An Example of a Language-Based Approach

Perhaps the first large-scale practical example of a language-based approach was the Java programming language [8]. Java's mechanism is designed to protect against malicious applets. The Java runtime environment includes a bytecode verifier that ensures basic properties such as memory, control flow, and type safety. Additionally, a trusted security manager enforces higher-level safety policies, such as restricted disk I/O. The Java compiler generates platform-independent virtual machine instructions or bytecode, which can be verified by the consumer before execution. This bytecode is either interpreted by a Java Virtual Machine (JVM) interpreter or further compiled into native code.

Early versions of Java contained several well-publicized security flaws [3]. For instance, a subtle defect in the Java type system allowed an applet to create and control a partially instantiated class loader. This could be exploited to load a malicious security manager, granting unlimited disk access.

Some authors [3, 9] attributed these issues to the lack of an adequate semantic model for Java. Steps have since been taken to address this [1, 19]. Despite initial shortcomings, the Java approach was a significant step forward in practical programming language security. It not only provided a simple and effective means of ensuring a basic level of security but also highlighted critical security issues arising from the rise of the Internet.

The machine-independent bytecode produced by the Java compiler is still quite high-level, which is a disadvantage. Once downloaded, the bytecode must either be interpreted by a JVM interpreter or compiled to native code by a just-in-time (JIT) compiler. Both techniques incur a runtime penalty. If the safety certificate represented in the bytecode were mapped to the level of native code by a back-end Java VM compiler, the same degree of safety could be ensured without the runtime penalty, as the code supplier could perform the back-end compilation before downloading. This would trade the platform independence of the Java VM for the efficiency of native code.

### 5.2.2 Proof Carrying Code (PCC)

Proof carrying code (PCC) [13, 14, 15, 16, 17, 18] is a strategy for producing and verifying formal proofs that code meets general security policies. The software supplier generates the proof, and the software consumer checks the proof before running the code. The security policy is expressed in first-order logic, augmented with symbols for various language and machine constructs.

The most general version of PCC involves a two-phase interaction between the supplier and the consumer. In the first phase, the supplier delivers a program consisting of annotated object code. These annotations, which include loop invariants and function pre- and post-conditions, simplify the next phase. The consumer formulates a safety policy and uses an automated tool to generate a verification condition from the policy and the annotated program. The verification condition is a logical formula that implies the program satisfies its security policy. In the second phase, the supplier proves the verification condition and sends the proof to the consumer. The consumer then runs a proof checker to validate the proof.

The verification condition generator is part of the consumer's trusted computing base and defines the security policy. Communication costs can be reduced if the supplier uses the same verification condition generator as the consumer. The consumer then verifies that the verification condition produced by the supplier matches the one generated locally.

A certifying compiler, such as the Touchstone compiler [14], produces the initial annotation of the code using information from the program source and program analysis during compilation. The Touchstone compiler supports a type-safe subset of C and allows common optimizations like dead code elimination, common subexpression elimination, copy propagation, instruction scheduling, register allocation, loop invariant hoisting, redundant code elimination, and the elimination of array bounds checks.

The advantages of the PCC approach include its expressiveness and ability to handle code optimizations. In principle, any security policy that can be constructed by a verification condition generator and expressed as a first-order verification condition can be handled. The main disadvantages are the two-phase protocol, the need for heavy machinery like a full-fledged first-order theorem prover and proof checker, and the large size of the proofs, which are roughly 2.5 times the size of the object code for type safety and even larger for more complex safety policies. Given the limited space available on boot firmware, this size penalty makes PCC unsuitable for our problem.

### 5.2.3 Typed Assembly Language (TAL)

Typed assembly language (TAL) [4, 9, 10, 12] can be viewed as a specialized form of proof-carrying code focused on verifying type safety. It is a language-based system where type information from a strongly-typed high-level language is preserved as the source code is transformed through a platform-independent typed intermediate language (TIL) [11, 23] down to the level of the object code. The result is a type annotation of the object code that can be checked by an ordinary type checker. In this special case, proof checking is reduced to type checking.

TAL is less expressive than PCC but can handle any security policy expressible in terms of the type system, including memory, control flow, and type safety. TAL is robust with respect to compiler optimizations, as type annotations can be transformed along with the code. TAL proofs, though much smaller than those in PCC, are still significantly larger than those needed by ECC.

Proof size can be traded off against the complexity of the verifier, but this increases and complicates the amount of trusted code.

### 6. Current Project Status

The long-term goal of this project is to adapt the ECC technique to the analysis of Open Firmware fcode programs to detect malicious boot software. We are implementing a certifying compiler and verifier necessary for this method. The ECC-based verifier will be integrated into an existing commercial implementation of the Open Firmware standard to provide practical malicious boot firmware detection to the marketplace.

Currently, we have a working prototype of the J2F compiler, which converts Java Virtual Machine (JVM) bytecode to Forth fcode for a single-threaded subset of the Java language. This subset is suitable for writing device drivers and other firmware modules. The compiler provides access to Open Firmware services through an API currently being designed. The API takes advantage of the natural object-oriented structure of the Open Firmware device tree and allows access to Open Firmware services from within Java programs.

The BootSafe verifier will initially verify only basic type safety, similar to the level provided by ECC and the Java bytecode verifier. This initial version operates as a standalone program. We have successfully compiled sample device drivers for a block-oriented storage device and a PCI bus, which are good representatives of typical devices in current use. These drivers can run under SmartFirmware (a commercial Open Firmware implementation) in simulation mode.

### 7. Conclusions and Future Work

Typical boot firmware is a combination of many pieces, including libraries, the main boot program, and boot-time device drivers from various vendors. To interact successfully, these pieces must respect well-defined abstraction boundaries and communicate only via standardized interfaces. However, at boot time, all pieces run in the same address space with no isolation and no external enforcement of good citizenship. The existing Open Firmware standard does not address this problem; it only helps non-malicious designers by defining the standard for device interaction and process management during bootup.

Our approach has the potential to ensure that all pieces of boot firmware are good citizens, respecting each other’s boundaries and interacting only via published standardized interfaces. Moreover, this guarantee is refreshed each time the boot program runs with inexpensive static checks, countering the threat of substituting malicious boot firmware components for approved ones.

We believe Open Firmware is the right context because it is a clear, well-designed, and widely used standard. We have designed a Java-to-fcode certifying compiler and built an early prototype. Our current effort is directed toward making this novel form of protection a practical reality by integrating the verifier with a commercial implementation of Open Firmware.

Although we are developing our techniques in the context of the Open Firmware standard, there is nothing preventing non-Open Firmware compliant boot firmware from being made verifiable using similar techniques.

Among the large-scale issues still to be addressed are:
- Designing a Java API for Open Firmware that is both convenient to use and supports the kind of verification we require.
- Enhancing the verifier to check compliance of fcode programs with the second-order security policy (this version of the verifier will run as a standalone program and will be directly integrated with SmartFirmware).
- Modifying the J2F compiler to accommodate the refined Open Firmware API and enhanced verification.

### Acknowledgments

We are indebted to T. J. Merritt for valuable ideas and comments and to David Baca and Kori Oliver for their assistance with the implementation. We also thank the anonymous reviewers for their suggestions. This work was supported in part by DARPA contracts DAAH01-02-C-R080 and DAAH01-01-C-R026, NSF grant CCR-0105586, and ONR Grant N00014-01-1-0968. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of these organizations or the US Government.

### References

[1] M. Abadi and R. Stata. A type system for Java bytecode subroutines. In Proc. 25th Symp. Principles of Programming Languages, pages 149–160. ACM SIGPLAN/SIGACT, January 1998.

[2] William A. Arbaugh, David J. Farber, and Jonathan M. Smith. A secure and reliable bootstrap architecture. In Proc. 1997 Symposium on Security and Privacy, pages 65–71. IEEE, May 1997.

[3] Drew Dean, Ed Felten, and Dan Wallach. JAVA security: From HotJava to Netscape and beyond. In Proc. Symp. Security and Privacy. IEEE, May 1996.

[4] N. Glew and G. Morrisett. Type-safe linking and modular assembly language. In Proc. 26th Symp. Principles of Programming Languages, pages 250–261. ACM SIGPLAN/SIGACT, January 1999.

[5] IEEE. IEEE Standard for Boot (Initialization Configuration) Firmware: Core Requirements and Practices, 1994. Standard 1275-1994.

[6] Dexter Kozen. Efficient code certification. Technical Report 98-1661, Computer Science Department, Cornell University, January 1998.

[7] Dexter Kozen and Matt Stillerman. Eager class initialization for Java. In W. Damm and E.R. Olderog, editors, Proc. 7th Int. Symp. Formal Techniques in Real-Time and Fault Tolerant Systems (FTRTFT’02), volume 2469 of Lecture Notes in Computer Science, pages 71–80. IFIP, Springer-Verlag, September 2002.

[8] Tim Lindholm and Frank Yellin. The JAVA virtual machine specification. Addison Wesley, 1996.

[9] G. Morrisett, K. Crary, N. Glew, D. Grossman, R. Samuels, F. Smith, D. Walker, S. Weirich, and S. Zdancewic. TALx86: A realistic typed assembly language. In Proc. Workshop on Compiler Support for System Software, pages 25–35. ACM SIGPLAN, May 1999.

[10] G. Morrisett, K. Crary, N. Glew, and D. Walker. Stack-based typed assembly language. In Xavier Leroy and Atsushi Ohori, editors, Proc. Workshop on Types in Compilation, volume 1473 of Lecture Notes in Computer Science, pages 28–52. Springer-Verlag, March 1998.

[11] G. Morrisett, D. Tarditi, P. Cheng, C. Stone, R. Harper, and P. Lee. The TIL/ML compiler: Performance and safety through types. In 1996 Workshop on Compiler Support for Systems Software, 1996.

[12] Greg Morrisett, David Walker, Karl Crary, and Neal Glew. From System F to typed assembly language. In 25th ACM SIGPLAN/SIGSIGACT Symposium on Principles of Programming Languages, pages 85–97, San Diego California, USA, January 1998.

[13] George C. Necula. Proof-carrying code. In Proc. 24th Symp. Principles of Programming Languages, pages 106–119. ACM SIGPLAN/SIGACT, January 1997.

[14] George C. Necula. Compiling with proofs. PhD thesis, Carnegie Mellon University, September 1998.

[15] George C. Necula and Peter Lee. Safe kernel extensions without run-time checking. In Proc. 2nd Symp. Operating System Design and Implementation. ACM, October 1996.

[16] George C. Necula and Peter Lee. The design and implementation of a certifying compiler. In Proc. Conf. Programming Language Design and Implementation, pages 333–344. ACM SIGPLAN, 1998.

[17] George C. Necula and Peter Lee. Efficient representation and validation of proofs. In Proc. 13th Symp. Logic in Computer Science, pages 93–104. IEEE, June 1998.

[18] George C. Necula and Peter Lee. Safe, untrusted agents using proof-carrying code. In Giovanni Vigna, editor, Special Issue on Mobile Agent Security, volume 1419 of Lect. Notes in Computer Science, pages 61–91. Springer-Verlag, June 1998.

[19] Robert O’Callahan. A simple, comprehensive type system for Java bytecode subroutines. In Proc. 26th Symp. Principles of Programming Languages, pages 70–78. ACM SIGPLAN/SIGACT, January 1999.

[20] Fred B. Schneider. Towards fault-tolerant and secure entry. In Proc. 11th Int. Workshop WDAG ’97, volume 1320 of Lecture Notes in Computer Science, pages 1–14. ACM SIGPLAN, Springer-Verlag, September 1997.

[21] Fred B. Schneider. Enforceable security policies. Technical Report TR98-1664, Computer Science Department, Cornell University, January 1998.

[22] Fred B. Schneider, editor. Trust in Cyberspace. Committee on Information Systems Trustworthiness, Computer Science and Telecommunications Board, National Research Council. National Academy Press, 1999.

[23] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and P. Lee. TIL: A type-directed optimizing compiler for ML. In Conf. Programming Language Design and Implementation. ACM SIGPLAN, 1996.

[24] R. Wahbe, S. Lucco, T. E. Anderson, and S. L Graham. Efficient software-based fault isolation. In Proc. 14th Symp. Operating System Principles, pages 203–216. ACM, December 1993.