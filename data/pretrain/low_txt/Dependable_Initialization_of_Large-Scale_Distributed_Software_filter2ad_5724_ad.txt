### 3.2. Experimental Results

Using the example application described in the previous subsection, we conducted an experimental study to evaluate our dependable distributed initialization algorithm. In this study, a coordinator was implemented to construct interdependency and recovery graphs, as well as to coordinate initialization tasks and recovery activities among components. The total number of components ranged from 100 to 900, and the number of processes ranged from 10 to 50. Each task associated with an application component had a randomly generated execution time, uniformly distributed between one and ten seconds. A tuning variable in our experimentation allowed us to control the degree of interdependency between components. All dependencies between components were specified as operational dependencies. One-third of these dependencies were assigned between `LocalInitialize` tasks of different components, another third between `SetupConfiguration` tasks of different components, and the remaining third involved both types of dependencies between pairs of components.

In the study, our dependable initialization algorithm was compared with a conventional initialization algorithm that takes immediate recovery actions once a failure is detected and does not have the ability to skip tasks that do not need to be re-executed. Instead of comparing absolute initialization times for both algorithms, we compared the initialization overhead. The results are presented in Figure 8. Initialization overhead represents the fractional increase in system initialization time needed to handle failures during initialization, compared to the initialization time that would be achieved if no failures occurred. It is calculated as: \((T_{\text{failure}} - T_{\text{failure-free}}) / T_{\text{failure-free}}\), where \(T\) is the measured system initialization time.

For simplicity in discussing the results of Figure 8, we triggered a single component failure at the same point in time in the initialization procedure for both our algorithm and the conventional initialization algorithm. We assigned 5% of the total number of system components to the component that failed and randomly assigned the rest of the components to the remaining processes. In Figure 8(a), there are 300 components in total, with 15 of these components running in the process associated with the failed component. The percentage of components with dependencies on tasks associated with the failed component ranges between 10% and 90%, with roughly 70% of the cases resulting in deferred recovery.

The results of Figure 8(a) show that initialization overhead increases for both algorithms as the number of failure-free components affected by the component failure increases. This is due to the additional synchronization and coordination required among the tasks associated with the failed component, as well as the time spent re-executing necessary tasks in response to recovery from the component failure.

As expected, our dependable initialization algorithm has lower initialization overhead than the conventional initialization algorithm. For example, in the case where 40% of failure-free system components have dependencies associated with the failed component, the overhead for the dependable initialization algorithm is 12%, while the overhead for the conventional initialization algorithm is 32%. In this study, the overhead of the dependable initialization algorithm is about one-third of that of the conventional initialization algorithm. Additionally, the initialization overhead of our algorithm increases much more slowly than that of the conventional initialization algorithm. For applications with a high degree of component interdependencies, the ability to avoid re-executing tasks unless required helps to flatten the overhead. The conventional algorithm, without this ability, performs poorly when the components are highly interdependent.

Figure 8(b) shows initialization overhead versus the number of components in the system. In the figure, the total number of components ranges from 100 to 900, a single component fails at the same time during initialization for both initialization schemes, and 30% of the failure-free components have dependencies on tasks associated with the failed component. As expected, as the number of components increases, so do the initialization overheads for both algorithms. Two factors contribute to this increase: the increasing number of initialization tasks that depend on tasks re-executed as part of recovery, and the additional overhead needed to synchronize and coordinate among re-executed tasks and remaining initialization tasks. For example, for a system comprised of 500 components, our dependable initialization algorithm incurs 13% overhead, compared to a 29% overhead for the conventional algorithm.

### 4. Assumptions Revisited

Many simplifying assumptions were made in this paper to present the novel aspects of the work as clearly as possible. In practice, several of these assumptions are not very realistic, especially when considering large-scale distributed systems. Our work relies on more practical assumptions than those discussed herein. Three areas of the work are impacted: the fault model, dependency types, and other events affecting initialization, and the coordinator.

#### 4.1. Fault Model

The "fail silent" and "hang" model for processor, process, and component failures seems to hold quite well in practice for applications such as call processing systems, as bad inputs received by a component are silently discarded. Failures that lead to network partitioning must be considered for geographically distributed applications, though this is less of a problem for cluster-based solutions, which tend to have redundant network connectivity. Our work to date focuses on cluster-based systems.

#### 4.2. Dependency Types and Other Events Affecting Initialization

The sequential and operational dependency types presented in this paper are only two of a vast array of types of dependencies important to initialization. A key type of dependency not discussed in the paper is one that specifies a relationship between active and standby components. We refer to such a dependency as a fail-over dependency, defined as follows:

A fail-over dependency of task \(T_j\) on task \(T_i\) specifies that \(T_j\) can only execute after \(T_i\) has failed (effectively, the component associated with \(T_j\) is promoted). This type of dependency is represented in the interdependency graph by an arc from task \(T_i\) to task \(T_j\) labeled with an 'F'. Task \(T_j\) will not be executed until the component associated with \(T_i\) fails. The recovery graph created as a result of the failure of the component associated with \(T_i\) will include task \(T_j\) and all of its associated dependencies. It will also include a new standby component with the same set of dependencies as existed between the original active and standby components.

Other dependency types that we support but were not discussed in the paper include non-essential dependencies (where a component is not required for initialization to complete—the challenge is to determine what to do when the non-essential component becomes available partway through the initialization procedure), dependencies on resources not managed by the initialization coordinator, dependencies related to fault escalation (e.g., recovery attempts during initialization failed, so more drastic recovery attempts are needed), etc. Also, events other than failures, such as a processor becoming available during initialization, can impact initialization.

#### 4.3. Initialization Coordinator

We have not discussed the implementation of the initialization coordinator, except to assume it is a centralized entity. In fact, no mention was made in the paper regarding what happens if the coordinator fails during initialization. Our work specifically deals with failures of the coordinator. A failed coordinator is restarted, then efficiently rediscovers the initialization states of all entities in the system to complete the initialization procedure. Ongoing work is examining distributed coordination.

### 5. Related Work

There is a spectrum of research work on rollback recovery for distributed applications. This work falls into two broad categories: checkpointing protocols and log-based recovery protocols [3]. Checkpointing protocols require periodic checkpoints to be taken, with varying degrees of coordination [1, 2]. Log-based rollback recovery protocols combine checkpointing with logging of non-deterministic events [8, 11]. Work on checkpointing includes Chandy-Lamport’s distributed snapshot protocol [2], Wang-Fuchs’s smart message scheduling techniques [12], and Elnozahy-Zwaenpoel’s Manetho [4], etc.

Failure recovery during initialization can be accomplished using rollback recovery—the failure of one component may cause other dependent components to re-execute initialization tasks that have already completed (hence, the execution of initialization tasks appears to roll back). However, special characteristics of initialization can be leveraged to improve initialization performance in the presence of failures over what can be achieved using rollback recovery. Specifically, portions of initialization state information are derived from hard-coded or persistent information. This state information can easily be recreated and thus does not require checkpointing. In addition, in some initialization tasks such as setting up communication channels, re-executing the tasks results in different local state information being generated. Hence, there is not much value in preserving this state information (e.g., by checkpointing). Also, we have observed that some initialization tasks associated with a component need to be executed only once during the lifetime of that component, regardless of failures of other system entities. As such, these tasks can be skipped during recovery procedures.

There are research efforts on task scheduling and recovery that rely on the use of dependency graphs [6, 7, 9]. For example, Isovic and Fohler derived the minimum processor utilization by re-executing a task on the same processor on which it failed [7]. Kandsamy, Hayes, and Murray handle intermittent faults by constructing a fault-tolerant schedule with sufficient slack to accommodate recovery [9]. However, there is no notion of re-executing tasks that have successfully completed and limited notion of re-executing tasks on a specific processor.

### 6. Conclusion

Handling failures during the initialization of large-scale distributed systems adds complexity to both the initialization and failure recovery processes. In this paper, we presented a dependable initialization model that captures the architecture of the system to be initialized along with task interdependencies among components. Our model enables appropriate initialization tasks to be skipped during failure recovery, which can greatly reduce recovery overhead and result in faster initialization. We showed that initialization would complete more quickly in some cases if recovery actions were deferred rather than started immediately after a failure is detected. A recovery decision function was introduced that dynamically assesses, based on current initialization conditions, whether or not recovery actions are taken immediately or deferred. We then described a dependable initialization algorithm that combines our dependable initialization model with the recovery decision function. Experimental results show that our algorithm initializes a system in much less time than that of a conventional initialization algorithm when failures occur during initialization. This work is the first effort we are aware of that studies the challenges of initializing a distributed system in the presence of failures.

### 7. Acknowledgements

We wish to thank the reviewers for their helpful comments, with special thanks to Keith Marzullo, Mike Artamanov, and Tim Pevzner for their questions and insightful comments. The valuable input received has helped greatly improve the paper.

### 8. References

[1] L. Alvisi, E. N. Elnozahy, S. Rao, S. A. Husain, and A. Del Mel, “An Analysis of Communication-induced Checkpointing,” Proc. of 29th Intl. Symposium on Fault-Tolerant Computing, pp. 242-249, Jun. 1999.

[2] K. M. Chandy and L. Lamport, “Distributed Snapshots: Determining Global States of Distributed Systems,” ACM Transactions on Computer Systems, vol. 3, no. 1, pp. 63-75, 1985.

[3] E. N. Elnozahy, L. Alvisi, Y. M. Wang, and D. B. Johnson, “A Survey of Rollback-Recovery Protocols in Message-Passing Systems,” Technical Report CMU-CS-99-148, Department of Computer Science, Carnegie Mellon University, June 1999.

[4] E. N. Elnozahy and W. Zwaenepoel, “On the Use and Implementation of Message Logging,” Proc. of the 24th Intl. Symposium on Fault-Tolerant Computing, pp. 298-307, 1994.

[5] I. Foster and C. Kesselman, “The Globus Project: A Status Report,” Proc. of the Heterogeneous Computing Workshop, pp. 4-18, 1998.

[6] S. Ghosh, R. Melhem, D. Mosse, and J. S. Sarma, “Fault-Tolerant Rate-Monotonic Scheduling,” Journal of Real-Time Systems, vol. 5, no. 2, pp. 120-129, 1998.

[7] D. Isovic and G. Fohler, “Efficient Scheduling of Sporadic, Aperiodic, and Periodic Tasks with Complex Constraints,” Proc. of IEEE Real-Time Systems Symposiums, pp. 207-216, 2000.

[8] D. B. Johnson, “Distributed System Fault Tolerance Using Message Logging and Checkpointing,” Ph.D. Thesis, Rice University, Dec. 1989.

[9] N. Kandasamy, J. Hayes, and B. Murray, “Transparent Recovery from Intermittent Faults in Time-Triggered Distributed Systems,” IEEE Transactions on Computers, vol. 52, no. 2, pp. 113-125, 2003.

[10] M. Litzkow, M. Livny, and M. Mutka, “Condor – A Hunter of Idle Workstations,” Proc. of 8th Intl. Conf. On Distributed Computing Systems, pp. 104-111, 1988.

[11] S. Rao, L. Alvisi, and H. M. Vin, “The Cost of Recovery in Message Logging Protocols,” Proc. of 17th IEEE Symposium on Reliable Distributed Systems (SRDS), pp. 10-18, 1998.

[12] Y. M. Wang and W. K. Fuchs, “Scheduling Message Processing for Reducing Rollback Propagation,” Proc. of IEEE Fault-Tolerance Computing Symposium, pp. 204-211, 1992.

---

This optimized version of the text is more structured, clear, and professional, making it easier to read and understand.