### Evaluation

#### 7.1 Simulation Methodology

To evaluate our design, we simulated a 16-node (P=16, N=32) target system using the Simics full-system, multiprocessor, functional simulator [12]. We extended Simics with a memory hierarchy simulator to compute execution times. Each node in our modern Symmetric Multi-Processor (SMP) system consists of a processor, two levels of cache, a cache controller, a portion of shared memory, a memory controller, and a network interface.

**Simics:**
Simics is a system-level architectural simulator developed by Virtutech AB. We used Simics/sun4u, which simulates Sun Microsystems' SPARC V9 platform architecture (e.g., used for Sun E6000s) in sufficient detail to boot unmodified Solaris 8. Simics is a functional simulator that assumes each instruction takes one cycle to execute (although I/O may take longer). It provides an interface to support detailed memory hierarchy simulation.

**Processor Model:**
We modeled a processor core that, given a perfect memory system, would execute four billion instructions per second and generate blocking requests to the cache hierarchy and beyond. This simple processor model enables tractable simulation times for full-system simulation of commercial workloads. While an out-of-order processor model might affect the absolute values of the results, it would not qualitatively change them (e.g., whether an error is detected).

**Memory Model:**
We implemented a memory hierarchy simulator that supports the MOSI broadcast snooping cache coherence protocol. The simulator captures all state transitions, including transient states, of our coherence protocol in the cache and memory controllers. We modeled the interconnection network and the contention within it, including the small additional contention due to SafetyNet messages. Table 2 presents the design parameters of our target memory system. With a checkpoint interval, \( T_c \), of 300 broadcast coherence requests and four outstanding checkpoints, SafetyNet can tolerate fault detection latencies greater than the latency for global communication.

**SafetyNet:**
Our memory system simulator models the details of the SafetyNet support for checkpoint/recovery. For evaluating the overhead for checkpointing register state, we modeled a conservative latency of 100 cycles. We conservatively charged eight cycles for logging store overwrites (8 bytes/cycle for 64-byte cache blocks), but these are only about 0.1% of instructions. We integrated the global reduction of local signatures with the existing mechanism for validating checkpoints in SafetyNet. For each checkpoint that a cache or memory controller agrees to validate, it computes both signatures based on the \( T_c \) coherence requests (i.e., address messages) processed in that checkpoint interval and sends these signatures to the system service processor (i.e., a central controller often found in servers, such as the Sun E10000 [6]). The service processor performs the checks CCL and CML. If the checks detect no errors (i.e., CCL=CML=false), it completes the validation by notifying every node. Otherwise, it triggers a system recovery.

#### 7.2 Workloads

Commercial applications are crucial for high availability systems. Therefore, we evaluated our system with four commercial applications and one scientific application, briefly described in Table 3 and in more detail by Alameldeen et al. [2]. To address the variability in runtimes for commercial workloads, we used a methodology described by Alameldeen et al. [2] to simulate each design point multiple times with small, pseudo-random perturbations of memory latencies. This approach causes alternative scheduling paths and provides statistically meaningful results. Error bars in our performance results represent one standard deviation in each direction from the mean.

#### 7.3 Results

In this section, we present the results of our experiments using the two end-to-end invariant checkers in parallel. We injected errors into the system at a high rate and observed their impact. The only quantitative results concern the performance impact of invariant checking. As expected from the results shown in Sorin et al. [16], the performance degradation caused by SafetyNet overhead is low. Remarkably, as shown in Figure 5, the performance impact even in the presence of ten errors (i.e., ten system recoveries) per second is low for all benchmarks.

**Coherence-Level Checking:**
We injected errors into the system, including dropped messages and incorrectly processed messages, and the signature analysis detected all of these errors. This scheme can detect errors that are difficult or impossible to detect with localized error detection schemes. For example, if a Shared node processes an incoming RequestForExclusive from another node but does not invalidate its copy of the block, the system can violate both cache coherence and memory consistency.

**Message-Level Checking:**
To test the checker's ability to detect errors in this model, we periodically dropped and reordered messages. The signature analysis scheme successfully detected the errors and triggered SafetyNet recoveries. This scheme can also detect some errors that are difficult to detect with localized error detection schemes, such as when the interconnect erroneously reorders two broadcast requests delivered to a given node in a broadcast snooping system.

### 8. Related Work

Previous research has explored dynamic verification at different levels. At the intra-processor level, DIVA [3] adds a simple, provably correct checker core that dynamically verifies the aggressively designed core. DIVA can detect device errors in either core and design errors in the aggressive core, but it limits itself to the processor. The Thinking Machines CM-5 [11] dynamically computed a variant of Kirchoff’s Current Law (i.e., inflow of messages equals outflow of messages) to determine when all messages had been delivered. Cantin et al. propose a scheme for dynamically verifying snooping cache coherence protocols [5]. This scheme uses a validation bus, and a node that changes coherence state broadcasts its new state on this bus so that other nodes can check that their states are compatible. However, this scheme requires manual construction of the checker protocol and significant extra bandwidth for validation, and it does not provide a way to integrate it with a recovery mechanism. Cain and Lipasti propose an algorithm based on vector clocks for dynamically verifying sequential consistency, but they leave for future work the issues of implementation and integration of the algorithm with a checkpoint/recovery mechanism [4].

### 9. Conclusions

In this paper, we have argued for dynamic verification of end-to-end, system-wide invariants in shared memory multiprocessors. We have developed two signature analysis schemes for detecting violations of system-wide invariants and have used full-system simulation to demonstrate that they detect the targeted errors while not degrading system performance. The viability of dynamic verification of end-to-end invariants, in conjunction with backward error recovery, enables improved system availability.

While this work applies the end-to-end argument to encompass SMP coherence protocols and interconnects, future work can seek end-to-end approaches that encompass complete SMP hardware (e.g., by integrating processor dynamic verification [3]) or even software. Future work will also address the issue of fault diagnosis.

### Acknowledgments

We thank Alvy Lebeck, Milo Martin, the Duke Systems Group, and the Wisconsin Multifacet Group for helpful discussions of this research. We thank Trey Cain and Jason Cantin for feedback on the final draft.

This work is supported in part by the National Science Foundation, with grants EIA-9971256, CCR-0105721, and EIA-0205286, an Intel Fellowship (Sorin), a Warren Faculty Scholarship (Sorin), two Wisconsin Romnes Fellowships (Hill and Wood), Universitat Politècnica de Catalunya and Secretaría de Estado de Educación y Universidades de España (Hill sabbatical), and donations from IBM, Intel, Microsoft, and Sun.

### References

[1] Miron Abramovici, Melvin A. Breuer, and Arthur D. Friedman. Digital Systems Testing and Testable Design. IEEE Press, 1990.
[2] Alaa R. Alameldeen, Milo M.K. Martin, Carl J. Mauer, Kevin E. Moore, Min Xu, Daniel J. Sorin, Mark D. Hill, and David A. Wood. Simulating a $2M Commercial Server on a $2K PC. IEEE Computer, 36(2), February 2003.
[3] Todd M. Austin. DIVA: A Reliable Substrate for Deep Submicron Microarchitecture Design. In Proceedings of the 32nd Annual IEEE/ACM International Symposium on Microarchitecture, pages 196–207, November 1999.
[4] Harold W. Cain and Mikko H. Lipasti. Verifying Sequential Consistency Using Vector Clocks. In Revue in conjunction with Symposium on Parallel Algorithms and Architectures, August 2002.
[5] J. F. Cantin, M. H. Lipasti, and J. E. Smith. Dynamic Verification of Cache Coherence Protocols. In Workshop on Memory Performance Issues, June 2001. In conjunction with ISCA.
[6] Alan Charlesworth. Starfire: Extending the SMP Envelope. IEEE Micro, 18(1):39–49, Jan/Feb 1998.
[7] David E. Culler and J.P. Singh. Parallel Computer Architecture: A Hardware/Software Approach. Morgan Kaufmann Publishers, Inc., 1999.
[8] E.N. Elnozahy, D.B. Johnson, and Y.M. Wang. A Survey of Rollback-Recovery Protocols in Message-Passing Systems. Technical Report CMU-CS-96-181, Department of Computer Science, Carnegie Mellon University, September 1996.
[9] E.N. Elnozahy and W. Zwaenepoel. Manetho: Transparent Rollback-Recovery with Low Overhead, Limited Rollback, and Fast Output Commit. IEEE Transactions on Computers, 41(5):526–531, May 1992.
[10] S. W. Golumb. Shift Register Sequences. Aegean Park Press, revised edition, 1982.
[11] Charles E. Leiserson et al. The Network Architecture of the Connection Machine CM-5. In Proceedings of the Fourth ACM Symposium on Parallel Algorithms and Architectures, pages 272–285, June 1992.
[12] Peter S. Magnusson et al. Simics: A Full System Simulation Platform. IEEE Computer, 35(2):50–58, February 2002.
[13] J. H. Saltzer, D. P. Reed, and D. D. Clark. End-to-End Arguments in Systems Design. ACM Transactions on Computer Systems, 2(4):277–288, November 1984.
[14] Ashok Singhal et al. Gigaplane: A High Performance Bus for Interconnects in Large SMPs. In Proceedings of 4th Hot Symposium, pages 41–52, August 1996.
[15] Daniel J. Sorin. Using Lightweight Checkpoint/Recovery to Improve the Availability and Designability of Shared Memory Multiprocessors. PhD thesis, University of Wisconsin, August 2002.
[16] Daniel J. Sorin, Milo M.K. Martin, Mark D. Hill, and David A. Wood. SafetyNet: Improving the Availability of Shared Memory Multiprocessors with Global Checkpoint/Recovery. In Proceedings of the 29th Annual International Symposium on Computer Architecture, pages 123–134, May 2002.
[17] Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. The SPLASH-2 Programs: Characterization and Methodological Considerations. In Proceedings of the 22nd Annual International Symposium on Computer Architecture, pages 24–37, June 1995.