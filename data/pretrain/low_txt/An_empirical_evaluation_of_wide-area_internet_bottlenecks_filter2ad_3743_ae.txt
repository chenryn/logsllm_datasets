### Latency and Bottlenecks in Peering Links

When a high-latency peering link is encountered in a network path, it is very likely to become a bottleneck. In contrast, high-latency intra-ISP links are less likely to be bottlenecks, accounting for only 11% of all bottlenecks and 13.5% of overall hops on paths to tier-2 ISPs.

Figure 6 indicates that peering links, regardless of their latency (low, medium, or high), have a higher likelihood of being bottlenecks. For example, even though very few paths include medium-latency peering links, these links account for a significant proportion of bottlenecks across all types of paths. Additionally, low-latency peering links on paths to lower-tier ISPs (tier-3 and tier-4) have a particularly high likelihood of being bottlenecks compared to paths to tier-1 and tier-2 destinations. As shown in Figures 5(b) and (c), these lower-tier peering bottlenecks also have much less available bandwidth.

### Bottlenecks at Public Exchange Points

One of our goals, as mentioned in Section 2, was to investigate the common perception that public exchanges are often network choke points and should be avoided whenever possible. Using the procedure outlined in Section 2.2.2, we identified a large number of paths passing through public exchanges and applied BFind to identify any bottlenecks along these paths.

As indicated in Figure 7(a), we tested 466 paths through public exchange points. Of the measured paths, 170 (36.5%) had a bottleneck link. Among these, only 70 bottlenecks (15% overall) were located at the exchange point. This finding contrasts with the expectation that many exchange point bottlenecks would be identified on such paths. However, the probability that a bottleneck link is located at the exchange point is about 41% (70/170).

In comparison, Figures 3(a) and 4(a) do not show any other type of link (intra-ISP or peering) responsible for a larger percentage of bottlenecks. This observation suggests that if there is a bottleneck on a path through a public exchange point, it is likely to be at the exchange itself nearly half of the time.

### Discussion

Our study, while confirming some conventional wisdom about the location of Internet bottlenecks, yields several interesting and unexpected findings. For example, we found a substantial number of bottleneck links within carrier ISPs. Additionally, we observed that low-latency links, whether within ISPs or between them, can also constrain available bandwidth with a small but significant probability.

These observations can provide guidance when considering related issues such as choosing an access provider, optimizing routes through the network, or analyzing the performance implications of bottlenecks in practice. In this section, we discuss some of these issues in the context of our empirical findings.

#### Providers and Provisioning

Our measurements show a clear performance advantage to using a tier-1 provider. Small regional providers, exemplified by the tier-4 ASes in our study, have relatively low-speed connectivity to their upstream carriers, irrespective of the upstream carrier’s size. Their networks often exhibit bottlenecks, which may reflect the impact of economics on network provisioning. Carriers lower in the AS hierarchy may be less inclined to overprovision their networks if their typical customer traffic volume does not require it. Therefore, there is a clear disadvantage to using a tier-4 provider for high-speed connectivity. However, the trade-offs between tier-2 and tier-3 networks are less clear.

Paths to tier-3 destinations had a larger percentage of bottleneck links than tier-2 paths. Despite this, tier-2 and tier-3 bottlenecks show similar characteristics in terms of available capacity, with tier-3 bottlenecks (both intra-AS and peering links) performing slightly better in some cases. This might be explained by the higher degree of reachability of tier-2 ASes, which carry a larger volume of traffic relative to their capacity compared to tier-3 ASes. If a stub network desires reasonably wide connectivity, choosing a tier-3 provider might be beneficial both economically and in terms of performance, assuming that connectivity to tier-3 providers is less expensive.

#### Network Under-utilization

More than 50% of the paths we probed seemed to have an available capacity close to 40-50 Mbps or more. This holds true across most non-access links, irrespective of their type. We hypothesize that large portions of the network are potentially under-utilized on average, confirming what many large ISPs report about the utilization of their backbone networks. The fact that this holds even for smaller providers (e.g., tier-3) and most peering links, including those at NAPs, is surprising.

This observation about under-utilization, combined with our results about the existence of potential hot-spots with low available bandwidth, raises the question: Is it possible to avoid these bottlenecks by leveraging existing routing protocols? While there has been considerable work on load-sensitive routing within an AS, little is known about how to extend this across ASes. We plan to explore this in the future.

#### Route Optimization

It is sometimes suggested that a large proportion of the peering links between large carrier ISPs (tier-1) could emerge as bottlenecks due to the lack of economic incentive to provision these links and the large volume of traffic carried over them. However, our measurements suggest otherwise. This could imply that either the peering links are well-provisioned, or a smaller portion of the entire Internet traffic traverses these links than expected.

While it is difficult to discern the exact cause for this lack of bottlenecks, it may have important implications for the design of systems or choice of routes. For example, purchasing bandwidth from two different tier-1 ISPs may be significantly better from a performance perspective than buying twice as much bandwidth from a single tier-1 ISP. It might also be more economical to purchase from one ISP. Similarly, a shorter route to a destination that passes through a tier-1 to tier-1 peering link might be better than a longer route that stays within a single, lower-tier provider.

### Related Work

Several earlier research efforts have shared our high-level goal of measuring and characterizing wide-area network performance. This past work can be roughly divided into two areas: 1) measurement studies of the Internet, and 2) novel algorithms and tools for measuring Internet properties.

#### Measurement Studies

Typically, measurement studies to characterize performance in the Internet have taken two forms: 1) active probing to evaluate end-to-end properties of Internet paths, and 2) passive monitoring or packet traces of Internet flows to observe their performance.

In [23], multiple TCP bulk transfers between pairs of measurement end-points are monitored to show evidence of significant packet reordering, correlated packet losses, and frequent delay variations on small scales. The authors also describe the distribution of bottleneck capacities observed in the transfers. The study by Savage et al. used latency and loss measurements between network endpoints to compare the quality of direct and indirect paths between nodes [32]. The authors note that performance gains come from avoiding congestion and using shorter latency paths. Using active measurements in the NIMI [25] infrastructure, Zhang et al. study the constancy of Internet paths in terms of delay, loss, and throughput [36]. They observed that all three properties were steady on at least a minute’s timescale. A recent study of delay and jitter across several large backbone providers aimed to classify paths according to their suitability for latency-sensitive applications [19]. The authors found that most paths exhibited very little delay variation, but very few consistently experienced no loss.

Compared to these efforts, our work has a few key differences. First, rather than exploring true end-to-end paths, our measurement paths are intended to probe the non-access part of the Internet, i.e., the part responsible for carrying data between end networks. Second, we measure which part of the network may limit the performance of end-to-end paths.

In [2], the authors study packet-level traces to and from a very large collection of end-hosts and observe a wide degree of performance variation, as characterized by the observed TCP throughput. With a similar set of goals, Zhang et al. analyze packet traces to understand the distribution of Internet flow rates and the causes thereof [35]. They find that network congestion and TCP receiver window limitations often constrain the observed throughput. In this paper, our aim is not to characterize what performance end-hosts typically achieve and what constrains the typical performance. Instead, we focus on well-connected and unconstrained end-points (e.g., no receiver window limitations) and comment on how ISP connectivity constrains the performance seen by such end-points.

#### Measurement Tools

The development of algorithms and tools to estimate the bandwidth characteristics of Internet paths continues to be an active research area. Tools like bprobe [5], Nettimer [17], and PBM [23] use packet-pair mechanisms to measure the raw bottleneck capacity along a path. Other tools like clink [7], pathchar [12], pchar [18], and pipechar [10] characterize hop-by-hop delay, raw capacity, and loss properties of Internet paths by observing the transmission behavior of different-sized packets. A different set of tools, well-represented by pathload [14], focus on the available capacity on a path. These tools, unlike BFind, require control over both the end-points of the measurement. Finally, the TReno tool [20] follows an approach most similar to ours, using UDP packets to measure available bulk transfer capacity. It sends hop-limited UDP packets toward the destination, emulating TCP congestion control by using sequence numbers contained in the ICMP error responses. TReno probes each hop along a path in turn for available capacity. Therefore, when used to identify bottlenecks along a path, TReno will likely consume ICMP processing resources for every probe packet at each router being probed, making it more intrusive than our tool for high-speed links.

In addition to available bandwidth, link loss and delay are often performance metrics of interest. Recent work by Bu et al. describes algorithms that infer and estimate loss rates and delay distributions on links in a network using multicast trees [4].

In this paper, we develop a mechanism that measures the available capacity on the path between a controlled end-host and an arbitrary host in the Internet. In addition, we identify the portion of the network responsible for the bottleneck. Our tool uses a heavyweight approach in the amount of bandwidth it consumes.

### Summary

The goal of this paper was to explore the following fundamental issue: if end networks upgrade their access speeds, which portions of the rest of the Internet are likely to become hot-spots? To answer this question, we performed a large set of diverse measurements of typical paths traversed in the Internet. We identified non-access bottlenecks along these paths and studied their key characteristics such as location and prevalence (links within ISPs vs. between ISPs), latency (long-haul vs. local), and available capacity. Table 4 summarizes some of our key observations.

The results from our measurements mostly support conventional wisdom by quantifying the key characteristics of non-access bottlenecks. However, some of our key conclusions show trends in the prevalence of non-access bottlenecks that are unexpected. For example, our measurements show that the bottleneck on any path is roughly equally likely to be either a peering link or a link inside an ISP. We also quantify the likelihood that paths through public exchange points have bottlenecks appearing in the exchange.

In addition, our measurements quantify the relative performance benefits offered by ISPs belonging to different tiers in the AS hierarchy. Interestingly, we find that there is no significant difference between ISPs in tiers 2 and 3 in this respect. As expected, we find that tier-1 ISPs offer the best performance and tier-4 ISPs contain the most bottlenecks.

In summary, we believe that our work provides key insights into how the future network should evolve on two fronts. Firstly, our results can be used by ISPs to help them evaluate their providers and peers. Secondly, the observations from our work can also prove helpful to stub networks in picking suitable upstream providers.

### Acknowledgment

We are very grateful to Kang-Won Lee, Jennifer Rexford, Albert Greenberg, Brad Karp, Bruce Maggs, and Prashant Pradhan for their valuable suggestions on this paper. We also thank our anonymous reviewers for their detailed feedback.

### References

[1] D. Andersen, H. Balakrishnan, M. Kaashoek, and R. Morris. Resilient Overlay Networks. In Proceedings of the 18th Symposium on Operating System Principles, Banff, Canada, October 2001.

[2] H. Balakrishnan, S. Seshan, M. Stemm, and R. H. Katz. Analyzing stability in wide-area network performance. In Proceedings of ACM SIGMETRICS, Seattle, WA, June 1997.

[3] L. S. Brakmo, S. W. O’Malley, and L. L. Peterson. TCP Vegas: New Techniques for Congestion Detection and Avoidance. In Proceedings of the SIGCOMM ’94 Symposium on Communications Architectures and Protocols, August 1994.

[4] T. Bu, N. Dufﬁeld, F. L. Presti, and D. Towsley. Network tomography on general topologies. In Proceedings of ACM SIGMETRICS, Marina Del Ray, CA, June 2002.

[5] R. L. Carter and M. E. Crovella. Measuring bottleneck link speed in packet-switched networks. Performance Evaluation, 27–28:297–318, October 1996.

[6] Cooperative Association for Internet Data Analysis (CAIDA). Internet tools taxonomy. http://www.caida.org/tools/taxonomy/, October 2002.

[7] A. Downey. Using pathchar to estimate internet link characteristics. In Proceedings of ACM SIGCOMM, Cambridge, MA, August 1999.

[8] L. Gao. On inferring autonomous system relationships in the Internet. IEEE/ACM Transactions on Networking, 9(6), December 2001.

[9] R. Govindan and V. Paxson. Estimating router ICMP generation delays. In Proceedings of Passive and Active Measurement Workshop (PAM), Fort Collins, CO, 2002.

[10] J. Guojun, G. Yang, B. R. Crowley, and D. A. Agarwal. Network characterization service (NCS). In Proceedings of IEEE International Symposium on High Performance Distributed Computing (HPDC), San Francisco, CA, August 2001.

[11] U. Hengartner, S. Moon, R. Mortier, and C. Diot. Detection and analysis of routing loops in packet traces. In Proceedings of ACM SIGCOMM Internet Measurement Workshop (IMW), November 2002.

[12] V. Jacobson. pathchar – A Tool to Infer Characteristics of Internet Paths. ftp://ee.lbl.gov/pathchar/, 1997.

[13] M. Jain and C. Dovrolis. End-to-end available bandwidth: Measurement methodology, dynamics, and relation with TCP throughput. In Proceedings of ACM SIGCOMM, Pittsburgh, PA, August 2002.

[14] M. Jain and C. Dovrolis. Pathload: A measurement tool for end-to-end available bandwidth. In Proceedings of Passive and Active Measurement Workshop (PAM), Fort Collins, CO, March 2002.

[15] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, and D. Towsley. Measurement and classification of out-of-sequence packets in a tier-1 IP backbone. In Proceedings of ACM SIGCOMM Internet Measurement Workshop (IMW), November 2002.

[16] C. Labovitz, A. Ahuja, and F. Jahanian. Experimental study of Internet stability and backbone failures. In Proceedings of IEEE International Symposium on Fault-Tolerant Computing (FTCS), Madison, WI, June 1999.

[17] K. Lai and M. Baker. Nettimer: A tool for measuring bottleneck link bandwidth. In Proceedings of USENIX Symposium on Internet Technologies and Systems, March 2001.

[18] B. A. Mah. pchar: A tool for measuring internet path characteristics. http://www.employees.org/~bmah/Software/pchar/, June 2000.

[19] A. P. Markopoulou, F. A. Tobagi, and M. J. Karam. Assessment of VoIP quality over Internet backbones. In Proceedings of IEEE INFOCOM’02, New York, NY, June 2002.

[20] M. Mathis and J. Mahdavi. Diagnosing Internet Congestion with a Transport Layer Performance Tool. In Proc. INET '96, Montreal, Canada, June 1996. http://www.isoc.org/inet96/proceedings/.

[21] Network Characterization Service: Netest and Pipechar. http://www-didc.lbl.gov/pipechar, 1999.

[22] ns-2 Network Simulator. http://www.isi.edu/nsnam/ns/, 2000.

[23] V. Paxson. End-to-end internet packet dynamics. Proceedings of the SIGCOMM '97 Symposium on Communications Architectures and Protocols, pages 139–152, September 1997.

[24] V. Paxson. End-to-end routing behavior in the internet. IEEE/ACM Transactions on Networking, 5(5):601–615, October 1997.

[25] V. Paxson, A. Adams, and M. Mathis. Experiences with NIMI. In Proceedings of Passive and Active Measurement Workshop (PAM), Hamilton, New Zealand, April 2000.

[26] PlanetLab. http://www.planet-lab.org, 2002.

[27] RADB whois Server. whois.radb.net.

[28] RIPE whois Service. whois.ripe.net.

[29] BGP Tables from the University of Oregon RouteViews Project. http://moat.nlanr.net/AS/data.

[30] University of Oregon, RouteViews Project. http://www.routeviews.org.

[31] S. Savage, T. Anderson, A. Aggarwal, D. Becker, N. Cardwell, A. Collins, E. Hoffman, J. Snell, A. Vahdat, J. Voelker, and J. Zahorjan. Detour: a case for informed internet routing and transport. IEEE Micro, volume 19 no. 1:50–59, January 1999.

[32] S. Savage, A. Collins, E. Hoffman, J. Snell, and T. Anderson.