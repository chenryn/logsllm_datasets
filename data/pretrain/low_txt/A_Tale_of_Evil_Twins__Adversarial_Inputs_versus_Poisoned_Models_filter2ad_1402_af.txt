### Vectors under Alternative Settings
First, it is essential to explore the effectiveness of vectors in various settings, such as untargeted and black-box attacks. Second, enhancing other types of threats, such as latent backdoor attacks, within the input-model co-optimization framework is a promising direction for future research. Finally, developing a unified robustness metric that accounts for both types of vectors could serve as a valuable starting point for creating effective countermeasures.

### Acknowledgments
We are grateful to our shepherd, Xiangyu Zhang, and the anonymous reviewers for their valuable feedback. This work was supported by the National Science Foundation under Grant Nos. 1910546, 1953813, and 1846151. The opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. S. Ji received partial support from the NSFC under Nos. U1936215, 61772466, and U1836202, the National Key Research and Development Program of China under No. 2018YFB0804102, the Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars under No. LR19F020003, the Zhejiang Provincial Key R&D Program under No. 2019C01055, and the Ant Financial Research Funding. X. Luo was partially supported by the HK RGC Project (PolyU 152239/18E) and the HKPolyU Research Grant (ZVQ8).

### Figure 15: Detection of Basic and Ensemble STRIP against TrojanNN* on CIFAR10 and GTSRB
This figure illustrates the detection performance of basic and ensemble STRIP methods against TrojanNN* on the CIFAR10 and GTSRB datasets.

### Related Work
With the increasing use of Deep Neural Networks (DNNs) in security-sensitive domains, they have become new targets for malicious manipulations [3]. Two primary attack vectors have been considered in the literature: adversarial inputs and poisoned models.

#### Adversarial Inputs
Research on adversarial inputs can be divided into two main areas:
1. **Developing New Attacks**: One line of work focuses on creating new attacks against DNNs [7, 20, 40, 48], with the goal of crafting adversarial samples to force DNNs to misbehave. These attacks can be categorized as either untargeted (where the adversary aims to cause any misclassification) or targeted (where the adversary attempts to force the inputs to be misclassified into specific classes).
2. **Improving Resilience**: Another line of work aims to enhance DNN resilience against adversarial attacks by devising new training strategies, such as adversarial training [22, 28, 39, 49], or detection mechanisms [19, 33, 35, 53]. However, existing defenses are often penetrated or circumvented by even stronger attacks [2, 30], leading to an ongoing arms race between attackers and defenders.

#### Poisoned Models
Poisoned model-based attacks can be categorized based on their target inputs:
- **Poisoning Attacks**: In these attacks, the target inputs are non-modified, and the adversary's goal is to force such inputs to be misclassified by the poisoned DNNs [24, 25, 44, 47, 52].
- **Backdoor Attacks**: In backdoor attacks, specific trigger patterns (e.g., a particular watermark) are pre-defined, and the adversary's goal is to force any inputs embedded with such triggers to be misclassified by the poisoned models [21, 32]. Backdoor attacks leverage both adversarial inputs and poisoned models.

Existing defense methods against poisoned models primarily focus on backdoor attacks and can be categorized into:
- **Cleansing Potential Contaminated Data**: Methods like Neural Cleanse and STRIP aim to identify and mitigate backdoor attacks by detecting and removing contaminated data.

### References
[1] Rima Alaifari, Giovanni S. Alberti, and Tandri Gauksson. 2019. ADef: An Iterative Algorithm to Construct Adversarial Deformations. In Proceedings of International Conference on Learning Representations (ICLR).

[2] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In Proceedings of IEEE Conference on Machine Learning (ICML).

[3] Battista Biggio and Fabio Roli. 2018. Wild Patterns: Ten Years after The Rise of Adversarial Machine Learning. Pattern Recognition 84 (2018), 317–331.

[4] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. 2016. End to End Learning for Self-Driving Cars. ArXiv e-prints (2016).

[5] Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press.

[6] BVLC. 2017. Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.

[7] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[8] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. 2018. Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering. In ArXiv e-prints.

[9] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. 2019. DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks. In Proceedings of International Joint Conference on Artificial Intelligence.

[10] Edward Chou, Florian Tramer, Giancarlo Pellegrino, and Dan Boneh. 2018. SentiNet: Detecting Physical Attacks Against Deep Learning Systems. In ArXiv e-prints.

[11] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified Adversarial Robustness via Randomized Smoothing. In Proceedings of IEEE Conference on Machine Learning (ICML).

[12] G. Cybenko. 1989. Approximation by Superpositions of a Sigmoidal Function. Mathematics of Control, Signals, and Systems (MCSS) 2, 4 (1989), 303–314.

[13] J.M. Danskin. 1967. The Theory of Max-Min and Its Application to Weapons Allocation Problems. Springer-Verlag.

[14] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A Large-scale Hierarchical Image Database. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[15] Bao Doan, Ehsan Abbasnejad, and Damith Ranasinghe. 2020. Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems. In ArXiv e-prints.

[16] Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun. 2017. Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks. Nature 542, 7639 (2017), 115–118.

[17] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. 2017. Classification Regions of Deep Neural Networks. ArXiv e-prints (2017).

[18] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Ranasinghe, and Surya Nepal. 2019. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. In ArXiv e-prints.

[19] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. 2018. AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[20] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In Proceedings of International Conference on Learning Representations (ICLR).

[21] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. ArXiv e-prints (2017).

[22] Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten. 2018. Countering Adversarial Images Using Input Transformations. In Proceedings of International Conference on Learning Representations (ICLR).

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[24] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model-Reuse Attacks on Deep Learning Systems. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[25] Yujie Ji, Xinyang Zhang, and Ting Wang. 2017. Backdoor Attacks against Learning Systems. In Proceedings of IEEE Conference on Communications and Network Security (CNS).

[26] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of International Conference on Learning Representations (ICLR).

[27] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto (2009).

[28] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Machine Learning at Scale. In Proceedings of International Conference on Learning Representations (ICLR).

[29] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018. Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation. ArXiv e-prints (2018).

[30] X. Ling, S. Ji, J. Zou, J. Wang, C. Wu, B. Li, and T. Wang. 2019. DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[31] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. 2019. ABS: Scanning Neural Networks for Back-Doors by Artificial Brain Stimulation. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[32] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In Proceedings of Network and Distributed System Security Symposium (NDSS).

[33] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang. 2019. NIC: Detecting Adversarial Samples with Neural Network Invariant Checking. In Proceedings of Network and Distributed System Security Symposium (NDSS).

[34] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In Proceedings of International Conference on Learning Representations (ICLR).

[35] Dongyu Meng and Hao Chen. 2017. MagNet: A Two-Pronged Defense Against Adversarial Examples. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[36] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. 2017. Universal Adversarial Perturbations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[37] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, and Stefano Soatto. 2017. Analysis of Universal Adversarial Perturbations. ArXiv e-prints (2017).

[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. 2018. Robustness via Curvature Regularization, and Vice Versa. ArXiv e-prints (2018).

[39] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[40] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016. The Limitations of Deep Learning in Adversarial Settings. In Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P).

[41] A.D. Polyanin and A.V. Manzhirov. 2006. Handbook of Mathematics for Engineers and Scientists. Taylor & Francis.

[42] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).

[43] Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. 2019. Adversarial Training for Free!. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS).

[44] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. 2018. Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS).

[45] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature 7587 (2016), 484–489.

[46] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition. Neural Networks (2012), 323–32.

[47] Octavian Suciu, Radu Mărginean, Yiğitcan Kaya, Hal Daumé, III, and Tudor Dumitraş. 2018. When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks. In Proceedings of USENIX Security Symposium (SEC).

[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks. In Proceedings of International Conference on Learning Representations (ICLR).

[49] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. 2018. Ensemble Adversarial Training: Attacks and Defenses. In Proceedings of International Conference on Learning Representations (ICLR).

[50] Brandon Tran, Jerry Li, and Aleksander Madry. 2018. Spectral Signatures in Backdoor Attacks. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS).

[51] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao. 2019. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[52] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal Security Analysis of Neural Networks Using Symbolic Intervals. In Proceedings of USENIX Security Symposium (SEC).

[53] W. Xu, D. Evans, and Y. Qi. 2018. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. In Proceedings of Network and Distributed System Security Symposium (NDSS).

[54] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. 2019. Latent Backdoor Attacks on Deep Neural Networks. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[55] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J. Smola. 2010. Parallelized Stochastic Gradient Descent. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS).

### Appendix
#### A. Proofs
**A0. Preliminaries**
In the following proofs, we use the following definitions for notational simplicity:
- \(\alpha \equiv h/r\)
- \(y \equiv 1 - z\)

Further, we have the following result.