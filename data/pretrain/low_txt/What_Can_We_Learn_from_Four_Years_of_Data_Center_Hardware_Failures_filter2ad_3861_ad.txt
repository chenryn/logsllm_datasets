### Product Design and Hardware Failures

In modern distributed systems, the impact of hardware failures is significantly mitigated by robust product design. Redundancy levels are often restored automatically after a failure, making such issues less critical. This is particularly true for product lines that utilize large-scale Hadoop clusters. Additionally, operators can improve efficiency by batching similar failures for handling.

### Discussions

During our study, we found that many traditional methods to enhance system dependability remain relevant. Here are some key observations:

1. **Failure Independence and Correlation:**
   - Most hardware failures occur independently. However, software must handle occasional correlated or repeating failures on single or multiple components. As these correlated failures become more common, there is a need to enhance software resilience.

2. **Automatic Failure Handling:**
   - Automatic detection and handling of hardware failures can be highly accurate, reducing the need for human intervention. However, multi-component failures present new challenges, and existing systems must be improved to address these new failure types.

3. **Data Center Design:**
   - In designing data centers, it is crucial to avoid "bad spots" where failure rates are higher. Alternatively, ensure that all replicas of a service are not allocated in these vulnerable zones.

### Is Hardware Reliability Still Relevant in Data Centers?

The advancements in software redundancy and automatic failure handling have significantly improved the end-to-end dependability of internet services. The Mean Time to Repair (MTTR) for hardware components seems less critical than before, as observed in operators' response times (RT).

A substantial team within the company manages data center hardware and has developed several software tools to improve reliability. Figures 10 and 11 illustrate the RT for different component classes and the relationship between the median RT and the number of HDD failures, respectively.

- **Response Times:**
  - SSDs have the shortest RT (several hours), while hard drives, fans, and memory have the longest (7 to 18 days).
  - The longer RT for hard drives is due to their mature fault tolerance mechanisms, both in hardware and software, making these failures less urgent.
  - SSDs, being more expensive, have less redundancy, and only crucial, user-facing online services use them, adhering to stricter operation guidelines.
  - Miscellaneous failures, which are manually entered, have short RTs because they mostly occur during hardware deployment, when installation, testing, and debugging are streamlined.

### Response Time in Different Product Lines

Although the company centrally monitors all servers, they are partitioned into different product lines. Each product line has its own algorithms and operation guidelines to enhance hardware dependability. The monitoring system is highly effective, with very few false positives (Table I). A tool was also developed to predict component failures a few days in advance, but operators often do not act promptly on these predictions.

- **Operator Behavior:**
  - It is surprising that operators ignore predictive warnings and leave failed components unhandled for extended periods, sometimes even months.
  - Delayed repairs lead to increased software complexity and development costs. Open-source solutions can offset some of these costs but come with high management overhead.
  - Unhandled hardware failures reduce the overall system capacity and can eventually appear as batch or synchronous failures, requiring more aggressive fault tolerance methods in software.

We believe it is time to consider failure handling as a joint optimization effort across software, hardware, and operational layers to lower the total cost of ownership of data centers. Dependability should evolve to a level where we focus not only on the reliability metrics of individual components or layers but also on achieving these metrics at a low total cost.

### Problems with the “Stateless” Failure Handling System

Our Failure Management System (FMS) records failures as individual Failure Occurrence Tickets (FOTs), including diagnosis and operator responses. While this stateless design makes FMS simple and scalable, it loses the correlation information among different FOTs. Operators must treat each FOT independently, which is challenging given the high annual turnover rate of over 50%.

- **Improvements:**
  - Building a data mining tool to discover correlations among failures could be beneficial.
  - Providing operators with additional information about an FOT, such as the history of the component, server, environment, and workload, can help reduce repeating failures effectively.

### Limitations of the Study

This study, like all data-driven studies, has inherent limitations:
- **Data Coverage:**
  - Although the dataset covers most failures, there are missing data points.
  - FMS was incrementally rolled out over four years, leading to varying coverage over time and across different data centers.
- **Data Quality:**
  - Lacking matching workload and detailed monitoring data, we can only make statistical observations and educated guesses.
  - We confirmed our findings with operators, but some cases remain unconfirmed.
- **Generalizability:**
  - The dataset comes from a single company and may not represent the entire industry. However, the failures span different generations of hardware, various data centers, and diverse workloads, providing a good representation of modern internet data centers.

### Related Work

Previous studies have extensively analyzed system failures to understand their properties, focusing mainly on high-performance computers (HPCs). However, the failure model in data centers differs due to the heterogeneity in hardware and workload. Recent studies on commercial data centers include analyses by Vishwanath et al., Ford et al., Garraghan et al., and Birk et al. Our work uses a larger dataset with more component classes and observes more correlated failures and longer MTTR in some product lines.

### Conclusion

Hardware failure studies are a long-standing topic. As researchers and data center practitioners, we addressed two key questions:
1. How do failure patterns evolve given the significant technological, economic, and demand changes in internet service data centers?
2. Given the availability of advanced software fault tolerance from open-source communities, is hardware reliability still relevant in data center operations?

Our analysis of hundreds of thousands of failure tickets from dozens of production data centers provides a fresh and deeper understanding of failure patterns in modern data centers. This understanding not only helps in better data center operations but also calls for a joint effort in software and hardware fault tolerance mechanisms to minimize overall costs.

### Acknowledgment

We thank our colleagues at Baidu Inc., Yang Wang for providing the datasets, Wei Wang, Xingxing Liu, Kai Lang, and Qian Qiu for helpful comments on the failure analysis. We also thank the students of Tsinghua University, Yong Xiang, Xin Liu, Hao Xue, Cheng Yang, Han Shen, Yiran Li, and Yi Li, for their valuable feedback on the drafts of this paper. This research is supported in part by the National Natural Science Foundation of China (NSFC) grant 61532001, Tsinghua Initiative Research Program Grant 20151080475, MOE Online Education Research Center (Quantong Fund) grant 2017ZD203, and gift funds from Huawei and Ant Financial.

### References

[1] L. A. Barroso, J. Clidaras, and U. H¨olzle, “The datacenter as a computer: An introduction to the design of warehouse-scale machines,” Synthesis lectures on computer architecture, vol. 8, no. 3, pp. 1–154, 2013.
[2] D. Oppenheimer, A. Ganapathi, and D. A. Patterson, “Why do Internet services fail, and what can be done about it?” in USENIX symposium on internet technologies and systems, vol. 67. Seattle, WA, 2003.
[3] D. A. Patterson et al., “A simple way to estimate the cost of downtime.” in LISA, vol. 2, 2002, pp. 185–188.
[4] J. Gray, “Why do computers stop and what can be done about it?” in Symposium on reliability in distributed software and database systems. Los Angeles, CA, USA, 1986, pp. 3–12.
[5] B. Schroeder and G. Gibson, “A large-scale study of failures in high-performance computing systems,” IEEE Transactions on Dependable and Secure Computing, vol. 7, no. 4, pp. 337–350, 2010.
[6] A. Oliner and J. Stearley, “What supercomputers say: A study of five system logs,” in International Conference on Dependable Systems and Networks (DSN’07). IEEE, 2007.
[7] N. El-Sayed and B. Schroeder, “Reading between the lines of failure logs: Understanding how HPC systems fail,” in International Conference on Dependable Systems and Networks (DSN’13). IEEE, 2013, pp. 1–12.
[8] S. Ghiasvand, F. M. Ciorba, R. Tsch, W. E. Nagel et al., “Lessons learned from spatial and temporal correlation of node failures in high performance computers,” in PDP 2016. IEEE, 2016.
[9] C. Di Martino, Z. Kalbarczyk, R. K. Iyer, F. Baccanico, J. Fullop, and W. Kramer, “Lessons learned from the analysis of system failures at petascale: The case of blue waters,” in International Conference on Dependable Systems and Networks (DSN’14). IEEE, 2014.
[10] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. Sahoo, “Bluegene/L failure analysis and prediction models,” in International Conference on Dependable Systems and Networks (DSN’06). IEEE, 2006, pp. 425–434.
[11] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai, D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Understanding GPU errors on large-scale HPC systems and the implications for system design and operation,” in HPCA 2015. IEEE, 2015.
[12] Y. Liang, Y. Zhang, A. Sivasubramaniam, R. K. Sahoo, J. Moreira, and M. Gupta, “Filtering failure logs for a BlueGene/L prototype,” in International Conference on Dependable Systems and Networks (DSN’05).
[13] A. Pecchia, D. Cotroneo, Z. Kalbarczyk, and R. K. Iyer, “Improving log-based field failure data analysis of multi-node computing systems,” in International Conference on Dependable Systems and Networks (DSN’11). IEEE, 2011, pp. 97–108.
[14] S. Gupta, D. Tiwari, C. Jantzi, J. Rogers, and D. Maxwell, “Understanding and exploiting spatial properties of system failures on extreme-scale HPC systems,” in International Conference on Dependable Systems and Networks (DSN’15). IEEE, 2015.
[15] B. Schroeder and G. A. Gibson, “Understanding failures in petascale computers,” in Journal of Physics: Conference Series, vol. 78, no. 1. IOP Publishing, 2007, p. 012022.
[16] E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large disk drive population.” in FAST, 2007.
[17] B. Schroeder and G. A. Gibson, “Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?” in FAST, 2007.
[18] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild: a large-scale field study,” in ACM SIGMETRICS Performance Evaluation Review, vol. 37, no. 1. ACM, 2009, pp. 193–204.
[19] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, “Revisiting memory errors in large-scale production data centers: Analysis and modeling of new trends from the field,” in International Conference on Dependable Systems and Networks (DSN’15). IEEE, 2015.
[20] L. Bautista-Gomez, F. Zyulkyarov, O. Unsal, and S. Mcintosh-Smith, “Unprotected computing: a large-scale study of DRAM raw error rate on a supercomputer,” in International Conference for High PERFORMANCE Computing, Networking, Storage and Analysis, 2016.
[21] B. Nie, D. Tiwari, S. Gupta, E. Smirni, and J. H. Rogers, “A large-scale study of soft-errors on GPUs in the field,” in HPCA 2016. IEEE, 2016.
[22] R. K. Sahoo, M. S. Squillante, A. Sivasubramaniam, and Y. Zhang, “Failure data analysis of a large-scale heterogeneous server environment,” in International Conference on Dependable Systems and Networks (DSN’04). IEEE, 2004.
[23] T. N. Minh and G. Pierre, “Failure analysis and modeling in large multi-site infrastructures,” in IFIP International Conference on Distributed Applications and Interoperable Systems. Springer, 2013, pp. 127–140.
[24] P. Garraghan, P. Townend, and J. Xu, “An empirical failure-analysis of a large-scale cloud computing environment,” in HASE 2015. IEEE, 2014.
[25] R. Birke, I. Giurgiu, L. Y. Chen, D. Wiesmann, and T. Engbersen, “Failure analysis of virtual and physical machines: patterns, causes and characteristics,” in International Conference on Dependable Systems and Networks (DSN’14). IEEE, 2014.
[26] D. Tiwari, S. Gupta, and S. S. Vazhkudai, “Lazy checkpointing: Exploiting temporal locality in failures to mitigate checkpointing overheads on extreme-scale systems,” in International Conference on Dependable Systems and Networks (DSN’14). IEEE, 2014.
[27] J. Yang and F. B. Sun, “A comprehensive review of hard-disk drive reliability,” in Reliability and Maintainability Symposium, 1999. Proceedings.
[28] D. Ford, F. Labelle, F. I. Popovici, M. Stokely, V. A. Truong, L. Barroso, C. Grimes, and S. Quinlan, “Availability in globally distributed storage systems,” in Usenix Symposium on Operating Systems Design and Implementation, OSDI 2010.
[29] A. Fox, “Toward recovery-oriented computing,” in VLDB 2002. VLDB Endowment, 2002.
[30] K. V. Vishwanath and N. Nagappan, “Characterizing cloud computing hardware reliability.” in ACM Symposium on Cloud Computing, 2010.
[31] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, “A large-scale study of flash memory failures in the field,” ACM Sigmetrics Performance Evaluation Review, vol. 43, no. 1, pp. 177–190, 2015.
[32] I. Narayanan, D. Wang, M. Jeon, B. Sharma, L. Caulfield, A. Sivasubramaniam, B. Cutler, J. Liu, B. Khessib, and K. Vaid, “SSD failures in datacenters: What, when and why?” ACM Sigmetrics Performance Evaluation Review, vol. 44, no. 1, pp. 407–408, 2016.