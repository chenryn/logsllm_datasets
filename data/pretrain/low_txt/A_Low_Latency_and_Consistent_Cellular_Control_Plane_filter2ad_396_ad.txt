### 4. Impact on Data Applications
Our evaluation demonstrates the impact of frequent mobility on self-driving car and AR applications. Additionally, we assess the effects on web browsing and video streaming applications.

### 5. Factor Analysis
To better understand Neutrino’s design choices and to isolate the benefits of its various design elements, we conducted a series of micro-benchmarks. These benchmarks include:
- Comparisons with different state synchronization schemes.
- Evaluations of the overhead associated with message logging.
- Comparisons of various serialization techniques, which support our choice of FlatBuffers.

### 6.1 Setup and Methodology
Our test setup consists of two servers running Ubuntu 18.04.3 with kernel version 4.15.0-74-generic. Each server is equipped with dual sockets, each containing 18 cores (Intel Xeon Gold 5220 CPU @ 2.20GHz), and dual NUMA nodes, providing a total of 128GB of memory. Both servers are also fitted with Intel X710 40 Gb (4 x 10) NICs. For testing control traffic, we implemented the S1AP and NAS protocols [50] and handled request and response messages between the UE/BS and CPF for various control procedures. These experiments were conducted using real signaling traces from a commercial traffic generator and RAN emulator from ng4T [45]. We generated two types of traffic patterns: 
- 10 Gbps bursty traffic to simulate a large number of IoT devices sending synchronized requests.
- Uniform traffic to simulate a specified number of control procedure requests per second.
All experiments were run for 60 seconds.

### 6.2 Baselines
We compared the performance of Neutrino against the following designs:
- **Existing EPC**: A modified version of the OpenAirInterface [49] codebase that uses ASN.1-based serialization and requires UEs to re-attach on CPF failure. It uses DPDK [2] for fast I/O operations instead of kernel sockets.
- **Neutrino**: A modified version of the existing EPC that:
  - Uses optimized FlatBuffers-based serialization.
  - Implements fast failure recovery as described in §4.2.
  - Performs structured state replication.
- **DPCM**: Similar to the existing EPC but with modified control procedures (BS receives state from the UE) as described in [61].
- **SkyCore**: A modified version of the existing EPC that synchronizes user state on each control message [40].

### 6.3 Latency Improvements in Procedure Completion Time (PCT) with Neutrino
This section presents PCT for attach, handover, and service request procedures in non-failure scenarios.

#### PCT - Uniform Traffic
Figure 7 compares the service request PCT of the existing EPC, DPCM, and SkyCore with Neutrino. The results show that for a uniform traffic rate of up to 120K Procedures Per Second (PPS), Neutrino performs 2.3×, 1.3×, and 3.4× better than the existing EPC, DPCM, and SkyCore, respectively. Beyond 140 KPPS, the existing EPC and SkyCore cannot handle the arrival rate, leading to a significant increase in PCT. At 200 KPPS and higher rates, PCT increases drastically for all schemes, but Neutrino still outperforms the others.

#### PCT - Bursty Traffic
Figure 9 shows the PCT distribution for the initial attach procedure with varying numbers of active UEs, using both Neutrino and the existing EPC. Due to the high arrival rate in the bursty traffic model, queues build up for both systems. Neutrino performs up to 2× better than the existing EPC in this scenario.

### 6.4 PCT under Failure with Neutrino
We conducted experiments with CPF failures for both Neutrino and the existing EPC. For the existing EPC, PCT under failure includes the time taken by the UE to execute the procedure before the failure and the time to re-attach to another CPF after the failure. For Neutrino, PCT under failure includes the time taken by the UE to execute the procedure before the failure and the time the secondary CPF takes to replay stored messages to recover the lost user state. Figure 10 shows the PCT distribution under CPF failure for the handover procedure with uniform traffic. We observed an improvement of up to 5.6× in median PCT when the procedure arrival rate is less than 60 KPPS. This improvement is attributed to faster serialization and state recovery in Neutrino.

### 6.5 Fast Handover in Neutrino
Figure 11 compares the PCT for handover in the existing EPC, Neutrino - Default (where user state migration is required before handover completion), and Neutrino - Proactive (where user state is proactively replicated in the target region to implement fast handover, as discussed in §4.3). The results show that Neutrino - Proactive improves median PCT by up to 7× over the existing EPC when the procedure arrival rate is less than 60 KPPS. Above 60 KPPS, the existing EPC cannot meet the arrival rate, and PCT increases significantly.

### 6.6 Impact on Application Performance
To measure the impact of Neutrino on application performance, we interfaced Intel’s 5G UPF [29] with Neutrino. A UE connected to Neutrino can create, delete, and modify sessions and bearers on the UPF through the S11 interface [9].

#### Impact on Autonomous Vehicles and AR/VR
To measure the impact of mobility on application performance, we set up the client application (on UE) with the CARLA self-driving car emulator [17, 18] and an edge application that processes sensor data. Experiments were performed in two scenarios: (i) executing a single handover and (ii) executing multiple handovers during a 5-minute drive at 60 mph with BS spacing similar to Figure 12. In both scenarios, we set a deadline for the application data and generated sensor data at a frequency of 1KHz in the uplink direction. At the edge application, we noted the number of packets that missed their application-specific deadlines.

- **Impact on Autonomous Vehicles**: The time budget for a self-driving car to make decisions based on sensor data is around 100 ms [55]. Figure 13 shows that in both single and multiple handover scenarios, Neutrino performs up to 2.8× better than the existing EPC.
- **Impact on VR Applications**: VR applications require a latency of less than 16 ms [53] to achieve perceptual stability. Figure 14 shows that Neutrino performs up to 2.5× better than the existing EPC in terms of the number of application packets that miss the VR deadline requirement in both single and multiple mobility scenarios.

#### Impact on Video Startup Latency and Page Load
The second set of experiments involved a stationary UE in idle state starting a web browsing or video streaming application. To get data access, the UE needs to execute a service request procedure to set up a data channel for the application. Application startup latency in this scenario is a function of service request PCT. We measured the average (i) video startup delay and (ii) page load time (PLT). To avoid network variations in video startup delay, an Apache web server was used to replay locally stored videos. Video startup delay was measured using a DASH player. Figure 3 shows a comparison of video startup delay between Neutrino and the existing EPC while the CPF is handling a varying number of active users. Neutrino performs up to 37× better than the existing EPC in terms of median video startup delay.

Page load time is the sum of (i) service request PCT and (ii) the average page load time of the top 10 Alexa pages. To filter out network variations, a MITM proxy [6] was used to replay locally stored web pages. A Firefox web browser extension, Load Time, was used to measure page load time. Figure 3 shows that Neutrino performs up to 3.2× better than the existing EPC in terms of median PLT.

### 6.7 Factor Analysis
Below, we discuss the results of our micro-benchmark experiments.

#### Impact of State Synchronization on PCT
We compared the overhead of different replica synchronization schemes on control plane latency. Figure 15 shows the attach PCT distribution for three schemes: (i) No Rep: no message logging and state replication, (ii) Per Msg Rep: with message logging and per-message state replication, and (iii) Per Proc Rep: with message logging and per-procedure state replication. Per-message state replication has the highest median PCT due to frequent state locking for checkpointing. Per-procedure state replication has a slightly higher median PCT compared to No Rep but provides the best trade-off between consistency and overhead.

#### Impact of Message Logging on PCT
We performed the attach procedure with and without message logging enabled. Figure 16 shows that message logging has a negligible impact on PCT in Neutrino, primarily because in-memory logging is fast.

#### Message Log Size at the CTA
Figure 17 shows the maximum log size at the CTA with varying numbers of active users and the type of procedures being performed with per-procedure synchronization. The log size grows with the number of active users, but even with 200K active users, it remains below 400 MB.

#### Serialization Benefits
We motivated the choice of FlatBuffers (FBs) for serializing cellular control messages over several other serialization schemes: FlexBuffer [26], Protocol Buffers [27], Fast-CDR [3], and LCM [5] with ASN.1 [1]. We compared the time to decode and encode control messages. For these experiments, we constructed a custom message with varying numbers of data elements/fields.

- **Encoding + Decoding Times**: Figure 18 shows the speedup in total encoding plus decoding time compared to the ASN.1 serialization scheme for a custom control message with varying numbers of data fields. For messages with fewer than 7 data elements, Fast-CDR and LCM perform better. When data elements increase beyond 7, FBs are the clear winner. For 25 data elements, the total speedup in encoding + decoding time for FBs is twice that of the next best scheme. The speedup compared to ASN.1 ranges from 1.6× to 19.2×. All tested cellular control messages contained a minimum of 8 data elements.
- **Tests with Real Control Messages**: We compared Optimized FBs with FBs and ASN.1 over a subset of real control messages, specifically quantifying both the encoding + decoding times and the increase in encoded message size with FBs. Figure 19 shows a decrease of up to 5.9× in encoding + decoding times with FlatBuffers over ASN.1. There is a further decrease with Optimized FBs in some cases. However, this decrease comes at a cost: the encoded message size in FBs can add up to 300 bytes of metadata more than ASN.1 (Figure 20). With Optimized FBs, we can save up to 32 bytes of data per message.