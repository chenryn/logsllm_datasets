### Environment and Failure Analysis

**Failure Categories:**
- **Environment:** 34
- **Software:** 43, 10, 29, 80, 5, 64
- **Network Links:** 150, 12
- **Hardware:** 6, 56, 6, 2, 55, 2
- **Interrupt (No Interrupt):** 34
- **Interrupt (SWO):** 56
- **Interrupt (Failover):** 3
- **Heartbeat/Node Down:** 3, 70%
- **Link & Node Failure (User Job Failed):** 15, 12, 2
- **Unknown:** 43, 2.9%

**Distribution of Cumulative Node Repair Hours:**
- **Heartbeat/Node Down:** 20.8%
- **Network Links:** 10.9%
- **Software:** 20.1%
- **Hardware:** 23%
- **Unknown:** 2.9%

**Breakdown of the Count of the Top 3 Hardware and Software Failure Root Causes:**
- **Hardware:**
  - **Failure (No Interrupt):** 2
  - **Interrupt (SWO):** 2
  - **Interrupt (Failover):** 2
  - **Link Failure (User Job Failed):** 18
  - **Link & Node Failure (User Job Failed):** 20
  - **Single/Multiple Node Failure:**
    - **PSU:** 15
    - **IPMI:** 14
    - **Fan tray assy:** 33
    - **Compute Blade:** 17
    - **Storage module:** 15
    - **Moab/TORQUE:** 5
    - **CLE/kernel:** 8
    - **Warm swap:** 45
    - **EPO:** 1
    - **Lustre net (Lnet):** 12
    - **RAM:** 9
    - **GPU:** 8
    - **Gemini voltage regulator:** 2
    - **Processor:** 1
    - **Sonexion/storage:** 3
    - **Lustre:** 29
    - **CLE/Kernel:** 8
    - **Lustre:** 4
    - **Processor:** 1
    - **RAM:** 1
    - **GPU:** 2
    - **Lustre:** 1
    - **Sonexion/Storage:** 160
    - **IPMI:** 158
    - **Compute blade:** 38
    - **Storage module:** 30
    - **Lustre:** 16
    - **Processor:** 5

- **Software:**
  - **Moab/TORQUE:** 20
  - **CLE/kernel:** 14
  - **Lustre:** 33
  - **Lustre:** 15
  - **CLE/Kernel:** 8
  - **Lustre:** 29
  - **Lustre:** 8
  - **CLE/Kernel:** 4
  - **Lustre:** 2
  - **Sonexion/Storage:** 160
  - **Lustre:** 158
  - **Lustre:** 38
  - **Lustre:** 30
  - **Lustre:** 16
  - **Lustre:** 5

**Analysis:**
To identify the reasons for the differences in failure distribution, we analyzed the number of nodes involved in failures with hardware or software root causes. We found that:

- **Hardware Failures:**
  - 96.7% of hardware failures were limited to a single node.
  - 99.3% of hardware failures were limited to a single blade (4 nodes).
  - Only 0.7% of hardware failures propagated outside the boundary of a single blade.
  - Failures in the voltage converter module (VRM) of the mezzanine and/or problems with the cabinet controller caused full blade failures.

- **Software Failures:**
  - If they did not cause a system-wide failure, software failures propagated to more than one node in 14% of the cases, which is 20 times more often than hardware failures.
  - Hardware is easier to diagnose and fix in bulk, reducing the cost of field intervention.
  - The Mean Time to Repair (MTTR) for the system is 5.16 hours, but for single nodes, it is 32.7 hours.

**Unknown and Heartbeat Failures:**
- Failures with unknown root causes (2.9%) and lack of heartbeat activity (20.8%) account for less than 2% of the total node repair hours.
- Only 2% of the failures detected by the heartbeat mechanisms were symptoms of real problems with the nodes.
- A node can be marked as "down" and later pass the test and return to the "up" state without any intervention.

**Top 3 Hardware and Software Root Causes:**
- **Hardware:**
  - Processors, memory DIMMs, and GPUs are the most frequently replaced components, accounting for 72% of the replaced units.
  - Processors are replaced when specific hardware exceptions (e.g., L1 cache parity errors) are observed in the system console logs.
  - Voltage regulator issues accounted for about 14% of single/multiple node failures.

- **Software:**
  - Lustre, the CLE OS, and Sonexion/storage software are the top 3 software root causes.
  - Lustre file system and related software caused 44% of the single/multiple node failures attributed to software root causes.
  - The CLE OS and Sonexion/Storage caused 28% and 9% of the total single/multiple node failures, respectively.

**Lustre Failures:**
- Lustre includes a rich software stack and plays a crucial role in Blue Waters.
- 6 out of 7 failure categories in Table IV include Lustre-related failures.
- 18 out of 104 Lustre failures escalated to SWOs (System-Wide Outages).

**Breakdown of Lustre Failures:**
- **Failover:** 25%
- **LBUG (Panic-style assertion):** 19%
- **Out of Memory:** 5%
- **Lustre Drivers:** 4%
- **OS:** 9%
- **LNET:** 16%
- **Configuration:** 5%
- **Metadata Unavailable:** 4%
- **Storage Controller Module:** 11%
- **RAM:** 2%

**Effectiveness of Failover:**
- On average, the system experiences 1-2 critical failures per day triggering automatic failover procedures.
- 28% of all failures triggered some type of failover mechanism.
- Out of 138 recorded failover operations, 24.6% (39 out of 138) resulted in system-wide outages, while the remaining 99 were successfully recovered.

**Lustre Failover:**
- 26 out of 104 failover attempts failed.
- MDT failover failed 12 times out of 26.
- OSS failover failed 2 times out of 4.
- OST failover failed 12 times out of 27.
- Under heavy job loads, recovery can take between 15 and 30 minutes.
- Lock timeouts due to high client volume cause 20% of Lustre failover failures.

**Gemini Failover:**
- Gemini network failover is the most resilient.
- Out of 323 documented link failures, 38 caused the loss of one or more user jobs, while 285 were successfully recovered.
- Gemini's fault tolerance is attributed to better testing and the nature of the software stack.

**Hardware Error Resiliency:**
- Blue Waters maintenance specialists diagnose processor and memory-related problems using machine check exceptions.
- In the measurement period, 1,544,398 machine check events were recorded, with only 28 uncorrectable errors.
- 46% of the total Blue Waters nodes experienced at least one memory error.
- 82.3% of the nodes with machine checks were compute nodes, 19.4% were GPU nodes, and 7.34% were service nodes.

**Table V: Breakdown of Machine Check Errors:**
- **Compute Nodes:**
  - Count: 594
  - Error/MB: 0.26
- **Service Nodes:**
  - Count: 632
  - Error/MB: 3.48
- **GPU Nodes:**
  - Count: 1,098
  - Error/MB: 0.02
- **All Nodes:**
  - Count: 97,974
  - Error/MB: 9.73E-4

This comprehensive analysis provides insights into the failure patterns and resiliency of the Blue Waters system, highlighting the need for improved failover and testing techniques for large-scale deployments.