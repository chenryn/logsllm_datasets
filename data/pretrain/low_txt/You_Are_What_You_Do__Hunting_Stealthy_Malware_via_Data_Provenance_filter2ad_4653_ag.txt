### References

1. Information Processing, vol. 27, pp. 297–314, 2019.
2. S. T. King and P. M. Chen, “Backtracking Intrusions,” in SOSP '03, ACM, 2003.
3. T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph Convolutional Networks,” arXiv preprint arXiv:1609.02907, 2016.
4. B. Kolosnjaji, A. Zarras, G. Webster, and C. Eckert, “Deep Learning for Classification of Malware System Call Sequences,” in Australasian Joint Conference on Artificial Intelligence, Springer, 2016, pp. 137–149.
5. D. Korczynski and H. Yin, “Capturing Malware Propagations with Code Injections and Code-Reuse Attacks,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. 1691–1708.
6. C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna, “Automating Mimicry Attacks Using Static Binary Analysis,” in USENIX Security Symposium, vol. 14, 2005.
7. Y. Kwon, F. Wang, W. Wang, K. H. Lee, W.-C. Lee, S. Ma, X. Zhang, D. Xu, S. Jha, G. F. Ciocarlie et al., “MCI: Modeling-Based Causality Inference in Audit Logging for Attack Investigation,” in NDSS, 2018.
8. Q. Le and T. Mikolov, “Distributed Representations of Sentences and Documents,” in International Conference on Machine Learning, 2014, pp. 1188–1196.
9. K. H. Lee, X. Zhang, and D. Xu, “High Accuracy Attack Provenance via Binary-Based Execution Partition,” in NDSS, 2013.
10. ——, “LogGC: Garbage Collecting Audit Log,” in CCS, New York, NY, USA: ACM, 2013, pp. 1005–1016.
11. F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation Forest,” in 2008 IEEE Eighth International Conference on Data Mining, 2008, pp. 413–422.
12. Y. Liu, M. Zhang, D. Li, K. Jee, Z. Li, Z. Wu, J. Rhee, and P. Mittal, “Towards a Timely Causality Analysis for Enterprise Security,” in NDSS, 2018.
13. S. Ma, X. Zhang, and D. Xu, “ProTracer: Towards Practical Provenance Tracing by Alternating Between Logging and Tainting,” in NDSS, 2016.
14. L. v. d. Maaten and G. Hinton, “Visualizing Data Using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579–2605, 2008.
15. F. Maggi, M. Matteucci, and S. Zanero, “Detecting Intrusions Through System Call Sequence and Argument Analysis,” IEEE Transactions on Dependable and Secure Computing, vol. 7, no. 4, pp. 381–395, 2008.
16. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed Representations of Words and Phrases and Their Compositionality,” in Advances in Neural Information Processing Systems, 2013, pp. 3111–3119.
17. M. Mimura and H. Tanaka, “A Linguistic Approach Towards Intrusion Detection in Actual Proxy Logs,” in International Conference on Information and Communications Security, Springer, 2018, pp. 708–718.
18. A. Narayanan, M. Chandramohan, R. Venkatesan, L. Chen, Y. Liu, and S. Jaiswal, “graph2vec: Learning Distributed Representations of Graphs,” arXiv preprint arXiv:1707.05005, 2017.
19. D. Nguyen, J. Park, and R. Sandhu, “Adopting Provenance-Based Access Control in OpenStack Cloud IaaS,” in International Conference on Network and System Security, Springer, 2014, pp. 15–27.
20. S. Omar, A. Ngadi, and H. H. Jebur, “Machine Learning Techniques for Anomaly Detection: An Overview,” International Journal of Computer Applications, vol. 79, no. 2, 2013.
21. R. Paccagnella, P. Datta, W. U. Hassan, C. W. Fletcher, A. Bates, A. Miller, and D. Tian, “Custos: Practical Tamper-Evident Auditing of Operating Systems Using Trusted Execution,” in Proc. of the Symposium on Network and Distributed System Security (NDSS), 2020.
22. K. Padmanabhan, Z. Chen, S. Lakshminarasimhan, S. S. Ramaswamy, and B. T. Richardson, “Graph-Based Anomaly Detection,” Practical Graph Mining with R (2013), vol. 311, 2013.
23. M. Pagliardini, P. Gupta, and M. Jaggi, “Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features,” arXiv preprint arXiv:1703.02507, 2017.
24. C. Parampalli, R. Sekar, and R. Johnson, “A Practical Mimicry Attack Against Powerful System-Call Monitors,” in Proceedings of the 2008 ACM Symposium on Information, Computer and Communications Security, ACM, 2008.
25. J. Park, D. Nguyen, and R. Sandhu, “A Provenance-Based Access Control Model,” in Privacy, Security and Trust (PST), 2012 Tenth Annual International Conference on, IEEE, 2012, pp. 137–144.
26. G. Pék, Z. Lázár, Z. Várnagy, M. Félegyházi, and L. Buttyán, “Membrane: A Posteriori Detection of Malicious Code Loading by Memory Paging Analysis,” in European Symposium on Research in Computer Security, Springer, 2016, pp. 199–216.
27. P. Perera and V. M. Patel, “Learning Deep Features for One-Class Classification,” IEEE Transactions on Image Processing, 2019.
28. D. Pohly, S. McLaughlin, P. McDaniel, and K. Butler, “Hi-Fi: Collecting High-Fidelity Whole-System Provenance,” in ACSAC, Orlando, FL, USA, 2012.
29. T. M. Research, “2019 Midyear Security Roundup: Evasive Threats, Pervasive Effects,” Tech. Rep., Sep. 2019.
30. M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pp. 1135–1144.
31. A.-D. Schmidt, R. Bye, H.-G. Schmidt, J. Clausen, O. Kiraz, K. A. Yuksel, S. A. Camtepe, and S. Albayrak, “Static Analysis of Executables for Collaborative Malware Detection on Android,” in 2009 IEEE International Conference on Communications, IEEE, 2009, pp. 1–5.
32. B. Schölkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and J. C. Platt, “Support Vector Method for Novelty Detection,” in Advances in Neural Information Processing Systems, 2000, pp. 582–588.
33. M. Sebastián, R. Rivera, P. Kotzias, and J. Caballero, “Avclass: A Tool for Massive Malware Labeling,” in International Symposium on Research in Attacks, Intrusions, and Defenses, Springer, 2016, pp. 230–253.
34. M. Sharif, V. Yegneswaran, H. Saidi, P. Porras, and W. Lee, “Eureka: A Framework for Enabling Static Malware Analysis,” in European Symposium on Research in Computer Security, Springer, 2008, pp. 481–500.
35. X. Shu, D. Yao, and N. Ramakrishnan, “Unearthing Stealthy Program Attacks Buried in Extremely Long Execution Paths,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, 2015, pp. 401–413.
36. M. A. Siddiqui, A. Fern, R. Wright, A. Theriault, D. Archer, and W. Maxwell, “Detecting Cyberattack Entities from Audit Data via Multi-View Anomaly Detection with Feedback,” in Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
37. K. S. Tai, R. Socher, and C. D. Manning, “Improved Semantic Representations from Tree-Structured Long Short-Term Memory Networks,” arXiv preprint arXiv:1503.00075, 2015.
38. N. Tavabi, P. Goyal, M. Almukaynizi, P. Shakarian, and K. Lerman, “Darkembed: Exploit Prediction with Neural Language Models,” in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
39. B. E. Ujcich, S. Jero, A. Edmundson, Q. Wang, R. Skowyra, J. Landry, A. Bates, W. H. Sanders, C. Nita-Rotaru, and H. Okhravi, “Cross-App Poisoning in Software-Defined Networking,” in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, 2018, pp. 648–663.
40. C. Wagner, A. Dulaunoy, G. Wagener, and A. Iklody, “MISP: The Design and Implementation of a Collaborative Threat Intelligence Sharing Platform,” in Proceedings of the 2016 ACM on Workshop on Information Sharing and Collaborative Security, ACM, 2016, pp. 49–56.
41. D. Wagner and P. Soto, “Mimicry Attacks on Host-Based Intrusion Detection Systems,” in Proceedings of the 9th ACM Conference on Computer and Communications Security, ACM, 2002.
42. Q. Wang, W. U. Hassan, A. Bates, and C. Gunter, “Provenance Tracing in the Internet of Things,” in Proceedings of the 9th USENIX Conference on Theory and Practice of Provenance, 2017, pp. 9–9.
43. ——, “Fear and Logging in the Internet of Things,” in Network and Distributed Systems Symposium, 2018.
44. C. Wueest, “Internet Security Threat Report - Living Off the Land and Fileless Attack Techniques,” 2017.
45. K. Xu, K. Tian, D. Yao, and B. G. Ryder, “A Sharper Sense of Self: Probabilistic Reasoning of Program Behaviors for Anomaly Detection with Context Sensitivity,” in DSN, IEEE, 2016.
46. X. Yuan, O. Setayeshfar, H. Yan, P. Panage, X. Wei, and K. H. Lee, “DroidForensics: Accurate Reconstruction of Android Attacks via Multi-Layer Forensic Logging,” in Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 2017.
47. Z. Yuan, Y. Lu, Z. Wang, and Y. Xue, “Droid-Sec: Deep Learning in Android Malware Detection,” in ACM SIGCOMM Computer Communication Review, vol. 44, no. 4, ACM, 2014, pp. 371–372.

### Appendix

#### A. Neural Document Embedding Models

Word2vec [37] is one of the most well-known word embedding methods. It uses a simple and efficient feed-forward neural network architecture called "skip-gram" to learn distributed representations of words. Recently, Le and Mikolov proposed Paragraph Vector (i.e., doc2vec) [8], a straightforward extension of word2vec that is capable of learning distributed representations of arbitrary length word sequences such as sentences, paragraphs, and even whole large documents.

**Figure 10: The PV-DM model for learning a paragraph vector.**

PV-DM (Distributed Memory Model of Paragraph Vectors) is one version of doc2vec. The core idea of PV-DM is that a paragraph \( p \) can be represented as another vector (i.e., paragraph vector) contributing to the prediction of the next word in a sentence. In the PV-DM model, as illustrated in Figure 10, every paragraph is mapped to a paragraph vector, represented by a column in a paragraph matrix, and every word is mapped to a word vector, represented by a column in a word matrix. Then, the paragraph vector and word vectors are averaged or concatenated to predict the next word in a context. The contexts are fixed-length and sampled from a sliding window over the paragraph. The paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs. The PV-DM model uses stochastic gradient descent to train the paragraph vectors and word vectors. After being trained, the paragraph vectors can be used as features for the paragraph. At prediction time, the model also uses gradient descent to compute the paragraph vector for a new paragraph.

#### B. Comparison of Different Anomaly Detection Algorithms

In our current implementation, we use Local Outlier Factor (LOF) [11] as the default anomaly detector. We compare LOF with three other novelty detection or outlier detection algorithms in path-level accuracy. The three baseline methods are as follows:

- **Isolation Forest [11]:** This algorithm divides the data points into different partitions. Outliers need fewer cuts to be separated from other points, while inliers need more cuts.
- **One-Class SVM [32]:** The algorithm trains a hyperplane that separates all the training data from the origin while maximizing the distance from the origin to the hyperplane.
- **Robust Covariance (Elliptic Envelope) [9]:** The algorithm assumes that the data is Gaussian distributed and learns an ellipse.

In the evaluation of the above baseline methods, we follow the same experiment protocol as we did for PROVDETECTOR. For one-class SVM, we use the rbf kernel with `nu` set to 0.1 and `gamma` set to 0.5. For the other three models, we set the contamination to 0.04. The results are summarized in Table V.

**Table V: Comparison of different anomaly detection algorithms in path-level detection accuracy.**

| Algorithm             | Precision | Recall | F1-Score |
|-----------------------|-----------|--------|----------|
| Local Outlier Factor  | 0.959     | 0.991  | 0.974    |
| One-Class SVM         | 0.886     | 0.635  | 0.739    |
| Isolation Forest      | 0.955     | 0.467  | 0.627    |
| Robust Covariance     | 0.940     | 0.397  | 0.558    |

As shown in the table, LOF significantly outperforms other methods in terms of recall. This justifies our design choice of using LOF. We will further explain the results in Section VI-C.

#### C. Additional Experiments to Interpret the Result of PROVDETECTOR

In this section, we discuss several additional questions about the results of PROVDETECTOR. These questions are:

- Why does LOF perform better?
- What is learned by PROVDETECTOR?

**1. Why Does LOF Perform Better?**

As shown in Table V, LOF performs the best among the four evaluated algorithms. This is because LOF does not rely on an assumption about the distribution of the data. As shown in Figure 9, the embeddings of paths have multiple clusters and do not follow any single distribution.

Robust Covariance performs worst as it assumes the data obeys approximately a Gaussian distribution and tries to learn an ellipse to cover the normal data points. Consequently, it may degrade when the data is not unimodal. Isolation Forest and One-Class SVM outperform Robust Covariance because they do not rely on any assumption on the distribution of data. However, these two methods assume that the normal paths are all from one cluster; thus, they cannot achieve as high a detection accuracy as LOF.

On the other hand, LOF detects anomalous data points by measuring the local deviation of a given data point with respect to its neighbors, making it typically suitable for the case where different models in the data have different densities. As with our data, different workloads may generate paths that have different densities in distribution, thus LOF could achieve a high detection accuracy.

**2. What PROVDETECTOR Learns**

There are two possible kinds of features that PROVDETECTOR has learned: the path-level feature or the single node level feature. If PROVDETECTOR only learns single node level features, it could indicate that PROVDETECTOR only memorizes a small set of nodes to detect malicious paths. Still take the winword program as an example, a “bad” detection model which only learns node-level features might predict a path as malicious if a previously unseen process (e.g., PowerShell) node is in the path. Such a detection model can easily be evaded by attackers.

To answer this question, a naive method is to develop baseline detection methods that only rely on single node level features. However, this method may have a bias from the specific baseline detection methods we select. Instead, we use LIME [30], a model-agnostic prediction explanation tool, to calculate and rank the “impact” of each single node in a path to the final detection. LIME also produces a numeric value to evaluate how much the final result would change, in case we remove any node from the path.

We use LIME to calculate the “KEY” nodes for each benign path and malicious path. A set of nodes are considered as KEY nodes if they are the most impactful nodes identified by LIME and PROVDETECTOR would give a different detection result if we remove these nodes from the path. We try to find if there is a set of KEY nodes that are common across all the paths. If so, it indicates that PROVDETECTOR has only learned single node level features.

In our experiment, we find that there is not a set of KEY nodes that can be shared by most of the paths. For benign paths, 35% of the paths have their own unique KEY node. On average, the number of paths that share the same KEY node is 3.18. In other words, each KEY node is used to impact 3 benign paths on average in PROVDETECTOR. For malicious paths, about 50% of paths have their unique KEY node. The average number of paths that share the same KEY node is 3.1. In summary, combined with the results in §VI-B2, PROVDETECTOR relies on path-level features instead of single node level features to detect stealthy malware, which is consistent with our design motivation.