### X. CONCLUSION

In this work, we present a viable solution for integrating data provenance into commercial Endpoint Detection and Response (EDR) tools. We utilize the concept of tactical provenance to analyze causally related threat alerts, which are then encoded into a Tactical Provenance Graph (TPG). The TPG is leveraged for risk assessment of EDR-generated threat alerts and for reducing system log volume. We integrated our prototype system, RapSheet, into the Symantec EDR tool. Our evaluation results on an enterprise dataset demonstrate that RapSheet enhances the threat detection accuracy of the Symantec EDR. Additionally, our log reduction technique significantly reduces the overhead associated with long-term system log storage while preserving causal links between existing and future alerts.

### ACKNOWLEDGMENT

We extend our gratitude to our shepherd, Guofei Gu, and the anonymous reviewers for their valuable comments and suggestions. We also thank Akul Goyal, Riccardo Paccagnella, and Ben Ujcich for their feedback on early drafts of this paper, as well as all members of the NortonLifeLock Research Group. Wajih Ul Hassan was partially supported by the Symantec Graduate Fellowship. This research was supported in part by the National Science Foundation (NSF) under contracts CNS-16-57534 and CNS-17-50024. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of their employers or the sponsors.

### REFERENCES

[1] “Target Missed Warnings in Epic Hack of Credit Card Data,” https://bloom.bg/2KjElxM, 2019.
[2] “Equifax Says Cyberattack May Have Affected 143 Million in the U.S.,” https://www.nytimes.com/2017/09/07/business/equifax-cyberattack.html, 2017.
[3] “Inside the Cyberattack That Shocked the US Government,” https://www.wired.com/2016/10/inside-cyberattack-shocked-us-government/, 2016.
[4] “What's in a Name? TTPs in Info Sec,” https://posts.specterops.io/whats-in-a-name-ttps-in-info-sec-14f24480ddcc, 2019.
[5] “The Critical Role of Endpoint Detection and Response,” https://bit.ly/39NrNwo, 2019.
[6] “MITRE ATT&CK,” https://attack.mitre.org, 2019.
[7] “Why MITRE ATT&CK Matters,” https://symantec-blogs.broadcom.com/blogs/expert-perspectives/why-mitre-attck-matters.
[8] “Experts Advocate for ATT&CK,” https://www.cyberscoop.com/mitre-attck-framework-experts-advocate/.
[9] “ATT&CK Evaluations,” https://attackevals.mitre.org/.
[10] “Endpoint Detection and Response Solutions Market,” https://www.gartner.com/reviews/market/endpoint-detection-and-response-solutions, 2019.
[11] “File Deletion,” https://attack.mitre.org/techniques/T1107/, 2019.
[12] “Automated Incident Response: Respond to Every Alert,” https://swimlane.com/blog/automated-incident-response-respond-every-alert/, 2019.
[13] “New Research from Advanced Threat Analytics,” https://prn.to/2uTiaK6, 2019.
[14] G. P. Spathoulas and S. K. Katsikas, “Using a Fuzzy Inference System to Reduce False Positives in Intrusion Detection,” in International Conference on Systems, Signals and Image Processing, 2009.
[15] “How Many Alerts is Too Many to Handle?” https://www2.fireeye.com/StopTheNoise-IDC-Numbers-Game-Special-Report.html, 2019.
[16] “An ESG Research Insights Report,” http://pages.siemplify.co/rs/182-SXA-457/images/ESG-Research-Report.pdf.
[17] “Splunk,” https://www.splunk.com.
[18] “About Purging Reports,” https://support.symantec.com/us/en/article.howto129116.html, 2019.
[19] “Evaluating Endpoint Products,” https://redcanary.com/blog/evaluating-endpoint-products-in-a-crowded-confusing-market/, 2018.
[20] A. Bates, W. U. Hassan, K. Butler, A. Dobra, B. Reaves, P. Cable, T. Moyer, and N. Schear, “Transparent Web Service Auditing via Network Provenance Functions,” in WWW, 2017.
[21] A. Bates, D. Tian, K. R. B. Butler, and T. Moyer, “Trustworthy Whole-System Provenance for the Linux Kernel,” in USENIX Security, 2015.
[22] M. N. Hossain, S. M. Milajerdi, J. Wang, B. Eshete, R. Gjomemo, R. Sekar, S. D. Stoller, and V. Venkatakrishnan, “SLEUTH: Real-Time Attack Scenario Reconstruction from COTS Audit Data,” in USENIX Security, 2017.
[23] Y. Kwon, F. Wang, W. Wang, K. H. Lee, W.-C. Lee, S. Ma, X. Zhang, D. Xu, S. Jha, G. Ciocarlie et al., “MCI: Modeling-Based Causality Inference in Audit Logging for Attack Investigation,” in NDSS, 2018.
[24] K. H. Lee, X. Zhang, and D. Xu, “High Accuracy Attack Provenance via Binary-Based Execution Partition,” in NDSS, 2013.
[25] S. Ma, K. H. Lee, C. H. Kim, J. Rhee, X. Zhang, and D. Xu, “Accurate, Low Cost, and Instrumentation-Free Security Audit Logging for Windows,” in ACSAC. ACM, 2015.
[26] S. Ma, J. Zhai, F. Wang, K. H. Lee, X. Zhang, and D. Xu, “MPI: Multiple Perspective Attack Investigation with Semantic Aware Execution Partitioning,” in USENIX Security, 2017.
[27] W. U. Hassan, M. A. Noureddine, P. Datta, and A. Bates, “OmegaLog: High-Fidelity Attack Investigation via Transparent Multi-Layer Log Analysis,” in NDSS, 2020.
[28] S. M. Milajerdi, B. Eshete, R. Gjomemo, and V. Venkatakrishnan, “Poirot: Aligning Attack Behavior with Kernel Audit Records for Cyber Threat Hunting,” in CCS, 2019.
[29] W. U. Hassan, M. Lemay, N. Aguse, A. Bates, and T. Moyer, “Towards Scalable Cluster Auditing through Grammatical Inference over Provenance Graphs,” in NDSS, 2018.
[30] K. H. Lee, X. Zhang, and D. Xu, “LogGC: Garbage Collecting Audit Log,” in CCS, 2013.
[31] Y. Liu, M. Zhang, D. Li, K. Jee, Z. Li, Z. Wu, J. Rhee, and P. Mittal, “Towards a Timely Causality Analysis for Enterprise Security,” in NDSS, 2018.
[32] S. Ma, X. Zhang, and D. Xu, “ProTracer: Towards Practical Provenance Tracing by Alternating Between Logging and Tainting,” in NDSS, 2016.
[33] T. Pasquier, X. Han, T. Moyer, A. Bates, O. Hermant, D. Eyers, J. Bacon, and M. Seltzer, “Runtime Analysis of Whole-System Provenance,” in CCS. ACM, 2018.
[34] Z. Xu, Z. Wu, Z. Li, K. Jee, J. Rhee, X. Xiao, F. Xu, H. Wang, and G. Jiang, “High Fidelity Data Reduction for Big Data Security Dependency Analyses,” in CCS, 2016.
[35] S. Ma, J. Zhai, Y. Kwon, K. H. Lee, X. Zhang, G. Ciocarlie, A. Gehani, V. Yegneswaran, D. Xu, and S. Jha, “Kernel-Supported Cost-Effective Audit Logging for Causality Tracking,” in USENIX ATC, 2018.
[36] Y. Tang, D. Li, Z. Li, M. Zhang, K. Jee, X. Xiao, Z. Wu, J. Rhee, F. Xu, and Q. Li, “NodeMerge: Template Based Efficient Data Reduction for Big-Data Causality Analysis,” in CCS. ACM, 2018.
[37] M. N. Hossain, J. Wang, R. Sekar, and S. D. Stoller, “Dependence-Preserving Data Compaction for Scalable Forensic Analysis,” in USENIX Security Symposium, 2018.
[38] W. U. Hassan, S. Guo, D. Li, Z. Chen, K. Jee, Z. Li, and A. Bates, “NoDoze: Combatting Threat Alert Fatigue with Automated Provenance Triage,” in NDSS, 2019.
[39] Q. Wang, W. U. Hassan, D. Li, K. Jee, X. Yu, K. Zou, J. Rhee, Z. Chen, W. Cheng, C. Gunter, and H. Chen, “You Are What You Do: Hunting Stealthy Malware via Data Provenance Analysis,” 2020.
[40] X. Han, T. Pasquier, A. Bates, J. Mickens, and M. Seltzer, “UNICORN: Runtime Provenance-Based Detector for Advanced Persistent Threats,” in NDSS, 2020.
[41] A. Bates and W. U. Hassan, “Can Data Provenance Put an End to the Data Breach?” IEEE Security & Privacy, vol. 17, no. 4, pp. 88–93, July 2019.
[42] K. Pei, Z. Gu, B. Saltaformaggio, S. Ma, F. Wang, Z. Zhang, L. Si, X. Zhang, and D. Xu, “HERCULE: Attack Story Reconstruction via Community Discovery on Correlated Log Graph,” in ACSAC. ACM, 2016.
[43] S. M. Milajerdi, R. Gjomemo, B. Eshete, R. Sekar, and V. Venkatakrishnan, “HOLMES: Real-Time APT Detection through Correlation of Suspicious Information Flows,” in IEEE S&P, 2019.
[44] “Threat-Based Defense,” https://www.mitre.org/capabilities/cybersecurity/threat-based-defense, 2019.
[45] E. M. Hutchins, M. J. Cloppert, and R. M. Amin, “Intelligence-Driven Computer Network Defense Informed by Analysis of Adversary Campaigns and Intrusion Kill Chains,” Leading Issues in Information Warfare & Security Research, vol. 1, no. 1, p. 80, 2011.
[46] S. T. King and P. M. Chen, “Backtracking Intrusions,” in SOSP. ACM, 2003.
[47] “Windows Event Tracing,” https://docs.microsoft.com/en-us/windows/desktop/ETW/event-tracing-portal.
[48] “The Linux Audit Daemon,” https://linux.die.net/man/8/auditd.
[49] “MITRE Matrix,” https://attack.mitre.org/matrices/enterprise/.
[50] “APT 29 - Put Up Your Dukes,” https://www.anomali.com/blog/apt-29-put-up-your-dukes, 2019.
[51] “APT29,” https://attack.mitre.org/groups/G0016/, 2019.
[52] “CrowdStrike,” https://www.crowdstrike.com/.
[53] Airbus Cyber Security, “APT Kill Chain,” https://airbus-cyber-security.com/apt-kill-chain-part-2-global-view/, 2018.
[54] R. Paccagnella, P. Datta, W. U. Hassan, C. W. Fletcher, A. Bates, A. Miller, and D. Tian, “Custos: Practical Tamper-Evident Auditing of Operating Systems Using Trusted Execution,” in NDSS, 2020.
[55] W. Zhou, Q. Fei, A. Narayan, A. Haeberlen, B. T. Loo, and M. Sherr, “Secure Network Provenance,” in SOSP, 2011.
[56] “Endgame - Endpoint Protection,” https://www.endgame.com/sites/default/files/architecturesolutionbrief.pdf, 2019.
[57] “Endpoint Security in Today’s Threat Environment,” https://ziften.com/wp-content/uploads/2016/12/UserMode-Whitepaper.pdf, 2019.
[58] “Monitoring ALPC Messages,” http://blogs.microsoft.co.il/pavely/2017/02/12/monitoring-alpc-messages/, 2017.
[59] L. Lamport, “Time, Clocks, and the Ordering of Events in a Distributed System,” Commun. ACM, vol. 21, no. 7, pp. 558–565, Jul. 1978. [Online]. Available: http://doi.acm.org/10.1145/359545.359563
[60] “Common Attack Pattern Enumeration and Classification,” https://capec.mitre.org, 2019.
[61] MITRE, “Cyber Threat Intelligence Repository,” https://github.com/mitre/cti.
[62] “Registry Run Keys / Startup Folder,” https://attack.mitre.org/techniques/T1060/, 2019.
[63] “CAPEC-270: Modification of Registry Run Keys,” https://capec.mitre.org/data/definitions/163.html, 2019.
[64] “Apache TinkerPop,” http://tinkerpop.apache.org/, 2019.
[65] “RedisGraph - a graph database module for Redis,” https://oss.redislabs.com/redisgraph/, 2019.
[66] H. Lim, D. Han, D. G. Andersen, and M. Kaminsky, “Mica: A Holistic Approach to Fast In-Memory Key-Value Storage.” USENIX, 2014.
[67] MITRE, “Technology Transfer: CALDERA,” https://www.mitre.org/research/technology-transfer/open-source-software/caldera.
[68] “APT3,” https://attack.mitre.org/groups/G0022/, 2019.
[69] A. Valdes and K. Skinner, “Probabilistic Alert Correlation,” in International Workshop on Recent Advances in Intrusion Detection. Springer, 2001, pp. 54–68.
[70] W. Wang and T. E. Daniels, “A Graph-Based Approach Toward Network Forensics Analysis,” TISSEC, 2008.
[71] H. Debar and A. Wespi, “Aggregation and Correlation of Intrusion-Detection Alerts,” in International Workshop on Recent Advances in Intrusion Detection. Springer, 2001, pp. 85–103.
[72] Y. Shen and G. Stringhini, “Attack2Vec: Leveraging Temporal Word Embeddings to Understand the Evolution of Cyberattacks,” in USENIX Security, 2019.
[73] G. Gu, P. Porras, V. Yegneswaran, and M. Fong, “BotHunter: Detecting Malware Infection Through IDS-Driven Dialog Correlation,” in USENIX Security Symposium, 2007.
[74] “Endpoint Monitoring & Security,” https://logrhythm.com/solutions/security/endpoint-threat-detection/, 2019.
[75] “What is SIEM?” https://logz.io/blog/what-is-siem/, 2019.
[76] A. Bates, K. Butler, A. Haeberlen, M. Sherr, and W. Zhou, “Let SDN Be Your Eyes: Secure Forensics in Data Center Networks,” in SENT, 2014.
[77] Y. Wu, A. Chen, and L. T. X. Phan, “Zeno: Diagnosing Performance Problems with Temporal Provenance,” in NSDI, 2019.
[78] A. Chen, Y. Wu, A. Haeberlen, W. Zhou, and B. T. Loo, “The Good, the Bad, and the Differences: Better Network Diagnostics with Differential Provenance,” in ACM SIGCOMM, 2016.
[79] Y. Wu, M. Zhao, A. Haeberlen, W. Zhou, and B. T. Loo, “Diagnosing Missing Events in Distributed Systems with Negative Provenance,” ACM SIGCOMM Computer Communication Review, vol. 44, no. 4, pp. 383–394, 2014.
[80] W. Zhou, M. Sherr, T. Tao, X. Li, B. T. Loo, and Y. Mao, “Efficient Querying and Maintenance of Network Provenance at Internet-Scale,” in ACM SIGMOD, 2010.
[81] W. Zhou, S. Mapara, Y. Ren, Y. Li, A. Haeberlen, Z. Ives, B. T. Loo, and M. Sherr, “Distributed Time-Aware Provenance,” Proceedings of the VLDB Endowment, pp. 49–60, 2012.
[82] A. Gehani and D. Tariq, “SPADE: Support for Provenance Auditing in Distributed Environments,” in Middleware, 2012.

### APPENDIX A: DATASET CHARACTERIZATION

In this section, we characterize the dataset used in our evaluation. We collected 40 million system monitoring events from 34 hosts in a real-world enterprise environment. These host machines were used daily by employees for web browsing, software coding and compilation, quality assurance testing, project management, and other routine business tasks. We used 67 total alert rules to detect various MITRE ATT&CK techniques in our experiments. Some of these rules were written by us, while others were included by default in the Symantec EDR software.

#### Figure 13: Number of Matched MITRE ATT&CK Techniques During Our Evaluation with Their True Positive Rates

First, we examine how often the various MITRE ATT&CK technique and tactic rules caused alerts on the hosts in our experiment. Figure 13 shows which MITRE ATT&CK techniques were matched, how many times, and what proportion of the alerts for each technique were related to a true attack. 

From the figure, it is evident that rules for techniques like RunDLL32 (T1085) and Scripting (T1064) generated many alerts but had very low true positive rates, as these techniques are commonly used for benign purposes. On the other hand, techniques like "Change File Association" (T1042) and "System Service Discovery" (T1007) were triggered frequently and had high true positive rates because they usually occur during malicious activity. Thus, these techniques can be strong indicators of an attack campaign.

\[
\begin{array}{c}
\text{True Positive Rate} \\
\text{100\%} \\
\text{80\%} \\
\text{60\%} \\
\text{40\%} \\
\text{20\%} \\
\text{0\%} \\
\end{array}
\quad
\begin{array}{c}
\text{Total Number of Observations} \\
\text{1} \\
\text{10} \\
\text{100} \\
\text{1,000} \\
\text{10,000} \\
\end{array}
\]

\[
\begin{array}{c}
\text{False Alarms} \\
\text{-10} \\
\text{-100} \\
\text{-1,000} \\
\text{-10,000} \\
\text{-100,000} \\
\end{array}
\quad
\begin{array}{c}
\text{Defense Evasion} \\
\text{Lateral Movement} \\
\text{Initial Access} \\
\text{Privilege Escalation} \\
\end{array}
\]

This visualization helps in understanding the effectiveness of different alert rules and their contribution to the overall security posture.