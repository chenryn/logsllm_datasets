### Average Scrubbing Time and MRL

The average scrubbing time is given by \( M L (2) = 0.48 \times 10^2 \). As in Section 6, we assume \( MRL \approx 0 \) in Equation (2). The parameter \( p \) in Equation (3) is estimated using Equation (4) and the average parity update time when it runs concurrently with scrubbing.

### Multi-Feature Case: Scrubbing and Intra-Disk Parity

#### 8.2 Results for Trace T1

Scrubbing and intra-disk parity can be used simultaneously to improve Mean Time To Data Loss (MTTDL). In this section, we evaluate the performance of these two features when running concurrently. Since both features run in the background without any buffer requirement, their queue capacity is assumed to be infinite. Scrubbing generates infinite work, while parity updates require finite work. We evaluate a scenario where parity updates have higher priority than scrubbing, meaning scrubbing is scheduled only if there are no pending parity updates. As in previous sections, the performance degradation of user traffic is kept below a preset 7% threshold.

#### 8.1 MTTDL in Data Redundant Drives

We use Equation (3) to estimate the MTTDL improvement when both scrubbing and intra-disk parity are enabled. For trace T1, which is characterized by idle periods with low variability, scrubbing performs better with the body-based policy, while parity updates are more efficient under the tail-based policy. In addition to the body-based and tail-based policies, we also evaluate a "hybrid" scheduling policy that schedules scrubbing work via the body-based policy and parity updates via the tail-based policy. This policy is called the "body+tail" policy.

Table 9 presents the MTTDL improvement when scrubbing and intra-disk parity co-exist under the three scheduling policies. As expected, for T1, which has idle times of low variability, the body+tail policy achieves the best MTTDL improvement. Most importantly, the new combined policy provides an 8-order-of-magnitude improvement in MTTDL. Compared to running scrubbing and parity updates individually, using both features results in dramatically higher improvements while keeping the degradation of foreground performance within predefined limits.

**Table 9. MTTDL Improvement for Trace T1 (Low Variability) via Scrubbing and Intra-Disk Parity**

| Policy         | MTTDL Improvement |
|----------------|--------------------|
| Body-Based     | ...                |
| Tail-Based     | ...                |
| Body+Tail      | ...                |

Figure 5(a) shows the average time for a complete scrubbing when run individually and when merged with parity updates. If the body-based policy is used to schedule both types of background jobs, the performance degradation on scrubbing is significant. With the body+tail variation, each background activity (i.e., scrubbing or parity update) is scheduled using the policy under which it performs best. Parity updates, having higher priority than scrubbing, are not penalized as much as scrubbing (see Figure 5(b)). Furthermore, parity updates perform significantly better if they are scheduled using the tail-based policy, regardless of how scrubbing is scheduled.

**Figure 5. Average Time for (a) an Entire Scrubbing, (b) Parity Updates for Trace T1 (Low Variability)**

Figure 6 shows the overall system utilization for trace T1. The results are consistent with those shown in Figure 5: the body+tail policy utilizes the entire system most effectively, providing room for both scrubbing and parity updates to provide significant performance improvements.

**Figure 6. Overall System Utilization for Trace T1 (Low Variability) via Scrubbing and Intra-Disk Parity**

#### 8.3 Results for Trace T2

Next, we present the results for T2, which has highly variable idle times. For this trace, both scrubbing and parity updates individually perform better using the tail-based policy. Table 10 gives the MTTDL improvement attributed to scrubbing and intra-disk parity under this policy only. For the four variants of trace T2, the background activities dramatically improve the system reliability by improving its MTTDL by up to 8 orders of magnitude. Consistent with the results shown in Table 9, there are gains of at least 3 orders of magnitude in MTTDL.

**Table 10. MTTDL Improvement for Trace T2 (High Variability) via Scrubbing and Intra-Disk Parity**

| Policy         | MTTDL Improvement |
|----------------|--------------------|
| Tail           | ...                |

Figures 7(a) and 7(b) show the average scrubbing and parity update times. For comparison, the results of only disk scrubbing and only intra-disk parity are also included. For the case of scrubbing, all variants of trace T2 perform similarly because scrubbing is workload-independent. Although scrubbing has lower priority than intra-disk parity updates, enabling it concurrently with parity updates does not affect its performance considerably (i.e., only 10% WRITEs in the worst case). Similarly, parity updates see minimal change in their performance because they have higher priority than scrubbing. The only exception is the case with the smallest amount of parity updates (i.e., only 1% user WRITEs). As discussed in Section 7, the effect of parity updates on user traffic performance is almost zero for this case, and parity update times are the smallest. However, adding the infinite scrubbing work degrades parity update performance by up to 3 times.

**Figure 7. Average (a) Scrubbing and (b) Parity Update Times When Running Individually and Together**

Figure 8 shows the overall system utilization, which is dominated by the work done for scrubbing. Because the work related to parity updates is small, its completion barely adds to the system utilization. It is scrubbing, with its infinite amount of work, that keeps the system continuously utilized.

**Figure 8. Overall System Utilization Under Scrubbing and Parity Updates When They Run Individually and Together**

### Conclusions

In this paper, we evaluate the effectiveness of data loss prevention techniques, such as disk scrubbing and intra-disk data redundancy, when their execution should not affect user performance beyond predefined bounds. Our trace-driven evaluation indicates that treating these features as strictly background features and scheduling them during idle times, guided by advanced idleness management techniques, achieve the goal of maintaining user performance degradation at a minimum while significantly improving the storage system's MTTDL. Specifically, scrubbing improves MTTDL by up to five orders of magnitude, and intra-disk data redundancy improves MTTDL by up to two orders of magnitude. Running both features concurrently yields further gains in MTTDL, up to eight orders of magnitude, without violating user performance constraints. These results indicate that these two features complement each other and significantly improve data availability and reliability in the storage system while remaining strictly low-priority features.

### References

[1] Lakshmi N. Bairavasundaram, Garth R. Goodson, Shankar Pasupathy, and Jiri Schindler. An analysis of latent sector errors in disk drives. SIGMETRICS Perform. Eval. Rev., 35(1):289-300, 2007.

[2] M. Baker, M. Shah, D. S. H. Rosenthal, M. Roussopoulos, P. Maniatis, T. J. Giuli, and P. Bungale. A fresh look at the reliability of long-term digital storage. In Proceedings of European Systems Conference (EuroSys), pages 221-234, April 2006.

[3] A. Dholakia, E. Eleftheriou, X. Y. Hu, I. Iliadis, J. Menon, and K. K. Rao. Analysis of a new intra-disk redundancy scheme for high-reliability RAID storage systems in the presence of unrecoverable errors. Technical report, RZ3652, IBM Research, 2006.

[4] Fred Douglis, P. Krishnan, and Brian N. Bershad. Adaptive disk spin-down policies for mobile computers. In Proceedings of the 2nd USENIX Symposium on Mobile and Location-Independent Computing, pages 121-137, 1995.

[5] L. Eggert and J. D. Touch. Idletime scheduling with preemption intervals. In Proceedings of the International Symposium on Operating Systems Principles (SOSP), pages 249-262, October 2005.

[6] Jon G. Elerath and Michael Pecht. Enhanced reliability modeling of RAID storage systems. In Proceedings of the International Conference on Dependable Systems and Networks (DSN), pages 175-184, 2007.

[7] S. Ghemawat, H. Gobioff, and S. Leung. The Google file system. In Proceedings of ACM Symposium on Operating Systems Principles, pages 29-43, 2003.

[8] R. Golding, P. Bosch, C. Staelin, T. Sullivan, and J. Wilkes. Idleness is not sloth. In Proceedings of the Winter'95 USENIX Conference, pages 201-222, New Orleans, LA, January 1995.

[9] David P. Helmbold, Darrell D. E. Long, Tracey L. Sconyers, and Bruce Sherrod. Adaptive disk spin-down for mobile computers. Mobile Networks and Applications, 5(4):285-297, 2000.

[10] Hai Huang, Wanda Hung, and Kang G. Shin. FS2: dynamic data replication in free disk space for improving disk performance and energy consumption. In Proceedings of the twentieth ACM symposium on Operating systems principles (SOSP), pages 263-276, 2005.

[11] Gordon F. Hughes and Joseph F. Murray. Reliability and security of RAID storage systems and D2D archives using SATA disk drives. ACM Transactions on Storage, 1(1):95-107, 2005.

[12] Virginia Mary Lo, Daniel Zappala, Dayi Zhou, Yuhong Liu, and Shanyu Zhao. Cluster computing on the fly: P2P scheduling of idle cycles in the internet. In the 3rd International Workshop on Peer-to-Peer Systems (IPTPS), pages 227-236, 2004.

[13] C. Lueth. RAID-DP: Network Appliance implementation of RAID double parity for data protection. Technical report, Technical Report No. 3298, Network Appliance Inc, 2004.

[14] N. Mi, A. Riska, Q. Zhang, E. Smirni, and E. Riedel. Efficient management of idleness in systems. Proceedings of SIGMETRICS, 35(1):371-372, 2007.

[15] N. Mi, A. Riska, Q. Zhang, E. Smirni, and E. Riedel. Efficient utilization of idle times. Technical report, WM-CS-2008-01, Department of Computer Science, William and Mary, 2008.

[16] D. A. Patterson, G. Gibson, and R. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD Conference, pages 109-116. ACM Press, 1988.

[17] A. Riska and E. Riedel. Disk drive level workload characterization. In Proceedings of the USENIX Annual Technical Conference, pages 97-103, May 2006.

[18] Bianca Schroeder and Garth A. Gibson. Disk failures in the real world: what does an MTTF of 1,000,000 hours mean to you? In Proceedings of the 5th conference on USENIX Conference on File and Storage Technologies (FAST), pages 1-1, 2007.

[19] T. J. E. Schwarz, Q. Xin, E. L. Miller, D. D. E. Long, A. Hospodor, and S. Ng. Disk scrubbing in large archival storage systems. In Proceedings of the International Symposium on Modeling and Simulation of Computer and Communications Systems (MASCOTS), pages 409-418, 2004.

[20] S. Shah and J. G. Elerath. Reliability analysis of disk drive failure mechanism. In Proceedings of 2005 Annual Reliability and Maintainability Symposium, pages 226-231. IEEE, January 2005.

[21] Qi. Zhang, N. Mi, E. Smirni, A. Riska, and E. Riedel. Evaluating the performability of systems with background jobs. In Proceedings of the International Conference on Dependable Systems and Networks (DSN), pages 495-504, 2006.

---

This version of the text is more structured, clear, and professional, with improved readability and coherence.