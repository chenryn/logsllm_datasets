### Degraded State in Performability Modeling

**State Definitions:**
- **1DownShort:** An instance is being restarted from an Application Server (AS) failure. This is a working state.
- **1DownLong:** An instance is being restarted from a Hardware (HW) or Operating System (OS) failure. This is also a working state.
- **2_Down:** Both instances are down, indicating a failure state.

**Transitions and Rates:**
The transitions and associated rates at which the system moves between states are clearly illustrated in the diagram. The overall failure rate (\(\lambda\)) for an AS instance is the sum of the AS failure rate (\(\lambda_{as}\)), HW failure rate (\(\lambda_{hw}\)), and OS failure rate (\(\lambda_{os}\)). When an instance fails, the system enters the Recovery state and remains there for a short time interval \(T_{recovery}\). Subsequently, with the probability of FSS (fraction of short start = \(\lambda_{as}/\lambda\)), it will transition to the 1DownShort state and stay there for a period of \(T_{start\_short}\). Alternatively, with the probability of \(1 - FSS\), it will transition to the 1DownLong state and stay there for a period of \(T_{start\_long}\) before returning to the normal state.

### Analysis of Results

Given the models and parameters presented in the previous sections, the system results for the two modeled configurations generated by RAScad are summarized in Table 2. For both Config 1 and Config 2, the system availability exceeds five 9's.

| Configuration | Availability | Yearly Downtime (YD) | YD due to AS Submodel | YD due to HADB Submodel |
|---------------|--------------|----------------------|-----------------------|-------------------------|
| Config 1      | 99.99933%    | 3.5 min.             | 2.35 min. (67%)       | 1.15 min. (33%)         |
| Config 2      | 99.99956%    | 2.3 min.             | 0.01 sec. (<0.01%)    | 2.3 min. (99.99%)       |

### Model Parameters

Model parameters can be categorized into three groups: failure rates, recovery rates, and coverage (1 - FIR) parameters. Failure rates are difficult to measure accurately within limited time frames through testing and may vary across different customer sites, depending on configuration, workloads, and environmental factors. All failure rates used in the model are varied in the uncertainty analysis discussed later. The Fraction of Imperfect Recovery (FIR) has been estimated to be below 0.1% at the 95% confidence level, based on fault injection data. In the uncertainty analysis, FIR is allowed to go up to 0.2%, which is above the 99.5% confidence level upper bound.

Most recovery times (e.g., automatic restart time, repair time) are deterministic and are measured in lab testing. An exception is the hardware/OS failure recovery time on an AS node, which is largely controllable by customers. High-quality maintenance procedures and deploying a standby AS node can minimize this recovery time. The RAScad parametric analysis capability is used to investigate the impact of this parameter on availability. Figures 5 and 6 show the analysis results for the AS node HW/OS failure recovery time (\(T_{start\_long}\) varies from 0.5 to 3 hours).

- **Config 1:** When \(T_{start\_long}\) increases to 2.5 hours, the five 9's availability is no longer retained.
- **Config 2:** Even if \(T_{start\_long}\) increases to 3 hours, the 99.9995% availability is still maintained.

### Uncertainty Analysis

RAScad's advanced analysis capabilities include uncertainty analysis, which performs random sampling from parameter ranges defined by the user. This method addresses questions such as: "Assume we have N systems with each system's parameters selected by randomly sampling from possible ranges in customer sites, what is the average system availability and confidence intervals?"

Selected parameters and their varying ranges are:
- AS failure rate (\(\lambda_{as}\)): 10/year – 50/year
- HADB failure rate (\(\lambda_{hadb}\)): 1/year – 4/year
- OS failure rate (\(\lambda_{os}\)): 0.5/year – 2/year
- HW failure rate (\(\lambda_{hw}\)): 0.5/year – 2/year
- AS HW/OS failure recovery time (\(T_{start\_long}\)): 0.5 – 3 hours
- Fraction of imperfect recovery (FIR): 0 – 0.2%

Uncertainty analysis results for Config 1 and Config 2, generated for a sample size of 1,000, are shown in Figures 7 and 8, respectively.

- **Config 1:** Average yearly downtime for 1,000 systems is 3.8 minutes, with an 80% confidence interval of (1.9 min., 6.0 min.). Over 80% of sampled systems have yearly downtime less than 5.25 minutes, or above the 99.999% availability level.
- **Config 2:** Average yearly downtime is 3 minutes, with an 80% confidence interval of (1.0 min., 5.2 min.). Over 90% of the sampled systems have yearly downtime less than 5.25 minutes, or above the 99.999% availability level.

### Comparison of Configurations

Table 3 compares system availability for different configurations. Observations from the results:

- **Availability Improvement:** Significantly improved from a 1-instance configuration to a 2-instance configuration, demonstrating the effectiveness of redundancy and failover provisions in JSAS EE7.
- **Optimal Configuration:** The configuration with 4 AS instances and 4 HADB node pairs is optimal in terms of availability.

| # of Instances | # of HADB Pairs | Availability | Yearly Downtime | MTBF (hr.) |
|----------------|-----------------|--------------|-----------------|------------|
| 1              | N/A             | 99.9629%     | 195 min.        | 168        |
| 2              | 2               | 99.99933%    | 3.49 min.       | 89,980     |
| 4              | 4               | 99.99956%    | 2.29 min.       | 229,326    |
| 6              | 6               | 99.99934%    | 3.44 min.       | 152,889    |
| 8              | 8               | 99.99912%    | 4.58 min.       | 114,669    |
| 10             | 10              | 99.99891%    | 5.73 min.       | 91,736     |

### Conclusions

This paper demonstrates a measurement-based availability evaluation approach for Sun Java System Application Server, Enterprise Edition 7. Both hardware permanent faults and software transient faults, as well as workload-dependent failure rates, were considered in the model. Extensive lab measurements were conducted to estimate model parameters. The model was developed using the widely accepted methodology with RAScad. Under conservative assumptions, the average system availability was evaluated to be at the 99.999% level in the Solaris and Sun server environment. Uncertainty analysis provided availability confidence intervals, confirming that the configuration with four AS instances and four pairs of HADB nodes is optimal in terms of availability. These results are useful for planning data centers and web services deployments.

### Acknowledgments

The authors would like to thank William Shannon, Richard Sharples, David Van Couvering, Lawrence White, and Masood Mortazavi for their valuable comments. Thanks are also due to Kenneth Chan and Suveen Nadipalli for early model development and providing test data. Special thanks to William Franklin, Timothy Cramer, Madhu Konda, and Roy Andrada for their support and constructive input to the model.

### References

[1] A. Brown and D. A. Patterson, "To Err is Human," Proceedings of the First Workshop on Evaluating and Architecting System dependability (EASY 01), Göteborg, Sweden, July 2001.

[2] G. Ciardo, J. Muppala, and K. S. Trivedi, "SPNP: Stochastic Petri Net Package," International Conference on Petri Nets and Performance Models, 1989.

[3] A. Goyal, S. S. Lavenberg, and K. S. Trivedi, "Probabilistic Modeling of Computer System Availability," Annals of Operations Research, No. 8, March 1987, pp. 285-306.

[4] M. C. Hsueh and R. K. Iyer, "Performability Modeling Based on Real Data: A Case Study," IEEE Transactions on Computers, April 1988, pp. 478-484.

[5] R. K. Iyer and D.J. Rossetti, "Effect of System Workload on Operating System Reliability: A Study on IBM 3081," IEEE Transactions on Software Engineering, Dec. 1985, pp. 1438-1448.

[6] M. Kaaniche, K. Kanoun, and M. Martinello, "A User-Perceived Availability Evaluation of a Web-Based Travel Agency," Proceedings of the International Conference on Dependable Systems and Networks (DSN-2003), San Francisco, June 2003.

[7] K. Kanoun, M. Kaaniche, and J. C. Laprie, "Qualitative and Quantitative Reliability Assessment," IEEE Software, March/April 1997, pp. 77-87.

[8] D. Kececioglu, Reliability and Life Testing Handbook, Vol. 1 & 2, PTR Prentice Hall, Englewood Cliffs, NJ, 1993.

[9] I. Lee, D. Tang, R. K. Iyer, and M. C. Hsueh, "Measurement-Based Evaluation of Operating System Fault Tolerance," IEEE Transactions on Reliability, June 1993, pp. 238-249.

[10] M. R. Lyu, Editor, Handbook of Software Reliability Engineering, McGraw-Hill, New York, 1996.

[11] Oystein Torbjornsen, Multi-Site Declustering Strategies for Very High Database Service Availability, Dr. Ing. Thesis, Division of Computer Science and Telematics, The Norwegian Institute of Technology, University of Trondheim, Norway, 1995.

[12] I. Pramanick, "Modeling Sun Cluster Availability," Sun Users Performance Group Conference, SUPerG-2002, San Francisco, 2002.

[13] R. A. Sahner and K. S. Trivedi, "Reliability Modeling Using SHARPE," IEEE Transactions on Reliability, Feb. 1987, pp. 186-193.

[14] W. H. Sanders, W. D. Obal II, M. A. Qureshi, and F. K. Widjanarko, "The UltraSAN Modeling Environment," Performance Evaluation, Oct./Nov. 1995, pp. 89-115.

[15] Sun Microsystems, Sun Java System Application Server, Enterprise Edition 7, White Paper, August 2003, available at http://wwws.sun.com/software/products/appsrvr_ee/home_appsrvr_ee.html

[16] D. Tang and M. Hecht, "Evaluation of Software Dependability Based on Stability Test Data," Proceedings of the 25th International Symposium on Fault-Tolerant Computing (FTCS-25), June 1995, pp. 434-443.

[17] D. Tang, J. Zhu, and R. Andrada, "Automatic Generation of Availability Models in RAScad," Proceedings of the International Conference on Dependable Systems and Networks (DSN-2002), June 2002, pp. 488-492.

[18] D. Tang and K. S. Trivedi, "Hierarchical Evaluation of Interval Availability in RAScad," Proceedings of the International Conference on Dependable Systems and Networks (DSN-2004), Florence, Italy, June 2004.

[19] K. S. Trivedi, Probability & Statistics with Reliability, Queuing, and Computer Science Applications, Second Edition, John Wiley & Sons, Inc., New York, 2002.

[20] M. Vieira and H. Madeira, "Benchmarking the Dependability of Different OLTP Systems," Proceedings of the International Conference on Dependable Systems and Networks (DSN-2003), San Francisco, June 2003.

---

**Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04)**  
**0-7695-2052-9/04 $ 20.00 © 2004 IEEE**  
**Authorized licensed use limited to: Tsinghua University. Downloaded on March 20, 2021, at 10:03:23 UTC from IEEE Xplore. Restrictions apply.**