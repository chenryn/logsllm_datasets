### 7.2 Adversarial Patch Defenses

Empirical defenses, such as Digital Watermark (DW) [20] and Local Gradient Smoothing (LGS) [39], were initially proposed to detect and neutralize adversarial patches. However, these heuristic defenses are vulnerable to adaptive attackers who have knowledge of the defense mechanisms.

Recognizing the limitations of DW and LGS, Chiang et al. [9] introduced the first provable defense against adversarial patches using Interval Bound Propagation (IBP) [18, 38]. Despite its significant theoretical contribution, the IBP defense exhibits poor clean and provable robust accuracy, as illustrated in Table 4. Zhang et al. [59] proposed Clipped BagNet (CBN) for provable robustness, and Levine et al. [28] suggested constructing a 'smoothed' classifier (DS) that outputs the class with the highest count from local predictions on all small pixel patches.

We have demonstrated that CBN and DS are instances of our general defense framework (Section 6.1), and PatchGuard outperforms them due to the use of robust masking (Section 5.2). The Minority Report (MR) [34] defense, proposed concurrently, involves placing a mask at all possible locations and extracting patterns from model predictions. While MR can only provably detect an attack, PatchGuard ensures the recovery of the correct prediction. Additionally, MR's masking in the image space is computationally expensive and does not scale well to high-resolution images. However, for low-resolution images, MR achieves higher clean accuracy (90.6%) and provable accuracy (62.1% for a 2.4%-pixel patch on CIFAR-10) compared to PatchGuard (84.6% clean accuracy and 57.7% provable accuracy). Extending PatchGuard for attack detection is an interesting direction for future work.

Another concurrent line of research has focused on adversarial patch training [44, 54]. These works, however, emphasize empirical robustness and do not provide provable guarantees.

### 7.3 Receptive Fields of CNNs

Several studies have examined the influence of the receptive field [1, 5, 25, 32] on model performance to better understand model behavior. BagNet [5] adopted the structure of ResNet-50 [21] but reduced the receptive field size by replacing 3×3 kernels with 1×1 kernels. BagNet-17 achieved similar top-5 validation accuracy as AlexNet [24] on the ImageNet [12] dataset when each feature only considers a 17×17 pixel region. The smaller receptive field was used for better interpretability of model decisions in the original BagNet paper. In this work, we leverage the reduced receptive field size to create models that are robust to adversarial patch attacks.

### 7.4 Other Adversarial Example Attacks and Defenses

The development of adversarial example-based attacks and defenses has been a highly active research area in recent years. Conventional adversarial attacks [8, 17, 41, 50] craft adversarial examples that have a small Lp distance to clean examples but cause model misclassification. Many empirical defenses [35, 36, 42, 56] have been proposed to address this vulnerability, but most can be easily bypassed by strong adaptive attackers [2, 7, 52].

The fragility of empirical defenses has inspired the development of provable or certified defenses [10, 18, 26, 38, 43, 53] and work on learning-theoretic bounds in the presence of adversaries [3, 11, 13, 45, 57]. In contrast, this paper focuses on localized adversarial patch attacks. For a more detailed background on adversarial examples, we refer interested readers to survey papers [40, 58].

### 8 Conclusion

In this paper, we propose a general provable defense framework called PatchGuard, which mitigates localized adversarial patch attacks. We identify large receptive fields and insecure aggregation mechanisms in conventional CNNs as key sources of vulnerability to adversarial patches. To address these issues, our defense uses models with small receptive fields to limit the number of features corrupted by the adversary and employs robust masking to detect and mask the corrupted features, ensuring secure feature aggregation. Our defense achieves state-of-the-art provable robust accuracy on the ImageNet, ImageNette, and CIFAR-10 datasets. We hope that our general defense framework will inspire further research to fully mitigate adversarial patch attacks.

### Acknowledgements

We are grateful to David Wagner for shepherding the paper and to anonymous reviewers at USENIX Security for their valuable feedback. This work was supported in part by the National Science Foundation under grants CNS-1553437 and CNS-1704105, the ARL’s Army Artificial Intelligence Innovation Institute (A2I2), the Office of Naval Research Young Investigator Award, the Army Research Office Young Investigator Prize, a Faculty research award from Facebook, the Schmidt DataX award, and the Princeton E-filiates Award.

### References

[References listed here, formatted consistently with the rest of the document]

---

This revised version aims to improve clarity, coherence, and professionalism, making it easier to read and understand.