### References

1. **SON, C., AND KIRDA, E.** (2017). *Thou Shalt Not Depend on Me: Analyzing the Use of Outdated JavaScript Libraries on the Web*. In Proceedings of the Network and Distributed System Security Symposium (NDSS).

2. **LI, Z., ZHANG, K., XIE, Y., YU, F., AND WANG, X.** (2012). *Knowing Your Enemy: Understanding and Detecting Malicious Web Advertising*. In Proceedings of the 2012 ACM Conference on Computer and Communications Security, pp. 674–686.

3. **MICROSOFT CORPORATION.** (2017). *Cognitive Services Pricing – Bing Search API*. https://azure.microsoft.com/en-us/pricing/details/cognitive-services/search-api/web/

4. **MIRAMIRKHANI, N., STAROV, O., AND NIKIFORAKIS, N.** (2016). *Dial One for Scam: Analyzing and Detecting Technical Support Scams*. In 22nd Annual Network and Distributed System Security Symposium (NDSS 16).

5. **NELMS, T., PERDISCI, R., ANTONAKAKIS, M., AND AHAMAD, M.** (2016). *Towards Measuring and Mitigating Social Engineering Software Download Attacks*. In 25th USENIX Security Symposium (USENIX Security 16), pp. 773–789.

6. **NIKIFORAKIS, N., MAGGI, F., STRINGHINI, G., RAFIQUE, M. Z., JOOSEN, W., KRUEGEL, C., PIESSENS, F., VIGNA, G., AND ZANERO, S.** (2014). *Stranger Danger: Exploring the Ecosystem of Ad-Based URL Shortening Services*. In Proceedings of the 23rd International Conference on World Wide Web, pp. 51–62.

7. **NISHANT, D.** (2014). *Survey Scammers Moving to Pinterests*. https://www.symantec.com/connect/blogs/survey-scammers-moving-pinterest

8. **NLTK 3.2.3 DOCUMENTATION.** (2017). *Natural Language Toolkit*. http://www.nltk.org/

9. **OPINION MILES CLUB.** (2017). *Earn Award Miles for Sharing Your Opinions*. https://www.opinionmilesclub.com/

10. **OSCAR, A.** (2012). *Survey Scams Aimed at Social Networking Netizens*. https://www.trendmicro.com/vinfo/us/threat-encyclopedia/web-attack/109/survey-scams-aimed-at-social-networking-netizens

11. **PASGRIMAUD, G.** (2017). *Pyquery: A jQuery-Like Library for Python*. https://pythonhosted.org/pyquery/

12. **POLETTINI, N.** (2004). *The Vector Space Model in Information Retrieval - Term Weighting Problem*.

13. **PROVOS, N., MAVROMMATIS, P., RAJAB, M. A., AND MONROSE, F.** (2008). *All Your Iframes Point to Us*. In Proceedings of the 17th Conference on Security Symposium, pp. 1–15.

14. **SALTON, G., AND MCGILL, M. J.** (1986). *Introduction to Modern Information Retrieval*. McGraw-Hill, Inc., New York, NY, USA.

15. **SALTON, G., WONG, A., AND YANG, C. S.** (1975). *A Vector Space Model for Automatic Indexing*. In ACM, pp. 613–620.

16. **SATNAM, N.** (2013). *Instascam: Instagram for PC Leads to Survey Scam*. https://www.symantec.com/connect/blogs/instascam-instagram-pc-leads-survey-scam

17. **SCIKIT LEARN.** (2017). *Random Forest Algorithm*. http://scikit-learn.org/stable/modules/ensemble.html#random-forests

18. **SECRECTS, M. M.** (2016). *Earn Free United Miles if You Have a Lot of Spare Time, That Is!* http://millionmilesecrets.com/2014/02/27/earn-free-united-miles-if-you-have-a-lot-of-spare-time-that-is/

19. **SPRINGBORN, K., AND BARFORD, P.** (2013). *Impression Fraud in On-line Advertising via Pay-Per-View Networks*. In USENIX Security, pp. 211–226.

20. **STANFORD UNIVERSITY.** (2009). *The Vector Space Model for Scoring*. https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html

21. **STAROV, O., GILL, P., AND NIKIFORAKIS, N.** (2016). *Are You Sure You Want to Contact Us? Quantifying the Leakage of PII via Website Contact Forms*. Proceedings on Privacy Enhancing Technologies 2016, 1, pp. 20–33.

22. **STELIAN, P.** (2017). *Remove 2017 Annual Visitor Survey Pop-up Ads (Virus Removal Guide)*. https://malwaretips.com/blogs/remove-2017-annual-visitor-survey-popups/

23. **STELIAN, P.** (2017). *Remove Chrome Opinion Survey Pop-ups (Virus Removal Guide)*. https://malwaretips.com/blogs/remove-chrome-opinion-survey-popup/

24. **STONE-GROSS, B., ABMAN, R., KEMMERER, R. A., KRUEGEL, C., STEIGERWALD, D. G., AND VIGNA, G.** (2013). *The Underground Economy of Fake Antivirus Software*. In Economics of Information Security and Privacy III, pp. 55–78.

25. **THE WORLD WIDE WEB CONSORTIUM (W3C).** (2012). *HTTP Archive (HAR) Format*. https://dvcs.w3.org/hg/webperf/raw-file/tip/specs/HAR/Overview.html

26. **THOMAS, K., BURSZTEIN, E., GRIER, C., HO, G., JAGPAL, N., KAPRAVELOS, A., MCCOY, D., NAPPA, A., PAXSON, V., PEARCE, P., ET AL.** (2015). *Ad Injection at Scale: Assessing Deceptive Advertisement Modifications*. In Security and Privacy (SP), 2015 IEEE Symposium on, pp. 151–167.

27. **THOMAS, K., CRESPO, J.-M., PHILLIPS, C., DECOSTE, M.-A., SHARP, C., TIRELO, F., TOFIGH, A., COURTEAU, M.-A., BALLARD, L., SHIELD, R., JAGPAL, N., RAJAB, J. A. E., RASTI, R., PICOD, M. A., MAVROMMATIS, P., PROVOS, N., BURSZTEIN, E., AND MCCOY, D.** (2016). *Investigating Commercial Pay-Per-Install and the Distribution of Unwanted Software*. In 25th USENIX Security Symposium (USENIX Security 16), pp. 721–739.

28. **THOMAS, K., CRESPO, J. A. E., RASTI, R., PICOD, J. M., PHILLIPS, C., DECOSTE, M.-A., SHARP, C., TIRELO, F., TOFIGH, A., COURTEAU, M.-A., ET AL.** (2016). *Investigating Commercial Pay-Per-Install and the Distribution of Unwanted Software*. In USENIX Security Symposium, pp. 721–739.

29. **VADREVU, P., RAHBARINIA, B., PERDISCI, R., LI, K., AND ANTONAKAKIS, M.** (2013). *Measuring and Detecting Malware Downloads in Live Network Traffic*. In European Symposium on Research in Computer Security, pp. 556–573.

30. **VISSERS, T., JOOSEN, W., AND NIKIFORAKIS, N.** (2015). *Parking Sensors: Analyzing and Detecting Parked Domains*. In Annual Network and Distributed System Security Symposium, The Internet Society.

31. **WANG, Z., BOVIK, A. C., SHEIKH, H. R., AND SIMONCELLI, E. P.** (2004). *Image Quality Assessment: From Error Visibility to Structural Similarity*. IEEE Transactions on Image Processing 13, 4, pp. 600–612.

32. **WHALEY, B.** (1982). *Toward a General Theory of Deception*. The Journal of Strategic Studies 5, 1, pp. 178–192.

33. **XING, X., MENG, W., LEE, B., WEINSBERG, U., SHETH, A., PERDISCI, R., AND LEE, W.** (2015). *Understanding Malvertising Through Ad-Injecting Browser Extensions*. In Proceedings of the 24th International Conference on World Wide Web (WWW '15).

34. **ZARRAS, A., KAPRAVELOS, A., STRINGHINI, G., HOLZ, T., KRUEGEL, C., AND VIGNA, G.** (2014). *The Dark Alleys of Madison Avenue: Understanding Malicious Advertisements*. In Proceedings of the 2014 Conference on Internet Measurement Conference, pp. 373–380.

35. **ZAUNER, C.** (2010). *Implementation and Benchmarking of Perceptual Image Hash Functions*. http://www.phash.org/

### Appendix

#### A. Third-Party Inclusions in Survey Gateways

To motivate our use of third-party script incidence as a feature, we compared the usage of third-party scripts (e.g., advertisements) in survey gateways to benign survey pages. Figure 8 shows the number of unique third-party scripts used as advertisements on survey gateways versus the number of unique third-party scripts referenced by the baseline benign survey pages. The plot clearly shows that survey gateways include significantly more third-party scripts than benign survey pages.

**Figure 8:** Number of included third-party scripts in the survey scam pages.

#### B. Interacting with Survey Gateways

Our preliminary experiment on 10 survey gateways showed that the transmitted data to servers varies if different browser configurations and IP addresses are used, even when providing identical responses to the initial set of questions (e.g., age, gender). To better explore this, we conducted a larger-scale study on 200 randomly selected survey gateways to infer potential information flow into survey gateways.

Our analysis consists of two phases. In the first phase, we visited each survey gateway multiple times with an identical browser profile (i.e., same IP address and browser user-agent) and collected raw network traces. The goal of this phase is to construct a complete picture of the network behavior of a given survey gateway. Note that determining the number of times to visit a gateway website to draw a comprehensive view of its network behavior strongly depends on the complexity of network traces between the survey gateway and the browser. For example, the number and sources of non-deterministic parameters usually vary from website to website and can have significant impacts on our analysis in this experiment.

By performing a differential analysis on the collected traces, we empirically observed that running the first phase of the experiment three times is sufficient to reach convergence and identify potential discrepancies in the traces for a large number of websites in our dataset.

To this end, we collected the raw HTTP traffic sent to survey gateways and also monitored their interactions with browser features such as WebStorage APIs (i.e., LocalStorage, SessionStorage). We then checked the raw HTTP traffic and searched for values by string comparison to find any potential sources of non-determinism. We used similar techniques that prior work [40] employed to extract specific values in the network traffic. In fact, we labeled any parameters that varied during the first phase in which we did not alter any source of information. We then combined all the traces and defined the behavior summary of the given survey gateway, which was then used in the second phase of the experiment. In the second phase, after visiting the survey gateway with a different browser user-agent, we monitored the information sent to the server to identify potential sources of non-determinism. Figure 9 illustrates a simplified version of our experiment on one of the survey gateways.

From the 200 randomly selected survey gateways, we observed that 144 (72%) of them were interacting with browser WebStorage APIs by calling set and get functions. However, 112 (56%) of all the websites did not reach convergence during the first phase of our analysis, or we were not able to extract any particular patterns due to multiple levels of encoding.

Our empirical analysis on the network traces shows that among all the remaining 88 (44%) cases, the most common sources of non-determinism were timestamps, dates, cookies, or session identifiers assigned by gateways. This experiment clearly implies that the interaction with survey gateways relies on creating and maintaining unique IDs for visiting users. In fact, survey gateways differentiate among visiting users and redirect them to survey publishers based on their browser configuration and responses to the initial set of questions. However, we cannot claim that the interaction between survey gateways and publishers relies solely on the set of unique IDs that we observed by interacting with survey gateways. Furthermore, we do not know whether adversaries use any specific technique besides the user-agent analysis, nor have we investigated enough to make statistically significant claims about any particular types of fingerprinting techniques.

**Figure 9:** An example of how we analyzed the browser interaction with survey gateways. In the first phase, we create a behavior summary of a given survey gateway by visiting the website n times and locating non-deterministic parameters. In the second phase, we perform a differential analysis by changing the browser setting and IP address to identify differences in HTTP traffic.

#### C. Exposing Victims to Scam Pages After Filling Out a Survey Page

After completing a survey, victims are redirected to a new page and are usually asked to enter sensitive information, such as a credit card number, along with other information to receive rewards. Figure 10 shows an example of a scam page after completing a survey.

**Figure 10:** An example of a scam page that a user is exposed to after filling out a survey.