### 图表说明

**图4. SE服务的ROC和P-R曲线。显著性水平α = 0.01的点已标出。**

- (b) sign
- (c) LOF

**图5. 可疑虚拟机（黑色）与其他14台机器（灰色）的计数器对比。**

- (a) 检测日
- (b) 警告日

**图6. LG服务的检测性能。至少20-25%的故障有前兆隐性故障。显著性水平α = 0.01的点已标出。**

### 异常计数器分析

对于过载的机器，相关的计数器如图5所示。第二台检测到隐性故障的机器似乎没有明显的警告信号，但测试显示其内存使用率较低，与其他执行相同任务的机器相比。

### 隐性故障数量估计

某些故障在系统中未被发现的阶段较短，例如由于软件升级或网络服务中断引起的故障。我们在LG环境中进行了一项实验，旨在估计具有隐性阶段的故障所占的比例。

我们随机选择了80个故障事件，并检查我们的方法是否能在现有故障检测机制报告之前24小时检测到这些故障。作为对照，我们还随机选择了一组73台已知健康的机器。对于两组，要求事件来自不同的机器，并且覆盖不同时间和日期。

在该实验中，我们将“故障机器”定义为在前48小时内没有故障报告但在当前时刻被报告为故障的机器。“健康机器”则是在60天调查期内没有任何故障记录的机器。图6展示了该实验的ROC曲线。检测到隐性故障的故障机器被视为真正例，而被标记为可疑的健康机器被视为假正例。sign和Tukey两种方法均能以非常低的假正例率检测到20%-25%的故障机器。因此，我们得出结论：至少20%-25%的故障在很长一段时间内是隐性的。假设我们的估计准确，第四部分-B中实现的召回率接近最大可能值。

### 测试比较

本文提出的三种测试基于不同的原理，但它们往往标记相同的机器。例如，超过80%的被Tukey标记的机器也被sign测试标记。所有测试在所有服务上都实现了低假正例率，其中Tukey和sign测试达到了用户指定的极低率参数。为了更好地描述不同测试的敏感度，我们在人工生成的数据上进行了评估，注入了三种类型的“故障”：计数器位置（偏移）、计数器尺度或两者兼有。不同故障机器之间的差异强度不同，我们比较了每种测试对不同类型故障的敏感度。结果如图7所示。实验表明，sign测试对偏移变化非常敏感。LOF对偏移变化有一定敏感度，而Tukey测试对此类变化几乎没有敏感度。当尺度发生变化时，LOF在低假正例率范围内更敏感，但随后表现不佳。Tukey对尺度变化比sign更敏感。

### 计数器预处理过滤

如第二节-D所述，预处理阶段会删除一些计数器。表IV报告了每个服务中平均删除的计数器数量。

在删除违反无记忆假设的计数器时，我们测量了每个计数器在所有机器上的平均变异性，仅保留变异性较低的计数器。我们选择一个较低的固定阈值，这是出于保守设计的选择，以避免假正例，即使这意味着删除可能有用的计数器。图8证明了这一选择：大多数未被过滤的计数器在大多数服务上具有相对较低的变异性，而较高变异性范围（2-10）通常包含较少的计数器。超过10的计数器不可用：大多数情况下，这些计数器对于每台机器实际上是唯一的常数值。因此，预处理阶段不需要调整。

为进一步探索不同阈值的影响，我们测量了在LG服务的单日中，不同平均变异性阈值下的测试性能。在严格的显著性水平下，较高的阈值会导致略微更好的召回率，但精度略有下降，符合我们的预期。

### 相关工作

近年来，自动机器故障检测问题得到了多位研究人员的关注，迄今为止提出的技术大多是有监督的，或者依赖于文本控制台日志。

Chen等人[5]分析了测量集之间的相关性并随时间跟踪这些相关性。这种方法需要领域知识来选择计数器，并进行训练以建立基线相关性模型。Chen等人[8]提出了基于学习决策树的有监督方法。该系统需要故障示例和领域知识。此外，有监督方法对工作负载变化和平台变更适应性较差。Pelleg等人[22]使用决策树探索虚拟机中的故障检测。尽管基础是领域无关的，但该系统是有监督的，需要在标记示例上进行训练并手动选择计数器。Bronevetsky等人[23]监控MPI应用程序的状态转换，并通过观察状态转换的时间和概率来构建统计模型。他们的方法不需要领域知识，但仅限于基于MPI的应用程序，并且需要潜在的侵入式监控。此外，还需要在被监控应用程序的样本运行上进行训练以实现高准确性。Sahoo等人[7]比较了三种故障事件预测方法：基于规则的方法、贝叶斯网络和时间序列分析。他们成功地将这些方法应用于一个350节点集群，持续一年。他们的方法是有监督的，并且进一步依赖于对被监控系统的大量了解。Bodík等人[4]从聚合计数器生成指纹，描述整个数据中心的状态，并使用这些指纹识别系统危机。与其它有监督技术一样，该方法需要标记示例。作者展示了已经发生的系统故障的快速检测，而我们专注于在机器故障发生前检测隐性故障。Cohen等人[9]引入了一个树增强的贝叶斯网络分类器。虽然这种方法除了标记训练集外不需要领域知识，但分类器对工作负载变化敏感。模型集成用于[24]减少前者方法对工作负载变化的敏感性，但代价是当故障类型过多时准确性降低[25]。

Palatin等人[1]建议向服务器发送基准测试以找到执行异常值。像我们的方法一样，他们的方法基于异常检测，是无监督的，不需要领域知识。然而，通过与系统架构师的互动，我们了解到他们认为这种方法是侵入式的，因为它需要向被监控主机发送作业，从而实际上修改了运行的服务。Kasick等人[13]使用无监督直方图和基于阈值的技术分析选定的计数器。他们关于同质平台和工作负载的假设与我们相似。但他们只考虑分布式文件系统，依赖专家见解和精心选择的计数器。我们的技术不需要任何知识，并适用于所有领域。

还有几种无监督的文本控制台日志分析方法。Oliner等人[10]提出了Nodeinfo：一种无监督方法，通过假设类似机器产生类似日志来检测系统消息中的异常。Xu等人[12]分析源代码以解析控制台日志消息，并使用主成分分析来识别异常消息模式。Lou等人[11]通过识别控制台日志消息计数中的线性关系来表示代码流。与[10]、[11]不同，我们的方法具有强大的统计基础，可以保证性能，并且不需要调优。这三种技术都关注文本消息的异常出现，而我们的方法关注周期事件的数值。此外，我们专注于硬件或软件隐性故障的早期检测。最后，大型服务中的控制台日志分析在高交易量的情况下是不可行的。

### 结论

虽然目前的方法主要集中在已经发生的故障的识别上，但隐性故障表现为某些机器计数器的异常，这些异常最终将导致实际故障。尽管我们的实验表明，即使在管理良好的数据中心中，隐性故障也很常见，但我们是首次解决这个问题。

我们引入了一种新的框架来检测隐性故障，该框架足够灵活，可以在不同系统中使用，并且能够承受随时间的变化。我们证明了误检率的保证，并在几种生产服务类型上评估了我们的方法。我们的方法能够在基于规则的监视器之前几天甚至几周检测到许多隐性故障。我们已经证明，我们的方法是多功能的；相同的测试能够在不同的环境中检测故障，而无需重新训练或调整。我们的测试自然地处理工作负载变化和服务更新，无需干预。即使是基于虚拟机的服务也可以在不进行任何修改的情况下成功监控。我们的方法的可扩展性允许基础设施管理员添加尽可能多的服务敏感事件计数器。监控过程中的其他一切都将自动处理，无需进一步调整。

在一个更大的背景下，本文探讨了一个开放性问题：大型基础设施是否应该准备从“不可避免的故障”中恢复，正如通常建议的那样。即使存在先进的恢复机制，但由于在实时环境中测试的风险，这些机制通常不会被测试。事实上，大规模系统的高级恢复（超出基本故障转移）测试极其复杂且容易失败，很少涵盖所有故障场景。因此，Amazon EC2、Google搜索引擎、Facebook，甚至是2003年的东北部停电，都被归因于相互干扰的级联恢复过程。可以设想，有些大型系统的恢复过程从未得到适当的测试。

类似于[3]，我们提出了另一种方法：主动处理隐性故障可以大大减少对恢复过程的需求。因此，我们认为这项工作是朝着更灵敏的监控机制迈出的一步，这将带来可靠的大型服务。

### 致谢

本研究是在Moshe Gabel和Assaf Schuster访问微软研究院期间进行的。作者感谢欧盟LIFT项目的支持，该项目由欧盟FP7计划资助。

### 参考文献

[1] N. Palatin, A. Leizarowitz, A. Schuster, and R. Wolff, “Mining for misconﬁgured machines in grid systems,” in Proc. SIGKDD, 2006.

[2] E. B. Nightingale, J. R. Douceur, and V. Orgovan, “Cycles, cells and platters: An empirical analysis of hardware failures on a million consumer PCs,” in Proc. EuroSys, 2011.

[3] A. B. Nagarajan and F. Mueller, “Proactive fault tolerance for HPC with xen virtualization,” in Proc. ICS, 2007.

[4] P. Bodík, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen, “Fingerprinting the datacenter: Automated classification of performance crises,” in Proc. EuroSys, 2010.

[5] H. Chen, G. Jiang, and K. Yoshihira, “Failure detection in large-scale internet services by principal subspace mapping,” IEEE Trans. Knowl. Data Eng., 2007.

[6] M. Isard, “Autopilot: automatic data center management,” SIGOPS Oper. Syst. Rev., 2007.

[7] R. K. Sahoo, A. J. Oliner, I. Rish, M. Gupta, J. E. Moreira, S. Ma, R. Vilalta, and A. Sivasubramaniam, “Critical event prediction for proactive management in large-scale computer clusters,” in Proc. SIGKDD, 2003.

[8] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer, “Failure diagnosis using decision trees,” in Proc. ICAC, 2004.

[9] I. Cohen, M. Goldszmidt, T. Kelly, and J. Symons, “Correlating instrumentation data to system states: A building block for automated diagnosis and control,” in Proc. OSDI, 2004.

[10] A. J. Oliner, A. Aiken, and J. Stearley, “Alert detection in system logs,” in Proc. ICDM, 2008.

[11] J.-G. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from console logs for system problem detection,” in Proc. USENIXATC, 2010.

[12] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting large-scale system problems by mining console logs,” in Proc. SOSP, 2009.

[13] M. P. Kasick, J. Tan, R. Gandhi, and P. Narasimhan, “Black-box problem diagnosis in parallel file systems,” in Proc. FAST, 2010.

[14] C. McDiarmid, “On the method of bounded differences,” Surveys in Combinatorics, 1989.

[15] W. J. Dixon and A. M. Mood, “The statistical sign test,” Journal of the American Statistical Association, 1946.

[16] R. H. Randles, “A distribution-free multivariate sign test based on interdirections,” Journal of the American Statistical Association, 1989.

[17] J. Tukey, “Mathematics and picturing data,” in Proc. ICM, 1975.

[18] T. M. Chan, “An optimal randomized algorithm for maximum Tukey depth,” in Proc. SODA, 2004.

[19] J. A. Cuesta-Albertos and A. Nieto-Reyes, “The random Tukey depth,” Journal of Computational Statistics & Data Analysis, 2008.

[20] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, “LOF: Identifying density-based local outliers,” SIGMOD Rec., 2000.

[21] C. D. Manning, P. Raghavan, and H. Schütze, An Introduction to Information Retrieval. Cambridge University Press, 2008.

[22] D. Pelleg, M. Ben-Yehuda, R. Harper, L. Spainhower, and T. Adeshiyan, “Vigilant: out-of-band detection of failures in virtual machines,” SIGOPS Oper. Syst. Rev., 2008.

[23] G. Bronevetsky, I. Laguna, S. Bagchi, B. R. de Supinski, D. H. Ahn, and M. Schulz, “Statistical fault detection for parallel applications with AutomaDeD,” in Proc. SELSE, 2010.

[24] S. Zhang, I. Cohen, M. Goldszmidt, J. Symons, and A. Fox, “Ensembles of models for automated diagnosis of system performance problems,” in Proc. DSN, 2005.

[25] C. Huang, I. Cohen, J. Symons, and T. Abdelzaher, “Achieving scalable automated diagnosis of distributed systems performance problems,” HP Labs, Tech. Rep., 2007.