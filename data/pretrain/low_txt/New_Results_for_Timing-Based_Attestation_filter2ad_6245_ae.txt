### Potential Vulnerability to TOCTOU Attacks

Trusted computing systems can potentially fall victim to Time-Of-Check to Time-Of-Use (TOCTOU) attacks due to control flow violations that occur at strategic points before or during the attestation code execution. There are three primary requirements for a TOCTOU attack to be successful against a trusted computing system:

1. **Timing Knowledge**: The attacker must know when the measurement is possible after the measurement has finished.

These requirements provide a useful conceptual framework for examining trusted computing systems. If any one of these conditions is not met, the TOCTOU attack will not succeed with 100% probability. We differentiate between guaranteed success TOCTOU attacks and probabilistic TOCTOU attacks because real-world guaranteed attacks more severely undermine the trustworthiness of trusted computing systems. When an attacker is forced into a probabilistic TOCTOU attack, defenders effectively create a race condition for the attacker. This degradation in the attacker's control increases the likelihood of detection. For example, an attacker capturing keystrokes may miss important characters, an attacker hiding files may have them detected by third-party on-access scanning, or an attacker denying execution to security programs by terminating them before they launch could have an execution slip through.

### Countering Requirement 1: Timing Knowledge

**Virtualized Security Systems and SMM**

Virtualized security systems [8, 26, 19] or those using System Management Mode (SMM) [17, 1] typically counter the first requirement by assuming the attacker cannot reside at the same privilege level as the defender (i.e., the hypervisor layer). They utilize the opportunity to freeze the virtual machine (VM) and measure it at intervals unknown to the attacker. However, if measurements are triggered on-demand in response to actions within the virtualized environment, the attacker may remove modifications before the event is triggered. Tools like Copilot [15], which perform measurements from outside the CPU with direct memory access, can also measure memory without the attacker knowing when the measurement will occur.

**Self-Checking Systems**

For self-checking systems where the attacker is assumed to be at the same privilege level as the defender, it is challenging to fully counter the first requirement with current techniques. This is because it is often possible for an attacker to know when and what type of measurement is about to begin. An inline hook placed in the self-check code immediately before it reads its own memory can achieve this. The problem lies in the deterministic control flow paths to the self-check code. Even if runtime control flow integrity is guaranteed, it cannot ensure integrity before the code runs.

**Future Work**

One potential approach to counter this requirement is to augment control flow with a system like TEAS [5]. By dynamically injecting agents, an attacker cannot automatically analyze the code fast enough to recognize the new control flow. These agents could perform the prolog of a self-check, measure the existing self-check agent, and then jump into a random block of the self-check function. This way, the agent pushed to the system just in time can detect code integrity modifications that the existing self-check function cannot.

### Countering Requirement 2: Measurement of Memory

Some approaches have implicitly attempted to counter the second requirement by measuring or proposing to measure all of memory [21, 4, 7]. It was suggested to page out and overwrite all memory not used for verification. However, this assumes that every single page of memory subsequently checked can be validated. In practice, it is difficult to apply whitelisting to dynamically allocated memory pages, which can contain attacker code, and blacklisting sophisticated malware is unlikely.

For desktop systems, this would not require abandoning the kernel but may need augmenting a kernel agent with a smaller root of trust. Systems like PioneerNG, implemented in SMM, are conceptually isolated, but an attacker inside can still access all system memory. Self-checking code in these locations remains vulnerable to inline hooks placed at their start, followed by the attacker removing himself to a safe location in physical memory. A single unmeasured function pointer used by third-party code could allow the attacker back into SMM.

Therefore, approaches that attempt to measure all memory to prevent TOCTOU attacks only work when all measured memory can be isolated and controlled. Systems like Flicker [11], which use Intel Trusted Execution Technology, or SecVisor [19], which uses hardware support for virtualization plus an I/O Memory Management Unit, may be required to counter the attacker having an unmeasured location to hide in.

### Countering Requirement 3: Reinstallation of Malware

Kernel-mode self-checksumming systems face challenges in countering the first two requirements without significant assumption changes. We have made some improvements to counter the third requirement by removing as many generic, deterministic TOCTOU reinstallation avenues as possible. One area we mitigated is the ability for an attacker to corrupt return addresses and have our code return to attacker code undetected. Our extension of the minichecksum mechanism to cover arbitrary ranges helps mitigate this. Placing checksum data on the stack so that interrupts destroy it is another mechanism to maintain control flow integrity.

However, other mechanisms remain for the attacker to perform a TOCTOU and regain control soon after our code releases the processor. For instance, we have implemented a TOCTOU attack using a Windows Deferred Procedure Call (DPC) to schedule attacker code to run. The attacker places himself as the first to be removed from the DPC queue, which begins emptying soon after control is released by our kernel module. When the attacker code runs, it reinstalls the hook, allowing him to gain control when attestation is about to begin.

Future work can combat this by making it difficult for the attacker to know the true end time of the self-check function. This could be achieved by having multiple CPUs invoke the same self-check function in parallel. Existing approaches like PioneerNG & MTSRoT [28] try to synchronize checksum completion. Instead, checksum completion could be displaced in time so that when one self-check completes, others are still running on other CPUs. Randomizing the order in which CPUs are invoked would make it harder for the attacker to predict which CPU will finish last.

While there will always be the possibility for attacks to use TOCTOU with coarse granularity reinstallation (e.g., a conceptual "sleep(10)"), forcing the attacker to temporarily relinquish control is a higher measure of success than currently achieved. This opens a window of time in which the attacker is more vulnerable to detection and removal.

### Conclusion

In this paper, we have shown the results of independent implementations of both software and hardware timing-based attestation systems. We have demonstrated that an attestation system for a commodity OS can compensate for Address Space Layout Randomization (ASLR) effects, does not need to directly communicate with Network Interface Controller (NIC) hardware, and has detectable attacker overhead over ten network links. We have also clarified that most generic attacks against timing-based attestation systems are TOCTOU attacks. We described the conditions necessary for an attacker to achieve a TOCTOU attack and areas of future work to create generic countermeasures.

We consider timing-based remote attestation to be in its infancy, similar to the early days of cryptography. Just as cryptographic algorithms were accepted based on resistance to established attack techniques and performance trade-offs, we believe that increased research and implementation can make timing-based attestation mechanisms more robust. To this end, we will make our current reference implementation openly available [4], in the hope that others will help further improve the state of the art for remote timing-based attestation.

### References

[1] A. M. Azab, P. Ning, Z. Wang, X. Jiang, X. Zhang, and N. C. Skalsky. Hypersentry: enabling stealthy in-context measurement of hypervisor integrity. In Proceedings of the ACM conference on Computer and Communications Security, CCS, pages 38–49, 2010.

[2] S. Bratus, N. D’Cunha, E. Sparks, and S. W. Smith. TOC-TOU, traps, and trusted computing. In Proceedings of the International Conference on Trusted Computing and Trust in Information Technologies, Trust, pages 14–32, 2008.

[3] C. Castelluccia, A. Francillon, D. Perito, and C. Soriente. On the difficulty of software-based attestation of embedded devices. In Proceedings of the ACM conference on Computer and Communications Security, CCS, pages 400–409, 2009.

[4] Y. Choi, J. Kang, and D. Nyang. Proactive code verification protocol in wireless sensor network. In Proceedings of the International Conference on Computational Science and its Applications - Volume Part II, ICCSA, pages 1085–1096, 2007.

[5] J. A. Garay and L. Huelsbergen. Software integrity protection using timed executable agents. In Proceedings of the ACM Symposium on Information, Computer and Communications Security, ASIACCS, pages 189–200, 2006.

[6] Geeks3D. Aogenmark 1.3.0: Simple multi-core CPU benchmark. 2011, http://www.geeks3d.com/20110513/aogenmark-1-3-0-simple-multi-core-cpu-benchmark. Accessed: 10/21/2011.

[7] M. Jakobsson and K.-A. Johansson. Practical and secure software-based attestation. In Workshop on Lightweight Security Privacy: Devices, Protocols and Applications (LightSec), pages 1–9, March 2011.

[8] X. Jiang, X. Wang, and D. Xu. Stealthy malware detection through VMM-based “out-of-the-box” semantic view reconstruction. In Proceedings of the ACM conference on Computer and Communications Security, CCS, pages 128–138, 2007.

[9] Y. Li, J. M. McCune, and A. Perrig. SBAP: Software-Based Attestation for Peripherals. In Proceedings of the 3rd International Conference on Trust and Trustworthy Computing (Trust), June 2010.

[10] Y. Li, J. M. McCune, and A. Perrig. VIPER: Verifying the integrity of peripherals’ firmware. In Proceedings of the ACM Conference on Computer and Communications Security (CCS), October 2011.

[11] J. M. McCune, B. Parno, A. Perrig, Michael K. Reiter, and Hiroshi Isozaki. Flicker: An execution infrastructure for TCB minimization. In Proceedings of the ACM European Conference in Computer Systems (EuroSys), April 2008.

[12] Microsoft. (Ndis 5.1) Ndis drivers (Windows Driver Kit). September 7th 2011, http://msdn.microsoft.com/en-us/Library/ff556938(v=VS.85).aspx. Accessed: 11/01/2011.

[13] W. D. Norcott and D. Capps. Iozone filesystem benchmark. 20106, http://www.iozone.org/. Accessed: 10/21/2011.

[14] A. Perrig and L. van Doorn. Refutation of on the difficulty of software-based attestation of embedded devices. 2010, http://sparrow.ece.cmu.edu/group/pub/perrig-vandoorn-refutation.pdf. Accessed: 11/01/2011.

[15] N. L. Petroni, Jr., T. Fraser, J. Molina, and W. A. Arbaugh. Copilot - a coprocessor-based kernel runtime integrity monitor. In Proceedings of the 13th conference on USENIX Security Symposium - Volume 13, SSYM, pages 13–13, 2004.

[16] D. Schellekens, B. Wyseur, and B. Preneel. Remote attestation on legacy operating systems with trusted platform modules. Electron. Notes Theor. Comput. Sci., 197:59–72, February 2008.