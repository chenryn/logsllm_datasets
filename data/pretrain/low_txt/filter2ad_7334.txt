# Title: uvNIC: Rapid Prototyping Network Interface Controller Device Drivers

## Author: Matthew P. Grosvenor  
University of Cambridge Computer Laboratory  
PI: EMAIL

### Categories and Subject Descriptors
D.4.4 [Operating Systems]: Communications Management – Network communication

### General Terms
Design, Experimentation, Verification

### Keywords
Hardware, Device Driver, Emulation, Userspace, Virtualization

## 1. Introduction
Traditional approaches to NIC (Network Interface Controller) driver design focus on commodity network hardware, which typically has a slow-moving feature set and long product life cycles. The introduction of FPGA-based network adapters, such as [1][2], significantly changes this paradigm. While traditional ASIC-based NICs may undergo minor driver interface revisions over years, FPGA-based NIC interfaces can be completely re-implemented in months or even weeks. This presents a significant challenge for driver developers, as they cannot begin serious development without hardware support, but are now expected to work concurrently with hardware development.

To address this issue, I present the userspace, virtual NIC framework (uvNIC). uvNIC implements a custom virtual NIC as a standard userspace application, providing a functional equivalent to a physical device for the driver developer. Only minor modifications are required to switch a uvNIC-enabled driver to operate on real hardware. For hardware designers, uvNIC offers a rapid prototyping environment for initial specifications and a fully functional model against which HDL (Hardware Description Language) code can later be verified.

## 2. Design and Implementation
Typical NIC device drivers implement two interfaces: a device-facing PCI (Peripheral Component Interconnect) interface and a kernel-facing network stack interface. Normally, a device driver would send and receive packets by interacting with real hardware over the PCI interface. In contrast, uvNIC forwards these interactions to the uvNIC virtual NIC application, which emulates the hardware NIC and sends and receives packets over a commodity device operated in raw socket mode.

Implementing the uvNIC PCI virtualization layer is non-trivial. Operating system kernels are designed with strict one-way dependencies, where userspace applications depend on the kernel, and the kernel depends on the hardware. Importantly, the kernel is not designed to easily facilitate dependence on userspace applications. For the uvNIC framework, this is problematic because the virtual NIC should appear to the driver as a hardware device, but to the kernel, it appears as a userspace application.

Figure 1 illustrates the uvNIC implementation in detail. At its core, a message transport layer (uvBus) connects the kernel and the virtual device. uvBus uses file I/O operations (open(), ioctl(), mmap()) to establish shared memory regions between the kernel and userspace. Messages are exchanged by enqueuing and dequeuing fixed-size packets into lockless circular buffers, maintaining strict delivery order. uvBus also includes an out-of-band, bidirectional signaling mechanism for alerting message consumers about incoming data. Userspace applications signal the kernel by calling write() with a 64-bit signal value, and the kernel signals userspace by providing a 64-bit response to poll()/read() system calls.

A lightweight PCIe-like protocol (uvPCI) is implemented on top of uvBus. uvPCI supports posted (non-blocking) writes and non-posted (blocking) reads in both kernel and userspace. Non-posted reads in kernel space are implemented by spinning, with timeouts and appropriate calls to yield(). An important aspect of uvPCI is that it maintains read and write message ordering consistent with hardware PCIe implementations.

In addition to basic PCI read and write operations, uvPCI implements x86-specific PCIe restrictions, such as 64-bit register reads/writes, message-signaled interrupt generation, and 128B, 32-bit aligned DMA (Direct Memory Access) operations. DMA operations appear to the driver as they would in reality, with data appearing in DMA-mapped buffers asynchronously without direct driver involvement.

In practice, uvPCI provides a functionally equivalent, parallel implementation of the PCI stack. Driver writers need only perform a search/replace and recompile to switch to using the real PCI stack.

## 3. Related Work
Userspace networking has a long history [3][4] and continues to be widely used, especially in high-performance situations [5][6]. While uvNIC shares the basic concept of implementing part of the network stack in userspace, it is distinct from previous attempts because it emulates the network hardware in userspace software rather than parts of the network driver or IP stack, which is more common. It is crucial to note that network performance is not the primary goal of uvNIC; rather, the primary goal is rapid prototyping at the software/hardware interface.

The structure and function of uvNIC are highly similar to the virtual NICs found in hypervisors and virtual machines (VMs). Both VMware [6] and Xen [7] expose virtualized hardware devices to their guest operating systems and hence to the guest OS drivers. Custom hardware could potentially be designed and written in a VM context instead of uvNIC. However, this approach has the distinct disadvantage of being complex and time-consuming, which is contrary to the stated goal of aiding rapid prototyping of the NIC and device driver.

uvNIC is also similar to File System in User Space (FUSE) [9] systems. Like uvNIC, FUSE requires the kernel to become dependent on userspace applications. However, FUSE systems are simpler than uvNIC because they do not require direct emulation of hardware timing, ordering, and consistency parameters.

## 4. Preliminary Results
A uvNIC virtual network device has been implemented on a Linux 2.6 host. The virtual device was tested with simple Linux tools such as ping and traceroute and found to be functional. This test confirmed that the uvNIC framework is suitable for writing simple but functional network devices and device drivers. Since performance was not a consideration, iperf tests were not performed.

Additionally, a uvNIC driver has been written and tested against a NetFPGA 10G card [2] running a simple register and MSI (Message-Signaled Interrupt) interface firmware module. This test confirmed that the uvNIC framework is capable of writing simple device drivers that are portable to real hardware platforms.

Efforts are currently underway to port the feature-rich Intel IXGBE 10G driver to a uvNIC virtual device using an Intel e1000 1G as a physical device. This comprehensive test will demonstrate uvNIC's ability to augment simple physical hardware with additional virtual functionality.

## 5. Conclusions and Next Steps
uvNIC is a simple yet unique approach to a real problem. Initial work has indicated that the approach is valid and functional. Future steps may include migrating the virtualized hardware model to a cycle-accurate simulation of hardware RTL (Register-Transfer Level) code or using the framework as a measurement and debugging tool for real device drivers. uvNIC affords designers a unique opportunity to rapidly explore the NIC software/hardware interface at low cost. It is hoped that this ability will lead to faster and more stable NIC designs.

More information on the uvNIC project can be found at:  
http://www.cl.cam.ac.uk/research/srg/netos/mrc/projects/uvNIC

## 6. Acknowledgements
This work was jointly supported by the EPSRC INTERNET Project EP/H040536/1 and the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL), under contract FA8750-11-C-0249. The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.

## 7. References
[1] John W. Lockwood, Nick McKeown, Greg Watson, Glen Gibb, Paul Hartke, Jad Naous, Ramanan Raghuraman, and Jianying Luo. 2007. NetFPGA—An Open Platform for Gigabit-Rate Network Switching and Routing. In Proceedings of the 2007 IEEE International Conference on Microelectronic Systems Education (MSE '07). IEEE Computer Society, Washington, DC, USA, 160-161.

[2] NetFPGA 10G Project, NetFPGA website, http://www.netfpga.org

[3] T. von Eicken, A. Basu, V. Buch, and W. Vogels. 1995. U-Net: a user-level network interface for parallel and distributed computing. SIGOPS Oper. Syst. Rev. 29, 5 (December 1995), 40-53.

[4] Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. 1995. Myrinet: A Gigabit-per-Second Local Area Network. IEEE Micro 15, 1 (February 1995), 29-36.

[5] Ian Pratt, Keir Fraser. 2001. Arsenic: A user-accessible gigabit ethernet interface. Proceedings of the Twentieth Annual Joint Conference of the IEEE Computer and Communications Societies, INFOCOM01.

[6] David Riddoch, Steven Pope. 2008. OpenOnload, A user-level network stack. Google Tech Talk, http://www.openonload.org/openonload-google-talk.pdf

[7] Jeremy Sugerman, Ganesh Venkitachalam, and Beng-Hong Lim. 2001. Virtualizing I/O Devices on VMware Workstation's Hosted Virtual Machine Monitor. In Proceedings of the General Track: 2002 USENIX Annual Technical Conference, Yoonho Park (Ed.). USENIX Association, Berkeley, CA, USA, 1-14.

[8] Aravind Menon, Alan L. Cox, and Willy Zwaenepoel. 2006. Optimizing network virtualization in Xen. In Proceedings of the annual conference on USENIX '06 Annual Technical Conference (ATEC '06). USENIX Association, Berkeley, CA, USA, 2-2.

[9] Miklos Szeredi. 2012. File System in User Space. http://fuse.sourceforge.net