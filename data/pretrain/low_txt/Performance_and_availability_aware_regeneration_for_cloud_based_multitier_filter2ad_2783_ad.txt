### Table 1: Execution Times for Cluster and Cloud Scenarios

Table 1 presents the measured execution times for both cluster and cloud scenarios. The tests were conducted on a machine equipped with 2 Intel Xeon 3.00GHz processors and 4 GB of RAM. As shown, the execution time is influenced by the target workload and the LQN (Layered Queueing Network) model solver. Using simpler techniques could significantly reduce the controller's prediction time.

Most of the execution time is consumed by the system configuration, even in cloud computing scenarios, but it remains reasonable. The results indicate that the performance is acceptable, especially for systems with a small number of virtual machines (VMs). Due to the relatively small workload intensity, the gap between the Static and LL (Load Leveling) approaches is minimal. In some cases, the Static approach even outperforms the LL approach.

### Figure 5: Response Time Change After Failure

Figure 5 illustrates the percentage change in response times before and after a failure for the RUBIS-I application. The workloads vary between 23 and 56 requests per second, and the different data points are not directly comparable. The results show that our approach provides consistent performance, even when entire racks fail. For one set of emulation points, the average degradation was 9.5% for our approach, while for the Static and LL approaches, the degradations were 46% and 47%, respectively.

### Emulation Setup

To emulate failures, we randomly select a location for the failure, which can be an individual machine or a rack due to a common cause such as a power source or switch/router failure. Each experiment measures the mean response time for a period of 5 minutes before and 5 minutes after the emulated failure. The same failure scenario is applied to each strategy, ensuring that the same workload and failure location are used for all strategies at the time of the failure.

### Resource Allocation and Replication

In the initial configuration, VMs hosting Tomcat and MySQL replicas are allocated 80% of the CPU capacity, while VMs hosting Apache replicas are allocated 40% of the CPU capacity. The Static strategy maintains the same VM placement and capacity allocation throughout the experiment, while the Opt and LL strategies adjust the VM placement and capacity allocation based on the workload at the time of the failure.

### Related Work

Previous research has explored dynamic resource management and fault tolerance in distributed systems. For example, [23] uses regeneration of new replicas to account for failures, and the Google File System [14] similarly creates new file "chunks" when the number of available copies falls below a threshold. Commercial tools like VMware High Availability (HA) [26] allow a virtual machine to be reinstantiated on a new machine if the host fails.

Recent work on performance modeling and optimization in multi-tier applications (e.g., [25, 4, 9, 17]) addresses the impact of resource allocation and replica placement. Our work is the first to combine performance and availability requirements in multitier applications, addressing the tradeoff between cost and availability.

### Conclusions

In this paper, we have examined how virtual machine solutions can be used to provide high availability while maximizing performance on a fixed amount of resources. We use component redundancy to tolerate single machine failures and smart component placement based on queuing models to minimize performance degradation. Our simulation results show that our proposed approach provides better performance and maximum throughput than classical approaches.

### References

[1] K. Ahn, J. Kim, and S. Hong. Fault-tolerant real-time scheduling using passive replicas. In Proc. PRFTS, page 98, 1997.
[2] M. Arlitt and T. Jin. Workload characterization of the 1998 World Cup web site. HP Tech. Rep. HPL-99-35, 1999.
[3] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Wareld. Xen and the art of virtualization. In Proc. SOSP, pages 164-177, 2003.
[4] M. Bennani and D. Manesce. Resource allocation for autonomic data centers using analytic performance models. In Proc. ICAC, pages 217-228, 2005.
[5] E. Cecchet, A. Chanda, S. Elnikety, J. Marguerite, and W. Zwaenepoel. Performance architectures for generating dynamic web content. In Proc. Middleware, 2003.
[6] S. Chen, K. Joshi, M. Hiltunen, R. Schlichting, and W. Sanders. Gradient-based predictive models of multitier systems. In Proc. PMCCS, 2009.
[7] C. Clark, K. Fraser, S. Hand, C. Limpach, J. G. Hansen, E. Jul, and A. Warfield. Live migration of virtual machines. In Proc. NSDI, 2005.
[8] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson, and A. Warfield. Remus: High availability via asynchronous virtual machine replication. In Proc. NSDI, pages 161-174, 2008.
[9] I. Cunha, J. Almeida, V. Almeida, and M. Santos. Self-adaptive management for multi-tier environments. In Proc. IM, pages 129-138, 2007.
[10] H. de Meer and K. S. Trivedi. Guarded repair of dependable systems. Theoretical Comp. Sci., 128:179-210, 1994.
[11] J. Dean. Software engineering advice from building large-scale distributed systems. Stanford CS295 class lecture. http://research.google.com/people/jeff/stanford-295-talk.pdf, 2007.
[12] J. Dilley. Web server workload characterization. In HP Tech. Rep. HPL-96-160, 1996.
[13] G. Franks, S. Majumdar, J. Neilson, D. Petriu, J. Rolia, and M. Woodside. Performance analysis of distributed server systems. In Proc. Inti. Conf Software Quality, pages 15-26, 1996.
[14] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google file system. In Proc. SOSP, 2003.
[15] J. R. Hamilton. An architecture for modular data centers. In Proc. Innovative QoS Customization Practice and Experience, pages 306-313, 2007.
[16] J. He, M. Hiltunen, M. Rajagopalan, and R. Schlichting. Balancing dependability and timeliness in distributed object systems. Software-Practice and Experience, 33(3):295-320, 2003.
[17] G. Jung, K. Joshi, M. Hiltunen, R. Schlichting, and C. Pu. Generating adaptation policies for multi-tier applications in consolidated server environments. In Proc. ICAC, pages 23-32, 2008.
[18] H. Kopetz and W. Merker. The architecture of Mars. In Proc. FTCS, pages 274-279, 1985.
[19] P. L'Ecuyer, L. Meliani, and J. Vaucher. SSJ: a framework for stochastic simulation in Java. In Proc. Winter Simul. Conf, pages 234-242, 2002.
[20] A. Liestman and R. Campbell. A fault-tolerant scheduling problem. IEEE Trans. SE, SE-12(11):1089-1095, 1986.
[21] D. Mosse, R. Melhem, and S. Ghosh. A nonpreemptive scheduler with recovery from transient faults and its implementation. IEEE Trans. SE, SE-29(8):752-767, 2003.
[22] P. Narasimhan, T. Dumitras, A. Paulos, S. Pertet, C. Reverte, J. Slember, and D. Srivastava. Fault-tolerant CORBA. Concurrency and Computation: Practice and Experience, 17(12):1527-1545, 2005.
[23] C. Pu, J. Noe, and A. Proudfoot. Mead: Support for the regeneration of replicated objects: A technique and its Eden implementation. In Proc. Int. Conf. on Data Engineering, pages 175-187, 1986.
[24] K. G. Shin, C. M. Krishna, and Y.-H. Lee. Optimal dynamic control of resources in a distributed system. IEEE Trans. SE, 15(10):1188-1198, Oct 1989.
[25] B. Urgaonkar, G. Pacifici, P. Shenoy, M. Spreitzer, and A. Tantawi. An analytical model for multi-tier internet services and its applications. In Proc. SIGMETRICS, pages 291-302, 2005.
[26] VMware. VMware high availability (HA), restart your virtual machine. Accessed May 2009. World Wide Web. <http://www.vmware.com/products/vi/vc/ha.html>.
[27] C. M. Woodside, E. Neron, E. D. S. Ho, and B. Mondoux. An "active server" model for the performance of parallel programs written using rendezvous. Systems and Software, pages 125-131, 1986.

978-1-4244-7501-8/10/$26.00 Â©2010 IEEE

506
DSN 2010: Jung et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 07:14:00 UTC from IEEE Xplore. Restrictions apply.