### Table III: Accuracy Ratio of Model Estimates vs. Experimental Results

Table III compares the accuracy of our model in estimating Program/Erase (P/E) cycles and total page writes for various RAID schemes. The results show that our model accurately predicts these values, with most cases having a difference of less than 10% between the model estimates and experimental results. This high degree of accuracy indicates that our model is a reliable indicator of both performance and wear-out levels in flash memory storage systems employing different RAID configurations.

### Long-Term Reliability Projections

Using our validated model, we can project the long-term reliability of flash memory storage systems by extending the number of write requests for given workloads. A key metric for assessing the reliability of flash memory is the bit error rate (BER), particularly when specific error recovery techniques are applied. 

### Error Correction and Detection

We assume that ECC (Error-Correcting Code) can correct up to \( k \) bits and detect up to \( 2k \) bits. This assumption is consistent with coding theory, which suggests that the number of errors detectable by ECC is approximately twice the number of errors it can correct. Given this, the Correctable Page Error Rate (CPER) can be calculated as follows:

\[ \text{CPER}(n, k) = \sum_{i=0}^{k} \binom{n}{i} \cdot \text{RBER}^i \cdot (1 - \text{RBER})^{n-i} \]

where \(\text{RBER}\) is the raw bit error rate.

The Uncorrectable Page Error Rate (UPER) is then given by:

\[ \text{UPER}(n, k) = 1 - \text{CPER}(n, k) \]

### RAID System Assumptions

For the RAID system to function, it must be able to identify whether disks in a stripe have errors. ECC does not guarantee error detection if more than \( 2k \) bit errors occur in a page. Therefore, we conservatively assume that flash memory storage using RAID architecture can correct errors with parity only when a page has bit errors less than or equal to \( 2k \).

### Derivation of UPER for RAID Schemes

In a previous study, Lee et al. derived the UPER for an SSD after applying a RAID scheme, denoted as \(\text{UPER}_{\text{STR}}(N)\):

\[ \text{UPER}_{\text{STR}}(N) = 1 - F(1; N, \text{UPER}(n, k)) \]

where \( F(x; N, p) \) is the cumulative binomial distribution, and \( N \) is the number of pages in a stripe. However, this derivation does not account for undetected errors exceeding \( 2k \) bits. To address this, we derive the Correctable Stripe Error Rate (CSER), defined as:

\[ \text{CSER}_{\text{STR}}(N) = F(1; N, \text{UPER}(n, k)) \]

There are two cases where bit errors in a stripe can be corrected:
1. All pages in a stripe have \( \leq k \) bit errors, corrected by ECC.
2. \( N-1 \) pages in a stripe have \( \leq k \) bit errors, and one page has \( > k \) but \( \leq 2k \) bit errors, detected by ECC.

The probability for the first case is:

\[ \binom{N}{0} \cdot \text{CPER}(n, k)^N \]

The probability for the second case is:

\[ \binom{N}{1} \cdot \text{CPER}(n, k)^{N-1} \cdot \sum_{i=k+1}^{2k} \binom{n}{i} \cdot \text{RBER}^i \cdot (1 - \text{RBER})^{n-i} \]

Summing these probabilities gives:

\[ \text{CSER}_{\text{STR}}(N) = \binom{N}{0} \cdot \text{CPER}(n, k)^N + \binom{N}{1} \cdot \text{CPER}(n, k)^{N-1} \cdot \sum_{i=k+1}^{2k} \binom{n}{i} \cdot \text{RBER}^i \cdot (1 - \text{RBER})^{n-i} \]

Thus, the UPER for the RAID scheme is:

\[ \text{UPER}_{\text{STR}}(N) = 1 - \text{CSER}_{\text{STR}}(N) \]

### Comparative Analysis

Figure 12(a) shows the UPER values for SSDs using only ECC, RAID-5, and eSAP. The figure indicates that ECC has the highest UPER because it lacks parity protection. The reliability gap between ECC and RAID-5/eSAP narrows as more data is written due to increased P/E cycles from parity updates. Among the three schemes, eSAP has the lowest UPER, significantly below HDD levels, and its reliability degradation rate is lower than RAID-5 due to reduced parity write overhead.

Figure 12(b) compares the number of P/E cycles required for each scheme. eSAP requires fewer P/E cycles than RAID-5, and except for eSAP with a small stripe size, the number of P/E cycles remains below the limit for up to 200TB of data writes.

### Conclusion

eSAP-RAID improves reliability while limiting wear. SSDs using eSAP-RAID can maintain the same lifespan as current ECC-based SSDs, with reliability levels comparable to the early stages of ECC SSD usage throughout their entire lifetime.

### Acknowledgment

We thank the anonymous reviewers for their constructive comments and Eunjae Lee for automating the calculations. This research was supported by the Seoul Creative Human Development Program, the National Research Foundation of Korea, and the Basic Science Research Program.

### References

[1] UMASS TRACE REPOSITORY. http://traces.cs.umass.edu.
[2] Flash-memory Translation Layer for NAND Flash (NFTL). M-Systems. 1998.
[3] OCZ TECHNOLOGY EVEREST LAUNCHES SSD INDILINX CONTROLLER NEXT GENERATION PLATFORM. http://www.ocztechnology.com/aboutocz/press/2012/491.
[4] Samsung Releases TLC NAND Based 840 SSD. http://www.anandtech.com/show/6329/samsung-releases-tlc-nand-based-840-ssd.
[5] Understanding the Flash Translation Layer (FTL) Specification. Intel Corporation. 1998.
[6] M. Blaum, J. L. Hafner, and S. Hetzler. Partial-MDS Codes and Their Application to RAID Type of Architectures. IBM Research Report, RJ100498, February 2012.
[7] P. M. Chen and E. K. Lee. Striping in a RAID Level 5 Disk Array. In Proc. SIGMETRICS ’95, pages 136–145, Ottawa, Ontario, Canada, 1995.
[8] S. Chen. What types of ECC should be used on flash memory? http://www.spansion.com/Support/AppNotes/, 2007.
[9] E. Deal. Trends in NAND Flash Memory Error Correction. http://www.cyclicdesign.com/whitepapers/Cyclic Design NAND ECC.pdf, Cyclic Design, White Paper, Jun. 2009.
[10] J. Gary and C. van Ingen. Empirical Measurements of Disk Failure Rates and Error Rates. Technical Report MSR-TR-2005-166, December 2005.
[11] L. M. Grupp, A. M. Caulfield, J. Coburn, S. Swanson, E. Yaakobi, P. H. Siegel, and J. K. Wolf. Characterizing Flash Memory: Anomalies, Observations, and Applications. In Proc. MICRO 42, pages 24–33, New York, NY, 2009.
[12] L. M. Grupp, J. D. Davis, and S. Swanson. The Bleak Future of NAND Flash Memory. In Proc. FAST ’12, pages 17–24, San Jose, CA, 2012.
[13] X.-Y. Hu, E. Eleftheriou, R. Haas, I. Iliadis, and R. Pletka. Write Amplification Analysis in Flash-based Solid State Drives. In Proc. SYSTOR ’09, pages 10:1–10:9, Haifa, Israel, 2009.
[14] S. Im and D. Shin. Flash-Aware RAID Techniques for Dependable and High-Performance Flash Memory SSD. IEEE Transactions on Computers, 60(1):80–92, Jan. 2011.
[15] J. Kim, J. Lee, J. Choi, D. Lee, and S. H. Noh. Enhancing SSD Reliability through Efficient RAID Support. In Workshop on APSYS ’12, pages 4:1–4:6, Seoul, Republic of Korea, 2012.
[16] H. Kwon, E. Kim, J. Choi, D. Lee, and S. H. Noh. Janus-FTL: Finding the Optimal Point on the Spectrum between Page and Block Mapping Schemes. In Proc. EMSOFT ’10, pages 169–178, Scottsdale, AZ, 2010.
[17] S. Lee, B. Lee, K. Koh, and H. Bahn. A Lifespan-Aware Reliability Scheme for RAID-based Flash Storage. In Proc. SAC ’11, pages 374–379, TaiChung, Taiwan, 2011.
[18] Y. Lee, S. Jung, and Y. H. Song. FRA: a flash-aware redundancy array of flash storage devices. In Proc. CODES+ISSS ’09, pages 163–172, Grenoble, France, 2009.
[19] N. Mielke, T. Marquar, N. Wu, J. Kessenich, H. Belgal, E. Schares, F. Trivedi, E. Goodness, and L. Nevill. Bit Error Rate in NAND Flash Memories. In Proc. IEEE Int’l Reliability Physics Symp., pages 9–19, 2008.
[20] D. Narayanan, E. Thereska, A. Donnelly, S. Elnikety, and A. Rowstron. Migrating Server Storage to SSDs: Analysis of Tradeoffs. In Proc. EuroSys ’09, pages 145–158, Nuremberg, Germany, 2009.
[21] Y. Oh, J. Choi, D. Lee, and S. H. Noh. Caching Less for Better Performance: Balancing Cache Size and Update Cost of Flash Memory Cache in Hybrid Storage Systems. In Proc. FAST ’12, pages 313–326, San Jose, CA, 2012.
[22] V. Prabhakaran and T. Wobber. SSD Extension for DiskSim Simulation Environment. http://research.microsoft.com/en-us/downloads/b41019e2-1d2b-44d8-b512-ba35ab814cd4.
[23] M. Rosenblum and J. K. Ousterhout. The Design and Implementation of a Log-structured File System. ACM Transactions of Computer Systems, 10(1):26–52, Feb. 1992.
[24] H. Sun, P. Grayson, and B. Wood. Quantifying Reliability of Solid-State Storage from Multiple Aspects. In Workshop on SNAPI ’11, Denver, CO, May 2011.
[25] W. Wang, Y. Zhao, and R. Bunt. HyLog: A High Performance Approach to Managing Disk Layout. In Proc. FAST ’04, pages 145–158, San Francisco, CA, 2004.
[26] Y. Wang, L. A. D. Bathen, N. D. Dutt, and Z. Shao. Meta-Cure: a Reliability Enhancement Strategy for Metadata in NAND Flash Memory Storage Systems. In Proc. DAC ’12, pages 214–219, San Francisco, CA, 2012.