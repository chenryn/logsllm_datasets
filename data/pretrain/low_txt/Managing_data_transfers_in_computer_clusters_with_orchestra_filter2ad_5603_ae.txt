### Broadcast and Shuffle Algorithms

In these experiments, we compare the performance of applications before and after implementing Orchestra. Specifically, "before" refers to running with a HDFS-based broadcast implementation (with the default 3× replication) and a shuffle using 5 threads per receiver (the default in Hadoop). In contrast, "after" involves using Cornet for broadcast and a shuffle with 30 threads per receiver.

### Time Breakdown in Monarch Iterations

Figure 16 illustrates the time spent on different activities in each iteration of Monarch in a 30-node EC2 cluster. For \( n \) concurrent transfers, the theoretical maximum speedup of the average completion time using FIFO over fair sharing is given by:
\[ \frac{2n}{n+1} \]

### Scheduling and Management in Data-Intensive Applications

A variety of schemes exist for scheduling and managing tasks in data-intensive applications. Examples include fair schedulers for Hadoop [43] and Dryad [29], and Mantri [11] for outlier detection. The core principle of these systems is to achieve data locality to minimize network transfers. Mesos [26] provides a thin management layer to allow diverse cluster computing frameworks to efficiently share computation and storage resources, but it leaves the sharing of network resources to underlying transport mechanisms. Orchestra complements these systems by enabling the implementation of network sharing policies across applications.

### One-to-Many Data Transfer Mechanisms

Broadcast, multicast, and various group communication mechanisms have been extensively studied in both application and lower layers of the network stack. Diot et al. provide a comprehensive survey and taxonomy of relevant protocols and mechanisms for distributed multi-point communication [16]. Cornet is designed for transferring large amounts of data in high-speed datacenter networks.

**SplitStream [13]** improves network utilization and addresses the bottleneck problem observed in d-ary trees by creating multiple distribution trees with disjoint leaf sets. However, it is primarily designed for multimedia streaming over the Internet, where frame loss is acceptable. Maintaining its structural constraints in the presence of failures is complex.

**BitTorrent [4]** is widely used for file-sharing. BitTorrent and similar peer-to-peer mechanisms are also used for distributing planet-scale software updates [20]. **Murder [8]** is the only known BitTorrent deployment inside a datacenter. **Antfarm [36]** uses a central coordinator across multiple swarms to optimize content distribution over the Internet. Cornet, a BitTorrent-like system, is optimized for datacenters and uses an adaptive clustering algorithm in the Traffic Controller (TC) to infer and leverage network topologies.

### Incast or Many-to-One Transfers

TCP incast collapse is typically observed in barrier-synchronized request workloads where a receiver synchronously receives small amounts of data from many senders [41]. This issue has also been reported in MapReduce-like data-intensive workloads [14], which can be seen as a special case of shuffle with only one reducer. With Orchestra, the TC can effectively limit the number of simultaneous senders and their transmission rates to alleviate this problem for data-intensive workloads.

### Inferring Topology from Node-to-Node Latencies

Inferring node topology in CornetClustering (Section 5.3) is similar to inferring network coordinates [17]. These methods could substitute for the non-metric multidimensional scaling step in the CornetClustering procedure.

### Conclusion

We have argued that multi-node transfer operations significantly impact the performance of data-intensive cluster applications and presented an architecture called Orchestra that enables global control across and within transfers to optimize performance. We focused on two common transfer patterns: broadcasts and shuffles. For broadcasts, we proposed a topology-aware BitTorrent-like scheme called Cornet, which outperforms the status quo in Hadoop by 4.5×. For shuffles, we proposed an optimal algorithm called Weighted Shuffle Scheduling (WSS). Overall, our schemes can increase application performance by up to 1.9×. Additionally, inter-transfer scheduling can improve the performance of high-priority transfers by 1.7× and reduce average transfer times by 31%. Orchestra can be implemented at the application level without requiring hardware changes in current datacenters and cloud environments.

### Per-Iteration Completion Times

Figure 17 shows the per-iteration completion times when scaling the collaborative filtering application using Orchestra. We observe that the communication overhead in each iteration decreased from 42% to 28% of the run time, and iterations finished 22% faster overall. There was a 2.3× speedup in broadcast and a 1.23× speedup in shuffle. These improvements align with the findings in Sections 7.1 and 6.2.

Figure 17(b) presents the per-iteration completion times for the collaborative filtering job while scaling it up to 90 nodes using Cornet. Unlike the HDFS-based solution (Figure 17(a)), broadcast time increased from 13.4 to only 15.3 seconds using Cornet. As a result, the job could be scaled up to 90 nodes with a 1.9× improvement in iteration times. The average time spent in broadcast decreased by 3.6×, from 55.8s to 15.3s, for 90 nodes. These results are consistent with Section 7.1, given 385 MB broadcast per iteration.

### Related Work

#### Full Bisection Bandwidth Datacenter Networks

Many new datacenter network architectures have been proposed in recent years [9, 21, 23, 24, 35] to achieve full bisection bandwidth and improved network performance. However, full bisection bandwidth does not imply infinite bandwidth. Orchestra remains valuable in such networks to enable inter-transfer prioritization and scheduling, balance shuffle transfer rates using WSS, and speed up broadcasts using Cornet. For example, the experiments in Section 7 show that Orchestra improves job performance even on EC2’s network, which appears to have near-full bisection bandwidth.

#### Centralized Network Controllers

Centralized controllers for routing, access control, and load balancing in the network have been proposed by the 4D architecture [22] and projects like Tesseract [42], Ethane [12], PLayer [30], and Hedera [10]. While PLayer and Ethane focus on access control, our primary objective is to improve application-level performance. Our scope is limited to shared clusters and datacenters, whereas 4D, Tesseract, and Ethane are designed for wide-area and enterprise networks. Unlike Hedera or any existing proposals for centralized control planes, we work at the granularity of transfers to optimize overall application performance, not at the packet or flow level.

#### Performance Isolation in Datacenter Networks

Seawall [38] performs weighted fair sharing among cloud tenants running arbitrary numbers of TCP and UDP flows through a shim layer at the hypervisor using a cross-flow AIMD scheme. It can be leveraged by Orchestra to enforce inter-transfer scheduling policies. However, Seawall itself is not aware of transfer-level semantics.

### Acknowledgments

We thank the AMPLab members, the anonymous reviewers, and our shepherd, Yinglian Xie, for useful comments on the paper. We also thank Michael Armbrust, Jon Kuroda, Keith Sklower, and the Spark team for infrastructure support. This research was supported in part by gifts from AMPLab founding sponsors Google and SAP, AMPLab sponsors Amazon Web Services, Cloudera, Huawei, IBM, Intel, Microsoft, NEC, NetApp, and VMWare, and by matching funds from the State of California’s MICRO program (grants 06-152, 07-010), the National Science Foundation (grants CNS-0509559 and CNS-1038695), the University of California Industry/University Cooperative Research Program (UC Discovery) grant COM07-10240, and the Natural Sciences and Engineering Research Council of Canada.

### References

[1] Amazon EC2. http://aws.amazon.com/ec2.
[2] Apache Hadoop. http://hadoop.apache.org.
[3] BitTornado. http://www.bittornado.com.
[4] BitTorrent. http://www.bittorrent.com.
[5] DETERlab. http://www.isi.deterlab.net.
[6] Fragment replicate join – Pig wiki. http://wiki.apache.org/pig/PigFRJoin.
[7] LANTorrent. http://www.nimbusproject.org.
[8] Murder. http://github.com/lg/murder.
[9] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and A. Donnelly. Symbiotic routing in future data centers. In SIGCOMM, pages 51–62, 2010.
[10] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: Dynamic flow scheduling for data center networks. In NSDI, 2010.
[11] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica, Y. Lu, B. Saha, and E. Harris. Reining in the outliers in mapreduce clusters using Mantri. In OSDI, 2010.
[12] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown, and S. Shenker. Ethane: Taking control of the enterprise. In SIGCOMM, pages 1–12, 2007.
[13] M. Castro, P. Druschel, A.-M. Kermarrec, A. Nandi, A. Rowstron, and A. Singh. Splitstream: high-bandwidth multicast in cooperative environments. In SOSP, 2003.
[14] Y. Chen, R. Griffith, J. Liu, R. H. Katz, and A. D. Joseph. Understanding TCP incast throughput collapse in datacenter networks. In WREN, pages 73–82, 2009.
[15] J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In OSDI, pages 137–150, 2004.
[16] C. Diot, W. Dabbous, and J. Crowcroft. Multipoint communication: A survey of protocols, functions, and mechanisms. IEEE JSAC, 15(3):277–290, 1997.
[17] B. Donnet, B. Gueye, and M. A. Kaafar. A Survey on Network Coordinates Systems, Design, and Security. IEEE Communication Surveys and Tutorials, 12(4), Oct. 2010.
[18] C. Fraley and A. Raftery. MCLUST Version 3 for R: Normal mixture modeling and model-based clustering. Technical Report 504, Department of Statistics, University of Washington, Sept. 2006.
[19] P. Ganesan and M. Seshadri. On cooperative content distribution and the price of barter. In ICDCS, 2005.
[20] C. Gkantsidis, T. Karagiannis, and M. Vojnović. Planet scale software updates. In SIGCOMM, pages 423–434, 2006.
[21] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A scalable and flexible data center network. In SIGCOMM, 2009.
[22] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, J. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to network control and management. SIGCOMM CCR, 35:41–54, 2005.
[23] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and S. Lu. BCube: A high performance, server-centric network architecture for modular data centers. In SIGCOMM, pages 63–74, 2009.
[24] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. DCell: A scalable and fault-tolerant network structure for data centers. In SIGCOMM, pages 75–86, 2008.
[25] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, New York, NY, 2009.
[26] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. Joseph, R. Katz, S. Shenker, and I. Stoica. Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center. In NSDI, 2011.
[27] U. Hoelzle and L. A. Barroso. The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. Morgan and Claypool Publishers, 1st edition, 2009.
[28] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: Distributed data-parallel programs from sequential building blocks. In EuroSys, pages 59–72, 2007.
[29] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg. Quincy: Fair scheduling for distributed computing clusters. In SOSP, 2009.
[30] D. A. Joseph, A. Tavakoli, and I. Stoica. A policy-aware switching layer for data centers. In SIGCOMM, 2008.
[31] J. B. Kruskal and M. Wish. Multidimensional Scaling. Sage University Paper series on Quantitative Applications in the Social Sciences, 07-001, 1978.
[32] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. Pregel: A system for large-scale graph processing. In SIGMOD, 2010.
[33] Y. Mao and L. K. Saul. Modeling Distances in Large-Scale Networks by Matrix Factorization. In IMC, 2004.
[34] D. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith, A. Madhavapeddy, and S. Hand. Ciel: A Universal Execution Engine for Distributed Data-Flow Computing. In NSDI, 2011.
[35] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: A scalable fault-tolerant layer 2 data center network fabric. In SIGCOMM, pages 39–50, 2009.
[36] R. Peterson and E. G. Sirer. Antfarm: Efficient content distribution with managed swarms. In NSDI, 2009.
[37] B. Pfaff, J. Pettit, K. Amidon, M. Casado, T. Koponen, and S. Shenker. Extending networking into the virtualization layer. In HotNets 2009.
[38] A. Shieh, S. Kandula, A. Greenberg, and C. Kim. Sharing the data center network. In NSDI, 2011.
[39] D. B. Shmoys. Cut problems and their application to divide-and-conquer, chapter 5, pages 192–235. PWS Publishing Co., Boston, MA, USA, 1997.
[40] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song. Design and evaluation of a real-time URL spam filtering service. In IEEE Symposium on Security and Privacy, 2011.
[41] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen, G. R. Ganger, G. A. Gibson, and B. Mueller. Safe and effective fine-grained TCP retransmissions for datacenter communication. In SIGCOMM, pages 303–314, 2009.
[42] H. Yan, D. A. Maltz, T. S. E. Ng, H. Gogineni, H. Zhang, and Z. Cai. Tesseract: A 4D network control plane. In NSDI ’07.
[43] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker, and I. Stoica. Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling. In EuroSys, 2010.
[44] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: Cluster Computing with Working Sets. In HotCloud, 2010.
[45] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale parallel collaborative filtering for the Netflix prize. In AAIM, pages 337–348. Springer-Verlag, 2008.