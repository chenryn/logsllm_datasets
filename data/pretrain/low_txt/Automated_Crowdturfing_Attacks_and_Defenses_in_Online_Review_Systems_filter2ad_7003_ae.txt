### Distribution and Policy Proposal

Based on our observations, we propose a straightforward policy for service providers to enhance the detection of evasive reviews: implement a minimum review length requirement. Figure 13 illustrates how detection performance improves as the minimum review length is increased, specifically against an attack model with 1,024 hidden units. The average review length in the Yelp Training Data (Section 3.3) is 483 characters. By raising the minimum length requirement to 300 characters (still below the average), the F-score increases from 0.80 to 0.86. This improvement nullifies the attack success gained by increasing the model size from 512 to 1,024 hidden units (as shown in Figure 12). Consequently, attackers must now train significantly larger models, incurring higher training costs, to overcome the reduced attack success.

### Evading Detection by Varying Temperature

When reviews generated by a language model are detected as fake, one might expect the attacker to build a new model, thereby increasing their costs. However, the attacker can attempt to evade further detection by generating new reviews using the existing model, simply by adjusting the temperature parameter (without retraining). While our scheme can detect reviews at different temperatures when ground-truth information is available (Figure 10), its performance remains uncertain in the absence of such data. Specifically, can a defense scheme trained on reviews at a specific temperature effectively detect reviews generated at other temperatures? If so, this would enable the service provider to defend against such attacks without needing to collect new ground-truth information.

### Investigating Temperature Variations

In Figure 14, we explore this scenario by training the linguistic classifier and our scheme at a specific temperature (Ttrain) and evaluating detection performance at all other temperatures (Ttest). Both Ttrain and Ttest range from 0.1 to 1. The training configuration is the same as that used for 2K training samples in Table 3.

As expected, the linguistic classifier performs poorly at higher temperatures. At low temperatures, a defense trained at a given temperature remains effective for temperatures in the "neighborhood" of that region, likely due to similar linguistic characteristics. However, performance drops rapidly when Ttrain and Ttest are far apart.

Our approach, on the other hand, shows a different trend. Unlike the linguistic classifier, our performance maintains robustness whenever Ttrain > Ttest. This is because reviews generated at high temperatures include both infrequent and frequent patterns from the training sequence. As a result, a defense trained at a higher temperature can capture the frequent sequences present in the character distribution of a review at a lower temperature. Thus, our defense scheme maintains high performance even when Ttrain and Ttest are distant, as long as Ttrain ≥ Ttest. It is worth noting that attackers are incentivized to generate reviews at high temperatures, as these are more likely to deceive users (Section 4.2). Therefore, the service provider can likely obtain some initial ground-truth information about reviews at high temperatures and build a robust defense.

### Related Work

#### Text Generation

There are several natural language generation techniques based on pre-defined templates [57, 58, 70], which typically require domain knowledge and well-designed rules. Recently, learning-based approaches have become popular, involving statistical models trained on large corpora. The quality of the generated text is highly dependent on the model's quality. Previous work has shown that well-trained RNN models outperform simpler language models like N-grams [21], and RNN-based language models have emerged as a promising approach for text generation [14, 67]. Researchers have successfully generated text for various domains, including email responses [23], image descriptions [24], movie dialogues [64], and online social network conversations [65].

#### Adversarial Attacks and Defenses

Similar to our work, a few studies have investigated the feasibility of attacking online systems using Deep Neural Networks. These include proposals to automatically solve CAPTCHAs using Convolutional Neural Networks [13, 60] and to generate malware domains using Generative Adversarial Networks [1]. To our knowledge, our work is the first to explore attacks on online review systems using Deep Neural Networks.

#### Crowdturfing and Review Spam Detection

Previous work has characterized crowdturfing marketplaces that supply human labor to enable attacks on various online platforms, such as review systems, social media, and search engines [29, 73, 74]. Wang et al. have explored the detection of malicious crowdworkers using machine learning and the robustness of classifiers against evasive tactics. Additionally, researchers have extensively studied the detection of opinion spam or fake reviews in online review systems using features based on review content [32, 48, 53] and metadata [10, 19, 20]. Our study differs by focusing on automatically generating fake reviews that can evade detection by advanced machine learning classifiers and human investigation.

### Discussion and Conclusion

In this work, we focus on the potential misuse of deep learning models in attacking online review platforms. We demonstrate how RNNs can generate deceptive yet realistic-looking reviews targeting restaurants on Yelp. An extensive evaluation of the generated reviews indicates the difficulty in detecting them using existing algorithmic approaches and even by human examination.

We propose a novel defense approach that leverages a fundamental limitation of RNN-based models: information loss during the training process when fitting a large dataset to a fixed-size statistical model. Due to this information loss, generated reviews diverge from real reviews in their character-level distribution, even when higher-level linguistic characteristics are preserved. Our supervised learning-based scheme can detect machine-generated reviews with high accuracy (F-score ranging from 0.8 to 0.98, depending on the amount of available ground-truth) and outperforms existing ML-based fake review filters.

### Future Work

Future research could explore the role of user and content metadata in both attack and defense. Metadata can be crucial for deceiving users (e.g., by increasing the number of friends/contacts) and assisting defenses (e.g., by analyzing patterns in timestamps of user activities). Another direction is to apply deep learning-based generative text models to launch attacks in other scenarios, such as strengthening Sybil attacks and generating fake news. We hope our results will draw attention to the problem of malicious attacks using deep learning language models, particularly in the context of fake content on online services, and encourage the development of new defenses.

### Review Customization Details

Algorithm 2 provides the details of the review customization process (Section 3.1).

### Examples of Generated Fake Reviews

We present examples of generated reviews targeting a specific restaurant, generated under different temperatures and from models trained on five-star, three-star, and one-star Yelp training review corpora.

**Target Restaurant Description:**
Experience MKT, a market-to-table San Francisco restaurant and bar. Taste your way through ultra-creative handcrafted cocktails, small plates for sharing, and steaks from the finest California ranches. Meet friends and relax in our retro urban-chic atmosphere. In the heart of SoMa - with panoramic fifth-floor views of Market Street and downtown - this is one of San Francisco’s best restaurant venues for social drinks and dining.

**One-Star Reviews:**
- **Temperature 0.1:**
  "I was so excited to try this place out for the first time, but the food was awful. I ordered the chicken sandwich, and it was so salty that I could not eat it. I was very disappointed with the food and the service."

- **Temperature 0.3:**
  "I had a drink at this restaurant. I was there for a couple of months. The restaurant was clean, but the food was not good. The service was very slow, and I would not recommend this place to anyone."

- **Temperature 0.5:**
  "The worst place I’ve ever been to. I waited for a while, and the service was absolutely terrible. The server was rude and unprofessional. I will not be going back to this location."

- **Temperature 0.7:**
  "I have been here twice, and it is very expensive for what you get. We were seated in a line for 15 minutes, and the cashier recommended the drinks but forgot the cookie cover charge. I would never recommend this place to anyone."

- **Temperature 1.0:**
  "DO NOT WASTE YOUR TIME AND MONEY! The absolute worst service I have ever experienced. This place is a joke. The waitress was rude and said she would put the manager to come out, but it never happened. I wish I could give zero stars."

**Three-Star Reviews:**
- **Temperature 0.1:**
  "The food was good but not great. I had the lasagna, and the sauce was a little too sweet for my taste. The salad was good, but the sauce was a little too sweet for my taste."

- **Temperature 0.3:**
  "I’ve had the fried chicken and the chicken breast with chicken soup and sweet potato fries. The sauce was a bit salty and didn’t have much flavor. The salad was tasty and well presented, but the bread was super tasty."

- **Temperature 0.5:**
  "I was here for a weekend brunch, and the food was ok. I love the pizza, and the service is excellent. I had the spaghetti, and it was very good, but the hot dog was good. The red velvet chocolate cake special was very good, but the service was a little slow. The food was good, but not up to par with other places nearby."

- **Temperature 0.7:**
  "The food wasn’t bad. The cupcakes are okay, and the service is excellent, but the prices are a bit high. I do like the fresh-made salad and drink specials. I would recommend this place for a quick bite a couple of times."

- **Temperature 1.0:**
  "Came here for lunch today, and the place was pretty empty. The steak was good, but the overall experience was just okay."