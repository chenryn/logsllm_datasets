### 1. Introduction
We observed that the remote host responded with small (approximately 40-byte) TCP RST packets at intervals of 10 ms and 30 ms. This packet train did not saturate either the downstream or upstream links.

### 2. Probe Packet Size
We used 1,488-byte probes because some DSL links running PPPoE or PPPoA have a Maximum Transmission Unit (MTU) of less than 1,500 bytes.

### 3. Measured Broadband Link Properties
Our measurements are based on the simplifying assumption that the broadband access link is the only bottleneck along the Internet path between our measurement hosts and the remote broadband hosts (Figure 2). We validate this assumption in the next section. This section describes how we measured the properties of the broadband links based on this assumption.

#### 3.1. Link Bandwidth
To estimate the allocated downstream bandwidth, we calculated the fraction of answered probes in the large-TCP flood, which saturates the downstream link only. For example, if 60% of packets in our 10 Mbps large-TCP flood are answered, we estimate the downstream bandwidth to be 6 Mbps. We used a similar technique to estimate upstream bandwidths from the symmetric large-ICMP flood. The behavior of the large-ICMP flood is driven by the bandwidth of the slower link, which for cable and DSL is the upstream link.

Our techniques yield incorrect estimates in the presence of cross-traffic. To address this, we used IPID-based techniques described in [23] to identify and eliminate all measurement probes affected by cross-traffic.

#### 3.2. Packet Latencies and Jitter
We characterized three types of packet delays and their variation (jitter) for each link: queuing delay, propagation delay, and transmission delay.

- **Queuing Delay**: We estimated the maximum possible queuing delays by calculating the variation in Round-Trip Times (RTTs) of packets in our floods. To determine downstream queue lengths, we calculated the difference between the 95th percentile highest RTTs and minimum RTTs of packets in the large-TCP flood, which overflows only the downstream router queues. A similar calculation for the large-ICMP flood, which overflows queues in both directions, estimated the sum of downstream and upstream queue lengths. We subtracted the downstream queue length from this estimate to obtain the length of the upstream queue.
  
- **Propagation Delay**: We estimated the last-hop delays by calculating the difference between the latencies of small-TCP trickle probes to the broadband host and to its last-hop router. By comparing the last-hop delays for different packet sizes, we inferred the transmission delays in broadband links. We discuss transmission delays in more detail in Section 4.2.

- **Transmission Delay**: We discuss the details of transmission delays in Section 4.2.

#### 3.3. Packet Loss
We estimated typical packet loss rates in broadband networks by calculating the fraction of lost packets in the small-TCP trickle. To detect packet loss due to queue management policies, such as Random Early Detection (RED), we examined how the loss rate varies with the latencies of the packets. We discuss the details of RED detection in Section 4.3.

### 4. Validating Our Assumptions
Next, we discuss five important concerns about our methodology:

1. **Accuracy of Probes**: Our probes must traverse the entire Internet path reaching the broadband host and not be answered by an intermediate router. Do our measurements accurately reflect the properties of broadband access links?
   
2. **Bottleneck Assumption**: We assumed that the broadband links are the bottlenecks in the measured Internet paths. How often are broadband links the bottlenecks along the measured Internet paths?

3. **Host Response Delays**: We assumed broadband hosts respond to all probes without any delay. In practice, end hosts could drop or rate limit their responses. How often do broadband hosts delay or drop response packets?

4. **Ethical Considerations**: Our probes can be interpreted as port scans or attacks. What are the best practices we used in our measurements?

5. **Study Limitations**: Large-scale Internet studies suffer from limitations and shortcomings. What are some of the limitations of our study?

#### 4.1. Do Our Measurements Reflect Accurately the Properties of Broadband Access Links?
We ran controlled experiments using five broadband hosts (two cable and three DSL) under our control, located in North America and Europe. These experiments were performed on a small scale because they required end-host cooperation. Although we hoped to recruit more volunteers, the effort required to set up our experiments made it difficult to convince users to perform them. Our experiments require root access and manual changes to the modems’ firewalls.

- **Probe Path Verification**: We checked whether the probe packets were being sent over the broadband link or whether they were being answered by a router in the middle of the network. We found that in all cases, the probes were being responded to by the NAT-enabled modems in the customers’ premises. By configuring the modems to forward any arriving probe packets to end hosts, we were able to receive the probes at our end hosts (Figure 2).

- **Modem Impact**: We gathered two traces for each link: one when the modem responded to the probes, and another when the modem forwarded all probes to the broadband hosts. We configured the broadband hosts to respond to the probes without any delay (less than 100 µs) or rate-limiting. We compared the two traces with respect to latencies and losses of probes and responses. The two traces matched closely in all cases, suggesting that the modems do not adversely affect our measurements.

- **Bandwidth and Queue Length Accuracy**: We compared the measured bandwidths of the access links with the rate speeds advertised by their ISPs. We found that these bandwidths matched very closely—the average difference in downstream bandwidths was less than 3%. To validate our queue length estimates, we used our access to the end hosts to measure the upstream and downstream queue lengths separately and accurately. The measurements matched the estimated queue lengths very well, suggesting that both our bandwidth and queue length measurements are accurate.

#### 4.2. How Often Are Broadband Links the Bottlenecks Along the Measured Internet Paths?
Our methodology assumes the broadband link is the bottleneck on the Internet path measured. Because our probes are sent from well-connected academic hosts, the broadband links are likely to be the bottlenecks in these paths. To validate this assumption, we sent a large-TCP flood probe train to the broadband host and another train to its last-hop router. Comparing these two probe trains revealed that the broadband links are indeed the bottlenecks.

- **Comparison of Paths**: Figure 3 compares the available bandwidth, the RTT increases, and the packet loss rate of the two traces for 1,173 randomly selected broadband hosts. Most paths to the last-hop routers achieved the full 10 Mbps throughput, experiencing almost no losses or RTT fluctuations. By contrast, the paths including the broadband link had much lower throughput, considerable RTT increases, and high packet loss. This suggests that these variations are caused by the last hop (i.e., the broadband link).

#### 4.3. How Often Do Broadband Hosts Delay or Drop Response Packets?
Our methodology assumes broadband hosts respond to probes without any delay. Several factors could prevent hosts from responding to some or all of our probes. For example, a firewall may block certain types of probes, such as PINGs. Some routers add a delay between the arrival of a probe and the departure of the response [21]. Also, a host with limited processing power might delay or drop packets arriving at high rates.

- **Host Removal**: We removed all hosts that did not respond to our probes. We also removed the broadband hosts that rate-limited their probe responses. We identified such hosts by checking for large loss episodes occurring periodically.

- **Processing Power Experiment**: We performed an experiment to check whether our probe trains were too aggressive for the processing power of some hosts. We sent probe trains at 10 Mbps but with varying packet sizes. Although the trains consumed the same bandwidth, their packet sending rates were different. We checked whether hosts experienced higher losses at faster sending rates. A higher loss rate suggests that an end host cannot process packets at fast rates. We checked how losses vary with packet sending rates for all broadband hosts in our study. The loss rates remained constant for over 99% of the hosts in our study, suggesting that the end hosts have sufficient processing power to handle our probing rates.

#### 4.4. Best Practices Adopted
Performing active measurements on the Internet raises important usage concerns. Although it is difficult to address and eliminate all such concerns, we adopted a set of precautions to mitigate these concerns. We restricted our high-rate probe trains to no more than 10 seconds each. We also embedded a custom message in each of our probe packets, which described the experiment and included a contact email address. To date, we have not received any complaints.

- **Cost Concerns**: Another cause for concern was that users with a per-byte payment model end up paying for our unsolicited traffic. To mitigate this concern, we only measured hosts in ISPs that offer flat-rate payment plans, and we restricted the total amount of data sent to any single broadband host over our entire study.

#### 4.5. Limitations of Our Study
Two important limitations affect our measurements. First, we studied only major cable and DSL ISPs in North America and Europe. Our conclusions are unlikely to generalize to high-speed fiber-based broadband ISPs, such as those in Japan or South Korea [12]. Second, we removed all hosts that did not respond to our probes or that were rate-limited, which could introduce some unknown bias.

### 5. Characterizing Broadband Links
In this section, we analyze the data gathered from sending probe packet trains to a large number of residential broadband hosts in several major ISPs (see Table 1). We examine three important characteristics of broadband networks: link bandwidths, packet latencies, and packet loss. Analyzing these properties is important because they affect the performance of protocols and systems running over broadband.

#### 5.1. Allocated Link Bandwidth
Allocated link bandwidth refers to the bandwidth reserved by a provider to a single broadband user. In cable networks, allocated link bandwidth is the portion of the shared link’s capacity assigned to an individual user, whereas in DSL networks, it is the ISP’s cap on a user’s traffic rate. Characterizing allocated link bandwidths in broadband networks helps predict the maximum throughput any transport protocol (such as TCP Reno or TCP Vegas) or application (such as BitTorrent) can achieve. As described in Section 3.3, our probe streams measured allocated bandwidths by saturating the broadband links.

- **Allocated Link Bandwidths**: Figures 4(a) and (b) show the cumulative distributions of allocated downstream and upstream link bandwidths.