To ensure fault tolerance with a minimal number of servers, the system requires 3f + 1 servers to tolerate f faults. The trade-off for this minimal replication is that data must be copied to any new servers added to the system.

When additional machines are available, extra replicas can be used to expedite view changes. This capability is provided through the introduction of a new parameter, the spread (m). When m is non-zero, quorum operations involve more servers than strictly necessary. This margin allows the quorums to still intersect when a few new servers are added, enabling these view changes to proceed quickly. Consequently, there are now two types of view changes: one where data must be copied and one where no copy is necessary. In the latter case, we say that the old and new views belong to the same generation. Each view is tagged with a generation number (g) that is incremented at each generation change.

The parameters m and g, along with N, f, and t, are stored in the view metadata.

The additional servers do not necessarily need to be used to speed up view changes. Using a smaller m with a given n reduces the quorum size and the load on the system. Therefore, the parameter m allows administrators to balance between low load and quick view changes.

### Intra-Generation: Quorums Still Intersect

When clients write using the DQ-RPC operation, their message is received by a quorum of responsive servers. The quorum size depends on the current view parameters t, which are determined during the DQ-RPC operation. For a U-dissemination Byzantine protocol that tolerates b faulty servers, the quorum size is calculated as \( q(n, b, m) = \left\lceil \frac{n + b + 1}{2} + \frac{m}{4} \right\rceil \).

In the absence of view changes, our quorums intersect in \( b + 1 + \frac{m}{2} \) servers. If m new (blank) servers are added to the system, the quorums will still intersect in \( b + 1 \) servers, which is sufficient for correctness. Thus, up to m servers can be added before data must be copied to the new servers.

Similarly, if m servers that were part of a write quorum are removed, new quorums will still intersect in \( b + 1 \) servers, ensuring correct system behavior. If b is increased or reduced by up to m, new quorums will still intersect the old ones in \( b + 1 \) servers.

More generally, if after a write, a servers are added, d servers are removed, b is modified by c, and m is reduced to \( m_{\text{min}} \), the quorums will still intersect sufficiently as long as \( a + d + c \leq m_{\text{min}} \). If a view change would break this inequality, the value must be copied to some of the new servers before the view change completes, indicating that the old and new views are in different generations.

### View Changes: Closing the Generation Gap

Data copying across generations is part of the view change protocol. Unlike the simplified DQ-RPC, the full view change protocol terminates.

View changes are initiated by the administrator when machines need to be added, removed, or moved, or when the resilience f or the spread m needs to be changed. The `newView` method first determines whether the new view will be in the same generation as the previous one using the relation in Section 5.3.1. It then computes the key pairs and certificates for the new view. Finally, the administrator encodes the certificates using the appropriate shared key and sends them to all servers in t, re-sending when necessary and waiting for a quorum of responses.

Servers transition states according to the diagram in Figure 5. When they receive a new view message for a new generation (and they are part of that generation), servers piggyback that message on top of a read from a quorum of the old view. They update their value with what they read (if it is newer) and update their view certificate. If they are part of the new view but there is no generation change, the servers just update their view information. If they are not part of the new view, the servers update their certificates but will not vouch for the new view since they have no valid view certificate for it, but they will still direct clients to the current servers.

Servers are initially in the limbo state and after leaving the view. They are in the joining state while copying information from the older view and in the ready state otherwise. Servers process client requests in all three states. Servers in the joining state use the view certificate for the old view (if they have it) until they are ready.

The administrator's `newView` waits for a quorum of new servers to acknowledge the view change and then posts the new view to well-known locations and returns. At this point, the administrator knows that the data stored in the machines removed from the view are no longer needed, and the old machines can be powered off safely.

There may still be some machines in the joining stage at this point. These machines do not prevent operations from completing because DQ-RPC operations only need \( f + 1 \) servers in the new generation to complete, and any dissemination quorum contains at least \( f + 1 \) correct servers.

When `newView` returns, the old view has ended, and the new view has started and matured, meaning that at least one correct server has processed the view change message. This ensures that reads and writes to the new view will succeed, and reads and writes to the old view will be redirected to the new view (either by the old servers or after consulting the well-known locations).

The protocol requires the administrator to be correct. If the administrator crashes after sending the new view message to a single faulty new server, the new server can cause the servers in the old view to join the limbo state without informing the new servers to start serving. A variant that tolerates such crashes is presented in the extended technical report [15].

### DQ-RPC Satisfies Transquorums for Dissemination Quorums

We now prove our final theorem:

**Theorem 2.** U-dissemination, crash, and hybrid-d based on DQ-RPC provide atomic semantics.

The proof is presented in our technical report [15]. The main lemmas used in the proof are listed below.

**Lemma 3.** The view t chosen by a DQ-RPC operation is concurrent with the DQ-RPC operation.

**Lemma 4.** The DQ-RPC protocol in Figure 4 provides the transquorum properties for the ordering function o of Figure 3.

**Lemma 5.** When using DQ-RPC for the U-dissemination, crash, or hybrid-d protocol, no R operation returns ⊥.

### Conclusions

We present a methodology that easily transforms several existing Byzantine protocols for static quorum systems into corresponding protocols that operate correctly when the administrator is allowed to add or remove servers from the quorum system and change its resilience threshold. Performing the transformation does not require extensive changes to the protocols; it only requires replacing calls to the Q-RPC primitive used in static protocols with calls to DQ-RPC, a new primitive that behaves like Q-RPC in the static case but can handle operations across quorums that may not intersect while still guaranteeing consistency. Our methodology is based on a novel approach for proving the correctness of Byzantine quorum protocols: through our transquorum properties, we specify the characteristics of quorum-level primitives (such as Q-RPC) that are crucial to the correctness of Byzantine quorum protocols and show that it is possible to design primitives, such as DQ-RPC, that implement these properties even when quorums don’t intersect. We hope that designers of new quorum protocols will be able to leverage this insight to easily make their own protocols dynamic.

### Acknowledgments

The authors would like to thank Eunjin Jung and Jeff Napper for several interesting conversations and feedback on the paper presentation.

### References

[1] I. Abraham and D. Malkhi. Probabilistic quorums for dynamic systems. In Proc. 17th Intl. Symp. on Distributed Computing (DISC), Oct. 2003.
[2] L. Alvisi, D. Malkhi, E. Pierce, M. Reiter, and R. Wright. Dynamic Byzantine quorum systems. In Proc. of the Intl. Conference on Dependable Systems and Networks (DSN), June 2000.
[3] L. Alvisi, D. Malkhi, E. Pierce, and M. K. Reiter. Fault detection for Byzantine quorum systems. IEEE Trans. Parallel Distrib. Syst., 12(9):996–1007, 2001.
[4] G. V. Chockler, I. Keidar, and R. Vitenberg. Group communication specifications: a comprehensive study. ACM Computing Surveys (CSUR), 33(4):427–469, 2001.
[5] S. Davidson, H. Garcia-Molina, and D. Skeen. Consistency in a partitioned network: a survey. ACM Computing Surveys (CSUR) Volume 17, Issue 3, pages 341–370, Sept. 1985.
[6] S. Dolev, S. Gilbert, N. Lynch, A. Shvartsman, and J. Welch. Geoquorums: Implementing atomic memory in mobile ad hoc networks. In Proc. 17th Intl. Symp. on Distributed Computing (DISC), Oct. 2003.
[8] S. Gilbert, N. Lynch, and A. Shvartsman. RAMBO II: Rapidly reconﬁgurable atomic memory for dynamic networks. In Proc. 17th Intl. Symp. on Distributed Computing (DISC), pages 259–268, June 2003.
[9] G. R. Goodson, J. J. Wylie, G. R. Ganger, and M. K. Reiter. Efficient consistency for erasure-coded data via versioning servers. Technical Report CMU-CS-03-127, Carnegie Mellon University, 2003.
[10] L. Kong, A. Subbiah, M. Ahamad, and D.M. Blough. A reconﬁgurable Byzantine quorum approach for the agile store. In Proc. 22nd Intl. Symp. on Reliable Distributed Systems (SRDS), Oct. 2003.
[11] L. Lamport. On interprocess communications. Distributed Computing, pages 77–101, 1986.
[12] N. Lynch and A. Shvartsman. RAMBO: A reconﬁgurable atomic memory service for dynamic networks. In Proc. 16th Intl. Symp. on Distributed Computing (DISC), pages 173–190, Oct. 2002.
[13] D. Malkhi and M. Reiter. Byzantine quorum systems. Distributed Computing 11/4, pages 203–213, 1998.
[14] D. Malkhi and M. Reiter. Secure and scalable replication in Phalanx. In Proc. 17th IEEE Symp. on Reliable Distributed Systems (SRDS), Oct 1998.
[15] J-P. Martin and L. Alvisi. A framework for dynamic Byzantine storage. Technical Report TR04-08, The University of Texas at Austin, 2004.
[16] J-P. Martin, L. Alvisi, and M. Dahlin. Minimal Byzantine storage. In Proc. 16th Intl. Symp. on Distributed Computing (DISC), pages 311–325, Oct. 2002.
[17] J-P. Martin, L. Alvisi, and M. Dahlin. Small Byzantine quorum systems. In Proc. of the Intl. Conference on Dependable Systems and Networks (DSN), pages 374–383, June 2002.
[18] E. Pierce and L. Alvisi. A framework for semantic reasoning about Byzantine quorum systems. In Brief Announcements, Proc. 20th Symp. on Principles of Distributed Computing (PODC), pages 317–319, Aug. 2001.
[19] R. Rodrigues, B. Liskov, and L. Shrira. The design of a robust peer-to-peer system. In Tenth ACM SIGOPS European Workshop, Sept. 2002.