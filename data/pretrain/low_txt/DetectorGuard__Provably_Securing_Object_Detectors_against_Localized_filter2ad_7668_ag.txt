### References

1. **Tramer, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018.** "Physical Adversarial Examples for Object Detectors." In *12th USENIX Workshop on Offensive Technologies (WOOT 18)*.

2. **Geiger, Andreas, Philip Lenz, Christoph Stiller, and Raquel Urtasun. 2013.** "Vision Meets Robotics: The KITTI Dataset." *The International Journal of Robotics Research* 32, no. 11 (2013): 1231–1237.

3. **Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. 2015.** "Explaining and Harnessing Adversarial Examples." In *3rd International Conference on Learning Representations (ICLR)*.

4. **Gowal, Sven, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Arthur Mann, and Pushmeet Kohli. 2019.** "Scalable Verified Training for Provably Robust Image Classification." In *2019 IEEE/CVF International Conference on Computer Vision (ICCV)*, 4841–4850.

5. **Hayes, Jamie. 2018.** "On Visible Adversarial Perturbations & Digital Watermarking." In *2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)*, 1597–1604.

6. **He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. 2017.** "Mask R-CNN." In *IEEE International Conference on Computer Vision (ICCV 2017)*, 2980–2988.

7. **He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.** "Deep Residual Learning for Image Recognition." In *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 770–778.

8. **Karmon, Danny, Daniel Zoran, and Yoav Goldberg. 2018.** "LaVAN: Localized and Visible Adversarial Noise." In *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 2512–2520.

9. **Lécuyer, Mathias, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2019.** "Certified Robustness to Adversarial Examples with Differential Privacy." In *2019 IEEE Symposium on Security and Privacy (S&P)*, 656–672.

10. **Lee, Mark, and Zico Kolter. 2019.** "On Physical Adversarial Patches for Object Detection." *arXiv preprint arXiv:1906.11897* (2019).

11. **Levine, Alexander, and Soheil Feizi. 2020.** "(De)randomized Smoothing for Certifiable Defense against Patch Attacks." *arXiv preprint arXiv:2002.10733* (2020).

12. **Lin, Tsung-Yi, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. 2017.** "Focal Loss for Dense Object Detection." In *IEEE International Conference on Computer Vision (ICCV 2017)*, 2999–3007.

13. **Lin, Tsung-Yi, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.** "Microsoft COCO: Common Objects in Context." In *European Conference on Computer Vision (ECCV)*, Vol. 8693, 740–755.

14. **Liu, Aishan, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie, and Dacheng Tao. 2019.** "Perceptual-Sensitive GAN for Generating Adversarial Patches." In *The 33rd AAAI Conference on Artificial Intelligence (AAAI 2019)*, 1028–1035.

15. **Liu, Aishan, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, and Hang Yu. 2020.** "Bias-Based Universal Adversarial Patch Attack for Automatic Checkout." In *European Conference on Computer Vision (ECCV)*, Vol. 12358, 395–410.

16. **Liu, Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. 2016.** "SSD: Single Shot MultiBox Detector." In *European Conference on Computer Vision (ECCV)*, Vol. 9905, 21–37.

17. **Liu, Xin, Huanrui Yang, Ziwei Liu, Linghao Song, Yiran Chen, and Hai Li. 2019.** "DPATCH: An Adversarial Patch Attack on Object Detectors." In *AAAI Conference on Artificial Intelligence Workshop (AAAI Workshop) 2019*, Vol. 2301.

18. **Lu, Jiajun, Hussein Sibai, and Evan Fabry. 2017.** "Adversarial Examples that Fool Detectors." *arXiv preprint arXiv:1712.02494* (2017).

19. **Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018.** "Towards Deep Learning Models Resistant to Adversarial Attacks." In *6th International Conference on Learning Representations (ICLR)*.

20. **McCoyd, Michael, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu, and David Wagner. 2020.** "Minority Reports Defense: Defending Against Adversarial Patches." *arXiv preprint arXiv:2004.13799* (2020).

21. **Meng, Dongyu, and Hao Chen. 2017.** "MagNet: A Two-Pronged Defense against Adversarial Examples." In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)*, 135–147.

22. **Metzen, Jan Hendrik, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017.** "On Detecting Adversarial Perturbations." In *5th International Conference on Learning Representations (ICLR)*.

23. **Metzen, Jan Hendrik, and Maksym Yatsura. 2021.** "Efficient Certified Defenses Against Patch Attacks on Image Classifiers." In *9th International Conference on Learning Representations (ICLR)*. [https://openreview.net/forum?id=hr-3PMvDpil](https://openreview.net/forum?id=hr-3PMvDpil)

24. **Mirman, Matthew, Timon Gehr, and Martin T. Vechev. 2018.** "Differentiable Abstract Interpretation for Provably Robust Neural Networks." In *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 3575–3583.

25. **Naseer, Muzammal, Salman Khan, and Fatih Porikli. 2019.** "Local Gradients Smoothing: Defense Against Localized Adversarial Attacks." In *IEEE Winter Conference on Applications of Computer Vision (WACV)*, 1300–1307.

26. **Papernot, Nicolas, Patrick McDaniel, Arunesh Sinha, and Michael P. Wellman. 2018.** "SoK: Security and Privacy in Machine Learning." In *2018 IEEE European Symposium on Security and Privacy (EuroS&P)*, 399–414.

27. **Papernot, Nicolas, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016.** "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks." In *IEEE Symposium on Security and Privacy (S&P)*, 582–597.

28. **Raghunathan, Aditi, Jacob Steinhardt, and Percy Liang. 2018.** "Certified Defenses against Adversarial Examples." In *6th International Conference on Learning Representations (ICLR)*.

29. **Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016.** "You Only Look Once: Unified, Real-Time Object Detection." In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 779–788.

30. **Redmon, Joseph, and Ali Farhadi. 2017.** "YOLO9000: Better, Faster, Stronger." In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 7263–7271.

31. **Redmon, Joseph, and Ali Farhadi. 2018.** "YOLOv3: An Incremental Improvement." *arXiv preprint arXiv:1804.02767* (2018).

32. **Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2015.** "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks." In *Advances in Neural Information Processing Systems*, 91–99.

33. **Saha, Aniruddha, Akshayvarun Subramanya, Koninika Patil, and Hamed Pirsiavash. 2020.** "Role of Spatial Context in Adversarial Robustness for Object Detection." In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)*, 784–785.

34. **Salman, Hadi, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang, Sébastien Bubeck, and Greg Yang. 2019.** "Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers." In *Annual Conference on Neural Information Processing Systems 2019 (NeurIPS)*, 11289–11300.

35. **Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.** "Intriguing Properties of Neural Networks." In *2nd International Conference on Learning Representations (ICLR)*.

36. **Tan, Mingxing, Ruoming Pang, and Quoc V. Le. 2020.** "EfficientDet: Scalable and Efficient Object Detection." In *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020)*, 10781–10790.

37. **Thys, Simen, Wiebe Van Ranst, and Toon Goedemé. 2019.** "Fooling Automated Surveillance Cameras: Adversarial Patches to Attack Person Detection." In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*.

38. **Tramer, Florian, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. 2020.** "On Adaptive Attacks to Adversarial Example Defenses." *arXiv preprint arXiv:2002.08347* (2020).

39. **Vahab, Abdul, Maruti S. Naik, Prasanna G. Raikar, and Prasad SR. 2019.** "Applications of Object Detection System." *International Research Journal of Engineering and Technology (IRJET)* 6, no. 4 (2019): 4186–4192.

40. **Wang, Chien-Yao, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. 2020.** "Scaled-YOLOv4: Scaling Cross Stage Partial Network." *arXiv preprint arXiv:2011.08036* (2020).

41. **Wang, Derui, Chaoran Li, Sheng Wen, Xiaojun Chang, Surya Nepal, and Yang Xiang. 2019.** "Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples." *arXiv preprint arXiv:1902* (2019).

42. **Wei, Xingxing, Siyuan Liang, Ning Chen, and Xiaochun Cao. 2019.** "Transferable Adversarial Attacks for Image and Video Object Detection." In *Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI 2019)*, edited by Sarit Kraus. ijcai.org, 954–960.

43. **Wong, Eric, and J. Zico Kolter. 2018.** "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope." In *Proceedings of the 35th International Conference on Machine Learning (ICML)*, 5283–5292.

44. **Wu, Zuxuan, Ser-Nam Lim, Larry S. Davis, and Tom Goldstein. 2020.** "Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors." In *European Conference on Computer Vision (ECCV) 2020*, Vol. 12349, 1–17.

45. **Xiang, Chong, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. 2021.** "PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking." In *30th USENIX Security Symposium (USENIX Security)*.

46. **Xiang, Chong, and Prateek Mittal. 2021.** "PatchGuard++: Efficient Provable Attack Detection against Adversarial Patches." In *ICLR 2021 Workshop on Security and Safety in Machine Learning Systems*.

47. **Xie, Cihang, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan L. Yuille. 2017.** "Adversarial Examples for Semantic Segmentation and Object Detection." In *IEEE International Conference on Computer Vision (ICCV 2017)*, 1378–1387.

48. **Xu, Kaidi, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, and Xue Lin. 2020.** "Adversarial T-Shirt! Evading Person Detectors in a Physical World." In *European Conference on Computer Vision (ECCV) 2020*, Vol. 12350, 665–681.

49. **Xu, Weilin, David Evans, and Yanjun Qi. 2018.** "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks." In *25th Annual Network and Distributed System Security Symposium (NDSS)*.

50. **Yuan, Xiaoyong, Pan He, Qile Zhu, and Xiaolin Li. 2019.** "Adversarial Examples: Attacks and Defenses for Deep Learning." *IEEE Transactions on Neural Networks and Learning Systems* 30, no. 9 (2019): 2805–2824.

51. **Zhang, Haichao, and Jianyu Wang. 2019.** "Towards Adversarially Robust Object Detection." In *2019 IEEE/CVF International Conference on Computer Vision (ICCV 2019)*, 421–430.

52. **Zhang, Zhanyuan, Benson Yuan, Michael McCoyd, and David Wagner. 2020.** "Clipped BagNet: Defending Against Sticker Attacks with Clipped Bag-of-Features." In *3rd Deep Learning and Security Workshop (DLS)*.

53. **Zhao, Yue, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai Chen. 2019.** "Seeing Isn’t Believing: Towards More Robust Adversarial Attack Against Real-World Object Detectors." In *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security*, 1989–2004.

### Appendix A: Object Size and Patch Size

In Section 5.4, we used a 32×32 patch on 416×416 (or 224×740) images to evaluate provable robustness. This section provides additional details on object sizes and patch sizes in the PASCAL VOC, MS COCO, and KITTI datasets.

Small objects are the majority in all three datasets. Figure 8 shows the histogram of object sizes (in percentage of pixels) in the test/validation sets of PASCAL VOC, MS COCO, and KITTI. As shown in the plots, small or even tiny objects are the majority in these datasets. A 32×32 patch covers 0.6% of the pixels in a 416×416 (or 224×740) image. Our analysis reveals that 15.2% of PASCAL VOC objects, 44.5% of MS COCO objects, and 44.6% of KITTI objects are smaller than 0.6% of the image pixels. Additionally, more than 36.5% of PASCAL VOC objects, more than 66.3% of MS COCO objects, and 75.9% of KITTI objects are smaller than a 64×64 square. These numbers explain why the absolute numbers of certified recall in Table 3 are low.

Figure 9 further visualizes a 32×32 patch on a 416×416 image, demonstrating the challenge of perfect robust detection even with a small patch. In the left two examples, the person and the cow are completely blocked by the adversarial patch and thus are unrecognizable. In the rightmost example, the head of the dog is patched, making it difficult even for humans to determine if it is a dog or a cat.

**Additional Evaluation Results for Different Patch Sizes:**

Figure 10 varies the patch size to see how provable robustness is affected given different attacker capabilities (i.e., patch sizes). If we consider a smaller patch of 8×8 pixels, we can achieve a 2.0% higher CR for close-patch and a 2.8% higher CR for over-patch compared to our CRs for a 32×32 patch. Furthermore, in the far-patch model, the patch size has a limited influence on provable robustness. From Figure 10, we can also see that the CR decreases as the patch size increases. This analysis highlights the limitations of DetectorGuard and the challenge of robust object detection with larger patch sizes. We aim to push this limit further in future work.

### Appendix B: Additional Discussion of Robust Classifier Implementation

As introduced in Section 2.4, state-of-the-art provable robust image classifiers [36, 58] use DNNs with small receptive fields to bound the number of corrupted features and then perform secure aggregation on the partially corrupted feature map for robust classification. In DetectorGuard, we use BagNet [3] for small receptive fields and feature clipping for secure aggregation. This section provides additional details on BagNet and clipping aggregation. We also discuss alternative aggregation mechanisms and implement robust masking [58] to demonstrate the generality of our DetectorGuard framework.

**BagNet [3]:** BagNet was originally proposed for interpretable machine learning. It inherits the high-level architecture of ResNet-50 [20] and replaces 3x3 convolution kernels with 1x1 ones to reduce the receptive field size. The authors designed three BagNet architectures with small receptive fields of 9×9, 17×17, and 33×33, in contrast to ResNet-50, which has a receptive field of 483×483.

**Comparison Between Masking-Based and Clipping-Based Defenses of DetectorGuard:**

| Dataset | FAR | CR-far | CR-close | CR-over | AP |
|---------|-----|--------|----------|---------|----|
| **PASCAL VOC** | 99.3% | 0.9% | 99.0% | 1.2% | 99.0% | 1.5% |
| **MS COCO** | 28.6% | 31.6% | 7.0% | 8.3% | - |
| **KITTI** | 11.5% | 20.7% | 11.0% | 2.2% | - |

| Dataset | FAR | CR-far | CR-close | CR-over |
|---------|-----|--------|----------|---------|
| **PASCAL VOC** | 26.2% | 17.4% | 5.4% | - |
| **MS COCO** | 11.4% | 17.2% | 4.9% | - |
| **KITTI** | 11.4% | 17.2% | 4.9% | - |

This table compares the performance of clipping-based and masking-based defenses in DetectorGuard using a perfect clean detector.