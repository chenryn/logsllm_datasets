### Validation of Implementation Correctness

We validated the correctness of our implementation by generating input event traces for each synthesized module from the simulations described in §4. These traces were then used as inputs in the test bench employed for RTL verification using the Vivado Design Suite. The output traces generated from this process were compared with the corresponding output traces obtained from the simulator. Additionally, we utilized the Vivado HLS tool to export our RTL design and create IP blocks for our modules.

### 6.2.2 Synthesis Results

Our FPGA synthesis report is summarized in Table 2 and discussed below.

#### Resource Usage
The second and third columns in Table 2 detail the percentage of flip-flops (FF) and look-up tables (LUT) used for the four modules. No BRAM or DSP48E units were consumed. Each of IRN's packet processing modules consumes less than 1% FFs and 2% LUTs, with a total consumption of 1.35% FFs and 4% LUTs. Increasing the bitmap size to support 100Gbps links consumed a total of 2.66% FFs and 9.5% LUTs on the same device. We expect the relative resource usage to be smaller on a higher-scale device designed for 100Gbps links.

#### Performance
The third and fourth columns in Table 2 report the worst-case latency and throughput, respectively, for each module. The latency added by each module is at most 16.5ns. The `receiveData` module, which requires more complex bitmap operations, had the lowest throughput of 45.45Mpps. This throughput is sufficient to sustain a rate of 372Gbps for MTU-sized packets, which is higher than the maximum rate of 39.5Mpps observed on Mellanox MCX416A-BCAT RoCE NIC across different message sizes (2 bytes - 1KB), even after applying various optimizations such as batching and using multiple queue-pairs. A similar message rate was observed in prior work [25]. Note that we did not use pipelining within our modules, which could further improve throughput.

While IRN is expected to be implemented on an ASIC integrated with the existing RoCE implementation, the modest resources used on an FPGA board, supported as an add-on in recent RDMA-enabled NICs, provide some insight into the feasibility of the changes required by IRN. It is important to note that the results reported here are far from the optimal results that can be achieved on an ASIC implementation due to two sources of sub-optimality: (i) using HLS for FPGA synthesis has been found to be up to 2× less optimal than directly using Verilog [27], and (ii) FPGAs, in general, are known to be less optimal than ASICs.

### 6.3 Impact on End-to-End Performance

We now evaluate how IRN's implementation overheads impact the end-to-end performance. We identify two implementation aspects that could potentially affect end-to-end performance and model these in our simulations.

#### Delay in Fetching Retransmissions
While regular packets sent by a RoCE NIC are typically pre-fetched, we assume that the DMA request for retransmissions is sent only after the packet is identified as lost (i.e., when loss recovery is triggered or when a look-ahead is performed). The time taken to fetch a packet over PCIe is typically between a few hundred nanoseconds to <2µs [8, 32]. We set a worst-case retransmission delay of 2µs for every retransmitted packet, meaning the sender QP is allowed to retransmit a packet only after 2µs have elapsed since the packet was detected as lost.

#### Additional Headers
As discussed in §5, some additional headers are needed to DMA the packets directly to the application memory. The most extreme case is the 16 bytes of RETH header added to every Write packet. Send data packets have an extra header of 6 bytes, while Read responses do not require additional headers. We simulate the worst-case scenario where all Writes carry 16 bytes of additional header.

#### Results
Figure 12 shows the results for our default scenario after modeling these two sources of worst-case overheads. We find that they make little difference to the end-to-end performance, degrading it by 4-7% compared to IRN without overheads. The performance remains 35%-63% better than our baseline of RoCE (with PFC). We also verified that the retransmission delay of 2µs had a much smaller impact on end-to-end performance (2µs is very small compared to the network round-trip time taken to detect a packet loss and recover from it, which could be of the order of a few hundred microseconds). The slight degradation in performance observed here can almost entirely be attributed to the additional 16 bytes header in every packet. Therefore, we would expect the performance impact to be even smaller when there is a mix of Write and Read workloads.

### 6.4 Summary

Our analysis shows that IRN is well within the limits of feasibility, with small chip area and NIC memory requirements and minor bandwidth overhead. We validated our analysis through extensive discussions with two commercial NIC vendors (including Mellanox); both confirmed that the IRN design can be easily implemented on their hardware NICs. Inspired by the results presented in this paper, Mellanox is considering implementing a version of IRN in their next release.

### 7 Discussion and Related Work

#### Backwards Compatibility
We briefly sketch one possible path to incrementally deploying IRN. We envision that NIC vendors will manufacture NICs that support dual RoCE/IRN modes. The use of IRN can be negotiated between two endpoints via the RDMA connection manager, with the NIC falling back to RoCE mode if the remote endpoint does not support IRN. (This is similar to what was used in moving from RoCEv1 to RoCEv2.) Network operators can continue to run PFC until all their endpoints have been upgraded to support IRN, at which point PFC can be permanently disabled. During the interim period, hosts can communicate using either RoCE or IRN, with no loss in performance.

#### Reordering Due to Load-Balancing
Datacenters today use ECMP for load balancing [23], which maintains ordering within a flow. IRN’s OOO packet delivery support also allows for other load balancing schemes that may cause packet reordering within a flow [20, 22]. IRN’s loss recovery mechanism can be made more robust to reordering by triggering loss recovery only after a certain threshold of NACKs are received.

#### Other Hardware-Based Loss Recovery
MELO [28], a recent scheme developed in parallel to IRN, proposes an alternative design for hardware-based selective retransmission, where out-of-order packets are buffered in an off-chip memory. Unlike IRN, MELO targets PFC-enabled environments with the aim of greater robustness to random losses caused by failures. As such, MELO is orthogonal to our main focus, which is showing that PFC is unnecessary. Nonetheless, the existence of alternate designs such as MELO further corroborates the feasibility of implementing better loss recovery on NICs.

#### HPC Workloads
The HPC community has long been a strong supporter of losslessness. This is primarily because HPC clusters are smaller with more controlled traffic patterns, and hence the negative effects of providing losslessness (such as congestion spreading and deadlocks) are rarer. PFC’s issues are exacerbated on larger scale clusters [23, 24, 29, 35, 38].

#### Credit-Based Flow Control
Since the focus of our work was RDMA deployment over Ethernet, our experiments used PFC. Another approach to losslessness, used by Infiniband, is credit-based flow control, where the downlink sends credits to the uplink when it has sufficient buffer capacity. Credit-based flow control suffers from the same performance issues as PFC: head-of-the-line blocking, congestion spreading, and the potential for deadlocks. We, therefore, believe that our observations from §4 can be applied to credit-based flow control as well.

### 8 Acknowledgment

We would like to thank Amin Tootoonchian, Anirudh Sivaraman, Emmanuel Amaro, and Ming Liu for the helpful discussions on some of the implementation-specific aspects of this work, and Brian Hausauer for his detailed feedback on an earlier version of this paper. We are also thankful to Nandita Dukkipati and Amin Vahdat for the useful discussions in the early stages of this work. We would finally like to thank our anonymous reviewers for their feedback, which helped us in improving the paper, and our shepherd Srinivasan Seshan, who helped shape the final version of this paper. This work was supported in part by a Google PhD Fellowship and by Mellanox, Intel, and the National Science Foundation under Grant Nos. 1704941, 1619377, and 1714508.

### References

[1] http://omnetpp.org/.
[2] https://inet.omnetpp.org.
[3] Xilinx Vivado Design Suite. https://www.xilinx.com/products/design-tools/vivado.html.
[4] InfiniBand architecture volume 1, general specifications, release 1.2.1. www.infinibandta.org/specs, 2008.
[5] Supplement to InfiniBand architecture specification volume 1 release 1.2.2 annex A16: RDMA over Converged Ethernet (RoCE). www.infinibandta.org/specs, 2010.
[6] IEEE. 802.11Qbb. Priority based flow control, 2011.
[7] Vivado Design Suite User Guide. https://goo.gl/akRdXC, 2013.
[8] http://www.xilinx.com/support/documentation/white_papers/wp350.pdf, 2014.
[9] Supplement to InfiniBand architecture specification volume 1 release 1.2.2 annex A17: RoCEv2 (IP routable RoCE). www.infinibandta.org/specs, 2014.
[10] Mellanox ConnectX-4 Product Brief. https://goo.gl/HBw9f9, 2016.
[11] Mellanox ConnectX-5 Product Brief. https://goo.gl/ODlqMl, 2016.
[12] Mellanox Innova Flex 4 Product Brief. http://goo.gl/Lh7VN4, 2016.
[13] RoCE vs. iWARP Competitive Analysis. http://www.mellanox.com/related-docs/whitepapers/WP_RoCE_vs_iWARP.pdf, 2017.
[14] Sarita V Adve and Hans-J Boehm. Memory models: a case for rethinking parallel languages and hardware. Communications of the ACM, 2010.
[15] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. Data Center TCP (DCTCP). In Proc. ACM SIGCOMM, 2010.
[16] Mohammad Alizadeh, Shuang Yang, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. Deconstructing Datacenter Packet Transport. In Proc. ACM Workshop on Hot Topics in Networks (HotNets), 2012.
[17] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker. pFabric: Minimal Near-optimal Datacenter Transport. In Proc. ACM SIGCOMM, 2013.
[18] Appenzeller, Guido and Keslassy, Isaac and McKeown, Nick. Sizing router buffers. In Proc. ACM SIGCOMM, 2004.
[19] Theophilus Benson, Aditya Akella, and David Maltz. Network Traffic Characteristics of Data Centers in the Wild. In Proc. ACM Internet Measurement Conference (IMC), 2012.
[20] Advait Dixit, Pawan Prakash, Y Charlie Hu, and Ramana Rao Kompella. On the impact of packet spraying in data center networks. In Proc. IEEE INFOCOM, 2013.
[21] Aleksandar Dragojević, Dushyanth Narayanan, Miguel Castro, and Orion Hodson. FaRM: Fast Remote Memory. In Proc. USENIX NSDI, 2014.
[22] Soudeh Ghorbani, Zibin Yang, P. Brighten Godfrey, Yashar Ganjali, and Amin Firoozshahian. DRILL: Micro Load Balancing for Low-latency Data Center Networks. In Proc. ACM SIGCOMM, 2017.
[23] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye, and Marina Lipshteyn. RDMA over commodity ethernet at scale. In Proc. ACM SIGCOMM, 2016.
[24] Shuihai Hu, Yibo Zhu, Peng Cheng, Chuanxiong Guo, Kun Tan, Jitendra Padhye, and Kai Chen. Deadlocks in Datacenter Networks: Why Do They Form, and How to Avoid Them. In Proc. ACM Workshop on Hot Topics in Networks (HotNets), 2016.
[25] Anuj Kalia, Michael Kaminsky, and David G. Andersen. Using RDMA Efficiently for Key-value Services. In Proc. ACM SIGCOMM, 2014.
[26] Anuj Kalia, Michael Kaminsky, and David G. Andersen. Design Guidelines for High Performance RDMA Systems. In Proc. USENIX ATC, 2016.
[27] Bojie Li, Kun Tan, Layong (Larry) Luo, Yanqing Peng, Renqian Luo, Ningyi Xu, Yongqiang Xiong, Peng Cheng, and Enhong Chen. ClickNP: Highly Flexible and High Performance Network Processing with Reconfigurable Hardware. In Proc. ACM SIGCOMM, 2016.
[28] Yuanwei Lu, Guo Chen, Zhenyuan Ruan, Wencong Xiao, Bojie Li, Jiansong Zhang, Yongqiang Xiong, Peng Cheng, and Enhong Chen.