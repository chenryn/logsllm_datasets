### Optimizing Machine Learning Security in Adversarial Settings

Securing machine learning (ML) systems, especially deep learning (DL) models, in adversarial settings presents significant challenges due to their inherent complexity. Traditional ML models such as support vector machines and logistic regression are already complex, but DL models, with their significantly higher model complexity, pose even greater security risks [34].

One line of research focuses on developing new evasion attacks against deep neural networks (DNNs) [14, 25, 29, 44]. Another line aims to enhance DNN resilience against such attacks by introducing new training and inference strategies [25, 29, 31, 45]. However, these efforts have not yet considered the potential for DNN models to be exploited as vehicles to compromise entire ML systems.

### Integrity Checks and Malicious Models

Gu et al. [26] highlight the lack of integrity checks in the current practice of reusing primitive models, noting that many publicly available models do not match their hashes upon download. Liu et al. [36] further demonstrate the possibility of crafting malicious ML systems and inputs that can cause compromised systems to misclassify manipulated inputs with high probability. In contrast, this work assumes a more realistic scenario where the adversary has limited capabilities: 
- The compromised DNN model is only one part of the end-to-end system.
- The adversary has no knowledge or control over the system design choices or tuning strategies.
- The adversary cannot influence the inputs to the system.

Ji et al. [30] explore a similar setting but assume the adversary has access to training data in both source and target domains. Xiao et al. [63] investigate the vulnerabilities (e.g., buffer overflow) of popular deep learning platforms like Caffe, TensorFlow, and Torch. This work, however, addresses the vulnerabilities embedded within DNN models themselves, representing an orthogonal direction in the field.

### Conclusion

This study provides an in-depth analysis of the security implications of using third-party primitive models as building blocks in ML systems. Through four case studies—skin cancer screening, speech recognition, face verification, and autonomous driving—we demonstrated a broad class of model-reuse attacks that can cause host ML systems to malfunction predictably on predefined inputs. Our analytical and empirical results underscore the fundamental characteristics of today's ML models, including high dimensionality, non-linearity, and non-convexity, which make them inherently vulnerable.

We hope this work raises awareness in the security and ML communities about these critical issues. Future research avenues include:
1. Formulating the training of adversarial models as an optimization problem to develop more principled and generic attack models.
2. Exploring attacks that leverage multiple primitive models, such as feature extractors and classifiers, which could be more consequential and harder to detect.
3. Investigating model-reuse attacks against other types of ML systems, such as kernel machines.
4. Implementing and evaluating the countermeasures proposed in § 8 in real-world ML systems to develop effective defenses.

### References

[1] An Open-Source Software Library for Machine Intelligence 2015. https://www.tensorflow.org.

[2] M. Backes, S. Bugiel, and E. Derr. 2016. Reliable Third-Party Library Detection in Android and Its Security Applications. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[3] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. 2010. The Security of Machine Learning. Mach. Learn. 81, 2 (2010), 121–148.

[4] A. Bendale and T. Boult. 2016. Towards Open Set Deep Networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[5] Y. Bengio, A. Courville, and P. Vincent. 2013. Representation Learning: A Review and New Perspectives. IEEE Trans. Pattern Anal. Mach. Intell. 35, 8 (2013), 1798–1828.

[6] R. Bhoraskar, S. Han, J. Jeon, T. Azim, S. Chen, J. Jung, S. Nath, R. Wang, and D. Wetherall. 2014. Brahmastra: Driving Apps to Test the Security of Third-party Components. In Proceedings of USENIX Security Symposium (SEC).

[7] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli. 2013. Evasion Attacks Against Machine Learning at Test Time. In Proceedings of European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD).

[8] B. Biggio, G. Fumera, F. Roli, and L. Didaci. 2012. Poisoning Adaptive Biometric Systems. In Proceedings of Joint IAPR International Workshops on Statistical Techniques in PR and SSPR.

[9] B. Biggio, B. Nelson, and P. Laskov. 2012. Poisoning Attacks against Support Vector Machines. In Proceedings of IEEE Conference on Machine Learning (ICML).

[10] B. Biggio and F. Roli. 2018. Wild Patterns: Ten Years after the Rise of Adversarial Machine Learning. Pattern Recognition 84 (2018), 317–331.

[11] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. 2016. End to End Learning for Self-Driving Cars. ArXiv e-prints (2016).

[12] BVLC. 2017. Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.

[13] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. 2017. VGGFace2: A Dataset for Recognising Faces across Pose and Age. ArXiv e-prints (2017).

[14] N. Carlini and D. Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[15] G. Cauwenberghs and T. Poggio. 2000. Incremental and Decremental Support Vector Machine Learning. In Proceedings of Advances in Neural Information Processing Systems (NIPS).

[16] K. Chen, X. Wang, Y. Chen, P. Wang, Y. Lee, X. Wang, B. Ma, A. Wang, Y. Zhang, and W. Zou. 2016. Following Devil’s Footprints: Cross-Platform Analysis of Potentially Harmful Libraries on Android and iOS. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[17] P. Cooper. 2014. Meet AISight: The Scary CCTV Network Completely Run by AI. http://www.itproportal.com/.

[18] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. 2004. Adversarial Classification. In Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD).

[19] J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12 (2011), 2121–2159.

[20] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun. 2017. Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks. Nature 542, 7639 (2017), 115–118.

[21] R. C. Fong and A. Vedaldi. 2017. Interpretable Explanations of Black Boxes by Meaningful Perturbation. In Proceedings of IEEE International Conference on Computer Vision (ICCV).

[22] M. Fredrikson, S. Jha, and T. Ristenpart. 2015. Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures. In Proceedings of ACM SAC Conference on Computer and Communications (CCS).

[23] GitHub: The World’s Leading Software Development Platform 2008. https://github.com.

[24] I. Goodfellow, Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press.

[25] I. J. Goodfellow, J. Shlens, and C. Szegedy. 2014. Explaining and Harnessing Adversarial Examples. In Proceedings of International Conference on Learning Representations (ICLR).

[26] T. Gu, B. Dolan-Gavitt, and S. Garg. 2017. BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain. ArXiv e-prints (2017).

[27] K. He, X. Zhang, S. Ren, and J. Sun. 2015. Deep Residual Learning for Image Recognition. ArXiv e-prints (2015).

[28] K. Hornik. 1991. Approximation Capabilities of Multilayer Feedforward Networks. Neural Netw. 4, 2 (1991), 251–257.

[29] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari. 2015. Learning with a Strong Adversary. ArXiv e-prints (2015).

[30] Y. Ji, X. Zhang, and T. Wang. 2017. Backdoor Attacks against Learning Systems. In Proceedings of IEEE Conference on Communications and Network Security (CNS).

[31] Y. Ji, X. Zhang, and T. Wang. 2018. EagleEye: Attack-Agnostic Defense against Adversarial Inputs. ArXiv e-prints (2018).

[32] B. Kepes. 2015. eBrevia Applies Machine Learning to Contract Review. https://www.forbes.com/.

[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Proceedings of Advances in Neural Information Processing Systems (NIPS).

[34] Y. LeCun, Y. Bengio, and G. Hinton. 2015. Deep Learning. Nature 521, 7553 (2015), 436–444.

[35] B. Liang, M. Su, W. You, W. Shi, and G. Yang. 2016. Cracking Classifiers for Evasion: A Case Study on the Google’s Phishing Pages Filter. In Proceedings of International Conference on World Wide Web (WWW).

[36] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang. 2018. Trojaning Attack on Neural Networks. In Proceedings of Network and Distributed System Security Symposium (NDSS).

### Appendix

#### DNNs Used in Experiments

Table 11 summarizes the deep neural networks (DNNs) used in the experiments described in § 6 and § 7. Each DNN model is characterized by its name, the case study it was used in, the total number of layers, the total number of parameters, and the number of layers in its feature extractor.

| Case Study | # Layers (FE Only) | # Parameters (End to End) | # Layers | Model |
|------------|-------------------|---------------------------|----------|-------|
| I          | (6+6+6) + 3       | 23,851,784                | 46       | Inception.v3 |
| II         | (6+14+6) + 3      | 17,114,122                | 17       | SpeechNet |
| III        | (14+6+14) + 3     | 145,002,878               | 14       | VGG-Very-Deep-16 |
| IV         | 6+6 +6            | 170,616,961               | 48       | Inception.v3 |
| IV         | 6+14+6            | 248,009,281               | 19       | SpeechNet |
| IV         | 14+6+14           | 325,401,601               | 16       | VGG-Very-Deep-16 |

#### Implementation Details

All models and algorithms were implemented using TensorFlow [1], an open-source software library for numerical computation using data flow graphs. We leveraged TensorFlow's efficient gradient computation to craft adversarial models. Experiments were conducted on a Linux workstation running Ubuntu 16.04, equipped with two Intel Xeon E5 processors and four NVIDIA GTX 1080 GPUs.

#### Parameter Setting

- **Setting of λ**: The parameter λ controls the magnitude of updates to each parameter. Overly small λ may result in excessive iterations, while overly large λ may cause significant impact on non-trigger inputs. We propose a scheme to dynamically adjust λ, similar to Adagrad [19]. At the jth iteration, λ for a parameter w is set as:
  \[
  \lambda = \frac{\lambda_0}{\sqrt{\sum_{\j=1}^j (\phi_+^{(j)}(w))^2 + \epsilon_0}}
  \]
  where \(\lambda_0\) is the initial setting of λ, \(\phi_+^{(j)}(w)\) is the update for the jth iteration, and \(\epsilon_0\) is a smoothing term (set to \(10^{-8}\)).

- **Setting of θ and k**: These parameters are determined empirically (details in § 6 and § 7).

[37] D. Lowd and C. Meek. 2005. Adversarial Learning. In Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD).

[38] J.-H. Luo, J. Wu, and W. Lin. 2017. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression. In Proceedings of IEEE International Conference on Computer Vision (ICCV).

[39] B. Marr. 2017. First FDA Approval For Clinical Cloud-Based Deep Learning In Healthcare. https://www.forbes.com/.

[40] T. Minka. 2016. A Statistical Learning/Pattern Recognition Glossary. http://alumni.media.mit.edu/~tpminka/statlearn/glossary/.

[41] L. Muñoz González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C. Lupu, and F. Roli. 2017. Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization. In Proceedings of ACM Workshop on Artificial Intelligence and Security (AISec).

[42] B. Nelson, B. I. P. Rubinstein, L. Huang, A. D. Joseph, S. J. Lee, S. Rao, and J. D. Tygar. 2012. Query Strategies for Evading Convex-inducing Classifiers. J. Mach. Learn. Res. 13 (2012), 1293–1332.

[43] Pannous 2017. Tensorflow Speech Recognition. https://github.com/pannous/caffe-speech-recognition.

[44] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swamil. 2016. The Limitations of Deep Learning in Adversarial Settings. In Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P).

[45] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. 2016. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[46] O. M. Parkhi, A. Vedaldi, and A. Zisserman. 2015. Deep Face Recognition. In Proceedings of the British Machine Vision Conference (BMVC).

[47] J. Pennington, R. Socher, and C. D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).

[48] R. Polikar, L. Upda, S. S. Upda, and V. Honavar. 2001. Learn++: An Incremental Learning Algorithm for Supervised Neural Networks. Trans. Sys. Man Cyber Part C 31, 4 (2001), 497–508.

[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. Int. J. Comput. Vision 115, 3 (2015), 211–252.

[50] A. Satariano. 2017. AI Trader? Tech Vet Launches Hedge Fund Run by Artificial Intelligence. http://www.dailyherald.com/.

[51] W. J. Scheirer, L. P. Jain, and T. E. Boult. 2014. Probability Models for Open Set Recognition. IEEE Trans. Patt. An. Mach. Intell. 36, 11 (2014), 2317–2324.

[52] E. J. Schwartz, T. Avgerinos, and D. Brumley. 2010. All You Ever Wanted to Know About Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask). In Proceedings of IEEE Symposium on Security and Privacy (S&P).

[53] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M. Young, J.-F. Crespo, and D. Dennison. 2015. Hidden Technical Debt in Machine Learning Systems. In Proceedings of Advances in Neural Information Processing Systems (NIPS).

[54] K. Simonyan and A. Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. ArXiv e-prints (2014).

[55] Y. Sun, X. Wang, and X. Tang. 2014. Deep Learning Face Representation from Predicting 10,000 Classes. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[56] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. 2015. Going Deeper with Convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

[57] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 2015. Rethinking the Inception Architecture for Computer Vision. ArXiv e-prints (2015).

[58] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. 2016. Stealing Machine Learning Models via Prediction APIs. In Proceedings of USENIX Security Symposium (SEC).

[59] A. Versprille. 2015. Researchers Hack into Driverless Car System, Take Control of Vehicle. http://www.nationaldefensemagazine.org/.

[60] P. Warden. 2017. Speech Commands: A Public Dataset for Single-Word Speech Recognition. http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz.

[61] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. 2015. Is Feature Selection Secure against Training Data Poisoning?. In Proceedings of IEEE Conference on Machine Learning (ICML).

[62] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. 2015. Support Vector Machines under Adversarial Label Contamination. Neurocomput. 160, C (2015), 53–62.

[63] Q. Xiao, K. Li, D. Zhang, and W. Xu. 2017. Security Risks in Deep Learning Implementations. ArXiv e-prints (2017).

[64] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. 2017. Understanding Deep Learning Requires Rethinking Generalization. In Proceedings of International Conference on Learning Representations (ICLR).