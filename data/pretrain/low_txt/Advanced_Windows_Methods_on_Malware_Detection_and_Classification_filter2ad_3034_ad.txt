### 4.2.3 Malware Type Classification

The objective of this set of experiments is to evaluate the performance of Method 1 and Method 2 in classifying malware samples into their respective types. As previously discussed, our malicious samples belong to ten distinct malware types (Table 2). The same features used to classify samples as either malicious or benign are employed again to categorize the malicious samples into their specific types.

For robust validation and to ensure that the machine learning models are trained with a fair representation of each type, the dataset for each malware type is split into training and testing sets, comprising 80% and 20% of the samples, respectively. This results in a total of 5687 malware samples for training and 1418 for testing. The training dataset is used to train models using the same five machine learning algorithms as before. According to the experimental results in Figure 3, XGBoost outperforms the other four classifiers, achieving the highest accuracy values: 98.0253% for Method 1 and 97.9548% for Method 2.

We applied 5-fold cross-validation over the five machine learning algorithms, and the results are presented in the bottom half of Figure 4. The accuracy ranges between 91.9% and 93.4%. The decrease in accuracy can be attributed to the small number of samples in some types, such as Backdoor, Virus, Worm, PUP, Hack Tool, and Riskware. To address this issue, we excluded individual types with fewer than 200 samples from the model training. Consequently, the experiments were conducted using only the following four malware types: Trojan, Adware, Spyware, and Ransom. We then reapplied 5-fold cross-validation over the five machine learning algorithms. The results, shown in the top half of Figure 4, indicate that the accuracy improved to a range of 94.6% to 95.5%. These experiments demonstrate that increasing the number of samples for each malware type enhances the performance of both Method 1 and Method 2 in classifying malware samples into their respective types.

### 5. Comparison with State-of-the-Art

In this section, we compare our methods with other works that utilize API arguments in their techniques, specifically [1, 10, 40, 42, 44, 51]. Our comparison focuses on three key aspects: (i) detection accuracy, (ii) required API information, and (iii) limitations.

Several studies have used API arguments to establish malware detection and/or type classification models [1, 10, 40, 42, 44, 51]. Table 5 provides a detailed comparison of these approaches, including their F1 scores, techniques, required API information, and limitations.

- **[10]**: Uses information retrieval theory and TF-IDF weighting.
- **[44]**: Employs random forest, J48 decision tree, and Sequential Minimal Optimization (SMO).
- **[42]**: Utilizes n-gram and various Weka library algorithms.
- **[51]**: Relies on frequency counters of API calls with and without arguments.
- **[40]**: Uses frequent item-sets of the sequence of API calls with and without their arguments.
- **[1]**: Considers statistical features of API calls and their arguments.

Our methods differ from the above studies in several ways:
1. **Resilience to Malware Mutation and Obfuscation**: Our methods are robust against malware mutation and obfuscation techniques because they do not depend on the sequence or pattern of API calls and their arguments. Instead, they consider only the occurrence of API calls and their arguments.
2. **Exclusion of Statistical Features**: Unlike some existing methods, we do not use statistical features such as mean, frequency, or the size of API arguments.
3. **No Domain Knowledge Required**: Our approach does not require domain knowledge of complex arguments, as we implement novel feature generation functions to enhance the extracted API-based features for better processing by machine learning algorithms.
4. **Separate Argument Elements**: None of the existing approaches have studied the possibility of using each argument element of each API call separately, as presented in Method 2.

These advantages make our approach more scalable and less computationally intensive compared to high-dimensional feature spaces. Table 5 summarizes the comparison, showing that our proposed methods outperform the state-of-the-art methods. However, [44] provides the highest F1 score among the six works, so it is used as a baseline. Appendix D presents the evaluation of our proposed methods against the baseline [44] and [51] in terms of recall scores.

### 6. Limitations and Future Work

In this paper, we focused on Windows 7. Future work will explore the performance of our proposed methods on newer versions of Windows, such as Windows 10. Additionally, since Android is also an API-based operating system, we plan to test the performance of our methods on Android platforms.

All experiments were conducted using the Cuckoo sandbox, which limits us to the list of Cuckoo’s hooked API calls [46]. In the future, we aim to design a run-time malware analysis and detection tool that can extract API calls simultaneously while the program is running. Furthermore, we have tested our methods on a dataset of 14879 samples. To evaluate the performance on larger datasets, we are collecting daily and up-to-date malicious samples and generating daily benign samples.

In future work, we plan to leverage the existing model by considering more complex malware classification scenarios, where a single malware sample may perform multiple functionalities. This will require the use of multi-labeling algorithms.

### 7. Conclusions

This paper introduces a new direction for extracting API-based dynamic features by analyzing API calls together with their list of arguments. Using machine learning algorithms, we designed two methods to detect Windows malware samples and classify them into their types. The first method treats the entire list of arguments of each API call as one feature, while the second method treats each argument of each API call separately as one feature. We verified the performance of the proposed methods using a dataset of 7105 malicious samples belonging to ten distinct types and 7774 benign samples. Our approach outperformed other recent API arguments-based malware detection methods in terms of accuracy, limitations, and required API information. Experimental results showed that our malware detection approach achieved an accuracy of over 99.8992%, surpassing the state-of-the-art. Additionally, our malware classification approach achieved an accuracy of over 97.9548%.

Our approach has the potential for wide adoption in API-based malicious behavior detection for Windows platforms, particularly for applications that currently rely on statistical information of API-based dynamic features but desire better robustness against unfavorable sequence interruption attacks.

### Acknowledgments

The authors would like to thank Dr. Kevin Alejandro Roundy, our shepherd, and the anonymous reviewers for their valuable feedback on this work. We also thank Loo Jia Yi from the Cyber Security Department at the Institute for Infocomm Research for evaluating the machine learning classifiers mentioned in this work.

### References

[1] Faraz Ahmed, Haider Hameed, M Zubair Shafiq, and Muddassar Farooq. 2009. Using spatio-temporal information in API calls with machine learning algorithms for malware detection. In Proceedings of the 2nd ACM workshop on Security and artificial intelligence. ACM, 55–62.

[2] Mamoun Alazab, Sitalakshmi Venkataraman, and Paul Watters. 2010. Towards understanding malware behaviour by the extraction of API calls. In 2010 Second Cybercrime and Trustworthy Computing Workshop. IEEE, 52–59.

[3] Eslam Amer and Ivan Zelinka. 2020. A dynamic Windows malware detection and prediction method based on contextual understanding of API call sequence. Computers & Security 92 (2020), 101760.

[4] Sergii Banin and Geir Olav Dyrkolbotn. 2018. Multinomial malware classification via low-level features. Digital Investigation 26 (2018), S107–S117.

[5] Amine Boukhtouta, Serguei A Mokhov, Nour-Eddine Lakhdari, Mourad Debbabi, and Joey Paquet. 2016. Network malware classification comparison using DPI and flow packet headers. Journal of Computer Virology and Hacking Techniques 12, 2 (2016), 69–100.

[6] Davide Canali, Andrea Lanzi, Davide Balzarotti, Christopher Kruegel, Mihai Christodorescu, and Engin Kirda. 2012. A quantitative study of accuracy in system call-based malware detection. In Proceedings of the 2012 International Symposium on Software Testing and Analysis. 122–132.

[7] Silvio Cesare and Yang Xiang. 2012. Software similarity and classification. Springer Science & Business Media.

[8] Steve Chamberlain and Cygnus Solutions. [n.d.]. Cygwin. https://cygwin.com/.

[9] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 785–794.

[10] Julia Yu-Chin Cheng, Tzung-Shian Tsai, and Chu-Sing Yang. 2013. An information retrieval approach for malware classification based on Windows API calls. In 2013 International Conference on Machine Learning and Cybernetics, Vol. 4. IEEE, 1678–1683.

[11] In Kyeom Cho and Eul Gyu Im. 2015. Extracting representative API patterns of malware families using multiple sequence alignments. In Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems. ACM, 308–313.

[12] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research 7, Mar (2006), 551–585.

[13] G DATA. [n.d.]. Malware Naming Hell Part 1: Taming the mess of AV detection names. https://www.gdatasoftware.com/blog/2019/08/35146-taming-the-mess-of-av-detection-names.

[14] Omid E David and Nathan S Netanyahu. 2015. Deepsign: Deep learning for automatic malware signature generation and classification. In 2015 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–8.

[15] Manuel Egele, Theodoor Scholte, Engin Kirda, and Christopher Kruegel. 2012. A survey on automated dynamic malware-analysis techniques and tools. ACM computing surveys (CSUR) 44, 2 (2012), 6.

[16] Seoungyul Euh, Hyunjong Lee, Donghoon Kim, and Doosung Hwang. 2020. Comparative Analysis of Low-Dimensional Features and Tree-Based Ensembles for Malware Detection Systems. IEEE Access 8 (2020), 76796–76808.

[17] Sanchit Gupta, Harshit Sharma, and Sarvjeet Kaur. 2016. Malware characterization using Windows API call sequences. In International Conference on Security, Privacy, and Applied Cryptography Engineering. Springer, 271–280.

[18] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter 11, 1 (2009), 10–18.

[19] John T. Haller. [n.d.]. Portable Apps. https://portableapps.com/.

[20] Weijie Han, Jingfeng Xue, Yong Wang, Lu Huang, Zixiao Kong, and Limin Mao. 2019. MalDAE: Detecting and explaining malware based on correlation and fusion of static and dynamic characteristics. Computers & Security 83 (2019), 208–233.

[21] Xiang Huang, Li Ma, Wenyin Yang, and Yong Zhong. 2020. A Method for Windows Malware Detection Based on Deep Learning. Journal of Signal Processing Systems (2020), 1–9.

[22] Kazuki Iwamoto and Katsumi Wasaki. 2012. Malware classification based on extracted API sequences using static analysis. In Proceedings of the Asian Internet Engineering Conference. ACM, 31–38.

[23] Arzu Gorgulu Kakisim, Mert Nar, Necmettin Carkaci, and Ibrahim Sogukpinar. 2018. Analysis and evaluation of dynamic feature-based malware detection methods. In International Conference on Security for Information Technology and Communications. Springer, 247–258.

[24] Youngjoon Ki, Eunjin Kim, and Huy Kang Kim. 2015. A novel approach to detect malware based on API call sequence analysis. International Journal of Distributed Sensor Networks 11, 6 (2015), 659101.

[25] Bojan Kolosnjaji, Apostolis Zarras, George Webster, and Claudia Eckert. 2016. Deep learning for classification of malware system call sequences. In Australasian Joint Conference on Artificial Intelligence. Springer, 137–149.

[26] Malwarebytes Labs. [n.d.]. 2020 State of Malware Report. https://resources.malwarebytes.com/files/2020/02/2020_State-of-Malware-Report.pdf.

[27] Malshare Labs. [n.d.]. Malshare Website. https://malshare.com/.

[28] Malwarebytes Labs. [n.d.]. Malware Types. https://www.malwarebytes.com/malware/.

[29] Andrea Lanzi, Davide Balzarotti, Christopher Kruegel, Mihai Christodorescu, and Engin Kirda. 2010. Accessminer: using system-centric models for malware protection. In Proceedings of the 17th ACM conference on Computer and communications security. 399–412.

[30] VirusTotal malware intelligence services. https://www.virustotal.com.

[31] Stephen Marsland. 2015. Machine learning: an algorithmic perspective. CRC press.

[32] Microsoft. [n.d.]. Overview of the Windows API. https://docs.microsoft.com/en-us/previous-versions//aa383723(v=vs.85).

[33] Andreas Moser, Christopher Kruegel, and Engin Kirda. 2007. Limits of static analysis for malware detection. In Twenty-Third Annual Computer Security Applications Conference (ACSAC 2007). IEEE, 421–430.

[34] Vinod P Nair, Harshit Jain, Yashwant K Golecha, Manoj Singh Gaur, and Vijay Laxmi. 2010. Medusa: Metamorphic malware dynamic analysis using signature from API. In Proceedings of the 3rd International Conference on Security of Information.